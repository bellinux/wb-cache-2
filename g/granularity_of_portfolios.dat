0|10000|Public
40|$|Credit risk {{models are}} {{developed}} {{and used to}} estimate capital requirements for agricultural lenders under the New Basel Capital Accord. The study uses credit value-at-risk methods to calculate probability of default, loss given default, and expected and unexpected losses. Two applied models, CreditMetrics and Moody's KMV, are estimated using farm financial data. The {{results show that the}} necessary capital for agricultural lenders under the New Basel Accord varies substantially depending on the riskiness and <b>granularity</b> <b>of</b> the <b>portfolio.</b> Copyright 2005, Oxford University Press. ...|$|R
40|$|A <b>portfolio</b> <b>of</b> {{non-performing}} loans needs economic capital. We present two models for forecasting the non-performing portfolio’s loss and derive the probability distribution. In the first model, the loss for each loan is a Gaussian random variable, {{and the risk}} determinants are the portfolio concentration, as well as systematic and idiosyncratic risk. Our second model allows for diversification with a performing portfolio, because an investor typically owns a combination of performing and {{non-performing loans}}. This model is a mixture model. For both models, formulae for the economic capital and the fair contribution of a single loan are given. We calibrate the models with times series data and a benchmark portfolio. Our main finding is that the credit <b>portfolio</b> risk <b>of</b> non-performing loans depends on the volatility of economic activity, on the <b>granularity</b> <b>of</b> the <b>portfolio</b> and on the performing portfolio. Finally, we compare the economic capital charges for nonperforming loans from our models with the regulatory capital charges of Basel II. The main difference is that our capital charges are sensitive to economic activity volatility, whereas the regulatory ones are not...|$|R
40|$|This paper {{investigates the}} limit {{properties}} of mean-variance (mv) and arbitrage pricing (ap) trading strategies using a general dynamic factor model, {{as the number}} of assets diverge to in 8 ̆ 5 nity. It extends the results obtained in the literature for the exact pricing case to two other cases of asymptotic no-arbitrage and the unconstrained pricing scenar-ios. The paper characterizes the asymptotic behaviour of the portfo-lio weights and establishes that in the non-exact pricing cases the ap and mv portfolio weights are asymptotically equivalent and, moreover, functionally independent of the factors conditional moments. By im-plication, the paper sheds light {{on a number of issues}} of interest such as the prevalence of short-selling, the number of dominant factors and the <b>granularity</b> property <b>of</b> the <b>portfolio</b> weights...|$|R
40|$|The {{ability to}} {{dynamically}} calculate and allocate credit risk Economic Capital (EC) at loan level {{is crucial for}} portfolio steering through risk-based pricing. The industry standard credit risk model is the multi-factor normal copula model, which is able to account for portfolio dynamics such as specific risk and overexposure in clusters of assets. Estimating this model's portfolio credit EC and asset-level marginal contributions using simulations is computationally involved. Analytical methods acquire these estimates in a much shorter time span {{at the cost of}} reduced accuracy. We correct and implement one such published analytical method which uses Hermite polynomials to calculate portfolio EC for credit risk. Within this framework we mathematically derive an analytical algorithm for asset-level marginal EC contributions. We benchmark the portfolio EC and marginal contributions given by the corrected analytical method to estimates given by simulations on several portfolios. This benchmark yields the accuracy of the analytical estimates increases with the <b>granularity</b> <b>of</b> the <b>portfolio.</b> Analytically computed risk contributions are more accurate than those generated by simulation when the simulation's running time is restricted to a couple of hours. These improved marginal contribution estimates enable more accurate asset-level risk-based performance measurement, taking into account all diversification benefits present in a credit portfolio...|$|R
50|$|Optical {{switches}} build {{by companies}} such as Sycamore and Ciena (with STS-1 <b>granularity</b> <b>of</b> switching) and Tellium (with STS-48 <b>granularity</b> <b>of</b> switching) have been deployed in operational mesh networks. Calient has built all-optical switches based on 3D MEMS technology.|$|R
30|$|Estimated at the <b>granularity</b> <b>of</b> a heartbeat.|$|R
50|$|Different XML Pipeline {{implementations}} support different <b>granularity</b> <b>of</b> flow.|$|R
40|$|Impact {{analysis}} is a specialized process of program comprehension that investigates {{the nature and}} extent of a planned software change. Traditionally, impact analysis inspects dependencies among the software components of a fixed granularity; these components constitute a dependency graph. In this paper, we argue that the single granularity is insufficient and leads to imprecise analysis. We explain how the precision can be improved by variable granularity, where the programmers choose among the <b>granularity</b> <b>of</b> classes, the <b>granularity</b> <b>of</b> class members, and the <b>granularity</b> <b>of</b> code fragments. We assess the resulting precision by a case study on open-source software. 1...|$|R
40|$|In {{this paper}} we {{proposed}} {{an analysis of}} the financial crisis impact on the procedures formanagement <b>of</b> loan <b>portfolios</b> in several banking systems. Despite ample liquidity injectionprograms implemented by major central banks and government actions, credit risk remains a keychallenge of the current banking systems. On a medium term, the high percentage of bad loans hasbecome a structural vulnerability. To maintain an acceptable quality <b>of</b> loan <b>portfolios</b> and not todamage the prudential and profitability indicators, credit institutions in EU member states haveproceeded to apply various techniques for credit restructuring. The quantitative analysis carried out inthe last part of the paper revealed a relatively moderate <b>granularity</b> <b>of</b> banking systems considered, interms of capitalization, volume of bank reserves and net provisions, in response to the persistent trendof loan portfolio deterioration...|$|R
40|$|The committed-choice {{language}} Fleng can extract much parallelism easily {{even from}} irregular programs using dataflow synchronization. However, {{there is a}} large overhead because the <b>granularity</b> <b>of</b> execution is very fine. If <b>granularity</b> <b>of</b> a program is coarsened, such an overhead can be reduced. This can be attained by fusing several goals into one goal, but this may cause deadlock. In this paper, we propose a safe goal fusion algorithm that statically optimizes <b>granularity</b> <b>of</b> a Fleng program. We implemented the algorithm and evaluated it on a parallel computer PIE 64. The evaluation shows that enough speedup can be attained by this method. 1 Introduction Committed-choice languages can extract much parallelism easily even from programs with irregular parallelism such as symbolic computation. This is achieved by single assignment variable and dataflow synchronization. However, the <b>granularity</b> <b>of</b> execution is so fine that overheads of parallel execution, such as context switching, synchro [...] ...|$|R
40|$|in {{terms of}} proliferation, {{increase}} {{in size and}} <b>granularity</b> <b>of</b> keratinocytes, from primary|$|R
5000|$|Determine {{the lowest}} level (<b>granularity)</b> <b>of</b> summary in a fact table (e.g. sales dollars).|$|R
50|$|Szabo influentially {{argued that}} a minimum <b>granularity</b> <b>of</b> micropayments is set by mental {{transaction}} costs.|$|R
5000|$|Timestamp <b>granularity</b> <b>of</b> 10 ms for Create and Modified times (but not {{as fine as}} NTFS's 0.1 ms).|$|R
40|$|Self-adjusting {{computation}} (SAC) {{provides an}} evaluation model where computations can respond automatically to modifications to their data {{by using a}} mechanism for propagating modifications through the computation. Current approaches to self-adjusting computation guarantee correctness by recording dependencies in a trace at the <b>granularity</b> <b>of</b> individual memory operations. Tracing at the <b>granularity</b> <b>of</b> memory operations, however, has some limitations: it can be asymptotically inefficient (e. g., compared to optimal solutions) because it cannot take advantage of problem-specific structure, it requires keeping a large computation trace (often proportional to the runtime of the program on the current input), and it introduces moderately large constant factors in practice. In this paper, we extend dependence-tracing {{to work at the}} <b>granularity</b> <b>of</b> the query and update operations of arbitrary (abstract...|$|R
50|$|Provia 400X RXP used what Fujifilm {{described}} as Epitaxial Sigma Crystal (ESC) technology {{to achieve a}} <b>granularity</b> <b>of</b> RMS11.|$|R
5000|$|For {{procedural}} programming, the <b>granularity</b> <b>of</b> {{the code}} is largely {{determined by the}} number of discrete procedures or modules.|$|R
5000|$|If G-bit is zero limit has a <b>granularity</b> <b>of</b> 1 byte, i.e. segment size may be 1, 2, ..., 220 bytes.|$|R
40|$|Generally speaking, digital {{libraries}} have multiple <b>granularities</b> <b>of</b> semantic units: book, chapter, page, paragraph and word. However, {{there are two}} limitations of current eBook retrieval systems: (1) the <b>granularity</b> <b>of</b> retrievable units is either too big or too small, scales such as chapters, paragraphs are ignored; (2) the retrieval results should be grouped by facets to facilitate user’s browsing and exploration. To overcome these limitations, we propose a multi-granularity multi-facet eBook retrieval approach...|$|R
3000|$|..., to {{calculate}} the available bandwidth accurately without depending on the coarse clock <b>granularity</b> <b>of</b> TCP. So, TCP TIBET has an [...]...|$|R
40|$|Abstract — To {{support service}} {{guarantees}} in packetswitched networks, three approaches {{have been proposed}} and studied in the literature. They are the Stateless Core (SCORE) approach, the Integrated Services (IntServ) approach, and the Differentiated Services (DiffServ) approach. The <b>granularities</b> <b>of</b> service guarantees provided by these approaches at each router are respectively packet level, flow level, and class level. I n this paper, we propose a novel approach, called Link-Based Fair Aggregation (LBFA) approach to scalable support of service guarantees. While the <b>granularity</b> <b>of</b> service guarantees supported by LBFA is link level at each router, we show through analysis that the proposed LBFA approach can achieve as good as or even better per-flow service guarantees than the current three approaches. Keywords — Per-flow service guarantees, Link based fair aggregation (LBFA), <b>Granularity</b> <b>of</b> service guarantees I...|$|R
3000|$|... r The {{worst case}} (i.e. No Traffic) {{is not present}} so that a finer <b>granularity</b> <b>of</b> the {{presented}} simulations can be shown.|$|R
30|$|Hierarchy aspects: {{the aspect}} {{extraction}} technique {{described in the}} “Extracting aspects through hierarchy clustering” section, using the topic <b>granularity</b> <b>of</b> [2, 7].|$|R
50|$|Theoretically, {{coherence}} can {{be performed}} at the load/store granularity. However, in practice it is generally performed at the <b>granularity</b> <b>of</b> cache blocks.|$|R
40|$|Granularity is an {{integral}} feature of temporal data. For instance, a person's age is commonly given to the <b>granularity</b> <b>of</b> years {{and the time of}} their next airline flight to the <b>granularity</b> <b>of</b> minutes. A <b>granularity</b> creates a discrete image, in terms of granules, of a (possibly continuous) time-line. Pairs <b>of</b> <b>granularities</b> are related in that some granularities are finer or coarser with respect to other granularities. A granularity graph records all the relationships between granularities, even among granularities in different calendars. Indeterminacy, or "don't know when" information, is a companion to granularity. A birthday, given to the <b>granularity</b> <b>of</b> days, does not reveal precisely when a person was born, only that they were born sometime during the indicated day. Temporal granularity and indeterminacy are {{two sides of the same}} coin, in that a time at a given granularity is indeterminate at all finer granularities. We present a formal model for granularity in temporal operations. [...] ...|$|R
40|$|In {{this paper}} we propose a {{framework}} of verb semantic description in order to organize different <b>granularity</b> <b>of</b> similar-ity between verbs. Since verb mean-ings highly depend on their arguments we propose a verb thesaurus on the ba-sis of possible shared meanings with predicate-argument structure. Motiva-tions of this work are to (1) construct a practical lexicon for dealing with alter-nations, paraphrases and entailment re-lations between predicates, and (2) pro-vide a basic database for statistical learn-ing system as well as a theoretical lex-icon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several <b>granularities</b> <b>of</b> semantic classes to characterize verb meanings. The thesaurus form allows us to provide several <b>granularities</b> <b>of</b> shared meanings; thus, this gives us a further re-vision for applying more detailed analy-ses of verb meanings. ...|$|R
40|$|This paper {{develops}} a connection establishment framework for protecting connections against single-link failures using link protection at the <b>granularity</b> <b>of</b> a connection, {{referred to as}} Connection Switched Link Protection (CSLP). As a connection is routed only around a failed link, the channel assignment for the connection on the backup path of the failed link must be consistent {{with that of the}} primary path. Such a consistency is guaranteed at the time of call admission. The advantages of employing link protection at the connection level is established by comparing its performance through extensive simulations against link protection at the <b>granularity</b> <b>of</b> a fiber, referred to as Fiber Switched Link Protection (FSLP). Link protection at the connection level is shown to significantly outperform that at the <b>granularity</b> <b>of</b> a fiber, specifically when some traffic requires protection while others do not...|$|R
5000|$|In general, {{there is}} a wide range of {{possible}} definitions of functional equivalence covering comparisons between different levels of abstraction and varying <b>granularity</b> <b>of</b> timing details.|$|R
40|$|An {{important}} presupposition for HW/SW partitioning are sophisticated estimation algorithms at a {{high level}} of abstraction that obtain high quality results. Therefore the <b>granularities</b> <b>of</b> estimation and partitioning have to be adapted adequately. In this paper we discuss the effects that arise when the <b>granularities</b> <b>of</b> partitioning and estimation are not adapted in a necessary way. Furthermore we present our solution that allows to choose different levels <b>of</b> <b>granularities</b> adapted to the estimation and partitioning phase. The experiments show that this refinement in estimation {{at a high}} level of abstraction leads to an inprovement (in terms of run-time and chip area) of the whole mixed HW/SW system...|$|R
40|$|For many {{applications}} of scalable multicomputers with distributed memory {{it is desirable}} to provide transparent shared virtual memory. For such applications, the hardware and system software must maintain coherency among the local memories. Most existing coherency schemes for multicomputers manage memory uniformly at a single <b>granularity</b> <b>of</b> fixed size pages or cache blocks, leading to unnecessarily high storage overhead and poor performance. Coherency management at the <b>granularity</b> <b>of</b> pages (thousands of bytes), results in unnecessary network traffic. At the <b>granularity</b> <b>of</b> cache blocks (tens of bytes), for large systems unacceptably large mapping tables are needed. We propose {{a solution to this}} problem using hierarchical management, where mapping and transfer at the block level are done only when necessary [...] - during the time that the page is actually shared. When pages are not shared, they do not require more space for mapping tables than uniform page-level schemes. A detailed descript [...] ...|$|R
30|$|Flexibility: the <b>granularity</b> <b>of</b> service {{could be}} either coarse grained or fine grained {{according}} to particular requirements. It means a high flexibility of choices for integration blocks.|$|R
30|$|We use the TTNoC global time [24] as {{the time}} base for all timestamps. In our design the <b>granularity</b> <b>of</b> this time base is 2 - 21 s.|$|R
40|$|We {{investigate}} how the <b>granularity</b> <b>of</b> POS tags influences POS tagging, and furthermore, how POS tagging performance relates to parsing results. For this, {{we use the}} standard “pipeline” approach, in which a parser builds its output on previously tagged input. The experiments are performed on two German treebanks, using three POS tagsets <b>of</b> different <b>granularity,</b> and six different POS taggers, together with the Berkeley parser. Our findings show that less <b>granularity</b> <b>of</b> the POS tagset leads to better tagging results. However, both too coarse-grained and too fine-grained distinctions on POS level decrease parsing performance. ...|$|R
5000|$|The <b>granularity</b> <b>of</b> data {{refers to}} the size in which data fields are sub-divided. For example, a postal address can be recorded, with coarse granularity, as a single field: ...|$|R
40|$|In this article, {{we study}} on the effect <b>of</b> {{different}} congregated <b>granularities</b> <b>of</b> microencapsulated n-octadecane on polyurethane foamed system, {{and the making of}} the polyurethane foams with high contents of microencapsulated n-octadecane. The results show that, when relative low <b>granularities</b> <b>of</b> microencapsulated phase change materials (MicroPCMs) are used as additives, the remnant water and formaldehyde have adverse effects on the foam, so the MicroPCMs should better be heat-treated which is helpful for removing the residual, and more uniform foam can be obtained when relative low <b>granularities</b> <b>of</b> MicroPCMs are used. The polyurethane foams with maximum contents of 28 parts of n-octadecane can be made when the polyethylene glycol- 400 (PEG- 400) is interfused in the foamed system to regulate the viscosity of the system. The formed foams absorb heat at about 27 ? and dissipate heat at about 25 ?, which heat storage is bigger than 18 J/g and can be used as thermal insulation materials. </p...|$|R
50|$|Deep ocean sediment: A 1999 {{reconstruction}} of ocean current patterns {{based on the}} <b>granularity</b> <b>of</b> deep ocean sediment concluded there was a Roman Warm Period that peaked around AD 150.|$|R
