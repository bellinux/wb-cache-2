2|10000|Public
50|$|In the 1970s other {{programs}} have been started, such as electronics, electrotechnics, designing of tools and machining fixtures, and industrial laboratory technician. The Electronic <b>data</b> <b>processing</b> <b>course</b> {{has been established in}} 1985.|$|E
40|$|With {{the current}} {{increase}} of user generated data, {{the need for}} tools to process large quantities of data is increasing. One of the courses of the Computer Science BSc curriculum is the Big <b>Data</b> <b>Processing</b> <b>course.</b> The Big <b>Data</b> <b>Processing</b> <b>course</b> teaches students ways of doing so. A popular and teached method is using MapReduce, a programming model to process large quantities of data. Big Data Processing students currently write their implementations for MapReduce related assignments of the lab in the Cloudera Virtual Machine (VM). This VM is slow, cannot be used interactively and it cannot be used to teach all MapReduce principles like memory separation. Since there are no existing solutions that solves these problems we have decided to write our own. Writing our own solution required diverse knowledge including {{but not limited to}} JavaScript, ECMAScript 6, WebWorkers, HTML 5 (&CSS), PHP, MySQL, API development, Wordpress, Linux, QUnit and Selenium. Our solution, Trifle, overcomes previously mentioned and other shortcomings. Trifle is a web-based solution that simulates MapReduce within the browser. The framework uses JavaScript together with WebWorkers and our front-end is written using Wordpress. By simulating MapReduce within the browser we managed to create a solution that requires no cluster, is easy to use, works multi-platform and most importantly enables lecturers to teach some MapReduce principles that could not be taught before. Furthermore, we integrated a submissions system {{that can be used to}} do interactive lectures in which the lecturer can see problems in real-time and explain obstacles. A user test we have done verifies that Trifle helps to better grasp the idea behind MapReduce. Electrical Engineering, Mathematics and Computer ScienceIntelligent SystemsWeb Information System...|$|E
40|$|In {{determining}} {{the effect of}} computer assisted instructional method on students’ performance, two groups of student {{for a period of}} two semesters in the introduction to computer science and electronic <b>data</b> <b>processing</b> <b>courses</b> offered at the department of computer science, Adeniran Ogunsanya College of Education were used as samples. One group was taught with conventional teaching method and the other with computer assisted instruction software package. As this was the first attempt in deploying CAI in teaching computer concepts in the department, it was primarily employed as educational means of teaching with CAI software. This paper reports a personal experience and a case study of implementing computer assisted instructional method and the effect it has on students’ performance in the course. Through hypotheses testing, it is clearly possible that employing computer assisted instructional method in educational settings proves to have significant effect on students’ performanc...|$|R
50|$|Cross-disciplinary co-operation, the {{strengthening}} of key competences, project orientation, and a flexible study programme are the hallmarks of these new degree programmes, preparing students for many modern career options. For international students of law, the university offers an LL.M. course in German Law. All students, irrespective of their specialisation, are offered a number of programmes which {{can be taken to}} complement their main field of study. Among these are <b>data</b> <b>processing,</b> general language <b>courses,</b> specialised language courses (business, law etc.), courses in oral presentation and communication, and a programme in intercultural communication.|$|R
40|$|The 1999 {{annual report}} of the Max-Planck-Institut fuer extraterrestrische Physik (MPE) {{provides}} information about the current research projects of the institute, its activities in 1999 and about our plans for the future. The MPE {{has been involved in}} front-line space research since its foundation in 1963. Our goal is to make significant contributions to the exploration of the universe and to basic research in physics. This involves the development and use of efficient telescopes and instruments, the application of modern computer technology for the <b>data</b> <b>processing</b> and of <b>course</b> the search for new insights and the gain of new knowledge by interpretation of the data. These experimental activities are complemented by theoretical work. Another aspect of our work is the technology transfer into other areas of fundamental research or into industrial applications. Our past successes are the basis for plans which extend far into the future. (orig.) SIGLEAvailable from TIB Hannover: RA 234 :ET(273) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|The {{purpose of}} this {{dissertation}} {{was to develop a}} learning instrument, to be used by programmers preparing for the <b>Data</b> <b>Processing</b> Management Association Test as a self study book, or by college business programming and computer science students who have completed a <b>course</b> in <b>data</b> <b>processing</b> and a <b>course</b> in programming a higher level language. The mathematical ability requirement was minimized by developing the algorithms in parallel with the programs. The learner should experience _emphasis in the following. areas: l. The type of activities required to pass the DPMA test (the programming part) 2. Data Structures 3. Fortran (at the level of the DPMA test) I 4. RPG (at the level of the DPMA test) 5. Flow chart reading and writing Fortran and RPG (Report Program Generator) languages were used, since their proficiency is required for the DPMA test; however a subset of IBM Basic Assembler language was used, because the author believed that a person who is more than superficially interested in computers should demonstrate a proficiency with a machine language. An important part of this method of presentation are the cassette recordings which allow the learner to progress outside the classroom. The recordings plus the hard copy of the actual programs, diminished in size, give the learner material which he can move to any location and study without the presence of the instructor...|$|R
40|$|Description: This course {{introduces}} basic technology (algorithms, architectures, systems) {{and advanced}} research topics {{in connection with}} large-scale data management and information extraction techniques for big data. The course will start by introducing Big data models, databases and query languages, and cover modern distributed database systems and algorithms and Big data systems adopted in industry and science applications. Implementation of a distributed database on a standalone machine will be covered and students will learn how {{to build their own}} database for big data. Distributed storage and parallel processing and architectures that support data analytics will be examined, and students will learn how to implement a distributed <b>data</b> <b>processing</b> system. The <b>course</b> will also cover critical topics in mining and knowledge discovery of big data, with applications in social analytics, cyber security, and information networks, among others that are already in public eye. o Big Data: concept, research problems, hot research trends and emerging applications o Big Data models: Basics on relational data models and semi-structured data; o Graph Data: Graph data models; basics on graph theory o From Data to Knowledge: a knowledge discovery process...|$|R
30|$|The <b>data</b> <b>processing</b> {{during the}} <b>course</b> {{of a typical}} {{experiment}} is illustrated in Fig.  1, for {{the case of a}} weakly first-order transition, a transition with a small latent heat and considerable pre-transitional contributions, as commonly observed in liquid crystals [2]. Starting at a time t_ 0, a constant power P is applied to the sample, leading to an evolution of its temperature T(t). In the low-temperature phase, the temperature increases, but gradually slower when approaching the transition. During the phase transition between t_i and t_f, all added power is used for the phase conversion and the temperature does not change. Once the conversion is complete and the high-temperature phase is reached, the temperature increases again. Via numerical differentiation, the rate Ṫ is calculated from T(t), displaying the decreasing value close to the transition; the rate becomes zero at the transition. Division of P and Ṫ leads to C_p, showing the increase in the pre-transitional wings. In the enthalpy, the phase transition is visible as the jump, of which the height corresponds to the latent heat. In order to arrive at the specific values, the pre-calibrated contribution from the sample addenda needs to be subtracted and the sample mass taken into account.|$|R
40|$|In some {{applications}} such as quality control of metal components or inspection of large metal structures, inspection systems with rapid scanning capability are required. Of particular interest are systems capable of real-time detection and sizing of surface breaking cracks in metals. Since such systems have to perform the <b>data</b> <b>processing</b> in the <b>course</b> of scanning, they {{should be based on}} NDE techniques with easy detection and inversion capabilities. One technique with these virtues is the alternating current field measurement (ACFM) technique [1]. In this technique, a high frequency uniform field is made incident on the work-piece and the resulting surface potential is sampled using a two-leg contacting probe. The crack signal is essentially a rectangular pulse, Fig. 1. For long cracks it can be shown that the crack depth can be determined from the crack signal using the simple expression (1) d=V 2 −V 1 V 1 ⋅Δ 2 where V 1 and V 2 are voltages picked up by the probe just before and just after the crack, Fig. 1, and Δ is the probe length. For a non-uniform crack or a non-uniform incident current, however, the above expression is not accurate. In such cases the measured depth dm resulting from (1) should be modified using a multiplier. Multipliers are based on mathematical models relating dm and d for different crack shapes and for different incident fields [1]...|$|R
40|$|This is the {{accepted}} manuscript. The final version {{is available from}} Winchester University Press via [URL] case note comments on {{the judgment of the}} Court of Justice of the European Union (CJEU) in C- 212 / 13 Ryneš v Úrad pro ochranu osobních údaju. It argues that the CJEU has imposed a spatial logic on the interpretation of the exception for <b>data</b> <b>processing</b> “in the <b>course</b> of a purely personal or household activity”: Article 3 (2) of the Data Protection Directive 1995. It criticises that spatial logic as both too broad and too narrow. Too broad because the logic of the decision potentially captures many other forms of video-based recording and too narrow because it appears to exclude data protection from CCTV in the purely private setting, ignoring circumstances where individuals from outside the household might legitimately enter and be subject to intrusive monitoring. It examines the consequences of that logic for UK data protection law and guidance issued by the UK Information Commissioner’s Office (ICO). This note argues that the implications of the reasoning in Ryneš could extend beyond these narrower changes and represent part of wider expansion of data protection through the interpretation of the CJEU, which has pursued a course of broad interpretation for provisions of the Directive and narrow interpretations of its exceptions. The note questions the desirability of this extension...|$|R
5000|$|VisionTutor: Online image <b>processing</b> <b>course</b> {{including}} all the theory and application macro commands that are compatible with Aphelion.|$|R
40|$|<b>Data</b> <b>Processing</b> {{discusses}} the principles, practices, and associated tools in <b>data</b> <b>processing.</b> The book {{is comprised of}} 17 chapters that are organized into three parts. The first part covers the characteristics, systems, and methods of <b>data</b> <b>processing.</b> Part 2 deals with the <b>data</b> <b>processing</b> practice; this part {{discusses the}} data input, output, and storage. The last part discusses topics related to systems and software in <b>data</b> <b>processing,</b> which include checks and controls, computer language and programs, and program elements and structures. The text will be useful to practitioners of computer-re...|$|R
40|$|This paper {{proposes a}} {{conceptual}} matrix model with algorithms for biological <b>data</b> <b>processing.</b> The required elements for constructing a matrix model are discussed. The representative matrix-based methods and algorithms which have potentials in biological <b>data</b> <b>processing</b> are presented / proposed. Some application {{cases of the}} model in biological <b>data</b> <b>processing</b> are studied, which show the applicability of this model in various kinds of biological <b>data</b> <b>processing.</b> This conceptual model established a framework within which biological <b>data</b> <b>processing</b> and mining could be conducted. The model is also heuristic to other applications. <br /...|$|R
30|$|Quality <b>data</b> <b>processing</b> defines {{configuration}} {{and processing}} parameters, which are utilized {{in the evaluation}} process of quality attributes. Examples of configuration parameters include location of binary or configuration files, and environmental execution parameters (e.g. number of CPU cores, size of memory). Processing parameters refer to input data, which should be provided to <b>data</b> <b>processing</b> executables (e.g. Spark streaming). Supported quality <b>data</b> <b>processing</b> defines processing, which can be performed with a specific <b>data</b> <b>processing</b> tool. Especially, it can be specified what kind of quality attributes for a data source can be evaluated with a specific <b>data</b> <b>processing</b> tool.|$|R
40|$|Abstract. UnifiedViews is an Extract-Transform-Load (ETL) frame-work {{that allows}} users – publishers, consumers, or analysts – to define, execute, monitor, debug, schedule, and share RDF <b>data</b> <b>processing</b> tasks. The <b>data</b> <b>processing</b> tasks may use custom plugins created by users. UnifiedViews differs from other ETL {{frameworks}} by natively supporting RDF data and ontologies. The practical demonstration of UnifiedViews {{at the conference}} will (1) clearly demonstrate how UnifiedViews helps RDF/Linked Data users with RDF <b>data</b> <b>processing</b> (2) and show the real instance of UnifiedViews with tens of <b>data</b> <b>processing</b> tasks and DPUs motivated by real <b>data</b> <b>processing</b> use cases. ...|$|R
40|$|Significant {{numbers of}} {{physicians}} are using <b>data</b> <b>processing</b> services {{and a large}} number of firms are offering an increasing variety of services. This paper quantifies user dissatisfaction with office practice <b>data</b> <b>processing</b> systems and analyzes factors affecting dissatisfaction in large group practices. Based on this analysis, a proposal is made for a more structured approach to obtaining <b>data</b> <b>processing</b> services in order to lower the risks and increase satisfaction with <b>data</b> <b>processing...</b>|$|R
40|$|DE 102007026480 A 1 UPAB: 20081222 NOVELTY - The method {{involves}} attaching mobile <b>data</b> <b>processing</b> {{units to}} collection containers contained with products to be commissioned. The <b>data</b> <b>processing</b> units command over micro-controllers, local memory, sensor interfaces and wireless communication devices. The mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> {{system in a}} commissioning controlling system to identify the current collection containers. The numbers of products, which can be inferred, are indicated. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for the execution of a method for the commissioning of goods. USE - Method for the commissioning of goods. ADVANTAGE - The method involves attaching mobile <b>data</b> <b>processing</b> units to collection containers contained with products to be commissioned, where the mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> system in a commissioning controlling system to identify the current collection containers, and hence ensures to simplify the commissioning work by retrofitting the existing stock option and shelving systems...|$|R
5000|$|Pure {{communications}} and pure <b>data</b> <b>processing</b> {{have very different}} characteristics that led to different policy results. The markets that the technology existed on assisted the FCC make its policy decisions. [...] "The pure <b>data</b> <b>processing</b> market was viewed as an innovative, competitive market with low barriers to entry and little chance of monopolization." [...] The FCC established that no additional regulation or safeguards where required for the pure <b>data</b> <b>processing</b> market. The pure communications market {{on the other hand}} was being managed by an incumbent monopoly. The FCC had four concerns about the incumbent telephone companies which were: [...] "the sale of <b>data</b> <b>processing</b> services by carriers should not hurt the provision of common carrier services, the costs of such <b>data</b> <b>processing</b> services should not be passed on to telephone rate payers, revenues derived from common carrier services should not be used to cross subsidize <b>data</b> <b>processing</b> services, and the furnishing of such <b>data</b> <b>processing</b> services by carriers should not hurt the competitive computer market." ...|$|R
40|$|Includes bibliographical {{references}} (pages 38 - 39) An {{attempt was}} made to determine the difference between the retention of material obtained by a student studying in a classroom lecture environment as compared to a student who studies using a programmed instruction course. The methodology was to select two courses in computer fundamentals from courses offered by the Lockheed Evening Education Program of the Lockheed California Company, Burbank, California. One was a twenty-eight hour lecture course entitled ???Introduction to <b>Data</b> <b>Processing???.</b> The other <b>course,</b> ???Computer System Fundamentals???, was a programmed instruction course that averaged 23 hours to complete. After a random time lapse since training, a selected sample of students who had passed the original course were asked to complete the original summative evaluation again. Other selected data were collected and the tests were scored using the standardized scales used by the original instructors. 	An analysis of covariance was computed using ANCOVA, a special computer program particularly designed for statistical evaluation of this type. Analysis was made of the variance between the difference scores for each student against the time interval, and also between the two groups. Other analyses were made also and, even though the sample size was small and uneven, interesting trends became visible. 	The analysis of covariance showed the effect of the time difference as being a determining factor on retention but the main effect of classroom instruction versus programmed instruction was not significant...|$|R
50|$|The IEA <b>Data</b> <b>Processing</b> and Research Center (DPC) is the <b>data</b> <b>processing</b> and {{research}} department of IEA, located in Hamburg, Germany.|$|R
40|$|Moore's law {{was first}} {{postulated}} in 1968, and it loosely {{says that the}} cost of making calculations on a computer falls by 50 % each year. Securities markets are, in essence, a form of <b>data</b> <b>processing.</b> Consequently, Moore’s law has driven important changes in those markets over the past forty years. Faster <b>data</b> <b>processing</b> was essential for major changes in securities trading. Increased turnover of portfolios was a result of faster <b>data</b> <b>processing.</b> Consequently, the criticism of that turnover may be misplaced. The effectiveness of regulatory changes, such as the lowering of brokerage commissions and the reduction in bid ask spreads, depended on reduced <b>data</b> <b>processing</b> costs, that is on Moore's Law. Deregulation of brokerage commissions could not have reduced rates by as much as it did if we had not had decreasing costs of <b>data</b> <b>processing.</b> The reduction in bid ask spreads which followed decimalization of securities quotes depended on improved <b>data</b> <b>processing.</b> Continued reductions in <b>data</b> <b>processing</b> costs will require a new regulatory approach. Regulators should consider the improvements in <b>data</b> <b>processing</b> and <b>data</b> transmission when they establish capital requirements and haircuts. ...|$|R
40|$|<b>Data</b> <b>processing</b> complexity, partitionability, {{locality}} and provenance play {{a crucial}} role in the effectiveness of distributed <b>data</b> <b>processing.</b> Dynamics in <b>data</b> <b>processing</b> necessitates effective modeling which allows the understanding and reasoning of the fluidity of <b>data</b> <b>processing.</b> Through virtualization, resources have become scattered, heterogeneous, and dynamic in performance and networking. In this paper, we propose a new distributed <b>data</b> <b>processing</b> model based on automata where <b>data</b> <b>processing</b> is modeled as state transformations. This approach falls within a category of declarative concurrent paradigms which are fundamentally different than imperative approaches in that communication and function order are not explicitly modeled. This allows an abstraction of concurrency and thus suited for distributed systems. Automata give us a way to formally describe <b>data</b> <b>processing</b> independent from underlying processes while also providing routing information to route data based on its current state in a P 2 P fashion around networks of distributed processing nodes. Through an implementation, named Pumpkin, of the model we capture the automata schema and routing table into a <b>data</b> <b>processing</b> protocol and show how globally distributed resources can be brought together in a collaborative way to form a <b>processing</b> plane where <b>data</b> objects are self-routable on the plane...|$|R
40|$|This paper {{reviews the}} {{historical}} development of <b>data</b> <b>processing,</b> discerning three approximate decades of distinct evolutionary cycles. Each cycle {{is seen as}} forking, two contrasting styles of <b>data</b> <b>processing</b> forming and separating during the cycle. On this basis, a classification of the major general areas of <b>data</b> <b>processing</b> is suggested. For each decade, characteristic aspects are discussed and both the lines of development are described. Finally, some observations regarding the present decade and its requirements are given, and some predictions relating to the next decade and its prerequisites are made. DESCRIPTORS: Classification of <b>data</b> <b>processing.</b> Evolution of <b>data</b> <b>processing.</b> Philosophical implications. Computing milieu. Terminology. CR CATEGORIES: 1. 2, 1. 3, 2. ...|$|R
40|$|A {{system for}} {{assessing}} vestibulo-ocular function includes a motion sensor system adapted to be coupled to a user's head; a <b>data</b> <b>processing</b> system configured {{to communicate with}} the motion sensor system to receive the head-motion signals; a visual display system configured {{to communicate with the}} <b>data</b> <b>processing</b> system to receive image signals from the <b>data</b> <b>processing</b> system; and a gain control device arranged to be operated by the user and to communicate gain adjustment signals to the <b>data</b> <b>processing</b> system...|$|R
5000|$|Mivar-based {{technology}} of <b>data</b> <b>processing</b> {{is a method}} of creating logical inference system or automated algorithm construction from modules, services or procedures {{on the basis of}} active trained mivar network of rules with the linear computational complexity. Mivar-based {{technology of}} <b>data</b> <b>processing</b> is designed for <b>data</b> <b>processing</b> including logical inference, computational procedures and services.|$|R
5000|$|Roger Lee Sisson (June 24, 1926 [...] - [...] January 22, 1992) was {{an early}} <b>data</b> <b>processing</b> pioneer. Sisson worked on Project Whirlwind while a {{graduate}} student at MIT, co-founded the first consulting firm devoted to electronic <b>data</b> <b>processing,</b> and published a number of the earliest books and periodicals on computers and <b>data</b> <b>processing.</b>|$|R
40|$|Fourier {{transform}} spectrometry {{is a type}} {{of novel}} information obtaining technology, which integrated the functions of imaging and spectra, but the data that the instrument acquired is the interference data of the target, which is an intermediate data and couldn&# 39;t be used directly, so <b>data</b> <b>processing</b> must be adopted for the successful application of the interferometric data. In the present paper, <b>data</b> <b>processing</b> techniques are divided into two classes: general-purpose and special-type. First, the advance in universal interferometric <b>data</b> <b>processing</b> technique is introduced, then the special-type interferometric data extracting method and <b>data</b> <b>processing</b> technique is illustrated according to the classification of Fourier transform spectroscopy. Finally, the trends of interferogram <b>data</b> <b>processing</b> technique are discussed...|$|R
50|$|The term <b>Data</b> <b>Processing</b> (DP) {{has also}} been used {{previously}} {{to refer to a}} department within an organization responsible for the operation of <b>data</b> <b>processing</b> applications.|$|R
40|$|A {{field survey}} was {{conducted}} to study the factors {{associated with the use}} of <b>data</b> <b>processing</b> charge-back information in organizations. The aim of this research was to identify the organizational and budgetary characteristics associated with how the output of a chargeback system is used by user-managers to control their <b>data</b> <b>processing</b> costs. It was found that involvement in budget preparation, accountability for meeting the <b>data</b> <b>processing</b> budget, and cost variability of the charges were the- most important factors to consider when designing <b>data</b> <b>processing</b> chargeback systems...|$|R
40|$|Scientific {{workflow}} {{systems have}} become a necessary tool for many applications, enabling the composition and execution of complex analysis. CO(2) flux data observed by eddy covariance technique is large in quantity and the procedure of flux data is complex, scientific workflow technique plays {{a very important role}} in the sharing, reusing and automatic calculation of flux <b>data</b> <b>processing</b> method. In this paper, we discuss the feasibility and validity of applying scientific workflow technique to flux <b>data</b> <b>processing</b> and make a tentative approach to construct a scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> by taking Kepler scientific workflow system as the development platform. CO(2) flux data of Changbai Mountain in 2003 is used to verify the scientific workflow system. The results show that scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> can solve many problems of too much multifarious calculation, inconsistent development platform and complicated procedure in flux <b>data</b> <b>processing.</b> This approach indicates that the scientific workflow system applied to CO(2) flux <b>data</b> <b>processing</b> can provide an automatic calculation platform for flux <b>data</b> <b>processing</b> and prompt the communication and sharing of international flux <b>data</b> <b>processing</b> method, which make it easier for scientists to focus on their research and not computation management...|$|R
40|$|In this {{research}} aims {{to change the}} personnel staffing <b>data</b> <b>processing,</b> which is stillperformed in the conventional / manual, {{with the help of}} computer hardware for fastdata processing, the company makes computerized <b>data</b> <b>processing</b> personnel in orderto accelerate the process of doing <b>data</b> <b>processing</b> personnel and presenting reports -reports / information to the right, quickly and accurately...|$|R
40|$|In {{carrying}} out real work practices and preparing this paper, the authors obtain abroader knowledge about <b>data</b> <b>processing</b> {{system on the}} computerized system and cancompare with the <b>data</b> <b>processing</b> system manually. Given the <b>data</b> <b>processing</b> systemwith a computerized warehouse section can then be expected to facilitate thewarehouse stock control of goods entering derta out systematically and efficiently aspossible...|$|R
40|$|PREFACE Many {{books and}} {{articles}} generally recognize that management personnel other {{than those in the}} <b>data</b> <b>processing</b> installation need to be oriented towards <b>data</b> <b>processing</b> if effective utilization of these costly computers is to evolve. Present emphasis in the U. S. Marine Corps is on training of <b>data</b> <b>processing</b> personnel with limited orienta-tion or training of other managerial personnel in the capabilities and limitations of computers. The author carries an additional military occupational specialty as a <b>Data</b> <b>Processing</b> Officer and has been involved with Marine Corps <b>data</b> <b>processing</b> for a number of years. He found this lack of training of other officers a perplexing problem to him in the conduct of everyday tasks. It soon became evident to him that all officers dealing with him should have adequate training in <b>data</b> <b>processing.</b> This problem has been recognized by some Marine Corps officials, but to date a standard syste...|$|R
5000|$|IT audits {{are also}} known as [...] "automated <b>data</b> <b>processing</b> (ADP) audits" [...] and [...] "computer audits". They were {{formerly}} called [...] "electronic <b>data</b> <b>processing</b> (EDP) audits".|$|R
5000|$|Smagorinsky, J., 1965: Remarks on <b>data</b> <b>processing</b> in meteorology. In, Proceedings of the WMO/IUGG Symposium on Meteorological <b>Data</b> <b>Processing,</b> Brussels, Belgium, WMO Technical Note 73, pp. 1-2.|$|R
40|$|The {{tremendous}} impact of electronic <b>data</b> <b>processing</b> {{on the lives}} of the American people is being felt in every segment of our economy. Regardless of the area of activity, be it business, education, government, industry, or the service areas, electronic <b>data</b> <b>processing</b> is performing an increasingly· important function. The volume of paper work, the magnitude of government reports, and the demands of management for information, have created mammoth challenges for electronic <b>data</b> <b>processing.</b> These challenges are complex. However, electronic <b>data</b> <b>processing</b> equipment and personnel are making favorable gains on the problems posed by these challenges...|$|R
