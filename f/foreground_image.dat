69|325|Public
500|$|Other {{references}} to the first game include Alice examining the mansion by going outside; crows are visible for a moment. Crows are minor enemies that the player encounters throughout each game. Alice finds {{a picture of her}} wedding day with Spence, which is the same style as the photos in the first version of the Resident Evil game: in black and white with the <b>foreground</b> <b>image</b> noticeably spliced onto the background. On the newspaper {{at the end of the}} film, the words [...] "Horror in Raccoon City! More Victims Dead!" [...] are shown in the upper right corner. This is a reference to the same newspaper in the censored opening of the first Resident Evil game and the prologue chapter for the [...] novel. Near the beginning of the film, Alice examines a statue after the wind blows its cover off. This statue is similar in design to one in the mansion of the first game.|$|E
50|$|The Primatte chromakey {{algorithm}} {{is a method}} of color space segmentation, where Primatte segments all the colors in the <b>foreground</b> <b>image</b> into one of four separate categories. The result is a 'spill suppressed' <b>foreground</b> <b>image</b> and a matte {{which is used to}} apply the modified foreground to a suitable background.|$|E
50|$|For the memorial, Hamilton {{employed}} a technique known as relief {{in which the}} <b>foreground</b> <b>image</b> juts out from the background. Each side demonstrates both high relief (where the projection from the background {{is much greater than}} low relief) and low relief to display secondary objects, including the faces of other captives and abolitionists. The three portraits of Cinqué are sculpted in high relief.|$|E
40|$|Videos taken under fog {{suffer from}} {{degradation}} such as severe contrast loss. Unfortunately, that effect of fog cannot {{be overcome by}} simple image processing techniques. In this paper, a novel method for the contrast enhancement of foggy video sequences is proposed based on the Contrast Limited Adaptive Histogram Equalization (CLAHE), which limits the intensity of each pixel to user determined maximum. Thus, it mitigates the degradation due to fog and improves the visibility of the video signal. Initially, the background and <b>foreground</b> <b>images</b> are extracted from the video sequence. Then, the background and <b>foreground</b> <b>images</b> are separately defogged by applying CLAHE. The defogged background and <b>foreground</b> <b>images</b> are fused into the new frame. Finally, the defogged video sequence is obtained. The experimental {{results show that the}} proposed method is more effective than the traditional method. Performance of the proposed method is also analyzed with contrast improvement index (CI) and Tenengrad criterion (TEN) ...|$|R
40|$|In this paper, {{we propose}} a new method that can {{generate}} stereograms (stereoscopic photographs) automatically by compositing <b>foreground</b> <b>images</b> and background images with chromakey using a two-tone stripe background. As to the <b>image</b> composition of <b>foreground</b> <b>images</b> and background images, a chromakey is usually used. However, the conventional chromakey uses the unicolored blue background, {{and has a}} problem that one’s clothes are regarded as the background if their colors are same. Therefore, we utilize the adjacency condition between two-tone striped areas on the background, and extract the foreground regions whose colors are same with the background. In addition, the stereogram consists of images for a left eye and those for a right eye. The relationships between foreground objects and background objects in composite images must be consistent. The relationships of objects between the left image and the right one must be also consistent. Therefore, the <b>foreground</b> <b>images</b> and the background images are merged by considering the geometric constraints...|$|R
50|$|People {{tracking}} from videos {{usually involves}} {{some form of}} background subtraction to segment foreground from background. Once <b>foreground</b> <b>images</b> are extracted, then desired algorithms (such as those for motion tracking, object tracking, and facial recognition) may be executed using these images.|$|R
50|$|Primatte {{is usually}} {{activated}} on a <b>foreground</b> <b>image</b> {{with a person}} or other foreground object filmed or digitized against a solid colored background or backing screen; usually a bluescreen or a greenscreen. The solid colored background area is removed and replaced with transparency. This allows the user to replace the solid colored background with a background image of his choice.|$|E
50|$|The first {{successful}} analemma photograph ever made {{was created in}} 1978-79 by photographer Dennis di Cicco over Watertown, Massachusetts. Without moving his camera, he made 44 exposures on a single frame of film, all taken {{at the same time}} of day at least a week apart. A <b>foreground</b> <b>image</b> and three long-exposure images were also included in the same frame, bringing the total number of exposures to 48.|$|E
50|$|As {{a simple}} example, suppose two {{images of the}} same size are {{available}} and they are to be composited. The input images are referred to as the <b>foreground</b> <b>image</b> and the background image. Each image consists of the same number of pixels. Compositing is performed by mathematically combining information from the corresponding pixels from the two input images and recording the result in a third image, which is called the composited image.|$|E
40|$|Abstract. Estimation {{of human}} body {{orientation}} {{is an important}} cue to study and understand human behaviour, for different tasks such as video surveillance or human-robot interaction. In this paper, we propose an approach to simultaneously estimate the body orientation of multiple people in multi-view scenarios, which combines a 3 D human body shape and appearance model with a 2 D template matching approach. In par-ticular, the 3 D model is composed of a generic shape made up of elliptic cylinders, and a 3 D colored point cloud (appearance model), obtained by back-projecting pixels from <b>foreground</b> <b>images</b> onto the geometric surfaces. In order to match the reconstructed appearance to target im-ages in arbitrary poses, the appearance is re-projected onto each of the different views, by generating multiple templates that are pixel-wise, ro-bustly matched to the respective <b>foreground</b> <b>images.</b> The effectiveness of the proposed approach is demonstrated through experiments in in-door sequences with manually-labeled ground truth, using a calibrated multi-camera setup. ...|$|R
40|$|Image and Video Processing has a {{wide range}} of {{applications}} among various communities. Real Time moving object detection has become an important task in the Image and Video processing applications like robotics and video surveillance systems. There are various techniques in order to detect moving objects. In this paper we bring forward a hardware and software implementation of detection of moving objects using Background Subtraction technique. And we also bring forward the details of the moving objects which are detected, by using the Image reconstruction. For this purpose the following steps are executed – 1) A background image is stored in the SRAM Memory. 2) Low-pass filter is applied to both background <b>image</b> and the <b>foreground</b> <b>images.</b> 3) <b>Foreground</b> <b>images</b> are subtracted from the background image in order to identify the moving object. 4) Image processing techniques are applied to identify the moving object. 5) Image reconstruction techniques are used to obtain the details of the moving object. Interfacer is used either to store the resultant images in the memory or to display them on the screen...|$|R
5000|$|... #Caption: Nellie (<b>foreground)</b> with <b>images</b> of {{both women}} {{projected}} behind, from the Ensemble Theatre's 2016 production, photo by Clare Hawley ...|$|R
50|$|Mattes {{are used}} in {{photography}} and special effects filmmaking to combine two or more image elements into a single, final image. Usually, mattes are used to combine a <b>foreground</b> <b>image</b> (such as actors on a set, or a spaceship) with a background image (a scenic vista, a field of stars and planets). In this case, the matte is the background painting. In film and stage, mattes can be physically huge sections of painted canvas, portraying large scenic expanses of landscapes.|$|E
50|$|Primatte is a {{chroma key}} {{technology}} used in motion picture, television and photographic host applications to remove solid colored backgrounds (greenscreen or bluescreen usually) {{and replace them}} with transparency to facilitate ‘background replacement’. It uses a unique algorithm based on three multi-faceted polyhedrons floating in RGB colorspace that are used to isolate color regions in the <b>foreground</b> <b>image.</b> Primatte {{is often referred to as}} a compositing technology and is usually used as a plug-in for host products such as Adobe After Effects, Adobe Photoshop, Autodesk Media and Entertainment Inferno or Flame, Eyeon Fusion and several other compositing and editing software packages.|$|E
50|$|Courre Merlan (Whiting Chase) appears abstract. It {{consists}} of a large free-form shape that nearly fills the canvas and is placed on a dark, textured background. This large shape contains smaller, neatly outlined, puzzle-like shapes, {{most of which are}} white and filled with parallel lines. The main colors in the work are limited to red, white, blue, and black. These features are all characteristic of Dubuffet's style at the time. Although the shapes are abstracted, a boat or ship and fishermen can be seen, and the title suggests they are fishing for whiting. In contrast with the <b>foreground</b> <b>image,</b> the background is dark and heavily textured with brushwork.|$|E
40|$|Background {{updating}} {{is fundamental}} in mobile objects detection applications. This paper proposes a background updating method with a moving stereo camera. The proposed algorithm {{is based on}} the detection of the regions in the image that have major color intensity in the scene (called light zones). From these light zones some keypoints are extracted and matched between the previous background and the current <b>foreground</b> <b>images.</b> Image registration is performed by moving the old background image according to the keypoints matching so that the <b>foreground</b> and background <b>images</b> are mostly aligned. The proposed method requires that the camera moves slowly and it is used for moving objects detection with background subtraction. Three types of keypoints are tested using the same homography: light zone, SIFT and SURF keypoints. We show experimentally that, on the average, light zone keypoints performances are equal to or better than SIFT keypoints, and are faster to compute; moreover, the SURF keypoints perform worse. To get better performances, when the light zone keypoints fail, then the SIFT keypoints are used in a data fusion ramework...|$|R
5000|$|Segmentation: {{a binary}} mask {{application}} {{is used to}} represent with a white color, the pixels that belong to the hand and to apply the black color to the <b>foreground</b> skin <b>image.</b>|$|R
40|$|Video matting {{attempts}} to extract <b>foreground</b> <b>images</b> from an image sequence, {{as well as}} the alpha-mattes that describe their transparency. This papers presents an automated video matting framework using motion-based segmentation and bayesian matting. We build upon existing techniques for layer segmentation based on optical flow to generate a coarse trimap, by identifying image pixels as belonging to one of background, foreground or unknown. Furthermore, we estimate occluded background for each frame by warping background pixels from neighboring frames. Bayesian methods for video matting are used for identifying detailed alpha-matte and foreground based on a gaussian mixture model distribution for foreground pixels. ...|$|R
50|$|Improvements were developed, {{including}} that made in 1948 by Australian Bertram Pearl whose system incorporated {{a mirror and}} neon-pulse time signature in the winning-post which would provide a precisely aligned image in which {{both sides of the}} horses could be viewed, and on which the neon left a set of stripes at 100th/sec intervals for accurate timing. If the reflected image of the horses aligned vertically exactly with the <b>foreground</b> <b>image,</b> it was proof that the camera was not viewing the finish line at an angle (and therefore incorrectly recording the horses' relative positions). Pearl's partner was his friend, society portraitist Athol Shmith. Shmith's contribution was to formulate means to speed the processing of the strip of negative down to 55 seconds and then to a rapid 35 seconds. These times rivalled even the instant one-minute picture processing by Edwin Land's Model 95 Polaroid camera which became available at that time.|$|E
5000|$|Other {{references}} to the first game include Alice examining the mansion by going outside; crows are visible for a moment. Crows are minor enemies that the player encounters throughout each game. Alice finds {{a picture of her}} wedding day with Spence, which is the same style as the photos in the first version of the Resident Evil game: in black and white with the <b>foreground</b> <b>image</b> noticeably spliced onto the background. On the newspaper {{at the end of the}} film, the words [...] "Horror in Raccoon City! More Victims Dead!" [...] are shown in the upper right corner. This is a reference to the same newspaper in the censored opening of the first Resident Evil game and the prologue chapter for the Resident Evil: The Umbrella Conspiracy novel. Near the beginning of the film, Alice examines a statue after the wind blows its cover off. This statue is similar in design to one in the mansion of the first game.|$|E
5000|$|DjVu divides {{a single}} image into many {{different}} images, then compresses them separately. To create a DjVu file, the initial image is first separated into three images: a background image, a <b>foreground</b> <b>image,</b> and a mask image. The background and foreground images are typically lower-resolution color images (e.g., 100 dpi); the mask {{image is a}} high-resolution bilevel image (e.g., 300 dpi) and is typically where the text is stored. The background and foreground images are then compressed using a wavelet-based compression algorithm named IW44. The mask image is compressed using a method called JB2 (similar to JBIG2). The JB2 encoding method identifies nearly identical shapes on the page, such as multiple occurrences of a particular character in a given font, style, and size. It compresses the bitmap of each unique shape separately, and then encodes the locations where each shape appears on the page. Thus, instead of compressing a letter [...] "e" [...] in a given font multiple times, it compresses the letter [...] "e" [...] once (as a compressed bit image) and then records every place on the page it occurs.|$|E
5000|$|Parallax scrolling, {{also known}} as [...] "Asymmetrical scrolling", is a {{technique}} in computer graphics and web design, where background images move by the camera slower than <b>foreground</b> <b>images,</b> creating an illusion of depth in a 2D scene and adding to the immersion. The technique {{grew out of the}} multiplane camera technique used in traditional animation since the 1930s. Parallax scrolling was popularized in 2D computer graphics and video games by the arcade games Moon Patrol and Jungle Hunt, both released in 1982. Some parallax scrolling had earlier been used by the 1981 arcade game Jump Bug.|$|R
5000|$|... #Caption: Ruins of Nikaroi settlement. A {{residential}} {{tower in}} <b>foreground</b> of the <b>image.</b>|$|R
2500|$|Power and Schoonees [...] {{used the}} same {{algorithm}} to segment the <b>foreground</b> of the <b>image</b> ...|$|R
40|$|Abstract — This paper {{deals with}} area {{detection}} and tracking using color-based image processing. The application adressed concerns human head and hands tracking in real time. We propose an original method {{in order to}} locate colored areas, for a <b>foreground</b> <b>image.</b> <b>Foreground</b> <b>image</b> is provided by a probabilistic way using Gaussian Mixture Models (GMM) of probability density fonctions. The temporal tracking is achieved by a particle filter, which is well adapted to partial occlusions and non gaussian models. I...|$|E
40|$|AbstractReal-time video fire {{flame and}} smoke {{detection}} method based on <b>foreground</b> <b>image</b> accumulation and optical flow technique is presented. Accumulation images are calculated of the foreground images which are extracted using frame differential method. Two parameters are used for the <b>foreground</b> <b>image</b> accumulation to differentiate flame candidate areas from that of smoke. The flame regions are recognized by a statistical model build by foreground accumulation image, while the optical flow is calculated and a motion feature discriminating model to recognize smoke regions is used. The algorithm could realize the real-time fire detection of the following three detection cases: fire with flame and none smoke, fire with smoke and none flame, and fire with both {{flame and smoke}}...|$|E
40|$|In this paper, {{we propose}} a fast and {{efficient}} systematic for vehicle tracking. Codebook algorithm {{is used for}} motion detection. Haar-descriptors which are trained for vehicle images are used for vehicle detection. Instead of scanning the entire image, only detected blob areas are searched for haar-cascade detection. This increases the speed and performance of haar-cascade detection dramatically. Finally, we use the motion <b>foreground</b> <b>image</b> as a mask image to produce a back projection image from the input video for CamShift tracking. Experiments show that separate detection of vehicles in single blob can be achieved depending {{on the quality of}} haar-descriptor file and the performance of CamShift Tracker depends on the quality of motion <b>foreground</b> <b>image...</b>|$|E
5000|$|... #Caption: 1980 Thundarr the Barbarian promotional <b>image</b> <b>Foreground</b> {{from left}} to right Thundarr, Ariel, and Ookla ...|$|R
40|$|Abstract—We {{study the}} multi-view imaging problem where {{one has to}} {{reconstruct}} a set of l images, representing a single scene, from a few measurements made at different viewpoints. We first express {{the solution of the}} problem as the minimizer of a non-convex objective function where one needs to estimate one reference <b>image,</b> l <b>foreground</b> <b>images</b> modeling possible occlusions, and a set of l transformation parameters modeling the inter-correlation between the observations. Then, we propose an alternating descent method that attempts to minimize this objective function and produces a sequence converging to one of its critical points. Finally, experiments show that the method accurately recovers the original images and is robust to occlusions. I. PROBLEM FORMULATION In multi-view imaging, we have in hand l observations y 1, [...] ., yl ∈ R m of a reference image x 0 ∈ R n. As these observations are done from different viewpoints, the image x 0 undergoes geometric transformations. We consider here transformations represented by few parameters (e. g., homography) and denote θj ∈ R q the parameters associated to the j th observations. The reference image transformed according to θj is estimated using, e. g., a cubic spline interpolation and is equal to S(θj) x 0, with S(θj) ∈ R n×n. To handle realistic applications, we also assume that parts of the reference image might sometimes be occluded. We model these occlusions using l <b>foreground</b> <b>images</b> x 1, [...] ., xl ∈ R n, and assume that the image “viewed ” by the j th observer is S(θj) x 0 + xj. Finally, we model the acquisition device using a linear operator A ∈ R m×n, and the observation model satisf...|$|R
40|$|We {{present a}} unique {{occlusion}} and foreground overlap detection technique from depth sensor data using a fuzzy rule-based system. Features such as bounding box parameters and skeletonization were {{extracted from the}} <b>foreground</b> <b>images</b> and then input to the Fuzzy Inference System. Overlap and occlusion confidence measures were taken for each frame in the image sequence and compared against the extracted ground truth. This technique can help filter out occluded regions in the image sequence which, in an Eldercare environment, can then be used to compute accurate estimates of fall risk parameters such as stride time, stride length, and walking speed {{on a daily basis}} in in order to monitor the well-being of older adults in an ambient assisted living facility...|$|R
40|$|In {{this paper}} we propose a novel partial {{out-of-focus}} blur removal method developed within the Bayesian framework. We concentrate on the removal of background out-of-focus blursthatarepresentintheimagesinwhichthereisastrong interest to keep the foregroundin sharp focus. However, oftenthereisadesiretorecoverbackgrounddetailsoutofsuch partially blurred image. In this work, a non-convexlp-norm prior with 0 < p < 1 is used as the background and <b>foreground</b> <b>image</b> prior and a total variation (TV) based prior is utilizedforboththebackgroundblurandtheocclusionmask, thatis,themaskdeterminingthepixelsbelongingtotheforeground. In order to model transparent foregrounds, the valuesintheocclusionmaskareassumedtobelongtotheclosed interval [0, 1]. The proposed method is derived by utilizing boundson the priorsfor the backgroundand foregroundimage, the background blur and the occlusion mask using the majorization-minimizationprinciple. Maximumaposteriori Bayesian inference is performed and as a result, the background and <b>foreground</b> <b>image,</b> the background blur, the occlusion mask and the model parameters are simultaneously estimated. Experimentalresultsarepresentedtodemonstrate theadvantageoftheproposedmethodovertheexistingones...|$|E
30|$|The <b>foreground</b> <b>image</b> {{is used to}} {{generate}} blobs or groups of connected pixels, which are described by their bounding boxes (shown in Figure 1 as red rectangles). At this point, {{the rest of the}} processing is carried out only on the data structures that describe these bounding boxes, so that no other image processing stage is required. Therefore, the computational cost of the following steps is significantly reduced.|$|E
30|$|As regards shadow removal, {{this method}} does not assume a color {{model of the}} shadow pixels. Instead, {{it is based on}} a model of the shape of a shadow and of its {{relation}} to the object that casts it. So, shadow removal takes place after a first, tentative foreground detection: the parts of the tentative foreground that are consistent with the shadow model are removed, obtaining the final <b>foreground</b> <b>image.</b>|$|E
40|$|Moving object {{detection}} is {{an essential}} process before tracking and event recognition in video surveillance can take place. To monitor a wider field of view and avoid occlusions in pedestrian tracking, multiple cameras are usually used and homography can be employed to associate multiple camera views. Foreground regions detected {{from each of the}} multiple camera views are projected into a virtual top view according to the homography for a plane. The intersection regions of the foreground projections indicate the locations of moving objects on that plane. The homography mapping for a set of parallel planes at different heights can increase the robustness of the detection. However, homography mapping is very time consuming and the intersections of non-corresponding foreground regions can cause false-positive detections. In this thesis, a real-time moving object detection algorithm using multiple cameras is proposed. Unlike the pixelwise homography mapping which projects binary <b>foreground</b> <b>images,</b> the approach used in the research described in this thesis was to approximate the contour of each foreground region with a polygon and only transmit and project the polygon vertices. The foreground projections are rebuilt from the projected polygons in the reference view. The experimental results have shown that this method can be run in real time and generate results similar to those using <b>foreground</b> <b>images.</b> To identify the false-positive detections, both geometrical information and colour cues are utilized. The former is a height matching algorithm based on the geometry between the camera views. The latter is a colour matching algorithm based on the Mahalanobis distance of the colour distributions of two foreground regions. Since the height matching is uncertain in the scenarios with the adjacent pedestrian and colour matching cannot handle occluded pedestrians, the two algorithms are combined to improve the robustness of the foreground intersection classification. The robustness of the proposed algorithm is demonstrated in real-world image sequences...|$|R
40|$|Abstract—We {{present a}} unique {{occlusion}} and foreground overlap detection technique from depth sensor data using a fuzzy rule-based system. Features such as bounding box parameters and skeletonization were {{extracted from the}} <b>foreground</b> <b>images</b> and then input to the Fuzzy Inference System. Overlap and occlusion confidence measures were taken for each frame in the image sequence and compared against the extracted ground truth. This technique can help filter out occluded regions in the image sequence which, in an Eldercare environment, can then be used to compute accurate estimates of fall risk parameters such as stride time, stride length, and walking speed {{on a daily basis}} in in order to monitor the well-being of older adults in an ambient assisted living facility. Keywords—occlusion; fuzzy rules; depth image; activity analysis; machine learning I...|$|R
40|$|This paper {{presents}} a new method to track multiple people reliably using {{a network of}} calibrated smart cameras. The task of tracking multiple persons is very difficult due to non-rigid nature of the human body, occlusions and environmental changes. Our proposed method recursively updates the positions of all persons based on the observed <b>foreground</b> <b>images</b> from all smart cameras and the previously known location of each person. The performance of our proposed method is evaluated on indoor video sequences containing person– person/object–person occlusions and sudden illumination changes. The results show that our method performs well with Multiple Object Tracking Accuracy as high as 100 % and Multiple Object Tracking Precision as high as 86 %. Performance comparison {{to a state of}} the art tracking system shows that our method outperforms...|$|R
