2|10000|Public
40|$|This report {{describes}} the compilation of the A. -A. E. C, nuclear data card library and the conventions {{relating to the}} use of the data in neutronics calculations. Details of <b>data</b> <b>processing</b> <b>programmes,</b> library formats, and the extent of the information available for each nuclide are given in the appendices...|$|E
40|$|The Virtual Environments of Learning are <b>data</b> <b>processing</b> <b>programmes</b> {{that helps}} the learning, facilitating the {{communication}} among their users. The free software programmes allow to quickly create customized modular environments ready {{to be put in}} service. Moodle is a free software learning platform, based on the pedagogical principles of the constructivism and constructionism, according to which the student is the responsible for its own learning, and the tutor changes from being the transmitter of know-how to become the guide of the student in this process. All this happen inside an environment that facilitates the communication among all the participants. In this presentation we explain how the users training has been carried out with Moodle in the Library of the University of Malaga: the virtual training (following the Users Training Plan), the first steps with Moodle, what offers the platform to the tutor and to the student, design and development of a virtual course, and the training evaluation...|$|E
40|$|In this study, forward seismic {{modelling}} of four geological {{models with}} Hydrocarbon (HC) traps were performed by ray tracing method to produce synthetic seismogram of each model. The {{idea is to}} identify the Hydrocarbon Indicators (HCI‟s) such as bright spot, flat spot, dim spot and Bottom Simulating Reflector (BSR) in the synthethic seismogram. The modelling was performed in DISCO/FOCUS 5. 0 seismic <b>data</b> <b>processing</b> <b>programme.</b> Strong positive and negative reflection amplitudes and some artifact reflection horizons were observed on produced seismograms due to rapid changes in subsurface velocity and geometry respectively Additionally, Amplitude-versus-angle (AVA) curves of each HCIs was calculated by the Crewes Zoeppritz Explorer programme. AVA curves show that how the reflection coefficients change with the density and the P and S wave velocities of each layer such as oil, gas, gas hydrate or water saturated sediments. Due to AVA curves, an increase in reflection amplitude with incident angle of seismic waves corresponds to an indicator of a hydrocarbon reservoi...|$|R
40|$|A {{methodology}} {{was developed}} for analysing faults in distribution transformers using the statistical package for social sciences (SPSS); it consisted of organising and creating of database regarding failed equipment, incorporating such <b>data</b> into the <b>processing</b> <b>programme</b> and converting all the information into numerical variables to be processed, thereby obtaining descriptive statistics and enabling factor and discriminant analysis. The research was based on information provided by companies in areas served by Corpoelec (Valencia, Venezuela) and Codensa (Bogotá, Colombia) ...|$|R
40|$|Summary: A fluorometric assay in microtitre plates for the {{screening}} of phenylketonuria was evaluated and adapted to a neonatal screening programme. Using this assay, {{it is possible}} to deterraine quantitatively the phenylalanine concentration in dried blood Spots on fiiter paper. The test exhibited a linear calibration curve with a good slope äs well äs sufficient precision and accuracy in the statistical analysis. Interference by other amino acids and antibiotics was not observed. Only elevated concentrations of leucine inter-fered to a small degree. l! The phenylalanine concentration in dried blood spots of 13 phenylketonuria patients correlated to that in serum. | 7381 dried blood samples of newborn infants were tested simultaneously by both the fluorometric and the Guthrie test. The results did not show significant differences. We screened 29182 newborns using the fluorometric assay and an online <b>data</b> <b>processing</b> <b>programme.</b> The internal! (repetition rate was 0. 64 %, the external recall rate 0. 15 %. False negative results were not observed. In December 1991 the fluorometric methöd replaced the Guthrie test in our routine programme for phenylketonuria screening, and was introduced äs a follow up test for phenylketonuria patients. Introduction Succinic acid (No. S 7501); cupric sulphate (No. C 7631); sodium potassium tartrate (No. S 2377); sodium carbonate (No. S 4132) Phenylketonuria is a congenital disorder of phenylala- we * frora SiSma Chemicals, 82041 Deisenhofen, Germany...|$|R
40|$|This thesis compares and assesses {{accounting}} <b>data</b> <b>processing</b> in two chosen programmes Pohoda and Ekonom. The {{first part}} of thesis is a literary summary of the theme historical progress of the accounting, particular phases of the progress included, and describes economical infor-mation systems and general standards for the right choice of accounting programme for a firm. In the second part the author compares and describes both of the programmes, describes the way of <b>data</b> <b>processing</b> in <b>programmes</b> Ekonom and Pohoda. The analysis goes in the module of billing, stocks and pay. The author assesses both of the programmes and compares their methods. In the next part of the thesis are given advantages and disad-vantages for users. The last part of thesis describes a concrete firm, her way of account-ing processing included. On {{the basis of the}} price calculation of the programmes the author will recommend a system for the accounting <b>data</b> <b>processing...</b>|$|R
40|$|The {{purpose of}} the project was to provide an {{understanding}} of the ex isting farming systems and constraints to serve as a basis for local planning at the Se rvice Cooperative level {{within the context of the}} Fourth Livestock Development Project (FLDP);The project developed and tested a method of analysing farming syste ms consisting of a survey method, a computerized <b>data</b> <b>processing</b> <b>programme</b> and a re porting procedure, which together constitute a repeatable package for analysing the reso urces, systems, production and performance of farming at the Service Cooperative level. The method was applied to 28 Service Cooperatives during the project's lifetime. Th ree training courses were organized, combining field experience with computer experience and data interpretation, to ensure that the method could be applied after the end of proj ect support;The project found that the Service Cooperative is a suitable unit fo r data collection and local planning. However, the Farming Systems Development (FSD) a pproach is very time-consuming and costly, and was best applied to a selection of cooper atives in different agroecological zones. The main advantage of the method was that it pr ovides good information for planning, monitoring and evaluation. It was recommended that co nsideration be given to its wider application in the various regional development efforts. To this end, a local workshop could be organized to compare the different micro-level plannin g approaches, of which one would be the FSD approach developed by this project. F urther, the method could be included in agricultural training courses at both junior col lege and university level...|$|R
40|$|In {{the paper}} I {{examined}} {{the possibilities of}} variuos local spatial data import in Google Earth programme. I decided to import Kromberk cadastral municipality borders, digital ortophotos, which cover the municipality, digital terrain model with added contours and 3 D-model of Kromberk castle. Google Earth programme and KML schema are introduced. Also considered are various possibilities and methods of recording local spatial data in Google Earth programme. Every data type is examined in its individual chapter. For <b>processing</b> of spatial <b>data</b> I used different approaches and programme tools. Described in the paper are the methods of individual spatial <b>data</b> <b>processing</b> with established <b>programme</b> equipment and other possibilities, which are offered by an individual programme equipment with regard to spatial data. Final results are shown in graphics. In the conclusion I present the posibilities of use of imported local spatial data in Google Earth programme...|$|R
40|$|Subject {{of inquiry}} is double beta-processes, nuclear {{transitions}} into superdense state, cluster radioactivity, electron stability. The {{aim of the}} work is experimental detection or determination of liimting values for rare processes probability, theoretical analysis of the obtained data. New calculational algorithms and techniques realized in experiments simulator programmes using Monte Carlo method and in <b>processing</b> <b>programmes</b> for spectrometric <b>data</b> have been suggested. Limitations on nonconservation degree of lepton and electric charges, right-hand current impurities, masses of neutrino, photons and so on have been calculated. There were received experimental limitations for probabilities of 2 K-captures in 196 -Hg, various modes of 2 beta -decay of 116 -Cd, which are considered to be the most hard from all the known. For the first time there were determined limitations for probability of cluster decay of mercury various isotopes, and probability of titanium nuclei transition into superdense state. The designed programme parts of experiments simulation and experimental <b>data</b> <b>processing</b> is used in Institute for Nuclear Research of the Academy of Sciences of the Ukraine and in National Institute for Physics of Nucleus and Fundamental Particles (France). The programmes allow to simulate processes of electrons travel through any sibstance and double beta-decay, to computate the efficiency of recording of gamma-radiation from a solid specimen, to process one-dimensional spectra. Field of application: nuclear physics, fundamental particles theory, spectrometral <b>data</b> <b>processing,</b> radiobiology, determination of radionuclides concentration in level in a substanceAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|<b>Data</b> <b>Processing</b> {{discusses}} the principles, practices, and associated tools in <b>data</b> <b>processing.</b> The book {{is comprised of}} 17 chapters that are organized into three parts. The first part covers the characteristics, systems, and methods of <b>data</b> <b>processing.</b> Part 2 deals with the <b>data</b> <b>processing</b> practice; this part {{discusses the}} data input, output, and storage. The last part discusses topics related to systems and software in <b>data</b> <b>processing,</b> which include checks and controls, computer language and programs, and program elements and structures. The text will be useful to practitioners of computer-re...|$|R
40|$|This paper {{proposes a}} {{conceptual}} matrix model with algorithms for biological <b>data</b> <b>processing.</b> The required elements for constructing a matrix model are discussed. The representative matrix-based methods and algorithms which have potentials in biological <b>data</b> <b>processing</b> are presented / proposed. Some application {{cases of the}} model in biological <b>data</b> <b>processing</b> are studied, which show the applicability of this model in various kinds of biological <b>data</b> <b>processing.</b> This conceptual model established a framework within which biological <b>data</b> <b>processing</b> and mining could be conducted. The model is also heuristic to other applications. <br /...|$|R
30|$|Quality <b>data</b> <b>processing</b> defines {{configuration}} {{and processing}} parameters, which are utilized {{in the evaluation}} process of quality attributes. Examples of configuration parameters include location of binary or configuration files, and environmental execution parameters (e.g. number of CPU cores, size of memory). Processing parameters refer to input data, which should be provided to <b>data</b> <b>processing</b> executables (e.g. Spark streaming). Supported quality <b>data</b> <b>processing</b> defines processing, which can be performed with a specific <b>data</b> <b>processing</b> tool. Especially, it can be specified what kind of quality attributes for a data source can be evaluated with a specific <b>data</b> <b>processing</b> tool.|$|R
40|$|Abstract. UnifiedViews is an Extract-Transform-Load (ETL) frame-work {{that allows}} users – publishers, consumers, or analysts – to define, execute, monitor, debug, schedule, and share RDF <b>data</b> <b>processing</b> tasks. The <b>data</b> <b>processing</b> tasks may use custom plugins created by users. UnifiedViews differs from other ETL {{frameworks}} by natively supporting RDF data and ontologies. The practical demonstration of UnifiedViews {{at the conference}} will (1) clearly demonstrate how UnifiedViews helps RDF/Linked Data users with RDF <b>data</b> <b>processing</b> (2) and show the real instance of UnifiedViews with tens of <b>data</b> <b>processing</b> tasks and DPUs motivated by real <b>data</b> <b>processing</b> use cases. ...|$|R
40|$|Significant {{numbers of}} {{physicians}} are using <b>data</b> <b>processing</b> services {{and a large}} number of firms are offering an increasing variety of services. This paper quantifies user dissatisfaction with office practice <b>data</b> <b>processing</b> systems and analyzes factors affecting dissatisfaction in large group practices. Based on this analysis, a proposal is made for a more structured approach to obtaining <b>data</b> <b>processing</b> services in order to lower the risks and increase satisfaction with <b>data</b> <b>processing...</b>|$|R
40|$|DE 102007026480 A 1 UPAB: 20081222 NOVELTY - The method {{involves}} attaching mobile <b>data</b> <b>processing</b> {{units to}} collection containers contained with products to be commissioned. The <b>data</b> <b>processing</b> units command over micro-controllers, local memory, sensor interfaces and wireless communication devices. The mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> {{system in a}} commissioning controlling system to identify the current collection containers. The numbers of products, which can be inferred, are indicated. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for the execution of a method for the commissioning of goods. USE - Method for the commissioning of goods. ADVANTAGE - The method involves attaching mobile <b>data</b> <b>processing</b> units to collection containers contained with products to be commissioned, where the mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> system in a commissioning controlling system to identify the current collection containers, and hence ensures to simplify the commissioning work by retrofitting the existing stock option and shelving systems...|$|R
5000|$|Pure {{communications}} and pure <b>data</b> <b>processing</b> {{have very different}} characteristics that led to different policy results. The markets that the technology existed on assisted the FCC make its policy decisions. [...] "The pure <b>data</b> <b>processing</b> market was viewed as an innovative, competitive market with low barriers to entry and little chance of monopolization." [...] The FCC established that no additional regulation or safeguards where required for the pure <b>data</b> <b>processing</b> market. The pure communications market {{on the other hand}} was being managed by an incumbent monopoly. The FCC had four concerns about the incumbent telephone companies which were: [...] "the sale of <b>data</b> <b>processing</b> services by carriers should not hurt the provision of common carrier services, the costs of such <b>data</b> <b>processing</b> services should not be passed on to telephone rate payers, revenues derived from common carrier services should not be used to cross subsidize <b>data</b> <b>processing</b> services, and the furnishing of such <b>data</b> <b>processing</b> services by carriers should not hurt the competitive computer market." ...|$|R
50|$|The IEA <b>Data</b> <b>Processing</b> and Research Center (DPC) is the <b>data</b> <b>processing</b> and {{research}} department of IEA, located in Hamburg, Germany.|$|R
40|$|Moore's law {{was first}} {{postulated}} in 1968, and it loosely {{says that the}} cost of making calculations on a computer falls by 50 % each year. Securities markets are, in essence, a form of <b>data</b> <b>processing.</b> Consequently, Moore’s law has driven important changes in those markets over the past forty years. Faster <b>data</b> <b>processing</b> was essential for major changes in securities trading. Increased turnover of portfolios was a result of faster <b>data</b> <b>processing.</b> Consequently, the criticism of that turnover may be misplaced. The effectiveness of regulatory changes, such as the lowering of brokerage commissions and the reduction in bid ask spreads, depended on reduced <b>data</b> <b>processing</b> costs, that is on Moore's Law. Deregulation of brokerage commissions could not have reduced rates by as much as it did if we had not had decreasing costs of <b>data</b> <b>processing.</b> The reduction in bid ask spreads which followed decimalization of securities quotes depended on improved <b>data</b> <b>processing.</b> Continued reductions in <b>data</b> <b>processing</b> costs will require a new regulatory approach. Regulators should consider the improvements in <b>data</b> <b>processing</b> and <b>data</b> transmission when they establish capital requirements and haircuts. ...|$|R
40|$|<b>Data</b> <b>processing</b> complexity, partitionability, {{locality}} and provenance play {{a crucial}} role in the effectiveness of distributed <b>data</b> <b>processing.</b> Dynamics in <b>data</b> <b>processing</b> necessitates effective modeling which allows the understanding and reasoning of the fluidity of <b>data</b> <b>processing.</b> Through virtualization, resources have become scattered, heterogeneous, and dynamic in performance and networking. In this paper, we propose a new distributed <b>data</b> <b>processing</b> model based on automata where <b>data</b> <b>processing</b> is modeled as state transformations. This approach falls within a category of declarative concurrent paradigms which are fundamentally different than imperative approaches in that communication and function order are not explicitly modeled. This allows an abstraction of concurrency and thus suited for distributed systems. Automata give us a way to formally describe <b>data</b> <b>processing</b> independent from underlying processes while also providing routing information to route data based on its current state in a P 2 P fashion around networks of distributed processing nodes. Through an implementation, named Pumpkin, of the model we capture the automata schema and routing table into a <b>data</b> <b>processing</b> protocol and show how globally distributed resources can be brought together in a collaborative way to form a <b>processing</b> plane where <b>data</b> objects are self-routable on the plane...|$|R
40|$|This paper {{reviews the}} {{historical}} development of <b>data</b> <b>processing,</b> discerning three approximate decades of distinct evolutionary cycles. Each cycle {{is seen as}} forking, two contrasting styles of <b>data</b> <b>processing</b> forming and separating during the cycle. On this basis, a classification of the major general areas of <b>data</b> <b>processing</b> is suggested. For each decade, characteristic aspects are discussed and both the lines of development are described. Finally, some observations regarding the present decade and its requirements are given, and some predictions relating to the next decade and its prerequisites are made. DESCRIPTORS: Classification of <b>data</b> <b>processing.</b> Evolution of <b>data</b> <b>processing.</b> Philosophical implications. Computing milieu. Terminology. CR CATEGORIES: 1. 2, 1. 3, 2. ...|$|R
40|$|A {{system for}} {{assessing}} vestibulo-ocular function includes a motion sensor system adapted to be coupled to a user's head; a <b>data</b> <b>processing</b> system configured {{to communicate with}} the motion sensor system to receive the head-motion signals; a visual display system configured {{to communicate with the}} <b>data</b> <b>processing</b> system to receive image signals from the <b>data</b> <b>processing</b> system; and a gain control device arranged to be operated by the user and to communicate gain adjustment signals to the <b>data</b> <b>processing</b> system...|$|R
5000|$|Mivar-based {{technology}} of <b>data</b> <b>processing</b> {{is a method}} of creating logical inference system or automated algorithm construction from modules, services or procedures {{on the basis of}} active trained mivar network of rules with the linear computational complexity. Mivar-based {{technology of}} <b>data</b> <b>processing</b> is designed for <b>data</b> <b>processing</b> including logical inference, computational procedures and services.|$|R
5000|$|Roger Lee Sisson (June 24, 1926 [...] - [...] January 22, 1992) was {{an early}} <b>data</b> <b>processing</b> pioneer. Sisson worked on Project Whirlwind while a {{graduate}} student at MIT, co-founded the first consulting firm devoted to electronic <b>data</b> <b>processing,</b> and published a number of the earliest books and periodicals on computers and <b>data</b> <b>processing.</b>|$|R
40|$|Fourier {{transform}} spectrometry {{is a type}} {{of novel}} information obtaining technology, which integrated the functions of imaging and spectra, but the data that the instrument acquired is the interference data of the target, which is an intermediate data and couldn&# 39;t be used directly, so <b>data</b> <b>processing</b> must be adopted for the successful application of the interferometric data. In the present paper, <b>data</b> <b>processing</b> techniques are divided into two classes: general-purpose and special-type. First, the advance in universal interferometric <b>data</b> <b>processing</b> technique is introduced, then the special-type interferometric data extracting method and <b>data</b> <b>processing</b> technique is illustrated according to the classification of Fourier transform spectroscopy. Finally, the trends of interferogram <b>data</b> <b>processing</b> technique are discussed...|$|R
50|$|The term <b>Data</b> <b>Processing</b> (DP) {{has also}} been used {{previously}} {{to refer to a}} department within an organization responsible for the operation of <b>data</b> <b>processing</b> applications.|$|R
40|$|A {{field survey}} was {{conducted}} to study the factors {{associated with the use}} of <b>data</b> <b>processing</b> charge-back information in organizations. The aim of this research was to identify the organizational and budgetary characteristics associated with how the output of a chargeback system is used by user-managers to control their <b>data</b> <b>processing</b> costs. It was found that involvement in budget preparation, accountability for meeting the <b>data</b> <b>processing</b> budget, and cost variability of the charges were the- most important factors to consider when designing <b>data</b> <b>processing</b> chargeback systems...|$|R
40|$|Scientific {{workflow}} {{systems have}} become a necessary tool for many applications, enabling the composition and execution of complex analysis. CO(2) flux data observed by eddy covariance technique is large in quantity and the procedure of flux data is complex, scientific workflow technique plays {{a very important role}} in the sharing, reusing and automatic calculation of flux <b>data</b> <b>processing</b> method. In this paper, we discuss the feasibility and validity of applying scientific workflow technique to flux <b>data</b> <b>processing</b> and make a tentative approach to construct a scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> by taking Kepler scientific workflow system as the development platform. CO(2) flux data of Changbai Mountain in 2003 is used to verify the scientific workflow system. The results show that scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> can solve many problems of too much multifarious calculation, inconsistent development platform and complicated procedure in flux <b>data</b> <b>processing.</b> This approach indicates that the scientific workflow system applied to CO(2) flux <b>data</b> <b>processing</b> can provide an automatic calculation platform for flux <b>data</b> <b>processing</b> and prompt the communication and sharing of international flux <b>data</b> <b>processing</b> method, which make it easier for scientists to focus on their research and not computation management...|$|R
40|$|In this {{research}} aims {{to change the}} personnel staffing <b>data</b> <b>processing,</b> which is stillperformed in the conventional / manual, {{with the help of}} computer hardware for fastdata processing, the company makes computerized <b>data</b> <b>processing</b> personnel in orderto accelerate the process of doing <b>data</b> <b>processing</b> personnel and presenting reports -reports / information to the right, quickly and accurately...|$|R
40|$|In {{carrying}} out real work practices and preparing this paper, the authors obtain abroader knowledge about <b>data</b> <b>processing</b> {{system on the}} computerized system and cancompare with the <b>data</b> <b>processing</b> system manually. Given the <b>data</b> <b>processing</b> systemwith a computerized warehouse section can then be expected to facilitate thewarehouse stock control of goods entering derta out systematically and efficiently aspossible...|$|R
40|$|PREFACE Many {{books and}} {{articles}} generally recognize that management personnel other {{than those in the}} <b>data</b> <b>processing</b> installation need to be oriented towards <b>data</b> <b>processing</b> if effective utilization of these costly computers is to evolve. Present emphasis in the U. S. Marine Corps is on training of <b>data</b> <b>processing</b> personnel with limited orienta-tion or training of other managerial personnel in the capabilities and limitations of computers. The author carries an additional military occupational specialty as a <b>Data</b> <b>Processing</b> Officer and has been involved with Marine Corps <b>data</b> <b>processing</b> for a number of years. He found this lack of training of other officers a perplexing problem to him in the conduct of everyday tasks. It soon became evident to him that all officers dealing with him should have adequate training in <b>data</b> <b>processing.</b> This problem has been recognized by some Marine Corps officials, but to date a standard syste...|$|R
5000|$|IT audits {{are also}} known as [...] "automated <b>data</b> <b>processing</b> (ADP) audits" [...] and [...] "computer audits". They were {{formerly}} called [...] "electronic <b>data</b> <b>processing</b> (EDP) audits".|$|R
5000|$|Smagorinsky, J., 1965: Remarks on <b>data</b> <b>processing</b> in meteorology. In, Proceedings of the WMO/IUGG Symposium on Meteorological <b>Data</b> <b>Processing,</b> Brussels, Belgium, WMO Technical Note 73, pp. 1-2.|$|R
40|$|The {{tremendous}} impact of electronic <b>data</b> <b>processing</b> {{on the lives}} of the American people is being felt in every segment of our economy. Regardless of the area of activity, be it business, education, government, industry, or the service areas, electronic <b>data</b> <b>processing</b> is performing an increasingly· important function. The volume of paper work, the magnitude of government reports, and the demands of management for information, have created mammoth challenges for electronic <b>data</b> <b>processing.</b> These challenges are complex. However, electronic <b>data</b> <b>processing</b> equipment and personnel are making favorable gains on the problems posed by these challenges...|$|R
5000|$|Sisson, with Richard Canning, started one of {{the first}} {{consulting}} firms devoted exclusively to electronic <b>data</b> <b>processing,</b> Canning, Sisson, and Associates. Canning and Sisson also published {{one of the}} earliest computer periodicals, <b>Data</b> <b>Processing</b> Digest, starting in 1955. [...] Sisson went on to write a number of noted books on the subject of EDP, including The Management of <b>Data</b> <b>Processing,</b> and A Manager’s Guide to <b>Data</b> <b>Processing.</b> He wrote an early and influential paper in the field of Operations Research, [...] "Methods of Sequencing in Job Shops" [...] in the journal Operations Research in 1959.|$|R
5000|$|With {{concerns}} {{relating to}} communication facility, the FCC developed its [...] "Maximum Separation" [...] safeguards. The FCC {{made it so}} that if carrier wanted to enter the unregulated <b>data</b> <b>processing</b> market they could only do so by going through a fully separate subsidiary. The separate subsidiary needed to have a separate <b>data</b> <b>processing</b> corporation, accounting books, offices, personnel, equipment, and facilities. The carrier also could not use the separate subsidiary to promote their <b>data</b> <b>processing</b> services, use network computers for non-network purposes, or use network computers during peak hours to provision <b>data</b> <b>processing</b> services.|$|R
40|$|The {{purpose of}} the study was to {{determine}} the status of electronic <b>data</b> <b>processing</b> services and/or functions in the public schools of Indiana. To provide additional insight to the organizational structure of a <b>data</b> <b>processing</b> center, four existing regional <b>data</b> <b>processing</b> centers serving a group of school districts in adjoining states through a centrally located computer facility were examined. The procedures used in collecting and analyzing the data for the study included the following: (1) appropriate data gathering instruments were designed, (2) a questionnaire accompanied by a cover letter explaining the {{purpose of the}} study was mailed to each superintendent of all school corporations in Indiana, (3) a personal interview was conducted with the administrative personnel directly responsible for the operation of each of the four selected regional <b>data</b> <b>processing</b> centers, (3) <b>data</b> obtained from the questionnaire and the personal interview were compiled, tabulated, analyzed, and presented, and (4) the findings, conclusions and recommendations for further study were presented. The superintendents were asked to respond to the question, "Do you presently utilize <b>data</b> <b>processing</b> (computer and punch card) equipment in any part of the operation and/or administration of your school district?" Those school administrators responding "YES" were then asked to indicate those services and functions which were accomplished in their school corporation, to indicate the method of obtaining <b>data</b> <b>processing</b> services and to indicate methods. All of the superintendents were asked to indicate their future plans for the use of <b>data</b> <b>processing</b> services in their local school corporation(s). The systematic analysis of the procedures necessary for implementing educational <b>data</b> <b>processing</b> services to a group of school districts in adjoining states included the following elements: (1) background information, (2) objectives, (3) historical development, (4) equipment, (5) personnel, (6) services and/or functions, (7) orientation procedures, (8) advantages of a central computer facility, (9) disadvantages of a central computer facility, and (10) future plans. The findings indicated the following major general conclusions to be appropriate the level of their satisfaction with both services. There has been only limited development toward the organization of educational <b>data</b> <b>processing</b> systems throughout the state of Indiana. This situation lends itself to collective exploration by interested school administrators toward cooperative arrangements of obtaining <b>data</b> <b>processing</b> services, regardless of the size of the school district. Qualified and competent personnel must be responsible for the operation of the <b>data</b> <b>processing</b> center. The director of the <b>data</b> <b>processing</b> center should be totally familiar with all facets of school administration so that he may use electronic <b>data</b> <b>processing</b> services to facilitate the instructional program. A central computer facility organized to provide electronic <b>data</b> <b>processing</b> services to a group of school districts is capable of supporting the more sophisticated computer equipment at a reduction in cost, provided the equipment is operating at a near capacity work load. It is possible for a group of school districts to benefit from the developmental experience of existing <b>data</b> <b>processing</b> centers. The amount of time once considered necessary to progress from feasibility of operation to full-scale implementation has been greatly reduced by administrators' capitalizing on the systematic developments of existing regional <b>data</b> <b>processing</b> centers. School districts involved in <b>data</b> <b>processing</b> can benefit from the efforts of an administrative officer whose function is to systematically analyze administrative operations. This individual is referred to as a systems analyst or operations analyst. Teleprocessing has great potential for providing electronic <b>data</b> <b>processing</b> services to school districts regardless of size. Teleprocessing provides school administrators with direct access to sophisticated electronic <b>data</b> <b>processing</b> equipment and computer programs which have been successfully developed and implemented in other school districts. The Indiana Department of Education should assume the leadership in establishing regional service centers throughout Indiana to provide comprehensive educational <b>data</b> <b>processing</b> services to local school districts. Colleges and universities should serve as a catalyst of ideas for the intelligent use of electronic <b>data</b> <b>processing</b> services to perform administrative and instructional functions. Thesis (D. Ed. ...|$|R
