35|47|Public
2500|$|However, {{this did}} not prevent programmers from writing forward {{incompatible}} software. [...] "24-bit" [...] software that discarded the upper address byte, or used it for purposes other than addressing, could fail on 32-bit 68000 implementations. For example, early (pre-7.0) versions of Apple's Mac OS used the <b>high</b> <b>byte</b> of memory-block master pointers to hold flags such as locked and purgeable. Later versions of the OS moved the flags to a nearby location, and Apple began shipping computers which had [...] "32-bit clean" [...] ROMs beginning {{with the release of}} the 1989 Mac IIci.|$|E
50|$|For example, if {{you have}} the GB2312 code point 4566 ("外", which means foreign), the <b>high</b> <b>byte</b> will be 4566/94+158=206=0xCE, and the low byte will come from 4566%94+158=212=0xD4. So, the full {{encoding}} is 0xCED4=52948.|$|E
5000|$|MUL ($62) - Multiply 8-bit x 8-bit - Multiplies {{accumulator}} {{with the}} memory specified by the zero page X addressing mode and stores the <b>high</b> <b>byte</b> of the result on the stack and the low byte in the accumulator.|$|E
30|$|This {{value is}} {{represented}} in 16 bits Q 8.8 format and sent over the UART in chunks of <b>bytes</b> with <b>higher</b> <b>byte</b> followed by lower byte.|$|R
5000|$|Handling: If it is {{anticipated}} that overflow may occur and when it happens detected and other processing done. Example: {{it is possible to}} add two numbers each two bytes wide using just a byte addition in steps: first add the low bytes then add the <b>high</b> <b>bytes,</b> but if it is necessary to carry out of the low bytes this is arithmetic overflow of the byte addition and it necessary to detect and increment the sum of the <b>high</b> <b>bytes.</b> CPUs generally have a way of detecting this to support addition of numbers larger than their register size, typically using a status bit.|$|R
50|$|Although seven-bit {{communication}} was the norm, most computers internally used eight-bit bytes, and they mostly put {{some form of}} characters in the 128 <b>higher</b> <b>byte</b> positions. In the early days most of these were system specific, but gradually a few standards were settled on.|$|R
5000|$|LDA #25: JSR OSWRCH \ begin [...] "PLOT" [...] (ASCII 25) command LDA #4: JSR OSWRCH \ command k=4, or move {{absolute}} LDA #0: JSR OSWRCH: JSR OSWRCH: JSR OSWRCH: JSR OSWRCH \ send (0, 0) as low, <b>high</b> <b>byte</b> pairs ...|$|E
5000|$|LDA #25: JSR OSWRCH \ begin PLOT LDA #1: JSR OSWRCH \ k=1 - draw {{relative}} LDA #0: JSR OSWRCH: JSR OSWRCH \ x = 0 LDA #100: JSR OSWRCH \ y = 100 (low byte) LDA #0: JSR OSWRCH \ <b>high</b> <b>byte</b> ...|$|E
50|$|STRUCLEV {{contains}} the current structure level (in the <b>high</b> <b>byte)</b> and version (in the low byte) of the file system; ODS-2 being structure level 2. An {{increase in the}} version number indicates a backwards-compatible change that older software may ignore; changes in the structure level itself are incompatible.|$|E
30|$|In {{spite of}} this peculiarity, however, the effect of {{temporal}} locality is noticeably higher in our BitTorrent traffic workloads than that reported in Web traffic studies [7, 52]. This observation justifies the <b>higher</b> <b>byte</b> hit rate achieved by LRU recency-based replacement policy in our BitTorrent caching simulation.|$|R
50|$|Some {{architectures}} like MIPS {{have special}} unaligned load and store instructions. One unaligned load instruction gets the bytes from the memory word {{with the lowest}} byte address and another gets the bytes from the memory word with the <b>highest</b> <b>byte</b> address. Similarly, store-high and store-low instructions store the appropriate <b>bytes</b> in the <b>higher</b> and lower memory words respectively.|$|R
30|$|We {{find that}} LRU attains a <b>higher</b> <b>byte</b> hit rate than LFU, and SIZE {{in nearly all}} scenarios. The single {{exception}} is FileList with 50  % of the ideal cache size, where we cannot distinguish the byte hit rates of LRU and LFU. Overall, the maximum byte hit rate in the experiment lies between 50 – 80  % for all workloads with LRU and 50  % of the ideal cache size. For most cache sizes, the Random policy show to be less effective.|$|R
50|$|The {{expansion}} of the memory space caused an issue for any programs that used the <b>high</b> <b>byte</b> of an address to store data, a programming trick that was successful with those processors that only have a 24-bit address bus (68000 and 68010). A similar problem affected the 68020.|$|E
5000|$|DIV ($E2) - Divide 16-bit / 8-bit - Divides by {{accumulator}} the 16-bit {{data that}} is the contents of M(zz+x+1) for <b>high</b> <b>byte</b> and {{the contents of the}} next address memory for low byte, and stores the quotient in the accumulator and the remainder on the stack as one's complement.|$|E
5000|$|The 16 virtual registers, 32 bytes in total, {{are located}} in the zero page of the Apple II's real, {{physical}} memory map (at -), with values stored as low byte followed by <b>high</b> <b>byte.</b> [...] The SWEET16 interpreter itself is located from [...] to [...] in the Integer BASIC ROM.|$|E
40|$|News {{on demand}} {{features}} user interaction, interdependent media {{and is used}} by different client types. These requirements are not yet accommodated by a single solution. We address this issue by utilizing a priori knowledge about news articles' structure encoded in a presentation plan. Using this, we investigate the gains that knowledge about structure can have for a caching proxy. Our simulations show that structured partial caching of media object segments achieves a <b>higher</b> <b>byte</b> hit ratio compared to a non-structured caching scheme when access patterns follow a Multi-Selection Zipf distribution...|$|R
40|$|In {{this paper}} {{the idea that}} large objects, such as video files, should not be cached or {{replaced}} in their entirety, but rather be partitioned in chunks and replacement decisions be applied at the chunk level is examined. It is shown, that a <b>higher</b> <b>byte</b> hit ratio (BHR) can be achieved through partial replacement. The price paid for the improved BHR performance is that the replacement algorithm, e. g. LRU, takes a longer time to induce the steady state BHR. It is demonstrated that this problem could be addressed by a hybrid caching scheme that employs variable sized chunk...|$|R
50|$|More significantly, GBK {{extended}} {{the range of}} the bytes. Having two-byte characters in the ISO-2022 GR range gives a limit of 94²=8,836 possibilities. Abandoning the ISO-2022 model of strict regions for graphics and control characters, but retaining the feature of low bytes being 1-byte characters and pairs of <b>high</b> <b>bytes</b> denoting a character, you could potentially have 128²=16,384 positions. GBK takes part of that, extending the range from - (94 choices for each byte) to - (126 choices) for the first byte and - (191 choices) for the second byte, for a total of 24,066 positions.|$|R
50|$|The Chinese Foundation for Digitization Technology(中文數位化技術推廣委員會) {{introduced}} Big5+ in 1997, {{which used}} over 20000 code points to incorporate all CJK logograms in Unicode 1.1. However, the extra code points exceeded the original Big-5 definition (Big5+ uses <b>high</b> <b>byte</b> values 81-FE and low byte values 40-7E and 80-FE), preventing {{it from being}} installed on Microsoft Windows.|$|E
50|$|To map {{the code}} points to bytes, add 158 (0x98) to the row {{number of the}} code point to form the <b>high</b> <b>byte,</b> and add 158 column number of the code point to form the low byte. The row number is the code point integer divided by 94, and the column the code point modulo 94.|$|E
5000|$|Most {{parallel}} SCSI buses are terminated at each end. However {{where the}} bus width is not constant, {{it is sometimes}} necessary to provide special termination for the <b>high</b> <b>byte</b> and the associated parity bit. This high-byte termination can use standard SCSI termination techniques, such as a passive terminator, active terminator, or a forced-perfect terminator. This diagram shows a typical SCSI high-byte termination scheme: ...|$|E
40|$|Abstract. The {{importance}} of stencil-based algorithms in computational science has {{focused attention on}} optimized parallel implementations for multilevel cache-based processors. Temporal blocking schemes leverage the large bandwidth and low latency of caches to accelerate stencil updates and approach theoretical peak performance. A key ingredient is the reduction of data traffic across slow data paths, especially the main memory interface. In this work we combine the ideas of multi-core wavefront temporal blocking and diamond tiling to arrive at stencil update schemes that show large reductions in memory pressure compared to existing approaches. The resulting schemes show performance advantages in bandwidth-starved situations, which are exacerbated by the <b>high</b> <b>bytes</b> per lattice update case of variable coefficients. Our thread groups concept provides a controllable trade-off between concurrency and memory usage, shifting the pressure between the memory interface and the CPU. We present performance results on a contemporary Intel processor...|$|R
40|$|Disk {{space in}} shared Web caches can be {{diverted}} to serve some system users {{at the expense}} of others. Cache hits reduce server loads, and if servers desire load reduction to different degrees, a replacement policy which prioritizes cache space across servers can provide differential quality-of-service (QoS). We present a simple generalization of leastfrequently -used (LFU) replacement that is sensitive to varying levels of server valuation for cache hits. Through trace-driven simulation we show that under a particular assumption about server valuations our algorithm delivers a reasonable QoS relationship: <b>higher</b> <b>byte</b> hit rates for servers that value hits more. We furthermore adopt the economic perspective that value received by system users is a more appropriate performance metric than hit rate or byte hit rate, and demonstrate that our algorithm delivers higher "social welfare" (aggregate value to servers) than LRU or LFU. Submitted to the Fourth International Web Caching Workshop, S [...] ...|$|R
40|$|The {{importance}} of stencil-based algorithms in computational science has {{focused attention on}} optimized parallel implementations for multilevel cache-based processors. Temporal blocking schemes leverage the large bandwidth and low latency of caches to accelerate stencil updates and approach theoretical peak performance. A key ingredient is the reduction of data traffic across slow data paths, especially the main memory interface. In this work we combine the ideas of multicore wavefront temporal blocking and diamond tiling to arrive at stencil update schemes that show large reductions in memory pressure compared to existing approaches. The resulting schemes show performance advantages in bandwidth-starved situations, which are exacerbated by the <b>high</b> <b>bytes</b> per lattice update case of variable coefficients. Our thread groups concept provides a controllable trade-off between concurrency and memory usage, shifting the pressure between the memory interface and the CPU. We present performance results on a contemporary Intel processor...|$|R
5000|$|To {{set a new}} jump {{target for}} VVBLKD (Deferred) set the Y {{register}} to the low byte of the target address, the X register to the <b>high</b> <b>byte</b> of the target address, and the Accumulator to 7, then JSR SETVBV.. The user code called through VVBLKD must exit by jumping to the OS Vertical Blank exit routine with a JMP XITVBV ($E462hex/58466dec).|$|E
5000|$|To {{set a new}} jump {{target for}} VVBLKI (Immediate) set the Y {{register}} to the low byte of the target address, the X register to the <b>high</b> <b>byte</b> of the target address, and the Accumulator to 6, then JSR SETVBV. The user code called through VVBLKI simply needs to exit by jumping to the OS Vertical Blank with a JMP SYSVBV ($E45Fhex/58463dec).|$|E
50|$|As a timer counts down, {{its value}} {{can also be}} read {{directly}} by reading its I/O port twice, first for the low byte, and then for the <b>high</b> <b>byte.</b> However, in free-running counter applications {{such as in the}} x86 PC, it is necessary to first write a latch command for the desired channel to the control register, so that both bytes read will belong to one and the same value.|$|E
50|$|In July 2006, Hebrew Wikipedia {{had one of}} the <b>highest</b> {{amounts of}} <b>bytes</b> per article, and the highest of all editions on Wikipedia with over 20,000 articles.|$|R
40|$|Id: wlfu. tex,v 1. 28 1999 / 01 / 22 23 : 33 : 22 tpkelly Exp Disk {{space in}} shared Web caches can be {{diverted}} to serve some sys-tem users {{at the expense}} of others. Cache hits reduce server loads, and if servers desire load reduction to dierent degrees, a replacement policy which prioritizes cache space across servers can provide dieren-tial quality-of-service (QoS). We present a simple generalization of least-frequently-used (LFU) replacement that is sensitive to varying levels of server valuation for cache hits. Through trace-driven simulation we show that under a particular assumption about server valuations our algorithm delivers a reasonable QoS relationship: <b>higher</b> <b>byte</b> hit rates for servers that value hits more. We furthermore adopt the economic perspective that value received by system users is a more appropriate performance metric than hit rate or byte hit rate, and demonstrate that our algorithm delivers higher welfare " (aggregate value to servers) than LRU or LFU...|$|R
30|$|When a {{duplicate}} ACK is received, {{the sender}} checks FirstUnsacked and HighSACK {{to investigate whether}} it is due to congestion. If FirstUnsacked is equal to HighACK, {{it means that the}} FirstUnsacked segment was reordered or dropped due to congestion (we assume that an ACK acknowledges the sequence number or the first byte of the expected data). (HighACK is defined as the sequence number of the <b>highest</b> <b>byte</b> of data that has been cumulatively ACKed at a given point in [34].) Otherwise, if FirstUnsacked is less than HighSACK, it means that the FirstUnsacked segment was reordered or dropped due to congestion while waiting for a new ACK for the segment retransmitted by an ELN. If the segment of HighACK is already SACKed, it means that the segment was dropped in the receiver queue. Therefore, the segment needs to be transmitted again. If the counter of received duplicate ACKs becomes three, the sender triggers the congestion recovery algorithms of fast retransmit and recovery. In fast retransmit phase, the sender retransmits the FirstUnsacked segment. The other operations of ESACK are equal to SACK. Therefore, if there is no wireless loss, ESACK behaves just like SACK.|$|R
50|$|While a {{good use}} of very limited RAM space, this design caused {{problems}} when Apple introduced the Macintosh II, which used the 32-bit Motorola 68020 CPU. The 68020 had 32 physical address lines which could address up to 4 GB (232 bytes) of memory. The flags that the Memory Manager stored in the <b>high</b> <b>byte</b> of each pointer and handle were significant now, and could lead to addressing errors.|$|E
5000|$|The {{format of}} data on the tape is: 100 bytes with the value 0x16 (SYN, Synchronous Idle), one byte with the value 0x2A (*), the record {{identification}} number, the start address (two characters for the low byte of address, two characters for the <b>high</b> <b>byte),</b> the end address (in the same format), the actual data, one byte with the value 0x2F ("/" [...] character [...] ), a two-byte checksum, and two bytes with the value 0x04 (EOT, End Of Transmission).|$|E
5000|$|Parameters {{are pushed}} onto the {{hardware}} stack as 16-bit integers {{in the order}} specified in the [...] function in low byte, <b>high</b> <b>byte</b> order. The last value pushed to the stack is a byte indicating the number of arguments. The machine language code must remove all of these vaues before returning via the [...] instruction. A value can {{be returned to the}} BASIC program by placing it in addresses 21210 and 21310 (D416 and D516) as a 16-bit integer.|$|E
40|$|Abstract: System {{administrators are}} in an {{increasing}} degree involved with the troubleshooting of solving network problems concerning the quality of service for the different applications. Adding more bandwidth {{is not always the}} option to solve network bottlenecks. This is where QoS – Quality of Service aware network plays an important part. If your network has real time traffic like voice, video etc, configuring and maintaining the right QoS parameters becomes all the more important. QoS obviously means Quality of Service. On Analysis it was evaluated that the traffic is queued in "router A " because of the bottleneck. The Custom Queuing mechanism differentiated traffic between queues based on the type of service (TOS). Traffic is sent from each queue in a roundrobin fashion. Queues send traffic proportionally to their byte count. In this network queues with high index have <b>higher</b> <b>byte</b> count. As a result of this classification traffic with higher TOS gets better delay. In case study 2, the traffic is queued in router A because of the bottleneck. In this, the WFQ mechanism differentiates traffic between queues based on the type of service (TOS). Queues send traffic proportionally to their weight. Queues with high index have higher weight. As a result of this classification traffic with higher TOS gets better delay...|$|R
50|$|Some RAM drives use a {{compressed}} {{file system}} such as cramfs to allow compressed {{data to be}} accessed on the fly, without decompressing it first. This is convenient because RAM drives are often small due to the <b>higher</b> price per <b>byte</b> than conventional hard drive storage.|$|R
50|$|Historically, various {{media were}} used to {{transfer}} messages, some of them only supporting 7-bit data, so an 8-bit message had high chances to be garbled during transmission in the 20th century. But some implementations really did not care about formal discouraging of 8-bit data and allowed <b>high</b> bit set <b>bytes</b> to pass through.|$|R
