2146|10000|Public
5|$|Model {{believed}} that the situation represented not just a threat, but also an opportunity to counter-attack and possibly clear the Allies out of the Southern Netherlands. Towards this end, he forbade SS General Willi Bittrich and SS Lieutenant General Heinz Harmel, commanding II SS Panzer Corps and 10th SS Panzer respectively, from destroying the Nijmegen bridge. With the exception of this tactical <b>error,</b> <b>Model</b> is considered to have fought an outstanding battle and handed the Allies a sharp defeat. The bridge at Arnhem was held and the 1st Airborne Division destroyed, dashing the Allies' hopes for a foothold over the Rhine {{before the end of}} the year.|$|E
25|$|Given {{the above}} framework, {{different}} software solutions for detecting SNVs vary {{based on how}} they calculate the prior probabilities , the <b>error</b> <b>model</b> used to model the probabilities , and the partitioning of the overall genotypes into separate sub-genotypes, whose probabilities can be individually estimated in this framework.|$|E
25|$|A simple <b>error</b> <b>model</b> is to {{introduce}} a small error to the data probability term in the homozygous cases, allowing a small constant probability that nucleotides which don't match the A allele are observed in the AA case, and respectively a small constant probability that nucleotides not matching the B allele are observed in the BB case. However more sophisticated procedures are available which attempt to more realistically replicate the actual error patterns observed in real data in calculating the conditional data probabilities. For instance, estimations of read quality (measured as Phred quality scores) have been incorporated in these calculations, {{taking into account the}} expected error rate in each individual read at a locus. Another technique that has successfully been incorporated into error models is base quality recalibration, where separate error rates are calculated - based on prior known information about error patterns - for each possible nucleotide substitution. Research shows that each possible nucleotide substitution is not equally likely to show up as an error in sequencing data, and so base quality recalibration has been applied to improve error probability estimates.|$|E
40|$|In {{this note}} we {{consider}} the class of weak nondifferential measurement <b>error</b> <b>models,</b> which as a special case, contains the class of the nondifferential measurement <b>error</b> <b>models</b> (Carroll et al., 1995). Examples of measurement <b>error</b> <b>models</b> which are in this class and that are not nondifferential models are considered. Only simple linear regression models are used to illustrate the approach but as indicated it can be generalized to more complex situations. Some characterization results are also reported. Error-in-variables <b>models</b> Nondifferential <b>error</b> <b>models</b> Differential <b>error</b> <b>models</b> Weak nondifferential <b>error</b> <b>models...</b>|$|R
40|$|Measurement <b>error</b> <b>modeling</b> {{is crucial}} to any assay method. Realistic <b>error</b> <b>models</b> {{prioritize}} efforts to reduce key error components and provide a way to estimate total ("random" and "systematic") measurement error variances. This paper uses multi-laboratory data to estimate random error and systematic error variances for seven analytical chemistry destructive assay methods for five analytes (Gallium, Iron, Silicon, Plutonium, and Uranium). Because these variance estimates are based on multiple-component <b>error</b> <b>models,</b> strategies are described for choosing and then fitting <b>error</b> <b>models</b> that allow for lab-to-lab variation. </p...|$|R
40|$|This paper reviews recent {{developments}} in nonparametric identi. cation of mea- surement <b>error</b> <b>models</b> and their applications in applied microeconomics, in particular, in empirical industrial organization and labor economics. Measurement <b>error</b> <b>models</b> describe mappings from a latent distribution to an observed distribution. The identification and estimation of measurement <b>error</b> <b>models</b> focus on how to obtain the latent distribution and the measurement error distribution from the observed distribution. Such a framework may be suitable for many microeconomic models with latent variables, such as models with unobserved heterogeneity or unobserved state variables and panel data models with fixed effects. Recent developments in measurement <b>error</b> <b>models</b> allow very flexible specification of the latent distribution and the measurement error distribution. These developments greatly broaden economic applications of measurement <b>error</b> <b>models.</b> This paper provides an accessible introduction of these technical results to empirical researchers so as to expand applications of measurement <b>error</b> <b>models...</b>|$|R
2500|$|The <b>error</b> <b>model</b> used in {{creating}} a probabilistic method for variant calling {{is the basis for}} calculating the [...] term used in Bayes' theorem. If the data was assumed to be error free, then the distribution of observed nucleotide counts at each locus would follow a Binomial Distribution, with 100% of nucleotides matching the A or B allele respectively in the AA and BB cases, and a 50% chance of each nucleotide matching either A or B in the AB case. However, in presence of noise in the read data this assumption is violated, and the [...] values need to account for the possibility that erroneous nucleotides are present in the aligned reads at each locus.|$|E
5000|$|For example, {{the above}} {{fraction}} <b>error</b> <b>model</b> for values becomes the fractional <b>error</b> <b>model</b> for functions {{by adding a}} parameter x to the definition: ...|$|E
5000|$|Joseph Berkson (1899 - 1982) {{was trained}} as a {{physicist}} (M.A., 1922, Columbia), physician (M.D., 1927, Johns Hopkins), and statistician (Dr.Sc., 1928, Johns Hopkins). In 1950, as Head (1934-1964) of the Division of Biometry and Medical Statistics of the Mayo Clinic, Rochester, Minnesota, Berkson wrote a key paper entitled Are there two regressions?. In this paper Berkson proposed an <b>error</b> <b>model</b> for regression analysis that contradicted the classical <b>error</b> <b>model</b> until that point assumed to generally apply and this has since been termed the Berkson <b>error</b> <b>model.</b> Whereas the classical <b>error</b> <b>model</b> is statistically independent of the true variable, Berkson's model is statistically independent of the observed variable. Carroll et al. (1995) refer to {{the two types of}} error models as follows: ...|$|E
40|$|Compared with {{ordinary}} regression models, the computational cost for estimating parame-ters in general measurement <b>error</b> <b>models</b> is often {{much more expensive}} because the estimation procedures typically require solving integral equations. In addition, natural criteria functions are often unavailable for general measurement <b>error</b> <b>models.</b> Thus, the traditionally best vari-able selection procedures become infeasible in the measurement <b>error</b> <b>models</b> context. In this paper, we develop a new framework for variable selection in measurement <b>error</b> <b>models</b> via penalized estimating equations. We first propose {{a new class of}} variable selection procedures for general parametric measurement <b>error</b> <b>models</b> and for general semiparametric measurement <b>error</b> <b>models,</b> and study the asymptotic properties of the proposed procedures. Then, under certain regularity conditions and with a properly chosen regularization parameter, we demon-strate that the proposed procedure performs as well as an oracle procedure. We assess the finite sample performance by Monte Carlo simulation studies and illustrate the proposed methodology through the empirical analysis of a real data set. ...|$|R
40|$|The Transmission Control Protocol (TCP) {{has been}} {{optimized}} {{over the years}} for wired networks. With the proliferation of wireless communications, TCP has to perform well in transmission-media heterogeneous infrastructures, which exhibit different characteristics than the wired ones, e. g. random segment corruption over wireless links. Different improvement proposals attempt to alleviate TCP's inefficiencies in a wired-cum-wireless environment. Testing the comparative efficiency of these proposals requires realistic <b>error</b> <b>models.</b> However, modeling such an environment is challenging, and researchers have followed different approaches. This work focuses on the implications for TCP performance with respect to two broad characteristics of <b>error</b> <b>modeling.</b> Firstly, we consider rate-based <b>error</b> <b>models</b> vs. temporal <b>error</b> <b>models.</b> Rate-based <b>error</b> <b>models</b> drop a specified proportion of segments irrespective of protocol behavior. Temporal <b>error</b> <b>models</b> capture {{the effect of the}} channel going "bad" for a specified proportion of time, during which transmitted segments will be lost if they happen to be in transit during that time. Secondly, we consider <b>error</b> <b>models</b> that "synchronize" losses in the forward and reverse paths vs. models that do not do so, or which model losses in the forward path only...|$|R
5000|$|... <b>error</b> <b>models</b> {{including}} the Classical Measurement <b>Error</b> <b>models</b> and <b>Error</b> Calibration <b>Models,</b> where the conditional distribution of W given (Z, X) is modeled — {{use of such}} a model is appropriate when attempting to determine X directly, but this is prevented by various errors in measurement.|$|R
50|$|GeoDa has {{powerful}} {{capabilities to}} perform spatial analysis, multivariate exploratory data analysis, and global and local spatial autocorrelation. It also performs basic linear regression. As for spatial models, both the spatial lag {{model and the}} spatial <b>error</b> <b>model,</b> both estimated by maximum likelihood, are included.|$|E
50|$|The Berkson <b>error</b> <b>model</b> is a {{description}} of random error (or misclassification) in measurement. Unlike classical error, Berkson error causes little or no bias in the measurement. It was proposed by Joseph Berkson in an article entitled “Are there two regressions?,” published in 1950.|$|E
50|$|Given {{the above}} framework, {{different}} software solutions for detecting SNVs vary {{based on how}} they calculate the prior probabilities , the <b>error</b> <b>model</b> used to model the probabilities , and the partitioning of the overall genotypes into separate sub-genotypes, whose probabilities can be individually estimated in this framework.|$|E
40|$|Model {{validation}} and {{estimating the}} size of a possible <b>model</b> <b>error</b> is a central aspect of System Identification. In this contribution we discuss the <b>model</b> <b>error</b> concepts and <b>model</b> <b>error</b> <b>modeling</b> for control design. Of special interest is how to make use of periodic inputs, and how to deal with non-linear <b>error</b> <b>models.</b> The discussion is limited to SISO models and stability robustness issues...|$|R
40|$|The {{uncertainty}} in the geopotential model is currently {{one of the major}} error sources in the orbit determination of low-altitude Earth-orbiting spacecraft. The results of an investigation of different geopotential <b>error</b> <b>models</b> and modeling approaches currently used for operational orbit error analysis support at the Goddard Space Flight Center (GSFC) are presented, with emphasis placed on sequential orbit error analysis using a Kalman filtering algorithm. Several geopotential models, known as the Goddard Earth Models (GEMs), were developed and used at GSFC for orbit determination. The errors in the geopotential models arise from the truncation errors that result from the omission of higher order terms (omission errors) and the errors in the spherical harmonic coefficients themselves (commission errors). At GSFC, two <b>error</b> <b>modeling</b> approaches were operationally used to analyze the effects of geopotential uncertainties on the accuracy of spacecraft orbit determination - the lumped <b>error</b> <b>modeling</b> and uncorrelated <b>error</b> <b>modeling.</b> The lumped <b>error</b> <b>modeling</b> approach computes the orbit determination errors on the basis of either the calibrated standard deviations of a geopotential model's coefficients or the weighted difference between two independently derived geopotential <b>models.</b> The uncorrelated <b>error</b> <b>modeling</b> approach treats the errors in the individual spherical harmonic components as uncorrelated error sources and computes the aggregate effect using a combination of individual coefficient effects. This study assesses the reasonableness of the two <b>error</b> <b>modeling</b> approaches in terms of global error distribution characteristics and orbit error analysis results. Specifically, this study presents the global distribution of geopotential acceleration errors for several gravity <b>error</b> <b>models</b> and assesses the orbit determination errors resulting from these <b>error</b> <b>models</b> for three types of spacecraft - the Gamma Ray Observatory, the Ocean Topography Experiment, and the Cosmic Background Explorer...|$|R
2500|$|In statistics, errors-in-variables <b>models</b> or {{measurement}} <b>error</b> <b>models</b> ...|$|R
50|$|Raymond James Carroll is an American statistician, and Distinguished Professor of Statistics, Nutrition and Toxicology at Texas A&M University. He is a {{recipient}} of 1988 COPSS Presidents' Award and 2002 R. A. Fisher Lectureship. He has made fundamental contributions to measurement <b>error</b> <b>model,</b> nonparametric and semiparametric modeling.|$|E
50|$|Spelling {{correction}} is {{the process}} of automatically detecting and correcting spelling errors in search queries. Most spelling correction algorithms are based on a language model, which determines the a priori probability of an intended query, and an <b>error</b> <b>model</b> (typically a noisy channel model), which determines the probability of a particular misspelling, given an intended query.|$|E
50|$|A model audit is the colloquial {{term for}} the tasks {{performed}} when conducting due diligence on a financial model, in order to eliminate spreadsheet <b>error.</b> <b>Model</b> audits are {{sometimes referred to as}} model reviews, primarily to avoid confusion with financial audit. A study in 1998 concluded that even MBA students with over 250 hours of spreadsheet development experience had a 24% chance of introducing spreadsheet <b>error.</b> <b>Model</b> audits are typically requested by banking organisations, in order to reassure lenders and investors alike that the calculations and assumptions within the model are correct, and that the results produced by the model can be relied upon. When a comprehensive review of the model is required, the scope of review is often extended to include tax and accounting, sensitivity testing and the checking of data contained within the model back to the original financing and legal documentation.|$|E
40|$|Wrong human-robot {{interactions}} {{are at the}} origin of severe damages. Safety requirements ask the analysis of these interactions. At first, erroneous interactions have to be identified. In this paper, we propose to use UML (Unified Modeling Language) to specify human robot interaction. Then, generic <b>error</b> <b>models,</b> associated with the message feature provided by UML, are presented. These <b>error</b> <b>models</b> allow interaction <b>errors</b> to be automatically deduced from the modeling of the human-robot interactions. The use of these generic <b>error</b> <b>models</b> is illustrated on a medical robot for teleechography...|$|R
40|$|An area-level model {{approach}} to combining information from several sources is {{considered in the}} context of small area estimation. At each small area, several estimates are computed and linked through a system of structural <b>error</b> <b>models.</b> The best linear unbiased predictor of the small area parameter can be computed by the general least squares method. Parameters in the structural <b>error</b> <b>models</b> are estimated using the theory of measurement <b>error</b> <b>models.</b> Estimation of mean squared errors is also discussed. The proposed method is applied to the real problem of labor force surveys in Korea...|$|R
40|$|We have {{developed}} an on-line and in-field <b>error</b> <b>modeling</b> technique, which is a generalization of the calibration problem, that relies on {{a small number of}} inaccurate sensors with known error distributions to develop <b>error</b> <b>models</b> for the deployed in-field sensors. We demonstrate the applicability of our transitive <b>error</b> <b>modeling</b> technique and evaluate its performance in various scenarios by conducting experiments using traces of the light intensity measurements recorded by in-field deployed light sensors. In addition, statistical validation and evaluation methods such as resubsitution are used in order to establish the interval of confidence...|$|R
50|$|External data {{saved in}} *.xls or *.txt files {{can be added}} to a model {{creating}} a model-data-couple. A mapping dialog allows to connect data column names to observed species names. Meta information in the data files comprise information about the experimental setting. Measurement errors are either stored in the data files, will be calculated using an <b>error</b> <b>model,</b> or are estimated automatically.|$|E
50|$|While Bower was {{attending}} Yale for his degree in Experimental Psychology, he discovered {{a passion for}} learning theory and presented his findings on dual reward-punishment in rats to the American Psychological Association. During this time, he and Bill Estes also revised Edward Tolman's vicarious trial and <b>error</b> <b>model</b> to include human choices among commodity options. Bower married Sharon Anthony on January 30, 1957.|$|E
5000|$|... ==HadCRUT4== HadCRUT4 was {{introduced}} in March 2012. It [...] "includes the addition of newly digitised measurement data, both over land and sea, new sea-surface temperature bias adjustments and a more comprehensive <b>error</b> <b>model</b> for describing uncertainties in sea-surface temperature measurements". Overall, {{the net effect of}} HadCRUT4 versus HadCRUT3 is an increase in the average temperature anomaly, especially around 1950 and 1855, and less significantly around 1925 and 2005. [...] Current HadCRUT4 Graph.|$|E
40|$|A design {{verification}} methodology for microprocessor hardware based on <b>modeling</b> design <b>errors</b> and generating simulation vectors for the <b>modeled</b> <b>errors</b> via physical fault testing techniques is presented. We have systematically collected design error {{data from a}} number of microprocessor design projects. The error data is used to derive <b>error</b> <b>models</b> suitable for {{design verification}} testing. A class of basic <b>error</b> <b>models</b> is identified [...] ...|$|R
40|$|An {{extensive}} {{investigation was}} conducted to provide realistic <b>error</b> <b>models</b> for Global Positioning System (GPS) related numerical simulation. This study considers most of the important error sources for measurement and dynamic models which are currently being used for GPS geodetic applications. These <b>error</b> <b>models</b> were evaluated by comparing with real GPS data...|$|R
40|$|A design {{verification}} methodology for microprocessor hardware based on <b>modeling</b> design <b>errors</b> and generating simulation vectors for the <b>modeled</b> <b>errors</b> via physical fault testing techniques is presented. We have systematically collected design error {{data from a}} number of microprocessor design projects. The error data is used to derive <b>error</b> <b>models</b> suitable for {{design verification}} testing. A class of basic <b>error</b> <b>models</b> is identified and shown to yield tests that provide good coverage of common error types. To improve coverage for more complex errors, a new class of conditional <b>error</b> <b>models</b> is introduced. An experiment {{to evaluate the effectiveness of}} our methodology is presented. Single actual design errors are injected into a correct design, and it is determined if the methodology will generate a test that detects the actual errors. The experiment has been conducted for two microprocessor designs and the results indicate that very high coverage of actual design errors can be obtained with test sets that are complete for a small number of synthetic <b>error</b> <b>models...</b>|$|R
50|$|Carroll's {{many areas}} of {{research}} include measurement <b>error</b> <b>model,</b> nonparametric and semiparametric regression, inverse problem, functional data analysis, case-control studies, among others. His work has a broad variety of application fields, including radiation and nutritional epidemiology, molecular biology, genomics and many others. He has authored or coauthored 4 books, over 300 refereed papers and has given over 300 invited talks. He has supervised and mentored more than 30 Ph.D. students and can claim more than 90 descendants in his mathematical genealogy.|$|E
5000|$|... #Caption: A set of {{hypothetical}} NGS reads are shown, aligned {{against a}} reference sequence. At the annotated locus, the reads contain {{a mixture of}} A/G nucleotides, against the A reference allele. Depending on the prior genotype probabilities, and the chosen <b>error</b> <b>model,</b> this may be called as a heterozygous SNV (genotype AG predicted), the G nucleotides may be classified as errors and no variant called (genotype AA predicted), or alternatively the A nucleotides may be classified as errors and a homozygous SNV called (genotype GG predicted).|$|E
5000|$|A common {{example of}} an info-gap model is the {{fractional}} <b>error</b> <b>model.</b> The best estimate of an uncertain function [...] is , but the fractional error of this estimate is unknown. The following unbounded family of nested sets of functions is a fractional-error info-gap model:At any horizon of uncertainty , the set [...] contains all functions [...] whose fractional deviation from [...] is no greater than [...] However, the horizon of uncertainty is unknown, so the info-gap model is an unbounded family of sets, {{and there is no}} worst case or greatest deviation.|$|E
40|$|Data fusion using Kalman filters {{requires}} {{reasonably good}} <b>error</b> <b>models.</b> Our intention to fuse line segments, corners and edges {{obtained from a}} laser scanner and from advanced sonars provided the motivation on the investigation of Sick PLS laser scanner range measurement reliability, and line segment estimation precision. We present an approach for fitting lines straight in the lasers polar coordinate system, which enables a simple estimation of line parameter covariance. We also develop systematic <b>error</b> <b>models</b> for line parameter estimation. Finally we measure our systematic and random <b>error</b> <b>models</b> experimentally, and show, that systematic errors can be larger than random ones. Contents...|$|R
30|$|What’s more, Enming MIAO {{makes some}} {{research}} about the thermal <b>error</b> <b>modeling</b> method. As the existing of {{the volatility of}} temperature-sensitive points, the forecasting accuracy of multivariate regression model is severely affected, and the forecasting robustness becomes poor. In order to overcome this effect and enhance the robustness of thermal <b>error</b> compensation <b>model,</b> a modeling method of establishing thermal <b>error</b> <b>models</b> by using single temperature variable under the jamming of temperature-sensitive points’ volatility is put forward.|$|R
50|$|Other <b>error</b> <b>models</b> {{may also}} be considered, and {{thresholds}} found. In all cases studied so far, the code {{has been found to}} saturate the Hashing bound. For some <b>error</b> <b>models,</b> such as biased errors where bit errors occur more often than phase errors or vice versa, lattices other than the square lattice must be used to achieve the optimal thresholds.|$|R
