72|704|Public
25|$|Both {{the primal}} and the dual {{problems}} {{make use of}} the same matrix. In the primal space, this matrix expresses the consumption of physical quantities of inputs necessary to produce set quantities of outputs. In the dual space, it expresses the creation of the economic values associated with the outputs from set <b>input</b> <b>unit</b> prices.|$|E
2500|$|Before {{entering}} an LZB controlled section {{the driver}} must enable the train by entering the required {{information on the}} Driver <b>Input</b> <b>Unit</b> and enabling LZB. When enabled the train will light a [...] "B" [...] light.|$|E
2500|$|The primal problem {{deals with}} {{physical}} quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce {{so as to}} maximize total revenue? The dual problem deals with economic values. With floor guarantees on all output unit prices, and assuming the available quantity of all inputs is known, what <b>input</b> <b>unit</b> pricing scheme to set so as to minimize total expenditure? ...|$|E
50|$|Structurally, a {{neural network}} has three {{different}} classes of units: <b>input</b> <b>units,</b> hidden units, and output units. An activation pattern is {{presented at the}} <b>input</b> <b>units</b> and then spreads in a forward direction from the <b>input</b> <b>units</b> through one or more layers of hidden units to the output units. The activation coming into one unit from other unit is multiplied by the weights on the links over which it spreads. All incoming activation is then added together and the unit becomes activated only if the incoming result is above the unit’s threshold.|$|R
5000|$|... #Caption: A simple {{neural network}} with two <b>input</b> <b>units</b> and one output unit ...|$|R
50|$|A {{recurrent}} {{neural network}} for this algorithm consists of some <b>input</b> <b>units,</b> some output units and eventually some hidden units.|$|R
5000|$|... 6N3P {{was widely}} used for FM band radio <b>input</b> <b>unit</b> stages (nearly all 1960s Soviet radios with FM band {{employed}} the same <b>input</b> <b>unit</b> {{on a separate}} subchassis). Currently it has found use in DIY preamps. A ruggedized/industrial version of the tube is designated 6N3P-EV (Russian: 6Н3П-ЕВ) ...|$|E
5000|$|Before {{entering}} an LZB controlled section {{the driver}} must enable the train by entering the required {{information on the}} Driver <b>Input</b> <b>Unit</b> and enabling LZB. When enabled the train will light a [...] "B" [...] light.|$|E
50|$|Both {{the primal}} and the dual {{problems}} {{make use of}} the same matrix. In the primal space, this matrix expresses the consumption of physical quantities of inputs necessary to produce set quantities of outputs. In the dual space, it expresses the creation of the economic values associated with the outputs from set <b>input</b> <b>unit</b> prices.|$|E
50|$|Power is {{calculated}} based on rotational speed x torque x constant, with the constant varying with the output unit desired and the <b>input</b> <b>units</b> used.|$|R
40|$|By {{frame of}} {{reference}} transformations, an input variable in one coordinate system is transformed into an output variable in a different coordinate system depending on another input variable. If the variables are represented as neural population codes, then a sigma–pi network is a natural way of coding this transformation. By multiplying two inputs it detects coactivations of <b>input</b> <b>units,</b> and by summing over the multiplied <b>inputs,</b> one output <b>unit</b> can respond invariantly to different combinations of coactivated <b>input</b> <b>units.</b> Here, we present a sigma–pi network and a learning algorithm by which the output representation self-organizes to form a topographic map. This network solves the {{frame of reference}} transformation problem by unsupervised learning. r 2006 Elsevier B. V. All rights reserved...|$|R
30|$|Finally, the {{selected}} noise segment is scaled {{to reach the}} required SNR level and then is used to corrupt the clean speech signal. The noise-corrupted speech is fed into the DNN <b>input</b> <b>units</b> to conduct model training.|$|R
5000|$|The {{neural network}} {{is a type}} of parallel-processing {{architecture}} that transforms any stimulus received by the <b>input</b> <b>unit</b> (i.e., stimulus units) to a signal for the output unit (i.e., response units) through a series of mid-level hidden units. Each unit in the input layer is connected to each unit in the hidden layer and, in turn, to each unit in the output layer.|$|E
50|$|The {{computer}} term bootstrap {{began as}} a metaphor in the 1950s. In computers, pressing a bootstrap button caused a hardwired program to read a bootstrap program from an <b>input</b> <b>unit.</b> The computer would then execute the bootstrap program, which caused it to read more program instructions. It became a self-sustaining process that proceeded without external help from manually entered instructions. As a computing term, bootstrap has been used since at least 1953.|$|E
5000|$|The primal problem {{deals with}} {{physical}} quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce {{so as to}} maximize total revenue? The dual problem deals with economic values. With floor guarantees on all output unit prices, and assuming the available quantity of all inputs is known, what <b>input</b> <b>unit</b> pricing scheme to set so as to minimize total expenditure? ...|$|E
50|$|For a given set of (input, target) states, {{the network}} is trained to settle into a stable {{activation}} state with the output units in the target state, based on a given input state clamped on the <b>input</b> <b>units.</b>|$|R
50|$|The PFC (for output gating) has a localist {{one-to-one}} {{representation of}} the <b>input</b> <b>units</b> for every stripe. Thus, {{you can look at}} these PFC representations and see directly what the network is maintaining. The PFC maintains the working memory needed to perform the task.|$|R
30|$|In addition, for {{practical}} problems that {{come down to}} real-valued data, Gaussian-Bernoulli RBM is introduced {{to deal with this}} issue. <b>Input</b> <b>units</b> of this model are linear while hidden units are still binary. Learning procedure for Gaussian-Bernoulli RBM is very similar to binary RBM introduced above.|$|R
30|$|From the <b>input</b> <b>unit</b> to {{the first}} hidden layer H 1 it is {{calculated}} as follows.|$|E
40|$|DE 20208162 U UPAB: 20021031 NOVELTY - A {{display unit}} (7) {{produces}} large area images {{on a screen}} (8) from control signals. A user (9) <b>input</b> <b>unit</b> (1) is moved freely {{in front of the}} screen. An input is made using a control and associated sensor (2, 3). A tracking unit (4, 5) registers position and orientation of the <b>input</b> <b>unit</b> relative to the screen. The processor (6) varies the image in real time, in accordance with the inputs. A spray on the <b>input</b> <b>unit</b> (1) varies the image. USE - A system producing large area digital images for artists and commercial artists especially. ADVANTAGE - The system produces and records large area images using computerized means, without limiting artistic expression, or at least allowing greater freedom than other existing systems...|$|E
40|$|A {{digital signal}} {{generator}} includes an <b>input</b> <b>unit</b> configured to receive signal information {{of a target}} data signal, a controller configured to calculate at least two delay values {{and at least two}} data values, the at least two delay values and the at least two data values being used to generate a data signal corresponding to the signal information input through the <b>input</b> <b>unit,</b> a multi-phase clock generator configured to delay a reference clock signal based on the at least two delay values to generate at least two clock signals having different phases, a signal generator configured to generate at least two data signals by assigning the at least two data values to the at least two clock signals, and a logic gate unit configured to generate the data signal corresponding to the signal information input through the <b>input</b> <b>unit</b> based on the at least two data signals. Samsung Electronics Co., Ltd. Georgia Tech Research Corporatio...|$|E
40|$|Conditional {{restricted}} Boltzmann {{machines are}} undirected stochastic neural networks {{with a layer}} of <b>input</b> and output <b>units</b> connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the <b>input</b> <b>units,</b> parametrized by interaction weights and biases. We address the representational power of these models, proving results on the minimal size of universal approximators of condi...|$|R
3000|$|Equation 5 {{indicates}} that injecting noises to the <b>input</b> <b>units</b> {{is equivalent to}} placing a regularization on the cost function. This regularization {{is related to the}} second-order derivatives of the cost function with respect to the input, and its strength is controlled by the magnitude of the injected noise. Since ▽ 2 [...]...|$|R
40|$|Recurrent Neural Networks (RNNs) are connectionist {{models that}} operate in {{discrete}} time using feedback connections. An RNN has {{a set of}} units, each taking a real value in each timestep, {{and a set of}} weighted connections between its <b>units.</b> The <b>input</b> <b>units</b> are set by the environment and the output units ar...|$|R
40|$|Quality of Service (QoS) in IP {{multimedia}} converged networks {{which provides}} different serving levels of different traffic flows {{is a crucial}} issue. The token bucket mechanism is one of core QoS concept whose well understanding is required and improve modelling and implementation techniques required in such multimedia networks. This paper describes and defines two basic network elements- a work conserving link and a token bucket. We define {{two different types of}} a token bucket and we show that both of them can be transformed into some work conserving link. Transformation means that the work conserving link allows <b>input</b> <b>unit</b> to pass if and only if the token bucket would allow <b>input</b> <b>unit</b> to pass...|$|E
40|$|An event {{sequence}} detector {{is described}} with input units, each {{associated with a}} row of bistable elements arranged {{in an array of}} rows and columns. The detector also includes a shift register which is responsive to clock pulses from any of the units to sequentially provide signals on its output lines each of which is connected to the bistable elements in a corresponding column. When the event-indicating signal is received by an <b>input</b> <b>unit</b> it provides a clock pulse to the shift register to provide the signal on one of its output lines. The <b>input</b> <b>unit</b> also enables all its bistable elements so that the particular element in the column supplied with the signal from the register is driven to an event-indicating state...|$|E
30|$|The {{parameters}} γ(k) and β(k) are learned {{based on}} the original model parameters. By normalizing each <b>input</b> <b>unit</b> to have zero mean and unit variance, the batch normalization layer helps deal with poor initialization problems at the training stage and contributes to gradient flow in deeper models.|$|E
40|$|This paper {{discusses}} {{in detail}} the vast and rich agricultural knowledge that India has developed since ancient times, and the entire agricultural community trying to find an alternative sustainable farming system, which is ecologically sound, economically and socially acceptable. [Working Paper No. 2010 - 04 - 01]Efficiency, organic <b>input</b> <b>units,</b> DEA analysis, drivers for efficiency...|$|R
5000|$|For example, {{below is}} shown a neural network with two <b>input</b> <b>units</b> (i1 and i2), two hidden units (h1 and h2), and one output unit (o1). It has {{a total of}} six {{connections}} with six corresponding weights represented by the numerals 1-6 (for simplicity, the thresholds are all equal to 1 and are omitted): ...|$|R
40|$|On-line {{learning}} in layered perceptrons is often hampered by plateaus {{in the time}} dependence of the performance. Studies on backpropagation in networks with {{a small number of}} <b>input</b> <b>units</b> have revealed that correlations between subsequently presented patterns shorten the length of such plateaus. We show how to extend the statistical mechanics framework to quantitatively check the effect of correlations on {{learning in}} networks with a large number of <b>input</b> <b>units.</b> The surprisingly compact description we obtain makes it possible to derive properties of on-learning with correlations directly from studies on on-line learning without correlations. Real World Computing Program y Foundation for Neural Networks 1 Introduction In recent years, considerable {{progress has been made in}} the study of on-line learning [1, 2, 3, 4]. The usual assumption is that presented examples are uncorrelated in time. This assumption is not only unnatural for biological learning systems, but also for artificial [...] ...|$|R
40|$|Abstract. Electricity-machinery {{converter}} is the <b>input</b> <b>unit</b> of electro-hydraulic {{proportional valve}} and mechanical and electrical switching device, and the input electrical signal is proportionally converted into mechanical quantity by electricity-machinery converter. The moving coil electromechanical converter {{is used as}} the main research object in this paper. The static magnetic fields of electric-machine converter are analyzed that includes four aspects. These aspects include the structure of thrust coil framework, the permanent magnet material, single magnetization technique of permanent magnet, a number of permanent magnet magnetization array structures. Introduction of Electric- Machine Converter With the rapid development of hydraulic transmission technology, electro-hydraulic proportional valve {{as a representative of}} the electric- machine conversion component which has been widely used in mechanical engineering. Electric- machine converter as <b>input</b> <b>unit</b> of electro-hydraulic proportional valve and mechanical and electrical switching device, the input signal (voltage, current) is continuou...|$|E
40|$|The {{invention}} {{relates to}} {{a method for}} monitoring user activity on a mobile device, comprising an input and an output unit, comprising the following steps preferably in the following order: detecting and / or logging user activity on said <b>input</b> <b>unit,</b> identifying a foreground running application, hashing of a user-interface-element management list of the foreground running application, and creating a screenshot comprising items displayed on said <b>input</b> <b>unit.</b> The invention also relates to a method for analyzing user activity at a server, comprising the following step: obtaining {{at least one of}} an information about detected and / or logged user activity, an information about a foreground running application, a hashed user-interface-element management list and a screenshot from a mobile device. Further, a computer program product is provided, comprising one or more computer readable media having computer executable instructions for performing the steps of {{at least one of the}} aforementioned methods...|$|E
40|$|The aim of {{my project}} was {{introduction}} with {{pulse code modulation}} and synchronous digital hierarchy. Obtained knowledge used with PCM 30 U device and Marconi's SDH device. These devices are currently new in our lab. Practical part consists of connect devices in to functional state. System PCM 30 U is using as the <b>input</b> <b>unit</b> of SDH ring. Functionality of connection verify...|$|E
30|$|The back {{propagation}} technique of multi layer perceptron {{has a significant}} role in supervised learning procedure. The network has been trained for optimization of classification performance by using the procedure of {{back propagation}}. For each training tuple, the weights were modified so as to minimize the mean squared error between the network prediction and the target value. These modifications {{have been made in the}} backward direction through each hidden layer down to the first hidden layer. The input feature vectors have been fed to the <b>input</b> <b>units</b> which comprised the input layer. The number of <b>input</b> <b>units</b> has been dependent on the summation of the number of attributes in the feature vector dataset and the bias node. The subsequent layer has been the hidden layer whose number of nodes has to be determined by considering the half of the summation of the number of classes and the number of attributes per class. The inputs that have passed the input layer have to be weighted and fed simultaneously to the hidden layer for further processing. Weighted output of the hidden layer was used as input to the final layer which has been named as the output layer. The number of units in the output layer has been denoted by the number of class labels. The feed forward property of this architecture does not allow the weights to cycle back to the <b>input</b> <b>units.</b>|$|R
2500|$|... {{concept of}} {{material}} <b>input</b> per <b>unit</b> of service (MIPS) is quantified {{in terms of}} the second law of thermodynamics, allowing the calculation of both resource input and service output in exergy terms. This exergetic material <b>input</b> per <b>unit</b> of service (EMIPS) has been elaborated for transport technology. The service not only takes into account the total mass to be transported ...|$|R
50|$|In GEP neural {{networks}} (GEP-NN or GEP nets), the network architecture is encoded {{in the usual}} structure of a head/tail domain. The head contains special functions/neurons that activate the hidden and output units (in the GEP context, all these units are more appropriately called functional units) and terminals that represent the <b>input</b> <b>units.</b> The tail, as usual, contains only terminals/input units.|$|R
