13|55|Public
40|$|GIS Systems {{excel at}} {{manipulating}} spatial information, {{but may not}} always adequately reflect the level of uncertainty associated with that information. Spatial simulations, conditioned to honor existing data {{as well as a}} variogram model, can provide both qualitative and quantitative evaluations of spatial uncertainty in interpolated data. Display of several alternate simulations can graphically illustrate the degree of uncertainty to the <b>data</b> <b>interpreter.</b> The use of alternate simulations as input to other models provides a mechanism for quantifying the sensitivity of complex systems to uncertainty or spatial variability...|$|E
40|$|The essay retraces the {{evolution}} of consumers’ private law – in particular, it emphasises the rights and remedies available to users of national and local public services: the analyses of recent legislation – with a particular reference to the code of consumption – explains how the users of public services have acquired new rights (among which that of predetermination of qualitative and quantitative standards) which may be sought in sources that {{are different from the}} legislative ones and, most of all, from the service charters. The interaction among these new sources and users’ contracts offers the <b>data</b> <b>interpreter</b> an interesting picture, from which the author draws some stunning conclusions...|$|E
40|$|Abstract “The Inertial Measurement Unit (IMU) is an {{electronic}} unit which records angular velocity and linear acceleration data which is fed into a {{central processing unit}} for data interpreting and logging. The unit constitutes of two independent sensors. The first sensor is the 3 -axis gyroscope and second sensor is the 3 -axis accelerometer. The IMU should also have a <b>data</b> <b>interpreter</b> that can draw the track it went through. This should enable the engineers to calculate the co-ordinates, based on longitude and latitude measures. To improve {{the effectiveness of the}} device, the device could possibly be designed to transfer the data wirelessly using Bluetooth technology to the nearby data logging machine, which can simultaneously calculate the algorithms to figure the correct of the vessel piping system...|$|E
40|$|Abstract. In model-driven development, {{the use of}} both model translators {{and model}} interpreters is widespread. It is also {{well-known}} that partial evaluation can turn an interpreter into a translator. In this paper we show that a simple online partial evaluator is effective at specializing a model interpreter {{with respect to a}} model to create a compiled model interpretation. Data models pose a particular problem, because {{it is not clear what}} a <b>data</b> model <b>interpreter</b> would do, given that data is generally considered to be passive. We show how a <b>data</b> model <b>interpreter</b> can be defined in an object-oriented style as a dynamic message-processing function. Partial evaluation can then be applied to this <b>data</b> model <b>interpreter</b> to create a static dispatch function, analogous to a normal static class definition. We also consider the case of user interface model interpreters, and show that partial evaluation and deforestation can produce good specialized code. The user interface interpreter illustrates a solution to integrating two modeling languages. The system described here is bootstrapped from Scheme, although the goal is to build a complete software development environment based on model interpreters. ...|$|R
40|$|The {{problem of}} {{interpreting}} what is “behind the turns”, not explicitly said by participants in their utterances, has presented a dilemma {{in studies of}} dialogue interpreting, leading to controversies about how far interpreters should engage in dealing with implicit issues they know about, but which are not made clear by the interlocutors. In this paper, I analyse data where a guide and an interpreter present a group of tourists with locations where the history and tradition of local products are exhibited. In my <b>data</b> <b>interpreters</b> expand the guides’ presentation in their rendition, adding {{quite a lot of}} information they know about, but which has not been explicitly mentioned by the guide. I suggest that the notion of epistemics, developed in conversation analysis, may help explain the dynamics regulating the distribution of responsibilities of guides and interpreters in dealing with relevant contents and I conclude that rights and obligations to explicate what is behind the guides’ talk can largely be seen as a product of the interaction...|$|R
50|$|Instead of {{compiling}} whole functions, TraceMonkey was a tracing JIT, {{which operates}} by recording control flow and <b>data</b> types during <b>interpreter</b> execution. This <b>data</b> then informed {{the construction of}} Trace Trees, highly specialized paths of native code.|$|R
40|$|R topics documented: gooJSON-package [...] . 2 goo 2 df [...] 2 gooadd [...] 3 goomap [...] . 4 Index 6 1 2 goo 2 df gooJSON-package Google JSON <b>Data</b> <b>Interpreter</b> for R Description A {{suite of}} helper {{functions}} for obtaining {{data from the}} Google Maps API JSON objects. Calls Google Maps API and returns results as an R data frame. Details There are currently various packages for providing an R interface to the Google Maps API. However, {{this is the first}} package that returns data from geocode queries (place names, accuracy codes, coordinates, et cetera) passed as JSON objects. Both native calls to the API (R internet access required) and JSON files saved onto the users hard-drive are supported. Users must supply a Googl...|$|E
40|$|Abstract. In {{this paper}} we present an {{application}} that exploits a {{geographic information system}} as a front-end of a complex information system supporting the management of landslide hazard in Valtellina, an alpine valley in Northern Italy. A decision support system (EYDENET, operational since October 1996), incorporating a geographic information system and a <b>data</b> <b>interpreter</b> based on artificial intelligence techniques, processes the readings of the 250 most significant instruments of a monitoring net of about 1000 sensors installed on different landslides in several alpine valleys. Data gathered by extensometers, clinometers and pluviometers, to check both movements of rocks and climatic conditions which could affect them, are processed by EYDENET, that provides on-line interpretation of data, helps the users analyse them, and generates natural language explanations and alarm messages for the people responsible for the environmental management and the civil protection...|$|E
40|$|Several {{algorithms}} {{and tools}} {{have been developed}} to (semi) automate the process of glycan identification by interpreting Mass Spectrometric data. However, each has limitations when annotating MSn data with thousands of MS spectra using uncurated public databases. Moreover, the existing tools are not designed to manage MSn data where n > 2. We propose a novel software package to automate the annotation of tandem MS data. This software consists of two major components. The first, is a free, semi-automated MSn <b>data</b> <b>interpreter</b> called the Glycomic Elucidation and Annotation Tool (GELATO). This tool extends and automates the functionality of existing open source projects, namely, GlycoWorkbench (GWB) and GlycomeDB. The second is a machine learning model called Smart Anotation Enhancement Graph (SAGE), which learns the behavior of glycoanalysts to select annotations generated by GELATO that emulate human interpretation of the spectra. Comment: To be submitted to Bioinformatics journal, Oxford pres...|$|E
40|$|The façades of {{buildings}} {{are almost always}} organized according to Gestalt principles such as good continuation, repetition in similarity, or symmetry etc. Coding such principles in production systems yields a very flexible frame to explore the usefulness of such principles in automatic façade understanding. Capturing images and image sequences of façades in the thermal domain and understanding such data is of importance e. g. for energy saving. In this contribution two different production systems are compared using the same <b>data</b> and <b>interpreter...</b>|$|R
40|$|Continuous ST-segment {{monitoring}} {{has been}} used to detect acute myocardial ischemia, determine the success of the reperfusion therapy, and predict out-comes in both research and a variety of clinical set-tings. However, analyzing the abundant electrocardi-ography (ECG) data recorded using continuousmultilead ST-segment monitoring techniques is time consuming and requires expertise. Experienced <b>data</b> <b>interpreters</b> in dedicated ECG core laboratories handle many con-tinuous ECG data records from large clinical trials. Little information on measurement issues for com-puter-assisted ST-segment analysis is available for individual investigators. Unsupervised or inexperi-enced computer analysis of ST-segment deviations can, under certain circumstances, yield invalid or unreliable summary indices. The goal {{of this article is to}} discuss basic ST-segment measurement principles in evaluating acute myocardial ischemia and method-ological issues surrounding the use of computer-assisted ST-segment analysis for continuous ECG data. Variables affecting ST-segment measurements will be examined. Sources and examples of variability for these potential errors will be identified. Key words: Computer assisted, ST-segment, myocardial ischemia, electrocardiography, postural, contrast media Continuous ST-segment monitoring is important in numerous patient populations that are at risk for myo-cardial ischemia. Therefore, it is useful in research and a variety of clinical settings. For example, continuous ST-segmentmonitoring provides valuable information related to the occurrence of reperfusion and reocclu-sion in patients with acute coronary syndromes (Krucoff and others 1993;Klootwijk and others 1996). Interpre-tation of the ST-segment trend data aswell as detection and quantification of myocardial ischemia can yield important prognostic information (Barbash and other...|$|R
50|$|Code {{injection}} vulnerabilities (injection flaws) {{occur when}} an application sends untrusted <b>data</b> to an <b>interpreter.</b> Injection flaws {{are most often}} found in SQL, LDAP, XPath, or NoSQL queries; OS commands; XML parsers, SMTP headers, program arguments, etc. Injection flaws tend to be easier to discover when examining source code than via testing. Scanners and fuzzers can help find injection flaws.|$|R
40|$|Abstract Background Several {{mathematical}} {{and statistical}} {{methods have been}} proposed {{in the last few}} years to analyze microarray data. Most of those methods involve complicated formulas, and software implementations that require advanced computer programming skills. Researchers from other areas may experience difficulties when they attempting to use those methods in their research. Here we present an user-friendly toolbox which allows large-scale gene expression analysis to be carried out by biomedical researchers with limited programming skills. Results Here, we introduce an user-friendly toolbox called GEDI (Gene Expression <b>Data</b> <b>Interpreter),</b> an extensible, open-source, and freely-available tool that we believe will be useful to a wide range of laboratories, and to researchers with no background in Mathematics and Computer Science, allowing them to analyze their own data by applying both classical and advanced approaches developed and recently published by Fujita et al. Conclusion GEDI is an integrated user-friendly viewer that combines the state of the art SVR, DVAR and SVAR algorithms, previously developed by us. It facilitates the application of SVR, DVAR and SVAR, further than the mathematical formulas present in the corresponding publications, and allows one to better understand the results by means of available visualizations. Both running the statistical methods and visualizing the results are carried out within the graphical user interface, rendering these algorithms accessible to the broad community of researchers in Molecular Biology. </p...|$|E
40|$|This {{study is}} {{concerned}} with lexical ambiguity in the script of Romeo and Juliet adapted by David Hundsness which is seen from stylistic perspective. This research has three objectives: (1) to identify the forms of lexical ambiguity in the script of Romeo and Juliet adapted by David Hundsness, (2) {{to find out how}} lexical ambiguity is represented through pun and wordplay in the script of Romeo and Juliet adapted by David Hundsness, and (3) to interpret the effects of lexical ambiguity represented through pun and wordplay towards the meanings in the script of Romeo and Juliet adapted by David Hundsness. This research employed a descriptive qualitative method. It was concerned with the description of the data in the script of Romeo and Juliet. The primary source of the data was the script of Romeo and Juliet adapted by David Hundsness. The primary instrument was the researcher who acted as the data collector, data analyst and <b>data</b> <b>interpreter.</b> Then, the secondary instrument was the data sheet. In collecting the data, she selected the data, provided three data sheets to categorize the data, and classified the data into the data sheets. In analysing the data, she identified the data into the categorizations, and interpreted each datum based on the theories. Finally, the trustworthiness of this study was achieved by conducting a triangulation technique. The findings of the research show that the two types of lexical ambiguity are found. They are homonymy (78. 26...|$|E
40|$|This {{research}} {{is concerned with}} english wordplay in SpongeBob movies and their translation in Bahasa Indonesia subtitle. This research has three objectives: 1) to describe the types of the wordplay, 2) to describe the techniques used by translator to translate the wordplay, and 3) to describe the degree of equivalence of the translation of the wordplay in SpongeBob movies. This research employed descriptive qualitative method as the main method because this research was concerned with the narrative descriptive of the translation phenomena in SpongeBob movies. The forms of data of this research were words and phrases containing wordplay in the movies. They were collected manually from the original VCDs of SpongeBob movies: The SpongeBob SquarePants Movie and SpongeBob Movie: Sponge Out of Water and their Bahasa Indonesia subtitle. The main instrument was the researcher who acted as data collector, data analyst and <b>data</b> <b>interpreter.</b> The trustworthiness {{of this study was}} achieved by using a triangulation technique. 	The findings of this research show that six types of wordplay are found. They are homonymy, homography, paronymy, idiom, syntactic structure and morphological development. Among them, morphological development is in the highest number. Then, the techniques used by translator to translate the wordplay are wordplay to wordplay, wordplay to non-wordplay, wordplay to zero, wordplay in the target text which is similar with the source text, and multiple techniques: editorial and wordplay in the target text which is similar with the source text technique. The most used strategy is wordplay to non-wordplay. In terms of degree of equivalence, from the last findings show 74...|$|E
40|$|The {{main goal}} of this {{bachelor}} thesis is to design and implement a high-level language for declarative configuration of Open-Flow networks. The compiler of the language, <b>data</b> model and <b>interpreter</b> were implemented employing Trema as the underlaying Open-Flow controller. The implementation was tested in a virtual network environment and it was shown that it is fully functional for basic network configurations...|$|R
40|$|Basic {{problems}} encountered {{when trying}} to accurately and reasonably measure dynamic properties of a program are discussed. These include problems in determining and assessing specific, desirable metric qualities that may be perturbed by subtle and unexpected program behaviour, as well as technical limitations on <b>data</b> collection. Some <b>interpreter</b> or Java-specific problems are examined, and the general issue of how to present metrics is also discussed...|$|R
40|$|Abstract. This paper {{presents}} a new robust approach for registration and segmentation. Segmentation {{as well as}} registration is attained by morphing of an N-dimensional model, the Morphon, ontotheNdimensional data. The approach is general and can, in fact, be said to encompass much of the deformable model ideas that have evolved over the years. However, in contrast to commonly used models, a distinguishing feature of the Morphon approach {{is that it allows}} an intuitive interface for specifying prior information, hence the expression paint on priors. In this way it is simple to design Morphons for specific situations. The priors determine the behavior of the Morphon and can be seen as local <b>data</b> <b>interpreters</b> and response generators. There are three different kinds of priors:- material parameter fields (elasticity, viscosity, anisotropy etc.),- context fields (brightness, hue, scale, phase, anisotropy, certainty etc.) and- global programs (filter banks, estimation procedures, adaptive mechanisms etc.). The morphing is performed using a dense displacement field. The field is updated iteratively until a stop criterion is met. Both the material parameter and context fields are addressed via the present displacement field. In each iteration the neighborhood operators are applied, using both data and the displaced parameter fields, and an incremental displacement field is computed. An example of the performance is given using a 2 D ultrasound heart image sequence where the purpose is to segment the heart wall. This is a difficult task even for trained specialists yet the Morphon generated segmentation is highly robust. Further it is demonstrated how the Morphon approach can be used to register the individual images. This is accomplished by first finding the displacement field that aligns the morphon model with the heart wall structure in each image separately and then using the displacement field differences to align the images. ...|$|R
40|$|The {{objectives}} {{of this research}} are to reveal the behavior, causes, and impacts of superiority complex suffered by Holden Caulfield that is appeared in Salinger's The Catcher in the Rye. The objectives are psychology of the main character and the internal conflict. Thus, the researcher analyses the main character using individual psychology by Alfred Adler because it is related with psychology particularly in superiority complex. To analyze the data, the researcher used qualitative approach. The main data are in the forms of expressions related to the research objectives in Salinger's The Catcher in the Rye. The key instrument for collecting the data was the researcher himself. As the main instrument, the researcher took a role as the designer, data collector, <b>data</b> <b>interpreter,</b> and also reporter of the finding of the study. In addition, to ensure trustworthiness of the data, the researcher applied triangulation. There are three findings of the research. The first finding is Holden’s superiority complex behaviors. There are five peculiar behaviors indicating that Holden suffers from superiority complex: giving excuses, aggression, depreciation, accusation, and self-accusation. The second finding is the causes of Holden’s superiority complex. The researcher finds {{that there are three}} main causes of Holden’s superiority complex: low social interest, pampered life style, and neglected life style. The last finding is the impact of superiority complex toward Holden’s personality. There are two major impacts that are creating unrealistic goals and having a narrow perspective. Thus, it can be concluded that the protagonist of Salinger’s The Cather in the Rye suffers from mental illness of superiority complex...|$|E
40|$|Under an {{alternating}} electrical signal, biological tissues {{produce a}} complex electrical bioimpedance {{that is a}} function of tissue composition and applied signal frequencies. By studying the bioimpedance spectra of biological tissues over a wide range of frequencies, we can noninvasively probe the physiological properties of these tissues to detect possible pathological conditions. Electrical impedance spectroscopy (EIS) can provide the spectra that are needed to calculate impedance parameters within a wide range of frequencies. Before impedance parameters can be calculated and tissue information extracted, impedance spectra should be processed and analyzed by a dedicated software program. National Instruments (NI) Inc. offers LabVIEW, a fast, portable, robust, user-friendly platform for designing dataanalyzing software. We developed a LabVIEW-based electrical bioimpedance spectroscopic <b>data</b> <b>interpreter</b> (LEBISDI) to analyze the electrical impedance spectra for tissue characterization in medical, biomedical and biological applications. Here, we test, calibrate and evaluate the performance of LEBISDI on the impedance data obtained from simulation studies as well as the practical EIS experimentations conducted on electronic circuit element combinations and the biological tissue samples. We analyze the Nyquist plots obtained from the EIS measurements and compare the equivalent circuit parameters calculated by LEBISDI with the corresponding original circuit parameters to assess the accuracy of the program developed. Calibration studies show that LEBISDI not only interpreted the simulated and circuitelement data accurately, but also successfully interpreted tissues impedance data and estimated the capacitive and resistive components produced by the compositions biological cells. Finally, LEBISDI efficiently calculated and analyzed variation in bioimpedance parameters of different tissue compositions, health and temperatures. LEBISDI can also be used for human tissue impedance analysis for electrical impedance-based tissue characterization, health analysis and disease diagnosis...|$|E
40|$|This study aims to {{determine}} the level of welfare {{of the people in}} Malang, considering the Malang city is a city of education and processing industry. The total population of less or prosperous in the Malang city of 2015 sebasar 19, 974 families of the total family of Malang city amounted to 204, 179 families. Malang City is divided into five districts, namely District Klojen, Sukun, Lowokwaru, Blimbing, and Kedung Kandang. The lowest level of community welfare from some of these districts is Kecamatan Sukun, amounting to 6, 813 pre-prosperous families. Prosperous family aspect was collected by using 21 indicators in accordance with the thought of sociology experts in building prosperous families by knowing the dominant factors that become the needs of every family. These dominant factors consist of (1) the fulfillment of basic needs; (2) the fulfillment of psychological needs; (3) development needs; and (4) self actualization needs in contributing to society in its environment. In this case, the groups categorized as Malang by BKKBN are KPS and KS-I. The type of this research is descriptive research with qualitative approach. The position of the researcher in qualitative research is the planner, the implementer of data collection, the analyst, the <b>data</b> <b>interpreter,</b> and finally become the reporter of the research result. So researchers are key research instruments. Data type in this research is primary data and secondary data. Technique of collecting data by interview, observation, and documentation. Data analysis is guided by interactive data analysis model. While to know the policy strategy that can be taken by government by using SWOT analysis. The results {{of this study indicate that}} the level of welfare of the community in the city of Malang has improved quality, which is indicated by the decrease in pre-prosperous family level. This is supported by the role of government, private, and universities that contribute in the form of providing basic services (health and education) for free for indigenous people of Malang City by the government. In addition, community service programs conducted by the university, also able to improve the quality of human resources community. While the private sector contributes in the form of CSR funds that are used to increase public facilities and infrastructure development and scholarships...|$|E
30|$|Good {{results from}} high {{resolution}} satellite images depend on positional accuracy. If {{the images were}} not observed from exactly the same point in space, then they can have different displacements, which could cause misregistration errors. Although positional calibration is a basic element of image analysis <b>data</b> flow, <b>interpreters</b> often face problems due to systematic or unsystematic errors in satellite images. Geometric or ortho-rectification (especially in mountain area) of the satellite images is vital to overcome the distortions related to the sensor (e.g. jitter, view angle effects), satellite (e.g. attitude deviations from nominal), and Earth (e.g. rotation, curvature, relief). The study area described here lies in a mountainous region, and topographic effects {{in terms of the}} earth’s curvature, mountain shadow, and clouds represent major obstacles that need {{to be taken into account}} (Itten and Meyer 1993).|$|R
50|$|The {{line between}} program and data can become blurry. An interpreter, for example, is a program. The input <b>data</b> to an <b>interpreter</b> {{is itself a}} program, just not one {{expressed}} in native machine language. In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program (more normally associated with plain text data). Metaprogramming similarly involves programs manipulating other programs as data. Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data.|$|R
40|$|Much {{scientific}} data is not obtained from measurements but rather derived from other data by {{the application of}} computational procedures. We hypothesize that explicit representation of these procedures can enable documentation of data provenance, discovery of available methods, and on-demand data generation (socalled “virtual data”). To explore this idea, we have developed the Chimera virtual data system, which combines a virtual data catalog, for representing data derivation procedures and derived data, with a virtual <b>data</b> language <b>interpreter</b> that translates user requests into data definition and query operations on the database. We couple the Chimera system with distributed “Data Grid ” services to enable on-demand execution of computation schedules constructed from database queries. We have applied this system to two challenge problems, the reconstruction of simulated collision event data from a high-energy physics experiment, and the search of digital sky survey data for galactic clusters, with promising results. ...|$|R
40|$|Pathways are formal {{descriptions}} of the biological processes involving finely regulated structures by which a cell converts molecules or processes signals. The study of gene expression in terms of pathways is defined as pathway analysis and aims at identifying groups of functionally related genes that show coordinated expression changes. Recently, pathway analysis moved from algorithms using merely gene list to ones exploiting the topology that define gene connections. A crucial, and unfortunately limiting step for these novel methods are {{the availability of the}} pathways as gene networks in which nodes are genes and edges are the relations between two elements. To this aim, we develop a pathway <b>data</b> <b>interpreter,</b> called graphite, able to uniformly store, process and convert pathway information into gene networks. graphite has been made publicly available as R package within the Bioconductor platform. In the field of the topological pathway analysis, graphite fills the existing gap lying between technical and methodological aspects. graphite i) allows performing more informative analysis on omics data and ii) allows developing new methods based on the increased accessibil- ity of biological knowledge. However, the pathways of the four main public resources integrated into graphite (KEGG, Reactome, Biocarta and PID), still lack of crucial interactors: the microRNAs. The microRNAs are small non-coding RNAs that post-transcriptionally regulate gene expression, their function on the messenger target is repressive but their effect on the transcription is dependent of the topology of the pathway in which the miRNA is involved. In the last decade, many targets have been discovered and experimentally validated, dedicated databases are available providing these information. Thus, I worked on an extension of graphite package able to integrate microRNAs in pathway topology, i) linking the non-coding RNAs to their validated target genes, ii) providing integrated networks suitable for the topological pathway analyses. The feasibility of this approach has been validated on a specific biological context, the early stage of Epithelial Ovarian Cancer (EOC). EOC has long been considered as a single disease. The emerging opinion, however, sees ovarian cancer as a general term that encloses a group of histo-pathological subtypes sharing a common anatomic location. In collaboration with the Mario Negri institute, 257 stage I EOC tumour biopsies were collected and stratified into training and validation sets. miRNA microarray data was used to generate the most highly reproducible signatures for each histotype through a dedicated resampling inferential strategy. qRT- PCR was used to validate the results in both the training and validation set. The results indicate that the clear cell histotype is characterized by high expression levels of miR- 30 a and miR- 30 a*, while mucinous patients by high levels of miR- 192 and miR- 194, interestingly as well as mucinous non-ovarian tissues. Then, the integrative approach that combines mRNA and miRNA profiles using graphite has been applied to identify the mucinous specific regulatory circuits. Taken together our findings demonstrate that EOC histotypes have discriminant regulatory circuits that drive the differentiation of the tumour environment. Our approach successfully guides us towards important biological results with interesting therapeutic implications in EOC...|$|E
40|$|STEP-NC {{is a new}} {{implementation}} in CAx chain for manufacturing technology to replace ISO 6983 formally known as G/M code. It {{plays a key role}} in the field of advanced manufacturing. It this paper an open architecture controller via LabVIEW is proposed. In this study, the software and hardware platform are involved and tool path <b>data</b> exchange between <b>interpreter</b> and LabVIEW platform realization methodology for the CNC system is determined in the mode of "offline". An open CNC controller system is successfully integrated for 3 -axis milling machine...|$|R
40|$|Abstract A {{suitable}} single instruction multiple <b>data</b> GP <b>interpreter</b> {{can achieve}} high (Giga GPop/second) {{performance on a}} SIMD GPU graphics card by simultaneously running multiple diverse members of the genetic programming population. SPMD dataflow parallelisation is achieved because the single interpreter treats the different GP programs as data. On a single 128 node parallel nVidia GeForce 8800 GTX GPU, the interpreter can out run a compiled approach, where data parallelisation comes only by running a single program at a time across multiple inputs. The RapidMind GPGPU Linux C++ system has been demonstrated by predicting ten year+ outcome of breast cancer from a dataset containing a million inputs. NCBI GEO GSE 3494 contains hundreds of Affymetrix HG-U 133 A and HG-U 133 B GeneChip biopsies. Multiple GP runs each {{with a population of}} five million programs winnow useful variables from the chaff at more than 500 million GPops per second. Sources available via FTP. ...|$|R
40|$|Event {{data from}} {{different}} parts of a system might be found recorded in event logs. Often the individual logs only show {{a small part of the}} system, but by correlating different sources into a consistent context it will be possible to gain further information and a wider view. This would facilitate in finding source of errors or certain behaviors within the system. This thesis will present the correlation possibilities between event data from different layers of the Ericsson Connectivity Packet Platform (CPP). This was done first by developing and using a test base application for the OSE operating system through which the event data can be recorded for the same test cases. The log files containing the event data have been studied and results will be presented regarding format, structure and content. For reading and storing the event <b>data,</b> suggestions of <b>interpreters</b> and <b>data</b> models are also provided. Finally a prototype application will be presented, which will provide the defined <b>interpreters,</b> <b>data</b> models and a graphical user interface to represent the event data and event data correlations. The programming was conducted using Java and the application is implemented as an Eclipse Plug-in. With the help of the application the user will get a better overview and a more intuitive way of working with the event data...|$|R
40|$|Interpreter is {{a program}} for {{translates}} some form of source code into a target representation that it can immediately execute and evaluate. Where in this interpreter can interpret some of source code, which this code is with author coding style but still {{not much different from}} parsing in other programming language (like Java or PHP or etc). This interpreter build from two steps of analysis, namely Syntactic Analysis and Semantic Analysis and one step to parse to produces an output, namely <b>Data</b> Parsing. This <b>interpreter</b> is also use tree data structure, namely General Tree to save every break words into tokens...|$|R
40|$|As the {{prevalence}} of BIM increases in A/E/C-FM disciplines it is timely to review the standards that are being utilised and how well they are serving the discipline. The analysis presented analyses the most common standard, the IAI’s IFC, from a metalevel and asks questions about the evolving model {{from the viewpoint of}} metrics for data models as well as a low level analysis of the accuracy and correctness of implementations of the <b>data</b> model <b>interpreters.</b> Metrics applied to the evolving versions of the IFC schema can indicate the trajectory of the schema and profile areas which may be of concern in the maintenance of the schema and applications that have to utilise the schema. Analysis of the approaches to importing and exporting data for design tools, based on the schema, help indicate how market ready the technology really is. Where commercial projects are starting to rely on the standards as a mechanism to reliably transfer semantically correct information there must be guarantees of the accuracy of the data as it is manipulated by these design tools...|$|R
40|$|Functional {{programming}} {{has come}} of age: {{it is now}} a standard course in any computer science curriculum. Ideas that were first developed in the -laboratory environment of functional programming have proved their values in wider settings, such as generic Java and XML. The time is ripe, therefore, to teach a second course on functional programming, delving deeper into the subject. This book is the text for such a course. The emphasis is on the fun of programming in a modern, well designed programming language such as Haskell. There are chapters that focus on applications, in particular pretty printing, musical composition, hardware description, and graphical design. These applications are interspersed with chapters on techniques, such as the design of efficient <b>data</b> structures, <b>interpreters</b> for other languages, program testing and optimisation. These topics are of interest to every aspiring programmer, not just to those who choose to work in a functional language. Haskell just happens to be a very convenient vehicle for expressing the ideas, and the theme of functional programming as a lingua franca to communicate ideas runs throughout the book. </p...|$|R
40|$|This paper {{contributes}} to the discussion of how free indirect style (FIS) and interpreter's renditions are accommodated in a relevance theoretic approach to communication. Within relevance theory, {{it has been argued}} that FIS and interpreting are cases of attributive use: FIS representations and interpreters’ renditions are representations of the author's/interpreter's thoughts about attributed thoughts. We ask whether this approach can accommodate FIS representations and interpreters’ renditions which contain perspective dependent discourse markers, and in particular whether it captures the role played by these expressions in encouraging the reader/hearer to think that s/he has direct access to the thoughts of fictional characters/original speakers. We apply Blakemore's (2010) account of discourse markers in FIS to <b>data</b> from <b>interpreter</b> mediated police interviews where renditions include discourse markers added by the interpreter to develop an alternative relevance theoretic account. This allows us to reconcile the hearer's impression that the interpreter's voice is suppressed with research in interpreting studies which shows that interpreters are in reality both visible and active co-participants in these exchanges...|$|R
40|$|Cílem této bakalářské práce je navrhnout vysoko-úrovňový jazyk OpenFlow-Network Control Language pro popis síťové topologie a chování počítačových sítí. Překladač jazyka a jeho interpretr byl implementován pomocí Trema OpenFlow kontroléru. Výsledná implementace byla otestována ve virtuálním prostředí, kde byla demonstrována její plná funkčnost pro základní síťové konfigurace. The {{main goal}} of this {{bachelor}} thesis is to design and implement a high-level language for declarative configuration of Open-Flow networks. The compiler of the language, <b>data</b> model and <b>interpreter</b> were implemented employing Trema as the underlaying Open-Flow controller. The implementation was tested in a virtual network environment and it was shown that it is fully functional for basic network configurations. ...|$|R
40|$|This {{study was}} carried out to analyze stress factors such as anxiety and {{depression}} on interpreters and interpreting students during simultaneous and remote interpreting. The study was taylored to interpeting conditions. The project was devided in two stages. During the first, <b>data</b> about the <b>interpreters</b> were collected to assess baseline values for psychological factors important for stress evaluation. In the second, {{the same group of}} interpreters underwent specific tests during simultaneous and remote interpreting. Psychometric instruments were chosen to this end. The results of the study show lower anxiety and depression values for interpreters than in the normal sample population, with interpeters being characterized by lower values than interpreting-students...|$|R
40|$|We {{describe}} how aspect oriented programming techniques can be exploited {{to support the}} development of workflow-based grid applications. In particular, we use aspects to adapt simple Java workflow code to be executed on top of muskel, our experimental, macro data flow based skeleton programming environment. Aspects are used to extract “on-the-fly” macro data flow graphs from plain Java code where the nodes of the workflow are explicitly identified by the programmers. The macro data flow instructions in the graph are automatically submitted to the muskel distributed macro <b>data</b> flow <b>interpreter</b> for the execution. A proper manager, instantiated by the programmer, is used to exploit stream parallelism on the workflow. Experimental results will be presented that demonstrate scalability of the approach for suitably grained workflows. Overall, the approach discussed here concentrates workflow exploitation responsibilities on the aspect (i. e. system) programmers leaving the application programmers only the task of properly defining logical steps in the workflow. This results in a complete separation of concerns that sensibly enhances the efficiency in workflow application development, while keeping both the system size and the additional knowledge required to application programmers reasonably small...|$|R
40|$|Catamorphisms ("foldr" on lists, but {{generally}} applicable to any regular recursive datatype) {{are not just}} useful but are an effective basis for a recursion-pattern-based discipline of program design. A new presentation for catamorphisms makes it clear how they provide functional semantics for symbolic datatypes, with the capacity to expose significant variations in program design. A further development of the new presentation exploits the higher-order capabilities of functional languages. This is the key enabler for a comprehensive replacement of symbolic <b>data</b> and their <b>interpreters,</b> either implicit or explicit, with direct functional representations. These extensions, of the applicability of catamorphisms and of their presentations, make them even more attractive as bases for program structuring and design, and likewise as targets for software reverse engineering and design recovery...|$|R
