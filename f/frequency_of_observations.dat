67|10000|Public
30|$|The {{second set}} of estimations, those for the private {{investment}} level, use quarterly data for the period 1988 – 2016 (ending {{in the third quarter}} of the latter year), thus covering the entire period after the liberalization of the trade regime in Mexico. In this case, the large number and high <b>frequency</b> <b>of</b> <b>observations</b> allow us to control for a wide set of macroeconomic determinants of investment, and to carry out a detailed analysis of short-run effects and trajectories.|$|E
40|$|This paper studies how the Hodrick-Prescott filter {{should be}} {{adjusted}} when changing the <b>frequency</b> <b>of</b> <b>observations.</b> It complements {{the results of}} Baxter and King (1999) with an analytical analysis, demonstrating that the filter parameter should be adjusted by multiplying it with the fourth power of the observation frequency ratios. This yields an HP parameter value of 6. 25 for annual data given a value of 1600 for quarterly data. The relevance of the suggestion is illustrated empiricall...|$|E
40|$|Suppose one {{has given}} {{discrete}} observations of a continuous-time random process (like e. g. stock market data) and {{one wants to}} test {{for the presence of}} jumps. Then the power of the tests will depend on the <b>frequency</b> <b>of</b> <b>observations.</b> We show, that if the data are observed at intervals of length 1 =n, at best one can detect jumps of height p log(n) =n. We construct a test which achieves this rate in the case of di¤usion-type processes...|$|E
5000|$|Division into zones, {{with the}} {{addition}} <b>of</b> rules governing <b>frequencies</b> <b>of</b> <b>observations</b> in each zone ...|$|R
40|$|Over 4 million <b>observations</b> <b>of</b> ship&#x 27;s drift are on file at the U. S. National Oceanographic Data Centre, in Washington, D. C., {{representing}} {{a vast amount}} of information on ocean surface currents. The observed drift speeds are dependent on the <b>frequency</b> <b>of</b> occurence of the particular current speeds and the <b>frequency</b> <b>of</b> <b>observation.</b> By comparing <b>frequency</b> <b>of</b> <b>observation</b> with the drift speeds observed it is possible to confirm known current patterns and detect singularities in surface currents...|$|R
5000|$|... where [...] is the <b>frequency</b> <b>of</b> <b>observation,</b> [...] is the conductivity, and [...] is {{the vacuum}} permittivity.|$|R
40|$|A coordinated {{program to}} observe Jupiter at high spatial {{resolution}} in the 5 -micrometer wavelength region was undertaken to support Voyager 1 imaging and infrared radiation experiment targeting. Jupiter was observed over a 5 -month period from Palomar and Mauna Kea observatories. The <b>frequency</b> <b>of</b> <b>observations</b> allowed the selection of interesting areas for closer Voyager examination and also provided good short-term monitoring of variations in cloud morphology. Significant global changes in the 5 -micrometer distribution are seen over this time period...|$|E
40|$|Abstract—This paper studies how the Hodrick-Prescot t � lter {{should be}} {{adjusted}} when changing the <b>frequency</b> <b>of</b> <b>observations.</b> It complements {{the results of}} Baxter and King (1999) with an analytical analysis, demonstrating that the � lter parameter should be adjusted by multiplying it with the fourth power of the observation frequency ratios. This yields an HP parameter value of 6. 25 for annual data given a value of 1600 for quarterly data. The relevance of the suggestion is illustrated empirically...|$|E
40|$|This Paper studies how the HP-Filter {{should be}} adjusted, when {{changing}} the <b>frequency</b> <b>of</b> <b>observations.</b> It complements {{the results of}} Baxter and King (1999) with an analytical analysis, demonstrating that the filter parameter should be adjusted by multiplying it with the fourth power of the observation frequency ratios. This yields an HP parameter value of 6. 25 for annual data given a value of 1600 for quarterly data. The relevance of the suggestion is illustrated empirically. Business cycles; historical business cycle properties; HP-filter; temporal aggregation; trends...|$|E
5000|$|The <b>frequency</b> <b>of</b> <b>observation</b> for the {{floating}} {{leg of the}} swap (for example, 3 month Libor paid quarterly) ...|$|R
5000|$|... #Caption: Histogram {{of travel}} time (to work), US 2000 census. Histograms depict the <b>frequencies</b> <b>of</b> <b>observations</b> {{occurring}} in certain ranges of values ...|$|R
5000|$|... "Effects <b>of</b> Study Duration, <b>Frequency</b> <b>of</b> <b>Observation,</b> and Sample Size on Power in Studies of Group Differences in Polynomial Change." [...] Psychological Methods, 6(4), 387.401, 2001. (With X. Liu).|$|R
40|$|International audienceWe model {{decision}} making under ambiguity based on available data. Decision makers express preferences over actions and data sets. We derive an α-max–min representation of preferences, in which beliefs combine objective {{characteristics of the}} data (number and <b>frequency</b> <b>of</b> <b>observations)</b> with subjective features of the decision maker (similarity of observations and perceived ambiguity). We identify the subjectively perceived ambiguity and separate it into ambiguity due to {{a limited number of}} observations and ambiguity due to data heterogeneity. The special case of no ambiguity provides a behavioral foundation for beliefs as similarity-weighted frequencies as in Billot et al. (2005...|$|E
40|$|Previous {{studies have}} {{examined}} the profitability of European index options arbitrage. This paper adds to the literature by investigating the arbitrage profitability of American index options—the Nikkei 225 index futures options traded on the Singapore Stock Exchange (SGX). Using the real-time bid-ask prices, we find evidence of profitable arbitrage opportunities, while the <b>frequency</b> <b>of</b> <b>observations</b> violating no-arbitrage bounds and the magnitude of arbitrage profits decrease with the level of transaction costs. Our results have implications for the analysis of American options market efficiency. Failure to use bid-ask prices may lead to biased conclusions. Futures options, Nikkei 225 index futures, Bid-ask price, Arbitrage profitability,...|$|E
40|$|We analyze {{a simple}} and {{feasible}} practical scheme displaying Zeno, anti-Zeno, and inverse-Zeno effects in the observation of wave-packet spreading caused by free evolution. The scheme is valid both in spatial diffraction of classical optical waves and in time diffraction of a quantum wave packet. In the optical realization, diffraction spreading is observed by placing slits between a light source and a light-power detector. We show that the occurrence of Zeno or anti-Zeno effects depends just on the <b>frequency</b> <b>of</b> <b>observations</b> between the source and detector. These effects are seen {{to be related to}} the diffraction mode theory in Fabry-Perot resonators. Comment: 7 pages, 8 figure...|$|E
30|$|The above {{observation}} equations for multi-GNSS DD operations can {{be generalized}} to inter-system mixed DD which can be further categorized into those between the same frequencies or the diverse <b>frequencies</b> <b>of</b> <b>observations</b> (Li et al., 2017).|$|R
40|$|This paper {{studies the}} effects <b>of</b> {{increasing}} the <b>frequency</b> <b>of</b> <b>observation</b> {{and the data}} span on the Johansen cointegration tests. The ability of the tests to detect cointegration depends more on the total sample length than the number <b>of</b> <b>observations.</b> (C) 2000 Elsevier Science S. A. All rights reserved. ...|$|R
50|$|<b>Frequency</b> <b>of</b> <b>observation</b> has {{increased}} in the 21st century, {{with the development of}} mass tourism. A video was taken and shown in the local Chinese media where numerous unidentifiable creatures can be seen.According to CCTV10, the creatures living in the lake are speculated by Chinese scholar as giant Hucho taimen.|$|R
40|$|The {{research}} {{field of}} plant phenology, which often involves the monitoring of several {{to hundreds of}} species of different life forms and/or different vegetation types, has increased exponentially {{over the last three}} decades. This has occurred in general, without consideration of the comparability of data and patterns across areas, and its influence on the interpretation of resultant patterns. In this chapter we address the influence of sampling method, sample size and the <b>frequency</b> <b>of</b> <b>observations</b> on the analysis of tropical tree phenology. Our approach is to compare the results of direct observations on transects with those obtained from litter traps. Transects and litter traps are the two most common methods used to sample and monitor plant phenology. Data from 3 locations were used to simulate different sample sizes and frequencies, and results were then compared with the original data. We conclude that sample size influences the patterns observed and there is a clear trade off between sample size and the <b>frequency</b> <b>of</b> <b>observations.</b> We show that direct observations were more accurate in defining both the beginning and the peak of phenological phases, and {{there was a significant difference}} between the peaks and seasonal patterns detected by both sampling methods. For tropical tree forest applications we recommend a minimum sample size of 15 trees and that a fortnightly frequency of observation be used especially if the sample size is small. We advocate the combination of presence/absence data and a quantification method to estimate plant phenology, a careful application of indices and a cautious generalization of pattern. ...|$|E
40|$|ABSTRACT Remote sensing {{has become}} a {{valuable}} tool in snow and ice studies because of its unique capability for acquiring measurements of glaciological conditions over large areas. From research conducted to date, {{it seems that the}} airborne gamma radiation approach to obtaining snow water equivalent is operational. Visible and near infrared snow-cover mapping techniques are well developed for operational snowmeIt-runoff prediction with the major problems being inadequate <b>frequency</b> <b>of</b> <b>observations</b> and poor turnaround time with high resolution data. Further development of visible and near infrared techniques for albedo measurements is needed. Microwave techniques require additional research; however, the prospects are extremely good for estimation of snow cover, snow water equivalent, ice condition and type, and ice thickness...|$|E
40|$|While the Economic Review {{primarily}} contains {{articles by}} economists {{associated with the}} Bank or the Board of Governors, occasionally we receive comments from readers that are appropriate for the Review. Prof. Hvidding's comment on an earlier Review article by Michael Bryan and William Gavin is one such case. This comment extends Bryan and Gavin's earlier Economic Review article (1986 Quarter 3) on measuring inflation expectations. Using a different <b>frequency</b> <b>of</b> <b>observations,</b> Prof. Hvidding's results support Bryan and Gavin's findings that the Michigan Survey dominates the Livingston Survey as a forecast of inflation. Using quarterly observations, he finds, however, that the Michigan survey forecasts inflation slightly better than the time series method, while Bryan and Gavin find the opposite using semiannual data...|$|E
5000|$|The count or <b>frequency</b> <b>of</b> process <b>observations</b> in the {{corresponding}} bin {{in the other}} dimension ...|$|R
40|$|With the {{accelerated}} longitudinal design data of different age cohorts {{are used to}} study individual development over a broad age span {{during a period of}} shorter duration. When planning an accelerated longitudinal study one must decide on the number of cohorts, the degree of overlap among cohorts, and the <b>frequency</b> <b>of</b> <b>observation.</b> This paper provides a framework to study the effects of these three design factors on the statistical power to detect a linear change. As no simple mathematical formulae for these relations exist, an example is used to illustrate how the effects of these three design factors can be evaluated. It is shown that the optimal number of cohorts, the optimal degree of overlap among cohorts, and the optimal <b>frequency</b> <b>of</b> <b>observation</b> depend on the total number of subjects and the total number of measurements. R code for evaluating the power of longitudinal designs is provided...|$|R
40|$|In this paper, {{we provide}} both {{qualitative}} and quantitative measures of the precision of measuring integrated volatility by realized volatility for a fixed <b>frequency</b> <b>of</b> <b>observation.</b> We start by characterizing for a general diffusion the dierence between realized and integrated volatility for a given <b>frequency</b> <b>of</b> <b>observation.</b> Then we compute the mean and variance of this noise and {{the correlation between the}} noise and the integrated volatility in the Eigenfunction Stochastic Volatility model of Meddahi (2001 a). This model has as special cases log-normal, affine and GARCH diusion models. Using previous empirical results, we show that the noise is substantial compared with the unconditional mean and variance of integrated volatility, even if one employs five-minute returns. We also propose a simple approach to capture the information about integrated volatility contained in the returns through the leverage eect. We show that in practice, the leverage effect does not matter...|$|R
40|$|This paper {{deals with}} {{probabilistic}} upper bounds for the error in functional estimation defined on some interpolation and extrapolation designs, when the function to estimate {{is supposed to}} be analytic. The error pertaining to the estimate may depend on various factors: the <b>frequency</b> <b>of</b> <b>observations</b> on the knots, the position and number of the knots, and also on the error committed when approximating the function through its Taylor expansion. When the number of observations is fixed, then all these parameters are determined by the choice of the design and by the choice estimator of the unknown function. The scope of the paper is therefore to determine a rule for the minimal number of observation required to achieve an upper bound of the error on the estimate with a given maximal probability...|$|E
40|$|This paper studies how the HP-Filter {{should be}} adjusted, when {{changing}} the <b>frequency</b> <b>of</b> <b>observations.</b> The usual {{choices in the}} literature are to adjust the smoothing parameter by multiplying it with either {{the square of the}} observation frequency ratios or simply with the observation frequency. In contrast, the paper recommends to adjust the filter parameter by multiplying it with the fourth power of the observation frequency ratios. Based on this suggestion, some well-known comparisons of business cycles moments across countries and time periods are recomputed. In particular, we overturn a finding by Backus and Kehoe (1992) on the historical changes in output volatility and return instead to older conventional wisdom (Baily, 1978, Lucas, 1977) : based on the new HP-Filter adjustment rule, output volatility turns out to have decreased after the Second World War. ...|$|E
40|$|Geostationary Carbon Process Mapper (GCPM) is an earth science {{mission to}} measure key {{atmospheric}} trace gases related {{to climate change}} and human activity. Understanding of sources and sinks of CO 2 is currently limited by <b>frequency</b> <b>of</b> <b>observations</b> and uncertainty in vertical transport. GCPM improves this situation by making simultaneous high resolution measurements of CO 2, CH 4, CF, and CO in near-IR, many times per day. GCPM is able to investigate processes with time scales of minutes to hours. CO 2, CH 4, CF, Co selected because their combination provides information needed to disentangle natural and anthropogenic sources/sinks. Quasi-continuous monitoring effectively eliminates atmospheric transport uncertainties from source/sink inversion modeling. will have one instrument (GeoFTS), hosted on a commercial communications satellite, planned for two years operation. GCPM will affordably advance the understanding of observed cycle variability improving future climate projections...|$|E
40|$|In this work, the {{equation}} which properly governs cavity radiation is presented. Given thermal equilibrium, the radiation contained within an arbitrary cavity {{depends upon the}} nature of its walls, in addition to its temperature and its <b>frequency</b> <b>of</b> <b>observation.</b> With this realization, the universality of cavity radiation col lapses. The constants of Planck and Boltzmann can no longer be viewed as universal...|$|R
40|$|This paper {{sets out}} the {{theoretical}} foundations for continuous-time signal extraction in econometrics. Continuous-time modeling gives an effective strategy for treating stock and flow data, irregularly spaced data, and changing <b>frequency</b> <b>of</b> <b>observation.</b> We rigorously derive the optimal continuous-lag filter when the signal component is nonstationary, and provide several illustrations, {{including a new}} class of continuous-lag Butterworth filters for trend and cycle estimation. Time-series analysis; Econometrics...|$|R
40|$|Power {{functions}} of {{tests of the}} random walk hypothesis versus stationary first order autoregressive alternatives are tabulated for samples of fixed span but various <b>frequencies</b> <b>of</b> <b>observation.</b> For a t-test and normalized test, power is found to depend, for a substantial range of parameter values, more on the span of the data in time than on the number <b>of</b> <b>observations.</b> For a runs test, power rapidly declines as the number <b>of</b> <b>observations</b> is increased beyond a certain point. Random walk, unit roots, power function, efficient markets hypothesis...|$|R
40|$|There {{has been}} a steady {{increase}} {{with respect to the}} available capability to monitor the earth's surface from satellite platforms. Developments regarding instrumentation should now be considered {{in the context of the}} possible commercialization of space systems. The satellite data user is to select his data with many variables in mind, including cost. The present investigation provides a statistical comparison of simultaneously acquired data from the Landsat- 4 Thematic Mapper (TM) and the Multispectral Scanner (MSS) for five scenes acquired over agricultural areas. Simple examination of photographic products and manipulation and display of image data suggests that the utility of the TM data will be far greater than that of the MSS. However, factors of cost and <b>frequency</b> <b>of</b> <b>observations</b> can also be important. The approach employed in the investigation should facilitate evaluation of future remote sensing systems...|$|E
40|$|The {{objective}} of this study iu to develop a typology of nonverbal gestural communicati) n behavior as evidenced by student teachers in art. An attempt is rade to develop techniques for the systematic observation and analysis of such behavior. Data from a pilot study, and analysis of a video-recorded sample of 15 student teachers {{led to the development}} of a typology of seven categories of nonvt behavior, and seven categories of terms descriptive of affect qualities. An instrument was constructed to measure the relation between student teachers ' gestural behavior and reflected qualities within task-setting, demonstration, and evaluation contests. As d test of validity, the researc 4 er and six judges used the instrument in observing three student teachers. Analysis of the data was conducted to ascertain <b>frequency</b> <b>of</b> <b>observations</b> relative to patterns of gestural behavior and reflected c_alities, and a facto...|$|E
40|$|In {{the present}} work we study {{modeling}} of HIV disease progression via multistate Markov model. The difficulty {{in this approach}} is how to define HIV disease states. These are usually {{defined in terms of}} CD 4 + T lymphocyte counts, but this marker is a subject to biological fluctuation and, in real life, measurement errors as well. Estimating the model on such a data will lead to intensity estimates depending on <b>frequency</b> <b>of</b> <b>observations.</b> That is why we usually smooth the data before fitting the Markov model. In this work we studied two different approaches - linear mixed-effects model and local polynomial kernel estimator. All modeling is performed on real data and also an illustrative simulation example is included. Another issue considered in this work is determination of sero-conversion time. The sero-conversion distribution is derived based on time of last negative observation, first positive observation and last performed measurement...|$|E
40|$|The {{accretion}} {{of comets}} onto DA white dwarfs can produce observable metal absorption lines. We show here that comet systems around the progenitor main sequence star {{are vulnerable to}} being lost during asymptotic giant branch mass loss, if the mass loss is sufficiently asymmetric to impart modest linear momentum to the white dwarf. This may have bearing on the <b>frequency</b> <b>of</b> <b>observation</b> <b>of</b> heavy elements in white dwarf stars, and on inferences regarding the <b>frequency</b> <b>of</b> comet systems, if the imparted linear velocities of white dwarfs can be estimated. Comment: 19 pages, 8 figures, AASTeX- 4. 0, {{to be published in}} ApJ (7 / 1 / 98); replaced with slight modifications to intro text and reference...|$|R
30|$|The {{arrival of}} MSG gave a {{breakthrough}} for {{the classification of}} clouds. The SEVIRI radiometer (Spinning Enhanced Visible and Infrared) on board MSG provides more condensed information and its <b>frequency</b> <b>of</b> <b>observation</b> changes from 30 to 15  min. Multispectral capacity increases to 12 channels. The spatial resolution changes from 2.5 to 1  km at nadir for broadband visible channel and 5 – 3  km for all other channels.|$|R
40|$|Collagen is the {{structural}} molecule {{that is most}} correlated with strength in blood vessels. In this study, we compared the properties of collagen in engineered and native blood vessels. Transmission electron microscopy (TEM) was used to image sections of engineered and native arteries. Band periodicities of engineered and native collagen fibrils indicated that spacing between collagen molecules was similar in engineered and native tissues. Engineered arteries, however, had thinner collagen fibrils and fibers than native arteries. Further, collagen fibrils were more loosely packed within collagen fibers in engineered arteries than in native arteries. The sensitivity of TEM analysis allowed measurement <b>of</b> the relative <b>frequency</b> <b>of</b> <b>observation</b> for alignment <b>of</b> collagen. These <b>observations</b> showed that collagen in both engineered and native arteries was aligned circumferentially, helically, and axially, but that engineered arteries had less circumferential collagen and more axial collagen than native arteries. Given that collagen is primarily responsible for dictating the ultimate mechanical properties of arterial tissue, future efforts should focus on using relative <b>frequency</b> <b>of</b> <b>observation</b> for alignment <b>of</b> collagen as a descriptive input for models of the mechanical properties of engineered or native tissues...|$|R
