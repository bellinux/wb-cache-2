4|129|Public
40|$|Abstract: The {{complexity}} of Enterprise Information Systems {{can be overwhelming}} to users, yet they are an often over-looked domain for usability research. To better understand {{the ways in which}} users interact with these systems, we have designed an infrastructure for <b>input</b> <b>logging</b> that is built upon a data model relating system compo-nents, user inputs, and tasks. This infrastructure is aware of user representations, task representations, and the history of user interactions. The interface components themselves log user inputs, so that timing data and action events are automatically aligned and are linked to specific tasks. The knowledge gained about user interactions at varying levels of granularity, ranging from keystroke analysis to higher-level task performance, is a valuable resource for both assessing and enhancing system usability. ...|$|E
40|$|A {{first order}} low pass Class A type log domain filter is {{synthesized}} by using computer simulation program (PSpice), and realized in laboratory. For front-end circuitry, i. e. <b>input</b> <b>logging</b> and level shifting part, number of alternatives exist. Two of these alternatives {{are known as}} better choices. T type is chosen in this work. The filter has approximately the cutoff frequency of 260 kHz and the transistors in the filter have 10 mA DC currents. The filter's cutoff frequency is electronically tunable, i. e. changing the magnitudes of the current sources defines the filter's cutoff frequency. The filter is simulated in PSpice using 'ideal', default PSpice model with BF= 10000, and BC 546 B/BC 557 B type transistors. Simulation results confirmed with the theoretical results. In laboratory, the filter is realized by using these transistors, not specially designed for log domain filtering, {{and a number of}} tests are performed. The laboratory results are compared with those of the simulation results. Due to some nonideal laboratory conditions, some acceptable differences are observed. Although such filters are very sensitive for mismatches and require integration technology, we have observed that realization of this filter in the laboratory has provided satisfactory results...|$|E
40|$|Bugs in {{deployed}} {{software can}} be extremely difficult to track down. Invasive logging techniques, such as logging all nondeterministic inputs, can incur substantial runtime overheads. This paper shows how symbolic analysis {{can be used to}} re-create path equivalent executions for very long running programs such as databases and web servers. The goal is to help developers debug such long-running programs by allowing them to walk through an execution of the last few requests or transactions leading up to an error. The challenge is to provide this functionality without the high runtime overheads associated with traditional replay techniques based on <b>input</b> <b>logging</b> or memory snapshots. Our approach achieves this by recording a small amount of information about program execution, such as the direction of branches taken, and then using symbolic analysis to reconstruct the execution of the last few inputs processed by the application, as well as the state of memory before these inputs were executed. We implemented our technique in a new tool called bbr. In this paper, we show that {{it can be used to}} replay bugs in long-running single-threaded programs starting from the middle of an execution. We show that bbr incurs low recording overhead (avg. of 10 %) during program execution, which is much less than existing replay schemes. We also show that it can reproduce real bugs from web servers, database systems, and other common utilities...|$|E
30|$|In addition, {{the tool}} {{can read the}} unencrypted requests/responses and can decrypt the logs if it is encrypted. This {{assumption}} is necessary since the tool needs to compare the generated request after performing an action with the <b>input</b> <b>log.</b> We also assume that the <b>input</b> <b>log</b> is “complete”; this {{means that there is}} no need to have the traffic from the previous session to reconstruct the current session, and the log is recorded from the start of the session.|$|R
40|$|An {{application}} of Kohonen's self-organizing map (SOM), learning-vector quantization (LVQ) algorithms, and commonly used backpropagation neural network (BPNN) to predict petrophysical properties obtained from well-log data are presented. A modular, {{artificial neural network}} (ANN) comprising a complex network made up {{from a number of}} subnetworks is introduced. In this approach, the SOM algorithm is applied first to classify the well-log data into a predefined number of classes, This gives an indication of the lithology in the well. The classes obtained from SOM are then appended back to the training <b>input</b> <b>logs</b> for the training of supervised LVQ. After training, LVQ can be used to classify any unknown <b>input</b> <b>logs.</b> A set of BPNN that corresponds to different classes is then trained. Once the network is trained, it is then used as the classification and prediction model for subsequent input data. Results obtained from example studies using the proposed method have shown to be fast and accurate as compared to a single BPNN network...|$|R
5000|$|A Sawzall script has {{a single}} <b>input</b> (a <b>log</b> record) and can output only by {{emitting}} to tables. The script can have no other side-effects.|$|R
40|$|This thesis explores {{interactive}} translation dictation (ITD), {{a translation}} technique that involves interaction with multimodal interfaces equipped with voice recognition (VR) technology {{throughout the entire}} translation process. Its main objective {{is to provide a}} solid theoretical background and an analysis of empirical qualitative and quantitative data that demonstrate ITD’s advantages and challenges, with a view to integrating this technique into the translation profession. Many empirical studies in human-computer interaction have strived to demonstrate the efficiency of voice input versus keyboard input. Although it was implicit in the earliest works that voice input was expected to completely replace—rather than complement—text-input devices, it was soon proposed that VR often performed better in combination with other input modes. This study introduces multimodal interaction to translation, taking advantage of the unparallelled robustness of commercially available voice-and-touch-enabled multimodal interfaces such as touch-screen computers and tablets. To that end, an experiment was carried out with 14 professional English-to-French translators, who performed a translation task either with the physical prototype of an ITD environment, or with a traditional keyboard-and-mouse environment. The hypothesis was that the prototypical environment would consistently provide translators with a better translator experience (TX) than the traditional environment, considering the translation process as a whole. The notion of TX as introduced in this work is defined as a translator’s perceptions of and responses to the use or anticipated use of a product, system or service. Both quantitative and qualitative data were collected using different methods, such as video and screen recording, <b>input</b> <b>logging</b> and semi-structured interviews. The combined analysis of objective and subjective usability measures suggests a better TX with the experimental environment versus the keyboard-and-mouse workstation, but significant challenges still need to be overcome for ITD to be fully integrated into the profession. Thus, this doctoral study provides a basis for better-grounded research in translator-computer interaction and translator-information interaction and, more specifically, for the design and development of an ITD environment, which is expected to support professional translators’ cognitive functions, performance and well-being. Lastly, this research aims to demonstrate that translation studies research, and translation technology in particular, needs to be more considerate of the translator, the TX, and the changing realities of the interaction between humans, computers and information in the twenty-first century...|$|E
40|$|Abstract. Well log {{and rock}} sample data from {{fourteen}} offshore petroleum exploration wells {{have been successfully}} up-scaled and integrated using facies classification. Over 100 rock sampleswere categorised into six petrofacies classes based on their composition and texture. Cluster {{analysis was used to}} classify well log data into five electrofacies units, after careful conditioning and selection of <b>input</b> <b>logs.</b> The result is a direct link between well logs and rock samples. Electrofacies profiles clearly illustrate stratigraphic information previously hidden in the well logs. In addition, well log acoustic rock property relationships based on the new electrofacies classes are found to be better constrained than lithology-based models. Previous facies studies either have been applied at a field scale or have had conventional core for petrofacies calibration. In this paper, I illustrate how facies analysis can be successfully applied at a regional scale with only sidewall sample calibration. Particular attentionwas given to conditioning the cluster analysis input logsby removing all effects offluidfill and mechanical compaction,whichvaried significantly across the studyarea. A lesson learned fromtheprojectwas that it is easy to generate misleading results with cluster analysis, so care was taken to select only the most appropriate <b>input</b> <b>logs,</b> and to thoroughly quality control the output electrofacies...|$|R
40|$|Feature {{extraction}} {{and feature}} selection {{are the first}} tasks in pre-processing of <b>input</b> <b>logs</b> in order to detect cyber security threats and attacks while utilizing machine learning. When {{it comes to the}} analysis of heterogeneous data derived from different sources, these tasks are found to be time-consuming and difficult to be managed efficiently. In this paper, we present an approach for handling feature extraction and feature selection for security analytics of heterogeneous data derived from different network sensors. The approach is implemented in Apache Spark, using its python API, named pyspark...|$|R
5000|$|... mod_logio - <b>logs</b> <b>input</b> {{and output}} {{number of bytes}} received/sent per request ...|$|R
3000|$|Note {{that the}} {{complexity}} of an L-point FFT/IFFT (via split radix FFT [23]) with L complex <b>inputs</b> is L <b>log</b> 2 [...]...|$|R
30|$|Logging {{provides}} {{a mechanism for}} recording values within an executing model for later analysis. Data collection is usually performed at regular intervals during model execution both for the overall model and for sets of agents. Data collection is usually performed automatically based on user <b>input.</b> User-programmed <b>logging</b> {{will be used in}} this paper.|$|R
40|$|This paper {{presents}} a novel technique for process discovery. In {{contrast to the}} current trend, which only considers an event log for discovering a process model, we assume two additional inputs: an independence relation {{on the set of}} logged activities, and a collection of negative traces. After deriving an intermediate net unfolding from them, we perform a controlled folding giving rise to a Petri net which contains both the <b>input</b> <b>log</b> and all independence-equivalent traces arising from it. Remarkably, the derived Petri net cannot execute any trace from the negative collection. The entire chain of transformations is fully automated. A tool has been developed and experimental results are provided that witness the significance of the contribution of this paper. Comment: This is the unabridged version of a paper with the same title appearead at the proceedings of ATVA 201...|$|R
40|$|This paper {{shows how}} fuzzy {{rule-based}} systems help predict permeability in sedimentary rocks using well-log responses. The fuzzy rule-based approach represents a global nonlinear relationship between permeability {{and a set}} of <b>input</b> <b>log</b> responses as a smooth concatenation of a finite family of flexible local submodels. The fuzzy in-ference rules expressing the local input–output relation-ships are obtained automatically from a set of observed measurements using a fuzzy clustering algorithm. This approach simplifies the process of constructing fuzzy sys-tems without much computation effort. The benefits of the methodology are demonstrated with a case study in the Lake Maracaibo basin, Venezuela. Special core anal-yses from three early development wells provide the data for the learning task. Core permeability and well-log data from a fourth well provide the basis for model validation. Numerical simulation results show that the fuzzy system is an improvement over conventional empirical methods in terms of predictive capability...|$|R
40|$|In {{this paper}} we propose a {{framework}} to improve word segmentation accuracy us-ing <b>input</b> method <b>logs.</b> An <b>input</b> method is software used to type sentences in lan-guages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable <b>input</b> method that <b>logs</b> user behavior (like typ-ing and selection of word candidates), and 3) a method for improving word segmen-tation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. ...|$|R
30|$|Sawing {{simulation}} {{was performed}} using the simulation software Saw 2003, developed by Nordmark [16]. The <b>input</b> was <b>log</b> models, {{based on the}} CT scanned logs. The log models were constructed by the parameterized knot models and the outer shape of the log. For the logs scanned specifically for this study, a knot detection method developed by Johansson et al. [17] was used.|$|R
40|$|International audienceTouch screens have a delay between {{user input}} and {{corresponding}} visual interface feedback, called input “latency” (or “lag”). Visual latency is more noticeable during continuous input actions like dragging, so methods to display feedback {{based on the}} most likely path {{for the next few}} input points have been described in research papers and patents. Designing these “next-point prediction” methods is challenging, and there have been no standard metrics to compare different approaches. We introduce metrics to quantify the probability of 7 spatial error “side-effects” caused by next-point prediction methods. Types of side-effects are derived using a thematic analysis of comments gathered in a 12 participants study covering drawing, dragging, and panning tasks using 5 state-of- the-art next-point predictors. Using experiment logs of actual and predicted input points, we develop quantitative metrics that correlate positively with the frequency of perceived side-effects. These metrics enable practitioners to compare next- point predictors using only <b>input</b> <b>logs...</b>|$|R
40|$|In {{this paper}} we {{investigate}} {{the effectiveness of}} ensemble-based learners for web robot session identification from web server logs. We also perform multi fold robot session labeling to improve the performance of learner. We conduct a comparative study for various ensemble methods (Bag-ging, Boosting, and Voting) with simple classifiers in perspective of classification. We also {{evaluate the effectiveness of}} these classifiers (both ensemble and simple) on five different data sets of va-rying session length. Presently the results of web server log analyzers are not very much reliable because the <b>input</b> <b>log</b> files are highly inflated by sessions of automated web traverse software’s, known as web robots. Presence of web robots access traffic entries in web server log repositories imposes a great challenge to extract any actionable and usable knowledge about browsing beha-vior of actual visitors. So web robots sessions need accurate and fast detection from web server log repositories to extract knowledge about genuine visitors and to produce correct results of lo...|$|R
40|$|Abstract—The goal of {{this work}} is to explore the {{relationship}} between pervasive software and user engagement towards environmental issues. We study this relationship {{in the context of}} an art installation that concerns the water cycle in nature. The research question is: How can we design and evaluate software that becomes a medium to engage and inform the user? We have gathered empirical data during a two days exhibition of two versions of a pervasive art installation by: observations, questionnaires, and <b>input</b> <b>logs.</b> Data analysis reveals that the art installation engaged users, with focus on young children, and communicated the intended message. The results are organized according to five important factors for developing and evaluating interacting art installations. These are: 1) data collection method; 2) user interaction; 3) social interaction; 4) issues about children; 5) message comprehension. We suggest that these factors can inform engineering practices for engaging software like video-games. Keywords-Social engagement; pervasive software; art installation; interactivity; evaluation I...|$|R
40|$|Process {{discovery}} {{is a technique}} for deriving a conceptual high-level process model from the execution logs of a running implementation. The technique is particularly useful when no high-level model is available or in case of significant gaps between process documentation and implementation. The discovered model makes the implementation accessible to various kinds of analysis for functional and non-functional properties. In this paper we extend process discovery to mediator services (or adapters) which adapt the messaging protocols of 2 or more otherwise incompatible services. We propose a technique that takes as <b>input</b> <b>logs</b> of communication behaviors—one log for each service connected to the adapter—and a library of high-level data transformation rules relevant for {{the domain of the}} adapter, and then returns an operational adapter model describing the control-flow and the data flow of the adapter in terms of Coloured Petri Nets – if such model exists. We discuss benefits and limitations of this idea and evaluate it with a prototype implementation on industrial size models. Keywords: Process Mining; Service Mining; Pattern Based Design; Coloured Petri nets; synthesi...|$|R
40|$|This paper {{presents}} "BPMN Miner", {{a process}} discovery technique that uses BPMN as the representational language for the discovery result. The proposed approach is novel {{in the sense}} that it is able to represent control-flow with BPMN constructs, but also because it augments the control-flow perspective with an organizational dimension by discovering swimlanes that represent organizational roles in the business process. Additional advantages of the proposed mining approach can be summarized as follows: it provides intuitive and easy-to-use abstraction/specification functionality which makes it applicable to event logs with various complexity levels, it provides instant feedback about the conformance between the <b>input</b> <b>log</b> and the resulting model based on a dedicated fitness metric, it is robust to noise, and it can easily integrate with modeling and other BPM tools with exporting functionality through the XPDL-format. In this way, BPMN Miner will take process mining one step closer to the status of indispensable for business process reengineering as discovered models are immediately available in the preferred language of a majority of practitioners, educators and researchers. status: accepte...|$|R
50|$|One {{technique}} is {{to store the}} series of <b>Inputs</b> in a <b>log.</b> During times of transient behavior, replicas may request copies of a log entry from another replica in order to fill in missing Inputs.|$|R
30|$|Sawing {{simulation}} {{was performed}} using the simulation software Saw 2003, developed by Nordmark [24]. Saw 2003 {{has been used}} extensively in earlier research [19, 24 – 26]. The <b>input</b> was <b>log</b> models, based on the CT scanned logs of the stem bank. Saw 2003 models a sawmill that employs cant sawing with two sawing machines, with curve sawing in the second saw. It is also possible to control positioning of the logs during sawing.|$|R
40|$|Abstract. An {{intrusion}} detection system (IDS) usually has to analyse Giga-bytes of audit information. In the case of anomaly IDS, the information is used to build a user profile characterising normal behaviour. Whereas for misuse IDSs, {{it is used to}} test against known attacks. Probabilistic methods, e. g. hidden Markov models, have proved to be suitable to profile formation but are prohibitively expensive. To bring these methods into practise, this paper aims to reduce the audit information by folding up subsequences that commonly occur within it. Using n-grams language models, {{we have been able to}} successfully identify the n-grams that appear most frequently. The main contribution of this paper is a n-gram extraction and identification process that significantly reduces an <b>input</b> <b>log</b> file keeping key information for {{intrusion detection}}. We reduced log files by a factor of 3. 6 in the worst case and 4. 8 in the best case. We also tested reduced data using hidden Markov models (HMMs) for intrusion detection. The time needed to train the HMMs is greatly reduced by using our reduced log files, but most importantly, the impact on both the detection and false positive ratios are negligible. ...|$|R
40|$|Online service {{failures}} in production computing envi-ronments are notoriously difficult to debug. When those failures occur, the software developer often has little information for debugging. In this paper, we present Insight, {{a system that}} reproduces the execution path of a failed service request onsite immediately after a failure is detected. Upon a request failure is detected, Insight dynamically creates a shadow copy of the production server and performs guided binary execution exploration in the shadow node to gain useful knowledge on how the failure occurs. Insight leverages both environment data (e. g., <b>input</b> <b>logs,</b> configuration files, states of interacting components) and runtime outputs (e. g., console logs, system calls) to guide the failure path finding. Insight does not require source code access or any special system recording during normal production run. We have implemented Insight and evaluated it using 13 failures from a production cloud management system and 8 open source software systems. The experimental results show that Insight can successfully find high fidelity failure paths within a few minutes. Insight is light-weight and unobtrusive, making it practical for online service failure inference in the production computing environment. ...|$|R
50|$|Security {{breaches}} {{on these}} kinds of applications are a major concern because it can involve both enterprise information and private customer data. Protecting these assets {{is an important part}} of any web application and there are some key operational areas that must be included in the development process. This includes processes for authentication, authorization, asset handling, <b>input,</b> and <b>logging</b> and auditing. Building security into the applications from the beginning can be more effective and less disruptive in the long run.|$|R
30|$|In this study, we {{aimed to}} apply {{two classes of}} process mining {{techniques}} (i.e., Discovery and Conformance Analysis) in order to discover models, organizational structures, and bottlenecks related to the handling of proceedings’ peer reviews in an international conference in Thailand. Knowing that process mining analysis tools receive the <b>input</b> <b>logs</b> only in MXML (Mining eXtensible Markup Language) and XES (eXtensible Event Stream) formats, we initially converted the collected event log into a MXML-formatted log. Later, Alpha (α) Algorithm, Heuristic, Fuzzy and Social Network mining techniques (from Discovery class) were used in order to automatically construct the proceedings’ review models based on the authentic data and without having any priori model. Though the Heuristic Miner looked much similar to the Alpha algorithm, the technique had the privilege to better deal with XOR and AND connectors based on the dependency relations of the event log. Next, the actual process behavior was projected onto fuzzy models. The result was an animation movie which helped us {{to better understand the}} real activities occurred (during the proceeding’ peer review process). Furthermore, by using Social Network Miner technique we aimed to analyze the organizational perspective of the peer review process in terms of three metrics, namely as: (a) Handover of Work, (b) Working Together, and (c) Similar Tasks (Aalst 2011).|$|R
40|$|Several {{investigators}} have provided evidence {{that more than}} one Ly- 1 T cell subset is involved in helper effector (HE) 1 function for the B cell response to T-dependent antigens. Distinctive functional Ly- 1 subsets have been defined by additional phenotypic markers (notably I-A, I-J, and Qa- 1) (1 - 5), adherence properties (3), priming environment (6 - 9), major histocompatibility complex restrictions (4), antigen specificity (4, 7), and theoretical interpretations of the slope of dose-response curves at limiting effector cell <b>input</b> (<b>log</b> antibody response vs. log effector cell number) (6, 7). This report resolves a major discrepancy we have had in relation to other reports {{as to whether or not}} the Qa- 1 immunogenetic marker distinguishes two functional Ly- 1 : HE subsets. We find that isolated T cells primed with antigen-pulsed macrophages generate appreciable HE activity only in the Ly- 1 :Qa- 1 - T cell subset. However, an Ly-l:Qa-I + T cell subset associated with HE function can be generated if priming takes place (in vivo or in vitro) in an environment containing B cells. Materials and Methods Mice. C 57 BL/ 6 (B 6) mice (H- 2 b:Lyt-l. 2 :Lyt- 2. 2 :Qa- 1 -) were obtained from The Jackso...|$|R
40|$|We {{aimed to}} {{identify}} risk pathways for postmenopausal osteoporosis (PMOP) via establishing an microRNAs- (miRNA-) regulated pathway network (MRPN). Firstly, we identified differential pathways through calculating gene- and pathway-level statistics {{based on the}} accumulated normal samples using the individual pathway aberrance score (iPAS). Significant pathways based on differentially expressed genes (DEGs) using DAVID were extracted, followed by identifying the common pathways between iPAS and DAVID methods. Next, miRNAs prediction was implemented via calculating TargetScore values with precomputed <b>input</b> (<b>log</b> fold change (FC), TargetScan context score (TSCS), and probabilities of conserved targeting (PCT)). An MRPN construction was constructed using the common genes in the common pathways and the predicted miRNAs. Using false discovery rate (FDR) < 0. 05, 279 differential pathways were identified. Using the criteria of FDR < 0. 05 and log⁡FC≥ 2, 39 DEGs were retrieved, and these DEGs were enriched in 64 significant pathways identified by DAVID. Overall, 27 pathways were the common ones between two methods. Importantly, MAPK signaling pathway and PI 3 K-Akt signaling pathway were {{the first and second}} significantly enriched ones, respectively. These 27 common pathways separated PMOP from controls with the accuracy of 0. 912. MAPK signaling pathway and PI 3 K/Akt signaling pathway might play crucial roles in PMOP...|$|R
40|$|A {{portable}} unit is {{for video}} communication {{to select a}} user name in a user name network. A transceiver wirelessly accesses a communication network through a wireless connection to a general purpose node coupled to the communication network. A user interface can receive user <b>input</b> to <b>log</b> on to a user name network through the communication network. The user name network has a plurality of user names, {{at least one of}} the plurality of user names is associated with a remote portable unit, logged on to the user name network and available for video communication...|$|R
40|$|An {{analysis}} of man {{power has been}} conducted in several plywood mills in Sumatera and Kalimantan in year 1987, and the result presented in this paper. The result shows that the average employment is 993 men per mill. Based on mill capasity and <b>log</b> <b>input,</b> the average level of labour absorption is 11. 33 men/ 1000 cum designed capasity, and 10. 66 men/ 1000 cum <b>log</b> <b>input.</b> The latter figure {{is more or less}} {{similar to that of the}} developed country in which the average level of labour absorption is. 11. 33 men/ 1000 cum. However labour absorption in each mill it is not proportional to both capasity and material utilization (logs). Organization structure of mills surveyed have a long span of control so it is recommended to be simplified...|$|R
40|$|<b>Input</b> data, code, <b>logs,</b> graphs {{and output}} {{data for the}} Sussex LHM Drosophila melanogaster hemiclones. Aim is to {{generate}} single, standardised values of female and male reproductive fitness for each hemiclone genome, for using in genome-wide association test using Plink software. Notes on how to run are provided in the code...|$|R
40|$|The advent in {{technology}} {{has shown that}} the individual differences and cognitive styles influence the way the process of writing transpires. Setting up the quasi-experimental study, the researchers sought to explore the writing behaviors of EFL learners in a blended learning environment in the light of critical thinking styles. To this end, 30 advanced language students took part in the four weeks experiment of this study. They were asked to write texts based on the provided information presented by the online module, designed by the researchers. Watson-Glaser Critical Thinking Appraisal (CTA) was used to measure the students’ critical thinking ability. The detailed basis to describe the writing and learning processes from different perspectives was created through using <b>input</b> <b>log</b> program. The results of data analysis indicated a significant and positive correlation between different sections of CTA (inference, interpretation and evaluation) and online module (theory, practice and case). The best predictors for the pausing, revision and switching behavior of the participants were found to be interpretation, evaluation of arguments, and interpretation, respectively. The findings of the present study necessitate an understanding of different factors which play significant roles in the learning process. Therefore, teachers as well as teacher educators need to develop their awareness of the fact that learners have various thinking abilities which consequently affects the way they approach different learning tasks...|$|R
30|$|Selected miRNA {{species to}} assess {{metabolic}} disease risk {{and to explore}} energy expenditure and storage will be measured as published by our group (Russell et al. 2012; Güller and Russell 2010; Russell et al. 2013; Zacharewicz et al. 2014). For miRNA analysis, extracted RNA will be reverse transcribed using target specific primers followed by qPCR using target specific probes as published routinely by our group. All qPCR analysis will be performed using the Stratagene MX 3000 p thermocycler (Stratagene, La Jolla, CA); miRNA results will be normalized to RNA <b>input</b> and <b>log</b> transformed if not normally distributed.|$|R
5000|$|Biconical (or [...] "bicon") antennas {{are often}} used in {{electromagnetic}} interference (EMI) testing either for immunity testing, or emissions testing. While the bicon is very broadband, it exhibits poor efficiency at low frequencies, resulting in low field strengths {{when compared to the}} <b>input</b> power. <b>Log</b> periodic dipole arrays, Yagi-Uda antennas, and reverberation chambers have shown to achieve much higher field strengths for the power input than a simple biconical antenna in an anechoic chamber. However, reverberation chambers, especially, are poor choices when the goal is to fully characterize a modulated or impulse signal rather than merely measuring peak and average spectrum energy content.|$|R
40|$|Predicting missing {{log data}} {{is a useful}} {{capability}} for geophysicists. Geophysical measurements in boreholes are frequently affected by gaps in the recording {{of one or more}} logs. In particular, sonic and shear sonic logs are often recorded over limited intervals along the well path, but the information these logs contain is crucial for many geophysical applications. Estimating missing log intervals from a set of recorded logs is therefore of great interest. In this work, I propose to estimate the data in missing parts of velocity logs using a genetic algorithm (GA) optimisation and I demonstrate that this method is capable of extracting linear or exponential relations that link the velocity to other available logs. The technique was tested on different sets of logs (gamma ray, resistivity, density, neutron, sonic and shear sonic) from three wells drilled in different geological settings and through different lithologies (sedimentary and intrusive rocks). The effectiveness of this methodology is demonstrated by a series of blind tests and by evaluating the correlation coefficients between the true versus predicted velocity values. The combination of GA optimisation with a Gibbs sampler (GS) and subsequent Monte Carlo simulations allows the uncertainties in the final predicted velocities to be reliably quantified. The GA method is also compared with the neural networks (NN) approach and classical multilinear regression. The comparisons show that the GA, NN and multilinear methods provide velocity estimates with the same predictive capability when the relation between the <b>input</b> <b>logs</b> and the seismic velocity is approximately linear. The GA and NN approaches are more robust when the relations are non-linear. However, in all cases, the main advantages of the GA optimisation procedure over the NN approach is that it directly provides an interpretable and simple equation that relates the <b>input</b> and predicted <b>logs.</b> Moreover, the GA method is not affected by the disadvantages that characterise gradient descent techniques such as the NN method...|$|R
