2|65|Public
40|$|This study {{examines}} whether a trend following managed futures fund can improve its performance by changing its position sizing method. Trades {{for a simple}} trend following strategy was simulated on 47 futures contracts over the period 1990 - 2012, using varying methods for determining position size. Eleven different position sizing methods where investigated, among them Target Volatility, Omega Optimization and correlation ranking methods. Both methods previously detailed in academic papers as well as novel approaches was implemented, and compared to the baseline performance of the strategy. The {{results from this study}} show that the Target Volatility method, and to some degree Max Drawdown Minimize and <b>Dynamic</b> <b>Stop</b> Lock-In, improved the performance of strategy. The final recommendation for a trend following managed futures fund is to use Target Volatility as position sizing method, possibly in conjunction with Max Drawdown Minimize. Denna studie undersöker huruvida en trendföljande managed futures-fond kan förbättra sina resultat genom att ändra positionsskalningsmetod. Handel med en enkel trendföljande strategi simulerades på 47 futureskontrakt åren 1990 - 2012, för olika metoder att för bestämma positionsstorlek. Elva positionsskalningmetoder undersöktes, exemplevis Target Volatility, Omega Optimization och metoder baserade i korrelationsrankning. Både tidigare beskrivna metoder och nya tillvägagångssätt testades, och jämfördes med den grundläggande strategin med avseende på risk och avkastning. Denna studies resultat visar att framförallt Target Volatility, och i viss uträckning Max Drawdown Minimize och <b>Dynamic</b> <b>Stop</b> Lock-In förbättrade nyckeltalen för den handlade strategin. Den slutgiltiga rekommendationen för en trendföljande managed futures-fond är att använda Target Volatility som positionsskalningsmetod, möjligtvis tillsammans med Max Drawdown Minimize...|$|E
40|$|A {{retrieval}} {{system is a}} very important part in a question answering framework. It reduces the number of documents to be considered for finding an answer. For further refinement, the documents are split up into smaller chunks to deal with topic variability in larger documents. In our case, we divided the documents into single sentences. Then a language model based approach was used to re-rank the sentence collection. For this purpose, we developed a new language model toolkit. It implements all standard language modeling techniques and is more flexible than other tools in terms of backingoff strategies, model combinations and design of the retrieval vocabulary. With the aid of this toolkit we conducted re-ranking experiments with standard language model based smoothing methods. On top of these algorithms we developed some new, improved models including <b>dynamic</b> <b>stop</b> word reduction and stemming. We also experimented with query expansion {{depending on the type of}} a query. On a TREC corpus, we demonstrate that our proposed approaches provide a performance superior to the standard methods. In terms o...|$|E
40|$|In recent years, in {{an attempt}} to {{maximize}} performance, machine learning approaches for event-related potential (ERP) spelling have become more and more complex. In this paper, we have taken a step back as we wanted to improve the performance without building an overly complex model, that cannot be used by the community. Our research resulted in a unified probabilistic model for ERP spelling, which is based on only three assumptions and incorporates language information. On top of that, the probabilistic nature of our classifier yields a natural <b>dynamic</b> <b>stopping</b> strategy. Furthermore, our method uses the same parameters across 25 subjects from three different datasets. We show that our classifier, when enhanced with language models and <b>dynamic</b> <b>stopping,</b> improves the spelling speed and accuracy drastically. Additionally, we would {{like to point out that}} as our model is entirely probabilistic, it can easily be used as the foundation for complex systems in future work. All our experiments are executed on publicly available datasets to allow for future comparison with similar techniques...|$|R
40|$|Ensuring {{safety for}} human robot {{interaction}} requires {{knowledge of the}} <b>dynamic</b> <b>stopping</b> behavior of the robot. This paper presents a multibody model developed to simulate the movement of industrial robots during the braking process. The main contribution is the suggested state-based braking model to overcome numerical problems of the friction behavior upon transitions between sliding and sticking. An analysis of the model upon ocurrance of these transitions is presented. Concluding this paper the simulation model is evaluated using measurement data from a KUKA KR 16 industrial robot...|$|R
40|$|Constrained {{counting}} {{is important}} in domains ranging from artificial intelligence to software analysis. There are already a few approaches for counting models over various types of constraints. Recently, hashing-based approaches achieve both theoretical guarantees and scalability, but still rely on solution enumeration. In this paper, a new probabilistic polynomial time approximate model counter is proposed, {{which is also a}} hashing-based universal framework, but with only satisfiability queries. A variant with a <b>dynamic</b> <b>stopping</b> criterion is also presented. Empirical evaluation over benchmarks on propositional logic formulas and SMT(BV) formulas shows that the approach is promising...|$|R
40|$|Background People {{with severe}} disabilities, e. g. due to {{neurodegenerative}} disease, depend on technology {{that allows for}} accurate wheelchair control. For those who cannot operate a wheelchair with a joystick, brain-computer interfaces (BCI) may offer a valuable option. Technology depending on visual or auditory input may not be feasible as these modalities are dedicated to processing of environmental stimuli (e. g. recognition of obstacles, ambient noise). Herein we thus validated the feasibility of a BCI based on tactually-evoked event-related potentials (ERP) for wheelchair control. Furthermore, we investigated use of a <b>dynamic</b> <b>stopping</b> method to improve speed of the tactile BCI system. Methods Positions of four tactile stimulators represented navigation directions (left thigh: move left; right thigh: move right; abdomen: move forward; lower neck: move backward) and N[*]=[*] 15 participants delivered navigation commands by focusing their attention on the desired tactile stimulus in an oddball-paradigm. Results Participants navigated a virtual wheelchair through a building and eleven participants successfully completed the task of reaching 4 checkpoints in the building. The virtual wheelchair was equipped with simulated shared-control sensors (collision avoidance), yet these sensors were rarely needed. Conclusion We conclude that most participants achieved tactile ERP-BCI control sufficient to reliably operate a wheelchair and <b>dynamic</b> <b>stopping</b> was of high value for tactile ERP classification. Finally, this paper discusses feasibility of tactile ERPs for BCI based wheelchair control...|$|R
40|$|Even {{though the}} P 300 based speller {{has proved to}} be usable by real patients, it is not a {{user-friendly}} system. The necesarry calibration session and slow spelling make the system tedious. We present a machine learning approach to P 300 spelling that enables us to remove the calibration session. We achieve this by a combination of unsupervised training, transfer learning across subjects and language models. On top of that, we can increase the spelling speed by incorporating a <b>dynamic</b> <b>stopping</b> approach. This yields a P 300 speller that works instantly and with high accuracy and spelling speed, even for unseen subjects...|$|R
40|$|We {{present an}} {{in-depth}} treatment of model checking lgorithms {{for a class}} of infinite-state continuous-time Markov chains known as quasi-birth death processes. The model class is described in detail, {{as well as the}} logic CSL to express properties of interest. Using a new property-independency concept, we provide model checking algorithms for all CSL operators. Special emphasis is given to the time-bounded until operator for which we present a new and efficient computational procedure named uniformization with epresentatives. By the use of an application-driven <b>dynamic</b> <b>stopping</b> criterion the algorithm stops whenever the property to be checked can be certified (or falsified). A comprehensive case study of a connection management system shows the versatility of our new algorithms...|$|R
40|$|Abstract] This paper {{presents}} new identi 8 ̆ 5 cation {{results for}} the class of structural <b>dynamic</b> optimal <b>stopping</b> time models that are built upon {{the framework of the}} structural discrete Markov decision processes proposed by Rust (1994). We demonstrate how to semiparametrically identify the deep structural parameters of interest in the case where the utility function of an absorbing choice in the model is parametric but the distribution of unobserved heterogeneity is nonparametric. Our identi 8 ̆ 5 cation strategy depends on availability of a continuous observed state variable that satis es certain exclusion restrictions. If such excluded variable is accessible, we show that the <b>dynamic</b> optimal <b>stopping</b> model is semiparametrically identi 8 ̆ 5 ed using control function approaches...|$|R
3000|$|The {{validity}} of the <b>dynamic</b> iterative <b>stopping</b> algorithms combined with weighted extrinsic information exchange is then tested. Monte Carlo simulations based on MATLAB software {{are performed to evaluate}} the performance of the proposed systems in the Rayleigh fading channel (i.e. slow flat Rayleigh fading channel, E [...]...|$|R
40|$|This article investigates a {{possible}} Brain Computer Interface (BCI) based on semantic relations. The BCI determines which prime word a subject {{has in mind}} by presenting probe words using an intelligent algorithm. Subjects indicate when a presented probe word {{is related to the}} prime word by a single finger tap. The detection of the neural signal associated with this movement is used by the BCI to decode the prime word. The movement detector combined both the evoked (ERP) and induced (ERD) responses elicited with the movement. Single trial movement detection had an average accuracy of 67 %. The decoding of the prime word had an average accuracy of 38 % when using 100 probes and 150 possible targets, and 41 % after applying a <b>dynamic</b> <b>stopping</b> criterium, reducing the average number of probes to 47. The article shows that the intelligent algorithm used to present the probe words has a significantly higher performance than a random selection of probes. Simulations demonstrate that the BCI also works with larger vocabulary sizes, and the performance scales logarithmically with vocabulary size...|$|R
40|$|Recently, brain-computer {{interfaces}} (BCIs) {{based on}} visual evoked potentials (VEPs) {{have been shown}} to achieve remarkable communication speeds. As they use electroencephalography (EEG) as non-invasive method for recording neural signals, the application of gel-based EEG is time-consuming and cumbersome. In order to achieve a more user-friendly system, this work explores the usability of dry EEG electrodes with a VEP-based BCI. While the results show a high variability between subjects, they also show that communication speeds of more than 100 bit/min are possible using dry EEG electrodes. To reduce performance variability and deal with the lower signal-to-noise ratio of the dry EEG electrodes, an averaging method and a <b>dynamic</b> <b>stopping</b> method were introduced to the BCI system. Those changes were shown to improve performance significantly, leading to an average classification accuracy of 76 % with an average communication speed of 46 bit/min, which is equivalent to a writing speed of 8. 8 error-free letters per minute. Although the BCI system works substantially better with gel-based EEG, dry EEG electrodes are more user-friendly and still allow high-speed BCI communication...|$|R
40|$|Contents: {{introduction}} to WFIRST (Wide-Field Infrared Survey Telescope) and integrated modeling; WFIRST stability requirement summary; instability mitigation strategies; <b>dynamic</b> jitter results; <b>STOP</b> (structural-thermal-optical performance) (thermal distortion) results; STOP and jitter capability limitations; model validation philosophy...|$|R
40|$|Representing an {{intuitive}} spelling interface for brain–computer interfaces (BCI) in the auditory domain is not straight-forward. In consequence, all existing approaches based on event-related potentials (ERP) rely {{at least partially}} on a visual representation of the interface. This online study introduces an auditory spelling interface that eliminates the necessity for such a visualization. In up to two sessions, a group of healthy subjects (N[*]=[*] 21) was asked to use a text entry application, utilizing the spatial cues of the AMUSE paradigm (Auditory Multi-class Spatial ERP). The speller relies on the auditory sense both for stimulation and the core feedback. Without prior BCI experience, 76 % {{of the participants were}} able to write a full sentence during the first session. By exploiting the advantages of a newly introduced <b>dynamic</b> <b>stopping</b> method, a maximum writing speed of 1. 41 [*]char/min (7. 55 [*]bits/min) could be reached during the second session (average: 0. 94 [*]char/min, 5. 26 [*]bits/min). For the first time, the presented work shows that an auditory BCI can reach performances similar to state-of-the-art visual BCIs based on covert attention. These results represent an important step toward a purely auditory BCI...|$|R
30|$|Furthermore, in both studies, {{the authors}} {{argued that the}} trail system {{formation}} <b>dynamic</b> usually <b>stops</b> before reaching the optimal structure (i.e., MWS or MST structure). In this context, both studies claimed that {{the evolution of the}} trail system usually stops at a semi-optimal structure that they refer to as a MDS and “pro-Steiner” structure, respectively. However, despite the similarities between the findings of two studies, some differences are also observable between the Helbing et al. and Goldstone et al. perspectives.|$|R
30|$|The rest of {{the paper}} is {{organized}} as follows. Section 2 introduces {{the system of the}} q-ary LDPC-coded PRCPM and the modified maximum a posteriori (MAP) algorithm for CPM in the Rayleigh fading channel. Section 3 investigates the convergence speed in the iteration and the performance of different extrinsic information exchange methods at various SNRs. Section 4 studies the technique of weighed extrinsic information exchange and its parameter setting. Section 5 expounds the CE and HDA <b>dynamic</b> iteration <b>stopping</b> algorithms. Section 6 discusses the simulations. Finally, we conclude the paper in Section 7.|$|R
40|$|In this paper, we {{consider}} a challenging optimal control problem {{in which the}} terminal time is determined by a stopping criterion. This stopping criterion is defined by a smooth surface in the state space; when the state trajectory hits this surface, the governing <b>dynamic</b> system <b>stops.</b> By restricting the controls to piecewise constant functions, we derive a finite-dimensional approximation of the optimal control problem. We then develop an efficient computational method, based on nonlinear programming, for solving the approximate problem. We conclude the paper with four numerical examples...|$|R
30|$|This paper mainly {{studies the}} {{association}} between non-binary low-density parity-check codes and high-order partial response continuous phase modulation, which prevents information loss in the mutual conversion of bit and symbol probabilities. Although the iterative detection and decoding technique applied in this system can obtain good performance/complexity tradeoff, the iterative process still encounters the problems of positive feedback and relatively large decoding delay, similar to other iterative coded modulation systems. The inhibitory effects of different extrinsic information exchange methods on positive feedback under different signal-to-noise ratio (SNR) conditions are investigated in this work to address this issue. Two <b>dynamic</b> iterative <b>stopping</b> algorithms, namely, cross entropy and hard decision aided combined with weighted extrinsic information exchange for cases with medium and high SNRs, are then proposed. Extrinsic information exchange between the demodulator and the decoder is conducted in the two algorithms through weighted processing. Iterative detection is subsequently performed based on two <b>stopping</b> criteria of <b>dynamic</b> iterative decoding. Theoretic analysis and simulation results for the Rayleigh fading channel show {{that the combination of}} weighted extrinsic information exchange and the two <b>dynamic</b> iterative <b>stopping</b> algorithms effectively resists positive feedback and improves the convergence of iterative detection and bit error rate performance. Such a combination also reduces the average iteration number to improve the real-time performance of iterative detection and decoding.|$|R
40|$|Objective. Most BCIs have {{to undergo}} a {{calibration}} session in which data is recorded to train decoders with machine learning. Only recently zero-training methods have become a subject of study. This work proposes a probabilistic framework for BCI applications which exploit event-related potentials (ERPs). For {{the example of a}} visual P 300 speller we show how the framework harvests the structure suitable to solve the decoding task by (a) transfer learning, (b) unsupervised adaptation, (c) language model and (d) <b>dynamic</b> <b>stopping.</b> Approach. A simulation study compares the proposed probabilistic zero framework (using transfer learning and task structure) to a state-of-the-art supervised model on n = 22 subjects. The individual influence of the involved components (a) –(d) are investigated. Main results. Without any need for a calibration session, the probabilistic zero-training framework with inter-subject transfer learning shows excellent performance—competitive to a state-of-the-art supervised method using calibration. Its decoding quality is carried mainly by the effect of transfer learning in combination with continuous unsupervised adaptation. Significance. A high-performing zero-training BCI is within reach {{for one of the most}} popular BCI paradigms: ERP spelling. Recording calibration data for a supervised BCI would require valuable time which is lost for spelling. The time spent on calibration would allow a novel user to spell 29 symbols with our unsupervised approach. It could be of use for various clinical and non-clinical ERP-applications of BCI...|$|R
40|$|Objective. Reliability is a {{desirable}} characteristic of brain-computer interface (BCI) systems {{when they are}} intended to be used under non-experimental operating conditions. In addition, their overall usability is influenced by the complex and frequent procedures that are required for configuration and calibration. Earlier studies examined the issue of asynchronous control in P 300 -based BCIs, introducing <b>dynamic</b> <b>stopping</b> and automatic control suspension features. This report proposes and evaluates an algorithm for the automatic recalibration of the classifier's parameters using unsupervised data. Approach. Ten healthy subjects participated in five P 300 -based BCI sessions throughout a single day. First, we examined whether continuous adaptation of control parameters improved the accuracy of the asynchronous system over time. Then, we assessed the performance of the self-calibration algorithm with respect to the no-recalibration and supervised calibration conditions with regard to system accuracy and communication efficiency. Main results. Offline tests demonstrated that continuous adaptation of the control parameters significantly increased the communication efficiency of asynchronous P 300 -based BCIs. The self-calibration algorithm correctly assigned labels to unsupervised data with 95 % accuracy, effecting communication efficiency that was comparable with that of supervised repeated calibration. Significance. Although additional online tests that involve end-users under non-experimental conditions are needed, these preliminary results are encouraging, from which we conclude that the self-calibration algorithm is a promising solution to improve P 300 -based BCI usability and reliability...|$|R
40|$|Challenges for {{the next}} {{generation}} of Brain Computer Interfaces (BCI) are to mitigate the common sources of variability (electronic, electrical, biological) and to develop online and adaptive systems following the evolution of the subject's brain waves. Studying electroencephalographic (EEG) signals from their associated covariance matrices allows the construction of a representation which is invariant to extrinsic perturbations. As covariance matrices should be estimated, this paper first presents a thorough study of all estimators conducted on real EEG recording. Working in Euclidean space with covariance matrices is known to be error-prone, one might take advantage of algorithmic advances in Riemannian geometry and matrix manifold to implement methods for Symmetric Positive-Definite (SPD) matrices. Nonetheless, existing classification algorithms in Riemannian spaces are designed for offline analysis. We propose a novel algorithm for online and asynchronous processing of brain signals, borrowing principles from semi-unsupervised approaches and following a <b>dynamic</b> <b>stopping</b> scheme to provide a prediction as soon as possible. The assessment is conducted on real EEG recording: this is the first study on Steady-State Visually Evoked Potential (SSVEP) experimentations to exploit online classification based on Riemannian geometry. The proposed online algorithm is evaluated and compared with state-of-the-art SSVEP methods, which are based on Canonical Correlation Analysis (CCA). It is shown to improve both the classification accuracy and the information transfer rate in the online and asynchronous setup...|$|R
40|$|We {{address the}} issue of knots {{selection}} for Gaussian predictive process methodology. Predictive process approximation provides an effective solution to the cubic order computational complexity of Gaussian process models. This approximation crucially depends on a set of points, called knots, at which the original process is retained, while the rest is approximated via a deterministic extrapolation. Knots should be few in number to keep the computational complexity low, but provide a good coverage of the process domain to limit approximation error. We present theoretical calculations to show that coverage must be judged by the canonical metric of the Gaussian process. This necessitates having in place a knots selection algorithm that automatically adapts to the changes in the canonical metric affected by changes in the parameter values controlling the Gaussian process covariance function. We present an algorithm toward this by employing an incomplete Cholesky factorization with pivoting and <b>dynamic</b> <b>stopping.</b> Although these concepts already exist in the literature, our contribution lies in unifying them into a fast algorithm and in using computable error bounds to finesse implementation of the predictive process approximation. The resulting adaptive predictive process offers a substantial automatization of Guassian process model fitting, especially for Bayesian applications where thousands of values of the covariance parameters are to be explored. Comment: 20 pages, 5 figure...|$|R
40|$|This article {{specifies}} {{and estimates}} a structural dynamic model of consumer demand {{for new and}} used durable goods. Its primary contribution is to provide an explicit estimation procedure for transaction costs. Identification of transaction costs is achieved from the variation in the share of consumers choosing to hold a given car type each period, and from the share of consumers choosing to purchase the same car type that period. Specifically, I estimate a random-coefficient discrete-choice model that incorporates a <b>dynamic</b> optimal <b>stopping</b> problem. I apply this model to evaluate the impact of scrappage subsidies on the Italian automobile market...|$|R
40|$|The {{background}} for, {{and design}} of a third generation, general purpose, all electronic spatial scanning system, the DSR is described. Its specified performance capabilities provide <b>dynamic</b> and <b>stop</b> action three dimensional spatial reconstructions of any portion of the body {{based on a minimum}} exposure time of 0. 01 second for each 28 multiplanar 180 deg scanning set, a maximum scan repetition rate of sixty 28 multiplane scan sets per second, each scan set consisting of a maximum of 240 parallel cross sections of a minimum thickness of 0. 9 mm, and encompassing a maximum cylindrical volume about 23 cm in length and up to 38 cm in diameter...|$|R
40|$|Threshold {{estimation}} with sequential procedures is justifiable on the {{surmise that}} the index {{used in the}} so-called <b>dynamic</b> <b>stopping</b> rule has diagnostic value for identifying when an accurate estimate has been obtained. The performance of five types of Bayesian sequential procedure was compared here to that of an analogous fixed-length procedure. Indices for use in sequential procedures were: (1) {{the width of the}} Bayesian probability interval, (2) the posterior standard deviation, (3) the absolute change, (4) the average change, and (5) the number of sign fluctuations. A simulation study was carried out to evaluate which index renders estimates with less bias and smaller standard error at lower cost (i. e. lower average number of trials to completion), in both yes–no and two-alternative forced-choice (2 AFC) tasks. We also considered the effect of the form and parameters of the psychometric function and its similarity with themodel function assumed in the procedure. Our results show that sequential procedures do not outperform fixed-length procedures in yes–no tasks. However, in 2 AFC tasks, sequential procedures not based on sign fluctuations all yield minimally better estimates than fixed-length procedures, although most of the improvement occurs with short runs that render undependable estimates and the differences vanish when the procedures run for a number of trials (around 70) that ensures dependability. Thus, none of the indices considered here (some of which are widespread) has the diagnostic value that would justify its use. In addition, difficulties of implementation make sequential procedures unfit as alternatives to fixed-length procedures...|$|R
25|$|Georges Cornuéjols and licensees of his patents (Brdi, Hymatom) {{introduced}} {{the principle of}} HDR video image, in 1986, by interposing a matricial LCD screen {{in front of the}} camera's image sensor, increasing the sensors <b>dynamic</b> by five <b>stops.</b> The concept of neighborhood tone mapping was applied to video cameras by a group from the Technion in Israel led by Dr. Oliver Hilsenrath and Prof. Y.Y.Zeevi who filed for a patent on this concept in 1988.|$|R
30|$|The {{number of}} outer {{iterations}} in the iterative detection process of q-ary LDPC-CPM systems is usually set to a fixed positive integer. However, not all received sequences have optimal decoding results {{at the same}} number of iterations. For several specific sequences, error-free decoding can be achieved by a few iterations. Continuous iteration would increase computational complexity and iterative decoding delay. A dynamic iteration would possibly improve detection efficiency and reduce decoding delay if some iterative stopping criteria are introduced into the detection process. In this study, cross entropy (CE) [30] and hard decision-aided (HDA) [31] stopping criteria are incorporated into the iterative process. Two <b>dynamic</b> iteration <b>stopping</b> algorithms based on weighted extrinsic information exchange are then developed. The application of the two criteria in q-ary LDPC-CPM systems’ signal detection is subsequently deduced.|$|R
40|$|This paper {{presents}} new identification {{results for}} the class of structural dynamic discrete choice models that are built upon {{the framework of the}} structural discrete Markov decision processes proposed by Rust (1994). We demonstrate how to semiparametrically identify the deep structural parameters of interest in the case where utility function of one choice in the model is parametric but the distribution of unobserved heterogeneities is nonparametric. The proposed identification method does not rely on the availability of terminal period data and hence can be applied to infinite horizon structural dynamic models. For identification we assume availability of a continuous observed state variable that satisfies certain exclusion restrictions. If such excluded variable is accessible, we show that the structural dynamic discrete choice model is semiparametrically identified using the control function approach. This is a substantial revision of "Semiparametric identification of structural <b>dynamic</b> optimal <b>stopping</b> time models", CWP 06 / 07. ...|$|R
40|$|A non-stationary stopped {{decision}} process is investigated under rather weak convergence assumptions on the expected total rewards. Sufficient conditions are {{given for the}} approximation of the maximal conditional expected rewards from infinite stage play by the maximal conditional expected rewards from finite stage play. General criteria of optimality are derived. The results are essentially based on two lemmas given in this paper. The existence of optimal plans is established using results of non-stationary dynamic programming. <b>dynamic</b> programming gambling <b>stopped</b> {{decision process}}es optimality equation maxiaml expected reward criteria of optimality value iteration optimal plan...|$|R
40|$|In this paper, I {{develop a}} career {{concerns}} model of government policy choice within a <b>dynamic</b> optimal <b>stopping</b> framework {{to predict the}} degree of surfing (opportunistic timing) and manipulation (politically motivated economic intervention) under alternate institutional structures and voter characteristics. Among other results, I find that the likelihood of opportunistic elections rises with exogenous economic performance, with longer maximum term lengths, with future electoral uncertainty, and with economic volatility but diminishes {{in the value of}} office-holding; manipulation increases with the maximum term length and with the value of office-holding but decreases with exogenous economic performance and with economic volatility. The model suggests that single-party governments should be highly opportunistic in calling elections and that countries that allow opportunistic election timing should experience less economically distortionary political intervention than their fixed-timing counterparts. 1 “A wise person does at once, what a fool does at last. Both do the same thing; only at different times. ...|$|R
50|$|The size of {{the portion}} of the input {{processed}} in the dynamic programming algorithm is determined to be the maximum between the longest dictionary match and the longest repeated match found at the start position (which is capped by the maximum LZMA match length, 273); furthermore, if a match longer than nice_len is found at any point in the range just defined, the <b>dynamic</b> programming algorithm <b>stops,</b> the solution for the subproblem up to that point is output, the nice_len-sized match is output, and a new dynamic programming problem instance is started at the byte after the match is output.|$|R
30|$|Reference [19] {{reported}} that CPM is a promising solution for future satellite communication {{systems in the}} Ka band because it yields a constant envelope signal that enables the nonlinear power amplifier to operate near saturation. From the power efficiency point of view, a serially coded CPM can provide high power gain when the iterative decoding process is performed. The complete analysis in [2] has proven that SCCPM is a valid alternative scheme for the uplink of satellite communications regardless {{of the presence of}} broadband and narrowband transmissions. In view of the information loss in the combination of high-order CPM and binary codes, the NB-LDPC-coded high-order PRCPM scheme is considered a potential alternative for the uplink of satellite communications. Unfortunately, the scheme with iterative detection and decoding technique exhibits positive feedback and relatively large decoding delay during iterative detection, similar to other iterative coded modulation systems. To address these problems, two <b>dynamic</b> iterative <b>stopping</b> algorithms, namely, cross entropy (CE) and hard decision aided (HDA) based on weighted extrinsic information exchange, are proposed in this paper for cases with medium and high SNRs.|$|R
40|$|Under non-exponential discounting, {{we develop}} a <b>dynamic</b> theory for <b>stopping</b> {{problems}} in continuous time. Our framework covers discount functions that induce decreasing impatience. Due to the inherent time inconsistency, {{we look for}} equilibrium stopping policies, formulated as fixed points of an operator. Under appropriate conditions, fixed-point iterations converge to equilibrium stopping policies. This iterative approach corresponds to the hierarchy of strategic reasoning in Game Theory, and provides "agent-specific" results: it assigns one specific equilibrium stopping policy to each agent according to her initial behavior. In particular, it leads to a precise mathematical connection between the naive behavior and the sophisticated one. Our theory is illustrated in a real options model...|$|R
40|$|In this work, {{we address}} an {{investment}} problem where the investment {{can either be}} made immediately or postponed to a later time, {{in the hope that}} market conditions become more favourable. In our case, uncertainty is introduced through market price. When the investment is undertaken, a fixed sunk cost must be paid and a series of cash flows are to be received. Therefore, we are faced with an irreversible investment. Real options analysis provides an adequate framework for this type of problems by recognizing these two characteristics, uncertainty and irreversibility, explicitly. We describe algorithmic solutions for this type of problems by modelling market prices evolution by Markov jump processes. Irreversible investment, optimal <b>stopping,</b> <b>dynamic</b> programming, Markov jump processes...|$|R
40|$|This paper speci…es and {{estimates}} a structural dynamic model of consumer demand {{for new and}} used durable goods. Its primary contribution is to provide an explicit estimation procedure for transaction costs, which are crucial to capturing the dynamic nature of consumer decisions. In particular, transaction costs {{play a key role}} in determining consumer replacement behavior in both primary and secondary markets for durable goods. The unique data set used in this paper has been collected by the Italian Motor Registry and covers the period from 1994 to 2004. It includes information about sales dates for individual cars over time as well as the initial stock of cars in the sample period. Identi…cation of transaction costs is achieved from the variation in the share of consumers choosing to hold a given car type each period, and from the share of consumers choosing to purchase the same car type that period. Speci…cally, I estimate a random coe ¢ cients discrete choice model that incorporates a <b>dynamic</b> optimal <b>stopping</b> problem in the spirit of Rust (1987). I apply this model to evaluate the impact of scrappage subsidies on the Italian automobile market in 1997 and 1998. ...|$|R
40|$|This paper {{specifies}} {{and estimates}} a structural dynamic model of consumer demand {{for new and}} used durable goods. Its primary contribution is to provide an explicit estimation procedure for transaction costs, which are crucial to capturing the dynamic nature of consumer decisions. In particular, transaction costs {{play a key role}} in determining consumer replacement behavior in both primary and secondary markets for durable goods. The unique data set used in this paper has been collected by the Italian Motor Registry and covers the period from 1994 to 2004. It includes information about sales dates for individual cars over time as well as the initial stock of cars in the sample period. Identification of transaction costs is achieved from the variation in the share of consumers choosing to hold a given car type each period, and from the share of consumers choosing to purchase the same car type that period. Specifically, I estimate a random coefficients discrete choice model that incorporates a <b>dynamic</b> optimal <b>stopping</b> problem in the spirit of Rust (1987). I apply this model to evaluate the impact of scrappage subsidies on the Italian automobile market in 199...|$|R
