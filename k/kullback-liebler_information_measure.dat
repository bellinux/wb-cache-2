0|5748|Public
2500|$|... Short {{introduction}} to the axioms of information theory, entropy, mutual <b>information,</b> <b>Kullback–Liebler</b> divergence, and Jensen–Shannon distance.|$|R
40|$|Abstract: In this paper, we {{established}} the s- convexity property of mutual {{information in the}} view of Renyi’s entropy and KL-distance function. We know there is a wide range for application of Kullback-Liebler distance function in information theory. So, we are going to prove its s-convexity. Key word: s- convexity, Renyi’s entropy, KL-distance function, <b>Kullback-Liebler</b> distance, <b>information</b> gain. I...|$|R
40|$|AbstractThis paper {{concerns}} an axiomatic {{characterization of}} <b>information</b> <b>measures</b> of dimension k. The Shannon entropy is an <b>information</b> <b>measure</b> of dimension one. Directed divergence and information improvement {{are examples of}} 2 -dimensional and 3 -dimensional <b>information</b> <b>measures.</b> From these k-dimensional <b>information</b> <b>measures,</b> one can obtain all dimensions (including 1, 2, and 3, which have already proven useful) at once. We determine all the <b>information</b> <b>measures</b> that depend upon several discrete probability distributions, have the sum property, and satisfy the additivity property. It is shown that the additive k-dimensional <b>information</b> <b>measures</b> along with the sum property are essentially the linear combination of the Shannon entropies and Kerridge inaccuracies...|$|R
40|$|The R-norm <b>information</b> <b>measure</b> is {{discussed}} and its properties, {{as well as}} an axiomatic characterization, are given. The measure is extended to conditional and joint measures. Applications to coding and hypothesis testing are given. The R-norm <b>information</b> <b>measure</b> includes Shannon's <b>information</b> <b>measure</b> as a special case...|$|R
40|$|In statistics, Fisher was {{the first}} to {{introduce}} the measure of the amount of information supplied by the data about the unknown parameter. We analyze the disadvantages of Fisher <b>information</b> <b>measure</b> for optimization of sampling designs. To overcome this problem, we modify Fisher <b>information</b> <b>measure</b> and we upgrade it to the multivariate setting. It turns out that a reasonable modification of Fisher <b>information</b> <b>measure</b> leads to a special case of Kullback <b>information</b> <b>measure,</b> both in the univariate and multivariate setting. Using Shannon’s and Wiener’s concept of information we also show a simple derivation of Kullback <b>information</b> <b>measure</b> for a special case when the prior distribution of the parameter is uniform and the posterior distribution is truncated normal. ...|$|R
40|$|We {{propose a}} {{generalized}} cumulative residual <b>information</b> <b>measure</b> based on Tsallis entropy and its dynamic version. We study the characterizations {{of the proposed}} <b>information</b> <b>measure</b> and define new classes of life distributions based on this measure. Some applications are provided in relation to weighted and equilibrium probability models. Finally the empirical cumulative Tsallis entropy is proposed to estimate the new <b>information</b> <b>measure...</b>|$|R
40|$|It is {{proved that}} the only {{additive}} and isotropic <b>information</b> <b>measure</b> that can depend on the probability distribution and also on its first derivative is a linear combination of the Boltzmann-Gibbs-Shannon and Fisher <b>information</b> <b>measures.</b> Power law equilibrium distributions are found {{as a result of}} the interaction of the two terms. The case of second order derivative dependence is investigated and a corresponding additive <b>information</b> <b>measure</b> is given. Comment: 10 pages, 1 figures, shortene...|$|R
40|$|The net Fisher <b>information</b> <b>measure</b> IT, {{defined as}} the product of {{position}} and momentum Fisher <b>information</b> <b>measures</b> Ir and Ik and derived from the non-relativistic Hartree-Fock wave functions for atoms with Z = 1 − 102, is found to correlate well with the inverse of the experimental ionization potential. Strong direct correlations of IT are also reported for the static dipole polarizability of atoms with Z = 1 − 88. The complexity measure, {{defined as the}} ratio of the net Onicescu <b>information</b> <b>measure</b> ET and IT, exhibits clearly marked regions corresponding to the periodicity of the atomic shell structure. The reported correlations highlight the need for using the net <b>information</b> <b>measures</b> in addition to either the position or momentum space analogues. With reference to the correlation of the experimental properties considered here, the net Fisher <b>information</b> <b>measure</b> is found to be superior than the net Shannon information entropy...|$|R
40|$|We study data {{processing}} inequalities that {{are derived from}} a certain class of generalized <b>information</b> <b>measures,</b> where a series of convex functions and multiplicative likelihood ratios are nested alternately. While these <b>information</b> <b>measures</b> {{can be viewed as}} a special case of the most general Zakai–Ziv generalized <b>information</b> <b>measure,</b> this special nested structure calls for attention and motivates our study. Specifically, a certain choice of the convex functions leads to an <b>information</b> <b>measure</b> that extends the notion of the Bhattacharyya distance (or the Chernoff divergence) : While the ordinary Bhattacharyya distance is based on the (weighted) geometric mean of two replicas of the channel’s conditional distribution, the more general <b>information</b> <b>measure</b> allows an arbitrary number of such replicas. We apply the {{data processing}} inequality induced by this <b>information</b> <b>measure</b> to a detailed study of lower bounds of parameter estimation under additive white Gaussian noise (AWGN) and show that in certain cases, tighter bounds can be obtained by using more than two replicas. While the resulting lower bound may not compete favorably with the best bounds available for the ordinary AWGN channel, the advantage of the new lower bound, relative to the other bounds, becomes significant in the presence of channel uncertainty, like unknown fading. This different behavior in the presence of channel uncertainty is explained by the convexity property of the <b>information</b> <b>measure...</b>|$|R
40|$|The paper {{introduces}} two new parametric generalizations of one {{of existing}} R norm fuzzy <b>information</b> <b>measures</b> with the proof of their validity. In addition, particular cases and important properties of the proposed measures are discussed. A numerical example is given to establish the similarity between the proposed R norm fuzzy <b>information</b> <b>measures</b> {{with one of the}} existing R norm fuzzy <b>information</b> <b>measures.</b> Further, a comparison among them is shown {{with the help of a}} table and graph...|$|R
40|$|Information {{theory is}} widely {{accepted}} as {{a powerful tool for}} analyzing complex systems and it has been applied in many disciplines. Recently, some central components of information theory - multivariate <b>information</b> <b>measures</b> - have found expanded use in the study of several phenomena. These <b>information</b> <b>measures</b> differ in subtle yet significant ways. Here, we will review the information theory behind each measure, as well as examine the differences between these measures by applying them to several simple model systems. In addition to these systems, we will illustrate the usefulness of the <b>information</b> <b>measures</b> by analyzing neural spiking data from a dissociated culture through early stages of its development. We hope that this work will aid other researchers as they seek the best multivariate <b>information</b> <b>measure</b> for their specific research goals and system. Finally, we have made software available online which allows the user to calculate all of the <b>information</b> <b>measures</b> discussed within this paper. Comment: Manuscript (15 pages, 3 figures, 8 tables...|$|R
40|$|We {{deal with}} {{conditional}} decomposable <b>information</b> <b>measures,</b> directly defined as functions on a suitable set of conditional events satisfying {{a class of}} axioms. For these general measures we introduce a notion of independence and study its main properties in order to compare it with classical definitions present in the literature. The particular case of Wiener-Shannon <b>information</b> <b>measure</b> is taken in consideration and the links between the provided independence for <b>information</b> <b>measures</b> and the independence for the underlying probability are analyzed...|$|R
40|$|Abstract — It is {{well known}} that the Shannon <b>information</b> <b>measures</b> are {{continuous}} functions of the probability distribution when the support is finite. This, however, does not hold when the support is countably infinite. In this paper, we investigate the continuity of the Shannon <b>information</b> <b>measures</b> for countably infinite support. With respect to a distance based on the Kullback-Liebler divergence, we use two different approaches to show that all the Shannon <b>information</b> <b>measures</b> are in fact discontinuous at all probability distributions with countably infinite support. I...|$|R
40|$|In {{the present}} paper the {{generalized}} mean codeword length is studied and characterized a new generalized <b>information</b> <b>measure</b> by obtaining bounds {{in terms of a}} new generalized <b>information</b> <b>measure</b> using Lagrange’s Multiplier method. The Shannon’s Noiseless coding theorem is verified by considering Huffman coding scheme and Shannon Fano coding scheme on taking empirical data. We study the monotone behaviour of the new generalized <b>information</b> <b>measure</b> with respect to parameters and. The important properties of the new generalized <b>measure</b> of <b>information</b> have also been studied...|$|R
40|$|The {{contribution}} of the thesis lies {{in the definition of}} new <b>information</b> <b>measures</b> in the context of censoring (random censoring, quantal random censoring), which results from classical <b>information</b> <b>measures,</b> and the study of various properties of the statistical information theory and new properties resulting from the nature of the censored <b>information</b> <b>measures.</b> In addition, the property of the loss of information on the uncensored case, due to descretization of a random variable, is also under study, based on a general and finite partition of the sample space. ...|$|R
40|$|We {{evaluate}} the asymptotics of equivocations, their exponents {{as well as}} their second-order coding rates under various Rényi <b>information</b> <b>measures.</b> Specifically, we consider the effect of applying a hash function on a source and we quantify the level of non-uniformity and dependence of the compressed source from another correlated source when the number of copies of the sources is large. Unlike previous works that use Shannon <b>information</b> <b>measures</b> to quantify randomness, information or uniformity, we define our security measures in terms of a more general class of <b>information</b> <b>measures</b> [...] the Rényi <b>information</b> <b>measures</b> and their Gallager-type counterparts. A special case of these Rényi <b>information</b> <b>measure</b> is the class of Shannon <b>information</b> <b>measures.</b> We prove tight asymptotic results for the security measures and their exponential rates of decay. We also prove bounds on the second-order asymptotics and show that these bounds match when the magnitudes of the second-order coding rates are large. We do so by establishing new classes non-asymptotic bounds on the equivocation and evaluating these bounds using various probabilistic limit theorems asymptotically. Comment: 47 pages, 9 figures; Presented at the 2015 International Symposium on Information Theory (Hong Kong); Submitted to the IEEE Transactions on Information Theory; v 3 : fixed typos and added some clarifications to the proof...|$|R
2500|$|Arndt, C. [...] <b>Information</b> <b>Measures,</b> <b>Information</b> and its Description in Science and Engineering (Springer Series: Signals and Communication Technology), 2004, ...|$|R
5000|$|... #Subtitle level 3: <b>Information</b> <b>measure</b> for {{stereoscopic}} images ...|$|R
40|$|Abstract. In {{the present}} communication, I have defined the new <b>information</b> <b>measure</b> called “α-R-norm informa-tion measure”. It has been {{characterized}} using infimum oiperation in Section 2 and axiomatically in Section 3. Its properties have been studied in Section 4, joint and conditional α-R-norm <b>information</b> <b>measure</b> are studied in Sec-tion 5. 1...|$|R
40|$|We {{analyze the}} problem of {{estimating}} some particular indices (based on <b>information</b> <b>measures)</b> in simple-stage cluster sampling, and conclude that the indices based on <b>information</b> <b>measures</b> of degree [beta] = 2 allow us to construct unbiased estimates of their population values. diversity finite population income inequality simple-stage cluster sampling unbiased estimation. ...|$|R
40|$|Information Theory is {{a branch}} of mathematics, more {{specifically}} probability theory, that studies information quantification. Recently, several researches have been successful {{with the use of}} Information Theoretic Learning (ITL) as a new technique of unsupervised learning. In these works, <b>information</b> <b>measures</b> are used as criterion of optimality in learning. In this article, we will analyze a still unexplored aspect of these <b>information</b> <b>measures,</b> their dynamic behavior. Autoregressive models (linear and non-linear) will be used to represent the dynamics in <b>information</b> <b>measures.</b> As a source of dynamic information, videos with different characteristics like fading, monotonous sequences, etc., will be used...|$|R
40|$|I n {{the present}} communication, we review the {{existing}} <b>measures</b> of fuzzy <b>information.</b> Wedefine and characterize two fuzzy <b>information</b> <b>measures</b> which are sub additive and differentfrom known <b>measures</b> of fuzzy <b>information.</b> We also study monotonic behavior and particularcases of these fuzzy <b>information</b> <b>measures.</b> 2000 Mathematics Subject Classification: 94 A 17 and 94 D 0...|$|R
5000|$|Institute of Nanotechnologies, <b>Information</b> <b>Measuring</b> and Specialized Computer Systems in Power Industry ...|$|R
40|$|Construction, {{estimation}} {{and application}} of the mutual <b>information</b> <b>measure</b> have been presented in this paper. The simulations {{have been carried out}} to verify its usefulness to detect nonlinear serial dependencies. Moreover, the mutual <b>information</b> <b>measure</b> has been applied to the indices and the sector sub-indices of the Warsaw Stock Exchange. nonlinearity, mutual information coefficient, mutual information, serial dependencies. ...|$|R
40|$|The main quantum <b>information</b> <b>measures</b> are {{discussed}} {{with respect to}} their relation to physics. It is argued that the basic term to choose between the possible ways to <b>measure</b> quantum <b>information</b> is compatibility/incompatibility of the quantum states, resulting in coherent information and here suggested compatible <b>information</b> <b>measures.</b> A sketch of information optimization of a quantum experimental setup is proposed...|$|R
40|$|Reading {{times on}} {{words in a}} {{sentence}} depend {{on the amount of}} information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by <b>information</b> <b>measures.</b> Three types of language models estimated four different <b>information</b> <b>measures</b> on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the <b>information</b> <b>measures</b> and ERPs revealed a reliable correlation between N 400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N 400 amplitude. These findings suggest that different <b>information</b> <b>measures</b> quantify cognitively different processes and that readers do not make use of a sentence’s hierarchical structure for generating expectations about the upcoming word...|$|R
50|$|Akaike, H. (1983). <b>Information</b> <b>measures</b> {{and model}} selection. International Statistical Institute 44, 277-291.|$|R
40|$|Abstract. In a {{thermodynamic}} {{process with}} measurement and feedback, the second law of thermodynamics {{is no longer}} valid. In its place, various second-law-like inequalities have been advanced that each incorporate a distinct additional term accounting for the information gathered through measurement. We quantitatively compare {{a number of these}} <b>information</b> <b>measures</b> using an analytically tractable model for the feedback cooling of a Brownian particle. We find that the <b>information</b> <b>measures</b> form a hierarchy that reveals a web of interconnections. To untangle their relationships, we address the origins of the information, arguing that each <b>information</b> <b>measure</b> represents the minimum thermodynamic cost to acquire that information through a separate, distinct measurement protocol...|$|R
40|$|The {{problem to}} {{characterize}} and investigate structured objects by using information theory is currently of considerable interest. In this paper, we describe {{a method for}} characterizing structured objects representing graphs by means of information inequalities. For this, we deal with information inequalities which describe relations between <b>information</b> <b>measures</b> for graphs. Additionally, we sketch an approach for comparing such <b>information</b> <b>measures</b> qualitatively. ...|$|R
40|$|We show, {{starting}} from first principles, that thermodynamics' first law can be microscopically obtained for Fisher's <b>information</b> <b>measure</b> without need of invoking the adiabatic theorem. Further, it is proved that enforcing the Fisher-first law requirements {{in a process}} in which the probability distribution is infinitesimally varied is equivalent to minimizing Fisher's <b>information</b> <b>measure</b> subject to appropriate constraints. (c) 2006 Elsevier B. V. All rights reserved...|$|R
40|$|International audienceWe {{studied the}} time {{fluctuations}} in the dynamics of geoelectrical data, recorded in Tito site, which {{is located in a}} seismic area of southern Italy. We used the Fisher <b>Information</b> <b>Measure,</b> which is a powerful tool to investigate complex and nonstationary signals. The time evolution of the Fisher <b>Information</b> <b>Measure</b> calculated for our signal reveals links with the earthquakes occurring in the investigated area...|$|R
40|$|An {{iterative}} phase retrieval algorithm {{based on}} the maximum entropy method (MEM) is presented. Introducing a new generalized <b>information</b> <b>measure,</b> we derive a novel class of algorithms which includes the conventionally used error reduction algorithm and a MEM-type iterative algorithm which is presented for the first time. These different phase retrieval methods are unified {{on the basis of}} the framework of <b>information</b> <b>measures</b> used in <b>information</b> theory...|$|R
40|$|We {{studied the}} time {{fluctuations}} in the dynamics of geoelectrical data, recorded in Tito site, which {{is located in a}} seismic area of southern Italy. We used the Fisher <b>Information</b> <b>Measure,</b> which is a powerful tool to investigate complex and nonstationary signals. The time evolution of the Fisher <b>Information</b> <b>Measure</b> calculated for our signal reveals links with the earthquakes occurring in the investigated area...|$|R
40|$|In a {{thermodynamic}} {{process with}} measurement and feedback, the second law of thermodynamics {{is no longer}} valid. In its place, various second-law-like inequalities have been advanced that each incorporate a distinct additional term accounting for the information gathered through measurement. We quantitatively compare {{a number of these}} <b>information</b> <b>measures</b> using an analytically tractable model for the feedback cooling of a Brownian particle. We find that the <b>information</b> <b>measures</b> form a hierarchy that reveals a web of interconnections. To untangle their relationships, we address the origins of the information, arguing that each <b>information</b> <b>measure</b> represents the minimum thermodynamic cost to acquire that information through a separate, distinct measurement protocol. Comment: 29 pages, 5 figure...|$|R
40|$|In {{the present}} communication, we review the {{existing}} <b>measures</b> of fuzzy <b>information.</b> We define and characterize two fuzzy <b>information</b> <b>measures</b> which are sub additive and different from known <b>measures</b> of fuzzy <b>information.</b> We also study monotonic behavior and particular cases of these fuzzy <b>information</b> <b>measures.</b> 2000 Mathematics Subject Classification: 94 A 17 and 94 D 05 Key Words: Fuzzy sets; fuzzy directed divergence; monotonic functions and convex functions. 1...|$|R
40|$|AbstractPosition and {{momentum}} <b>information</b> <b>measures</b> are evaluated for the ground {{state of the}} relativistic hydrogen-like atoms. Consequences {{of the fact that}} the radial momentum operator is not self-adjoint are explicitly studied, exhibiting fundamental shortcomings of the conventional uncertainty measures in terms of the radial position {{and momentum}} variances. The Shannon and Rényi entropies, the Fisher <b>information</b> <b>measure,</b> as well as several related <b>information</b> <b>measures,</b> are considered as viable alternatives. Detailed results on the onset of relativistic effects for low nuclear charges, and on the extreme relativistic limit, are presented. The relativistic position density decays exponentially at large r, but is singular at the origin. Correspondingly, the momentum density decays as an inverse power of p. Both features yield divergent Rényi entropies away from a finite vicinity of the Shannon entropy. While the position space <b>information</b> <b>measures</b> can be evaluated analytically for both the nonrelativistic and the relativistic hydrogen atom, this is not the case for the relativistic momentum space. Some of the results allow interesting insight into the significance of recently evaluated Dirac–Fock vs. Hartree–Fock complexity measures for many-electron neutral atoms...|$|R
