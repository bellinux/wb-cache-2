4|26|Public
5000|$|The CRC-4 {{multiframe}} alignment word {{only takes}} up {{six of the}} first eight bits of the TS0 without FAS. There are two bits in every second block or submultiframe, whose task is to indicate block errors in {{the far end of}} the communication. The mechanismis as follows: Both bits (called E-bits) have [...] "1" [...] as their default value. When {{the far end of the}} communication receives a 2 Mbit/s frame and detects an <b>erroneous</b> <b>block,</b> it puts a [...] "0" [...] in the E-bit that corresponds to the block in the frame being sent along the return path to the transmitter. This way, the near end of the communication is informed that an <b>erroneous</b> <b>block</b> has been detected, and both ends have the same information: one from the CRC-4 procedure and the other from the E bits. If we number the frames in the multiframe from 0 to 15, the E-bit of frame 13 refers to the submultiframe I (block I) received at the far end, and the E-bit of frame 15 refers to the submultiframe II (block II).|$|E
40|$|In this paper, {{the problem}} of spatial error {{concealment}} for real-time applications is addressed. The proposed method can be categorized in exemplar-based error concealment approaches. In this category, a patch of corrupted pixels are replaced by another patch of the image that contains correct pixels. For splitting the <b>erroneous</b> <b>block</b> to different patches, a novel context-dependent exemplar-based algorithm based on a previously proposed segmentation method is proposed. The capability of the proposed method for concealment in diverse image regions is depicted. Our detailed conducted experiments show that the proposed method outperforms the state-of-the-art spatial error concealment methods in terms of output quality. Key word...|$|E
40|$|With the {{increasing}} number of image communication applications especially in the low complexity domain, error concealment has become a very important field of research. Since many compression standards for images and videos are block-based a lot of methods were applied to conceal block losses in monocular images. The fast progress of capture, representation and display technologies for 3 D image data advances the efforts on 3 D concealment strategies. Because of their psycho-visual characteristics, stereoscopic images have to fulfill a very high quality demand. We propose an algorithm that makes use of the redundancies between two views of a stereo image pair. In many cases <b>erroneous</b> <b>block</b> bursts occur and can be highly disturbing, thus we will mainly concentrate on these errors. In addition, we focused on the quality assessment of several error concealment strategies. Beside the objective evaluation measures, we carried out a subjective quality test following the DSCQS methodology as proposed by MPEG. The results of this test demonstrate the efficiency of our approach...|$|E
50|$|Block Error Rate (BLER) is a {{ratio of}} the number of <b>erroneous</b> <b>blocks</b> to the total number of blocks {{received}} on a digital circuit.|$|R
40|$|An Adaptive Spatio-Temporal Error Concealment (ASTEC) scheme {{based on}} H. 264 /AVC is {{proposed}} in the paper {{to improve the quality}} of the rebuilt frame in H. 264 /AVC decoder side. Combining the spatial error concealment method with the temporal one, a new scheme is proposed to conceal an <b>erroneous</b> macro <b>block</b> according to its encoded mode. The temporal error concealment scheme adopts the weighted Boundary Matching Algorithm to recover the <b>erroneous</b> macro <b>block.</b> The spatial error concealment scheme recovers the <b>erroneous</b> macro <b>block</b> by exploiting the directional interpolation which subdivides the macro block according to the number and the direction of edges going through it. Experiments show that the proposed concealment scheme can restrain the diffusion of <b>erroneous</b> macro <b>block</b> and improve the quality of the rebuilt frame subjectively as well as objectively...|$|R
40|$|In network {{delivery}} of compressed video, packets {{may be lost}} if the channel is unreliable. Such losses tend to occur in burst. In this paper, we develop an error resilient video encoding approach to help error concealment at the decoder. We introduce a new block shuffling scheme to isolate <b>erroneous</b> <b>blocks</b> caused by packet losses. And we apply data hiding to add additional protection for motion vectors. The incorporation of these scheme adds little complexity to the standard encoder. Experimental results suggest that our approach can achieve a reasonable quality for packet loss up to 30 % {{over a wide range}} of video materials...|$|R
30|$|Two simple {{background}} initialization {{techniques are}} the pixel-wise temporal mean and median filters {{over a large}} number of video frames [20, 21]. For the pixel-wise temporal median filter, it is assumed that for each pixel within the estimation duration, the exposure of the background must be more than that of the foreground. Based on the block-wise strategy, Farin et al. [19] used a block similarity matrix to segment the input video frames into foreground and background regions, which contain the block-wise temporal differences between any video frame pair. Reddy et al. [22] proposed a block selection approach using the discrete cosine transform (DCT) among some neighboring blocks to estimate the unconstructed parts of the background. This approach is usually degraded by similar frequency content within a block candidate set and error propagation if some blocks in a video frame are erroneously estimated. Note that, to obtain the processing results, the whole video sequence should be available to Reddy et al.’s approach. Then, the DCT is replaced by the Hadamard transform to reduce the computation time for block selection [23]. In addition, a block selection refinement step using spatial continuity along block borders is added to prevent <b>erroneous</b> <b>block</b> selection. Most block-wise background initialization approaches need large memories and are computationally expensive. Furthermore, one free-background video frame is usually obtained as its output during the “learning” duration.|$|E
40|$|This paper {{discusses}} the error effects in wavelet com-pression codecs for real-time ECG monitoring in a wire-less telecardiology application. Two different strategies for ECG coding are presented and the error {{effects in the}} received ECG signals are discussed. Both quantitative (RMS error index) and qualitative (cardiologist opinions) are presented in order to decide if {{it is useful to}} monitor retrieved information from ECG packets received with er-rors or erroneous packets should be discarded. Although RMS index suggests that it would be recommendable to show them, cardiologists conditioned their preference to the number of <b>erroneous</b> <b>blocks</b> in the monitoring process: few number of errors, monitoring no information; high number of errors, monitoring erroneous information. 1...|$|R
40|$|Datalink layer framing in {{wireless}} sensor networks usually faces {{a trade-off}} between large frame sizes for high channel bandwidth utilization and small frame sizes for effective error recovery. Given the high error rates of intermote communications, TinyOS opts in favor of small frame sizes {{at the cost of}} extremely low channel bandwidth utilization. In this paper, we describe Seda: a streaming datalink layer that resolves the above dilemma by decoupling framing from error recovery. Seda treats the packets from the upper layer as a continuous stream of bytes. It breaks the data stream into <b>blocks,</b> and retransmits <b>erroneous</b> <b>blocks</b> only (as opposed to the entire erroneous frame). Consequently, the frame-error-rate (FER), the main factor that bounds the frame size in the current design, becomes irrelevant to erro...|$|R
40|$|In {{the process}} of {{transmission}} of digital encoded video bit streams over a physical network, the data is corrupted by random bit errors, which has a severe effect on the decoded video quality. The success of an error concealment algorithm depends heavily on the detection of the corrupted regions. We propose a new error detection algorithm for discrete cosine transform based video coding. Our algorithm detects thus far undetectable erroneous regions with a very little increase in computational complexity. We show that <b>erroneous</b> <b>blocks</b> in the decoded frames can be detected by monitoring the variation of average energy from block to block. To avoid false detection due {{to the presence of}} strong edges we employ a very fast DCT coefficient based edge detection algorithm. Computer simulation shows that the quality of the recovered image is significantly improved in comparison to well-known methods...|$|R
40|$|The {{stability}} behaviour of the iterative decoding (ID) of Turbo-Codes and {{its influence}} on the overall performance are discussed. Several criteria to characterize the ID are proposed and the most reliable ones are determined. These criteria are used to partition the transnitted blocks into stable and unstable ones. For unstable blocks the decoding step is determined which most likely produces a low number of decoding errors. These operations allow to lower the bit error rate (BER) significantly and to detect the <b>erroneous</b> <b>blocks</b> with high probability. Simulations show an improvement in BER by factor 10 and more. 1 Introduction For moderate reliability requirements Turbo-Codes (TC) ([1]) are currently the most powerful known error control codes. The coding scheme of TC's is a parallel concatenation of constituent codes. Decoding of these codes is performed via iterative decoding (ID) of the constituent codes. TC's allow to create very long codewords and to achieve a moderate BER at an [...] ...|$|R
40|$|This paper proposes an {{innovative}} technique for correcting of <b>erroneous</b> data <b>blocks</b> in transmitted over the wireless channels of global computer network. Proposed method {{is based on}} recovering of whole data damaged blocks instead of individual symbols correction. Efficiency is achieved by separating the error detection from the correction process and using different codes for each case. The proposed technique is based on simple mathematical operations and is suitable for implementation in FPGA devices...|$|R
50|$|Significant overblocking of Internet sites by mobile {{operators}} is reported, {{including the}} blocking of political satire, feminism and gay content. Research by the Open Rights Group highlighted the widespread nature of unjustified site blocking. In 2011 the group set up Blocked.org.uk, a website allowing {{the reporting of}} sites and services that are 'blocked' on their mobile network. The website received hundreds of reports of the blocking of sites covering blogs, business, internet privacy and internet forums across multiple networks. The Open Rights Group also demonstrated that correcting the <b>erroneous</b> <b>blocking</b> of innocent sites can be difficult. No UK mobile operator provides an on-line tool for identifying blocked websites. The O2 Website status checker was available {{until the end of}} 2013 but was suspended in Decemberafter it had been widely used {{to determine the extent of}} overblocking by O2. Not only were civil liberties and computing sites being blocked, but also Childline, the NSPCC, the Police. An additional opt-in whitelist service aimed at users under 12 years is provided by O2. The service only allows access to websites on a list of categories deemed suitable for that age group.|$|R
5000|$|As {{mentioned}} above, {{there is}} an exception for [...] "bona fide research". An institution can disable filters for adults {{in the pursuit of}} bona fide research or another type of lawful purpose. However, the law provides no definition for [...] "bona fide research". However, in a later ruling the U.S. Supreme Court said that libraries would be required to adopt an Internet use policy providing for unblocking the Internet for adult users, without a requirement that the library inquire into the user's reasons for disabling the filter. Justice Rehnquist stated [...] "assuming that such <b>erroneous</b> <b>blocking</b> presents constitutional difficulties, any such concerns are dispelled by the ease with which patrons may have the filtering software disabled. When a patron encounters a blocked site, he need only ask a librarian to unblock it or (at least in the case of adults) disable the filter". This effectively puts the decision of what constitutes [...] "bona fide research" [...] {{in the hands of the}} adult asking to have the filter disabled. The U.S. Federal Communications Commission (FCC) subsequently instructed libraries complying with CIPA to implement a procedure for unblocking the filter upon request by an adult.|$|R
50|$|Safetica Technologies is a European {{data loss}} {{prevention}} software (DLP) vendor. Safetica DLP {{is designed for}} contextual protection of data at the endpoint level. It <b>blocks</b> <b>erroneous</b> or malicious actions which might lead to sensitive files leaving a company. Safetica can also identify wasted costs connected with low work productivity or ineffective use of software licenses or wasted print.|$|R
40|$|Abstract—This paper proposes two {{data hiding}} {{approaches}} using compressed MPEG video. In the first approach, the quantization scale of a Constant Bit Rate (CBR) video is either incremented or decremented {{according to the}} underlying message bit. A secondorder multivariate regression is used to associate macroblock-level features with the hidden message bit. The decoder makes use of this regression model to predict the message bits. However, the message payload is restricted to one bit per macroblock. The second approach of our work for both CBR and variable bit rate (VBR) coding and achieves a message payload of 3 bits per macroblock. The Flexible Macroblock Ordering (FMO) was used to allocate macroblocks to slice groups according {{to the content of}} the message. In existing network delivery of compressed video, packets may be lost if the channel is unreliable. Such losses tend to occur in burst. We can enhance our work to robustness of the existing work against packet losses in video steganalysis methods. We propose a robust error resilient approach for MPEG video transmission over internet. In this work, we develop an error resilient video encoding approach to help error concealment at the decoder. We introduce a new block shuffling scheme to isolate <b>erroneous</b> <b>blocks</b> caused by packet losses. And we apply data hiding to add additional protection for motion vectors. The existing solutions are superior in terms of message payload while causing less distortion and compression overhead and the proposed solution reduces the packet loss during transmission...|$|R
40|$|Most of all data-reduced HD-VCRs use {{powerful}} error protection schemes {{in order}} to prevent error propagation. Under normal conditions these schemes are able to correct all errors from the magnetic tape channel. However, under bad conditions some uncorrected errors may be left. Therefore, it must be possible to conceal <b>erroneous</b> DCT <b>blocks</b> {{in order to}} avoid visible degradations in this worst-case situation. In this context four different concealment techniques (spatial, temporal, motion-adaptive and motion-compensated concealment) are discussed and compared with respect to their efficiency. The results of subjective tests and the discussion of some system aspects show that the motion-adaptive approach is the most suitable one for the underlying application...|$|R
40|$|In this paper, a {{transmission}} system is proposed to enable reliable transmission of the vulnerable compressed images over fading channels, We introduce two techniques: 1) a block shuffling scheme which effectively isolates the <b>erroneous</b> picture <b>blocks</b> and thus improves the performance of error concealment at the decoder and 2) an adaptive interleaving scheme which adaptively changes the interleaving size such that bursty errors are randomized and become correctable by the embedded forward error control (FEC) scheme. The proposed system permits some well-known standards to be employed without modifying the source coding algorithms. Using JPEG [1] coded images and simulated fading channels, we confirm the robustness of the proposed techniques {{over a wide range}} of operation conditions...|$|R
5000|$|Automatic repeat request (ARQ) (sometimes also {{referred}} to as backward error correction): This is an error control technique whereby an error detection scheme is combined with requests for retransmission of <b>erroneous</b> data. Every <b>block</b> of data received is checked using the error detection code used, and if the check fails, retransmission of the data is requested - this may be done repeatedly, until the data can be verified.|$|R
40|$|Two {{experiments}} {{examined the}} extent to which <b>erroneous</b> recall <b>blocks</b> veridical recall using, as a vehicle for study, the disruptive impact of distractors that are semantically similar to a list of words presented for free recall. Instructing participants to avoid erroneous recall of to-be-ignored spoken distractors attenuated their recall but this did not influence the disruptive effect of those distractors on veridical recall (Experiment 1). Using an externalised output-editing procedure—whereby participants recalled all items that came to mind and identified those that were erroneous—the usual between-sequence semantic similarity effect on erroneous and veridical recall was replicated but the relationship between the rate of erroneous and veridical recall was weak (Experiment 2). The results suggest that forgetting is not due to veridical recall being blocked by similar events...|$|R
30|$|The article “Outage {{probability}} of a relay strategy allowing intra-link errors utilizing Slepian-Wolf theorem” by M. Cheng, K. Anwar, and T. Matsumoto considers a relaying strategy {{that is based on}} the Slepian-Wolf (SW) theorem originally formulated for the compression of correlated sources. As opposed to conventional decode-and-forward (DF) relaying, where <b>erroneous</b> data <b>blocks</b> at the relay are discarded, the proposed SW relaying scheme allows for intra-link errors between source and relay, while employing a joint decoding scheme at the destination which exploits the correlation between the source signal and the relayed signal. The authors provide novel integral expressions for the resulting outage probability as well as asymptotic results illustrating the impact of intra-link errors on the resulting diversity order. The theoretical results are corroborated by simulation results obtained for a SW relaying scheme based on bit-interleaved coded modulation with iterative detection.|$|R
40|$|Abstract—Broadcast of {{a message}} to nodes in a network is one of {{elementary}} but inevitable techniques in wireless ad-hoc and sensor networks. In this paper, we apply the protocol, which the authors proposed for cooperative multi-hop relay networks, to message broadcast over random error channels. Performance is evaluated {{in terms of the}} delivery ratio by means of computer simulations. The proposed protocol utilizes an MDS code and a relay node randomly transfers one of partitioned codeword blocks rather than the original message. We suppose a network of square-lattice topology as a preliminary example. Numerical results show that the significant performance improvement can be achieved, in particular, if a relay node can make use of previously received <b>erroneous</b> codeword <b>blocks</b> in the decoding procedure of the MDS code. Index Terms—Broadcast, communications, Delivery ratio, MDS codes, Random error channel...|$|R
40|$|A novel hybrid {{automatic}} repeat request {{system based}} on turbo codes, called turbo HARQ system, is proposed. The iterative turbo decoding procedure is exploited to request retransmission of not decodable blocks without the necessity of an outer error [...] detecting code. It is shown that the turbo HARQ scheme with code rate R c = 1 = 2 and interleaver length 1024 significantly outperforms the classical turbo coding scheme [...] - especially for low SNR [...] - without essential loss in throughput. 1 INTRODUCTION Automatic repeat request (ARQ) protocols are well [...] known methods to achieve high reliability in digital transmission schemes. The information is protected by an error [...] detecting block code. If the decoder detects an <b>erroneous</b> transmitted <b>block,</b> retransmission of this block is requested via a feedback channel which is assumed to be error [...] free throughout this paper. Additionally, the request is assumed to be repeated until the decoder detects error [...] free transmission. The great advantage of s [...] ...|$|R
40|$|Abstract—Technology scaling {{advancement}} {{coupled with}} op-erational and environmental effects make embedded memories {{more vulnerable to}} both manufacturing and transient errors including multi-bit upsets. Conventional error correcting codes incur high latency, area, and power overheads to correct multi-bit errors. In this paper, we propose embedded erasure coding (EEC), a low-cost technique that can correct multi-bit upsets with low overheads. It employs interleaved parity bits to provide a fast and low-cost multi-bit error detection. Using the erasure coding concept, the error correction is done by reconstructing {{the contents of the}} <b>erroneous</b> cache <b>blocks</b> within each cache set. It trades the performance for higher reliability by reserving a part of the cache (e. g. one way) to store the erasure codes. Our simulation results show that EEC provides high reliability (100 % error detection and correction) with a faster error recovery with lower area overhead and less than 3 % performance loss as compared to other state-of-the-art techniques. I...|$|R
40|$|Background: During evolution, {{large-scale}} genome rearrangements of chromosomes shuffle {{the order}} of homologous genome sequences ("synteny blocks”) across species. Some years ago, a controversy erupted in genome rearrangement studies over whether rearrangements recur, causing breakpoints to be reused. Methods: We investigate this controversial issue using the synteny block’s for human-mouse-rat reported by Bourque et al. {{and a series of}} synteny blocks we generated using Mauve at resolutions ranging from coarse to very fine-scale. We conducted analyses to test how resolution affects the traditional measure of the breakpoint reuse rate. Results: We found that the inversion-based breakpoint reuse rate is low at fine-scale synteny block resolution and that it rises and eventually falls as synteny block resolution decreases. By analyzing the cycle structure of the breakpoint graph of human-mouse-rat synteny blocks for human-mouse and comparing with theoretically derived distributions for random genome rearrangements, we showed that the implied genome rearrangements at each level of resolution become more “random ” as synteny block resolution diminishes. At highest synteny block resolutions the Hannenhalli-Pevzner inversion distance deviates from the Double Cut and Join distance, possibly due to small-scale transpositions or simply due to inclusion of <b>erroneous</b> synteny <b>blocks.</b> At synteny block resolutions as coarse as the Bourque et al. blocks, we show the breakpoint graph cycle structure has alread...|$|R
40|$|Abstract Background During evolution, {{large-scale}} genome rearrangements of chromosomes shuffle {{the order}} of homologous genome sequences ("synteny blocks") across species. Some years ago, a controversy erupted in genome rearrangement studies over whether rearrangements recur, causing breakpoints to be reused. Methods We investigate this controversial issue using the synteny block's for human-mouse-rat reported by Bourque et al. {{and a series of}} synteny blocks we generated using Mauve at resolutions ranging from coarse to very fine-scale. We conducted analyses to test how resolution affects the traditional measure of the breakpoint reuse rate. Results We found that the inversion-based breakpoint reuse rate is low at fine-scale synteny block resolution and that it rises and eventually falls as synteny block resolution decreases. By analyzing the cycle structure of the breakpoint graph of human-mouse-rat synteny blocks for human-mouse and comparing with theoretically derived distributions for random genome rearrangements, we showed that the implied genome rearrangements at each level of resolution become more &# 8220;random&# 8221; as synteny block resolution diminishes. At highest synteny block resolutions the Hannenhalli-Pevzner inversion distance deviates from the Double Cut and Join distance, possibly due to small-scale transpositions or simply due to inclusion of <b>erroneous</b> synteny <b>blocks.</b> At synteny block resolutions as coarse as the Bourque et al. blocks, we show the breakpoint graph cycle structure has already converged to the pattern expected for a random distribution of synteny blocks. Conclusions The inferred breakpoint reuse rate depends on synteny block resolution in human-mouse genome comparisons. At fine-scale resolution, the cycle structure for the transformation appears less random compared to that for coarse resolution. Small synteny blocks may contain critical information for accurate reconstruction of genome rearrangement history and parameters...|$|R
40|$|Applications for {{smartphones}} {{or other}} mobile devices (“apps”) {{are used by}} billions of subscribers worldwide. Apps and their users are at risk, however: due to their unique and unprecedented properties, apps have a high potential for faults and errors, and due to their popularity, apps have become a lucrative target for attackers’ malicious activities. This dissertation introduces static and dynamic analysis techniques for fault discovery, localization, and recovery in apps. The existing approaches for fault discovery and localization either are not mature enough (i. e., lack of sound static and dynamic techniques and low coverage with high overhead) or have severe limitations (e. g., work only on emulators). We facilitate the discovery of faults through Automatic Android App Explorer(A 3 E). As the name suggests, A 3 E generates test cases on-the-fly and exercises them. It achieves that through two distinctive strategies: an exploration in a depth-first manner and targeted exploration from a particular activity (page) inside the app. After fault discovery, fault localization is necessary to contain the error. To permit fault localization wehave developed three techniques: app state recreation through user defined deep links, automatic GUI-input generation through AndroidArrow, and the program slicing framework AndroidSlicer. uLink, our next solution, is a demand-driven record-and-replay library that creates replayable links on-the-fly; these links can be directly re-executed to re-constructa particular application state. AndroidArrow is a toolset to generate UI element event sequences which can be injected into an app to trigger a target method or program point. Our next localization strategy is AndroidSlicer; a slicing framework for Android apps. Whileprogram slicing is not new, no substantial slicing {{work has been done}} so far for smartphone platforms. As smartphone apps have new, unprecedented characteristics, slicing techniques need to evolve accordingly. Apart from fault localization, slicing has many other applications, such as improving dynamic taint analysis, finding relevant inputs, and undo computing. Finally, we have developed an app recovery technique that uses automatedpatching while the app runs to seal off faulty code with specific recovery routines, so the app can recover from faults on-the-fly. In summary, this dissertation explores and employs static and dynamic analysis and techniques, algorithms and, in some cases, improves existing approaches to creating high coverage test cases (fault discovery), identifying program faults (fault localization), and finally, sealing off <b>erroneous</b> code <b>blocks</b> (fault recovery) to render a smooth and uninterruptedapp experience...|$|R
40|$|The {{aim of this}} {{research}} is to enhance the quality of service in video applications when they are operating over error prone environments. With high compression ratio and low complexity block transform video coders, such as ITU-T H. 263 standard algorithm for very low bit rate video coding, compressed video streams can be used for multimedia services. However, when transmitting compressed video streams over channels with degraded conditions, several problems arise and undermine the video decoder from correctly reconstructing the video signal. This thesis presents several techniques employed to enhance the quality of service of a video communication application under <b>erroneous</b> conditions. First, <b>block</b> transform methods for video coding are examined, and their strengths and weaknesses are assessed in terms of performance and error robustness. Packetised video signals are considered and two different techniques are implemented for packet video networks {{to improve the quality of}} service on one hand and help resolve the state of congestion that might occur on any video communication medium on the other hand. Additionally, zero redundancy error concealment techniques are considered and applied on the block based video decoder to improve the quality of the reconstructed video signal without any redundancy added on the video bitstream. Then, the aspects of error resilience issues in block transfom video coders are discussed. Based on the different categories of errors encountered in coded video streams, several novel techniques are implemented to render the video coder more immune to channel deterioration. Some of these techniques are combined to form an error resilience algorithm that is implemented on H. 263 to enhance its performance over error prone environments such as mobile radio links. In parallel with the development process of the MPEG- 4 video coder, we apply the two-way decoding with reversible codewords on the H. 263 standard. Results are shown throughout the thesis to evidence the effectiveness of the proposed techniques and illustrate the improvement on the quality of service on both the objective and subjective scales. We conclude with thoughts for future expansion of error control strategies in block based video coding for mobile multimedia services over the foreseen universal mobile telecommunication systems (UMTS) network...|$|R

