592|258|Public
5|$|In 2010, some {{manufacturers}} (notably Samsung) introduced SSD controllers that extended {{the concept of}} BGC to analyze the file system used on the SSD, to identify recently deleted files and unpartitioned space. Samsung claimed that this would ensure that even systems (operating systems and SATA controller hardware) which do not support TRIM could achieve similar performance. The operation of the Samsung implementation appeared to assume and require an NTFS file system. It is not clear if this feature is still available in currently shipping SSDs from these manufacturers. Systematic <b>data</b> <b>corruption</b> has been reported on these drives {{if they are not}} formatted properly using MBR and NTFS.|$|E
25|$|Memory pages can {{be marked}} as read-only, to prevent <b>data</b> <b>corruption.</b>|$|E
25|$|Can store a user-specified {{number of}} copies of data or metadata, or {{selected}} types of data, to improve the ability to recover from <b>data</b> <b>corruption</b> of important files and structures.|$|E
40|$|Abstract — Cloud storage systems provide storage {{service on}} the internet. Since cloud is {{accessed}} via internet, data stored in clouds should remain private. Towards privacy and correctness of the data in cloud, several intrusion detection techniques, authentication methods and access control policies are being used. But they suffer from storage overhead. To minimize this overhead, several designs are currently using erasure codes. Now, the current research on cloud storage systems focuses on efficiency and robustness. But here, the major challenge is how to prevent silent <b>data</b> <b>corruptions</b> during reconstruction of data. This paper uses secure decentralized erasure-coded storage systems to guarantee a better privacy, efficiency and robustness and proposes a method that can reconstruct data in clouds by preventing silent <b>data</b> <b>corruptions.</b> Keywords-cloud, privacy, silent <b>data</b> <b>corruptions</b> I...|$|R
50|$|Spectasia has an {{automatic}} repair and recovery system which copes with any <b>data</b> <b>corruptions</b> and takes thesystem {{back to the}} last valid state.|$|R
25|$|The worst type {{of errors}} are silent <b>data</b> <b>corruptions</b> which are errors undetected by the disk {{firmware}} or the host operating system; {{some of these}} errors {{may be caused by}} hard disk drive malfunctions.|$|R
25|$|Despite the Crash {{which caused}} much <b>data</b> <b>corruption,</b> {{technology}} in the game is advanced. Cyberware (technical implants) and Bioware (genetically engineered implants which enhance a person's abilities) emerged. Characters can also augment their bodies with nanotechnology implants.|$|E
25|$|Enterprise HDDs {{can have}} sector sizes larger than 512 bytes (often 520, 524, 528 or 536 bytes). The {{additional}} per-sector space {{can be used}} by hardware RAID controllers or applications for storing Data Integrity Field (DIF) or Data Integrity Extensions (DIX) data, resulting in higher reliability and prevention of silent <b>data</b> <b>corruption.</b>|$|E
25|$|A 2012 {{research}} {{showed that}} neither {{any of the}} then-major and widespread filesystems (such as UFS, Ext, XFS, JFS, or NTFS) nor hardware RAID (which has some issues with data integrity) provided sufficient protection against <b>data</b> <b>corruption</b> problems. Initial research indicates that ZFS protects data better than earlier efforts. It is also faster than UFS and {{can be seen as}} its replacement.|$|E
5000|$|... use a {{client-server}} architecture {{to protect the}} <b>data</b> from <b>corruption</b> by rogue application code; ...|$|R
40|$|Conventional {{sampling}} techniques {{fall short}} of drawing descriptive sketches of the data when the data is grossly corrupted as such corruptions break the low rank structure required for them to perform satisfactorily. In this paper, we present new sampling algorithms which can locate the informative columns in presence of severe <b>data</b> <b>corruptions.</b> In addition, we develop new scalable randomized designs of the proposed algorithms. The proposed approach is simultaneously robust to sparse corruption and outliers and substantially outperforms the state-of-the-art robust sampling algorithms as demonstrated by experiments conducted using both real and synthetic data...|$|R
40|$|We expect {{computers}} to func-tion correctly, despite potential problems like design bugs and physical faults. The con-sequences of incorrect functionality include silent <b>data</b> <b>corruptions</b> and crashes. The goal of dependable computing {{is to reduce}} the probabilities of these events at the lowest cost in terms of performance loss, hardware, power consumption, and design and verifica-tion time. Here, we focus on detecting incor-rect behavior caused by design bugs or physical faults before it leads to a data cor-ruption or crash. To detect incorrect behavior, we must first precisely specify what constitutes correc...|$|R
25|$|If {{the disks}} are {{connected}} to a RAID controller, it is most efficient to configure it as a HBA in JBOD mode (i.e. turn off RAID function). If a hardware RAID card is used, ZFS always detects all <b>data</b> <b>corruption</b> but cannot always repair <b>data</b> <b>corruption</b> because the hardware RAID card will interfere. Therefore, the recommendation is to not use a hardware RAID card, or to flash a hardware RAID card into JBOD/IT mode. For ZFS {{to be able to}} guarantee data integrity, it needs to either have access to a RAID set (so all data is copied to at least two disks), or if one single disk is used, ZFS needs to enable redundancy (copies) which duplicates the data on the same logical drive. Using ZFS copies is a good feature to use on notebooks and desktop computers, since the disks are large and it at least provides some limited redundancy with just a single drive.|$|E
25|$|Only a tiny {{fraction}} of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 1016 bits, and another SAS enterprise disk from 2013 specifies similar error rates. Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 1016 bits. An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the Data Integrity Field standard to combat <b>data</b> <b>corruption,</b> specifies similar error rates in 2005.|$|E
25|$|One {{major feature}} that distinguishes ZFS from other file systems {{is that it}} is {{designed}} with a focus on data integrity by protecting the user's data on disk against silent <b>data</b> <b>corruption</b> caused by data degradation, current spikes, bugs in disk firmware, phantom writes (the previous write did not make it to disk), misdirected reads/writes (the disk accesses the wrong block), DMA parity errors between the array and server memory or from the driver (since the checksum validates data inside the array), driver errors (data winds up in the wrong buffer inside the kernel), accidental overwrites (such as swapping to a live file system), etc.|$|E
40|$|Abstract: After {{making a}} case that more {{attention}} needs {{to be given to}} the quality of <b>data</b> on <b>corruption,</b> we analyze and test the validity of <b>data</b> on <b>corruption</b> using the full range of data sets employed in corruption research. First, we show that different classes of sources of data on indicators of corruption, a distinction based on who evaluates a country’s level of corruption, rely on different standards to assess corruption and that the difference in standards does not hold consistently across countries. Second, we show that the problem with indicators is simply imported into Transparency International’s Corruption Perception Index (CPI) and the World Bank’s Control of Corruption Index (CCI). Systematic differences among indicators are disregarded in the selection of indicators used in these indices. And the rule to aggregate indicators further induces bias that undermines the comparability of the CPI and the CCI. The implications of this assessment for the analysis and production of <b>data</b> on <b>corruption</b> are spelled out...|$|R
40|$|International audienceHardware faults {{can cause}} <b>data</b> <b>corruptions</b> during computation, {{and they are}} {{especially}} harmful if these <b>corruptions</b> happen in <b>data</b> pointers. Existing solutions, however, incur high performance overheads, which is unacceptable for compute-intensive applications. In this work, we present an efficient fault-tolerance approach against hardware faults by exploiting the new extensions to the x 86 architecture. In particular, we propose that Intel MPX can be effectively used to detect faults in data pointers, while Intel TSX can provide roll-back recovery against these corruptions. Our preliminary evaluation supports this hypothesis, and we estimate the average overhead to be roughly around 50 %...|$|R
40|$|Fault-tolerance {{has become}} an {{essential}} concern for pro-cessor designers due to increasing transient and permanent fault rates. Executing instruction streams redundantly in chip multi processors (CMP) provides high reliability since it can detect both transient and permanent faults and silent <b>data</b> <b>corruptions.</b> However, comparing {{the results of the}} instruction streams, checkpointing the entire system and re-covering from the detected errors present high performance degradation in execution time. This paper presents FaulTM-multi, transactional memory based fault detection and re-covery scheme for multi threaded applications running on transactional memory hardware in order to reduce these per-formance degradations. 1...|$|R
25|$|Before launch, {{testing of}} the {{assembled}} spacecraft uncovered a potential <b>data</b> <b>corruption</b> problem with an interface card {{that was designed to}} route MARDI image data as well as data from various other parts of the spacecraft. The potential problem could occur if the interface card were to receive a MARDI picture during a critical phase of the spacecraft's final descent, at which point data from the spacecraft's Inertial Measurement Unit could have been lost; this data was critical to controlling the descent and landing. This was judged to be an unacceptable risk, and it was decided to not use MARDI during the mission. As the flaw was discovered too late for repairs, the camera remained installed on Phoenix but it was not used to take pictures, nor was its built-in microphone used.|$|E
25|$|For ZFS, data {{integrity}} {{is achieved by}} using a Fletcher-based checksum or a SHA-256 hash throughout the file system tree. Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues {{all the way up}} the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree. In-flight <b>data</b> <b>corruption</b> or phantom reads/writes (the data written/read checksums correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.|$|E
25|$|ZFS is unusual, because {{unlike most}} other storage systems, it unifies {{both of these}} roles and acts as both the volume manager and the file system. Therefore, it has {{complete}} knowledge of both the physical disks and volumes (including their condition, status, their logical arrangement into volumes, and also of all the files stored on them). ZFS is designed to ensure (subject to suitable hardware) that data stored on disks cannot be lost due to physical error or misprocessing by the hardware or operating system, or bit rot events and <b>data</b> <b>corruption</b> which may happen over time, and its complete control of the storage system is used to ensure that every step, whether related to file management or disk management, is verified, confirmed, corrected if needed, and optimized, {{in a way that}} storage controller cards, and separate volume and file managers cannot achieve.|$|E
5000|$|As an example, ZFS creator Jeff Bonwick {{stated that}} the fast {{database}} at Greenplum, which is a database software company specializing in large-scale data warehousing and analytics, faces silent corruption every 15 minutes. [...] As another example, a real-life study performed by NetApp on more than 1.5 million HDDs over 41 months found more than 400,000 silent <b>data</b> <b>corruptions,</b> out of which more than 30,000 were not detected by the hardware RAID controller. Another study, performed by CERN over six months and involving about 97 petabytes of data, found that about 128 megabytes of data became permanently corrupted.|$|R
40|$|As the {{exascale}} era approaches, {{the increasing}} capacity of high-performance computing (HPC) systems with targeted {{power and energy}} budget goals introduces significant challenges in reliability. Silent <b>data</b> <b>corruptions</b> (SDCs) or silent errors {{are one of the}} major sources that corrupt the executionresults of HPC applications without being detected. In this work, we explore a low-memory-overhead SDC detector, by leveraging epsilon-insensitive support vector machine regression, to detect SDCs that occur in HPC applications that can be characterized by an impact error bound. The key contributions are three fold. (1) Our design takes spatialfeatures (i. e., neighbouring data values for each data point in a snapshot) into training data, such that little memory overhead (less than 1...|$|R
40|$|Data {{collection}} {{in a hostile}} environment requires dealing with malicious failures in the sensing devices and underlying transport network: such as <b>data</b> <b>corruptions</b> and message timeliness violations. Functional replication is employed to deal with failures, with voting among the replica devices to deliver a correct data to the end-user. The paper describes the protocol-level design issues for voting with low message overhead and delivery latency under various failure scenarios. The well-known 2 -phase voting protocol is employed as a building-block, with different protocol variants dynamically selected based on the network conditions and device-level fault severity. A case study of replicated web services is also presented to illustrate the usefulness of our dynamic adaptation mechanism...|$|R
25|$|Little-used {{floppy disk}} drives sitting unused {{for years in}} {{computers}} can actually damage disks, due to computer case air cooling that sucks air through the drive openings. This pulls dust into the mechanism, which coats all surfaces including the exposed read/write head. Upon loading a disk on the rare occasion the drive is needed, the dust is stirred up when the disk is inserted into the mechanism, and the dust on the head is ground into the medium while the stirred-up dust is deposited onto the disk through the open shutters, leading to nearly instant fouling of the media. The dust also gets into the drive mechanism lubricant, {{turning it into a}} sticky slime, which may jam the mechanism and prevent head movement. This causes the computer user to see read errors on every disk they try to use, and it may also lead to <b>data</b> <b>corruption</b> through writing in incorrect locations. Some users incorrectly assume that this means all their disks have gone bad, when it is in fact the drive that is the problem. The solution is for a technician to clean the drive before it is used after a long idle period.|$|E
2500|$|End-to-end checksumming, using {{a kind of}} [...] "Data Integrity Field", {{allowing}} <b>data</b> <b>corruption</b> detection (and recovery if {{you have}} redundancy in the pool). A choice of 3 hashes can be used, optimized for speed (fletcher), standardization and security (SHA256) and salted hashes (BLAKE).|$|E
2500|$|A {{critical}} bug {{has been}} identified which causes memory corruption when using the WLAN connection. This could result in system instability and <b>data</b> <b>corruption.</b> Owners of the 770 are encouraged to apply the preferably before having used the WLAN connection for the first time.|$|E
40|$|Reinikka and Svensson {{demonstrate}} that, {{with appropriate}} survey methods and interview techniques, {{it is possible}} to collect quantitative micro-level <b>data</b> on <b>corruption.</b> Public expenditure tracking surveys, service provider surveys, and enterprise surveys are highlighted with several applications. While often broader in scope, these surveys permit measurement of corruption at the level of individual agents, such as schools, health clinics, or firms. They also permit the study of mechanisms responsible for corruption, including leakage of funds and bribery, as <b>data</b> on <b>corruption</b> can be combined with other data collected in these surveys. ICT Policy and Strategies,Public Health Promotion,Health Monitoring&Evaluation,Decentralization,Corruption&Anitcorruption Law,Health Monitoring&Evaluation,Governance Indicators,National Governance,TF 054599 -PHRD-KYRGYZ REPUBLIC: WATER MANAGEMENT IMPROVEMENT PROJECT,ICT Policy and Strategies...|$|R
50|$|Streaming backup is {{a backup}} method where copies of all desired {{database}} files and the necessary log files are {{made during the}} backup process. File copies may be saved directly to tape or {{can be made to}} any other storage device. No quiescing of activity of any kind is required with streamed backups. Both the database and log files are check summed to ensure that no <b>data</b> <b>corruptions</b> exist within the data set during the backup process. Streaming backups may also be incremental backups. Incremental backups are ones in which only the log files are copied and which can be restored along with a previous full backup to bring all databases to a recent state.|$|R
40|$|Factorial Switching Linear Dynamical Systems for PhysiologicalPremature Baby Monitoring Factors Factorial Switching Kalman Filter Inference Parameter {{estimation}} Results Unknown co Why model this <b>data?</b> Artifact <b>corruption,</b> {{leading to}} false alarms Our {{aim is to}} determine the baby’s state of health despite these problem...|$|R
2500|$|ZFS is a {{combined}} file system and logical volume manager designed by Sun Microsystems. The features of ZFS include protection [...] against <b>data</b> <b>corruption,</b> support for high storage capacities, efficient data compression, {{integration of the}} concepts of filesystem and volume management, snapshots and copy-on-write clones, continuous integrity checking and automatic repair, RAID-Z and native NFSv4 ACLs.|$|E
2500|$|... ext3 lacks [...] "modern" [...] {{filesystem}} features, such as dynamic inode allocation and extents. This situation might {{sometimes be}} a disadvantage, but for recoverability, it {{is a significant}} advantage. The file system metadata is all in fixed, well-known locations, and data structures have some redundancy. In significant <b>data</b> <b>corruption,</b> ext2 or ext3 may be recoverable, while a tree-based file system may not.|$|E
2500|$|In {{addition}} to handling whole-disk failures, RAID-Z can also detect and correct silent <b>data</b> <b>corruption,</b> offering [...] "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, {{and if the}} data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the damaged data and returns good data to the requestor.|$|E
40|$|Application {{developers}} often place executable assertions [...] {{equipped with}} program-specific predicates [...] in their system, targeting programming errors. However, these detectors can detect data errors resulting from transient hardware faults in main memory as well. But while an assertion reduces silent <b>data</b> <b>corruptions</b> (SDCs) {{in the program}} state they check, they add runtime to the target program that increases the attack surface for the remaining state. This article outlines an approach to find an optimal subset of assertions that minimizes the SDC count, without the need to run fault-injection experiments for every possible assertion subset. Comment: Editor: Gilles Tredan. 12 th European Dependable Computing Conference (EDCC 2016), September 5 - 9, 2016, Gothenburg, Sweden. Fast Abstracts Proceedings [...] EDCC 201...|$|R
40|$|AbstractA {{country that}} isn’t {{able to control}} and {{eliminate}} the problem of corruption suffers important losses at {{the economic and social}} wellness level. The aim {{of this study is to}} analyze if the components of the macroeconomic environment are influenced by corruption and to show the nature of this influence. Analyzing the <b>corruption</b> <b>data</b> from <b>Corruption</b> Perceptions Index 2013 and the data for the Government budget balance, Gross national savings, inflation, Gross general debt and Country credit rating from The Global Competitiveness Report 2013 - 2014, the results are expected to reveal the existence of strong, but different connections between these variables...|$|R
50|$|While hard <b>data</b> on <b>corruption</b> is {{difficult}} to collect, corruption in Indonesia is clearly seen through public opinion, collated through surveys as well as observation of how each system runs. According to 2016 results of Corruption Perceptions Index of Transparency International, Indonesia ranks 90th place out of 176 countries.|$|R
