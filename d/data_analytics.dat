3589|2143|Public
5|$|Paxata is {{a privately}} owned {{software}} company headquartered in Redwood City, California. It develops self-service data preparation software that gets data ready for <b>data</b> <b>analytics</b> software. Paxata's software {{is intended for}} business analysts, as opposed to technical staff. It is used to combine data from different sources, then check it for data quality issues, such as duplicates and outliers. Algorithms and machine learning automate certain aspects of data preparation and users work with the software through a user-interface similar to Excel spreadsheets.|$|E
25|$|The {{application}} of continuous delivery and DevOps to <b>data</b> <b>analytics</b> has been termed DataOps. DataOps seeks to integrate data engineering, data integration, data quality, data security, and data privacy with operations. It applies principles from DevOps, Agile Development and the statistical process control, used in lean manufacturing, {{to improve the}} cycle time of extracting value from <b>data</b> <b>analytics.</b>|$|E
25|$|Performance {{measures}} {{for the social}} media strategy such as evaluation, <b>data</b> <b>analytics,</b> etc.|$|E
40|$|A system, {{method and}} computer-readable storage devices for {{providing}} a climate <b>data</b> <b>analytic</b> services {{application programming interface}} distribution package. The example system can provide various components. The system provides a climate <b>data</b> <b>analytic</b> services application programming interface library that enables software applications running on a client device to invoke the capabilities of a climate <b>data</b> <b>analytic</b> service. The system provides a command-line interface that provides a means of interacting with a climate <b>data</b> <b>analytic</b> service by issuing commands directly to the system's server interface. The system provides sample programs that call on {{the capabilities of the}} application programming interface library and can be used as templates for the construction of new client applications. The system can also provide test utilities, build utilities, service integration utilities, and documentation...|$|R
40|$|Companies analyse {{large amounts}} of {{sensitive}} data on clusters of machines, using a framework such as Apache Hadoop to handle inter-process communication, and big <b>data</b> <b>analytic</b> tools such as Apache Spark and Apache Flink to analyse the growing amounts of <b>data.</b> Big <b>data</b> <b>analytic</b> tools are mainly tested on performance and reliability. Security and authentication have not been enough considered and they lack behind. The goal {{of this research is}} to improve the authentication and security for <b>data</b> <b>analytic</b> tools. Currently, the aforementioned big <b>data</b> <b>analytic</b> tools are using Kerberos for authentication. Kerberos has difficulties in providing multi factor authentication. Attacks on Kerberos can abuse the authentication. To improve the authentication, an analysis of the authentication in Hadoop and the <b>data</b> <b>analytic</b> tools is performed. The research describes the characteristics to gain an overview of the security of Hadoop and the <b>data</b> <b>analytic</b> tools. One characteristic is that the usage of the transport layer security (TLS) for the security of data transportation. TLS usually establishes connections with certificates. Recently, certificates with a short time to live can be automatically handed out. This thesis develops new authentication mechanism using certificates for <b>data</b> <b>analytic</b> tools on clusters of machines, providing advantages over Kerberos. To evaluate the possibility to replace Kerberos, the mechanism is implemented in Spark. As a result, the new implementation provides several improvements. The certificates used for authentication are made valid with a short time to live and are thus less vulnerable to abuse. Further, the authentication mechanism solves new requirements coming from businesses, such as providing multi-factor authenticationand scalability. In this research a new authentication mechanism is developed, implemented and evaluated, giving better data protection by providing improved authentication...|$|R
5000|$|OptumInsight - Health <b>Data</b> <b>Analytic,</b> Payment Integrity, Life Science, Risk Quality & Network Solutions, Medical Billing ...|$|R
25|$|KNIME: The Konstanz Information Miner, a user {{friendly}} and comprehensive <b>data</b> <b>analytics</b> framework.|$|E
25|$|In 2013, {{the bank}} started {{building}} a new IT system called NEXEN. NEXEN uses open source technology and includes components such as an API store, <b>data</b> <b>analytics,</b> and a cloud computing environment.|$|E
25|$|Honavar {{is known}} for his {{research}} contributions in artificial intelligence, machine learning, data mining, knowledge representation, neural networks, semantic web, big <b>data</b> <b>analytics,</b> and bioinformatics and computational biology. He has published over 250 research articles, including many highly cited ones, as well as several books on these topics. His recent work has focused on scalable algorithms for constructing predictive models from large, semantically disparate distributed data, learning predictive models from linked open data, big <b>data</b> <b>analytics,</b> analysis and prediction of protein-protein, protein-RNA, and protein-DNA interfaces and interactions, social network analytics, health informatics, secrecy-preserving query answering, representing and reasoning about preferences, and causal inference and meta analysis.|$|E
5000|$|... 2014: Invented Reveel, a <b>data</b> <b>analytic</b> {{tool that}} {{measures}} the performance subscription-based services and predicts growth ...|$|R
3000|$|Data processing, e.g. <b>data</b> <b>analytic</b> systems, cloud {{computing}} models, middleware architectures, including software tools and database systems [...]...|$|R
50|$|On Dec 22, 2011 Lavastorm was de-merged from Martin Dawes Systems and re-launched as a <b>data</b> <b>analytic</b> company.|$|R
25|$|The Cloud Innovation Centre {{is a joint}} joint {{initiative}} between Newcastle City Council and Newcastle University’s Digital Institute. It is an independent resource that equips regional businesses with the skills and tools necessary to exploit the benefits of cloud technologies and big <b>data</b> <b>analytics.</b>|$|E
25|$|Acadia's {{research}} programs explore coastal environments, ethno-cultural diversity, social justice, environmental monitoring and climate change, organizational relationships, data mining, {{the impact of}} digital technologies, and lifestyle choices contributing to health and wellness. Acadia's research centres include the Tidal Energy Institute, the Acadia Institute for <b>Data</b> <b>Analytics,</b> and the Beaubassin Field Station. Applied research opportunities include research with local wineries and grape growers, alternative insect control techniques and technologies.|$|E
25|$|In the {{beginning}} of 2016, the BI market noted record profits of approximately $9 billion, as modern suites respond to greater productivity demands than plain <b>data</b> <b>analytics.</b> The apps of today are expected to solve marketing problems, carry out detailed business health diagnoses, {{and most of all}} to operate in all business environments and corporate ecosystems. Another recognizable feature is customization, which allows companies to make every BI system work in accordance with their operational rules.|$|E
30|$|Development of {{advanced}} <b>data</b> <b>analytic</b> models for event-driven data and {{machine learning algorithms}} can be incorporated in the existing methodologies.|$|R
5000|$|Jeff Leek in {{his book}} The Elements of <b>Data</b> <b>Analytic</b> Style {{summarizes}} the characteristics of tidy data as the points: ...|$|R
5000|$|NATIONAL CONFERENCE ON BIG <b>DATA</b> & <b>ANALYTIC</b> (NCBDA): It is a National Conference of Computer Science on Big <b>data</b> and <b>Analytic</b> ...|$|R
25|$|The Stamford Campus of the University of Connecticut offers {{complete}} {{undergraduate degree}} programs in thirteen majors {{as well as}} the Bachelor of General Studies Degree Program. Majors are American Studies, Business Administration (BSBA), Business <b>Data</b> <b>Analytics</b> (BSBDA), Financial Management (BSFM), Digital Media and Design (BA) and (BFA), Economics, English, Human Development and Family Studies, History, Political Science, Psychology, and a Certificate Entry into Nursing (CEIN/BS), an accelerated pre-licensure program. At the graduate level, Masters of Business Administration (MBA) and an MS in Financial Risk Management are offered.|$|E
25|$|The season premiered on March 15, 2017, in New York City, {{with the}} full season of 13 episodes {{released}} on Netflix on March 17. It received generally negative reviews from critics, particularly for its pace and storytelling, underwhelming fight sequences, and Jones' performance. However, Henwick's performance {{and the use of}} established characters Claire Temple and Jeri Hogarth was met with some praise. Third-party <b>data</b> <b>analytics</b> determined the series had strong initial viewership, but this quickly dropped. A second season was ordered in July 2017.|$|E
25|$|From 2006 onwards, the {{positive}} effects of cloud-stored information and data management transformed itself to a completely mobile-affectioned one, mostly to the benefit of decentralized and remote teams looking to tweak data or gain full visibility over it out of office. As a response to the large success of fully optimized uni-browser versions, vendors have recently begun releasing mobile-specific product applications for both Android and iOS users. Cloud-hosted <b>data</b> <b>analytics</b> made it possible for companies to categorize and process large volumes of data, which is how we can currently speak of unlimited visualization, and intelligent decision making.|$|E
50|$|Industry {{structure}} {{has to be}} reshaped, redefining its boundaries, and new skills are required, e.g. software development, systems engineering, <b>data</b> <b>analytic,</b> online security enterprise.|$|R
40|$|The Web {{observatory}} {{is proposed}} {{as a global}} catalogue for sharing data-sets and analytic applications to support researchers {{from a variety of}} disciplines for analysing huge amount of research data for Web Science research. However, often these users fail to understand various transformations and consequences of complex data processing involved in a <b>data</b> <b>analytic</b> application. Therefore, {{there is a need to}} enable these users develop and re-use analytic applications on web observatory. In this study, we propose formal design patterns called "Observlets" for analytic applications to "observe" various web phenomena. The observlets provide abstract definitions for intermediate analysis required for a <b>data</b> <b>analytic</b> application. The users can share observlets across distributed web observatory nodes. The observlets are aimed to enhance end-users' awareness and engagement on web observatory and support programmers for innovating various <b>data</b> <b>analytic</b> application...|$|R
40|$|Abstract—The {{popularity}} and commercial use of cloud com-puting has prompted an increased concern among cloud service providers for energy efficiency while still maintaining quality of service. One {{of the key}} techniques used for the efficient use of cloud server resources is virtual machine placement. This work introduces a precise VM placement algorithm that ensures energy efficiency and also prevents Service Level Agreement (SLA) violation. The mathematical model of the algorithm {{is supported by a}} sophisticated <b>data</b> <b>analytic</b> system implemented as a service. The precision of the algorithm is achieved by allowing each individual VM to build its own data model on demand over an appropriate time horizon. Thus the data model can reflect accurately the characteristics of resource usage of the VM. The algorithm can communicate synchronously or asynchronously with the <b>data</b> <b>analytic</b> service which is deployed as a cloud-based solution. In the experiments, several advanced data modelling and use forecasting techniques were evaluated. Results from simulation-based experiments show that the VM placement algorithm (supported by the <b>data</b> <b>analytic</b> service) can effectively reduce power consumption, the number of VM migrations, and prevent SLA violation; it also compares very favourably with other placement algorithms. Keywords-cloud computing; VM placement; energy effi-ciency; <b>data</b> <b>analytic</b> services; I...|$|R
2500|$|Svetha Venkatesh, Director of the Centre for Pattern Recognition and <b>Data</b> <b>Analytics</b> ...|$|E
2500|$|The Master of Science in Business Analytics {{program is}} ranked as a [...] "Big <b>Data</b> <b>Analytics</b> Master's Degrees: 20 Top Programs" [...] by InformationWeek.|$|E
2500|$|Cognitive {{automation}} {{relies on}} multiple disciplines: natural language processing, real-time computing, machine learning algorithms, big <b>data</b> <b>analytics</b> and evidence-based learning. According to Deloitte, cognitive automation enables the replication of human tasks and judgment “at rapid speeds and considerable scale.” ...|$|E
40|$|Very {{large amounts}} of data are being {{generated}} due to digital technologies and online activities. Traditional database solutions, which were used as intelligent tools to handle small data, are ineffective in handling the present day big data which has the characteristics of high volume, velocity and variety. Big <b>data</b> <b>analytic</b> tools are employed to get useful insights out of big data. There are some open source big <b>data</b> <b>analytic</b> tools {{which can be used}} by librarians for various operations and studies in library environment. Some possible areas include user studies, research, bibliometric studies, searching etc...|$|R
5000|$|Statistical theory {{provides}} {{the basis for}} a number of <b>data</b> <b>analytic</b> methods that are common across scientific and social research. Some of these are: Interpreting data is an important objective of statistical research: ...|$|R
40|$|Abstract Background Efficient {{analysis}} of results from mass spectrometry-based proteomics experiments requires access to disparate data types, including native mass spectrometry files, output from algorithms that assign peptide sequence to MS/MS spectra, and annotation for proteins and pathways from various database sources. Moreover, proteomics technologies and experimental methods are not yet standardized; hence {{a high degree of}} flexibility is necessary for efficient support of high- and low-throughput <b>data</b> <b>analytic</b> tasks. Development of a desktop environment that is sufficiently robust for deployment in <b>data</b> <b>analytic</b> pipelines, and simultaneously supports customization for programmers and non-programmers alike, {{has proven to be a}} significant challenge. Results We describe multiplierz, a flexible and open-source desktop environment for comprehensive proteomics data analysis. We use this framework to expose a prototype version of our recently proposed common API (mzAPI) designed for direct access to proprietary mass spectrometry files. In addition to routine <b>data</b> <b>analytic</b> tasks, multiplierz supports generation of information rich, portable spreadsheet-based reports. Moreover, multiplierz is designed around a "zero infrastructure" philosophy, meaning that it can be deployed by end users with little or no system administration support. Finally, access to multiplierz functionality is provided via high-level Python scripts, resulting in a fully extensible <b>data</b> <b>analytic</b> environment for rapid development of custom algorithms and deployment of high-throughput data pipelines. Conclusion Collectively, mzAPI and multiplierz facilitate a wide range of data analysis tasks, spanning technology development to biological annotation, for mass spectrometry-based proteomics research. </p...|$|R
2500|$|... eBay uses {{a system}} that allows {{different}} departments in the company to check out data from their data mart into sandboxes for analysis. According to Goul, eBay has already experienced significant business successes through its <b>data</b> <b>analytics.</b> eBay employs 5,000 data analysts to enable data-driven decision making.|$|E
2500|$|Cartwright is an advisor {{for several}} {{corporate}} entities involved in global management consulting, technology services and program solutions. predictive and big <b>data</b> <b>analytics.</b> and advanced systems engineering, integration, and decision-support services. [...] He {{serves as an}} advisor {{to the board of}} directors for Accenture, Enlightenment Capital, IxReveal, Logos Technologies, Opera Solutions, and TASC Inc. He is also affiliated with a number of professional organizations, including the Aspen Strategy Group, The Atlantic Council, Nuclear Threat Initiative, and The Sanya Initiative.|$|E
2500|$|Across {{the globe}} loyalty {{programs}} are increasingly finding {{the need to}} outsource strategic and operational aspects of their programs, given the size and complexity a loyalty program entails. Program managers are typically agencies with specialist skilled in loyalty consulting, creativity, communication, <b>data</b> <b>analytics,</b> loyalty software, and back end operations. [...] The advent of Web 2.0 and SaaS online-based services has provided lower-cost options for small businesses to offer and manage their own loyalty programs. Moreover, many free and open source alternatives are available to manage user awards and incentives.|$|E
30|$|In {{the study}} of <b>data</b> <b>analytic,</b> {{ensemble}} learning is a methodology to compensate the results of each single classifier by utilizing diversity. It can reduce aggregated variance and tend to increase accuracy over the individuals [33].|$|R
40|$|BACKGROUND. Efficient {{analysis}} of results from mass spectrometry-based proteomics experiments requires access to disparate data types, including native mass spectrometry files, output from algorithms that assign peptide sequence to MS/MS spectra, and annotation for proteins and pathways from various database sources. Moreover, proteomics technologies and experimental methods are not yet standardized; hence {{a high degree of}} flexibility is necessary for efficient support of high- and low-throughput <b>data</b> <b>analytic</b> tasks. Development of a desktop environment that is sufficiently robust for deployment in <b>data</b> <b>analytic</b> pipelines, and simultaneously supports customization for programmers and non-programmers alike, {{has proven to be a}} significant challenge. RESULTS. We describe multiplierz, a flexible and open-source desktop environment for comprehensive proteomics data analysis. We use this framework to expose a prototype version of our recently proposed common API (mzAPI) designed for direct access to proprietary mass spectrometry files. In addition to routine <b>data</b> <b>analytic</b> tasks, multiplierz supports generation of information rich, portable spreadsheet-based reports. Moreover, multiplierz is designed around a "zero infrastructure" philosophy, meaning that it can be deployed by end users with little or no system administration support. Finally, access to multiplierz functionality is provided via high-level Python scripts, resulting in a fully extensible <b>data</b> <b>analytic</b> environment for rapid development of custom algorithms and deployment of high-throughput data pipelines. CONCLUSION. Collectively, mzAPI and multiplierz facilitate a wide range of data analysis tasks, spanning technology development to biological annotation, for mass spectrometry-based proteomics research. Dana-Farber Cancer Institute; National Human Genome Research Institute (P 50 HG 004233); National Science Foundation Integrative Graduate Education and Research Traineeship grant (DGE- 0654108...|$|R
5000|$|Besides the {{philosophy}} underlying statistical inference, statistical theory has {{the task of}} considering the types of questions that data analysts might want {{to ask about the}} problems they are studying and of providing <b>data</b> <b>analytic</b> techniques for answering them. Some of these tasks are: ...|$|R
