33|4|Public
5000|$|In the {{imaginary}} {{language in which}} the last example is written, the compiler would assume that, {{in the absence of}} information to the contrary, [...] takes two integers and returns one integer. (This is how it works in, for example, OCaml.) From this, the type <b>inferencer</b> can infer that the type of [...] is an integer, which means [...] is an integer and thus the return value of [...] is an integer. Similarly, since [...] requires both of its arguments be of the same type, [...] must be an integer, and thus, [...] accepts one integer as an argument.|$|E
40|$|Abstract. Helium is a {{compiler}} {{for a large}} {{subset of}} Haskell under development at Universiteit Utrecht. A major design criterion {{is the ability to}} give superb error messages. This is especially needful for novice functional programmers. In this paper we document the implementation of the Helium type <b>inferencer.</b> For purposes of experimentation with various methods of type inferencing, the type <b>inferencer</b> can be parameterized in a number of ways. Among the instances we find not only standard algorithms such as M and W, but also more global type inferencers based on type graphs. ...|$|E
40|$|A bad {{software}} development process leads to wasted effort and inferior products. In {{order to improve}} a software process, {{it is important to}} first understand it. Our unique approach in this paper is to use code and test changes to understand conformance to a process. We analyze the meaning of these changes to obtain a deep, rich understanding about the process. In this paper we use Test Driven Development (TDD) as a case study to validate our approach. We designed a visualization to enable developers to better understand their TDD software process. We analyze our visualization by using the Cognitive Dimensions framework to discuss some findings and design adjustments. To enable this visualization, we developed a novel automatic <b>inferencer</b> that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate our TDD <b>inferencer</b> by performing an empirical evaluation on a corpus of 2601 TDD sessions. Our <b>inferencer</b> achieves an accuracy of 87 %...|$|E
40|$|Knowledge-based {{techniques}} provide tremendous leverage {{for managing}} and using repositories of reusable software. The Unisys Reusability Library Framework demonstrates this by providing an intelligent Librarian application that operates on {{a model of}} a software domain. The Librarian uses structured inheritance networks to model the library domain and organize the repository contents. The resulting structure of the domain model allows convenient repository browsing and component retrieval and provides a foundation for a variety of powerful tools. Rule-based <b>inferencers</b> complement this structure by making heuristic knowledge and advice available to the user. This makes the domain knowledge and repository organization more accessible to the user. The RLF uses a hybridization of these two knowledge representation paradigms, taking advantage of the strengths of each to provide intelligent assistance in the use and management of a software repository. Raymond McDowell {{is a member of the}} resea [...] ...|$|R
40|$|We expound {{a view of}} type {{checking}} as evaluation with ‘abstract values’. Whereas dynamic semantics, evaluation, {{deals with}} (dynamic) values like 0, 1, etc., static semantics, type checking, deals with approximations like int. A type system is sound if it correctly approximates the dynamic behavior and predicts its outcome: if the static semantics predicts that a term has the type int, the dynamic evaluation of the term, if it terminates, will yield an integer. As object language, we use simply-typed and let-polymorphic lambda calculi with integers and integer operations as constants. We use Haskell as a metalanguage in which to write evaluators, type checkers, type reconstructors and <b>inferencers</b> for the object language. We explore the deep relation between parametric polymorphism and ‘inlining’. Polymorphic type checking then is an optimization allowing us to type check a polymorphic term at the place of its definition {{rather than at the}} places of its use. 1 Introduction: Untype...|$|R
40|$|We {{show how}} {{programming}} language semantics and definitions of their corresponding type systems can both {{be written in}} a single framework amenable to proofs of soundness. The framework is based on full rewriting logic (not {{to be confused with}} context reduction or term rewriting), where rules can match anywhere in a term (or configuration). We present an extension of the syntactic approach to proving type system soundness presented by Wright and Felleisen [1994] that works in the above described semantics-based domain. As before, the properties of preservation and progress are crucial. We use an abstraction function to relate semantic configurations in the language domain to semantic configurations in the type domain, and then proceed to use the preservation and progress properties as usual. We also develop an abstract type system, which is a type system modulo certain structural characteristics. To demonstrate the method, we give examples of five languages and corresponding type systems. They include two imperative languages and three functional languages, and three type checkers and two type <b>inferencers.</b> We then proceed to prove that preservation holds for each. unpublishednot peer reviewe...|$|R
40|$|Abstract. Meseguer and Ros,u {{proposed}} rewriting logic semantics (RLS) as a programing language definitional {{framework that}} unifies operational and algebraic denotational semantics. RLS {{has already been}} used to define a series of didactic and real languages, but its benefits in connection with defining and reasoning about type systems have not been fully investigated. This paper shows how the same RLS style employed for giving formal definitions of languages {{can be used to}} define type systems. The same termrewriting mechanism used to execute RLS language definitions can now be used to execute type systems, giving type checkers or type inferencers. The proposed approach is exemplified by defining the Hindley-Milner polymorphic type <b>inferencer</b> W as a rewrite logic theory and using this definition to obtain a type <b>inferencer</b> by executing it in a rewriting logic engine. The <b>inferencer</b> obtained this way compares favorably with other definitions or implementations of W. The performance of the executable definition is within an order of magnitude of that of highly optimized implementations of type inferencers, such as that of OCaml. ...|$|E
40|$|We have {{implemented}} a non-null type checker for Java {{and a new}} non-null inferencing algorithm for analyzing legacy code. The tools are modular extensions to the JastAdd extensible Java compiler, illustrating how pluggable type systems can be achieved. The resulting implementation is compact, less than 230 lines of code for the non-null checker and 460 for the <b>inferencer.</b> Non-null checking is a local analysis with little effect on compilation time. The inferencing algorithm is a whole-program analysis, yet it runs sufficiently fast for practical use, less than 10 seconds for 100. 000 lines of code. We ran our <b>inferencer</b> on {{a large part of}} the JDK library, and could detect that around 70 % of the dereferences, and around 24 % of the method return values, were guaranteed to be non-null. ...|$|E
30|$|We next {{generated}} five imputed datasets (Figure 12 in the Appendix) {{running the}} analyses separately on each dataset, and combining, by using Rubin’s (1987) rules, the parameter estimates and standard errors {{into a single}} <b>inference.r</b> The resulting estimates accounted for both within- and between-imputation uncertainty, reflecting {{the fact that the}} imputed values were not observed values.|$|E
40|$|Abstract: 2 ̆ 2 Quite a few {{interesting}} {{experiments have}} been done applying neural networks to natural language tasks. Without detracting from {{the value of these}} early investigations, this paper argues that current neural network architectures are too weak to solve anything but toy language problems. Their downfall is the need for 2 ̆ 7 dynamic inference, 2 ̆ 7 in which several pieces of information not previously seen together are dynamically combined to derive the meaning of a novel input. The first half of the paper defines a hierarchy of classes of connectionist models, from categorizers and associative memories to pattern transformers and dynamic <b>inferencers.</b> Some well-known connectionist models that deal with natural language are shown to be either categorizers or pattern transformers. The second half examines in detail a particular natural language problem: prepositional phrase attachment. Attaching a PP to an NP changes its meaning, thereby influencing other attachments. So PP attachment requires compositional semantics, and compositionality in non-toy domains requires dynamic inference. Mere pattern transformers cannot learn the PP attachment task without an exponential training set. Connectionist-style computation still has many valuable ideas to offer, so this is not an indictment of connectionism 2 ̆ 7 s potential. It is an argument for a more sophisticated and more symbolic connectionist approach to language. 2 ̆...|$|R
40|$|We {{present the}} sound CCured type system for C {{programs}} that classifies pointers {{according to their}} usage and instructs a source-to-source translator to extend the program with run-time checks in order to guarantee memory safety. CCured {{can be used on}} existing C programs thanks to a simple pointer-kind <b>inferencer</b> which on many programs discovers that over 80 % of the pointers are type-safe...|$|E
40|$|Semantic {{repositories}} – RDF databases with <b>inferencer</b> and query answering engine – are set {{to become}} a cornerstone of the Semantic Web (and Linked Open Data) due {{to their ability to}} store and reason with the massive quantities of data involved. In this paper, we describe the features of BigOWLIM that have allowed it to penetrate into the commercial sector, focusing on one particular use-case, that being its use in the BBC’s World Cup website...|$|E
40|$|We {{present a}} non-null {{annotations}} <b>inferencer</b> for the Java bytecode language. We previously proposed an analysis to infer non-null annotations and proved it soundness and completeness {{with respect to}} a state of the art type system. This paper proposes extensions to our former analysis {{in order to deal with}} the Java bytecode language. We have implemented both analyses and compared their behaviour on several benchmarks. The results show a substantial improvement in the precision and, despite being a whole-program analysis, production applications can be analyzed within minutes...|$|E
40|$|We present αKanren, an {{embedding}} of nominal {{logic programming}} in Scheme. αKanren {{is inspired by}} αProlog and MLSOS, and allows programmers to easily write interpreters, type inferencers, and other programs that must reason about scope and binding. αKanren subsumes the functionality, syntax, and implementation of miniKanren, itself an embedding of logic programming in Scheme. We present the complete implementation of αKanren, written in portable R 5 RS Scheme. In addition to the implementation, we provide introductions to miniKanren and αKanren, and several example programs, including a type <b>inferencer</b> for the simply typed λ-calculus...|$|E
40|$|International audienceWe {{present a}} non-null {{annotations}} <b>inferencer</b> for the Java bytecode language. We previously proposed an analysis to infer non-null annotations and proved it soundness and completeness {{with respect to}} a state of the art type system. This paper proposes extensions to our former analysis {{in order to deal with}} the Java bytecode language. We have implemented both analyses and compared their behaviour on several benchmarks. The results show a substantial improvement in the precision and, despite being a whole-program analysis, production applications can be analyzed within minutes...|$|E
40|$|Although type {{systems do}} detect type errors in programs, they often produce uninformative error messages, that hardly give {{information}} about how to repair a program. One important shortcoming is the inability to highlight the most likely cause for the detected inconsistency. This paper presents a type <b>inferencer</b> with improved error reporting facilities, based {{on the construction of}} type constraints. Unication of types is replaced by the construction of constraint graphs. This approach increases the chance to report the actual source of a type conict, resulting in more useful error messages. ...|$|E
40|$|Interaction between {{automated}} {{planners and}} intent inferencers {{is a difficult}} problem that is not always well understood. This problem can be framed {{in terms of a}} number of key issues: shared ontology, human interaction, and display of proposals. Shared ontology is a key issue from both practical and theoretical perspectives. Without sharing, intent inference outputs must be interpreted by the planner into its own representations, and, of course, the intent <b>inferencer</b> must do the same with the planner outputs. Furthermore, absent a commitment to a shared ontology, it is unclear practically whether there is any commitment between the design teams to interact in any way whatsoever that would support true collaborative interaction between the human operator and th...|$|E
40|$|International audienceIn {{experimental}} research into percussion ‘languages', an interactive computer system, the Bol Processor, {{has been developed}} by the authors to analyse the performances of expert musicians and generate its own musical items that were assessed for quality and accuracy by the informants. The problem of transferring knowledge from a human expert to a machine {{in this context is}} the focus of this paper. A prototypical grammatical <b>inferencer</b> named QAVAID (Question Answer Validated Analytical Inference Device, an acronym also meaning ‘grammar' in Arabic/Urdu) is described and its operation in a real experimental situation is demonstrated. The paper concludes {{on the nature of the}} knowledge acquired and the scope and limitations of a cognitive-computational approach to music...|$|E
40|$|Haskell-style {{functional}} dependencies [2] {{provide a}} relational specification of user-programmable type improvement [1] connected to type class instances. The more recent type families (also known as type functions) [3] equip the programmer with similar functionality, {{but in a}} functional form and decoupled from type classes. Functional dependencies are supported by both GHC and Hugs, while the most recent version of GHC also supports type functions. There was an enthusiastic and lively debate about which feature should be adopted by the next Haskell standard, Haskell-Prime 3 Currently, further progress in the standardization appears to be stalled on this issue. In this work, we attempt to rekindle the debate with new insights in type inference issues behind functional dependencies (FDs) and type functions (TFs), without taking sides. Specifically, we address the non-confluence issue of Constraint Handling Rules (CHRs) resulting from non-full FDs [4]. CHRs serve as a meta-language to describe the constraint solver underlying the type <b>inferencer.</b> Confluence of CHRs is important {{to ensure that the}} type <b>inferencer</b> is complete. We propose an alternative encoding which achieves confluence even for nonfull FDs. We achieve this result by encoding FDs via TFs. Technically, we could use a different CHR encoding but by using TFs to encode FDs we can also explain their commonalities and differences. We give a sketch of the alternative encoding below. The long version of this extended abstract will give an in-depth discussion of several encodings and also address issues when violating the (Weak) Coverage Condition [4]. The Non-Full FD Problem Consider the following type class program: class F a b c | a-> b instance F a b Bool => F [a] [b] Boo...|$|E
40|$|Abstract — Chuck is a {{new code}} browser that allows {{navigation}} of a code base along semantic structures, such as data-flow and higher-order control-flow relationships. Employing the fast DDP type <b>inferencer,</b> it is effective on dynamically typed code bases {{an order of magnitude}} larger than the code bases supported by previous browsers. Chuck supports the full Smalltalk language, and is now shipped as a standard component of the Squeak open-source Smalltalk system, where it routinely works with code bases exceeding 300, 000 lines of code. Chuck’s implementation is tuned for interactive use, and is transparently integrated with the Squeak system’s existing code-browsing tools. Thus, it provides semantic navigation of a live code base that is still being edited without requiring long pauses for reanalysis due to edits of the code. I...|$|E
40|$|This thesis {{addresses}} {{the problem of}} checking programs written in an object-oriented language {{to ensure that they}} satisfy the information flow policies, confidentiality and integrity. Existing work on information flow inference for object-oriented languages does not scale due to lack of modularity. In this thesis we present a constraint-based inference system for secure information flow for a Java-like language with method inheritance and override. The inference is inter-procedural and modular: incremental inference can be performed on individual program components, and result used as libraries for other components. The algorithm uses access control information to improve the precision of the inference. The soundness of the inference algorithm is formally proved. A compiler with a built-in <b>inferencer,</b> SecJ, is implemented to perform the security check on a subset of Java...|$|E
40|$|A lexicon is {{the heart}} of any {{language}} processing system. Accurate words with grammatical and semantic attributes are essential or highly desirable for any application - be it machine translation, information extraction, various forms of tagging or text mining. However, good quality lexicons are difficult to construct requiring enormous amount of time and manpower. In this paper, we present a method for automatically generating the dictionary from an input document - making use of the WordNet. The dictionary entries are in the form of Universal Words (UWs) which are language words (primarily English) concatenated with disambiguation information. The entries are associated with syntactic and semantic properties - most of which too are generated automatically. In addition to the WordNet, the system uses a word sense disambiguator, an <b>inferencer</b> and the knowledge base (KB) of the Universal Networking Language which is a recently proposed interlingua. The lexicon so constructed is sufficiently accurate and reduces the manual labour substantially...|$|E
40|$|Abstract ⎯ A lexicon is {{the heart}} of any {{language}} processing system. Accurate words with grammatical and semantic attributes are essential or highly desirable for any application- be it machine translation, information extraction, various forms of tagging or text mining. However, good quality lexicons are difficult to construct requiring enormous amount of time and manpower. In this paper, we present a method for automatically generating multilingual Universal Word (UW) dictionaries (for English, Hindi and Marathi) from an input document- making use of English, Hindi and Marathi WordNets. The dictionary entries are in the form of Universal Words (UWs) which are language words (primarily English) concatenated with disambiguation information. The entries are associated with syntactic and semantic properties- most of which too are generated automatically. In addition to the WordNet, the system uses a word sense disambiguator, an <b>inferencer</b> and the knowledge base (KB) of the Universal Networking Language which is a recently proposed interlingua. The lexicon so constructed is sufficiently accurate and reduces the manual labor substantially...|$|E
40|$|We {{implement}} the classical Damas-Milner type inference algorithm in a calculus with higher-order abstractions and 1 st-order constraints. We encode mono(morphic) types as constraints, and poly(morphic) types as abstractions over monotypes. Our calculus ae deep is intended as computation {{model of a}} concrete programming language, i. e., to be what the-calculus is for ML, or the fl-calculus [8] for Oz [2]. Unlike usual in programming language research, ae deep allows reduction below abstraction. This allows to simplify our encoding of polytypes before usage. In the envisaged programming language, the type <b>inferencer</b> can be run just as specified. We claim that deep reduction {{is one of the}} essential features for such a high-level implementation of type inference or abstract interpretation algorithms [4]. The algorithm can also be viewed as an extension of Wand's [9] towards let-polymorphism. Like Wand's, our algorithm is easily modified to supply other monomorphic type systems with ML style polymorphism...|$|E
40|$|We {{describe}} a new method for polymorphic type inference for the dynamically typed language Scheme. The method infers both types and explicit run-time type operations (coercions) {{for a given}} program. It {{can be used to}} statically debug Scheme programs and to give a high-level translation to ML, in essence providing an "embedding" of Scheme into ML. Our method combines the following desirable properties: ffl It is liberal in that no legal Scheme programs are "rejected " outright by the type <b>inferencer.</b> ffl It is modular in that definitions can be analyzed independent of the context of their use. The inferred type scheme for a definition is safe for all contexts. This is accomplished by admitting type coercion parameters, resulting in a form of polymorphic qualified type system. ffl It infers both types and run-time type operations and places the latter opportunistically at any suitable program point, not only at data creation and destruction points. ffl It is very efficient. Its mo [...] ...|$|E
40|$|Abstract. Information {{about the}} nondeterminism {{behavior}} of a functional logic program is important for various reasons. For instance, a nondeterministic choice in I/O operations results in a run-time error. Thus, it is desirable to ensure at compile time that a given program {{is not going to}} crash in this way. Furthermore, knowledge about nondeterminism can be exploited to optimize programs. In particular, if functional logic programs are compiled to target languages without builtin support for nondeterministic computations, the transformation can be much simpler if it is known that the source program is deterministic. In this paper we present a nondeterminism analysis of functional logic programs in form of a type/effect system. We present a type <b>inferencer</b> to approximate the nondeterminism behavior via nonstandard types and show its correctness w. r. t. the operational semantics of functional logic programs. The type inference is based on a new compact representation of sets of types and effects. ...|$|E
40|$|Abstract We {{observe that}} closed type {{annotations}} impose some serious restrictions on programs {{making use of}} typingfeatures such as polymorphic recursion, type classes and guarded recursive data types. The type <b>inferencer</b> often cannot be supplied with the necessary amount of information to type check many reasonable programs. To address this, we introduce a novel form of lexically scoped annotation {{as an extension of}} Hindley/Milner. We further show that type inference based on algorithm W or its variants is not well-suited to deal withtype annotations in general. Hence, we propose a novel inference scheme which improves over all previous formulations we are aware of. Our approach has been fully implemented as part of the Chameleon system (anexperimental version of Haskell). 1. Introduction Type annotations are a common feature found in many programming languages. The assumption made inHaskell 98 [10] is that type annotations are closed, i. e. all variables appearing in such annotations are implicitl...|$|E
40|$|For decades programmers {{have had}} access to production-quality tools for {{generating}} lexers and parsers from high-level declarative specifications. Recent work on nano-pass compiler frameworks [3] {{makes it possible to}} develop a many-pass compiler using a grammar-based domain-specific language. Yet we still do not have mature, flexible tools for generating efficient type checkers and type inferencers from a high-level description (ideally, directly from the typing judgements). Although researchers use logic programming, term writing, and proof assis-tants to express type systems in a declarative fashion, in practice developers— including computer science researchers—write production type inferencers and type checkers by hand using general-purpose programming languages. This point was driven home for one of us during development of the compiler for Harlan [2] (a language for high-level GPGPU programming). After defining the type system for Harlan’s region-based memory management system, we wanted to generate a type <b>inferencer</b> from the typing judgements. We knew of no too...|$|E
40|$|Starkiller {{is a type}} <b>inferencer</b> and {{compiler}} for {{the dynamic}} language Python designed to generate fast native code. It analyzes Python source programs and converts them into equivalent C++ programs. Starkiller’s type inference algorithm {{is based on the}} Cartesian Product Algorithm but has been significantly modified to support a radically different language. It includes an External Type Description Language that enables extension authors to document how their foreign code extensions interact with Python. This enables Starkiller to analyze Python code that interacts with foreign code written in C, C++, or Fortran. The type inference algorithm also handles data polymorphism in addition to parametric polymorphism, thus improving precision. Starkiller supports the entire Python language except for dynamic code insertion features such as eval and dynamic module loading. While the system is not yet complete, early numeric benchmarks show that Starkiller compiled code performs almost as well as hand made C code and substantially better than alternative Pytho...|$|E
40|$|Pure Python code is slow, {{primarily}} due to {{the dynamic}} nature of the language. I have begun building a compiler to produce fast native code from Python source programs. While compilation can improve performance, any such gains will be modest unless the compiler can statically resolve most of the dynamism present in source programs. Static type inference for Python programs would enable the safe removal of most type checks and most instances of dynamic dispatch and dynamic binding from the generated code. Removing dynamic dispatch and binding leads to large performance benefits since their existence precludes many traditional optimization techniques, such as inlining. I have built a static type <b>inferencer</b> for Python called Starkiller. Given a Python source file, it can deduce the types of all expressions in the program without actually running it. Starkiller’s primary goal {{is to take a}} Python source program as input and deduce the information needed to make native code compilation of that program easy. This paper is describes Starkiller’s design and operation. It is partiall...|$|E
40|$|This paper {{introduces}} {{a new type}} of automated testing oracle, called the execution equivalence (EE) invariants. These invariants can be mined from application logs that capture both application events and application states. The EE-invariants express an equivalence relation on the sequences of application events in terms of equality of respective initial and final states, which these sequences leave in the logs during the run-time. We claim that even equivalences up to a length of four events already provide useful testing oracle. We extended our tool LOPI (LOg-based Pattern <b>Inferencer)</b> with the algorithm for mining EE-invariants, and evaluated the effectiveness of these invariants on a case-study [...] - the web application Flex Store. The evaluation is carried out based on two parameters: the false positive rate and the fault finding capability. Moreover, we compared the strength of LOPI's execution equivalences with Daikon's data invariants. This comparison has shown that Daikon was slightly more effective than LOPI in testing Flex Store. However, we have found a suitable confidence level for LOPI which allows to outperform Daikon...|$|E
40|$|The {{order in}} which unfications take place during type {{inference}} algorithms strongly determines where an error is detected. We use a constraint based type <b>inferencer</b> which separates between collecting, ordering, and solving constraints. We use a small set of combinators {{as part of the}} type rules to specify a degree of non-determinism in the {{order in which}} constraints may be solved. In the ordering phase, the user can make the type inference process (more) deterministic by choosing an appropriate ordering, yielding a large degree of control on his part: the scientist can easily compare the behaviors of various existing type inferencers and algorithms by describing them in terms of these orderings; the programmer can experiment with various orderings, and choose the one which suits his style of programming best. Compilers based on this technology naturally support multiple solvers, multiple error message for a single compilation, and using various orderings “in parallel”, so that the user can easily browse through different views on the type error. The framework has been implemented, and used to build the Helium compiler for the language Haskell...|$|E
40|$|Meseguer and Rosu [MR 04,MR 07] {{proposed}} rewriting logic semantics (RLS) as a programing language definitional {{framework that}} unifies operational and algebraic denotational semantics. Once a language {{is defined as}} an RLS theory, many generic tools are immediately available for use {{with no additional cost}} to the designer. These include a formal inductive theorem proving environment, an efficient interpreter, a state space explorer, and even a model checker. RLS has already been used to define a series of didactic and real languages [MR 04,MR 07], but its benefits in connection with defining and reasoning about type systems have not been fully investigated yet. This paper shows how the same RLS style employed for giving formal definitions of languages can be used to define type systems. The same term-rewriting mechanism used to execute RLS language definitions can now be used to execute type systems, giving type checkers or type inferencers. Since both the language and its type system are defined uniformly as theories in the same logic, one can use the standard RLS proof theory to prove properties about languages and type systems for those languages. The proposed approach is exemplified by defining Milner’s polymorphic type <b>inferencer</b> W as a rewrite logic theory and using the definition: (1) to prove its soundness using Wright and Felleisen’...|$|E
40|$|This thesis {{relates to}} the {{application}} of Artificial Intelligence to tool wear monitoring. The main objective is to develop an intelligent condition monitoring system able to detect when a cutting tool is worn out. To accomplish this objective it is proposed to use a combined Expert System and Neural Network able to process data coming from external sensors and combine this with information from the knowledge base and thereafter estimate the wear state of the tool. The novelty of this work is mainly associatedw ith the configurationo f the proposeds ystem. W ith the combination of sensor-baseidn formation and <b>inferencer</b> ules, {{the result is an}} on-line system that can learn from experience and can update the knowledge base pertaining to information associated with different cutting conditions. Two neural networks resolve the problem of interpreting the complex sensor inputs while the Expert System, keeping track of previous successe, stimatesw hich of the two neuraln etworks is more reliable. Also, mis-classificationsa re filtered out {{through the use of a}} rough but approximate estimator, the Taylor's tool life equation. In this study an on-line tool wear monitoring system for turning processesh as been developed which can reliably estimate the tool wear under common workshop conditions. The system's modular structurem akesi t easyt o updatea s requiredb y different machinesa nd/or processesT. he use of Taylor's tool life equation, although weak as a tool life estimator, proved to be crucial in achieving higher performance levels. The application of the Self Organizing Map to tool wear monitoring is, in itself, new and proved to be slightly more reliable then the Adaptive Resonance Theory neural network...|$|E
40|$|Graduation date: 2017 A bad {{software}} development process leads to wasted effort and inferior products. In {{order to improve}} a software process, it must be first understood. In this work I focus on understanding software processes. The first process we seek to understand is Continuous Integration (CI). CI systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers {{make decisions based on}} folklore instead of data. We use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34, 544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1, 529, 291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI. Furthermore, we present a qualitative study of the barriers and needs developers face when using CI. In this paper, we conduct 16 semi-structured interviews with developers from different industries and development scales. We triangulate our findings by running two surveys. The Focused Survey samples 51 developers at a single company. The Broad Survey samples a population of 523 developers from all over the world. We identify trade-offs developers face when using and implementing CI. Developers face trade-offs between speed and certainty (Assurance), between better access and information security (Security), and between more configuration options and better ease of use (Flexibility). We present implications of these trade-offs for developers, tool builders, and researchers. Additionally, we seek to use code and test changes to understand conformance to the Test Driven Development (TDD) process. We designed and implemented TDDViz, a tool that supports developers in better understanding how they conform to TDD. TDDViz supports this understanding by providing novel visualizations of developers’ TDD process. To enable TDDViz’s visualizations, we developed a novel automatic <b>inferencer</b> that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate TDDViz using two complementary methods: a controlled experiment with 35 participants to evaluate the visualization, and a case study with 2601 TDD Sessions to evaluate the inference algorithm. The controlled experiment shows that, in comparison to existing visualizations, participants performed significantly better when using TDDViz to answer questions about code evolution. In addition, the case study shows that the inferencing algorithm in TDDViz infers TDD phases with an accuracy (F-measure) of 87...|$|E

