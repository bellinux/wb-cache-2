341|319|Public
25|$|Fibre Channel (FC) is a {{successor}} to parallel SCSI interface on enterprise market. It is a serial protocol. In disk drives usually the Fibre Channel Arbitrated Loop (FC-AL) connection topology is used. FC has much broader usage than mere disk interfaces, {{and it is the}} cornerstone of storage area networks (SANs). Recently other protocols for this field, like iSCSI and ATA over Ethernet have been developed as well. Confusingly, drives usually use copper twisted-pair cables for Fibre Channel, not fibre optics. The latter are traditionally reserved for larger devices, such as servers or <b>disk</b> <b>array</b> controllers.|$|E
50|$|For example, one of {{the checks}} in the risk {{assessment}} report lists volumes whose mirrors are composed of logical units from the same <b>disk</b> <b>array.</b> Logical volumes that are mirrored have higher availability when mirrored across separate <b>disk</b> <b>array</b> controllers. Mirroring logical volumes across separate <b>disk</b> <b>array</b> controllers allows the logical volumes to continue to operate should one <b>disk</b> <b>array</b> fail.|$|E
50|$|Typically a <b>disk</b> <b>array</b> {{provides}} increased availability, resiliency, and maintainability {{by using}} existing components (controllers, power supplies, fans, etc.), often {{up to the}} point where all single points of failure (SPOFs) are eliminated from the design. Additionally, <b>disk</b> <b>array</b> components are often hot-swappable.|$|E
50|$|Of course, {{the overall}} {{performance}} of a system is not only relevant {{to the performance of}} host and network, but also influenced by the performance of the disk constituting file system. So, BWFS file system can be structured by the LUN from multiple <b>disk</b> <b>arrays.</b> It equals to another layer of RAID structured between multiple <b>disk</b> <b>arrays,</b> which maximizes the performance of <b>disk</b> <b>arrays.</b>|$|R
40|$|In this paper, {{we examine}} how <b>disk</b> <b>arrays</b> and shared memory multiprocessors {{lead to an}} {{effective}} method for constructing database machines for general-purpose complex query processing. We show that <b>disk</b> <b>arrays</b> can lead to cost-effective storage systems if they are configured from suitably small formfactor disk drives. We introduce the storage system metric data temperature {{as a way to}} evaluate how well a disk configuration can sustain its workload, and we show that <b>disk</b> <b>arrays</b> can sustain the same data temperature as a more expensive mirrored-disk configuration. We use the metric to evaluate the performance of <b>disk</b> <b>arrays</b> in XPRS, an operational shared-memory multiprocessor database system being developed at the University of California, Berkeley...|$|R
50|$|The Hitachi Universal Storage Platform {{was first}} {{introduced}} in 2004. An entry level enterprise and high-end midrange model, the Network Storage Controller was introduced in 2005. The Universal Storage Platform {{was one of the}} first <b>disk</b> <b>arrays</b> to virtualize other <b>disk</b> <b>arrays</b> in the appliance instead of in the network.|$|R
5000|$|Clariion (styled CLARiiON) is a {{discontinued}} [...] SAN <b>disk</b> <b>array</b> manufactured {{and sold}} by EMC Corporation, it occupied the entry-level and mid-range of EMC's SAN <b>disk</b> <b>array</b> products. In 2011, EMC introduced the EMC VNX Series, designed to replace both the Clariion and Celerra products.|$|E
50|$|Protocol analyzers {{can also}} be hardware-based, either in probe format or, as is {{increasingly}} common, combined with a <b>disk</b> <b>array.</b> These devices record packets (or {{a slice of the}} packet) to a <b>disk</b> <b>array.</b> This allows historical forensic analysis of packets without users having to recreate any fault.|$|E
5000|$|... #Caption: SPARCserver 1000 and SPARC Storage Array <b>disk</b> <b>array</b> ...|$|E
40|$|<b>Disk</b> <b>arrays</b> {{have been}} {{designed}} with two competing goals in mind, the ability to reconstruct erased disks (reliability), and {{the speed with which}} information can be read, written, and reconstructed (performance). The substantial loss in performance of write operations as reliability requirements increase has resulted in an emphasis on performance at the expense of reliability. This has proved acceptable given the relatively small numbers of disks in current <b>disk</b> <b>arrays.</b> We develop a method for improving the performance of write operations in <b>disk</b> <b>arrays</b> capable of correcting any double erasure, by ordering the columns of the erasure code to minimize the amount of parity information that requires updating. For large <b>disk</b> <b>arrays,</b> this affords a method to support the reliability needed without the generally accepted loss of performance. Regular or Revue? Regular. Ordering Disks for Double Erasure Codes Myra B. Cohen and Charles J. Colbourn Computer Science, University of [...] ...|$|R
40|$|Redundant <b>disk</b> <b>arrays</b> are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) <b>disk</b> <b>arrays,</b> but none has yet been done on how to stripe data in redundant <b>disk</b> <b>arrays</b> such as RAID Level 5, or on how the choice of striping unit varies {{with the number of}} disks. Using synthetic workloads, we derive simple design rules for striping data in RAID Level 5 <b>disk</b> <b>arrays</b> given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems. We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read-modify writes or reconstruct writes. This additional factor causes [...] ...|$|R
40|$|Redundant <b>disk</b> <b>arrays</b> are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) <b>disk</b> <b>arrays,</b> but none has yet been done on how to stripe data in redundant <b>disk</b> <b>arrays</b> such as RAID Level 5. In this paper we discuss the tradeoffs involved in striping data in RAID Level 5 <b>disk</b> <b>arrays,</b> particularly for write-intensive workloads. We find that the overhead of maintaining parity causes fullstripe writes (writes that span the entire error-correction group) to be more efficient than readmodify writes or reconstruct writes. This additional factor causes the optimal striping unit for RAID Level 5 to be smaller for write-intensive workloads than for read-intensive workloads. We synthesize simple design rules for how to choose an optimal striping unit for RAID Level 5 when workload concurrency is known and also when no workload information is known. Last, we investigate how the optimal striping unit varies with [...] ...|$|R
50|$|The AX {{series is}} {{considered}} the entry-level <b>disk</b> <b>array.</b>|$|E
50|$|A <b>disk</b> <b>array</b> {{controller}} provides front-end interfaces and back-end interfaces.|$|E
5000|$|<b>Disk</b> <b>array</b> {{controller}}, {{also known}} as RAID controller, a type of storage controller ...|$|E
5000|$|... 00:01:FE Hewlett-Packard - EVA <b>disk</b> <b>arrays.</b> Formerly Digital Equipment Corporation. WWIDs {{begin with}} 5000.1fe1 or 6000.1fe1 ...|$|R
40|$|<b>Disk</b> <b>arrays</b> are {{commonly}} {{designed to ensure}} that stored data will {{always be able to}} withstand a disk failure, but meeting this goal comes at a significant cost in performance. We show that this is unnecessary. By trading away a fraction of the enormous reliability provided by <b>disk</b> <b>arrays,</b> it is possible to achieve performance that is almost as good as a non-parityprotected set of disks. In particular...|$|R
40|$|Large <b>arrays</b> {{of small}} <b>disks</b> are {{providing}} an attractive approach for high performance I/O systems. In {{order to make}} effective use of <b>disk</b> <b>arrays</b> and other multi-disk architectures, {{it is necessary to}} develop intelligent software tools that allow automatic tuning of the <b>disk</b> <b>arrays</b> to varying workloads. In this paper we describe an adaptive method for data allocation and load balancing in <b>disk</b> <b>arrays.</b> Our method deals with dynamically changing access frequencies of files by reallocating file extents, thus "cooling down" hot disks. In addition, the method takes into account the fact that some files may exhibit periodical access patterns, and considers explicitly the cost of performing the "cooling" operations. Preliminary performance studies based on real-life I/O traces demonstrate the effectivity of this approach...|$|R
5000|$|... 9337 <b>Disk</b> <b>Array</b> Subsystem {{used the}} IBM 0662 (Spitfire) or 0663 (Corsair) HDDs.|$|E
5000|$|... 2002 - StorageTek {{introduces}} BladeStore, a <b>disk</b> <b>array</b> {{based on}} ATA disk technology.|$|E
50|$|The Digital {{video is}} stored on a {{storage area network}} (SAN) hard <b>disk</b> <b>array.</b>|$|E
40|$|Redundant <b>disk</b> <b>arrays</b> {{provide a}} way for {{achieving}} rapid recovery from media failures with a relatively low storage cost for large scale database systems requiring high availability. In this paper a method is proposed for using redundant <b>disk</b> <b>arrays</b> to support rapid-recovery from system crashes and transaction aborts {{in addition to their}} role in providing media failure recovery. A twin page scheme is used to store the parity information in the array so that the time for transaction commit processing is not degraded. Using an analytical model, it is shown that the proposed method achieves {{a significant increase in the}} throughput of database systems using redundant <b>disk</b> <b>arrays</b> by reducing the number of recovery operations needed to maintain the consistency of the database...|$|R
5000|$|Servers are {{the devices}} under management. Servers can be <b>disk</b> <b>arrays,</b> {{virtualization}} engines, host bus adapters, switches, tape drives, etc.|$|R
50|$|Universal Storage Platform (USP) was {{the brand}} name for an Hitachi Data Systems line of {{computer}} data storage <b>disk</b> <b>arrays</b> circa 2004 to 2010.|$|R
50|$|Thinking Machines also {{introduced}} an early commercial RAID2 <b>disk</b> <b>array,</b> the DataVault, circa 1988.|$|E
50|$|Note that <b>disk</b> <b>array</b> controllers, {{as opposed}} to disk controllers, usually have normal cache memory of around 0.5 - 8 GiB.|$|E
50|$|Used to {{be called}} P9000 XP, to match with other <b>disk</b> <b>array</b> {{products}} they sold at the time (P2000, P4000, P6000 and P10000).|$|E
50|$|The {{economic}} {{consolidation of}} <b>disk</b> <b>arrays</b> has accelerated {{the advancement of}} several features including I/O caching, snapshotting, and volume cloning (Business Continuance Volumes or BCVs).|$|R
40|$|Redundancy {{based on}} a parity {{encoding}} has been proposed for insuring that <b>disk</b> <b>arrays</b> provide highly reliable data. Parity-based redundancy will tolerate many independent and dependent disk failures (shared support hardware) without on-line spare disks and many more such failures with on-line spare disks. This paper explores the design of reliable, redundant <b>disk</b> <b>arrays.</b> In {{the context of a}} 70 <b>disk</b> strawman <b>array,</b> it presents and applies analytic and simulation models for the time until data is lost. It shows how to balance requirements for high data reliability against the overhead cost of redundant data, on-line spares, and on-site repair personnel in terms of an array's architecture, its component reliabilities, and its repair policies...|$|R
5000|$|... #Caption: Top to bottom: Users connect via the [...] "front-end" [...] to NAS {{appliances}} running FluidFS, which connect through [...] "back-end" [...] {{switches to}} <b>disk</b> <b>arrays.</b>|$|R
50|$|Since {{the crucial}} {{function}} of a file server is storage, technology has been developed to operate multiple disk drives together as a team, forming a <b>disk</b> <b>array.</b> A <b>disk</b> <b>array</b> typically has cache (temporary memory storage that is faster than the magnetic disks), as well as advanced functions like RAID and storage virtualization. Typically disk arrays increase level of availability by using redundant components other than RAID, such as power supplies. Disk arrays may be consolidated or virtualized in a SAN.|$|E
50|$|A <b>disk</b> <b>array</b> {{controller}} name {{is often}} improperly shortened to a disk controller. The two {{should not be}} confused as they provide very different functionality.|$|E
5000|$|Managed Disk (MDisk) - a unit {{of storage}} (a LUN) from a real, {{external}} <b>disk</b> <b>array,</b> virtualized by the SVC. An MDisk is the base to create an image mode VDisk.|$|E
50|$|TACC's {{long-term}} {{mass storage}} solution is an Oracle® StorageTek Modular Library System, named Ranch. Ranch utilizes Oracle's Sun Storage Archive Manager Filesystem (SAM-FS) for migrating files to/from a tape archival {{system with a}} current offline storage capacity of 40 PB. Ranch's disk cache is built on Oracle's Sun ST6540 and DataDirect Networks 9550 <b>disk</b> <b>arrays</b> containing approximately 110 TB of usable spinning disk storage. These <b>disk</b> <b>arrays</b> are controlled by an Oracle Sun x4600 SAM-FS Metadata server which has 16 CPUs and 32 GB of RAM.|$|R
40|$|In a {{video-on-demand}} (VOD) environment, disk-arrays {{are often}} used to support the disk bandwidth requirement. This can pose serious problems on available disk bandwidth upon disk failure. In this paper, we explore the approach of replicating frequently accessed movies to provide high data bandwidth and fault-tolerance required in a disk-array-based video server. An isochronous continuous video stream imposes different requirements from a random access pattern on databases or files. Explicitly, we propose a new replica placement method, called rotational mirrored declustering (RMD), to support high data availability for <b>disk</b> <b>arrays</b> in a VOD environment. In essence, RMD {{is similar to the}} conventional mirrored declustering in that replicas are stored in different <b>disk</b> <b>arrays,</b> however different from the latter in that the replica placements in different <b>disk</b> <b>arrays</b> under RMD are properly rotated. Combining the merits of prior chained and mirrored declustering methods, RMD is particularly [...] ...|$|R
40|$|This paper {{pointed out}} methods for {{providing}} fault tolerance of <b>disk</b> <b>arrays,</b> so that failures {{of one or}} several disks can be masked without data loss. The advantages of striping to improve performance and redundancy to improve reliability predominate most disadvantages, that results {{in the use of}} <b>disk</b> <b>arrays</b> in many applications of a wide variety. They are utilized in supercomputing environments (e. g. simulation in the sector of aerodynamics), technical applications (e. g. le{ servers for a CAD (Computer{Aided{Design) {system and data base applications such as telecommunications...|$|R
