1380|406|Public
5|$|Ritualistic behavior: Unvarying {{pattern of}} daily activities, {{such as an}} unchanging menu or a {{dressing}} ritual. This is closely associated with sameness and an <b>independent</b> <b>validation</b> has suggested combining the two factors.|$|E
25|$|An <b>independent</b> <b>validation</b> of COROT-7b as {{a planet}} is {{supplied}} by follow-up {{performed with the}} space based Spitzer telescope. Its observations confirmed the transits of the planet, with the same depth, at different wavelengths than the ones observed by COROT. The data then allows to validate COROT-7b as a bona-fide planet with a very high degree of confidence, independently from the noisy radial velocity data.|$|E
500|$|After {{odds ratios}} and P-values have been {{calculated}} for all SNPs, a common {{approach is to}} create a Manhattan plot. In the context of GWA studies, this plot shows the negative logarithm of the P-value as a function of genomic location. Thus the SNPs with the most significant association stand out on the plot, usually as stacks of points because of haploblock structure. Importantly, the P-value threshold for significance is corrected for multiple testing issues. The exact threshold varies by study, but the conventional threshold is [...] to be significant in the face of hundreds of thousands to millions of tested SNPs. GWA studies typically perform the first analysis in a discovery cohort, followed by validation of the most significant SNPs in an <b>independent</b> <b>validation</b> cohort.|$|E
40|$|Through clean-slate {{implementation}} of two storage optimizations—track-aligned extents and track-aligned RAIDs—this paper shows {{the values of}} <b>independent</b> <b>validations.</b> The experience revealed many unanticipated disk and storage data path behaviors as potential roadblocks for wide deployment of these optimizations, and also identified implementation issues to retrofit these concepts to legacy data paths. ...|$|R
5000|$|<b>Independent</b> Verification and <b>Validation</b> Facility, Fairmont, West Virginia ...|$|R
40|$|Continuous Positive Airway Pressure (CPAP) is {{accepted}} as first line therapy for Sleep Obstructive Apnea-Hypopnea Syndrome. Over {{the past few}} years, several new devices have been made available, aiming to improve patient comfort and compliance. In 25 years, major changes in technology have occurred, and CPAP has evolved from fixed pressure devices to auto-adjusting bi-level positive airway pressure. The algorithms and software of the newly commercialized devices are sometimes difficult to apprehend. In spite of impressive changes in technology, an obvious benefit in terms of compliance or comfort {{is yet to be}} demonstrated, and <b>independent</b> <b>validations</b> of these new devices are necessary...|$|R
2500|$|Due to its {{involvement}} in cancer development, inhibition of beta-catenin continues to receive significant attention. But the targeting of the binding site on its armadillo domain {{is not the}} simplest task, due to its extensive and relatively flat surface. However, for an efficient inhibition, binding to smaller [...] "hotspots" [...] of this surface is sufficient. This way, a [...] "stapled" [...] helical peptide derived from the natural β-catenin binding motif found in LEF1 was sufficient for the complete inhibition of β-catenin dependent transcription. Recently, several small-molecule compounds have also been developed to target the same, highly positively charged area of the ARM domain (CGP049090, PKF118-310, PKF115-584 and ZTM000990). In addition, β-catenin levels can also be influenced by targeting upstream components of the Wnt pathway {{as well as the}} β-catenin destruction complex. The additional N-terminal binding pocket is also important for Wnt target gene activation (required for BCL9 recruitment). This site of the ARM domain can be pharmacologically targeted by carnosic acid, for example. That [...] "auxiliary" [...] site is another attractive target for drug development. Despite intensive preclinical research, no β-catenin inhibitors are available as therapeutic agents yet. However, its function can be further examined by siRNA knockdown based on an <b>independent</b> <b>validation.</b>|$|E
50|$|The {{standard}} stipulates {{three types}} of validation of the achievement of neutrality: self validation, other party validation and third party <b>independent</b> <b>validation.</b> Other party validation occurs when the methodology and data has been audited and verified by an external organisation; third party <b>independent</b> <b>validation</b> occurs when it is verification is by an agent registered with UKAS.|$|E
50|$|Function of CEBPB gene can be {{effectively}} examined by siRNA knockdown {{based on an}} <b>independent</b> <b>validation.</b>|$|E
25|$|Edison2 was an exception. The {{engines in}} both Very Light Cars failed before validation. Edison2 {{was allowed to}} {{circumvent}} the rules governing procedures and provided <b>independent</b> third-party <b>validation</b> reports instead of undergoing the same validation testing as other finalist teams.|$|R
40|$|<b>Independent</b> {{external}} <b>validation</b> {{of cardiovascular}} disease mortality in women utilising Framingham and SCORE risk models: a mortality follow-up discrimination and calibration performance. Lower treatment thresholds are proposed for better identification of Goh et al. BMC Women's Health 2014, 14 : 11...|$|R
40|$|BACKGROUND AND PURPOSE: The ABCD {{system was}} derived to predict early risk of stroke after {{transient}} ischemic attack. <b>Independent</b> <b>validations</b> have reported conflicting results. We therefore systematically reviewed published and unpublished data to determine predictive value and generalizability to different clinical settings and users. METHODS: Validations of the ABCD and ABCD 2 scores {{were identified by}} searching electronic databases, reference lists, relevant journals, and conference abstracts. Unpublished tabulated data were obtained where available. Predictive value, expressed as pooled areas under the receiver operator characteristic curves (AUC), was calculated using random-effects meta-analysis, and analyses for heterogeneity were performed by categorization according to study setting and method. RESULTS: Twenty cohorts were identified reporting {{the performance of the}} ABCD system in 9808 subjects with 456 strokes at 7 days. Among the 16 studies of both the ABCD and ABCD 2 scores, pooled AUC for the prediction of stroke at 7 days were 0. 72 (0. 66 to 0. 78) and 0. 72 (0. 63 to 0. 82), respectively (P diff= 0. 97). The pooled AUC for the ABCD and ABCD 2 scores in all cohorts reporting relevant data were 0. 72 (0. 67 to 0. 77) and 0. 72 (0. 63 to 0. 80), respectively (both P< 0. 001). Predictive value varied significantly between studies (P< 0. 001), but 75 % of the variance was accounted for by study method and setting, with the highest pooled AUC for face-to-face clinical evaluation and the lowest for retrospective extraction of data from emergency department records. CONCLUSION: <b>Independent</b> <b>validations</b> of the ABCD system showed good predictive value, with the exception of studies based on retrospective extraction of nonsystematically collected data from emergency department records...|$|R
50|$|It {{provides}} software development, {{maintenance and}} <b>independent</b> <b>validation</b> services to companies in banking, finance, insurance, manufacturing and other domains.|$|E
5000|$|The Customer Service Excellence, (previously the [...] "Charter Mark") is an {{accreditation}} for organisations, {{intended to}} indicate an <b>independent</b> <b>validation</b> of achievement.|$|E
50|$|IV&V view - <b>independent</b> <b>validation</b> and {{verification}} of functionality and proper {{operation of the}} system in satisfaction of requirements. Does system as designed and developed meet goals and objectives.|$|E
40|$|New {{expressions}} for computable error bounds {{are derived}} for Nyström method approximate solutions of one-dimensional second-kind Fredholm integral equations. The bounds are computed using only the numerical solution, and so require no a priori {{knowledge of the}} exact solution. The analysis is implemented on test problems with both well-behaved and “Runge-phenomenon” solutions, and the computed predictions are shown to be in impressive quantitative agreement with the true errors obtained from known exact solutions of the test problems. For <b>independent</b> computational <b>validation,</b> both Lagrange and barycentric interpolation are employed on grids with both regularly spaced nodes and those located at the roots or extrema of orthogonal polynomials. For <b>independent</b> theoretical <b>validation,</b> asymptotic estimates are derived for the convergence rates of the observed computational errors...|$|R
40|$|Several {{biomarkers}} {{have been}} reported to be associated with survival in breast cancer over the past decades. Unfortunately, {{it is often difficult to}} distinguish between the prognostic and predictive values of these proposed markers and <b>independent</b> <b>validations</b> are frequently lacking. Patients with early-stage breast cancer receive various combinations of treatments including surgery, postoperative radiation therapy, adjuvant endocrine treatment and chemotherapy. Each of these can have an impact on survival, and most biomarker studies included heterogeneously treated patient cohorts. The classical methodology of assessing the true prognostic value of a marker was restricted to patients who underwent local regional therapy alone and received no systemic adjuvant treatment. However, the clinical utility of such prognostic markers in the absence of systemic therapy is limited in this era. Endocrin...|$|R
40|$|We {{develop an}} {{experimental}} study of {{the combination of the}} Coverage Inference Scheme proposed by Cristofor and Simovici with the bounded-negation association rules proposed by Fortes, Balc'azar, and Morales. Our main contributions are: 1 / A fully <b>independent</b> experimental <b>validation</b> of the advantages of the Coverage Inference Scheme on two different datasets...|$|R
5000|$|Ritualistic behavior: Unvarying {{pattern of}} daily activities, {{such as an}} unchanging menu or a {{dressing}} ritual. This is closely associated with sameness and an <b>independent</b> <b>validation</b> has suggested combining the two factors.|$|E
50|$|HDAC5 is {{involved}} in memory consolidation and suggests that development of more selective HDAC inhibitors {{for the treatment of}} Alzheimer's disease should avoid targeting HDAC5. Its function can be effectively examined by siRNA knockdown based on an <b>independent</b> <b>validation.</b>|$|E
50|$|Advanced {{software}} development; cyber security; database engineering; enterprise architecture; <b>independent</b> <b>validation</b> and verification; information assurance; intelligence, surveillance, and reconnaissance; interoperability certification; missile {{research and}} development; modeling and simulation; requirements analysis; test and evaluation; threat assessment; training; weapon system design.|$|E
40|$|Studies of noisy {{supersonic}} flow past aerodynamic objects are very limited. On the experimental side, {{the presence of}} inherent noise in high-speed wind tunnels make them unreliable predictors of fight performance, street for stationary cylinders (2 S pattern). This phenomenon is discussed in detail in [2] and <b>independent</b> experimental <b>validation</b> {{can be found in}} [4]...|$|R
5000|$|NASA <b>Independent</b> Verification and <b>Validation</b> Facility, {{governed by}} the Goddard Space Flight Center, houses more than 150 {{full-time}} employees and more than 20 in-house partners and contractors.|$|R
50|$|The <b>Independent</b> Verification and <b>Validation</b> Facility (IV&V) in Fairmont, West Virginia was {{established}} in 1993 to improve the safety, reliability, and quality of software used in NASA missions.|$|R
50|$|Research done by Salomé Adam, Sophie E. Polo, and Geneviève Almouzni {{indicate}} that HIRA proteins {{are involved in}} restarting transcription after UVC damage Function of HIRA gene can be effectively examined by siRNA knockdown based on an <b>independent</b> <b>validation.</b>|$|E
50|$|In practice, early {{stopping}} {{is implemented}} by training on a training set and measuring accuracy on a statistically <b>independent</b> <b>validation</b> set. The model is trained until {{performance on the}} validation set no longer improves. The model is then tested on a testing set.|$|E
50|$|Membership of {{a cluster}} {{provides}} <b>independent</b> <b>validation</b> of distances determined using recent Hubble Space Telescope and Hipparcos parallaxes. This strongly constrains the star's distance: 363 ± 9(σ2) ± 26(σ) parsecs. Zeta Geminorum is thus an important calibrator for the Cepheid period-luminosity relation used for establishing the cosmic distance ladder.|$|E
50|$|When Satellite Tool Kit's high-precision orbit propagator and {{parameter}} {{and coordinate}} frame transformations underwent an <b>Independent</b> Verification and <b>Validation</b> effort in 2000, TRACE v2.4.9 was the standard against which STK was compared.|$|R
50|$|The FBI's Criminal Justice Information Services Division {{is located}} in Clarksburg, NASA's <b>Independent</b> Verification and <b>Validation</b> Facility in Fairmont, and the U.S. Department of Treasury's Bureau of the Public Debt {{is located in}} Parkersburg.|$|R
40|$|Soil {{salinity}} negatively impacts {{the productivity}} and profitability of western San Joaquin Valley (WSJV) farmland. Many factors, including drought, climate change, reduced water allocations, and land-use changes could worsen salinity conditions there, {{and in other}} agricultural lands in the state. Mapping soil salinity at regional and state levels is essential for identifying drivers and trends in agricultural soil salinity, and for developing mitigation strategies, but traditional soil sampling for salinity {{does not allow for}} accurate large-scale mapping. We tested remote-sensing modeling to map root zone soil salinity for farmland in the WSJV. According to our map, 0. 78 million acres are salt affected (i. e., ECe > 4 dS/m), which represents 45 % of the mapped farmland; 30 % of that acreage is strongly or extremely saline. <b>Independent</b> <b>validations</b> of the remote-sensing estimations indicated acceptable to excellent correspondences, except in areas of low salinity and high soil heterogeneity. Remote sensing is a viable tool for helping landowners make decisions about land use and also for helping water districts and state agencies develop salinity mitigation strategies...|$|R
50|$|The highest {{potential}} risk for brake system failure {{has proven to}} be the Brake Control System software. Recurring failures have occurred in over 200 cases documented in NTSB documents. Because each manufacturer guards the confidentiality of their system design and software, there is no <b>independent</b> <b>validation</b> of the systems.|$|E
50|$|In 1998, {{the essay}} helped the final push for Netscape Communications Corporation {{to release the}} source code for Netscape Communicator and start the Mozilla project; it was cited by Frank Hecker and other {{employees}} as an outside <b>independent</b> <b>validation</b> of his arguments. Netscape's public recognition of this influence brought Raymond renown in hacker culture.|$|E
50|$|Subsampling {{allows one}} to define an out-of-bag {{estimate}} of the prediction performance improvement by evaluating predictions on those observations which were not used {{in the building of}} the next base learner. Out-of-bag estimates help avoid the need for an <b>independent</b> <b>validation</b> dataset, but often underestimates actual performance improvement and the optimal number of iterations.|$|E
50|$|The Woodland Carbon Code is the UK {{standard}} for afforestation projects for climate change mitigation. It provides <b>independent</b> verification and <b>validation</b> and assurance about {{the levels of}} carbon sequestration from managed woodland and their contribution to climate change mitigation.|$|R
40|$|An {{increasingly}} relevant {{question in}} evaluating commercial DNA tests is "What {{proportion of the}} additive genetic variation in the target trait is {{accounted for by the}} test? " Therefore, several estimators of this quantity were evaluated by simulation of a population of 1000 animals with 100 sires, each with 10 progeny. Three heritabilities (0. 1, 0. 3, and 0. 5) of the target trait and four proportions of genetic variation (0. 04, 0. 16, 0. 36, and 0. 64) accounted for by the molecular breeding value (MBV) for the DNA test were simulated. The first estimator evaluated is the reduction in estimated sire variance (R̂gRV 2) when the MBV is added as a fixed covariate to a single-trait model for the target trait divided by the sire variance from the model without the MBV. The second estimator is based on the regression of phenotype on MBV (R̂gRPM 2) from a single trait sire model in which the MBV is a fixed covariate (this is the model that has been standard in <b>independent</b> <b>validations</b> since DNA tests began being reported as MBV). This estimator is computed as R̂gRPM 2 ≅ b...|$|R
40|$|In silico {{prediction}} of drug-target interactions from heterogeneous biological data can advance our system-level search for drug molecules and therapeutic targets, which efforts {{have not yet}} reached full fruition. In this work, we report a systematic approach that efficiently integrates the chemical, genomic, and pharmacological information for drug targeting and discovery on a large scale, based on two powerful methods of Random Forest (RF) and Support Vector Machine (SVM). The performance of the derived models was evaluated and verified with internally five-fold cross-validation and four external <b>independent</b> <b>validations.</b> The optimal models show impressive performance of prediction for drug-target interactions, with a concordance of 82. 83 %, a sensitivity of 81. 33 %, and a specificity of 93. 62 %, respectively. The consistence of the performances of the RF and SVM models demonstrates the reliability and robustness of the obtained models. In addition, the validated models were employed to systematically predict known/unknown drugs and targets involving the enzymes, ion channels, GPCRs, and nuclear receptors, which can be further mapped to functional ontologies such as target-disease associations and target-target interaction networks. This approach is expected to help fill the existing gap betwee...|$|R
