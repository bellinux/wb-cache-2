7|176|Public
40|$|For an {{advanced}} implementation of spelling correction via machine learning, a multi-level featurebased framework is developed. In order {{to use as}} much information as possible, we simultaneously include features from the character level, phonetic level, word level, syntax level, and semantic level. These are evaluated by a support vector machine to predict the correct candidate. Our method allows to correct non-word errors as well as real-word errors simultaneously using the same feature extraction methods, and it closes the gap separating <b>isolated</b> <b>error</b> correction techniques from context-sensitive methods. In contrast to previous approaches, our technique is not confined to correct only words from precompiled lists of “confused ” words. Regarding the correction capabilities of our system, we outperform Microsoft Word, Google, Hunspell, Aspell and FST in recall by at least 3 % even if confined to non-word errors. The recall of our system ranges from 90 % for the first candidate to 97 % for all five candidates presented. Index Terms — context-sensitive spelling correction, lexical disambiguation, machine learning, <b>isolated</b> <b>error</b> correction. I...|$|E
40|$|One of the {{foremost}} advantages of corpus work {{is that it allows}} us to uncover facts about language {{that would not have been}} noticed with more traditional methods. What would earlier have been dismissed as an <b>isolated</b> <b>error,</b> an unfortunate turn of phrase or a misunderstanding may well, when the computer presents all the instances of the phenomenon in the corpus, turn out to be a significant innovation in the language. A case in point may be alls. In the CobuildDirect Corpus, a 57 -million-word corpus of modern British, American and Australian English, there are a number of occurrences of the word alls. Here are a few specimens: (1) You’re hanging roun...|$|E
40|$|International audienceThis study {{focuses on}} <b>isolated</b> <b>error</b> {{detection}} in a retro-digitized newspaper corpus published from 1946 to 1990 {{in the former}} German Democratic Republic. As there are OCR errors throughout the corpus but no clean reference for this variant of German, automatic OCR correction implies to overcome data sparseness and non-standard spelling, including compounds and inflected forms. The contributions of this paper are (1) a method to bootstrap detection of potential misspellings, (2) an assessment of several types of training data, and (3) an evaluation of several off-the-shelf candidate selection techniques. The chosen solution based on statistical affix analysis reaches an accuracy 10 points higher than existing morphological analysis systems on error detection, while a combination of fuzzy and approximate string search performs best for error correction. The criteria are met since {{it is possible to}} correct erroneous tokens without introducing too much noise...|$|E
40|$|Correct {{real-time}} {{software is}} vital for robots insafety-critical roles such as service and disasterresponse. These systems depend on software forlocomotion, navigation, manipulation, and even seemingly innocuous tasks such as safely regulating battery voltage. A multi-process software design increases robustness by <b>isolating</b> <b>errors</b> to a single process, allowing the rest o...|$|R
50|$|The M&E is, as {{its name}} indicates, {{separated}} into two distinguished categories: evaluation and monitoring. An evaluation is a systematic and objective examination concerning the relevance, effectiveness, efficiency and impact of activities {{in the light of}} specified objectives. The idea in evaluating projects is to <b>isolate</b> <b>errors</b> not to repeat them and to underline and promote the successful mechanisms for current and future projects.|$|R
40|$|This paper {{describes}} a tool called vpoiso {{that was developed}} to automatically <b>isolate</b> <b>errors</b> in the vpo compiler system. The two general types of compiler <b>errors</b> <b>isolated</b> by this tool are optimization and nonoptimization <b>errors.</b> When <b>isolating</b> optimization <b>errors,</b> vpoiso relies on the vpo optimizer to identify sequences of changes, referred to as transformations, that result in semantically equivalent code and to provide the ability to stop performing improving (or unnecessary) transformations after a specified number have been performed. Acompilation of a typical program by vpo often results in thousands of improving transformations being performed. The vpoiso tool can automatically isolate the first improving transformation that causes incorrect output of {{the execution of the}} compiled program by using a binary search that varies the number of improving transformations performed. Not only is the illegal transformation automatically isolated, but vpoiso also identifies the location and instant the transformation is performed in vpo. Nonoptimization errors occur from problems in the front end, code generator, and necessary transformations in the optimizer. Ifanother compiler is available that can produce correct (but perhaps more inefficient) code, then vpoiso can <b>isolate</b> nonoptimization <b>errors</b> to a single function. Automatic isolation of compiler errors facilitates retargeting a compiler to a new machine, maintenance of the compiler, and supporting experimentation with new optimizations...|$|R
40|$|Typically a type {{error is}} {{reported}} when unification fails, {{even though the}} programmer 's actual error may have occurred much earlier in the program. The and inference algorithms report the site where a type conflict is detected, but the error message is isolated information: {{it is not clear}} what the relationship is between the site where error is reported and the context in which the subexpression was typed. As a result, the error message may give little help to locate the source of the error. This report investigates better methods of explaining type conflicts. We aim to find a method that may be effective even when the user has little knowledge of type checking. The philosophy of our approach is to find sources of type errors by reporting which parts of the program conflict, rather than <b>isolated</b> <b>error</b> sites. We implement two new inference algorithms with this philosophy: the Unification of Assumption Environments (AE) and the Incremented Error Inference (). ...|$|E
40|$|A Vessel Offset Survey {{was carried}} out on the vessel Shogun via {{traversing}} with a Total Station. While the survey was successful In obtaining reliable and accurate results, as evident by observing single points from multiple stations, the accuracies reached were not at the sub cm level, as per the client’s request. As a Total Station has precision quoted at the mm level, a further survey with a different control network {{was carried out}} on the vessel Falcon Spirit {{in an attempt to}} identify the errors. The surveys, conducted in January and July 2013 were carried out in varying conditions and time frames at different locations, using identical surveying equipment {{in the form of a}} Leica TS 02 Total Station. Personnel deployed on each survey also differed, being more numerous on the second. A three station closed loop traverse was conducted in the initial Shogun survey whereas a braced quadrilateral traverse was conducted on the subsequent Falcon Spirit survey for comparison purposes. Offsite analysis of data revealed a weak control point in the latter, which was subsequently <b>isolated.</b> <b>Error</b> analysis revealed no likely systematic source of error, however both gross and random errors are considered to have occurred. Station stability and faulty tribrachs are considered the most likely sources of gross error. The relative merits and cost benefit implications of braced quadrilateral versus three station closed loop traverses are considered...|$|E
40|$|OBJECTIVE [...] To {{find the}} reasons for legal claims against {{hospital}} doctors. DESIGN [...] Prospective analysis of requests for medical opinion submitted by solicitors during 1984 - 93 on legal claims against hospital doctors. SUBJECTS [...] 100 successive cases: 98 from the United Kingdom and two from the Republic of Ireland. MAIN OUTCOME MEASURES [...] Principal underlying causes of claims. RESULTS [...] In 44 cases there was no serious clinical error. Of the 56 cases of clinical fault, seven were a failure of communication by doctors, 15 were an <b>isolated</b> <b>error</b> in otherwise good clinical management, 21 were errors {{that might not have}} occurred with better control of clinical practice (doctors exceeding their competence, poor clinical judgment, and poor teamwork), and 13 were major errors due to carelessness or incompetence. In 34 cases there was evidence of clinical fault that might escape clinical audit and medicolegal processes. Most of these legal claims have been or are likely to be withdrawn: only five plaintiffs have settled out of court, and 11 are pursuing their actions. CONCLUSIONS [...] To reduce the incidence of errors, hospital doctors should consult colleagues about difficult cases and specialists should maintain a broad interest in disease. The NHS clinical complaints procedure should be extended to cover potential claims, and serious cases should be subject to independent external assessment by experienced consultants...|$|E
5000|$|In normal use, Btrfs {{is mostly}} {{self-healing}} and can recover from broken root trees at mount time, thanks to making periodic data flushes to permanent storage every 30 seconds (which is the default period). Thus, <b>isolated</b> <b>errors</b> {{will cause a}} maximum of 30 seconds of filesystem changes to be lost at the next mount. [...] This period can be changed by specifying a desired value (in seconds) for the [...] mount option.|$|R
5000|$|A WSF may {{be useful}} for <b>isolating</b> <b>errors.</b> Its modular nature {{prevents}} one script reference from interfering with another. Here is a WSF example with one module that produces an error and one that does not: [...] The first script module will produce a [...] "divide by zero" [...] error. Typically this would cause the script to end in the Windows Script Host but this modular method allows the script to continue and execute the second script module.|$|R
2500|$|Some {{physicians}} {{recognize that}} adverse outcomes from errors {{usually do not}} happen because of an <b>isolated</b> <b>errors</b> and actually reflect system problems. This concept {{is often referred to}} as the Swiss Cheese Model. This is the concept that there are layers of protection for clinicians and patient to prevent mistakes from occurring. Therefore, even if a doctor or nurse makes a small error (e.g. incorrect dose of drug written on a drug chart by doctor), this is picked up before it actually affects patient care (e.g. pharmacist checks the drug chart and rectifies the error). Such mechanisms include: ...|$|R
40|$|Background: Aortic {{pressure}} waveforms {{are calculated}} non-invasively by applying generalized transfer functions (GTF) to tonometric radial pressure waveforms. Input errors mainly during acquisition and calibration of tonometric pressures are '' transferred '' to aortic pressure calculation. The present study aimed {{to quantify the}} proportion of specific input errors which is '' transferred '' by the GTFs {{in a wide range}} of hemodynamic conditions and for different error combinations in brachial systolic (SBP) and diastolic (DBP) blood pressure measurements. Methods: Aortic pulse wave analysis was performed in 103 subjects (52 normotensive and 51 untreated hypertensive) by the SphygmoCor (R) System. Each pressure waveform was initially calibrated by sphygmomanometrical brachial pressures. Isolated, parallel and reverse errors in brachial SBP/DBP from - 10 to + 10 mmHg were simulated, by recalibration of the recorded radial pressure waveforms, inducing specific '' errors '' of GTF-input values. For every recalculated aortic SBP and DBP, the difference from the initial estimated value was considered to represent the '' transferred error '' to the aortic pressure estimation. Results: Parallel errors by 5 mmHg in both SBP and DBP resulted to an identical change in GTF-derived aortic pressures, as expected. When an overestimation in SBP by 5 mmHg and an underestimation in DBP by - 5 mmHg occurred (reverse errors), almost 56 % of this error (- 2. 8 mmHg) was transferred. An <b>isolated</b> <b>error</b> in brachial SBP by 5 mmHg was transmitted by 76 % (- 3. 8 mmHg) to GTF-derived aortic SBP. In subjects with mean blood pressure > 117 mmHg or with heart rates < 74 bpm, a greater percent of the calibration error was transferred to GTF-derived blood pressures. Conclusions: Input errors in brachial pressure values result in a quantifiable effect on transfer function output (aortic pressures). The percent of the '' error transfer '' by the GTFs depends on heart rate and BP levels, which should be taken into account when applying GTFs at populations with different hemodynamic conditions. (c) 2005 Elsevier Ireland Ltd. All rights reserved...|$|E
40|$|We {{describe}} {{a project to}} refine the idea of proof-directed debugging. The intention is to clarify the mechanisms by which failed verification attempts {{can be used to}} <b>isolate</b> <b>errors</b> in code, in particular by exploiting {{the ways in which the}} branching structure of a proof can match the the structure of the functional program being verified. Our intention is to supply tools to support this process. We then further discuss how the proof planning paradigm might be used to supply additional automated support for this and, in particular ways in which the automation of proof-directed debugging with proof planning would allows code patches to by synthesised at the same time that a bug is located and diagnosed. ...|$|R
40|$|IS. DSTUPPEMNTAYTEET(ftiSeot Layoriuto laouti validati graphimorhmbprttegaps It. KEY WRDS (Cont~inue on {{reverse side}} It neessary? mid Identify by block number) Gemini is a circuit {{comparison}} {{program that is}} widely used to compare 5 circuit layout against a specification. In this paper we describe recent extensions made to Gemini that make it faster, enable it to <b>isolate</b> <b>errors</b> better, and extend its domain of application. This {{has been done by}} changes to the labeling algorithm, extensions to the local matching algorithm, better handling of symmetrical circuits and the accommodation of series-connected transistors. GeminiII's MD 1 A 7 43 EIIOOIOSIOSLT Continued on back [...] S/N 01 03. LF. 0 I 4. 5601 SECURITY CLASSIFICATION OF THIS PAGEt (When 00 # 0 ftioeod) (continued from front page) algorithm is separated into global labeling and local matching phases...|$|R
40|$|Multi-chip, {{board-level}} designs form a {{large portion}} of today's digital system designs. Unfortunately, traditional methods for debugging these designs, such as prototype fabrication, wire-wrap and software simulation, are inadequate. Prototype fabrication is time-consuming {{and it is difficult to}} <b>isolate</b> <b>errors,</b> wire-wrap is complex and error-prone, and simulation is too slow for full testing. Recent advances in FPGA-based systems offer hope for significant improvements in board-level prototyping, yet currently focus exclusively on ASIC prototyping. In this paper, we present Springbok, an integrated system for board-level prototyping that promises near-speed testing with the construction and modification simplicity of software simulation. It is composed of an integrated software system for mapping designs to a flexible, extensible hardware implementation platform. This allows designs to take advantage of FPGA flexibility while using the complex chips that will implement the final design [...] ...|$|R
40|$|In this paper, {{we present}} a {{platform}} for collaborative acoustic signal processing, and demonstrate its use with an example application. Our platform is built upon the Stargate Linux-based microserver, and supports synchronized multi-channel acoustic data acquisition. We implement a dataflow-like staged event-driven programming model within the Emstar software framework that simplifies the development of collaborative processing applications. Unlike previous dataflow systems that emphasize real-time constraints, our framework emphasizes collaborative processing across nodes in a distributed system connected by an energy-conserving wireless network with non-deterministic message latency. In our model, an application is constructed by wiring together multiple stages, where each stage is implemented by an EmStar module. The modular approach simplifies development by <b>isolating</b> <b>errors</b> to specific stages, and enables run-time system reconfigurability by allowing users to swap out implementations of individual stages, and to reconfigure the dataflow at run time. 1...|$|R
40|$|The {{results of}} a study of test data taken on the simplex strapdown {{navigation}} system were presented. That system consisted of the following components: strapdown platform, altimeter, digital computer, tape recorder, typewriter, and power source. The objective of these tests was to <b>isolate</b> <b>error</b> sources which may cause degradation of the system's accuracy and to recommend appropriate changes to the system test procedures or computer software. The following recommendations were made: (1) addition of a gyro compassing alignment program into the navigation program, (2) addition of line drivers at the signal processor end of the transmission line, (3) need for extensive laboratory testing to determine sensor misalignments, biases, and scale factors, (4) need to stabilize the power source to prevent transients during power transfer, (5) need to isolate and eliminate {{the source of the}} large noise inputs...|$|R
40|$|Shape unifying {{is a very}} {{efficient}} preprocessing technique used in lossy SPM-JBIG 2 systems. It permits <b>isolated</b> <b>errors</b> between the current bitmap and its reference to improve refinement coding efficiency. Compared to lossless coding, it can improve compression by about 32 % while causing very little visual information loss. When bigger error clusters are permitted in shape unifying, further compression gain can be achieved but {{at the price of}} more noticeable visual information loss and even character substitution errors. In this paper we propose a feature monitored shape unifying procedure that can significantly lower the risk of substitution errors when permitting bigger errors. Experiments show that, compared to the unmonitored shape unifying, the feature monitored version can suppress more than 2 / 3 of all substitution errors while achieving additional compression improvements of 30 - 40 %. 1...|$|R
40|$|The {{discovery}} of a C Band radar pulse Doppler error is discussed {{and use of the}} GEOS 3 satellite's coherent transponder to <b>isolate</b> the <b>error</b> source is described. An analysis of the pulse Doppler tracking loop is presented and a mathematical model for the error was developed. Error correction techniques were developed and are described including implementation details...|$|R
40|$|This paper {{proposes a}} new error {{recovery}} technique for interactive video transmission over error-prone, slow, and bandwidth constrained networks. The technique effectively combines {{forward error correction}} (FEC) and retransmission to solve the error propagation problem which is inherent in any motion-based video codecs. Prior work has mainly taken an approach that prevents display errors from occurring from the first place. This approach is not effective for interactive video transmission over the target network environments because data losses inevitably occur in such networks and repairing them tends to introduce the delays in video playout. Our approach {{is to focus on}} eliminating <b>error</b> propagation and <b>isolating</b> <b>errors</b> when they occur. This approach allows more time for packet repairs and can effectively mask out delays in repairing lost packets. A simulation study shows that the proposed techniques can be effectively used to transmit high-frame-rate interactive video over slow, erro [...] ...|$|R
40|$|Abstract — In {{wireless}} video transmission, burst packet errors generally {{produce more}} catastrophic results than {{equal number of}} <b>isolated</b> <b>errors.</b> To minimize the playback distortion, it is crucial for the sender to know the packet errors at the receiver and then optimally schedule next transmissions. Unfortunately, in practice, feedback errors result in inaccurate observations of the receiving status. In this paper, we develop an optimal scheduling framework to minimize the expected distortion by first estimating the receiving status. Then, we jointly consider the source and channel characteristics and optimally choose the packets to transmit. The optimal transmission strategy is computed through a partially observable Markov decision process. The experimental {{results show that the}} proposed framework improves the average peak signal-to-noise ratio (PSNR) by 0. 6 - 1. 3 dB upon using a traditional system without packets scheduling. Moreover, we show that the proposed method smoothes out the bursty distortion periods and results in less fluctuating PSNR values. I...|$|R
40|$|Random matrix {{theory has}} {{successfully}} modeled many systems in physics and mathematics, {{and often the}} analysis and results in one area guide development in the other. Hughes and Rudnick computed 1 -level density statistics for low-lying zeros {{of the family of}} primitive Dirichlet L-functions of fixed prime conductor Q, as Q →∞, and verified the unitary symmetry predicted by random matrix theory. We compute 1 - and 2 -level statistics of the analogous family of Dirichlet L-functions over F_q(T). Whereas the Hughes-Rudnick results were restricted by the support of the Fourier transform of their test function, our test function is periodic and our results are only restricted by a decay condition on its Fourier coefficients. We show the main terms agree with unitary symmetry, and also <b>isolate</b> <b>error</b> terms. In concluding, we discuss an F_q(T) -analogue of Montgomery's Hypothesis on the distribution of primes in arithmetic progressions, which Fiorilli and Miller show would remove the restriction on the Hughes-Rudnick results. Comment: 22 pages. Comments are welcom...|$|R
40|$|Abstract. In {{the event}} that a system does not satisfy a specification, a model checker will {{typically}} automatically produce a counterexample trace that shows a particular instance of the undesirable behavior. Unfortunately, the important steps that follow {{the discovery of a}} counterexample are generally not automated. The user must first decide if the counterexample shows genuinely erroneous behavior or is an artifact of improper specification or abstraction. In {{the event that}} the error is real, there remains the difficult task of understanding the error well enough to isolate and modify the faulty aspects of the system. This paper describes an automated approach for assisting users in understanding and <b>isolating</b> <b>errors</b> in ANSI C programs. The approach is based on distance metrics for program executions. Experimental results show that the power of the model checking engine can be used to provide assistance in understanding <b>errors</b> and to <b>isolate</b> faulty portions of the source code. ...|$|R
40|$|Debugging, an {{integral}} part of software development, is difficult for end-user programmers, {{especially in the case of}} complex programs. The process of <b>isolating</b> <b>errors</b> is time consuming without the help of debugging support provided by the tool. For example, the visual programming tool LondonTube supports creation of custom mobile-cloud-web applications, but previous research indicated that the users of LondonTube had questions on usage of program components and run time status of a program even while creating a simple application. To address these issues, this project was focused on creating two features, ‘Analyze’ and ‘Trace’, improving the visual programming tool by providing static analysis and runtime status for helping users to find/fix errors. A laboratory experiment evaluated the effectiveness of the prototype in comparison to the baseline (without the new features). The results of this study revealed that the users of this prototype were more satisfied with the system, took less time to complete assigned tasks, and asked fewer questions about usage of program components...|$|R
40|$|Errors {{in quantum}} {{computers}} are of two kinds: sudden perturbations to isolated qubits, and slow random drifts {{of all the}} qubits. The latter may be reduced, but not eliminated, by means of symmetrization, namely by using many replicas of the computer, and forcing their joint quantum state to be completely symmetric. On the other hand, <b>isolated</b> <b>errors</b> can be corrected by quantum codewords that represent a logical qubit in a redundant way, by several physical qubits. If one of the physical qubits is perturbed, for example if it gets entangled with an unknown environment, there still is enough information encoded in the other physical qubits to restore the logical qubit, and disentangle it from the environment. The recovery procedure may consist of unitary operations, without the need of actually identifying the error. Comment: 18 pages LaTeX + 1 figure PostScript. Proceedings of PhysComp' 96 workshop, Boston 21 - 24 November 1996, to appear in Physica D (1997...|$|R
40|$|In {{wireless}} video transmission, burst packet errors generally {{produce more}} catastrophic results than {{equal number of}} <b>isolated</b> <b>errors.</b> To minimize the playback distortion, it is crucial for the sender to know the packet errors at the receiver and then optimally schedule next transmissions. Unfortunately, in practice, feedback errors result in inaccurate observations of the receiving status. In this paper, we develop an optimal scheduling framework to minimize the expected distortion by first estimating the receiving status. Then, we jointly consider the source and channel characteristics and optimally choose the packets to transmit. The optimal transmission strategy is computed through a partially observable Markov decision process. The experimental {{results show that the}} proposed framework improves the average peak signal-to-noise ratio (PSNR) by 0. 6 - 1. 3 dB upon using a traditional system without packets scheduling. Moreover, we show that the proposed method smoothes out the bursty distortion periods and results in less fluctuating PSNR values...|$|R
40|$|L 2 writing {{revision}} techniques {{based on}} corrective feedback from teachers {{are common in}} EFL courses, but their efficacy for low-level students is becoming increasingly questioned. The present study tests {{the effectiveness of a}} traditional revision technique - signaling with symbols all errors in a composition - by comparing it to a new one that involves L 2 -L 1 translation and peer revision. The study was carried out with two intact groups of EFL learners in Secondary Education. All learners were asked to write two compositions and to correct them following both revision techniques. Results indicated that the new technique helped students notice and correct errors that hinder communication between writer and reader, but it was not more effective than traditional techniques in the identification of <b>isolated</b> grammar <b>errors.</b> Therefore, this new technique seems appropriate to be used in combination with other techniques that focus on the correction of <b>isolated</b> grammar <b>errors.</b> Máster Universitario en Formación del Profesorado de Educación SecundariaUnibertsitate Masterra Bigarren Hezkuntzako Irakasleen Prestakuntza...|$|R
40|$|We {{describe}} PVS's capabilities for representing tabular specifications of {{the kind}} advocated by Parnas and others, and show how PVS's Type Correctness Conditions (TCCs) are used to ensure certain well-formedness properties. We then show how these and other capabilities of PVS {{can be used to}} repre-sent the AND/OR tables of Leveson and the Decision Tables of Sherry, and we demonstrate how PVS_s TCCs can expose and help <b>isolate</b> <b>errors</b> in the latter. We extend this approach to represent the mode transition tables of the Software Cost Reduction (SCR) method in an attractive rammer. We show how PVS can check these tables for well-formedness, and how PVS's model checking capalfilities can he used to verify invariants and reaehability properties of SCR requirelnents specifications, and inclusion relations between the behaviors of different specifica-tions. These exalnples demonstrate how sew_ral capabilities of the PVS language and verification system can be used in combination to provide customized support fo...|$|R
5000|$|Some {{physicians}} {{recognize that}} adverse outcomes from errors {{usually do not}} happen because of an <b>isolated</b> <b>errors</b> and actually reflect system problems. This concept {{is often referred to}} as the Swiss Cheese Model. This is the concept that there are layers of protection for clinicians and patient to prevent mistakes from occurring. Therefore, even if a doctor or nurse makes a small error (e.g. incorrect dose of drug written on a drug chart by doctor), this is picked up before it actually affects patient care (e.g. pharmacist checks the drug chart and rectifies the error). Such mechanisms include: Practical alterations (e.g.-medications that cannot be given IV, are fitted with tubing which means they cannot be linked to an IV even if a clinician makes a mistake and tries to), systematic safety processes (e.g. all patients must have a Waterlow score assessment and falls assessment completed on admission), training programmes/continuing professional development courses [...] are measures that may be put in place.|$|R
5000|$|The {{received}} vector [...] {{is the sum}} of {{the correct}} codeword [...] and an unknown error vector The syndrome values are formed by considering [...] as a polynomial and evaluating it at Thus the syndromes arefor [...] to Since [...] are the zeros of [...] of which is a multiple, Examining the syndrome values thus <b>isolates</b> the <b>error</b> vector so one can begin to solve for it.|$|R
40|$|Despite continual {{increases}} in numerical model resolution and {{significant improvements in}} the forecasting of many meteorological parameters, progress in quantitative precipitation forecasting (QPF) has been slow. This is attributable in part to deficiencies in the bulk microphysical parameterization (BMP) schemes used in mesoscale models to simulate cloud and precipitation processes. These deficiencies have become more apparent as model resolution has increased. To address these problems requires comprehensive data sets {{that can be used}} to <b>isolate</b> <b>errors</b> in QPF due to BMP schemes from those due to other sources. These same data sets can then be used to evaluate and improve the microphysical processes and hydrometeor fields simulated by BMP schemes. In response to the need for such data, a group of researchers are collaborating on a study entitled IMPROVE (for Improvement of Microphysical Parameterizations through Observational Verification Experiments). IMPROVE has included two field campaigns carried out in the Pacific Northwest: an offshore frontal precipitation study off the Washington coast in January/February 2001, and an orographic precipitation study in th...|$|R
40|$|Relative {{debugging}} is {{a system}} which allows a programmer to compare the state of two executing programs. It is embedded in a conventional debugger, and {{makes it possible to}} <b>isolate</b> <b>errors</b> after a program has been modified or "ported " from one system to another. It has found been that relative debugging is useful for finding errors in large scientific programs. The technique is applicable to sequential programs, but is extremely valuable when sequential programs are ported to parallel platforms, because complex errors are often introduced at this stage. Accordingly, we have been investigating the application of relative debugging to parallel programs. Whilst the general approach used for sequential programs can be applied, the implementation becomes much more difficult. We have developed a new execution mechanism based on data-flow semantics, which allows the debugger to control a number of processes. Further, we are currently building a prototype parallel debugger, and are exploring how a programmer might interact with such a tool. This includes issues in common with other parallel debuggers, but also requires research into appropriate mechanisms for specifying how comparisons are performed...|$|R
40|$|Retiming is a {{powerful}} logic optimization technique that repositions registers in a circuit. However, its verification is difficult. In this work we implement a classical retiming algorithm and check it using a sequential verification methodology that evaluates the correctness of retiming using fast simulation. Unlike traditional verification techniques that are demanding in memory and computing power, this methodology quickly discovers and <b>isolates</b> most <b>errors</b> caused by retiming, thus reducing verification effort. 1...|$|R
40|$|The {{measurements}} of sunspot positions and {{areas that were}} published initially by the Royal Observatory, Greenwich, and subsequently by the Royal Greenwich Observatory (RGO), as the Greenwich Photo-heliographic Results (GPR), 1874 [*]–[*] 1976, exist in both printed and digital forms. These printed and digital sunspot datasets have been archived in various libraries and data centres. Unfortunately, however, typographic, systematic and <b>isolated</b> <b>errors</b> {{can be found in}} the various datasets. The purpose of the present paper is to begin the task of identifying and correcting these errors. In particular, the intention is to provide in one foundational paper all the necessary background information on the original solar observations, their various applications in scientific research, the format of the different digital datasets, the necessary definitions of the quantities measured, and the initial identification of errors in both the printed publications and the digital datasets. Two companion papers address the question of specific identifiable errors; namely, typographic errors in the printed publications, and both <b>isolated</b> and systematic <b>errors</b> in the digital datasets. The existence of two independently prepared digital datasets, which both contain information on sunspot positions and areas, makes it possible to outline a preliminary strategy for the development of an even more accurate digital dataset. Further work is in progress to generate an extremely reliable sunspot digital dataset, based on the programme of solar observations supported for more than a century by the Royal Observatory, Greenwich, and the Royal Greenwich Observatory. This improved dataset should be of value in many future scientific investigations...|$|R
40|$|Mazo's {{concept of}} Faster Than Nyquist {{signaling}} is extended to pulse trains that modulate adjacent subcarriers, {{in a manner}} similar to orthogonal frequency division multiplex (OFDM) transmission. Despite pulses that are faster than the Nyquist limit and subcarriers that significantly overlap, the transmission system achieves the <b>isolated</b> pulse <b>error</b> performance. Systems with at least twice the spectral efficiency of OFDM can be achieved at the same error probability. Receiver design is challenging, and we report tests of several options...|$|R
