6|12|Public
40|$|To those {{brought up}} in a logic-based {{tradition}} {{there seems to be a}} simple and clear definition of proof. But this is largely a 20 th century invention; many earlier proofs had a different nature. We will look particularly at the <b>faulty</b> <b>proof</b> of Euler’s Theorem and Lakatos ’ rational reconstruction of the history of this proof. We will ask: how is it possible for the errors in a <b>faulty</b> <b>proof</b> to remain undetected for several years – even when counter-examples to it are known? How is it possible to have a proof about concepts that are only partially defined? And can we give a logicbased account of such phenomena? We introduce the concept of schematic proofs and argue that they offer a possible cognitive model for the human construction of proofs in mathematics. In particular, we show how they can account for persistent errors in proofs...|$|E
40|$|We {{present a}} new method of {{analysis}} of associative algebras. This method bears a certain resemblance to the famous analysis of commutative $C^*$-algebras in which an important role is played by multiplicative functionals over the algebra. These functionals {{can be considered as}} "rank 1 " in the framework of our method. In our approach a special position is taken by generic (maximal rank) functionals, which reveal a wealth of information on the structure of non-commutative algebras. We apply this method to derive interesting properties of index of Lie algebras and to analyze finite-dimensional associative algebras with unity and Lie index 1. Comment: Removed <b>faulty</b> <b>proof</b> of $V(1) =Stab_F$ {{at the very end of}} section...|$|E
40|$|The aim of {{this paper}} is to {{establish}} a theory of random variables on domains. Domain theory is a fundamental component of theoretical computer science, providing mathematical models of computational processes. Random variables are the mainstay of probability theory. Since computational models increasingly involve probabilistic aspects, it's only natural to explore the relationship between these two areas. Our main results show how to cast results about random variables using a domain-theoretic approach. The pay-off is an extension of the results from probability measures to sub-probability measures. We also use our approach to extend the class of domains for which we can classify the domain structure of the space of sub-probability measures. Comment: This revision corrects the original, <b>faulty</b> <b>proof</b> of Corollary 3. ...|$|E
40|$|AbstractSimultaneous rigid E-unification was {{introduced}} in 1987 by Gallier, Raatz and Snyder. It {{is used in the}} area of automated reasoning with equality in extension procedures, like the tableau method or the connection method. Many articles in this area assumed the existence of an algorithm for the simultaneous rigid E-unification problem. There were several <b>faulty</b> <b>proofs</b> of the decidability of this problem. In this paper we prove that simultaneous rigid E-unification is undecidable. As a consequence, we obtain the undecidability of the ℶ∗-fragment of intuitionistic logic with equality...|$|R
40|$|This paper {{reports the}} use of proof {{planning}} to diagnose errors in program code. In particular it looks at the errors that arise in the base cases of recursive programs produced by undergraduates. It describes two classes of error that arise in this situation. The use of test cases would catch these errors but would fail to distinguish between them. The system adapts proof critics, commonly used to patch <b>faulty</b> <b>proofs,</b> to diagnose such errors and distinguish between the two classes. It has been implemented in Lambda-clam, a proof planning system, and applied successfully to a small set of examples...|$|R
40|$|Simultaneous rigid E-unification {{has been}} {{introduced}} {{in the area of}} theorem proving with equality. It is used in extension procedures, like the tableau method or the connection method. Many articles in this area tacitly assume the existence of an algorithm for simultaneous rigid E-unification. There were several <b>faulty</b> <b>proofs</b> of the decidability of this problem. In this article we prove several results about the simultaneous rigid E-unification. Two results are reductions of known problems to simultaneous rigid E-unification. Both these problems are very hard. The word equation solving (unification under associativity) is reduced to the monadic case of simultaneous rigid E-unification. The variable-bounded semi-unification problem is reduced to the general simultaneous rigid E-unification. The word equation problem used in the first reduction is known to be decidable, but the decidability result is extremely non-trivial. As for the variablebounded semi-unification, its decidability is [...] ...|$|R
40|$|AbstractIn 1878 Georg Cantor {{proved that}} unique, {{one-to-one}} mappings could be constructed between spaces of arbitrary yet different dimension. This paper {{is devoted to}} {{a detailed analysis of}} the earliest attempts to deal with the implications of that proof. Dedekind was the first to suggest that continuity was a key to the problem of dimensional invariance. Lüroth, Thomae, Jürgens and Netto offered solutions, Netto's being the most interesting in terms of the specifically topological character of his paper. Cantor finally offered a <b>faulty</b> <b>proof</b> in 1879 that domains of different dimension could not be mapped continuously onto each other by means of a one-to-one correspondence. Finally, consideration is given to the reasons why Netto's and Cantor's faulty proofs went unchallenged for twenty years, until Jürgens criticized them both in 1899...|$|E
40|$|Abstract. We give a new {{formula for}} the number of {{spanning}} trees of a graph and for its asymptotics. A special case answers a question of McKay (1983) for regular graphs. The general answer involves a quantity for infinite graphs that we call “tree entropy”, which we show is a logarithm of a normalized determinant of the graph Laplacian for infinite graphs. We relate tree entropy to the metric entropy of the uniform spanning forest process on quasi-transitive amenable graphs, correcting and extending a <b>faulty</b> <b>proof</b> of Burton and Pemantle (1993). § 1. Introduction. Methods of enumeration of spanning trees in a finite graph G and relations to various areas of mathematics and physics have been investigated for more than 150 years. The number of spanning trees is often called the complexity of the graph, denoted here by τ(G). The best known formula for the complexity, proved in every basic text on grap...|$|E
40|$|Zusammenfassung (dt.) Abstract: This thesis {{discusses}} {{several problems}} related to cycles and the independence number in graphs. Chapter 2 contains problems on independent sets of cycles. It is known {{that it is hard}} to compute the maximum cardinality of edge-disjoint and vertex-disjoint cycle packings, even if restricted to subcubic graphs. Therefore, the first section discusses the complexity of a simpler problem: packing cycles of fixed length l in graphs of maximum degree Delta. The results of Caprara and Rizzi, who have solved this problem for l= 3 are generalised to arbitrary lengths. The second section describes the structure of graphs for which the edge-disjoint resp. vertex-disjoint cycle packing number differs from the cyclomatic number by a constant. The corresponding classes of 2 -connected graphs can be obtained by a simple extension rules applied to a finite set of graphs. This result implies a fixed-parameter-tractability result for the edge-disjoint and vertex-disjoint cycle packing numbers. Chapter 3 contains an approximation of the minimum number of cycle lengths in a Hamiltonian graph with q chords. A family of examples shows that no more than the square root of q+ 1 can be guaranteed. The main result is that the square root of 4 / 7 *q cycle lengths can be guaranteed. The proof relies on a lemma by Faudree et al., which states that the graph that contains a path with endvertices x and y and q chords of equal length contains paths between x and y of at least q/ 3 different lengths. The first section corrects the originally <b>faulty</b> <b>proof</b> and derives additional bounds. The second section uses these bounds to derive the lower bound on the size of the cycle spectrum. Chapter 4 focuses on lower bounds on the independence ratio, i. e. the quotient of independence number and order of a graph, for graphs of given density. In the introduction, best-possible bounds both for arbitrary graphs and large connected graphs are derived from known results. Therefore, the rest of this chapter considers classes of graphs defined by forbidding small odd cycles as subgraphs. The main result of the first section is a generalisation of a result of Heckman and Thomas that determines the best possible lower bound for connected triangle-free graphs with average degree at most 10 / 3 and characterises the extremal graphs. The rest of the chapter is devoted to conjectures with similar statements on connected triangle-free graphs of average degree in [10 / 3, 54 / 13] and on connected graphs of odd girth 7 with average degree up to 14 / 5, similar conjectures for the bipartite ratio, possible classes of extremal graphs for these conjectures, and observations in support of the conjectures...|$|E
40|$|The {{simultaneous}} rigid E-unification {{problem is}} used in automated reasoning with equality. In our previous paper we proved the undecidability of this problem by reduction of monadic semiunification. Here we give a simpler and more intuitive proof of the undecidability by reduction of second-order unification. 1 Introduction Simultaneous rigid E-unification {{plays a crucial role}} in extending to first order languages with equality automatic proof methods based on sequent calculi, such as semantic tableaux [Fitting 88], the connection method [Bibel 82] (also known as the mating method [Andrews 81]), model elimination [Loveland 68] and a dozen other procedures. Simultaneous rigid E-unification was defined in [GaRaSn 87] (see also [GNRS 92]) and witnessed several <b>faulty</b> <b>proofs</b> of the decidability. The undecidability of simultaneous rigid E-unification was proven in [DeVo 95 b] by reduction of monadic semi-unification [Baaz 93]. Here we give a more elementary and more intuitive proof using [...] ...|$|R
40|$|Simultaneous rigid E-unification {{has been}} {{introduced}} {{in the area of}} theorem proving with equality. It is used in extension procedures, like the tableau method or the connection method. Many articles in this area assume the existence of an algorithm for simultaneous rigid E-unification. There were several <b>faulty</b> <b>proofs</b> of the decidability of this problem. In this paper we prove that simultaneous rigid E-unification is undecidable. As a consequence we obtain the undecidability of the 9 -fragment of intuitionistic logic with equality. 1 Introduction Simultaneous rigid E-unification plays a crucial role in extending to first order languages with equality automatic proof methods based on sequent calculi, such as semantic tableaux [Fitting 88], the connection method [Bibel 82] (also known as the mating method [Andrews 81]), model elimination [Loveland 68] and a dozen other procedures. The usability of simultaneous rigid E-unification has been explained in [Bibel 87, GaRaSn 87]. Since [...] ...|$|R
40|$|This paper {{reports the}} use of proof {{planning}} to diagnose errors in program code. In particular it looks at the errors that arise in the base cases of recursive programs produced by undergraduates. It describes two classes of error that arise in this situation. The use of test cases would catch these errors but would fail to distinguish between them. The system adapts proof critics, commonly used to patch <b>faulty</b> <b>proofs,</b> to diagnose such errors and distinguish between the two classes. It has been implemented in λClam, a proof planning system, and applied successfully to a small set of examples. The use of mathematical proof {{to show that a}} computer program meets its specification has a long history in Computer Science (e. g. [14, 13]). Considerable time and effort has been invested in creating computer-based tools to support the process of proving programs correct (e. g. [15, 8]). However the technique and tools are only used in very specialised situations in industry where programmers generally rely on testing and bug reports from users to assess {{the extent to which a}} program meets its specification...|$|R
40|$|In this paper, {{we present}} a scheme where a (d; 1) -dimensional subcube is {{allocated}} in a faulty d-dimensional circuit-switched hypercube {{in the presence of}} up to 2 (d; 1) faulty nodes. The scheme is then extended to allocate a (d; 1) -dimensional subcube {{in the presence of a}} combination of faulty nodes and <b>faulty</b> links. Theoretical <b>proofs</b> and simulation results are presented to analyze the performance of the scheme...|$|R
40|$|We {{report the}} {{discovery}} of an unexpected connection between {{the invention of the}} concept of uniform convergence and the occurs check in the unification algorithm. This discovery suggests the invention of further interesting concepts in analysis and a technique for automated concept formation. Part of this technique has been implemented. The discovery arose as part of an attempt to understand the role of proof analysis in mathematical reasoning, so as to incorporate it into a computer program. Silver (1986) and Mitchell (1983) have investigated the automatic analysis of model proofs in order to extract and learn knowledge about controlling search, including the knowledge of new concepts. We focus on the analysis and correction of <b>faulty</b> <b>proofs</b> or 'poofs'. especially where that correction involves the invention of new mathematical concepts. A classic example of where the analysis of a poof leads to a new concept is the invention, by Weierstrass, Seidel, Cauchy and others, of uniform convergence {{as a result of an}} analysis of Cauchy's poof that the limit of a * A 'poor, according to one of my mathematics lecturers, is a proof with something vital missing. Artificial Intelligence and its Applications, edited by A. G. Cohn and J. R. Thomas @ A. Bundy 1986, Published by John Wiley & Sons Ltd. 52 ARTIFICIAL INTELLIGENCE AND ITS APPLICATIONS convergent series of continuous functions is itself continuous. The correction consists of substituting in the theorem the new concept of 'uniformly convergent' for 'convergent'. We will investigate this example. The bug in Cauchy's poof is a violation of the occurs check. This observation suggests a technique for automatically correcting the proof and this leads to the concept of uniform convergence. The correction technique involves search. Other branches in the search space lead to alternative corrections and alternative concepts, one of which seemed interesting to me and turns out to have been discovered by the mathematician, Ascoli. 1...|$|R
40|$|Abstract. There is {{something}} of a discontinuity at the heart of popular tactical theorem provers. Low-level, fully-checked mechanical proofs are large trees consisting of primitive logical inferences. Meanwhile, high-level human inputs are lexically structured formal texts which include tactics describing search procedures. The proof checking process maps from the high-level to low-level, but after that, explicit connections are usually lost. The lack of connection can make it difficult to understand the proof trees produced by successful tactic proofs, and difficult to debug <b>faulty</b> tactic <b>proofs.</b> We propose the use of hierarchical proofs, also known as hiproofs, to help bridge these levels. Hiproofs superimpose a labelled hierarchical nesting on an ordinary proof tree, abstracting from the underlying logic. The labels and nesting are used to describe the organisation of the proof, typically relating to its construction process. In this paper we introduce a foundational tactic language Hitac which constructs hiproofs in a generic setting. Hitac programs can be evaluated using a big-step or a small-step operational semantics. The big-step semantics captures the intended meaning, whereas the small-step semantics is closer to possible implementations and provides a unified notion of proof state. We prove that the semantics are equivalent and construct valid proofs. We also explain how to detect terms which are stuck in the small-step semantics, and how these suggest interaction points with debugging tools. Finally we show some typical examples of tactics, constructed using tactical combinators, in our language...|$|R
40|$|The {{question}} of how teachers can mediate self-regulated learning in college students is discussed. For {{the purposes of this}} study, self-regulated learning is defined as the ongoing process in which the learner makes sense of the learning task, creates goals and strategies, and implements actions designed to meet goals for the given learning context. Learning oriented students see faulty performances as an indication that learning strategies need improvement, while performance oriented students see <b>faulty</b> performance as <b>proof</b> of their inadequacies. Convincing learning oriented students to improve their self-regulated learning is fairly easy. Performance oriented students may actually avoid such feedback from teachers. Increasing student motivation lies in eliciting responses about behaviors and helping students to think through the strategies involved. Challenging students to find within themselves the energy to improve by themselves not only assists them by teaching self-regulated learning, it offers them the tools to become educated, responsible adults. A practical guide, "Four Steps to Working Smarter, not Harder, " is appended. (Contains 24 references.) (LH) * Reproductions supplied by EDRS are the best that can be made * * from the original document. ...|$|R
40|$|In the 1920 s, David Hilbert {{proposed}} a research {{program with the}} aim of providing mathematics with a secure foundation. This was to be accomplished by first formalizing logic and mathematics in their entirety, and then showing [...] -using only so-called finitistic principles [...] -that these formalizations are free of contradictions.;In the area of logic, the Hilbert school accomplished major advances both in introducing new systems of logic, and in developing central metalogical notions, such as completeness and decidability. The analysis of unpublished material presented in Chapter 2 shows that a completeness proof for propositional logic was found by Hilbert and his assistant Paul Bernays already in 1917 [...] 18, and that Bernays's contribution was much greater than is commonly acknowledged. Aside from logic, the main technical contribution of Hilbert's Program are the development of formal mathematical theories and proof-theoretical investigations thereof, in particular, consistency proofs. In this respect Wilhelm Ackermann's 1924 dissertation is a milestone both {{in the development of the}} Program and in proof theory in general. Ackermann gives a consistency proof for a second-order version of primitive recursive arithmetic which, surprisingly, explicitly uses a finitistic version of transfinite induction up to www. He also gave a <b>faulty</b> consistency <b>proof</b> for a system of second-order arithmetic based on Hilbert's 2 ̆ 6 egr;-substitution method. Detailed analyses of both proofs in Chapter 3 shed light on the development of finitism and proof theory in the 1920 s as practiced in Hilbert's school.;In a series of papers, Charles Parsons has attempted to map out a notion of mathematical intuition which he also brings to bear on Hilbert's finitism. According to him, mathematical intuition fails to be able to underwrite the kind of intuitive knowledge Hilbert thought was attainable by the finitist. It is argued in Chapter 4 that the extent of finitistic knowledge which intuition can provide is broader than Parsons supposes. According to another influential analysis of finitism due to W. W. Tait, finitist reasoning coincides with primitive recursive reasoning. The acceptance of non-primitive recursive methods in Ackermann's dissertation presented in Chapter 3, together with additional textual evidence presented in Chapter 4, shows that this identification is untenable as far as Hilbert's conception of finitism is concerned. Tait's conception, however, differs from Hilbert's in important respects, yet it is also open to criticisms leading to the conclusion that finitism encompasses more than just primitive recursive reasonin...|$|R
40|$|Abstract. In 1993, {{the authors}} {{presented}} a fixed point theorem of Meir-Keeler type. The proposed {{proof of a}} lemma—on which the said theorem depends on—is invalid. In this note, we alter the statement of this lemma and give a valid proof thereof, so that the main result of the previous paper is still true. Keywords and phrases. Compatiblemaps of type (A), a generalized (,δ) -{S,T}-contraction, common fixed point. 2000 Mathematics Subject Classification. Primary 54 H 25, 47 H 10. In 1993, we introduced the concept of compatible maps of type (A) and “proved” the following theorem. Theorem 1 [1, Theorem 3. 2]. Let A,B,S and T be mappings of a complete metric space (X,d). Suppose that the pair {A,B} is a generalized (,δ) -{S,T}-contraction with δ lower semi-continuous. If the following conditions are satisfied: (i) one of A,B,S or T is continuous, and (ii) the pairs A,S and B,T are compatible of type (A) on X, then A,B,S and T have a unique common fixed point in X. The purpose of this note {{is to ensure that}} the above is indeed true. This is necessary since the proof of Theorem 1 relies on Lemma 3. 1 in [1]. However, the proof of part (1) of this lemma is <b>faulty</b> and the <b>proof</b> of part (2) is not “tight. ” In the following, we provide a thorough and complete proof of Lemma 4 below which is a “reshuffled and revamped ” version of Lemma 3. 1 in [1]. This accomplishes ourmission, since the proof of Theorem 1 is valid if the lemma is true. The proof of part (3) of Lemma 4 below is much like the proof of Lemma 3. 1 (c) in [2] with minor initial modifications. We include all the proof of part (3) for ease of reading and completeness sake. We need the following definitions given in [1]. Definition 2 [2]. Let A,B,S and T be mappings of a metric space (X,d) into itself such that A(X) ⊂ T(X) and B(X) ⊂ S(X). For x 0 ∈X, any sequence {yn} defined by y 2 n− 1 =Tx 2 n− 1 =Ax 2 n− 2, y 2 n =Sx 2 n = Bx 2 n− 1 (1) forn∈N (the set of positive integers) is called an {S,T}-iteration of x 0 underA and B. The following definition was given in [1], but erroneously required that δ() <. 508 Y. J. CHO ET AL. Definition 3. Let A,B,S and T be mappings of a metric space (X,d) into itself. The pair {A,B} is called a generalized (,δ) -{S,T}-contraction i...|$|R

