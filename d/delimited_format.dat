11|22|Public
50|$|Formats {{that use}} delimiter-separated values (also DSV) store {{two-dimensional}} arrays of data by separating the values in each row with specific delimiter characters. Most database and spreadsheet programs {{are able to}} read or save data in a <b>delimited</b> <b>format.</b> Due to their wide support, DSV files {{can be used in}} data exchange among many applications.|$|E
50|$|Data {{could be}} moved between these {{programs}} relatively easily by using comma <b>delimited</b> <b>format</b> files (now more {{commonly known as}} CSV files), which enhanced {{the utility of the}} package. The manuals assumed no computer background, the programs were straightforward to use, and thus it was possible to find the CEO of a small company developing the applications needed in-house.|$|E
50|$|The file {{is divided}} into 5 sections: Start, Global, Directory Entry, Parameter Data, and Terminate {{indicated}} by the characters S, G, D, P, or T in column 73. The characteristics and geometric information for an entity is split between two sections; one in a two record, fixed-length format (the Directory Entry, or DE Section), {{the other in a}} multiple record, comma <b>delimited</b> <b>format</b> (the Parameter Data, or PD Section), {{as can be seen in}} a more human-readable representation of the file.|$|E
50|$|Note {{that since}} netstrings pose no {{limitations}} on {{the contents of the}} data they store, netstrings can not be embedded verbatim in most <b>delimited</b> <b>formats</b> without the possibility of interfering with the delimiting of the containing format.|$|R
50|$|Netstrings avoid {{complications}} {{that arise in}} trying to embed arbitrary data in <b>delimited</b> <b>formats.</b> For example, XML may not contain certain byte values and requires a nontrivial combination of escaping and delimiting, while generating multipart MIME messages involves choosing a delimiter that must not clash with {{the content of the}} data.|$|R
2500|$|... $ {{is used in}} the ALGOL 68 {{language}} to <b>delimit</b> transput <b>format</b> regions.|$|R
5000|$|Version 4.2 (Oct 3rd, 2016) of PHI-base {{provides}} information on 4460 genes from 264 pathogens and 176 hosts {{and their impact on}} 8046 interactions as well on efficacy information on ~20 drugs and the target sequences in the pathogen. PHI-base currently focuses on plant pathogenic and human pathogenic organisms including fungi, oomycetes and bacteria. The entire contents of the database can be downloaded in a tab <b>delimited</b> <b>format.</b> Since 2015 the web-site includes an online literature curation tool called PHI-Canto for community literature curation of various pathogenic species. Since the launch of version 4, the PHI-base is also searchable using the PHIB-BLAST search tool, which uses the BLAST algorithm to compare a user's sequence against the sequences available from PHI-base. In 2016 the plant portion of PHI-base was used to establish a Semantic PHI-base search tool" ...|$|E
40|$|Library to {{interact}} with Kripo fragment, fingerprint and similarity data files. KRIPO stands for Key Representation of Interaction in POckets, see reference for more information. Added 	Added sub commands to read/write distance matrix in tab <b>delimited</b> <b>format</b> (# 13) 	Created repo for Knime example and plugin at [URL] (# 8...|$|E
40|$|The World Glacier Inventory {{contains}} {{information for}} over 67, 000 glaciers through out the world. Parameters within the inventory include geographic location, area, length, orientation, elevation and classification of morphological type and moraines. The inventory entries {{are based upon}} a single observation in time and {{can be viewed as}} a "snapshot" of the glacier at this time. The database can be searched by geographic region or by entering one or more parameters such as glacier name, location, elevation, size or type of glacier. Data output is in comma <b>delimited</b> <b>format.</b> Educational levels: Graduate or professional, Undergraduate lower division, Undergraduate upper division...|$|E
5000|$|CSV is a <b>delimited</b> data <b>format</b> {{that has}} fields/columns {{separated}} by the comma character and records/rows terminated by newlines.|$|R
50|$|Structured {{data are}} most often {{produced}} in <b>delimited</b> text <b>format.</b> When the number of tables subject to discovery is large or relationships between the tables are of essence, the data are produced in native database format or as a database backup file.|$|R
40|$|A {{recurrent}} {{issue in}} medical information exchange standards {{has been the}} relative merits of <b>delimited</b> message <b>formats</b> versus tagged message formats. In view of this ongoing discussion, we report here our experience in implementing a Laboratory Information System to Hospital Information System bidirectional interface using CCITT protocols X. 25 and X. 409. Use of vendor supplied X. 25 software resulted in shorter and simpler application programs. Use of X. 409 resulted in a 26 % increase in message size compared to a <b>delimited</b> record <b>format</b> but provided greater flexibility in the transmission of encoded data. Use of standards will become even more attractive when commercial vendors make available X. 409 software packages that match the quality of the current X. 25 implementations...|$|R
30|$|Data {{collection}} in Repast Simphony {{consists of three}} phases, namely initialization, recording, and termination. Initialization and termination occur once per run, whereas recording occurs at some regular interval (e.g., once every three ticks). Crucial to the proper functioning of data collection is specification {{of a set of}} notifications associated with these phases. For example, when writing a row of data in tabular <b>delimited</b> <b>format,</b> it is necessary to notify the writer that all of the data for that row has been collected and can now be written. Similarly, a histogram component requires notification that the record phase has ended and that the histogram should now be produced with the recorded data.|$|E
40|$|PhyloTrack is a JavaScript-based {{software}} tool that integrates the D 3. js library for data visualization with the JBrowse tool for genome browser representation. It requires a phylogenetic tree {{of the common}} Newick data format as input, as well as three meta data files for samples, clade-defining nodes and clade color definitions - all in tab <b>delimited</b> <b>format.</b> Functionality within PhyloTrack shows the informative markers at each node in the phylogenetic tree, therefore highlighting clade-defining polymorphism. This functionality has been implemented using the tabix tool on the server side, providing simple and rapid access to the information at each tree node, including informative SNPs stored in VCF-similar files. These informative variants have been established by comparing allele frequencies between strain-types using ancestral node comparisons and FST measures of population differentiation...|$|E
40|$|Abstract Background Functional {{genomics}} {{involves the}} parallel experimentation with large sets of proteins. This requires management of large sets of open reading frames {{as a prerequisite}} of the cloning and recombinant expression of these proteins. Results A Java program was developed for retrieval of protein and nucleic acid sequences and annotations from NCBI GenBank, using the XML sequence format. Annotations retrieved by ORFer include sequence name, organism and also the completeness of the sequence. The program has a graphical user interface, although {{it can be used}} in a non-interactive mode. For protein sequences, the program also extracts the open reading frame sequence, if available, and checks its correct translation. ORFer accepts user input in the form of single or lists of GenBank GI identifiers or accession numbers. It can be used to extract complete sets of open reading frames and protein sequences from any kind of GenBank sequence entry, including complete genomes or chromosomes. Sequences are either stored with their features in a relational database or can be exported as text files in Fasta or tabulator <b>delimited</b> <b>format.</b> The ORFer program is freely available at [URL]. Conclusion The ORFer program allows for fast retrieval of DNA sequences, protein sequences and their open reading frames and sequence annotations from GenBank. Furthermore, storage of sequences and features in a relational database is supported. Such a database can supplement a laboratory information system (LIMS) with appropriate sequence information. </p...|$|E
5000|$|Translation memory {{management}}: Creation {{and basic}} management of databases for multilingual (in {{the case of}} memoQ, bilingual) translation information in units known as [...] "segments". This information is often exchanged between translation management and assistance systems using the file format TMX. memoQ is also able to import translation memory data in <b>delimited</b> text <b>format.</b>|$|R
50|$|Typically a <b>delimited</b> file <b>format</b> is {{indicated}} by a specification. Some specifications provide conventions for avoiding delimiter collision, others do not. Delimiter collision {{is a problem that}} occurs when a character that is intended as part of the data gets interpreted as a delimiter instead. Comma- and space-separated formats often suffer from this problem, since in many contexts those characters are legitimate parts of a data field.|$|R
50|$|Terminology {{management}}: Storage {{and management}} of terminology and meta information about the terminology to assist in translation or quality assurance. memoQ is able to import terminology data in TMX and <b>delimited</b> text <b>formats</b> and export it in delimited text and an XML format. memoQ also includes an integrated facility for statistical terminology extraction from a chosen combination of documents to translate, translation memory databases and reference corpora.|$|R
40|$|There is {{an urgent}} need to {{standardize}} the semantics of biomedical data values, such as phenotypes, to enable comparative and integrative analyses. However, {{it is unlikely that}} all studies will use the same data collection protocols. As a result, retrospective standardization is often required, which involves matching of original (unstructured or locally coded) data to widely used coding or ontology systems such as SNOMED CT (clinical terms), ICD- 10 (International Classification of Disease) and HPO (Human Phenotype Ontology). This data curation process is usually a time-consuming process performed by a human expert. To help mechanize this process, we have developed SORTA, a computer-aided system for rapidly encoding free text or locally coded values to a formal coding system or ontology. SORTA matches original data values (uploaded in semicolon <b>delimited</b> <b>format)</b> to a target coding system (uploaded in Excel spreadsheet, OWL ontology web language or OBO open biomedical ontologies format). It then semi-automatically shortlists candidate codes for each data value using Lucene and n-gram based matching algorithms, and can also learn from matches chosen by human experts. We evaluated SORTA's applicability in two use cases. For the LifeLines biobank, we used SORTA to recode 90 000 free text values (including 5211 unique values) about physical exercise to MET (Metabolic Equivalent of Task) codes. For the CINEAS clinical symptom coding system, we used SORTA to map to HPO, enriching HPO when necessary (315 terms matched so far). Out of the shortlists at rank 1, we found a precision/recall of 0. 97 / 0. 98 in LifeLines and of 0. 58 / 0. 45 in CINEAS. More importantly, users found the tool both a major time saver and a quality improvement because SORTA reduced the chances of human mistakes. Thus, SORTA can dramatically ease data (re) coding tasks and we believe it will prove useful for many more projects...|$|E
5000|$|TreeLine {{outlines}} can be exported as HTML, per-data type {{user defined}} formatting. In addition, it supports exporting outlines as an OpenDocument ODT file, various <b>delimited</b> text file <b>formats,</b> and as [...] "plain" [...] XML.|$|R
50|$|The {{translation}} memory (TM) format of memoQ is proprietary and stored {{as a group}} of files in a folder bearing the name of the {{translation memory}}. External data can be imported in <b>delimited</b> text <b>formats</b> and Translation Memory eXchange format (TMX), and translation memory data can be exported as TMX. memoQ can also work with server-based translation memories on the memoQ Server or, using a plug-in, other external translation memory sources. memoQ Translation memories are bilingual.|$|R
5000|$|Glossaries {{are handled}} by the {{integrated}} terminology module. Glossaries can be imported in TMX or <b>delimited</b> text <b>formats</b> and exported as delimited text or MultiTerm XML. Glossaries can include two or more languages or language variants. Term matching with glossary entries can be based on many different parameters, taking into consideration capitalization, partial or fuzzy matches and other factors. Terms to be avoided can be marked as [...] "forbidden" [...] in the properties of a particular glossary entry.|$|R
30|$|The {{motivation}} for the tabular format was two-fold. First, {{we wanted to be}} able to load data into a variety of standard databases easily, and a tabular format made that relatively straightforward. Second, we wanted the state of agents to be editable offline. When saved to a <b>delimited</b> file <b>format,</b> the state of the simulation and its agents can be loaded into common tools, such as Microsoft Excel, and easily edited. Moreover, additional objects (e.g., agents) can be created offline by adding rows to the relevant table.|$|R
40|$|We model {{a scheme}} for the {{coherent}} control of light waves and currents in metallic nanospheres which applies {{independently of the}} nonlinear multiphoton processes at the origin of waves and currents. Using exact mathematical formulae, we calculate numerically with a custom fortran code the effect of an external control field which enable us to change the radiation pattern and suppress radiative losses or to reduce absorption, enabling the particle to behave as a perfect scatterer or as a perfect absorber. Data are provided in tabular, comma <b>delimited</b> value <b>format</b> and illustrate narrow features in {{the response of the}} particles that result in high sensitivity to small variations in the local environment, including subwavelength spatial shifts...|$|R
40|$|GEOROC (Geochemistry of Rocks of the Oceans and Continents) is {{intended}} to become a global geochemical database containing published chemical and isotopic data as well as extensive metadata for rocks, minerals and melt/fluid inclusions. GEOROC currently covers igneous rocks from island arcs, oceanic islands, and large igneous provinces such as seamounts, oceanic plateaus, submarine ridges, and oceanic and continental flood basalts. It contains thousands of entries for whole rock, mineral, and inclusion geochemical data. The database can be searched by bibliography, tectonic setting, geographic coordinates, chemistry, or petrography. The results of a customized query can be directly downloaded in an Excel compatible (comma <b>delimited</b> text) <b>format.</b> Educational levels: Graduate or professional, Undergraduate lower division, Undergraduate upper division...|$|R
40|$|Experimental data {{generated}} by the Very High Temperature Reactor Program {{need to be more}} available to users in the form of data tables on Web pages that can be downloaded to Excel or in <b>delimited</b> text <b>formats</b> that can be used directly for input to analysis and simulation codes, statistical packages, and graphics software. One solution that can provide current and future researchers with direct access to the data they need, while complying with records management requirements, is the Nuclear Data Management and Analysis System (NDMAS). This report describes the NDMAS system and its components, defines roles and responsibilities, describes the functions the system performs, describes the internal processes the NDMAS team uses to carry out the mission, and describes the hardware and software used to meet Very High Temperature Reactor Program needs...|$|R
30|$|Future radio {{transceivers}} {{supporting the}} software radio concept will provide increased features for radio access networks. However, the reconfiguration of radio equipment requires {{the existence of}} an architecture, a common framework, which allows the flexible management of software running on radio processors. Such a framework must take into account the heterogeneity of hardware devices and platforms for radio applications. Since the flexibility has a cost in terms of added overhead, a conceptually simple but efficient structure that allows powerful mechanisms to develop and deploy software radio applications is required. This paper describes our approach, the reasons that motivated it, and some implementation issues. The proposed framework is essentially based on four items: an abstraction layer which hides any platform-dependent issue, a simple time-driven software structure, a <b>delimited</b> interface <b>format</b> for software blocks which does not actually constrain communication, and a global time-reference mechanism to guarantee real-time behaviour.|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. This Special Licence access dataset contains names and addresses from the entire Integrated Census Microdata (I-CeM) dataset of the censuses of Great Britain for the period 1851 to 1911. It is available in ASCII <b>delimited</b> text <b>format</b> only. The anonymised main I-CeM database that complements these names and addresses is available under SN 7481. It comprises the Censuses of Great Britain for the period 1851 - 1911; data are available for England and Wales for 1851 - 1861 and 1881 - 1911 and for Scotland for 1851 - 1901. The database contains over 180 million individual census records and was digitised and harmonised from the original census enumeration books. It details characteristics for all individuals resident in Great Britain at each census from 1851 - 1911. The original digital data has been coded and standardised; the I-CeM database has consistent geography over time and standardised coding schemes for many census variables. This dataset of names and addresses for individual census records is organised per country (England and Wales; Scotland) and per census year. Within each data file each census record contains first and last name, street address and an individual identification code that allows linking with the corresponding anomymised I-CeM record. These data are made available under Special Licence (SL) access conditions due to commercial sensitivity. Data cannot be used for true linking of individual census records across census years for commercial genealogy purposes nor for any other commercial purposes. The SL arrangements are required to ensure that commercial sensitivity is protected. For information on making an application, see the 'Administrative and Access Information' section below. Further information about I-CeM {{can be found on}} the I-CeM Integrated Microdata Project and I-CeM Guide webpages. Main Topics : Names, addresses and matching identifiers for the I-CeM database...|$|R
40|$|Abstract Background Protein-protein {{interactions}} (PPIs) play {{a crucial}} role in initiating infection in a host-pathogen system. Identification of these PPIs is important for understanding the underlying biological mechanism of infection and identifying putative drug targets. Database resources for studying host-pathogen systems are scarce and are either host specific or dedicated to specific pathogens. Results Here we describe "HPIDB” a host-pathogen PPI database, which will serve as a unified resource for host-pathogen interactions. Specifically, HPIDB integrates experimental PPIs from several public databases into a single, non-redundant web accessible resource. The database can be searched with a variety of options such as sequence identifiers, symbol, taxonomy, publication, author, or interaction type. The output is provided in a tab <b>delimited</b> text file <b>format</b> that is compatible with Cytoscape, an open source resource for PPI visualization. HPIDB allows the user to search protein sequences using BLASTP to retrieve homologous host/pathogen sequences. For high-throughput analysis, the user can search multiple protein sequences at a time using BLASTP and obtain results in tabular and sequence alignment formats. The taxonomic categorization of proteins (bacterial, viral, fungi, etc.) involved in PPI enables the user to perform category specific BLASTP searches. In addition, a new tool is introduced, which allows searching for homologous host-pathogen interactions in the HPIDB database. Conclusions HPIDB is a unified, comprehensive resource for host-pathogen PPIs. The user interface provides new features and tools helpful for studying host-pathogen interactions. HPIDB can be accessed at [URL]. </p...|$|R
40|$|The EcoTrends Project {{began in}} 2004 {{to promote and}} enable the use of {{long-term}} data to better understand processes within the Earth’s ecosystems. Collected by local, {{state and federal agencies}} and institutions, these data are quality checked and then simplified into a common data format, called “derived data”, for the EcoTrends database. A large-format book containing numerous time-series plots, along with vignettes of the derived data, will be published in 2008. Foresight by project coordinators realized the importance of also providing these data via the World Wide Web. A web-based portal would make possible the use of on-line tools that would streamline the discovery and exploration of these data, {{while at the same time}} facilitate adding new time-series data to the growing EcoTrends database. The EcoTrends Web Portal is now at its first milestone in production and includes features for data discovery and access by registered users, plotting both derived and smoothed data values, downloading both summary and statistically annotated data in comma <b>delimited</b> and HTML <b>formats,</b> and saving markers to high-value data in an on-line store that simplifies future access. The EcoTrends Web Portal utilizes the Provenance Aware Synthesis Tracking Architecture framework being developed by the LTER Network Office. This framework combines community developed open source tools, such as Metacat and Ecological Metadata Language, into a data warehouse workflow system that, when fully operational, will automate the extraction and uploading of site-based data into a permanent and persistent archive that can be utilized by synthesis projects. ...|$|R

