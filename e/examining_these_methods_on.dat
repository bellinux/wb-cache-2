0|10000|Public
40|$|Tasks {{of image}} {{clustering}} and classification often deal with data {{of very high}} dimensions. To alleviate the dimensionality curse, several methods, such as Isomap, LLE and KPCA, have recently been proposed and applied to learn low-dimensional non-linear embedded manifolds in high-dimensional spaces. Unfortunately, the scenarios in which <b>these</b> <b>methods</b> appear to be effective are very contrived. In this work, we empirically <b>examine</b> <b>these</b> <b>methods</b> <b>on</b> a realistic but not-so-difficult dataset. We discuss the promises and limitations of these dimensionreduction schemes. 1...|$|R
40|$|Recursive {{branching}} network (RBN) was proposed in [1] to solve linearly non-separable problems using output-coded perceptrons. It relies on splitting the training patterns, at random, between parallel perceptrons. However, the random splitting mechanism can trap the perceptron in conflicting patterns. Optimized splitting methods are proposed here to insure meaningful way of splitting. We propose three splitting methods which use different similarity measures between patterns. We <b>examine</b> <b>these</b> <b>methods</b> <b>on</b> five standard data sets. In general, <b>these</b> <b>methods</b> enhance {{the performance of}} RBN {{and in many cases}} contribute to lowering the network complexity...|$|R
40|$|The {{differential}} equation is mathematical tool widely used for description various linear or nonlinear systems and behaviour {{in the nature}} {{not only in the}} industry. The numerical solution of the {{differential equation}} is basic tool of the modelling and simulation procedure. There are various types of numerical methods, the ones described in this contribution comes from the Taylor’s series and big advantage of all of them is in easy programmability or even more some of them are included as a build-in functions in mathematical softwares such as Mathematica or MATLAB. The goal of this contribution is to show how proposed Euler and Runge-Kutta’s methods could be programmed and implemented into MATLAB and <b>examine</b> <b>these</b> <b>methods</b> <b>on</b> various examples. The comparable parameters are accuracy and also speed of the computation...|$|R
40|$|In this study, {{the authors}} {{compared}} logistic regression and predictive data mining {{techniques such as}} decision trees (DTs), artificial neural networks (ANNs), and support vector machines (SVMs), and <b>examined</b> <b>these</b> <b>methods</b> <b>on</b> whether they could discriminate between adolescents who were charged or not charged for initial juvenile offending in a large Asian sample. Results were validated and tested in independent samples with logistic regression and DT, ANN, and SVM classifiers achieving accuracy rates of 95 % and above. Findings from receiver operating characteristic analyses also supported these results. In addition, the authors examined distinct patterns of occurrences within and across classifiers. Proactive aggression and teacher-rated conflict consistently emerged as risk factors across validation and testing data sets of DT and ANN classifiers, and logistic regression. Reactive aggression, narcissistic exploitativeness, being male, and coming from a nonintact family were risk factors that emerged {{in one or more}} of these data sets across classifiers, while anxiety and poor peer relationships failed to emerge as predictors...|$|R
40|$|ABSTRACT In this paper, {{we report}} our {{experiments}} using a real-world image dataset {{to examine the}} effectiveness of Isomap, LLE and KPCA. The 1, 897 -image dataset we used consists of 14 image categories. We have used this dataset in several settings, both supervised and unsupervised, and have {{found it to be}} relatively “well behaved ” (clusters do exist in a lower-dimensional space) compared to many other real-world datasets we have used. We did not use a “harder ” database because all dimension-reduction methods would have failed miserably, and we {{would not be able to}} observe, identify, and explain the limitations of manifold learning. Tasks of image clustering and classification often deal with data of very high dimensions. To alleviate the dimensionality curse, several methods, such as Isomap, LLE and KPCA, have recently been proposed and applied to learn low-dimensional, non-linear embedded manifolds in high-dimensional spaces. Unfortunately, the scenarios in which <b>these</b> <b>methods</b> appear to be effective are very contrived. In this work, we empirically <b>examine</b> <b>these</b> <b>methods</b> <b>on</b> a realistic but not-so-difficult dataset. We discuss the promises and limitations of these dimension-reduction schemes. The rest of this paper is organized into three sections. Section 2 briefly summarizes Isomap, LLE, and KPCA. Section 3 presents the results of our empirical studies. We offer our observations and concluding remarks i...|$|R
30|$|The {{analysis}} of soil–foundation–structure interaction {{can be carried}} out by different <b>methods,</b> depending <b>on</b> the part of the system that is <b>examined.</b> <b>These</b> <b>methods</b> can be classified into: (1) analytical, usually referring to simple foundation geometries lying on elastic half-space; (2) semi-analytical, combining analytical formulations for the half-space with numerical procedures; (3) numerical, usually FEM; (4) simplified discrete models, which allow fast calculation of the foundation–soil–structure system properties.|$|R
40|$|In my bachelor's thesis I'm {{comparing}} {{three different}} website presentations of Stavmont company. Each {{of the proposals}} - Red, Green and Blue I have created by different technology. I will closer <b>examine</b> <b>these</b> <b>method</b> of website development, establish {{advantages and disadvantages of}} selected solution and suggest the best variant to the company for its marketing presentation...|$|R
30|$|From {{the methods}} tested, we {{selected}} three feature filters which included Chi-squared, ANOVA, and mutual information. The advantages of using these feature selection techniques are their speed, scalability and their {{independence of the}} classification. Our reasoning for choosing <b>these</b> <b>methods</b> is their {{ability to deal with}} sparse data. On the other hand, <b>these</b> <b>methods</b> have some drawbacks as well, they ignore feature dependencies and also they ignore interaction with the classifier [69]. In this section, we <b>examine</b> <b>these</b> <b>methods</b> and the results of applying them to our dataset.|$|R
40|$|Swedish text Several POS-taggers {{are trained}} and tested <b>on</b> Swedish text. <b>Methods</b> {{to improve the}} {{accuracy}} of the tagging are then <b>examined.</b> <b>These</b> <b>methods</b> include voting, letting taggers change their voting contribution depending on how confident they are and training a new second level classifier on the output of the taggers. All <b>these</b> <b>methods</b> are more accurate than the most accurate original tagger, with 15 % less errors or better. Which types of errors <b>these</b> <b>methods</b> correct and which types remain are also examined. The number of errors in some common error categories actually increase, while many uncommon errors are corrected...|$|R
40|$|In this thesis, we have {{developed}} two numerical methods for evaluating option prices under the regime switching model of stock price processes: the Finite Difference lattice method and the Monte Carlo lattice method. The Finite Difference lattice <b>method</b> is based <b>on</b> the explicit finite difference scheme for parabolic problems. The Monte Carlo lattice <b>method</b> is based <b>on</b> the simulation of the Markov chain. The advantage of <b>these</b> <b>methods</b> is their flexibility to compute the option prices for any given stock price at any given time. Numerical examples are presented to <b>examine</b> <b>these</b> <b>methods.</b> It {{has been shown that}} the proposed methods provides fast and accurate approximations of option prices. Hence they should be helpful for practitioners working in this field...|$|R
50|$|Decoding methods like belief {{propagation}} {{are also}} used for low-density parity-check codes are of special interest. Performance analysis of <b>these</b> <b>methods</b> <b>on</b> sudoku codes can help to understand decoding problems for low-density parity-check codes better.|$|R
30|$|The second {{principle}} {{task was}} to perform an initial test of <b>these</b> <b>methods</b> <b>on</b> both real life plankton data of high contour line variability and a synthetic sample data with similar intra-class structure and symmetry.|$|R
40|$|This paper {{illustrates}} exploration {{processes of}} developing a toolkit that would help designers in incorporating shape-change technologies into their design. We propose three unique methods for designers to program motions into shape-changing interfaces without specific technical knowledge. An exploratory toolkit was created to <b>examine</b> <b>these</b> <b>methods</b> within design contexts. The tool- kit was shown {{to a group of}} design students with various technical knowledge and asked to explore the potentials of the toolkit, The means to create motion without programming skills proved to be an empowering tool for designers with a little technical knowledge, in the process of prototyping with actuators and sensors...|$|R
40|$|The {{contribution}} {{contains a}} description of two different methods for discrete minimax approximation. <b>These</b> <b>methods</b> {{can be applied to}} electrical filter design. We demonstrate properties of <b>these</b> <b>methods</b> <b>on</b> the optimal design of a fourth order band-pass filter. Numerical results of our computational experiments and corresponding conclusions are presented...|$|R
40|$|This paper {{presents}} some {{relations between}} noncommutative symmetric functions and free Lie algebras. We show in particular how to decompose a Lie projector of Solomon's descent algebra on Dynkin's and Klyachko's bases. We illustrate <b>these</b> <b>methods</b> <b>on</b> {{the example of}} the Hausdorff series...|$|R
40|$|In {{this paper}} we present our ideas for {{conducting}} a {{cost benefit analysis}} by using three different methods: scenario analysis, decision trees and simulation. Then we introduce our case study and <b>examine</b> <b>these</b> <b>methods</b> in a real world situation. We show how these tools can be used and what the results are for each of them. Our aim is to conduct a comparison of <b>these</b> different probabilistic <b>methods</b> of estimating costs for port security risk assessment studies. Methodologically, {{we are trying to}} understand the limits of all the tools mentioned above by focusing on rare events. Comment: International Workshop of Applied Modelling and Simulation (WAMS), 5 - 7 May, Buizos, Brasil, 201...|$|R
40|$|Diploma thesis {{deals with}} the {{strategic}} company analysis. First part is about charac-terisation of strategy and methods of internal and external strategic analysis. <b>These</b> <b>methods</b> are PEST analysis, 4 C analysis, sector analysis, Porter's five forces model, analysis of business resources and financial analysis. Second {{part of the work}} is practical aplication of <b>these</b> <b>methods</b> <b>on</b> the specific business entity...|$|R
40|$|Application of {{numerical}} optimization {{techniques to}} automated conceptual aircraft design is <b>examined.</b> <b>These</b> <b>methods</b> are {{shown to be}} a general and efficient way to obtain quantitative information for evaluating alternative new vehicle projects. Fully automated design is compared with traditional point design methods and time and resource requirements for automated design are given. The NASA Ames Research Center aircraft synthesis program (ACSYNT) is described with special attention to calculation of the weight of a vehicle to fly a specified mission. The ACSYNT procedures for automatically obtaining sensitivity of the design (aircraft weight, performance and cost) to various vehicle, mission, and material technology parameters are presented. Examples are used to demonstrate the efficient application of these techniques...|$|R
40|$|Poster. Colloque avec actes et comité de lecture. internationale. International audienceBased on an {{application}} of symbolic data mining <b>methods</b> <b>on</b> a test database, we underline {{the role played by}} the analyst in the knowledge discovery process. Encouraged by positive results, we plan to apply <b>these</b> <b>methods</b> <b>on</b> a large database for investigating the relationships between gene polymorphisms and cardiovascular diseases intermediate phenotypes...|$|R
40|$|We {{consider}} multiple resolution {{methods for}} filtering and segmenting multispectral astronomical images. For filtering, we use noise modeling, wavelet transform, and the Karhunen-Loeve transform. For segmentation, {{we use a}} quadtree followed by the fitting of a Markov model. We illustrate <b>these</b> <b>methods</b> <b>on</b> Hubble Space Telescope near infrared NICMOS camera images...|$|R
40|$|We {{present the}} results of an {{unbiased}} search for Lyα emission from continuum-selected 6 4 σ) emission lines using two different automated detection methods, free of any visual inspection biases. Applying <b>these</b> <b>methods</b> <b>on</b> photometrically-selected high-redshift candidates between 6 7 (140. 3 +/- 19. 0 Å). Comment: 12 pages, 6 figures, ApJ Submitte...|$|R
40|$|We {{present a}} Bayesian {{framework}} for inferring {{the parameters of}} a mixture of experts model based on ensemble learning by varia-tional free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate <b>these</b> <b>methods</b> <b>on</b> artificial problems and sunspot time series prediction...|$|R
40|$|Parametric methods from {{different}} modelling paradigms were proposed {{to students in}} a design studio. The influence of <b>these</b> <b>methods</b> <b>on</b> creativity was studied using qualitative methodology. “Generation” was found to stimulate “abundance” of ideas and to lead to different evolution and flexibility, {{as well as to}} higher “engagement” in the design process...|$|R
40|$|International audienceIn non-contrast-{{enhanced}} x-ray sequences, {{the image}} quality of stents {{can be enhanced}} by motion compensating and integrating images of exposure sequences. Three-dimensional stent reconstruction has been developed to allow enhanced stent visualization and assessment in three dimensions. This article gives {{an overview of the}} different methods used for enhanced stent visualization, describes studies that have evaluated <b>these</b> <b>methods,</b> and summarizes results of <b>these</b> <b>methods</b> <b>on</b> other cardiac and non-cardiac devices...|$|R
40|$|Abstract. We compare three {{well known}} methods for solving the PDEs such as Finite Difference Method (FDM), Spectral Method, Wavelet Galerkin Method (WGM). We test all <b>these</b> <b>methods</b> <b>on</b> Advection Equation and Klein-Gordon Equation. We plot {{comparison}} and error graphs using MATLAB. PACS: 65 L 10, 65 L 12, 65 L 50, 65 L 9...|$|R
40|$|This {{project is}} {{specialized}} <b>on</b> the <b>method</b> generation digital watermark in pictures. Aim work is study bent on basic watermark techniques, principles embedding of watermark to {{the pictures and}} <b>on</b> possibilities usage <b>these</b> <b>methods</b> <b>on</b> protection author's rights. In terms of project was tested possibility embedding of watermark to the picture with usage programme Matlab...|$|R
40|$|This {{bachelor}} s thesis focuses <b>on</b> aplication of <b>methods</b> {{used to describe}} actual state of non-govermental organisations. The aim was {{to find out whether}} methods from theoretical part could be applicated to facilities of NKP. This thesis consists from theoretical searches of literary works and practical part where I was applying <b>these</b> <b>methods</b> <b>on</b> real situation...|$|R
40|$|Recently {{a variety}} of {{algorithms}} for the approximation of parametric curves and surfaces by algebraic representations have been developed. We test three of <b>these</b> <b>methods</b> <b>on</b> several test cases, keeping track of time and memory consumption of the implementations {{and the quality of}} the approximation. Additionally we discuss some qualitative aspects of the different methods...|$|R
40|$|Policy {{recommendations}} {{based on}} elasticity parameters have assumed greater {{significance for the}} firm 2 ̆ 7 s financial viability and success in a competitive set up. So {{there is a need}} to examine with more prudence the elasticity estimates obtained from various parametric as well nonparametric methods. The main aim of this paper is first to critically <b>examine</b> <b>these</b> <b>methods,</b> then to point out their limitations, and finally to propose an alternative method to be considered for while pursuing further elasticity studies. Research supported by Grant-in-Aid for Scientific Research (C) Japan Society for the Promotion of ScienceRevised version: "Nonparametric and Parametric Measures of Scale Elasticity: A Comparative Evaluation" Tone, Kaoru; Sahoo, Biresh K.; Tsutsui, Miki, I- 2002 - 0004 Rev. 2005 / 1 / 17 [URL]...|$|R
40|$|This {{justification}} {{examines the}} rhetorical {{significance of the}} film Pump Up the Volume using Karlyn Kohrs Campbell's Critiques of Contemporary Rhetoric and Alasdair Maclntyre's Theory of Rhetorical Conversation and Moral Action. Before drawing conclusions about the rhetorical impact of the film itself, I will briefly describe the film, and apply Campbell's and Maclntyre's methods. I will also define and explain "Generation X" and find similarities between the film and the generation. Finally, I will examine some tools for future research. By <b>examining</b> <b>these</b> <b>methods</b> and artifacts, I will underscore {{the importance of the}} rhetoric of film and the impact it had on a generation. This study examines the importance of film as rhetoric and Pump Up the Volume's impact as a rhetorical tool. Honors CollegeThesis (B. ?. ...|$|R
40|$|BY {{means of}} phase {{contrast}} cine-photomicrography applied to tissuecultures {{it has become}} possible to study the division of living vertebrate cells in much greater detail than hitherto. Observations have already been made by <b>these</b> <b>methods</b> <b>on</b> mitosis in chick cells (Hughes and Swann, 1948; Hughes and Fell, 1949) and in amphibian tissue (Hughes and Preston, 1949) ...|$|R
40|$|Relaxed non-stationary multisplitting {{methods are}} studied for the {{parallel}} solution of nonsingular linear systems. Convergence {{results of the}} synchronous and asynchronous versions for H-matrices are presented. Computational results of <b>these</b> <b>methods</b> <b>on</b> a shared memory multiprocessor vector computer are reported. Λ Supported by the ESPRIT III basic research programme of the EC under contract No. 9072 (project GEPPCOM) ...|$|R
40|$|In this paper, we {{investigate}} {{the effectiveness of}} several techniques commonly recommended for overcoming convergence problems with coevolutionary algorithms. In particular, {{we investigate}} effects of the Hall of Fame, and of several diversity maintenance <b>methods,</b> <b>on</b> a problem designed to test the ability of coevolutionary algorithms {{to deal with an}} intransitive superiority relation between solutions. We measure and analyse the effects of <b>these</b> <b>methods</b> <b>on</b> population diversity and on solution quality...|$|R
40|$|Abstract- In {{this paper}} {{we examine the}} optimal {{throughput}} of recursive filter using varies optimization techniques, which are relevant for real time application. By formulating filter design as a multi-objective optimization problem and approaching. Different approaches use for implementing of <b>these</b> <b>methods</b> <b>on</b> hardware. In this work FPGA implementation of recursive filters are examined and the comparison of <b>these</b> <b>methods</b> is done by analyzing the hardware cost and performance. Index terms –IIR filter, VHDL, FPG...|$|R
40|$|This paper {{presents}} an empirical comparison between different methods and tools for segmenting texts. After presenting segmentation tools {{and more specifically}} linear segmentation algorithms, we present a comparison of <b>these</b> <b>methods</b> <b>on</b> both French and English text corpora. This evalutation {{points out that the}} performance of each <b>method</b> heavilly relies <b>on</b> the topic of the documents, and the number of boundaries to be found. ...|$|R
40|$|This study {{examines}} how two theatrical methods originally developed in distinct cultural settings, namely the Jacques Lecoq training developed in Europe and {{the method of}} Tadashi Suzuki developed in Japan. In <b>examining</b> <b>these</b> training <b>methods</b> and analyzing performances by students of <b>these</b> <b>methods</b> we see that both methods contribute to enhancing the physical vocabulary of the actor and extending the actors' movement abilities. <b>These</b> two <b>methods</b> hold great potential to compliment each other in training actors to perform in an ever changing theatrical environment {{that seems to be}} requiring more physical movement from the actors...|$|R
