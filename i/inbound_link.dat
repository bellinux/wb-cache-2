15|67|Public
25|$|Some {{crawlers}} {{intend to}} download as many resources {{as possible from}} a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no <b>inbound</b> <b>link</b> would {{have been found in}} regular crawling.|$|E
50|$|Some {{other words}} for {{backlink}} are incoming link, <b>inbound</b> <b>link,</b> inlink, inward link, and citation.|$|E
5000|$|... #Caption: Search engines use complex {{mathematical}} algorithms {{to guess}} which websites a user seeks. In this diagram, if each bubble represents a website, programs sometimes called spiders examine which sites link to which other sites, with arrows representing these links. Websites getting more inbound links, or stronger links, are {{presumed to be}} more important and what the user is searching for. In this example, since website B is the recipient of numerous inbound links, it ranks more highly in a web search. And the links [...] "carry through", such that website C, even though it only has one <b>inbound</b> <b>link,</b> has an <b>inbound</b> <b>link</b> from a highly popular site (B) while site E does not. Note: Percentages are rounded.|$|E
5000|$|... #Subtitle level 2: <b>Inbound</b> <b>links,</b> {{outbound}} links, internal links ...|$|R
50|$|Expired domains {{that were}} {{formerly}} websites may be sought for parked domain monetization. If the expired domain has many <b>inbound</b> <b>links,</b> {{it may also}} be used as a site in a Private Blog Network, {{due to the fact that}} an expired domain will still usually maintain most of its prior <b>inbound</b> <b>links.</b>|$|R
5000|$|Indexing text on <b>inbound</b> <b>links</b> {{from other}} {{documents}} (or other social tagging ...|$|R
50|$|Spam in blogs is the placing or {{solicitation}} {{of links}} randomly on other sites, placing a desired keyword into the hyperlinked {{text of the}} <b>inbound</b> <b>link.</b> Guest books, forums, blogs, and any site that accepts visitors' comments are particular targets and are often victims of drive-by spamming where automated software creates nonsense posts with links that are usually irrelevant and unwanted.|$|E
50|$|Some {{crawlers}} {{intend to}} download as many resources {{as possible from}} a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. For example, when given a seed URL of , it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no <b>inbound</b> <b>link</b> would {{have been found in}} regular crawling.|$|E
50|$|Search engines {{measure a}} website’s value and {{relevance}} {{by analyzing the}} links to the site from other websites. The resulting “link popularity” {{is a measure of}} the number and quality of links to a website. It is an integral part of a website’s ranking in search engines. Search engines examine each of the links to a particular website to determine its value. Although every link to a website is a vote in its favor, not all votes are counted equally. A website with similar subject matter to the website receiving the <b>inbound</b> <b>link</b> carries more weight than an unrelated site, and a well-regarded site (such as a University) has higher link quality than an unknown or disreputable website.|$|E
5000|$|<b>Inbound</b> and {{outbound}} <b>links</b> {{are those}} that hyperlink two independent web pages together whereas <b>inbound</b> <b>links</b> would hyperlink domain “A” to domain “B” and outbound links would hyperlink domain “B” to domain “A.” ...|$|R
50|$|Notes:* Data for {{research}} rankings is not publicly available. * Source: Innovative Knowledge rankings, Technological Impact rankings, Website rankings and Domain <b>Inbound</b> <b>Links</b> rankings for Pakistan, SCImago Institutions Rankings as of 2014.|$|R
50|$|Expired domains {{that were}} {{formerly}} websites are also sought after for parked domain monetization. A domain {{that was used}} as a website and is allowed to expire will still maintain most of its prior <b>inbound</b> <b>links.</b> These types of domains usually attract their largest amount of visitor traffic initially after being claimed from the domain drop lists. As website operators and search engines begin to remove the former <b>inbound</b> <b>links,</b> the traffic to the parked domain will begin to decline. The process of re-registering expired names is known as dropcatching and various domain name registries have differing views on it.|$|R
50|$|The Governor ordered {{connecting}} {{roads that}} lead into the Fort Point Channel Tunnel and several ramps to the westbound section {{from within the}} city be closed. These closures caused dramatic overflow congestion throughout the city as motorists sought alternate routes to and from Logan International Airport and several other key arterial routes. Beyond the difficulties posed within the city, the Fort Point Channel Tunnel and Ted Williams Tunnel link the Massachusetts Turnpike and Interstate 93 to Logan, so this also blocked a key <b>inbound</b> <b>link</b> for airport travelers coming from outside the city, forcing them to seek alternate routes like the Callahan Tunnel or follow poorly marked detours that wound through the city, often resulting in additional travel times of one hour or more.|$|E
30|$|Two {{different}} populations {{with equal}} numbers should be generated {{for the two}} {{different parts of the}} chromosome. Each member of the matrix part of the chromosome shows a possible route from supplier to potential DCs. To generate an initial population for this part, the algorithm is fed by all inbound links into intermediate nodes along with their types. The algorithm starts from potential DC 1 and selects a random <b>inbound</b> <b>link</b> from the set of corresponding inbound links. Then starting node of the selected link is determined before a random <b>inbound</b> <b>link</b> is assigned to the nodes. This process continues until the node 0 (supplier) is reached. The obtained route is then transformed into the corresponding chromosome representation as described above. Repeating the process for other potential DCs, matrix rows and columns are formed.|$|E
30|$|Social network {{analysis}} for ranking users in online communities {{is based on}} CEN which is created among users. <b>Inbound</b> <b>link</b> to a node in CEN indicates that the user linked to this node answers to the user who is on {{other side of the}} link. Whatever the number of inbound links to a node is more, indicates that linked user to that node has higher expertise.|$|E
50|$|Some sources {{describe}} the neologism campaign as a prank. However, despite {{three times as}} many <b>inbound</b> <b>links,</b> observers have noted that search engines Bing and Yahoo had been presenting the offending links second behind Santorum's web site.|$|R
50|$|In {{the field}} of search engine optimization, link {{building}} describes actions aimed at increasing the number and quality of <b>inbound</b> <b>links</b> to a webpage {{with the goal of}} increasing the search engine rankings of that page or website.|$|R
50|$|At the time, Blekko {{announced}} plans to earn revenue by selling ads based on slashtags and search results. The company also planned to provide data on its algorithm for ranking search results, including details for <b>inbound</b> <b>links</b> to specific sites.|$|R
40|$|Abstract — Time {{division}} {{multiple access}} (TDMA) based medium access control (MAC) protocols can provide QoS with guaranteed access to wireless channel. However, in multi-hop wireless networks, these protocols may introduce delay when packets are forwarded from an <b>inbound</b> <b>link</b> to an outbound link on a node. Delay occurs if the outbound link is scheduled to transmit before the <b>inbound</b> <b>link.</b> The total round trip delay can be quite large since it accumulates at every hop in the path. This paper presents a method that finds schedules with minimum round trip scheduling delay. We show that the scheduling delay {{can be interpreted as}} a cost collected over a cycle on the conflict graph. We use this observation to formulate a min-max program for the delay across a set of multiple paths. The min-max delay program is NPcomplete since the transmission order of links is a vector of binary integer variables. We design heuristics to select appropriate transmission orders. Once the transmission orders are known, a modified Bellman-Ford algorithm finds the schedules. The simulation results confirm that the proposed algorithm can find effective min-max delay schedules...|$|E
40|$|The World Wide Web (WWW) {{is being}} {{prolonged}} by an impulsive speed. As a result, search engines encounter many challenges such as yielding accurate and conversant {{results to the}} users, and responding them in an appropriate timely manner. A crawler is a program that downloads and stores Web pages, often for a Web search engine. Web crawler (also known as a Web spider or Web robot) is a program or automated script which browses the World Wide Web crawlers are mainly {{used to create a}} copy of all the visited pages for later processing by a search engine, which will index the downloaded pages to provide fast searches. We have concluded that the advantage with Path-mounting crawler is that they are very effective in finding isolated resources, or resources for which no <b>inbound</b> <b>link</b> would have been found in regular crawling...|$|E
40|$|The final {{publication}} {{is available}} at www. springerlink. com We study the static pricing problem for a network service provider in a loss system with a tree structure. In the network, multiple classes share a common <b>inbound</b> <b>link</b> and then have dedicated outbound links. The motivation is from a company that sells phone cards and needs to price calls to different destinations. We characterize the optimal static prices {{in order to maximize}} the steady-state revenue. We report new structural findings as well as alternative proofs for some known results. We compare the optimal static prices versus prices that are asymptotically optimal, and through a set of illustrative numerical examples we show that in certain cases the loss in revenue can be significant. Finally, we show that static prices obtained using the reduced load approximation of the blocking probabilities can be easily obtained and have near-optimal performance, which makes them more attractive for applications. 1...|$|E
5000|$|According to Technorati, , ScienceBlogs had an [...] "authority" [...] of 9,581 and its {{number of}} <b>inbound</b> <b>links</b> ranks it 37th among blogs worldwide. , Quantcast charts it as having over 1.1 million monthly unique visitors, 65% {{of whom are}} from the United States.|$|R
40|$|Technical communicators {{reveal how}} y they {{constructed}} and updated their Web sites to attract {{both human and}} search engine audiences. Human and search engine audiences y can invite different Web site communication techniques. Techniques associated with higher y levels of search engine traffic include writing longer Web page titles and generating higher numbers of <b>inbound</b> <b>links.</b> Purpose: This article explores how businesses offering technical communication services used search engine optimization techniques to attract prospective clients to their business Web sites. Method: The study draws on a survey of 240 principals of these businesses, brief interviews with half of them, analyses of their sites, and tallies of <b>inbound</b> <b>links</b> to their sites. Results: The interviews and analyses reveal how businesses oriented their sites not only to a human audience of prospective clients but also {{to an audience of}} search engines. Businesses that reported search engines to be more helpful in directing traffic to their sites had sites that, in comparison with those of their less successful peers, featured longer home page titles and received more <b>inbound</b> <b>links.</b> Conclusion: Though search engine optimization techniques can increase Web site traffic, technical communication businesses varied widely in how extensively and expertly they used such techniques...|$|R
50|$|Forum {{signature}} linking is {{a technique}} used to build backlinks to a website. This {{is the process of}} using forum communities that allow outbound hyperlinks in a member's signature. This can be a fast method to build up <b>inbound</b> <b>links</b> to a website's Search Engine Optimization value.|$|R
40|$|We {{study the}} static pricing {{problem for a}} network service {{provider}} in a loss system with a tree structure. In the network, multiple classes share a common <b>inbound</b> <b>link</b> and then have dedicated outbound links. The motivation is from a company that sells phone cards and needs to price calls to different destinations. We characterize the optimal static prices {{in order to maximize}} the steady-state revenue. We report new structural findings as well as alternative proofs for some known results. We compare the optimal static prices versus prices that are asymptotically optimal, and through a set of illustrative numerical examples we show that in certain cases the loss in revenue can be significant. Finally, we show that static prices obtained using the reduced load approximation of the blocking probabilities can be easily obtained and have near-optimal performance, which makes them more attractive for applications. Massachusetts Institute of Technology. Center for Digital BusinessUnited States. Office of Naval Research (Contract N 00014 - 95 - 1 - 0232) United States. Office of Naval Research (Contract N 00014 - 01 - 1 - 0146) National Science Foundation (U. S.) (Contract DMI- 9732795) National Science Foundation (U. S.) (Contract DMI- 0085683) National Science Foundation (U. S.) (Contract DMI- 0245352...|$|E
40|$|Time {{division}} {{multiple access}} (TDMA) based medium access control (MAC) protocols can provide QoS with guaranteed access to the wireless channel. However, in multihop wireless networks, these protocols may introduce scheduling delay if, on the same path, an outbound link on a router is scheduled to transmit before an <b>inbound</b> <b>link</b> on that router. The total scheduling delay can be quite large since it accumulates at every hop on a path. This paper presents a method that finds conflict-free TDMA schedules with minimum scheduling delay. We show that the scheduling delay {{can be interpreted as}} a cost, in terms of transmission order of the links, collected over a cycle in the conflict graph. We use this observation to formulate an optimization, which finds a transmission order with the minmax delay across a set of multiple paths. The min-max delay optimization is NP-complete since the transmission order of links is a vector of binary integer variables. We devise an algorithm that finds the transmission order with the minimum delay on overlay tree topologies and use it with a modified Bellman-Ford algorithm, to find minimum delay schedules in polynomial time. The simulation results in 802. 16 mesh networks confirm that the proposed algorithm can find effective min-max delay schedules...|$|E
40|$|Abstract—Time {{division}} {{multiple access}} (TDMA) based medium access control (MAC) protocols can provide QoS with guaranteed access to the wireless channel. However, in multi-hop wireless networks, these protocols may introduce scheduling delay if, on the same path, an outbound link on a router is scheduled to transmit before an <b>inbound</b> <b>link</b> on that router. The total scheduling delay can be quite large since it accumulates at every hop on a path. This paper presents a method that finds conflict-free TDMA schedules with minimum scheduling delay. We show that the scheduling delay {{can be interpreted as}} a cost, in terms of transmission order of the links, collected over a cycle in the conflict graph. We use this observation to formulate an optimization, which finds a transmission order with the min-max delay across a set of multiple paths. The min-max delay optimization is NP-complete since the transmission order of links is a vector of binary integer variables. We devise an algorithm that finds the transmission order with the minimum delay on overlay tree topologies and use it with a modified Bellman–Ford algorithm, to find minimum delay schedules in polynomial time. The simulation results in 802. 16 mesh networks confirm that the proposed algorithm can find effective min-max delay schedules. Index Terms—Scheduling delay, stop-and-go queueing, TDMA scheduling algorithms. I...|$|E
5000|$|<b>Inbound</b> and {{outbound}} <b>links</b> {{are essential}} to web page visibility often enhancing web page relevance, ranking, & placement. There are few instances where <b>inbound</b> <b>links</b> would be discouraged. Outbound links however should be given sparingly and should only link material to other material of same or similar relevance. Often, developers will utilize a nofollow tag used mostly to further optimize hyperlinks by “instructing” search engines not to distribute any PageRank from the hyperlink. An example of a nofollow tag might be: ...|$|R
40|$|Abstract: Backlinks, {{which are}} {{sometimes}} called <b>inbound</b> <b>links,</b> are incoming links to a web page or the entire website. Search engines have measure {{the number of}} backlinks a website or web page has, and ranks those web pages with more backlinks in a higher position as <b>inbound</b> <b>links</b> are important to search engine rankings. Back links play two important roles they direct traffic to your web site and factor in the search engines deciding the position of your web pages in the index of results. Web pages use different types of backlinks to make the linking a mutually beneficial objective, {{as well as to}} build the number of backlinks to the website. A web page that has more backlinks than another with similar content will rank higher than the other page, simply because it seems to be more popular with visitors and other websites...|$|R
50|$|These {{types of}} domains usually attract their largest amount of visitor traffic {{initially}} after being claimed from the domain drop lists. As website operators and search engines begin {{to remove the}} former <b>inbound</b> <b>links,</b> the traffic to the parked domain will decline. The process of re-registering expired names is known as dropcatching and various domain name registries have differing views on it.|$|R
40|$|The {{objective}} {{of this study is}} to evaluate the performance of public services through the website of local governments in Indonesia. Research variables are characteristic of web services, website popularity, and web metrics. Secondary and primary data are deployed to measure those variables. Data analysis focused on identifying the digital divide views from government levels and geographic location of Java and outside Java. Result finding show that on web metrics rank of local governments outside Java shows, website of province is more dominant than the city or county website, while for Java Island, the website of the district or the city is more dominant than the provincial web. Further it shows that the province is better than the district and city for a webpage and <b>inbound</b> <b>links,</b> while the city is better for the popularity. District is lower than the provincial and city for all web metrics. Finally it shows that there are digital divide between Java and outside Java for a webpage, <b>inbound</b> <b>links</b> and traffic...|$|R
5000|$|The list of <b>inbound</b> <b>links</b> on Google Webmaster Tools is {{generally}} {{much larger than}} the list of <b>inbound</b> <b>links</b> that can be discovered using the link:example.com search query on Google itself. Google is tight lipped about the discrepancy. The list on Google Webmaster Tools includes nofollow links that do not convey search engine optimization authority to the linked site. On the other hand, the list of links generated with a link:example.com type query are deemed by Google to be [...] "important" [...] links in a controversial way. Google Webmaster Tools, as well as the Google index, seems to routinely ignore link spam.Once a manual penalty has been removed, Google Webmaster Tools will still display the penalty for another 1-3 days. After the Google Search Console rebrand, information has been produced demonstrating that Google Search Console creates data points that do not reconcile with Google Analytics or ranking data, particularly within the local search market.|$|R
50|$|When the Google {{search engine}} became popular, search engine optimizers learned that Google's ranking {{algorithm}} depended {{in part on}} a link weighting scheme called PageRank. Rather than simply count all <b>inbound</b> <b>links</b> equally, the PageRank algorithm determines that some links may be more valuable than others, and therefore assigns them more weight than others. Link farming was adapted to help increase the PageRank of member pages.|$|R
40|$|We present two new {{algorithms}} {{for generating}} uniformly random samples of pages from the World Wide Web, building upon recent work by Henzinger et al. (Henzinger et al. 2000) and Bar-Yossef et al. (Bar-Yossef et al. 2000). Both algorithms {{are based on}} a weighted random-walk methodology. The first algorithm (DIRECTED-SAMPLE) operates on arbitrary directed graphs, and so is naturally applicable to the web. We show that, in the limit, this algorithm generates samples that are uniformly random. The second algorithm (UNDIRECTED-SAMPLE) operates on undirected graphs, thus requiring a mechanism for obtaining <b>inbound</b> <b>links</b> to web pages (e. g., access to a search engine). With this additional knowledge of <b>inbound</b> <b>links,</b> the algorithm can arrive at a uniform distribution faster than DIRECTEDSAMPLE, and we derive explicit bounds on the time to convergence. In addition, we evaluate the two algorithms on simulated web data, showing that both yield reliably uniform samples of pages. We also compare our results with those of previous algorithms, and discuss the theoretical relationships among the various proposed methods...|$|R
40|$|One {{protocol}} (called {{the primary}} protocol) {{is independent of}} other protocols (jointly called the secondary protocol) if the question whether the primary protocol achieves a security goal never depends on whether the secondary protocol is in use. In this paper, we use multiprotocol strand spaces ([27], cf. [28]) to prove that two cryptographic protocols are independent if they use encryption in non-overlapping ways. This theorem (Proposition 7. 2) applies even if the protocols share public key certificates and secret key “tickets. ” We use the method of [8, 7] to study penetrator paths, namely sequences of penetrator actions connecting regular nodes (message transmissions or receptions) in the two protocols. Of special interest are <b>inbound</b> <b>linking</b> paths, which lead from a message transmission in the secondary protocol to a message reception in the primary protocol. We show that bundles can be modified to remove all <b>inbound</b> <b>linking</b> paths, if encryption does not overlap in the two protocols. The resulting bundle {{does not depend on}} any activity of the secondary protocol. We illustrate this method using the Neuman-Stubblebine protocol as an example [21, 27]. ...|$|R
40|$|In {{a network}} of pages (the web), {{outbound}} and <b>inbound</b> <b>links</b> are the only connecting flow making the vertexes; while the pages represent the edges as is found on directed graphs. The ranking of a page is dependent on its content {{and the number of}} <b>inbound</b> <b>links</b> which are recommendations to the page. Contrary, an outbound link demotes the page. If the Home page (target page) is "good", then there is a flow of Page Rank booster out of it. In this work, we introduced the TrustRank (TrustRank = M- 1 * X) method to become the start process of the Page Rank method (M * PageRank = (1 - d)), since the basic idea is, taking the link structure to generate a measure for the quality of a page by the selection of good (trusted) pages for a start (by hand). These pages are the sources of trust. Trust can be transferred to other page by linking to them. Trust is propagated {{in the same way as}} PageRank from thence. Ranked result from this point would be better than those not guided by the "trusted selections"...|$|R
