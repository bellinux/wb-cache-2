5|8|Public
40|$|This work {{investigates the}} process of selecting, extracting and reorganizing content from Semantic Web {{information}} sources, to produce an ontology meeting the specifications of a particular domain and/or task. The process is combined with traditional text-based ontology learning methods to achieve tolerance to <b>knowledge</b> <b>incompleteness.</b> The paper describes the approach and presents experiments in which an ontology was built for a diet evaluation task. Although the example presented concerns the specific case of building a nutritional ontology, the methods employed are domain independent and transferrable to other use cases...|$|E
40|$|Mistakes or foresights in {{the earlier}} phases of a product {{development}} tend to be amplified {{over the course of}} a project. Therefore, having a rigorous approach and supporting tools to identify and filter a development portfolio at the early stages can be highly rewarding. This paper presents an executable specification language, Object-Process Network (OPN), that can be used by system designers to formally represent the development option space, and automate certain model refinement activities at earlier phases of product development. Specifically, an OPN specification model can be automatically enumerated into a set of alternative development portfolios. OPN also provides an algebraic mechanism to handle the <b>knowledge</b> <b>incompleteness</b> problems at varying phases of planning, so that uncertain properties of different portfolios can be represented and analyzed under algebraic principles. In addition, it has a recursively defined model transformation operator that can iteratively refine the specification models to simplify or enhance the details of the machine-generated alternatives. A list of successful application cases is presented. Pages: 104 - 11...|$|E
40|$|In {{this paper}} we survey Milord II—a KBS design tool. We {{concentrate}} on its object level and meta-level languages, with {{special emphasis on}} the control tech-niques and the communication between both languages. The control, declarative in nature, is based on reflection techniques over a meta-language equipped with a declarative backtracking mechanism. Reflection and subsumption techniques are used to tackle the problem of <b>knowledge</b> <b>incompleteness.</b> This meta-level approach is based on assumptions over {{the current state of}} the object deductive process. Reflection makes meta-level deduction effective at the object level. Whenever the assumptions made at the meta-level are proved to be erroneous, a declarative back-tracking mechanism retracts them. The deductive calculus at the object level is based on a rule specialization calculus. Complex reasoning tasks can be implemented using a combination of the overall set of Milord II meta-control techniques. To illustrate the use of such modelling techniques we present first a scheduling reasoning system, second a method for solving a general class of default reasoning problems, and finally a legal reasoning problem involving default rules and priorities...|$|E
40|$|In {{this paper}} {{we present a}} {{methodology}} for goal-directed data mining of association rules and incorporation of these rules into a probabilistic knowledge base. The purpose of the data mining and rule extraction process is to repair <b>knowledge</b> base <b>incompleteness</b> uncovered during validation. We discuss how this incompleteness is uncovered and show the fundamental forms this incompleteness can take. We describe how association rules can be extracted from databases {{in order to address}} excluded information and to express missing relationships in a probabilistic knowledge base. The current implementation of this goal-directed data mining within an integrated generic expert system tool is also described. Our methodology can benefit many data intensive and imprecise domains such as stock market analysis, intelligence analysis, and operational management...|$|R
40|$|Semantic search {{has been}} one of the motivations of the Semantic Web since it was envisioned. We propose a model for the {{exploitation}} of ontology-based knowledge bases to improve search over large document repositories. In our view of Information Retrieval on the Semantic Web, a search engine returns documents rather than, or in addition to, exact values in response to user queries. For this purpose, our approach includes an ontology-based scheme for the semiautomatic annotation of documents and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to <b>knowledge</b> base <b>incompleteness.</b> Experiments are shown where our approach is tested on corpora of significant scale, showing clear improvements with respect to keyword-based search...|$|R
40|$|Abstract [...] Semantic search {{has been}} one of the motivations of the Semantic Web since it was envisioned. We propose a model for the {{exploitation}} of ontology-based knowledge bases to improve search over large document repositories. In our view of Information Retrieval on the Semantic Web, a search engine returns documents rather than, or in addition to, exact values in response to user queries. For this purpose, our approach includes an ontology-based scheme for the semi-automatic annotation of documents, and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to <b>knowledge</b> base <b>incompleteness.</b> Experiments are shown where our approach is tested on corpora of significant scale, showing clear improvements with respect to key-word-based search. Index Terms [...] information retrieval models, ontology languages, semantic search, semantic web I...|$|R
40|$|The {{last decade}} {{has seen the}} advent of the agent {{paradigm}} as a reference model in several fields of research, mainly but not limited to artificial intelligence and distributed systems. In open and distributed environments, where most facts are not known at all, the agent metaphor proves particularly useful if agents are able to autonomously perform some form of reasoning, possibly obviating <b>knowledge</b> <b>incompleteness</b> by means of hypotheses assumed on the unknown facts. A suitable mechanism to deal with incomplete and multiple knowledge is abductive reasoning. The aim {{of this paper is to}} describe LAILA, a language that can be used by logic-based agents capable of abductive reasoning, by enabling them to express at a high level several ways to join and coordinate with one another. In particular, we considered collaboration and competition as possible interaction patterns in the abductive reasoning that must be carried out by multiple agents. Syntax and operational semantics of the LAILA language are given along with a clarifying example; a section is also devoted to a brief description of the current LAILA implementation...|$|E
40|$|International audienceCarbon dioxide {{capture and}} {{geological}} storage {{is seen as}} a promising technology to mitigate greenhouse gas atmospheric emissions. Its wide-scale implementation necessitates demonstrating its safety for humans and the environment. We have developed a generic approach to provide references for safety assessment of CO 2 storage. It is composed of a series of simple tools for identifying risk scenarios, modelling risk events and exposure. It incorporates a rigorous management of uncertainty, distinguishing between variability and <b>knowledge</b> <b>incompleteness.</b> We applied this approach on a case study in the Paris Basin. This demonstrates how it delivers conditions mixing qualitative and quantitative elements for guaranteeing safety. This approach is flexible; it can be used for various sites and with various amounts of data. It can be carried out in a time-efficient manner at various stages of a project. In particular, it provides an operator or an authority with safety indicators in an early phase or for reviewing a risk assessment. Though not a complete risk assessment workflow, it thus partly compensates for the current lack of commonly acknowledged assessment methods or safety standards for CO 2 geological storage...|$|E
40|$|The use of Ontologies to {{overcome}} the limitations of keyword-based search, it has been put forward {{as one of the}} motivations of the Semantic IR. An approach includes an ontology-based scheme for the automatic annotation of Tamil documents and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to <b>knowledge</b> base <b>incompleteness.</b> Automatic Document annotation system - Automatic tagging of documents with keywords involving word sense disambiguation – instance based, context based, concept based, information retrieval from annotated collection with proper ranking algorithms. It proposes a framework for preprocessing like tokenizing,stop word removal, Root word extraction, noun & verb separation and frequency count for key word based search of Tamil documents. Ontology based Automatic annotation of documents in a controlled environment like a Knowledge Management portal. This is done through the exploitation of presentation structures embedded within documents. The approach introduced is fairly simple yet highly effective in annotating documents automatically. Tf-idf annotation weighting for indexed words. Ontology based search combination withkeyword search work for retrieval system...|$|R
40|$|Personal {{use of this}} {{material}} is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing {{this material}} for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. P. Castells, M. Fernández, D. Vallet. "An Adaptation of the Vector-Space Model for Ontology-Based Information Retrieval", IEEE Transactions on Knowledge and Data Engineering, vol. 19, no. 2, pp. 261 - 272, February 2007. Semantic search {{has been one of}} the motivations of the semantic Web since it was envisioned. We propose a model for the exploitation of ontology-based knowledge bases to improve search over large document repositories. In our view of information retrieval on the semantic Web, a search engine returns documents rather than, or in addition to, exact values in response to user queries. For this purpose, our approach includes an ontology-based scheme for the semiautomatic annotation of documents and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to <b>knowledge</b> base <b>incompleteness.</b> Experiments are shown where our approach is tested on corpora of significant scale, showing clear improvements with respect to keyword-based searchThis research was supported by the European Commission (FP 6 - 027685 —MESH), and the Spanish Ministry of Science and Education (TIN 2005 - 06885) ...|$|R
40|$|This paper {{addresses}} the following question: {{how should we}} update our beliefs after observing some incomplete data, {{in order to make}} credible predictions about new, and possibly incomplete, data? There may be several answers to this question according to the model of the process that creates the incompleteness. This paper develops a rigorous modelling framework that makes it clear the conditions that justify the different answers; and, on this basis, it derives a new conditioning rule for predictive inference to be used {{in a wide range of}} states of <b>knowledge</b> about the <b>incompleteness</b> process, including near-ignorance, which, surprisingly, does not seem to have received attention so far. Such a case is instead particularly important, as modelling incompleteness processes can be highly impractical, and because there are limitations to statistical inference with incomplete data: it is generally not possible to learn how incompleteness processes work by using the available data; and it may not be possible, as the paper shows, to measure empirically the quality of the predictions. Yet, these depend heavily on the assumptions made...|$|R
40|$|Engineering {{complete}} domain descriptions {{is often}} very costly {{because they are}} prone to errors of omission or commission. While many have studied knowledge acquisition, relatively few have studied the synthesis of low risk plans when actions may have missing or incorrect preconditions or effects. In this work, we focus upon omitted features (i. e., preconditions and effects) of actions, where the action features are {{in one of three}} groups: those that are i) known to be included, ii) known not to be included, and iii) not known to be either. Prior work has evaluated the correctness of complete plans synthesized with the incomplete domain theory, but no prior work has studied how a planner can lower the risk of failure by reasoning about its <b>knowledge</b> of the <b>incompleteness.</b> That is, by guiding search in terms of the potential risks of partial plans {{it should be possible to}} find lower risk plans. We present and empirically evaluate a forward heuristic search planner called FFRISKY, which computes estimates of plan risk on planning graphs. Aside from being the first to address the problem, the unique aspect of the planner’s heuristic is how it selectively propagates the risk to balance between risks incurred supporting propositions and generating risks that will effect propositions supported later. We compareFFRISKY with a control planner that uses the FF heuristic to measure plan length and ignore risk...|$|R
40|$|In {{the last}} decade the Italian {{authorities}} have tried to adapt themselves to a united European asylum system, in the European Union, through the adoption of legal reforms, intended to improve the conditions of asylum seekers in Italy. Despite these reforms, asylum seekers and refugee’s living conditions are far from covered. The {{aim of this study}} is to explore the different functions of social work in the Italian reception process regarding unaccompanied minors and their implications on the wellbeing of the unaccompanied minors. This study was conducted through several interviews with both social workers, which in some way worked with unaccompanied minors, and the unaccompanied minors themselves. This study shows that social workers emphasize the lack of resources, <b>knowledge</b> and the <b>incompleteness</b> of the asylum laws in Italy. This causes great problems that jeopardize the quality and possibilities of social work. The unaccompanied minors on the other hand emphasized the importance of school and the existence of sports in their life as primary means of integration to the new society. From the social workers accounts it becomes clear that large parts of the Italian migration system needs improvement and that the current laws and regulations are not applied all over Italy due to the lack of economic resources and overall knowledge in the matter, all over the country. Even if there is lot to desire, both workers and minors we have interviewed have expressed that there are many aspects that do work...|$|R

