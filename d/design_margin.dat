111|340|Public
50|$|There are two {{categories}} of techniques to reduce the probability of failure:Fault avoidance techniques increase the reliability of individual items (increased <b>design</b> <b>margin,</b> de-rating, etc.).Fault tolerance techniques increase {{the reliability of the}} system as a whole (redundancies, barriers, etc.).|$|E
50|$|A {{proof test}} {{is a form}} of stress test to {{demonstrate}} the fitness of a load-bearing structure, or rescue strop. An individual proof test may apply only to the unit tested, or to its design in general for mass-produced items. Such a structure is often subjected to loads above that expected in actual use, demonstrating safety and <b>design</b> <b>margin.</b> Proof testing is nominally a nondestructive test, particularly if both design margins and test levels are well-chosen. However, unit failures are by definition considered to have been destroyed for their originally-intended use and load levels.|$|E
50|$|In {{semiconductor}} manufacturing, {{a process}} corner {{is an example}} of a design-of-experiments (DoE) technique that refers to a variation of fabrication parameters used in applying an integrated circuit design to a semiconductor wafer. Process corners represent the extremes of these parameter variations within which a circuit that has been etched onto the wafer must function correctly. A circuit running on devices fabricated at these process corners may run slower or faster than specified and at lower or higher temperatures and voltages, but if the circuit does not function at all at any of these process extremes the design is considered to have inadequate <b>design</b> <b>margin.</b>|$|E
40|$|The {{life cycle}} of complex {{products}} is subject to multiple uncertainties. Designers include margins into the product to cater for these uncertainties: safety margins, which is typically included in the requirements against the uncertainties of use and <b>design</b> <b>margins</b> against the uncertainties of design, such as changing requirements or engineering change. In practise the terms <b>design</b> <b>margins</b> and safety margins are sometimes used interchangeably. At {{the end of the}} design process, a product might include considerable overdesign, which can increase the initial cost as well as the running cost of many complex systems. Late discovery of both overdesign and lack of safety margins are also expensive to deal with in late phases. This paper explore the role of safety <b>margins</b> and <b>design</b> <b>margins</b> in the <b>design</b> process based on two case studies in truck design and jet engine component design. The paper shows that margins are added both to the requirements and the design parameters, so that companies run the risk of duplicating margins throughout the process without different teams being aware of it. This paper argues that a clearer and common description of <b>design</b> <b>margins</b> can reduce undesired iteration in development processes arising from misconceptions and aggregation effects in the development of complex systems...|$|R
40|$|This paper {{examines}} {{the addition of}} <b>design</b> <b>margins</b> for building services energy infrastructure during the design process. It argues that {{care must be taken}} when applying margins; ensuring cumulative effects do not undermine the ability of systems to be energy efficient. An example of a hospital Trust is provided showing the addition of <b>design</b> <b>margins</b> impacting the energy efficiency of services provided. Tensions are found between delivery of flexibility, adaptability and other change parameters and the need for the system to be bounded, so as to encourage effectiveness. ...|$|R
40|$|Oxide trapped charge, field {{effects from}} emitter metallization, and high level {{injection}} phenomena moderate enhanced gain degradation of lateral pnp transistors at low dose rates. Hardness assurance tests at elevated irradiation temperatures require larger <b>design</b> <b>margins</b> for low power measurement biases...|$|R
50|$|Ratios of {{acceptable}} M/U for safety critical systems {{can vary from}} application to application. Studies have cited acceptable M/U ratios as being in the 2:1 to 10:1 range for nuclear weapons stockpile decision-making. Intuitively, the larger the value of M/U, the less of the available performance margin is being consumed by uncertainty in the simulation outputs. A ratio of 1:1 {{could result in a}} simulation run where the simulated performance threshold is not exceeded when in actuality the entire <b>design</b> <b>margin</b> may have been consumed. It {{is important to note that}} rigorous QMU does not ensure that the system itself is capable of meeting its performance margin; rather, it serves to ensure that the decision-making authority can make judgments based on accurately characterized results.|$|E
50|$|In {{fabrication}} {{the yield}} {{is one of}} the most important measures. Also in the design phase engineers already try to maximize the yield by using simulation techniques and statistical models. Often the data follows the well-known bell-shaped normal distribution, and for such distributions there is a simple direct relation between the <b>design</b> <b>margin</b> and the yield. If we express the specification margin in terms of standard deviation sigma, we can immediately calculate yield Y according this specification. The concept of worst-case distance (WCD) extends this simple idea for applying it to more complex problems (like having non-normal distributions, multiple specs, etc.). The WCD is a metric originally applied in electronic design for yield optimization and design centering, nowadays also applied as a metric for quantifying electronic system and device robustness.|$|E
50|$|QMU {{focuses on}} {{quantification}} of {{the ratio of}} <b>design</b> <b>margin</b> to model output uncertainty. The process begins with {{the identification of the}} key performance thresholds for the system, which can frequently be found in the systems requirements documents. These thresholds (also referred to as performance gates) can specify an upper bound of performance, a lower bound of performance, or both in the case where the metric must remain within the specified range. For each of these performance thresholds, the associated performance margin must be identified. The margin represents the targeted range the system is being designed to operate in to safely avoid the upper and lower performance bounds. These margins account for aspects such as the design safety factor the system is being developed to as well as the confidence level in that safety factor. QMU focuses on determining the quantified uncertainty of the simulation results as they relate to the performance threshold margins. This total uncertainty includes all forms of uncertainty related to the computational model as well as the uncertainty in the threshold and margin values. The identification and characterization of these values allows the ratios of margin-to-uncertainty (M/U) to be calculated for the system. These M/U values can serve as quantified inputs that can help authorities make risk-informed decisions regarding how to interpret and act upon results based on simulations.|$|E
40|$|Power {{has become}} an {{important}} aspect {{in the design of}} general purpose processors. This thesis explores how design tradeoffs affect the power and performance of the processor. Scaling the technology is an attractive way to improve the energy efficiency of the processor. In a scaled technology a processor would dissipate less power for the same performance or higher performance for the same power. Some microarchitectural changes, such as pipelining and caching, can significantly improve efficiency. Unfortunately many other architectural tradeoffs leave efficiency unchanged. This is because a large fraction of the energy is dissipated in essential functions and is unaffected by the internal organization of the processor. Another attractive technique for reducing power dissipation is scaling the supply and threshold voltages. Unfortunately this makes the processor more sensitive to variations in process and operating conditions. <b>Design</b> <b>margins</b> must increase to guarantee operation, which reduces the efficiency of the processor. One way to shrink these <b>design</b> <b>margins</b> is to use feedback control to regulate the supply and threshold voltages thus reducing the <b>design</b> <b>margins.</b> Adaptive techniques {{can also be used to}} dynamically trade excess performance for lower power. This results in lower average power and therefore longer battery life. Improvements are limited, however, by the energy dissipation of the rest of the system...|$|R
40|$|The general {{objective}} is to evaluate the Japan 2011 nuclear event’s operational, technical and regulatory implications on Canadian nuclear power plants (NPPs); specifically, provisions taken by licensees in the design basis (initiating events) and beyond the design basis (available <b>design</b> <b>margins,</b> diversity, redundancy, barrier integrity, and physical separation) for NPPs...|$|R
40|$|Early {{design studies}} showed that the UV {{spectrometer}} thermal <b>design</b> <b>margins</b> were very small so that an experimental confirmation of the analytical model would be desirable. At that time the prototype unit was scheduled too far downstream to be of value, so a separate thermal model was built for use in verifying the analytical model...|$|R
40|$|Abstract — Traditional {{worst-case}} {{design is}} becoming much difficult. The increase in parameter variations requires large design margins. However since the worst-case situations {{do not always}} occur, the <b>design</b> <b>margin</b> is often overestimated. To eliminate the excessive <b>design</b> <b>margin,</b> designers should focus on typical case rather than worst case. We are investigating canary logic {{that is one of}} the typical-case designs. The canary logic can eliminate the excessive <b>design</b> <b>margin</b> by combined with Dynamic Voltage Scaling (DVS) system. A naive combination of the canary and the DVS suffers serious performance loss due to useless voltage scaling. In this paper, we show where the performance loss comes from and improve the canary logic to eliminate the excessive <b>design</b> <b>margin</b> without performance loss. I...|$|E
40|$|VLSI-SoC 2008 : Rhodes Island, Greece : October 13 - 15, 2008 Traditional {{worst-case}} {{design is}} becoming much difficult. The increase in parameter variations requires large design margins. However since the worst-case situations {{do not always}} occur, the <b>design</b> <b>margin</b> is often overestimated. To eliminate the excessive <b>design</b> <b>margin,</b> designers should focus on typical case rather than worst case. We are investigating canary logic {{that is one of}} the typical-case designs. The canary logic can eliminate the excessive <b>design</b> <b>margin</b> by combined with Dynamic Voltage Scaling (DVS) system. A naive combination of the canary and the DVS suffers serious performance loss due to useless voltage scaling. In this paper, we show where the performance loss comes from and improve the canary logic to eliminate the excessive <b>design</b> <b>margin</b> without performance loss...|$|E
40|$|Analog {{design in}} future deep {{submicron}} technology will suffer more problems in systematic design rather than circuit design. Most circuit design {{issues in the}} pass 20 years have been well defined with numerous researchers and engineering. However, systematic issues, like mass production issue, nanometer technology issue, and analog/digital co-exist issue in SOC design, will {{getting more and more}} important in the future design. For most analog circuit blocks, the main reason for worst-case design and large <b>design</b> <b>margin</b> is the yield issue in mass-production, especially when the analog circuit blocks are integrated with digital blocks on the same chip. The yield of the analog blocks should be at least comparable with digital blocks such that analog circuit will not be the bottleneck for yield improvement in mass-production. The analog block yield loss usually comes from process variation, poor device modeling, and insufficient <b>design</b> <b>margin.</b> Process variation and device modeling problems can be reduced with more <b>design</b> <b>margin,</b> unfortunately, increase <b>design</b> <b>margin</b> will consume more power and need more chip area. The previous situation can alleviate the <b>design</b> <b>margin</b> increasing via provide more detai...|$|E
40|$|Complex systems {{engineering}} projects are increasingly prevalent in our world. Technical requirements for complex systems usually break out individual subsystems parameters. For instance, each subsystem on a spacecraft {{can be assigned}} target mass, volume, power consumption, and other technical requirements. These tangible variables are often traded between subsystems engineers to maximize subsystem design utility. This in turn helps to maximize overall system utility. <b>Design</b> <b>margins</b> are also often assigned to these design variables during {{early stages of the}} design process. Risk, reliability, robustness, and uncertainty have until this point not been part of subsystems parameter trading and <b>design</b> <b>margins.</b> This research aims to formalize a method of trading and margining these design variables among subsystems with the end effect of maximizing system utility and system integrity. Further, this research will investigate how different stakeholders in the complex system design process value and perceive risk, reliability, robustness, and uncertainty. Thi...|$|R
40|$|Scaling CMOS {{technology}} into nanometer feature-size nodes {{has made it}} practically impossible to precisely control the manufacturing process. This results in variation in the speed and power consumption of a circuit. As a solution to process-induced variations, circuits are conventionally implemented with conservative <b>design</b> <b>margins</b> to guarantee the target frequency of each hardware component in manufactured multiprocessor chips. This approach, referred to as worst-case design, results in a considerable circuit upsizing, in turn {{reducing the number of}} dies on a wafer. 3 ̆cbr/ 3 ̆e 3 ̆cbr/ 3 ̆eThis work deals with the design of real-time systems for streaming applications (e. g., video decoders) constrained by a throughput requirement (e. g., frames per second) with reduced <b>design</b> <b>margins,</b> referred to as better-than-worst-case design. To this end, the first contribution of this work is a complete modeling framework that captures a streaming application mapped to an NoC-based multiprocessor system with voltage-frequency islands under process-induced die-to-die and within-die frequency variations. The framework is used to analyze the impact of variations in the frequency of hardware components on application throughput at the system level. The second contribution of this work is a methodology to use the proposed framework and estimate the impact of reducing circuit <b>design</b> <b>margins</b> on the number of good dies that satisfy the throughput requirement of a real-time streaming application. We show on both synthetic and real applications that the proposed better-than-worst-case design approach can increase the number of good dies by up to 9. 6...|$|R
40|$|Integrated Circuit and System Design. Power and Timing Modeling, Optimization and Simulation : 17 th International Workshop, PATMOS 2007, Gothenburg, Sweden, September 3 - 5, 2007. The deep {{submicron}} semiconductor {{technologies will}} make the worst-case design impossible, since they can not provide <b>design</b> <b>margins</b> that it requires. Research directions should go to typical-case design methodologies, where designers are focusing on typical cases rather than worrying about very rare worst cases. They enable to eliminate <b>design</b> <b>margins</b> {{as well as to}} tolerate parameter variations. We are investigating canary logic, which we proposed as a promising technique that enables the typical-case design. Currently, we utilize the canary logic for power reduction by exploiting input variations, and its potential of 30 % power reduction in adders has been estimated at gate-level simulations. In this paper, we evaluate how canary logic is effective for power reduction of the entire microprocessor and find 9 % energy reduction...|$|R
40|$|In modern deep scaled CMOS ICs, the process, voltage and {{temperature}} (PVT) variations result to a randomized {{variation of the}} transistor’s parameters. The variability is becoming so large that, by adapting conventional corner based design methodology, the <b>design</b> <b>margin</b> introduced by variation might over-kill the benefit of scaling. This PhD project aims to exploit the <b>design</b> <b>margin</b> that circuit might experience none or small amount of error, yet delivers qualified output. nrpages: 190 status: publishe...|$|E
40|$|The deep {{submicron}} semiconductor technologies increase parameter variations. The {{increase in}} parameter variations requires excessive <b>design</b> <b>margin</b> that has serious impact on performance and power consumption. In order {{to eliminate the}} excessive <b>design</b> <b>margin,</b> we are investigating canary Flip-Flop (FF). Canary FF requires additional circuits consisting of an FF and a comparator. Thus, it suffers large area overhead. In {{order to reduce the}} area overhead, this paper proposes a selective replacement method for canary FF and evaluates it. In the case of Renesas’s M 32 R processor, the area overhead of 2 % is achieved...|$|E
40|$|With {{technology}} scaling, circuit {{performance has}} become more sensitive to various sources of variability, including manufacturing variations, ambient fluctuations, and circuit wear-out. These increased variations have created new challenges for conventional hardware guardbanding, as the additional <b>design</b> <b>margin</b> diminishes the benefits of technology scaling. This dissertation aims at reducing total system <b>design</b> <b>margin</b> with cross-layer approaches on monitoring, margining and mitigation of circuit variability. Since hardware and software adaptation {{can be used to}} reduce <b>design</b> <b>margin</b> with theexposed hardware variability provided by hardware monitors, we start by proposing twodifferent types of performance monitors that can achieve better monitoring accuracy andsmaller monitoring overhead. We also demonstrate the use of these performance monitors in system adaptation with our end-to-end implementation of software testbeds. We also study the dynamic variations and reliability margining problem in presence ofmonitor-and-actuate adaptation and emerging system contexts. In a system with monitor-and-actuate adaptation, dynamic variations require extra margin for monitor and actuate latencies. We analyze and study the margining problem considering different choices of the monitor and actuator types. System reliability margining strategies are also proposed for circuits in the “dark silicon” era, where the low-level <b>design</b> <b>margin</b> should consider the contexts of high-level power/thermal constraints. Last, we propose a clock gating methodology to mitigate the aging induced clock skew,which is difficult to monitor and resolve through adaptation. For certain phenomena andvariation sources, for example, soft error rates at different location/altitude, we also proposesystem/cloud-based monitors. An emulation platform is built to study the impacts ofdynamic power management schemes on system reliability...|$|E
40|$|This work {{is devoted}} to {{description}} of methodology of determining process and power equipment <b>design</b> <b>margins,</b> {{which is based on}} assessment of a relative process importance of the equipment in question. The methodology is theoretically analysed, brought into context of common design practice and demonstrated on actual cases. Substantial part of this work analyses heat transfer in general as a base of heat exchanger design process...|$|R
40|$|The deep {{submicron}} semiconductor {{technologies will}} make the worst-case design impossible, since they can not provide <b>design</b> <b>margins</b> that it requires. We are investigating a typical-case design methodology, which we call the Constructive Timing Violation (CTV). This paper extends the CTV concept to collapse dependent instructions, resulting in performance improvement and power reduction. Based on detailed simulations, we find the proposed mechanism effectively collapses dependent instructions. 1...|$|R
40|$|An {{evolutionary}} {{approach to}} develop STV system features that provide modularity, design margines, and simple interfaces is presented. A modular design minimizes modifications and requalification for block changes. Since several subsystems {{do not have}} to be modified in response to a change in another subsystem, <b>design</b> <b>margins</b> provide options for growth. It is concluded that the approach will lower program schedule and cost risk, while enhancing safety and reliability...|$|R
40|$|A {{confidence}} level based approach to total dose radiation hardness assurance is presented for spacecraft electronics. It is applicable to both ionizing and displacement damage dose. Results are {{compared to the}} traditional approach that uses radiation <b>design</b> <b>margin</b> and advantages of the new approach are discussed...|$|E
40|$|Duke Power Corporation {{plans to}} {{implement}} the Westinghouse best-estimate large break LOCA methodology for McGuire and Catawba Nuclear Stations (Reference 1). This change in LOCA methodology will gain margin in the calculated large break LOCA peak cladding temperature, thus allowing more operational and core <b>design</b> <b>margin.</b> The first cycle t...|$|E
40|$|Weight {{growth has}} been a {{significant}} factor in nearly every space and launch vehicle development program. In order to account for weight growth, program managers allocate a <b>design</b> <b>margin.</b> However, methods of estimating <b>design</b> <b>margin</b> are not well suited for the task of assigning a <b>design</b> <b>margin</b> for a novel concept. In order to address this problem, a hybrid method of estimating margin is developed. This hybrid method utilizes range estimating, a well-developed method for conducting a bottom-up weight analysis, and a new forecasting technique known as executable morphological analysis. Executable morphological analysis extends morphological analysis in order to extract quantitative information from the morphological field. Specifically, the morphological field is extended by adding attributes (probability and mass impact) to each condition. This extended morphological field is populated with alternate baseline options with corresponding probabilities of occurrence and impact. The overall impact of alternate baseline options can then be estimated by running a Monte Carlo analysis over the extended morphological field. This methodology was applied to two sample problems. First, the historical design changes of the Space Shuttle Orbiter were evaluated utilizing original mass estimates. Additionally, the FAST reference flight system F served as the basis for a complete sample problem; both range estimating and executable morphological analysis were performed utilizing the work breakdown structure created during the conceptual design of this vehicle. Ph. D...|$|E
40|$|The most {{limiting}} {{design criteria}} for nuclear reactor normal operating conditions (ANS Condition I) {{are known to}} be rod internal pressure and cladding oxidation, while those for nuclear reactor transient operating conditions (ANS Conditon II) to be fuel centerline temperature and transient cladding total tensile strain. However, the <b>design</b> <b>margins</b> against fuel temperature and transient cladding tensile strain become smaller since power uprating is being or will be utilized for the most of nuclear power reactors to enhance the economics of nuclear power. In order to secure sufficient <b>design</b> <b>margins</b> against fuel temperature and cladding total tensile strain even for power uprating, the current axial rod power profiles used in the reactor transient analysis were optimized to reduce over-conservatism, considering that 118 % overpower of a steady-state peak rod average power was not exceeded during the reactor transients. The comparison of the current axial rod power profiles and the optimized ones indicates that the latter reduces the fuel centerline temperature and cladding total tensile strain by 26 °C and 0. 02 %, respectively...|$|R
40|$|Detailed SINDA/FLUINT {{models have}} been {{developed}} to assess the adequacy of the Space Station Freedom External Thermal Control System's radiator design, as well as to simulate the effects of various operational scenarios. <b>Design</b> <b>margins</b> can be deduced by quantifying such real-world effects as flow maldistribution due to line-pressure drop and uneven heat loads. The factor which most reduced the heat-rejection capacity is an uneven heat load split between the two low temperature loops...|$|R
40|$|Abstract. Thermal {{design and}} {{analysis}} of a satellite instrument is introduced in this paper. Some methods were adopted to help heat conduct and a finite element model was built. The analysis {{results showed that the}} temperature scopes of the main structures are from 45 ℃ to 65. 8 ℃ and all of junction temperatures of the components are lower than the derated maximum junction temperatures themselves and leave enough <b>design</b> <b>margins,</b> which match the requirements of thermal analysis...|$|R
40|$|Abstract — Random {{telegraph}} signal (RTS) {{is shown to}} be an intrinsic component of the shift in MOSFET threshold voltage (V th) due to bias temperature instability (BTI). This is done by starting from a well-known model for negative BTI (NBTI), to derive the formula for RTS-induced V th shift. Based on this analysis, RTS simply contributes an offset in NBTI degradation, with an acceleration factor that {{is dependent on the}} gate voltage and temperature. This is verified by 3 -dimensional (3 -D) device simulations and measurements of 45 nm-node bulk-Si PMOS transistors. It has an important implication for design of robust SRAM arrays in the future: <b>design</b> <b>margin</b> for RTS should not be simply added, because it is already partially accounted for within the <b>design</b> <b>margin</b> for NBTI degradation. Keywords-component; random {{telegraph signal}}; negative bias temperature instability; hole traps; SRAM I...|$|E
40|$|Historically, <b>design</b> <b>margin</b> and defects {{have been}} viewed as {{different}} topics, one part of design and {{the other part of}} test. Shrinking process geometries are making the two part of a continuum. This paper discusses the leakage and delay behavior associated with classic resistive defects and compares it with transistor variation due to lithography. ...|$|E
40|$|Variability of {{the space}} {{radiation}} environment is investigated with regard to parts categorization for total dose hardness assurance methods. It is shown that it can have a significant impact. A modified approach is developed that uses current environment models more consistently and replaces the radiation <b>design</b> <b>margin</b> concept with one of failure probability during a mission...|$|E
40|$|Designed {{to yield}} a motion and force upon initiation, and is used for latch release of {{instrument}} covers. Shock output is quite low and will not have any significant affect on instruments. 2. Pvro Valves Used as assembly internal devices. Primary application is release of high-pressure gas from storage bottles to pressurize pneumatic systems after spacecraft launch. Provides safety, lower system pressure <b>design</b> <b>margins</b> and lower weight. Shock generated will not affect other assemblies or equipment. ...|$|R
50|$|Traditionally, dynamic voltage and {{frequency}} scaling on main memory DRAM has not been possible due to limitations in the DDR JEDEC standards. However, these limitations exist because the conventional wisdom in memory design is that large <b>design</b> <b>margins</b> are needed for good yield under worst-case manufacturing process variations, voltage fluctuations, and temperature changes. Thus, scaling voltage {{and frequency}}, which is commonly done in CPUs, is considered difficult, impractical, or too risky for data corruption to apply in memories.|$|R
40|$|The most {{significant}} {{challenges facing the}} Space Transfer Vehicle (STV) program are making space basing affordable and developing an easily evolvable STV. Study efforts try minimizing {{the high cost of}} astronauts activities and Space Station modification to enhance affordability. Modularity, <b>design</b> <b>margins,</b> and simple interfaces are being evaluated to develop an evolutionary approach for space basing starting in the 1990 's. In this STV study, the concept definition approach is presented, including concept optimization trades as well as concept selection and sensitivity analyses...|$|R
