0|150|Public
40|$|The Higher-Order Unification {{approach}} to ellipsis presented in [DSP 91] lacks an <b>interface</b> with <b>linguistic</b> <b>modules</b> other than semantics. As a result, it cannot handle such phenomena as the many-pronouns puzzle or {{the distinction between}} full and pronominal NPs. In this paper, we extend [GK 96]'s Higher-Order Coloured Unification {{approach to}} handle those cases. 1 Introduction [DSP 91, SPD 96] (henceforth, DSP) present what is today {{one of the most}} influential theories of ellipsis. The basic idea underlying this theory is very simple: ellipses are represented by free variables whose values are then determined using Higher-Order Unification (HOU). For instance, the semantic representation of Jon likes Mary and Peter does too is like(j; m) R(p) and the value of R (the semantic representation of the ellipsis) is determined by the equation like(j; m) = R(j). The process of solving such equations is called Higher-Order Unification and can be stated as follows: given an equation M = N, find a sub [...] ...|$|R
40|$|This paper {{proposes a}} model ofreading {{processing}} based mainly on Fodor's (1983) theory ofthe modularity ofthe mind. According to this model {{there are many}} domains ofprocessing in reading. Some ofthem {{are part of the}} <b>linguistic</b> <b>module</b> and others are part ofthe cognitive processor. Among the domains of the <b>linguistic</b> <b>module</b> there are lexical access and sintactic parsing. To build up the propositional scheme of a text and to integrate the propositional macrostructure of the text to the reader's previous knowledge are tasks ofthe cognitive processor. This model also proposes aninterface between the <b>linguistic</b> <b>module</b> and the cognitive processor, where semantic parsing takes place...|$|R
50|$|Levin, B. and M. Rappaport Hovav (1995) Unaccusativity: At the Syntax-Lexical Semantics <b>Interface,</b> <b>Linguistic</b> Inquiry Monograph 26, MIT Press, Cambridge, MA.|$|R
40|$|As the {{complexity}} of spoken dialogue systems has increased, there has been increasing interest in spoken language generation (SLG). SLG promises portability across application domains and dialogue situations {{through the development of}} applicationindependent <b>linguistic</b> <b>modules.</b> However in practice, rulebased SLGs often have to be tuned to the application. Recently, a number of research groups have been developing hybrid methods for spoken language generation, combining general <b>linguistic</b> <b>modules</b> with methods for training parameters for particular applications. This paper describes the use of boosting to train a sentence planner to generate recommendations for restaurants in MATCH, a multimodal dialogue system providing entertainment information for New York. 1...|$|R
40|$|We {{present a}} finite state {{morphology}} system augmented with typed feature structures as weights on transitions. Using a semiring interpretation, {{the weight of}} a result represents the possible linguistic interpretations of an input word, while the resulting character string itself represents the lemma of the input. Long-distance phenomena and infixation can be handled in an easy and elegant manner, simultaneously providing a seamless <b>interface</b> to subsequent <b>linguistic</b> processing <b>modules.</b> ...|$|R
50|$|Igor Stanislа́vovich Ashmа́nov (born 9 January 1962, Moscow) — Russian {{entrepreneur}} {{specializing in}} information technology, artificial intelligence, software development, project management. He directed {{development of the}} <b>linguistic</b> <b>module</b> ORFO of the Russian edition of Microsoft Office, a family of Multilex electronic dictionaries, Spamtest spam filter etc.|$|R
5000|$|The {{hypothetical}} Martian anthropologist {{is described}} {{in the writings of}} Noam Chomsky as one who, upon studying the world's languages, would conclude that they are all dialects of a single language embodying a [...] "universal grammar" [...] reflecting a hardwired, genetically determined <b>linguistic</b> <b>module</b> inherent in the human brain.|$|R
40|$|While neural {{networks}} have been employed to handle several different text-to-speech tasks, ours {{is the first}} system to use {{neural networks}} throughout, for both linguistic and acoustic processing. We divide the text-to-speech task into three subtasks, a <b>linguistic</b> <b>module</b> mapping from text to a linguistic representation, an acoustic module mapping from the linguistic representation to speech, and a video module mapping from the linguistic representation to animated images. The <b>linguistic</b> <b>module</b> employs a letter-to-sound neural network and a postlexical neural network. The acoustic module employs a duration neural network and a phonetic neural network. The visual neural network is employed in parallel to the acoustic module to drive a talking head. The use of neural networks that can be retrained {{on the characteristics of}} different voices and languages affords our system a degree of adaptability and naturalness heretofore unavailable. 1...|$|R
40|$|International audienceThe work {{reported}} in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain. A hybrid method is roposed, combining statistical classification algorithms and linguistic knowledge to automatize post-OCR error detection and correction. The current paper deals with the integration of <b>linguistic</b> <b>modules</b> {{and their impact on}} error detection...|$|R
40|$|The paper {{gives the}} {{background}} {{leading to the}} development of current quality control procedures used in modification of the Russian-English system. A special program showing target language translation differences has become the central control mechanism. Procedures for modification of dictionaries, homographs and lexicals, and generalized <b>linguistic</b> <b>modules</b> are discussed in detail. A final assessment is made of the procedures and the quantitative results that can be obtained when they are used. The Air Force Russian-to-English MT system, operational since July, 1970, is an immense software package that has translated about 500, 000 pages of Russian text to date. Through the years the system has been enhanced by the addition of new routines and extensive modification of the dictionaries and <b>linguistic</b> <b>modules.</b> These optimization efforts were of course subjected to some degree of quality control at all stages. However, in this paper I intend to concentrate on th...|$|R
40|$|Surveillance Levels (SLs) are {{categories}} for medical patients (used in Brazil) that represent {{different types of}} medical recommendations. SLs are defined according to risk factors and the medical and developmental history of patients. Each SL is associated with specific educational and clinical measures. The objective of the present paper was to verify computer-aided, automatic assignment of SLs. The present paper proposes a computer-aided approach for automatic recommendation of SLs. The approach {{is based on the}} classification of information from patient electronic records. For this purpose, a software architecture composed of three layers was developed. The architecture is formed by a classification layer that includes a <b>linguistic</b> <b>module</b> and machine learning classification modules. The classification layer allows for the use of different classification methods, including the use of preprocessed, normalized language data drawn from the <b>linguistic</b> <b>module.</b> We report the verification and validation of the software architecture in a Brazilian pediatric healthcare institution. The results indicate that selection of attributes can have a great effect on the performance of the system. Nonetheless, our automatic recommendation of surveillance level can still benefit from improvements in processing procedures when the <b>linguistic</b> <b>module</b> is applied prior to classification. Results from our efforts can be applied to different types of medical systems. The results of systems supported by the framework presented in this paper may be used by healthcare and governmental institutions to improve healthcare services in terms of establishing preventive measures and alerting authorities about the possibility of an epidemic. FAPESPFAPESPRUSPRUS...|$|R
40|$|In this study, we {{attempted}} to bind the simple linguistic processes of combining verbs, objects, and the simple behavioral processes of object-related actions by using the RNNPB scheme (Sugita & Tani, 2003). The study was inspired by Arbib (2002) ’s hypothesis that mirror neurons, which become active both for generating and recognizing object handling behaviors, play crucial roles in language development, especially in pairing verbs and objects. Modeling and task setting Figure 1 (a) illustrates the RNNPB scheme used in the co-learning of word sequences and their corresponding behavior patterns. The <b>linguistic</b> <b>module</b> on the left-hand side receives word sequences, beginning with a “start symbol ” for each sequence. The behavior module {{on the right-hand side}} receives sensory-motor sequences. During colearning, word sequences are bound to the corresponding behavior sequences. More specifically, P Bl in the <b>linguistic</b> <b>module</b> and P Bb in the behavior module are simultaneously updated, under the constraint that the difference between these two vector...|$|R
40|$|This paper {{presents}} a stochastic model of French intonation contours {{for use in}} text-to-speech synthesis. The model has two <b>modules,</b> a <b>linguistic</b> <b>module</b> that generates abstract prosodic labels from text, and a phonetic module that generates an F 0 curve from the abstract prosodic labels. This model differs from previous work in the abstract prosodic labels used, which can be automatically derived from the training corpus. This feature {{makes it possible to}} use large corpora or several corpora of different speech styles, in addition to making it easy to adapt to new languages. The present paper focuses on the <b>linguistic</b> <b>module,</b> which does not require full syntactic analysis of the text but simply relies on a part-of-speech tagging technique. The results were validated by means of a perception test which showed that listeners did not perceive a significant difference in quality between the sentences synthesized with the original F 0 curve (from a recording), and those synthesized with the [...] ...|$|R
40|$|This paper {{proposes a}} method for {{organizing}} linguistic knowledge in both systematic and flexible fashion. We introduce a purely applicative language (PAL) as an intermediate representation and an object-oriented computation mechanism for its interpretation. PAL enables {{the establishment of a}} principled and well-constrained method of interaction among lexicon-oriented <b>linguistic</b> <b>modules.</b> The object-oriented computation mechanism provides a flexible means of abstracting modules and sharing common knowledge. 1...|$|R
40|$|Theoretical thesis. "Australian Research Council Centre of Excellence in Cognition and its Disorders, Australian Hearing Hub, Department of Linguistics, Macquarie University Sydney, Australia; Institut für Psycholinguistik der Universität Leipzig" [...] title page. Bibliography: pages 163 - 179. Chapter 1. Introduction and {{exposition}} [...] Chapter 2. Bilingual {{first language}} acquisition [...] Chapter 3. A longitudinal case study [...] Chapter 4. V 2 transfer? [...] Chapter 5. Acceleration of infl(ection) ? [...] Chapter 6. Delay of head negation? [...] Chapter 7. What about interrogatives? [...] Chapter 8. Synopsis and outlook. This thesis investigates cross-linguistic transfer in a bilingual German-English child, Kayla, age 2; 10 to 5; 06 years. The study draws {{on data from}} elicited production probes in addition to spontaneous production data {{in order to provide}} a robust data set for investigation. Recent literature assumes that bilingual children differentiate their two languages but endorses the possibility of cross-linguistic influence. It has been proposed that cross-linguistic influence can take one of three forms: transfer, acceleration or delay in the acquisition of certain grammatical constructions (Paradis & Genesee, 1996). The conditions under which cross-linguistic influence occurs {{have been the subject of}} considerable debate. One prominent proposal by Hulk & Müller (2000) suggests that overlap in the two languages is a key factor for cross-linguistic influence when it involves a grammatical structure influenced by properties at the <b>interface</b> of <b>linguistic</b> <b>modules.</b> Another recent proposal by Jakubowicz (2006) proposes that derivational complexity is the critical factor conditioning transfer. According to Hulk & Müller’s proposal, verb movement is a candidate for cross-linguistic influence because German and English exhibit considerable surface overlap in regards to word order, giving rise to a potential verb second (V 2) analysis of both languages. In addition, V 2 implicates the C-domain, which interfaces with pragmatic information. On the other hand, Jakubowicz (2006) does not anticipate transfer of V 2 as this involves a derivationally more complex structure. The child data presented in this thesis find support for Hulk & Müller’s proposal. The study focuses on an investigation of the child’s simple negative sentences and interrogatives, both of which potentially expose transfer of V 2 properties to English. The current data reveal that the child’s development of German is similar to monolingual peers, while her English exposes crosslinguistic influence. At the first stage, the V 2 property is transferred to English, resulting in non-adult like utterances such as *’That opens not’ (KAY, 2; 11 years) and *’What wants you eat?’ (KAY, 3; 06 years). Second, raising finite main verbs from V to C (via I) in English causes acceleration in the proportion of Inflection in the child’s utterances. In a later stage, the child’s negative sentences conform to the early productions of monolingual English speaking children. The child produces sentential negation exclusively with the adverbial form not, which is consistent with negation in German. This permits later productions such as *’It don’t goes this way’ (KAY, 4; 01 years). The child is considerably delayed in the acquisition of the head form of negation (‘n’t’) and consequently use of dosupport and use of negative auxiliaries such as doesn’t. Mode of access: World wide web 1 online resource (xi, 181 pages) diagrams, graph...|$|R
40|$|We {{present the}} TermWatch system which {{combines}} linguistic engineering with clustering and visualisation techniques for text data analysis. This paper {{focuses on the}} recent enhancements made on the <b>linguistic</b> <b>module</b> which includes {{the choice of a}} more efficient term extraction tool, the addition of semantic relations to the syntactic ones used for clustering. The results obtained are quite intuitive and show how thematics are structured in the field as well as interconnections between seemingly distinct themes...|$|R
40|$|Speech Maker is a {{framework}} {{designed to be}} the basis of an implementation of a text-to-speech system. The core of Speech Maker is a multilevel synchronized information structure through which all information transferred between <b>linguistic</b> <b>modules</b> passes. A user interface is available to inspect and manipulate this structure in an interactive session, and a rule formalism to manipulate the information structure algorithmically by means of rules. The three together constitute the framework of Speech Maker, which is described...|$|R
40|$|Web is a {{rich and}} diversified source of information. In this article, we propose to benefit from this {{richness}} to collect and analyze documents, {{with the aim of}} a relational indexation based on noun phrases. Proposed data processing chain includes a spider collecting data to build textual corpora, and a <b>linguistic</b> <b>module</b> analyzing text to extract information. Comparison of obtained corpus with corpus from Amaryllis conference shows the linguistic diversity of collected corpora, and particularly the richness of extracted noun phrases...|$|R
40|$|CarSim is an {{automatic}} text-to-scene conversion system. It analyzes written descriptions of car accidents and synthesizes 3 D scenes of them. The conversion process {{consists of two}} stages. An information extraction module creates a tabular description of the accident and a visual simulator generates and animates the scene. We implemented a first version of Car-Sim that considered a corpus of texts in French. We redesigned its <b>linguistic</b> <b>modules</b> and its <b>interface</b> and we applied it to texts in English from th...|$|R
40|$|This {{presentation}} {{will focus}} on Port 4 NooJ, the open source NooJ Portuguese <b>linguistic</b> <b>module,</b> which integrates a bilingual extension for Portuguese-English machine translation, work in progress. It describes {{the main components of}} the module, particularly, the electronic dictionaries, the rules which formalize and document Portuguese inflectional and derivational descriptions, and the different types of grammar: morphological, disambiguation, syntactic-semantic, multiword expressions and translation grammars. It explains how the different components interact and shows the application of the linguistic resources to text...|$|R
40|$|We {{present an}} engine for text adventures – {{computer}} games {{with which the}} player interacts using natural language. The system employs current methods from computational linguistics and an efficient inference system for description logic to make the interaction more natural. The inference system is especially useful in the <b>linguistic</b> <b>modules</b> dealing with reference resolution and generation and we show how we use it to rank different readings {{in the case of}} referential and syntactic ambiguities. It turns out that the player’s utterances are naturally restricted in the game scenario, which simplifies the language processing task...|$|R
40|$|In this paper, {{we propose}} a multi-strategic {{matching}} and merging approach to find correspondences between ontologies {{based on the}} syntactic or semantic characteristics and constraints of the Topic Maps. Our multi-strategic matching approach consists of a <b>linguistic</b> <b>module</b> and a Topic Map constraints-based <b>module.</b> A <b>linguistic</b> <b>module</b> computes similarities between concepts using morphological analysis, string normalization and tokenization and language-dependent heuristics. A Topic Map constraints-based module takes advantage of several Topic Maps-dependent techniques such as a topic property-based matching, a hierarchy-based matching, and an association-based matching. This is a composite matching procedure and need not generate a cross-pair of all topics from the ontologies because unmatched pairs of topics can be removed by characteristics and constraints of the Topic Maps. Merging between Topic Maps follows the matching operations. We set up the MERGE function to integrate two Topic Maps into a new Topic Map, which satisfies such merge requirements as entity preservation, property preservation, relation preservation, and conflict resolution. For our experiments, we used oriental philosophy ontologies, western philosophy ontologies, Yahoo western philosophy dictionary, and Wikipedia philosophy ontology as input ontologies. Our experiments show that the automatically generated matching results conform to the outputs generated manually by domain experts and can be of great benefit to the following merging operations. Ó 2006 Published by Elsevier Ltd...|$|R
40|$|This thesis {{develops}} a cognitive linguistic approach to language comprehension. The cognitive approach differs from traditional linguistic approaches in that linguistic description {{is seen as}} an integral part of the description of cognition, and that the object of description is the nature of conceptual structures, the processes which relate these conceptual structures, and the effect of context upon these processes. As a cognitive description within cognitive science, a computational approach is adopted: language comprehension is described in terms of two <b>modules,</b> a <b>linguistic</b> processing <b>module</b> and a discourse processing module. Within these modules, conceptual structures and processes are given a uniform characterization: structures are characterized as partial objects which are extended by processes into (potentially) less partial objects. In the <b>linguistic</b> processing <b>module,</b> <b>linguistic</b> expressions are characterized as signs which combine as head and modifier. The conceptual structu [...] ...|$|R
40|$|We {{describe}} an approach for a humanoid robot {{to understand its}} internal state (Infantino et al. (2013)). The method is based on self observation and communication with the ex-ternal world, according {{to the idea of}} introspection given by Sloman (2010). The robot introspection arises from infor-mation about physical components and software modules. This information is translated in a spatial representation of the hardware and software components of the robot through a SOM, which links the state representation of the robot with an high level representation given by an ontology. The ontol-ogy is furthermore linked to a <b>linguistic</b> <b>module</b> that makes it possible the interaction with human beings though a con-versational agent...|$|R
40|$|The Legal Theory Dpt. of the Institute of State and Law {{has been}} running and {{processing}} databases since 1985. Their primary purpose is the research and analysis of legal texts and legal language. In principle, they involve two projects: CS LEGSYS (the database of legal regulations, judicature and bill justification reports), and PES (Legal Electronic Vocabulary) – the database of doctrinal language (textbooks, comments and laws) complemented by a <b>linguistic</b> <b>module</b> {{for the analysis of}} law-related reference publications and their comparison with the general language. All relevant software tools for work with the Legal Electronic Vocabulary have been developed in cooperation with the Informatics Faculty of Masaryk University...|$|R
40|$|Under {{the support}} of Intelligent Computing Program, National 863 Project, a {{national}} assessment of speech synthesis systems for Chinese {{has been carried out}} since 1994. On March 1998, the 3 rd testing was carried out in Beijing, and four different systems were evaluated. Both phonetic (acoustic) <b>modules</b> and <b>linguistic</b> <b>modules</b> of speech synthesis and the ability of text pre-processing of six TTS systems were examined. All of the testing materials for the TTS systems were distributed and the outputs were gathered through the network. We will introduce the test methods on the network. The testing results of speech Articulation and naturalness in MOS (Mean Opinion Score) were given...|$|R
5000|$|... 2007. Oxford Handbook of <b>Linguistic</b> <b>Interfaces.</b> Oxford University Press.|$|R
40|$|Explosive {{growth of}} the World Wide Web {{as well as its}} {{heterogeneity}} call for powerful and easy to use search tools capable to provide the user with a moderate number of relevant answers. This paper presents analysis of key aspects of recently developed Web search methods and tools: visual representation of subject trees, interactive user <b>interfaces,</b> <b>linguistic</b> approaches, image search, ranking and grouping of search results, database search, and scientific information retrieval. Current trends in Web search include topics such as exploiting Web hyperlinking structure, natural language processing, software agents, influence of XML markup language on search efficiency, and WAP search engines...|$|R
40|$|In {{this paper}} we treat {{question}} answering (QA) as a classification problem. Our motivation {{is to build}} systems for many languages {{without the need for}} highly tuned <b>linguistic</b> <b>modules.</b> Consequently, word tokens and web data are used extensively but no explicit linguistic knowledge is incorporated. A mathematical model for answer retrieval, answer classification and answer length prediction is derived. The TREC 2002 QA task is used for system development where 33 % of questions are answered correctly. Performance is then evaluated on the factoid questions of the TREC 2003 QA task where 23 % of questions were answered correctly, which would rank the system in the top 10 of contemporary QA systems on the same task. 1...|$|R
40|$|In this paper, I {{argue that}} filled pause {{selection}} (um/uh) is a sociolinguistic variable, conditioned by {{both internal and}} external factors. There appears to be a language change in progress towards selecting um more often than uh. In all respects, the (UHM) variable appears to pattern quantiatively just like all other sociolinguistic variables which have been examined, even though the locus of (UHM) variation would seem to be firmly in the speech planning domain. Combined with the quantitative systematicity of sociolinguistic variables across the full range of <b>linguistic</b> <b>modules,</b> I argue that the locus of variation may not be in the grammar, but rather constitutes a separate domain of knowledge, perhaps what Preston (2004) called the “sociocultural selection device. ...|$|R
40|$|Carsim is a text-to-scene {{converter}}. A text-to-scene converter is a program, {{which reads}} a text and, by analyzing it, generates an image or 3 D scene representing the objects and/or actions {{described in the}} text. Carsim is designed to do just this, and more specifically {{to do it for}} traffic accident reports. The program itself can be divided into two separate <b>modules,</b> one <b>linguistic</b> <b>module,</b> which analyzes the text and one graphical module, which generates the 3 D scene. Carsim has been expanded several times since the project started to incorporate more languages and a wider range of reports. Therefore, the intermediate representation of data has changed and thus a new graphical module was needed...|$|R
40|$|Spraakmaker {{integrates}} {{a number}} of modules resulting from the combined efforts of several institutes in the Netherlands which aim at doing {{research in the field}} of automatic text-to-speech conversion. Spraakmaker is intended as a demonstration model for Dutch as well as a flexible tool for future research. In its design, Spraakmaker differs from most other existing text-to-speech systems. This paper first discusses the general language-independent framework on which Spraakmaker is built, which concerns the data structure used to store relevant data for the text-to-speech process and the general means of control. Then Spraakmaker is discussed as a specific implementation, which illustrates the use of the data structure and how <b>linguistic</b> <b>modules</b> operate on this structure...|$|R
50|$|Natural {{language}} {{user interfaces}} (LUI or NLUI) are {{a type of}} computer human <b>interface</b> where <b>linguistic</b> phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.|$|R
40|$|Natural Language Processing (NLP) {{techniques}} {{have been successfully}} used to automatically extract information from unstructured text through {{a detailed analysis of}} their content, often to satisfy particular information needs. In this paper, an automatic concept map construction technique, Fuzzy Association Concept Mapping (FACM), is proposed for the conversion of abstracted short texts into concept maps. The approach consists of a <b>linguistic</b> <b>module</b> and a recommendation <b>module.</b> The <b>linguistic</b> <b>module</b> is a text mining method that does not require the use to have any prior knowledge about using NLP techniques. It incorporates rule-based reasoning (RBR) and case based reasoning (CBR) for anaphoric resolution. It aims at extracting the propositions in text so as to construct a concept map automatically. The recommendation module is arrived at by adopting fuzzy set theories. It is an interactive process which provides suggestions of propositions for further human refinement of the automatically generated concept maps. The suggested propositions are relationships among the concepts which are not explicitly found in the paragraphs. This technique helps to stimulate individual reflection and generate new knowledge. Evaluation was carried out by using the Science Citation Index (SCI) abstract database and CNET News as test data, which are well known databases {{and the quality of the}} text is assured. Experimental results show that the automatically generated concept maps conform to the outputs generated manually by domain experts, since the degree of difference between them is proportionally small. The method provides users with the ability to convert scientific and short texts into a structured format which can be easily processed by computer. Moreover, it provides knowledge workers with extra time to re-think their written text and to view their knowledge from another angle. Department of Industrial and Systems Engineerin...|$|R
40|$|AbstractRelation {{extraction}} is a {{very useful}} task for several natural language processing applications, such as automatic summarization and question answering. In this paper, we present our hybrid approach to extracting relations between Arabic named entities. Given that Arabic is a rich morphological language, we build a linguistic and learning model to predict the positions of words that express a semantic relation within a clause. The main idea is to employ <b>linguistic</b> <b>modules</b> to ameliorate the results that are obtained from a machine learning-based method. Our method achieves encouraging performance. The empirical {{results indicate that the}} hybrid approach outperformed both the rule-based system (by 12 %) and the machine learning-based approaches (by 9 %) in terms of the F-score, to achieve 75. 2 % when applied to the same standard testing dataset, ANERCorp...|$|R
40|$|Abstract. Initial {{experiments}} in learning valence (subcategorisation) frames of Polish verbs from a morphosyntactically annotated corpus are reported here. The learning algorithm {{consists of a}} <b>linguistic</b> <b>module,</b> responsible for very simple shal-low parsing of the input text (nominal and prepositional phrase recognition) and for the identification of valence frame cues (hypotheses), and a statistical module which implements three well-known inferential statistics (likelihood ratio, t test, binomial miscue probability test). The results of the three statistics are evaluated and compared with a baseline approach of selecting frames {{on the basis of}} the rel-ative frequencies of frame/verb co-occurrences. The results, while clearly reflecting the many deficiencies of the linguistic analysis and the inadequacy of the statis-tical measures employed here for a free word order language rich in ellipsis and morphosyntactic syncretisms, are nevertheless promising. ...|$|R
