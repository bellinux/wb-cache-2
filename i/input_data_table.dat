5|10000|Public
40|$|Abstract. The paper {{presents}} an utilization of formal concept analysis in input data preprocessing for machine learning. Two preprocessing methods are presented. The first one consists in extending {{the set of}} attributes describing objects in <b>input</b> <b>data</b> <b>table</b> by new attributes and the second one consists in replacing the attributes by new attributes. In both methods the new attributes are defined by certain formal concepts computed from <b>input</b> <b>data</b> <b>table.</b> Selected formal concepts are so-called factor concepts obtained by boolean factor analysis, recently described by FCA. The ML method used to demonstrate the ideas is decision tree induction. The experimental evaluation and comparison of performance of decision trees induced from original and preprocessed input data is performed with standard decision tree induction algorithms ID 3 and C 4. 5 on several benchmark datasets. ...|$|E
40|$|Abstract: Class-oriented concept lattices are {{systems of}} {{conceptual}} clusters, called class-oriented concepts, which are partially ordered by a subconcept-superconcept hierarchy. The hierarchical structure represents a structured information obtained automatically from the <b>input</b> <b>data</b> <b>table.</b> This paper presents the correspondent relations between matroids and class-oriented concept lattices. Under isomorphism, it presents necessary and sufficient conditions to discuss sub-contexts to be compatible by matroid theory. The paper also contains illustrative examples...|$|E
40|$|In {{this paper}} we present some {{strategies}} for synthesis of decision algorithms studied by us. These strategies {{are used by}} systems of communicating agents and lead from the original (<b>input)</b> <b>data</b> <b>table</b> to a decision algorithm. The agents are working with parts of data and they compete for the decision algorithm with the best quality of object classification. We give examples of techniques for searching for new features and we discuss some adaptive strategies based on the rough set approach {{for the construction of}} a decision algorithm from a data table. We also discuss a strategy of clustering by tolerance. 1...|$|E
40|$|The paper {{discusses}} oil distillation into target {{products with}} regard to the unclear separation effect. As the <b>input</b> <b>data,</b> the <b>table</b> of physical and chemical properties of narrow fractions is used. The problem is solved using the dynamic programming method. The methodology is realized in the authors’ software product...|$|R
40|$|Abstract. When the {{evolution}} of variables over time is relevant to a classification task, established classifiers cannot be applied directly as the typical <b>input</b> format (<b>data</b> <b>table)</b> is not appropriate. We propose a new representation of temporal patterns that includes constraints on (partial) presence, (partial) absence {{as well as the}} duration of temporal predicates. A general-to-specific search-based algorithm is presented to derive classification rules. The approach evaluates promising on artificial and real data. ...|$|R
40|$|This {{document}} presents Mars Global Reference Atmospheric Model 2001 Version (Mars-GRAM 2001) and its new features. As {{with the}} previous version (mars- 2000), all parameterizations fro temperature, pressure, density, and winds versus height, latitude, longitude, time of day, and season (Ls) use <b>input</b> <b>data</b> <b>tables</b> from NASA Ames Mars General Circulation Model (MGCM) for the surface through 80 -km altitude and the University of Arizona Mars Thermospheric General Circulation Model (MTGCM) for 80 to 70 km. Mars-GRAM 2001 is based on topography from the Mars Orbiter Laser Altimeter (MOLA) and includes new MGCM data at the topographic surface. A new auxiliary program allows Mars-GRAM output {{to be used to}} compute shortwave (solar) and longwave (thermal) radiation at the surface and top of atmosphere. This memorandum includes instructions on obtaining Mars-GRAN source code and data files and for running the program. It also provides sample input and output and an example for incorporating Mars-GRAM as an atmospheric subroutine in a trajectory code...|$|R
40|$|Concept lattice is an {{efficient}} tool for knowledge representation and knowledge discovery and {{is applied to}} many fields successfully. However, in many real life applications, the problem under investigation cannot be described by formal concepts. Such concepts are called the non-definable concepts. The hierarchical structure of formal concept (called concept lattice) represents a structural information which obtained automatically from the <b>input</b> <b>data</b> <b>table.</b> We {{deal with the problem}} in which how further additional information be supplied to utilize the basic object attribute data table. In this paper, we provide rough concept lattice to incorporate the rough set into the concept lattice by using equivalence relation. Some results are established to illustrate the paper...|$|E
40|$|Abstract. Formal concept {{analysis}} {{is a method}} of exploratory data analysis that aims at the extraction of natural clusters from objectattribute data tables. The clusters, called formal concepts, can be similar to human-perceived concepts in a traditional sense and can be partially ordered by a subconcept-superconcept hierarchy. The hierarchical structure of formal concepts (so-called concept lattice) represents structured information obtained automatically from the <b>input</b> <b>data</b> <b>table.</b> The goal {{of this paper is}} to describe a method of evaluation of ANEWS questionnaire by Formal concept analysis. We describe a method adjustment of questionnaire by scaling to classical formal context. After that we separate some attributes to groups and make so-called ”aggregate atributes”. This way we make modified formal context and calculate formal concept lattice. We define term ”characteristic function ” for every concept. This is function, which for given extent or intent return a real number, which characterized this concept and is important for evaluation. Our method i...|$|E
40|$|To {{investigate}} the running {{characteristics of a}} switched reluctance motor, the static characteristics and related <b>input</b> <b>data</b> <b>tables</b> are required. The static characteristics comprise of flux linkage, co-energy and static torque characteristics. The co-energy and static torque are calculated once data of magnetization characteristics is available. The data of co-energy {{is required for the}} calculation of static torque characteristics. The simulation model includes the data of static characteristics for prediction of the instantaneous and steady state performance of the motor. In this research a computer based procedure of experiments is carried out for measurement of the magnetization characteristics. For every set of measurements, the removal of eddy current is carefully addressed. The experiments are carried out on an existing 8 / 6 pole rotary switched reluctance motor. Additionally, the instantaneous phase current, instantaneous torque and flux waveforms are produced by using linear, which is by default and spline data interpolation separately. The information obtained from theses simulation results will help in an improved simulation model for predicting the performance of the machine...|$|R
40|$|We present Ringo, {{a system}} for {{analysis}} of large graphs. Graphs provide a way to represent and analyze systems of interacting objects (people, proteins, webpages) with edges between the objects denoting interactions (friendships, physical interactions, links). Mining graphs provides valuable insights about individual objects {{as well as the}} relationships among them. In building Ringo, we take advantage of the fact that machines with large memory and many cores are widely available and also relatively affordable. This allows us to build an easy-to-use interactive high-performance graph analytics system. Graphs also need to be built from <b>input</b> <b>data,</b> which often resides in the form of relational tables. Thus, Ringo provides rich functionality for manipulating raw <b>input</b> <b>data</b> <b>tables</b> into various kinds of graphs. Furthermore, Ringo also provides over 200 graph analytics functions that can then be applied to constructed graphs. We show that a single big-memory machine provides a very attractive platform for performing analytics on all but the largest graphs as it offers excellent performance and ease of use as compared to alternative approaches. With Ringo, we also demonstrate how to integrate graph analytics with an iterative process of trial-and-error data exploration and rapid experimentation, common in data mining workloads. Comment: 6 pages, 2 figure...|$|R
40|$|This report {{presents}} Mars Global Reference Atmospheric Model 2000 Version (Mars-GRAM 2000) and its new features. All parameterizations for temperature, pressure, density, {{and winds}} versus height, latitude, longitude, time of day, and L(sub s) {{have been replaced}} by <b>input</b> <b>data</b> <b>tables</b> from NASA Ames Mars General Circulation Model (MGCM) for the surface through 80 -km altitude and the University of Arizona Mars Thermospheric General Circulation Model (MTGCM) for 80 to 170 km. A modified Stewart thermospheric model is still used for higher altitudes and for dependence on solar activity. "Climate factors" to tune for agreement with GCM data are no longer needed. Adjustment of exospheric temperature is still an option. Consistent with observations from Mars Global Surveyor, a new longitude-dependent wave model is included with user input to specify waves having 1 to 3 wavelengths around the planet. A simplified perturbation model has been substituted for the earlier one. An input switch allows users to select either East or West longitude positive. This memorandum includes instructions on obtaining Mars-GRAM source code and data files and for running the program. It also provides sample input and output and an example for incorporating Mars-GRAM as an atmospheric subroutine in a trajectory code...|$|R
40|$|Novel {{computing}} {{devices are}} exploited for numerical computation. The solution of a numerical problem is sought, {{which has been}} solved many times before, but {{this time with a}} different set of <b>input</b> <b>data.</b> A <b>table</b> is a classical way to collect the old solutions in order to exploit them to find the new one. This process is extended to more general problems than the usual function value approximation. To do this, a new concept of table is introduced. These tables are addressed associatively. Several problems are treated both theoretically and computationally. These problems include solving linear systems of equations, partial differential equations, nonlinear systems of ordinary differential equations, and Karmarkar's algorithm. Hardware requirements are discussed...|$|R
40|$|Description The qrfactor package {{simultaneously}} runs both Q and R mode factor analyses. The pack-age {{contains only}} one function called qrfactor() that can perform PCA, R-mode Factor Analy-sis, Q-mode Factor Analysis, Simultaneous R- and Q-mode Factor Analysis, Principal Coordi-nate Analysis, as wells as Multidimensional Scaling (MDS). Loadings and scores can eas-ily be computed from the simulation. The plot. qrfactor() function offers several annotated bi-plots for all possible combinations of eigenvectors, loadings, and scores. <b>Input</b> <b>data</b> in-cludes shapefiles, <b>tables</b> and datafram...|$|R
40|$|The article {{describes}} the training program {{that takes into account}} the laws of human mental activity. The peculiarity of this training is that it enables specialists to generate additional intellectual potentials. This potentials relate to various kinds of illogical (intuitive) and hybrid (human-machine) intelligences that are associated with the quality of professional and creative activity in future. Progress in trans-phenomenal functional systems of the brain is possible by means of special procedures and computer-based training programs. The article offers a logical system of computer-aided training. This system repeatedly compares its <b>input</b> and output <b>data</b> and uses a feedback as the innate human characteristic to obtain the final result. Computer-aided training can be implemented in analog and numeric form. The article deals with the numeric widely used MS Excel-based software. It provides tabular and graphic style visualization desirable for training. A specific technical program is used for training. It allows specialists to develop a reinforcement shell of square cell type of minimal weight. In designing various constraints can be considered. On the main page of MS Excel program there are <b>input</b> <b>data</b> <b>tables,</b> various forms of the cell, methods of its manufacturing, and a scale table. Coefficients needed for calculations are in the next tables. The next tables contain also technological and design limitations. The basic table enables us to vary the geometric complexes of the reinforcement shell. Using the tabular and graphical results of calculations an operator can define values of further steps (iterations). The operator’s aim is to design the shell of minimum weight for the shortest time or number of steps. There is number of counters using macros in the MS Excel tables to calculate the number of past iterations. </p...|$|R
40|$|In {{this paper}} we {{introduce}} {{a method to}} detect words or phrases in a given sequence of alphabets without knowing the lexicon. Our linear time unsupervised algorithm relies entirely on statistical relationships among alphabets in the input sequence to detect location of word boundaries. We compare our algorithm to previous approaches from unsupervised sequence segmentation literature and provide superior segmentation over number of benchmarks. Comment: This paper has been withdrawn by the authors. The paper has been withdrawn due to error <b>data</b> <b>input</b> in <b>table</b> no. ...|$|R
5000|$|... #Caption: A {{self-organizing}} map showing U.S. Congress voting patterns. The <b>input</b> <b>data</b> was a <b>table</b> with a row for {{each member of}} Congress, and columns for certain votes containing each member's yes/no/abstain vote. The SOM algorithm arranged these members in a two-dimensional grid placing similar members closer together. The first plot shows the grouping when the data are split into two clusters. The second plot shows average distance to neighbours: larger distances are darker. The third plot predicts Republican (red) or Democratic (blue) party membership. The other plots each overlay the resulting map with predicted values on an input dimension: red means a predicted 'yes' vote on that bill, blue means a 'no' vote. The plot was created in Synapse.|$|R
40|$|Databases which {{store and}} manage {{long-term}} scientific {{information related to}} life science are used to store huge amount of quantitative attributes. Introduction of a new entity attribute requires modification of the existing <b>data</b> <b>tables</b> and the programs that use these <b>data</b> <b>tables.</b> A feasible solution is increasing the virtual <b>data</b> <b>tables</b> {{while the number of}} screens remains the same. The main objective {{of the present study was}} to introduce a logic called Joker Tao (JT) which provides universal data storage for cloud-based databases. It means all types of <b>input</b> <b>data</b> can be interpreted as an entity and attribute at the same time, in the same <b>data</b> <b>table...</b>|$|R
40|$|Abstract. The Boolean factor {{analysis}} is an established method for analysis and preprocessing of Boolean data. In the basic setting, {{this method is}} designed for finding factors, new variables, which may explain or describe the original <b>input</b> <b>data.</b> Many real-world data sets are more complex than a simple <b>data</b> <b>table.</b> For example almost every web database is composed from many <b>data</b> <b>tables</b> and relations between them. In this paper we present {{a new approach to}} the Boolean {{factor analysis}}, which is tailored for multi-relational data. We show our approach on simple examples and also propose future research topics. ...|$|R
40|$|The Aircraft Noise Prediction Program's High Speed Research {{prediction}} system (ANOPP-HSR) is introduced. This mini-manual is {{an introduction}} which gives {{a brief overview}} of the ANOPP system and the components of the HSR prediction method. ANOPP information resources are given. Twelve of the most common ANOPP-HSR control statements are described. Each control statement's purpose and format are stated and relevant examples are provided. More detailed examples of the use of the control statements are presented in the manual along with ten ANOPP-HSR templates. The purpose of the templates is to provide the user with working ANOPP-HSR programs which can be modified to serve particular prediction requirements. Also included in this manual is a brief discussion of common errors and how to solve these problems. The appendices include the following useful information: a summary of all ANOPP-HSR functional research modules, a data unit directory, a discussion of one of the more complex control statements, and <b>input</b> <b>data</b> unit and <b>table</b> examples...|$|R
40|$|The {{concept of}} {{automated}} testing {{and the importance}} of its use in the process of software development is considered. Describes the procedure for the automated testing software with complex logic. A notion decision tables and the need for the use in automated testing. By introducing the concept of the system, which corresponds to a certain combination of conditions Modification of decision <b>tables.</b> <b>Input</b> <b>data</b> in the <b>tables</b> are generated using methods of expert assessments. Subject to the availability of several alternative actions for a certain combination of conditions applied the methods of decision making under uncertainty, in particular, the Hurwitz criterion to determine the usefulness of the execution of a strategy based on the specified resources to conduct testing to ensure the highest possible quality of the product. Hurwitz criterion considered for several values of the coefficient of optimism, a modification of the calculation criteria, by ignoring the zero values of expert evaluations. ??????????????? ??????? ??????????????????? ???????????? ? ???????????? ??? ????????????? ? ???????? ?????????? ???????????? ???????????. ??????????? ???????? ?????????? ??????????????????? ???????????? ???????????? ???????? ?? ??????? ???????. ???????????? ??????? ?????? ???????? ??????? ? ????????????? ?? ?????????? ? ?????????????????? ????????????. ????? ???????? ??????? ????????? ???????, ??????? ????????????? ???????????? ?????????? ??????? ????????? ??????????? ?????? ???????? ???????. ??????? ?????? ? ??????? ??????????? ? ??????? ??????? ?????????? ??????. ??? ??????? ??????? ?????????? ?????????????? ???????? ??? ???????????? ?????????? ??????? ????????? ?????? ???????? ??????? ? ???????? ????????????????, ? ????????? ???????? ??????? ??? ??????????? ?????????? ?????????? ??? ??? ???? ????????? ? ??????????? ?? ???????? ???????? ?? ?????????? ???????????? ??? ??????????? ??????????? ?????????? ???????? ???????????? ????????. ???????? ??????? ?????????? ??? ?????????? ???????? ???????????? ?????????, ?????????????? ???????? ??????? ????????, ????? ????????????? ??????? ???????? ?????? ?????????...|$|R
40|$|In Paper I {{we present}} a new model for the Galactic {{distribution}} of free electrons. In this paper we describe the <b>input</b> <b>data</b> and methodology for determining the structure and parameters of the model. <b>Tables</b> of the <b>input</b> <b>data</b> are provided and several figures are used to demonstrate why particular Galactic structures are needed. We identify lines of sight on which discrete regions either enhance or diminish the dispersion or the scattering. Most do not coincide with known H 2 regions or supershells, most likely because the enhancements correspond to column densities smaller than detection thresholds for the emission measure in recombination-line surveys. Comment: 41 pages, 13 Figures, extensive <b>data</b> <b>table...</b>|$|R
40|$|A near {{real-time}} physical transportation {{network routing}} system comprising: a traffic simulation computing grid and a dynamic traffic routing service computing grid. The traffic simulator produces traffic network travel time predictions for a physical transportation network using a traffic simulation model and common <b>input</b> <b>data.</b> The physical transportation network {{is divided into}} a multiple sections. Each section has a primary zone and a buffer zone. The traffic simulation computing grid includes multiple of traffic simulation computing nodes. The common <b>input</b> <b>data</b> includes static network characteristics, an origin-destination <b>data</b> <b>table,</b> dynamic traffic information data and historical traffic data. The dynamic traffic routing service computing grid includes multiple dynamic traffic routing computing nodes and generates traffic route(s) using the traffic network travel time predictions...|$|R
40|$|The Integrated Math Model for Cryogenic Systems is a flexible, broadly {{applicable}} systems parametric analysis tool. The {{program will}} effectively accommodate systems of considerable complexity involving {{large numbers of}} performance dependent variables such as {{are found in the}} individual and integrated cryogen systems. Basically, the program logic structure pursues an orderly progression path through any given system in much the same fashion as is employed for manual systems analysis. The system configuration schematic is converted to an alpha-numeric formatted configuration <b>data</b> <b>table</b> <b>input</b> starting with the cryogen consumer and identifying all components, such as lines, fittings, and valves, each in its proper order and ending with the cryogen supply source assembly. Then, for each of the constituent component assemblies, such as gas generators, turbo machinery, heat exchangers, and accumulators, the performance requirements are assembled in <b>input</b> <b>data</b> tabulations. Systems operating constraints and duty cycle definitions are further added as <b>input</b> <b>data</b> coded to the configuration operating sequence...|$|R
40|$|While {{developing}} data-centric programs, users often run (portions of) {{their programs}} over real data, {{to see how}} they behave and what the output looks like. Doing so makes it easier to formulate, understand and compose programs correctly, compared with examination of program logic alone. For large <b>input</b> <b>data</b> sets, these experimental runs can be time-consuming and inefficient. Unfortunately, sampling the <b>input</b> <b>data</b> does not always work well, because selective operations such as filter and join can lead to empty results over sampled inputs, and unless certain indexes are present {{there is no way to}} generate biased samples efficiently. Consequently new methods are needed for generating example <b>input</b> <b>data</b> for data-centric programs. We focus on an important category of data-centric programs, dataflow programs, which are best illustrated by displaying the series of intermediate <b>data</b> <b>tables</b> that occur between each pair of operations. We introduce and study the problem of generating example intermediate data for dataflow programs, in a manner that illustrates the semantics of the operators while keeping the example data small. We identify two major obstacles that impede naive approaches, namely (1) highly selective operators and (2) noninvertible operators, and offer techniques for dealing with these obstacles. Our techniques perform well on real dataflow programs used at Yahoo! for web analytics...|$|R
40|$|A {{computer}} program for rapid parametric evaluation {{of various types}} of cryogenics spacecraft systems is presented. The mathematical techniques of the program provide the capability for in-depth analysis combined with rapid problem solution {{for the production of}} a large quantity of soundly based trade-study data. The program requires a large data bank capable of providing characteristics performance data {{for a wide variety of}} component assemblies used in cryogenic systems. The program data requirements are divided into: (1) the semipermanent <b>data</b> <b>tables</b> and source <b>data</b> for performance characteristics and (2) the variable <b>input</b> <b>data</b> which contains <b>input</b> parameters which may be perturbated for parametric system studies...|$|R
40|$|We present {{sensitivity}} analysis for results of query executions in a relational model of data extended by ordinal ranks. The underlying model of data {{results from the}} ordinary Codd's model of data in which we consider ordinal ranks of tuples in <b>data</b> <b>tables</b> expressing degrees to which tuples match queries. In this setting, we show that ranks assigned to tuples are insensitive to small changes, i. e., small changes in the <b>input</b> <b>data</b> do not yield large changes in the results of queries. Comment: The paper will appear in Proceedings of the 19 th International Conference on Applications of Declarative Programming and Knowledge Management (INAP 2011...|$|R
50|$|<b>Input</b> <b>data</b> is {{classified}} into two categories: relative <b>input</b> <b>data</b> and absolute <b>input</b> <b>data.</b>|$|R
5000|$|Integrated <b>Data</b> <b>table</b> supportTesters {{can write}} {{data-driven}} tests {{by using the}} Integrated <b>data</b> <b>table</b> support. <b>Data</b> <b>tables</b> can either be as Excel files or XML format.|$|R
40|$|The {{acoustic}} {{representation of}} complex visual structures involves both synthesized speech and non-speech audio signals. Though progress in speech synthesis allows the consistent control of {{an abundance of}} parameters, like prosody through appropriate mark-up, {{there is not enough}} experimentally proven specification <b>input</b> <b>data</b> to drive a Voice Browser for such purposes. This paper reports on the results from a series of psychoacoustic experiments aiming to provide natural speech prosodic specification for the task of vocalizing tables. Blind and sighted listeners were asked to reconstruct simple and complex <b>data</b> <b>tables</b> from naturally spoken descriptions. From the listeners’ feedback it was deducted that consistent prosodic rendering can model the underlying semantic structure of tables. ...|$|R
40|$|The {{modeling}} and simulation of Switched Reluctance (SR) machine and drives is challenging for its dual pole salient structure and magnetic saturation. This paper presents the <b>input</b> <b>data</b> in form of experimentally obtained magnetization characteristics. This data was used for computer simulation based model of SR machine, “Selecting Best Interpolation Technique for Simulation Modeling of Switched Reluctance Machine” [1], “Modeling of Static Characteristics of Switched Reluctance Motor” [2]. This data is primary source of other <b>data</b> <b>tables</b> of co energy and static torque which are also among the required data essential for the simulation and {{can be derived from}} this data. The procedure and experimental setup for collection of the data is presented in detail...|$|R
30|$|In this paper, by an unlabeled <b>input</b> <b>data,</b> we {{mean that}} it is unknown to which group the <b>input</b> <b>data</b> belongs. If all the <b>input</b> <b>data</b> are unlabeled, {{it means that the}} {{distribution}} of the <b>input</b> <b>data</b> is unknown.|$|R
40|$|This paper gives a {{full and}} {{detailed}} account of the design and development of a web-based learning portfolio (WBLP) system for authentic assessment, in the hope to help record, display, and monitor student learning process. The functions of the WBLP system include portfolio creation, portfolio browse, portfolio guide, portfolio discussion board, portfolio class bulletin, suggestion board, student data maintenance, and system management. Databases used in the WBLP include student portfolio database (including student basic <b>data</b> <b>table,</b> portfolio <b>data</b> <b>table,</b> course work <b>data</b> <b>table),</b> discussion database (including topic <b>data</b> <b>table,</b> article <b>data</b> <b>table),</b> bulletin database (including news <b>data</b> <b>table)</b> ...|$|R
5000|$|... where Yi• is {{the mean}} of the ith row of the <b>data</b> <b>table,</b> Y•j is {{the mean of}} the jth column of the <b>data</b> <b>table,</b> and Y•• is the overall mean of the <b>data</b> <b>table.</b>|$|R
40|$|Compton {{scattering}} is {{a potential}} tool for the determination of bone moneral content or tissue density for dose planning purposes, and requires knowledge of the energy distribution of the X-rays through biological materials of medical interest in the X-ray and gamma-ray region. The energy distribution is utilized {{in a number of}} ways in diagnostic radiology, for example, in determining primary photon spectra, electron densities in separate volumes, and in tomography and imaging. The choice of the Xray energy is more related to X-ray absorption, where as that of the scattering angle is more related to geometry. The evaluation of all the contributions are mandatory in Compton profile measurements and is important in X-ray imaging systems in order to achieve good results. In view of this, Compton profile cross-sections for few biological materials are estimated at nineteen Kalpha X-ray energies and 60 keV (Am- 241) photons. Energy broadening,geometrical broadening from 1 to 180 degrees, FWHM of J(Pz) and FWHM of Compton energy broadening has been evaluated at various incident photon energies. These values are estimated around the centroid of the Compton profile with an energy interval of 0. 1 keV and 1. 0 keV for 60 keV photons. The interaction cross sections for the above materials are estimated using fractions-by-weight of the constitutent elements. <b>Input</b> <b>data</b> for these <b>tables</b> are purely theoretical...|$|R
40|$|Recently, {{an article}} by Simoni et al. (2000), who used (i) SAAP {{analysis}} to analyze the population frequencies of mtDNA haplogroups and (ii) AIDA analysis to examine both the frequency and the sequence similarity of truncated mtDNA sequences, appeared in this Journal. The main outcome of their study was that “the overall patterns of mtDNA diversity appear to be poorly significant in Europe. ” The raw data comprised 2, 619 hypervariable segment I (HVS-I) sequences (denoted as “HVR-I” [hypervariable region I] sequences by Simoni et al. [2000]) that were obtained from 36 regions or populations of Europe, the Near East, and the Caucasus and that were collected from both the literature and unpublished sources. Simoni et al. ostensibly grouped the HVS-I sequences according to haplogroup motifs proposed elsewhere (Richards et al. 1998), and they reported the resulting frequencies for each region/population in table 3 in their study. We have checked the <b>input</b> <b>data</b> displayed in <b>table</b> 3 and have found serious technical errors affecting numerous entries. More critically, the mtDNA categories that they report correspond neither to their own criteria nor to the haplogroup definitions established in the literature (to which they refer). Furthermore, their decision to truncate HVS-I information (and to disregard RFLP information) renders these data inadequate to differentiate even African and East Asian sequences from European sequences in many cases...|$|R
40|$|The {{steps of}} the process for {{conducting}} a simulation modeling and analysis project include: problem formulation, project planning, system definition, <b>input</b> <b>data</b> collection, model translation, verification, validation, experimental design, analysis. Simulation project involves the collection of <b>input</b> <b>data,</b> analysis of the <b>input</b> <b>data,</b> {{and use of the}} analysis of the <b>input</b> <b>data</b> in the simulation model. The <b>input</b> <b>data</b> may be either obtained from historical records or collected in real time as a task in the simulation project. The analysis involves the identification of the theoretical distribution that represents the <b>input</b> <b>data.</b> The use of the <b>input</b> <b>data</b> in the model involves specifying the theoretical distributions in the simulation program code. If we successfully fit the observed data to a theoretical distribution, then any data value from the theoretical distribution may drive the simulation model...|$|R
