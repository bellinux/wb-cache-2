28|35|Public
5000|$|Python Software Transactional Memory and <b>Dual-Port</b> <b>Memory</b> Based Python Software Transactional Memory, two {{versions}} of Python STM that is being developed on University of Novi Sad.|$|E
50|$|IDT’s {{first product}} {{was the first}} low-power, {{high-speed}} CMOS-based 6116 static random-access memory (SRAM) device, released in 1981, followed by the first CMOS FIFO introduced in 1982. Subsequent achievements include the first <b>dual-port</b> <b>memory,</b> pioneering in embedded RISC processors, leadership in network search engines and the first flow-control management device.|$|E
40|$|The {{application}} of <b>dual-port</b> <b>memory</b> (DPM) -based dual and triple microprocessor systems {{for improving the}} speed of transducer output signal processing which involves corrections, using inter-polating polynomials, for input-output nonlinearities and effects of disturbing variables, is presented. Significant processing speed improvement over single microprocessor implementation is shown by analytical evaluation...|$|E
5000|$|VRAM (Video {{random access}} memory) An older type of <b>dual-ported</b> <b>memory</b> once {{used for the}} frame buffers of video {{adapters}} (video cards).|$|R
40|$|Proposed is {{a unique}} cell {{histogram}} architecture which will process k data items in parallel to compute 2 q histogram bins per time step. An array of m/ 2 q cells computes an m-bin histogram with a speed-up factor of k; k ? 2 makes it faster than current <b>dual-ported</b> <b>memory</b> implementations. Furthermore, simple mechanisms for conflict-free storing of the histogram bins into an external memory array are discussed...|$|R
40|$|ISBN: 0818621575 The authors {{present a}} novel {{approach}} to the test of multi-port RAMs. A novel fault model {{that takes into account}} complex couplings resulting from simultaneous access of memory cells is used in order to ensure a very high fault coverage. A novel algorithm for the test of <b>dual-port</b> <b>memories</b> is detailed. This algorithm achieves O(n) complexity thanks to the use of some topological restrictions. The authors also present a novel built-in self-test (BIST) scheme, based on programmable schematic generators, that allows great flexibility for ASIC (application-specific integrated circuit) design...|$|R
30|$|Similarly to Arch. V-B, a three-port memory {{would be}} {{required}} because of the decoding pipeline; the same considerations of Section 5.2 still hold, and an optimum partitioning of the v 2 c memory onto two banks with some redundancy can be found. Note that, as opposed to Arch. V-B, a customary <b>dual-port</b> <b>memory</b> is enough for c 2 v messages.|$|E
30|$|Athalye et al. [7] {{presented}} generic architectures for {{the implementation}} of the Sequential Importance Resampling Filter (SIRF). The proposed architecture is based on using <b>dual-port</b> <b>memory.</b> The memory stores the addresses of the particles in its upper half, while the sampled particles are stored in {{the lower half of the}} memory. The idea is that the resampling unit returns the set of indexes (pointers) of the replicated particles instead of the particles themselves. Using index addressing alone does not ensure that the scheme with the single memory will work correctly. They used other memories to store the indexes of the replicated particles and the discarded particles. The size of overall used memory is 4 M: 2 M depth <b>dual-port</b> <b>memory</b> to store the addresses and particles state vectors, M depth memory to store the replicated particles indexes and M depth FIFO to store the discarded particle indexes. They proposed two architectures to implement the SIRF using systematic resampling (SR) and using residual systematic resampling (RSR) algorithms.|$|E
30|$|The v 2 c buffer is the {{key element}} that allows the {{architecture}} to work in pipeline. This has to sustain one reading and one writing access concurrently and can be efficiently implemented with shift-register based architectures for EOP (first-in, first-out, FIFO buffer) and ROP (last-in, first-out, LIFO buffer). On the contrary, UOP needs to map the buffer onto a <b>dual-port</b> <b>memory</b> bank, whose (reading) address is provided by and extra configuration memory (ROM).|$|E
50|$|In a {{distributed}} memory system there is typically a processor, a memory, and {{some form of}} interconnection that allows programs on each processor to interact with each other. The interconnect can be organised with point to point links or separate hardware can provide a switching network. The network topology is {{a key factor in}} determining how the multiprocessor machine scales. The links between nodes can be implemented using some standard network protocol (for example Ethernet), using bespoke network links (used in for example the Transputer), or using <b>dual-ported</b> <b>memories.</b>|$|R
30|$|The buffer stores each AABB’s data in an {{efficient}} manner for processing by the subsequent compare operation. During initialisation, the buffer reads each AABB and stores {{the data in}} 6 m internal <b>dual-port</b> <b>memories.</b> The 6 m memories correspond to m memories {{for each of the}} minimum x, maximum x, minimum y, maximum y, minimum z and maximum z values. The data are replicated across each set of m memories, so that each of the m memories contains the same data. This results in six logical 2 m-port memories, allowing 12 m data to be outputted in a single clock cycle.|$|R
30|$|Finally, the Virtex 2 6000 FPGA on the Wildstar II {{has some}} useful {{features}} {{that we use}} to our advantage. A large amount of on-chip memory {{is available in the}} form of BlockRAMs, which are configurable in width and depth but can hold at most 2 [*]KB of data each. One hundred forty four of these <b>dual-ported</b> <b>memories</b> are available, each of which can be accessed independently. This makes BlockRAMs a good candidate for storing and accessing input projection data (see Sections 4.2. 4 and 4.4. 3.) BlockRAMs can also be configured as FIFOs, and due to their dual-ported nature, can be used to cross clock domains.|$|R
40|$|Stratix ® II devices {{contain a}} {{two-dimensional}} row- and column-based architecture to implement custom logic. A series of column and row interconnects of varying length and speed provides signal interconnects between logic array blocks (LABs), memory block structures (M 512 RAM, M 4 K RAM, and M-RAM blocks), and {{digital signal processing}} (DSP) blocks. Each LAB consists of eight adaptive logic modules (ALMs). An ALM is the Stratix II device family’s basic building block of logic providing efficient implementation of user logic functions. LABs are grouped into rows and columns across the device. M 512 RAM blocks are simple <b>dual-port</b> <b>memory</b> blocks with 512 bits plus parity (576 bits). These blocks provide dedicated simple dual-port or single-port memory up to 18 -bits wide at up to 500 MHz. M 512 blocks are grouped into columns across the device in between certain LABs. M 4 K RAM blocks are true <b>dual-port</b> <b>memory</b> blocks with 4 K bits plus parity (4, 608 bits). These blocks provide dedicated true dual-port, simple dual-port, or single-port memory up to 36 -bits wide at up to 550 MHz. These blocks are grouped into columns across the device in between certain LABs. M-RAM blocks are true <b>dual-port</b> <b>memory</b> blocks with 512 K bits plus parity (589, 824 bits). These blocks provide dedicated true dual-port, simple dual-port, or single-port memory up to 144 -bits wide at up to 420 MHz. Several M-RAM blocks are located individually in the device's logic array. DSP blocks can implement up to either eight full-precision 9 × 9 -bit multipliers, four full-precision 18 × 18 -bit multipliers, or one full-precision 36 × 36 -bit multiplier with add or subtract features. The DSP blocks support Q 1. 15 format rounding and saturation in the multiplier and accumulator stages. These blocks also contain shift registers for digital signal processing applications, including finite impulse response (FIR) and infinite impulse response (IIR) filters. DSP blocks are grouped into columns across the device and operate at up t...|$|E
40|$|In this paper, a {{systematic}} approach is proposed to develop high throughput decoder for structured (quasi-cyclic) low density parity check (LDPC) block codes. Based on {{the properties of}} quasi-cyclic LDPC codes, the two stages of belief propagation decoding algorithm could be overlapped and thus the overall decoding latency is reduced. To avoid the memory access conflict, the maximum concurrency of the two stages is explored by a novel scheduling algorithm. Consequently, the decoding throughput could be increased by almost twice assuming <b>dual-port</b> <b>memory</b> is available...|$|E
3000|$|... in Figure 8) and one writing. This memory can be {{implemented}} by distributing data on several banks of customary <b>dual-port</b> <b>memory,</b> {{in such a way}} that two readings always involve different banks. Actually, in a layered decoder a same memory location needs to be accessed several times per iteration and concurrently to several other data, so that resorting to only two memory banks would be unfeasible. On the other hand, the management of a higher number of banks would add a significant overhead to the complexity of the whole design.|$|E
40|$|The {{testability}} {{problem of}} <b>dual-port</b> <b>memories</b> is investigated. A functional model is defined, and architectural modifications {{to enhance the}} testability of such chips are described. These modifications allow multiple access of memory cells for increased test speed with minimal overhead on both silicon area and device performance. New fault models are proposed, and efficient O(√n) test algorithms are described for both the memory array and the address decoders. The new fault models account for the simultaneous dual-access property of the device. In addition to the classical static neighborhood pattern-sensitive faults, the array test algorithm covers {{a new class of}} pattern sensitive faults, duplex dynamic neighborhood pattern-sensitive faults (DDNPSF...|$|R
40|$|This paper {{presents}} a streaming processor specif- ically designed for adaptronic and biomedical engineering applications. The main {{characteristics of the}} streaming processor are the exibility to implement oating-point-based scienti c computations commonly performed in the digital signal processing application. The oating-point operators are connected to <b>dual-port</b> <b>memories</b> through separated 3 operand-buses and 2 resultant-buses. Synthesized with 130 -nm technology, the Spectron can be clocked at 480 MHz. The processor can perform 4 parallel streaming/pipeline oating-point operations using its FPMAC and CORDIC cores, resulting in a performance of about 4 485 = 1 : 94 GFlops (Giga Floating-point operation per second), which is suitable for high performance image processing in biomedical electronic engineering application...|$|R
50|$|It {{was invented}} by F. Dill, D. Ling and R. Matick at IBM Research in 1980, with a patent issued in 1985 (US Patent 4,541,075). The first {{commercial}} use of VRAM was in a high-resolution graphics adapter introduced in 1986 by IBM for the PC/RT system, which set a new standard for graphics displays. Prior {{to the development of}} VRAM, <b>dual-ported</b> <b>memory</b> was quite expensive, limiting higher resolution bitmapped graphics to high-end workstations. VRAM improved the overall framebuffer throughput, allowing low cost, high-resolution, high-speed, color graphics. Modern GUI-based operating systems benefitted from this and thus it provided a key ingredient for proliferation of graphical user interfaces (GUIs) throughout the world at that time.|$|R
40|$|Two highly {{segmented}} plastic-scintillator arrays {{have been}} developed for proton detection in electron scattering experiments. The detectors subtend solid angles of 225 and 550 msr and cover energy ranges of 50 - 225 and 25 - 165 MeV, respectively. The charge and arrival time of each photomultiplier signal are digitized by flash ADCs and temporarily stored in a <b>dual-port</b> <b>memory.</b> The readout parameters are computer controlled, tuned, and monitored. These detectors have been employed in (e, e′p) and (e, e′pp) experiments for proton emission angles greater than 30 ° and for luminosities up to 1...|$|E
30|$|In the {{following}} sections, the first port of each <b>dual-port</b> <b>memory</b> will {{be referred to}} as A and the second port will {{be referred to as}} B. The six memories that contain each AABB’s data and that share an index will be referred to as a memory group. For example, memory group 0 contains minimum x memory 0, maximum x memory 0, minimum y memory 0, maximum y memory 0, minimum z memory 0 and maximum z memory 0. In {{the following}} sequence, the inputs to each memory belonging to a given memory group remain the same at all times.|$|E
30|$|Due to the {{significant}} memory access requirement for video encoding tasks, {{a large amount}} of clock cycles is consumed by the processing core while waiting for the data fetch from local memory spaces. To reduce or avoid the overhead of memory data access, the memory storage of video frame data can be organized to utilize multiple independent memory spaces (SRAM and DRAM) and <b>dual-port</b> <b>memory</b> (BRAM), in order to enable the parallel and pipelined memory access during the video encoding. This optimized requirement can practically provide the system architecture with the multi-port memory storage to reduce the data access bandwidth for each of the individual memory space.|$|E
40|$|INTERNATIONNAL JOURNAL OF APPLIED BIOMEDICAL ENGINEERING VOL. 6, NO. 1 2013 This paper {{presents}} a streaming processor {{specifically designed for}} adaptronic and biomedical engineering applications. The main characteristics of the streaming processor are the flexibility to implement floating-point-based scientific computations commonly performed in the digital signal processing application. The floating-point operators are connected to <b>dual-port</b> <b>memories</b> through separated 3 operand-buses and 2 resultant-buses. Synthesized with 130 -nm technology, the Spectron can be clocked at 480 MHz. The processor can perform 4 parallel streaming/pipeline floating-point operations us- ing its FPMAC and CORDIC cores, resulting in a performance of about 4 ?? 485 = 1. 94 GFlops (Giga Floating-point operation per second), which is suitable for high performance image processing in biomedical electronic engineering applications...|$|R
40|$|Abstract- A pipelined time {{digitizer}} CMOS gate-array {{has been}} developed using 0. 5 µm Sea-of-Gate technology. Precise timing signals which are used to sample input signals are generated from 32 taps of an asymmetric ring oscillator. The frequency of the oscillator is controlled by a PLL circuit which runs in the 10 - 50 MHz frequency range. A test chip {{has been developed}} and tested; a time resolution of 250 ps rms at 40 MHz clock was measured. The chip has 4 channels and encoding circuits for both the rising and the falling edges of the input signals. The chip has 128 -word <b>dual-port</b> <b>memories,</b> allowing the histories of the input signals to be stored and causing no deadtime for the conversion. I...|$|R
40|$|In {{computer}} vision crass-correlation {{is a standard}} approach for local feature matching in feature tracking applications. For this Normalized cross-correlation (NCC) is implemented on spatial domain, {{because it does not}} have a simple and efficient frequency domain expression. Therefore in applications which demand real-time processing, a dedicated hardware implementation of NCC is essential to meet the computational cost in spatial domain. In this paper we present a Field Programmable Gate Array (FPGA) implementation of NCC based on the multi-port memory controller together with Fast Normalize Cross-correlation. Practical experimentation shows that our system can achieve frame rates closer to 100 for a search window size of 100 x 100 and template size of 15 x 15, with only using two <b>dual-port</b> <b>memories...</b>|$|R
40|$|Abstract: Two {{embedded}} memory designs are proposed for video-signal processing. Concurrent line access performs multiple-port memory accesses at the hardware cost and access {{time of a}} single port. It uses 62. 24 % of the area required by a conventional <b>dual-port</b> <b>memory</b> and is only 7. 6 % larger than a single-port 2 K x 8 memory. The block-access mode combines address decoders and generators, yielding block-access mode times 26 % faster than conventional schemes for a 256 words x 32 bits memory size. Despite some preferred-access-order restrictions, the designs incur no loss of generality because video algorithms possess high data parallelism and low dependence. ...|$|E
40|$|Abstract—In this paper, a {{systematic}} approach is proposed to de-velop a high throughput decoder for quasi-cyclic low-density parity check (LDPC) codes, whose parity check matrix is constructed by circularly shifted identity matrices. Based on {{the properties of}} quasi-cyclic LDPC codes, the two stages of belief propagation decoding algorithm, namely, check node update and variable node update, could be overlapped and thus the overall decoding latency is reduced. To avoid the memory access conflict, the maximum concurrency of the two stages is explored by a novel scheduling algorithm. Consequently, the decoding throughput could be increased by about twice assuming <b>dual-port</b> <b>memory</b> is available. Index Terms—High throughput, low-density parity check (LDPC) codes, overlapped message passing (MP), quasi-cyclic codes. I...|$|E
40|$|In {{the advent}} of very high data rates of the {{upcoming}} 3 G long-term evolution telecommunication systems, there is a crucial need for efficient and flexible turbo decoder implementations. In this study, a max-log-MAP turbo decoder is implemented as an application-specific instruction-set processor. The processor is accompanied with accelerating computing units, which can be controlled in detail. With a novel memory interface, the <b>dual-port</b> <b>memory</b> for extrinsic information is avoided. As a result, processing one trellis stage with max-log-MAP algorithm takes only 1. 02 clock cycles on average, which is comparable to pure hardware decoders. With six turbo iterations and 277 [*]MHz clock frequency 22. 7 [*]Mbps, decoding speed is achieved on 130 [*]nm technology...|$|E
40|$|We have built, {{and used}} to take physics data, a {{digitizing}} and readout system for Brookhaven AGS Experiment 791, a high-rate search for rare kaon decays. All digitization- of charge and time information is “flash ” (performed in less than 200 ns), followed by front-end buffering and a pipelined readout with massive parallelism. A data transfer rate of [...] 0. 5 - Gigabytelsec into <b>dual-port</b> <b>memories</b> in eight 3081 -emulating processors has been achieved. A readout-supervising circuit coordinates the three levels of event triggering {{and the movement of}} data throughout the system. The host Micro-VAX is interrupted only for the uploading of packets of fully filtered events from the 3081 /E’s, Digitizing and data transfer from the front end to the 3081 /E’s contribute negligible deadtime to the experiment...|$|R
40|$|The CHORUS {{experiment}} {{searches for}} nu(mu) nu(tau) oscillation. To {{aid in the}} momentum reconstruction of charged hadrons, a honeycomb tracker was built with three orientations of six planes each. The planes are manufactured by point-welding together two precision folded conductive polycarbonate foils, forming hexagonal tubes with 30 mu m thick anode wires in the center. The honeycomb tracker in CHORUS is read out using a bitstream principle. The amplified signal of each wire is binary sampled every 5 ns and stored in a 256 bit circular buffer, implemented in <b>dual-port</b> <b>memories.</b> This technique allows a full reconstruction of a 1. 28 mu s history of each wire. Eighteen cards, each handling 72 wires, are read out over a single flat cable using a card-to-card pipeline. (C) 1998 Elsevier Science B. V. All rights reserved...|$|R
50|$|Reflective Memory is a {{means to}} share common data between {{different}} and independent systems deterministically 1. Such systems using a common reflective memory form a reflective memory network which is a deterministic one, when any system of the network acquired data and writes it to its local memory, such data is written locally to all other systems, this behaviour is like a <b>dual-ported</b> <b>memory</b> system. Reflective memory networks are real-time local area networks where each device or computer always has a local up-to-date copy of the shared data set. These networks are designed for highly deterministic data communications delivering tightly timed performance required on distributed control systems or simulations. Reflective memory technologies are focused to applications where determinism, simplicity for implementation and lack of software overhead are very important considerations.|$|R
40|$|Abstract—This article {{presents}} a floating-point exponential operator generator targeting recent FPGAs with embedded memories and DSP blocks. A single-precision operator consumes just one DSP block, 18 Kbits of <b>dual-port</b> <b>memory,</b> and 392 slices on Virtex- 4. For larger precisions, a generic approach based on polynomial approximation is used and proves more resource-efficient than the literature. For instance a doubleprecision operator consumes 5 BlockRAM and 12 DSP 48 blocks on Virtex- 5, or 10 M 9 k and 22 18 x 18 multipliers on Stratix III. This approach is flexible, scales well beyond double-precision, and enables frequencies {{close to the}} FPGA’s nominal frequency. All the proposed architectures are last-bit accurate for all the floating-point range. They {{are available in the}} open-source FloPoCo framework. I...|$|E
30|$|This can {{be handled}} in several ways: Read data from memory 2 samples by 2 samples with 2 clock latency for each radix operation, double memory clock {{frequency}} and read 2 samples by 2 samples with 1 clock latency for each radix operation {{at the expense of}} double memory power, or use 4 -port memories. Each of the above techniques have drawbacks to different degrees like lower throughput, power or both. In [10] we proposed an address scheme to solve the above problem with conflict-free memory access. The scheme is contingent on partitioning the memory to 4 <b>dual-port</b> <b>memory</b> banks as well as the specific way data is distributed between the banks. This guarantees that at any stage we have at most two accesses to the same memory bank.|$|E
30|$|As {{the main}} video {{encoding}} functions (such as ME, DCT/Q, IDCT/Q- 1, MC, Deblocking Filter, and CAVLC) {{can be accelerated}} by IP modules, the interconnection between those video processing accelerators has an important impact on the overall system performance. To make the IP accelerators execute main computational encoding routines in full parallel and pipelining mode, the IP integration architecture has to be optimized. A few of caches are inserted between the video IP accelerators to facilitate the encoding concurrent performance. The caches can be organized as parallel <b>dual-port</b> <b>memory</b> (BRAM) or pipelined memory (FIFO). The interconnection control of data streaming between IP modules will be defined using those caches targeting to eliminate the extra overhead of processing routines, for encoding functions can be operated in full parallel and pipelining stages.|$|E
40|$|The {{performance}} {{analysis of the}} VPS (Virtual Processor System) multiprocessor is presented. The system incorporates four to twenty 8 -MHz 8086 / 8087 processor boards and is built with two main objectives in mind: speed and transparent operation. The von Neumann shared bus bottleneck is alleviated by executing the code in private memories and distributing the synchronization using <b>dual-port</b> <b>memories.</b> Furthermore the system is easily programmable using the LEM analyzer, an automatic program partitioner especially developed for the VPS. Both hardware and software performance measures were taken, using detailed minimal-intrusive timing probes and emulation traces. The test covered six programs, including linear system solving, sorting, matrix multiplication and Fast Fourier transformation. Apart from the raw speedup figures, four sources of overhead were analyzed: the processor idle time, the synchronization overhead, the bus delay and the task-switch time. The {{results show that the}} task granularity has the largest impact on the performance...|$|R
40|$|A {{compact and}} {{flexible}} controller {{has been developed}} to provide MIL-STD- 1553 B Remote Terminal (RT) communications and supporting and related functions with minimal demand on {{the resources of the}} system in which the controller is to be installed. (MIL-STD- 1553 B is a military standard that encompasses a method of communication and electrical-interface requirements for digital electronic subsystems connected to a data bus. MIL-STD- 1553 B is commonly used in defense and space applications.) Many other MIL-STD- 1553 B RT controllers are complicated, and to enable them to function, it is necessary to provide software and to use such ancillary separate hardware devices as microprocessors and <b>dual-port</b> <b>memories.</b> The present controller functions without need for software and any ancillary hardware. In addition, it contains a flexible system interface and extensive support hardware while including on-chip error-checking and diagnostic support circuitry. This controller is implemented within part of a modern field-programmable gate array...|$|R
40|$|Abstract — Multi-core {{processors}} {{are about}} to conquer embedded systems — {{it is not the}} question of whether they are coming but how the architectures of the microcontrollers should look with respect to the strict requirements in the field. We present the step from one to multiple cores in this paper, establishing coherence and consistency for different types of shared memory by hardware means. Also support for point-to-point synchronization between the processor cores is realized implementing different hardware barriers. The practical examinations focus on the logical first step from single- to dual-core systems, using an FPGA-development board with two hard PowerPC processor cores. Best- and worst-case results, together with intensive benchmarking of all synchronization primitives implemented, show the expected superiority of the hardware solutions. It is also shown that <b>dual-ported</b> <b>memory</b> outperforms single-ported memory if the multiple cores use inherent parallelism by locking shared memory more intelligently using an address-sensitive method. I...|$|R
