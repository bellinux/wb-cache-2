2|23|Public
40|$|International audienceIn this work, a group contribution-associating {{equation}} of state namely GCA-EoS, is extended to model the phase behavior of alkyl amine + hydrocarbon and alkyl amine + alcohol systems while considering the association of functional groups. A <b>generalized</b> <b>routine</b> has been implemented to quantify the association effects through functional groups with associating sites. The predictions of the model are found in good agreement with experimental data reported in the literature. The model parameters were estimated in the temperature range 218 - 428 K and pressures up to 735 kPa...|$|E
40|$|A geometrically {{raw image}} of Washington, D. C. was {{acquired}} and radiometrically corrected. The data show {{little of the}} detector stripping common in earlier MSS images. The radiometrically corrected data have uniform {{means and standard deviations}} for the detectors in each band; however, the data for different detectors utilize a different pattern of DN levels, resulting in ubiquitous stripping of 1 DN amplitude. Band-to-band registration was assessed using color composites and small area correlation techniques. The spectral equivalency of the first four bands of the thematic mapper with the four bands of the MSS is being examined. Geometric analysis of the Washington, D. C. scene have started and a <b>generalized</b> <b>routine</b> for examining the contents of the label files and nonvideo data files was implemented. Several discrepancies from the documentation are described. Night scenes and daytime ocean scenes required for radiometric purposes were identified and the data ordered...|$|E
40|$|An {{extension}} of the reacting H 2 -air computer code SPARK is presented, which enables the code to be used on any reacting flow problem. Routines are developed calculating in a general fashion, the reaction rates, and chemical Jacobians of any reacting system. In addition, an equilibrium routine is added so that the code will have frozen, finite rate, and equilibrium capabilities. The reaction rate for the species is determined from the law of mass action using Arrhenius expressions for the rate constants. The Jacobian routines are determined by numerically or analytically differentiating the law of mass action for each species. The equilibrium routine {{is based on a}} Gibbs free energy minimization routine. The routines are written in FORTRAN 77, with special consideration given to vectorization. Run times for the <b>generalized</b> <b>routines</b> are generally 20 percent slower than reaction specific routines. The numerical efficiency of the generalized analytical Jacobian, however, is nearly 300 percent better than the reaction specific numerical Jacobian used in SPARK...|$|R
40|$|Program slicing is a {{well known}} {{technique}} for program analysis {{with a variety of}} useful applications. This thesis investigates a new method for program slicing based on relational representation of program facts and algorithms. A core slicing algorithm is presented and it is shown that it can be applied to a broad range of problems. A number of <b>generalized</b> <b>routines</b> are developed to extract program facts from source code without being constrained by a particular programming language. The advantages of the proposed approach are demonstrated by applying it to slicing Pico and Java programs. 1 Acknowledgements I would like to thank Prof. dr. Paul Klint for introducing me to the subject and guiding me throughout the whole project. He gave me a lot of freedom and let me enjoy my graduation work from the start to the end. Jurgen Vinju from the Programming Research Group assisted me when I was stuck with variou...|$|R
40|$|PLOTAN, a <b>generalized</b> plot {{analysis}} <b>routine</b> {{written for}} the IBM 7094 computer, minimizes the difficulties in adding plot capabilities to large existing programs. PLOTAN is {{used in conjunction with}} a binary tape writing routine and has the ability to plot any variable on the intermediate binary tape as a function of any other...|$|R
40|$|This {{appendix}} first {{illustrates how}} to use the Markov-chain approximation method to solve a standard consumption–savings problem in continuous time. We then show {{how to use}} the method to find the equilibrium of the dynamic Markov game in the paper, and finally show how to extend the model in various dimensions. In particular, Section S. 1 shows how to compute value and policy functions for a sin-gle household and how to find the distributions of households over the state space re-sulting from the optimal policies. We consider both transitory shocks to income (Brow-nian motion) and persistent risk (a Poisson process) so as to illustrate the method. Sec-tion S. 2 augments Section S. 1 with the elements needed to compute the equilibrium of the game in the paper, in which there are two decision makers who are imperfectly altruistic for each other. Section S. 3 then shows how to adapt the altruism model to an overlapping-generations framework and a finite-horizon economy, as well as how to augment the model by an endogenous risk-taking decision or other choices. Matlab code for the single-agent consumption–savings problem as well as the al-truism model (and some of its extensions) is available on the journal website; see the respective passages of this appendix for references to the code. The code uses an object with a large number of <b>generalized</b> <b>routines</b> for continuous-time finite-element meth-ods; see Doc_FinElObj. pdf for documentation. S. 1. A consumption–savings problem Consider the problem of a consumer with initial wealth W 0 ≥ 0, and a stochastic income stream with time-dependent mean yt and standard deviation σ, who has access to a saf...|$|R
40|$|OBJECTIVES: Flexible {{sigmoidoscopy}} (FS) {{screening for}} colorectal cancer {{will be introduced}} into the National Cancer Screening Programmes in England in 2013. Patient-reported outcome measures (PROMs) from trial participants indicate high acceptability and no adverse physical or psychological consequences, but this may not <b>generalize</b> to <b>routine</b> screening in the community. This study examined PROMs in a community-based FS screening programme. METHODS: Eligible adults aged 58 - 59 (n = 2016) registered at 34 London general practices were mailed a National Health Service-endorsed invitation to attend FS screening. Pain and side-effects were assessed in a 'morning-after' questionnaire, and satisfaction was assessed in a three-month follow-up questionnaire. Anxiety, self-rated health and colorectal symptoms were assessed at prescreening and follow-up. RESULTS: In total, 1020 people attended screening and {{were included in the}} current analyses, of whom 913 (90...|$|R
40|$|By {{means of}} coupled-cluster theory, {{molecular}} properties can be computed with an accuracy often exceeding that of experiment. The high-degree polynomial scaling of the coupled-cluster method, however, remains {{a major obstacle}} in the accurate theoretical treatment of mainstream chemical problems, despite tremendous progress in computer architectures. Although {{it has long been}} recognized that this super-linear scaling is non-physical, the development of efficient reduced-scaling algorithms for massively parallel computers has not been realized. We here present a locally correlated, reduced-scaling, massively parallel coupled-cluster algorithm. A sparse data representation for handling distributed, sparse multidimensional arrays has been implemented along with a set of <b>generalized</b> contraction <b>routines</b> capable of handling such arrays. The parallel implementation entails a coarse-grained parallelization, reducing interprocessor communication and distributing the largest data arrays but replicating as many arrays as possible without introducing memory bottlenecks. The performance of the algorithm is illustrated by several series of runs for glycine chains using a Linux cluster with an InfiniBand interconnect...|$|R
40|$|Strongly {{correlated}} quantum impurity problems {{appear in}} a wide variety of contexts ranging from nanoscience and surface physics to material science and the theory of strongly correlated lattice models, where they appear as auxiliary systems within dynamical mean-field theory. Accurate and unbiased solutions must usually be obtained numerically, and continuous-time quantum Monte Carlo algorithms, a family of algorithms based on the stochastic sampling of partition function expansions, perform well for such systems. With the present paper we provide an efficient and generic implementation of the hybridization expansion quantum impurity solver, based on the segment representation. We provide a complete implementation featuring most of the recently developed extensions and optimizations. Our implementation allows one to treat retarded interactions and provides <b>generalized</b> measurement <b>routines</b> based on improved estimators for the self-energy and for vertex functions. The solver is embedded in the ALPS-DMFT application package. Comment: accompanied by an open source implementation of the progra...|$|R
40|$|Restenosis {{has long}} {{remained}} the major limitation of intracoronary stenting, but several randomized trials have recently {{shown that the}} use of drug-eluting stents appear to reduce markedly the risk of recurrence following treatment of de novo lesions. To evaluate whether the results of randomized trials can be <b>generalized</b> to <b>routine</b> clinical practice, all patients receiving at least one sirolimus-eluting stent (SES) in two Swiss hospitals were entered into a prospective registry. Only target vessels with a reference diameter > 3. 5 mm were excluded. Clinical follow-up was obtained after 6 months. A total of 183 patients were included. The procedural success was 97. 8 % and the incidence of in-hospital MACE was 2. 2 %. At 7 +/- 2 months, 95. 6 % of the patients were event-free, and target lesion revascularization was required in only three patients (1. 6 %). The excellent medium-term results obtained with the SES in randomized trials can be replicated in routine clinical practice...|$|R
40|$|A novice {{programmer}} {{was once}} assigned to code a simple financial package. The novice worked furiously for many days, {{but when his}} master reviewed his program, he discovered that it contained a screen editor, a set of <b>generalized</b> graphics <b>routines,</b> an artificial intelligence interface, but not the slightest mention of anything financial. When the master asked about this, the novice became indignant. “Don’t be so impatient, ” he said. “I’ll put in the financial stuff eventually. ” (James, 1986) This paper explores how assigning software developers the identity of “engineers ” metes out specific assumptions about IT projects. To this end, the paper describes an alternative metaphor of programming as art, which is commonly used by the programmers interviewed. In addition, the discussion draws conclusions from the discrepancies between the two views {{as well as from}} the proposed metaphor, explaining organizational reluctance to aesthetical vocabulary. This paper discusses occupational identity—emphasizing the identity of programmers—using qualitative research methods. As such, it enriches the literature currently available on this profession...|$|R
40|$|A FIA {{system with}} {{electrochemical}} detection {{was developed for}} the automatic determination of paracetamol in pharmaceutical formulations. The analytical difficulties caused by adsorption of matrix excipients in the electrode surface were surpassed by an on-line electrochemical regeneration of the glassy carbon tubular electrode, using the carrier solution, enabling the renewal of the electrode surface in a simple and rapid way. A single channel FIA manifold was developed, which provided reproducibility of sample transport to the detector and minimized the contact time between samples and electrode surface, thus reducing its cleaning frequency and permitting a high sampling rate to be achieved. Furthermore, the physical characteristics of the electrochemical cell permitted its easy incorporation in the FIA manifold, offering robustness to the system, and allowed it to be <b>generalized</b> to <b>routine</b> analysis applications. With the optimized parameters, a linear correlation between paracetamol concentration and peak current intensity was obtained up to 4 ´ 10 - 4 mol L- 1, with a detection limit of 2. 5 ´ 10 - 5 mol L- 1. Paracetamol was quantified in pharmaceutical dosage forms {{and the results were}} compared to those obtained for the official spectrophotometric method (BP 1998). Correlation between the results obtained by both methods was linear and no statistical difference between methods was found at the 95 % confidence level...|$|R
40|$|A {{variety of}} {{statistical}} graphical {{models have been}} defined to represent the conditional independences underlying a random vector of interest. Similarly, many different graphs embedding various types of preferential independences, as for example conditional utility independence and generalized additive independence, have more recently started to appear. In this paper we define a new graphical model, called a directed expected utility network, whose edges depict both probabilistic and utility conditional independences. These embed a very flexible class of utility models, much larger than those usually conceived in standard influence diagrams. Our graphical representation, and various transformations of the original graph into a tree structure, are then used to guide fast routines for the computation of a decision problem's expected utilities. We show that our <b>routines</b> <b>generalize</b> those usually utilized in standard influence diagrams' evaluations under much more restrictive conditions. We then proceed {{with the construction of}} a directed expected utility network to support decision makers in the domain of household food security...|$|R
40|$|With great {{interest}} {{we read the}} article by Tu et al. on the ex-ternal quality assessment for the genotypic HIV- 1 corecep-tor tropism testing fromproviral DNA (1). This largemulticenter, multinational study conducted by opinion leaders in the field will undoubtedly have implications for the tropism testing for the rou-tine clinical laboratory. We feel, however, that several points need clarification prior to a <b>generalized</b> translation into <b>routine</b> clinical tropism testing. 1. A number of protocols have been published for the geno-typic HIV- 1 coreceptor tropism testing from virus or pro-virus (2 – 5), but no optimal primers and protocol have been defined for a most reliable amplification of the highly vari-able V 3 loop region. On this basis it is rather surprising that {{the study did not}} at first standardize amplification and se-quencing methods for all laboratories. For example, in Ta-ble 1 of the article by Tu et al. we noticed that laboratory...|$|R
40|$|In Experiment 1, haptically {{available}} object {{properties that}} would be diagnostic for constrained common object classification at the basic and subordinate levels were elicited in a questionnaire. The results are considered {{in terms of the}} nature of the haptically derived representations of common objects. Initial data are also presented regarding knowledge of the natural co-occurrence of properties in hap-tic object perception. In Experiment 2, the hand movements executed during haptic classification of manipulable common objects were examined. Manual ex-ploration consisted of a two-stage sequence, an initial <b>generalized</b> “grasp-and-lift ” <b>routine,</b> followed by a series of more specialized hand-movement pat-terns strongly driven by knowledge of the property diagnosticity for the specific object (obtained in Experiment 1). The current results may guide computational models of human haptic object classification and the development of perceptual systems for robots equipped with sensate dextrous hands, capable of intelligent exploration, recognition, and manipulation of concrete objects. 8 1990 Academic Press. Inc...|$|R
40|$|This paper {{summarizes}} and organizes presentations in the Round Table's {{presentations and}} discussions, draws conclusions when possible, {{and points out}} where opinions differ. It is divided in three main sections. First, the presentations and discussions provided {{an overview of the}} advances, promises, and pitfalls of current research on the economic impacts of investments in transport infrastructure. a first recurring theme was that advances in the analysis of "wider impacts" were acknowledged, but their transferability across projects was questioned, so there are "no simple rules" for <b>generalizing</b> results. Moreover, <b>routine</b> analysis is difficult because of shortcomings both in data availability and in the analytical framework. This theme is developed in some detail in section two. A second recurring issue was the major differences in the approach to transport project appraisal between countries. The impact of economic appraisal on policy decisions varies greatly from one region to another and this has consequences for the way wider economic impacts might be taken into account. ...|$|R
40|$|It has {{frequently}} been questioned whether results of trials can be <b>generalized</b> to <b>routine</b> clinical practice. Results obtained with standard cyclophosphamide, doxorubicin, vincristine, prednisone (CHOP) chemotherapy for aggressive non-Hodgkin's lymphoma {{in the control}} arm of prospective randomized phase III clinical trials during the past 25 years appear to be comparable. As the possibility to generalize trial results is favored by such consistency, we tested this hypothesis and tried to indicate explanatory 'moderator variables' (inclusion and exclusion criteria, therapy characteristics, sample sizes, inequalities {{in the distribution of}} patients over prognostic categories across different trials) in the case of divergent trial results. Trial results on conventional CHOP chemotherapy were obtained from literature research. Overall response (OR), complete response (CR) and two-year overall survival (2 YS) were considered as outcome measures. Although OR rates and 2 YS rates were within acceptable limits of comparability, the absolute differences within the results were remarkably wide, particularly with regard to CR and 2 YS. Divergent rates could not be properly explained by differences in the moderator variables. We conclude that absolute results obtained with CHOP in the control arm of trials appeared to be poorly generalizable to routine clinical practice, particularly in the case of elderly patients. This analysis underlines the need for the strict application of internationally agreed response criteria and the WHO classification system, large sample sizes and stratifying patients on the basis of prognostic factors, preferably in intergroup clinical trials. We expect those factors to lead to a better consistency of results in future trials and improved possibilities to generalize the result...|$|R
40|$|In {{this study}} {{we are going to}} discuss an {{extended}} form of Pearson, including the reversed generalized Pearson curves distribution as its subfamily, and refer to it as the extended generalized same distribution. Because of many difficulties described in the literature in modeling the parameters, we propose here a new extended model. The model associated to this heuristic is implemented to validate the result of the <b>generalized</b> Pearson family <b>routine</b> in the specific cases. This study is presents same applications of Pearson family’s of distributions, and give the new extended, which extends the classical Pearson family. Various properties of this new family are investigated and then exploited to derive several related results, especially characterizations, in probability. As a motivation, the statistical applications of the results based on health related data are included. It is hoped that the findings of this work will be useful for the practitioners in various fields of theoretical and applied sciences. Key words: Pearson and Burr family distribution extended, special, families, goodness of fit...|$|R
40|$|Early {{diagnosis}} of disease has potential to reduce morbidity and mortality. Biomarkers {{may be useful}} for detecting disease at early stages before it becomes clinically ap-parent. Prostate-specific antigen (PSA) is one such marker for prostate cancer. This article is concerned with modeling receiver operating characteristic (ROC) curves as-sociated with biomarkers at various times prior to the time at which the disease is detected clinically, by two methods. The first models the biomarkers statistically using mixed-effects regression models, and uses parameter estimates from these models to estimate the time-specific ROC curves. The second directly models the ROC curves {{as a function of}} time prior to diagnosis and may be implemented using software pack-ages with binary regression or <b>generalized</b> linear model <b>routines.</b> The approaches are applied to data from 71 prostate cancer cases and 71 controls who participated in a lung cancer prevention trial. Two biomarkers for prostate cancer were considered: total serum PSA and the ratio of free to total PSA. Not surprisingly, both markers performed better as the interval between PSA measurement and clinical diagnosis decreased. Although the two markers performed similarly eight years prior to diagnosis, it appear...|$|R
40|$|Here, I {{provide a}} {{practical}} overview on some statistical approaches that {{are able to}} handle the constraints that frequently emerge in the study of animal behaviour. When collecting or analysing behavioural data, several sources of limitations, which can raise either uncertainties or biases in the parameter estimates, need to be considered. In particular, these can be issues about (1) limited sample size and missing data, (2) uncertainties about the identity of subjects and the dangers posed by pseudoreplication, (3) large measurement errors resulting from the use of indicator variables with nonperfect reliability or variables with low repeatability, (4) the confounding effect of the within-individual variation of behaviour and (5) phylogenetic nonindependence of data (e. g. when substitute species are used). I suggest some simple analytical solutions to these problems based on existing methodologies and on a consumable language to practitioners. I highlight how randomization and simulation <b>routines,</b> <b>generalized</b> linear mixed models, autocorrelation models, phylogenetic comparative methods and Bayesian statistics can be exploited to overcome the inefficient performance of some conventional statistical approaches with typical behavioural data. To enhance the accessibility of these methodologies, I demonstrate how they can be brought into practice in the R statistical environment, which offers flexible statistical designs. Although the primary motivation behind this discussion was to help animal behaviourists who address questions in relation to conservation, I also hope that researchers working on the evolutionary ecology of behaviour will also find some material useful. Peer reviewe...|$|R
40|$|Studies in {{academic}} research centres with selected patients {{have shown that}} several cognitive behaviour therapies are effective {{in the treatment of}} PTSD following traumas affecting individuals or small groups. Little information is available {{on the extent to which}} these positive findings will <b>generalize</b> to more <b>routine</b> clinical settings with less selected patients or to a trauma that affects a whole community. The present study addresses these generalization issues. A consecutive series of 91 patients with PTSD resulting from a car bomb which exploded in the centre of Omagh, Northern Ireland in August 1998 were treated with cognitive therapy, along lines advocated by Ehlers and Clark (2000). There were no major exclusion criteria and 53 % of patients had an additional axis I disorder (comorbidity). Therapists were NHS staff with heavy caseloads and modest prior training in CBT for PTSD. A brief training in specialist procedures for PTSD was provided. Patients received an average of eight treatment sessions. Significant and substantial improvements in PTSD were observed. Degree of improvement was comparable to that in previously reported research trials. Comorbidity was not associated with poorer outcome, perhaps because comorbid patients were given more sessions of treatment (average 10 vs 5 sessions). Patients who were physically injured improved less than those who were not physically injured. Overall, the results indicate that the positive findings obtained in research settings generalize well to a frontline, non-selective service...|$|R
40|$|The Atmospheric Release Advisory Capability (ARAC) {{provides}} real-time dose assessments for {{airborne pollutant}} releases. ARAC {{is currently in}} the process of developing an entirely new suite of models and system infrastructure. Diagnostic and dispersion algorithms are being created in-house and a prognostic model NO-RAPS, imported from the Naval Research Laboratory, Monterey, is currently being adapted to ARAC`s needs. Diagnostic models are essential for an emergency response capability since they provide the ability to rapidly assimilate available meteorological data and generate the mass-consistent three-dimensional wind fields required by dispersion models. The resulting wind fields may also serve to initialize and validate prognostic models. In general, the performance of diagnostic models strongly correlates with the density and distribution of measurements in the area of interest and the resolution of the terrain. problem, data can be extracted from user-specified databases within a region defined by a metdata grid. Typically the data collection region will cover a geographic domain significantly larger than the area involved in the dispersion simulation in order to provide the most complete set of meteorological information relevant to the problem. This also permits the user to redefine the problem grid size and location, within limits, without reaccessing the meteorological data extraction system. After the data has been collected, an associated meteorological preprocessor places it in a standard form for further processing. The pre-processor does not alter or interpolate wind values; it only performs reversible transformations to convert the data to a standard unambiguous form, e. g. latitude, longitude, height, wind speed and direction. This allows the diagnostic models to use a <b>generalized</b> data ingest <b>routine,</b> not dependent on the form or format of the meteorological data source or database...|$|R
40|$|In 1949 the Indian {{mathematician}} D. R. Kaprekar {{discovered a}} curious {{relationship between the}} number 6174 and other 4 -digit numbers. For any 4 -digit number n, whose digits are not all the same, let ri and n " be the numbers formed by arranging the digits of n in descending and ascending order, respectively. Find the difference of these two numbers: T(n) = n f-n f!. Repeat this process, known as the Kaprekar routine, on T{ri). In 7 or fewer steps, the number 6174 will occur. Moreover, 6174 is invariant; that is, J(6174) = 6174. In the literature {{it is common to}} <b>generalize</b> the Kaprekar <b>routine</b> and apply it to any Ar-digit number in base g. Since there are only a finite number of &-digit numbers, repeated applications of T always become periodic. The result is not necessarily a single invariant; more frequently one or more cycles occur. The characterization of such cycles is a difficult problem which has not been completely solved. Among the questions studied are the following: Given k, for what value(s) of g does the Kaprekar routine produce a single invariant? When nontrivial cycles arise for a given g and k, how many cycles are there and what are their lengths? This author, among others, has studied these problems as well as many other fascinating questions associated with the above procedure. (See [1]-[12].) Recently I was describing the Kaprekar routine to faculty colleagues. To demonstrate that not all &-digit numbers in base 10 give rise to a constant, I chose to illustrate the routine for 2 -digit numbers. In that case, either one or two applications of J yields one of the numbers in the cycl...|$|R
40|$|Tinnitus is common, {{and some}} {{individuals}} with tinnitus display {{high levels of}} distress. Cognitive behavioural therapy (CBT) is effective in reducing tinnitus distress, but is rarely available. CBT-based self-help, with or without guidance, has yielded positive results in other problem areas, and one initial randomized controlled trial (RCT) has shown promising results for tinnitus. This thesis is based on four studies; Study I showed that Internet-based self-help treatment with e-mail guidance alleviated tinnitus distress among consecutive patients and was rated as credible as traditional treatments. Active participation in treatment predicted outcome. Study II, an RCT, showed that an extended and more interactive version of the Internet-based self-help treatment with e-mail therapist support appeared to be equally effective as a group treatment. In study III, another RCT, a self-help book with weekly telephone support was superior to a wait-list control group. No strong evidence {{for the importance of}} telephone contact on outcome was found. In both study II and III, the positive outcome remained after one year and self-help approaches appeared more therapist time-effective compared to group treatment. Also, the received treatment-dose for patients in guided self-help was not lower than in the group treatment. Study IV found that the ‘Stages of Change’, from the transtheoretical model, are probably not the right theoretical framework to use with tinnitus patients. Predictors of outcome were found, but they were not in line with the theory behind the Stages of Change. The predictors were better understood when conceptualized as coping, showing that helplessness and less coping before treatment correlated with better outcome. In sum, guided cognitive behavioural self-help can decrease tinnitus distress. It appears to be therapist time-effective and shows effects comparable to or slightly below traditional CBT for tinnitus. Effects remain one year after treatment and <b>generalize</b> to a <b>routine</b> clinical setting...|$|R
40|$|A coupled Computational Fluid Dynamic (CFD) and Computational Structural Dynamics (CSD) {{methodology}} {{is extended}} {{to analyze the}} effectiveness of a leading edge slat (LE-Slat) for mitigating the adverse effects of dynamic stall on rotor blade aerodynamic and dynamic response. This involved the following improvements over the existing CFD methodology to handle a multi-element airfoil rotor: incorporating the so-called Implicit Hole Cutting method for inter-mesh connectivity, implementing a <b>generalized</b> force transfer <b>routine</b> for transferring LE-Slat loads onto the main blade, and achieving increased parallelization of the code. Initially, the structured overset mesh CFD solver is extensively validated against available 2 -D experimental wind tunnel test cases in steady and unsteady flight conditions. The solver predicts the measurements with sufficient accuracy for test cases with both the baseline airfoil and that with two slat configurations, S- 1 and S- 6. As expected, the addition of the slat is found to be highly effective in delaying stall until larger angles for the case of a static airfoil and ameliorating the effects of dynamic stall for a 2 -D pitching airfoil. The 3 -D coupled CFD/CSD model is extensively validated against flight test data of a UH- 60 A rotor in a high-altitude, high-thrust flight condition, namely C 9017, characterized by distinct dynamic stall events in the retreating side of the rotor disk. The validated rotor analysis tool is then used to successfully demonstrate {{the effectiveness of a}} LE-Slat in mitigating (or eliminating) dynamic stall on the rotor retreating side. The calculations are performed with a modified UH- 60 A blade with a 40 %-span slatted airfoil section. The addition of the slat is effective in the mitigation (and/or elimination) of lift and moment stall at outboard stations, which in turn is accompanied by a reduction of torsional structural loads (upto 73 %) and pitch link loads (upto 62 %) as compared to the baseline C 9017 values. The effect of a dynamically moving slat, actuating between slat positions S- 1 and S- 6, is thoroughly investigated, firstly on 2 -D airfoil dynamic stall, and then on the UH- 60 A rotor. Three slat actuation strategies with upto [1, 3, 5]/rev harmonics, respectively, are considered. However, it is noted that the dynamic slat does not necessarily result in better rotor performance as compared to a static slat configuration. The coupled CFD/CSD platform is further used to successfully demonstrate the capability of the slat (S- 6) to achieve upto 10 % higher thrust than C 9017, which is beyond the conventional thrust limit imposed by McHugh's stall boundary. Stall mitigation due to the slat results in a reduction of torsional load upto 54 % and reduction of pitch link load upto 32 % as compared to the baseline C 9017 flight test values, even for an increase in thrust of 10 %...|$|R

