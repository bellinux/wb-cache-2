16|25|Public
50|$|Cullinet {{attempted}} to continue competing against IBM's DB2 and other relational databases {{by developing a}} relational front-end {{and a range of}} productivity tools. These included Automatic System Facility (ASF), which made use of a pre-existing IDMS feature called LRF (Logical Record Facility). ASF was a fill-in-the-blanks <b>database</b> <b>generator</b> that would also develop a mini-application to maintain the tables.|$|E
50|$|The Second Life Enterprise {{program is}} {{cancelled}} by Linden Lab in Spring of 2010. Significant {{value in the}} unique approach to virtual world simulation had been identified as well as investment in the platform. To ensure continuity in the research and the investment in content, a pivot was made to a compatible Open Source platform, the OpenSimulator. Support for OpenSimulator terrain {{was added to the}} RUGUD (Rapid Unified Generation of Urban Databases RUGUD) terrain <b>database</b> <b>generator.</b> Real world terrain is now available for use inside the MOSES, and by extension the OpenSimulator. This represents the first major technology transfer offering of the MOSES project to the OpenSimulator community.|$|E
40|$|Sharing {{scientific}} data containing complex information requires new concepts and new technology. NEUROGENERATOR is a <b>database</b> <b>generator</b> for the neuroimaging community. A <b>database</b> <b>generator</b> is a database that generates new databases. The scientists submit raw PET and fMRI data to NEUROGENERATOR, which then processes {{the data in}} a uniform way to create databases of homogenous data suitable for data sharing, met-analysis and modelling the human brain at the systems level. These databases are then distributed to the scientists...|$|E
5000|$|Relational <b>database</b> <b>generators</b> for {{creating}} and incrementally updating of: ...|$|R
50|$|Scaffolding is an {{evolution}} of <b>database</b> code <b>generators</b> from earlier development environments, such as Oracle's CASE Generator, {{and many other}} 4GL client-server software development products.|$|R
5000|$|The Oracle <b>Database</b> {{implements}} this <b>generator</b> in its DBMS_RANDOM package (available in Oracle 8 and newer versions).|$|R
40|$|A semiempirical abrasion-ablation {{model has}} been {{successful}} in generating a large nuclear database for the study of high charge and energy (HZE) ion beams, radiation physics, and galactic cosmic ray shielding. The cross sections that are generated are compared with measured HZE fragmentation data from various experimental groups. A research program for improvement of the <b>database</b> <b>generator</b> is also discussed...|$|E
40|$|To improve searching and {{processing}} {{of information on the}} web, a web warehousing system called WHOWEDA is being developed at the Centre for Advanced Information Systems (CAIS). This system incorporates a Web Information Coupling Model that describes the web objects and their relationships and a web algebra consisting of web operators to manipulate the web objects. In order to measure the performance of WHOWEDA and similar systems that manipulate web information, a synthetic web <b>database</b> <b>generator</b> called WEDAGEN (WEb <b>DAtabase</b> <b>GENerator)</b> has been developed. It has the capability of generating web databases of different sizes and complexities determined by a set of user specified parameters. This paper presents the issues in the design and implementation of WEDAGEN. It also gives a detailed description of its system components and the strategy to generate synthetic web databases. A preliminary assessment of the use of WEDAGEN has been reported. 1 Introduction 1. 1 Background The Wold Wi [...] ...|$|E
40|$|To support {{intelligent}} {{data analysis}} on the web information, a data warehousing system called WHOWEDA (WareHouse Of WEb DAta) has been proposed. Unlike other relational data warehouses, WHOWEDA incorporates a Web Data Model that describes the web objects and their relationships as they are maintained within a data warehouse. A set of web operations has also been developed to manipulate the warehoused web information. In order to measure the performance of WHOWEDA and other similar systems that store and manipulate web information, a synthetic web <b>database</b> <b>generator</b> called WEDAGEN (WEb <b>DAtabase</b> <b>GENerator)</b> has been developed. It has the capability of generating collections of web objects of different sizes and complexities determined {{by a set of}} user-specified parameters. This paper presents the issues in the design and implementation of WEDAGEN. It also gives a detailed description of its system components and the strategy to generate synthetic web databases. A formal analysis of the generated web database and an empirical assessment of WEDAGEN has been reported...|$|E
50|$|Database script generatorsThe <b>database</b> code <b>generators</b> write {{incremental}} scripts for creating, and updating, {{relational database}} models {{based on the}} RISE model. A RISE model contains - unless it's been refactored - the entire life-cycle of the data model. The code generator translates this life-cycle into an incremental database specific script. Currently, code generators for SQL Server, MySQL and PostgreSQL are available.|$|R
5000|$|The RISE suit {{provides}} {{a range of}} code generators for transforming RISE models into software. A RISE code generator is a separate software application that operates on a RISE model (XML file) and produces some kind of output. There are two main categories of code generators: <b>database</b> script <b>generators</b> and application source code generators.RISE code generators can be launched in several different ways: ...|$|R
40|$|FIELD GROUP SUBGROUP Productivity; Software development; <b>Database</b> {{application}} <b>generators.</b> 19. ABSTRACT (continue on reverse {{if necessary}} and identify by block number) Reductions in available Information Resources (IR) {{dollars in the}} budget places increased emphasis on the productivity of both system developers and users. New technologies have been proposed to improve these productivities. Three software development tools in particular have been proposed to the Navy. One was developed using Naval amet Another is available Off-The-Shelf and the third was developed by bot...|$|R
40|$|Abstract Testing a {{specific}} feature of a DBMS requires controlling the {{inputs and outputs}} of the operators in the query execution plan. However, that is practically difficult to achieve because the inputs/outputs of a query depend {{on the content of}} the test database. In this paper, we propose a framework to test DBMS features. The framework includes a <b>database</b> <b>generator</b> called QAGen so that the generated test databases are able to meet the test requirements defined on the test queries. The framework also includes a set of tools to automate test case constructions and test executions. A wide range of DBMS feature testing tasks can be facilitated by the proposed framework...|$|E
40|$|A recent set {{of light}} ion {{experiments}} are analyzed using the Green's function method {{of solving the}} Boltzmann equation for ions of high charge and energy (the GRNTRN transport code) and the NUCFRG 2 fragmentation <b>database</b> <b>generator</b> code. Although the NUCFRG 2 code reasonably represents the fragmentation of heavy ions, the effects of light ion fragmentation requires a more detailed nuclear model including shell structure and short range correlations appearing as tightly bound clusters in the light ion nucleus. The most recent NTJCFRG 2 code is augmented with a quasielastic alpha knockout model and semiempirical adjustments (up to 30 percent in charge removal) in the fragmentation process allowing reasonable agreement with the experiments to be obtained. A final resolution of the appropriate cross sections must await the full development of a coupled channel reaction model in which shell structure and clustering can be accurately evaluated...|$|E
40|$|A semiempirical abrasion-ablation {{model has}} been {{successful}} in generating a large nuclear database for the study of high charge and energy (HZE) ion beams, radiation physics, and galactic cosmic ray shielding. The version reported herein has coulomb trajectory corrections, improved transmission factors, improved surface energy corrections, and light fragment emission was added. The cross sections that are generated are compared with measured HZE fragmentation data from various experimental groups. A research program for improvement of the <b>database</b> <b>generator</b> is discussed. Introduction An adequate and reliable nuclear database that assesses the quality of heavy ion beams for various technological efforts is needed. For example, the nuclear fragmentation properties of shielding materials can alter the protection of astronauts by an order of magnitude through the selection of appropriate shield materials (refs. 1 and 2). The radiation quality of heavy ions, which is related to the ability [...] ...|$|E
5000|$|However, {{it could}} also be said that there are, in fact, a range of silver bullets today, {{including}} lightweight methodologies (see [...] "Project management"), spreadsheet calculators, customized browsers, in-site search engines, <b>database</b> report <b>generators,</b> integrated design-test coding-editors with memory/differences/undo, and specialty shops that generate niche software, such as information web sites, {{at a fraction of the}} cost of totally customized web site development. Nevertheless, the field of software engineering appears too complex and diverse for a single [...] "silver bullet" [...] to improve most issues, and each issue accounts for only a small portion of all software problems.|$|R
40|$|International audienceThis paper {{deals with}} a smart {{algorithm}} allowing reversed polarity fault diagnosis and prognosis in PV generators. The proposed prognosis (prediction) approach {{is based on the}} hybridization of a support vector regression (SVR) technique optimized by a k-NN regression tool (K-NNR) for undetermined outputs. To test the proposed algorithm performance, a PV <b>generator</b> <b>database</b> containing sample data is used for simulation purposes...|$|R
40|$|This work {{describes}} {{a system that}} performs morphological analysis and generation of Pali words. The system works with regular inflectional paradigms and a lexical <b>database.</b> The <b>generator</b> is used to build a collection of inflected and derived words, {{which in turn is}} used by the analyzer. Generating and storing morphological forms along with the corresponding morphological information allows for efficient and simple look up by the analyzer. Indeed, by looking up a word and extracting the attached morphological information, the analyzer does not have to compute this information. As we must, however, assume the lexical database to be incomplete, the system can also work without the dictionary component, using a rule-based approach. Comment: Bachelor Thesi...|$|R
40|$|A {{procedure}} for podded propulsion simulation using IOT OSIS and Kongsberg Polaris software simulators was developed {{based on the}} Mackinaw model test in open water. The data to be simulated in OSIS and Polaris is prepared from the data of static and dynamic model testing. The process to prepare the data involved determining the data that would be appropriate for a certain simulation and preparing the data into the right format. This involved {{the use of the}} IOT analysis software SWEET for file conversion and the Pod Analysis program developed prior to the project. The data was then applied to the Data Management program to adjust and modified the data before transferring to OSIS and the Polaris <b>database</b> <b>generator.</b> To ensure the accuracy for both simulators, the results of Mackinaw simulation in OSIS and Polaris were compared. Additional instruction on the Data Management program is also provided. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|At the Centre for Advanced Information Systems (CAIS), a Web {{warehousing}} {{system is}} being developed to store and manipulate Web information. The system named WHOWEDA (WareHouse Of WEb DAta) stores extracted Web information as Web tables and provides several Web operators, eg. Web join, Web select, global coupling, etc., to manipulate Web tables. During the implementation of WHOWEDA, {{it is necessary to}} perform systematic testing on the system and to evaluate its system performance. While it is possible for WHOWEDA to be tested or evaluated using actual Web pages downloaded from WWW, the amount of time required for such testing and evaluation would be so high that a comprehensive experimentation would not be possible. To overcome these difficulties, we have proposed to develop a synthetic Web <b>database</b> <b>generator</b> called WEDAGEN to efficiently generate synthetic Web information for performance evaluation and testing purposes. We present the major objectives to be accomplished by the proposed synthetic Web database generato...|$|E
40|$|There {{is often}} a need for using large test {{databases}} for benchmarking of DBMS query optimizers, performance tuning, and testing queries and database programs. One seemingly obvious alternative is {{to use one of}} the many publicly available databases. Such databases are, however, by their very nature domain-specific and, in addition, have their own data distributions that render them inappropriate for controlled experimentation. In this work, we have developed SDG (Synthetic <b>Database</b> <b>Generator),</b> which is capable of generating very large (millions of records) synthetic databases. SDG can generate one or more related relations according to use-defined parameters that specify such characteristics as number of relations in the schema, relation sizes, value distributions, selectivity factors, and key/foreign-key relationships. The domain of a generated application is specified to SDG by a small (about a dozen rows) sample from the desired domain whereby the sample acts as a seed for the generated synthetic database. SDG is written in SQL and generates relational data directly. Therefore, {{there is no need to}} go through the extra steps of loading flat files into a database...|$|E
40|$|International audienceThis paper {{deals with}} a smart {{algorithm}} allowing short-circuit faults detection and diagnosis of PV generators. The proposed algorithm {{is based on the}} hybridization of a support vector machines (SVM) technique optimized by a k-NN tool for the classification of observations on the classifier itself or located in its margin. To test the proposed algorithm performance, a PV <b>generator</b> <b>database</b> containing observations distributed over classes is used for simulation purposes...|$|R
40|$|The {{bachelor}} thesis discribes {{design and}} implementation of <b>database</b> schema documentation <b>generator.</b> It uses Schemagic application to create wellarranged documentation in user-defined output format. Schemagic is used to load information about schema into a XML document. For output documentation HTML, PDF, Postscript, RTF, DocBook and FO (XML file of formating objects) formats are supported. The Gendok application also generates some graphic information, e. g. graphs of dependencies between tables...|$|R
30|$|Due to {{the privacy}} concerns, {{we did not}} use the real audit trails for testing of the algorithms. Instead, we used an audit trail {{obtained}} from a synthetic <b>database</b> load <b>generator.</b> It allowed us to generate the periodic processing of database tasks with the pre-specified parameters such that the created periodic patterns and their traces could be compared with the outcome of the discovery and derivation algorithms. The main component of a synthetic workload generator was a process that iteratively executed a given sequence of SQL statements in a given period of time. For example, a given sequence of SELECT, UPDATE, SELECT statements was iteratively processed in a period of 10  min. The process could be nested such that practically any combination of periodic processing of SQL statement could be obtained in a multiprocessing Unix environment.|$|R
40|$|Web {{services}} {{are often used}} for retrieving data from servers providing information of different kinds. A data providing web service operation returns collections of objects for a given set of arguments without any side effects. In this project a web service benchmark (WSBENCH) is developed to simulate the performance of web service calls. Web service operations are specified as SQL statements. The function generator of WSBENCH converts user specified SQL queries into functions and automatically generates a web service. WSBENCH can automatically both generate and deploy web the service operations for exported functions. Furthermore WSBENCH supports controlled experiments, since users can control the characteristics of web service operations such as scalability of data and delay time. The database used in this project is generated by the Berlin Benchmark <b>database</b> <b>generator.</b> A WSBENCH demo is built to demonstrate the functionality. The demo is implemented as a JavaScript program acting as a SOAP client, directly calls WSBENCH services from a web browser. Users can make a web service request by simply providing the web service operation’s name and parameter values list as the input. It makes the WSBENCH very simple to the use...|$|E
40|$|Today, {{a common}} {{methodology}} for testing a {{database management system}} (DBMS) is to generate a set of test databases and then execute queries on top of them. However, for DBMS testing, {{it would be a}} big advantage if we can control the input and/or the output (e. g., the cardinality) of each individual operator of a test query for a particular test case. Unfortunately, current database generators generate databases independent of queries. As a result, it is hard to guarantee that executing the test query on the generated test databases can obtain the desired (intermediate) query results that match the test case. In this paper, we propose a novel way for DBMS testing. Instead of first generating a test database and then seeing how well it matches a particular test case (or otherwise use a trialand-error approach to generate another test database), we propose to generate a query-aware database for each test case. To that end, we designed a query-aware test <b>database</b> <b>generator</b> called QAGen. In addition to the database schema and the set of basic constraints defined on the base tables, QAGen takes the query and the set of constraints defined on the query as input, and generates a queryaware test database as output. The generated database guarantees that the test query can get the desired (intermediate) query results as defined in the test case. This approach of testing facilitates a wide range of DBMS testing tasks such as testing of memory managers and testing the cardinality estimation components of query optimizers...|$|E
40|$|Ma W., "Connectionist vector {{quantization}} in automatic speech recognition", Proefschrift voorgedragen tot het behalen van het doctoraat in de toegepaste wetenschappen, K. U. Leuven, 162 pp., January 1999, Leuven, Belgium. In this thesis, we successfully apply connectionist approaches, particularly the Multi-Layer Perceptron (MLP), to tasks of speech recognition. We present {{in detail the}} Back Propagation theory and its implementation issues, including a modified weight adaptation algorithm. We provide a weight updating strategy {{to speed up the}} convergence during network training. The training data is balanced phonetically such that the network treats all phonemes equally. We introduce a random <b>database</b> <b>generator</b> to obtain a robust MLP network. We introduce the fuzzy MLP into speech recognition and use the overlapped Hamming window as the fuzzy membership function for the MLP output. We design and implement the Multi-Layer Perceptron {{to be used as a}} labeler for the Hidden Markov Model (HMM) system, which combines the good short-time classification properties of MLPs with the good integration and overall recognition capabilities of discrete HMMs. The standard {{vector quantization}} has been replaced by an MLP labeler giving phone-like labels in an MLP/HMM hybrid system. Compared with using MLPs as probability generators for HMMs, our system is more flexible in system design because it can use the word models instead of phonetic models. Moreover, as it does not need to be trained to reach a global minimum, the network can have fewer hidden units and therefore can be trained faster. Also, we do not need to retrain our MLPs with segmentations generated by a Viterbi alignment. Compared to Euclidean labeling, our method has the advantages of needing fewer HMM parameters per state and of obtaining higher recognition accuracy. We use histograms to illustrate the MLP output value for each phonetic class. From those MLP output histograms, we observe that the winner take-all MLPs ignore the relativity of different phonetic classes. We extend our base-line winner-take-all method to several Top-N methods. A series of MLP/HMM hybrid models are discussed to fully use the MLP output information and to improve the speech recognition performance. Those investigated models are: MLP multi-dimensional labeling, MLP multi-labeling, MLP fuzzy-labeling, multi-MLP multi-labeling and multi-MLP fuzzy labeling. status: publishe...|$|E
40|$|Abstract- Decision Augmentation Theory (DAT) {{holds that}} humans inte-grate {{information}} obtained by anomalous cognition into the usual decision process. The result is that, to a statistical degree, such decisions are biased to-ward volitional outcomes. We summarize our model {{and show that}} the do-main over which it is applicable is within a few standard deviations from chance. We contrast the theory's experimental consequences with those of models that treat anomalous effects as due to a force. We derive mathemati-cal expressions for DAT and for force-like models using the normal distribu-tion. The model's predictions for the random number <b>generator</b> <b>database</b> are significantly different for force-like versus informational mechanisms. For large random number <b>generator</b> <b>databases,</b> DAT predicts a zero slope for a least squares fit to a (Z 2,n) scatter diagram, where n {{is the number of}} bits re-sulting from a single run and Z is the resulting Z-score. We find a slope of (1. 73 k 3. 19) x ~ O- ~ (t = 0. 543, df = 126, p = 0. 295) for the historical binary random number <b>generator</b> <b>database</b> which strongly suggests that some infor-mational mechanism is responsible for the anomaly. In a 2 -sequence length analysis of a limited set of data from the Princeton Engineering Anomalies Research laboratory, we find that a force-like explanation misses the ob-served data by 8. 60; however, the observed data is within 1. l o of the DAT prediction. We also apply DAT to one pseudorandom number generator study and find that its predicted slope is not significantly different from the expected value. We provide six circumstantial arguments, which are based upon experimental outcomes against force-like hypotheses. Our anomalous cognition research suggests that the quality of the data is proportional to the total change of Shannon entropy of the target system. We demonstrate that the change of Shannon entropy of a binary sequence from chance is indepen-dent of sequence length; thus, we suggest that the change of target entropy may account for successful anomalous cognition and random number gener-ator experiments...|$|R
40|$|To {{simulate}} human motions in DHM tools, using techniques {{which are}} based on real human data is one promising solution. We have presented a solution in this study to connect motion databases with DHM tools. We have showed that using a motion database with MTM-based annotations is a promising way in order to synthesize natural looking motions. A platform consists of a Motion <b>Database,</b> a Motion <b>Generator,</b> and a DHM tool was introduced and tested. The results showed successful application of the presented platform in the designed test case...|$|R
40|$|The CEDAR {{collaboration}} is extending and combining the JetWeb and HepData systems {{to provide a}} single service for tuning and validating models of high-energy physics processes. The centrepiece of this activity is the fitting by JetWeb of observables computed from Monte Carlo event generator events against their experimentally determined distributions, as stored in HepData. Caching {{the results of the}} JetWeb simulation and comparison stages provides a single cumulative <b>database</b> of event <b>generator</b> tunings, fitted against a wide range of experimental quantities. An important feature of this integration is a family of XML data formats, called HepML. Comment: 4 pages, 0 figures. To be published in proceedings of CHEP 0...|$|R
40|$|Database {{applications}} play {{an important}} role in nearly every organization, yet little has been done on testing of database applications. They are becoming increasingly complex and are subject to constant change. They are often designed to be executed concurrently by many clients. Testing of database application hence is of utmost importance to avoid any future errors encountered in the application, since a single fault in database application can result in unrecoverable data loss. Many tools and frameworks for performing testing of database applications has been proposed to populate the test database and generate test cases which checks the correctness of application. They check database applications for consistency constraints and transactions concurrency. In this paper we present a DBGEN- <b>database</b> (test) <b>GENerator,</b> an automated framework for database application testing. In this framework Test Strategies for testing of embedded SQL queries within imperative language are presented. Finally we present strategies for performing efficient regression tests by reducing the resets that may occur while testing database applications. We have also computed the coverage of various test cases to predict the quality of testing. By this, we reduce the testing time and cost by approximately by 30 %, thereby easing the tester to manage his testing activities easily...|$|R
40|$|EDF is {{involved}} with CEA and AREVA in a com-mon effort {{for the development of}} the future nuclear reactor generations. The studies, currently performed by the partners, concentrate on the design of Sodium Fast Reactor types that may include different kinds of innovative circuits and components as compared to the SPX (Super PheniX) plant design. Based on previous knowledge on SG developed at EDF, with Sodium as hot fluid, and with the help of more recent methods of modeling using the Mode-lica libraries, a new model for the simulation of steam generator has been developed in order to help the designers of the heat exchangers to meet the re-quirements for a Sodium Fast Reactor plant design. The paper will present the current status of the model and a comparison of the results with those of the ac-tual SuperPhenix steam <b>generator</b> <b>database...</b>|$|R
40|$|International audienceThis paper {{deals with}} a new {{algorithm}} allowing short-circuit and impedance faults smart diagnosis of PV generators. It {{is based on the}} use of the SVM technique for the classification of observations not located in its margin, otherwise the proposed algorithm is used a k-NN method. A PV <b>generator</b> <b>database</b> containing observations distributed over classes is used for testing the new algorithm performance, which shows therefore its contribution and its effectiveness in the diagnosis area. * Parameter j of new observation x *. I' Identity matrix. J Tuning parameter for error accepted. I Current. V Voltage. P Power. PH Photocurrent. I/V Cell Current / Voltage of PV cell. I/V Group Current / Voltage of PV group. I/V Module Current / Voltage of PV module. I/V String Current / Voltage of PV string. I Bypass_Diode Bypass diode current. R s series resistance. t Temperature...|$|R
40|$|Federated Database Systems allow {{users to}} work with data stored in {{multiple}} databases by formulating queries over an integrated view. This kind of systems are of special interest for the emerging type of information systems. In this paper we present an active mechanism to keep consistency in Federated Database Systems. Event-Condition-Action rules are used as the knowledge model. First, we show the types of events and {{the features of the}} event generator. These type of events are related to changes on both, the extension and the intension of the component <b>databases.</b> The event <b>generator</b> uses facilities provided by the operating system as well as available active mechanisms supported by the database management systems that take part of the federation. Second, we describe the type of conditions that appear in this context and show the degree of difficulty that entails verifying some of them. Third, we explain the type of actions considered. 1 Introduction It is of special interest for man [...] ...|$|R
40|$|Advanced Technology Life-cycle Analysis System (ATLAS) is a system-of-systems technology-portfolio-analysis {{software}} tool. ATLAS affords {{capabilities to}} (1) compare {{estimates of the}} mass and cost of an engineering system based on competing technological concepts; (2) estimate life-cycle costs of an outer-space-exploration architecture for a specified technology portfolio; (3) collect data on state-of-the-art and forecasted technology performance, and on operations and programs; and (4) calculate an index of the relative programmatic value of a technology portfolio. ATLAS facilitates analysis by providing a library of analytical spreadsheet models {{for a variety of}} systems. A single analyst can assemble a representation of a system of systems from the models and build a technology portfolio. Each system model estimates mass, and life-cycle costs are estimated by a common set of cost models. Other components of ATLAS include graphical-user-interface (GUI) software, algorithms for calculating the aforementioned index, a technology <b>database,</b> a report <b>generator,</b> and a form generator for creating the GUI for the system models. At the time of this reporting, ATLAS is a prototype, embodied in Microsoft Excel and several thousand lines of Visual Basic for Applications that run on both Windows and Macintosh computers...|$|R
40|$|Gold Rush is an {{electronic}} resource and discovery service {{developed by the}} Colorado Alliance of Research Libraries to help libraries manage subscriptions to electronic resources and provide improved public access for their electronic journals and databases. Gold Rush is a Web-based central digital registry of databases and electronic journals and includes content from indexing/abstracting services, publishers and aggregators. Gold Rush is an OpenURL-compliant link server that is available by acquiring the Gold Rush Complete package. GOLD RUSH REPORTS – LEVEL 1 This level allows a library's staff to compare title lists from aggregators, publishers and indexing/abstracting services that have been loaded into Gold Rush. It allows comparison of the content within packages. All reports are generated in real time over the Web and may be viewed on any standard browser or downloaded into Microsoft Excel. Title lists are updated on a periodic basis. In addition to the <b>database</b> comparison title-list <b>generator,</b> Gold Rush Reports contains a suite of other tools. Some are general in nature and some are specific to a particular library if the library is using other portions of the Service. If a Library subscribes only to Gold Rush Reports, that library will have acces...|$|R
