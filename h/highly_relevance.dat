4|29|Public
40|$|Abstract—Comparing {{one thing}} with another {{is a typical}} part of human {{decision}} making process. However, {{it is not always}} easy to know what to compare and what are the alternatives. In this paper, we present a novel way to automatically mine comparable entities from comparative questions that users posted online to address this difficulty. To ensure high precision and high recall, we develop a weakly supervised bootstrapping approach for comparative question identification and comparable entity extraction by leveraging a large collection of online question archive. The experimental results show our method achieves F 1 -measure of 82. 5 percent in comparative question identification and 83. 3 percent in comparable entity extraction. Both significantly outperform an existing state-of-the-art method. Additionally, our ranking results show <b>highly</b> <b>relevance</b> to user’s comparison intents in web. Index Terms—Information extraction, bootstrapping, sequential pattern mining, comparable entity mining Ç...|$|E
40|$|A presente tese abordou o tema de potenciais biomarcadores em uma população altamente vulnerável – díades mães/bebês com história de exposição ao crack na gestação. Foram avaliados 57 bebês expostos e 99 não expostos, e as suas mães. No primeiro artigo, a ênfase foi à ativação inflamatória, onde se detectou um aumento da IL- 6 (perfil pró-inflamatório) nos expostos, mesmo mediante ajuste para confundidores (10. 208, 54 IC 95 % 1. 328, 54 – 19. 088, 55 vs. 2. 323, 03 IC 95 % 1. 484, 64 – 3. 161, 21; p= 0. 007). A IL- 10 (perfil antiinflamatório) também se mostrou elevada nos bebês expostos (432, 22 IC 95 % 51, 44 – 812, 88 vs. 75, 52 IC 95 % 5, 64 – 145, 39, p = 0. 014). A IL- 6 esteve aumentada nas mães expostas ao crack (25. 160, 05, IC 95 % 10. 958, 15 – 39. 361, 99 vs. 8. 902, 14 IC 95 % 5. 774, 97 – 12. 029, 32; p = 0. 007), sem alterações de IL- 10 entre as puérperas. Não houve correlação entre os níveis de citocina materna e do bebê (Spearman test; p ≥ 0. 28). Neste estudo, concluiu-se que IL- 6 e IL- 10 podem ser marcadores da ativação inflamatória precoce em bebês com exposição intrauterina ao crack. Nossos resultados corroboram com os achados da {{literatura}} indicando que as citocinas possam ser mediadores potenciais para explicar os efeitos comportamentais e cognitivos do estresse prénatal sobre o feto, integrando imunologia e a hipótese da neuroinflamação a saúde mental da criança. No segundo artigo, avaliou-se uma medida de estresse oxidativo (EO), o TBARS, e de Fator Neurotrófico Derivado do Cérebro (BDNF), nas referidas díades. Os resultados encontrados na análise multivariada do TBARS no sangue de cordão umbilical (SCU) apontam para um menor EO nos bebês expostos (63, 97 IC 95 % 39, 43 – 88, 50 em expostos vs 177, 04 IC 95 % 140, 93 – 213, 14 em não expostos, p 0. 28). In this study, it {{was concluded}} that IL- 6 and IL- 10 could be markers of early inflammatory activation on babies exposed to crack/cocaine. Our results support the findings of the literature indicating that the cytosine could be potential mediators to explain the behavior and cognitive effects of prenatal stress on the fetus, integrating immunology and the hypothesis of the neuroinflammation to the child’s mental health. In the second article, we assessed a measurement of oxidative stress, the TBARS and the BDNF in the referred dyads. The results found in the multivariate analysis of the TBARS in the umbilical chord’s blood (UCB) point to a less oxidative stress in the babies exposed to (63. 97, 95 %CI 39. 43 – 88. 50 in exposed vs. 177. 04, 95 %CI 140. 93 – 213. 14 non exposed, p < 0. 001). This is an innovative finding, pointing in the direction of an endogenous antioxidant activation system on the newly born exposed, {{on the basis of the}} homeostasis rupture caused by the crack toxicity during the pregnancy. The fetus would mobilize endogenous antioxidant routes since very early in its development, as promoted by Cocaine and Amphetamine Regulated Transcript (CART). Still in this study, we can see the increase of the BDNF in the exposed babies (3. 86, 95 %CI 2. 29 – 5. 43 vs 0. 85, 95 %CI 0. 47 - 1. 23; p < 0. 001), but a decrease in the exposed pregnant women in relation to the non-exposed (4. 03, 95 %CI 2. 87 – 5. 18 vs. 6. 67, 95 %CI 5. 60 – 7. 74; p = 0. 006). The data of BDNF in babies exposed to crack are highly innovative, but consistent to the literature in the sense of a reaction of neuroplasticity. In pregnant women the data is surprising, having in mind that the adult literature indicates an increase of BDNF in crack/cocaine users in relation to controllers. A possibility to explain this finding is a possible variation in the practicality and intensity of usage of crack/cocaine from the test group. Alternatively the most prevalence post-traumatic stress (PTSD) in the crack pregnant users could justify this finding. Patients with PTSD tend to present levels of BDNF less than the normal controllers. This leads us to four possible biomarkers, in a population difficult to access and in a <b>highly</b> <b>relevance</b> to public health. We can tell that there are different answers according to the development stage and that the pregnant women could have a neurotrophins recruitment profile and perhaps other biomarkers, different to non pregnant. Therefore, we can observe that structural, physiological, molecular changes promoted by cocaine result from an involvement from a vast network of neurotransmitters integrated and active in different areas of the brain and in different moments of maturation...|$|E
40|$|Marketing {{budgeting}} {{is one of}} {{the most}} important aspects of management and of <b>highly</b> <b>relevance</b> for business success. Due to rising competitive pressure and a considerable increase in marketing investments the importance of this subject has additionally grown in the last years. For this reason, marketing budgeting receives a huge amount of attention by research and practitioners alike. Accordingly, it is stated in the CMO Council Report of 2007 : „The number-one challenge for most chief marketing officers is to quantify, measure, and improve the value of marketing investments and resource allocations“. The Marketing Science Instituteset this issue as top research priority for the time period 2010 - 2012 : „How should firms determine the absolute level of marketing spending and how should spending be allocated at the strategic level - that is, across products, customer groups, and geographies?” The academic literature has been dealing with questions regarding the marketing budget process for a long time and therefore this issue has been discussed and analyzed in multiple ways. The focus of this literature has been on the allocation of budgets as previous research has shown that profit improvement from better allocation is much higher than from improving the overall budget. To give an overview of the existing literature we may distinguish between two main research streams: (1) the descriptive and (2) the normative analysis of marketing budgeting. Descriptive literature discusses the status quo of the marketing budgeting process in companies, i. e. it identifies how marketing budgets are actually determined and allocated by managers. Two types of descriptive studies have emerged in the literature. The first type covers a broad range of manager surveys about budgeting behavior. They indicate that budget decisions are mainly based on the application of some simple budgeting rules, such as the “Percentage of Sales” or “Competitive Parity” method, which are easy to apply and therefore be preferred by manager. But these studies ignore for the high complexity of the budgeting process and are exposed to several biases of survey studies. Therefore insights on the budgeting process based on survey results are quite limited. The second type of descriptive studies try to explain budgeting behavior by estimating the impact of relevant factors on the observable size and allocation of the marketing budget to identify determinants of budget setting. But as all of these studies apply highly different approaches in model design results across studies about the impact of determinants on budgeting are characterized by high heterogeneity. So in summary, literature may only provide a fuzzy and fragmented picture on how manager determine their marketing budget. Normative literature discusses how the marketing budget should be determined. A large body of work assists practitioners by developing diverse approaches for allocation optimization, covering several aspects of resource allocation. All of these solutions offer important general insights into the budgeting problem but generally are not implemented in the marketing practice as they cover only some aspects of the budget allocation problem and/or give suggestions on budget allocation which are not understood and therefore are not accepted by manager. For this reason, researcher developed several heuristics or decision calculus models which address the problem that optimization models cannot be well implemented in companies and offer easy to understand and close to optimum solutions for the complex allocation problem. But while all of the heuristics are focused on short-term profit maximization and thus ignore for dynamic effects which are highly important for budget allocation, the decision calculus models may only give imprecise implications for budget allocation. This explains why the application of scientific models for resource allocation by practitioners is quite rare. The objective of this dissertation is to offer a comprehensive analysis of marketing budgeting. Therefore this work contributes to descriptive as well as normative research by addressing two main research gaps which exist in the literature. In terms of descriptive analysis the existing literature provides only a fragmented picture about influential factors in the budgeting process. In terms of normative literature no method has been developed which address the complexity of the budget allocation task for a multi-country, multi product-firm as well as the need of practitioners for simple allocation rules. The first two papers of this dissertation address the descriptive analysis issues by (1) reviewing and structuring the fragmented literature of marketing budgeting behavior, and (2) developing an innovative approach to analyze empirically the application of budgeting methods in pharmaceutical companies. The last two papers of this dissertation address the normative analysis issues by (3) introducing and implementing an innovative solution to the dynamic marketing allocation budget problem for multi-product, multi-country firms, and (4) analyzing and comparing the performance of different allocation rules by simulation analysis. In summary, the dissertation’s focus is to understand how marketing budgets are set by practitioners, and how the allocation decision process can be improved...|$|E
40|$|It {{has long}} been {{recognized}} that the primary obstacle to effective performance of classical models {{is the need to}} estimate a relevance model with no training data. We propose a novel technique for estimating such models using the query alone. We demonstrate that our technique can produce <b>highly</b> accurate <b>relevance</b> models. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval. The main contribution of this work is an effective formal method for estimating a relevance model with no training data. ...|$|R
50|$|Ranking in XML-Retrieval can {{incorporate}} both content relevance and structural similarity, {{which is the}} resemblance between the structure given in the query {{and the structure of}} the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is <b>highly</b> relevant. <b>Relevance</b> can be defined according to the notion of specificity, which {{is the extent to which}} a retrieval unit focuses on the topic of request.|$|R
40|$|We {{explore the}} {{relation}} between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models {{is the need to}} estimate a relevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce <b>highly</b> accurate <b>relevance</b> models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data. 1...|$|R
40|$|The book is {{proposed}} {{to have a}} scholarly/ theoretical overview with potential for practical applicability. In essence, the various sections of the book will draw on theoretical arguments to offer different perspectives on how literacy can be repositioned as a curriculum area {{in the context of}} contemporary globalised society. One of the more consistent conclusions arrived at in the contemporary discourse on literacy is the recognition of different models, particularly, a social model or approach to adult literacy (Hamilton, 2006, Barton, 1974, Gee et al, 1996, Street, 1984, 1996, Ade-Ojo, 2009, 2011). Perhaps even more constant, however, is the consensus that one model, variously referred to as cognitive, traditional, is dominant and informs both policy and practice. Writers and practitioners have consistently challenged this so-called dominance in two ways. One way is through what might be seen as a deficit model which highlights the limitations of this approach to literacy both in theory and practice. The other, which we shall label an enhancement model, has consistently used the principles embedded in the sociological approach to literacy to offer pedagogical and theoretical standpoints. In spite of these interventions, however, the so-called dominance has persisted. A crucial question, therefore is; why is this so? Scholars like Street (1996, 2003, 2008) have long argued that the dominance of the cognitive model is anchored upon {{the fact that it has}} been able to generate a range of instruments, agents and organs, which allow it to dominate because they are more functionally available to educators and policy makers. Similarly, Gee and his associates (1996) have emphasised the ability of a dominant cognitive model of the literacy curriculum to enforce a ‘new working order’. Drawing from this, therefore, we offer the argument that the curriculum is the driver for this edited book. We contend that the so- called dominance has been promoted mostly by the readily available curriculum available for its use. In essence, therefore, this collection is a call for a repositioning of literacy curriculum. The key question in this context is; does the social model of literacy offer curricula that reflects its underpinning ideology and which is capable of selling its world view in practice? Sadly, the answer is no. This, in spite of several individualised contributions by different authors on the strength of a socially motivated curriculum which synergises with the principles of the social model of literacy. The underpinning argument that drives this book, therefore, is a call for the recognition of literacy curricula for specific purposes. In tandem with the recognition of the fact that literacy should be seen as reflecting distinct social practices, this book argues for distinct literacy curricula for distinct social practices. The ongoing immediately evokes the age long philosophical debate and dichotomy between the Aristotelian concept of curriculum which pits Plato’s art of the dialectician that involves ‘holding together both the one and the many’ against the Aristotelian view which emphasises the necessity of ‘eliminating contradictions by choosing whether a person has a characteristic or not’ (Whitehead, 1999). The philosophical essence of the contemporary literacy curriculum renders it a complication along the Platonic construct and, therefore, enables it to dominate other potential constructs. The focus of this book is to show that proponents of the social model of literacy must like Aristotle embrace a non-contradictory construct by developing specific curricula for specific practices such that they are able to foreground the social characteristics of literacy. The engagement with distinction of literacy curricula immediately evokes Hirst’s dated but <b>highly</b> <b>relevance</b> engagement with disciplinary knowledge. Hirst’s discipline thesis in curriculum theory argues that knowledge must come in ‘form’ which is ‘a distinct way in which our experience becomes structured round the use of accepted public symbols’ (1974 : 38). Developing from this, the notion of literacy for specific purposes which this book is designed to advocate argues for a disciplinary development of literacy curricula such that each discipline, together with the vocation and profession aligned to it is seen as a social practice. Without such a disciplinary distinction, literacy practice has been dominated by the curriculum of a single social practice and this has effectively led to the much decried dominance. So, although there has been much outcries by the proponents of the sociological approach to literacy, its have remained more of ‘literacy in theory and less of literacy in practice’. The result is the dominance of the cognitive model leading to what Gee, Hull and Lankshear (1996 : Xiii) famously referred to as ‘creating new social identities or new kinds of people’. What this book hopes to achieve is to draw on the principle of creating new identities to create ‘new literate people’. The underpinning argument being that if literacy curricula are shaped around specific disciplines and, therefore, specific social practices that many within society are socially aligned to, opportunity for creating new socially literate society will emerge. Therefore, this book aims to map out a theoretical framework for utilising the principles of a sociocultural approach to literacy in curriculum development by focusing on disciplinary characteristics. The salient question in this respect is; how can we make literacy social in practice? Ade-Ojo’s chapter will set the scene for socialising literacy practice in disciplines through the development of specific literacy curricula. Ade-Ojo will argue that different subjects, vocational and professional areas are to be seen as social learning areas, and as such will require specific social literacies for their practices. He will draw on sociological, psychological, educational and learning, as well as linguistic theories to make a case for developing literacy curricula for specific social purposes. Other chapters will provide illustrations of how this principle can be, and has been utilised in different social learning areas. Ultimately, the expectation is that the book will make a strong case for policy makers, awarding bodies and curriculum developers to re-think their approach to literacy practice in the learning context...|$|E
40|$|This paper {{tested the}} {{abnormal}} earnings persistence in the Jordanian context through Ohlson’s (1995) first Linear Information Dynamics (LID) Model using an unbalanced panel regression analysis for {{a sample of}} (840) public firms listed in the Amman Security Exchange during the period 2007 to 2011. The results showed a <b>highly</b> value <b>relevance</b> for the industrial, financial and services sectors indicated by the coefficient of the abnormal earnings persistence. The services sector had the highest value relevance. However, the industrial and the financial sectors were closed. Finally, {{the results for the}} detailed industry analysis of the sub-sectors had shown different values for the coefficient of the abnormal earnings persistence, and bank firms, printing & packaging firms, and the utilities & communication firms had the highest values for the coefficient of the abnormal earnings persistence. </p...|$|R
40|$|ENSO) is a {{naturally}} occurring mode of tropical Pacific variability, which has global impacts of <b>highly</b> societal <b>relevance.</b> It {{has long been}} known that no two El Niño events are the same, as events differ in amplitude, location of maximum sea surface temperature anomalies, evolution, and triggering mechanisms. However, the recognition that differences in the longitudinal location of the anomalies lead to different atmospheric teleconnections and impacts has stimulated a renewed interest in the ENSO phenomenon and spurred animated debates on whether there are two distinct modes of variability, such as the “Eastern Pacific ” and the “Central Pacific ” types, as a large body of literature has emphasized, or whether ENSO diversity can be more properly described as a continuum with some interesting flavors. A U. S. CLIVAR workshop on ENSO diversity was held in Boulder, CO...|$|R
40|$|In {{adversarial}} {{and noisy}} search settings as the Web, the document-query surface level similarity {{can be a}} <b>highly</b> misleading <b>relevance</b> signal. Thus, devising content-based relevance estimation (ranking) approaches becomes highly challenging. We address this challenge using two methods that utilize inter-document similarities in an initially retrieved list. The first removes documents from the list that exhibit high query similarity, but {{for which there is}} insufficient additional support for relevance that is based on interdocument similarities. The method is based on a probabilistic model that decouples document-query similarities from relevance estimation. The second method re-ranks the list by “rewarding ” documents that exhibit high similarity both to the query and to other documents in the list. Both methods incorporate, in addition, at the model level, queryindependent document quality estimates. Extensive empirical evaluation demonstrates the merits of our methods...|$|R
40|$|Traditional {{search engines}} on World Wide Web (WWW) focus {{essentially}} on relevance ranking at the page level. But this lead to missing innumerable structured information about real-world objects embedded in static Web pages and online Web databases. Page-level information retrieval (IR) can unfortunately lead to <b>highly</b> inaccurate <b>relevance</b> ranking in answering object-oriented queries. On the other hand, Object Oriented Information Computing (OOIC) is promising and greatly reduces {{the complexity of}} the system while improving reusability and manageability. The most distinguishing requirement of today’s complex heterogeneous systems is the need of the computing system to instantly adapt to vigorously changing conditions. OOIC allows reflecting the dynamic characteristics of the applications by instantiating objects dynamically. In this paper, major challenges of OOIC as well as its rudiments are recapped. The review includes the insight to PopRank Model and comparison analysis of conventional page rank based IR with OOIC...|$|R
40|$|A new simple reaction-diffusion {{system is}} {{presented}} focusing on pattern formation phenomena as consecutive precipitation fronts and {{distortion of the}} precipitation front. The chemical system investigated here {{is based on the}} amphoteric property of aluminum hydroxide and exhibits two unique phenomena. Both the existence of consecutive precipitation fronts and distortion are reported for the first time. The precipitation patterns could be controlled by the pH field, and the distortion of the precipitation front can be practical for microtechnological applications of reaction-diffusion systems. Technological application of precipitation reaction-diffusion systems has a great and <b>highly</b> increasing <b>relevance.</b> 1 - 3 Pre-cipitation patterns (PP) can emerge, where the diffusion of two, usually inorganic, species are coupled with their reaction. 4 - 6 The existence of such patterns is usually explained by the instability of colloidal substances arising from the reactants. 6, 7 The classical precipitation (Liesegang) patterns are static and stationary {{in the sense that the}} formed precipitation objects (bands, rings, or more complex patterns) stay at the given forme...|$|R
40|$|Previous {{studies on}} {{histamine}} release by corticotropin peptides and melittin peptides were extended, {{leading to the}} identification of a synthetic peptide intermediate, Lys(Z) -Arg(NO 2) -Arg(NO 2) OMe, (I) as an active non-cytolytic histamine releaser from rat mast cells. However, {{significant differences in the}} releasing capacity of optical isomers of this compound, and of Lys-Lys-Arg-ArgOMe [methyl ester of corticotropin-(15 - 18) -tetrapeptide; 'basic core'] were observed, with the L-forms being markedly more active. A study of various analogues of the tripeptide compound (I) indicated that the structural basis for mast-cell triggering by such peptidic agents was <b>highly</b> specific. The <b>relevance</b> of these observations to the immunologically induced histamine-release processes is discussed...|$|R
40|$|In this work, we {{evaluate}} {{a number of}} machine learning techniques {{for the purpose of}} ranking answers to why-questions. We use a set of 37 linguistically motivated features that characterize questions and answers. We experiment with a number of machine learning techniques in various settings. The purpose of the experiments is to assess how the different machine learning techniques can cope with our <b>highly</b> imbalanced binary <b>relevance</b> data. We find that with all machine learning techniques, we eventually obtain an MRR score that is significantly above the TF-IDF baseline of 0. 25 and not significantly lower than the best score of 0. 35. Regression techniques seem the best option for our learning problem. 1...|$|R
40|$|With {{the advent}} of Web search and {{the large amount of}} data {{published}} on the Web sphere, a tremendous amount of documents become strongly time-dependent. In this respect, the time dimension has been extensively exploited as a <b>highly</b> important <b>relevance</b> criterion to improve the retrieval effectiveness of document ranking models. Thus, a compelling research interest is going on the temporal information retrieval realm, which gives rise to several temporal search applications. In this article, we intend to provide a scrutinizing overview of time-aware information retrieval models. We specifically put the focus on the use of timeliness {{and its impact on the}} global value of relevance as well as on the retrieval effectiveness. First, we attempt to motivate the importance of temporal signals, whenever combined with other relevance features, in accounting for document relevance. Then, we review the relevant studies standing at the crossroads of both information retrieval and time according to three common information retrieval aspects: the query level, the document content level and the document ranking model level. We organize the related temporal-based approaches around specific information retrieval tasks and regarding the task at hand, we emphasize the importance of results presentation and particularly timelines to the end user. We also report a set of relevant research trends and avenues that can be explored in the future...|$|R
40|$|Aim. To {{present the}} nursing {{outcomes}} from {{the evaluation of}} developments in the care environment in residential settings for older people. Design. The evaluation data reported here is derived from a larger national programme of work {{that focused on the}} development of person-centred practice in residential services for older people using an emancipatory practice development framework. A multi-method evaluation framework was utilised. Outcome data were collected at three time points between December 2007 and September 2009. The data reported here were collected using an instrument called the ‘Person-Centred Nursing Index’. Findings. Heavy workload was the main cause of stress among nurses. Personal and professional satisfaction with the job was scored highest by the total sample of nurses. Nineteen factors were examined using the Person-Centred Nursing Index. Statistically significant changes were observed in 12 of these. In addition, there were statistically significant changes in nurses’ perceptions of caring, indicating a shift from a dominant focus on ‘technical’ aspects of care, to one where ‘intimate’ aspects of care were more <b>highly</b> valued. <b>Relevance</b> to clinical practice. The findings highlight the importance of the development of effective teamwork, workload management, time management and staff relationships {{in order to create a}} culture where there is a more democratic and inclusive approach to practice and space for the formation of person-centred relationships...|$|R
30|$|Moreover, {{unlike the}} {{intrinsic}} Seebeck coefficient of metals, the Seebeck coefficient of semiconductor is <b>highly</b> extrinsic in <b>relevance</b> with the doping density, crystal structure, and resistivity. For example, the Seebeck coefficient of p-type silicon varied from 300 to 1, 600 [*]μV/K {{at room temperature}} [22]. Because of its relevance to fabrication process, the thermopower of semiconductor-based thermocouple may vary from batch to batch or even from one to another, which was hard to put into industrial batch productions. As for the metal-based thermocouples, they were put into wide applications for many years, and industrial standard thermocouples were all made of metals or alloys. Thin-film metal thermocouple can generate a similar thermopower given similar film thicknesses. Varrenti et al. first reported the reproducibility of Cr-Ni TFTCs, both inter- and intra-batch [14]. Our work {{also contributed to the}} high stability of Cr-Ni TFTCs by massive calibrations [15].|$|R
40|$|A {{remarkable}} {{feature of}} typical ground states of strongly-correlated many-body systems {{is that the}} entanglement entropy is not an extensive quantity. In one dimension, there exists a proof that a finite correlation length sets a constant upper-bound on the entanglement entropy, called the area law. However, the known bound exists only in a hypothetical limit, rendering its physical <b>relevance</b> <b>highly</b> questionable. In this paper, we give a simple proof of the area law for entanglement entropy in one dimension under the condition of exponentially decaying correlations. Our proof dramatically reduces the previously known bound on the entanglement entropy, bringing it, for the first time, into a realistic regime. The proof is composed of several simple and straightforward steps based on elementary quantum information tools, which reveal how the internal structure of many-body states is restricted by exponentially decaying correlations. We argue that the functional dependence of the bound on the correlation length is close to optimal. Comment: Substantially revise...|$|R
40|$|Abstract In this paper, we {{evaluate}} {{a number of}} machine learning techniques for the task of ranking answers to why-questions. We use TF-IDF together {{with a set of}} 36 linguistically motivated features that characterize questions and answers. We experiment with a number of machine learning techniques (among which several classifiers and regression techniques, Ranking SVM and SV M map) in various settings. The purpose of the experiments is to assess how the different machine learning approaches can cope with our <b>highly</b> imbalanced binary <b>relevance</b> data, with and without hyperparameter tuning. We find that with all machine learning techniques, we can obtain an MRR score that is significantly above the TF-IDF baseline of 0. 25 and not significantly lower than the best score of 0. 35. We provide an in-depth analysis of the effect of data imbalance and hyperparameter tuning, and we relate our findings to previous research on Learning to Rank for Information Retrieval. Keywords Learning to Rank · Question Answering · Why-Question...|$|R
40|$|In this paper, we {{evaluate}} {{a number of}} machine learning techniques for the task of ranking answers to why-questions. We use TF-IDF together {{with a set of}} 36 linguistically motivated features that characterize questions and answers. We experiment with a number of machine learning techniques (among which several classifiers and regression techniques, Ranking SVM and SVMmap) in various settings. The purpose of the experiments is to assess how the different machine learning approaches can cope with our <b>highly</b> imbalanced binary <b>relevance</b> data, with and without hyperparameter tuning. We find that with all machine learning techniques, we can obtain an MRR score that is significantly above the TF-IDF baseline of 0. 25 and not significantly lower than the best score of 0. 35. We provide an in-depth analysis of the effect of data imbalance and hyperparameter tuning, and we relate our findings to previous research on learning to rank for Information Retrieval. © 2010 The Author(s) ...|$|R
40|$|Atherina boyeri Risso, 1810 is a {{euryhaline}} species of ecological and economic <b>relevance,</b> <b>highly</b> diffused in most Mediterranean coastal lagoons and characterized by high morphological variability among populations. To study the genetic relationships among A. boyeri populations, 11 Italian lagoons and 2 freshwater lakes were sampled and a random amplified polymorphic DNA analysis was performed. Six arbitrarily designed primers were used. Of the 43 scored markers, 39 were polymorphic. No population-specific markers were found. Statistical analysis based on inferred allele frequencies (using Wright's F(st) statistic) and on {{presence or absence}} of bands (using analysis of molecular variance) revealed significant population structure. Unweighted pair group method with arithmetic mean cluster analysis based on Nei's genetic distances showed a geographic clustering of populations. Mantel tests confirmed a high correlation between genetic and geographic distances along coasts. These results suggest the occurrence of coastal gene flows among populations, probably because the anadromic behavior of this species may be only partially phylopatric. However, these migratory movements have not blurred the existing differences among populations...|$|R
40|$|Abstract Simulation {{is a very}} {{powerful}} tool for hard-ware designers. It generally allows the preliminary eval-uation of a chip's performance before its final tape out. As security against side-channel attacks is an increas-ingly important issue for cryptographic devices, simu-lation also becomes a desirable option for preliminary evaluation in this case. However, its <b>relevance</b> <b>highly</b> depends on the proper modeling of all the attack pecu-liarities. For example, several works in the literature directly exploit SPICE-like simulations without con-sidering measurement peripherals. But the outcome of such analyses may be questionable, as witnessed by the recent results of Renauld et al. at CHES 2011, which showed how far the power traces of an AES S-box implemented using a dynamic and differential logic style fabricated in 65 nm CMOS can lie from their post-layout simulations. One important difference was found in the linear dependencies between the (simulated and actual) traces and the S-box input/output bits. While simulations exhibited highly non-linear traces, actual measurements were much more linear. As linearity is a crucial parameter for the application of non-profiled side-channel attacks (which are only possible under th...|$|R
40|$|Various {{types of}} natural {{biological}} conduits {{have been investigated}} as alternatives to the current surgical standard approach for peripheral nerve injuries. Autologous nerve graft, the current gold standard for peripheral nerve damage, is limited by clinical challenges such as donor-site morbidity and limited availability. The {{purpose of this study}} was to evaluate the efficacy of using acellular xenographic conduits (nerve, artery, and dermis) for the repair of a 1. 2 cm critical size defect of peripheral nerve in a rodent model. Four months post surgery, the animal group receiving acellular artery as a nerve conduit showed excellent physiological outcome in terms of the prevention of muscle atrophy and foot ulcer. Histological assessment of the bridged site revealed excellent axon regeneration, as opposed to the nonrepaired control group or the group receiving dermal conduit. Finally, the study evaluated the potential improvement via the addition of undifferentiated mesenchymal stem cells into the artery conduit during the bridging procedure. The mesenchymal stem cell–dosed artery conduit group resulted in significantly higher concentration of regenerated axons over artery conduit alone, and exhibited accelerated muscle atrophy rescue. Our results demonstrated that xenographic artery conduits promoted excellent axonal regeneration with <b>highly</b> promising clinical <b>relevance...</b>|$|R
40|$|Aims: To address {{physical}} inactivity as a key, modifiable {{risk factor for}} morbidity, disease related disability, and early mortality and to highlight the potential contribution of future physiotherapists to this national public health priority. The objectives of the project were threefold; (i) to create high quality teaching resources on physical activity and long term conditions, (ii) to support Higher Education Institutions (HEIs) to achieve excellence in their delivery of undergraduate physical activity learning, (iii) to equip student physiotherapists to deliver evidence-based interventions to increase physical activity in people with long term conditions. Methods: In 2014 Exercise Works! developed resources for all UK undergraduate medical degrees on exercise medicine and chronic disease. In September 2015, these resources were made available for all undergraduate nursing, midwifery and allied health professions courses internationally. The teaching resources {{are a series of}} approximately 30 presentations that cover topics relevant to physical activity and long term conditions, titles include "Cancer and physical activity" "Obesity and exercise" "Diabetes and exercise" and "Dementia and exercise". The resources are securely housed on the web and are editable to enable HEIs to adapt them to suit their needs. Sheffield Hallam University is an example of one HEI that has integrated the resources into its undergraduate curriculum. A webfolio has been created that houses the resources and approximately 300 physiotherapy students now have secure access. This augments the existing delivery of physical activity and exercise in long term conditions within the undergraduate physiotherapy curriculum. Outcomes: Evaluation {{of the quality of the}} resources was funded by Public Health England and carried out by The University of Nottingham. An international panel of experts concluded that the resources rated <b>highly</b> for <b>relevance,</b> evidence-base and for the clarity and relevance of the learning outcomes. The resources are now being used nationally and internationally. Over 400 secure resource downloads from schools of medicine and health have been recorded. The resources have been successfully embedded at Sheffield Hallam University and evaluation of one module shows that 58...|$|R
40|$|BACKGROUND: General practitioners' (GPs') {{attendance}} at {{continuing medical education}} (CME) events has increased since {{the introduction of the}} Post Graduate Educational Allowance (PGEA) in 1990. However, few studies have examined doctors' perceptions about their continuing education, and explored their views in depth. AIM: To investigate general practitioners' experience of CME events, what personal impact they had, and how the GPs perceived the influence of CME in their professional practice and patient care. METHOD: A qualitative study, with in-depth semi-structured interviews, of a purposive sample of 25 general practitioners in Dorset was conducted. Content analysis was used to identify major themes from the transcripts. RESULTS: GPs perceived CME events as beneficial. Confidence levels rose, and the events provided a break from practice that refreshed and relaxed, thus indirectly benefiting patients. The opportunities provided by formal events for informal learning and exchange of ideas, with both peers in general practice and consultant colleagues, were <b>highly</b> valued. The <b>relevance</b> of the subject to general practice, and the appropriateness of the educational format, were considered of paramount importance. Few responders identified major changes in their practice as a result of formal CME events, and information was seldom disseminated among practice colleagues. CONCLUSION: The results of this study challenge GP educators to provide CME that is relevant, to recognize the value of peer contact, and to facilitate the incorporation of new information into practic...|$|R
40|$|AbstractThe mineral {{fixation}} of CO 2 has {{a significant}} but <b>highly</b> varying <b>relevance</b> in {{carbon capture and storage}} (CCS) models. One reason is the impact of the minerals’ concentrations, but the choice of the thermodynamic database on which the geochemical model calculations are based, has probably an equivalent or even more important impact on the model results. In this example, natural groundwater and a mineral assemblage consisting of calcite, dolomite, albite, and K-feldspar in two different concentrations were used to perform scenario simulations with the geochemical model PHREEQC. The four different thermodynamic datasets phreeqc. dat, wateq 4 f. dat, llnl. dat, and minteq. dat were used to calculate the amount of CO 2 which could be fixed through the reactions with the described minerals. In the simulations using low reactive mineral concentrations (1 – 2. 5  wt%), the range of the amount of fixed CO 2 in the mineral and dissolved phases lies between 0. 29 and 0. 31  mol/kgw for the four chosen thermodynamic databases, which reflects the complete transformation of albite and K-feldspar to secondary minerals in all the simulations and only minor differences in the amounts of dissolved calcite and dolomite. On the other hand, the amount of fixed CO 2 shows a variance of between 1. 51 and 2. 17  mol/kgw when using high reactive mineral concentrations (10 – 25  wt%). Especially the transformation of the K-feldspar depended significantly on the chosen thermodynamic database which affected the calculated product minerals of the reaction. Using the phreeqc. dat, the wateq 4 f. dat, and the minteq. dat databases, the amount of CO 2 fixation due to its reaction with K-feldspar is up to six times larger than the amount calculated with the llnl. dat database. It is currently not clearly possible to decide which thermodynamic database should be used to receive the most realistic modeling results. Consequently, the uncertainty in the modeling results due to varying thermodynamic databases should be regarded in addition to the uncertainty resulting from factors like the heterogeneous distribution of mineral phases in the aquifers. It still has to be discussed whether a risk-based approach offers a secure way of modeling CCS related scenarios or whether worst-case scenarios have to be applied...|$|R
40|$|Abstract. Simulation {{is a very}} {{powerful}} tool for hardware designers. It generally allows the preliminary evaluation of a chip's performances before its final tape out. As security against side-channel attacks is an increasingly important issue for cryptographic devices, simulation also becomes a desirable option for preliminary evaluation in this case. How-ever, its <b>relevance</b> <b>highly</b> depends on the proper modeling of all the attack peculiarities. For example, several works in the literature directly exploit SPICE-like simulations without considering measurement peripherals. But the outcome of such analyses may be questionable, as witnessed by the recent results of Renauld et al. at CHES 2011, which showed how far the power traces of an AES S-box implemented using a dynamic and differential logic style fabricated in 65 nm CMOS can lie from their post-layout simulations. One important difference was found in the linear dependencies between the (simulated and actual) traces and the S-box input/output bits. While simulations exhibited highly non-linear traces, actual measurements were much more linear. As linearity is a crucial pa-rameter for the application of non-profiled side-channel attacks (which are only possible under the assumption of sufficiently linear leakages), this observation motivated us to study the reasons of such differences. Consequently, this work discusses the relevance of simulation in security evaluations, and highlights its dependency on the proper modeling of measurement setups. For this purpose, we present a generic approach to build an adequate model to represent measurements artifacts, based upon real data from equipment providers for our AES S-box case study. Next, we illustrate the transformation of simulated leakages, from highly non-linear to reasonably linear, exploiting our model and regression-based side-channel analysis. While improving the relevance of simulations in security evaluations, our results also raise doubts regarding the possibil-ity to design dual-rail implementations with highly non-linear leakages. ...|$|R
40|$|Summary. Usnic acid (UA) was {{extracted}} at 1. 5 - 1. 9 % {{dry weight}} from samples of Cladonia stellaris, a carpet-forming lichen growing abundantly in mountain areas in southeastern Norway. UA {{is known for}} its antibiotic activity as well as other bio-inhibitory functions, and is currently used in a number of formulations. Thus, the objective was to isolate and analyze UA to assess its availability. Preparations, made from lichen: acetone extraction (1 : 15) and from refining by recrystallization, were analyzed by one- and two-dimensional thin layer chromatography, gas chromatography, melting point, polarimetry and NMR. Only minor contaminants were observed, and both crude and refined preparations demonstrated properties (Rf-value, retention time, melting point and optical rotation) similar to the reference compound. However, polarimetry showed that C. stellaris contained the (-) enantiomer (> 97 %) as opposed to Usnea-species where (+) UA is dominant. Both proton and 13 C NMR confirmed structure identity to UA. Samples from four locations at different altitudes (250 - 650 m) around 62 o north did not differ significantly (p< 0. 05) in UA content. The study area has for a long time been carefully managed commercially for ornamental lichen harvesting to sustain regrowth. Thus applying the same practice, harvesting the lichen for recovering UA at high purity is <b>highly</b> feasible. Industrial <b>relevance.</b> Usnic acid has for a long time been subject to research for its biological activities, especially as a broad-sceptered antibiotic and more recently for its anti-cancerous effects in vitro. Considering the alarming spread of antibiotic resistance, UA can be a progressive alternative for those antibiotics, and to improve cancer treatment. Accumulating research data point to different biological effects of the two enantiomers. While the toxicity of the compound is still debated, UA is frequently used in health product formulations. The main commercial source Usnea barbata producing (+) UA is ubiquitous but limited to old-growth vegetation. C. stellaris remain a widespread, high-yielding producer of (-) UA in the boreal north, Fenno-scandinavia in particular. Under careful management, it can be efficiently harvested as already demonstrated in areas of Norway, and thereby become a valuable and sustainable source for large-scale production of UA...|$|R
40|$|This is an {{open access}} article. You can find it online by {{following}} this link: [URL] The article is published under a Creative Commons Attribution 3. 0 Unported licens (CC BY 3. 0) : [URL] Usnic acid (UA) was extracted at 1. 5 - 1. 9 % dry weight from samples of Cladonia stellaris, a carpet-forming lichen growing abundantly in mountain areas in southeastern Norway. UA {{is known for its}} antibiotic activity as well as other bio-inhibitory functions, and is currently used in a number of formulations. Thus, the objective was to isolate and analyze UA to assess its availability. Preparations, made from lichen: acetone extraction (1 : 15) and from refining by recrystallization, were analyzed by one- and two-dimensional thin layer chromatography, gas chromatography, melting point, polarimetry and NMR. Only minor contaminants were observed, and both crude and refined preparations demonstrated properties (Rf-value, retention time, melting point and optical rotation) similar to the reference compound. However, polarimetry showed that C. stellaris contained the (-) enantiomer (> 97 %) as opposed to Usnea-species where (+) UA is dominant. Both proton and 13 C NMR confirmed structure identity to UA. Samples from four locations at different altitudes (250 - 650 m) around 62 o north did not differ significantly (p< 0. 05) in UA content. The study area has for a long time been carefully managed commercially for ornamental lichen harvesting to sustain regrowth. Thus applying the same practice, harvesting the lichen for recovering UA at high purity is <b>highly</b> feasible. Industrial <b>relevance.</b> Usnic acid has for a long time been subject to research for its biological activities, especially as a broad-sceptered antibiotic and more recently for its anti-cancerous effects in vitro. Considering the alarming spread of antibiotic resistance, UA can be a progressive alternative for those antibiotics, and to improve cancer treatment. Accumulating research data point to different biological effects of the two enantiomers. While the toxicity of the compound is still debated, UA is frequently used in health product formulations. The main commercial source Usnea barbata producing (+) UA is ubiquitous but limited to old-growth vegetation. C. stellaris remain a widespread, high-yielding producer of (-) UA in the boreal north, Fenno-scandinavia in particular. Under careful management, it can be efficiently harvested as already demonstrated in areas of Norway, and thereby become a valuable and sustainable source for large-scale production of UA...|$|R
40|$|The {{objective}} {{of this study was}} to analyse the incidence of symptoms probably related to Repetitive Strain Injury (RSI) and to compare three groups of South African employees: employees highly engaged and not burned out (high vitality and dedication, low exhaustion and cynicism); employees who are engaged, but also exhausted (high vitality and dedication, high exhaustion); and burned out employees (high exhaustion and cynicism, low vitality and dedication – i. e. not engaged). A cross-sectional field survey approach was used and a convenient sample was utilised (N = 15, 663). Of the employees who indicated that they experienced RSI-related symptoms sometimes and frequently, 47 % (7427) indicated experiencing neck, shoulder and/or upper back pain (28. 9 % experienced it sometimes; 18. 5 % experienced it frequently), followed by 42 % (6595) reporting eyestrain (27. 5 % experienced it sometimes; 14. 7 % experienced it frequently), and 24 % (3838) experiencing muscle stiffness (17. 5 % experienced it sometimes; 7. 00 % experienced it frequently). From the total sample, sub-samples were created to represent the three groups (n = 4411). Significant differences existed between highly engaged employees (n = 1645), engaged employees with exhaustion (n = 1196), and burned out employees (n = 1570) with regard to eyestrain, F(2, 2739. 50) = 656. 60, p < 0. 001; muscle stiffness, F(2, 2618. 76) = 477. 05, p < 0. 001; and neck, shoulder and/or upper back discomfort F(2, 2741. 85) = 795. 48, p < 0. 001. Burned out employees and engaged employees with exhaustion experienced significantly higher RSI-related symptoms compared to the <b>highly</b> engaged group. <b>Relevance</b> to industry: RSI is the most common form of work-related ill health and has signiﬁcant implications for organisations in terms of lost productivity, drops in work quality and costly compensation claims. Not much research is available within South Africa on the incidence of RSI and how RSI-related symptoms (such as neck, shoulder and/or upper back pain, eyestrain and muscle stiffness) may differ between employees with different levels of burnout and work engagement. This research can serve to raise awareness, provide evidence on the incidence of RSI-related symptoms as well as allow for adjustments in workplace behaviour related to burnout and work engagement that can reduce the risk of RSI risk symptoms. [URL]...|$|R
40|$|Philosophiae Doctor - PhDThis {{research}} {{is in the}} area of child language acquisition, especially, acquisition of figurative language. It investigates how native Cicewa speaking children learn to interpret Cicewa idioms. This is done through examination of sociocultural contexts in which idioms are produced and consumed. It involves the identification of factors influencing children’s acquisition of idioms and strategies employed by children to interpret idioms. The study also investigates how children rerank language constraints in the process of acquiring Cicewa idiomatic meanings. The study is informed by two theories: Systemic Functional Linguistics (SFL) and Optimality Theory (OT). SFL is used to explore the sociocultural contexts within which Cicewa idioms are acquired, produced and consumed. It also helps to explain the social cultural factors influencing children’s choices of meaning options and idiom acquisition strategies. OT is used to establish how children rerank language constraints in the process of acquiring idioms in Cicewa with an aim to identify the developmental stages in idiomatic meaning acquisition. The study adopted cross-sectional and experimental designs. Experiments were conducted on 20 typically developing native Cicewa speaking children with ages 4, 6, 9, 12 and 14 drawn from Mpalume Village, in Chinamawali Township, Zomba Malawi. Deliberately developed stories, sentences containing idioms and idiom lists were used to collect data in five experiments. The data were analysed both qualitatively and quantitatively. Qualitative analysis involved identification of types of responses given by children, strategies employed by children to interpret idioms and factors that influence children’s interpretation and acquisition of idioms. Quantitative analysis was done using Statistical Package for Social Sciences (SPSS) to determine how often a response was given, differences in the responses given by children of different age groups and to establish if there was a relationship between idiom interpretation and the tested factors. The research finds that children produce more idiomatic interpretations when the idioms are presented in stories than when the idioms are presented in sentences and out of context. It also finds that idiom acquisition starts with idiom recognition at around 4 years and interpretation starts at around 6 years with a child interpreting idiomatic expressions involving daily activities of human experience. It also finds that 14 years is the age at which the child’s idiomatic knowledge starts to resemble adult’s knowledge although at this age acquisition of idiomatic meaning is still taking place. In addition, it identifies a number of strategies that children employ to interpret and acquire idiomatic expressions. Among the identified strategies ‘inferring from sociocultural context’ is the only successful strategy. Additionally, the thesis establishes that children learn first idioms that involve daily activities of human experience then idioms with clear cultural frames reference and finally idioms with obsolete cultural frames of reference. It also establishes that knowledge of the sociocultural context in which idioms are consumed is critical in idiomatic meaning acquisition. A child can have skills to use the contextual cues and have knowledge of the grammar but if s/he lacks the sociocultural knowledge cannot correctly interpret an idiom nor acquire it. It also establishes that the language constraints Full Interpretation and CONSISTENT are <b>highly</b> ranked dominating <b>Relevance</b> Principle at the initial stage in idiomatic meaning acquisition and that the two constraints are demoted as the child acquires idiomatic meaning. The thesis also identifies five developmental stages that children go through in idiom acquisition. These are Stage 1 : 4 – 5 years, an initial stage in which a child is able to recognize an idiomatic expression as an instance of use; Stage 2 : 6 – 8 years, a stage in which a child is able to interpret idioms of daily activities involving human experience when presented in supportive context; Stage 3 : 9 – 11 years, a transitional stage in the development of idiomatic meaning in which a child is able to interpret idioms involving daily activities of human experience when presented without supportive context; Stage 4 : 12 – 13 years, a stage in which a child is able to interpret idioms with clear cultural frames of reference when presented without supportive context and Stage 5 : 14 years and above, a stage in which the child’s idiomatic knowledge is close to adults’ knowledge and a child is able to interpret idioms with absolete cultural frames of reference when presented without supportive context. The study makes a contribution to the idiom acquisition debate by pointing out that sociocultural knowledge is crucial in the acquisition of idioms thereby clarifying {{what goes on in the}} process of idiom acquisition. It has also identified and described developmental stages in idiom acquisition. The study is the first not only to use SFL, but more so in conjunction with OT to account for idiomatic meaning acquisition and interpretation. This eclectic mix of theoretical frameworks is novel and thus offers a new perspective of theorizing never done before. Thus, the thesis contributes to the development of linguistic theory, from both SFL and OT perspectives. Overall, the thesis concludes that children come to know an idiomatic expression as a text before they even understand the sociocultural context in which it is consumed. It argues that idioms are acquired as texts and they are acquired together with the sociocultural context in which they are acquired, consumed and produced, and therefore the sociocultural context forms part of the idioms...|$|R

