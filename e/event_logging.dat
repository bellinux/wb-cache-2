140|1578|Public
25|$|Also, {{there is}} no support for <b>event</b> <b>logging</b> and tracing or error {{reporting}} which the Windows NT family of operating systems has, although software like Norton CrashGuard {{can be used to}} achieve similar capabilities on Windows 9x.|$|E
25|$|Windows Vista {{includes}} a completely overhauled and rewritten <b>Event</b> <b>logging</b> subsystem, known as Windows Event Log which is XML-based and allows applications to more precisely log events, offers better views, filtering and categorization by criteria, automatic log forwarding, centrally logging and managing events {{from a single}} computer and remote access.|$|E
25|$|Windows Vista {{contains}} {{a range of}} new technologies and features that are intended to help network administrators and power users better manage their systems. Notable changes include a complete replacement for NTLDR based on the Windows Preinstallation Environment, completely rewritten image-based deployment mechanisms, a significantly improved Task Scheduler, a revamped <b>event</b> <b>logging</b> infrastructure, Windows Recovery Environment, support for per-application Remote Desktop sessions, new diagnostic, health monitoring and system administration tools, {{and a range of}} new Group Policy settings covering many of the new features.|$|E
40|$|Abstract. Today, <b>event</b> <b>logs</b> contain {{vast amounts}} of data that can easily {{overwhelm}} a human. Therefore, the mining of frequent patterns from <b>event</b> <b>logs</b> is an important system and network management task. This paper discusses the properties of <b>event</b> <b>log</b> data, analyses the suitability of popular mining algorithms for processing <b>event</b> <b>log</b> data, and proposes an efficient algorithm for mining frequent patterns from <b>event</b> <b>logs.</b> ...|$|R
50|$|Event {{collection}} {{is the process}} of collecting event occurrences in a filtered <b>event</b> <b>log</b> for analysis. A filtered <b>event</b> <b>log</b> is <b>logged</b> <b>event</b> occurrences that can be of meaningful use in the future; this implies that event occurrences can be removed from the filtered <b>event</b> <b>log</b> if they are useless in the future. <b>Event</b> <b>log</b> analysis {{is the process of}} analyzing the filtered <b>event</b> <b>log</b> to aggregate <b>event</b> occurrences or {{to decide whether or not}} an event occurrence should be signalled. Event signalling is the process of signalling event occurrences over the event bus.|$|R
40|$|The growing {{interest}} in process mining is fueled by the growing availability of event data. Process mining techniques use <b>event</b> <b>logs</b> to automatically discover process models, check conformance, identify bottlenecks and deviations, suggest improvements, and predict processing times. Lion's share of process mining research {{has been devoted to}} analysis techniques. However, the proper handling of problems and challenges arising in analyzing <b>event</b> <b>logs</b> used as input is critical for the success of any process mining effort. In this paper, we identify four categories of process characteristics issues that may manifest in an <b>event</b> <b>log</b> (e. g. process problems related to event granularity and case heterogeneity) and 27 categories of <b>event</b> <b>log</b> quality issues (e. g., problems related to timestamps in <b>event</b> <b>logs,</b> imprecise activity names, and missing events). The systematic identification and analysis of these issues calls for a consolidated effort from the process mining community. Five real-life <b>event</b> <b>logs</b> are analyzed to illustrate the omnipresence of process and <b>event</b> <b>log</b> issues. We hope that these findings will encourage systematic logging approaches (to prevent <b>event</b> <b>log</b> issues), repair techniques (to alleviate <b>event</b> <b>log</b> issues) and analysis techniques (to deal with the manifestation of process characteristics in <b>event</b> <b>logs)</b> ...|$|R
500|$|Server Message Block: Performance and <b>event</b> <b>logging</b> quality improvements, {{support for}} Hyper-V Live Migration over SMB, {{bandwidth}} prioritization management, {{and the ability}} to remove SMB 1.0 support ...|$|E
2500|$|Once installed, a rootkit takes active {{measures}} to obscure its presence within the host system through subversion or evasion of standard operating system security tools and {{application programming interface}} (APIs) used for diagnosis, scanning, and monitoring. Rootkits achieve this by modifying the behavior of core parts of an operating system through loading code into other processes, the installation or modification of drivers, or kernel modules. Obfuscation techniques include concealing running processes from system-monitoring mechanisms and hiding system files and other configuration data. It {{is not uncommon for}} a rootkit to disable the <b>event</b> <b>logging</b> capacity of an operating system, in an attempt to hide evidence of an attack. Rootkits can, in theory, subvert any operating system activities. The [...] "perfect rootkit" [...] {{can be thought of as}} similar to a [...] "perfect crime": one that nobody realizes has taken place. Rootkits also take a number of measures to ensure their survival against detection and [...] "cleaning" [...] by antivirus software in addition to commonly installing into Ring 0 (kernel-mode), where they have complete access to a system. These include polymorphism (changing so their [...] "signature" [...] is hard to detect), stealth techniques, regeneration, disabling or turning off anti-malware software. and not installing on virtual machines where it may be easier for researchers to discover and analyze them.|$|E
50|$|Introduced {{with the}} launch of the Cisco ASA 5580 products, NetFlow Security <b>Event</b> <b>Logging</b> {{utilizes}} NetFlow v9 fields and templates in order to efficiently deliver security telemetry in high performance environments. NetFlow Security <b>Event</b> <b>Logging</b> scales better than syslog while offering the same level of detail and granularity in logged events.|$|E
40|$|Process mining methods use data {{recorded}} by information systems {{to analyze the}} real execution of processes. This event data is stored in an <b>event</b> <b>log,</b> which is the main input to most process mining methods. The XES standard provides a uniform way to store <b>event</b> <b>logs.</b> OpenXES is the XES reference implementation, which is used widely by research tools. However, OpenXES is not scalable towards large <b>event</b> <b>log.</b> XESLite provides solutions to manage large <b>event</b> <b>logs</b> that are compatible with the OpenXES interfaces. Therefore, {{it can be used}} as d 3 ̆cbr/ 3 ̆erop-in replacement for existing algorithms. This report investigates the storage requirements of different types of <b>event</b> <b>logs,</b> describes XESLite, and contains a benchmark of XESLite and OpenXES based on real-life <b>event</b> <b>logs...</b>|$|R
40|$|Process-oriented {{data mining}} (process mining) uses {{algorithms}} and data (in {{the form of}} <b>event</b> <b>logs)</b> to construct models that aim to provide insights into organisational processes. The quality of the data (both form and content) presented to the modeling algorithms {{is critical to the}} success of the process mining exercise. Cleaning <b>event</b> <b>logs</b> to address quality issues prior to conducting a process mining analysis is a necessary, but generally tedious and ad hoc task. In this paper we describe a set of data quality issues, distilled from our experiences in conducting process mining analyses, commonly found in process mining <b>event</b> <b>logs</b> or encountered while preparing <b>event</b> <b>logs</b> from raw data sources. We show that patterns are used in a variety of domains as a means for describing commonly encountered problems and solutions. The main contributions of this article are in showing that a patterns-based approach is applicable to documenting commonly encountered <b>event</b> <b>log</b> quality issues, the formulation of a set of components for describing <b>event</b> <b>log</b> quality issues as patterns, and the description of a collection of 11 <b>event</b> <b>log</b> imperfection patterns distilled from our experiences in preparing <b>event</b> <b>logs.</b> We postulate that a systematic approach to using such a pattern repository to identify and repair <b>event</b> <b>log</b> quality issues benefits both the process of preparing an <b>event</b> <b>log</b> and the quality of the resulting <b>event</b> <b>log.</b> The relevance of the pattern-based approach is illustrated via application of the patterns in a case study and through an evaluation by researchers and practitioners in the field...|$|R
40|$|Abstract—Process mining is a {{relatively}} new research area aiming to extract process models from <b>event</b> <b>logs</b> of real systems. A lot of new approaches and algorithms are developed in this field. Researches and developers usually have a need to test end evaluate the newly constructed algorithms. In this paper we propose a new approach for generation of <b>event</b> <b>logs.</b> It serves to facilitate the process of evaluation and testing. Presented approach allows to generate <b>event</b> <b>logs,</b> and sets of <b>event</b> <b>logs</b> to support a large scale testing in a more automated manner. Another feature of the approach is a generation of <b>event</b> <b>logs</b> with noise. This feature allows to simulate real-life system execution with inefficiencies, drawbacks, and crashes. In this work we also consider other existing approaches. Their forces and weaknesses are shown. The approach presented as well as the corresponding tool can be widely used in the research and development process. Keywords—Process mining, Petri net, <b>event</b> <b>log,</b> <b>event</b> <b>log</b> generation, ProM. I...|$|R
5000|$|<b>Event</b> <b>logging</b> {{provides}} system administrators {{with information}} useful for diagnostics and auditing. The different classes {{of events that}} will be logged, {{as well as what}} details will appear in the event messages, are often considered early in the development cycle. Many <b>event</b> <b>logging</b> technologies allow or even require each class of event to be assigned a unique [...] "code", which is used by the <b>event</b> <b>logging</b> software or a separate viewer (e.g., Event Viewer) to format and output a human-readable message. This facilitates localization and allows system administrators to more easily obtain information on problems that occur.|$|E
5000|$|ChanServ, which {{provides}} facilities such as <b>event</b> <b>logging</b> and advanced user/ban management.|$|E
50|$|<b>Event</b> <b>logging</b> {{and chat}} logging in either plain text or HTML with {{optional}} encryption.|$|E
40|$|The 'Hospital Billing' <b>event</b> <b>log</b> was {{obtained}} from the financial modules of the ERP system of a regional hospital. The <b>event</b> <b>log</b> contains <b>events</b> that are related to the billing of medical services that have been provided by the hospital. Each trace of the <b>event</b> <b>log</b> records the activities executed to bill a package of medical services that were bundled together. The <b>event</b> <b>log</b> does not contain information about the actual medical services provided by the hospital. The 100, 000 traces in the <b>event</b> <b>log</b> are a random sample of process instances that were recorded over three years. Several attributes such as the 'state' of the process, the 'caseType', the underlying 'diagnosis' etc. are included in the <b>event</b> <b>log.</b> <b>Events</b> and attribute values have been anonymized. The time stamps of events have been randomized for this purpose, but the time between events within a trace has not been altered. More information about the <b>event</b> <b>log</b> {{can be found in the}} related publications...|$|R
40|$|This dataset {{comprises}} <b>event</b> <b>logs</b> (XES = Extensible Event Stream) {{regarding the}} {{activities of daily living}} performed by several individuals. The <b>event</b> <b>logs</b> were derived from sensor data which was collected in different scenarios and represent activities of daily living performed by several individuals. These include e. g., sleeping, meal preparation, and washing. The <b>event</b> <b>logs</b> show the different behavior of people in their own homes but also common patterns. The attached <b>event</b> <b>logs</b> were created with Fluxicon Disco () ...|$|R
5000|$|Access a {{persistent}} <b>event</b> <b>log,</b> stored in protected memory. The <b>event</b> <b>log</b> is available OOB, {{even if the}} OS is down or the hardware has already failed.|$|R
5000|$|The {{types of}} {{messages}} that are logged are often less stable {{through the development}} cycle than for <b>event</b> <b>logging.</b>|$|E
50|$|Because <b>event</b> <b>logging</b> {{is used to}} log {{high-level}} information (often failure information), {{performance of}} the logging implementation is often less important.|$|E
5000|$|Complete IPMI 2.0 implementation, {{providing}} sensor {{and health}} monitoring, alerting, <b>event</b> <b>logging</b> Serial over LAN, et cetera. This firmware utilizes Linux 2.6.|$|E
40|$|Abstract. In {{the area}} of process mining, the ILP Miner is known {{for the fact that}} it always returns a Petri net that {{perfectly}} fits a given <b>event</b> <b>log.</b> However, the downside of the ILP Miner is that its complexity is exponential in the number of event classes in that <b>event</b> <b>log.</b> As a result, the ILP Miner may take a very long time to return a Petri net. Partitioning the traces in the <b>event</b> <b>log</b> over multiple <b>event</b> <b>logs</b> does not really alleviate this problem. Like for most process discovery algorithms, the complexity is linear {{in the size of the}} <b>event</b> <b>log</b> and exponential in the number of event classes (i. e., distinct activities). Hence, the potential gain by partitioning the event classes is much higher. This paper proposes to use the so-called passages to split up the event classes over multiple <b>event</b> <b>logs,</b> and shows what the results are for seven large <b>event</b> <b>logs.</b> The results show that indeed the use of passages alleviates the complexity, but that much hinges on the size of the largest passage detected: The smaller this passage, the better the run time...|$|R
40|$|With organisations facing {{significant}} {{challenges to}} remain competitive, Business Process Improvement (BPI) initiatives are often conducted {{to improve the}} efficiency and effectiveness of their business processes, focussing on time, cost, and quality improvements. <b>Event</b> <b>logs</b> which contain a detailed record of business operations over a certain time period, recorded by an organisation’s information systems, are the first step towards initiating evidence-based BPI activities. Given an (original) <b>event</b> <b>log</b> as a starting point, an approach to explore better ways to execute a business process was developed, resulting in an improved (perturbed) <b>event</b> <b>log.</b> Identifying {{the differences between the}} original <b>event</b> <b>log</b> and the perturbed <b>event</b> <b>log</b> can provide valuable insights, helping organisations to improve their processes. However, {{there is a lack of}} automated techniques and appropriate visualisations to detect the differences between two <b>event</b> <b>logs.</b> Therefore, this research aims to develop visualisation techniques to provide targeted analysis of resource reallocation and activity rescheduling. The differences between two <b>event</b> <b>logs</b> are first identified. The changes between the two <b>event</b> <b>logs</b> are conceptualised and realised with a number of visualisations. With the proposed visualisations, analysts are able to identify resource- and time-related changes that resulted in a cost reduction, and subsequently investigate and translate them into actionable items for BPI in practice. Ultimately, analysts can make use of this comparative information to initiate evidence-based BPI activities. status: publishe...|$|R
40|$|Nowadays, {{information}} systems in organizations logged {{many kinds of}} <b>event</b> <b>logs.</b> These logs can be analyzed with process mining. The goal of process mining is to extract information from <b>event</b> <b>logs</b> {{and use it as}} an insights into the business process of the organizations. Unfortunately, most process mining techniques only consider process instance in isolatkategorialnfluence the analysis of the organizations. In this paper, we developed an approach to get insights in to context-related information based on <b>event</b> <b>logs,</b> such that useful insights can be easily obtained. Our approach has been evaluated through two case studies. The evaluation outcome confirms that most of the context-related information from <b>event</b> <b>logs</b> can be analyzed...|$|R
50|$|SystemDiagnostics: Defines {{types that}} provide the ability to {{diagnose}} applications. It includes <b>event</b> <b>logging,</b> performance counters, tracing and interaction with system processes.|$|E
5000|$|In {{operating}} systems, tracing {{is sometimes}} useful in situations (such as booting) {{where some of}} the technologies used to provide <b>event</b> <b>logging</b> may not be available.|$|E
5000|$|Server Message Block: Performance and <b>event</b> <b>logging</b> quality improvements, {{support for}} Hyper-V Live Migration over SMB, {{bandwidth}} prioritization management, {{and the ability}} to remove SMB 1.0 support ...|$|E
50|$|Process {{discovery}} {{is one of}} the three main types of process mining. The other two types of process mining are conformance checking and model extension/enhancement. All of these techniques aim at extracting process related knowledge from <b>event</b> <b>logs.</b> In the case of process discovery, there is no prior process model; the model is discovered based on <b>event</b> <b>logs.</b> Conformance checking aims at finding differences between a given process model and <b>event</b> <b>log.</b> This way it is possible to quantify compliance and analyze discrepancies. Enhancement takes an a priori model and improves or extends it using information from the <b>event</b> <b>log,</b> e.g., show bottlenecks.|$|R
40|$|Increasingly {{information}} systems log historic {{information in a}} systematic way. Workflow management systems, but also ERP, CRM, SCM, and B 2 B systems often provide a so-called <b>event</b> <b>log,</b> i. e., a log recording the execution of activities. Unfortunately, the information in these <b>event</b> <b>logs</b> is rarely {{used to analyze the}} underlying processes. Process mining aims at improving this by providing techniques and tools for discovering process, control, data, organizational, and social structures from <b>event</b> <b>logs.</b> This paper focuses on the mining social networks. This is possible because <b>event</b> <b>logs</b> typically record information about the users executing the activities recorded in the log. To do this we combine concepts from workflow management and social network analysis. This paper introduces the approach, defines metrics, and presents a tool to mine social networks from <b>event</b> <b>logs...</b>|$|R
40|$|Process mining is a {{relatively}} new ﬁeld of computer science, which deals with process discovery and analysis based on <b>event</b> <b>logs.</b> In this paper we consider the problem of discovering a high-level business process model from a low-level <b>event</b> <b>log,</b> i. e. automatic synthesis of process models based on the information stored in <b>event</b> <b>logs</b> of information systems. Events in a high-level model are abstract events, which can be reﬁned to low-level subprocesses, whose behavior is recorded in <b>event</b> <b>logs.</b>  Models synthesis is intensively studied in the frame of process mining research, but only models and event logs of the same granularity are mainly considered in the literature. Here we present an algorithm for discovering high-level acyclic process models from <b>event</b> <b>logs</b> and some speciﬁed partition of low-level events into subsets associated with abstract events in a high-level model. </p...|$|R
5000|$|Data logging records basic {{compliance}} info or detailed <b>event</b> <b>logging,</b> {{allowing the}} sleep physician (or patient) to download {{and analyse data}} recorded by the machine to verify treatment effectiveness.|$|E
50|$|Various {{groups are}} working on draft {{standards}} detailing the use of syslog {{for more than just}} network and security <b>event</b> <b>logging,</b> such as its proposed application within the health care environment.|$|E
50|$|The log {{service is}} {{intended}} for <b>event</b> <b>logging,</b> that is, for collecting cluster-wide, function-based (as opposed to implementation specific) information about the system, which is suited for system administrators or automated tools.|$|E
40|$|A {{collection}} of artificial <b>event</b> <b>logs</b> describing 4 variants {{of a simple}} loan application process. Variant 1 is the most complex process with parallelism and choices. The other 3 variants have a simpler, more sequential, control flow and some activities of variant 1 are missing or split into 2. These <b>event</b> <b>logs</b> are used to test different approaches of discovering a configurable process model from a {{collection of}} <b>event</b> <b>logs...</b>|$|R
50|$|Process mining is {{a process}} {{management}} technique that allows {{for the analysis of}} business processes based on <b>event</b> <b>logs.</b> During process mining, specialized data-mining algorithms are applied to <b>event</b> <b>log</b> datasets in order to identify trends, patterns and details contained in <b>event</b> <b>logs</b> recorded by an information system. Process mining aims to improve process efficiency and understanding of processes. Process mining is also known as Automated Business Process Discovery (ABPD).|$|R
40|$|Abstract. The {{basic idea}} of process mining is to extract {{knowledge}} from <b>event</b> <b>logs</b> recorded by an information system. Until recently, {{the information in}} these <b>event</b> <b>logs</b> was rarely {{used to analyze the}} underlying processes. Process mining aims at improving this by providing techniques and tools for discovering process, organizational, social, and performance information from <b>event</b> <b>logs.</b> Fuelled by the omnipresence of <b>event</b> <b>logs</b> in transactional information systems (cf. WFM, ERP, CRM, SCM, and B 2 B systems), process mining has become a vivid research area [1, 2]. In this paper we introduce the challenging process mining domain and discuss a heuristics driven process mining algorithm; the so-called “HeuristicsMiner ” in detail. HeuristicsMiner is a practical applicable mining algorithm that can deal with noise, and can be used to express the main behavior (i. e. not all details and exceptions) registered in an <b>event</b> <b>log.</b> In the experimental section of this paper we introduce benchmark material (12. 000 different <b>event</b> <b>logs)</b> and measurements by which the performance of process mining algorithms can be measured...|$|R
