3071|1756|Public
5|$|Many {{problems}} in data analysis concern clustering, grouping <b>data</b> <b>items</b> into clusters of closely related items. Hierarchical clustering is {{a version of}} cluster analysis in which the clusters form a hierarchy or tree-like structure rather than a strict partition of the <b>data</b> <b>items.</b> In some cases, this type of clustering may be performed {{as a way of}} performing cluster analysis at multiple different scales simultaneously. In others, the data to be analyzed naturally has an unknown tree structure and the goal is to recover that structure by performing the analysis. Both of these kinds of analysis can be seen, for instance, in the application of hierarchical clustering to biological taxonomy. In this application, different living things are grouped into clusters at different scales or levels of similarity (species, genus, family, etc). This analysis simultaneously gives a multi-scale grouping of the organisms of the present age, and aims to accurately reconstruct the branching process or evolutionary tree that in past ages produced these organisms.|$|E
5|$|Dataflow {{programming}} languages describe {{systems of}} operations on data streams, and {{the connections between}} the outputs of some operations and the inputs of others. These languages can be convenient for describing repetitive data processing tasks, in which the same acyclically-connected collection of operations is applied to many <b>data</b> <b>items.</b> They can be executed as a parallel algorithm in which each operation is performed by a parallel process as soon as another set of inputs becomes available to it.|$|E
25|$|The {{count of}} <b>data</b> <b>items</b> should be {{reported}} at the beginning (the first byte).|$|E
30|$|We {{found that}} the {{proposed}} algorithm is presently in the broadcast structure. The wireless broadcast scheduling has been considered the <b>data</b> <b>item</b> frequency of the fixed and it has an unreasonable supposition. The <b>data</b> <b>item</b> frequency would be {{the request of the}} client for a change under the factual dynamic environments. Each of the <b>data</b> <b>item</b> has a frequency value itself and the each frequency of <b>data</b> <b>item</b> should been computed for its weight value and adjusted for dynamic broadcast adaptive so the frequency of <b>data</b> <b>item</b> has no fixed probability value.|$|R
3000|$|... {{age of a}} <b>data</b> <b>item,</b> {{calculated}} by taking {{the difference between the}} current time, t_curr, and the measurement time of that <b>data</b> <b>item</b> t(d); [...]...|$|R
5000|$|With read caches, a <b>data</b> <b>item</b> {{must have}} been fetched from its {{residing}} location {{at least once in}} order for subsequent reads of the <b>data</b> <b>item</b> to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a <b>data</b> <b>item</b> may be realized upon the first write of the <b>data</b> <b>item</b> by virtue of the <b>data</b> <b>item</b> immediately being stored in the cache's intermediate storage, deferring the transfer of the <b>data</b> <b>item</b> to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand, ...|$|R
25|$|All <b>data</b> <b>items</b> of the In-use Performance Tracking record {{consist of}} two (2) bytes and are {{reported}} in this order (each message contains two items, hence the message length is 4).|$|E
25|$|Race and {{ethnicity}} in the United States Census, {{defined by the}} federal Office of Management and Budget (OMB) and the United States Census Bureau, are self-identification <b>data</b> <b>items</b> in which residents choose the race or races with which they most closely identify, and indicate {{whether or not they}} are of Hispanic or Latino origin (the only categories for ethnicity).|$|E
25|$|Hollerith {{initially}} {{did business}} {{under his own}} name, as The Hollerith Electric Tabulating System, specializing in punched card data processing equipment. He provided tabulators and other machines under contract for the Census Office, which used them for the 1890 census. The net effect of the many changes from the 1880 census: the larger population, the <b>data</b> <b>items</b> to be collected, the Census Bureau headcount, the scheduled publications, {{and the use of}} Hollerith's electromechanical tabulators, was to reduce the time required to process the census from eight years for the 1880 census to six years for the 1890 census.|$|E
500|$|An 88 level-number {{declares}} a [...] (a so-called 88-level) {{which is}} true when its parent <b>data</b> <b>item</b> contains one of the values specified in its [...] clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the [...] <b>data</b> <b>item.</b> When the <b>data</b> <b>item</b> contains a value of , the condition-name [...] is true, whereas when it contains a value of [...] or , the condition-name [...] is true. If the <b>data</b> <b>item</b> contains some other value, both of the condition-names are false.|$|R
50|$|A value {{written by}} a process on a <b>data</b> <b>item</b> X will be always {{available}} to a successive read operation performed by the same process on <b>data</b> <b>item</b> X.|$|R
50|$|Monotonic read {{consistency}} {{guarantees that}} after a process reads a value of <b>data</b> <b>item</b> x at time t, it will never see the older value of that <b>data</b> <b>item.</b>|$|R
25|$|When {{the dummy}} has been {{determined}} {{to be ready for}} testing, calibration marks are fastened {{to the side of the}} head to aid researchers when slow-motion films are reviewed later. The dummy is then placed inside the test vehicle, set to seating position and then marked on either the head and knees. Up to fifty-eight data channels located in all parts of the Hybrid III, from the head to the ankle, record between 30000 and 35000 <b>data</b> <b>items</b> in a typical 100–150millisecond crash. Recorded in a temporary data repository in the dummy's chest, these data are downloaded to computer once the test is complete.|$|E
500|$|<b>Data</b> <b>items</b> in COBOL are {{declared}} hierarchically {{through the}} use of level-numbers which indicate if a data item is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level <b>data</b> <b>items,</b> with a level-number of 1, are called [...] Items that have subordinate aggregate data are called those that do not are called [...] Level-numbers used to describe standard <b>data</b> <b>items</b> are between 1 and 49.|$|E
500|$|A 77 level-number {{indicates}} the item is stand-alone, {{and in such}} situations {{is equivalent to the}} level-number 01. For example, the following code declares two 77-level <b>data</b> <b>items,</b> [...] and , which are non-group <b>data</b> <b>items</b> that are independent of (not subordinate to) any other data items: ...|$|E
5000|$|Each <b>data</b> <b>item</b> {{behaviour}} {{is defined}} by the Major Type and Additional Type. The major type is used for selecting the main behaviour or type of each <b>data</b> <b>item.</b>|$|R
5000|$|The quorum-based {{voting for}} replica control {{is due to}} 1979.Each copy of a {{replicated}} <b>data</b> <b>item</b> is assigned a vote. Each operation then has to obtain a read quorum (Vr) or a write quorum (Vw) to read or write a <b>data</b> <b>item,</b> respectively. If a given <b>data</b> <b>item</b> has a total of V votes, the quorums have to obey the following rules: ...|$|R
50|$|The {{first rule}} ensures that a <b>data</b> <b>item</b> is not read and written by two {{transactions}} concurrently. Additionally, it ensures that a read quorum contains {{at least one}} site with the newest version of the <b>data</b> <b>item.</b> The second rule ensures that two write operations from two transactions cannot occur concurrently on the same <b>data</b> <b>item.</b> The two rules ensure that one-copy serializability is maintained.|$|R
500|$|A [...] (or [...] ) clause is {{a string}} of characters, each of which {{represents}} {{a portion of the}} data item and what it may contain. Some picture characters specify the type of the item and how many characters or digits it occupies in memory. For example, a [...] indicates a decimal digit, and an [...] indicates that the item is signed. Other picture characters (called [...] and [...] characters) specify how an item should be formatted. For example, a series of [...] characters define character positions as well as how a leading sign character is to be positioned within the final character data; the rightmost non-numeric character will contain the item's sign, while other character positions corresponding to a [...] to the left of this position will contain a space. Repeated characters can be specified more concisely by specifying a number in parentheses after a picture character; for example, [...] is equivalent to [...] Picture specifications containing only digit (...) and sign (...) characters define purely [...] <b>data</b> <b>items,</b> while picture specifications containing alphabetic (...) or alphanumeric (...) characters define [...] <b>data</b> <b>items.</b> The presence of other formatting characters define [...] or [...] <b>data</b> <b>items.</b>|$|E
500|$|... {{according}} to some chosen radix; then, {{the part of the}} key used for the th pass of the algorithm is the th digit in the positional notation for the full key, starting from the least significant digit and progressing to the most significant. For this algorithm to work correctly, the sorting algorithm used in each pass over the data must be stable: items with equal digits should not change positions with each other. For greatest efficiency, the radix should be chosen to be near the number of <b>data</b> <b>items,</b> [...] Additionally, using a power of two near [...] as the radix allows the keys for each pass to be computed quickly using only fast binary shift and mask operations. With these choices, [...] and with pigeonhole sort or counting sort as the base algorithm, the radix sorting algorithm can sort [...] <b>data</b> <b>items</b> having keys in the range from [...] to [...] in time [...]|$|E
500|$|Pigeonhole sort or {{counting}} sort {{can both}} sort [...] <b>data</b> <b>items</b> having {{keys in the}} range from [...] to [...] in time [...] In pigeonhole sort (often called bucket sort), pointers to the <b>data</b> <b>items</b> are distributed to a table of buckets, represented as collection data types such as linked lists, using the keys as indices into the table. Then, all of the buckets are concatenated together to form the output list. Counting sort uses a table of counters in place of a table of buckets, to determine the number of items with each key. Then, a prefix sum computation is used to determine the range of positions in the sorted output at which the values with each key should be placed. Finally, in a second pass over the input, each item is moved to its key's position in the output array. Both algorithms involve only simple loops over the input data (taking time [...] ) and over the set of possible keys (taking time [...] ), giving their [...] overall time bound.|$|E
5000|$|In COBOL, a fully {{qualified}} <b>data</b> <b>item</b> name can {{be created}} by suffixing a potentially ambiguous identifier with an [...] (or [...] ) phrase. For example, multiple <b>data</b> <b>item</b> records might contain a member item named , so specifying [...] serves to disambiguate a specific [...] <b>data</b> <b>item,</b> specifically, {{the one that is}} a member of the parent [...] <b>data</b> <b>item.</b> Multiple clauses may be necessary to fully disambiguate a given identifier, for example, [...] This syntax is equivalent to the [...] "dotted" [...] notation employed in many object-oriented programming languages, but with the identifiers specified in reverse order.|$|R
5000|$|... a pop or pull operation: a <b>data</b> <b>item</b> at {{the current}} {{location}} pointed to by the stack pointer is removed, and the stack pointer is adjusted {{by the size of}} the <b>data</b> <b>item.</b>|$|R
40|$|This paper gives {{notification}} of changes {{to be included}} in the NHS Data Dictionary & Manual and the NHS CDS Manual as appropriate. These will be consolidated into the publications in due course. Summary of Changes: Due to the transfer to HTML, <b>data</b> <b>item</b> names in the NHS Data Dictionary & Manual have been changed as follows: (i) (ii) remove the characters / : * ? “ ” . ! space from entity names, attribute names, and <b>data</b> <b>item</b> note names shorten <b>data</b> <b>item</b> note names to 50 characters or less. Change Proposa...|$|R
2500|$|... and A'i and t'i are {{the number}} of <b>data</b> <b>items</b> of type A and {{the total number of}} items in the ith sample.|$|E
2500|$|All <b>data</b> <b>items</b> {{consist of}} two (2) bytes and are {{reported}} in this order (each message contains two items, hence message length is 4): ...|$|E
2500|$|No {{limits for}} what they call [...] "non-personally {{identifiable}} information ('Non-PII')". [...] "Non-PII ... may include... anonymously generated device identifiers", which are tied to most other <b>data</b> <b>items</b> listed above.|$|E
30|$|In {{order to}} ensure strong consistency, N = W; that is, all replicas of the <b>data</b> <b>item</b> need to be updated (written) before the <b>data</b> <b>item</b> can be {{accessed}} by an application. However such level of consistency results in poor efficiency as all replicas need to be synchronized. Weaker level of consistency {{can be achieved by}} setting W < N, such as W =  1 and N =  3 (assume that the number of replicas is 3). In this case, write operation has to update one replica of the <b>data</b> <b>item.</b> Other replicas can be updated after certain delay and made consistent (which is the case of eventual consistency).|$|R
50|$|Column # 4 {{shows the}} units for each raw <b>data</b> <b>item.</b>|$|R
50|$|A time-stamping service {{provides}} {{evidence that}} a <b>data</b> <b>item</b> existed before {{a certain point in}} time. Time-stamp services produce time-stamp tokens, which are data structures containing a verifiable cryptographic binding between a <b>data</b> <b>item's</b> representation and a time-value. This part of ISO/IEC 18014 defines time-stamping mechanisms that produce independent tokens, which can be verified one by one.|$|R
2500|$|The {{principal}} {{benefit of}} a linked list over a conventional array is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the <b>data</b> <b>items</b> need not be stored [...] in memory or on disk, while an array has to be declared in the source code, before compiling and running the program. Linked lists allow insertion and removal of nodes {{at any point in}} the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.|$|E
2500|$|Lightning Memory-Mapped Database (LMDB) is a {{software}} library {{that provides a}} high-performance embedded transactional database {{in the form of}} a key-value store. [...] LMDB is written in C with API bindings for several programming languages. LMDB stores arbitrary key/data pairs as byte arrays, has a range-based search capability, supports multiple <b>data</b> <b>items</b> for a single key and has a special mode for appending records at the end of the database (MDB_APPEND) which gives a dramatic write performance increase over other similar stores. LMDB is not a relational database, it is strictly a key-value store like Berkeley DB and dbm.|$|E
2500|$|Another {{disadvantage}} of linked lists is the extra storage needed for references, which often makes them impractical for lists of small <b>data</b> <b>items</b> such as characters or boolean values, because the storage overhead for the links may exceed {{by a factor}} of two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data)., where [...] is a per-array constant, [...] is a per-dimension constant, and [...] is the number of dimensions. [...] and [...] are typically on the order of 10 bytes. [...] It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.|$|E
40|$|In {{recent years}} we have been {{watching}} a tremendous increase {{in the growth of}} online social networks(OSNs). OSNs enable people to share personal and public information and make social connections with friends, family members and other peoples. In addition to the rapid increase in the use of social network, it raises a number of security and privacy issues. While OSNs allow users to restrict access to shared data, they currently do not provide any mechanism to totally enforce privacy issue solver associated with multiple users. The proposed method implements a solution yo facilitate collaborative management of common <b>data</b> <b>item</b> in OSNs. Each controller of the <b>data</b> <b>item</b> can set his privacy settings to the shared <b>data</b> <b>item.</b> The proposed method also identifies privacy conflicting segments and helps in resolving the privacy conflicts and a final decision is made whether or not to provide access to the shared <b>data</b> <b>item...</b>|$|R
40|$|Replicated data is {{employed}} in distributed databases to enhance data availability. However, {{the benefit of}} data availability is only realized {{at the cost of}} elaborate algorithms which hide the underlying complexity of maintaining multiple copies of a single <b>data</b> <b>item.</b> The concern about the performance impact of replica control is part of the reasons why replication, although extensively researched, has yet to receive wide acceptance in practice. This paper makes use of a simulation model to explore the performance tradeoffs of data replication. Keywords: replication control, transaction response time, throughput, data availability. 1 INTRODUCTION Replicated data {{is employed}} in distributed databases to enhance data availability: multiple copies of a critical <b>data</b> <b>item</b> are maintained, typically on separate sites, so that the <b>data</b> <b>item</b> can be retrieved even if some copies of the <b>data</b> <b>item</b> cannot be accessed due to system failures. However, this benefit of data availability is only reali [...] ...|$|R
5000|$|All primary {{replicas of}} each <b>data</b> <b>item</b> resides {{on the same}} {{computer}} called server.|$|R
