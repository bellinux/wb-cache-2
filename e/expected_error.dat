677|1329|Public
25|$|A simple {{error model}} is to {{introduce}} a small error to the data probability term in the homozygous cases, allowing a small constant probability that nucleotides which don't match the A allele are observed in the AA case, and respectively a small constant probability that nucleotides not matching the B allele are observed in the BB case. However more sophisticated procedures are available which attempt to more realistically replicate the actual error patterns observed in real data in calculating the conditional data probabilities. For instance, estimations of read quality (measured as Phred quality scores) have been incorporated in these calculations, {{taking into account the}} <b>expected</b> <b>error</b> rate in each individual read at a locus. Another technique that has successfully been incorporated into error models is base quality recalibration, where separate error rates are calculated - based on prior known information about error patterns - for each possible nucleotide substitution. Research shows that each possible nucleotide substitution is not equally likely to show up as an error in sequencing data, and so base quality recalibration has been applied to improve error probability estimates.|$|E
2500|$|A tie-breaking {{rule that}} is less biased [...] is round half to even on the {{assumption}} that the original numbers are precise (even if positive or negative with unequal probability). [...] By this convention, if the fraction of [...] is 0.5, then [...] is the even integer nearest to [...] [...] Thus, for example, +23.5 becomes +24, as does +24.5; while −23.5 becomes −24, as does −24.5. This approach is intended to minimize the <b>expected</b> <b>error</b> when summing over rounded figures.|$|E
2500|$|A 1999 metastudy {{combined}} {{data from}} five studies from western countries. The metastudy reported mortality ratios, where lower numbers indicated fewer deaths, for fish eaters to be 0.82, vegetarians to be 0.84, occasional meat eaters (eat meat less than once per week) to be 0.84. Regular meat eaters had the base mortality rate of 1.0, {{while the number}} for vegans was very uncertain (anywhere between 0.7 and 1.44) due to too few data points. The study reported the numbers of deaths in each category, and <b>expected</b> <b>error</b> ranges for each ratio, and adjustments made to the data. However, the [...] "lower mortality was due largely to the relatively low prevalence of smoking in these [...] cohorts". Out of the major causes of death studied, only one difference in mortality rate was attributed to the difference in diet, as the conclusion states: [...] "...vegetarians had a 24% lower mortality from ischaemic heart disease than non-vegetarians, but no associations of a vegetarian diet with other major causes of death were established".|$|E
40|$|In this article, we {{analyzed}} the <b>expected</b> training <b>error</b> and the <b>expected</b> generalization <b>error</b> for neural networks in unidentifiable case, {{in which a}} set of output data {{is assumed to be}} a Gaussian noise sequence. Firstly, the results on the bounds of the <b>expected</b> training <b>error</b> and the <b>expected</b> generalization <b>error</b> for a general neural networks are reviewed. Secondly, we gave the order of the <b>expected</b> <b>errors</b> for a special Gaussian unit by combining the notion of # [...] covering and the extreme value theory...|$|R
40|$|The {{estimation}} of a diversity index is studied using the Bayesian framework {{provided by the}} superpopulation model theory. The main theoretical properties are studied when Fager and Simpson indexes are used. Different distributions are considered for illustrating the behaviour of the <b>expected</b> <b>errors</b> under the analysed models...|$|R
40|$|A {{mathematical}} {{analysis of the}} formation of hemolytic plaques in agar is presented. The results can be used to calculate the rate constant for antigen-antibody association and the distribution of antibody secretion rates. The limitations of the treatment and estimates of the <b>expected</b> <b>errors</b> involved are briefly discussed...|$|R
5000|$|The {{goal of this}} {{learning}} {{problem is}} to find a function that fits or predicts the outcome (label) that minimizes the <b>expected</b> <b>error</b> over all possible inputs and labels. The <b>expected</b> <b>error</b> of a function [...] is: ...|$|E
5000|$|Therefore, for any {{constant}} [...] {{there is}} a constant [...] such that the <b>expected</b> <b>error</b> of the estimate is at most [...] For example, 400 hashes would be required to estimate [...] with an <b>expected</b> <b>error</b> {{less than or equal to}} [...]05.|$|E
5000|$|<b>Expected</b> <b>error</b> reduction: label {{those points}} that would most reduce the model's {{generalization}} error ...|$|E
5000|$|<b>Expected</b> user <b>errors</b> in {{identifying}} risky situations (this study); ...|$|R
5000|$|... #Subtitle level 3: Approximate Minimizer of <b>Expected</b> Squared <b>Error</b> ...|$|R
40|$|As {{could be}} <b>expected</b> <b>errors</b> and {{omissions}} {{occurred in the}} previous compilations which are corrected here. Indonesia — Wanariset (WAN) : Herbarium, Wanariset Samboja, Forestry Research Institute of Samarinda, POB 1206, Samarinda, East Kalimantan, Borneo. Tel. (62) 542 - 35206. Fax (62) 541 - 42298 / 542 - 22640. E-mail: trobos@server. indo. net. i...|$|R
50|$|By {{standard}} Chernoff bounds for sampling without replacement, this estimator has <b>expected</b> <b>error</b> , {{matching the}} performance of the multiple-hash-function scheme.|$|E
5000|$|... the {{irreducible}} error [...] Since {{all three}} terms are non-negative, this forms a lower bound on the <b>expected</b> <b>error</b> on unseen samples.|$|E
50|$|Alternatively, the IASI Level 1 {{data can}} be {{processed}} by least square fit algorithms. Again, the <b>expected</b> <b>error</b> {{must be taken into}} consideration.|$|E
30|$|Calculate the <b>expected</b> {{reconstruction}} <b>error</b> is denoted D 0 using expression (5).|$|R
5000|$|A more exact formal {{analysis}} shows the <b>expected</b> static <b>error.</b> We assume: ...|$|R
40|$|The {{influence}} of the Stiles-Crawford effect on visual performance can be investigated by filters based on the apodisation model of the Stiles-Crawford effect. We describe the development of practical filters to achieve neutralisation. We present some results of the Stiles-Crawford function showing that the filters work well for <b>expected</b> <b>errors</b> in aligning filters {{in front of the}} eye...|$|R
5000|$|Time Specific Bug Pattern - Expose the bug {{by writing}} a {{continuous}} test that runs continuously and fails when an <b>expected</b> <b>error</b> occurs. This {{is useful for}} transient bugs.|$|E
5000|$|... #Caption: <b>Expected</b> <b>error</b> in {{the mean}} of A for a sample of n data points with sample bias {{coefficient}} ρ. The unbiased standard error plots as the ρ=0 diagonal line with log-log slope -½.|$|E
5000|$|Typically in {{learning}} problems, only {{a subset of}} input data and labels are available, measured with some noise. Therefore, the <b>expected</b> <b>error</b> is unmeasurable, and the best surrogate available is the empirical error over the [...] available samples: ...|$|E
40|$|Mathematical {{models are}} often {{described}} by multivariate functions, {{which are usually}} approximated by a sum of lower dimensional functions. A major problem is the approximation error introduced and the factors that affect it. This paper investigates the error of approximating a multivariate function by a sum of lower dimensional functions {{in the setting of}} high dimensional model representations. Two kinds of approximations are studied, namely, the approximation based on the ANOVA (analysis of variance) decomposition and the approximation based on the anchored decomposition. We prove new theorems for the <b>expected</b> <b>errors</b> of approximations based on anchored decomposition when the anchor is chosen randomly and establish the relationship of the <b>expected</b> approximation <b>errors</b> with the global sensitivity indices of Sobol’. The <b>expected</b> approximation <b>error</b> give indications on how good or how bad could be the approximation based on anchored decomposition and when the approximation is good or bad. The influence of the anchor on the goodness of approximation is studied. Methods for choosing good anchors are presented. ...|$|R
40|$|This paper {{presents}} an active learning method that directly optimizes <b>expected</b> future <b>error.</b> This {{is in contrast}} to many other popular techniques that instead aim to reduce version space size. These methods are popular because for many learning models, closed form calculation of the <b>expected</b> future <b>error</b> is intractable. Our approach is made feasible by taking a Monte Carlo approach to estimating the <b>expected</b> reduction in <b>error</b> due to the labeling of a query. In experimental results on three real-world data sets we reach high accuracy with four times fewer labelled examples than competing methods. 1...|$|R
30|$|As <b>expected,</b> <b>errors</b> {{generally}} decrease when predictions {{are made}} later. Of more {{interest is the}} difference among the type of votes, particularly for votes from other fans. Early votes are mainly from submitter’s fans and non-fans, so the ability to predict differences in behavior for those groups based on early votes could be useful in quickly distinguishing stories likely to be of broad or niche interest to the user community.|$|R
5000|$|... {{where the}} {{evaluation}} points [...] are randomly chosen. It {{is well known}} that the <b>expected</b> <b>error</b> of Monte Carlo is of order [...] Thus the cost of the algorithm that has error [...] is of order [...] breaking the curse of dimensionality.|$|E
50|$|These {{questions}} help illuminate {{missing or}} ambiguous requirements. Additional {{details such as}} a due-date {{can be added to}} the expected result. Other acceptance tests can check that conditions such as attempting to check out a book that is already checked out produces the <b>expected</b> <b>error.</b>|$|E
50|$|Ideally, the {{reconstruction}} formula is derived by minimizing the <b>expected</b> <b>error</b> variance. This requires {{that either the}} signal statistics is known or a prior probability for the signal can be specified. Information field theory is then an appropriate mathematical formalism to derive an optimal reconstruction formula.|$|E
40|$|We {{estimate}} the <b>expected</b> <b>errors</b> of nuclear matrix elements {{coming from the}} uncertainty on the NN interaction. We use a coarse grained (GR) interaction fitted to NN scattering data, with several prescriptions for the long-part of the interaction, including one pion exchange and chiral two-pion exchange interactions. Comment: Presented at the 22 th European Conference On Few-Body Problems In Physics: EFB 22 9 - 13 Sep 2013, Krakow (Poland...|$|R
40|$|Generalization {{of large}} margin {{classification}} methods from the binary classification setting {{to the more}} general multicategory setting is often found to be non-trivial. In this paper, we study large margin classification methods that can be seamlessly applied to both settings, with the binary setting simply, as a special case. In particular, we explore the Fisher consistency properties of multicategory majorization losses and present a construction framework of majorization losses of the 0 - 1 loss. Under this framework, we conduct an in-depth analysis about three widely used multicategory hinge losses. Corresponding to the three hinge losses, we propose three multicategory majorization losses based on a coherence function. The limits of the three coherence losses as the temperature approaches zero are the corresponding hinge losses, {{and the limits of}} the minimizers of their <b>expected</b> <b>errors</b> are the minimizers of the <b>expected</b> <b>errors</b> of the corresponding hinge losses. Finally, we develop multicategory large margin classification methods by using a so-called multiclass C-loss. (C) 2014 Elsevier B. V. All rights reserved...|$|R
40|$|The {{problem of}} optimum {{distribution}} of seismic observation points here discussed {{is what will}} be the best location for the new station which is to be set up in addition to the present ones in Yugoslavia. An epicenter was assumed and then the errthquake parameters were calculated using the data of arrival times of the present eight stations which are not without a certain amount of observation errors. If the observation error is normally distributed and has a standard deviation 0. 1 sec the average value of the <b>expected</b> <b>errors</b> for the epicenter location and the origin time are about 3. 1 km and 0. 77 sec respectively. Then a new station and its location was changed uniformly with a constant stepping within the territory of Yugoslavia. If an adequate location is chosen for the additional station, the average values of <b>expected</b> <b>errors</b> all over the country decrease as much as 20 %. In this way the optimum location of the new station was discussed, the calculation being based on the Monte Carlo method. ユーゴスラヴィアには現在 8 個の地震観測所があるが(第 1 表および第 1 図参照),さらに新しく一つの観測所を建設しようとの計画がある. その位置をどこにとつたならもつとも有効であるか,という問題を,以前からとつていると同様のシミュレイションの方法によつて考えてみた...|$|R
50|$|The exact {{solution}} to the unregularized least squares learning problem will minimize the empirical error, but may fail to generalize and minimize the <b>expected</b> <b>error.</b> By limiting , the only free parameter in the algorithm above, the problem is regularized on time which may improve its generalization.|$|E
5000|$|Finding an [...] that generalizes to points {{outside of}} the {{training}} set {{can be done with}} any of the countless algorithms used for supervised learning. It turns out that whichever function [...] we select, we can decompose its <b>expected</b> <b>error</b> on an unseen sample [...] as follows: ...|$|E
5000|$|The {{resulting}} {{data are}} not exact, but a statistical approximation. [...] "The actual amount of error is usually more than one sampling period. In fact, if a value is n times the sampling period, the <b>expected</b> <b>error</b> in it is the square-root of n sampling periods." ...|$|E
40|$|Large margin linear {{classification}} {{methods have}} been successfully applied to many applications. For a linearly separable problem, {{it is known that}} under appropriate assumptions, the <b>expected</b> misclassification <b>error</b> of the computed "optimal hyperplane" approaches zero at a rate proportional to the inverse training sample size. This rate is usually characterized by the margin and the maximum norm of the input data. In this paper, we argue that another quantity, namely the robustness of the input data distribution, also {{plays an important role in}} characterizing the convergence behavior of <b>expected</b> misclassification <b>error.</b> Based on this concept of robustness, we show that for a large margin separable linear classification problem, the <b>expected</b> misclassification <b>error</b> may converge exponentially in the number of training sample size...|$|R
40|$|This paper studies remote state {{estimation}} {{in the presence}} of an eavesdropper. A sensor transmits local state estimates over a packet dropping link to a remote estimator, while an eavesdropper can successfully overhear each sensor transmission with a certain probability. The objective is to determine when the sensor should transmit, in order to minimize the estimation error covariance at the remote estimator, while trying to keep the eavesdropper error covariance above a certain level. This is done by solving an optimization problem that minimizes a linear combination of the <b>expected</b> estimation <b>error</b> covariance and the negative of the <b>expected</b> eavesdropper <b>error</b> covariance. Structural results on the optimal transmission policy are derived, and shown to exhibit thresholding behaviour in the estimation error covariances. In the infinite horizon situation, it is shown that with unstable systems one can keep the <b>expected</b> estimation <b>error</b> covariance bounded while the <b>expected</b> eavesdropper <b>error</b> covariance becomes unbounded. An alternative measure of security, constraining the amount of information revealed to the eavesdropper, is also considered, and similar structural results on the optimal transmission policy are derived. In the infinite horizon situation with unstable systems, it is now shown that for any transmission policy which keeps the <b>expected</b> estimation <b>error</b> covariance bounded, the expected amount of information revealed to the eavesdropper is always lower bounded away from zero. An extension of our results to the transmission of measurements is also presented...|$|R
40|$|International audienceWe {{study the}} {{validation}} of prediction rules such as regression models and classification algorithms through two out-of-sample strategies, cross-validation and accumulated prediction error. We use {{the framework of}} Efron (1983) where measures of prediction errors are defined as sample averages of <b>expected</b> <b>errors</b> and show through exact finite sample calculations that cross-validation and accumulated prediction error yield different smoothing parameter choices in non-parametric regression. The difference in choice does not vanish as sample size increases...|$|R
