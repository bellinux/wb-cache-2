3|10000|Public
40|$|Naive Bayes {{is often}} used in text {{classification}} applications and experiments because of its simplicity and effectiveness. However, its performance is often degraded {{because it does not}} model text well, and by <b>inappropriate</b> <b>feature</b> <b>selection</b> and the lack of reliable confidence scores. We address these problems and show that they can be solved by some simple corrections. We demonstrate that our simple modifications are able to improve the performance of Naive Bayes for text classification significantly...|$|E
40|$|Abstract. A {{new concept}} for pattern {{classification}} systems is proposed {{in which the}} feature selection and the learning classifier are simultaneously carried out on-line. An advantage of this concept is that classification systems can improve their performance constantly even if insufficient training samples are given when the learning starts, often resulting in <b>inappropriate</b> <b>feature</b> <b>selection</b> and poor classifier performance. To implement this concept, we propose an adaptive evolving connectionist model in which Incremental Principal Component Analysis and Evolving Clustering Method are effectively combined. The proposed on-line learning scheme has two major desirable properties. First, the performance is improved as the learning proceeds and it converges to an acceptable level from any initial conditions. Second, the learning is sequentially carried out without retaining all the training data given so far; thus, the learning is conducted efficiently in term of the computation and memory costs. To evaluate the proposed model, the recognition performance is investigated using three standard datasets in the UCI machine learning repository. From the experimental results, we verify that the proposed scheme possesses the above two characteristics...|$|E
40|$|Though naïve Bayes text {{classifiers}} {{are widely}} used because of its simplicity and effectiveness, the techniques for improving performances of these classifiers have been rarely studied. Naïve Bayes classifiers which {{are widely used}} for text classification in machine learning {{are based on the}} conditional probability of features belonging to a class, which the features are selected by feature selection methods. However, its performance is often imperfect because it does not model text well, and by <b>inappropriate</b> <b>feature</b> <b>selection</b> and some disadvantages of the Naive Bayes itself. Sentiment Classification or Text Classification is the act of taking a set of labeled text documents, learning a correlation between a document’s contents and its corresponding labels and then predicting the labels of a set of unlabeled test documents as best as possible. Text Classification is also sometimes called Text Categorization. Text classification has many applications in natural language processing tasks such as E-mail filtering, Intrusion detection systems, news filtering, prediction of user preferences, and organization of documents. The Naive Bayes model makes strong assumptions about the data: it assumes that words in a document are independent. This assumption is clearly violated in natural language text: there are various types of dependences between words induced by the syntactic, semantic, pragmatic and conversational structure of a text. Also, the particular form of the probabilistic model makes assumptions about the distributio...|$|E
40|$|Recently, machine {{learning}} (ML) algorithms have widely been applied in Internet traffic classification. However, {{due to the}} <b>inappropriate</b> <b>features</b> <b>selection,</b> ML-based classifiers are prone to misclassify Internet flows as that traffic occupies majority of traffic flows. To address this problem, a novel <b>feature</b> <b>selection</b> metric named weighted mutual information (WMI) is proposed. We develop a hybrid <b>feature</b> <b>selection</b> algorithm named WMI_ACC, which filters most of the features with WMI metric. It further uses a wrapper method to select features for ML classifiers with accuracy (ACC) metric. We evaluate our approach using five ML classifiers on the two different network environment traces captured. Furthermore, we also apply Wilcoxon pairwise statistical test {{on the results of}} our proposed algorithm to find out the robust features from the selected set of features. Experimental results show that our algorithm gives promising results in terms of classification accuracy, recall, and precision. Our proposed algorithm can achieve 99 % flow accuracy results, which is very promising...|$|R
40|$|Over {{the years}} {{significant}} {{research has been}} performed for machine vision based fabric inspection systems in order to replace manual inspection, which is time consuming and not accurate enough. Automated fabric inspection systems mainly involve two challenging problems: one is defect detection and another is classification, which remains elusive despite considerable research effort in automated fabric inspection. The research reported to date to solve the defect classification problem appears to be insufficient, particularly in selecting appropriate set of features. Scene analysis and <b>feature</b> <b>selection</b> play {{a very important role}} in the classification process. Insufficient scene analysis results in an <b>inappropriate</b> set of <b>features.</b> <b>Selection</b> of an <b>inappropriate</b> <b>feature</b> set increases complexities of subsequent steps and makes the classification task harder. Considering this observation, we present a possibly appropriate feature set in order {{to address the problem of}} fabric defect classification using neural network (NN). We justify the features from the point of view of distinguishing quality and feature extraction difficulty. We performed some experiments in order to show the utility of proposed features and compare performances with recently reported relevant works. More than 98 % classification accuracy has been found, which appears to be very promising...|$|R
40|$|This paper {{proposes a}} {{two-level}} <b>feature</b> <b>selection</b> to improves Naïve Bayes with kernel density estimation. The {{performance of the}} proposed <b>feature</b> <b>selection</b> is evaluated on question item set based on Bloom's cognitive levels. This two-level <b>feature</b> <b>selection</b> contains of filter and wrapper based <b>feature</b> <b>selection.</b> This paper uses chi square and information gain as the filter based <b>feature</b> <b>selection</b> and forward <b>feature</b> <b>selection</b> and backward <b>feature</b> elimination as the wrapper based <b>feature</b> <b>selection.</b> The result shows that the two-level <b>feature</b> <b>selection</b> improves the Naïve Bayes with kernel density estimation. The combination of chi square and backward feature elimination give more optimal quality than the other combination. IEEE The 5 th International Conference on Information Technology and Electrical Engineering (ICITEE), 2013 	 [URL]...|$|R
40|$|We {{study the}} {{effectiveness}} of non-uniform randomized <b>feature</b> <b>selection</b> in decision tree classification. We experimentally evaluate two <b>feature</b> <b>selection</b> methodologies, based on information extracted from the provided dataset: (i) leverage scores-based and (ii) norm-based <b>feature</b> <b>selection.</b> Experimental evaluation of the proposed <b>feature</b> <b>selection</b> techniques indicate that such approaches might be more effective compared to naive uniform <b>feature</b> <b>selection</b> and moreover having comparable performance to the random forest algorithm [3]. ...|$|R
40|$|We {{introduce}} a new method of <b>feature</b> <b>selection</b> for text categorization. Our MMR-based <b>feature</b> <b>selection</b> method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization. Empirical results show that MMR-based <b>feature</b> <b>selection</b> {{is more effective than}} Koller & Sahami’s method, which is one of greedy <b>feature</b> <b>selection</b> methods, and conventional information gain which is commonly used in <b>feature</b> <b>selection</b> for text categorization. Moreover, MMRbased <b>feature</b> <b>selection</b> sometimes produces some improvements of conventional machine learning algorithms over SVM which is known to give the best classification accuracy. ...|$|R
40|$|Data mining {{applications}} addressing classification problems must master two key tasks: <b>feature</b> <b>selection</b> {{and model}} selection. This paper proposes a random <b>feature</b> <b>selection</b> procedure integrated within the multinomial logit (MNL) classifier to perform both tasks simultaneously. We assess {{the potential of}} the random <b>feature</b> <b>selection</b> procedure (exploiting randomness) as compared to an expert <b>feature</b> <b>selection</b> method (exploiting domain-knowledge) on a CRM cross-sell application. The results show great promise as the predictive accuracy of the integrated random <b>feature</b> <b>selection</b> in the MNL algorithm is substantially higher than that of the expert <b>feature</b> <b>selection</b> method. ...|$|R
30|$|Many <b>feature</b> <b>selection</b> {{approaches}} {{have been proposed}} in recent years. According to the availability of class label information, they can be categorized into three classes, including supervised <b>feature</b> <b>selection</b> [6], semi-supervised <b>feature</b> <b>selection</b> [7], and unsupervised <b>feature</b> <b>selection</b> [8, 9]. Supervised-based <b>feature</b> <b>selection</b> approaches search the optimal feature subset with {{the guidance of the}} class label information. However, in many real applications, there are small amount of labeled data or labeling all the data requires quite expensive human labor and computational costs. Therefore, supervised-based <b>feature</b> <b>selection</b> approaches are not feasible in the case of partially labeled data. Under this circumstance, a series of semi-supervised <b>feature</b> <b>selection</b> {{approaches have}} been designed, which take the information of the labeled and unlabeled data into account. Compared with the aforementioned <b>feature</b> <b>selection</b> techniques, unsupervised <b>feature</b> <b>selection</b> approaches determine an optimal feature subset, without any label information, and only depend on maintaining or revealing the intrinsic structures of the original data. Hence, how to incorporate the intrinsic structure information of the data into unsupervised <b>feature</b> <b>selection</b> is very critical.|$|R
40|$|Customer churn is {{the term}} used to {{describe}} customers who terminate their relationship with a company. Since churn means a loss of revenue to a company, {{it is important to}} identify customer churn and provide incentives to them in order to retain them to the company. This paper aims to design methodologies for the customer churn prediction problem in wireless telecommunications industry. Since the number of features in customer churn dataset is rather large, the performance of decision trees is significantly degraded if <b>inappropriate</b> <b>features</b> are selected and used for building decision trees. This paper finds features that have high effect on customer churn, and to design methodologies that will cope with high dimensionality of dataset and the customer churn prediction problem. This paper provides experimental results of a set of data mining schemes and <b>feature</b> <b>selection</b> methods for customer churn datasets. customer churn; classification models; data mining; decision trees; customer retention; customer relationship management; CRM; wireless communications; telecommunications industry...|$|R
40|$|<b>Feature</b> <b>selection</b> is {{a process}} which selects the subset of {{attributes}} from the original dataset by removing the irrelevant and redundant attribute. Clustering is the technique in data mining which group thesimilar object in to one cluster and dissimilar object into other cluster. Some clustering technique does not support high dimensional dataset. By applying the <b>feature</b> <b>selection</b> as a preprocessing step for the clustering {{make it possible to}} handle the high dimensional dataset. <b>Feature</b> <b>selection</b> reduce the computational time greatlydue to reduced feature subset and also improve clustering quality. <b>Feature</b> <b>selection</b> methods are available for supervised and unsupervised learning. This paper is related to working of <b>feature</b> <b>selection</b> method which is applied on different <b>feature</b> <b>selection</b> algorithm. The result proved that <b>Feature</b> <b>selection</b> through featureclustering algorithm is reduced the more attributes than the standard <b>feature</b> <b>selection</b> algorithm like relief andfisher filte...|$|R
40|$|International audienceVariable-weighting {{approaches}} are well-known {{in the context}} of embedded <b>feature</b> <b>selection.</b> Generally, <b>feature</b> <b>selection</b> is performed in a global way, when the algorithm selects a single cluster-independent subset of <b>features</b> (global <b>feature</b> <b>selection).</b> However, other approaches aim to select cluster-specific subsets of <b>features</b> (local <b>feature</b> <b>selection).</b> Global and local <b>feature</b> <b>selection</b> have different objectives, nevertheless, in this paper we propose a novel embedded approach which locally weighs the variables towards a global <b>feature</b> <b>selection.</b> The proposed approach is presented in the semi-supervised paradigm. Experiments on some known data sets are presented to validate our model and compare it with some representative methods...|$|R
40|$|This thesis {{contains}} {{research on}} <b>feature</b> <b>selection,</b> in particular <b>feature</b> <b>selection</b> using evolutionary algorithms. <b>Feature</b> <b>selection</b> {{is motivated by}} increasing data-dimensionality {{and the need to}} construct simple induction models. A literature review of evolutionary <b>feature</b> <b>selection</b> is conducted. After that a abstract <b>feature</b> <b>selection</b> algorithm, capable of using many different wrappers, is constructed. The algorithm is configured using a low-dimensional dataset. Finally it is tested {{on a wide range of}} datasets, revealing both it's abilities and problems. The main contribution is the revelation that classifier accuracy is not a sufficient metric for <b>feature</b> <b>selection</b> on high-dimensional data. </p...|$|R
40|$|Spectral <b>Feature</b> <b>Selection</b> for Data Mining {{introduces}} a novel <b>feature</b> <b>selection</b> technique that establishes a general platform for studying existing <b>feature</b> <b>selection</b> algorithms and developing new algorithms for emerging problems in real-world applications. This technique represents a unified framework for supervised, unsupervised, and semisupervised <b>feature</b> <b>selection.</b> The book explores {{the latest research}} achievements, sheds light on new research directions, and stimulates readers to make the next creative breakthroughs. It presents the intrinsic ideas behind spectral <b>feature</b> <b>selection,</b> its t...|$|R
40|$|This paper {{developed}} a CAD (Computer Aided Diagnosis) {{system based on}} neural network and a proposed <b>feature</b> <b>selection</b> method. The proposed <b>feature</b> <b>selection</b> method is Maximum Difference <b>Feature</b> <b>Selection</b> (MDFS). Digital mammography is reliable method for early detection of breast cancer. The most important step in breast cancer diagnosis is <b>feature</b> <b>selection.</b> Computer automated <b>feature</b> <b>selection</b> is reliable and also it helps to improve the classification accuracy. GLCM (Gray Level Co-occurrence Matrix) features are extracted from the mammogram. The extracted features are selected based on a proposed MDFS method. Experiments have been conducted on datasets from DDSM (Digital database for Screening Mammography) database. Several <b>feature</b> <b>selection</b> methods are available. The accuracy of the model depends on the relevant <b>feature</b> <b>selection.</b> The proposed MDFS method selects only essential features and eliminates the irrelevant features. The experiment results show that neural network based model with proposed <b>feature</b> <b>selection</b> method improved the classification accuracy...|$|R
40|$|<b>Feature</b> <b>selection,</b> as a data {{preprocessing}} strategy, {{has been}} proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of <b>feature</b> <b>selection</b> include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities for <b>feature</b> <b>selection</b> research. In this survey, we provide a comprehensive and structured overview of recent advances in <b>feature</b> <b>selection</b> research. Motivated by current challenges and opportunities in the big data age, we revisit <b>feature</b> <b>selection</b> research from a data perspective, and review representative <b>feature</b> <b>selection</b> algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing <b>feature</b> <b>selection</b> algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present an open-source <b>feature</b> <b>selection</b> repository that consists {{of most of the}} popular <b>feature</b> <b>selection</b> algorithms (...|$|R
40|$|This study {{addresses}} <b>feature</b> <b>selection</b> {{for breast}} cancer diagnosis. The present process uses a wrapper approach using GA-based on <b>feature</b> <b>selection</b> and PS-classifier. The results of experiment show that the proposed model {{is comparable to the}} other models on Wisconsin breast cancer datasets. To evaluate effectiveness of proposed <b>feature</b> <b>selection</b> method, we employed three different classifiers artificial neural network (ANN) and PS-classifier and genetic algorithm based classifier (GA-classifier) on Wisconsin breast cancer datasets include Wisconsin breast cancer dataset (WBC), Wisconsin diagnosis breast cancer (WDBC), and Wisconsin prognosis breast cancer (WPBC). For WBC dataset, it is observed that <b>feature</b> <b>selection</b> improved the accuracy of all classifiers expect of ANN and the best accuracy with <b>feature</b> <b>selection</b> achieved by PS-classifier. For WDBC and WPBC, results show <b>feature</b> <b>selection</b> improved accuracy of all three classifiers and the best accuracy with <b>feature</b> <b>selection</b> achieved by ANN. Also specificity and sensitivity improved after <b>feature</b> <b>selection.</b> The results show that <b>feature</b> <b>selection</b> can improve accuracy, specificity and sensitivity of classifiers. Result of this study is comparable with the other studies on Wisconsin breast cancer dataset...|$|R
30|$|Ensemble <b>feature</b> <b>selection</b> (EnFS) [25]: a {{supervised}} <b>feature</b> <b>selection</b> method {{based on}} transfer learning, which transfer the shared information between different classifiers {{by adding a}} joint ℓ 2, 1 -norm on multiple <b>feature</b> <b>selection</b> matrices.|$|R
30|$|To {{handle this}} challenge, <b>feature</b> <b>selection</b> [4 – 8] and {{subspace}} learning [9, 10] {{have been developed}} to obtain suitable <b>feature</b> representations. <b>Feature</b> <b>selection</b> is commonly used as a preprocessing step for classification, so most <b>feature</b> <b>selection</b> algorithms are only designed for better predictability, such as high prediction accuracy. Although many <b>feature</b> <b>selections</b> have taken both feature relevance and redundancy into account simultaneously for predictability [11], they neglect stability [12]. If a <b>feature</b> <b>selection</b> method has poor stability, the selected feature subsets change significantly due to the variation of training data. Therefore, using only predictability to evaluate <b>feature</b> <b>selection</b> methods may result in inconsistent results of ranking for data representation.|$|R
40|$|We present {{applications}} of rough set methods for <b>feature</b> <b>selection</b> in pattern recognition. We emphasize {{the role of}} the basic constructs of rough set approach in <b>feature</b> <b>selection,</b> namely reducts and their approximations, including dynamic reducts. In the overview of methods for <b>feature</b> <b>selection</b> we discuss <b>feature</b> <b>selection</b> criteria, including the rough set based methods. Our algorithm for <b>feature</b> <b>selection</b> is based on an application of a rough set method to the result of principal components analysis (PCA) used for feature projection and reduction. Finally, the paper presents numerical results of face and mammogram recognition experiments using neural network, with <b>feature</b> <b>selection</b> based on proposed PCA and rough set methods...|$|R
40|$|Sparse {{learning}} {{has been proven}} to be a powerful tech-nique in supervised <b>feature</b> <b>selection,</b> which allows to embed <b>feature</b> <b>selection</b> into the classification (or re-gression) problem. In recent years, increasing attention has been on applying spare learning in unsupervised <b>feature</b> <b>selection.</b> Due to the lack of label information, the vast majority of these algorithms usually generate cluster labels via clustering algorithms and then formu-late unsupervised <b>feature</b> <b>selection</b> as sparse learning based supervised <b>feature</b> <b>selection</b> with these generated cluster labels. In this paper, we propose a novel unsuper-vised <b>feature</b> <b>selection</b> algorithm EUFS, which directly embeds <b>feature</b> <b>selection</b> into a clustering algorithm via sparse learning without the transformation. The Alter-nating Direction Method of Multipliers is used to ad-dress the optimization problem of EUFS. Experimental results on various benchmark datasets demonstrate the effectiveness of the proposed framework EUFS...|$|R
40|$|Many {{learning}} {{applications are}} characterized by high dimensions. Usually {{not all of these}} dimensions are relevant and some are redundant. There are two main approaches to reduce dimensionality: <b>feature</b> <b>selection</b> and <b>feature</b> transformation. When one wishes to keep the original meaning of the <b>features,</b> <b>feature</b> <b>selection</b> is desired. <b>Feature</b> <b>selection</b> and transformation are typically presented separately. In this paper, we introduce a general approach for converting transformationbased methods to <b>feature</b> <b>selection</b> methods through ℓ 1 /ℓ ∞ regularization. Instead of solving <b>feature</b> <b>selection</b> as a discrete optimization, we relax and formulate the problem as a continuous optimization problem. An additional advantage of our formulation is that our optimization criterion optimizes for feature relevance and redundancy removal automatically. Here, we illustrate how our approach can be utilized to convert linear discriminant analysis (LDA) and the dimensionality reduction version of the Hilbert-Schmidt Independence Criterion (HSIC) to two new <b>feature</b> <b>selection</b> algorithms. Experiments show that our new <b>feature</b> <b>selection</b> methods out-perform related state-of-the-art <b>feature</b> <b>selection</b> approaches...|$|R
40|$|Stability (robustness) of <b>feature</b> <b>selection</b> methods is a {{topic of}} recent {{interest}} yet often neglected importance with direct impact on the reliability of machine learning systems. We investigate the problem of evaluating the stability of <b>feature</b> <b>selection</b> processes yielding subsets of varying size. We introduce several novel <b>feature</b> <b>selection</b> stability measures and adjust some existing measures in a unifying framework that offers broad insight into the stability problem. We study in detail the properties of considered measures and demonstrate on various examples what information about the <b>feature</b> <b>selection</b> process can be gained. We also introduce an alternative approach to <b>feature</b> <b>selection</b> evaluation in form of measures that enable comparing the similarity of two <b>feature</b> <b>selection</b> processes. These measures enable comparing, e. g., the output of two <b>feature</b> <b>selection</b> methods or two runs of one method with different parameters. The information obtained using the considered stability and similarity measures is shown usable for assessing <b>feature</b> <b>selection</b> methods (or criteria) as suc...|$|R
40|$|<b>Feature</b> subset <b>selection</b> can be {{analyzed}} as the practice of identifying and removing {{as a lot of}} <b>inappropriate</b> and unnecessary <b>features</b> as achievable. This is for the reason that, irrelevant features do not contribute to the predictive accuracy and redundant features do not redound to receiving a better analyst for that they provide typically information which is previously present in other features. To deal with these we develop a novel algorithm called as a fast clustering-based <b>feature</b> <b>selection</b> algorithm which can efficiently and effectively obtain a good feature subset. One more an important issue in <b>feature</b> subset <b>selection</b> is <b>Feature</b> interaction. Still, the majority of the presented algorithms only spotlight on dealing with irrelevant and redundant features. In this paper, a hybrid FOIL Rule based <b>Feature</b> subset <b>Selection</b> algorithm (FRFS) is developed, with combination of RIPPER algorithm which not only retains relevant features and eliminates irrelevant and redundant ones but also reflects on feature interaction for high dimensional data. First, FRRFS(FOIL Rules RIPPER Based <b>Feature</b> <b>Selection)</b> combines the <b>features</b> emerged in the predecessors of the entire FOIL rules by using RIPPER it generates best FOIL rules for both negative and positive rules by achieving a candidate feature subset which eliminates redundant features and reserves interactive ones. After that, it identifies and eliminates irrelevant features by evaluating features in the candidate feature subset with a new metric Cover Ratio, and achieves the final feature subset. The experimented results shows that the efficiency and effectiveness of FRRFS(FOIL Rules RIPPER Based <b>Feature</b> <b>Selection)</b> upon both synthetic and real world data sets, and it is evaluated with other <b>feature</b> subset <b>selection</b> algorithms and the classification accuracies before and after <b>feature</b> <b>selection...</b>|$|R
40|$|Due to {{the absence}} of class labels, {{unsupervised}} <b>feature</b> <b>selection</b> is much more difficult than supervised <b>feature</b> <b>selection.</b> Traditional unsupervised <b>feature</b> <b>selection</b> algorithms usually select features to preserve the structure of the data set. Inspired from the recent developments on discriminative clustering, we propose in this paper a novel unsupervised <b>feature</b> <b>selection</b> approach via Joint Clustering and <b>Feature</b> <b>Selection</b> (JCFS). Specifically, we integrate Fisher score into the clustering framework. We select those features such that the fisher criterion is maximized and the manifold structure can be best preserved simultaneously. We also discover the connection between JCFS and other clustering and <b>feature</b> <b>selection</b> methods, such as discriminative K-means, JELSR and DCS. Experimental results on real world data sets demonstrated the effectiveness of the proposed algorithm. © 2013 Springer-Verlag Berlin Heidelberg. Due {{to the absence}} of class labels, unsupervised <b>feature</b> <b>selection</b> is much more difficult than supervised <b>feature</b> <b>selection.</b> Traditional unsupervised <b>feature</b> <b>selection</b> algorithms usually select features to preserve the structure of the data set. Inspired from the recent developments on discriminative clustering, we propose in this paper a novel unsupervised <b>feature</b> <b>selection</b> approach via Joint Clustering and <b>Feature</b> <b>Selection</b> (JCFS). Specifically, we integrate Fisher score into the clustering framework. We select those features such that the fisher criterion is maximized and the manifold structure can be best preserved simultaneously. We also discover the connection between JCFS and other clustering and <b>feature</b> <b>selection</b> methods, such as discriminative K-means, JELSR and DCS. Experimental results on real world data sets demonstrated the effectiveness of the proposed algorithm. © 2013 Springer-Verlag Berlin Heidelberg...|$|R
30|$|<b>Feature</b> <b>selection</b> via joint ℓ 2, 1 -norms {{minimization}} (FSNM) [22]: a supervised <b>feature</b> <b>selection</b> method {{built by}} employing joint ℓ 2, 1 -norms minimization on both loss function and regularization to realize <b>feature</b> <b>selection</b> across all data points.|$|R
30|$|Embedded <b>feature</b> <b>selection</b> {{methods that}} {{consider}} <b>feature</b> <b>selection</b> as an internal process of a forecasting methodology.|$|R
30|$|<b>Feature</b> <b>selection</b> and {{extraction}} {{methods can}} be conventionally divided into five classes (exogenous and endogenous feature filtering, wrapper <b>feature</b> <b>selection</b> and embedded <b>feature</b> <b>selection</b> methods and dimension reduction methods). We analysed {{the dynamics of}} method applications from different classes {{in the field of}} spatiotemporal traffic forecasting and concluded that the general trend has recently shifted from exogenous feature filtering to a variety of data-driven <b>feature</b> <b>selection</b> methods.|$|R
40|$|Robustness of <b>feature</b> <b>selection</b> {{techniques}} is a {{topic of}} recent interest, especially in high dimensional domains with small sam-ple sizes, where selected feature subsets are subsequently analysed by domain experts to gain more insight into the problem modelled. In this work, we investigate the robustness of various <b>feature</b> <b>selection</b> techniques, and provide a general scheme to improve robust-ness using ensemble <b>feature</b> <b>selection.</b> We show that ensemble <b>feature</b> <b>selection</b> tech-niques show great promise for small sample domains, and provide more robust feature subsets than a single <b>feature</b> <b>selection</b> tech-nique. In addition, we also investigate the ef-fect of ensemble <b>feature</b> <b>selection</b> techniques on classification performance, giving rise to a new model selection strategy. 1...|$|R
40|$|An {{important}} step {{in the analysis of}} high-dimensional biomedical data is <b>feature</b> <b>selection.</b> Typically, a <b>feature</b> subset selected by a <b>feature</b> <b>selection</b> method is evaluated for relevance towards a task such as prediction or classification. Another important property of a <b>feature</b> <b>selection</b> method is stability that refers to robustness of the selected features to perturbations in the data. In biomarker discovery, for example, domain experts prefer a parsimonious subset of features that are relatively robust to slight changes in the data. We present a stability measure called the adjusted stability measure that computes robustness of a <b>feature</b> <b>selection</b> method with respect to random <b>feature</b> <b>selection.</b> This measure is useful for comparing the robustness of <b>feature</b> <b>selection</b> methods and is superior to similar measures that do not account for random <b>feature</b> <b>selection.</b> We demonstrate the application of this measure on a biomedical dataset...|$|R
40|$|Objective(s) : This study {{addresses}} <b>feature</b> <b>selection</b> {{for breast}} cancer diagnosis. The present process uses a wrapper approach using GA-based on <b>feature</b> <b>selection</b> and PS-classifier. The results of experiment show that the proposed model {{is comparable to the}} other models on Wisconsin breast cancer datasets. Materials and Methods: To evaluate effectiveness of proposed <b>feature</b> <b>selection</b> method, we employed three different classifiers artificial neural network (ANN) and PS-classifier and genetic algorithm based classifier (GA-classifier) on Wisconsin breast cancer datasets include Wisconsin breast cancer dataset (WBC), Wisconsin diagnosis breast cancer (WDBC), and Wisconsin prognosis breast cancer (WPBC). Results: For WBC dataset, it is observed that <b>feature</b> <b>selection</b> improved the accuracy of all classifiers expect of ANN and the best accuracy with <b>feature</b> <b>selection</b> achieved by PS-classifier. For WDBC and WPBC, results show <b>feature</b> <b>selection</b> improved accuracy of all three classifiers and the best accuracy with <b>feature</b> <b>selection</b> achieved by ANN. Also specificity and sensitivity improved after <b>feature</b> <b>selection.</b> Conclusion: The results show that <b>feature</b> <b>selection</b> can improve accuracy, specificity and sensitivity of classifiers. Result of this study is comparable with the other studies on Wisconsin breast cancer datasets...|$|R
50|$|<b>Feature</b> <b>selection</b> {{with social}} media data - Transforming <b>feature</b> <b>selection</b> {{to harness the}} power of social media.|$|R
40|$|<b>Feature</b> <b>selection</b> is an {{important}} task in effective data mining. A new challenge to <b>feature</b> <b>selection</b> is the so-called “small labeled-sample problem ” in which labeled data is small and unlabeled data is large. The paucity of labeled instances provides insufficient information about {{the structure of the}} target concept, and can cause supervised <b>feature</b> <b>selection</b> algorithms to fail. Unsupervised <b>feature</b> <b>selection</b> algorithms can work without labeled data. However, these algorithms ignore label information, which may lead to downgraded performance. In this work, we propose to use both (small) labeled and (large) unlabeled data in <b>feature</b> <b>selection,</b> which is a topic has not yet been addressed in <b>feature</b> <b>selection</b> research. We present a semi-supervised <b>feature</b> <b>selection</b> algorithm based on spectral analysis. The algorithm exploits both labeled and unlabeled data through a regularization framework, which provides an effective way to address the “small labeled-sample ” problem. Experimental results demonstrated the efficacy of our approach and confirmed that using labeled and unlabeled data together does help <b>feature</b> <b>selection</b> with small labeled samples...|$|R
40|$|We are {{surrounded}} by huge amounts of large-scale high dimensional data. It is desirable to reduce the dimensionality of data for many learning tasks due to the curse of dimensionality. <b>Feature</b> <b>selection</b> has shown its effectiveness in many applications by building simpler and more comprehensive model, improving learning performance, and preparing clean, understandable data. Recently, some unique characteristics of big data such as data velocity and data variety present challenges to the <b>feature</b> <b>selection</b> problem. In this paper, we envision these challenges of <b>feature</b> <b>selection</b> for big data analytics. In particular, we first give a brief introduction about <b>feature</b> <b>selection</b> and then detail the challenges of <b>feature</b> <b>selection</b> for structured, heterogeneous and streaming data {{as well as its}} scalability and stability issues. At last, to facilitate and promote the <b>feature</b> <b>selection</b> research, we present an open-source <b>feature</b> <b>selection</b> repository (scikit-feature), which consists of most of current popular <b>feature</b> <b>selection</b> algorithms. Comment: Special Issue on Big Data, IEEE Intelligent Systems, 2016. arXiv admin note: text overlap with arXiv: 1601. 0799...|$|R
40|$|Text Categorization (classification) is {{the process}} of {{classifying}} documents into a predefined set of categories based on their content. Text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge number of <b>features.</b> <b>Feature</b> <b>selection</b> tries to find a set of relevant terms to improve both efficiency and generalization. There are two main approaches for <b>feature</b> <b>selection,</b> local and global. In Arabic text categorization it was found that using global <b>feature</b> <b>selection</b> gives higher results but may affect some documents in a way so that they do not show any terms in the set of selected features. On the other hand local <b>feature</b> <b>selection</b> is used to overcome this problem but gives lower classification rate. In this paper a hybrid approach of global and local <b>feature</b> <b>selection</b> technique is proposed and compared with both local and global <b>feature</b> <b>selection</b> techniques. Results are reported on a set of 1132 document of six different topics showing that the proposed hybrid <b>feature</b> <b>selection</b> overcome the disadvantages of both of <b>feature</b> <b>selection</b> approaches...|$|R
