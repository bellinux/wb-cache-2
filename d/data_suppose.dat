34|257|Public
50|$|The Bus-Invert {{encoding}} technique uses {{an extra}} signal (INV) {{to indicate the}} “polarity” of the <b>data.</b> <b>Suppose</b> we have an bus-invert code word INV@x where @ is the concatenation operator, and x denotes either the source word or its ones' complement. The bus-invert decoder takes the code word and produces the corresponding source word. If the INV signal is 1, the result is one’s complement of x, otherwise it is x.|$|E
5000|$|One primary {{motivation}} {{for the use of}} deterministic encryption is the efficient searching of encrypted <b>data.</b> <b>Suppose</b> a client wants to outsource a database to a possibly untrusted database service provider. If each entry is encrypted using a public-key cryptosystem, anyone can add to the database, and only the distinguished [...] "receiver" [...] who has the private key can decrypt the database entries. If, however, the receiver wants to search for a specific record in the database, this becomes very difficult. There are some Public Key encryption schemes that allow keyword search, however these schemes all require search time linear in the database size. If the database entries were encrypted with a deterministic scheme and sorted, then a specific field of the database could be retrieved in logarithmic time.|$|E
3000|$|In the VCA method, by {{solving the}} {{generator}} of vanishing ideal (i.e., Grobner basis), it is feasible {{to obtain the}} generation features of a manifold pattern, so that an input space can be switched into a feature space. In a feature space, {{it is easier to}} judge the class of <b>data.</b> <b>Suppose</b> an input space is S[*]⊆[*]R [...]...|$|E
3000|$|Routing control {{overhead}} {{expresses the}} ratio of the total generated routing control messages to the total number of <b>data</b> messages <b>supposed</b> to be received.|$|R
40|$|A set of <b>data</b> <b>supposed</b> to give {{possible}} axioms for spacetimes with {{a sufficient}} number of isometries in spectral geometry is given. These data are shown to be sufficient to obtain 1 + 1 dimensional de Sitter spacetime. The data rely at the moment somewhat on the guidance given by a required symmetry, in part to allow explicit calculations in a specific model. The framework applies also to the noncommutative case. Finite spectral triples are discussed as an example. Comment: AMS-LaTeX (32 pages...|$|R
40|$|A set of <b>data</b> <b>supposed</b> to give {{possible}} axioms for spacetimes. It {{is hoped}} that such a proposal can serve to become a testing ground {{on the way to}} a general formulation. At the moment, the axioms are known to be sufficient for cases with a sufficient number of symmetries, in particular for 1 + 1 de Sitter spacetime. Comment: AMS-LaTeX, 7 pages, presented at the Euroconference "BRANE NEW WORLD and Noncommutative Geometry", Torino, Villa Gualino (Italy), October 2 - 7, 200...|$|R
40|$|This report {{describes}} {{our experiences}} with disaggregating time series <b>data.</b> <b>Suppose</b> we have gathered data every two seconds {{and want to}} guess the data at one-second intervals. Under certain assumptions, there are several reasonable disaggregation methods {{as well as several}} performance measures to judge their performance. Here we present results for both simulated and real data for two methods using several performance criteria...|$|E
40|$|The {{reliability}} of the findings derived from a hedonic regression analysis depends on — among other things — {{the quality of the}} data source. Sometimes, scanner data provide such <b>data.</b> <b>Suppose,</b> for example, that T periods are analysed and that the relationship between the products ’ prices and qualitative characteristics in each time period t is given by ln pti =...|$|E
40|$|The paper {{includes}} data on {{species composition}} of chironomid larvae which were {{encountered in the}} mantle cavity of zebra mussels (Dreissena polymorpha) within 7 waterbodies in the Republic of Belarus. All {{were found to be}} free-living species commonly present in periphyton and/or benthos. A long-term study of the seasonal dynamics of these larvae in Dreissena did not reveal any typical pattern. Our <b>data</b> <b>suppose</b> that chironomids do not have an obligate association with zebra mussels and possibly enter their mantle cavity inadvertently...|$|E
40|$|Abstract. A set of <b>data</b> <b>supposed</b> to give {{possible}} axioms for spacetimes with {{a sufficient}} number of isometries in spectral geometry is given. These data are shown to be sufficient to obtain 1 + 1 dimensional de Sitter spacetime. The data rely at the moment somewhat on the guidance given by a required symmetry, in part to allow explicit calculations in a specific model. The framework applies also to the noncommutative case. Finite spectral triples are discussed as an example. 1...|$|R
3000|$|Average packet {{delivery}} ratio (PDR) represents the average {{ratio of the}} number of successfully received data packets at the destination node to the number of <b>data</b> packets <b>supposed</b> to be delivered.|$|R
40|$|International audienceAfter a quick {{overview}} {{of the field of}} study known as “Lexical Semantics”, where we advocate the need of accessing additional information besides syntax and Montague- style semantics at the lexical level in order to complete the full analysis of an utterance, we summarize the current formulations of a well-known theory of that field. We then propose and justify our own model of the Generative Lexicon Theory, based upon a variation of classical compositional semantics, and outline its formalization. Additionally, we discuss the theoretical place of informational, knowledge-related <b>data</b> <b>supposed</b> to exist within the lexicon as well as within discourse and other linguistic constructs...|$|R
30|$|CA {{will evolve}} in a {{sequence}} of discrete time steps. The real time of each step can be calibrated by historical <b>data.</b> <b>Suppose</b> that the periods of input and output data for CA training be T 1 and T 2, respectively. The interval “N” between two consecutive time steps can be calculated: N[*]=[*]T 2 -T 1. Thus, the sequent time of each step can be calculated: T 3 [*]=[*]T 2 [*]+[*]N, T 4 [*]=[*]T 3 [*]+[*]N, T 5 [*]=[*]T 4 [*]+[*]N….After the structure, parameter, and transition rules are defined, the CA model will evolve dynamically.|$|E
40|$|We {{consider}} the nonparametric {{estimation of the}} regression functions for dependent <b>data.</b> <b>Suppose</b> that the covariates are observed with additive error sin the data and we employ nonparametric deconvolution kernel techniques to estimate the regression functions in this paper. We investigate how the strength of time dependence affects the asymptotic properties of the local constant and linear estimators. We treat both short-range dependent and long-range dependent linear processes in a unified way and demonstrate that the long-range dependence (LRD) of the covariates affects the asymptotic properties of the nonparametric estimators {{as well as the}} LRD of regression errors does. グローバルCOEプログラム = Global COE Progra...|$|E
3000|$|Let [...] X [...] is {{the quality}} of interest, [...] x^L and [...] x^U are the minimum value and maximum value of the <b>data.</b> <b>Suppose</b> that [...] a = [x^L,x^U] [...] be the {{required}} range for the sample data. The neutrosophic interval probability (NIP) based on [...] x^L and [...] x^U is [...] p = 〈[x^L,x^U], (p_ND,p_I,p_D [...]) 〉. Note that [...] p_ND, [...] p_I and [...] p_D denote non-defective probability belong to determine part, indeterminacy probability belong to an intermediate-defective part and falsity-probability belong to failure range of interval [...] p < [x^L,x^U], (p_ND,p_I,p_D [...]), respectively. The total probability for the three cases satisfies [...] p_ND + p_I + p_D> 1.|$|E
40|$|Acoustic Doppler flow-meters are {{commonly}} used for continuously measuring flow-rates in sewers. They rely on velocity sensors which provide <b>data</b> <b>supposed</b> to be representative, but not equal, to the mean velocity of the flow. The relationship between a measured value and the actual mean velocity of the flow depends on the volume sampled by the velocity sensor, and on the hydrodynamic features of the measuring site. This paper focuses on the first item, because parameters of sensors are often little detailed in manufacturers' specification sheets. The principles of testing procedures are outlined, and their implementation in an experimental benchmark is presented, along with {{the results obtained on}} a panel of flow-meters...|$|R
40|$|This {{thesis is}} focused on testing the {{so-called}} unique items hypothesis on Czech language <b>data.</b> <b>Supposed</b> Czech unique items were chosen from lexical units, word-formation phenomena, syntactic structures and language use phenomena. Their frequency in a comparable monolingual corpus of contemporary Czech was established and the differences in frequency were statistically tested. This quantitative research {{was accompanied by a}} qualitative probe into the English source texts from which sentences containing selected unique items were translated using an aligned parallel corpus of English-Czech translations. The results reveal a general tendency of unique items to be underrepresented in translated language and a variety of source- language phenomena that underlie unique items usage in the target language...|$|R
5000|$|<b>Suppose</b> <b>data</b> set [...] {{contains}} n independent {{statistical units}} {{corresponding to the}} model above. Then their joint log-likelihood function is ...|$|R
40|$|November 2009 (Revised: February 2010) We {{consider}} the nonparametric {{estimation of the}} regression functions for dependent <b>data.</b> <b>Suppose</b> that the covariates are observed with additive errors in the data and we employ nonparametric deconvolution kernel techniques to estimate the regression functions in this paper. We investigate how the strength of time dependence affects the asymptotic properties of the local constant and linear estimators. We treat both short-range dependent and long-range dependent linear processes in a unified way and demonstrate that the long-range dependence (LRD) of the covariates affects the asymptotic properties of the nonparametric estimators {{as well as the}} LRD of regression errors does. グローバルCOEプログラム = Global COE ProgramRevised in February 201...|$|E
40|$|A {{method is}} {{presented}} for using connectionist networks of simple computing elements {{to discover a}} particular type of constraint in multidimensional <b>data.</b> <b>Suppose</b> that some data source provides samples consisting of n-dimensional feature-vectors, but that this data all happens to lie on an m-dimensional surface embedded in the n-dimensional feature space. Then occurrences of data can be more concisely described by specifying an m-dimensional location on the embedded surface than by reciting all n components of the feature vector. The recoding of data in such a way is known as dimensionality-reduction. This paper describes a method for performing dimensionality-reduction in a wide class of situations for which an assumption of linearity need not be made about the underlying constraint surface...|$|E
40|$|This thesis {{discusses}} some {{existence and}} uniqueness problems of harmonic maps. It {{consists of two}} parts: Part I. Existence of harmonic maps with prescribed finite singularities. Here we {{address the question of}} existence of a harmonic map from a spatial domain to the sphere S 2 which has a prescribed finite set of singularities. Part II. Uniqueness of energy minimizing harmonic maps for almost all smooth boundary <b>data.</b> <b>Suppose</b> Ω is a smooth domain in Rm and N is a compact smooth manifold. Here we show roughly that almost all smooth maps from ∂Ω to N serve as boundary values for a unique energy minimizing map u from Ω to N. This involves constructing a finite measure on a suitable (infinite dimensional) space of smooth boundary values...|$|E
40|$|In this paper, we {{estimate}} the distribution function and {{the expectation of}} an increasing process. The <b>data</b> are <b>supposed</b> to be censored. The novel idea is to extend the Kaplan-Meier approach to such processes, by a randomization method. A martingale application is also provided. Kaplan-Meier estimator Fuzzy times...|$|R
5000|$|In the Star Trek: The Next Generation episode [...] "The Measure of a Man (Star Trek: The Next Generation)", which {{aired on}} 13 February 1989, Lieutenant Commander Data, an android, has his sentience and, consequently, his rights as an individual, challenged. Throughout {{the run of}} this series, Data is {{identified}} as a positronic android, and Asimov is even mentioned by name in the episode [...] "Datalore". While Data routinely references his [...] "ethical subroutines", indicating some sort of moral guide, {{it is unclear whether}} Data is explicitly bound by the Three Laws. For example, the episode [...] "Clues" [...] explored Data's capacity to lie to the crew in order to protect them from aliens, and the episode [...] "The Most Toys" [...] explored <b>Data's</b> <b>supposed</b> inability to murder in cold blood.|$|R
5000|$|It {{is desired}} for [...] to supply [...] with some <b>data,</b> let us <b>suppose</b> some [...] <b>data.</b> A compile time {{solution}} is: ...|$|R
40|$|We {{propose a}} new {{algorithm}} {{to detect the}} community structure in a network that utilizes both the network structure and vertex attribute <b>data.</b> <b>Suppose</b> we have the network structure together with the vertex attribute data, that is, the information assigned to each vertex associated with the community to which it belongs. The problem addressed this paper is the detection of the community structure from the information of both the network structure and the vertex attribute data. Our approach {{is based on the}} Bayesian approach that models the posterior probability distribution of the community labels. The detection of the community structure in our method is achieved by using belief propagation and an EM algorithm. We numerically verified the performance of our method using computer-generated networks and real-world networks. Comment: 23 pages, 9 figure...|$|E
40|$|Most {{daily and}} {{scientific}} data are sequential in nature. Discovering important patterns from such data can benefit the user and scientist by predicting coming activities, interpreting recurring phenomena, extracting outstanding {{similarities and differences}} for close attention, compressing data, and detecting intrusion. We consider the following incremental discovery problem for large and dynamic sequential <b>data.</b> <b>Suppose</b> that patterns were previously discovered and materialized. An update is made to the sequential database. An incremental discovery {{will take advantage of}} discovered patterns and compute only the change by accessing the affected part of the database and data structures. In addition to patterns, the statistics and position information of patterns need to be updated to allow further analysis and processing on patterns. We present an efficient algorithm for the incremental discovery problem. The algorithm is applied to sequential data that honors several sequential patterns [...] ...|$|E
40|$|The main {{objective}} of this chapter is to discuss different approaches to searching for optimal approximation spaces. Basic notions concerning rough set concept based on generalized approximation spaces are presented. Different constructions of approximation spaces are described. The problems of attribute and object selection are discussed. 1 Introduction Rough set theory was proposed [21, 22] as {{a new approach to}} processing of incomplete <b>data.</b> <b>Suppose</b> we are given the finite non-empty set U of objects, called the universe. Each object of U is characterized by a description, for example a set of attribute values. In standard rough sets [21, 22] introduced by Pawlak an equivalence relation (reflexive, symmetric and transitive relation) on the universe of objects is defined based on the attribute values. In particular, this equivalence relation is constructed based on the equality relation on attribute values. Many attempts were made to resolve limitations of this approach and many [...] ...|$|E
40|$|A {{simple beam}} {{subjected}} to a row of regularly distributed moving forces and simultaneous vertical motions of its supports is described using a simplied theoretical model and a nite dierences approach. Several levels of simplication of the structure and input <b>data</b> are <b>supposed.</b> Numerical results conrm legitimacy of the assumptions...|$|R
40|$|The {{problem of}} {{parallel}} distributed computation of some commonly found statistical functions is considered. <b>Data</b> are <b>supposed</b> {{to belong to}} a horizontally partitioned and possibly duplicated Distributed Database. Results for Homogeneously and Nonhomogeneously Decomposable Functions are shown applied to the case of the most common built-in functions in Information Systems...|$|R
40|$|AbstractWe {{prove the}} global {{existence}} theorem to the Smoluchovsky coagulation equation with space inhomogeneous velocity fields. Coagulation kernels are unbounded with multiplicative growth on infinity and include almost all physically reasonable cases. Initial <b>data</b> are <b>supposed</b> to be sufficiently small. The a priori estimate obtained allows {{to prove the}} uniqueness of solution as well...|$|R
40|$|A {{new method}} is {{presented}} to get insight into univariate time series data. The problem addressed here is how to identify patterns and trends on multiple time scales (days, weeks, seasons) simultaneously. The solution presented is to cluster similar daily data patterns, and to visualize the average patterns as graphs and the corresponding days on a calendar. This presentation provides a quick insight into both standard and exceptional patterns. Furthermore, it is well suited to interactive exploration. Two applications, numbers of employees present and energy consumption, are presented. 1 Introduction Time series data are ubiquitous. The aim of time series analysis is to obtain insight into phenomena, to discover repetitive patterns and trends, and to predict the future. We focus here on the analysis of univariate time series <b>data.</b> <b>Suppose,</b> we have collected energy consumption or air pollution data at short time intervals during one year, then how can we extract information from these [...] ...|$|E
40|$|We’ll {{cover the}} chi-squared (χ 2) test for {{categorical}} data (goodness-of-fit test) and extend it {{to examine whether}} two categorical variables are related (contingency test). By the way, chi is pronounced kai, not chai. Related supplementary material is presented {{for those who are}} interested. Stuff with a solid edge, like this, is important. But remember — you can totally ignore stuff with single/double wavy borders. 5. 1 The chi-squared (χ 2) test One categorical variable, two categories The χ 2 test, sometimes called Pearson’s χ 2 test, is all about analysing categorical <b>data.</b> <b>Suppose</b> we ask 100 people to choose between chocolate and garibaldi biscuits (so every person falls into one of two categories); 65 choose chocolate and 35 choose garibaldi. Does this differ from chance, i. e. a 50 : 50 split? The expected values based on the null hypothesis are 50 chocolate and 50 garibaldi. The observed values are 65 and 35. From this, we can calculate the χ 2 statistic...|$|E
40|$|Let u(x,t) be the {{possibly}} discontinuous entropy {{solution of}} a nonlinear scalar conservation law with smooth initial <b>data.</b> <b>Suppose</b> u sub epsilon(x,t) {{is the solution}} of an approximate viscosity regularization, where epsilon greater than 0 is the small viscosity amplitude. It is shown that by post-processing the small viscosity approximation u sub epsilon, pointwise values of u and its derivatives can be recovered with an error as close to epsilon as desired. The analysis relies on the adjoint problem of the forward error equation, {{which in this case}} amounts to a backward linear transport with discontinuous coefficients. The novelty of this approach is to use a (generalized) E-condition of the forward problem in order to deduce a W(exp 1,infinity) energy estimate for the discontinuous backward transport equation; this, in turn, leads one to an epsilon-uniform estimate on moments of the error u(sub epsilon) - u. This approach does not follow the characteristics and, therefore, applies mutatis mutandis to other approximate solutions such as E-difference schemes...|$|E
40|$|We {{consider}} a wave equation with an internal damping {{represented by a}} fractional derivative of lower order than one. An exponential growth result is proved in presence of a source of polynomial type. This result improves an earlier one where the initial <b>data</b> are <b>supposed</b> to be very large in some norm. A new argument based on a new functional is proposed...|$|R
5000|$|In a {{synchronous}} digital system, <b>data</b> is <b>supposed</b> {{to move in}} lockstep, advancing {{one stage}} on each tick of the clock signal. This is enforced by synchronizing elements such as flip-flops or latches, which copy their input to their output when instructed {{to do so by}} the clock. Only two kinds of timing errors are possible in such a system: ...|$|R
40|$|Major {{search engines}} deploy {{personalized}} Web results to enhance users 2 ̆ 7 experience, by showing them <b>data</b> <b>supposed</b> {{to be relevant}} to their interests. Even if this process may bring benefits to users while browsing, it also raises concerns on {{the selection of the}} search results. In particular, users may be unknowingly trapped by search engines in protective information bubbles, called "filter bubbles", which can have the undesired effect of separating users from information that does not fit their preferences. This paper moves from early results on quantification of personalization over Google search query results. Inspired by previous works, we have carried out some experiments consisting of search queries performed by a battery of Google accounts with differently prepared profiles. Matching query results, we quantify the level of personalization, according to topics of the queries and the profile of the accounts. This work reports initial results and it is a first step a for more extensive investigation to measure Web search personalization...|$|R
