209|428|Public
2500|$|This {{discussion}} {{is limited to}} the <b>error</b> <b>sensitivity</b> analysis for the case of statistical uncertainties. Here the actual noise covariances are denoted by [...] and [...] respectively, whereas the design values used in the estimator are [...] and [...] respectively. The actual error covariance is denoted by [...] and [...] as computed by the Kalman filter {{is referred to as the}} Riccati variable. When [...] and , this means that [...] While computing the actual error covariance using , substituting for [...] and using the fact that [...] and , results in the following recursive equations for [...] : ...|$|E
30|$|It {{could be}} argued that even though a {{scalable}} bitstream is not necessarily progressive, decoding dependencies may subsist in the bitstream. Actually, this decoding dependency is the cause of the unequal <b>error</b> <b>sensitivity</b> observed in a scalable bitstream. Additionally, the proposed solution measures this <b>error</b> <b>sensitivity</b> through a model and unequally distributes the protection accordingly. Therefore, the joint source-channel model is a central tool that allows the algorithm to gradually match the protection level to the <b>error</b> <b>sensitivity</b> and thus taking into account the possible decoding dependencies.|$|E
40|$|Technology scaling is {{reducing}} {{the reliability of}} integrated circuits. This makes it important to provide computers with mechanisms that can detect and correct hardware errors. This thesis deals {{with the problem of}} assessing the hardware <b>error</b> <b>sensitivity</b> of computer systems. <b>Error</b> <b>sensitivity,</b> which is the likelihood that a hardware error will escape detection and produce an erroneous output, measures a system’s inability to detect hardware errors. This thesis present the results of a series of fault injection experiments that investigated how er- ror sensitivity varies for different system characteristics, including (i) the inputs processed by a program, (ii) a program’s source code implementation, and (iii) the use of compiler optimizations. The study focused on the impact of tran- sient hardware faults that result in bit errors in CPU registers and main memory locations. We investigated how the <b>error</b> <b>sensitivity</b> varies for single-bit errors vs. double-bit errors, and how <b>error</b> <b>sensitivity</b> varies with respect to machine instructions that were targeted for fault injection. The results show that the in- put profile and source code implementation of the investigated programs had a major impact on <b>error</b> <b>sensitivity,</b> while using different compiler optimizations caused only minor variations. There {{was no significant difference in}} error sen- sitivity between single-bit and double-bit errors. Finally, the <b>error</b> <b>sensitivity</b> seems to depend more on the type of data processed by an instruction than on the instruction type...|$|E
50|$|The AAC payload can be {{subdivided}} into parts with different <b>error</b> <b>sensitivities.</b>|$|R
40|$|Abstract — In digital {{transmission}} systems for speech, audio, and video signals source encoders extract parameters which are quantized and {{converted into a}} digital representation. As the individual bits of this representation exhibit different bit <b>error</b> <b>sensitivities,</b> usually channel coding with unequal error protection (UEP) is applied. However, some transmission systems do not include channel coding for several reasons. In this situation a concept called modulation with unequal power allocation (MUPA) can be applied which achieves UEP by allocating different transmission power to the modulation symbols according to the individual bit <b>error</b> <b>sensitivities.</b> The average transmitted energy per bit remains unaffected. In this contribution we present a new detailed analysis of BPSK-MUPA and 16 -QAM-MUPA, and discuss the performance improvements in terms of parameter SNR compared to systems with constant symbol energy. I...|$|R
40|$|We {{demonstrate}} {{the existence of}} a new approximate ambiguity in structure from motion which occurs as generically as the bas [...] relief ambiguity but applies more strongly to scenes with larger depth variation. It occurs for moderate translations and field of view (as for the bas [...] relief ambiguity) and applies to multi [...] frame, finite [...] motion sequences where the camera moves roughly along a line, as well as to optical flow. Previous work on the bas [...] relief ambiguity gave a partial characterization of the <b>error</b> <b>sensitivities</b> in recovering the camera heading, assuming that the scene was non [...] planar and that the heading was sufficiently different from the view direction. Our analysis completes the understanding of the <b>error</b> <b>sensitivities</b> under these conditions. 1 Introduction This paper demonstrates {{the existence of a}} new approximate ambiguity in structure from motion (SFM) which occurs as generically as the bas [...] relief ambiguity but differs from it in important ways [...] -for instance, the new [...] ...|$|R
40|$|This thesis {{addresses}} {{the problem of}} measuring hardware <b>error</b> <b>sensitivity</b> of computer systems. Hardware <b>error</b> <b>sensitivity</b> is {{the probability that a}} hardware error will result in an erroneous output. Measuring the hardware <b>error</b> <b>sensitivity</b> is important since the rate of transient, intermittent and permanent transistors faults increases as a result of integrated circuit technology scaling. <b>Error</b> <b>sensitivity</b> is influenced by several parameters. This thesis investigates six such parameters, or sources of variation in <b>error</b> <b>sensitivity,</b> in a series of fault injection experiments. In these experiments, bit flip errors were injected into a microprocessors instruction set architecture (ISA) registers and main memory words in order to emulate the errors caused by transient hardware faults. The sources of variation that were addressed include, the ones that deal with systems characteristics, namely, (i) the input processed by a program, (ii) the program’s source code implementation, (iii) the distribution of machine instructions, and (iv) the level of compiler optimization; and the ones that deal with the measurement setup, namely, (v) the number of bits that are targeted in each fault injection experiment and (vi) the significance of the bit, or bits, targeted for fault injection. The experiments identified four factors that had a strong impact on error sensitivity: (1) the location of the erroneous bit, or bits, within a register or memory word, (2) the type of machine instruction targeted for fault injection, (3) the input to program and (4) a programs source code implementation. In contrast, variations in compiler optimization were shown to have a minor impact on <b>error</b> <b>sensitivity.</b> The experiments also show that {{there was no significant difference}} in <b>error</b> <b>sensitivity</b> between single and double bit flips when these occurred within same register or memory word...|$|E
40|$|Time-domain channel {{characterization}} (TCC) for deembedding of an asymmetric fixture is introduced. Two {{design criteria}} {{for the design of}} a 2 x-thru are proposed. <b>Error</b> <b>sensitivity</b> regarding a small error in the S-parameters of the 1 x-fixture is analyzed with an insertion loss error-coefficient and a return loss error-coefficient. The TCC procedure, including proposed design criteria and <b>error</b> <b>sensitivity,</b> is also introduced to reduce the error in the TCC application. Three different 2 x-thru structures are investigated for the verification of the two proposed design criteria and analyzed for <b>error</b> <b>sensitivity.</b> Test fixtures on a printed circuit boards are fabricated for the experimental verification. clos...|$|E
40|$|Technology and voltage scaling {{is making}} {{integrated}} circuits increasingly susceptible to failures caused by soft errors. The source of soft errors are temporary hardware faults that alter data and signals in digital circuits. Soft errors are predominately caused by ionizing particles, electrical noise and wear-out effects, {{but may also}} occur {{as a result of}} marginal circuit designs and manufacturing process variations. Modern computers are equipped with a range of hardware and software based mechanisms for detecting and correcting soft errors, as well as other types of hardware errors. While these mechanisms can handle a variety of errors and error types, protecting a computer completely from the effects of soft errors is technically and economically infeasible. Hence, in applications where reliability and data integrity is of primary concern, it is desirable to assess and measure the system's ability to detect and correct soft errors. This thesis is devoted to the problem of measuring hardware <b>error</b> <b>sensitivity</b>  of computer systems. We define hardware <b>error</b> <b>sensitivity</b> as the probability that a hardware error results in an undetected erroneous output. Since the complexity of computer systems makes it extremely demanding to assess the effectiveness of error handling mechanisms analytically, <b>error</b> <b>sensitivity</b> and related measures, e. g., error coverage, are in practice determined experimentally by means of fault injection experiments. The <b>error</b> <b>sensitivity</b> of a computer system depends not only on the design of its error handling mechanism, but also on the program executed by the computer. In addition, measurements of <b>error</b> <b>sensitivity</b> is affected by the experimental set-up, including how and where the errors are injected, and the assumptions about how soft errors are manifested, i. e., the error model. This thesis identifies and investigates six parameters, or sources of variation, that affect measurements of <b>error</b> <b>sensitivity.</b> These parameters consist of two subgroups, those that deal with systems characteristics, namely, (i) the input processed by a program, (ii) the program's source code implementation, (iii) the level of compiler optimization; and those that deal with measurement setup, namely, (iv) the number of bits that are targeted in each experiment, (v) the target location in which faults are injected, (vi) the time of injection. To accurately measure the <b>error</b> <b>sensitivity</b> of a system, one needs to conduct several sets of fault injection experiments by varying different sources of variations. As these experiments are quite time-consuming, it is desirable to improve the efficiency of fault injection-based measurement of <b>error</b> <b>sensitivity.</b> To this end, the thesis proposes and evaluates different error space optimization and error space pruning techniques to reduce the time and effort needed to measure the <b>error</b> <b>sensitivity...</b>|$|E
40|$|The {{demand for}} {{computer}} simulation of multiconductor interconnections in VLSI structures necessitates that such structures should be modelled. Calculations {{of the model}} capacitance matrix from measurements, for the multiconductor transmission lines which are coupled interconnections in a VLSI structures, using the "two-terminal" capacitances indirect measurement procedure is strongly corrupted by the measuring <b>errors.</b> <b>Sensitivity</b> analysis and <b>error</b> propagation relations demonstrates it. The paper presents alternative method for direct capacitance measurements in the multiconductor structure using active separation of the capacitance network to increase the measurements accuracy. 1...|$|R
40|$|The {{possible}} methodologies {{to handle}} the uncertain parameter are reviewed. The core idea of the desensitized Kalman filter is introduced. A new cost function consisting of a posterior covariance trace and trace of a weighted norm of the state <b>error</b> <b>sensitivities</b> matrix is minimizing to obtain a well-known analytical gain matrix, which {{is different from the}} gain of the desensitized Kalman filter. The pre-estimated uncertain parameter covariance is set as a referential sensitivity-weighting matrix in the new framework, and the rationality and validity of the covariance are tested. Then, these results are extended to the linear continuous system. Comment: 15 pages, 4 figure...|$|R
40|$|The {{familiar}} suboptimal regulator design {{approach is}} recast as a constrained optimization problem and incorporated in a Computer Aided Design (CAD) package where both design objective and constraints are quadratic cost functions. This formulation permits the separate consideration of, for example, model following <b>errors,</b> <b>sensitivity</b> measures and control energy as objectives to be minimized or limits to be observed. Efficient techniques for computing the interrelated cost functions and their gradients are utilized {{in conjunction with}} a nonlinear programming algorithm. The effectiveness of the approach and the degree of insight into the problem which it affords is illustrated in a helicopter regulation design example...|$|R
40|$|Abstract In many data {{acquisition}} tasks, the place-ment {{of a real}} camera can vary signicantly in com-plexity from one scene to another. Optimal camera po-sitioning should be governed not only by least <b>error</b> <b>sensitivity,</b> {{but in addition to}} real-world practicalities given by various physical, nancial and other types of constraints. It would be a laborious and costly task to model all these constraints if one were to rely solely on fully automatic algorithms to make the decision. In this work, we present a study using 2 D and 3 D visualization methods to assist in single camera positioning based on <b>error</b> <b>sensitivity</b> of reconstruction and other physical and nancial constraints. We develop a collection of vi-sual mappings that depict the composition of multiple <b>error</b> <b>sensitivity</b> elds that occur for a given camera po-sition. Each camera position is then mapped to a 3 D visualization that enables visual assessment of the cam-era conguration. We nd that the combined 2 D an...|$|E
40|$|In {{this paper}} we study the {{performance}} and error sensitivities of several CELP based codecs operating between 8 and 4 kbits/s. Both forward and backward adaption techniques are used for the {{short and long term}} predictors, and both trained and Algebraic excitation codebooks are used. Three codecs which employ backward adaption of their short term predictors and operate with frame-lengths of 3 ms or less are described. These codecs operate between 8 and 4 kbits/s and their performance at these bit rates is compared to a traditional forward adaptive Algebraic CELP codec operating at 4. 7, 6. 5 and 7. 1 kbits/s. Furthermore, the <b>error</b> <b>sensitivity</b> of the backward adaptive codecs, and means of improving this <b>error</b> <b>sensitivity,</b> are investigated. Finally, we compare the <b>error</b> <b>sensitivity</b> of the low delay, backward adaptive codecs to the high delay, forward adaptive codec. Surprisingly, we found {{that it is possible to}} achieve good error resilience, comparable to that of the forward adaptive codec, using low delay backward adaptive codecs...|$|E
30|$|An {{enhanced}} {{version of}} this model was presented in [11] leading to more realistic local variations and significant reduction in the mean <b>error</b> <b>sensitivity.</b> The main modification consists in the introduction of multiple paths diffracted at the window edges.|$|E
40|$|The HILN (Harmonic and Individual Lines plus Noise) MPEG- 4 {{parametric}} audio coding tool allows efficient {{representation of}} general audio signals {{at very low}} bit rates. Therefore possible applications include transmission over IP or wireless channels which are both characterised by specific transmission error models. On the other hand, since parametric audio coding {{is a relatively new}} technique compared to transform coding and CELP speech coding, there have been only very limited investigations on HILN's behaviour in error prone environments. In this paper we present an analysis of <b>error</b> <b>sensitivities</b> and approaches to error protection and concealment...|$|R
5000|$|... #Subtitle level 2: Estimation of <b>errors</b> in quoted <b>sensitivity</b> or {{specificity}} ...|$|R
5000|$|The {{variance}} is <b>error</b> from <b>sensitivity</b> {{to small}} {{fluctuations in the}} training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).|$|R
40|$|Abstract—Several popular {{bistatic}} calibration {{techniques are}} in-vestigated and comparisons {{made between the}} relative merits of the various techniques. The analysis addresses sensitivity to object alignment <b>error,</b> <b>sensitivity</b> to polarization impurity, and ease of implementation. Both theoretical concepts and practical consider-ations are discussed based on measurements accomplished at th...|$|E
40|$|Abstract – Claude Shannon’s {{pioneering}} work quantified the performance limits of communications systems operating over classic wireline Gaussian channels. However, his source and channel coding theorems were derived {{for a range}} of idealistic conditions, which may not hold in low-delay, interactive wireless multimedia communications. Firstly, Shannon’s ideal lossless source encoder, namely the entropy encoder may have an excessive codeword length, hence exhibiting a high delay and a high <b>error</b> <b>sensitivity.</b> However, in practice most multimedia source signals are capable of tolerating lossy, rather than lossless delivery to the human eye, ear and other human sensors. The corresponding lossy and preferably low-delay multimedia source codecs however exhibit unequal <b>error</b> <b>sensitivity,</b> which is not the case for Shannon’s ideal entropy codec. There are further numerous differences between the Shannonian lessons originally outlined for Gaussia...|$|E
40|$|This paper {{presents}} a direct closed-form {{solution to the}} general relative orientation problem. We shall concentrate on the existence and correctness of this solution. The <b>error</b> <b>sensitivity</b> analysis {{is beyond the scope}} of this paper. However, a numerical example and a real image application are provided. 240 CHAPTER 8. RELATIVE ORIENTATIO...|$|E
40|$|Abstract- A {{well known}} {{technique}} to protect bits with different <b>sensitivities</b> against channel <b>errors</b> is {{unequal error protection}} (UEP) by selective channel coding. However, some transmission systems such as Bluetooth and DECT include only weak or no channel coding. For those systems modulation with unequal power allocation (MUPA) is an appropriate technique to realize UEP without channel coding. MUPA allocates different transmission power to the modulation symbols according to the individual bit <b>error</b> <b>sensitivities.</b> In this paper we apply the MUPA concept to systems with softbit source decoding and soft demodulation, and enhance the MUPA approach by taking a 1 st order Markov model in terms of lst order a priori knowledge (AKI) into account (MUPA-AK 1). I...|$|R
40|$|In color-plus-depth map {{three-dimensional}} (3 -D) video representation, {{color and}} depth map sequences exhibit different <b>error</b> <b>sensitivities</b> {{to the overall}} quality. Recognizing that, in this paper, we propose to exploit the relay cooperation benefits to achieve unequal error protection (UEP) for better quality 3 -D video transmission. Specifically, we propose to apply the first best amplify-and-forward (AF) relay for transmitting color information with high priority (HP) while the second best AF relay is employed for the depth information with low priority (LP). In particular, we drive closed-form expressions for the bit error probability (BEP). Simulation results of the BEP are presented to demonstrate the proposed UEP scheme {{in terms of the}} received 3 -D video quality. ...|$|R
40|$|This paper {{presents}} an embedded joint source-channel coding scheme of speech. The source coder is an embedded {{variable bit rate}} perceptually based sub-band coder producing bits with different <b>error</b> <b>sensitivities.</b> The channel encoder is a Rate Compatible Punctured Trellis code (RCPT) which permits rate variability and unequal error protection by puncturing symbols. Furthermore, RCPT code design naturally incorporates large constellations, allowing high information rate per symbol. The embedded speech coder and the rate compatible puncturing of symbols provide the embeddibility of the joint coding scheme. The coder is robust to acoustic noise and produces good quality speech {{for a wide range}} of channel conditions (AWGN or fading), allowing digital transmission of speech with analog-like graceful degradation...|$|R
40|$|This paper {{presents}} the results of an extensive experimental study of bit-flip errors in instruction set architecture registers and main memory locations. Comprising more than two million fault injection experiments conducted with thirteen benchmark programs, the study provides insights on whether it is necessary to consider double bit-flip errors in dependability benchmarking experiments. The results show that the proportion of silent data corruptions in the program output, is almost the same for single and double bit errors. In addition, we present detailed statistics about the <b>error</b> <b>sensitivity</b> of different target registers and memory locations, including bit positions within registers and memory words. These show that the <b>error</b> <b>sensitivity</b> varies significantly between different bit positions and registers. An important observation is that injections in certain bit positions always have the same impact regardless of when the error is injected...|$|E
40|$|Output (MIMO) {{system is}} capable of {{achieving}} a high bandwidth efficiency. The SVD operation effectively splits the MIMO channel into orthogonal eigen-beams having unequal Bit-error-ratios (BERs). This property may be exploited for the transmission of speech, audio, video and other multimedia source signals, where the source-coded bits have different <b>error</b> <b>sensitivity.</b> Index Terms—SVD,MIMO,H. 264,UEP I...|$|E
30|$|The {{streaming}} {{of video}} either {{directly over the}} physical layer or using IP packets, is subject to transmission errors. Retransmission of lost/corrupted data or packets is viable, but decreases interactivity and real-time requirements. Thus, forward error correction (FEC) is generally adopted, and the channel code rate may be matched to compressed data <b>error</b> <b>sensitivity,</b> performing unequal error protection (UEP) [11 – 13].|$|E
40|$|This paper {{presented}} a robust integrated navigation algorithm {{based on a}} special robust desensitized extended Kalman filtering with analytical gain (ADEKF) during the Mars atmospheric entry. The robust ADEKF is designed by minimizing a new function penalized by a trace weighted norm of the state <b>error</b> <b>sensitivities</b> and giving a closed-form gain matrix. The uncertainties of the Mars atmospheric density and the lift-to-drag ratio (LDR) percentage are modeled. Sensitivity matrices are defined to character the parameter uncertainties, and corresponding perturbation matrices are proposed to describe the navigation errors respected to the parameter uncertainties. The numerical simulation {{results show that the}} robust integrated navigation algorithm based on the robust ADEKF effectively reduces the negative effects of the two parameter uncertainties and has good consistency during the Mars entry. Comment: 11 pages, 7 figure...|$|R
40|$|This thesis {{presents}} {{an overview of}} the main strategies employed for error detection and error concealment in different real-time transmission systems for digital audio. The “Adaptive Differential Pulse-Code Modulation (ADPCM) ”, the “Audio Processing Technology Apt-x 100 ”, the “Extended Adaptive Multi-Rate Wideband (AMR-WB+) ”, the “Advanced Audio Coding (AAC) ”, the “MPEG- 1 Audio Layer II (MP 2) ”, the “MPEG- 1 Audio Layer III (MP 3) ” and finally the “Adaptive Transform Coder 3 (AC 3) ” are considered. As an example of error management, a simulation of the AMR-WB+ codec is included. The simulation allows an evaluation of the mechanisms included in the codec definition and enables also an evaluation of the different bit <b>error</b> <b>sensitivities</b> of the encoded audio payload. Ingeniería Técnica en Telemátic...|$|R
40|$|This is {{part two}} of a series on the optical {{modeling}} activities for JWST. Starting with the linear optical model discussed in part one, we develop centroid and wavefront <b>error</b> <b>sensitivities</b> for the special case of a segmented optical system such as JWST, where the primary mirror consists of 18 individual segments. Our approach extends standard sensitivity matrix methods used for systems consisting of monolithic optics, where the image motion is approximated by averaging ray coordinates at the image and residual wavefront error is determined with global tip/tilt removed. We develop an exact formulation using the linear optical model, and extend it to cover multiple field points for performance prediction at each instrument aboard JWST. This optical model is then driven by thermal and dynamic structural perturbations in an integrated modeling environment. Results are presented...|$|R
40|$|The {{development}} of the HIRLAM 4 -dimensional variational data assimilation (4 D-Var) started already 11 {{years ago with the}} coding of the tangent- linear and adjoint versions of the Eulerian spectral and adiabatic HIRLAM and the application of these models to forecast <b>error</b> <b>sensitivity</b> experiments (Gustafsson and Huang, 1996). The {{development of}} tangent-linear and adjoint versions of the HIRLAM non-linear physical parameterizatio...|$|E
40|$|We present Tetra, a star {{identification}} algorithm {{that uses}} the minimum possible computation time and number of database accesses to solve the calibrationless lost-in-space problem. To solve the calibrationless lost-in-space problem, a star tracker must determine its attitude with no a priori knowledge, not even lens parameters such as the field-of-view or distortions. Tetra {{is based on a}} directly-addressed hash table data structure, which enables it to identify star patterns with a single database access. We prove a tight bound on Tetra 2 ̆ 7 s false positive rate and empirically compare Tetra 2 ̆ 7 s runtime, centroiding <b>error</b> <b>sensitivity,</b> and field-of-view <b>error</b> <b>sensitivity</b> with earlier lost-in-space algorithms: Pyramid and Nondimensional Star ID. We also compare Tetra with hash table based modifications of Pyramid’s cross-referencing step and Nondimensional Star ID 2 ̆ 7 s database lookup, which improve Pyramid and Nondimensional Star ID 2 ̆ 7 s runtimes by an order of magnitude without otherwise impacting their performance...|$|E
40|$|International audienceIntegral Equations {{are widely}} used in Computational Electromagnetics for solving {{radiation}} and scattering problems. When solved with the Boundary Element Method (BEM), integral equations give rise to dense linear systems. When the system's dimensionality is large, fast iterative or direct solvers have to be used and the BEM matrix has to be well-conditioned to ensure numerical stability. In fact, in solving the matrix associated linear system, the matrix condition number (the ratio of the matrix largest and smallest singular value) is related to iterative solvers' convergence rates and to the <b>error</b> <b>sensitivity</b> of the solution: the highest the condition number, the highest the convergence time and the <b>error</b> <b>sensitivity.</b> Unfortunately, many commonly used formulations suffer from severe ill-conditionings especially for large, multiscale, and complex problems. The problem complexity of current applications is steadily and rapidly increasing. For this reason, the impact of well-conditioned formulations on state-of-the-art computational technology is destined to {{be more and more}} predominant and pervasive...|$|E
40|$|This {{undergraduate}} meteorology tutorial from Texas A&M University {{describes the}} common sources of weather forecasting computer model error, ways to identify model error, {{and how to}} correct a forecast for some simple types of <b>error.</b> Model <b>sensitivity</b> to parameterization and topography are covered. Educational levels: High school, Undergraduate lower division...|$|R
30|$|When compared, it is {{observed}} that the ranges for the statistical indicators used for the study are similar for all the stations and respective variables. The results show that the model gives satisfactory R 2 for all the parameters except DO. However, the results obtained from other statistical indicators, including RMSE, MBE, and SDBE, show that the difference between observed and simulated DO values is low, indicating satisfactory results. The results also show that statistically QUAL 2 Kw model is able to explain the relationship between observed and simulated parameters for all the three locations. The results, when compared to the previous studies (Paliwal et al. 2007; Parmar and Keshari 2012), show that the model is able to simulate the extensive dataset with minimal errors. However, to further quantify the <b>errors,</b> <b>sensitivity</b> and uncertainty analysis has been done.|$|R
40|$|Abstract. SCIAMACHY CO 2 {{measurements}} show a large {{variability in}} total column CO 2 over the Sahara desert {{of up to}} 10 %, which is not anticipated from in situ measure-ments and cannot be explained by results of atmospheric models. Comparisons with colocated aerosol measurements by TOMS and MISR over the Sahara indicate that the sea-sonal variation of SCIAMACHY-observed CO 2 strongly re-sembles seasonal variations of windblown dust. Correlation coefficients of monthly datasets of colocated MISR aerosol optical depth and SCIAMACHY CO 2 vary between 0. 6 and 0. 8, indicating {{that about half of}} the CO 2 variance is ex-plained by aerosol optical depth. Radiative transfer model calculations confirm the role of dust and can explain the size of the <b>errors.</b> <b>Sensitivity</b> tests suggest that the remaining vari-ance may largely be explained by variations in the vertica...|$|R
