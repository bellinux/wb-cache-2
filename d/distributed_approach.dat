1028|2255|Public
50|$|IoT-Gateway - Benchmark standardizes {{assumptions}} about gateway operational conditions to ensure meaningful comparisons between gateway products; utilizes a <b>distributed</b> <b>approach</b> with client-server interactions and workloads generated across multiple physical ports.|$|E
50|$|Intec {{maintains}} a <b>distributed</b> <b>approach</b> {{to support and}} development, with main R&D facilities in USA, South Africa, Australia and support centres throughout North America, Central & Latin America, Europe, Middle East, Africa and Asia.|$|E
50|$|The planner used a <b>distributed</b> <b>approach</b> {{based on}} the Traveline journey {{planners}} for each Traveline region with the JourneyWeb protocol being used to manage journeys between Traveline regions. Each Journey Planner integrated information from many different transport operators and sources.|$|E
30|$|Improve local state {{estimation}} over purely <b>distributed</b> <b>approaches.</b>|$|R
30|$|<b>Distributed</b> <b>approaches</b> {{typically}} {{focus on}} VM consolidation {{and will be}} covered {{as part of our}} analysis of VM migration, in the next subsection.|$|R
30|$|There {{are also}} other <b>distributed</b> <b>approaches</b> for target {{tracking}} {{that are based}} on cluster [6, 33, 38] and tree [17, 31, 37] organizations for in-network data processing.|$|R
5000|$|Continuing {{with this}} thread in her practice, Sneha founded and co-directed Polytechic, an {{organisation}} {{working with a}} hands-on, open and <b>distributed</b> <b>approach</b> to art & technology where she produced, curated and led a range of national and international activity between 2002 and 2012 (see projects below) ...|$|E
50|$|Like GNU arch, {{and unlike}} Subversion, Monotone takes a <b>distributed</b> <b>approach</b> to version control. Monotone uses SHA-1 hashes to {{identify}} specific files {{or groups of}} files, as with Git and Mercurial, in place of linear revision numbers. Each participant maintains their own revision history, stored in a local SQLite database.|$|E
50|$|Architecture {{is based}} on a <b>distributed</b> <b>approach</b> and on the {{utilization}} of standard software products complying with the industrial and market standards must be utilized (such as: UNIX operating systems, MS-Windows, local area network based on Ethernet and TCP/IP protocols, relational database management systems based on SQL language or Oracle databases, C programming language).|$|E
40|$|Reputation {{systems are}} {{indispensable}} {{for the operation}} of Internet mediated services, electronic markets, document ranking systems, P 2 P networks and Ad Hoc networks. Here we survey available <b>distributed</b> <b>approaches</b> to the graph based reputation measures. Graph based reputation measures {{can be viewed as}} random walks on directed weighted graphs whose edges represent interactions among peers. We classify the <b>distributed</b> <b>approaches</b> to graph based reputation measures into three categories. The first category is based on asynchronous methods. The second category is based on the aggregation/decomposition methods. And the third category is based on the personalization methods which use local information...|$|R
40|$|Automated {{generation}} of axioms from streaming data, such as traffic and text, {{can result in}} very large ontologies that single machine reasoners cannot handle. Reasoning with large ontologies requires distributed solutions. Scalable reasoning techniques for RDFS, OWL Horst and OWL 2 RL now exist. For OWL 2 EL, several <b>distributed</b> reasoning <b>approaches</b> have been tried, but are all perceived to be inefficient. We analyze this perception. We analyze completion rule based <b>distributed</b> <b>approaches,</b> using different characteristics, such as dependency among the rules, implementation optimizations, how axioms and rules are distributed. We also present a <b>distributed</b> queue <b>approach</b> for the classification of ontologies in description logic EL+ (fragment of OWL 2 EL) ...|$|R
30|$|For a <b>distributed</b> and decentralised <b>approach</b> to {{managing}} the data centre, the authors in [43, 64] proposed a peer-to-peer protocol that enables nodes to communicate directly without a centralised controller. A periodic node discovery service enables nodes {{to find new}} neighbouring nodes to communicate with. On each round of the protocol, two cooperating nodes determine to migrate a VM based on defined objectives. The <b>distributed</b> <b>approaches</b> in [43, 64] are used to redistribute the load across the cluster as well consolidate VMs. Using simulation, the authors claim their approaches can manage more than 100, 000 nodes. A challenge with <b>distributed</b> <b>approaches</b> {{is the lack of}} a global view of the infrastructure, which impact the ability to reach a globally optimal solution. Additionally, gossip approaches consume considerable bandwidth to implement propagation of node state across the entire data centre infrastructure.|$|R
50|$|Since the two {{paradigms}} provide {{essentially the}} same functionality, the choice between themdepends on which fits the management model of an organization better. For instance, the centralizedapproach fits large computer centres well because of its strictly controlled work-flow, whereas multi-siteorganizations such as GRIF prefer the <b>distributed</b> <b>approach</b> because it allows different parts ofthe whole configuration set to be handled autonomously.|$|E
50|$|The {{criticism}} {{comes from}} a difference in philosophies of the delivery of digital projects. Manzoni has publicly asserted his belief in a small centre of government and getting departments to take on as much capacity as possible. This <b>distributed</b> <b>approach</b> to government technology is widely {{considered to be a}} retrograde step, returning UK government to a pre-2010 era of frequent major IT disasters.|$|E
50|$|However, Resuna also {{provides}} a method for One True to control its human units completely, as needed. While working towards essentially common goals, this <b>distributed</b> <b>approach</b> means that Resuna is willing to sacrifice individuals if it determines that the greater good will be served. Free will is also removed, as the distributed network can override any individual actions that it determines {{are not in the}} best interests of the community.|$|E
40|$|International audienceWe propose two asynchronously <b>distributed</b> <b>approaches</b> for {{graph-based}} semi-supervised learning. The first {{approach is}} based on stochastic approximation, whereas the second approach {{is based on}} randomized Kaczmarz algorithm. In addition {{to the possibility of}} <b>distributed</b> implementation, both <b>approaches</b> can be naturally applied online to streaming data. We analyse both approaches theoretically and by experiments. It appears that there is no clear winner and we provide indications about cases of superiority for each approach...|$|R
40|$|Abstract — This paper {{presents}} all {{parallel programming}} models available today. It reviews shared and <b>distributed</b> memory <b>approaches.</b> Hybrid programming models are also playing {{important role in}} High Performance Computing (HPC) era. This makes best use of both shared and <b>distributed</b> <b>approaches.</b> The study shows multi-core CPU’s have given different impulse to shared memory model programming. Along with this, heterogeneous programming is explored, to exploit combined effects of CPUs and Graphics Processing Units (GPUs) Graphics Processing. This work introduces the contribution of Open standards such as Open Mult...|$|R
40|$|Alongside {{existing}} {{research into the}} social, political and economic impacts of the Web, {{there is also a}} need to explore the effects of the Web on our cognitive profile. This is particularly so as the range of interactive opportunities we have with the Web expands under the influence of a range of emerging technologies. Embodied, extended and <b>distributed</b> <b>approaches</b> to cognition are relevant to understanding the potential cognitive impact of these new technologies because of the emphasis they place on extra-neural and extra-corporeal factors in the shaping of our cognitive capabilities at both an individual and collective level. The current paper outlines a number of areas where embodied, extended and <b>distributed</b> <b>approaches</b> to cognition are useful in understanding the impact of emerging Web technologies on future forms of both human and machine intelligence...|$|R
50|$|Quattor’s modular {{architecture}} allows the three configuration management subsystems to bedeployed {{in either a}} distributed or centralized fashion. In the <b>distributed</b> <b>approach,</b> profile compilation(at development stage) is carried out on client systems, templates are then checked into asuitable database, and finally the deployment is initiated by invoking a separate operation on theserver. The centralized approach provides strict control of configuration data. The compilationburden is placed onto the central server, and users can only access and modify templates via adedicated interface.|$|E
5000|$|The Reputations {{system and}} the {{subsequent}} generation of multiple points {{of view of a}} portal {{is one of the most}} innovative aspects of the program. Unlike [...] "traditional" [...] systems where the computational work (calculation of statistics, indexing of content, etc.) is always made by a central server, Osiris use a <b>distributed</b> <b>approach,</b> where the majority of the works is made by users of a portal, due to this there may be more distinct points of view of a portal, depending on used account.|$|E
50|$|IA-BF - The Impairment Aware Best Fit (IA-BF) {{algorithm}} {{was proposed}} in. This algorithm is a <b>distributed</b> <b>approach</b> that is dependent upon {{a large amount of}} communication to use global information to always pick the shortest available path and wavelength. This is accomplished through the use of serial multi-probing. The shortest available path and wavelength are attempted first, and upon failure, the second shortest available path and wavelength are attempted. This process continues until a successful path and wavelength have been found or all wavelengths have been attempted.|$|E
50|$|The {{implementation}} of the SDN control plane can follow a centralized, hierarchical, or decentralized design. Initial SDN control plane proposals focused on a centralized solution, where a single control entity has a global view of the network. While this simplifies the {{implementation of}} the control logic, it has scalability limitations as the size and dynamics of the network increase. To overcome these limitations, several approaches have been proposed in the literature that fall into two categories, hierarchical and fully <b>distributed</b> <b>approaches.</b> In hierarchical solutions, distributed controllers operate on a partitioned network view, while decisions that require network-wide knowledge are taken by a logically centralized root controller. In <b>distributed</b> <b>approaches,</b> controllers operate on their local view or they may exchange synchronization messages to enhance their knowledge. Distributed solutions are more suitable for supporting adaptive SDN applications.|$|R
40|$|ABSTRACT: The HLA data {{distribution}} management (DDM) services {{have been implemented}} in the MAK High Performance Runtime Infrastructure (RTI). The implementation of the DDM services has followed a <b>distributed</b> region <b>approach.</b> This paper describes the <b>distributed</b> region <b>approach</b> and its implementation in the MAK High Performance RTI using multicast groups. The <b>distributed</b> region <b>approach</b> <b>distributes</b> region information throughout the federation to dynamically match publication and subscription regions. The distribution of region information throughout the federation as well as processing updates, interactions, and interest advisories with respect to regions introduces overhead. The paper examines this overhead in relation to RTI and federate processing and network bandwidth. Suggestions for optimizing the use of DDM {{both internal and external}} to the RTI are addressed. 1...|$|R
5000|$|... #Subtitle level 2: <b>Distributed</b> {{morphology}} <b>approach</b> to core theoretical issues ...|$|R
50|$|National Disaster Management Authority (NDMA) is {{an agency}} of the Ministry of Home Affairs whose primary purpose is to {{coordinate}} response to natural or man-made disasters and for capacity-building in disaster resiliency and crisis response. NDMA was established through the Disaster Management Act enacted by the Government of India in December 2005. The Prime Minister is the ex-officio chairperson of NDMA. The agency is responsible for framing policies, laying down guidelines and best-practices and coordinating with the State Disaster Management Authorities (SDMAs) to ensure a holistic and <b>distributed</b> <b>approach</b> to disaster management.|$|E
5000|$|If {{less secure}} {{encryption}} is used due to processor {{limitations on the}} smartcards, the system is vulnerable to [...] using distributed processing. While most secure Internet and online banking transactions require 128-bit encryption, 56-bit codes are not uncommon in video encryption. A cryptographic attack against a 56-bit DES code would still be prohibitively time-consuming on a single processor. A <b>distributed</b> <b>approach</b> in which many users each run software to scan just {{a portion of the}} possible combinations, then upload results to one or more central points on a network such as the Internet, may provide information of value to pirates who wish to break security. Distributed processing attacks were used, successfully in some cases, against the D2-MAC/EuroCrypt system used in Europe during the 1990s.|$|E
5000|$|A <b>distributed</b> <b>approach</b> for {{interference}} networks with link {{rates that}} {{are determined by}} the signal-to-noise-plus-interefernce ratio (SINR) can be carried out using randomization. [...] Each node randomly decides to transmit every slot t (transmitting a [...] "null" [...] packet if it currently does nothave a packet to send). The actual transmission rates, and the corresponding actual packets to send,are determined by a 2-step handshake:On the first step, the randomly selected transmitter nodes send a pilot signal with signal strength proportionalto that of an actual transmission. On the second step,all potential receiver nodes measure the resulting interference and send that information back to thetransmitters. The SINR levels for all outgoing links (n,b) are then known to all nodes n,and each node n can decideits [...] and [...] variables based on this information.The resulting throughput is not necessarily optimal. However, the random transmission process {{can be viewed as a}} part of the channel state process (provided that null packets are sent in cases of underflow, so that the channel state process does not depend on past decisions). Hence, the resulting throughput of this distributed implementation is optimal over the class of all routing and scheduling algorithms that use such randomized transmissions.|$|E
5000|$|Talking for a Change: A <b>distributed</b> {{dialogue}} <b>approach</b> {{to complex}} issues, 2010 ...|$|R
30|$|The {{problem of}} {{coordinating}} parameters {{can be solved}} adopting basically two types of solutions: (1) centralized approach, which yields the optimal global parameter vector, and (2) <b>distributed</b> <b>approaches,</b> which on one hand often provide suboptimal solutions through greedy techniques such as non-cooperative games, {{but on the other}} hand can provide near-optimal solutions by using a factor graph.|$|R
40|$|The {{formalism}} of chronicles {{has been}} proposed to monitor and diagnose dynamic physical systems. Even if efficient chronicle recognition algorithms exist, it is now well-known that <b>distributed</b> <b>approaches</b> are better suited to monitor actual systems. In this article, we adapt the chronicle-based <b>approach</b> to a <b>distributed</b> context and illustrate this work on the monitoring of software components...|$|R
40|$|This paper {{deals with}} Chinese, English and Japanese {{multilingual}} information retrieval. Several merging strategies, including raw-score merging, round-robin merging, normalized-score merging, and normalizedby-top-k merging, were investigated. Experimental {{results show that}} centralized approach is better than <b>distributed</b> <b>approach.</b> In <b>distributed</b> <b>approach,</b> normalized-by-top-k with consideration of translation penalty outperforms the other merging strategies...|$|E
40|$|International audienceIn this paper, {{we focus}} on {{providing}} new tools to help doctors in their diagnosis. We study the feasibility of using a <b>distributed</b> <b>approach</b> over Wireless Sensor Networks (WSN). We define a Wireless Body Area Network (WBAN) {{as a set of}} wireless sensors which equipped a patient. Each sensor senses a health parameter, for example, temperature or heart pulse. Our aim is to understand how a <b>distributed</b> <b>approach</b> can be a fair alternative to the common centralized paradigm. We study a <b>distributed</b> <b>approach</b> based on the token paradigm. Then, we compare this approach to the centralized one, throw simulations and experimentations over real sensors...|$|E
30|$|Herein, for the <b>distributed</b> <b>approach</b> locally {{available}} information is required and we restrict the internode communication between neighbor nodes only.|$|E
40|$|The {{absence of}} {{electrical}} regenerators in transparent WDM networks significantly contributes {{to reduce the}} overall network cost. In transparent WDM networks, a proper resource allocation requires {{that the presence of}} physical impairments in Routing and Wavelength Assignment (RWA) and lightpath provisioning be taken into account. In this article a centralized, a hybrid centralized-distributed and two <b>distributed</b> <b>approaches</b> that integrate information about most relevant physical impairments in RWA and lightpath provisioning are presented and assessed. Both centralized and hybrid approaches perform a centralized path computation at the management-plane level, utilizing physical impairment information, while the lightpath provisioning is done by the management plane or the control plane, respectively. The <b>distributed</b> <b>approaches</b> fall entirely within the scope of the ASON/GMPLS control plane. For these two approaches, we provide functional requirements, architectural functional blocks, and protocol extensions for implementing either an impairment-aware real-time RWA, or a lighpath provisioning based on impairment-aware signaling...|$|R
40|$|Naval {{research}} programs {{are focusing on}} a <b>distributed</b> architecture <b>approach</b> for successful migration of data from one region to another. This presents a challenging area of modeling and managing the <b>distributed</b> <b>approaches.</b> This article describes a service-based model, which comprises of two steps that are decomposition and orchestration, for effectively managing distributed cluster networks. The contribution {{of this paper is}} two-fold. First, the article describes in detail the information transfer and entropy based decomposition approach. Second, it demonstrates the effectiveness of the orchestration approach to address challenges in distributed clusters using a simulation. We have implemented the decomposition approach in both Java and Matlab for verification...|$|R
30|$|As to {{the signal}} processing, the {{tracking}} approaches {{can be classified}} as centralized or distributed. Usually the tree-based schemes are centralized approaches, while the cluster-based schemes are distributed schemes in which the cluster head is the leader node in the processing. The works in [13, 18] are centralized approaches, while those in [3, 4, 14, 16, 19] are <b>distributed</b> <b>approaches.</b>|$|R
