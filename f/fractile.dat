101|43|Public
2500|$|Swamy, S. (1963). Notes on <b>Fractile</b> Graphical Analysis. Econometrica, 31(3), 551-554. doi:1. https://www.jstor.org/stable/1909994 doi:1 ...|$|E
5000|$|... #Subtitle level 2: Profit {{function}} {{and the critical}} <b>fractile</b> formula ...|$|E
50|$|The {{critical}} <b>fractile</b> {{formula is}} known as Littlewood's rule in the yield management literature.|$|E
40|$|On one hand, {{eliciting}} subjective probabilities (<b>fractiles)</b> is {{a well-established}} procedure. On the other hand, {{knowledge of a}} subjective variable's central moments or distribution function is often assumed. However, the problem of converting elicited <b>fractiles</b> into the required moments or distribution function has been largely ignored. We show that this conversion problem is far from trivial, and that {{the most commonly used}} conversion procedures often produce huge errors. Alternative procedures are proposed; the `Tocher's curve' and `linear function of fractiles' methods are shown to be much more accurate than the commonly used procedures. link_to_subscribed_fulltex...|$|R
30|$|S 30). The most {{probable}} explanation {{comes from}} the fact that simultaneously matching 10 and 90 % <b>fractiles</b> for a pair of proxies corresponds to less frequent combinations, with more differentiated site conditions, than for a single proxy (as also shown in Fig.  3).|$|R
40|$|PERT-type {{subjective}} estimations {{are used}} in many stochastic decision models to estimate the random variables' {{mean and standard deviation}} (s. d.). The approach is based on the beta-distribution assumption; also, most PERT-type formulas use only three estimated <b>fractiles.</b> We point out that: (i) it is desirable to consider a substantially richer set of distributions than the beta in developing PERT-type formulas; (ii) it may be beneficial to use more than three fractile-estimates in PERT-type formulas. We then develop formulas for estimating the mean and s. d. that are based on a substantially richer set of distributions than the beta and that use more than three estimated <b>fractiles.</b> These formulas perform better than the best currently-available formulas when the subjective distribution is not restricted to be beta. link_to_subscribed_fulltex...|$|R
5000|$|Swamy, S. (1963). Notes on <b>Fractile</b> Graphical Analysis. Econometrica, 31(3), 551-554. doi:1. http://www.jstor.org/stable/1909994 doi:1 ...|$|E
5000|$|In the {{following}} cases, {{assume that the}} retail price, , is $7 per unit and the purchase price is , is $5 per unit. This gives a critical <b>fractile</b> of ...|$|E
5000|$|Intuitively, this ratio, {{referred}} to as the critical <b>fractile,</b> balances the cost of being understocked (a lost sale worth [...] ) and the total costs of being either overstocked or understocked (where the cost of being overstocked is the inventory cost, or [...] so total cost is simply [...] ).|$|E
40|$|Parameter {{estimation}} {{uncertainty is}} often neglected in reliability studies, i. e. point estimates of distribution parameters {{are used for}} representative <b>fractiles,</b> and in probabilistic models. A numerical example examines {{the effect of this}} uncertainty on structural reliability using Bayesian statistics. The study reveals that the neglect of parameter estimation uncertainty might lead to an order of magnitude underestimation of failure probability...|$|R
40|$|The {{inclusion}} of epistemic uncertainties, generally via logic trees (Kulkarni et al., 1984), within probabilistic seismic‐hazard assessments (PSHAs) is becoming standard {{for all types}} of studies (commercial, governmental, or research; site specific, national, regional, or global). Consequently many studies publish expected ground motions for a given annual frequency of exceedance (AFE) or return period derived from the hazard curves for the mean, median, and various <b>fractiles</b> (percentiles) ...|$|R
40|$|A new {{format for}} the gust factor G = 1 + g sigmaX/etaX and the peak factor g is {{proposed}} {{in such a}} way that these quantities have a given probability of exceedance. With reference to a SDoF model, the largest value CDF of diisplacement X is obtained by means of both Monte Carlo simulation and analytical tools. G and g are computed from the <b>fractiles</b> of this CDF...|$|R
50|$|Incorporating {{uncertainty}} into model outputs {{helps to}} provide more realistic and informative projections. Uncertain quantities in Analytica can be specified using a distribution function. When evaluated, distributions are sampled using either Latin hypercube or Monte Carlo sampling, and the samples are propagated through the computations to the results. The sampled result distribution and summary statistics can then be viewed directly (mean, <b>fractile</b> bands, probability density function (PDF), cumulative distribution function (CDF)), Analytica supports collaborative decision analysis and probability management {{through the use of}} the SIPMath(tm) standard.|$|E
5000|$|Henry's dry humor {{attracted}} {{attention in}} the entertainment community. He became a cast member on TV programs such as The New Steve Allen Show (1961) and That Was The Week That Was (1964-65). He was a co-creator and writer for Get Smart (1965-70), with Mel Brooks. Two of his TV projects had short runs but are fondly remembered by fans: Captain Nice (1967) with William Daniels as a reluctant superhero, and Quark (1978), with Richard Benjamin in command of a garbage scow in outer space. He also played Dr. Victor Rudman, a <b>fractile</b> scientist who dated Murphy, on the [...] "My Dinner With Einstein" [...] episode (1989) of Murphy Brown.|$|E
5000|$|Testfact {{features}} - Marginal {{maximum likelihood}} (MML) {{exploratory factor analysis}} and classical item analysis of binary data- Computes tetrachoric correlations, principal factor solution, classical item descriptive statistics, <b>fractile</b> tables and plots- Handles up to 10 factors using numerical quadrature: up to 5 for non-adaptive and up to 10 for adaptive quadrature- Handles up to 15 factors using Monte Carlo integration techniques- Varimax (orthogonal) and PROMAX (oblique) rotation of factor loadings- Handles an important form of confirmatory factor analysis known as [...] "bifactor" [...] analysis: Factor pattern consists of one main factor plus group factors- Simulation of responses to items based on user specified parameters- Correction for guessing and not-reached items- Allows imposition of constraints on item parameter estimates- Handles omitted and not-presented items- Detailed online HELP documentation includes syntax and annotated examples.|$|E
40|$|Laboratory {{values of}} the most {{commonly}} assayed clinical chemistry variables were determined in selected elderly and healthy ambulatory populations. The upper and lower limits (2. 5 and 97. 5 <b>fractiles)</b> were compared with the adult reference values in use in university hospitals of Switzerland. The results suggest that conventional adult reference values can be used for most variables in the elderly and that these values are also useful in an ambulatory population...|$|R
40|$|A general {{approach}} to the decision problem confronting the individual player in a noncooperative game is outlined and applied to one player's choice of bid in a competitive sealed auction for a valuable object. That player's Bayes-optimal bid is characterized under both general and specific assumptions as to the subjective probability distributions expressing his judgments. Under the specific assumptions, analytically tractable results (in terms of Gamma <b>fractiles)</b> are presented. Equilibrium and maximin bids are presented for comparative purposes. ...|$|R
40|$|The {{generalized}} bootstrap, {{introduced by}} Lo [1991] and further by Mason and Newton [1992] {{is applied to}} survey sampling. The idea is to choose adequate random weights (or resampling plans) so as to imitate the initial fluctuations of the probability sampling, maybe {{taking into account the}} priorities of the survey statistician, to obtain an estimator of the distribution of the statistics. The method is applied to the construction of confidence intervals for means, ratios and <b>fractiles</b> of food consumption data from the household panel surveys of Secodip. ...|$|R
40|$|Abstract. This year {{celebrates the}} 50 th aniversary of <b>Fractile</b> Graphical Analy-sis {{proposed}} by Prashanta Chandra Mahalanobis (Mahalanobis, 1961) {{in a series}} of papers and seminars as a method for comparing two distributions controlling for the rank of a covariate through <b>fractile</b> groups. We revisit the technique of <b>fractile</b> graphical analysis with some historical perspectives. We propose a new non-parametric regression method called <b>Fractile</b> on Quantile Regression where we condition on the ranks of the covariate, and compare it with existing quan-tile regression techniques. We apply this method to compare mutual fund inow distributions after conditioning on returns...|$|E
40|$|Sengupta and Portillo-Campbell have {{recently}} proposed a <b>fractile</b> decision criterion for risk situations {{as an alternative}} to the expected value and E-V criteria [Sengupta, J. K., J. H. Portillo-Campbell. 1970. A <b>fractile</b> approach to linear programming under risk. Management Sci. 16 (5, January) 298 - 308. ]. The <b>fractile</b> model seems to add little to the current literature on risk programming models under normality assumptions except to provide a direct solution procedure for identification of a specific solution in a Baumol efficient E-L set. ...|$|E
40|$|Abstract — In this paper, {{possibilistic}} {{linear programming}} problems are investigated. After reviewing relations among conjunction and implication functions, necessity <b>fractile</b> optimization models with various implication functions {{are applied to}} the possibilistic linear problems. We show that the necessity <b>fractile</b> optimization models are reduced to semi-infinite linear programming problems. A simple numerical example is given to demonstrate the correctness of the result. The paper is concluded with some remarks for further developments...|$|E
40|$|International audienceThe {{empirical}} Green's functions (EGF) {{technique is}} used to investigate two methods for predicting ground motion in a sedimentary basin for a future earthquake, including variability assessment. This study focuses on the Grenoble basin (French Alps). The basic principle of both methods is to generate a variety of source parameter sets based on a grid testing approach. Next, these sets are used to compute a population of ground motions {{by means of a}} kinematic EGF method and to estimate ground-motion variability. The first method tried, called the direct-parameter-input approach, selects input parameter combinations from assumed source parameter probability density functions. It is demonstrated that this approach leads to overestimated variability. Moreover, these simulation results are not calibrated. A new (screened-parameter-input) procedure is therefore proposed: (1) reference rock site response spectra are simulated for <b>fractiles</b> of several orders using empirical ground-motion prediction equations; (2) a large population of rock site response spectra is generated by means of the EGF method with varying rupture parameter combinations; (3) the spectra that do not fit the empirical motion for the chosen <b>fractiles</b> are screened out and sets of permissible source parameter combinations are thus obtained; (4) sediment site response spectra are computed with this EGF procedure and with these permissible parameter combinations; and (5) for each frequency median spectral acceleration and standarddeviation values are derived...|$|R
40|$|The {{success of}} on agents {{realistic}} probability assessment for an unknown quantity are greatly enhanced if a pre-stated interval is evaluated, rather then {{produced by the}} same agent (Hansson, Winman and Juslin, 2004). In order to explain this format dependence effect we have developed {{what we call a}} Naive Sampling Model (here after NSM) (Juslin, Winman and Hansson, 2004). The NSM assumes that a Subjective Probability Distribution for an unknown quantity is assessed by a retrieval of similar objects from memory which provide a sample distribution. This sample distribution is directly taken as an estimate of the corresponding population distribution. With interval production the sample dispersion is interpreted as an estimate of the population dispersion, with the <b>fractiles</b> in the distribution defining the upper an...|$|R
40|$|ABSTRACT- We {{propose a}} new {{approach}} to perform reliable seismic ground motion predictions for a future earthquake: (1) we generate a large population of response spectra on rock site (the reference site) by using a kinematic EGFs method and varying the future event rupture parameters; (2) we also simulate response spectra on reference site at several <b>fractiles,</b> using ground-motion prediction equations of Ambraseys et al. (2005) corrected according to the site-specific rock conditions; (3) we perform an inversion process to select the EGFs simulated response spectra corresponding to these fractiles; (4) resulting EGFs summation schemes are applied to sediment site recorded EGFs. We finally obtain wedged medium ground motion on specific sediment sites and a standard deviation in agreement with the ground-motion prediction equations of Ambraseys et al. (2005) ...|$|R
40|$|In the {{classical}} newsvendor model, when demand {{is represented by}} the normal distribution singly truncated at point zero, the standard optimality condition does not hold. Particularly, we show that the probability not to have stock-out during the period is always greater than the critical <b>fractile</b> which depends upon the overage and the underage costs. For this probability we derive the range of its values. Writing the safety stock coefficient as a quantile function of both the critical <b>fractile</b> and the coefficient of variation we obtain appropriate formulae for the optimal order quantity and the maximum expected profit. These formulae enable us to study the changes of the two target inventory measures when the coefficient of variation increases. For the optimal order quantity, the changes are studied for different values of the critical <b>fractile.</b> For the maximum expected profit, its changes are examined for different combinations of the critical <b>fractile</b> and the loss of goodwill. The range of values for the loss of goodwill ensures that maximum expected profits are positive. The sizes of the relative approximation error which result in by using the normal distribution to compute the optimal order quantity and the maximum expected profit are also investigated. This investigation is extended to different values of the critical <b>fractile</b> and the loss of goodwill. The results indicate that it is naïve to suggest for the coefficient of variation a maximum flat value under which the normal distribution approximates well the target inventory measures. ...|$|E
40|$|The {{concept of}} <b>Fractile</b> Graphical Analysis (FGA) was {{introduced}} by Prasanta Chandra Mahalanobis (see Mahalanobis, 1960). It {{is one of the}} earliest nonparametric regression techniques to compare two regression functions for two bivariate populations (X, Y). This method is particularly useful for comparing two regression functions where the covariate (X) forthetwo populations are not necessarily on comparable scales. For instance, in econometric studies, the prices of commodities and people’s incomes observed at different time points may not be on comparable scales due to inflation. In this paper, we consider a smooth estimate of the <b>fractile</b> regression function and study its statistical properties. We prove the consistency and asymptotic normality of the estimated <b>fractile</b> regression function defined through general weight functions. We also investigate some procedures based on the idea of resampling to test the equality of the <b>fractile</b> regression functions for two different populations. These methods are applied to some real data sets obtained from the Reserve Bank of India, and this leads to some interesting and useful observations. In course of our investigation, we review many of Mahalanobis ’ original ideas relating to FGA vis a vis some of the key ideas used in nonparametric kernel regression. AMS (2000) subject classification. Primary 62 G 08; secondary 62 G 09, 62 G 10...|$|E
40|$|Mobility of top incomes {{matters for}} both the {{openness}} of the income elite and the share of total income that this group receives. It is thus an important complement information to the growing snapshot literature on top income concentration. I use microlevel panel data of German income tax files that is highly representative for top income households. Top income mobility is assessed in four dimensions: (i) its stability over time, (ii) the degree of mobility between top income fractiles, (iii) the degree of mobility between equally sized groups and mobility in ranks, both of which do not depend on <b>fractile</b> sizes, and (iv) mobility's impact on distributional results. Mobility in terms of annual <b>fractile</b> changes is high between the richest top income fractiles, which is primarily due to tiny <b>fractile</b> sizes. When the fractiles' sizes are controlled for, top income recipients' mobility is {{lower than that of}} lower income tax units...|$|E
40|$|Abstract: The {{behaviour}} {{of individual}} {{movements in the}} wage distribution over time can be described by a Markov process. To investigate wage mobility in terms of transitions between quintiles in the wage distribution we apply a fixed effects panel estimation method suggested by Honorè and Kyriazidou (2000). This method of mobility measurement is robust to data contamination like all methods that treat <b>fractiles.</b> Moreover it allows {{for the inclusion of}} exogenous variables that change over time. We apply the estimator to a set of individual data form the Austrian social security records and find that disregarding unobserved heterogeneity greatly underestimates wage mobility. Simulated earnings profiles show that women are less mobile than men and have a tendency to be stuck in {{the lower part of the}} wage distribution. ...|$|R
40|$|This paper {{reviews the}} methods for {{combining}} loads idealized as Poisson process. Hasofer's method for combining a Poisson square wave load with a Poisson impulsive load is {{extended to the}} case of the combination of one square wave load with an arbitrary number of impulsive loads. The problem of considering the effects of dead load is addressed. Both the instantaneous and the largest value in structural life time statistics of a combination are determined. The following load combinations are considered in the applications: (1) dead and live load on floors; (2) dead and wind load; (3) a combination involving four loads. As the percentage of dead load with respect to the sum af all load aveages decreases, it is found that all <b>fractiles</b> of the largest value distribution increase, even considerably...|$|R
40|$|The {{behaviour}} {{of individual}} {{movements in the}} wage distribution over time can be described by a Markov process. To investigate wage mobility in terms of transitions between quintiles in the wage distribution we apply a fixed effects panel estimation method suggested by Honor 8 and Kyriazidou (2000). This method of mobility measurement is robust to data contamination like all methods that treat <b>fractiles.</b> Moreover it allows {{for the inclusion of}} exogenous variables that change over time. We apply the estimator to a set of individual data form the Austrian social security records and find that disregarding unobserved heterogeneity greatly underestimates wage mobility. Simulated earnings profiles show that women are less mobile than men and have a tendency to be stuck in {{the lower part of the}} wage distribution...|$|R
40|$|Abstract—A fractal array is {{an antenna}} array which holds a {{property}} called “self-similarity”. This means that {{parts of the}} whole structure {{are similar to the}} whole. A recursive procedure for evaluating the impedance matrix is allowed primarily by exploiting the self-similarity. However, numerous fractal arrays are extremely complicated in structure. Therefore, for these arrays, it is extremely elaborate to formulate explicitly a recursive relation. This paper proposes a simple procedure for evaluating, without formulating explicitly a recursive relation, the impedance matrix of fractal and <b>fractile</b> arrays; a <b>fractile</b> array is any array with a fractal boundary contour that tiles the plane without gaps or overlaps. 1...|$|E
40|$|We propose an {{interactive}} fuzzy decision making method for multiobjective fuzzy random linear programming problems through <b>fractile</b> criteria optimization. In the proposed method, {{it is assumed}} that the decision maker has fuzzy goals for not only objective functions but also permissible probability levels in a <b>fractile</b> optimization model, and such fuzzy goals are quantified by eliciting the corresponding membership functions. Using the fuzzy decision, such two kinds of membership functions are integrated. In the integrated membership space, the satisfactory solution is obtained from among an extended Pareto optimal solution set through the interaction with the decision maker. An illustrative numerical example is provided to demonstrate the feasibility and efficiency of the proposed method...|$|E
40|$|Various {{approaches}} {{have been offered}} as aids for assessing subjective probability distributions. These include the <b>fractile</b> method, the method of relative heights and applications of psychometric methods, e. g. multi-dimensional scaling. Previous laboratory research has examined and compared these methods primarily using undergraduate students rather than relatively experienced or practising managers. In this study of the <b>fractile</b> method, the experimental subjects were managers who had an ongoing instructional relationship with the authors. Therefore, conclusions can be drawn about practising managers 2 ̆ 7 biases in probability assessment. Groups with more expertise in statistics and more managerial experience {{were found to be}} better calibrated than those with lower expertise and less experience. Training was also found to improve assessment performance...|$|E
40|$|Intervals {{are used}} to {{represent}} imprecise numerical values. Modelling uncertain values with precise bounds without considering their probability distribution is infeasible in many applications. As a solution, this paper proposes the use of probability density functions instead of intervals# we consider evaluation of an arithmetical function of random variables. Since the result density cannot in general be solved algebraically, an interval method for determining its guaranteed bounds is developed. This possibility challenges traditional Monte Carlo methods in which only stochastic characterizations for the result distribution, such as confidence bounds for <b>fractiles,</b> can be determined. 0 Introduction An interval is a range of possible values but says nothing about the probabilitydistribution. In many applications such information is essential. For example, the manufacturing tolerance of a resistance R is not an interval but rather a truncated normal distribution. This means [...] ...|$|R
40|$|This article {{deals with}} the compact {{representation}} of incomplete probabilistic knowledge which can be encountered in risk evaluation problems, for instance in environmental studies. Various kinds of knowledge are considered such as expert opinions about characteristics of distributions or poor statistical information. Our approach is based on probability families encoded by possibility distributions and belief functions. In each case, a technique for representing the available imprecise probabilistic information faithfully is proposed, using different uncertainty frameworks (possibility theory, probability theory, belief functions [...] .). Moreover the use of probability-possibility transformations enables confidence intervals to be encompassed by cuts of possibility distributions, thus making the representation stronger. The respective appropriateness of pairs of cumulative distributions, continuous possibility distributions or discrete random sets for representing information about the mean value, the mode, the median and other <b>fractiles</b> of ill-known probability distributions is discussed in detail...|$|R
40|$|Worldwide {{research}} has now reached {{a level of}} integration where an effort towards the harmonization of procedures is absolutely needed. Such harmonization may regard, for example, the various steps {{that lead to the}} definition of capacity models to be included in design codes, specifically: definition of the test setup, quantities to be measured, identification of the basic variables influencing the phenomenon, distinction between average values and other <b>fractiles,</b> disaggregation of the model in different parts accounting for mechanics, fine-tuning and randomnesses, and, finally, assessment of the model against the experimental results. Test results and ensuing model developed according to this procedure would naturally lend themselves to be easily shared among the scientific community and would facilitate the task of calibrating the partial coefficients, with the ambitious aim of attaining a uniform reliability level among all capacity equations. © 2008 Elsevier Ltd. All rights reserved...|$|R
