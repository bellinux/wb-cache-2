6|10000|Public
50|$|For his {{daughter}} Yehoishema's dowry, Ananiah had transferred to her partial {{ownership of the}} house he shared with Tamut. After making more repairs to the building, Ananiah transferred a further section of the house, described in this document, to the dowry. <b>Image</b> <b>of</b> <b>document</b> in gallery.|$|E
5000|$|Three {{years after}} {{purchasing}} {{the house from}} Bagazust and Ubil, Ananiah transferred ownership of an apartment within the now renovated house to his wife, Tamut. Although Tamut thereafter owned the apartment, Ananiah required that at her death it pass to their children, Palti and Yehoishema. As with all property transfers within a family, this gift was described as made [...] "in love". <b>Image</b> <b>of</b> <b>document</b> in gallery.|$|E
50|$|Drawn up {{thirty years}} after the {{preceding}} papyrus, this document {{is one of several}} that gradually transferred ownership of Ananiah and Tamut's house to their daughter,Yehoishema, as payment on her dowry. The legal descriptions of the house preserve the names of Ananiah's neighbors. They included an Egyptian who held the post of gardener of the Egyptian god Khnum and, on the other side, two Persian boatmen. <b>Image</b> <b>of</b> <b>document</b> in gallery.|$|E
5000|$|Collections <b>of</b> microfilms <b>of</b> <b>documents</b> and {{collections}} <b>of</b> digital <b>images</b> <b>of</b> <b>documents</b> (13th - 20th centuries) ...|$|R
50|$|Close-up <b>images</b> <b>of</b> <b>documents</b> present anomalies, such as fisheye effect, glare, and darkened corners.|$|R
40|$|The Humberto Delgado’s Digital Archive {{is a way}} of {{spreading}} out, through the Internet, the digitalized <b>images</b> <b>of</b> <b>documents</b> that exist in portuguese and foreign archives, related with General Humberto Delgado, who was an important portuguese figure in the 20 th century. The search engine of the Humberto Delgado Digital Archive allows a multiple combination of search strategies and an easy access to more than 45 000 digitalized <b>images</b> <b>of</b> <b>documents</b> and to their description...|$|R
50|$|This papyrus {{records the}} sale of the {{remaining}} portion of Ananiah and Tamut's house to Yehoishema's husband. Possibly because the clients were dissatisfied with something the scribe had written, at one point the text of the document breaks off and then starts over again, repeating what has gone on before with some additions. The boundary description included here refers to the Temple of Yahou in Elephantine, now rebuilt eight years after its destruction in 410 BCE during a civil war conflict that arose out of a land dispute. <b>Image</b> <b>of</b> <b>document</b> in gallery below.|$|E
50|$|Sometime in December 402 BCE, Ananiah son of Haggai {{borrowed}} two monthly rations {{of grain}} from Pakhnum son of Besa, an Aramaean with an Egyptian name. This receipt {{would have been}} held by Pakhnum and returned to Ananiah son of Haggai when he repaid the loan. No interest is charged {{but there is a}} penalty for failing to repay the loan by the agreed date. The receipt demonstrates that friendly business relations continued between Egyptians and Jews in Elephantine after the expulsion of the Persians by pharaoh Amyrtaios of the 28th Dynasty. <b>Image</b> <b>of</b> <b>document</b> is in gallery below.|$|E
40|$|Extraction of text {{areas is}} a {{necessary}} first step for taking a complex document image for character recognition task. In digital libraries, such OCR'ed text facilitates access to the <b>image</b> <b>of</b> <b>document</b> page through keyword search. Gabor filters, known to be simulating certain characteristics of the Human Visual System (HVS), have been employed for this task by {{a large number of}} scientists, in scanned document images. Adapting such a scheme for camera based document images is a relatively new approach. Moreover, design of the appropriate filters to separate text areas, which are assumed to be rich in high frequency components, from non-text areas is a difficult task. The difficulty increases if the clutter is also rich in high frequency components. Other reported works, on separating text from non-text areas, have used geometrical/structural information like shape and size of the regions in binarized document images. In this work, we have used a combination of the above mentioned approaches for the purpose. We have used connected component analysis (CCA), in binarized images, to segment non-text areas based on the size information of the connected regions. A Gabor function based filter bank is used to separate the text and the non-text areas of comparable size. The technique is shown to work efficiently on different kinds of scanned document images, camera captured document images and sometimes on scenic images. Key Words: Gabor filter, connected component analysis, document image, multi-channel filtering...|$|E
2500|$|<b>Images</b> <b>of</b> {{original}} <b>documents</b> alongside transcribed, critically edited versions ...|$|R
40|$|This paper {{presents}} a scheme for generating paper texture <b>of</b> historical <b>documents.</b> A new entropy based segmentation algorithm {{is used to}} decompose the <b>image</b> <b>of</b> <b>documents</b> into the <b>image</b> <b>of</b> the paper background and the printing <b>of</b> the <b>document.</b> Statistical analysis allows filling in the gaps from the printing, yielding a blank sheet of paper with similar texture to the original document. 1...|$|R
5000|$|Make {{electronic}} <b>images</b> <b>of</b> printed <b>documents</b> searchable, e.g. Google Books ...|$|R
40|$|This paper {{presents}} a new entropy-based segmentation algorithm for <b>images</b> <b>of</b> <b>documents.</b> The algorithm {{is used to}} eliminate the noise inherent to the paper itself specially in documents written on both sides. It generates good quality monochromatic images increasing the hit rate of OCR commercial tools. I...|$|R
40|$|Information {{retrieval}} is {{the task}} <b>of</b> finding <b>documents,</b> usually text, which are relevant to a user's information need. A conventional approach to information management <b>of</b> paper <b>documents</b> is normally based on classifying them into a hierarchical classification structure. More recently we have seen electronic document management systems which manage scanned <b>images</b> <b>of</b> <b>documents</b> {{in the same way}} as paper, or which do some OCR to determine machine-readable document content which may then be used for content-based information retrieval. In this paper we present an alternative to full-scale OCR <b>of</b> scanned <b>document</b> <b>images</b> in which words in scanned <b>images</b> <b>of</b> <b>documents</b> are represented as word shape tokens (WSTs) representing the approximate shape of words in text. We describe an approach to WST-based information retrieval that can be entirely automatic or can involve the user in refining the retrieval process. To measure the effectiveness of WSTbased retrieval we have implemented and refined [...] ...|$|R
50|$|Digital <b>images</b> <b>of</b> each <b>document</b> will {{be stored}} in each state DMV database.|$|R
40|$|The problem <b>of</b> <b>document</b> {{categorization}} is considered. The set of domains and the keywords specific {{for these}} domains {{is supposed to}} be selected beforehand as initial data. We apply the well-known statistical hypothesis test that considers <b>images</b> <b>of</b> <b>documents</b> and domains as normalized vectors. In comparison with existing methods, such approach allows to take into account a random character of initial data. The classifier is developed in the framework <b>of</b> <b>Document</b> Investigator software package...|$|R
40|$|Abstract: “Back-to-front interference”, “bleeding ” and “show-through ” is {{the name}} given to the {{phenomenon}} found whenever documents are written {{on both sides of}} translucent paper and the print of one side is visible on the other one. The binarization <b>of</b> <b>documents</b> with back-tofront interference with standard algorithms yields unreadable documents. This paper presents a fast entropy-based segmentation method for generating high-quality binarized <b>images</b> <b>of</b> <b>documents</b> with back-to-front interference...|$|R
5000|$|Multi Document Interface - The {{multiple}} {{document interface}} menu contains small preview <b>images</b> <b>of</b> the <b>documents</b> currently open ...|$|R
40|$|In {{this paper}} we have {{presented}} a scheme for transcoding document images for presentation on handheld devices like PDA’s, e-books etc. We have proposed technqiues suitable, in particular,for <b>images</b> <b>of</b> <b>documents</b> <b>of</b> Indian languages having Devanagari based scripts (viz. Hindi, Marathi, Bengali, Assamese, etc). Appropriate compression scheme for textual component <b>of</b> <b>document</b> <b>images</b> exploiting script specific characteristics has been suggested. We have also explored {{use of the}} knowledge <b>of</b> the <b>document</b> model represented through standard ontology language for generation <b>of</b> <b>document</b> summary. An experimented system has been developed for validation of these schemes. 1...|$|R
40|$|Digital {{libraries}} {{are increasingly}} based on digital page <b>images</b> <b>of</b> <b>documents,</b> but techniques for constructing usable versions <b>of</b> these page <b>images</b> are still largely folklore. This paper documents some issues encountered in creating document icons, page thumbnails, and page images for the Up-Lib digital library system, and suggests answers {{for each of}} them, based on both problem analysis and user feedback. In particular, several algorithms for determining useful sizes <b>of</b> both <b>document</b> icons and large page images are discussed...|$|R
50|$|The National Archives website {{provides}} {{information about the}} archives and catalog data which allow the holdings to be searched online. The site offers English and Japanese versions, the holdings themselves are, of course, mainly in Japanese. The website facilitates access to brief descriptions and some <b>images</b> <b>of</b> <b>documents,</b> books, and cultural properties. The Digital Gallery may be searched using keywords or various categories, opening access to digitised <b>images</b> <b>of</b> scrolls; maps; photographs; drawings; posters; and <b>documents.</b> English summaries <b>of</b> publications from the National Archives (journal and annual report) are available for downloading from the site.|$|R
40|$|Post {{processing}} uses mathematical morphologies {{to improve}} the quality <b>of</b> Binary <b>image</b> <b>of</b> a <b>Document.</b> It is observed that when a document is written using a ball point pen or when there is a problem in toner of printer then in scanned copy <b>of</b> such <b>documents</b> there are some minimal stroke gaps which are not easily visible to human eye. But a machine can easily recognise these gaps. These gaps lower the quality <b>of</b> binary <b>image</b> <b>of</b> a <b>document.</b> Post processing very important for improve the accuracy of binarization <b>of</b> degraded <b>documents.</b> In this paper a mathematical technique is discussed {{to improve the}} quality <b>of</b> binary <b>image...</b>|$|R
5000|$|There {{are several}} <b>images</b> <b>of</b> official-looking <b>documents,</b> copies <b>of</b> birth and {{marriage}} certificates etc., giving her birth year as 1909, {{on the website}} of the French Ministry of Culture.|$|R
40|$|<b>Images</b> <b>of</b> <b>document</b> pages have {{different}} characteristics than <b>images</b> <b>of</b> natural scenes, {{and so the}} sharpness measures developed for natural scene images do not necessarily extend to <b>document</b> <b>images</b> primarily composed <b>of</b> text. We present an efficient and simple method for effectively estimating the sharpness/blurriness <b>of</b> <b>document</b> <b>images</b> that also performs well on natural scenes. Our method {{can be used to}} predict the sharpness in scenarios where images are blurred due to camera-motion (or hand-shake), defocus, or inherent properties of the imaging system. The proposed method outperforms the perceptually-based, no-reference sharpness work of [1] and [4], which was shown to perform better than 14 other no-reference sharpness measures on the LIVE dataset. 1...|$|R
40|$|We {{present a}} {{generative}} probabilistic model, inspired by historical printing processes, for transcribing <b>images</b> <b>of</b> <b>documents</b> from {{the printing press}} era. By jointly modeling the text <b>of</b> the <b>document</b> and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31 % relative reduction in word error rate over the leading commercial system for historical transcription, and a 47 % relative reduction over Tesseract, Google’s open source OCR system. ...|$|R
40|$|Abstract: In {{this paper}} {{we present a}} {{database}} for the research of Arabic off-line and on-line handwriting optical recognition {{as well as for}} machine printed text optical recognition. Digital <b>images</b> <b>of</b> <b>documents,</b> text phrases, words/sub-words, isolated characters, digits, signatures, soon are and included in ARABASE. Data corresponds to a variety of lexes (cities names, literal amounts, isolated characters, digits, free texts, etc.). The database organization offers interesting commodities to be explored via an Arabic writing recognition system. A useful tool enables the user, via a graphical interface to experiment different classical tasks <b>of</b> <b>image</b> processing...|$|R
40|$|Extracting and {{archiving}} {{information from}} digital <b>images</b> <b>of</b> <b>documents</b> is one <b>of</b> {{the goals of}} the project AMMIRA (multispectral acquisition, enhancing, indexing and retrieval of artifacts), led by Tea- Sas, a service firm based in Catanzaro, Italy, with the collaboration of two Italian research teams, the Institute of Information Science and Technologies of CNR in Pisa, and the Department of Mechanical Engineering of the University of Calabria in Cosenza. AMMIRA is supported by European funding, through the Italian regional program for integrated support to enterprises...|$|R
40|$|Even {{though the}} digital {{processing}} <b>of</b> <b>documents</b> is increasingly widespread in industry, printed documents are still largely in use. In order to process electronically the contents <b>of</b> printed <b>documents,</b> information must be extracted from digital <b>images</b> <b>of</b> <b>documents.</b> When dealing with complex documents, {{in which the}} contents of different regions and fields can be highly heterogeneous with respect to layout, printing quality and the utilization of fonts and typing standards, {{the reconstruction of the}} contents <b>of</b> <b>documents</b> from digital <b>images</b> can be a difficult problem. In the present article we present an efficient solution for this problem, in which the semantic contents of fields in a complex document are extracted from a digital image. Opus SoftwareOpus Softwar...|$|R
40|$|This paper {{presents}} {{a study on}} thresholding algorithms applied to <b>images</b> <b>of</b> historical <b>documents.</b> Thresholding methods are used to create monochromatic <b>images.</b> The generation <b>of</b> bi-level <b>images</b> <b>of</b> this kind <b>of</b> <b>documents</b> increases the compression rate and make them more easily available for Internet download. We also introduce a new thresholding algorithm which achieves high quality monochromatic images in some extreme situations. 1...|$|R
40|$|At the Institute for Cultural Interaction Studies, Kansai University, {{there is}} an ongoing project for the {{creation}} of a database/archive for Chinese language related documents by multiple groups in which this author is involved. This essay introduces the process and methodology for the creation <b>of</b> digital <b>images</b> <b>of</b> <b>documents</b> as well as the creation of a database/archive. It also investigates recent problems with the creation of databases, the collection of documentary materials on modern China, and the need to consider other issues outside of the creation of a database...|$|R
5000|$|FamilySearch Indexing is a {{volunteer}} project established {{and run by}} FamilySearch, a genealogy organization of The Church of Jesus Christ of Latter-day Saints. The project aims to create searchable digital indexes for scanned <b>images</b> <b>of</b> historical <b>documents.</b> The documents are drawn primarily from a collection of over a billion photographic <b>images</b> <b>of</b> historical <b>documents</b> from 110 countries and principalities. They include census records, birth and death certificates, marriage licenses, military and property records, and other vital records maintained by local, state, and national governments. However, to access the billions of names that appear on these images, indexes are needed {{to be able to}} search them efficiently.|$|R
40|$|The use <b>of</b> digital <b>images</b> <b>of</b> {{handwritten}} historical <b>documents</b> {{has become}} more popular in recent years. Volunteers around the world now read thousands <b>of</b> these <b>images</b> as part <b>of</b> their indexing process. Handwritten text <b>images</b> <b>of</b> old <b>documents</b> are sometimes difficult to read or noisy due to the preservation <b>of</b> the <b>document</b> and quality <b>of</b> the <b>image.</b> In this paper, we present a technique that allows interactive smoothing and text reconstruction. This makes parts <b>of</b> the <b>image</b> where the text ink is hard to recognize clearer and visually better. Results show that this technique helps selectively reduce the noise <b>of</b> the <b>image</b> and makes the text more readable...|$|R
40|$|The aim of {{this paper}} is to present the {{experience}} of the organizations in the government of the Sultanate of Oman in archiving their documents. It started by defining the term “archives” and types documents according to their use. The second part of the paper presents the experiences of the Ministry of Housing and Electricity and Water in archiving their documents. It stated the electronic record management systems in the ministry concerning housing division. The paper concentrated on the Real State documents archiving starting from traditional archiving to the archiving <b>of</b> digital <b>images</b> <b>of</b> <b>documents</b> – fully electronic archiving...|$|R
50|$|Zooming {{features}} were an early feature of fractal-generating software so that users could magnify an area to reveal more detail. When data entry operators enter data from digital <b>images</b> <b>of</b> scanned <b>documents</b> a zoom user interface {{is used to}} discern handwriting.|$|R
40|$|International audienceIn this article, {{we propose}} a method <b>of</b> {{characterization}} <b>of</b> <b>images</b> <b>of</b> old <b>documents</b> {{based on a}} texture approach. This characterization is carried out {{with the help of}} a multi-resolution study of the textures contained in the <b>images</b> <b>of</b> the <b>document.</b> Thus, by extracting five features linked to the frequencies and to the orientations in the different areas of a page, it is possible to extract and compare elements of high semantic level without expressing any hypothesis about the physical or logical structure <b>of</b> the analyzed <b>documents.</b> Experimentation based on segmentation, data analysis and document image retrieval tools demonstrate the performance of our propositions and the advances that they represent in terms of characterization of content of a deeply heterogeneous corpus...|$|R
50|$|A Collection <b>of</b> <b>images</b> and <b>documents</b> {{available}} through the Iowa Digital Library.|$|R
