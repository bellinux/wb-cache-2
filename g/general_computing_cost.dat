0|3710|Public
50|$|Methods for {{determining}} when costs are recognized in <b>computing</b> <b>cost</b> of goods sold {{or to be}} sold.|$|R
40|$|Abstract: I {{investigate}} the <b>computing</b> <b>costs</b> of imaging for an SD+FPA telescope {{such as the}} xNTD or KAT by conducting simulations of continuum and spectral line imaging. I find that the spectral line imaging costs dominate by many orders of magnitude. For antennas with equatorial mounts, the computing load is about 20 TFlops. With Moore’s Law and possible algorithmic improvements, a plausible hardware cost lies in the range A$ 2 M to A$ 4 M (2009). Equatorial mounts are cost effective in controlling <b>computing</b> <b>costs</b> since with current algorithms the processing for alt-azimuth {{is at least a}} factor of 3 – 4 more expensive. <b>Computing</b> <b>costs</b> for a Big Gulp solution are half those of the SD+FPA with equatorial mounts. 1...|$|R
40|$|In {{recent years}} {{development}} {{in the area of}} Single Board Computing has been advancing rapidly. At Wolters Kluwer's Corporate Legal Services Division a prototyping effort was undertaken to establish the utility of such devices for practical and <b>general</b> <b>computing</b> needs. This paper presents the background of this work, the design and construction of a 64 core 96 GHz cluster, and their possibility of yielding approximately 400 GFLOPs from a set of small footprint InSignal boards created for just over $ 2, 300. Additionally this paper discusses the software environment on the cluster, the use of a standard Beowulf library and its operation, as well as other software application uses including Elastic Search and ownCloud. Finally, consideration will be given to the future use of such technologies in a business setting in order to introduce new Open Source technologies, reduce <b>computing</b> <b>costs,</b> and improve Time to Market. Index Terms: Single Board Computing, Raspberry Pi, InSignal Exynos 5420, Linaro Ubuntu Linux, High Performance Computing, Beowulf clustering, Open Source, MySQL, MongoDB, ownCloud, Computing Architectures, Parallel Computing, Cluster ComputingComment: 9 Figure...|$|R
5000|$|<b>General</b> <b>Computing,</b> various - {{articles}} about no specific topic ...|$|R
5000|$|ALGOL, as ALGOL-10 v10B, a {{compiler}} {{used for}} <b>general</b> <b>computing</b> ...|$|R
5000|$|BASIC, as BASIC-10 v17F, an {{interpreter}} used for <b>general</b> <b>computing</b> ...|$|R
5000|$|... 3 Atmel ATmega328P {{microcontrollers}} - for video, USB and <b>general</b> <b>computing</b> ...|$|R
30|$|There is {{no clear}} {{advantage}} of <b>computing</b> the <b>cost</b> over a future time window, compared with <b>computing</b> the <b>cost</b> only at the next scheduling time.|$|R
40|$|Embedded control {{networks}} commonly use checksums {{to detect}} data transmission errors. However, design decisions about which checksum to use are {{difficult because of}} a lack of information about the relative effectiveness of available options. We study the error detection effectiveness of the following commonly used checksum computations for embedded networks: exclusive or (XOR), two’s complement addition, one’s complement addition, Fletcher checksum, Adler checksum, and cyclic redundancy codes (CRC). A study of error detection capabilities for random independent bit errors and burst errors reveals that XOR, two’s complement addition, and Adler checksums are suboptimal for typical application use. Instead, one’s complement addition should be used for applications willing to sacrifice error detection effectiveness to reduce <b>compute</b> <b>cost,</b> Fletcher checksum for applications looking for a balance of error detection and <b>compute</b> <b>cost,</b> and CRCs for applications willing to pay a higher <b>compute</b> <b>cost</b> for further improved error detection...|$|R
40|$|In {{this paper}} we study several fixed step and {{adaptive}} Runge-Kutta methods suitable for transporting track parameters through an inhomogeneous magnetic field. Moreover, {{we present a}} new adaptive Runge-Kutta-Nystrom method which estimates the local error of the extrapolation without introducing extra stages to the original Runge-Kutta-Nystrom method. Furthermore, these methods are compared for propagation accuracy and <b>computing</b> <b>cost</b> efficiency in the simultaneous track and error propagation (STEP) algorithm of the common ATLAS tracking software. The tests show the new adaptive Runge-Kutta-Nystrom method {{to be the most}} <b>computing</b> <b>cost</b> efficient...|$|R
40|$|Abstract. Relationship-based {{access control}} (ReBAC) {{originated}} {{in the context of}} social network systems and recently is being generalized to be suitable for <b>general</b> <b>computing</b> systems. This paper defines a ReBAC model, based on Crampton and Sellwood’s RPPM model, designed to be suitable for <b>general</b> <b>computing</b> systems. Our ReBAC model includes a comprehensive administrative model. The administrative model is com-prehensive {{in the sense that it}} allows and controls changes to all aspects of the ReBAC policy. To the best of our knowledge, it is the first com-prehensive administrative model for a ReBAC model suitable for <b>general</b> <b>computing</b> systems. The model is illustrated with parts of a sample ac-cess control policy for electronic health records in a healthcare network. ...|$|R
5000|$|... #Caption: An artistic {{representation}} of a Turing machine. Turing machines are used to model <b>general</b> <b>computing</b> devices.|$|R
40|$|Open Access. Astronomical wide-field imaging of {{interferometric}} radio data is computationally expensive, {{especially for}} the large data volumes created by modern non-coplanar many-element arrays. We present a new wide-field interferometric imager that uses the w-stacking algorithm and can {{make use of the}} w-snapshot algorithm. The performance dependences of casa's w-projection and our new imager are analysed and analytical functions are derived that describe the required <b>computing</b> <b>cost</b> for both imagers. On data from the Murchison Widefield Array, we find our new method to be an order of magnitude faster than w-projection, as well as being capable of full-sky imaging at full resolution and with correct polarization correction. We predict the <b>computing</b> <b>costs</b> for several other arrays and estimate that our imager is a factor of 2 – 12 faster, depending on the array configuration. We estimate the <b>computing</b> <b>cost</b> for imaging the low-frequency Square Kilometre Array observations to be 60 PetaFLOPS with current techniques. We find that combining w-stacking with the w-snapshot algorithm does not significantly improve computing requirements over pure w-stacking. The source code of our new imager is publicly released...|$|R
50|$|The AICC {{was formed}} in 1988 by Aircraft {{manufacturers}} (Boeing, Airbus, and McDonnell Douglas) to address Airline concerns about non-standard <b>computing</b> (<b>cost)</b> issues arising from the proliferation of new multimedia training materials emerging at that time.|$|R
30|$|The pBYY {{implementation}} {{repeats the}} Ying step and the Yang step alternatively. It {{gets out of}} the repeating circle in two cases. One is that learning is finally completed as the repeating circle converges with an unchanged k. The other is after trimming one Gaussian component with k reducing by 1, after which it goes to the line of initialization and start a new repeating circle. This re-initialization is helpful to avoid accumulation of estimating bias, though it requires extra <b>computing</b> <b>costs.</b> Whether we need this depending on a trade-off of <b>computing</b> <b>cost</b> versus estimating accuracy. We may remove this re-initialization by simply deleting the line ‘go to Initialization’.|$|R
40|$|Innovations are {{necessary}} to ride the predictable wave of change. Most of enterprises are striving to reduce their <b>computing</b> <b>cost</b> through the means of virtualization. This demand of reducing the <b>computing</b> <b>cost</b> {{has led to the}} innovation of Cloud Computing. Cloud Computing offers better computing through improved utilization and reduced administration and infrastructure <b>costs.</b> Cloud <b>Computing</b> is the sum of Software as a Service (SaaS) and Infrastructure as a service (IaaS) and Platform as a Service (PaaS). In this paper, I would first compare two related computing paradigms – On premise Computing and Cloud computing in. Discuss the importance of Cloud to Manufacturing Industry i...|$|R
50|$|Other {{more recent}} methods, {{such as the}} Stockmayer potential, {{describe}} the interaction of molecules more accurately. Quantum chemistry methods, Møller-Plesset perturbation theory, coupled cluster method, or full configuration interaction can give extremely accurate results, but require large <b>computing</b> <b>cost.</b>|$|R
50|$|Financial {{accounting}} aims {{at finding}} out results of accounting {{year in the}} form of Profit and Loss Account and Balance Sheet. Cost Accounting aims at <b>computing</b> <b>cost</b> of production/service in a scientific manner and facilitate cost control and cost reduction.|$|R
40|$|AbstractAccurate {{computation}} of multimode propagation in {{and radiation}} from a turbofan duct {{is important for}} aircraft noise prediction. A formulation of linearised Euler equations (LEE) is proposed for efficient computation. Its solutions are sought in time-domain using high-order computational aeroacoustic (CAA) algorithms. An advantage of the proposed method is {{that the number of}} computation is the cut-on mode number at the highest frequency regardless frequency sample interval. Furthermore the <b>computing</b> <b>cost</b> only increases marginally despite a doubling of the governing equations in real space. Tests show that the method could potentially achieve <b>computing</b> <b>cost</b> saving. Examples including a generic bypass engine duct are given to illustrate the reliability and efficiency of the method...|$|R
40|$|The {{search for}} {{continuous}} gravitational waves {{in a wide}} parameter space at fixed <b>computing</b> <b>cost</b> is most efficiently done with semicoherent methods, e. g. StackSlide, due to the prohibitive <b>computing</b> <b>cost</b> of the fully coherent search strategies. Prix&Shaltev arXiv: 1201. 4321 have developed a semi-analytic method for finding optimal StackSlide parameters at fixed <b>computing</b> <b>cost</b> under ideal data conditions, i. e. gap-less data and constant noise floor. In this work we consider more realistic conditions by allowing for gaps in the data and changes in noise level. We show how the sensitivity optimization can be decoupled from the data selection problem. To find optimal semicoherent search parameters we apply a numerical optimization using as example the semicoherent StackSlide search. We also describe three different data selection algorithms. Thus {{the outcome of the}} numerical optimization consists of the optimal search parameters and the selected dataset. We first test the numerical optimization procedure under ideal conditions and show that we can reproduce the results of the analytical method. Then we gradually relax the conditions on the data and find that a compact data selection algorithm yields higher sensitivity compared to a greedy data selection procedure. Comment: 14 pages, 6 figure...|$|R
50|$|The Finnish <b>general</b> <b>computing</b> {{magazine}} MikroBITTI {{awarded the}} game 91%. Its review praised the game's playability and mentioned voice {{acting as the}} only clearly negative feature.|$|R
40|$|Abstract. Database {{systems are}} {{constantly}} extending their application area towards more <b>general</b> <b>computing.</b> However, applications that combine database access and <b>general</b> <b>computing</b> still {{suffer from the}} conceptual and technical gap between relational algebra and procedural programming. In this paper, we show that procedural programs may be effectively represented in a Datalog-like language with functions and aggregates. Such a language may then {{be used as a}} common representation for both relational and procedural part of an application. procedural IR embedded-SQL procedural program parser relational algebra...|$|R
40|$|This work {{presents}} some <b>general</b> {{procedures for}} <b>computing</b> dissimilarities between {{elements of a}} database or, more generally, nodes of a weighted, undirected, graph. It {{is based on a}} Markov-chain model of random walk through the database. The model assigns transition probabilities to the links between elements, so that a random walker can jump from element to element. A quantity, called the average first-passage <b>cost,</b> <b>computes</b> the average <b>cost</b> incurred by a random walker for reaching element k for the first time when starting from element i...|$|R
3000|$|... is {{designed}} by the sampled data, which needs less information from the network outputs than the controller in [16, 17]. The sampled-data estimation approach {{can lead to a}} significant reduction of the information communication burden in the network and save more <b>computing</b> <b>cost.</b>|$|R
5000|$|... "Research, design, develop, {{and test}} {{operating}} systems-level software, compilers, and network distribution software for medical, industrial, military, communications, aerospace, business, scientific, and <b>general</b> <b>computing</b> applications"—Bureau of Labor Statistics ...|$|R
5000|$|P5 Accelerators - With the {{flattening}} of <b>general</b> <b>computing</b> performance, special architectures {{for addressing}} {{next level of}} performance will be investigated for specialized tasks like signal processing and analysis.|$|R
40|$|In {{this paper}} we develop a {{numerical}} method to compute high-order approximate solutions to Bellman's dynamic programming equation that arises in the optimal regulation of discrete-time nonlinear control systems. The method uses a patchy technique to build Taylor polynomial approximations defined on small domains {{which are then}} patched {{together to create a}} piecewise-smooth approximation. Using the values of the <b>computed</b> <b>cost</b> function as the step-size, levels of patches are constructed such that their radial boundaries are level sets of the <b>computed</b> <b>cost</b> functions and their lateral boundaries are invariants sets of the closed loop dynamics. To minimize the computational effort, an adaptive scheme is used to determine the number of patches on each level depending on the relative error of the computed solutions...|$|R
40|$|It can {{be easily}} {{understood}} that carrying out a complete optimization (whether multidisciplinary or not) with a highly refined model can rapidly lead to prohibitive <b>computing</b> <b>costs.</b> Conversely, if a relatively coarse model is used, an optimum can still be obtained, but with little confidence in the results. Thus, it appears to necessary {{to be able to}} vary the degree of refinement of a model throughout the optimization process in order to make this process both accurate and affordable in terms of <b>computing</b> <b>cost.</b> However, such a multilevel model optimization process does raise numerous questions. we will try to answer these questions and obtain a global picture of the main multilevel model optimization approaches available. This will enable us to propose original and innovative multilevel methods...|$|R
40|$|This {{article has}} been {{accepted}} for publication in Monthly Notices of the Royal Astronomical Society. ?? 2014 The Authors. Published by Oxford University Press {{on behalf of the}} Royal Astronomical Society. Astronomical widefield imaging of interferometric radio data is computationally expensive, especially for the large data volumes created by modern non-coplanar many-element arrays. We present a new widefield interferometric imager that uses the w-stacking algorithm and can make use of the w-snapshot algorithm. The performance dependencies of CASA's w-projection and our new imager are analysed and analytical functions are derived that describe the required <b>computing</b> <b>cost</b> for both imagers. On data from the Murchison Widefield Array, we find our new method to be an order of magnitude faster than w-projection, as well as being capable of full-sky imaging at full resolution and with correct polarisation correction. We predict the <b>computing</b> <b>costs</b> for several other arrays and estimate that our imager is a factor of 2 - 12 faster, depending on the array configuration. We estimate the <b>computing</b> <b>cost</b> for imaging the low-frequency Square-Kilometre Array observations to be 60 PetaFLOPS with current techniques. We find that combining w-stacking with the w-snapshot algorithm does not significantly improve computing requirements over pure w-stacking. The source code of our new imager is publicly released...|$|R
3000|$|CI is {{the total}} holding <b>costs</b> <b>computed</b> as the <b>costs</b> Ch per average number of parts in buffer storages; [...]...|$|R
50|$|The Beginner's Guide to Computers is a {{book about}} {{microcomputers}} and <b>general</b> <b>computing.</b> It was published in 1982 as an accompaniment to the BBC Computer Literacy Project and The Computer Programme.|$|R
3000|$|CMP is {{the total}} costs of {{preventive}} maintenance <b>costs</b> <b>computed</b> as the <b>cost</b> Cp per number of preventive maintenance actions; [...]...|$|R
40|$|Intel {{manufacturing}} {{relies heavily}} on IT and Factory Automation during the manufacturing processes. At Intel, everything from scheduling products {{on the floor and}} product delivery systems to statistical process control is done through automation systems. Shortly after an Intel meeting described in the case, a new position <b>Computing</b> <b>Cost</b> Reduction Manager - was created to lead a team within Factory Automation to drive cost reduction efforts which was a top priority for Intel in 2003. The <b>computing</b> <b>cost</b> reduction team s task was to come up with specific recommendations on how to achieve the cost goals established by and report out on a strategy in the next two weeks. In the case, the organization and business processes are examined and enough information is given to provide recommendations for cost reductio...|$|R
40|$|This study {{compares the}} {{performances}} of three numerical approaches (Lagrangian, ALE and control volume) for modelling the response of a short cylindrical pipe representing {{a portion of the}} intestines subjected to large and rapid compressions. While not being able to simulate sustained fluid flow, the Lagrangian approach provided similar results as the ALE for moderate levels of compression. However, it was the stiffest approach for larger levels, and had numerical issues for extreme compressions. While the ALE did not have these issues, its <b>computing</b> <b>cost</b> was very high, which would be problematic for large models. The control volume approach had the lowest <b>computing</b> <b>cost</b> and seemed promising for larger compressions. However, its response was the softest and further investigations are needed to define its dependency to modelling parameters...|$|R
50|$|He is {{an outspoken}} {{advocate}} of better {{education of the}} <b>general</b> <b>computing</b> population about floating-point issues, and regularly denounces decisions {{in the design of}} computers and programming languages that may impair good floating-point computations.|$|R
50|$|As {{its name}} suggests, Focused D* is an {{extension}} of D* which uses a heuristic to focus the propagation of RAISE and LOWER toward the robot. In this way, only the states that matter are updated, {{in the same way that}} A* only <b>computes</b> <b>costs</b> for some of the nodes.|$|R
