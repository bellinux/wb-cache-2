4678|8557|Public
25|$|If a <b>data</b> <b>distribution</b> is {{approximately}} normal then about 68 {{percent of the}} data values are within one standard deviation of the mean (mathematically, μ±σ, where μ is the arithmetic mean), about 95 percent are within two standard deviations (μ±2σ), and about 99.7 percent lie within three standard deviations (μ±3σ). This {{is known as the}} 68-95-99.7 rule, or the empirical rule.|$|E
25|$|Sensitivity to the <b>data</b> <b>distribution</b> {{can be used}} to an advantage. For example, scaled {{correlation}} {{is designed}} to use the sensitivity to the range in order to pick out correlations between fast components of time series. By reducing the range of values in a controlled manner, the correlations on long time scale are filtered out and only the correlations on short time scales are revealed.|$|E
500|$|Weather buoys, {{like other}} types of weather stations, measure {{parameters}} such as air temperature above the ocean surface, wind speed (steady and gusting), barometric pressure, and wind direction. [...] Since they lie in oceans and lakes, they also measure water temperature, wave height, and dominant wave period. [...] Raw data is processed and can be logged on board the buoy and then transmitted via radio, cellular, or satellite communications to meteorological centers for use in weather forecasting and climate study. [...] Both moored buoys and drifting buoys (drifting in the open ocean currents) are used. [...] Fixed buoys measure the water temperature {{at a depth of}} [...] [...] Many different drifting buoys exist around the world that vary in design and the location of reliable temperature sensors varies. [...] These measurements are beamed to satellites for automated and immediate <b>data</b> <b>distribution.</b> [...] Other than their use as a source of meteorological data, their data is used within research programs, emergency response to chemical spills, legal proceedings, and engineering design. Moored weather buoys can also act as a navigational aid, like {{other types of}} buoys.|$|E
30|$|The unlabeled {{data set}} can be from any <b>data</b> <b>distributions,</b> but the labeled data shall {{be from the}} same <b>data</b> <b>distributions.</b>|$|R
40|$|Appendix 2. 1 Graphs Showing <b>Data</b> <b>Distributions</b> for Organic Contaminants in Water Sampled in Response to the Deepwater Horizon Spill, 2010 [...] . 3 Appendix 2. 2 Graphs Showing <b>Data</b> <b>Distributions</b> for Organic Contaminants in Whole Sediment Sampled in Response to the Deepwater Horizon Oil Spill, 2010 [...] . 87 Appendix 2. 3 Graphs Showing <b>Data</b> <b>Distributions</b> for Trace and Major Elements, Nutrients, and Specific Conductance in Water Sampled in Response to the Deepwater Horizon Oil Spill, 2010 [...] 147 Appendix 2. 4 Graphs Showing <b>Data</b> <b>Distributions</b> for Trace and Major Elements in Whole Sediment Sampled in Response to the Deepwater Horizon Oil Spill, 2010 [...] . 165 Appendix 2. 5 Graphs Showing <b>Data</b> <b>Distributions</b> for Trace and Major Elements in the Less Than 63 -Micrometer Fraction of Sediment Sampled in Response to the Deepwater Horizon Oil Spill, 2010 [...] 18...|$|R
40|$|<b>Data</b> <b>distributions</b> {{gained a}} {{considerable}} {{interest in the}} field of data parallel programming. In most cases they are key factors for the efficiency of the implementation. In this paper we analyze <b>data</b> <b>distributions</b> suited for parallel image processing and those defined by some of todays more popular parallel languages (HPF, Vienna Fortran, pC++) and libraries (ScaLAPACK). The majority of them belong to the class of bit permutations. These permutations can efficiently be realized on networks that are based on shuffle permutations. As a result we propose to widen the scope of <b>data</b> <b>distributions</b> tolerated by parallel languages and libraries towards classes of distributions. For the large class of the so called normal algorithms we demonstrate {{that it is possible to}} implement library functions that can handle a large subclass of distributions thereby avoiding redistributions. At the application level of programming <b>data</b> <b>distributions</b> are to be handled analogously to data types. The concepts introduced are implemented in the parallel image processing system PIPS [1] which is presented briefly...|$|R
500|$|SST {{was one of}} {{the first}} oceanographic {{variables}} to be measured. [...] Benjamin Franklin suspended a mercury thermometer from a ship while travelling between the United States and Europe in his survey of the Gulf stream in the late eighteenth century. [...] SST was later measured by dipping a thermometer into a bucket of water that was manually drawn from the sea surface. [...] The first automated technique for determining SST was accomplished by measuring the temperature of water in the intake port of large ships, which was underway by 1963. [...] These observations have a warm bias of around [...] due to the heat of the engine room. [...] This bias has led to changes in the [...] perception of global warming since 2000. [...] Fixed weather buoys measure the water temperature at a depth of [...] Measurements of SST have had inconsistencies over the last 130 years due to the way they were taken. In the nineteenth century, measurements were taken in a bucket off of a ship. However, there was a slight variation in temperature because of the differences in buckets. Samples were collected in either a wood or an uninsulated canvas bucket, but the canvas bucket cooled quicker than the wood bucket. The sudden change in temperature between 1940 and 1941 was the result of an undocumented change in procedure. The samples were taken near the engine intake because it was too dangerous to use lights to take measurements over the side of the ship at night. Many different drifting buoys exist around the world that vary in design, and the location of reliable temperature sensors varies. [...] These measurements are beamed to satellites for automated and immediate <b>data</b> <b>distribution.</b> [...] A large network of coastal buoys in U.S. waters is maintained by the National Data Buoy Center (NDBC). [...] Between 1985 and 1994, an extensive array of moored and drifting buoys was deployed across the equatorial Pacific Ocean designed to help monitor and predict the El Niño phenomenon.|$|E
2500|$|If a <b>data</b> <b>distribution</b> is {{approximately}} normal, then {{the proportion of}} data values within z standard deviations of the mean is defined by: ...|$|E
2500|$|Some theoreticians have {{attempted}} to determine an optimal number of bins, but these methods generally make strong assumptions about {{the shape of the}} distribution. [...] Depending on the actual <b>data</b> <b>distribution</b> and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.|$|E
40|$|This thesis {{looks at}} various issues in {{providing}} application-level software support for parallel I/O. We {{show that the}} performance of the parallel I/O system varies greatly as a function of <b>data</b> <b>distributions.</b> We present runtime I/O primitives for parallel languages which allow the user to obtain a consistent performance over a wide range of <b>data</b> <b>distributions.</b> In order to design these primitives, we study various parameters used in the design of a parallel file system. We evaluate the performance of Touchstone Delta Concurrent File System and study the effect of parameters like number of processors, number of disks, file size on the system performance. We compute the I/O costs for common <b>data</b> <b>distributions.</b> We propose an alternative strategy-two phase data access strategy- to optimize the I/O costs connected with <b>data</b> <b>distributions.</b> We implement runtime primitives using the two-phase access strategy and show that using these primitives not only I/O access rates are improved but also [...] ...|$|R
3000|$|..., {{which leads}} to {{improved}} approximations of the <b>data</b> <b>distributions,</b> as well as better change detection performance.|$|R
40|$|Pre-PrintIn {{this paper}} an {{efficient}} open address hash function called exponential hashing is developed. The motivation for this hash function resulted from our ongoing efforts to apply dynamical systems theory {{to the study}} of hashing; however, the analysis conducted in this paper is primarily based on traditional number theory. Proofs of optimal table parameter choices are provided for a number of hash functions. We also demonstrate experimentally that exponential hashing essentially matches the performance of a widely-used optimal double hash function for uniform <b>data</b> <b>distributions,</b> and performs significantly better for nonuniform <b>data</b> <b>distributions.</b> We show that exponential hashing exhibits a higher integer Lyapunov exponent and entropy than double hashing for initial data probes, which offers one explanation for its improved performance on nonuniform <b>data</b> <b>distributions.</b> AC...|$|R
2500|$|The Spamhaus Project {{consists}} {{of a number of}} independent companies which focus on different aspects of Spamhaus anti-spam technology or provide services based around it. At the core is The Spamhaus Project Ltd., which tracks spam sources and publishes free DNSBLs. Further companies include Spamhaus Logistics Corp., which owns the large server infrastructure used by Spamhaus and employs engineering staff to maintain it. Spamhaus Technology Ltd., a data delivery company which [...] "manages <b>data</b> <b>distribution</b> and synchronization services". Spamhaus Research Corp., a company which [...] "develops anti-spam technologies". The Spamhaus Whitelist Co. Ltd., which manages the Spamhaus Whitelist. Also there are several references on the Spamhaus website to The Spamhaus Foundation, whose charter is [...] "to assure the long-term security of The Spamhaus Project and its work".|$|E
50|$|ScaleBase <b>data</b> <b>distribution</b> is policy-based and transparent. That is, ScaleBase {{provides}} {{visibility and}} control of the variables that impact the <b>data</b> <b>distribution</b> policy.|$|E
5000|$|The Object Management Group's [...] <b>Data</b> <b>Distribution</b> Service (DDS) {{has added}} many new {{standards}} {{to the basic}} DDS specification. See Catalog of OMG <b>Data</b> <b>Distribution</b> Service (DDS) Specifications for more details.|$|E
50|$|Mode used {{mainly for}} {{robotics}} applications and large superstructures. Key is {{lower number of}} frames and better <b>data</b> <b>distributions.</b>|$|R
40|$|Evaluation and {{applicability}} of many database techniques, ranging from access methods, histograms, and optimization strategies to data normalization and mining, crucially {{depend on their}} ability to cope with varying <b>data</b> <b>distributions</b> in a robust way. However, comprehensive real data is often hard to come by, and there is no flexible data generation framework capable of modelling varying rich <b>data</b> <b>distributions.</b> This has led individual researchers to develop their own ad-hoc data generators for specific tasks. As a consequence, the resulting <b>data</b> <b>distributions</b> and query workloads are often hard to reproduce, analyze, and modify, thus preventing their wider usage. In this paper we present a flexible, easy to use, and scalable framework for database generation. We then discuss how to map several proposed synthetic distributions to our framework and report preliminary results. ...|$|R
40|$|Self-organizing {{neural network}} {{which is an}} {{unsupervised}} learning algorithm is to discover the inherent relationships of data. Such technique has become an important tool for data mining, machine learning and pattern recognition. Most self-organizing neural networks have a difficulty in reflecting <b>data</b> <b>distributions</b> precisely if <b>data</b> <b>distributions</b> are very complex. And meanwhile, it is also hard for these algorithms to learn new data incrementally without destroying the previous learnt data. In this paper, we propose a robust energy artificial neuron based incremental self-organizing neural network with a dynamic structure (REISOD). It can adjust the scale of network automatically to adapt {{the scale of the}} data set and learn new data incrementally with preserving the former learnt results. Moreover, several experiments show that our algorithm can reflect <b>data</b> <b>distributions</b> precisely...|$|R
5000|$|One network generates {{candidates}} and one evaluates them. Typically, the generative network learns to map from a latent space {{to a particular}} <b>data</b> <b>distribution</b> of interest, while the discriminative network discriminates between instances from the true <b>data</b> <b>distribution</b> and candidates produced by the generator. The generative network's training objective {{is to increase the}} error rate of the discriminative network (i.e., [...] "fool" [...] the discriminator network by producing novel synthesized instances that appear to have come from the true <b>data</b> <b>distribution).</b>|$|E
5000|$|<b>Data</b> <b>distribution</b> policies: policy definition, design {{guidance}} and verification ...|$|E
5000|$|... the Network Working Area offers Access and <b>data</b> <b>Distribution</b> {{facilities}} and ...|$|E
40|$|Abstract. This paper {{presents}} a neural approach to sensor modelling and classification {{as the basis}} of local data fusion in a wireless sensor network. <b>Data</b> <b>distributions</b> are non-Gaussian. <b>Data</b> clusters are sufficiently complex that the classification problem is markedly non-linear. We prove that a Continuous Restricted Boltzmann Machine can model complex <b>data</b> <b>distributions</b> and can autocalibrate against real sensor drift. To highlight the adaptation, two trained but subsequently non-adaptive neural classifiers (SLP and MLP) were employed as benchmarks. ...|$|R
40|$|A {{similarity}} {{measure is}} a measure evaluating the degree of similarity between two fuzzy data sets and has become an essential tool in many applications including data mining, pattern recognition, and clustering. In this paper, we propose a similarity measure capable of handling non-overlapped data as well as overlapped data and analyze its characteristics on <b>data</b> <b>distributions.</b> We first design the similarity measure based on a distance measure {{and apply it to}} overlapped <b>data</b> <b>distributions.</b> From the calculations for example <b>data</b> <b>distributions,</b> we find that, though the similarity calculation is effective, the designed similarity measure cannot distinguish two non-overlapped <b>data</b> <b>distributions,</b> thus resulting in the same value for both data sets. To obtain discriminative similarity values for non-overlapped data, we consider two approaches. The first one is to use a conventional similarity measure after preprocessing non-overlapped data. The second one is to take into account neighbor data information in designing the similarity measure, where we consider the relation to specific data and residual data information. Two artificial patterns of non-overlapped data are analyzed in an illustrative example. The calculation results demonstrate that the proposed similarity measures can discriminate non-overlapped data...|$|R
40|$|Abstract. The aim of {{this work}} is to improve load balance of the MPI {{parallel}} version of the STEM-II air quality model. Several dynamic <b>data</b> <b>distributions</b> are proposed and evaluated on different systems: homogeneous and dedicated, and heterogeneous and/or non-dedicated. Results prove that dynamic distribution strategies perform better than traditional static distributions. Although all the <b>data</b> <b>distributions</b> presented here {{have been developed to}} be used with the STEM–II air quality model, they are also very suitable for use in other parallel applications. ...|$|R
50|$|When signal frequency/(useful <b>data</b> <b>distribution</b> frequency) {{coincides with}} noise frequency/(noisy <b>data</b> <b>distribution</b> frequency) we have inband noise. In this {{situations}} frequency discrimination filtering {{does not work}} since the noise and useful signal are indistinguishable and where AVT excels. To achieve filtering in such conditions there are several methods/algorithms available which are briefly described below.|$|E
50|$|The company {{shied away}} from using market <b>data</b> <b>distribution</b> {{products}} from companies such as Reuters, instead choosing to develop its own systems in-house. A small development team based in London created BIDDS (Broadgate Information <b>Data</b> <b>Distribution</b> System) which included the Montage front-end package that traders used to obtain data from data feeds and broker screens.|$|E
5000|$|GTS (BATHY, TESSAC, {{drifting}} buoys messages) {{channel of}} low resolution <b>data</b> <b>distribution</b> ...|$|E
40|$|We {{propose a}} new deep network {{structure}} for unconstrained face recognition. The proposed network integrates several key components together {{in order to}} characterize complex <b>data</b> <b>distributions,</b> such as in unconstrained face images. Inspired by recent progress in deep networks, we consider some important concepts, including multi-scale feature learning, dense connections of network layers, and weighting different network flows, for building our deep network structure. The developed network is evaluated in unconstrained face matching, showing the capability of learning complex <b>data</b> <b>distributions</b> caused by face images with various qualities. Comment: 12 page...|$|R
40|$|TheStarSchemaBenchmark(SSB),hasbeenwidelyusedto {{evaluate}} {{the performance of}} database management systems when executing star schema queries. SSB, based on the well knownindustrystandardbenchmarkTPC-H,sharessomeof its drawbacks, most notably, its uniform <b>data</b> <b>distributions.</b> Today’s systems rely heavily on sophisticated cost-based query optimizers to generate the most efficient query execution plans. A benchmark that evaluates optimizer’s capability to generate optimal execution plans under all circumstances must provide the rich data set details on which optimizers rely (uniform and non-uniform <b>distributions,</b> <b>data</b> sparsity, etc.). This is also true for other database system parts, such as indices and operators, and ultimately holds for an end-to-end benchmark as well. SSB’s data generator, based on TPC-H’s dbgen, {{is not easy to}} adapt to different <b>data</b> <b>distributions</b> as its meta data and actual data generation implementations are not separated. In this paper, we motivate the need for a new revision of SSB that includes non-uniform <b>data</b> <b>distributions.</b> We list what specific modificationsarerequiredtoSSBtoimplementnon-uniformdata sets and we demonstrate how to implement these modifications in the Parallel Data Generator Framework to generate both the data and query sets...|$|R
30|$|Apart of the {{large-scale}} data available for USA {{we have used}} two smaller-size datasets for Hungary and Italy. These two additional datasets contain the same three data subsets: settlement data, commuting <b>data</b> and population <b>distribution</b> <b>data.</b> The population <b>distribution</b> <b>data</b> was used in its original form with cells of sizes 1  km 2.|$|R
5000|$|GTSPP (channel of {{real-time}} <b>data</b> <b>distribution</b> {{maintained by}} the US institute NOAA) ...|$|E
50|$|The Clustrix {{database}} automatically splits and distributes data evenly across nodes {{with each}} slice having copies on other nodes. Uniform <b>data</b> <b>distribution</b> is maintained as nodes are added, removed or if data is inserted unevenly. This automatic <b>data</b> <b>distribution</b> approach removes {{the need to}} shard and enables Clustrix to maintain database availability {{in the face of}} node loss.|$|E
5000|$|I-nvolve: Talk to {{stakeholders}} {{and give}} assurances about the <b>data</b> <b>distribution</b> and use.|$|E
40|$|Vienna Fortran is a machine-independent {{language}} extension of Fortran, {{which is based}} upon the Single-Program-Multiple-Data (SPMD) paradigm and allows the user to write programs for distributed-memory systems using global addresses. The language features focus mainly on the issue of distributing data across virtual processor structures. In this paper, we discuss those features of Vienna Fortran that allow the <b>data</b> <b>distributions</b> of arrays to change dynamically, depending on runtime conditions. We discuss the relevant language features, outline their implementation and describe how they may be used in applications. Keywords: <b>data</b> <b>distributions.</b> Distributed-memory multiprocessors, <b>data</b> parallel algorithms, dynami...|$|R
40|$|Query {{optimization}} is {{an integral}} part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying <b>data</b> <b>distributions,</b> with applications to databases, statistics, and simulation. Histograms built on the cumulative <b>data</b> <b>distributions</b> give very good approximations with limited space usage. We give fast algorithms for constructing histograms and usin...|$|R
40|$|Transport-based {{techniques}} for signal and data analysis have received increased attention recently. Given {{their abilities to}} provide accurate generative models for signal intensities and other <b>data</b> <b>distributions,</b> they {{have been used in}} a variety of applications including content-based retrieval, cancer detection, image super-resolution, and statistical machine learning, to name a few, and shown to produce state of the art in several applications. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of <b>data</b> <b>distributions.</b> Here we provide an overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications...|$|R
