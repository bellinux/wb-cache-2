9|27|Public
40|$|We {{discuss the}} {{implications}} of quantum-classical Yule-Simpson effect for quantum hypothesis testing {{in the presence of}} noise, and provide an experimental demonstration of its occurrence in the problem of discriminating which polarization quantum measurement has been actually performed by a <b>detector</b> <b>box</b> designed to measure linear polarization of single-photon states along a fixed but unknown direction. Comment: 4 pages, 2 figs, published versio...|$|E
40|$|The {{collimator}} is {{an important}} part of the IEM-XINTEGRAL experiment. In fact in addition to the function of limiting the detector field of view (FOV), to minimize the detector background level, it has also the structural function of supporting the beryllium window of the detector. Indeed it has to support the 5 atm gas pressure inside the <b>detector</b> <b>box.</b> The aim of this report is to describe the guidelines followed by the Ferrara group for the collimator projrct in order to optimize the scientific and mechanical requiremen...|$|E
40|$|The TT {{geometry}} in {{the software}} has been updated {{to comply with the}} latest technical drawings. The main difference is in the description of the beam pipe insulation, where the amount of material has increased from 7. 5 % to 15. 4 % of X_ 0. Mother volumes are added to decrease the CPU consumption and finally several scans are made to compare the material budget between the DC 06 geometry and the new 2008 geometry. In addition, the survey measurements of the TT detector have been analysed. These measurements can be subdivided into surveys of the <b>detector</b> <b>box,</b> photogrammetry of the balconies and metrology of the half-modules. The offsets with the nominal geometry are implemented in the alignment condition database...|$|E
40|$|The LHCb Technical Proposal {{described}} a tracking {{system in which}} the Inner Tracker covered a rectangular surface of 60 cm× 40 cm around the beampipe. In the meantime, simulation {{studies have shown that the}} borderline between Inner and Outer Trackers has to be shifted further away from the beampipe in order to obtain acceptably low occupancies in the innermost regions of the Outer Tracker. In an effort to keep the surface covered by the relatively expensive Inner Tracker technology as small as possible, a new layout of the tracking stations has been proposed, in which the Inner Tracker covers a cross-shaped area around the beampipe. The layout described in LHCb note 2000 - 107, in which each Inner Tracker station consisted of two "L"-shaped <b>detector</b> <b>boxes,</b> would not be well suited to cover such a cross-shaped area. A more elegant solution is proposed here, in which each Inner Tracker station consists of four rectangular <b>detector</b> <b>boxes,</b> one above, one below and one on each side of the beampipe...|$|R
5000|$|... #Caption: Animation {{showing the}} five motions {{possible}} with a four-circle kappa goniometer. The rotations {{about each of}} the four angles φ, κ, ω and 2θ leave the crystal within the X-ray beam, but change the crystal orientation. The <b>detector</b> (red <b>box)</b> can be slid closer or further away from the crystal, allowing higher resolution data to be taken (if closer) or better discernment of the Bragg peaks (if further away).|$|R
40|$|The outer {{tracking}} {{system of the}} LHCb experiment is discussed. The outer {{tracking system}} (OT) is made of three stations and every station {{is made up of}} four detecting planes with a double layer of straw tubes. The straw tubes are mounted in <b>detector</b> module <b>boxes</b> made up of sandwich panels. The use of a counting gas with a high drift velocity is suggested to cope with high bunch crossing rate at the LHCb experiment. (Edited abstract) 3 Refs...|$|R
40|$|During the TASCA {{commissioning}} phase competitive {{tests of}} {{two types of}} focal plane detectors for TASCA – a Position Sensitive Silicon Stripe Detector (PSSSD) and a Double Sided Silicon Strip Detector (DSSSD) - have been performed. The DSSSD {{proved to be more}} reliable and more sensitive for the detection of decay chains due to the high granularity of the DSSSD; moreover the position resolution of the DSSSD is independent of implantation position and deposited energy [1, 2]. The new TASCA focal plane detector setup consists of a Multi Wire Proportional Counter (MWPC) and a Focal Plane <b>Detector</b> <b>Box</b> (FPDB). The implantation detector of the FPDB consists of two side-by-side mounted 300 µm-thick DSSSDs with an active size of 72 x 48 mm 2 each mounted on a PCB frame. Eight 500 µm-thick Single Sided Silicon Strip Detector...|$|E
40|$|Isomer decays, {beta}-{gamma} and electron-{gamma} spectroscopy, {alpha}-{gamma} fine {{structure}} studies, and Coulomb excitation are cases of nuclear structure experiments where the ideal detector is a compact, highly segmented, very efficient germanium (Ge) <b>detector</b> <b>box.</b> In {{order to develop}} such a structure, {{we are working on}} the R&D of large, segmented, High Purity planar Ge strip detectors (HPGeDSSD) which form the walls of such a box. We have developed a 92 mm x 92 mm x 20 mm HPGeDSSD, which has 16 x 16 orthogonal strips of 5 mm width. We {{are in the process of}} designing a focal plane detector for the Argonne Fragment Mass Analyzer (FMA) which consists of a 5 sided box, each side having a HPGeDSSD backed by a large segmented clover HPGe detector. MCNP simulations indicate this detector would have an efficiency of {approx} 60 % for 122 keV gamma rays and {approx} 15 % for 1. 33 MeV radiation, which is ideal for studying the decays of nuclei far from stability that are usually produced with very low cross sections...|$|E
40|$|AbstractThe Fast Acquisition Laue Camera for Neutrons (FALCON) is {{a thermal}} neutron Laue {{diffractometer}} at HZB in Berlin. The instrument comprises two scintillator plate detectors coupled to four iCCD cameras each. One detector {{is placed in}} the backscattering position, enabling neutrons to pass through the centre of the <b>detector</b> <b>box.</b> The second detector is free to be placed either in the direct transmission position, or rotated to positions around the sample. The image-intensified CCDs, supplied by Photonic Science Ltd as components of the detector units, are capable of obtaining 20 -bit digitization Laue images in under ten seconds. Whilst our instrument will be used as a diffractometer primarily for crystal structure determination, the configuration of the detectors is similar to ICON at PSI in Switzerland, especially in their ‘double detector set-up’. In 2015 FALCON enters the commissioning phase whereby one of our activities will be to calibrate the detector units using CONRAD, the cold neutron imaging instrument of HZB. CONRAD will be used to characterise those factors which affect the total efficiency of the detectors i. e. cameras, lenses, CCD chips and the scintillators themselves e. g. homogeneity of the scintillator plate thickness...|$|E
40|$|The {{design of}} the 300 mK system for Herschel-SPIRE is complex, with many difficult, {{sometimes}} conflicting, requirements and constraints placed upon it. Five detector arrays, mounted from a 2 K box, are linked to a single 3 He sorption-cooler tip by a high-conductance copper strap network. This strap retains high thermal conductance, even though it incorporates an electrical break {{to comply with the}} SPIRE grounding scheme. It requires stiffness to withstand launch vibrations, but needs compliance to avoid transmission of loads to the detector arrays. The strap is stiffly supported by novel, compact cryogenic stand-offs which provide a high degree of thermal isolation from the 2 K stage. An additional complication is that the detectors reside in a 2 K environment, whilst the cooler tip is in a 4 K environment. Two of the cryogenic stand-offs also act as light-tight feed-throughs to pass the strap from the 4 K environment {{to the inside of the}} 2 K <b>detector</b> <b>boxes.</b> Active thermal control is provided on the 300 mK system to address the detector stability requirements. This paper describes the system, and gives results of the performance in SPIRE flight model ground tests...|$|R
40|$|A {{new area}} in {{particle}} physics has begun with {{the start of}} the Large Hadron Collider (LHC) at CERN at the end of 2009, which collides protons at energies never reached before. To detect the products of the collisions, four main experiments are located around the LHC, among which the LHCb experiment. LHCb has been designed as a single- arm forward spectrometer and is dedicated to measurements of CP violation and rare decays of B hadrons. These searches could also potentially lead to the discovery of phenomena that cannot be described by the model used to date, called the Standard Model. The physics beyond the Standard Model is referred to as New Physics. A measurement of the phase of the B_s^ 0 - B̅_s^ 0 oscillation amplitude with respect to that of the b→ c^+ W^- tree decay amplitude, called ϕ_s^J/ψϕ, {{is one of the key}} goals of the LHCb experiment with first data. In the Standard Model, this phase equals - 2 β_s, with β_s the smallest angle of the unitary triangle of the CKM matrix relevant to B^ 0 _s. The ϕ_s^J/ψϕ phase is hence predicted to be small, ϕ_ s^ J/ψϕ≡- 2 β_s=- 0. 0360 ^+ 0. 0020 _- 0. 0016 rad. However, possible contributions of New Physics to the B_s^ 0 - B̅_s^ 0 box diagram, such as new particles entering into it, could modify the ϕ_ s^ J/ψϕ value from its Standard Model expectation. Due to its very small theoretical uncertainty in the Standard Model, ϕ_ s^ J/ψϕ is therefore a very sensitive probe to detect the presence of New Physics. The ϕ_ s^ J/ψϕ phase will be measured from a time-dependent angular analysis to B_s^ 0 → J/ψϕ events with tagging the initial flavor of the B_s^ 0 mesons. Due to the pseudo-scalar to vector-vector particle nature of the decay, an angular analysis is required to disentangle statistically the CP- even and CP-odd components present in the final state. Already with 2 fb^- 1 of data taken at the nominal luminosity L= 2 · 10 ^ 32 cm^ 2 s^- 1, corresponding to ∼ 117, 000 B_s^ 0 → J/ψϕ signal events, the LHCb experiment is expected to achieve a statistical uncertainty σ(ϕ_ s^ J/ψϕ) ≃ 0. 03 rad, similar to the value predicted by the Standard Model. On the way to this measurement, we present prospects for the time- dependent angular analysis to B_s^ 0 → J/ψϕ events without tagging the initial flavor of the B_s^ 0 mesons. This analysis is less sensitive to ϕ_ s^ J/ψϕ, but it has the advantages of being independent of the tagging calibration and less stringent about the proper time resolution, as it does not have to resolve the fast B_s^ 0 - B̅_s^ 0 oscillations. Consequently, it can be applied on first data. Sensitivity results for ϕ_ s^ J/ψϕ, but also for the other parameters entering in the analysis are given. In particular, the amplitudes of the CP-even and CP-odd components and the B_s^ 0 meson lifetime can already be measured with a better precision than the latest CDF results (June 2010) with only 0. 2 fb^ - 1 of data at nominal luminosity with the untagged analysis. To perform this analysis, the trajectories through the spectrometer of the particles coming from the B_s^ 0 → J/ψ (μ^+ μ^-) ϕ (K^+K^-) decay, the muons and kaons, need to be reconstructed very precisely. Indeed, from the curvature of their trajectory in the magnetic field, the momentum of the particles is obtained, which is then used in the calculation of the angles forming the basis for the angular analysis to B_s^ 0 → J/ψϕ events. For particles flying in the innermost part of LHCb, the Inner Tracker is the detector that provides information about the tracks. It consists of twelve <b>detector</b> <b>boxes</b> fixed on three tracking stations located downstream of the magnet in a cross-shaped configuration around the beam pipe. They are each filled with four layers of silicon microstrip sensors. We have assembled the twelve Inner Tracker <b>detector</b> <b>boxes</b> in a clean environment at CERN. By summer 2008, they were all assembled and installed in the LHCb cavern. To reconstruct track trajectories very accurately, two ingredients are essential: a good alignment description of the detectors used for tracking and an accurate description of the magnetic field. Before closing the Inner Tracker <b>detector</b> <b>boxes,</b> we organized with the survey team at CERN measurements of the silicon sensor positions. Once installed in the LHCb cavern, the Inner Tracker <b>detector</b> <b>boxes</b> were aligned as close to their nominal position as possible on the tracking stations previously adjusted. From the final measurements, we provided to the LHCb software a first realistic geometry description of the Inner Tracker, which was validated using particles coming from LHC injection tests of September 2008 and June 2009. The magnetic field map used initially in the LHCb software had been generated with a model based on finite element calculation. To obtain a better magnetic field map description, we developed a method for parameterizing magnetic field measurements that were recorded in December 2005 in the LHCb cavern using Hall probes. This new map replaced the former field map in the LHCb software. However, the magnetic field measurements showed some different features compared to the simulated magnetic field values, which needed to be validated with real data. As an example, an asymmetry of the magnetic field between the lower and upper parts of the magnet is observed in the measurements, while simulated values do not have this by design of the dipole magnet. Using reconstructed masses of K_s→π^+π^-, Λ→ p^+π^- and Λ̅→ p^-π^+ decays coming from 2009 and early 2010 real data, we validated the magnetic field map based on the measurements against the one based on simulated values according to several criteria, one of which being the confirmation of the up-down asymmetry in the field. From the same studies, we calculated a factor to correct globally for the momentum scale. As this factor should be universal, it can be calculated for one resonance and validated using the results for other resonances...|$|R
5000|$|One {{night in}} October of that year, a Canadian Pacific freight was {{operating}} eastbound on the Kaministiquia Subdivision between Ignace and Thunder Bay, Ontario. Shortly after departing the former location, the crew received a [...] "no defects" [...] report from a hot <b>box</b> <b>detector.</b> Similar messages were received from two other detectors passed later on.|$|R
40|$|A Multiwire Proportional Chamber (MWPC) {{camera for}} Emission Tomography with {{positron}} emitting isotopes has been built. The coincident 511 KeV gammas are detected by their interaction with lead-on-plastic honeybomc converters coupled to planar MWPC detectors with sensitive area 48 x 48 cm{sup 2 } placed 50 cm apart with clear space in between for the patient. Each <b>detector</b> <b>box</b> has two MWPC: the innermost MWPC are coupled to two converters, while the outer ones have only 1 converter. This configuration {{leads to an}} effective sensitivity of 1600 coincidence cts. /min. {mu}Ci for a source placed {{in the center of}} the median plane between the detectors. With the present electronics and lead-on-plastic honeycomb converters the spatial resolution is 8 mm FWHM and the event rate at which images are taken with the camera is 30, 000 - 40, 000 events/min. The camera can be improved both in maximum event rate and spatial resolution by changing the dimensions of the converters. Decreasing the aperture size of the converters for the same thickness of lead wall leads to an increase in detection efficiency and hence improved count rate, as well as a decrease in the FWHM of the spatial resolution. They are continuing to investigate the use of surface conducting glass tube arrays with high concentrations of lead oxide (50 % or more) and with hole size between 1. 5 and 2 mm. Figure 2 b shows the expected detection efficiency of various converters as a function of lead content and wall thickness. Alternative techniques under investigation are the use of etched hole patterns on stacks of thin lead-antimony or tungsten sheets. All of these gamma converters can be made with hole sizes 1. 5 - 2. 0 mm, compared to the 3. 5 mm of the present honeycomb converters. Other factors which contribute to image degradation are multiple scattering of the 511 KeV gammas in the converters; effects of this are seen in the images of point sources which have long tails outside the main peak. Electronic techniques which they have developed are capable of rejecting over 95 % of these scatter events. Data acquisition at high rates introduces a background distribution in the images due to accidental coincidence events of two 511 KeV gammas not originating from the same positron annihilation. Duplicate electronics, which is being implemented, can map out this accidentals distribution and will allow them to make a properly normalized pixel by pixel correction. These modifications, when implemented in a new camera, are expected to yield a sensitivity and event rate a factor of 10 better than the present camera. The spatial resolution is expected to be between 4 - 6 mm, depending on the amount of scatter within the object...|$|E
40|$|The Standard Model of {{elementary}} particles, which is summarized {{briefly in the}} second chapter, incorporates a number of successful theories to explain the nature and consistency of matter. However not all building blocks of this model could yet be tested by experiment. To confirm existing theories and to improve nowadays understanding of matter a new machine is currently being built at CERN, the Large Hadron Collider (LHC), described in the third chapter. LHC is a proton-proton collider which will reach unprecedented luminosities and center of mass energies. Five experiments are attached to it to give answers to questions like {{the existence of the}} Higgs meson, which allows to explain the mass content of matter, and the origin of CP-violation, which {{plays an important role in}} the baryogenesis of the universe. Supersymmetric theories, proposing a bosonic superpartner for each fermion and vice versa, will be tested. By colliding heavy ions, high energy and particle densities can be achieved and probed. This state of matter is called the quark gluon plasma and is similar to the state of our universe short after the Big Bang. In addition high precision measurments are performed to improve current models in high energy physics. The design of the two experiments related to this thesis are described in more detail in chapter 3 : CMS, the 'Compact Muon Solenoid', which is an exploratory experiment symmetrically surrounding the interaction point and LHCb, the 'LHC beauty experiment', which is a forward spectrometer to CP-violation in the B-meson system. A crucial part of these experiments are the tracking detectors which measure the tracks of charged decay products after collision in the accelerator. From the bending under an applied magnetic field the momentum of charged particles can be derived. LHC imposes a harsh environment and heavy constraints on these detectors due to the fast readout requirements of 25 ns as well as the harsh radiation environment. Silicon microstrip detectors are the favourable choice for large area tracking devices. The resulting radiation damage is well understood and it can be expected that they will withstand the accumulated dose of 10 years of operation at LHC. In addition signal formation occurs within the bunch crossing time of 25 ns. Reproducibility at high precision is ensured by standard processes known from micro electronics industry. After a short introduction to semiconductor detectors in chapter 4 the layout and performance of the silicon trackers of the LHCb and CMS experiments are described in chapter 5. Radiation damage in silicon detectors under proton and neutron irradiation was investigated intensively within the ROSE collaboration (RD 48). Based on their results good performance of the LHCb Inner Tracker is still expected after 10 years of operation. It was found that the degraded signal over noise ratio due to radiation damage of all modules lay above the given thresholds for track reconstruction efficiency and the expected depletion voltage after ten years of operation will be well below the specifications. Due to radiation damage the leakage currents inside the silicon detectors increase. This generates additional heat which in turn generates additional leakage current. If the provided cooling is not sufficient this could result in a break down called thermal runaway. Cooling for the sensors inside the detector boxes of the Inner Tracker is provided just via natural heat convection. A short review on natural heat convection and the equations used here are given in the appendix. It was investigated whether natural heat convection from the detector surface is sufficient to suppress thermal runaway. The results are summarized in chapter 5. In the Silicon Strip Tracker of the CMS experiment additional cooling to the sensors is provided via cooling pipes. In the outer part of the tracker the sensors are placed on a sophisticated rod structure which is later on inserted into a carbon barrel. An experimental setup to study the cooling performance of this rod structure is described in chapter 6. The temperature distributions on the fully equipped rod were measured under different conditions. For the first time the temperatures of the optohybrids when operated on the rod were measured. In a final test it could be concluded that the cooling performance of the rods is sufficient to guarantee a temperature difference of 10 ^o C between silicon surface and ambient. For understanding the behaviour of a detector it is essential to understand as well the behaviour of its readout electronics. The requirements for the readout electronics at LHC are quite demanding due to the high bunch crossing frequency. The readout chips for tracking detectors used for LHC show similar design principles: a charge sensitive preamplifier followed by a CR-RC shaper. In chapter 7 the BEETLE preamplifier chip used for the LHCb Silicon Tracker is introduced. Signal formation and potential noise sources are discussed. An in depth study of the resulting pulse shape is given, supported by numeric simulations. By means of Laplace transformation models for the pulse shape could be derived which were used later on for analysis. The parameter space of both the preamplifier and the shaper settings are investigated with respect to undershoot, spill-over and ballistic deficit. Performance studies on prototype modules under testbeam conditions are crucial for the final design of tracking detectors. Prototype modules were built for the LHCb Silicon Tracker with different length, thickness and readout pitch. To better understand the impact of strip width and readout pitch prototype modules from multigeometry sensors were built. The setup and the outcome of the comprehensive measurement program of the testbeam experiments are described in chapter 8. The main parameter describing the performance of silicon microstrip detectors is the ratio of signal over noise. This parameter was derived as both a function of the total strip capacitance and the impact point of the traversing particle. A signal loss is observed in the inter-strip region. Together with laboratory measurements of the strip capacitance the signal over noise ratio could be interpolated as a function of readout pitch and strip width. Bias voltage scans were performed to test whether the 500 mum thick detectors are sensitive to ballistic deficit. The studies were supported by numeric simulations of the signal formation in silicon sensors and suggest no significant signal loss due to ballistic deficit. In addition the measurements underline the hypothesis that the charge loss in the inter-strip region arrises due to charge traps on the sensor surface. Pulseshapes are studied for different bias voltage settings and shaper settings. The signal remainder after 25 ns of the peaking time contributes to the spill-over rate in the next event and is studied carefully. A parametrization as a function of the total strip capacitance could be given. Undershoot and crosstalk were investigated and tabled for the various prototype modules. The cluster finding efficiency is a crucial parameter for the tracking performance in the final experiment. It was found to be sufficient for all prototype modules, except the two- and three- sensor modules built from 320 mum multigeometry sensors. A direct comparison is given between the performance of a 500 mum thick three sensor modules once with long interconnect cable and once without. The interconnect cable is important for the Trigger Tracker station where the readout electronics is located outside the <b>detector</b> <b>box.</b> Before the detectors are placed into the final experiment they have to be tested carefully. Especially for long modules where silicon microstrip sensors are wire bonded together, short circuits and missing wire bonds are to be expected. It turned out that the internally generated testpulses on the readout chip are a powerfull tool to test for such defects. The response of the whole readout chip is highly dependent on the total capacitance on the input node of the preamplifier. Short circuits double the strip capacitance whereas missing wire bonds are leading to a decrease. Characteristic pulseshape parameters like the pulseheight are investigated whether the change due to the different capacitances are significant. Measurements were performed on a prototype module with artificially introduced defects. All defects could be detected at a significance higher than three standard deviations. No misidentification occured using an algorithm which automatically returns a channel map with type of defect and significance. Punch throughs in the dielectricum between implant strip and aluminum strip lead to a DC-coupling of the readout strips. In the case of the APV readout chip used for the CMS Silicon Strip Tracker it was shown that the occurance of about five such pinholes could lead to a loss of the whole readout chip. To detect pinholes a very sufficient method requiring no additional hardware is suggested based again on internal calibration pulses. The results of these tests together with a characterization of the testpulses are given in chapter 9...|$|E
40|$|LHCb {{is one of}} {{the four}} main {{experiments}} hosted at the Large Hadron Collider (LHC) at CERN. The LHC first started in September 2008 and, after a one-year hiccough, restarted in November 2009. In the course of three weeks, the HEP community witnessed the first LHC proton-proton collisions and a new record of the most energetic particle beam. The ease shown by the operators of the complex LHC machine augurs very well for the extended period of data-taking scheduled to start at the end of February 2010. LHCb is the LHC experiment primarily dedicated to the b realm, through the study of CP violation and rare decays. Its physics goals are ambitious: it aims at the indirect search of New Physics and at the precise measurements of CP violation parameters. The LHCb detector was designed as a single-arm forward spectrometer. The branching fraction of the yet-unobserved B^ 0 _s→μ^+μ^- decay is currently considered {{as one of the most}} stringent tests for the existence of physics beyond the Standard Model (SM) of particle physics. There exists indeed a large unexplored range between the current experimental upper limit and the SM prediction about this branching fraction, making a measurement incompatible with the SM possible. Furthermore, there exist predictions for this branching fraction in the frame of numerous theoretical models that aim at a more accurate description of matter than that offered by the Standard Model. Some of those predictions significantly differ from the SM one, opening the possibility of an indirect discovery of New Physics. In this thesis, I present the study of the sensitivity of the LHCb experiment to this branching fraction with Monte Carlo simulations. I show that LHCb will compete with the Tevatron for the exclusion limit of the B^ 0 _s→μ^+μ^- branching fraction already in 2010 and that approximately 3 fb^- 1 at √(s) = 14 Tev are enough for a 3 σ evidence of a SM signal. In a particle physics experiment, the precise measurement of the charged particle trajectories is essential. Indeed tracking gives experimental access to their momentum and allows the reconstruction of the physical properties of the short-lived particle of which they are the decay products. The Inner Tracker is the detector that provides tracking information for the particles flying in the innermost part of LHCb. Because of its central position, the Inner Tracker calls for the use of very light-weighted material which compete with the precision and rigidity required by such a detector. During my thesis work, I contributed to the construction of the Inner Tracker through the set up of an assembly procedure for the <b>detector</b> <b>boxes,</b> uncovering several design flaws and contributed to solve them pragmatically. I defined quality requirements at key steps of the assembly and implemented tests to assess them. I conducted the assembly of the twelve Inner Tracker <b>detectors</b> <b>boxes</b> that were installed in the LHCb cavern in Summer 2008. After software alignment, the overall precision of the Inner Tracker modules position is on average 19 micrometers along the relevant direction. The careful box assembly and the quality tests along the procedure allowed to keep the dead strips fraction below 1...|$|R
5000|$|In 1993, a {{test section}} of slab {{track of the}} Züblin type was {{installed}} Between Wittenberg and Dergenthin (129.3 to 135.4 km). In 1994, a modified Rheda type slab track was installed. Also in the section between Breddin and Glöwen (93 to about 101 km) slab track was established. [...] Along the route 13 hot wheel and hot <b>box</b> <b>detectors</b> were also installed.|$|R
40|$|Local image features, such as blobs and corners, {{have proven}} to be very useful for several {{computer}} vision ap-plications. However, for enabling applications such as vi-sual search and augmented reality with near-realtime la-tency, blob detection can be quite computationally expen-sive due to numerous convolution operations. In this paper, we present a sparse convex formulation to determine a min-imal set of box filters for fast yet robust approximation to the Gaussian kernels used for blob detection. We call our feature detector as CABOX (CAscade of <b>BOX)</b> <b>detector.</b> Al-though <b>box</b> approximations to a filter have been studied in the literature, previous approaches suffer from {{one or more of the}} following problems: 1) ad hoc box filter design, 2) non-elegant trade-off between filter reconstruction quality and speed and, 3) limited experimental evaluation consid-ering very small datasets. This paper, on the other hand, contributes: 1) an elegant optimization approach to deter-mine an optimal sparse set of box filters, and 2) a com-prehensive experimental evaluation including a large scale image matching experiment with about 16 K matching and 170 K non-matching image pairs. Our experimental re-sults show a substantial overlap (89 %) between the fea-tures detected with our proposed method and the popular Difference-of-Gaussian (DoG) approach. And yet CABOX is 44 % faster. Moreover, the large scale experiment shows that CABOX closely reproduces DoG’s performance in an end-to-end feature detection and matching pipeline. 1...|$|R
40|$|In {{this thesis}} we asses the {{capabilities}} of the Crystal <b>Box</b> <b>detector</b> and evaluate its advantages over the Neutral Meson Spectrometer (NMS) detector in a planned experiment at the High Intensity Gamma Source (HI[gamma]S) at Duke University. After discussing the relevance of the experiment and briefly reviewing the physics at play, we delve into the details of the Crystal <b>Box</b> <b>detector</b> and explain how it is being modeled in the simulation. We calculate the acceptance of each detector and their resolution in measuring physical quantities from each pion photoproduction event detected. We then simulate the extraction of raw data from the experiment using both the Crystal Box and the NMS detectors, and present our results as to how well we believe each detector will perform at measuring the physical quantities of interest. Finally, we discuss possible refinements that could be implemented in the simulation to further improve the accuracy of these predictions. by Angel Roberto Solis Ortíz. Thesis (S. B.) [...] Massachusetts Institute of Technology, Dept. of Physics, 2006. Includes bibliographical references (leaf 39) ...|$|R
40|$|In {{this paper}} we {{consider}} the problem of recovering the free space of an indoor scene from its single image. We showthatexploitingtheboxlikegeometricstructureoffurniture and constraints provided by the scene, allows us to recover the extent of major furniture objects in 3 D. Our “boxy ” <b>detector</b> localizes <b>box</b> shaped objects oriented parallel to the scene across different scales and object types, and thus blocks out the occupied space in the scene. To localize the objects more accurately in 3 D we introduce a set of specially designed features that capture the floor contact points of the objects. Image based metrics are not very indicative of performance in 3 D. We make the first attempt to evaluate single view based occupancy estimates for 3 D errorsandproposeseveraltaskdrivenperformancemeasures towards it. On our dataset of 592 indoor images marke...|$|R
40|$|Action {{recognition}} from well-segmented 3 D skeleton video has been intensively studied. However, {{due to the}} difficulty in representing the 3 D skeleton video {{and the lack of}} training data, action detection from streaming 3 D skeleton video still lags far behind its recognition counterpart and image based object detection. In this paper, we propose a novel approach for this problem, which leverages both effective skeleton video encoding and deep regression based object detection from images. Our framework consists of two parts: skeleton-based video image mapping, which encodes a skeleton video to a color image in a temporal preserving way, and an end-to-end trainable fast skeleton action <b>detector</b> (Skeleton <b>Boxes)</b> based on image detection. Experimental results on the latest and largest PKU-MMD benchmark dataset demonstrate that our method outperforms the state-of-the-art methods with a large margin. We believe our idea would inspire and benefit future research in this important area. Comment: 4 pages, 3 figures, icmew 201...|$|R
40|$|Face Detection {{makes it}} {{possible}} to use the facial images of a person to authenticate him into secure system, for criminal identification, for passport verification etc. It is done by Principal Component Analysis (PCA). Face images are projected onto a face space that encodes best variation among known face images. The face space is collection of Eigen face. In the algorithm, initially video segmented using shot boundary detection techniques. Specifically, it can detect both the cut and gradual shot transitions in video. For detecting the shot boundary haar wavelet transform is used. In this method, each frame and its haar wavelet transform image is correlated for detection the shot. By setting the threshold of frame correlation shot boundaries can be detected. Video segmentation can be used in various application like video summarization, video search, and video annotation. General Terms For detecting face there are various algorithms including skin color based algorithms like Image acquisition, get video frame, run <b>detector,</b> bonding <b>box</b> around face etc...|$|R
40|$|This {{thesis is}} part of a large project {{concerning}} the development of a functional positron emission tomography (PET) system {{for the study of the}} human brain, the so-called BrainPET. Compared to the scanners that already exist on the market, this scanner should provide a higher resolution and a higher sensitivity. This will allow more accurate diagnosis of possible lesions, a lower radiological impact in the patient and better functional and anatomical characterization of the brain when used in combination with a magnetic resonance imaging (MRI) or computed tomography (CT) scanner. The development of this scanner is a cooperation between the VUB (Belgium), Forschungszentrum Jülich (Germany) and CIEMAT (Spain) under the Crystal Clear Collaboration framework. The scanner will be constructed using a new concept of gamma radiation detectors, based on monolithic detector blocks coupled to arrays of avalanche photodiodes. The outputs of these arrays will be the inputs of a neural network (NN). After a training period, this NN will provide the point of entrance of the gamma ray on the detector surface. The system will be designed and tested to be compatible with intense magnetic fields allowing to couple the scanner to a MRI system. The work carried out included the development of the software that makes it possible to implement a NN managing all the events on a computer. Moreover the possibility to use a graphical card as a computing device for the NN algorithm, using CUDA as software language, has been investigated. An experimental setup was built to evaluate the energy resolution and spatial resolution for different surface wrappings of the detector blocks. Also, the repeatability of the setup and the energy response of the setup has been evaluated. We managed to obtain the desired computing speed for the NN algorithm and made some progress in the use of a graphical card as a computing device. A second experimental setup was successfully built in order to allow the operation of two <b>detector</b> <b>boxes</b> in coincidence. The energy response of different detector blocks measured with this setup was linear. Moreover, it provided spatial and energy resolution results that were satisfying...|$|R
40|$|The {{design and}} {{performance}} of grazing incidence telescopes for celestial extreme ultraviolet (EUV) astronomy are described. The telescopes basically consist of a star tracker, collimator, grazing incidence mirror, vacuum box lid, vacuum housing, filters, a ranicon <b>detector,</b> an electronics <b>box,</b> and an aspect camera. For the survey mirror a Wolter-Schwarzschild type II configuration was selected. Diamond-turning was used for mirror fabrication, a technique which machines surfaces {{to the order of}} 10 microns over the required dimensions. The design of the EUV spectrometer is discussed with particular reference to the optics for a primarily spectroscopic application and the fabrication of the f/ 10 optics...|$|R
40|$|The {{development}} of an on-train self-powered hot <b>box</b> <b>detector</b> utilizing hardware-in-the-loop (HIL) testing method is presented. Real-time simulations of the energy harvesting system enables thorough studies and optimization, before implementing prototypes and conducting in-service tests. The integration of a vibration energy harvesting system, power storage and smart sensor node is described {{as well as the}} derivation of real-time models delivering an on-line estimation of the generated electrical power. During HIL tests with vibration records of a real running gear, energy generation is balanced with consumption by data acquisition, signal processing and communication. Afterwards, the optimized prototype is implemented and tested...|$|R
50|$|Although wheeltappers still {{operate in}} some eastern European {{countries}}, {{in countries with}} modern planned maintenance procedures and line-side defect detectors, such as hot <b>box</b> <b>detectors,</b> wheeltappers are redundant. The job is mostly associated with the steam age. Wheeltappers were vital to the smooth running of the railways as a cracked wheel or overheated axle bearing would lead to delays {{and the loss of}} revenue. These were particularly common in the 19th century, when axle bearings were lubricated by grease. At this time, metallurgy was a more haphazard science and thus it was impossible to test steel wheels for cracks: the role of the wheeltapper was of crucial importance.|$|R
50|$|Between Teltow (near Berlin) (12.36 km) and Bitterfeld (132.10 km) it {{has been}} {{possible}} to the operate on the since 28 May 2006 at 200 km/h. However, {{in the area of}} Wittenberg (km 92.9 to 97.5) the scheduled maximum speed limit is 160 km/h. From June 2010 to 13 December 2013, there were two sections, each around ten km long, Graefenhainichen-Muldenstein (km 126 to 116) and Blönsdorf-Zahna (km 84 to 75) that could be operated at only 160 km/h. Since December 2013, these sections can be operated at 200 km/h again. The reason for the three-year speed limit was the lack of new hot <b>box</b> <b>detectors.</b>|$|R
40|$|The {{laboratory}} tests are described which {{were conducted on}} new and damaged bearings to determine the feasibility of using high-frequency vibration as a diagnostic tool. A high-frequency band pass filter and demodulator was assembled to permit field measurements of the high-frequency vibrations. Field tests were conducted on an actual truck and on an axle assembly run in a grease test rig. These field tests were directed toward demonstration of the suitability and capabilities of the high-frequency technique for field application. Two specific areas of field application were identified as being cost effective for railroad use. One area is the examination of railroad roller bearings at a derailment site, {{and the second is}} as a wayside detector to supplement present hot <b>box</b> <b>detectors</b> for defective roller bearings...|$|R
40|$|Abstract This paper {{describes}} {{a system for}} interpret-ing a scene by assigning a semantic label at every pixel and inferring the spatial extent of individual object instances together with their occlusion relationships. First we present a method for labeling each pixel aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. This method combines region-level features with per-exemplar slid-ing window detectors. Unlike traditional bounding <b>box</b> <b>detectors,</b> per-exemplar detectors perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. Next, we use per-exemplar detections to generate a set of candidate object masks for a given test image. We then select a subset of objects that explain the image well and have valid overlap relationships and occlusion ordering. This is done by minimizing an integer quadratic program ei-ther using a greedy method or a standard solver. W...|$|R
40|$|In this work, we {{tackle the}} problem of {{instance}} segmentation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction prediction. Building {{on top of the}} Faster-RCNN object <b>detector,</b> the predicted <b>boxes</b> provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining semantic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different semantic classes including background, while the direction prediction, estimating each pixel's direction towards its corresponding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incorporating recent successful methods from both segmentation and detection (i. e. atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance segmentation benchmark and shows comparable performance with other state-of-art models. Comment: 10 pages including referenc...|$|R
40|$|We {{introduce}} Intelligent Annotation Dialogs for {{bounding box}} annotation. We train an agent to automatically choose {{a sequence of}} actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification [37], where the annotator verifies a box generated by an object <b>detector,</b> and manual <b>box</b> drawing. We explore two kinds of agents, one based on predicting {{the probability that a}} box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the difficulty of an input image, the desired quality of the boxes, the strength of the detector, and other factors; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also out- performing any fixed combination of verification and draw- ing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger...|$|R
40|$|This thesis {{examines}} {{the possibilities of}} a more efficient condition-based maintenance in the railway sector by using trend analysis with temperature data from hot <b>box</b> <b>detectors.</b> The study shows {{that the use of}} trend analysis directly on hot box measurements could induce misleading predictions due to certain detector characteristics. Because of this, multiple linear regression models were used to estimate the temperature of bearings based on several known parameters for each detector. A second type of linear model was then used to predict future values of the difference between the estimated temperature and the measured temperature. Results from this study show a large variation in the predicted values. The variations of the predictions depend on the amount of measurements included in the model, the time difference between the measurements, the uncertainty of each measurement, the mean value of the measurements included, and the weight function of the model. Large prediction variations resulted in predicted values where it was difficult to separate scenarios that could be considered "normal" from scenarios where a hot box warning would be desirable. For the prediction model to be able to reduce the risk for false alarms without increasing the risk of overlooking actual faults the model needs to be improved or combined with other models...|$|R
40|$|Presently, the {{existing}} failure reporting system at Banverket (the Swedish Rail Administration), which handles {{reports on the}} failures of the railway infrastructures and possible causes of failure, is not optimum (or most suitable) when it comes to analysis of electromagnetic compatibility (EMC) - related failures and causes. This failure reporting system is reliant on correct reporting into the system, so that the right information can be sent back to the users, the maintenance and service personnel, in real time. In general railway infrastructure operates in a complex and non-homogeneous environment where low power electronics has to function in the same environment as large voltages and currents from trains. The environment close to the railway tracks is heavily polluted by electromagnetic (EM) noise from the railway systems themselves. The reliability of railway signalling, communication, and control systems depends on the degree of galvanic isolation from EM noise. When new technologies are implemented into old installations, the complexity of the system increases, leading to new challenges which necessitate new forms of skill and competence to deal with these issues and challenges. The new technologies which are to be integrated into the old systems, or which are to be applied to build new systems, must meet the requirements for EMC in order to obtain a high degree of system reliability and ensure the problem-free operation of such systems. The complexity of the infrastructure is not easy to simulate or calculate, and consequently it is important to observe the real systems and their characteristics in real situations. Therefore, measurements were performed on real systems in operation. To perform an audit of the problems in the railway systems leading to EM noises and failures, investigations were made by studying the real systems in operation and using {{the existing}} failure (and inspection) reporting system of Banverket. A large number of measurements were made on site at detectors and signalling systems and installations. By studying and analyzing the measured data and failure reports from the databases, an effort was made to understand the causes of faults related to EMI (Electromagnetic interferences). Thereafter, visual inspection was carried out in the engineering constructions, i. e. signal and <b>detector</b> <b>boxes,</b> to verify if the recurring faults are caused by poorly designed installations and their physical environment. The visual inspections were concentrated on EMC and to the areas from where the power and communication cables entered the locations selected for study, by examining the areas where the sensitive equipment was placed. Through the visual inspection it was found that there could be improvements in two problem areas, namely installations and instructions from the suppliers of equipment, which often result in wrong installations. During this study, measurements were performed at sites which had extensive EMC and EMI problems. The measurements clearly show the erratic characteristics of the equipment and systems used in signalling and detector installations, mainly leading to EMC and EMI problems. For example, the measurements from detectors show that random transients appear even when a train is not present, and the measurements in the signal box show a completely different behaviour where the measuring equipment showed a reading of over - 100 volts in a 27 V system. These measurements and the results from the subsequent analyses show EMI characteristics which are totally unexpected. The detailed analysis of the failure reporting systems and databases shows that most of the failure causes are related to EMC problems. The fault reporting system is not configured to identify the failure causes as EMC or EMI problems. Therefore, this has to be investigated to identify the cause so that corrective actions can be initiated to restore the system to an operating condition. The research study has helped in understanding the function of railway signalling and detector installations from an EMC and EMI point of view. The knowledge generated will be of assistance in designing new signalling and detector equipment which will have a higher level of reliability, leading to a smaller number of failures and EMC problems. The study has made a contribution towards an understanding of the EMC and EMI characteristics of the signalling and detector infrastructure of the railway system. These characteristics often lead to failures resulting in train delays. Godkänd; 2008; 20081128 (ysko...|$|R
40|$|The PANDA {{experiment}} at {{the facility}} for anti-proton and ion research (FAIR) will study the strong interaction by precision spectroscopy. A detector system with excellent particle identification (PID) is therefore required. Charged hadron identification in the barrel region will be performed by a compact ring imaging Cherenkov detector based on the DIRC (Detection of Internally Reflected Cherenkov light) principle. Figure 1 : Display of a simulated event with three tracks in the barrel DIRC <b>detector.</b> The bar <b>boxes</b> are shown in cyan, the locations of Cherenkov photons on the detector plane are shown in yellow. The design of the PANDA barrel DIRC [1], shown in Fig. 1, {{is based on the}} BABAR DIRC [2] with several important improvements, such as focusing optics and fast photon timing. The radiators are long, rectangular bars made from synthetic fused silica, each bar is 2500 mm long with a cross-section of 17 mm × 33 mm. Six bars comprise one bar box and 16 bar boxes surround the beam line at a radial distance of 50 cm. Cherenkov photons, emitted by a particle traversing the radiator bar, propagate along the length of a bar. A mirror, attached to the forward end of the bar, reflects them back to the readout end where they are focused via a doublet lens and an expansion volume on a flat detector plane. An array of multi-anode MCP-PMTs is used to detect the photons and measure the arrival time with a precision of approximately 100 ps. Several key design elements still need to be optimized...|$|R
40|$|The {{principle}} {{objective of}} this proposal {{is to develop a}} positron emission tomography (PET) detector with depth-of-interaction (DOI) positioning capability that will achieve state of the art spatial resolution and sensitivity performance for small animal PET imaging. When arranged in a ring or <b>box</b> <b>detector</b> geometry, the proposed detector module will support 15 % absolute detection efficiency. The detector will also be compatible with operation in a MR scanner to support simultaneous multi-modality imaging. The detector design will utilize a thick, monolithic crystal scintillator readout by a two-dimensional array of silicon photomultiplier (SiPM) devices using a novel sensor on the entrance surface (SES) design. Our hypothesis is that our single-ended readout SES design will provide an effective DOI positioning performance equivalent to more expensive dual-ended readout techniques and at a significantly lower cost. Our monolithic crystal design will also lead to a significantly lower cost system. It is our goal to design a detector with state of the art performance but at a price point that is affordable so the technology can be disseminated to many laboratories. A second hypothesis is that using SiPM arrays, the detector will be able to operate in a MR scanner without any degradation in performance to support simultaneous PET/MR imaging. Having a co-registered MR image will assist in radiotracer localization and may also be used for partial volume corrections to improve radiotracer uptake quantitation. The far reaching goal of this research is to develop technology for medical research that will lead to improvements in human health care...|$|R
40|$|The Neutron Residual Stress Mapping Facility (NRSF 2) at HB- 2 B {{is a new}} generation-diffraction instrument, adding {{many new}} Second Generation features, such as larger beam tube, large sample XYZ goniometer, and KAPPA {{orienter}} for {{a broad range of}} materials behavior studies. One key feature is the NRSF 2 monochromator, which is a double focusing, double crystal monochromator system consisting of two sets of stacked Si crystal wafers. One set of wafers has Si[400] plane normal to the surface while the other set of wafers has the Si[500] normal to the surface. The monochromator crystal diffracts at a fixed diffraction angle of 88 {sup o} selecting a neutron wavelength determined by the monochromator d{sub hkl}-spacing. This 'Missouri' monochromator system has two independent monochromators, which enable diffraction from the following set of six diffraction planes: Si(511), Si(422), Si(331) AF (Anti-Fankuchen geometry), Si(400), Si(311), and Si(220). These diffraction planes can provide 6 different neutron wavelengths: approximately 1. 45, 1. 54, 1. 73, 1. 89 {angstrom}, 2. 27, and 2. 66 also incorporate seven position sensitive detectors located in a <b>detector</b> shield <b>box.</b> To use this advanced instrument for scientific and engineering measurements, careful calibration needs to be performed to accurately calibrate the seven position sensitive detectors, neutron wavelength, and 2 {theta}{sub 0 }. Just as in the X-ray diffraction technique, neutron diffraction directly measures the diffraction angle (2 {theta}) or diffraction peak position, then based on Bragg's law and a strain free lattice spacing, the strain can be calculated. Therefore anything that can affect the diffracting angle measurement can influence the accuracy of the strain measurements. The sources of difficulties in achieving accurate neutron diffraction peak positions can be classified into three categories. (1) Instrument - These difficulties come from alignment of the monochromator, alignment of the incident and detector slits, leveling of the sample table, 2 {theta}{sub 0 } offset, and response of the position sensitive detector; (2) Counting statistics - if the peak profile count is too low, then the peak position derived from fitting a profile and background will have larger error. Therefore, adequate counting statistics and well-defined peaks are always good for precise peak position determination; and (3) Sample - Large grain size materials make it difficult to get enough diffracting grains, contributing to the different profile. With a low number the peak becomes 'spot' and results in inaccuracy in peak position. Texture in the sample can change the effective elastic constants and also affect the peak intensity. Phase and composition inhomogeneity can make it difficult to determine an accurate stress-free d{sub 0 } for strain calculation. A partially buried gauge volume due to proximity to the sample surface or buried interface can also shift the peak position. The calibration method presented in this report will address the first two categories of difficulties listed above. The FWHM can be minimized for each sample d-spacing by adjusting the horizontal bending of the monochromator crystal. For the monochromator, the optimum FWHM lies between 70 and 110 degree. This range is selected in order to maintain an approximately equiaxed gauge volume and avoid significant increases in peak breadth for the detectors above and below the horizontal plane. To adequately calibrate the position sensitive detectors, 2 {theta}{sub 0 }, and wavelength, a set of high purity reference powders were selected. Since the selected reference powders have define grain size is, the measurement errors from sample grain size and texture can be excluded, although there may still be micro-strain in the powders, which can broaden the reference peak. In this report, the calibration procedure for the NRSF 2 instrument will be presented and calibration results for five monochromator settings from HFIR cycle 403 will be presented. The monochromator settings calibrated include Si(331) AF (Anti-Fankuche n geometry), Si(220), Si(511), Si(422), Si(400), and Si(311). The report presents calibration results for the single PSD that is in the horizontal plane defined by the center of the monochromator, sample, and PSD. Calibration for the out of plane detectors will require additional corrections related to the out of plane angle and finite height of each PSD detector...|$|R

