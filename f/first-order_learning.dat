40|87|Public
40|$|<b>First-order</b> <b>learning</b> {{involves}} {{finding a}} clause-form {{definition of a}} relation from examples of the relation and relevant background information. In this paper, a particular <b>first-order</b> <b>learning</b> system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other <b>first-order</b> <b>learning</b> systems might benefit from similar specialization. Comment: See [URL] for any accompanying file...|$|E
40|$|<b>First-order</b> <b>learning</b> {{involves}} {{finding a}} clause-form {{definition of a}} relation from examples of the relation and relevant background information. In this paper, a particular <b>first-order</b> <b>learning</b> system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other <b>first-order</b> <b>learning</b> systems might benefit from similar specialization. 1. Introduction Empirical learning is the subfield of AI that develops algorithms for constructing theories from data. Most classification {{research in this area}} has used the attribute-value formalism, in which data are represented as vectors of values of a fixed set of attributes and are labelled with one of a small number of discrete classes. A learning system then develops a mapping from attribute values to classes {{that can be used to}} classify unseen data. Despite the well-documented successes of algorithms develope [...] ...|$|E
40|$|The Rprop {{algorithm}} {{proposed by}} Riedmiller and Braun {{is one of}} the best performing <b>first-order</b> <b>learning</b> methods for neural networks. We introduce modifications of the algorithm that improve its learning speed. The resulting speedup is experimentally shown for a set of neural network learning tasks as well as for artificial error surfaces...|$|E
40|$|This paper {{presents}} {{a method to}} induce relational concepts with neural networks using the inductive logic programming system LINUS. Some <b>first-order</b> inductive <b>learning</b> tasks taken from machine learning literature were applied successfully, thus confirming {{the quality of the}} hypothesis generated by neural networks...|$|R
40|$|<b>First-order</b> <b>learn</b> ing systems (e. g., FOIL, FOCL, FORTE) {{generally}} rely on hill-climbing heuristics {{in order}} to avoid the combinatorial explosion inheren t in <b>learn</b> ing <b>first-order</b> concepts. However, hill-climbing leaves these systems vulnerable to local maxima and local plateaus. We presen t a method, called relational pathfinding, which has proven highly effective in escaping local maxima and crossing local plateaus. We presen t our algorithm and provide learn ing results in two domains: family relationships and qualitative model building. 1 Introduction Many recent learning sy stems for first-order Horn clauses, such as FOIL, FOCL, and Forte ([Quinlan, 1990], [Pazzani, Brunk, and Silverstein, 1991], and [Richards and Mooney, 1991]) employ hill-climbing to learn clauses one literal at a time. One of the problems faced by such hill-climbing sy stems is what we call the local plateau problem (see Figure 1). This arises when, {{in order to}} improve the classification accuracy of a rule, [...] ...|$|R
40|$|We present our {{research}} produced about Higher-order Logic Learning (HOLL), {{which consists of}} adapting <b>First-order</b> Logic <b>Learning</b> (FOLL), like Inductive Logic Programming (ILP), within a Higher-order Logic (HOL) context. We describe a first working implementation of lambda-Progol, a HOLL system adapting the ILP system Progol and the HOL formalism lambda-Prolog. We compare lambda-Progol and Progol on the learning of recursive theories showing that HOLL can, in these cases, outperform FOLL...|$|R
40|$|Abstract. We present {{compelling}} {{evidence that the}} World Wide Web is a domain in which applications can benefit from using <b>first-order</b> <b>learning</b> methods, since the graph structure inherent in hypertext naturally lends itself to a relational representation. We demonstrate strong advantages for two applications – learning classifiers for Web pages, and learning rules to discover relations among pages. ...|$|E
40|$|While {{considerable}} {{research on}} progress functions {{has been done}} in industrial and manufacturing sectors, little emphasis on this topic has been observed in software development field. Sherdil pioneered the progress functions study in the field of software development, which, if effectively used, can significantly help reduce the software cost and time and improve product quality. In his research, he found that an average of 22 % autonomous (<b>first-order)</b> <b>learning</b> takes place in all the three fields, productivity, product quality and personal skills. Whereas second-order training and technology seems to have helped software engineers by up to 13 % more in progress, as compared to <b>first-order</b> <b>learning.</b> This thesis presents an external replication of Sherdil's experiment, so as to assess whether his results can be generalizable. Our replication can be categorized as fairly precise where all the procedural variables were duplicated as closely as possible. (Abstract shortened by UMI. ...|$|E
40|$|Outerplanar graphs form a {{practically}} relevant {{class of}} graphs which appear efficiently computable bottom-up refinement operator for tenuous outerplanar graphs defined by combining techniques from <b>first-order</b> <b>learning,</b> algebraic graph theory, and combinatorial pattern matching. Since the coverage test for outerplanar graphs is decidable in polynomial time, our {{results can be}} used to develop practical algorithms for bottom-up induction of tenuous outerplanar graph patterns...|$|E
40|$|This paper {{presents}} a <b>first-order</b> logic <b>learning</b> approach to determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report {{a statistically significant}} improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIP-PER and Naive Bayes. For discourse parsing, our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline...|$|R
40|$|We {{present a}} new margin-based {{approach}} to <b>first-order</b> rule <b>learning.</b> The approach addresses {{many of the}} prominent challenges in <b>first-order</b> rule <b>learning,</b> such as the computational complexity of optimization and capacity control. Optimizing {{the mean of the}} margin minus its variance, we obtain an algorithm linear in the number of examples and a handle for capacity control based on error bounds. A useful parameter in the optimization problem tunes how evenly the weights are spread among the rules. Moreover, the search strategy for including new rules can be adjusted flexibly, to perform variants of propositionalization or relational learning. The implementation of the system includes plugins for logical queries, graphs and mathematical terms. In extensive experiments, we found that, at least on the most commonly used toxicological datasets, overfitting is hardly an issue. In another batch of experiments, a comparison with margin-based ILP approaches using kernels turns out to be favorable. Finally, an experiment shows how many features are needed by propositionalization and relational learning approaches to reach a certain predictive performance...|$|R
40|$|Learning {{probabilistic}} models {{has been}} an important direction of research in the machine learning community, as has been <b>learning</b> <b>first-order</b> logic models. Ideally, we {{would like to be able}} to combine the two, i. e., to <b>learn</b> <b>first-order</b> probabilistic models. Because of their ability to handle uncertainty and compactly model complex domains, these models are the object of growing research interest. This research comprises three main directions: knowledge-based model construction (KBMC), stochastic logic programs (SLPs), and probabilistic relational models (PRMs). This paper surveys these approaches, and suggests opportunities for further research and improvement, particularly with regard to modifying them so they may scale to handle large amounts of training data...|$|R
40|$|FOIL is a <b>first-order</b> <b>learning</b> {{system that}} uses {{information}} {{in a collection of}} relations to construct theories expressed in a dialect of Prolog. This paper provides an overview of the principal ideas and methods used in the current version of the system, including two recent additions. We present examples of tasks tackled by foil and of systems that adapt and extend its approach...|$|E
40|$|Al~raet. Handling {{numerical}} {{information is}} one of the most important research issues for practical applications of <b>first-order</b> <b>learning</b> systems. This paper is concerned with the problem of inducing first-order classification rules frum both numeric and symbolic d__ ~ We propose a spocialiTation operator that dis~-~etizes continuous d~_~ _ during the learning proce ~ The heuristic function used to choose among different discretizations satisfies a property that can be profitably exploited to improve the efficiency of the specialiTmlon operator. The operator has been implemented and tested on the domnnent understanding domain. 1 Background and motivation One of the most important research issues for practical applications of <b>first-order</b> <b>learning</b> systems is the handling of numerical information [7, 18]. Indeed, in many applications, continuous data are predominant and should be treated suitably. Image analysis and understsnding {{is one of}} the widest application fields where the capability of handling both symbolic and numeric descriptors (that is, attributes and relations) is essential for the successftd application of machine learning techniques. I...|$|E
40|$|Many {{well-known}} learning {{systems like}} CN 2 and C 4. 5 use an attributevalue representation for the examples. In Inductive Logic Programming (ILP) a first-order logical formalism is used, {{which is a}} more expressive representation. We will introduce a 5 -step methodology to upgrade an existing propositional learner towards a <b>first-order</b> <b>learning</b> system. The system ICL (Inductive Classification Logic) {{is used as a}} case study. Keywords: inductive logic programming, propositional learning, learning from interpretations...|$|E
40|$|Abstract. Process mining is the {{automated}} construction of process models from information system event logs. In this paper we identify three fundamental difficulties related to process mining: {{the lack of}} neg-ative information, the presence of history-dependent behavior {{and the presence of}} noise. These difficulties can elegantly dealt with when pro-cess mining is represented as <b>first-order</b> classification <b>learning</b> on event logs supplemented with negative events. A first set of process discovery experiments indicates the feasibility of this learning technique. ...|$|R
40|$|The {{amount of}} data in our society has been {{exploding}} {{in the era of}} big data today. In this paper, we address several open challenges of big data stream classification, including high volume, high velocity, high dimensionality, and high sparsity. Many existing studies in data mining literature solve data stream classification tasks in a batch learning setting, which suffers from poor efficiency and scalability when dealing with big data. To overcome the limitations, this paper investigates an online learning framework for big data stream classification tasks. Unlike some existing online data stream classification techniques that are often based on <b>first-order</b> online <b>learning,</b> we propose a framework of Sparse Online Classification (SOC) for data stream classification, which includes some state-of-the-art <b>first-order</b> sparse online <b>learning</b> algorithms as special cases and allows us to derive a new effective second-order online learning algorithm for data stream classification. We conduct an extensive set of experiments, in which encouraging results validate the efficacy of the proposed algorithms in comparison to a family of state-of-the-art techniques on a variety of data stream classification tasks...|$|R
40|$|The paper {{introduces}} LogAn-H [...] - {{a system}} for <b>learning</b> <b>first-order</b> function-free Horn expressions from interpretations. The system {{is based on an}} interactive algorithm (that asks questions) that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system by reducing some inefficiencies...|$|R
40|$|<b>First-order</b> <b>learning</b> systems (e. g., FOIL, FOCL, FORTE) {{generally}} rely on hill-climbing heuristics {{in order}} to avoid the combinatorial explosion inherent in learning first-order concepts. However, hill-climbing leaves these systems vulnerable to local maxima and local plateaus. We present a method, called relational pathfinding, which has proven highly effective in escaping local maxima and crossing local plateaus. We present our algorithm and provide learning results in two domains: family relationships and qualitative model building...|$|E
40|$|Learning to {{transform}} English verbs from present to past tense {{has been studied}} extensively in the connectionist literature. A recent paper describes a symbolic approach that outperforms neural networks on this task, but the new system is still constrained by propositional-level attribute-value representations. A <b>first-order</b> <b>learning</b> method that uses a relatively natural representation is found to give slightly better results again. In the course of experiments, several weaknesses in the firstorder approach are identified that suggest areas for further research...|$|E
40|$|Handling {{numerical}} {{information is}} one of the most important research issues for practical applications of <b>first-order</b> <b>learning</b> systems. This paper is concerned with the problem of inducing first-order classification rules from both numeric and symbolic data. We propose a specialization operator that discretizes continuous data during the learning process. The heuristic function used to choose among different discretizations satisfies a property that can be profitably exploited to improve the efficiency of the specialization operator. The operator has been implemented and bested on the document understanding domain...|$|E
40|$|In this paper, we {{consider}} <b>learning</b> <b>first-order</b> Horn programs from entailment. In particular, {{we show that}} any subclass of first-order acyclic Horn programs with constant arity is exactly learnable from equivalence and entailment membership queries provided it allows a polynomial-time subsumption procedure and satisfies some closure conditions. One consequence {{of this is that}} first-order acyclic determinate Horn programs with constant arity are exactly learnable from equivalence and entailment membership queries. 1 Introduction <b>Learning</b> <b>first-order</b> Horn programs [...] -sets of first-order Horn clauses [...] -is an important problem in inductive logic programming with applications ranging from speedup learning to grammatical inference. We are interested in speedup learning, which concerns learning domainspecific control knowledge to alleviate the computational hardness of planning. One kind of control knowledge, which is particularly useful in many domains, is represented as goal-decomposition [...] ...|$|R
40|$|WISDOM++ is an {{intelligent}} document processing system that transforms a paper document into HTML/XML format. The main design requirement is adaptivity, which is realized {{through the application}} of machine learning methods. This paper illustrates the application of symbolic learning algorithms to the first three steps of document processing, namely document analysis, document classification and document understanding. Machine learning issues related to the application are: Efficient incremental induction of decision trees from numeric data, handling of both numeric and symbolic data in <b>first-order</b> rule <b>learning,</b> learning mutually dependent concepts. Experimental results obtained on a set of real-world documents are illustrated and commented...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with FS-FOIL – an extension of Quinlan’s <b>First-Order</b> Inductive <b>Learning</b> Method (FOIL). In contrast to the classical FOIL algorithm, FS-FOIL uses fuzzy predicates and, thereby, allows to deal not only with categorical variables, but also with numerical ones, without the need to draw sharp boundaries. This method is described in full detail along with discussions {{how it can be}} applied in different traditional application scenarios – classification, fuzzy modeling, and clustering. We provide examples of all three types of applications in order to illustrate the efficiency, robustness, and wide applicability of the FS-FOIL method...|$|R
40|$|Individuals {{can expect}} {{continuous}} improvement in productivity {{as a consequence}} of (i) a growing stock of knowledge and experience gained by repeatedly doing the same task (<b>first-order</b> <b>learning)</b> or (ii) due to technological and training programs injected by the organization (second-order learning). Organizations have used this type of progress behavior in making managerial decisions regarding cost estimating and budgeting, production and labor scheduling, product pricing, etc. This progress was studied in productivity, product-quality and personal skills, in an experiment involving a sample of 12 subjects, who completed one project every week for ten weeks. Second-order training was provided to the subjects through the Personal Software Process, PSP, of Humphrey. A within-subject repeated measure time-series quasi-experimental design was used along with a modified G/Q/M method. It was found that on average, progress takes place at a rate of 20 %, with the second-order training adding up to 13 % more improvement in addition to the <b>first-order</b> <b>learning.</b> Detailed statistical methods were used to produce linear and log-linear models of high correlations, involving four variables: productivity, defect-rate, complexity and cumulative output. The motivation of the subjects did not change significantly during the experiment. It was also found that the McCabe's and Halstead's complexity metrics had a correlation of 0. 80 amongst each other. However, no relationship could be found between the personal capabilities of the individuals and the progress rate...|$|E
40|$|Discovering {{significant}} meta-information from document collections is {{a critical}} factor for knowledge distribution and preservation. This paper presents a system that implements intelligent document processing techniques, by combining strategies for the layout analysis of electronic documents with incremental <b>first-order</b> <b>learning</b> in order to automatically classify the documents and their layout components according to their semantics. Indeed, an in-deep analysis of specific layout components can allow the extraction of useful information to improve the semantic-based document storage and retrieval tasks. The viability of the proposed approach is confirmed by experiments run in the real-world application domain of scientific papers...|$|E
40|$|Gradient-based {{optimization}} algorithms are {{the standard}} methods for adapting the weights of neural networks. The natural gradient gives the steepest descent direction {{based on a}} non-Euclidean, from a theoretical point of view more appropriate metric in the weight space. While the natural gradient has already proven to be advantageous for online learning, we explore its benefits for batch learning: We empirically compare Rprop (resilient backpropagation), {{one of the best}} performing <b>first-order</b> <b>learning</b> algorithms, using the Euclidean and the non-Euclidean metric, respectively. As batch steepest descent on the natural gradient is closely related to Levenberg-Marquardt optimization, we add this method to our comparison. It turn...|$|E
40|$|Abstract—The {{amount of}} data in our society has been {{exploding}} {{in the era of}} big data today. In this paper, we ad-dress several open challenges of big data stream classification, including high volume, high velocity, high dimensionality, and high sparsity. Many existing studies in data mining literature solve data stream classification tasks in a batch learning setting, which suffers from poor efficiency and scalability when dealing with big data. To overcome the limitations, this paper investigates an online learning framework for big data stream classification tasks. Unlike some existing online data stream classification techniques that are often based on <b>first-order</b> online <b>learning,</b> we propose a framework of Sparse Online Classification (SOC) for data stream classification, which includes some state-of-the-art <b>first-order</b> sparse online <b>learning</b> algorithms as special cases and allows us to derive a new effective second-order online learning algorithm for data stream classification. We conduct an extensive set of experiments, in which encouraging results validate the efficacy of the proposed algorithms in comparison to a family of state-of-the-art techniques on a variety of data stream classification tasks. Keywords-data stream classification; sparse; online learning; I...|$|R
40|$|Abstract. Learning from multi-relational domains {{has gained}} {{increasing}} attention {{over the past few}} years. Inductive logic programming (ILP) systems, which often rely on hill-climbing heuristics in <b>learning</b> <b>first-order</b> concepts, have been a dominating force in the area of multi-relational concept learning. However, hill-climbing heuristics are susceptible to local maxima and plateaus. In this paper, we show how we can exploit the links between objects in multi-relational data to help a <b>first-order</b> rule <b>learning</b> system direct the search by explicitly traversing these links to find paths between variables of interest. Our contributions are twofold: (i) we extend the pathfinding algorithm by Richards and Mooney [12] to make use of mode declarations, which specify the mode of call (input or output) for predicate variables, and (ii) we apply our extended path finding algorithm to saturated bottom clauses, which anchor one end of the search space, allowing us to make use of background knowledge used to build the saturated clause to further direct search. Experimental results on a medium-sized dataset show that path finding allows one to consider interesting clauses that would not easily be found by Aleph. ...|$|R
40|$|We {{study and}} {{implement}} algorithms to revise and <b>learn</b> <b>first-order</b> logical theories, written in clausal form (as in Prolog), describing multiple concepts, and employing negation. These algorithms derive ideas from decision trees and gradient descent for learning, and are founded on basic logical principles like unification, constraints, satisfiability and induction. We compare the scopes of these algorithms, and evaluate {{them with a}} set of experiments. ...|$|R
40|$|The Rprop {{algorithm}} {{proposed by}} Riedmiller and Braun {{is one of}} the best performing <b>first-order</b> <b>learning</b> methods for neural networks. We discuss modifications of this algorithm that improve its learning speed. The new optimization methods are empirically compared to the existing Rprop variants, the conjugate gradient method, Quickprop, and the BFGS algorithm on a set of neural network benchmark problems. The improved Rprop outperforms the other methods; only the BFGS performs better in the later stages of learning on some of the test problems. For the analysis of the local search behavior, we compare the Rprop algorithms on general hyperparabolic error landscapes, where the new variants confirm their improvement...|$|E
40|$|This article {{analyses}} {{the learning}} experiences gained by 19 Dutch companies when implementing {{the concept of}} corporate social responsibility in their own business practices. It is concluded that learning processes took place at individual level and, in certain cases, at group level. Learning {{at the level of}} the organization as a whole was still one bridge too far for practically all companies. Moreover, the analysis showed that transferring <b>first-order</b> <b>learning</b> experiences turned out to be relatively easy. Getting across the fundamentals behind corporate social responsibility that could lead to second-order learning was much harder. Much depended on the extent to which the concept of corporate social responsibility had become an integral part of the business cultur...|$|E
40|$|Abstract—This paper {{describes}} a new algorithm with neuronby-neuron computation methods for the gradient vector and the Jacobian matrix. The algorithm can handle networks with arbitrarily connected neurons. The training speed is comparable with the Levenberg–Marquardt algorithm, {{which is currently}} considered {{by many as the}} fastest algorithm for neural network training. More importantly, it is shown that the computation of the Jacobian, which is required for second-order algorithms, has a similar computation complexity as the computation of the gradient for <b>first-order</b> <b>learning</b> methods. This new algorithm is implemented in the newly developed software, Neural Network Trainer, which has unique capabilities of handling arbitrarily connected networks. These networks with connections across layers can be more efficient than commonly used multilayer perceptron networks. Index Terms—Learning, neural network. I...|$|E
30|$|For {{a copy of}} the {{instructions}} given to the participants, please see Appendix B. We were aware that the task of stopping the bouncing could be challenging, so we presented the models to the test subjects always in the following order during the training phase: NF-HINT to immediately provide insight into an optimal strategy, followed by NF, MED, WEAK, and STRNG. During the testing phase, each of the ten successful subjects received the same five models ordered according to a balanced Latin square to minimize <b>first-order</b> residual <b>learning</b> effects during testing. If a subject made a mistake, the subject could repeat the test trial until satisfied with his or her test trial.|$|R
40|$|This {{exploratory}} paper sketches some of {{the behavioral}} processes {{that give rise to}} the learning curve. Using data from two manufacturing departments in an electronic equipment company, we construct a model of productivity improvement as a function of cumulative output and two managerial variables [...] -engineering changes and workforce training. Exploration of this model highlights the complex relationship between <b>first-order</b> and second-order <b>learning.</b> productivity, learning, learning curve, manufacturing, engineering changes, training...|$|R
40|$|Networks {{of neurons}} can perform {{computations}} that even modern computers find {{very difficult to}} simulate. Most of the existing artificial neurons and artificial neural networks are considered biologically unrealistic, nevertheless the practical success of the backpropagation algorithm and the powerful capabilities of feedforward neural networks have made neural computing very popular in several application areas. A challenging issue {{in this context is}} learning internal representations by adjusting the weights of the network connections. To this end, several firstorder and second-order algorithms have been proposed in the literature. This paper provides an overview of approaches to backpropagation training, emphazing on <b>first-order</b> adaptive <b>learning</b> algorithms that build on the theory of nonlinear optimization, and proposes a framework for their analysis in the context of deterministic optimization...|$|R
