3|105|Public
50|$|The {{share capital}} must be {{maintained}} throughout {{the life of the}} company. It may be reduced only in the manner allowed by law, e.g. through a capital reduction. The ban also applies to the <b>hidden</b> <b>distribution</b> of profits. A payment from the required share capital covering the company's assets is not possible for the shareholders under any title.|$|E
40|$|Traditionally, when {{generative}} {{models of}} data are developed via deep architectures, greedy layer-wise pre-training is employed. In a well-trained model, the lower {{layer of the}} architecture models the data distribution conditional upon the hidden variables, while the higher layers model the <b>hidden</b> <b>distribution</b> prior. But due to the greedy scheme of the layerwise training technique, the parameters of lower layers are fixed when training higher layers. This makes it extremely challenging for the model to learn the <b>hidden</b> <b>distribution</b> prior, which in turn leads to a suboptimal model for the data distribution. We therefore investigate joint training of deep autoencoders, where the architecture is viewed as one stack {{of two or more}} single-layer autoencoders. A single global reconstruction objective is jointly optimized, such that the objective for the single autoencoders at each layer acts as a local, layer-level regularizer. We empirically evaluate the performance of this joint training scheme and observe that it not only learns a better data model, but also learns better higher layer representations, which highlights its potential for unsupervised feature learning. In addition, we find that the usage of regularizations in the joint training scheme is crucial in achieving good performance. In the supervised setting, joint training also shows superior performance when training deeper models. The joint training framework can thus provide a platform for investigating more efficient usage of different types of regularizers, {{especially in light of the}} growing volumes of available unlabeled data. Comment: 11 pages, 4 figure...|$|E
40|$|International audienceThe problem {{considered}} {{in this paper}} is regression with a constraint on the precision of each prediction {{in the framework of}} data streams subject to concept drifts (when the <b>hidden</b> <b>distribution</b> which generates the observations can change over time). Concept drifts can diminish the reliability of the predictions over time and it might not be possible to output a prediction which satisfies the constraints on the precision. In this case, we claim that if the costs associated with a good and with a bad prediction are known beforehand, the overall prediction cost can be improved by allowing the regressor to abstain. To this end, we propose a generic method, compatible with any regressor, which uses an ensemble of reliability estimators to estimate whether the constraints on the precision of a given prediction can be met or not. In the later case, the regressor is allowed to abstain. Empirical results on 30 datasets including different types of drifts back our claim...|$|E
40|$|The {{contemporary}} object {{distribution spectrum}} consists of proxybased middleware solutions {{on the one}} hand and distributed programming languages on the other hand. Middleware suffers from numerous technical problems due to its inability to <b>hide</b> <b>distribution</b> and current day languages fail precisely because they <b>hide</b> <b>distribution</b> too much. The paper presents a distributed programming language feature that occupies middle ground by treating proxies as delegation-based descendants in the prototype-based sense. Several distribution idioms are shown to be elegantly expressible using the proposed feature...|$|R
50|$|Some {{problems}} in mixture model estimation {{can be solved}} using spectral methods.In particular it becomes useful if data points xi are points in high-dimensional real space, and the <b>hidden</b> <b>distributions</b> {{are known to be}} log-concave (such as Gaussian distribution or Exponential distribution).|$|R
40|$|This paper {{argues that}} object {{oriented}} distributed programming {{should not be}} about wrapping existing technologies with object oriented like interfaces nor should it be about <b>hiding</b> <b>distribution</b> behind classical abstractions of traditional (centralized) object oriented languages. Roughly speaking, object oriented distributed programming has {{very little to do}} with building a distributed object oriented language or system. The metaphor of a community of independent objects communicating through message passing should not hide the fact that distributed interaction is not local interactio...|$|R
40|$|In {{analogy to}} superstatistics, which connects Boltzmann-Gibbs {{statistical}} mechanics to its generalizations through temperature fluctuations, complex networks are constructed from the fluctuating Erdos-Renyi random graphs. Here, using the quantum mechanical method, the exact analytic formula is presented for the <b>hidden</b> variable <b>distribution,</b> which describes the fluctuation and generates a generic degree distribution through the Poisson transformation. As an example, a static scale-free network is discussed {{and the corresponding}} <b>hidden</b> variable <b>distribution</b> is found to decay as a power law. Comment: 12 pages and 1 figur...|$|R
40|$|Abstract — An {{important}} {{problem in}} data-analysis tasks {{is to find}} suitable representations that make hidden structure in the data explicit. In this paper, we present a probabilistic latent variable model that is equivalent to a matrix decomposition of nonnegative data. Data is modeled as histograms of multiple draws from an underlying generative process. The model expresses the generative distribution as a mixture of <b>hidden</b> <b>distributions</b> which capture the latent structure. We extend the model to incorporate sparsity constraints by using an entropic prior. We derive algorithms for parameter estimation and show how the model can be applied for unsupervised feature extraction and supervised classification tasks...|$|R
40|$|The paper {{describes}} {{design and}} implementation of soft-ware infrastructure for building augmented reality applica-tions for ubiquitous computing environments. Augmented reality {{is one of the}} most important techniques to achieve the vision of ubiquitous computing. Traditional toolkits for aug-mented reality provide the high level abstraction that makes it easy to build augmented reality applications. However, the applications programmers need to contemplate distri-bution and context-awareness that make the development of applications very hard, but they are necessary to build ubiq-uitous computing environments. Our infrastructure pro-vides the high level abstraction and <b>hides</b> <b>distribution</b> and context-awareness from programmers. Therefore, the cost to develop augmented reality applications will be reduced dramatically by using our middleware infrastructure. 1...|$|R
40|$|International audienceGrid {{architectures}} are execution {{environments that}} {{are known to be}} at the same time distributed, parallel, heterogeneous and dynamic. While current tools focus solutions for <b>hiding</b> <b>distribution,</b> parallelism and heterogeneity, this approach does not t well their dynamic aspect. Indeed, if applications are able to adapt themselves to environmental changes, they can benet from it to achieve better performance. This article presents Afpac, a model extending Dynaco for designing self-adaptable parallel components that can be assembled to build applications for Grid. This model includes the denition of a consistency criterion for the dynamic adaptation of SPMD components. We propose a solution to implement this criterion. It has been evalued using both synthetic and real codes to exhibit the behavior of several proposed strategies...|$|R
40|$|Abstract. Grid {{architectures}} are execution {{environments that}} {{are known to be}} at the same time distributed, parallel, heterogeneous and dynamic. While current tools focus solutions for <b>hiding</b> <b>distribution,</b> parallelism and heterogeneity, this approach does not fit well their dynamic aspect. Indeed, if applications are able to adapt themselves to environmental changes, they can benefit from it to achieve better performance. This article presents Afpac, a tool for designing self-adaptable parallel components that can be assembled to build applications for Grid. This model includes the definition of a consistency criterion for the dynamic adaptation of SPMD components. We propose a solution to implement this criterion. It has been evalued using both synthetic and real codes to exhibit the behavior of several proposed strategies. Key words. dynamic adaptation, consistency, parallel computing 1. Introduction. Gri...|$|R
40|$|Abstract. This article {{presents}} Grid’BnB, a parallel {{branch and bound}} framework for grids. Branch and bound (B&B) algorithms find optimal solutions of search problems and NP-hard optimization problems. Grid’BnB is a Java framework that helps programmers to distribute problems over grids by <b>hiding</b> <b>distribution</b> issues. It is built over a masterworker approach and provides a transparent communication system among tasks. This work also introduces a new mechanism to localize computational nodes on the deployed grid. With this mechanism, we can determine if two nodes {{are on the same}} cluster. This mechanism is used in Grid’BnB to reduce inter-cluster communications. We run experiments on a nationwide grid. With this test bed, we analyze the behavior of a communicant application deployed on a large-scale grid that solves the flow-shop problem. ...|$|R
40|$|Object {{middleware}} is an {{enabling technology}} for distributed applications {{that are required}} to operate in heterogeneous computing and communication environments. Although <b>hiding</b> <b>distribution</b> aspects to application designers proves beneficial, in an operational environment system managers may need detailed information on information flows and the locality of objects in order to track problems or tune the system. Therefore, hooks are required inside the processing core of the middleware to obtain inside-information and to influence the processing of information flows. We present the use of portable interceptors {{for the management of}} CORBA as well as COM/DCOM middleware. Management information is structured in a middleware technology independent way, using XML for representation. Our approach shows two aspects of “management transparency��?: application designers are not burdened with designing management functionality, and system managers can manage CORBA and (D) COM from a single set of management tools...|$|R
40|$|Hidden Markov models using Gaussian mixture {{models as}} their <b>hidden</b> state <b>distributions</b> have been {{successfully}} applied in text-dependent speaker identification applications. Nevertheless, it is well-known that Gaussian mixture models are very vulnerable {{to the presence of}} outliers in the fitting set used for their estimation. Student's-t mixture models have been proposed recently as a heavy-tailed, tolerant to outliers alternative t...|$|R
40|$|Mediation {{architectures}} {{like the}} Context Interchange re-search project, {{from which this}} work stems, integrate dis-parate information sources, <b>hiding</b> <b>distribution</b> and recon-ciling heterogeneity. As a result of transparent access, the notion of distinct sources often disappears from queries and results. However, there are situations where users or ap-plications do {{need to know the}} sources from which a par-ticular datum is drawn: for example, enforcement of intel-lectual property, evaluation of data quality or measurement of the timeliness of data. In this paper, we define attribu-tion as the association of a value in the result of a query with the sources either from which the data was extracted or which contributed to the selection. Motivated by multi-source querying, attribution has particular applicability {{in the context of the}} World Wide Web. After beginning with an example that both describes and motivates the need for attribution within the semi-structured environment of the Web, we offer a general attribution framework and algebra and discuss the implementation of a prototype. ...|$|R
40|$|Hidden Markov (chain) models using finite Gaussian mixture {{models as}} their <b>hidden</b> state <b>distributions</b> have been {{successfully}} applied in sequential data modeling and classification applications. Nevertheless, Gaussian mixture models are well known to be highly intolerant {{to the presence of}} untypical data within the fitting data sets used for their estimation. Finite Student's t-mixture models have recently emerged as a heavier-tailed, robust alternative to Gaussian mixture models, overcoming these hurdles. To exploit these merits of Student's t-mixture models {{in the context of a}} sequential data modeling setting, we introduce, in this paper, a novel hidden Markov model where the <b>hidden</b> state <b>distributions</b> are considered to be finite mixtures of multivariate Student's t-densities. We derive an algorithm for the model parameters estimation under a maximum likelihood framework, assuming full, diagonal, and factor-analyzed covariance matrices. The advantages of the proposed model over conventional approaches are experimentally demonstrated through a series of sequential data modeling applications. © 2009 IEEE...|$|R
40|$|Hidden Markov models using finite Gaussian mixture {{models as}} their <b>hidden</b> state <b>distributions</b> {{have been applied}} in {{modeling}} of time series that result from various noisy signals. Nevertheless, Gaussian mixture models are well-known to be highly intolerant {{to the presence of}} outliers within the fitting sets used for their estimation. Finite Student's-t mixture models have recently emerged as a heaviertailed, robust alternative to Gaussian mixture models, overcoming these hurdles. To exploit those merits of Student's-t mixture models, we introduce in this paper a novel hidden Markov chain model where the <b>hidden</b> state <b>distributions</b> are considered to be finite mixtures of multivariate Student's-t densities and we derive an algorithm for the model parameters estimation under a maximum likelihood framework. We apply this novel approach in automatic gesture recognition and we show that our model provides a substantial improvement in data representation performance and computational efficiency over the standard Gaussian model. © 2008 IEEE...|$|R
40|$|In {{the mixture}} models problem {{it is assumed}} that there are K {{distributions}} θ 1, [...] ., θK and one gets to observe a sample from a mixture of these distri-butions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the <b>hidden</b> <b>distributions.</b> In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sam-ple, we prove {{that it is possible to}} use the differences between the samples to better recover the underlying structure. We present algorithms that re-cover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data. ...|$|R
3000|$|However, to {{find the}} equilibrium, we must know the <b>hidden</b> state’s <b>distribution</b> for any period and for any {{possible}} signal sequence. Obviously, there is an enumeration method: calculate the distribution at each period given each signal sequence using the above algorithm until all possible cases are enumerated. But this method might waste {{a significant amount of}} computing resources. For example, when calculating Pr(w [...]...|$|R
30|$|Interactive {{networks}} have been represented as somewhat “ideal” environments where nodes are equally dispersed and connected {{in an environment}} devoid of the constraints of space or time. While {{it is certainly true}} that P 2 P interactivity offers immense new possibilities for learners, we must strive to understand the dialectical nature of the new environment. What appears to be an ideally democratic social space can in fact be subjected to <b>hidden</b> power <b>distribution</b> and arcane control.|$|R
40|$|This book {{provides}} {{solutions to}} the challenges involved in fringe pattern analysis, covering techniques for full-field, noncontact, and high-sensitivity measurement. The primary goal of fringe pattern analysis is to extract the <b>hidden</b> phase <b>distributions</b> that generally relate to the physical quantities being measured. Both theoretical analysis and algorithm development are covered to facilitate the work of researchers and engineers. The information presented is also appropriate as a specialized subject for students of optical and computer engineering...|$|R
40|$|Middleware {{facilitates}} {{the development of}} distributed systems by accommodating heterogeneity, <b>hiding</b> <b>distribution</b> details and providing a set of common and domain specific services. It plays a central and essential role for developing distributed systems. However, middleware is considered a mean rather than core elements of development process in the existing distributed systems software process. This paper explains the concept of middleware by categorizes middleware and analysis the problems of current middleware architectures. It also extracts three essential non-functional requirements of middleware and proposes a middleware-based distributed systems software process. The proposed software process consists in five phases: requirements analysis, design, validation, development and testing. The characteristics of middleware are considered in the entire software process. Component-Based Software Engineering, Separation of Concerns, Model-Driven Architecture, formal methods and Aspect Oriented Programming are five active research areas that {{have been around for}} several years now. In this paper, we present how these five paradigms can be put together {{in the context of a}} new software development method and we show how they can complement each other at different stages in the development life-cycle of middlewaremediated applications. Keywords: Distributed systems, Software process, Middleware, Model-Driven Architecture 1...|$|R
40|$|An {{axiomatics}} for indistinguishability {{of elementary}} particles {{in terms of}} hidden variables is presented in a manner which depart from the standard approaches usually given to <b>hidden</b> variables. Quantum <b>distribution</b> functions are also discussed and some related lines of work are suggested. Comment: 17 pages, LaTeX forma...|$|R
40|$|Abstract — This paper {{continues}} {{to explore the}} potential of newly introduced Fuzzy Gaussian Inference (FGI) [1]. It aims at constructing fuzzy membership functions by modelling <b>hidden</b> probability <b>distributions</b> underlying human motions. A fuzzy rule-based system has been employed to assist boxing motion classification from natural human Motion Capture data. In this experiment, FGI alone is able to recognise seven different boxing stances simultaneously with an accuracy superior to a GMM-based classifier. Results indicate that adding a Fuzzy Inference Engine on top of FGI improves {{the accuracy of the}} classifier in a consistent way. I...|$|R
40|$|ABSTRACT: We {{prove that}} a {{probabilistic}} average over possible, but not actual, <b>hidden</b> variable <b>distributions</b> maximizes predictive accuracy (defined {{in terms of}} the Kullback-Leibler discrepancy) within a context in which only the relative frequencies of hidden variables are known. Our detailed analysis of the Bernoulli model (e. g, coin flipping) reveals striking similarities with the derivation of thermodynamics from microphysics. In both cases, the macroscopic description is derived from a probabilistic average over possible microstates, or counterfactual hidden variables. In neither case is the macroscopic description deducible from a description of the microstate. ...|$|R
40|$|Programmers of {{parallel}} processes that communicate through shared globally distributed data structures (DDS) face a difficult choice. Either they must explicitly program DDS management, by partitioning or replicating it over multiple distributed memory modules, or {{be content with}} a high latency coherent (sequentially consistent) memory abstraction that <b>hides</b> the DDS' <b>distribution...</b>|$|R
40|$|Motivation: Reconstructing the full- length {{expressed}} transcripts (a. k. a. {{the transcript}} assembly problem) from the short sequencing reads produced by RNA-seq protocol plays {{a central role}} in identifying novel genes and transcripts as well as in studying gene expressions and gene functions. A crucial step in transcript assembly is to accurately determine the splicing junctions and boundaries of the expressed transcripts from the reads alignment. In contrast to the splicing junctions that can be efficiently detected from spliced reads, the problem of identifying boundaries remains open and challenging, {{due to the fact that}} the signal related to boundaries is noisy and weak. Results: We present DeepBound, an effective approach to identify boundaries of expressed transcripts from RNA-seq reads alignment. In its core DeepBound employs deep convolutional neural fields to learn the <b>hidden</b> <b>distributions</b> and patterns of boundaries. To accurately model the transition probabilities and to solve the label- imbalance problem, we novelly incorporate the AUC (area under the curve) score into the optimizing objective function. To address the issue that deep probabilistic graphical models requires large number of labeled training samples, we propose to use simulated RNA- seq datasets to train our model. Through extensive experimental studies on both simulation datasets of two species and biological datasets, we show that DeepBound consistently and significantly outperforms the two existing methods...|$|R
40|$|Abstract This {{research}} introduces {{and builds}} {{on the concept of}} Fuzzy Gaussian Inference(FGI) [1][2] as a novel way to build Fuzzy Membership Functions that map to <b>hidden</b> Probability <b>Distributions</b> underlying human motions. This method is now combined with a Genetic Programming Fuzzy rule-based system in order to classify boxing moves from natural human Motion Capture data. In this experiment, FGI alone is able to recognise seven different boxing stances simultaneously with an accuracy superior to a GMM-based classifier. Results seem to indicate that adding an evolutionary Fuzzy Inference Engine on top of FGI improves the accuracy of the classifier in a consistent way. ...|$|R
40|$|A point H {{is hidden}} in a rooted tree Q which is endowed with {{asymmetric}} distances (travel times) between nodes. We determine the randomized search strategy, starting from the root, which minimizes the expected time to reach H, in the worst case. This is equivalent to a zero-sum search game Γ(Q), with minimizing Searcher, maximizing Hider, and payoff equal to the capture time. The worst <b>Hiding</b> <b>distribution</b> (over the leaves) from the Searcher's viewpoint is one where at every node i the probability of each branch {{is proportional to the}} minimum time required to tour it from i. The optimal randomized search is a mixture over depth-first searches. We also consider briefly some other networks and the possibility of a mobile Hider. Our formulation with asymmetric travel times generalizes that of Gal [SIAM J. Control Optim., 17 (1979), pp. 99 – 122] for symmetric travel times and also the search games of Kikuta [J. Oper. Res., 38 (1995), pp. 70 – 88] and Kikuta and Ruckle [Naval Res. Logist., 41 (1994), pp. 821 – 831], who posited search costs c_i at each node i which were added to the travel time to obtain the payoff. We also briefly consider what happens if we allow the Searcher (Hider) to start (hide) at any leaf node. We determine when properties found by Dagan and Gal [Networks, 52 (2008), pp. 156 – 161] for the symmetric version of such games hold in our asymmetric context...|$|R
30|$|There are two {{termination}} {{conditions of}} the loop, one is to set the maximum termination algebra, {{and the other is}} to terminate the loop when the conditions are met. The other is when the variance between the individual fitness in the population is less than a certain set value, the loop is terminated. In this paper, the algorithm is terminated if it meets the condition that the upper bound reached by the evolution algebra is 200 or the fourth consecutive generation has no change condition. After the individual with the highest fitness value finally decodes according to the encoding rules, it becomes the information <b>hiding</b> optimal <b>distribution</b> scheme.|$|R
40|$|AdeepBoltzmannmachine(DBM) isarecentlyintroducedMarkovrandomfield {{model that}} has {{multiple}} layers of hidden units. It has been shown empirically {{that it is difficult}} to train a DBM with approximate maximum-likelihood learning using the stochastic gradient unlike its simpler special case, restricted Boltzmann machines (RBM). In this paper, we propose a novel pretraining algorithm that consists of two stages; obtaining approximate posterior <b>distributions</b> over <b>hidden</b> units from a simpler model and maximizing the variational lower-bound given the fixed <b>hidden</b> posterior <b>distributions.</b> We show empirically that the proposed method overcomes the difficulty in training DBMs from randomly initialized parameters and results in a better, or comparable, generative model when compared totheconventional pretraining algorithm. ...|$|R
30|$|After that, {{prediction}} error expansion (PEE) was proposed by Thodi and Rodriguez [4] as a generalized form of difference expansion. Prediction error which means {{difference between the}} original pixel and the predicted pixel is expanded for reversible data <b>hiding.</b> Probability <b>distribution</b> function of the {{prediction error}}s is sharper and narrower {{than that of the}} simple difference of the pixel values, which is better for reversible data hiding. Small distortion with large embedding capacity is a desirable feature of the reversible data hiding. Thodi and Rodriguez [4] also used the median edge detector (MED) as a predictor introduced for the lossless image compression standard such as JPEG-LS [5].|$|R
40|$|Abstract. This paper {{combines}} the novel concept of Fuzzy Gaussian Inference(FGI) with Genetic Programming (GP) {{in order to}} accurately classify real natural 3 d human Motion Capture data. FGI builds Fuzzy Membership Functions that map to <b>hidden</b> Probability <b>Distributions</b> underlying human motions, providing a suitable modelling paradigm for such noisy data. Genetic Programming (GP) is {{used to make a}} time dependent and context aware filter that improves the qualitative output of the classifier. Results show that FGI outperforms a GMM-based classifier when recognizing seven different boxing stances simultaneously, and that the addition of the GP based filter improves the accuracy of the FGI classifier significantly. ...|$|R
40|$|Multimedia {{systems are}} {{expected}} to support applications in almost every sector of our society. For such systems, the key information technology is the management of multimedia documents, as it is witnessed by the rapid proliferation of the Web. Synchronization and Quality of Service (QoS) management axe the two distinct multimedia application requirements for such systems. In this thesis, we focus on modeling, indexing and presentation aspects of multimedia documents in distributed network environments. ^ Unlike traditional alphanumeric data, multimedia documents require synchronized integration of different media types. Such an integration in turn needs identification of temporal and spatial relationships among multimedia objects. For this purpose, we propose a formal synchronization model, called Generalized Object Composition Petri Net (GOCPN). By incorporating the presentation information, time semantics, event-based logic and flexible execution rules, the proposed GOCPN can represent temporal, spatial, content synchronizations and causal dependencies and independencies of events within multimedia documents. In order to achieve efficient retrieval and indexing of multimedia information in the databases, We extend GOCPN and other media data in an object-oriented domain and develop a framework for specifying meta-schema for multimedia information. Based on the models, we developed a query processing system for searching multimedia objects and documents in multiple dimensions (content, temporal and spatial) with fuzzy matching capabilities in distributed databases. To support broad ranges of applications, we present an approach {{for the construction of}} a comprehensive system for creation, storing, searching, management, retrieval and interactive presentation of multimedia documents. We demonstrate how the system realizes the easy access and manipulation of multimedia information by <b>hiding</b> <b>distribution</b> aspects of the multimedia objects and servers. ...|$|R
40|$|Abstract. A deep Boltzmann machine (DBM) is a {{recently}} introduced Markov random field model that has {{multiple layers of}} hidden units. It has been shown empirically {{that it is difficult}} to train a DBM with approximate maximum-likelihood learning using the stochastic gradient unlike its simpler special case, restricted Boltzmann machine (RBM). In this paper, we propose a novel pretraining algo-rithm that consists of two stages; obtaining approximate posterior <b>distributions</b> over <b>hidden</b> units from a simpler model and maximizing the variational lower-bound given the fixed <b>hidden</b> posterior <b>distributions.</b> We show empirically that the proposed method overcomes the difficulty in training DBMs from randomly initialized parameters and results in a better, or comparable, generative model when compared to the conventional pretraining algorithm...|$|R
40|$|The ocean fishery and the {{corresponding}} environment are highly interrelated according to the production experiences of ocean fishing population. The spatial cluster patterns are constructed using the remote sensed data and long-time series fishery production data under the uniform coordinate based on GIS techniques. Thus, the <b>hidden</b> information of <b>distribution</b> regularities between ocean-hydrologic factors and central fishing ground can be extracted from these patterns. It is important to forecast the ocean fishery production...|$|R
