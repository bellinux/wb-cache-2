683|10000|Public
5|$|The <b>estimated</b> <b>probability</b> of Hale-Bopp's {{striking}} Earth {{in future}} passages through the inner Solar System is remote, about 2.5×10−9 per orbit. However, {{given that the}} comet nucleus is around 60km in diameter, the consequences of such an impact would be apocalyptic. Weissman conservatively estimates the diameter at 35km; an estimated density of 0.6 g/cm3 then gives a cometary mass of 1.3×1019 g. At a probable impact velocity of 52.5km/s, impact energy can be calculated as 1.9×1032 ergs, or 4.4×109 megatons, about 44 times the estimated energy of the K-T impact event.|$|E
500|$|The turrets were {{designed}} to mount the 8.8 cm KwK 43 L/71 gun. Combined with the Turmzielfernrohr 9d (German [...] "turret telescopic sight") monocular sight by Leitz, which {{all but a few}} early Tiger IIs used, it was a very accurate and deadly weapon. During practice, the <b>estimated</b> <b>probability</b> of a first round hit on a [...] high, [...] wide target only dropped below 100 percent at ranges beyond , to 95–97 percent at [...] and 85–87 percent at , depending on ammunition type. Recorded combat performance was lower, but still over 80 percent at 1,000m, in the 60s at 1,500m and the 40s at 2,000m. Penetration of armoured plate inclined at 30 degrees was [...] at [...] and [...] respectively for the Panzergranate 39/43 projectile (PzGr—armour-piercing shell), and [...] for the PzGr. 40/43 projectile between the same ranges. The Sprenggranate 43 (SpGr) high-explosive round was available for soft targets, or the Hohlgranate or Hohlgeschoss 39 (HlGr—HEAT or High-explosive anti-tank warhead) round, which had [...] penetration at any range, {{could be used as a}} dual-purpose munition against soft or armoured targets.|$|E
500|$|Suppose {{we start}} with one {{electron}} at a certain place and time (this place and time being given the arbitrary label A) and a photon at another place and time (given the label B). A typical question from a physical standpoint is: 'What is the probability of finding an electron at C (another place and a later time) and a photon at D (yet another place and time)?'. The simplest process to achieve this end is for the electron to move from A to C (an elementary action) and for the photon to move from B to D (another elementary action). From {{a knowledge of the}} probability amplitudes of each of these sub-processes – E(A to C) and P(B to D) – then we would expect to calculate the probability amplitude of both happening together by multiplying them, using rule b) above. This gives a simple estimated overall probability amplitude, which is squared to give an <b>estimated</b> <b>probability.</b> [...] But there are other ways in which the end result could come about. The electron might move to a place and time E where it absorbs the photon; then move on before emitting another photon at F; then move on to C where it is detected, [...] while the new photon moves on to D. The probability of this complex process can again be calculated by knowing the probability amplitudes of each of the individual actions: three electron actions, two photon actions and two vertexes – one emission and one absorption. We would expect to find the total probability amplitude by multiplying the probability amplitudes of each of the actions, for any chosen positions of E and F. We then, using rule a) above, have to add up all these probability amplitudes for all the alternatives for E and F. (This is not elementary in practice, and involves integration.) But there is another possibility, which is that the electron first moves to G where it emits a photon which goes on to D, while the electron moves on to H, where it absorbs the first photon, before moving on to C. Again we can calculate the probability amplitude of these possibilities (for all points G and H). We then have a better estimation for the total probability amplitude by adding the probability amplitudes of these two possibilities to our original simple estimate. Incidentally the name given to this process of a photon interacting with an electron in this way is Compton scattering.|$|E
40|$|The {{research}} {{examines how}} {{high school students}} solve simple probability problems, and the probability estimations of students, boys and girls, in a discrete and uniform probability space. In discrete uniform probability spaces problem solving using estimations of probabilities and strategies of solving problems {{in a variety of}} situations were described. Solving probability problems among elementary intermediate and high school students were studied, however, reviewing the literature in probability; I am not familiar with any research on <b>estimating</b> <b>probabilities</b> or characterizations of rules people use for <b>estimating</b> and solving <b>probability</b> problems relating specifically to numerically symmetric and asymmetric objects. The research describes the rules and heuristics that student use to solve <b>probability</b> problems and <b>estimate</b> <b>probabilities.</b> The research has shown that students at different levels of proficiency in solving probability problems take a different approach when solving and <b>estimating</b> <b>probabilities.</b> When students are asked to <b>estimate</b> the <b>probability</b> of selecting two beads with different colors from a bag containing beads of different colors some calculate <b>probabilities</b> and other <b>estimate</b> them using a variety of rules. The use of rules for <b>estimating</b> <b>probabilities</b> is directly related to numerical relations and symmetries of objects. I describe a variety of rules students use for <b>estimating</b> <b>probabilities,</b> e. g., the "pair rule: if the number of pairs of objects in a group is greater than the number of the other pairs the probability of drawing a pair of first  kind is greater than drawing pairs of the second kind". I present a mechanism for the estimations and decisions taken by students when <b>estimating</b> <b>probabilities.</b> </p...|$|R
5000|$|Requirement to use {{long-term}} data horizons to <b>estimate</b> <b>probabilities</b> of default, ...|$|R
25|$|A <b>probability</b> {{model for}} <b>estimating</b> <b>probability</b> of an {{earthquake}} during a specified interval.|$|R
2500|$|... 5. <b>Estimated</b> <b>probability</b> of a M≥6.7 {{event in}} 30 years. From UCERF-2 Table 12.|$|E
2500|$|... 2182: With an <b>estimated</b> <b>probability</b> of 0.07%, Apollo {{asteroid}} 1999 RQ36 {{could hit}} the Earth.|$|E
2500|$|In binary classification, {{the class}} {{prediction}} for each instance is often made {{based on a}} continuous random variable , which is a [...] "score" [...] computed for the instance (e.g. <b>estimated</b> <b>probability</b> in logistic regression). Given a threshold parameter , the instance is classified as [...] "positive" [...] if , and [...] "negative" [...] otherwise. [...] follows a probability density [...] if the instance actually belongs to class [...] "positive", and [...] if otherwise. Therefore, the true positive rate is given by [...] and the false positive rate is given by [...]|$|E
40|$|Accurately <b>estimating</b> <b>probabilities</b> from {{observations}} {{is important}} for probabilistic-based approaches to problems in computational biology. In this paper we present a biologically-motivated method for <b>estimating</b> <b>probability</b> distributions over discrete alphabets from observations using a mixture model of common ancestors. The method {{is an extension of}} substitution matrix-based probability estimation methods. In contrast to previous substitution matrix-based methods, our method has a simple Bayesian interpretation. The method presented in this paper has the advantage over Dirichlet mixtures that it is both effective and simple to compute for large alphabets. The method is applied to <b>estimate</b> amino acid <b>probabilities</b> based on observed counts in an alignment and is shown to perform comparable to previous methods. The method is also applied to <b>estimate</b> <b>probability</b> distributions over protein families and improves protein classification accuracy...|$|R
2500|$|Uncalibrated class {{membership}} probabilities -- SVM {{stems from}} Vapnik's theory which avoids <b>estimating</b> <b>probabilities</b> on finite data ...|$|R
50|$|An {{advantage}} of <b>estimating</b> <b>probabilities</b> using empirical probabilities {{is that this}} procedure is relatively free of assumptions.|$|R
2500|$|At {{times there}} has been {{disagreement}} about which mapping Illumina actually uses. The user guide (Appendix B, page 122) for version 1.4 of the Illumina pipeline states that: [...] "The scores are defined as Q=10*log10(p/(1-p)) , where p is {{the probability of a}} base call corresponding to the base in question". In retrospect, this entry in the manual appears to have been an error. The user guide (What's New, page 5) for version 1.5 of the Illumina pipeline lists this description instead: [...] "Important Changes in Pipeline v1.3 [...] The quality scoring scheme has changed to the Phred [...] scoring scheme, encoded as an ASCII character by adding 64 to the Phred value. A Phred score of a base is: , where e is the <b>estimated</b> <b>probability</b> of a base being wrong.|$|E
5000|$|Wetland Indicator Status: FACU (Facultative Upland) Usually {{occurs in}} non-wetlands (<b>estimated</b> <b>probability</b> 67%-99%), but {{occasionally}} found on wetlands (<b>estimated</b> <b>probability</b> 1%-33%).|$|E
5000|$|Facultative upland (FACU). Usually {{occur in}} non-wetlands (<b>estimated</b> <b>probability</b> 67% - 99%), but {{occasionally}} found in wetlands (<b>estimated</b> <b>probability</b> 1% - 33%).|$|E
5000|$|Uncalibrated class {{membership}} probabilities -- SVM {{stems from}} Vapnik's theory which avoids <b>estimating</b> <b>probabilities</b> on finite data ...|$|R
40|$|Economists {{attribute}} many common {{behaviors to}} risk aversion and frequently {{focus on how}} wealth moderates risk preferences. This paper highlights a problem associated with empirical tests {{of the relationship between}} wealth and risk aversion that can arise when the probabilities individuals face are unobservable to researchers. The common remedy for unobservable probabilities involves the estimation of probabilities in a profit or production that includes farmer, farm and agro-climatic variables. Unfortunately, these variables are often correlated with wealth such that <b>estimated</b> <b>probabilities</b> are likely to leave statistical fingerprints on subsequently-estimated risk aversion coefficients and may thereby introduce spurious correlations between wealth and risk preferences. In this paper, we use data from an experiment conducted among 290 Indian farmers to detect these spurious correlations. We estimate coefficients of risk aversion with known <b>probabilities</b> and with <b>estimated</b> <b>probabilities</b> and compare subsequent correlations with wealth and other farmer traits. We <b>estimate</b> 'unobservable' <b>probabilities</b> in conjunction with risk preferences following a standard field data approach. We explore the statistical implications of <b>estimating</b> <b>probabilities</b> by comparing correlations between wealth and these two sets of estimated risk preferences. These comparisons show how <b>estimated</b> <b>probabilities</b> can change risk aversion coefficients substantially and introduce spurious correlations between risk aversion and wealth. Risk and Uncertainty,...|$|R
40|$|This paper {{describes}} how to <b>estimate</b> <b>probabilities</b> and outcome values for decision trees. Probabilities are usually derived from published studies, but occasionally {{are derived from}} existing databases, primary data collection, or expert judgment. Outcome values represent quantitative estimates of the desirability of the outcome states, and are often expressed as utility values between 0 and 1. Utility values for different health states {{can be derived from}} the published literature, from direct measurement in appropriate subjects, or from expert opinion. Methods for assigning utilities to complex outcome states are described, and the concept of quality-adjusted life years is introduced. Key words: decision analysis; expected value; utility; sensitivity analysis; decision trees; probability. (Med Decis Making 1997; 17 : 136 - 141) Probabilities and outcome values are two of the basic elements of a decision analysis. A probability is a quantitative estimate of the likelihood that a given outcome depicted in the tree will occur. An outcome value is a quantitative expression of the desirability of such an outcome. The validity of a decision analysis depends on the accuracy of these numerical estimates. This paper reviews some practical approaches for <b>estimating</b> <b>probabilities</b> and outcome values. <b>Estimating</b> <b>Probabilities</b> The goal of <b>estimating</b> <b>probabilities</b> for a decision tree is to find the most accurate <b>estimate</b> for the <b>probability</b> of each event in the model. The best <b>estimate</b> for each <b>probability</b> value is called the “baseline” estimate. The analysis that uses the best <b>estimates</b> of the <b>probabilities</b> is called a “baseline” analysis. Since there is usually some uncertainty about the best <b>estimate</b> for each <b>probability,</b> th...|$|R
5000|$|Facultative (FAC). Equally {{likely to}} occur in {{wetlands}} (<b>estimated</b> <b>probability</b> 34% - 66%) or non-wetlands.|$|E
5000|$|Obligate upland (UPL). Occur {{almost always}} (<b>estimated</b> <b>probability</b> > 99%) in non-wetlands under natural conditions.|$|E
5000|$|Obligate wetland (OBL). Almost always {{occurs in}} {{wetlands}} (<b>estimated</b> <b>probability</b> > 99%) under natural conditions ...|$|E
2500|$|A {{clinical}} prediction rule {{is available}} to <b>estimate</b> <b>probability</b> of nephropathy (increase ≥25% and/or ≥0.5mg/dl in serum creatinine at 48 h): ...|$|R
40|$|The {{result of}} {{training}} a HMM using supervised training is <b>estimated</b> <b>probabilities</b> for emissions and transitions. There are two difficulties {{with this approach}} Firstly, sparse training data causes poor <b>probability</b> <b>estimates.</b> Secondly, unseen <b>probabilities</b> have emission probability of zero. In this thesis, we report on different smoothing techniques and their implementations. We further report on our experimental results using standard precision and recall for various smoothing techniques...|$|R
40|$|Business {{continuity}} {{planning is}} a process to ensure that an organisation can continue to function effectively and resiliently when faced with crisis events. A key phase of the process is risk analysis, which involves identifying events, determining causes, and <b>estimating</b> <b>probabilities</b> and impact. In this paper we focus on <b>estimating</b> <b>probabilities.</b> Current practice often relies on ad hoc methods such as questionnaires or perusing historical records. We ground our discussion in concepts of reliability theory (used successfully {{over the years in}} <b>estimating</b> failure <b>probabilities</b> for physical systems) and simulation modelling. We develop and exercise some elementary models to illustrate the power of using these analytical methods. business continuity planning; BCP; reliability; simulation; risk assessment; risk modelling; failure probability...|$|R
5000|$|Regions 1-5: Facultative Equally (FAC) {{likely to}} occur in {{wetlands}} or non-wetlands (<b>estimated</b> <b>probability</b> 34%-66%).|$|E
5000|$|... 2182: With an <b>estimated</b> <b>probability</b> of 0.07%, Apollo {{asteroid}} 1999 RQ36 {{could hit}} the Earth.|$|E
50|$|Region 6: Facultative Wetland (FACW) Usually {{occurs in}} {{wetlands}} (<b>estimated</b> <b>probability</b> 67%-99%), but occasionally found in non-wetlands.|$|E
3000|$|..., {{formulas}} {{similar to}} Equation (28) yield the <b>estimated</b> <b>probabilities</b> for r(t) and the pairs (r(t),r(t+ 1)). The transition probabilities are then {{obtained by the}} relation [...]...|$|R
40|$|Probabilities are {{important}} measures of random performances. They {{are widely used}} in financial and service industries. <b>Probabilities</b> are often <b>estimated</b> through running simulation experiments. Probability sensitivities provide information on how changes in the input parameters affect the output probabilities. They {{are important}} for understanding and controlling stochastic systems. In this paper, we show {{how to use the}} simulated data, which are used to <b>estimate</b> <b>probability,</b> to <b>estimate</b> <b>probability</b> sensitivities. Our estimator is consistent, asymptotically normally distributed, and works for both terminating and steady-state simulations. ...|$|R
3000|$|Stochastic {{processes}} are defined based on probability theory. When we use it, {{a large sample size}} {{is needed to}} <b>estimate</b> <b>probability</b> distribution based on long-run frequency. However, Liu [...]...|$|R
5000|$|Facultative wetland (FACW). Usually {{occurs in}} {{wetlands}} (<b>estimated</b> <b>probability</b> 67% - 99%), but occasionally found in non-wetlands.|$|E
5000|$|Similarly, for {{a student}} who studies 4 hours, the <b>estimated</b> <b>probability</b> of passing the exam is 0.87: ...|$|E
50|$|In some applications, the {{triangular}} distribution is used directly as an <b>estimated</b> <b>probability</b> distribution, {{rather than for}} the derivation of estimated statistics.|$|E
40|$|<b>Estimating</b> the <b>probability</b> of {{relevance}} for a document is fundamental in information retrieval. From a theoretical point of view, risk {{exists in the}} estimation process, {{in the sense that}} the <b>estimated</b> <b>probabilities</b> may not be the actual ones precisely. The estimation risk is often considered to be dependent on the rank. For example, the probability ranking principle assumes that ranking documents in the order of decreasing probability {{of relevance}} can optimize the rank effectiveness. This implies that a precise estimation can yield an optimal rank. However, an optimal (or even ideal) rank does not always guarantee that the <b>estimated</b> <b>probabilities</b> are precise. This means that part of the estimation risk is rank-independent. It imposes practical risks in the applications, such as pseudo relevance feedback, where different <b>estimated</b> <b>probabilities</b> of relevance in the first-round retrieval will make a difference even when two ranks are identical. In this paper, we will explore the effect and the modeling of such rank-independent risk. A risk management method is proposed to adaptively adjust the rank-independent risk. Experimental results on several TREC collections demonstrate the effectiveness of the proposed models for both pseudo-relevance feedback and relevance feedback. ...|$|R
3000|$|..., {{one has to}} <b>estimate</b> the <b>probability</b> densities f_X^ 1 (x) and f_X^ 0 (x). In this paper, we {{will focus}} on the use of Fourier {{inversion}} approach and its use for <b>estimating</b> error <b>probability.</b>|$|R
40|$|Summary Estimating the {{likelihood}} of future climate change has become a topical matter within the research community. This is the case because of the advancement of science, user demand and the central role played by prediction in guiding policy. But are probabilities what climate policy really needs? This paper reviews three key questions: (1) why might we need probabilities of climate change? (2) what are the problems in <b>estimating</b> <b>probabilities?</b> (3) how are researchers <b>estimating</b> <b>probabilities?</b> The first question is primarily analysed {{within the context of}} adaptation to climate change, but mitigation and integrated assessment are also briefly discussed. The second question explores the types and sources of uncertainties involved in <b>estimating</b> <b>probabilities</b> of climate change and how their characterisation can be controversial. For the third question, an extensive review of the literature is conducted on research that is creating the building blocks towards estimating {{the likelihood}} of climate change. Overall, we conclude that {{the jury is still out}} on whether probabilities are useful for climate policy. The answer is highly context dependent and thus is a function of the goals an...|$|R
