6434|3699|Public
25|$|The Hermite polynomials are {{orthogonal}} {{with respect}} to the <b>Gaussian</b> <b>distribution</b> with zero mean value.|$|E
25|$|One {{example of}} this {{situation}} is when (X, Y) have a bivariate normal (<b>Gaussian)</b> <b>distribution.</b>|$|E
25|$|A random {{variable}} with a <b>Gaussian</b> <b>distribution</b> {{is said to}} be normally distributed and is called a normal deviate.|$|E
40|$|This paper {{analyses}} the kernel density estimation on {{spaces of}} <b>Gaussian</b> <b>distributions</b> endowed with different metrics. Explicit expressions of kernels are {{provided for the}} case of the 2 -Wasserstein metric on multivariate <b>Gaussian</b> <b>distributions</b> and for the Fisher metric on multivariate centred distributions. Under the Fisher metric, the space of multivariate centred <b>Gaussian</b> <b>distributions</b> is isometric to the space of symmetric positive definite matrices under the affine-invariant metric and the space of univariate <b>Gaussian</b> <b>distributions</b> is isometric to the hyperbolic space. Thus kernel are also valid on these spaces. The density estimation is successfully applied to a classification problem of electro-encephalographic signals...|$|R
5000|$|It is inefficient (37% efficiency) at <b>Gaussian</b> <b>distributions.</b>|$|R
40|$|A novel {{in-service}} signal {{quality monitoring}} technique for nonreturn-to-zero {{differential phase-shift keying}} signals using asynchronous amplitude histogram evaluation has been demonstrated. As a result of phase-to-amplitude modulation, there are three <b>Gaussian</b> <b>distributions</b> in the amplitude histogram. The widths of the <b>Gaussian</b> <b>distributions</b> {{are related to the}} amount of amplified spontaneous emission, while the separations between the <b>Gaussian</b> <b>distributions</b> increase monotonically with fiber dispersion. The amount of dispersion and optical signal-to-noise ratio can, thus, be directly extracted and discriminated from the amplitude histogram. Department of Electronic and Information Engineerin...|$|R
25|$|For large k the gamma {{distribution}} converges to <b>Gaussian</b> <b>distribution</b> with mean μ = kθ and variance σ2 = kθ2.|$|E
25|$|The gamma {{distribution}} {{is a special}} case of the generalized {{gamma distribution}}, the generalized integer gamma distribution, and the generalized inverse <b>Gaussian</b> <b>distribution.</b>|$|E
25|$|When {{the mean}} is not known, the minimum {{mean squared error}} {{estimate}} of the variance of a sample from <b>Gaussian</b> <b>distribution</b> is achieved by dividing by nnbsp&+nbsp&1, rather than nnbsp&−nbsp&1 or nnbsp&+nbsp&2.|$|E
5000|$|... #Caption: Density-based {{clusters}} {{cannot be}} modeled using <b>Gaussian</b> <b>distributions</b> ...|$|R
50|$|In that example, {{the first}} model has a higher {{dimension}} than the second model (the zero-mean model has dimension 1). Such is usually, but not always, the case. As a different example, the set of positive-mean <b>Gaussian</b> <b>distributions,</b> which has dimension 2, is nested within the set of all <b>Gaussian</b> <b>distributions.</b>|$|R
30|$|Context {{recognition}} {{was performed}} {{using the method}} presented in Section 4.1. The number of <b>Gaussian</b> <b>distributions</b> in the GMM model was fixed to 32 for each context class. This amount of <b>Gaussian</b> <b>distributions</b> was found to give a good compromise between computational complexity and recognition performance in the preliminary studies conducted with the development set.|$|R
25|$|In {{probability}} theory, {{the normal}} (or <b>Gaussian)</b> <b>distribution</b> {{is a very}} common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known.|$|E
25|$|Much as {{the central}} limit theorem {{requires}} certain kinds of random variables to have as a focus of convergence the <b>Gaussian</b> <b>distribution</b> and express white noise, the Tweedie convergence theorem requires certain non-Gaussian random variables to express 1/f noise and fluctuation scaling.|$|E
25|$|The line {{shape of}} the phonon side band {{is that of a}} Poisson {{distribution}} as it expresses a discrete number of events, electronic transitions with phonons, during a period of time. At higher temperatures, or when the chromophore interacts strongly with the matrix, the probability of multiphonon is high and the phonon side band approximates a <b>Gaussian</b> <b>distribution.</b>|$|E
3000|$|... and for {{heteroscedastic}} <b>Gaussian</b> <b>distributions</b> {{were derived}} in (Richter [2009], [2013]), respectively.|$|R
40|$|It is rigorously {{shown that}} the {{generalized}} Laplace distributions and the normal inverse <b>Gaussian</b> <b>distributions</b> are the only subclasses of the generalized hyperbolic distributions that are closed under convolution. The result is obtained by showing that the corresponding two classes of variance mixing distributionsgamma and inverse Gaussianare the only convolution-invariant classes of the generalized inverse <b>Gaussian</b> <b>distributions...</b>|$|R
40|$|Background {{identification}} {{is a fundamental}} task in many video processing systems. The Gaussian Mixture Model is a background identification algorithm that models the pixel luminance {{with a mixture of}} K <b>Gaussian</b> <b>distributions.</b> The number of <b>Gaussian</b> <b>distributions</b> determines the accuracy of the background model and the computational complexity of the algorithm. This paper compares two hardware implementations of the Gaussian Mixture Model that use three and five Gaussians per pixel. A trade off analysis is carried out by evaluating the quality of the processed video sequences and the hardware performances. The circuits are implemented on FPGA by exploiting state of the art, hardware oriented, formulation of the Gaussian Mixture Model equations and by using truncated binary multipliers. The results suggest that the circuit that uses three <b>Gaussian</b> <b>distributions</b> provides video with good accuracy while requiring significant less resources than the option that uses five <b>Gaussian</b> <b>distributions</b> per pixel...|$|R
25|$|One can {{generate}} Student-t samples {{by taking the}} ratio of variables from the normal distribution and the square-root of chi-squared distribution. If we use instead of the normal distribution, e.g., the Irwin–Hall distribution, we obtain over-all a symmetric 4-parameter distribution, which includes the normal, the uniform, the triangular, the Student-t and the Cauchy distribution. This is also more flexible than some other symmetric generalizations of the <b>Gaussian</b> <b>distribution.</b>|$|E
25|$|The <b>Gaussian</b> <b>distribution</b> {{belongs to}} the family of stable {{distributions}} which are the attractors of sums of independent, identically distributed distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance. It {{is one of the few}} distributions that are stable and that have probability density functions that can be expressed analytically, the others being the Cauchy distribution and the Lévy distribution.|$|E
25|$|Gamma rays {{detected}} in a spectroscopic system produce {{peaks in the}} spectrum. These peaks can also be called lines by analogy to optical spectroscopy. The width of the peaks {{is determined by the}} resolution of the detector, a very important characteristic of gamma spectroscopic detectors, and high resolution enables the spectroscopist to separate two gamma lines that are close to each other. Gamma spectroscopy systems are designed and adjusted to produce symmetrical peaks of the best possible resolution. The peak shape is usually a <b>Gaussian</b> <b>distribution.</b> In most spectra the horizontal position of the peak is determined by the gamma ray's energy, and the area of the peak is determined by the intensity of the gamma ray and the efficiency of the detector.|$|E
50|$|This {{identify}} {{is useful}} in developing a Bayes estimator for multivariate <b>Gaussian</b> <b>distributions.</b>|$|R
50|$|Two {{statistical}} models are nested {{if the first}} model can be transformed into the second model by imposing constraints on {{the parameters of the}} first model. For example, the set of all <b>Gaussian</b> <b>distributions</b> has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all <b>Gaussian</b> <b>distributions</b> to get the zero-mean distributions.|$|R
3000|$|<b>Gaussian</b> <b>distributions,</b> and the {{one that}} maximizes the {{probability}} of producing that color is selected; [...]...|$|R
25|$|Using several {{data sets}} (including {{breeding}} bird surveys from New York and Pennsylvania and moth collections from Maine, Alberta and Saskatchewan) Frank W. Preston (1948) argued that species abundances (when binned logarithmically in a Preston plot) follow a Normal (<b>Gaussian)</b> <b>distribution,</b> {{partly as a}} result of the Central Limit Theorem (Figure 4). This means that the abundance distribution is Lognormal. According to his argument, the right-skew observed in species abundance frequency histograms (including those described by Fisher et al. (1943)) was, in fact, a sampling artifact. Given that species toward the left side of the x-axis are increasingly rare, they may be missed in a random species sample. As the sample size increases however, the likelihood of collecting rare species in a way that accurately represents their abundance also increases, and more of the normal distribution becomes visible. The point at which rare species cease to be sampled has been termed Preston's veil line. As the sample size increases Preston's veil is pushed farther to the left and more of the normal curve becomes visible(Figure 6). Interestingly, Williams' moth data, originally used by Fisher to develop the logseries distribution, became increasingly lognormal as more years of sampling were completed.|$|E
25|$|In the {{original}} Franck–Condon principle, after the electronic transition, the molecules which {{end up in}} higher vibrational states immediately begin to relax to the lowest vibrational state. In the case of solvation, the solvent molecules will immediately try to rearrange themselves {{in order to minimize}} the interaction energy. The rate of solvent relaxation depends on the viscosity of the solvent. Assuming the solvent relaxation time is short compared with the lifetime of the electronic excited state, emission will be from the lowest solvent energy state of the excited electronic state. For small-molecule solvents such as water or methanol at ambient temperature, solvent relaxation time is on the order of some tens of picoseconds whereas chromophore excited state lifetimes range from a few picoseconds to a few nanoseconds. Immediately after the transition to the ground electronic state, the solvent molecules must also rearrange themselves to accommodate the new electronic configuration of the chromophore. Figure 7 illustrates the Franck–Condon principle applied to solvation. When the solution is illuminated by with light corresponding to the electronic transition energy, some of the chromophores will move to the excited state. Within this group of chromophores there will be a statistical distribution of solvent-chromophore interaction energies, represented in the figure by a <b>Gaussian</b> <b>distribution</b> function. The solvent-chromophore interaction is drawn as a parabolic potential in both electronic states. Since the electronic transition is essentially instantaneous on the time scale of solvent motion (vertical arrow), the collection of excited state chromophores is immediately far from equilibrium. The rearrangement of the solvent molecules according to the new potential energy curve is represented by the curved arrows in Figure 7. Note that while the electronic transitions are quantized, the chromophore-solvent interaction energy is treated as a classical continuum due to the large number of molecules involved. Although emission is depicted as taking place from the minimum of the excited state chromophore-solvent interaction potential, significant emission can take place before equilibrium is reached when the viscosity of the solvent is high or the lifetime of the excited state is short. The energy difference between absorbed and emitted photons depicted in Figure 7 is the solvation contribution to the Stokes shift.|$|E
500|$|The {{technical}} {{literature on}} air pollution dispersion is quite extensive and {{dates back to}} the 1930s and earlier. One of the early air pollutant plume dispersion equations was derived by Bosanquet and Pearson. Their equation did not assume <b>Gaussian</b> <b>distribution</b> nor did it include the effect of ground reflection of the pollutant plume. [...] Sir Graham Sutton derived an air pollutant plume dispersion equation in 1947 which did include the assumption of <b>Gaussian</b> <b>distribution</b> for the vertical and crosswind dispersion of the plume and also included the effect of ground reflection of the plume. [...] Under the stimulus provided by the advent of stringent environmental control regulations, there was an immense growth in the use of air pollutant plume dispersion calculations between the late 1960s and today. A great many computer programs for calculating the dispersion of air pollutant emissions were developed {{during that period of time}} and they were called [...] "air dispersion models". The basis for most of those models was the Complete Equation For Gaussian Dispersion Modeling Of Continuous, Buoyant Air Pollution Plumes [...] The Gaussian air pollutant dispersion equation requires the input of H which is the pollutant plume's centerline height above ground level—and H is the sum of Hs (the actual physical height of the pollutant plume's emission source point) plus ΔH (the plume rise due the plume's buoyancy).|$|E
3000|$|Emission probabilities are modeled as <b>Gaussian</b> <b>distributions</b> whose {{parameters}} are the mean vector and covariance matrix ((μ [...]...|$|R
3000|$|... is {{a mixture}} of <b>Gaussian</b> <b>{{distribution}}s.</b> Sampling from this, a distribution is very simple. First, draw a sample [...]...|$|R
40|$|Abstract We explicate a semi-automated {{statistical}} algorithm for object identi-fication and {{segregation in}} both gray scale and color images. The algorithm makes optimal {{use of the}} observation that definite objects in an image are typically represented by pixel values having narrow <b>Gaussian</b> <b>distributions</b> about charac-teristic mean values. Furthermore, for visually distinct objects, the corresponding <b>Gaussian</b> <b>distributions</b> have negligible overlap {{with each other and}} hence the Ma-halanobis distance between these distributions are large. These statistical facts enable one to sub-divide images into multiple thresholds of variable sizes, each segregating similar objects. The procedure incorporates the sensitivity of human eye to the gray pixel values into the variable threshold size, while mapping the <b>Gaussian</b> <b>distributions</b> into localized δ−functions, for object separation. The effec-tiveness of this recursive statistical algorithm is demonstrated using a wide variety of images...|$|R
2500|$|... {{which can}} be {{estimated}} as the R squared from a non-linear regression of Y on X, using data drawn from the joint distribution of (X,Y). When [...] has a <b>Gaussian</b> <b>distribution</b> (and is an invertible function of X), or Y itself has a (marginal) <b>Gaussian</b> <b>distribution,</b> this explained component of variation sets a lower bound on the mutual information: ...|$|E
2500|$|This {{distribution}} {{results from}} compounding a <b>Gaussian</b> <b>distribution</b> (normal distribution) with mean [...] and unknown variance, with an inverse gamma distribution {{placed over the}} variance with parameters [...] and [...] In other words, the random variable X is assumed to have a <b>Gaussian</b> <b>distribution</b> with an unknown variance distributed as inverse gamma, and then the variance is marginalized out (integrated out). The reason for the usefulness of this characterization is that the inverse gamma distribution is the conjugate prior distribution of the variance of a <b>Gaussian</b> <b>distribution.</b> As a result, the non-standardized Student's t-distribution arises naturally in many Bayesian inference problems. [...] See below.|$|E
2500|$|Normal {{distribution}} (<b>Gaussian</b> <b>distribution),</b> for {{a single}} such quantity; the most common continuous distribution ...|$|E
5000|$|... #Caption: Illustration of the Kullback-Leibler (KL) {{divergence}} for two normal <b>Gaussian</b> <b>distributions.</b> Note {{the typical}} asymmetry for the Kullback-Leibler divergence is clearly visible.|$|R
30|$|In this study, we extend {{previous}} approaches [3] {{by allowing}} the source priors to be a mixture of multivariate <b>Gaussian</b> <b>distributions</b> for each pixel.|$|R
3000|$|Evaluate {{the sum of}} the <b>Gaussian</b> <b>distributions</b> {{using the}} squared Euclidean {{distances}} between the test feature vector x and the training vectors x [...]...|$|R
