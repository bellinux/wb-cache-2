37|27|Public
25|$|Unlike {{solid-state}} memory, {{hard drives}} {{are susceptible to}} damage by shock (e.g., a short fall) and vibration, have limitations on use at high altitude, and although they are shielded by their casings, they are vulnerable when exposed to strong magnetic fields. In terms of overall mass, hard drives are usually larger and heavier than flash drives; however, hard disks sometimes weigh less per unit of storage. Like flash drives, hard disks also suffer from <b>file</b> <b>fragmentation,</b> which can reduce access speed.|$|E
25|$|Other {{high-level}} mechanisms may read in {{and process}} larger parts or the complete FAT on startup or on demand when needed and dynamically build up in-memory tree {{representations of the}} volume's file structures different from the on-disk structures. This may, on volumes with many free clusters, occupy even less memory than {{an image of the}} FAT itself. In particular on highly fragmented or filled volumes, seeks become much faster than with linear scans over the actual FAT, even if an image of the FAT would be stored in memory. Also, operating on the logically high level of files and cluster-chains instead of on sector or track level, it becomes possible to avoid some degree of <b>file</b> <b>fragmentation</b> in the first place or to carry out local file defragmentation and reordering of directory entries based on their names or access patterns in the background.|$|E
5000|$|... #Caption: Oversimplified {{example of}} how free space {{fragmentation}} and <b>file</b> <b>fragmentation</b> occur ...|$|E
50|$|In computing, <b>file</b> system <b>fragmentation,</b> {{sometimes}} called <b>file</b> system aging, is {{the tendency of}} a file system {{to lay out the}} contents of files non-contiguously to allow in-place modification of their contents. It is a special case of data <b>fragmentation.</b> <b>File</b> system <b>fragmentation</b> increases disk head movement or seek time, which are known to hinder throughput. In addition, file systems cannot sustain unlimited fragmentation. The correction to existing fragmentation is to reorganize files and free space back into contiguous areas, a process called defragmentation.|$|R
5000|$|Since writes are permanent, {{the file}} system is append-only (which {{allows for a}} simple {{implementation}} with lower chance of data-destroying bugs); no <b>file</b> system <b>fragmentation</b> occurs.|$|R
5000|$|... #Caption: A roaming {{profile that}} is several years old can contain tens of {{thousands}} of cookies, which make network login and logout extremely slow, and contribute to <b>file</b> system <b>fragmentation.</b>|$|R
5000|$|Individual <b>file</b> <b>fragmentation</b> {{occurs when}} a single file has been broken into {{multiple}} pieces (called extents on extent-based file systems). While disk file systems attempt to keep individual files contiguous, this is not often possible without significant performance penalties. File system check and defragmentation tools typically only account for <b>file</b> <b>fragmentation</b> in their [...] "fragmentation percentage" [...] statistic.|$|E
5000|$|ADFS, {{the file}} system used by RISC OS and earlier Acorn Computers, keeps <b>file</b> <b>fragmentation</b> under control without {{requiring}} manual defragmentation.|$|E
50|$|According to Scott Hanselman of Microsoft, Windows 7 {{and later}} do {{defragment}} a solid-state disk (SSD) {{but in a}} completely different way. There is less incentive for defragmentation of SSDs because <b>file</b> <b>fragmentation</b> has less performance impact on them and they handle a finite number of storage cycles before their lifespan expires. However, file systems cannot support infinite <b>file</b> <b>fragmentation</b> as they reach their metadata limit. In addition, Disk Defragmenter is also responsible for performing the TRIM command on SSDs.|$|E
50|$|In {{addition}} to resolving <b>file</b> system <b>fragmentation,</b> Diskeeper also prevents fragmentation before it happens by optimizing {{the process of}} data being written to disk. Diskeeper can also defragment files on-the-fly, using only idle system resources as needed.|$|R
5000|$|There can {{be severe}} <b>file</b> system <b>fragmentation</b> in LFS, {{especially}} for slowly growing files or multiple simultaneous large writes. This inflicts a severe performance penalty, {{even though the}} design rationale for log-structured file systems assumes disk reads will mostly be cached away.|$|R
50|$|<b>File</b> system <b>fragmentation</b> is more {{problematic}} with consumer-grade hard disk drives {{because of the}} increasing disparity between sequential access speed and rotational latency (and {{to a lesser extent}} seek time) on which file systems are usually placed. Thus, fragmentation is an important problem in file system research and design. The containment of fragmentation not only depends on the on-disk format of the file system, but also heavily on its implementation. <b>File</b> system <b>fragmentation</b> has less performance impact upon solid-state drives, as there is no mechanical seek time involved. However, the file system needs to store one additional piece of metadata for the corresponding file. Each piece of metadata itself occupies space and requires processing power and processor time. If the maximum fragmentation limit is reached, write requests fail.|$|R
5000|$|The aim of any defragger is {{to reduce}} or {{eliminate}} <b>file</b> <b>fragmentation,</b> and thereby improve {{the performance of the}} file system. Vopt achieves this by means of the following features: ...|$|E
50|$|While ext3 is {{resistant}} to <b>file</b> <b>fragmentation,</b> ext3 can get fragmented over time or for specific usage patterns, like slowly writing large files. Consequently, ext4 (the successor to ext3) has an online filesystem defragmentation utility e4defrag and currently supports extents (contiguous file regions).|$|E
50|$|Many of today's file systems {{attempt to}} preallocate longer chunks, or chunks from {{different}} free space fragments, called extents to files that are actively appended to. This largely avoids <b>file</b> <b>fragmentation</b> when several files are concurrently being appended to, thus avoiding their becoming excessively intertwined.|$|E
50|$|Transfer rate can be {{influenced}} by <b>file</b> system <b>fragmentation</b> and {{the layout of the}} files. Defragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk. Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, the procedure can slow response when performed while the computer is in use.|$|R
50|$|The {{majority}} of photo recovery programs work {{by using a}} technique called file carving (data carving).There are many different file carving techniques {{that are used to}} recover photos. Most of these techniquesfail in the presence of <b>file</b> system <b>fragmentation.</b> Simson Garfinkel showed that on average 16% of JPEGs are fragmented, whichmeans on average 16% of jpegs are recovered partially or appear corrupt when recovered using techniques thatcan't handle fragmented photos.|$|R
40|$|As the {{multimedia}} handheld devices using NAND flash memory as storage media {{is becoming more}} popular, {{the importance of the}} flash file system is increasing. Although there has been substantial research on file systems dedicated to NAND flash memory, little {{attention has been paid to}} legacy file systems running on top of NAND flash memory. In this paper, we propose an anti-fragmentation cluster allocation (AFCA) scheme for the Linux Ext 2 file system. For the proposed scheme, we re-define the concept of <b>file</b> system <b>fragmentation</b> that takes into account the characteristics of NAND flash memory. The proposed AFCA scheme tries to minimize the performance degradation resulting from the fragmentation problem by discriminately allocating free clusters to files depending on their size relative to the flash memory block size. Evaluation results show that compared to the original cluster allocation scheme in Ext 2, the proposed AFCA scheme significantly reduces the <b>file</b> system <b>fragmentation</b> according to the new definition and improves the file system performance, especially the write performance, by up to 43 %...|$|R
50|$|Contig {{does not}} move any data except that {{belonging}} to the file in the question, so the amount it can defragment a file {{is limited to the}} largest contiguous block of free space on a system. Use of contig exchanges decreased <b>file</b> <b>fragmentation</b> for increased free space fragmentation.|$|E
5000|$|SSD Optimize consolidates {{free space}} on a Solid State Drive to improve SSD write performance. SSDs are not {{affected}} by <b>file</b> <b>fragmentation</b> like traditional electromechanical disk drives and defragmentation can shorten the lifespan of the SSD. SSD Optimize improves SSD performance without unnecessarily and harmfully defragmenting the drive.|$|E
5000|$|There are no {{programs}} to specifically defragment a ReiserFS file system, although tools {{have been written}} to automatically copy the contents of fragmented files hoping that more contiguous blocks of free space can be found. However, a [...] "repacker" [...] tool was planned for the next Reiser4 file system to deal with <b>file</b> <b>fragmentation.</b>|$|E
5000|$|Some file {{systems have}} since been {{designed}} {{to take advantage of}} this unused space, and can pack the tails of several files in a single shared tail block. While this may, at first, seem like it would significantly increase <b>file</b> system <b>fragmentation,</b> the negative effect can be mitigated with readahead features on modern operating systems [...] - [...] when dealing with short files, several tails may be close enough to each another to be read together, and thus a disk seek is not introduced. Such file systems often employ heuristics in order to determine whether tail packing is worthwhile in a given situation,and defragmentation software may use a more evolved heuristic.|$|R
50|$|Free (unallocated) space {{fragmentation}} {{occurs when}} there are several unused areas of the file system where new files or metadata can be written to. Unwanted free space fragmentation is generally caused by deletion or truncation of files, but file systems may also intentionally insert fragments ("bubbles") of free space {{in order to facilitate}} extending nearby <b>files</b> (see preventing <b>fragmentation</b> below).|$|R
50|$|For example, {{files in}} a file system are usually managed in units called blocks or clusters. When a file system is created, there is free space to store file blocks {{together}} contiguously. This allows for rapid sequential file reads and writes. However, as files are added, removed, and changed in size, the free space becomes externally fragmented, leaving only small holes {{in which to}} place new data. When a new file is written, or when an existing file is extended, the operating system puts the new data in new non-contiguous data blocks {{to fit into the}} available holes. The new data blocks are necessarily scattered, slowing access due to seek time and rotational latency of the read/write head, and incurring additional overhead to manage additional locations. This is called <b>file</b> system <b>fragmentation.</b>|$|R
5000|$|Prep for Shrink is {{specifically}} designed to pack all data {{to the beginning of}} a volume and may result in the creation of <b>file</b> <b>fragmentation.</b> The primary purpose is to prep a partition or volume for the best possible resize or shrink. As such, it should not be used unless it is the user's intent to resize or shrink a volume or partition.|$|E
50|$|An {{extent is}} a {{contiguous}} area of storage reserved for a file in a file system, {{represented as a}} range. A file can consist of zero or more extents; one file fragment requires one extent. The direct benefit is in storing each range compactly as two numbers, instead of canonically storing every block number in the range. Also, extent allocation results in less <b>file</b> <b>fragmentation.</b>|$|E
50|$|Information {{regarding}} files {{is displayed}} in a circular graph representing the physical disk, indicating sections that are fragmented, compressed, reserved for system use, along with other information. From this, the user can select sections of the disk, see {{a list of the}} files in that section showing information regarding individual <b>file</b> <b>fragmentation</b> and location, and individually defragment files or drag and drop them to the desired section on the disk.|$|E
2500|$|... {{a typical}} 7,200-rpm desktop HDD has a {{sustained}} [...] "disk-to-buffer" [...] {{data transfer rate}} up to 1,030Mbit/s. This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the [...] "buffer-to-computer" [...] interface is 3.0Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by <b>file</b> system <b>fragmentation</b> and {{the layout of the}} files.|$|R
40|$|A new disk {{allocation}} algorithm, {{added to}} a recent version of the UNIX Fast File System, attempts to improve file layout by exploiting the clusters of free space that {{are available in the}} file system. In this paper I study the effectiveness of this algorithm at reducing <b>file</b> system <b>fragmentation.</b> To do this, I have created a program that artificially ages a file system by replaying a workload similar to that experienced by a real file system. To evaluate the effectiveness of the new disk allocation algorithm, I use this program to replay four months of activity on two file systems that differ only in the disk allocation algorithms that they use. At the end of the four month simulation, the file system that used the new allocation algorithm had 3 % more file data blocks that were contiguously laid out. ...|$|R
5000|$|... {{a typical}} 7,200-rpm desktop HDD has a {{sustained}} [...] "disk-to-buffer" [...] {{data transfer rate}} up to 1,030 Mbit/s. This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the [...] "buffer-to-computer" [...] interface is 3.0 Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by <b>file</b> system <b>fragmentation</b> and {{the layout of the}} files.|$|R
50|$|The {{efficiency}} of the importing and editing process is heavily dependent {{on the amount of}} <b>file</b> <b>fragmentation</b> of the hard disk. The most reliable results can be obtained by adding an extra hard disk dedicated for scratch space, and regularly re-formatting/defragmenting it, rather than simply deleting the files {{at the end of the}} project. Fragmented AVI files result in jerky playback on the editing screen, and make the final rendering process much longer.|$|E
5000|$|Mass storage {{consisted}} of 6-foot long rotating drums that held 256KW (in the FH-432) to 2MW (in the FH-1782). The highest capacity mass storage was the FASTRAND drum, which held 22 MW (99 MB). <b>File</b> <b>fragmentation</b> was {{dealt with by}} a process called a [...] "file save", which was generally done once per day, at night. It involved rolling all files out to tape, reinitializing the drum file system, then reading the files back in.|$|E
5000|$|Empirical {{tests with}} a 4.2 GB {{fragmented}} file on a DVD media {{have shown that}} Microsoft Windows XP supports this, while Mac OS X (as of 10.4.8) does not handle this case properly. In the case of Mac OS X, the driver appears not to support <b>file</b> <b>fragmentation</b> at all (i.e. it only supports ISO 9660 Level 2 but not Level 3). Linux supports multiple extents. [...] FreeBSD only shows and reads the last extent of a multi-extent file.|$|E
40|$|The 4. 4 BSD {{file system}} {{includes}} a new algorithm for allocating disk blocks to files. The {{goal of this}} algorithm is to improve file clustering, {{increasing the amount of}} sequential I/O when reading or writing files, thereby improving file system performance. In this paper we study the effectiveness of this algorithm at reducing <b>file</b> system <b>fragmentation.</b> We have created a program that artificially ages a file system by replaying a workload similar to that experienced by a real file system. We used this program {{to evaluate the effectiveness of}} the new disk allocation algorithm by replaying ten months of activity on two file systems that differed only in the disk allocation algorithms that they used. At the end of the ten month simulation, the file system using the new allocation algorithm had approximately half the fragmentation of a similarly aged file system that used the traditional disk allocation algorithm. Measuring the performance difference between the two file systems by reading [...] ...|$|R
50|$|Fragmentation {{occurs when}} the file system cannot or will not {{allocate}} enough contiguous space to store a complete file as a unit, but instead puts parts of it in gaps between existing files (usually those gaps exist because they formerly held a file that the file system has subsequently deleted or because the file system allocated excess space for the file in the first place). Files that are often appended to (as with log files) {{as well as the}} frequent adding and deleting of files (as with emails and web browser cache), larger files (as with videos) and greater numbers of <b>files</b> contribute to <b>fragmentation</b> and consequent performance loss. Defragmentation attempts to alleviate these problems.|$|R
40|$|This work {{is based}} on the problem of {{information}} hiding in computer disk drives. Firstly, the object of study, i. e. clustered <b>file</b> systems, their <b>fragmentation,</b> ways to hide information, has been analyzed. Moreover, currently existing methods to hide information in computer disks has been reviewed. Then, a new covert channel method, designed to hide data in clustered file systems, has been proposed. The method uses multiple cover files and hides data by changing relative positions of clusters of those files. Finally, an experimental application that exploits the proposed method has been created. With the help of the application the method has been examined and it is proved that it has the property of two-fold plausible deniability...|$|R
