6|153|Public
50|$|In April 2013 NDP Environment Minister Sterling Belliveau, sent {{a letter}} to Halifax Regional Council expressing that both the <b>front-end</b> <b>processor</b> (<b>FEP)</b> and the waste {{stabilization}} facility (WSF) will continue part of the Department of Energy regulations and a commitment to the local communities. On April 30, Liberal Andrew Younger questioned the Minister's authority to get involved in Halifax Regional Council proposed changes to their waste management strategy.|$|E
50|$|An {{example of}} a UK-based {{time-sharing}} bureau business is OLS Computer Services (UK) Ltd. Previously Leasco Response and later acquired by On-Line Systems of Pittsburgh, PA, the company operated four HP-2000 TSB (Time shared Basic) systems from its Knightsbridge (later Islington) headquarters offering off-the-shelf business packages as well as raw time to universities. In addition to its HP estate a DEC PDP-10 operated as a <b>front-end</b> <b>processor</b> (<b>FEP)</b> connecting UK users to 16 DEC PDP-11s based in the US. Connectivity was provided via leased Westrex ASR 33 or Data Dynamics 390 punched-tape enabled teletype machines connected via Post Office (ex GPO) type 2 modems or acoustic couplers connecting telephone handsets operating at up to 110cps.|$|E
40|$|ITC/USA 2008 Conference Proceedings / The Forty-Fourth Annual International Telemetering Conference and Technical Exhibition / October 27 - 30, 2008 / Town and Country Resort & Convention Center, San Diego, CaliforniaA {{traditional}} Receiver, Modulator, Bit Synchronizer, Frame Synchronizer and <b>Front-End</b> <b>Processor</b> (<b>FEP)</b> {{with local}} RAID storage from numerous satellite ground station equipment providers is typically used to satisfy current needs in mission ground stations. The development of Software Defined Radios (SDRs) with reprogrammable personalities {{has led to}} the consolidation of these processing elements, and will become the standard for years to follow. CVG-Avtec Systems, Inc. has been a pioneer in the SDR industry, integrating several ground station functions into a one system solution. Its High Data Rate Modem (HDRM) architecture replaces racks of previous generation equipment, providing greater functionality in a smaller footprint. The Field Programmable Gate Array (FPGA) based HDRM is a one system solution that inputs Intermediate Frequency (IF) data and outputs packetized data over IP for data distribution. These new architectures are capitalizing on the revolution in electronics and networking technologies. This paper will discuss the architecture of the HDRM and how it optimizes ground station data processing in a high-rate environment...|$|E
50|$|The typical {{data flow}} in a DMS has the SCADA system, the Information Storage & Retrieval (ISR) system, Communication (COM) Servers, <b>Front-End</b> <b>Processors</b> (<b>FEPs)</b> & Field Remote Terminal Units (FRTUs).|$|R
40|$|The NIF Integrated Computer Control System (ICCS) {{application}} software uses {{a set of}} service frameworks that assures uniform behavior spanning the <b>front-end</b> <b>processors</b> (<b>FEPs)</b> and supervisor programs. This uniformity is visible both in the way each program employs shared services and in the flexibility it affords for attaching graphical user interfaces (GUIs). Uniformity of structure across applications is desired {{for the benefit of}} programmers who will be maintaining the many programs that constitute the ICCS. In this paper, the framework components that have the greatest impact on the application structure are discussed. ...|$|R
40|$|Abstract: A {{response}} time analysis {{for a general}} class of terminals-to-computer subsystem is presented in this paper. The model used {{is based on the}} most advanced data communications system in which terminals are connected to Terminal Control Units (TCU) that are in turn connected to local <b>Front-End</b> <b>Processors</b> (<b>FEP).</b> The line control procedures used to interface a TCU and an FEP may be half-duplex Binary Synchronous Communications (BSC), half-duplex Synchronous Data Link Control (SDLC), or full-duplex SDLC. The models presented here can be used to determine bottlenecks in the entire system and to facilitate the initial phase of system design and configuration...|$|R
40|$|The NASA Lewis Research Center is {{investigating}} the benefits of parallel processing to applications in computational fluid and structural mechanics. To aid this investigation, NASA Lewis is developing the Hypercluster, a multi-architecture, parallel-processing test bed. The initial operating capability (IOC) being developed for the Hypercluster is described. The IOC will provide a user with a programming/operating environment that is interactive, responsive, and easy to use. The IOC effort includes {{the development of the}} Hypercluster Operating System (HYCLOPS). HYCLOPS runs in conjunction with a vendor-supplied disk operating system on a <b>Front-End</b> <b>Processor</b> (<b>FEP)</b> to provide interactive, run-time operations such as program loading, execution, memory editing, and data retrieval. Run-time libraries, that augment the FEP FORTRAN libraries, are being developed to support parallel and vector processing on the Hypercluster. Special utilities are being provided to enable passage of information about application programs and their mapping to the operating system. Communications between the FEP and the Hypercluster are being handled by dedicated processors, each running a Message-Passing Kernel, (MPK). A shared-memory interface allows rapid data exchange between HYCLOPS and the communications processors. Input/output handlers are built into the HYCLOPS-MPK interface, eliminating the need for the user to supply separate I/O support programs on the FEP...|$|E
40|$|International Telemetering Conference Proceedings / October 22 - 25, 2001 / Riviera Hotel and Convention Center, Las Vegas, NevadaA {{traditional}} <b>Front-end</b> <b>Processor</b> (<b>FEP)</b> {{with local}} RAID storage can limit the operational throughput of a high-rate telemetry ground station. The Front-end processor must perform pass processing (frame synchronization, decoding, routing, and storage), post-pass processing (level-zero processing), and tape archiving. A typical fifteen minute high-rate satellite pass can produce data files of 10 to 20 GB. The FEP may require up to 2 hours {{to perform the}} post-pass processing and tape archiving functions for these size files. During this time, it is not available to support real-time pass operations. Honeywell faced this problem {{in the design of}} the data management system for the DataLynx Ã¤* ground stations. Avtec Systems, Inc. and Honeywell worked together to develop a data management system that utilizes a Storage Area Network (SAN) in conjunction with multiple High-speed Front-end Processors (HSFEP) for Pass Processing (PFEP), multiple HSFEPs for Post-pass Processing (PPFEP), and a dedicated Tape Archive server. A SAN consists of a high-capacity, high-bandwidth shared RAID that is connected to multiple nodes using 1 Gbps Fibre Channel interfaces. All of the HSFEPs as well as the Tape Archive server have direct access to the shared RAID via a Fibre Channel network. The SAN supports simultaneous read/write transfers between the nodes at aggregate rates up to 120 Mbytes/sec. With the Storage Area Network approach, the High-Speed Front-end Processors can quickly transfer the data captured during a pass to the shared RAID for post-processing and tape archiving so that they are available to support another satellite pass. This paper will discuss the architecture of the Storage Area Network and how it optimizes ground station data management in a high-rate environment...|$|E
40|$|Indian Institute of ScienceThis {{dissertation}} {{discusses the}} principles, techniques and approaches {{adopted in the}} design of task scheduling algorithms for Distributed Parallel Processing Computer Systems (DPCSs) connected with network of front-end systems (FSs), The primary goal {{in the design of}} scheduling algorithms is to minimise the total turnaround time of the jobs to be scheduled by maximizing the utilisation of the resources of the DPCS with minimum data communication overhead, The users present their jobs to be scheduled at the FS, The FS receives a job and generates a finite set of independent tasks based on mutually independent sections having inherent parallelism, Each task could be scheduled to different available processors of DPCS for concurrent execution, The tasks are of three groups viz,, compute intensive tasks, input. output intensive tasks and the combination of compute and input-output intensive tasks. They may have the execution time almost the same. Some of the tasks may have the execution time larger due to precedence constraints than that of other tasks and they are provided with logical breakpoints which can be utilised to further break the tasks into subtasks during scheduling, The technique of using breakpoint of the tasks is more appropriate when the number of available processors is more than the number of tasks to be scheduled. The tasks of a job thus generated are sent to the <b>front-end</b> <b>processor</b> (<b>FEP</b> or the host processor) of the DPCS in the form of data flow graph (DFG), The DFG is used to model the tasks and represent the precedence (or data dependencies) among the tasks, In order to preserve the constraints among the tasks during scheduling and realise efficient utilisation of the resources of DPCS, the DFG is structured in the form of levels, The FBP of DPCS has a resident Task Manager (TM). The key function of the TM is to schedule the tasks to the appropriate processors of DPCS either statically or dynamically based on the required resources. To realise efficient scheduling and utilisation of the processors of DPCS, the TM uses a set of buffers known as Task Forwarding Buffer (TFB), Task Output Buffer (TOB) and Task Status Buffer (TSB) maintained by the FEP of DPCS. The tasks of a job from the FS are received at the TFB. The TM picks up a set of tasks pertaining to a level for scheduling into a temporary buffer C and obtains the status of the processors of DPCS. In order to realise both static and dynamic approaches of allocation, task to processor relation is considered in the scheduling algorithm. If the number of tasks in C is equal to or greater than the number of processors available, one task per processor is allocated, the remaining tasks of C are scheduled subsequently as and when the processors become available. This method of allocation is called static approach. If the number of tasks in C is less than the number of processors available, the TM makes use of the logical breakpoints of the tasks to generate subtasks equal to the number of available processors. Each subtask is scheduled to a processor. This method of scheduling is called the dynamic approach. In all the case the precedence constraints among the tasks are preserved by scheduling the successor task to the parent processor or near neighbouring processor, maintaining minimum data communication between them. Various examples of Computational Fluid Dynamics problems' were tested and the objective of reduced total turnaround time and maximum utilisation of the processors was achieved. The total turnaround time achieved for different jobs varies between 51 % and 86 % with static approach and 16 % and 89 % with dynamic approach. The utilisation of the processors varies between the 50 % and 92. 5 %. Hence a speed-up of 5 to 8 folds is realised...|$|E
40|$|The {{main idea}} behind the {{intelligent}} networks (IN) concept is to place the intelligence/service logic in the dedicated server and to meet various service requirements of subscribers {{and the development of}} new services in time. The (N + 1) âtype architecture of the IN service system consists of <b>front-end</b> <b>processors</b> (<b>FEP)</b> and back-end <b>processors</b> (BEP), which are currently in service by a telecommu-nications company in Korea. The main characteristic of this architecture is a separation between the data plane and control plane of the network, such that the load-balancing table is used to distribute the traffic load among the <b>processors,</b> <b>FEP</b> and BEP, taking into account services. Since the values used in the load-balancing table affect the IN system performance significantly, this paper simulates the IN system and evaluates system performance depending on the values used in the table...|$|R
40|$|The {{strategy}} used {{to develop}} the NIF Integrated Computer Control System (ICCS) calls for incremental cycles of construction and formal test to deliver a total of 1 million lines of code. Each incremental release takes {{four to six months}} to implement specific functionality and culminates when offline tests conducted in the ICCS Integration and Test Facility verify functional, performance, and interface requirements. Tests are then repeated on line to confirm integrated operation in dedicated laser laboratories or ultimately in the NIF. Test incidents along with other change requests are recorded and tracked to closure by the software change control board (SCCB). Annual independent audits advise management on software process improvements. Extensive experience has been gained by integrating controls in the prototype laser preamplifier laboratory. The control system installed in the preamplifier lab contains five of the ten planned supervisory subsystems and seven of sixteen planned <b>front-end</b> <b>processors</b> (<b>FEPs).</b> Beam alignment, timing, diagnosis and laser pulse amplification up to 20 joules was tested through an automated series of shots. Other laboratories have provided integrated testing of six additional FEPs. Process measurements including earned-value, product size, and defect densities provide software project controls and generate confidence that the control system will be successfully deployed. Comment: Submitted to ICALEPCS 200...|$|R
40|$|The control {{network for}} the National Ignition Facility (NIF) is {{designed}} to meet the needs for common object request broker architecture (CORBA) inter-process communication, multicast video transport, device triggering, and general TCP/IP communication within the NIF facility. The network will interconnect approximately 650 systems, including the embedded controllers, <b>front-end</b> <b>processors</b> (<b>FEPs),</b> supervisory systems, and centralized servers involved in operation of the NIF. All systems are networked with Ethernet to serve the majority of communication needs, and asynchronous transfer mode (ATM) is used to transport multicast video and synchronization triggers. CORBA software infra-structure provides location-independent communication services over TCP/IP between the application processes in the 15 supervisory and 300 FEP systems. Video images sampled from 500 video cameras at a 10 -Hz frame rate will be multicast using direct ATM Application Programming Interface (API) communication from video FEPs to any selected operator console. The Ethernet and ATM control networks are used to broadcast two types of device triggers for last-second functions in a large number of FEPs, thus eliminating the need for a separate infrastructure for these functions. Analysis, design, modeling, and testing of the NIF network has been performed to provide confidence that the network design will meet NIF control requirements. ...|$|R
40|$|The Integrated Computer Control System (ICCS) for the National Ignition Facility (NIF) is a layered {{architecture}} of 300 <b>front-end</b> <b>processors</b> (<b>FEP)</b> coordinated by supervisor subsystems including automatic beam alignment and wavefront control, laser and target diagnostics, pulse power, and shot control timed to 30 ps. FEP computers incorporate either VxWorks on PowerPC or Solaris on UltraSPARC processors that interface to over 45, 000 control points attached to VME-bus or PCI-bus crates respectively. Typical devices are stepping motors, transient digitizers, calorimeters, and photodiodes. The front-end layer {{is divided into}} another segment comprised of an additional 14, 000 control points for industrial controls including vacuum, argon, synthetic air, and safety interlocks implemented with Allen-Bradley programmable logic controllers (PLCs). The computer network is augmented asynchronous transfer mode (ATM) that delivers video streams from 500 sensor cameras monitoring the 192 laser beams to operator workstations. Software {{is based on an}} object-oriented framework using CORBA distribution that incorporates services for archiving, machine configuration, graphical user interface, monitoring, event logging, scripting, alert management, and access control. Software coding using a mixed language environment of Ada 95 and Java is one-third complete at over 300 thousand source lines. Control system installation is currently under way for the first 8 beams, with project completion scheduled for 2008. ...|$|R
40|$|The National Ignition Facility (NIF) {{is housed}} within a large {{facility}} {{about the size}} of two football fields. The Integrated Computer Control System (ICCS) is distributed throughout this facility and requires the integration of about 40, 000 control points and over 500 video sources. This integration is provided by approximately 700 control computers distributed throughout the NIF facility and a network that provides the communication infrastructure. A main control room houses a set of seven computer consoles providing operator access and control of the various distributed <b>front-end</b> <b>processors</b> (<b>FEPs).</b> There are also remote workstations distributed within the facility that allow provide operator console functions while personnel are testing and troubleshooting throughout the facility. The operator workstations communicate with the FEPs which implement the localized control and monitoring functions. There are different types of FEPs for the various subsystems being controlled. This report describes the design of the NIF ICCS network and how it meets the traffic loads that will are expected and the requirements of the Sub-System Design Requirements (SSDR's). This document supersedes the earlier reports entitled Analysis of the National Ignition Facility Network, dated November 6, 1996 and The National Ignition Facility Digital Video and Control Network, dated July 9, 1996. For an overview of the ICCS, refer to the document NIF Integrated Computer Controls System Description (NIF- 3738) ...|$|R
5000|$|... $IAS.SYS - Input assist {{subsystem}} (IAS) with {{front end}} <b>processor</b> (<b>FEP)</b> support driver ...|$|R
50|$|<b>Front-end</b> <b>processors</b> have {{connections}} to various card associations and supply authorization and settlement {{services to the}} merchant banksâ merchants. Back-end processors accept settlements from <b>front-end</b> <b>processors</b> and, via The Federal Reserve Bank for example, move {{the money from the}} issuing bank to the merchant bank.|$|R
5000|$|Baidu Wireless {{provides}} various {{services for}} mobile phones, including a Chinese-input front end <b>processor</b> (<b>FEP)</b> for various popular operating systems including Android, Symbian S60v5, and Windows Mobile.|$|R
30|$|Zhao and Shao [20, 21] used CASA as a <b>front-end</b> <b>processor</b> for robust speaker {{identification}} (SID).|$|R
5000|$|PU4 nodes are <b>front-end</b> <b>processors</b> {{running the}} Network Control Program (NCP) {{such as the}} IBM 37xx series ...|$|R
50|$|The DATANET-30 was a {{computer}} manufactured by General Electric {{designed to be}} used as a <b>front-end</b> <b>processor</b> for data communications.|$|R
50|$|New in CP-6 {{was the use}} of {{communications}} and terminal interfaces through minicomputer (Honeywell Level 6)-based <b>front-end</b> <b>processors,</b> connected locally, remotely, or in combination.|$|R
50|$|MTS can {{and does}} use {{communication}} controllers {{such as the}} IBM 2703 and the Memorex 1270 to support dial-in terminals and remote batch stations over dial-in and dedicated data circuits, but these controllers proved to be fairly inflexible and unsatisfactory for connecting large numbers of diverse terminals and later personal computers running terminal emulation software at ever higher data rates. Most MTS sites choose {{to build their own}} <b>front-end</b> <b>processors</b> or to use a <b>front-end</b> <b>processor</b> developed by one of the other MTS sites to provide terminal support.|$|R
50|$|These <b>front-end</b> <b>processors,</b> usually DEC PDP-8, PDP-11, or LSI-11 based with locally {{developed}} custom {{hardware and}} software, {{would act as}} IBM control units attached to the IBM input/output channels {{on one side and}} to modems and phone lines on the other. At the University of Michigan the <b>front-end</b> <b>processor</b> was known as the Data Concentrator (DC). The DC was developed as part of the CONCOMP project by Dave Mills and others and was the first non-IBM device developed for attachment to an IBM I/O Channel. Initially a PDP-8 based system, the DC was upgraded to use PDP-11 hardware and a Remote Data Concentrator (RDC) was developed that used LSI-11 hardware that connected back to a DC over a synchronous data circuit. The University of British Columbia (UBC) developed two PDP-11 based systems: the Host Interface Machine (HIM) and the Network Interface Machine (NIM). The University of Alberta used a PDP-11 based <b>Front-end</b> <b>processor.</b>|$|R
50|$|All Connection Machine models {{required}} a serial <b>front-end</b> <b>processor,</b> which was most often a Sun Microsystems workstation, but on early models {{could also be}} a DEC VAXminicomputer or Symbolics LISP machine.|$|R
5000|$|Model 4 - {{attached}} to an IBM 1130 minicomputer via the storage access channel (SAC). The 1130 could run {{either as a}} standalone processor or as a <b>front-end</b> <b>processor</b> connected to a remote System/360.|$|R
40|$|A {{dissertation}} {{submitted to}} the Faculty of Engineering, University of the Witwctersrand, Johannesburg as a partial requirement tor the Degree of Master of Sciencc in Engineering. Johannesburg, 1984. This dissertation describes the design, {{development and implementation of}} the software for -i real-time data acquisition system. A microcomputer war employed as an intelligent <b>front-end</b> <b>processor</b> to a mult, user minicomputer system for the real-time input and output of analog signals. The data acquisition system, BADEDAS (Basic Analog and Digital Experimental Data Acquisition System), provides the users of an Eclipse minicomputer systen with an efficient means of acquiring digital samples of analog signals and disseminating digital samples as analog signals. BADEDAS exploits the concept of de Ice independent i/o to achieve an othogonal and flexible architecture. BADEDAS provides the choice of two user interfaces to the system and the ability to include simple user-written processing routines in the <b>front-end</b> <b>processor's</b> control software. The use of a microcomputer as a <b>front-end</b> <b>processor</b> removes the real-time I/O requirements from the minicomputer and also reduces the risk electrical damage to it...|$|R
40|$|An {{efficient}} storage format {{was developed}} for computer-generated holograms for use in electron-beam lithography. This method employs run-length encoding and Lempel-Ziv-Welch compression and succeeds in exposing holograms that were previously infeasible owing to the hologram 2 Ì 7 s tremendous pattern-data file size. These holograms also require significant computation; thus the algorithm was implemented on a parallel computer, which improved performance by 2 orders of magnitude. The decompression algorithm was integrated into the Cambridge electron-beam machine 2 Ì 7 s <b>front-end</b> <b>processor.</b> Although this provides much-needed ability, some hardware enhancements will be required {{in the future to}} overcome inadequacies in the current <b>front-end</b> <b>processor</b> that result in a lengthy exposure time...|$|R
50|$|In the 1980s, Honeywell's Datanet 8 line of {{communications}} processors, {{often used as}} <b>front-end</b> <b>processors</b> for DPS 8 mainframes, shared many hardware components with DPS 6. Another specialised derivative of the Level 6 was the Honeywell Page Printing System.|$|R
50|$|EXCP {{may also}} be used to access {{communications}} devices attached to IBM 2701, 2702 and 2703 communications controllers and IBM 370x or Amdahl 470x <b>front-end</b> <b>processors</b> (and their respective follow-ons) operating in emulator mode (EP) or partitioned emulator mode (PEP).|$|R
50|$|The site is on 190-acres of {{land that}} is {{a major role in}} HRM's {{integrated}} waste/recourse management strategy which requires several important conditions about materials that can be placed in the site: hazardous waste, recyclable material and unstabilized organic material. The site features a front end <b>processor</b> (<b>FEP)</b> which will sort out unwanted materials. It is located off exit 3 of Nova Scotia Highway 103.|$|R
40|$|An {{important}} component of EPICS (Experimental Physics and Industrial Control System) is iocCore, which is the core software in the IOC (input/output controller) <b>front-end</b> <b>processors.</b> Currently iocCore requires the vxWorks operating system. This paper describes the porting of iocCore to other operating systems...|$|R
40|$|Dense Wavelength-division {{multiplexing}} (DWDM) {{technology offers}} tremendous transmission ca-pacity in optical fiber communications. However, switching and routing capacity lags behind the trans-mission capacity, {{since most of}} todayâs packet switches and routers are implemented using slower electronic components. Optical packet switches {{are one of the}} potential candidates to improve switching capacity to be comparable with optical transmission capacity. In this paper, we present an optically transparent ATM (OPATM) switch that consists of a photonic <b>front-end</b> <b>processor</b> and a WDM switching fabric. A WDM loop memory is deployed as a multi-ported shared-memory in the switching fabric. The photonic <b>front-end</b> <b>processor</b> performs the cell delineation, VPI/VCI overwriting, and cell synchroniza-tion functions in the optical domain under the control of electronic signals. The WDM switching fabric stores and forwards aligned cells from each input port to the appropriate output ports under the control of an electronic route controller. We have demonstrated with experiments the functions and capabilities of the <b>front-end</b> <b>processor</b> and the switching fabric at the header-processing rate of 2. 5 Gb/s. Other than ATM, the switching architecture can be easily modified to apply to other types of fixed-length payload formats with different bit rates. Using this kind of photonic switches to route information, a...|$|R
50|$|Telenet {{supported}} remote concentrators for IBM 3270 family intelligent terminals, which communicated, via X.25 to Telenet-written {{software that}} ran in IBM 370x series <b>front-end</b> <b>processors.</b> Telenet also supported Block Mode Terminal Interfaces (BMTI) for IBM Remote Job Entry terminals supporting the 2780/3780 and HASP Bisync protocols.|$|R
50|$|The term 37xx {{refers to}} IBM's family of SNA {{communications}} controllers. The 3745 supports {{up to eight}} high-speed T1 circuits, the 3725 is a large-scale node and <b>front-end</b> <b>processor</b> for a host, and the 3720 is a remote node that functions as a concentrator and router.|$|R
40|$|This paper {{describes}} an integrated approach to pattern classification where a self-organising Boolean neural network architecture {{is used as}} a <b>front-end</b> <b>processor</b> to a feedforward neural architecture based on goal-seeking principles (the GSN architecture). The performance of the integrated architecture is illustrated by considering its application to a character recognition problem...|$|R
50|$|The {{operating}} system supported inter-system communication, job submission and file transfer between CP-6 systems and between CP-6 and CP-V and {{to and from}} IBM and other HASP protocol systems. The system used communications and terminal interfaces through a Honeywell Level 6 minicomputer-based <b>front-end</b> <b>processor.</b> Asynchronous, bisynchronous and TCP/IP communications protocols were supported.|$|R
