50|25|Public
5000|$|People are {{generally}} {{much more likely}} to act pro-socially in a public setting rather than in a private setting. One explanation for this finding has to do with perceived status, being publicly recognized as a pro-social individual often enhance one's self-image and desirability to be considered for inclusion in social groups. Other research has shown that merely given people the [...] "illusion" [...] that they are being observed (e.g., by hanging up posters of [...] "staring" [...] human eyes) can generate significant changes in pro-social acts such as charitable giving and less littering. Pictures of human eyes trigger an involuntary neural <b>gaze</b> <b>detection</b> mechanism, which primes people to act pro-socially.|$|E
5000|$|A study {{conducted}} by the University of Sydney's Vision Centre concluded that when test subjects are presented with limited cues {{to determine whether or not}} they are being stared at, they tend to assume that they are, despite having too little information to make that determination. Test subjects asked to determine if an individual wearing dark sunglasses is looking at them, for example, would assume they are indeed being stared at, even though the starer's eyes were obscured and the actual position of their eyes was not known to the subject. Humans, they reason, are [...] "hard-wired" [...] to err on the side of caution since the consequences of not recognizing such a potential threat or important social cue would be more detrimental than a false positive error. Studies in social psychology have shown that <b>gaze</b> <b>detection</b> can be leveraged to enforce cooperation in social dilemma situations. For example false images of eye cues have shown to elicit prosocial behavior, primarily because when people are under the (false) impression of being watched, they have a strong desire to maintain a good public reputation.|$|E
40|$|Human eye <b>gaze</b> <b>detection</b> {{plays an}} {{important}} role in various fields, including human-computer interaction, virtual reality and cognitive science. Although different relatively accurate systems of eye tracking and <b>gaze</b> <b>detection</b> exist, they are usually either too expensive to be bought for low cost applications or too complex to be implemented easily. In this article, we propose a non-wearable system for eye tracking and <b>gaze</b> <b>detection</b> with low complexity and cost. The proposed system provides a medium accuracy which makes it suitable for general applications in which low cost and easy implementation is more important than achieving very precise <b>gaze</b> <b>detection.</b> The proposed method includes pupil and marker detection using infrared image processing, and gaze evaluation using an interpolation-based strategy. The interpolation-based strategy exploits the positions of the detected pupils and markers in a target captured image and also in some previously captured training images for estimating the position of a point that the user is gazing at. The proposed system has been evaluated by three users in two different lighting conditions. The experimental results show that the accuracy of this low cost system can be between 90 % and 100 % for finding major gazing directions...|$|E
40|$|An {{important}} social signal encoded in faces is eye <b>gaze.</b> The <b>detection</b> and monitoring of eye gaze direction {{is essential for}} effective social learning and communication among humans (Bloom, 2000; Csibra & Gergely, 2006). Eye gaze provides information about the target of another person’s attention and expression, and it also conveys information abou...|$|R
40|$|In a prior study, {{intersection}} detection {{failures of}} individuals with hemianopia were strongly associated with inadequate head scanning; however, eye position was not tracked. In this pilot study, we tracked eye and head movements, and {{examined the relationship between}} <b>gaze</b> scanning and <b>detection</b> of pedestrians at intersections in a driving simulator. Gaze scan deficits, in particular not scanning sufficiently far into the blind hemifield, were the main reason for detection failures at the extreme edge of the clear-sight triangle in the blind hemifield. In addition, the <b>gaze</b> data revealed <b>detection</b> failures due to looked-but-failed-to-see events. The results suggest that HH drivers may be at increased risk for collisions at intersections...|$|R
40|$|Abstract. This paper proposes an {{algorithm}} {{to estimate}} gaze direction without on-site calibration {{even when the}} position of the head relative to the camera moves. We define eye direction as geometrical direction of the eyeball and gaze direction as long fixed looking detection in this paper. In the algorithm to detect the eye direction, a cornea curvature center is detected based on a relation model between a light source and the cornea curvature center. A shape of the pupil is approximated using the detected center by ellipse Hough transform. The eye direction is estimated as a rotation angle of the approximated pupil using a cornea curvature radius and distance between the cornea curvature center and a pupil rim. This personal information on the eyeball is essential to the eye direction detection and the third personal information, a gap of the fovea, to the <b>gaze</b> direction <b>detection.</b> They are required measuring one time beforehand. However, on-site calibration is never to do in the <b>gaze</b> direction <b>detection,</b> which corrects the eye direction by the gap of the fovea. The effectiveness of our algorithm was confirmed about three point of views. The first is that the cornea curvature center was detected stably, the second that the eye direction for 4 examinees was measured with dispersion of almost same level as the involuntary eye movement, and the third that the gaze direction was detected stably moving the camera instead of the head. Key words: Eye direction <b>detection,</b> <b>gaze</b> direction, cornea curvature center, pupil shape, Hough transform...|$|R
40|$|Motivated by the Electronic Health Record (EHR) system's {{demand of}} {{capturing}} patients' multimodal {{activities and the}} wide application of <b>gaze</b> <b>detection,</b> we develop a minimally intrusive <b>gaze</b> <b>detection</b> system with Microsoft Kinect sensor and test its performance in a simulated clinical environment. Traditional methods require either a close distance between the camera and the user or a fixed head pose which may severely interrupt the clinical workflow and {{the interaction between the}} physician and the patient. Compared with the traditional methods, our system allows a wider range of detection, while achieving an accuracy around 70...|$|E
40|$|Lately, the {{integration}} of <b>gaze</b> <b>detection</b> systems in human-computer interaction (HCI) applications hasbeen increasing. For this to be available for everyday use and for everybody, the imbedded gaze trackingsystem should be one that works with low resolution images coming from ordinary webcams and permits awide range of head poses. We propose the 3 D Multi-Texture Active Appearance Model (MT-AAM) : an irismodel is merged with a local eye skin model where holes are put {{in the place of}} the sclera-iris region. Theiris model rotates under the eye hole permitting the synthesis of new gaze directions. Depending on thehead pose, the left and right eyes are unevenly represented in the webcam image. Thus, we additionallypropose to use the head pose information to ameliorate <b>gaze</b> <b>detection</b> through a multi-objectiveoptimization: we apply the 3 D MT-AAM simultaneously on both eyes and we sum the resulting errors whilemultiplying each by a weighting factor that {{is a function of the}} head pose. Tests show that our methodoutperforms a classical AAM of the eye region trained on people committing different gaze directions. Moreover, we compare our proposed approach to the state-of-art method of Heyman et al. [12] whichmanually initialize their algorithm: without any manual initialization, we obtain the same level of accuracyin <b>gaze</b> <b>detection...</b>|$|E
40|$|International audienceLately, the {{integration}} of <b>gaze</b> <b>detection</b> systems in human-computer interaction (HCI) applications has been increasing. For this to be available for everyday use and for everybody, the imbedded gaze tracking system should be one that works with low resolution images coming from ordinary webcams and permits {{a wide range of}} head poses. We propose the 3 D Multi-Texture Active Appearance Model (MT-AAM) : an iris model is merged with a local eye skin model where holes are put in the place of the sclera-iris region. The iris model rotates under the eye hole permitting the synthesis of new gaze directions. Depending on the head pose, the left and right eyes are unevenly represented in the webcam image. Thus, we additionally propose to use the head pose information to ameliorate <b>gaze</b> <b>detection</b> through a multi-objective optimization: we apply the 3 D MT-AAM simultaneously on both eyes and we sum the resulting errors while multiplying each by a weighting factor that {{is a function of the}} head pose. Tests show that our method outperforms a classical AAM of the eye region trained on people committing different gaze directions. Moreover, we compare our proposed approach to the state-of-art method of Heyman et al. [12] which manually initialize their algorithm: without any manual initialization, we obtain the same level of accuracy in <b>gaze</b> <b>detection...</b>|$|E
40|$|Automatic {{estimation}} of head pose faciliates human facial analysis. It has widespread applications such as, <b>gaze</b> direction <b>detection,</b> video teleconferencing and human computer interaction (HCI). It {{can also be}} integrated in a multi-view face detection and recognition system. Most current methods estimate pose in a limited range or treat pose as a classification problem by assigning the face to one of many discrete poses [1, 2]. Mainly tested on images taken in controlled environments e. g. the FacePix dataset [3] (Fig. 1 a) ...|$|R
40|$|To {{realize a}} {{symbiotic}} relationship between humans and computers, {{it is crucial to}} estimate the external and internal state of the human by observation. One promising approach is to acquire the same visual information as the human ac-quires. In this paper, we introduce our wearable vision sen-sor, which is equipped with a pair of active stereo cameras and a gaze-direction detector. From the visual information obtained by the wearable vision sensor, we present three ba-sic three functionalities: a 3 D <b>gaze</b> point <b>detection</b> and im-age retrieval, a 3 D digitization of a hand-held object, and the measurement of a walking trajectory. 1...|$|R
40|$|International audienceA {{computational}} model of visual attention using visual inferences is proposed. The dominant depth and the horizon line position are inferred from low-level visual features. This prior knowledge helps to find salient areas on still color pictures. Regarding the dominant depth, {{the idea is}} to favor the lowest spatial frequencies on close-up scenes whereas the highest spatial frequencies are used to predict salient areas on panoramic view. Some studies showed that the horizon line is a natural attractor of our <b>gaze.</b> Horizon <b>detection</b> is then used to improve the saliency prediction. Results show that the proposed model outperforms existing approaches. However, the dominant depth does not bring any gain in the saliency prediction...|$|R
40|$|<b>Gaze</b> <b>detection</b> {{and head}} {{orientation}} {{are an important}} part of many advanced human-machine interaction applications. Many systems have been proposed for <b>gaze</b> <b>detection.</b> Typically, they require some form of user cooperation and calibration. Additionally, they may require multiple cameras and/or restricted head positions. We present a new approach for inference of both face orientation and gaze direction from a single image with no restrictions on the head position. Our algorithm is based on a face and eye model, deduced from anthropometric data. This approach allows us to use a single camera and requires no cooperation from the user. Using a single image avoids the complexities associated with of a multi-camera system. Evaluation tests show that our system is accurate, fast and can be used in a variety of applications, including ones where the user is unaware of the system...|$|E
40|$|Atypical {{processing}} of eye contact {{is one of}} the significant characteristics of individuals with autism, but the mechanism underlying atypical direct gaze processing is still unclear. This study used a visual search paradigm to examine whether the facial context would affect direct <b>gaze</b> <b>detection</b> in children with autism. Participants were asked to detect target gazes presented among distracters with different gaze directions. The target gazes were either direct gaze or averted gaze, which were either presented alone (Experiment 1) or within facial context (Experiment 2). As with the typically developing children, the children with autism, were faster and more efficient to detect direct gaze than averted gaze, whether or not the eyes were presented alone or within faces. In addition, face inversion distorted efficient direct <b>gaze</b> <b>detection</b> in typically developing children, but not in children with autism. These results suggest that children with autism use featural information to detect direct gaze, whereas typically developing children use configural information to detect direct gaze...|$|E
40|$|We present how <b>gaze</b> <b>detection</b> {{can be used}} {{to enhance}} the Rapid Serial Visual Presentation (RSVP) format, a dynamic text {{presentation}} technique suitable for mobile devices. A camera mounted on the device is used to monitor the reader's gaze and control the onset of the text presentation accordingly. The underlying assumptions for the technique are presented together with a description of a prototype, Smart Bailando, as well as our directions for further work...|$|E
40|$|Interpersonal {{communication}} is largely dependent on interpretation of facial expression and emotion. Difficulties in face processing, {{and more specifically}} in gaze discrimination, have been described in schizophrenic patients. According to Baron-Cohen (Mindblindness. M. I. T. Press, Cambridge, MA, 1995), gaze discrimination relies on the functioning of a specific cognitive module, the Eye Direction Detector (EDD). It has been proposed [Rosse et al. (1994) Gaze discrimination in patients with schizophrenia: preliminary report. American Journal of Psychiatry 151, 919 - 921] that an impairment in gaze discrimination is present in schizophrenia, and plays {{a fundamental role in}} inducing the paranoid symptoms reported by many patients. However, in the previous studies, <b>gaze</b> direction <b>detection</b> and interpretation of gaze have never been completely dissociated. The present experiment attempts to test the schizophrenics' skill in a simple <b>gaze</b> direction <b>detection</b> task. A series of photographic portraits of models looking at different directions have been presented to 22 schizophrenic patients and 36 control subjects. For each portrait subjects were asked to determine whether gaze was directed to the right or to the left by pressing a keyboard key. A forced choice paradigm was used. No differences were reported between schizophrenic patients and control subjects. That is, in the present paradigm, schizophrenic patients did not show any specific impairment in detecting the direction of gaze of the portraits. The results are discussed according to the notion that a dissociation is present in schizophrenia between implicit and explicit processes. The present case illustrates how the more automatic elementary functions, such as the <b>detection</b> of <b>gaze</b> direction, may be spared in schizophrenic patients, whereas explicit cognitive functions are likely more affected...|$|R
40|$|A {{computational}} model of visual attention using visual inferences is proposed. The dominant depth and the horizon line position are inferred from low-level visual features. This prior knowledge helps to find salient areas on still color pictures. Regarding the dominant depth, {{the idea is}} to favor the lowest spatial frequencies on close-up scenes whereas the highest spatial frequencies are used to predict salient areas on panoramic view. Some studies showed that the horizon line is a natural attractor of our <b>gaze.</b> Horizon <b>detection</b> is then used to improve the saliency prediction. Results show that the proposed model outperforms existing approaches. However, the dominant depth does not bring any gain in the saliency prediction. Index Terms — visual attention, contextual guidance, saliency, dominant depth, horizon line. 1...|$|R
30|$|In this study, a new {{generalized}} peak {{model for}} EEG signals peak classification {{has been identified}} using a novel AMSKF feature selection approach. The proposed algorithm considered 11, 781 peak candidate samples of real EEG data, which were collected from 30 healthy subjects instructed to direct their single eye blink, double eye blink, and horizontal eye <b>gaze.</b> The <b>detection</b> performance of the NNRW with four different peak detection models and new AMSKF model are compared. In general, the experimental {{results showed that the}} accuracy of the NNRW with new AMSKF model is better than the NNRW with other models. The statistical analysis showed that the detection performance of the NNRW with the new AMSKF model is significantly better in terms of testing accuracy compared to other models.|$|R
40|$|The {{advent of}} {{advanced}} mobile, gaming and augmented reality devices provides users with novel interaction modalities. Speech, finger and hand gesture recognition or even <b>gaze</b> <b>detection</b> {{are commonly used}} technologies, often enriched with data from embedded motion sensors. This paper describes a common human-machine interface which seamlessly combines actions based on various modalities. It discusses potential use cases, benefits and limitations of those technologies {{in the field of}} accelerator operations and maintenance...|$|E
40|$|Gaze-based {{interaction}} lets {{users to}} operate computers through eye movement, {{and promises to}} especially benefit severely disabled people who can barely move their hands or feet. However, existing computer operation by gaze may be neither effective nor comfortable, because of physical restrictions on <b>gaze</b> <b>detection</b> and because eye movement during gaze-based interaction is different from natural eye movement. This article describes the requirements for gaze-based interaction, and presents our approaches to making such interaction useful for anyone, anytime...|$|E
40|$|Robust and {{accurate}} pupil detection {{is a prerequisite}} for <b>gaze</b> <b>detection.</b> Hence, we propose a new eye/pupil detection method for <b>gaze</b> <b>detection</b> on a large display. The novelty of our research can be summarized by the following four points. First, in order to overcome the performance limitations of conventional methods of eye detection, such as adaptive boosting (Adaboost) and continuously adaptive mean shift (CAMShift) algorithms, we propose adaptive selection of the Adaboost and CAMShift methods. Second, this adaptive selection is based on two parameters: pixel differences in successive images and matching values determined by CAMShift. Third, a support vector machine (SVM) -based classifier is used with these two parameters as the input, which improves the eye detection performance. Fourth, the center of the pupil within the detected eye region is accurately located by means of circular edge detection, binarization and calculation of the geometric center. The experimental results show that the proposed method can detect the center of the pupil at a speed of approximately 19. 4 frames/s with an RMS error of approximately 5. 75 pixels, which is superior to the performance of conventional detection methods...|$|E
40|$|Background: <b>Gaze</b> {{direction}} <b>detection</b> {{plays an}} important role in socio-communicative development. Accurate eye gaze judgement appears problematic for individual with autism (Gepner et al., 1996; Sweetenham et al., 2001; Webster & Potter, 2008). However, the basic geometric understanding of gaze direction seems to be preserved in ASD (Leekam et al., 1997). Thus evidence concerning the eye <b>gaze</b> direction <b>detection</b> abilities in individual with Autism Spectrum Disorders (ASD) needs to be further investigated. Recently, researchers focused on tracking eye movements to better understand the strategies used to perform these tasks and detect possibly atypicity (Klin et al., 2002). Objectives: Investigate if children with ASD are sensitive to the orientation of an adult’s eye gaze in relation to an object which is within the child’s field of view. Methods: We compared on an eye-gaze detection task 30 children with ASD (20 M; 10 F), aged between 2 - 8 yrs (mean= 4; DS= 1, 7), IQ Leiter-R= 73 (DS= 22), and 30 TDs (12 M; 18 F), age-matched. The stimuli were presented with Tobii T 60 Eye Tracker and were a modified version of the ones used in Hoehl, Reid, Mooney, and Striano (2008) : an adult looking either toward or away from an object that was depicted on eye-level next to the adult’s head. There was a second object on the other side of the adult’s head. We defined 4 areas of interest for all the stimulus images: eyes, gaze target, no gaze target, screen-others (that is the area of the screen external to the other AOI). In order to define the strategies used to perform the tasks we considered Fixation Count (FC: the number of times that the child fixate an AOI), Time of First Fixation (TFF: how long it took to the participants to fixate an AOI for the first time) and Observation Length (OL: the total time in seconds for every time a person has looked within an AOI). Results: The results show that ASD children looked more times outside the AOI (FC screen-others= 34...|$|R
40|$|Automatic {{estimation}} of head pose from a face {{image is a}} sub-problem of human face analysis with widespread applications such as <b>gaze</b> direction <b>detection</b> and human computer interaction. Most current methods estimate pose in a limited range or treat pose as a classification problem by assigning the face to one of many discrete poses. Moreover they have mainly been tested on images taken in controlled environments. We {{address the problem of}} estimating pose as a continuous regression problem on “real world ” images with large variations in background, illumination and expression. We propose a probabilistic framework with a general representation that does not rely on locating facial features. Instead we represent a face with a non-overlapping grid of patches. This representation is used in a generative model for automatic {{estimation of}} head pose ranging from − 90 ◦ to 90 ◦ in images taken in uncontrolled environments. Our methods achieve a correlation of 0. 88 with the human estimates of pose. ...|$|R
40|$|Abstract. A {{system for}} {{detecting}} and evaluating driver’s gaze behavior was proposed. A system for recognizing the driver’s unsafe gaze behavior was established using multi-level information and fusion decision method as well. The driving environment and condition is complex {{as well as}} the gaze behavior characteristics, and given that, a solution consists of patten classification and the multi-information decision-level fusion were put forward to estimate the different kind model of the driver's gaze behavior. In order to test the proposed strategies，the real time driver's <b>gaze</b> behavior <b>detection</b> system was established. The “T ” characteristic curve proposed through the abnormal behavior parameters of the transverse width between the eyes and the vertical distance between mouth and the midpoint of two eyes, combined with the driver's eyelid closure and the proportion and location characteristics of iris- sclera were studied to get the characterization of the driver’s gaze status information. The simulation results indicate that the adaptability and accuracy {{as well as the}} intelligent level is significantly improved by using the pattern classification and decision-making technology through multi-source information fusion...|$|R
40|$|Conventional gaze {{tracking}} systems {{are limited in}} cases where the user is wearing glasses because the glasses usually produce noise due to reflections caused by the gaze tracker’s lights. This makes it difficult to locate the pupil and the specular reflections (SRs) from the cornea of the user’s eye. These difficulties increase the likelihood of <b>gaze</b> <b>detection</b> errors because the gaze position is estimated based on the location of the pupil center and the positions of the corneal SRs. In order to overcome these problems, we propose a new {{gaze tracking}} method that can be used by subjects who are wearing glasses. Our research is novel in the following four ways: first, we construct a new control device for the illuminator, which includes four illuminators that are positioned at the four corners of a monitor. Second, our system automatically determines whether a user is wearing glasses or not in the initial stage by counting the number of white pixels in an image that is captured using the low exposure setting on the camera. Third, if it is determined that the user is wearing glasses, the four illuminators are turned on and off sequentially in order to obtain an image that has a minimal amount of noise due to reflections from the glasses. As a result, it is possible to avoid the reflections and accurately locate the pupil center and the positions of the four corneal SRs. Fourth, by turning off one of the four illuminators, only three corneal SRs exist in the captured image. Since the proposed <b>gaze</b> <b>detection</b> method requires four corneal SRs for calculating the gaze position, the unseen SR position is estimated based on the parallelogram shape that is defined by the three SR positions and the gaze position is calculated. Experimental results showed that the average <b>gaze</b> <b>detection</b> error with 20 persons was about 0. 70 ° and the processing time is 63. 72 ms per each frame...|$|E
40|$|Abstract: Head {{orientation}} {{is an important}} part of many advanced human-machine interaction systems. We present a single image based head pose computation algorithm. It is deduced from anthropometric data. This approach allows us to use a single camera and requires no cooperation from the user. Using a single image avoids the complexities associated with of a multi-camera system. Evaluation tests show that our approach is accurate, fast and can be used in a variety of contexts. Application to <b>gaze</b> <b>detection,</b> with a working system, is also demonstrated. ...|$|E
40|$|AbstractIn this paper, {{we propose}} {{a simple and}} {{effective}} system to extract a pupil and <b>gaze</b> <b>detection.</b> This system is developed using an optical based method to extract user's pupil. For extracting the pupil of the eye and detecting gaze, we use a camera with an infrared filter, an optical see-through HMD, and infrared light. We can possible to move freely without fixing it to user's head because of optical see-through HMD with the camera, and infrared light. The proposed system displays the gaze point on the HMD after extracting user's pupil and detecting gaze real time...|$|E
40|$|The {{research}} aims of {{this thesis}} are {{to investigate the}} attention cues available to and used by apes, especially gorillas (Gorilla gorilla), to ascertain the direction of conspecific attention during social interactions with a special reference to social play. Minimal {{research has been conducted}} on the role of attention cues - eye gaze, head, and body orientation - to regulate natural social interactions, such as social play, in non-human primates. This thesis begins with an investigation of the "cooperative eye hypothesis", which poses that humans have evolved a unique white sclera adaptation for advertising and detecting gaze direction. Chapter 2 reports the existence of a natural white sclera variation in a proportion of gorilla eyes - contradicting the widely held assumption that white sclera is an exclusively human characteristic - and analyzes the presence of white sclera in relation to other morphological changes in the human eye. The study concludes that the morphological elongation of the eye might be a more important and unique change than the white sclera coloration. Chapter 3 experimentally explores the contribution of white sclera in both great ape and human eye gaze to the perception of <b>gaze</b> direction <b>detection</b> by human observers. This chapter concludes that although white sclera contributes to the accuracy and speed of <b>gaze</b> direction <b>detection</b> (an assumption that this thesis has put to experimental test for the first time), this merely adds to the already efficient gaze cues available in the eye areas of the ape face. Chapter 4 investigates the role of eye gaze, head, and body orientations during gorilla social play behavior, and more specifically, introduces a novel analysis of "vigilance periods" (VPs), in which gorillas may use the interaction between attention cues to gauge the attention and intentions of play partners to successfully navigate play. The final study (Chapter 5) complements Chapter 4 and investigates the role of gorilla postures, behaviors, and movements during changes in attentional cue orientations. This chapter concludes that gorillas often engage in physical rest during VPs but maintain attentional engagement and can assemble and impart socially relevant information based on the behaviors, movements, and attention orientations of their partner. Together, these studies suggest that attention orientation is conveyed and assessed by gorillas through a variety of interacting cues to navigate and modify social play interactions...|$|R
40|$|Abstract- This {{study shows}} that a precise eye {{tracking}} and pupillometer system, neither interfering visual field nor influenced by head movement, can be implemented by analyzing first Paikinje image of visual field which has a few reference points. If two or more reference points are sited around the center of visual field, then eye camera captures the overlapped image of reference points and pupil. Four high intensity infra-red light-emitting diodes (IR LEDs) are used as reference points in this study not to disturb the subject’s attention. These reference points appear as four highlighting points in the captured pupil image. The actual gazing point can be calculated by finding {{the distance between the}} center of these four reference points and pupil center and angle. Head movement causes some shift of location of pupil and reference points in the same direction and with the same amount hence does not change the relation of them. This technique can afford to make a system that has fast sampling time and high resolution without extra servo-mechanism to chase subject’s head movement. Index Terms – <b>Gazing</b> Point <b>Detection</b> ∗ I...|$|R
40|$|The {{human eye}} is unique amongst those of {{primates}} in having white sclera {{against which the}} dark iris is clearly visible. This high-contrast structure makes the gaze direction of a human potentially easily perceptible to others. For a social creature such as a human, the ability to perceive the direction of another’s gaze may be very useful, since gaze usually signals attention. We report data showing that the accuracy of <b>gaze</b> deviation <b>detection</b> is independent of viewing distance {{up to a certain}} critical distance, beyond which it collapses. This is, of itself, surprising since most visual tasks are performed better at closer viewing distances. Our data also show that the critical distance, but not accuracy, is affected by the position of the eyebrows so that lowering the eyebrows reduces the critical distance. These findings show that mechanisms exist by which humans could expand or restrict the availability of their gaze direction to others. A way to regulate the availability of the gaze-direction signal could be an advantage. We show that an interpretation of eyebrow function in these terms provides a novel explanation for several well-known eyebrow actions, including the eyebrow flash...|$|R
40|$|The {{prediction}} of eye direction detection {{is the one}} of the popular research topic in human computer interaction area. This paper defines eye <b>gaze</b> <b>detection</b> by using Discrete Cosine Transform. Actually, determining the position of eyes is difficult to estimate the location of gaze which is more challenging. The database of the suggested research is organized as gaze directions of right, left and centre. The database is created with varied ages of images. In this paper, Discrete Cosine Transform has been applied to the image database and the effect of image compression is tested by using back propagation neural networks...|$|E
40|$|Gaze {{tracking}} is a camera-vision based {{technology for}} identifying the location where a user is looking. In general, a calibration process is applied {{at the initial}} stage of most gaze tracking systems. This process is necessary to calibrate for {{the differences in the}} eyeballs and cornea size of the user, as well as the angle kappa, and to find the relationship between the user’s eye and screen coordinates. It is applied {{on the basis of the}} information of the user’s pupil and corneal specular reflection obtained while the user is looking at several predetermined positions on a screen. In previous studies, user calibration was performed using various types of markers and marker display methods. However, studies on estimating the accuracy of <b>gaze</b> <b>detection</b> through the results obtained during the calibration process have yet to be carried out. Therefore, we propose the method for estimating the accuracy of a final gaze tracking system with a near-infrared (NIR) camera by using a fuzzy system based on the user calibration information. Here, the accuracy of the final gaze tracking system ensures the <b>gaze</b> <b>detection</b> accuracy during the testing stage of the gaze tracking system. Experiments were performed using a total of four types of markers and three types of marker display methods. From them, it was found that the proposed method correctly estimated the accuracy of the gaze tracking regardless of the various marker and marker display types applied...|$|E
30|$|We {{noticed the}} issue of {{reliability}} and accuracy of Kinect data. Kinect sensor employs computer vision algorithms to detect facial features, such as eyes, nose, and mouth, and to fit a detailed 3 D face model. The detection sometimes fail and in other cases produce erroneous results. The reliability of <b>gaze</b> <b>detection</b> depends on {{the orientation of the}} face (frontal or not) and presence of obstructing objects such as hands. The similar issue is with person’s skeleton which is not accurate due to obstructed view of the person sitting behind a table. We could not include hand 3 D coordinates due to low reliability.|$|E
40|$|We define gaze agency as the {{awareness}} of the causal effect of one's own eye movements in gaze-contingent environments, which might soon become a widespread reality with the diffusion of gaze-operated devices. Here we propose a method for measuring gaze agency based on self-monitoring propensity and sensitivity. In one task, naïf observers watched bouncing balls on a computer monitor {{with the goal of}} discovering the cause of concurrently presented beeps, which were generated in real-time by their saccades or by other events (Discovery Task). We manipulated observers' self-awareness by pre-exposing them to a condition in which beeps depended on gaze direction or by focusing their attention to their own eyes. These manipulations increased propensity to agency discovery. In a second task, which served to monitor agency sensitivity at the sensori-motor level, observers were explicitly asked to detect <b>gaze</b> agency (<b>Detection</b> Task). Both tasks turned out to be well suited to measure both increases and decreases of gaze agency. We did not find evident oculomotor correlates of agency discovery or detection. A strength of our approach is that it probes self-monitoring propensity-difficult to evaluate with traditional tasks based on bodily agency. In addition to putting a lens on this novel cognitive function, measuring gaze agency could reveal subtle self-awareness deficits in pathological conditions and during development...|$|R
40|$|An {{experimental}} study {{at a large}} research university evaluated the role of image resolution on 60 participant 2 ̆ 7 s locomotion tasks in an interior virtual desktop wayfinding environment. Four virtual environments were developed using different resolutions for wayfinding images including high level 150 ppi, medium high level 100 ppi, medium level 75 and low level 30 ppi images. The environment was designed to accommodate forward, backward, sideways and figure 8 locomotion tasks {{as defined by the}} VEPAB. The effects of the different image resolutions on time-on-task performance scores to navigate through the environment, object-based visual attention as recorded by two eye movements - saccade and <b>gaze</b> fixation <b>detections</b> in viewing the images and the perceived usability of the computer system and virtual environment as measured by two post tests - the System Usability Scale and the Virtual Environment Presence Questionnaire were evaluated. Eye movement scores were collected with an eye tracking system that used the dark pupil method of eye analysis with a video lipstick camera. Results indicate that image resolution has a statistically (p 3 ̆c. 05) significant effect on time-on-task performance wayfinding tasks and on object-based visual attention as indicated by gaze fixation scores. Participants fixated longer on lower resolution images, which affected their time-on-task performance. Resolution did not have a statistically significant effect on the perceived usability of the computer system or virtual environment...|$|R
30|$|To {{test for}} {{a link between}} <b>gaze</b> and change <b>detection,</b> we tested whether {{duration}} of gaze at pre- and post-change properties predicted change detection. To test this hypothesis we defined rectangular regions of interest bounding the pre-change and post-change properties and tabulated the number of frames (temporally contiguous or not) for which gaze fell within those regions. In video  2, the changing property was visible for the entire duration of the video, so we limited the assessed fixation durations to the 10 [*]s {{before and after the}} change. These gaze durations were entered as predictors into a generalized estimating equations analysis with video, pre-change gaze duration, and post-change gaze duration predicting change detection. Increases in post-change (X 2 (1)[*]=[*] 11.580, p[*]=[*] 0.001) but not pre-change (X 2 (1)[*]=[*] 1.246, p[*]=[*] 0.264) gaze duration significantly predicted increased change detection.|$|R
