19|7956|Public
50|$|Erol {{was working}} on remixes for Björk and Interpol in summer 2007, but lost them due to a <b>hard</b> <b>drive</b> <b>failure.</b>|$|E
50|$|Events {{are stored}} online, {{so in the}} case of a user's <b>hard</b> <b>drive</b> <b>failure,</b> no data is lost. Calendar sharing is also available.|$|E
50|$|In January, having {{recovered}} from its <b>hard</b> <b>drive</b> <b>failure,</b> SMTH reopened without WWW access. At the same time, Tsinghua University BBS Regulations came into effect. At {{the end of}} February, due {{to the death of}} Deng Xiaoping, SMTH was temporarily closed. It reopened again on the third of April, with its IP address changed to 202.112.58.200.|$|E
50|$|Advantages include lower power {{consumption}} than standard RAID levels, {{the ability to}} use multiple <b>hard</b> <b>drives</b> with differing sizes to their full capacity and in the event of multiple concurrent <b>hard</b> <b>drive</b> <b>failures</b> (exceeding the redundancy), only losing the data stored on the failed <b>hard</b> <b>drives</b> compared to standard RAID levels which offer striping in which case all of the data on the array is lost when more <b>hard</b> <b>drives</b> fail than the redundancy can handle.|$|R
50|$|A <b>hard</b> disk <b>drive</b> <b>failure</b> {{occurs when}} a <b>hard</b> disk <b>drive</b> {{malfunctions}} and the stored information cannot be accessed with a properly configured computer.|$|R
50|$|The {{first album}} was the {{accidental}} by-product of Alan and Brent reuniting with former bandmates, to record old songs for posterity. The experience of reuniting, playing together and recording was awesome. The comradery {{and love for}} the music had not waned. Pre-production rehearsals and recording sessions were fun and effortless, until a power outage caused <b>hard</b> <b>drive</b> <b>failures.</b> All work was lost. The worst crash came next.|$|R
50|$|Kleo is a {{graphical}} wizard {{file system}} cloning system. It is {{designed as a}} disaster-recovery tool, producing complete file system images. In the event a server experiences a <b>hard</b> <b>drive</b> <b>failure,</b> the LiveCD can be booted, and the previous backup image {{can be used to}} recover the server to the point in time of the last backup.|$|E
5000|$|SpinRite, a {{hard disk}} {{scanning}} and data recovery utility first released in 1988. [...] As of February 2015 the current version was 6.0, which was first released in 2004. [...] SpinRite is a commercial product, costing $89 as of February 2015. [...] Gibson's work on SpinRite {{has led to}} him being considered an expert on <b>hard</b> <b>drive</b> <b>failure.</b>|$|E
50|$|On February 4, 2009, OhioLINK {{suffered}} a catastrophic <b>hard</b> <b>drive</b> <b>failure.</b> This affected all major services {{for several days}} {{with the exception of}} the catalog and the DRC, which were not affected. The drive failure resulted in data loss on the Electronic Journal Center; a complete reload of the Electronic Journal Center’s content was completed six weeks after the initial crash.|$|E
50|$|Google's 2007 study found, {{based on}} a large field sample of drives, that actual annualized failure rates (AFRs) for {{individual}} drives ranged from 1.7% for first year drives to over 8.6% for three-year-old drives. A similar 2007 study at CMU on enterprise drives showed that measured MTBF was 3-4 times lower than the manufacturer's specification, with an estimated 3% mean AFR over 1-5 years based on replacement logs for a large sample of <b>drives,</b> and that <b>hard</b> <b>drive</b> <b>failures</b> were highly correlated in time.|$|R
50|$|Netcell was a fabless {{semiconductor}} {{company that}} specialized {{in the development}} of computer storage solutions for the consumer and enterprise markets. The company pioneered the concept of a Storage Processing Unit (SPU), a device that accelerates <b>hard</b> <b>drive</b> intensive processing tasks, provides data protection against <b>hard</b> <b>drive</b> <b>failures</b> through RAID 3 (when used with multiple drives), and makes it simple to upgrade storage capacity. The company's flagship product, the Revolution SPU, was offered in two configurations: the NC3000 chip and the more powerful NC5000 chip. In 2005, various add-in-board manufacturers, such as XFX and PNY Technologies, released storage processing cards based on Netcell's Revolution SPU.|$|R
40|$|<b>Hard</b> disk <b>drive</b> <b>{{failures}}</b> {{are rare}} but are often costly. The ability to predict failures {{is important to}} consumers, drive manufacturers, and computer system manufacturers alike. In this paper we investigate the abilities of two Bayesian methods to predict disk <b>drive</b> <b>failures</b> based on measurements of drive internal conditions. We first view the problem from an anomaly detection stance...|$|R
50|$|Daniels later added a {{forum to}} her site {{because of the}} large volume of email she received. The forum allowed her {{visitors}} to get answers to the many questions they had. Lissa's HTML help forum had over 75,000 members, but due to a <b>hard</b> <b>drive</b> <b>failure</b> with site backups that occurred in August 2007, was offline for almost two years. The forums were reinstalled on 3 July 2009, but later went down again and are currently not available.|$|E
50|$|This {{software}} is useful when {{the computer is}} replaced or to recover after a <b>hard</b> <b>drive</b> <b>failure.</b> It also have the functionality to save music, playlists, podcasts and videos from iPods, iPhones or iPads directly into the hard drive or iTunes, along with album art, ratings, play counts and other song data. Also, because iTunes backups don't include synced content: music, playlists, photos, ebooks, etc. In order to thoroughly backup an iPhone to your computer, you will need additional software, such as TouchCopy.|$|E
50|$|Events {{are stored}} on a server {{which can be}} {{accessed}} through a network, meaning that the calendar can be viewed from any location that has Internet access. In {{the case of a}} local user experiencing a <b>hard</b> <b>drive</b> <b>failure,</b> it also means that no data is lost. The application can import iCalendar calendars (.ics, the de facto open calendaring file format). Multiple calendars can be added and shared, allowing various levels of permissions for users. This enables collaboration and sharing of schedules between groups.|$|E
50|$|Data {{recovery}} hardware {{was developed}} because data recovery software lacks {{the ability to}} deal with all lost or corrupted data files. Often the failures, such as media files with bad sectors, firmware failures, PCB (Printed circuit board) <b>failures,</b> <b>hard</b> <b>drive</b> head <b>failures,</b> etc., cannot be fixed.|$|R
5000|$|According to Backblaze, {{the company}} {{switched}} to Seagate 3 TB <b>hard</b> <b>drives</b> after the 2011 Thailand floods disrupted {{the supply of}} <b>hard</b> <b>drives</b> and increased their prices by 200-300%. Backblaze, which normally used HGST 3 TB <b>hard</b> <b>drives,</b> were only able to find Seagate 3 TB drives in [...] "decent quantity". Backblaze noted that the failure rates of the ST3000DM001 did not follow a bathtub curve typically followed by <b>hard</b> disk <b>drive</b> <b>failure</b> rates, instead having 2.7% failing in 2012, 5.4% failing in 2013, and 47.2% failing in 2014. Other 3 TB <b>hard</b> <b>drives</b> that Backblaze placed in service in 2012, which were operated in a similar environment as the Seagate drives, did not show signs of increased failure.|$|R
40|$|Motivated {{by three}} failure data sets (lifetime of patients, <b>failure</b> time of <b>hard</b> <b>drives</b> and <b>failure</b> timeof a product), we {{introduce}} three different three-parameter distributions, study basic mathematical properties, address estimation by {{the method of}} maximum likelihood and investigate finite sample performance of the estimators. We show {{that one of the}} new distributions provides a better fit toeach data set than eight other distributions each having three parameters and three distributions each having two parameters. Peer Reviewe...|$|R
50|$|In late September 2010 the Linux Game Publishing server {{suffered}} a massive <b>hard</b> <b>drive</b> <b>failure</b> which took down {{all of their}} online infrastructure, including related websites such as Tux Games and The Linux Game Tome. Various other unforeseen issues caused the recovery not to take place until late November, with partial service being restored on November 23, 2010, with full recovery not being made until December 8, 2010. They have since stated that work is going well on their current project, {{and that they have}} a working build of it in internal alpha testing.|$|E
50|$|In 2011, {{the flying}} height in modern drives {{was a few}} nanometers. Thus, the head can collide with even an {{obstruction}} as thin as a fingerprint or a particle of smoke. Despite the dangers of <b>hard</b> <b>drive</b> <b>failure</b> from such foreign objects, hard drives generally allow for ventilation (albeit through a filter) so that the air pressure within the drive can equalize with the air pressure outside. Because disk drives depend on the head floating on a cushion of air, they are not designed to operate in a vacuum. Regulation of flying height {{will become even more}} important in future high-capacity drives.|$|E
50|$|The DJ licence {{available}} {{is a combination}} CDR and Hard Drive licence which permits DJs to copy and compress sound recordings to a hard drive (MP3) {{for use as a}} professional DJ only. The licence permits DJs to prepare CD compilations of AVLA member-label released music for use solely in their DJ business, and not for distribution under any circumstances. In addition, it permits a DJ to have a back-up hard drive prepared and available {{in the event of a}} <b>hard</b> <b>drive</b> <b>failure.</b> The agreement stipulates that only original source media can be copied and the media must be owned by the DJ. Music from an MSS or AVLA licensed Music Supply Service (ERG, Multi Music, Promo Only, etc.) CAN NOT be copied.|$|E
40|$|Today's most {{reliable}} data storage systems {{are made of}} redundant arrays of inexpensive disks (RAID). The quantification of RAID system reliability is often based on models that omit critical <b>hard</b> disk <b>drive</b> <b>failure</b> modes, assume all failure and restoration rates are constant (exponential distributions), and assume the RAID group times to failure follow a homogeneous Poisson process (HPP). This paper presents a comprehensive reliability model that accounts for numerous failure causes for today's <b>hard</b> disk <b>drives,</b> allows proper representation of repair and restoration, and does not rely on the assumption of a HPP for the RAID group. The model does not assume <b>hard</b> disk <b>drives</b> have constant transition rates, but allows each <b>hard</b> disk <b>drive</b> "slot" in the RAID group {{to have its own}} set of distributions, closed form or user defined. <b>Hard</b> disk <b>drive</b> (HDD) <b>failure</b> distributions derived from field usage are presented, showing that failure distributions are commonly non-homogeneous, frequently having increasing hazard rates from time zero. <b>Hard</b> disks <b>drive</b> <b>failure</b> modes and causes are presented and used to develop a model that reflects not only complete failure, but also degraded conditions due to undetected, but corrupted data (latent defects). The model can represent user defined distributions for completion of "background scrubbing" to correct (remove) corrupted data. Sequential Monte Carlo simulation is used to determine the number of double disk failures expected as a function of time. RAID group can be any size up to 25. The results are presented as mean cumulative failure distributions for the RAID group. Results estimate the number of double disk failures can be as much as 5000 times greater than that predicted over 10 years when using the mean time to data loss method or Markov models when the characteristic lives of the input distributions is the same. Model results are compared to actual field data for two HDD families and two different RAID group sizes and show good correlation. Results show the rate of occurrence of failure for the RAID group may be increasing, decreasing or constant depending on the parameters used for the four input distributions...|$|R
40|$|We compare machine {{learning}} methods {{applied to a}} difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities {{in the case of}} discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like <b>hard</b> <b>drive</b> <b>failures...</b>|$|R
50|$|ReFS was {{designed}} to overcome issues that had become significant over the years since NTFS was conceived, which are related to how data storage requirements had changed. The key design advantages of ReFS include automatic integrity checking and data scrubbing, removal {{of the need for}} running chkdsk, protection against data degradation, built-in handling of <b>hard</b> disk <b>drive</b> <b>failure</b> and redundancy, integration of the RAID functionality, a switch to copy/allocate on write for data and metadata updates, handling of very long paths and filenames, and storage virtualization and pooling, including almost arbitrarily sized logical volumes (unrelated to the physical sizes of the used drives).|$|R
5000|$|Flaming Fish Music {{settled into}} {{dormancy}} and little was heard of lvl until 2006 {{when he was}} signed to Klayton's new label, FiXT Music. This is when [...] officially changed {{the name of his}} band from Level to lvl. He explained his absence since 2003's Denial and announced that he was working on new material in addition to the Re:issue project. However, apart from a handful of new demos, only one minor release has since been issued, that being a remix single of the song [...] "Home" [...] from the Denial album. [...] Various financial and hardware problems, including a <b>hard</b> <b>drive</b> <b>failure</b> that deleted an album's worth of songs, have prevented lvl from moving forward with his new album work and Re:Issue seems to have been indefinitely delayed.|$|E
5000|$|Before {{working as}} a film critic, Vishnevetsky worked as a translator, movie theater usher and {{laundromat}} attendant. Beginning in 2004, he became involved with Chicago's cinephile community, many of whose members he met through the video rental store [...] "Odd Obsession". He {{was involved in a}} screening space called North Western Avenue, whose participants later co-founded the film website Cine-File.info, to which Vishnevetsky continues to contribute. In 2006, Vishnevetsky wrote and directed a 45-minute short film; he also served as the film's editor and cinematographer. Soon after a final cut was completed, a <b>hard</b> <b>drive</b> <b>failure</b> destroyed much of the footage. In an interview with the podcast Film in Focus, Vishnevetsky stated that the experience lead him to pursue film criticism full-time. Vishnevetsky published a film zine called Zero for Conduct before joining the NYU-based film journal Tisch Film Review and then MUBI.|$|E
5000|$|The SugarSync program {{automatically}} refreshes its sync by constantly monitoring {{changes to}} files—additions, deletions, edits—and syncs these changes with the SugarSync servers. Any other linked devices then also {{sync with the}} SugarSync servers. Deleted files are archived in a [...] "Deleted Files" [...] folder. In the event the local sync folder is stored on a device which later becomes unavailable (secondary <b>hard</b> <b>drive</b> <b>failure,</b> etc.) the SugarSync program will interpret this event as if the user had purposely deleted the entire synchronization folder, resulting in deletion of all files from the user's storage account. Due to this limitation, {{it is best to}} only store the local synchronization folder on the boot drive. Files deleted by the user are not actually removed from SugarSync servers until the user does so manually; however, recovery of a larger nested folder structure may be difficult.|$|E
40|$|<b>Hard</b> disk <b>drive</b> <b>{{failures}}</b> {{are rare}} but are often costly. The ability to predict failures {{is important to}} consumers, drive manufacturers, and computer system manufacturers alike. In this paper we investigate the abilities of two Bayesian methods to predict disk <b>drive</b> <b>failures</b> based on measurements of drive internal conditions. We first view the problem from an anomaly detection stance. We introduce a mixture model of naive Bayes submodels (i. e. clusters) that is trained using expectation-maximization. The second method is a naive Bayes classifier, a supervised learning approach. Both methods are tested on realworld data concerning 1936 drives. The predictive accuracy of both algorithms is far higher than the accuracy of thresholding methods used in the disk drive industry today. 1...|$|R
5000|$|JBOD (abbreviated from [...] "just a {{bunch of}} disks/drives") is an {{architecture}} using multiple <b>hard</b> <b>drives</b> exposed as individual devices. <b>Hard</b> <b>drives</b> may be treated independently or may be combined into a one or more logical volumes using a volume manager like LVM or mdadm; such volumes are usually called [...] "spanned" [...] or [...] "linear | SPAN | BIG". [...] A spanned volume provides no redundancy, so failure of a single <b>hard</b> <b>drive</b> amounts to <b>failure</b> of the whole logical volume. [...] Redundancy for resilience and/or bandwidth improvement may be provided, in software, at a higher level.|$|R
5000|$|Another {{scenario}} {{involves a}} drive-level failure, {{such as a}} compromised file system or drive partition, or a <b>hard</b> disk <b>drive</b> <b>failure.</b> In any of these cases, the data is not easily read from the media devices. Depending on the situation, solutions involve repairing the logical file system, partition table or master boot record,or updating the firmware or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the <b>hard</b> disk <b>drive's</b> [...] "firmware"), to hardware replacement on a physically damaged drive which involves changes {{the parts of the}} damaged drive to make the data in a readable form and can be copied to a new drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.|$|R
40|$|A {{computer}} <b>hard</b> <b>drive</b> <b>failure</b> unfortunately {{means that}} the raw data is lost for figs 5 and 6 but the authors are happy to correspond and answer any queries anyone has about any element of the work. Citation: Effect of 1 -ethyl- 3 -(3 -dimethylaminopropyl) carbodiimide and N-hydroxysuccinimide concentrations on the mechanical and biological characteristics of cross-linked collagen fibres for tendon repair (CC) Zafar Ahmad, Jennifer H. Shepherd, David V. Shepherd, Siddhartha Ghose, Simon J. Kew, Ruth E. Cameron, Serena M. Best, Roger A. Brooks, John Wardale, Neil Rushton. Regenerative Biomaterials. First published online: 16 May 2015 DOI: [URL] work {{was supported by the}} EPSR...|$|E
40|$|Hard {{disk drives}} {{are used in}} {{everyday}} life to store critical data. Although they are reliable, failure of a hard disk drive can be catastrophic, especially in applications like medicine, banking, air traffic control systems, missile guidance systems, computer numerical controlled machines, and more. The use of Self-Monitoring, Analysis and Reporting Technology (SMART) can aid in failure prediction by monitoring specific drive attributes and warning the user of an impending failure so that the user can backup data while there is still time. As a consequence, <b>hard</b> <b>drive</b> <b>failure</b> prediction has become an important problem {{and the subject of}} active research. The best available approaches for <b>hard</b> <b>drive</b> <b>failure</b> prediction achieve acceptably low false alarm rates by first selecting a subset of features using non-parametric statistical methods such as reverse arrangements and then using the multiple-instance naïve Bayes classifier for the prediction task. However, the prediction accuracy of this approach is not sufficiently high. The focus of this dissertation was to improve the drive failure prediction accuracy while maintaining a low false alarm rate by using a genetic algorithm for feature set reduction in conjunction with the multiple-instance naïve Bayes classifier for the prediction task. This research achieved a failure detection rate of 81 % with a 0 % false alarm rate on 12 attributes selected by the genetic algorithm. As a secondary contribution, this dissertation investigated the tradeoff between feature subset reduction and prediction accuracy in the hard drive prediction problem. This research found that as the number of features decreased below 10, the detection accuracy decreased significantly...|$|E
40|$|Data centers use {{large numbers}} of hard drives as data storage devices {{and it is an}} {{increasing}} challenge to maintain the reliability of the storage system as the number of the hard drives increases exponentially. Manual monitoring {{does not seem to be}} efficient for large scale storage systems. Typically, the distributions of healthy hard drives and failed hard drives are highly imbalance. In addition, the size of the training data is large for large scale storage systems. The existence of such challenges makes the <b>hard</b> <b>drive</b> <b>failure</b> prediction problem interesting. In this thesis, several classification models are applied to the hard drive S. M. A. R. T. data from 34, 970 hard drives for failure prediction, and the results are compared. Based on the analysis, XGBoost provides the best overall prediction result and it is able to process the data efficiently...|$|E
40|$|A {{flexible}} {{model for}} estimating reliability of RAID storage systems is presented. This model corrects errors {{associated with the}} common assumption that system times to failure follow a homogeneous Poisson process. Separate generalized failure distributions are used to model catastrophic failures and usage dependent data corruptions for each <b>hard</b> <b>drive.</b> Catastrophic <b>failure</b> restoration is represented by a three-parameter Weibull, so the model can include a minimum time to restore {{as a function of}} data transfer rate and <b>hard</b> <b>drive</b> storage capacity. Data can be scrubbed as a background operation to eliminate corrupted data that, {{in the event of a}} simultaneous catastrophic failure, results in double disk failures. Field-based times to failure data and mathematic justification for a new model are presented. Model results have been verified and predict between 2 to 1, 500 times as many double disk failures as that estimated using the current mean time to data loss method. 1...|$|R
50|$|Some <b>hard</b> disk <b>drives</b> simply fail {{because of}} worn out parts, others fail prematurely. Drive {{manufacturers}} typically specify a {{mean time between}} failures (MTBF) or an annualized failure rate (AFR) which are population statistics that can not predict the behavior of an individual unit. These are calculated by constantly running samples of the drive for a short amount of time, analyzing the resultant wear and tear upon the physical components of the drive, and extrapolating to provide a reasonable estimate of its lifespan. <b>Hard</b> disk <b>drive</b> <b>failures</b> tend to follow the concept of the bathtub curve. Drives typically fail within a short time if there is a defect present from manufacturing. If a drive proves reliable for a period of a few months after installation, the drive has a significantly greater chance of remaining reliable. Therefore, even if a drive is subjected to several years of heavy daily use, it may not show any notable signs of wear unless closely inspected. On the other hand, a drive can fail at any time in many different situations.|$|R
40|$|With {{the advent}} of cloud {{computing}} and online services, large enterprises rely heavily on their datacenters to serve end users. A large datacenter facility incurs increased maintenance costs in addition to service unavailability when there are increased failures. Among different server components, <b>hard</b> disk <b>drives</b> are known to contribute significantly to server failures; however, {{there is very little}} understanding on the major determinants of disk failures in datacenters. In this work, we focus on the inter-relationship between temperature, workload, and <b>hard</b> disk <b>drive</b> <b>failures</b> in a large scale datacenter. We present a dense storage case study from a population housing thousands of servers and ten thousands of disk drives, hosting a large scale online service at Microsoft. We specifically establish correlation between temperatures and failures observed at different location granularities: a) inside drive locations in a server chassis, b) across server locations in a rack and c) across multiple racks in a datacenter. We show that temperature exhibits a stronger correlation to failures compared to the correlation of disk utilization with <b>drive</b> <b>failures.</b> We establish that variations in temperature are not significant in datacenters and have little impact on failures. We also explore workload impacts on temperature and disk failures and show that the impact of workload is not significant. We then experimentally evaluate knobs that control disk drive temperature, including workload and chassis design knobs. We corroborate our findings from th...|$|R
