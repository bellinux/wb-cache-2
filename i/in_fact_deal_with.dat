8|10000|Public
2500|$|Morris {{elsewhere}} {{argues that}} Karsh [...] "belabor minor points while completely ignoring, and hiding from his readers, the main pieces of evidence" [...] and argued, [...] "... Karsh, while {{claiming to have}} 'demolished' the whole oeuvre, <b>in</b> <b>fact</b> <b>deal</b> <b>with</b> only four pages of Birth. These pages tried {{to show that the}} Zionist leadership during 1937–38 supported a 'transfer solution' to the prospective Jewish state's 'Arab problem.'" [...] Commenting on the Revisited version of Morris'work, Karsh states that in [...] "an implicit acknowledgement of their inaccuracy, Morris has removed some of The Births most inaccurate or distorted quotations about transfer." ...|$|E
50|$|Pilkington {{argues that}} {{economics}} and any disciplines {{that deal with}} historical data rather than repeatable controlled experiments deal with material that is fundamentally uncertain. Such material can never be assumed to adhere to fixed laws in the way that, say, chemistry and physics can be thought to adhere to fixed laws. Neoclassical economists and econometricians evade this by claiming that economic and historical processes are characterised by known probability distributions. But Pilkington shows that this is impossible because economic and historical processes are open, not closed systems and so rather than dealing {{with a series of}} bounded probabilities we <b>in</b> <b>fact</b> <b>deal</b> <b>with</b> a series of unbounded possibilities.|$|E
40|$|Hoyer {{has given}} a {{generalisation}} of the Deutsch [...] Jozsa algorithm which uses the Fourier transform on a group G which is (in general) non-Abelian. His algorithm distinguishes between functions which are either perfectly balanced (m-to-one) or constant, with certainty, and using a single quantum query. Here, we show that this algorithm (which we call the Deutsch [...] Jozsa [...] Hoyer algorithm) can <b>in</b> <b>fact</b> <b>deal</b> <b>with</b> {{a broader range of}} promises, which we define in terms of the irreducible representations of G. Comment: 24 pages, 5 figures, to appear in LMS JCM Updated on 9 th August 2005 following the referees comments. Added: Overview of questions surrounding the Fourier transform; Appendix on group representations. Corrected typos and improved notatio...|$|E
50|$|One of his masterpieces is The Martyrdom of Saint Barbara (which <b>in</b> <b>fact</b> <b>deals</b> <b>with</b> Saint Agatha), {{which is}} kept in the Saint Michael's Church, Ghent.|$|R
5000|$|The theorem {{can also}} be reformulated to better suit {{numerical}} methods. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> real eigensystem problems, one often has an exact matrix , but knows only an approximate eigenvalue-eigenvector couple, [...] and needs to bound the error. The following version comes in help.|$|R
40|$|Elasticity has the {{reputation}} of being a rather boring and un-physical subject. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> second (stress-strain) or fourth (elastic constants) rank tensor guarantees that the notations are in general rather heavy, and that the underlying physics is not easily captured 1. The elastic stress-strai...|$|R
40|$|The paper explores {{transactional}} dismissal (TD), {{an illegal}} but common way of dismissing people in France. TD {{is often used}} {{as a means of}} making redundant senior managers when mergers, outsourcing or restructuring occurs. The paper first describes the procedure from several inputs (juridical, managerial, psychological, social). It further analyses the discourses of concerned actors (employees, employers, remaining employees, union representatives, lawyers) and interprets how the practice is appropriated and legitimized by the parties. Actors display an ambivalent discourse about TD. They acknowledge TD from a managerial perspective and consider it as an injustice with distressing psychological consequences. This ambiguity is interpreted {{as a consequence of the}} paradox of the firm: nodes of contracts that, <b>in</b> <b>fact,</b> <b>deal</b> <b>with</b> human beings. The paper also underlines the lack of statistics and quantitative data about TD and the “omerta ” surrounding the transaction. Such a silence remains essential for the survival of the practice...|$|E
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from CSLI Publications via [URL] accounts of HPSG assume a distinction between morphology and syntax. However, despite decades of research, no cross-linguistically valid definition of ‘word’ has emerged (Haspelmath, 2011), suggesting that no sharp distinction is justified. Under such a view, the basic units are morphemes, rather than words, {{but it has been}} argued this raises problems when analysing phenomena such as zero inflection, syncretism, stem alternations, and extended exponence. We argue that with existing HPSG machinery, a morpheme-based approach can <b>in</b> <b>fact</b> <b>deal</b> <b>with</b> such issues. To illustrate this, we consider Slovene nominal declension and Georgian verb agreement, which have both been used to argue against constructive morpheme-based approaches. We overcome these concerns through use of a type hierarchy, and give a morpheme-based analysis which is simpler than the alternatives. Furthermore, we can recast notions from Word-and-Paradigm morphology, such as ‘rule of referral’ and ‘stem space’, in our framework. We conclude that using HPSG as a unified morphosyntactic theory is not only feasible, but also yields fruitful insights...|$|E
40|$|This {{dissertation}} collects essays {{that focuses}} on two branches of the economic literature on imperfect competition: the one of IO on the decision of firms' location and the one of economic growth on market structure and technological progress. Chapter 2 deals {{with the issue of}} firms' location and proposes a model that sheds light on how the geographic distribution of firms is affected by strategic interaction in presence of vertically differentiated products. The remaining Chapters, instead, have a common root; they, <b>in</b> <b>fact,</b> <b>deal</b> <b>with</b> the link existing between market structure and growth. More precisely, Chapter 3 studies how knowledge spillovers across firms influence this link and introduces the basic framework which is further extended in Chapters 4 and 5 to allow for strategic interaction and multi-product firms respectively. In all these models technological progress takes the form of cost reductions. The model-setting considered in these Chapters belongs to the class of creative accumulation models, whose introduction in the economic literature is relatively recent and can be attributed to Smulders and van de Klundert and Peretto. Finally, Chapter 6 follows the creative destruction tradition initiated by Grossman and Helpman and Aghion and Howitt and develops a growth model with product quality innovations of random size. In all the Chapters of this dissertation, we adopt a normative perspective by comparing the market equilibrium solution to the optimal one. (ECON 3) [...] UCL, 200...|$|E
5000|$|A report {{released}} by the Rocky Mountain Information Network states that, [...] "Just {{because we do not}} understand this phenomenon fully, we can’t as gang detectives ignore it ... We in law enforcement must be willing to take that extra step in our intelligence gathering to see if we are <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> a gang member or just a crazed fan." ...|$|R
40|$|The recent {{literature}} on organizational {{theory has been}} dedicating a considerable attention to the relationship networks. In spite of this, however, {{it is possible to}} notice that there are many theoretical problems concerning this subject matter. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> relationship networks can be considered {{as one of the most}} complex challenges to be faced by researchers in organizational theory. This paper intends to contribute to clarify some of those theoretical problems. Through a review of part of the {{literature on}} organizational theory this paper tries to propose new analytical paths to understanding the complexity of the relationship networks...|$|R
5000|$|His {{research}} into the innate capacity for empathy among primates has led De Waal {{to the conclusion that}} non-human great apes and humans are simply different types of apes, and that empathic and cooperative tendencies are continuous between these species. His belief is illustrated in the following quote from The Age of Empathy: [...] "We start out postulating sharp boundaries, such as between humans and apes, or between apes and monkeys, but are <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> sand castles that lose much of their structure when the sea of knowledge washes over them. They turn into hills, leveled ever more, until we are back to where evolutionary theory always leads us: a gently sloping beach." ...|$|R
40|$|Realist {{evaluation}} (Pawson and Tilley, 1997) and realist synthesis (Pawson, 2006) {{were developed}} to assist policy makers and program staff to find pathways through complexity - specifically, to develop and test usable theory about complex and varied interventions applied across multiple contexts. One of the central ideas is that understanding how and why something works, or does not, in particular contexts can assist with practical decision-making: whether to use {{a particular kind of}} program in a particular situation; how to adapt a program to a particular context; and so on. But if complication and complexity are the norm in social programs (Rogers, 2012, Westhorp, 2012, Westhorp, 2013), how exactly does realist synthesis address this? Can it <b>in</b> <b>fact</b> <b>deal</b> <b>with</b> the complication and complexity of real programs and real decisions? And how might practitioners and policy makers actually use the results? This chapter explores these questions through the lens of a recently completed realist review of community accountability and empowerment initiatives. It begins with an overview of the review in terms of its scope, processes and findings. It then analyses the review in terms of Pawson&# 039;s VICTORE framework (discussed earlier in Cchapter 1) to demonstrate the complexity of the review topic, how complexity was reflected in the findings, and how the methodology of realist synthesis helped us to manage and deal with complexity. We then discuss how the findings from a realist synthesis can assist in dealing with the complexities of policy and program management in the real world...|$|E
40|$|The many {{established}} and mandatory data capture systems in healthcare offer the opportunity for' organisational learning. ' This thesis explores {{the use of}} such administrative data (AD), and specifically hospital data sets that have been collected primarily for funding and reporting, for analysis of process and outcome of care for strokes and transient ischaemic attacks (TIAs). As a first step the problems encountered when selecting stroke and TIA cohort from multi-year, state-level, hospital ADs were examined. The cohort selection relies on the recorded patient diagnoses information: International Classification of Disease (lCD) codes and/or Diagnosis Related Groups (DRGs). In the study years the difficulties in accurately selecting the cohort were found to be due to various interpretations of codes and DRGs, the placement of codes to groupers and the different classifications used. Discovered irregularities were reported to the appropriate government department. It became obvious that experienced coders' and clinicians' perspectives along with good knowledge of the changes to codes and groupers are required for accurate cohort selection. Since information about the number and types of non-principal diagnoses (NPDs) would be useful for predicting outcome and hence patient management, the NPDs were examined using the associated prefixes (that identify the onset or relevance of each NPD to the episode) recorded in three fiscal-years of Victorian admitted datasets. The study revealed that the presentation of TIA and strokes are varied and complex, with confirmation of some known relationships and other new ones revealed. The results showed that the prefix categories accompanying NPDs can help to better define the nature of the presentation and thus explain some ofthe observed outcomes. Apart from improving the definition and collection guidelines {{it is important for the}} regular statewide audits to also give feedback that helps improve the quality of prefixes. The treatment of patients with multiple conditions is a norm rather than an exception. Following the review of generic multi-morbidity instruments, it was concluded that most of the basic design features were acceptable. However important construct validity needed addressing, especially the embracing of a more precise and standardised definition such as the emerging condition/disease-specific complexity-of-patientpresentation construct, and a clearer delineation of application and limitations. Therefore a stroke-specific ICD-l O-based complexity-of-stroke-patient-presentation measure using AD was developed and tested. Condition-groups with a potential to contribute to the complexity-of-stroke-patient-presentation were identified and the mapping codes selected. Based on their relative potential to contribute towards the complexity-of-strokepatient- presentation, an index value was assigned to each condition-group. The index values of non-principal diagnoses were used to calculate the Total Complexity Weight for each patient. Convergent validity was tested using a derived 'pseudo gold standard. ' U sing a state AD the alignment of the complexity score to patient factors and outcomes was established. Finally, all of the above methodological improvements were used in analysing the ADs. Since Bayesian Networks (BNs) provide an easily understood representation of causal relationships and support ad hoc exploration of impacts on local distributions, it was the method of choice. Two BN studies are reported. First, formal transfers in and out ofEDs with and without Stroke Care Units (SCUs) were analyzed. This revealed that teaching hospitals with SCU s, while achieving shorter length of stay, <b>in</b> <b>fact</b> <b>deal</b> <b>with</b> younger patients with lower overall patient complexity than non-SCU teaching hospitals. Second, ED triaging of suspected stroke patients who subsequently experienced an inpatient admission was analyzed. It was notable that 45 % of TIAs were categorised as only 'Semi-urgent,' indicating an opportunity to improve emergency assessment ofTIAs. These studies demonstrated that the learning algorithm used with the hybrid BN when applied to ADs can reveal high-level details of the care journey and outcome. ADs are often the best available operational and historical data that are readily obtainable and relatively inexpensive. This project demonstrated that value can be drawn from these datasets to provide high-level insight into the process and outcome of care of a specific cohort of patient. In an era of diminishing resources better utilisation of these datasets should be encouraged. As electronic information systems are increasingly embraced, these collections need to be managed as valuable assets and powerful operational and patient management tools...|$|E
2500|$|The British {{authorities}} foresaw {{the need}} for a civil judicial system to be established when the colony was first established. [...] A court styled the “Court of Civil Jurisdiction” was established by the First Charter of Justice as well as a Court of Vice-Admiralty pursuant to letters patent from the High Admiralty in Great Britain. [...] The Governor of New South Wales would also in time authorise magistrates (the common name for justices of the peace in the colony at the time) to determine smaller debt claims that were taking up the time of these original civil courts. [...] Hilary Golder points to research that magistrates were <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> small debt claims as early 1789, one year after the commencement of the colony.|$|R
40|$|Closed-loop {{neuroscience}} {{is receiving}} increasing attention with recent technological advances that enable complex feedback loops {{to be implemented}} with millisecond resolution on commodity hardware. We summarize emerging conceptual and methodological frameworks {{that are available to}} experimenters investigating a brain in the loop using non-invasive brain stimulation and briefly review the experimental and therapeutic implications. We take the view that closed-loop neuroscience <b>in</b> <b>fact</b> <b>deals</b> <b>with</b> two conceptually quite different loops: A brain-state dynamics loop, used to couple with and modulating the trajectory of neuronal activity patterns, and a task dynamics loop, which is the bidirectional motor-sensory interaction between brain and (simulated) environment, and which enables goal-directed behavioral tasks to be incorporated. Both loops need to be considered and combined to realize the full experimental and therapeutic potential of closed-loop neuroscience...|$|R
2500|$|While Zen and the Art of Motorcycle Maintenance, by Robert M. Pirsig, was a 1974 bestseller, it <b>in</b> <b>fact</b> {{has little}} to do with Zen as a {{religious}} practice, nor with motorcycle maintenance for that matter. Rather it <b>deals</b> <b>with</b> the notion of the metaphysics of [...] "quality" [...] from the point of view of the main character. Pirsig was attending the Minnesota Zen Center at the time of writing the book. He has stated that, despite its title, the book [...] "should in no way be associated with that great body of factual information relating to orthodox Zen Buddhist practice". Though it may not <b>deal</b> <b>with</b> orthodox Zen Buddhist practice, Pirsig's book <b>in</b> <b>fact</b> <b>deals</b> <b>with</b> many of the more subtle facets of Zen living and Zen mentality without drawing attention to any religion or religious organization.|$|R
3000|$|... [...]. In the {{dynamical}} systems, the shadowing {{theory is}} a very useful notion. <b>In</b> <b>fact,</b> it <b>deals</b> <b>with</b> the stability theorem (see [1]). For instance, Robinson [2] proved that if a diffeomorphism f is structurally stable, then it has the shadowing property. In [3] Sakai showed that f belongs to the [...]...|$|R
40|$|The {{following}} research work dwells upon the etymologies and {{meanings of the}} terms “education” and “training” as stated both in different Italian language dictionaries and in specialized wordbooks, starting from the 1970 s to date. Next comes a critical analysis of several authors and their points of view about the Science of Education, to emphasize how {{the use of these}} two words is often unclear and confused. The present research aims to underline not only the linguistic difference but also and above all the dissimilarity in meaning between the concepts of education and training, besides the importance of this distinction in educational institutes. The last part of the work, <b>in</b> <b>fact,</b> <b>deals</b> <b>with</b> the legal reforms of the Italian scholastic system from the 1970 s up to the Gelmini Reform, in order to stress which of the two ideas, the educational or the training one, comes out in a sharper way...|$|R
40|$|Abstract. B 3 G is an {{emerging}} network technology which conceives {{the convergence of}} telecommunication and IP-based networks for providing enhanced services able to transfer both voice and non-voice data through wired and wireless networks. Moreover, B 3 G networks can be no longer considered as “passive ” entities which only transport data between endpoints, but they must be considered as “active ” parties that have their own behavior and provide services. This creates a completely new application domain where applying current software engineering design tools, such as software architectures, fails. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> B 3 G networks requires to explicit low-level details usually abstracted by the architectural descriptions. To this extent, we present an ongoing work on investigating B 3 Goriented application modeling. In particular, we propose an enhanced UML profile to define and analyze software architectures that explicitly exploit the B 3 G domain properties. ...|$|R
50|$|Rimfire irately pushes Ella up {{the edge}} of the chasm back up to the jungle above, and Cool Cat helps Ella up to the top, not noticing Rimfire's presence, and accidentally knocks the hunter back into the chasm. Still {{thinking}} that Ella is a real elephant, Cool Cat leads her away by her trunk to demonstrate some survival skills, while Rimfire climbs out of the chasm for a second time. As Cool Cat leads Ella through the jungle, an amorous male elephant gives chase to Ella, but is then caught by Rimfire who makes the opposite mistake to Cool Cat and assumes that the male elephant is <b>in</b> <b>fact</b> Ella. After yanking at the elephant's skin (angering it in the process) looking for an entry hatch, Rimfire finally realizes that he is <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> a real elephant, who he then apologizes to, though this does not save him from being battered into the ground by the angry elephant's trunk.|$|R
40|$|International audienceAfter {{thirty years}} of researching, the {{photometric}} stereo technique for 3 D shape recovery still does not provide reliable results {{if it is not}} constrained into very well-controlled scenarios. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> realistic materials and lightings yields a non-linear bidirectional reflectance distribution function which is primarily difficult to parametrize and then arduous to solve. With the aim to let the photometric stereo approach face more realistic assumptions, in this work we firstly introduce a unified irradiance equation describing both diffuse and specular reflection components in a general lighting setting. After that, we define a new equation we call unifying due to its basic features modeling the photometric stereo problem for heterogeneous materials. It is provided by making the ratio of irradiance equations holding both diffuse and specular reflections as well as non-linear light propagation features simultaneously. Performing a wide range of experiments, we show that this new approach overcomes state-of-the-art since it leads to a system of unifying equations which can be solved in a very robust manner using an efficient variational approach...|$|R
40|$|International audienceNowadays we are {{witnessing}} the democratization of cloud services. As a result, more and more end- users (individuals and businesses) are using these services for achieving their electronic transactions (shopping, administrative procedures, B 2 B transactions, etc.). In such scenarios, personal data is generally flowed between several entities and end-users need (i) {{to be aware of}} the management, processing, storage and retention of personal data, and (ii) to have necessary means to hold service providers accountable for the usage of their data. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> personal data raises several privacy and accountability issues that must be considered before to promote the use of cloud services. In this paper, we propose a framework for the representation of cloud accountability policies. Such policies offer to end-users a clear view of the privacy and accountability obligations asserted by the entities they interact with, as well as means to represent their preferences. This framework comes with two novel accountability policy languages. An abstract one devoted for the representation of preferences/obligations in an human readable fashion. And a concrete one for the mapping to concrete enforceable policies. We motivate our solution with concrete use case scenarios...|$|R
40|$|The {{regions of}} the Russian Federation are immensely diverse {{economically}} and geographically as well as {{when it comes to}} their national identity, civic awareness and political activity. We are <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> a ‘multi-speed Russia’: along with the post-industrial regions with their higher living standards and a need for pluralism in politics, there are poverty-stricken, inertial regions, dependent on subsidies from the centre. As a result of the policy of centralisation pursued by the Kremlin since 2000, the autonomy of the regions has been reduced fundamentally. This has affected the performance of the regional elites and made it difficult for the regions to use their natural advantages (such as resources or location) to their benefit. One of the effects of this policy has been the constantly decreasing number of the donor regions. The current model promotes the role of the region as a passive supplicant, for whom it is easier to seek support from the central government, offering loyalty in exchange, than to implement complex systemic reforms that would contribute to long-term development. Moscow’s control (political, economic and administrative) over the regions is currently so thorough that it contradicts the formally existing federal form of government in Russia...|$|R
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from IEEE via [URL] thirty years of researching, the photometric stereo technique for 3 D shape recovery still does not provide reliable results {{if it is not}} constrained into very well-controlled scenarios. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> realistic materials and lightings yields a non-linear bidirectional reflectance distribution function which is primarily difficult to parametrize and then arduous to solve. With the aim to let the photometric stereo approach face more realistic assumptions, in this work we firstly introduce a unified irradiance equation describing both diffuse and specular reflection components in a general lighting setting. After that, we define a new equation we call unifying due to its basic features modeling the photometric stereo problem for heterogeneous materials. It is provided by making the ratio of irradiance equations holding both diffuse and specular reflections as well as non-linear light propagation features simultaneously. Performing a wide range of experiments, we show that this new approach overcomes state-of-the-art since it leads to a system of unifying equations which can be solved in a very robust manner using an efficient variational approach. Experimental setups were provided by Toulouse Tech Transfer, and this collaboration was funded by CNRS GdR 2286 (MIA) ...|$|R
50|$|The main protagonist is Tatsuhiro Satō, a {{university}} dropout entering his fourth year of unemployment. He leads a reclusive {{life as a}} hikikomori, ultimately coming {{to the conclusion that}} this happened due to some sort of conspiracy. One day just when his life seems entirely unchanging, he meets Misaki Nakahara, a mysterious girl who claims to be able to cure Tatsuhiro of his hikikomori ways. She presents him with a contract basically outlining that once a day they would meet in the evening in a local park where Misaki would lecture to Tatsuhiro in an effort to rid him of his lifestyle. During these outings, many subjects are discussed, though they almost always pertain in some way to psychology or psychoanalysis. One of their first meetings <b>in</b> <b>fact</b> <b>deals</b> <b>with</b> interpreting Tatsuhiro's recent dreams. Both Tatsuhiro and Misaki, however, have a tendency of over-doing things, such as hiding the truth, especially from each other and themselves. Despite Misaki's offer and pressing attempts at salvation, it is Tatsuhiro's neighbor and high school friend, Kaoru Yamazaki, whom Tatsuhiro often turns to in moments of need and support. Despite his own idiosyncrasies, Yamazaki {{is one of the more}} stable characters in the story.|$|R
6000|$|... [Note that, {{though its}} Sign of Quantity {{tells us how}} many [...] {{existing}} Things are Members of its Predicate, it does not tell [...] us the exact number: <b>in</b> <b>fact,</b> it only <b>deals</b> <b>with</b> two [...] numbers, which are, in ascending order, [...] "0" [...] and [...] "1 or more."] ...|$|R
30|$|Multi drug {{resistant}} Enterobacteriaceae especially Klebsiella pneumoniae and Escherichia coli, are {{the leading}} causes of mortality and morbidity in neonatal bacterial sepsis caused by Gram negatives. Roughly 54 % of K. pneumoniae and 38 % of E. coli strains that caused neonatal sepsis were observed to be multi drug resistant (Investigators of the Delhi Neonatal Infection Study (DeNIS) collaboration 2016). Colistin {{is regarded as a}} drug of last resort in therapeutic management of Gram negative infections (Yau et al. 2009) and colistin resistance in carbapenem resistant Enterobacteriaceae implies that we are <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> pan drug resistant strains, with very limited/no therapeutic options. Colistin resistance was known to be chromosomally mediated (Yau et al. 2009). But of late, studies have shown that plasmid encoded mcr- 1 gene harbored by E. coli SHP 47 confers colistin resistance in farm animals in China (Liu et al. 2016), subsequently other reports have also highlighted spread of plasmid mediated colistin resistance in Europe (Skov and Monnet 2016). Resistance to colistin is typically caused by modification of LPS with 4 -amino 4 -dexoy arabinose or with phosphoethanolamine both of which alters surface charge, ultimately resulting in reduced binding of colistin to outer membrane of the bacteria (Olaitan et al. 2014). Among Enterobacteriaceae, especially with Klebsiella pneumoniae clinical isolates, mutation/disruption of mgrB was reported as the most common reason for colistin resistance (Cannatelli et al. 2014).|$|R
40|$|The {{process of}} social {{representation}} designates {{a form of}} specific knowledge which, “acting simultaneously on the stimulus and the response”, links a subject to an object. In its circumstance, we are <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> a “way of communicating with the outside”, with an “adjustment {{to the world of}} phenomena and deeds”, with an “articulation of the personality to the social context”, with a “mechanism with which theories about the social environment are built”, with a “style of conduct towards the outside”, or eventually, with a “restoration, a reconstruction of the environment through the perspective of our life philosophy”. By saying “social representation,” we actually mean a “modeling of an object into and through behavioral and material linguistic relations”. With this object, the representation type continue to be “in a relation of symbolization (maintaining it place) and “interpretation” (giving it meanings), being in a frame of expression or in a form of construction of the real”. What prerequisites must there exist for a certain element of the social environment (a person, a situation, an event, a deed, a phenomenon or an idea) to trigger the emer¬gence of a social representation, of a sui generis collective science capable of building «a common vision of the world», a «system of understanding and interpretation of reality», a «strategy of community agreement between individuals or groups»...|$|R
40|$|The old American dilemma was {{the contradiction}} between the racial caste {{system and the}} American liberal conscience. The Civil Rights Revolution of the Sixties {{abolished}} the old racial caste system in its legal aspects, but failed to completely alter the relation between racial groups in America {{in a way that}} satisfied the aspirations of the Peoples of Color. In response to this a system of racial corporatism has evolved that pursues a proportional racial division of rewards and privileges in American society, and that marches under the banner of "equality of result". The new American dilemma is the contradiction between this evolving system and the tenets of orthodox liberalism, that envisions American society in terms of individuals rather than racial groups. This essay explores these developments and the tensions between racial corporatism and the ideal of a "color-blind " constitution. Racial Corporatism and the New American Dilemma In 1944 when Gunnar Myrdal and his associates called attention to the "American Dilemma " 1 they had in mind the tensions created in the American psyche between the authentic liberalism of the American spirit and the inferior caste status that was ascribed to African Americans. Although they did not use that term, they were, <b>in</b> <b>fact,</b> <b>dealing</b> <b>with</b> the contradictions caused by the pre-Civil Rights Era racial corporatism of American society. This was a set of laws, practices, rhetorics, and forms o...|$|R
40|$|AbstractA Minkowski space Md=(Rd, ‖‖) is just Rd with {{distances}} {{measured using}} a norm ‖‖. A norm ‖‖ is completely determined by its unit ball {x∈Rd∣‖x‖⩽ 1 } {{which is a}} centrally symmetric convex body of the d-dimensional Euclidean space Ed. In this note we give upper bounds for {{the maximum number of}} times the minimum distance can occur among n points in Md, d⩾ 3. <b>In</b> <b>fact,</b> we <b>deal</b> <b>with</b> a somewhat more general problem namely, we give upper bounds for the maximum number of touching pairs in a packing of n translates of a given convex body in Ed, d⩾ 3...|$|R
40|$|In {{these days}} {{we are told}} that we are living in a "post-truth" world <b>in</b> which <b>facts</b> are no longer {{significant}} or relevant.   This statement is misleading.   There is nothing wrong with the world we are living in, {{the problem is that we}} are no longer capable of understanding it.   We cannot understand it, because we are using obsolete narratives and analytical tools developed decades ago to interpret a "reality" that no longer exists.   The talk starts with an overview of "conceptual blunders" used right now to frame the analysis of sustainability.   These blunders are behind the assumption that we will be able to sustain further economic growth by implementing a more circular economy, using the rationale of bioeconomy, reaching zero-emissions, and by developing green-energy.   The widespread acceptance of these assumptions flags the existence of a serious crisis in sustainability science, that is incapable of handling complexity.   <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> sustainability issues requires the ability of integrating in a coherent representation information referring to different dimensions and different scales of analysis.   This result is outside the reach of reductionism.   In the final part, the talk illustrates the existence of new scientific approaches based on complexity theory (relational analysis) and new conceptualizations of the interaction of human societies and their environment (the metabolic pattern of social-ecological systems) that can improve the usefulness of scientific inputs to be used for governance</p...|$|R
6000|$|... [Note that, {{though its}} Sign of Quantity {{tells us how}} many [...] Members of its Subject are also Members of its Predicate, it [...] does not tell us the exact number: <b>in</b> <b>fact,</b> it only <b>deals</b> <b>with</b> [...] three numbers, which are, in {{ascending}} order, [...] "0", [...] "1 or [...] more", [...] "the total number of Members of the Subject".] ...|$|R
40|$|We {{propose to}} use the self-seeding scheme with single crystal {{monochromator}} at the European X-ray FEL to produce monochromatic, high-power radiation at 16 keV. Based on start to end simulations we show that the FEL power of the transform-limited pulses can reach about 100 GW by exploiting tapering in the tunable-gap baseline undulator. The combination of high photon energy, high peak power, and very narrow bandwidth opens a vast new range of applications, and includes the possibility to considerably increase the user capacity and fully exploit the high repetition rate of the European XFEL. <b>In</b> <b>fact,</b> <b>dealing</b> <b>with</b> monochromatic hard X-ray radiation one may use crystals as deflectors with minimum beam loss. To this end, a photon beam distribution system based {{on the use of}} crystals in the Bragg reflection geometry is proposed for future study and possible extension of the baseline facility. They can be repeated a number of times to form an almost complete (one meter scale) ring with an angle of 20 degrees between two neighboring lines. The reflectivity of crystal deflectors can be switched fast enough by flipping the crystals with piezo-electric devices similar to those for X-ray phase retarders at synchrotron radiation facilities. It is then possible to distribute monochromatic hard X-rays among 10 independent instruments, thereby enabling 10 users to work in parallel. The unmatched repetition rate of the European XFEL would be therefore fully exploited...|$|R
40|$|The {{field of}} {{investigation}} of this thesis is a corpus of texts containing proposals for {{the introduction of}} random selection schemes in politics. The thesis questions the relevance and coherence of these texts: do the publications form a theoretical ensemble that could be labelled as a “theory of aleatory democracy”? If so, do the expectations raised by its supporters remain merely utopian or do they {{stand the test of}} the political practice? The quantitative and qualitative analysis of the text corpus leads to the conclusion that we are <b>in</b> <b>fact</b> <b>dealing</b> <b>with</b> the emergence of a theory and that the analysed authors develop a common argumentative frame and common expectations: the use of random selection in politics could be a solution to overcome the crises of liberal democracies. Its use would allow a better formal and substantial representation, a qualitatively and quantitatively increased participation, and give birth to a new, more procedural and dynamic form of legitimacy. These expectations are then tested with the help of two mini-publics, that is to say, experiments in participatory democracy that use random selection to recruit their participants: a Planungszelle and a Citizens’ Jury. The qualitative empirical investigation shows that most of the expectations are fulfilled, although only in a limited geographical, social and political frame. These results raise the double question of the conditions for the realization of the theory and of the possible improvement of the mini-public mechanisms through institutional engineering...|$|R
3000|$|Recently, many results {{appeared}} {{related to}} fixed points in complete metric spaces endowed with a partial ordering ⪯. Most {{of them are}} hybrids of two fundamental principles: the Banach contraction principle and the monotone iterative technique. <b>In</b> <b>fact,</b> these results <b>deal</b> <b>with</b> a monotone (either order-preserving or order-reversing) self-mapping T satisfying, with some restrictions, a classical contractive condition and such that for some [...]...|$|R
