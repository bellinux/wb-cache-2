6|737|Public
40|$|We {{develop a}} {{theoretical}} model to compare forecast uncertainty estimated from time series models to those available from survey density forecasts. The {{sum of the}} average variance of individual densities and the disagreement is shown to approximate the predictive uncertainty from well-specified time series models when the variance of the aggregate shocks is relatively small {{compared to that of}} the idiosyncratic shocks. Due to <b>grouping</b> <b>error</b> problems and compositional heterogeneity in the panel, individual densities are used to estimate aggregate forecast uncertainty. During periods of regime change and structural break, ARCH estimates tend to diverge from survey measures...|$|E
40|$|Abstract. We {{present a}} novel method for the {{visualization}} of speak-ers which is microphone independent. To {{solve the problem}} of lacking microphone independency we present two methods to reduce the in u-ence of the recording conditions on the visualization. The rst one is a registration of maps created from identical speakers recorded under dif-ferent conditions, i. e., dierent microphones and distances in two steps: Dimension reduction followed by the linear registration of the maps. The second method is an extension of the Sammon mapping method, which performs a non-linear registration during the dimension reduction proce-dure. The proposed method surpasses the two step registration approach with a mapping error ranging from 17 % to 24 % and a <b>grouping</b> <b>error</b> which is close to zero. ...|$|E
40|$|The joint {{probability}} density function, evaluated at the observed data, {{is commonly used}} as the likelihood function to compute maximum likelihood estimates. For some models, however, there exist paths in the parameter space along which this densityapproximation likelihood goes to infinity and maximum likelihood estimation breaks down. In applications, all observed data are discrete due to the round-off or <b>grouping</b> <b>error</b> of measurements. The “correct likelihood ” based on interval censoring can eliminate the problem of an unbounded likelihood. This paper categorizes the models leading to unbounded likelihoods into three groups and illustrates the density breakdown with specific examples. We also study {{the effect of the}} round-off error on estimation, and give a sufficient condition for the joint density to provide the same maximum likelihood estimate as the correct likelihood, as the round-off error goes to zero...|$|E
40|$|Study of {{the design}} and {{modeling}} of a highly reliable bubble-memory system that has the capabilities of: (1) correcting a single 16 -adjacent bit-group error resulting from failures in a single basic storage module (BSM), and (2) detecting with a probability greater than 0. 99 any double errors resulting from failures in BSM's. The {{results of the study}} justify the design philosophy adopted of employing memory data encoding and a translator to correct single <b>group</b> <b>errors</b> and detect double <b>group</b> <b>errors</b> to enhance the overall system reliability...|$|R
30|$|A final metric {{is created}} by <b>grouping</b> <b>errors</b> together. This {{approach}} {{is based on an}} intuitive examination of error detection. If a performer plays a note that is both the wrong pitch and rhythm, the listener may hear these as a single error. As well, a flustered performer may perform several errors in a row before regaining his and her composure, and a teacher is more likely to identify the section of music as being problematic than trying to analyze each individual error made during the sequence of <b>errors.</b> The evaluator <b>groups</b> <b>errors</b> in time. That is, all errors within a time threshold are combined as a single error. Using this approach, the evaluator counts the number of <b>error</b> <b>groups,</b> and normalizes this to an error rate per note. The <b>error</b> <b>group</b> does not replace the other analyses, but instead uses them to create a new feature for the classifier that may be meaningful during training. By using a clustering algorithm to find <b>error</b> <b>groups,</b> we are combining these other errors in a non-linear way, making it more likely that <b>error</b> <b>groups</b> could be a meaningful metric.|$|R
40|$|Background: Blood {{grouping}} is {{the single}} most important test performed by each and every transfusion service. A blood <b>group</b> <b>error</b> has a potential for causing severe life-threatening complications. A number of process strategies have been adopted at various institutions to prevent the occurrence of errors at the time of phlebotomy, pretransfusion testing, and blood administration. A delta check is one such quality control tool that involves the comparison of laboratory test results with results obtained on previous samples from the same patient. Materials and Methods: We retrieved the records of all transfusion-related incidents reported in our institute, between January 2008 and December 2014. Errors identified as “Failed Delta checks” and their root cause analyses (RCA) were reviewed. Results: A total of 17, 034 errors related to blood transfusion were reported. Of these, 38 were blood <b>grouping</b> <b>errors.</b> Seventeen blood <b>group</b> <b>errors</b> were identified due to failed delta checks, where the results of two individually drawn grouping samples yielded different blood group results. The RCA revealed that all of these errors occurred in the preanalytical phase of testing. Mislabeling resulting in wrong blood in tube was the most commonly identified cause, accounting for 11 of these errors, while problems with correct patient identification accounted for 5 failed delta checks. Conclusion: Delta checks proved to be an effective tool for detecting blood <b>group</b> <b>errors</b> and prevention of accidental mismatched blood transfusions. Preanalytical errors in patient identification or sample labeling were the most frequent...|$|R
40|$|We {{develop a}} {{theoretical}} framework to compare forecast uncertainty estimated from time series models to those available from survey density forecasts. The sum of the average variance of individual densities and the disagreement, {{which is the same}} as the variance of the aggregate density, is shown to approximate the predictive uncertainty from well specified time series models when the variance of the aggregate shocks is relatively small compared to that of the idiosyncratic shocks. We argue that due to <b>grouping</b> <b>error</b> problems, compositional effects of the panel, and other complications, the uncertainty measure has to be estimated from individual densities. Despite numerous reservations on the credibility of time series based measures of forecast uncertainty, we found that during normal times the uncertainty estimates based on ARCH models simulate the subjective survey measure remarkably well. However, during times of regime change and structural break, the two estimates do not overlap. We suggest ways to improve the time series measures during periods when forecast errors are apt to be large. The disagreement series is a good indicator of such periods. ...|$|E
40|$|Still the {{presence}} of errors {{in the application of}} qawa 2 ̆ 72 ̆ 7 id Arabic Arabic textbook writing in the neighborhood Dikdasmen PWM DIY shows that the preparation of textbooks are not of good quality. This research aims to locate faults textbook writing so that it can immediately be repaired. As of this writing done on the application of qawa 2 ̆ 7 id Arabic in textbook writing. The procedure used: data collection, classification or <b>grouping</b> <b>error,</b> frequency error types, the identification of the scope of the type of error, remedial and identification errors. Artifacts errors found as many as 524 items an error. The error types, divided into two aspects of the errors, namely (1) Error morphology (akhta 2 ̆ 7 al-sarafiyyah) include: isim gairu munsarif, jama 2 ̆ 7 and fi 2 ̆ 7 il Muta 2 ̆ 7 adiy, alif Layyinah, and typographical errors. (2) a structural or syntactic errors (akhta 2 ̆ 7 tarkibiyyah) include: na 2 ̆ 7 at man 2 ̆ 7 ut, nakirah-ma 2 ̆ 7 rifah, errors harokat, miscellaneous structures, tarkib Idafiy (mudaf-mudaf ilaih), tarkib isnadiy (mubtada 2 ̆ 7 khabar), maf 2 ̆ 7 ul mutlaq, mabni fathah, removal of language elements, ambiguous, one selected, couples idiom said, adding, writing qat 2 ̆ 7 a glottal stop, and hamzah wasal. Based on the analysis of the textbook errors, some errors are still common, namely the writing section glottal qat 2 ̆ 7 a, isim gairu munsarif, alif Layyinah, writing errors, nakirah-ma 2 ̆ 7 rifah, mabni fathah, and hamzah wasal. Some of the things that cause the occurrence of such errors that is the attitude of carelessness and a disregard for the rules by the author, the inconsistency of the author of the application of a rule, application of the rules are incomplete and excessive generalization...|$|E
40|$|This thesis {{contributes}} {{to the field of}} machine vision and the theory of the sampling of particulate material on conveyor belts. The objective is to address sources of error relevant to surface-analysis techniques when estimating the sieve-size distribution of particulate material using machine vision. The relevant sources of error are segregation and <b>grouping</b> <b>error,</b> capturing error, profile error, overlapping-particle error and weight-transformation error. Segregation and <b>grouping</b> <b>error</b> describes the tendency of a pile to separate into groups of similarly sized particles, which may bias the results of surface-analysis techniques. Capturing error describes the varying probability, based on size, that a particle will appear {{on the surface of the}} pile, which may also bias the results of surface-analysis techniques. Profile error is related to the fact that only one side of an entirely visible particle can be seen, which may bias the estimation of particle size. Overlapping-particle error occurs when many particles are only partially visible, which may bias the estimation of particle size because large particles may be treated as smaller particles. Weight-transformation error arises because the weight of particles in a specific sieve-size class might significantly vary, resulting in incorrect estimates of particle weights. The focus of the thesis is mainly on solutions for minimizing profile error, overlapping-particle error and weight-transformation error. In the aggregates and mining industries, suppliers of particulate materials, such as crushed rock and pelletised iron ore, produce materials for which the particle size is a key differentiating factor in the quality of the material. Manual sampling and sieving techniques are the industry-standard methods for estimating the size distribution of these particles. However, as manual sampling is time consuming, there are long response times before an estimate of the sieve-size distributions is available. Machine-vision techniques promise a non-invasive, frequent and consistent solution for determining the size distribution of particles. Machine-vision techniques capture images of the surfaces of piles, which are analyzed by identifying each particle on the surface of the pile and estimating its size. Sampling particulate material being transported on conveyor belts using machine vision has been an area of active research for over 25 years. However, there are still a number of sources of error in this type of sampling that are not fully understood. To achieve a high accuracy and robustness in the analysis of captured surfaces, detailed experiments were performed in the course of this thesis work, towards the development and validation of techniques for minimizing overlapping-particle error, profile error and weight-transformation error. To minimise overlapping-particle error and profile error, classification algorithms based on logistic regression were proposed. Logistic regression is a statistical classification method that is used for visibility classification to minimize overlapping-particle error and in particle-size classification to minimize profile error. Commonly used size- and shape-measurement methods were evaluated using feature-selection techniques, to find sets of statistically significant features that should be used for the abovementioned classification tasks. Validation using data not used for training showed that these errors can be overcome. The existence of an effect that causes weight-transformation error was identified using statistical analysis of variance (ANOVA). Methods to minimize weight-transformation error are presented herein, and one implementation showed a good correlation between the results using the machine-vision system and manual sieving results. The results presented in this thesis show that by addressing the relevant sources of error, machine vision techniques allow for robust and accurate analysis of particulate material. An industrial prototype was developed that estimates the sieve-size distribution of iron-ore pellets in a pellet plant and crushed limestone in a quarry during ship loading. The industrial prototype also enables product identification of crushed limestone to prevent the loading of incorrectly sized products. Godkänd; 2010; 20101115 (tobiasa); DISPUTATION Ämnesområde: Industriell elektronik/Industrial Electronics Opponent: Professor Anthony Maeder, University of Western Sydney, Australien Ordförande: Universitetslektor Matthew Thurley, Luleå tekniska universitet Tid: Torsdag den 16 december 2010, kl 12. 30 Plats: D 2214 - 15, Luleå tekniska universite...|$|E
40|$|A growing {{literature}} on inference in difference-in-differences (DiD) designs with <b>grouped</b> <b>errors</b> has been pessimistic about obtaining hypothesis {{tests of the}} correct size, particularly with few groups. We provide Monte Carlo evidence for three points: (i) {{it is possible to}} obtain tests of the correct size even with few groups, and in many settings very straightforward methods will achieve this; (ii) the main problem in DiD designs with <b>grouped</b> <b>errors</b> is instead low power to detect real effects; and (iii) feasible GLS estimation combined with robust inference can increase power considerably whilst maintaining correct test size 9 ̆ 6 again, even with few groups...|$|R
40|$|When input {{patterns}} are recognized, the recognized results are classfied into three groups. These are rejected <b>group,</b> <b>error</b> recognition <b>group</b> and correct recognition group. Chow {{has been studied}} the relation of these groups. However, in this paper, the author investigates about the case of normal distribution with different mean and variance which are not considered...|$|R
40|$|The {{application}} of discrete solution methods to the second-order wave equation can yield a dispersive {{representation of the}} non-dispersive wave propagation problem resulting in a phase speed that depends not only upon the wavelength of the signal being propagated but also upon the direction of propagation. In this work, the dependence of the dispersive errors on the wave propagation direction, mesh aspect ratio, and wave number is investigated {{with the goal of}} understanding and hopefully reducing the phase and <b>group</b> <b>errors</b> associated with the two-dimensional bilinear finite element. An analysis of the dispersive effects associated with the consistent, row-sum lumped and higher-order mass matrices has led to a reducedcoupling "penta-diagonal" mass matrix that yields improved phase and <b>group</b> <b>errors</b> with respect to wavelength and propagation direction. The influence of row-sum lumping the finite element mass matrix is demonstrated to always introduce lagging phase and group er [...] ...|$|R
40|$|Researchers {{estimating}} {{models with}} grouped data typically weight each observation by the square root of group size {{assuming that the}} model error term in the individual data is independently and identically distributed so that the <b>error</b> in the <b>grouped</b> data is heteroskedastic with variance proportional to group size. I argue that individual error terms {{are likely to be}} correlated due to <b>group</b> specific <b>error</b> component so weighting by the square root of group size is inappropriate. Tests for the presence of <b>group</b> <b>error</b> components and methods for obtaining efficient estimates of the parameters and consistent estimates of their standard errors are presented. Copyright 1990 by MIT Press. ...|$|R
40|$|This {{study is}} about the {{grammatical}} errors made by English 2 students of the Communication Department in their written work. The data of this research {{were taken from the}} first writing assignment of fifteen students of English 2 class. The writer asked the help from one of the students to borrow her friends? assignment and analyzed it. Then, on the basis of Richards? theory (1974), she classified the errors into relevant type namely errors in the production of verb <b>groups,</b> <b>errors</b> in the distribution of verb <b>groups,</b> miscellaneous <b>errors,</b> errors in the use of prepositions, errors in the use of articles, and errors in the use of questions. Concerning the analysis, the writer analyzed the errors, categorized them based on the theory of Richard (1974) and she also provided the correct form of the grammatical error. At the end of analysis, she found out the most frequent error made by the subjects of this research. The result showed that errors in the distribution of verb groups gain the highest percentage. Then, the next was errors in the use of prepositions, errors in the production of verb <b>groups,</b> <b>errors</b> in the use of articles and miscellaneous errors. As the conclusion, the writer thinks that the students are still poor in grammar since there were many basic errors occurred in their composition...|$|R
30|$|Independent samples t-test {{revealed}} no significant difference (p[*]<[*] 0.01) between BT and W groups. Mann–Whitney test was used {{due to the fact}} that values of standard deviations (SD) were larger than the mean in each of the <b>groups.</b> <b>Error</b> rates in trail making test A were significantly lower in BT (M[*]=[*] 0.31; SD[*]=[*] 1.01) than water (M[*]=[*] 1.31; SD[*]=[*] 1.66). Moreover, participants with BT made less error (p[*]<[*] 0.01).|$|R
40|$|AbstractThe program {{complexity}} {{to enumerate}} a finite set of words is found. The complexity is either an exponential or a linear {{function of the}} word length {{depending on whether the}} redundancy is either less or more than 100 %. A corollary: the Varshamov-Gilbert bound of the <b>group</b> <b>error</b> correcting code cardinality is tight for almost any channel with additive noise. The proofs are based on the concept of the collision index...|$|R
40|$|Generalized Relativistic Effective Core Potential (GRECP) {{calculation}} of spectroscopic constants for the HgH molecule and its ions {{is carried out}} {{with the help of}} Fock-space Relativistic Coupled Cluster method with Single and Double cluster amplitudes (RCC-SD). The calculated spectroscopic constants are compared with experimental data and results of calculations of other <b>groups.</b> <b>Errors</b> of the performed GRECP/RCC-SD calculations are analyzed. The Basis Set Superposition Errors (BSSE) are estimated and discussed...|$|R
40|$|Fig 4. Performance gains per task. A: Stacked {{difference}} scores (follow-up {{compared to}} session 1) {{for the five}} trained tasks for the sham group and the average for the active stimulation groups. B: Stacked difference scores (follow-up compared to session 1) for the three transfer tasks for the sham group and the average for the active stimulation <b>groups.</b> <b>Error</b> bars represent standard error of the mean. doi: 10. 1371 /journal. pone. 0129751. g 00...|$|R
5000|$|... and [...] blamed {{each other}} for the pink jersey <b>groups</b> <b>errors</b> on the day. Vinokourov felt that since his team was {{depleted}} and that Nibali and Ivan Basso stood a better chance to win the Giro than he did anyway that [...] should have taken up the chase. The Italian team, for their part, felt that since [...] had held the pink jersey at the time that it was on them to organize the chase.|$|R
40|$|This article {{presents}} {{a new approach}} to discriminative training that uses equal <b>error</b> <b>groups</b> of word strings as the unit of weighted error modeling. The proposed approach, Minimum <b>Group</b> <b>Error</b> (MGE), is based on a novel error-indexed Forward-Backward algorithm {{that can be used to}} generate group scores efficiently over standard recognition lattices. The approach offers many possibilities for group occupancy scaling, enabling, for instance, the boosting of <b>error</b> <b>groups</b> with low occupancies. Preliminary experiments examined the new approach using both uniformly and non-uniformly scaled group scores. Results for the new approach evaluated on the Corpus of Spontaneous Japanese (CSJ) lecture speech transcription task were compared with results for standard Minimum Classification Error (MCE), Minimum Phone Error (MPE) and Maximum Mutual Information (MMI), in tandem with I-smoothing. It was found that non-uniform scaling of group scores outperformed MPE when no I-smoothing is used. Index Terms: speech recognition, discriminative training 1...|$|R
40|$|This study aims {{to analyze}} the {{grammatical}} errors and to provide description of errors on speaking activities using simple present and present progressive tenses made by the second year students of English Education Department, Palangka Raya University. The subject {{for this study was}} 30 students. This research applied qualitative research to describe the types, source and causes of students’ errors taken from oral essay test which consisted of questions using the tenses of simple present and present progressive. The errors were indentified and classified according to Linguistic Category Taxonomy and Richard’s classification, well as the possible sources and causes of errors. The findings showed that the errors made by students were in 6 aspects; errors in production of verb <b>groups,</b> <b>errors</b> in the distribution of verb <b>groups,</b> <b>errors</b> in the use of article, errors in the use of preposition, errors in the use of questions and miscellaneous errors. In regard to resource and causes, it was found that intra-lingual interference was the major source of errors (82. 55 %) where overgeneralization took place as the major cause of the errors with total percentage of 44. 71 %. Keywords: grammatical errors, speaking skill, speaking activitie...|$|R
40|$|The first large {{class of}} quantum error-correcting codes that was {{constructed}} is {{the class of}} stabilizer codes. The construction method used for these codes is based on finding abelian subgroups of general Pauli groups. Stabilizer codes are a special case of a more general class of quantum error-correcting codes [...] -Clifford codes. The construction technique for Clifford codes generalizes the stabilizer code construction by introducing abstract <b>error</b> <b>groups.</b> Abstract <b>error</b> <b>groups</b> generalize the Pauli groups so that codes can be constructed for quantum systems with any dimension. Clifford codes are thus constructed by finding normal subgroups of abstract <b>error</b> <b>groups.</b> In this dissertation we show that Clifford codes are a special case of a larger class of codes, which we refer to as induced codes. In addition to this new construction method, we also present some new results pertaining to a subclass of stabilizer codes called degenerate stabilizer codes. ...|$|R
40|$|A {{random sample}} of {{continuous}} measurements can be partitioned into g groups or clusters by minimizing the within group dispersion {{as measured by the}} 1 -norm. The central limit theory associated with such partitions which are universally optimal or locally optimal is derived. A procedure is presented for determining the number of groups represented by the data based on a plot of a sequence of asymptotic nonparametric confidence intervals for the fractional reduction of within <b>group</b> <b>error</b> due to (g + 1) -clustering over g-clustering for g = 1, 2, [...] clustering quantization stratification...|$|R
50|$|Follow-up {{research}} by Leila Worth and Scott T. Allison attempted {{to identify the}} limits of the effect. These {{studies have shown that the}} error becomes stronger in perceptions of groups that are viewed as (a) more dissimilar to one's own group, (b) more monolithic, and (c) adversarial to one's own <b>group.</b> The <b>error</b> tends to disappear in perceptions of one's own group. Group members are more likely to attribute the decisions of their own group to structural constraints placed on the group, such as its decision rules, whereas members tend to attribute the decisions of another group to its members' attitudes. This tendency to draw different conclusions between in-groups and out-groups reflects a connection between <b>group</b> attribution <b>error</b> and ultimate attribution error. Additional research on the <b>group</b> attribution <b>error,</b> conducted by Diane M. Mackie and Scott T. Allison, has shown its consequences for making erroneous judgments about changes in group attitudes over time.|$|R
3000|$|... 2). All of {{the samples}} were {{classified}} into the two <b>groups</b> without <b>error.</b> The function used to discriminate {{between the two groups}} is represented as:Z= 1693 d 1 — 902 d [...]...|$|R
30|$|The main {{objective}} {{of this study is}} to prioritize the five <b>error</b> <b>groups</b> based on all the four important criteria using the AHP. However, an initial ranking of all the five <b>error</b> <b>groups</b> under each of the four individual criterions was carried out, but it only underscored the need for multi-criteria approach since different ranking errors were obtained under different criteria. Thus, it is reinforced that the <b>error</b> <b>groups</b> are to be prioritized by considering the effect of all the influencing factors simultaneously for fruitful results. It is for this reason that a MCDM tool like the AHP is adopted in this study.|$|R
2500|$|A {{cover-up}} {{may be used}} to deny, defend or obfuscate one's own (or one's allies or <b>group's)</b> <b>errors,</b> one's embarrassing {{actions or}} lifestyle, and/or one's lie(s) that they made previously. One may deny a lie made on a previous occasion, or one may alternatively claim that a previous lie was not as egregious as it actually was. For example, to claim that a premeditated lie was really [...] "only" [...] an emergency lie, or to claim that a self-serving lie was really [...] "only" [...] a white lie or noble lie. Not to be confused with confirmation bias in which the deceiver is deceiving themselves.|$|R
40|$|Translation {{has been}} used through many aspects of life. But to {{translate}} from source to target language correctly without losing the context and meaning of sentences is not easy; Errors happen because people usually translate the sentence words by words. Therefore, in this study the writer tries to analyze the translation errors done by students of Faculty of Letters Soegijapranata Catholic University {{to find out the}} errors mostly occur in their midterm test. Tre writer tries to figure out the translation errors from Indonesian to English especially in the field of grammatical errors. To address the research question, the writer used students’ midterm translation test. He collected the materials for this study from the teachers. He collected sixteen sample for analysis. For the data analysis, the writer categorizes the grammatical errors into three categories: errors in the production of verb <b>groups,</b> <b>errors</b> in the distribution of verb <b>groups</b> and miscellaneous <b>errors.</b> The data analysis showed that the errors in the production of verb groups appear the more often than the other categories...|$|R
30|$|Statistical {{significance}} {{of the differences between}} the groups was determined using Student’s t test. The statistical power was calculated using the Biomath online software ([URL] We analyzed 10 samples for each <b>group</b> (alpha <b>error</b> 0.05), which resulted in a statistical power of 92 %.|$|R
5000|$|All provider-created {{errors are}} passed to a {{collection}} of Error objects, while the Errors collection itself is contained in a Connection object. When an ADO operation creates an error, the collection is cleared and a new <b>group</b> of <b>Error</b> objects is created in the collection.|$|R
40|$|Four {{elementary}} school children were tested on 120 words containing the short e (e. g., ten, pen) and short a (e. g., tan, pan) sounds. Words were tested in the hear-write (H/W) and see-say (S/S) channels. No programmed consequences were scheduled during baseline (BL) tests 1 - 3. After BL, an error analysis categorized words based on channel error and topography of error. Praise was delivered during tests 4 - 6 for correct responses. Children's responses were variable within channel and across channels {{for a majority of}} words. By the end of the praise phase, there was a {{decrease in the number of}} words with errors, for all children in their <b>error</b> word <b>group.</b> <b>Error</b> topographies began to stabilize for some words during praise...|$|R
25|$|Errors: All {{provider}} created {{errors are}} passed to {{a collection of}} Error objects, while the Errors collection itself is contained in a Connection object. When an ADO operation creates an error, the collection is cleared and a new <b>group</b> of <b>Error</b> objects are created in the collection.|$|R
40|$|This note {{deals with}} {{estimating}} cluster-robust standard errors on {{one and two}} dimensions using R (see R Development Core Team [2007]). Cluster-robust stan-dard errors are an issue when the errors are correlated within groups of observa-tions. For discussion of robust inference under within <b>groups</b> correlated <b>errors,</b> se...|$|R
5000|$|Errors: All {{provider}} created {{errors are}} passed to {{a collection of}} Error objects, while the Errors collection itself is contained in a Connection object. When an ADO operation creates an error, the collection is cleared and a new <b>group</b> of <b>Error</b> objects are created in the collection.|$|R
40|$|Knill {{introduced}} a generalization of stabilizer codes, in this note called Clifford codes. It remained unclear {{whether or not}} Clifford codes can be superior to stabilizer codes. We show that Clifford codes are stabilizer codes provided that the abstract <b>error</b> <b>group</b> is given by an extraspecial p-group. Suppose that the abstract <b>error</b> <b>group</b> has an abelian index group, then we show that a Clifford code {{can be derived from}} an abelian normal subgroup...|$|R
50|$|To {{demonstrate}} the first form of <b>group</b> attribution <b>error,</b> research participants are typically given case studies about {{individuals who are}} members of defined groups (such as members of a particular occupation, nationality, or ethnicity), and then take surveys to determine their views of the groups as a whole. Often the participants may be broken up into separate test groups, some of which are given statistics about the group that directly contradict what they were presented in the case study. Others may even be told directly that the individual in the case study was atypical for the group as a whole. Researchers use the surveys to determine to what extent the participants allowed their views of the individual in the case study to influence their views of the group as a whole and also take note of how effective the statistics were in deterring this <b>group</b> attribution <b>error.</b> Ruth Hamill, Richard E. Nisbett, and Timothy DeCamp Wilson were the first to study this form of <b>group</b> attribution <b>error</b> in detail in their 1980 paper Insensitivity to Sample Bias: Generalizing From Atypical Cases. In their study, the researchers provided participants with a case study about an individual welfare recipient. Half of the participants were given statistics showing that the individual was typical for a welfare recipient and had been on the program for the typical amount of time, while the other half of participants were given statistics showing that the welfare recipient had been on the program much longer than normal. The results of the study revealed that participants did indeed draw extremely negative opinions of all welfare recipients {{as a result of the}} case study. It was also found that the differences in statistics provided to the two groups had trivial to no effect on the level of <b>group</b> attribution <b>error.</b>|$|R
