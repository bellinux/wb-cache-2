10000|27|Public
5|$|American mathematicians Stan Wagon and Stanley Rabinowitz {{produced}} a simple spigot algorithm in 1995. Its speed {{is comparable to}} arctan algorithms, but not as fast as <b>iterative</b> algorithms.|$|E
5|$|Matrix {{calculations}} can be often {{performed with}} different techniques. Many {{problems can be}} solved by both direct algorithms or <b>iterative</b> approaches. For example, the eigenvectors of a square matrix {{can be obtained by}} finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.|$|E
5|$|The {{first such}} method was {{developed}} in 2005 by Löytynoja and Goldman. The same authors released a software package called PRANK in 2008. PRANK improves alignments when insertions are present. Nevertheless, it runs slowly compared to progressive and/or <b>iterative</b> methods which {{have been developed for}} several years.|$|E
5|$|The <b>iterative</b> {{algorithms}} were independently {{published in}} 1975–1976 by American physicist Eugene Salamin and Australian scientist Richard Brent. These avoid reliance on infinite series. An <b>iterative</b> algorithm repeats a specific calculation, each iteration using the outputs from prior steps as its inputs, and produces a result in each step that converges {{to the desired}} value. The approach was actually invented over 160 years earlier by Carl Friedrich Gauss, {{in what is now}} termed the arithmetic–geometric mean method (AGM method) or Gauss–Legendre algorithm. As modified by Salamin and Brent, it is also referred to as the Brent–Salamin algorithm.|$|E
5|$|This <b>iterative</b> {{procedure}} {{keeps track}} of the search boundaries with the two variables. Some implementations may check whether the middle element {{is equal to the}} target {{at the end of the}} procedure. This results in a faster comparison loop, but requires one more iteration on average.|$|E
25|$|NLLSQ {{is usually}} an <b>iterative</b> process. The <b>iterative</b> process {{has to be}} {{terminated}} when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods, although problems {{with large numbers of}} parameters are typically solved with <b>iterative</b> methods, such as the Gauss–Seidel method.|$|E
25|$|These {{complexities}} {{are better}} handled {{with a more}} exploratory or <b>iterative</b> and incremental approach. Several models of <b>iterative</b> and incremental project management have evolved, including agile project management, dynamic systems development method, extreme project management, and Innovation Engineering®.|$|E
25|$|Both <b>iterative</b> {{and direct}} methods exist for sparse matrix solving.|$|E
25|$|In {{contrast}} to direct methods, <b>iterative</b> methods {{are not expected}} to terminate in a finite number of steps. Starting from an initial guess, <b>iterative</b> methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, <b>iterative</b> methods are generally needed for large problems.|$|E
25|$|<b>Iterative</b> {{algorithms}} can {{be implemented}} by means of recursive predicates.|$|E
25|$|Quasi-Newton methods: <b>Iterative</b> {{methods for}} medium-large {{problems}} (e.g. N<1000).|$|E
25|$|Logic/{{placement}} refinement: <b>Iterative</b> {{logical and}} placement transformations to close performance and power constraints.|$|E
25|$|Another example benefitting from {{extended}} precision arithmetic is <b>iterative</b> refinement schemes in numerical linear algebra.|$|E
25|$|<b>Iterative</b> {{methods are}} more common than direct methods in {{numerical}} analysis. Some methods are direct in principle but are usually used {{as though they were}} not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted {{in the same manner as}} for an <b>iterative</b> method.|$|E
25|$|The weights, wi and {{quantities}} y may be vectors. Values of {{the equilibrium}} constants are refined in an <b>iterative</b> procedure.|$|E
25|$|Much {{effort has}} been put in the {{development}} of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. <b>Iterative</b> methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General <b>iterative</b> methods can be developed using a matrix splitting.|$|E
25|$|Fermat and Lagrange found calculus-based {{formulae}} {{for identifying}} optima, while Newton and Gauss proposed <b>iterative</b> methods for moving towards an optimum.|$|E
25|$|Brockmann, R.J., & Sinatra, S. (1995). How the <b>iterative</b> process {{helped the}} Allies win the Persian Gulf War. STC Intercom, 42 (9), 1, 44.|$|E
25|$|Subgradient methods - An <b>iterative</b> {{method for}} large locally Lipschitz {{functions}} using generalized gradients. Following Boris T. Polyak, subgradient–projection methods {{are similar to}} conjugate–gradient methods.|$|E
25|$|Since the minimax {{algorithm}} and its variants are inherently depth-first, a strategy such as <b>iterative</b> deepening is usually {{used in conjunction}} with alpha–beta so that a reasonably good move can be returned even if the algorithm is interrupted before it has finished execution. Another advantage of using <b>iterative</b> deepening is that searches at shallower depths give move-ordering hints, as well as shallow alpha and beta estimates, that both can help produce cutoffs for higher depth searches much earlier than would otherwise be possible.|$|E
25|$|A {{recursive}} algorithm {{is one that}} invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. <b>Iterative</b> algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) <b>iterative</b> version, and vice versa.|$|E
25|$|This {{adaptive}} {{encoding procedure}} {{is not confined}} to algorithms that sample from a multivariate normal distribution (like evolution strategies), but can in principle be applied to any <b>iterative</b> search method.|$|E
25|$|Alternatively, the {{estimates}} {{provided by the}} method of moments can instead be used as initial values for an <b>iterative</b> solution of the maximum likelihood coupled equations {{in terms of the}} digamma functions.|$|E
25|$|Madhava {{developed}} some components of calculus such as differentiation, term-by-term integration, <b>iterative</b> methods for solutions of non-linear equations, {{and the theory}} that the area under a curve is its integral.|$|E
25|$|In a 2008 paper, Puslednik et al. {{identified}} considerable {{convergence of}} shell morphology in a subset species of gliding Pectinidae, which suggests <b>iterative</b> morphological evolution {{may be more}} prevalent in the family than previously believed.|$|E
25|$|This {{equation}} {{cannot be}} solved for r in closed form. If a numerical solution is desired, an <b>iterative</b> technique such as Newton's method can be used. Alternatively, the expectation–maximization algorithm can be used.|$|E
25|$|Strategic {{management}} {{is often described}} as involving two major processes: formulation and implementation of strategy. While described sequentially below, in practice the two processes are <b>iterative</b> and each provides input for the other.|$|E
25|$|Kantorovich {{showed that}} {{functional}} analysis {{could be used}} in the analysis of <b>iterative</b> methods, obtaining the Kantorovich inequalities on the convergence rate of the gradient method and of Newton's method (see the Kantorovich theorem).|$|E
25|$|Ellipsoid method: An <b>iterative</b> {{method for}} small {{problems}} with quasiconvex objective functions and of great theoretical interest, particularly {{in establishing the}} polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.|$|E
25|$|The {{calculations}} {{assume that}} the dive profile, including decompression, is known, but the process may be <b>iterative,</b> involving changes to the dive profile {{as a consequence of}} the gas requirement calculation, or changes to the gas mixtures chosen.|$|E
25|$|The {{matched filter}} may be {{generalized}} to an analogous procedure based on a Student-t distribution by also considering uncertainty (e.g. estimation uncertainty) in the noise spectrum. On the technical side, this entails repeated or <b>iterative</b> matched-filtering.|$|E
25|$|Those who {{commissioned}} {{construction of}} passive houses and zero-energy homes (over {{the last three}} decades) were essential to <b>iterative,</b> incremental, cutting-edge, technology innovations. Much has been learned from many significant successes, and a few expensive failures.|$|E
25|$|Pathak, J., Basu, S., and Honavar, V. (2006). Modeling Web Services by <b>Iterative</b> Reformulation of Functional and Non-Functional Requirements. Proceedings of the International Conference on Service Oriented Computing. Lecture Notes in Computer Science, Berlin: Springer, Vol. 4294, pp.314–326.|$|E
25|$|Fast {{b-spline}} interpolation on {{a uniform}} sample domain {{can be done}} by <b>iterative</b> mean-filtering. Alternatively, a rectangle function equals Sinc in Fourier domain. Therefore, cubic spline interpolation equals multiplying the signal in Fourier domain with Sinc^4.|$|E
25|$|Frequency-resolved optical gating (FROG): a {{nonlinear}} {{technique that}} yields {{the intensity and}} phase of a pulse. It's just a spectrally resolved autocorrelation. The algorithm that extracts the intensity and phase from a FROG trace is <b>iterative.</b>|$|E
25|$|Each new <b>iterative</b> of Newton's method will be {{denoted by}} x1. We will check during the {{computation}} whether the denominator (yprime) becomes too small (smaller than epsilon), {{which would be}} the case if , since otherwise a large amount of error could be introduced.|$|E
