33|181|Public
50|$|The {{performance}} of the naïve minimax algorithm may be improved dramatically, without affecting the result, {{by the use of}} alpha-beta pruning.Other <b>heuristic</b> <b>pruning</b> methods can also be used, but not all of them are guaranteed to give the same result as the un-pruned search.|$|E
40|$|In {{this paper}} we {{develop a new}} test to help {{identify}} whether a closed, orientable, irreducible 3 –manifold is non-Haken. The test builds on work by Jaco and Oertel, and also incorporates <b>heuristic</b> <b>pruning</b> techniques to test whether a normal surface is compressible. As an application, we settle Thurston’s old {{question of whether the}} Weber-Seifert dodecahedral space is non-Haken. AMS Classification 57 N 10 Keywords Haken manifold, Weber-Seifert dodecahedral space, normal surface, incompressible surface...|$|E
40|$|In {{this paper}} we settle Thurston's old {{question}} of whether the Weber-Seifert dodecahedral space is non-Haken, a problem that has been a benchmark for progress in computational 3 -manifold topology over recent decades. We resolve this question by combining recent significant advances in normal surface enumeration, new <b>heuristic</b> <b>pruning</b> techniques, and a new theoretical test that extends the seminal work of Jaco and Oertel. Comment: 22 pages, 10 figures, 3 tables; v 2 : expanded introduction; v 3 : minor revisions (accepted for Trans. Amer. Math. Soc. ...|$|E
5000|$|IDA* is a {{depth-first search}} that looks for {{increasingly}} longer solutions {{in a series}} of iterations, using a lower-bound <b>heuristic</b> to <b>prune</b> branches once a lower bound on their length exceeds the current iterations bound.|$|R
40|$|Abstract – As the Internet {{traffic volume}} increases, the IP over WDM network is {{expected}} to be the dominant architecture in future Internet backbone networks. For IP routers to transfer their traffic, the virtual topology should be constructed from lightpaths supplied by WDM networks. To reflect the demand-supply relation of IP and WDM layers, we present the total combined cost as the network design goal and propose a new virtual topology construction algorithm to minimize it. Given a fully connected virtual topology, the proposed packing <b>heuristic</b> <b>prunes</b> redundant lightpaths systematically and produces a new virtual topology. Simulation results show that the proposed algorithm outperforms the previous ones in every considered case...|$|R
40|$|Abstract: Systematic {{state space}} {{traversal}} {{is a very}} popular approach for detecting errors in mul-tithreaded programs. Nevertheless, it is very expensive because any non-trivial program exhibits {{a huge number of}} possible interleavings, and therefore some combination of guided search and bounded search is often used to achieve good performance. We present two heuristics that are based on a hybrid static-dynamic analysis that can identify possible accesses to shared objects. One heuristic changes the order in which transitions are explored, and the second <b>heuristic</b> <b>prunes</b> selected transitions. Results of experiments on several Java programs, which we performed using our prototype implementation in Java Pathfinder, show that the hybrid analysis together with heuristics improves the performance of error detection quite significantly...|$|R
40|$|In this paper, {{we propose}} a genetic {{algorithm}} for solving the shortest vector problem (SVP) based on sparse integer representations of short vectors in lattices as chromesomes, which, we prove, can guarantee finding the shortest lattice vector under a Markov chain analysis. Moreover, we also suggest some improvements by introducing heuristic techniques: local search and <b>heuristic</b> <b>pruning.</b> The experimental {{results show that}} the genetic algorithm runs rather good on the SVP challenge benchmarks[32], and performs much faster than other practical algorithms: the Kannan-Helfrich enumeration and enumeration with conservative pruning...|$|E
40|$|Abstract—Modern {{database}} management systems (DBMSs) {{are used to}} answer complex queries on large data sets. The available physical design features along with {{the complexity of the}} modern workloads have turned the database physical design into a very complicated task. Manual design is no longer an option and the need for automating physical design tools has become more demanding than ever. DBMSs provide automated physical design in order to maximize performance and reduce the total cost of ownership. In this paper, first we study the steps that a typical automated physical designer [1] follows to propose a physical database design. We identify performance bottlenecks in the procedure namely the high number of expensive calls to the optimizer during the evaluation of different configurations and the extensive use of <b>heuristic</b> <b>pruning</b> to reduce the search space of alternative configurations. Then, we present C-PQO [6] an approach that generates a compact representation of the optimization space with a single optimization call per query. C-PQO eliminates the overhead of calling the optimizer again and again during the evaluation of different configurations and improves performance by 30 x to over 450 x. We also present CoPhy [13]. CoPhy introduces a combinatorial optimization formulation for the index selection problem. Thanks to the convex property of the proposed formulation, CoPhy solves the problem without <b>heuristic</b> <b>pruning</b> of the search space and can predict the quality of the final solution...|$|E
40|$|In this work, {{we focus}} on the problem of image {{instance}} retrieval with deep descriptors extracted from pruned Convolutional Neural Networks (CNN). The objective is to heavily prune convolutional edges while maintaining retrieval performance. To this end, we introduce both data-independent and data-dependent heuristics to prune convolutional edges, and evaluate their performance across various compression rates with different deep descriptors over several benchmark datasets. Further, we present an end-to-end framework to fine-tune the pruned network, with a triplet loss function specially designed for the retrieval task. We show that the combination of <b>heuristic</b> <b>pruning</b> and fine-tuning offers 5 x compression rate without considerable loss in retrieval performance. Comment: 5 page...|$|E
40|$|Due to ever {{increasing}} design sizes more efficient tools for Automatic Test Pattern Generation (ATPG) are needed. The {{application of the}} Boolean satisfiability problem (SAT) to ATPG {{has been shown to}} be a robust alternative to traditional ATPG techniques. A major challenge of research in the field of SAT-based ATPG is to obtain a robust algorithm which can solve hard SAT instances reliably without slowing down easy-to-solve SAT instances. This is particular important, since easy-to-solve SAT instances form the majority of an ATPG run. This paper proposes two structural heuristics. The first one uses testability measurements to obtain an improved initial variable order, while the second <b>heuristic</b> <b>prunes</b> many easy-to-test faults by finding easy-to-control paths. Experimental results on large industrial designs confirm that the proposed methodologies result in a significant overall speed-up...|$|R
40|$|Systematic {{state space}} {{traversal}} {{is a popular}} approach for detecting errors in multithreaded programs. Nevertheless, it is very expensive because any non-trivial program exhibits {{a huge number of}} possi-ble interleavings. Some kind of guided and bounded search is often used to achieve good performance. We present two heuristics that are based on a hybrid static-dynamic analysis that can identify pos-sible accesses to shared objects. One heuristic changes the order in which transitions are explored, and the second <b>heuristic</b> <b>prunes</b> selected transitions. Results of experiments on several Java pro-grams, which we performed using our prototype implementation in Java Pathfinder, show that the hybrid analysis together with heuris-tics significantly improves the performance of error detection. CCS Concepts •Software and its engineering → Automated static analysis; Dynamic analysis; Software testing and debugging...|$|R
30|$|To further {{improve the}} {{performance}} of the protocol, we employed a pruning mechanism that temporarily (for the period of a single round) removes from consideration those neighbors that have no prospects for participating in a clique better than the current one. We validated by simulations that combining <b>heuristics</b> with <b>pruning</b> performs better than applying heuristics alone.|$|R
40|$|This letter {{proposes a}} fast {{constrained}} sphere decoder for ill conditioned communications systems that exhibits less complexity than but similar performance to the generalised sphere decoder. The operational principle {{is based on}} i) {{the reduction of the}} search space by setting the hypersphere initial radius to be equal to the distance to a semidefinite program (SDP) estimate; and ii) the introduction of a <b>heuristic</b> <b>pruning</b> rule to limit the GSD spanning tree. The new algorithm achieves significant reduction in the required computational effort at the expense of a small error penalty for large dimensional systems in low signal to noise ratio (SNR) regimes. © 2010 IEEE...|$|E
40|$|Achieving peak {{performance}} from the computational kernels that dominate application performance often requires extensive machine-dependent tuning by hand. Automatic tuning systems {{have emerged in}} response, and they typically operate by (1) generating {{a large number of}} possible, reasonable implementations of a kernel, and (2) selecting the fastest implementation by a combination of heuristic modeling, <b>heuristic</b> <b>pruning,</b> and empirical search (i. e. actually running the code). This paper presents quantitative data that motivate the development of such a search-based system, using dense matrix multiply as a case study. The statistical distributions of performance within spaces of reasonable implementations, when observed on a variety of hardware platforms, lead us to pose and address two genera...|$|E
40|$|In this paper, we {{identify}} a new task {{for studying the}} outlying degree of high-dimensional data, i. e. finding the subspaces (subset of features) in which given points are outliers, and propose a novel detection algorithm, called High-D Outlying subspace Detection (HighDOD). We measure the outlying degree of the point using the sum of distances between this point and its k nearest neighbors. <b>Heuristic</b> <b>pruning</b> strategies are proposed to realize fast pruning in the subspace search and an efficient dynamic subspace search method with a sample-based learning process has been implemented. Experimental results show that HighDOD is efficient and outperforms other searching alternatives such as the naive top-down, bottom-up and random search methods...|$|E
40|$|This paper {{presents}} a summarization model {{based on the}} Universal Networking Language (UNL), which is a conceptual language for representing texts sentence by sentence, using semantic binary relations that are claimed to convey all the information of the corresponding sentence in natural language. Our summarization model is based on <b>heuristics</b> for <b>pruning</b> sentences, focusing on UNL binary relations...|$|R
30|$|When {{we compare}} the {{convergence}} {{times of the}} simulations with partial views with Random implementation (see Figure 14) and the simulations of random subset <b>heuristic</b> without <b>pruning,</b> we do not observe any major discrepancies. To the contrary, the results for partial views with Pruning differ highly from the results for random subset <b>heuristic</b> with <b>pruning.</b> In all setups, adding pruning to random subset heuristic improved the convergence of the protocol significantly. This {{was not the case}} for partial views with Pruning. This differences {{can be explained by the}} fact that in case of the random subset heuristic nodes have the knowledge of all other nodes in the network and apply pruning to the full list of nodes anew at the beginning of each round, thus removing from a consideration a node has the effect only for duration of a single round. Conversely, if node decides to replace in its partial view one node with another, it must reckon that it might take some time before it will stumble upon this node again.|$|R
50|$|Given {{an input}} set of strings, he builds {{step by step}} a tree with each branch {{labelled}} by a reduced regular expression accepting a prefix of some input strings, and each node labelled with the set of lengths of accepted prefixes.He aims at learning correction rules for English spelling errors,rather than at theoretical considerations about learnability of language classes.Consequently, he uses <b>heuristics</b> to <b>prune</b> the tree-buildup, leading to a considerable improvement in run time.|$|R
40|$|This paper {{investigated}} a multi-objective order allocation planning {{problem in}} make-to-order manufacturing with {{the consideration of}} various real-world production features. A novel hybrid intelligent optimization model, integrating a multi-objective memetic optimization (MOMO) process, a Monte Carlo simulation technique and a <b>heuristic</b> <b>pruning</b> technique, is developed to tackle this problem. The MOMO process, combining a NSGA-II optimization process with a tabu search, is proposed to provide Pareto optimal solutions. Extensive experiments based on industrial data are conducted to validate the proposed model. Results show that (1) the proposed model can effectively solve the investigated problem by providing effective production decision-making solutions; (2) the MOMO process has better capability of seeking global optimum than an NSGA-II-based optimization process and an industrial method. Institute of Textiles and Clothin...|$|E
40|$|We {{introduce}} a principled computational framework and methodology for automated discovery of context-specific functional links between ontologies. Our model leverages over disparate free-text literature resources to score {{the model of}} dependency linking two terms under a context against their model of independence. We identify linked terms as those having a significant bayes factor (p < 0. 01). To scale our algorithm over massive ontologies, we propose a <b>heuristic</b> <b>pruning</b> technique as an efficient algorithm for inferring such links. We have applied this method to translationalize Gene Ontology to all other ontologies available at National Center of Biomedical Ontology (NCBO) BioPortal under the context of Human Disease ontology. Our results show {{that in addition to}} broadening the scope of hypothesis for researchers, our work can potentially be used to explore continuum of relationships among ontologies to guide various biological experiments. 1...|$|E
30|$|As said, {{we assume}} {{in this paper}} that {{resources}} cannot be copied or moved to a central location, or that it is undesirable to do so, for example, {{because there is a}} shared distrust in the integrity of the computations performed by a third party. This implies that we need to solve the k-clique matching problem in a decentralized manner. Our main contribution is a fully decentralized algorithm for solving this problem. The algorithm has been partly described and evaluated in [2]. In this paper, we describe important improvements that speed up the convergence of the algorithm, allow us to handle cases where the set of resources is subject to churn, and help in overcoming the communication overhead of the protocol in its basic version. In particular, we introduce the following modification to our k-clique matching protocol: random-subset <b>heuristic,</b> <b>pruning,</b> partial views, and gossiping of clique weights.|$|E
40|$|Rollout {{methodology}} is {{a constructive}} metaheuristic algorithm {{and its main}} characteristics are its modularity, the adaptability to different objectives and constraints and the easiness of implementation. Multi-heuristic Rollout extends the Rollout by incorporating several constructive heuristics in the Rollout framework and {{it is able to}} easily incorporate human experience inside its research patterns to fulfil complex requirements dictated by the application at hand. However, a drawback for both Rollout and multi-heuristic Rollout is often represented by the required computation time. This paper proposes some alternatives of the full multi-heuristic Rollout algorithm aimed at improving the efficiency by reducing the computational effort while preserving the effectiveness. Namely, we propose dynamic <b>heuristics</b> <b>pruning</b> and candidates reduction strategies. As illustrative case studies, we analyse complex deterministic identical parallel machine scheduling problems showing how Rollout procedures can be used to tackle several additional constraints arising in real contexts. More specifically, we considered both standard (batch production, family set-ups, release, due dates, etc.) and non-standard (machine unavailabilities, maximum campaign size) scheduling constraints. An extensive campaign of computational experiments shows the behaviour of the multi-heuristic Rollout approach and the effectiveness of the different proposed speed-up methods...|$|R
50|$|The large board (19×19, 361 intersections) {{is often}} noted {{as one of}} the primary reasons why a strong program is hard to create. The large board size {{prevents}} an alpha-beta searcher from achieving deep look-ahead without significant search extensions or <b>pruning</b> <b>heuristics.</b>|$|R
40|$|We {{provide a}} novel search {{technique}} {{which uses a}} hierarchical model and a mutual information gain <b>heuristic</b> to efficiently <b>prune</b> the search space when localizing faces in images. We show exponential gains in computation over traditional sliding window approaches, while keeping similar performance levels...|$|R
40|$|When {{plugged into}} instant {{interactive}} data analytics processes, pattern mining algorithms {{are required to}} produce small collections of high quality patterns in short amounts of time. In the case of Exceptional Model Mining (EMM), even heuristic approaches like beam search can fail to deliver this requirement, because in EMM each search step requires a relatively expensive model induction. In this work, we extend previous work on high performance controlled pattern sampling by introducing extra weighting functionality, to give more importance to certain data records in a dataset. We use the extended framework to quickly obtain patterns {{that are likely to}} show highly deviating models. Additionally, we combine this randomized approach with a <b>heuristic</b> <b>pruning</b> procedure that optimizes the pattern quality further. Experiments show that in contrast to traditional beam search, this combined method is able to find higher quality patterns using short time budgets...|$|E
40|$|Achieving peak {{performance}} from the computational kernels that dominate application performance often requires extensive machine-dependent tuning by hand. Automatic tuning systems {{have emerged in}} response, and they typically operate by (1) generating {{a large number of}} possible, reasonable implementations of a kernel, and (2) selecting the fastest implementation by a combination of heuristic modeling, <b>heuristic</b> <b>pruning,</b> and empirical search (i. e., actually running the code). This paper presents quantitative data that motivates the development of such a search-based system, using dense matrix multiply as a case study. The statistical distributions of performance within spaces of reasonable implementations, when observed on a variety of hardware platforms, lead us to pose and address two general problems which arise during the search process. First, we develop a heuristic for stopping an exhaustive compile-time search early if a near-optimal implementation is found. Second, we show how to construc...|$|E
40|$|This paper {{addresses}} a multi-objective order scheduling problem in production planning under a complicated production environment with {{the consideration of}} multiple plants, multiple production departments and multiple production processes. A Pareto optimization model, combining a NSGA-II-based optimization process with an effective production process simulator, is developed to handle this problem. In the NSGA-II-based optimization process, a novel chromosome representation and modified genetic operators are presented while a <b>heuristic</b> <b>pruning</b> and final selection decision-making process is developed to select the final order scheduling solution from a set of Pareto optimal solutions. The production process simulator is developed to simulate the production process in the complicated production environment. Experiments based on industrial data are conducted to validate the proposed optimization model. Results show that the proposed model can effectively solve the order scheduling problem by generating Pareto optimal solutions which are superior to industrial solutions. Institute of Textiles and Clothin...|$|E
40|$|Abstract. We {{present an}} {{application}} of multi-objective evolutionary optimization of feed-forward neural networks (NN) to two real world problems, car and face classification. The possibly conflicting requirements on the NN are speed and classification accuracy, both of which can enhance the embedding systems as a whole. We compare the results to {{the outcome of a}} greedy optimization <b>heuristic</b> (magnitude-based <b>pruning)</b> coupled with a multi-objective performance evaluation. For the car classification problem, magnitude-based pruning yields competitive results, whereas for the more difficult face classification, we find that the evolutionary approach to NN design is clearly preferable...|$|R
40|$|Abstract—We {{provide a}} novel search {{technique}} {{which uses a}} hierarchical model and a mutual information gain <b>heuristic</b> to efficiently <b>prune</b> the search space when localizing faces in images. We show exponential gains in computation over traditional sliding window approaches, while keeping similar performance levels. Index Terms—Active testing, face detection, visual search, coarse-to-fine search, face localization. ...|$|R
40|$|In this paper, {{we present}} a {{technique}} to synthesize machine-code instructions from a semantic specification, given as a Quantifier-Free Bit-Vector (QFBV) logic for-mula. Our technique uses an instantiation of the Counter-Example Guided Inductive Synthesis (CEGIS) framework, in combination with search-space <b>pruning</b> <b>heuristics</b> to syn-thesize instruction-sequences. To counter the exponential cost inherent in enumerative synthesis, our technique uses a divide-and-conquer strategy to break the input QFBV for-mula into independent sub-formulas, and synthesize instruc-tions for the sub-formulas. Synthesizers created by our tech-nique {{could be used to}} create semantics-based binary rewrit-ing tools such as optimizers, partial evaluators, program obfuscators/de-obfuscators, etc. Our experiments for Intel’s IA- 32 instruction set show that, in comparison to our base-line algorithm, our search-space <b>pruning</b> <b>heuristics</b> reduce the synthesis time by a factor of 473, and our divide-and-conquer strategy reduces the synthesis time by a further 3 to 5 orders of magnitude. 1...|$|R
40|$|There has {{recently}} been significant interest in using representations based on abstractions of Blum's skeleton into a graph, for qualitative shape matching. The application of these techniques to large databases of shapes hinges {{on the availability of}} numerical algorithms for computing the medial axis. Unfortunately, this computation can be extremely subtle. Approaches based on Voronoi techniques preserve topology, but <b>heuristic</b> <b>pruning</b> measures are introduced to remove unwanted edges. Methods based on Euclidean distance functions can localize skeletal points accurately, but often at the cost of altering the object's topology. In this paper we introduce a new algorithm for computing subpixel skeletons which is robust and accurate, has low computational complexity, and preserves topology. The key idea is to measure the net outward flux of a vector field per unit area, and to detect locations where a conservation of energy principle is violated. This is done in conjunction with a thinnin [...] ...|$|E
40|$|A {{criterion}} for pruning parameters from N-gram backoff language models is developed, {{based on the}} relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub 4 LM {{can be reduced to}} 26 % its original size without increasing recognition error. We also compare the approach to a <b>heuristic</b> <b>pruning</b> criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85 % overlap), with the exact relative entropy criterion giving marginally better performance. Comment: 5 pages. Typos in published version fixe...|$|E
40|$|Abstract]: In this paper, we {{identify}} a new task {{for studying the}} outlying degree (OD) of high-dimensional data, i. e. finding the subspaces (subsets of features) in which the given points are outliers, which are called their outlying subspaces. Since the state-of-the-art outlier detection techniques fail to handle this new problem, we propose a novel detection algorithm, called High-Dimension Outlying subspace Detection (HighDOD), to detect the outlying subspaces of high-dimensional data efficiently. The intuitive idea of HighDOD is that we measure the OD of the point using the sum of distances between this point and its k nearest neighbors. Two <b>heuristic</b> <b>pruning</b> strategies are proposed to realize fast pruning in the subspace search and an efficient dynamic subspace search method with a sample-based learning process has been implemented. Experimental results show that HighDOD is efficient and outperforms other searching alternatives such as the naive top–down, bottom–up and random search methods, and the existing outlier detection methods cannot fulfill this new task effectively...|$|E
40|$|This paper {{addresses}} {{the issue of}} reducing the storage requirements on Instance-Based Learning algorithms. Algorithms proposed by other researches use <b>heuristics</b> to <b>prune</b> instances of the training set or modify the instances themselves to achieve a reduced set of instances. Our work presents an alternative way. We propose to induce a reduced set of partially-defined instances with Evolutionary Algorithms. Experiments were performed with GALE, our fine-grained parallel Evolutionary Algorithm, and other well-known reduction techniques on several datasets. Results suggest that Evolutionary Algorithms are competitive and robust for inducing sets of partially-defined instances, achieving better reduction rates in storage requirements without losses in generalization accuracy...|$|R
40|$|Summarization {{based on}} {{expected}} distribution domain generalization (ExGen) graphs aggregates data into summaries {{in many ways}} and identifies summaries that are far from user expectations, i. e., interesting. In this paper, we tackle two problems. First, we propose how to consistently propagate an expected distribution given by the user for one node to the entire ExGen graph. Secondly, we propose three interestingness measures. Based on these measures, we propose <b>heuristics</b> to <b>prune</b> nodes from the ExGen while searching for interesting summaries. We also demonstrate the interactive experimental process of our method and show the results we obtained by applying it to the Saskatchewan weather data. 1...|$|R
40|$|Simultaneous {{support of}} {{multiple}} video streaming sessions over a shared wireless network requires careful resource allocation to achieve high utilization while dynamically adapting to network and video fluctuations. We propose a distributed algorithm for channel time allocation among multiple video streams, and investigate several <b>heuristic</b> packet <b>pruning</b> schemes for rate adaptation of highdefinition (HD) video streams. Simulation {{results are presented}} for streaming multiple HD video sequences over an 802. 11 a network. In comparison with TCP-Friendly Rate Control (TFRC) and a basic scheme without rate adaptation, it is shown that the proposed scheme can sustain higher video quality with lower packet delivery delay. 1...|$|R
