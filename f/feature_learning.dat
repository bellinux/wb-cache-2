859|3716|Public
2500|$|Compound hierarchical-deep models compose deep {{networks}} with non-parametric Bayesian models. Features can {{be learned}} using deep architectures such as DBNs, DBMs, deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse <b>feature</b> <b>learning,</b> RNNs, conditional DBNs, de-noising auto encoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a [...] ) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples, for example for computer vision, statistics and cognitive science.|$|E
50|$|Unsupervised <b>feature</b> <b>{{learning}}</b> {{is learning}} features from unlabeled data. The goal of unsupervised <b>feature</b> <b>learning</b> is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the <b>feature</b> <b>learning</b> is performed in an unsupervised way, it enables {{a form of}} semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following.|$|E
50|$|<b>Feature</b> <b>learning</b> can {{be divided}} into {{supervised}} and unsupervised.|$|E
5000|$|... 25 September 1966: [...] "Bright Boy, Bad Scholar" [...] - <b>features</b> <b>learning</b> {{disorders}} and their early therapy ...|$|R
40|$|We {{investigate}} {{the influence of}} low-level image features for aesthetics prediction. We show that the aesthetic quality of a photography depends on its context. Image <b>features</b> <b>learned</b> from a specific image category {{are not necessarily the}} same as <b>features</b> <b>learned</b> from a generic image collection. Ex-periments conducted on specific image categories show that specific features obtain statistically significantly better results than generic ones. Index Terms — aesthetic quality, feature extraction, clas-sification. 1...|$|R
50|$|The game {{featured}} conventional campaign style {{missions and}} multiplayer functionality. The game also <b>featured</b> <b>learning</b> Artificial Intelligence, internationalization and localization, and cross-platform compatibility.|$|R
5000|$|... #Subtitle level 2: PAC model based <b>feature</b> <b>learning</b> {{algorithm}} ...|$|E
50|$|Supervised <b>feature</b> <b>{{learning}}</b> {{is learning}} features from labeled data. Approaches include.|$|E
50|$|In {{supervised}} <b>feature</b> <b>learning,</b> {{features are}} learned {{in part with}} labeled input data. Examples include supervised neural networks, multilayer perceptron, and (supervised) dictionary learning.|$|E
40|$|Deep {{learning}} exploits {{large volumes}} of labeled data to learn powerful models. When the target dataset is small, {{it is a common}} practice to perform transfer learning using pre-trained models to learn new task specific representations. However, pre-trained CNNs for image recognition are provided with limited information about the image during training, which is label alone. Tasks such as scene retrieval suffer from <b>features</b> <b>learned</b> from this weak supervision and require stronger supervision to better understand the contents of the image. In this paper, we exploit the <b>features</b> <b>learned</b> from caption generating models to learn novel task specific image representations. In particular, we consider the state-of-the art captioning system Show and Tell SnT-pami- 2016 and the dense region description model DenseCap densecap-cvpr- 2016. We demonstrate that, owing to richer supervision provided during the process of training, the <b>features</b> <b>learned</b> by the captioning system perform better than those of CNNs. Further, we train a siamese network with a modified pair-wise loss to fuse the <b>features</b> <b>learned</b> by SnT-pami- 2016 and densecap-cvpr- 2016 and learn image representations suitable for retrieval. Experiments show that the proposed fusion exploits the complementary nature of the individual features and yields state-of-the art retrieval results on benchmark datasets. Comment: ICME 201...|$|R
30|$|It {{is obvious}} from Figures  5 and 6 that {{whitening}} transformation does affect image patches and corresponding <b>features</b> <b>learned</b> by sparse autoencoders. For example, more edge <b>features</b> can be <b>learned</b> when whitening processing is adopted. In addition, {{the value of}} ε also {{has an effect on}} the image patches and <b>learned</b> <b>features</b> when whitening pre-processing is used. As shown in Figure  6, the <b>learned</b> <b>features</b> look rather noisy if the value of ε is set to 0.01 and the data will be blurred if ε is set to 1. The best value of ε is 0.1 in our experiments.|$|R
40|$|The {{success of}} deep neural {{networks}} is mostly due {{their ability to}} <b>learn</b> meaningful <b>features</b> from the data. <b>Features</b> <b>learned</b> in the hidden layers of deep neural networks trained in computer vision tasks {{have been shown to}} be similar to mid-level vision features. We leverage this fact in this work and propose the visualization regularizer for image tasks. The proposed regularization technique enforces smoothness of the <b>features</b> <b>learned</b> by hidden nodes and turns out to be a special case of Tikhonov regularization. We achieve higher classification accuracy as compared to existing regularizers such as the L 2 norm regularizer and dropout, on benchmark datasets without changing the training computational complexity...|$|R
50|$|In {{unsupervised}} <b>feature</b> <b>learning,</b> {{features are}} learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization, and {{various forms of}} clustering.|$|E
50|$|Multilayer neural {{networks}} {{can be used}} to perform <b>feature</b> <b>learning,</b> since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer.|$|E
5000|$|Geometric <b>feature</b> <b>learning</b> is a {{technique}} combining machine learning and computer vision to solve visual tasks. The main goal of this method {{is to find a}} set of representative features of geometric form to represent an object by collecting geometric features from images and learning them using efficient machine learning methods. Humans solve visual tasks and can give fast response to the environment by extracting perceptual information from what they see. Researchers simulate humans' ability of recognizing objects to solve computer vision problems. For example, M. Mata et al.(2002) [...] applied <b>feature</b> <b>learning</b> techniques to the mobile robot navigation tasks in order to avoid obstacles. They used genetic algorithms for learning features and recognizing objects (figures). Geometric <b>feature</b> <b>learning</b> methods can not only solve recognition problems but also predict subsequent actions by analyzing a set of sequential input sensory images, usually some extracting features of images. Through learning, some hypothesis of the next action are given and according the probability of each hypothesis give a most probable action. This technique is widely used in the area of artificial intelligence.|$|E
5000|$|The series [...] "A House Around the Corner", {{produced}} {{and directed by}} Tricia Ziemer, which <b>features</b> <b>Learn</b> Local Centres across Australia. The show ran for two seasons from 2013 to 2014 on C31 in Melbourne, Perth, and Adelaide.|$|R
40|$|Abstract—HMM models {{based on}} MFCC {{features}} {{are widely used}} by researchers in Tibetan speech recognition. Although the shallow models of HMM are effective, they cannot reflect the speech perceptual mechanism in human being’s brain. In this paper, we propose to apply sparse auto-encoder to <b>learn</b> deep <b>features</b> based on MFCC features for speech data. The deep features not only simulate sparse touches signal of the auditory nerve, and are significant to improve speech recognition accuracy with HMM models. Experimental {{results show that the}} deep <b>features</b> <b>learned</b> by sparse auto-encoder perform better on Tibetan speech recognition than MFCC features and the deep <b>features</b> <b>learned</b> by MLP. Keywords-deep feature learning; sparse auto-encoder; tibetan speech recognition; MFCC features I...|$|R
50|$|The first fully <b>featured</b> <b>Learning</b> Management System (LMS) {{was called}} EKKO, {{developed}} and released by Norway's NKI Distance Education Network in 1991. Three years later, New Brunswick's NB Learning Network presented a similar system designed for DOS-based teaching, and devoted exclusively to business learners.|$|R
5000|$|... {{that under}} {{sparsity}} assumptions and when input data is pre-processed with the whitening transformation k-means produces {{the solution to}} the linear Independent component analysis task. This aids in explaining the successful application of k-means to <b>feature</b> <b>learning.</b>|$|E
5000|$|Unsupervised {{learning}}: No {{labels are}} given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (<b>feature</b> <b>learning).</b>|$|E
50|$|An RBM can {{be viewed}} as a single layer {{architecture}} for unsupervised <b>feature</b> <b>learning.</b> In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using Hinton's contrastive divergence (CD) algorithm.|$|E
5000|$|With {{the trend}} away from paper {{products}} towards {{geographical information systems}} (GISs), Ordnance Survey has been looking into ways of ensuring schoolchildren are {{made aware of the}} benefits of GISs and has launched [...] "MapZone", an interactive child-orientated website <b>featuring</b> <b>learning</b> resources and map-related games.|$|R
40|$|This paper {{presents}} {{a novel approach}} for human recog-nition by combining statistical gait features from real and synthetic templates. Real templates are directly computed from training silhouette sequences, while synthetic tem-plates are generated from training sequences by simulat-ing silhouette distortion. A statistical feature extraction ap-proach is used for <b>learning</b> effective <b>features</b> from real and synthetic templates. <b>Features</b> <b>learned</b> from real templates characterize human walking properties provided in training sequences, and <b>features</b> <b>learned</b> from synthetic templates predict gait properties under other conditions. A feature fu-sion strategy is therefore applied at the decision level to im-prove recognition performance. We apply the proposed ap-proach to USF HumanID Database. Experimental results demonstrate that the proposed fusion approach not only achieves better performance than individual approaches, but also provides large improvement in performance {{with respect to the}} baseline algorithm. ...|$|R
40|$|The {{correlation}} between Automatic Speech Recognition (ASR) and Speech Emotion Recognition (SER) is poorly understood. Studying such correlation may {{pave the way}} for integrating both tasks into a single system or may provide insights that can aid in advancing both systems such as improving ASR in dealing with emotional speech or embedding linguistic input into SER. In this paper, we quantify the relation between ASR and SER by studying the relevance of <b>features</b> <b>learned</b> between both tasks in deep convolutional neural networks using transfer learning. Experiments are conducted using the TIMIT and IEMOCAP databases. Results reveal an intriguing {{correlation between}} both tasks, where <b>features</b> <b>learned</b> in some layers particularly towards initial layers of the network for either task were found to be applicable to the other task with varying degree...|$|R
50|$|The data tend to form {{discrete}} clusters, {{and points}} in the same cluster {{are more likely to}} share a label (although data sharing a label may be spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to <b>feature</b> <b>learning</b> with clustering algorithms.|$|E
50|$|Feature {{engineering}} {{is the process}} of using domain knowledge of the data to create features that make machine learning algorithms work. Feature {{engineering is}} fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated <b>feature</b> <b>learning.</b>|$|E
50|$|In machine learning, <b>feature</b> <b>learning</b> or {{representation}} {{learning is}} a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.|$|E
40|$|Margin based feature {{extraction}} {{has become a}} hot topic in machine learn-ing and pattern recognition. In this paper, we present a novel feature extrac-tion method called Adaptive Margin Maximization (AMM) in which margin is defined to measure the discrimination ability of the features. The motivation comes principally from the iterative weight modification mechanism of the pow-erful boosting algorithms. In our AMM, the samples are dynamically weighted and <b>features</b> are <b>learned</b> sequentially. After one new <b>feature</b> is <b>learned</b> by max-imizing the weighted total margin of data, the weights are updated so that the samples with smaller margins receive larger weights. The <b>feature</b> <b>learned</b> in the next round will thus try to concentrate more on these “hard ” samples adaptively. We show that when the data are projected onto the <b>feature</b> space <b>learned</b> by AMM, most examples have large margins, and therefore the nearest neighbor classifier yields small generalization error. This {{is in contrast to}} existing margin maximization based {{feature extraction}} approaches, in which the goal is to max-imize the total margin. Extensive experimental results on benchmark datasets demonstrate the effectiveness of our method...|$|R
50|$|In April 2009, KeyPoint {{introduced}} a feature to scan personal data, including calendar entries, phonebook contacts, SMS and email inbox and sent items. The “Scan Facebook” <b>feature</b> <b>learns</b> {{words from the}} user’s Facebook profile. The new words learnt {{are added to the}} personal dictionary and offered as suggestions during text entry.|$|R
5000|$|The {{third season}} was called [...] "A Green House Around the Corner" [...] shown in 2015 and <b>featured</b> <b>Learn</b> Local Centres across Australia who remodeled to add {{sustainability}} features, such as Solar Power, to decrease their running costs and their carbon footprint {{so they could}} maintain low training fees for their students.|$|R
50|$|Geometric <b>feature</b> <b>learning</b> methods extract {{distinctive}} geometric features from images. Geometric {{features are}} features of objects constructed {{by a set}} of geometric elements like points, lines, curves or surfaces. These features can be corner features, edge features, Blobs, Ridges, salient points image texture and so on, which can be detected by feature detection methods.|$|E
50|$|Word {{embedding}} is {{the collective}} {{name for a}} set of language modeling and <b>feature</b> <b>learning</b> techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.|$|E
50|$|Extracting or {{selecting}} features is {{a combination}} of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is <b>feature</b> <b>learning,</b> where a machine not only uses features for learning, but learns the features itself.|$|E
5000|$|The Hazel McCallion Academic <b>Learning</b> Centre <b>features</b> {{specialized}} <b>learning</b> spaces, including: ...|$|R
40|$|We {{participated in}} the {{extraction}} of complaint and diagnosis Task and the normalization of complaint and diagnosis Task of MedNLP 2 in NTCIR 11. In the extraction Task, we use CRF based Named Entity Recognition method. Moreover, we incorporate unsupervised <b>features</b> <b>learned</b> from raw cor-pus into CRF. We show such unsupervised features improve system performance...|$|R
40|$|Sentiment {{relevance}} (SR) aims at identify-ing {{content that}} does not contribute to sen-timent analysis. Previously, automatic SR classification has been studied in a limited scope, using a single domain and feature augmentation techniques that require large hand-crafted databases. In this paper, we present experiments on SR classification with automatically <b>learned</b> <b>feature</b> repre-sentations on multiple domains. We show {{that a combination of}} transfer learning and in-task supervision using <b>features</b> <b>learned</b> unsupervisedly by the stacked denoising autoencoder significantly outperforms a bag-of-words baseline for in-domain and cross-domain classification. ...|$|R
