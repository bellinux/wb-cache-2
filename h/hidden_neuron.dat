96|703|Public
5000|$|Output {{value of}} each <b>hidden</b> <b>neuron</b> is {{calculated}} as a sum of all exclusive disjunctions (exclusive or) of input neurons and these weights: ...|$|E
50|$|Instantaneously trained neural {{networks}} are feedforward artificial {{neural networks}} {{that create a}} new <b>hidden</b> <b>neuron</b> node for each novel training sample. The weights to this <b>hidden</b> <b>neuron</b> separate out not only this training sample but others that are near it, thus providing generalization. This training {{can be done in}} a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use unary coding for an effective representation of the data sets.|$|E
5000|$|It {{consists}} of one input layer, one hidden layer and one output layer. The number of neurons in the output layer {{depends on the}} number of hidden units K. Each <b>hidden</b> <b>neuron</b> has N binary input neurons: The weights between input and hidden neurons are also binary: ...|$|E
30|$|Define {{the number}} of <b>hidden</b> <b>neurons</b> N and {{activation}} function g in <b>hidden</b> <b>neurons.</b>|$|R
40|$|This paper {{presents}} a new algorithm, called adaptive 	merging and growing algorithm (AMGA), in designing artificial 	neural networks (ANNs). This algorithm merges and adds <b>hidden</b> 	<b>neurons</b> during the training process of ANNs. The merge operation 	introduced in AMGA {{is a kind}} of a mixed mode operation, 	which is equivalent to pruning two neurons and adding one 	neuron. Unlike most previous studies, AMGA puts emphasis on 	autonomous functioning in the design process of ANNs. This is the 	main reason why AMGA uses an adaptive not a predefined fixed 	strategy in designing ANNs. The adaptive strategy merges or adds 	<b>hidden</b> <b>neurons</b> based on the learning ability of <b>hidden</b> <b>neurons</b> 	or the training progress of ANNs. In order to reduce the amount 	of retraining after modifying ANN architectures, AMGA prunes 	<b>hidden</b> <b>neurons</b> by merging correlated <b>hidden</b> <b>neurons</b> and adds 	<b>hidden</b> <b>neurons</b> by splitting existing <b>hidden</b> <b>neurons.</b> The proposed 	AMGA has been tested on a number of benchmark problems in 	machine learning and ANNs, including breast cancer, Australian 	credit card assessment, and diabetes, gene, glass, heart, iris, and 	thyroid problems. The experimental results show that AMGA 	can design compact ANN architectures with good generalization 	ability compared to other algorithms...|$|R
40|$|Abstract—This paper {{presents}} a new algorithm, called adaptive merging and growing algorithm (AMGA), in designing artificial neural networks (ANNs). This algorithm merges and adds <b>hidden</b> <b>neurons</b> during the training process of ANNs. The merge operation introduced in AMGA {{is a kind}} of a mixed mode operation, which is equivalent to pruning two neurons and adding one neuron. Unlike most previous studies, AMGA puts emphasis on autonomous functioning in the design process of ANNs. This is the main reason why AMGA uses an adaptive not a predefined fixed strategy in designing ANNs. The adaptive strategy merges or adds <b>hidden</b> <b>neurons</b> based on the learning ability of <b>hidden</b> <b>neurons</b> or the training progress of ANNs. In order to reduce the amount of retraining after modifying ANN architectures, AMGA prunes <b>hidden</b> <b>neurons</b> by merging correlated <b>hidden</b> <b>neurons</b> and adds <b>hidden</b> <b>neurons</b> by splitting existing <b>hidden</b> <b>neurons.</b> The proposed AMGA has been tested on a number of benchmark problems in machine learning and ANNs, including breast cancer, Australian credit card assessment, and diabetes, gene, glass, heart, iris, and thyroid problems. The experimental results show that AMGA can design compact ANN architectures with good generalization ability compared to other algorithms. Index Terms—Adding neurons, artificial neural network (ANN) design, generalization ability, merging neurons, retraining...|$|R
50|$|If the {{scalar product}} is 0, {{the output of}} the <b>hidden</b> <b>neuron</b> is mapped to -1 in order to ensure a binary output value. The output of neural network is then {{computed}} as the multiplication of all values produced by hidden elements: Output of the tree parity machine is binary.|$|E
50|$|The Kak neural network, {{which was}} first {{proposed}} by Subhash Kak, is an instantaneously trained neural network {{that creates a}} new <b>hidden</b> <b>neuron</b> for each training sample, achieving instantaneous training for binary data and also for real data if some small additional processing is allowed. These networks, therefore, model short-term biological memory.|$|E
50|$|This layer {{contains}} one neuron {{for each}} {{case in the}} training data set. It stores {{the values of the}} predictor variables for the case along with the target value. A <b>hidden</b> <b>neuron</b> computes the Euclidean distance of the test case from the neuron’s center point and then applies the radial basis function kernel function using the sigma values.|$|E
40|$|This paper reviews {{methods to}} fix a number of <b>hidden</b> <b>neurons</b> in neural {{networks}} for the past 20 years. And it also proposes a new method to fix the <b>hidden</b> <b>neurons</b> in Elman networks for wind speed prediction in renewable energy systems. The random selection {{of a number of}} <b>hidden</b> <b>neurons</b> might cause either overfitting or underfitting problems. This paper proposes the solution of these problems. To fix <b>hidden</b> <b>neurons,</b> 101 various criteria are tested based on the statistical errors. The results show that proposed model improves the accuracy and minimal error. The perfect design of the neural network based on the selection criteria is substantiated using convergence theorem. To verify the effectiveness of the model, simulations were conducted on real-time wind data. The experimental results show that with minimum errors the proposed approach can be used for wind speed prediction. The survey has been made for the fixation of <b>hidden</b> <b>neurons</b> in neural networks. The proposed model is simple, with minimal error, and efficient for fixation of <b>hidden</b> <b>neurons</b> in Elman networks...|$|R
40|$|This paper {{presents}} an approach for finding {{the effect of}} varying <b>hidden</b> <b>neurons</b> and data size on various parameters in neural ensemble classifier. The approach is based on incrementing <b>hidden</b> <b>neurons</b> in base classifiers and training them by decrementing the training data and testing using exactly same size data. The experimental analysis of <b>hidden</b> <b>neurons</b> and data size on clusters, layers, diversity and accuracy in neural ensemble classifier is conducted and presented. The experiments have been conducted using 10 benchmark datasets from UCI machine learning repository. A detailed analysis and results showing the effect of <b>hidden</b> <b>neurons</b> and data size on clusters, layers, diversity and accuracy are presented...|$|R
40|$|There {{has been}} a need for {{geodetic}} network densification {{since the early days}} of traditional surveying. In order to densify geodetic networks in a way that will produce the most effective reference frame improvements, the crustal velocity field must be modelled. Artificial Neural Networks (ANNs) are widely used as function approximators in diverse fields of geoinformatics including velocity field determination. Deciding the number of <b>hidden</b> <b>neurons</b> required for the implementation of an arbitrary function is one of the major problems of ANN that still deserves further exploration. Generally, the number of <b>hidden</b> <b>neurons</b> is decided on the basis of experience. This paper attempts to quantify the significance of pruning away <b>hidden</b> <b>neurons</b> in ANN architecture for velocity field determination. An initial back propagation artificial neural network (BPANN) with 30 <b>hidden</b> <b>neurons</b> is educated by training data and resultant BPANN is applied on test and validation data. The number of <b>hidden</b> <b>neurons</b> is subsequently decreased, in pairs from 30 to 2, to achieve the best predicting model. These pruned BPANNs are retrained and applied on the test and validation data. Some existing methods for selecting the number of <b>hidden</b> <b>neurons</b> are also used. The results are evaluated in terms of the root mean square error (RMSE) over a study area for optimizing the number of <b>hidden</b> <b>neurons</b> in estimating densification point velocity by BPANN...|$|R
50|$|For PNN {{networks}} {{there is}} one pattern neuron for each category of the target variable. The actual target category of each training case is stored with each hidden neuron; the weighted value {{coming out of a}} <b>hidden</b> <b>neuron</b> is fed only to the pattern neuron that corresponds to the hidden neuron’s category. The pattern neurons add the values for the class they represent.|$|E
5000|$|It {{consists}} of one output neuron, K hidden neurons and K*N input neurons. Inputs {{to the network}} take 3 values: The weights between input and hidden neurons take the values: Output value of each <b>hidden</b> <b>neuron</b> is calculated as a sum of all multiplications of input neurons and these weights: Signum is a simple function, which returns -1,0 or 1: ...|$|E
5000|$|In the CC4 network, {{which is}} a three-stage network, the number of input nodes is one more than {{the size of the}} {{training}} vector, with the extra node serving as the biasing node whose input is always 1. For binary input vectors, the weights from the input nodes to the <b>hidden</b> <b>neuron</b> (say of index j) corresponding to the trained vector is given by the following formula: ...|$|E
30|$|When {{the value}} of the width {{parameter}} is in a suitable range, the number of generated RBF <b>hidden</b> <b>neurons</b> will change, but a relatively stable classification accuracy can be achieved. For the proposed ILRBF-BP algorithm, once the width is given, it can learn the sample space automatically, and the changes in the width parameter will affect the coverage of RBF <b>hidden</b> <b>neurons</b> and generate different RBF <b>hidden</b> <b>neurons.</b> Thus, the incremental learning strategy can counteract the effect of the width to some extent.|$|R
3000|$|Once the ANN type is selected, {{its size}} is studied. An MLP {{structure}} with two layers (input, not computed as a layer, hidden and output layers) is selected {{because it is}} demonstrated {{to be enough to}} solve a lot of kind of problems [25]. The number of MLP inputs in the input layer corresponds to the number of sea state parameters selected for this study, i.e., three inputs, being summarized in Equation (10). The number of <b>hidden</b> <b>neurons</b> in the <b>hidden</b> layer is selected according to the following criteria: if a few <b>hidden</b> <b>neurons</b> are selected (4, as an example), poor performance is obtained after training; but if a lot of <b>hidden</b> <b>neurons</b> are selected (50, as an example), a high risk of over-fitting the training data set exists. In this way, an intermediate number of <b>hidden</b> <b>neurons</b> should be selected. As an example, in [26], where MLPs were used to create a ship detection system, the best number of <b>hidden</b> <b>neurons,</b> considering a trade-off between performance and computational cost, was 10. Empirical studies made during our research allow us to determine that no much better performances are obtained from 15 <b>hidden</b> <b>neurons</b> for both platforms, but a computational cost increase is observed. Therefore, 15 <b>hidden</b> <b>neurons</b> are selected. Finally, one output neuron is selected because only one output is needed in the proposed system to give an estimate of the H [...]...|$|R
30|$|In summary, {{we found}} that the best {{performance}} of 4 - and 5 -bit glossary space is achieved when the number of <b>hidden</b> <b>neurons</b> is between 20 and 40 neurons. This means that the BER of an ANN with a hidden layer of 20 neurons is approximately equal to an ANN configuration with higher hidden units. This phenomenon indicates that an increasing number of <b>hidden</b> <b>neurons</b> does not always improve the performance. As a result, the best PBCCS performance can be realized with a fixed-structure ANN of 27 inputs nodes and 20 <b>hidden</b> <b>neurons</b> for 3 -, 4 -, and 5 -bit glossaries.|$|R
5000|$|... where [...] is {{the number}} of neurons in the hidden layer, [...] is the center vector for neuron , and [...] is the weight of neuron [...] in the linear output neuron. Functions that depend only on the {{distance}} from a center vector are radially symmetric about that vector, hence the name radial basis function. In the basic form all inputs are connected to each <b>hidden</b> <b>neuron.</b> The norm is typically taken to be the Euclidean distance (although the Mahalanobis distance appears to perform better in general) and the radial basis function is commonly taken to be Gaussian ...|$|E
5000|$|Hidden layer: This layer has a {{variable}} number of neurons (determined by the training process). Each neuron {{consists of a}} radial basis function centered on a point with as many dimensions as predictor variables. The spread (radius) of the RBF function may be different for each dimension. The centers and spreads are determined by training. When presented with the x vector of input values from the input layer, a <b>hidden</b> <b>neuron</b> computes the Euclidean distance of the test case from the neuron’s center point and then applies the RBF kernel function to this distance using the spread values. The resulting value is passed to the summation layer.|$|E
5000|$|Ben-Jacob's {{studies in}} {{neuroscience}} are guided by {{an effort to}} simplify the complexity searching for principles of information coding, memory and learning. He has many unique contributions {{in the field of}} Systems Neuroscience and Neural Networks, including the relations between network size and its synchronized activity, the discovery of <b>hidden</b> <b>neuron</b> correlations, function-form relations and mutual synchronization in engineered networks, the effect of DNA damage on network synchronization, neuro-glia communication, new modeling of intra- and inter-cell calcium dynamics, using nano technology for network engineering, discovery and modeling of the dynamical motives (repertoire) of coupled neural networks, development of a novel system-level analysis of neural network activity (the functional holography analysis), mapping and assessments of epileptic foci, and more. Yet, the development of the first neuro-memory-chip with his doctoral student at the time, Itay Baruchi, is Ben-Jacob's most important contribution in systems neuroscience. While previous attempts were based on [...] "teaching by reward" [...] (enhancing excitatory synapses) or [...] "teaching by punishment" [...] (inhibition of excitatory synapses), Baruchi and Ben-Jacob's approach was [...] "teaching by liberation", or [...] "inhibition of inhibition" [...] (inhibition of inhibitory synapses). Being recognized as a groundbreaking discovery in systems neuroscience, the achievement was awarded in 2007 the SciAm 50, The Scientific American Award for the 50 most important achievements in all fields of science and technology. It marks a paradigm shift in the understanding of memory and learning in neural networks.|$|E
30|$|In the {{proposed}} incremental learning algorithm, using a potential function approach to construct RBF <b>hidden</b> <b>neurons</b> incrementally has {{to complete the}} effective coverage of the training sample space. As the samples in high-dimensional space are relatively sparse, if the width is too small, it may lead to establish the corresponding Gaussian kernel at each sample, and {{the proposed}} incremental learning algorithm is invalid. The reason is that although the potential value of each sample in the training sample space is measured, {{in the process of}} eliminating the potential value of the sample, the generated RBF <b>hidden</b> <b>neurons</b> do not cover other samples, which will lead to a failure of Eq. (9), and excessive RBF <b>hidden</b> <b>neurons</b> will lead to the redundancy of the network architecture, which affects the classification performance of the BP network. Thus, in the proposed ILRBF-BP algorithm, an effective kernel width parameter should be provided, which can generate proper RBF <b>hidden</b> <b>neurons</b> to cover the sample space. Note that the number of generated RBF <b>hidden</b> <b>neurons</b> should not be close to the number of the training samples; otherwise, the proposed algorithm is invalid.|$|R
30|$|Calculation of the <b>hidden</b> <b>neurons</b> {{error and}} the {{respective}} change of weights.|$|R
3000|$|There is {{no magic}} formula for {{selecting}} the optimum number of <b>hidden</b> <b>neurons.</b> However, some thumb rules {{are available for}} calculating number of <b>hidden</b> <b>neurons.</b> A rough approximation {{can be obtained by}} the geometric pyramid rule proposed by Masters [21]. For a three-layer network with n input and m output <b>neurons,</b> the <b>hidden</b> layer would have at least [...]...|$|R
3000|$|... that {{contains}} the connection weights between the <b>hidden</b> <b>neuron</b> outputs and the output neuron input, and [...]...|$|E
3000|$|... that stores the {{addition}} of the weighted inputs of each <b>hidden</b> <b>neuron.</b> Also, consider that the vector [...]...|$|E
40|$|In this paper, we {{investigate}} the dimension expansion property of 3 layer feedforward neural networks {{and provide a}} helpful insight into how neural networks define complex decision boundaries. First, we note that adding a <b>hidden</b> <b>neuron</b> is equivalent to expanding the dimension of the space defined by the outputs of the hidden neurons. Thus, {{if the number of}} hidden neurons is larger than the number of inputs, the input data will be warped into a higher dimensional space. Second, we will show that the weights between the hidden neurons and the output neurons always define linear boundaries in the <b>hidden</b> <b>neuron</b> space. Consequently, the input data is first mapped non-linearly into a higher dimensional space and divided by linear planes. Then the linear decision boundaries in the <b>hidden</b> <b>neuron</b> space will be warped into complex decision boundaries in the input space...|$|E
30|$|In summary, {{we found}} that for 3 -bit {{glossary}} the DB 2 wavelet has better performance compared with other wavelet families studied in our tests. It is also found that with a 27 −input ANN, the performance is better than that when using many extracted features. Furthermore, the use of 14 <b>hidden</b> <b>neurons</b> or more has a similar recognition rate to a network that contains 8 <b>hidden</b> <b>neurons.</b> As a result, an ANN {{that is based on}} 5 -level DWT can be realized with 27 inputs and as minimum as 14 <b>hidden</b> <b>neurons.</b> This reduction will use fewer resources during the hardware design realization.|$|R
30|$|This paper {{presents}} an incremental learning algorithm for the hybrid RBF-BP (ILRBF-BP) network classifier. A potential function is {{introduced to the}} training sample space in space mapping stage, and an incremental learning method {{for the construction of}} RBF <b>hidden</b> <b>neurons</b> is proposed. The proposed method can incrementally generate RBF <b>hidden</b> <b>neurons</b> and effectively estimate the center and number of RBF <b>hidden</b> <b>neurons</b> by determining the density of different regions in the training sample space. A hybrid RBF-BP network architecture is designed to train the output weights. The output of the original RBF hidden layer is processed and connected with a multilayer perceptron (MLP) network; then, a back propagation (BP) algorithm is used to update the MLP weights. The RBF <b>hidden</b> <b>neurons</b> are used for nonlinear kernel mapping and the BP network is then used for nonlinear classification, which improves classification performance further. The ILRBF-BP algorithm is compared with other algorithms in artificial data sets and UCI data sets, and the experiments demonstrate the superiority of the proposed algorithm.|$|R
40|$|This {{paper is}} {{intended}} to present {{the outcome of a}} study conducted on the cavitation data collected from accelerometer which is installed at the down stream of the cavitation test loop, to illustrate that the <b>hidden</b> <b>neurons</b> in an ANN modelling tool, indeed, do have roles to play in percentage of classification of cavitation signal. It sheds light {{on the role of the}} <b>hidden</b> <b>neurons</b> in an Elman Recurrent type ANN model which is used to classify the cavitation signals. The results confirmed that the hidden-output connection weights become small as the number of <b>hidden</b> <b>neurons</b> becomes large and also that the trade-off in the learning stability between input-hidden and hidden-output connections exists. The Elman recurrent network propagates data from later processing stage to earlier stage. A copy of the previous values of the hidden units are maintained which allows the network to perform sequence-prediction. In the present work, the optimum number of <b>hidden</b> <b>neurons</b> is evolved through an elaborate trial and error procedure. It is concluded that our approach has a significant improvement in learning and also in classification of cavitation signals...|$|R
3000|$|... <b>hidden</b> <b>neuron.</b> The bias {{allows the}} sigmoid {{function}} curve to be shifted horizontally along the input axis while leaving {{the shape of}} the function unchanged. w [...]...|$|E
40|$|Abstract: In this paper, {{we provide}} a {{thorough}} analysis of decision boundaries of neural networks {{when they are}} used as a classifier. First, we divide the classifying mechanism of the neural network into two parts: dimension expansion by hidden neurons and linear decision boundary formation by output neurons. In this paradigm, the input data is first warped into a higher dimensional space by the hidden neurons and the output neurons draw linear decision boundaries in the expanded space (<b>hidden</b> <b>neuron</b> space). We {{also found that the}} decision boundaries in the <b>hidden</b> <b>neuron</b> space are not completely independent. This dependency of decision boundaries is extended to multiclass problems, providing a valuable insight into formation of decision boundaries in the <b>hidden</b> <b>neuron</b> space. This analysis provides a new understanding of how neural networks construct complex decision boundaries and explains how different sets of weights may provide similar results. Key-Words: neural networks, analysis of decision boundary, dimension expansion, linear boundary, dependent decision boundary...|$|E
30|$|One {{advantage}} of the NNRW classifier is that the learning algorithm is less difficult than other conventional neural network classifier (i.e., gradient descent, Levenberg-Marquart, and particle swarm optimization-based learning algorithms). So that, with {{an enormous number of}} hidden neurons is possible to perform using the NNRW classifier. However, the optimal number of neurons of the NNRW classifier is required to be firstly identified for offering better generalization ability of the NNRW classifier. To find the optimal number of <b>hidden</b> <b>neuron,</b> an experiment is executed by varying the number of <b>hidden</b> <b>neuron</b> from 100 to 1200 in steps of 100.|$|E
40|$|The Back Propagation Neural Network {{algorithm}} was validated using {{hypothetical data}} from fifty patients with symptoms of Stroke. The data set {{was divided into}} training set and test set while the validation data set were chosen randomly from the testing data. Forty-two (42) data set {{were used for the}} training set while eight data set were used for the test. Four data were chosen from the test set and used for the validation. A MATLAB program was written for training, testing and validation of the neural network. Three different architectures with 5, 10 and 20 <b>hidden</b> <b>neurons</b> in the network architecture were tested to avoid overfitting and inaccuracy after which neural network with 10 <b>hidden</b> <b>neurons</b> was chosen as the best architecture. The training error converged to 0 after 50 iterations with architecture of 10 <b>hidden</b> <b>neurons</b> while convergence was almost achieved after 100 and 1000 iteration steps with 5 and 10 <b>hidden</b> <b>neurons</b> respectively. The ANN was trained and tested after optimizing the input parameters using Genetic Algorithm, the overall predictive accuracy obtained for the thrombo-embolic stroke was 90 %...|$|R
30|$|The {{simulated}} dynamic process {{operation data}} in (Li et al. 2015) {{were used to}} build data-driven models. The simulated data were generated from the mechanistic model implemented in gPROMS at University of Hull with a sampling time of 5  s. The data were divided into three groups: training data (56 %), testing data (24 %), and unseen validation data (20 %). Furthermore, the constructed model used the input data of the second batch in which the lean solution flow rate has a step change, to verify its accuracy. To demonstrate the good performance of bootstrap aggregated ELM, its results are compared with those from (Li et al. 2015). Before training, the data should be scaled to zero mean and unit variance. Both bootstrap aggregated neural network (BA-NNs) and BA-ELM models combine 30 neural networks. In addition, the numbers of <b>hidden</b> <b>neurons</b> used in BA-NNs and BA-ELM are selected {{within the range of}} 2 – 20 and 40 – 100 respectively. All models with the number of <b>hidden</b> <b>neurons</b> in the above ranges are developed and tested on the testing data. The models give the smallest mean squared errors (MSE) are considered as having the appropriate number of <b>hidden</b> <b>neurons.</b> The reason for ELM having more <b>hidden</b> <b>neurons</b> is due to the random nature of hidden layer weights in ELM and small number of <b>hidden</b> <b>neurons</b> would usually not be able to provide adequate function representation. The form of the dynamic model is shown in Eq. (15).|$|R
30|$|The {{performance}} of ILRBF-BP is {{compared with other}} well-known batch and sequential learning algorithms, such as SGBP, KM-RBF, KMRBF-BP, SVM and ELM, MRAN, GAP-RBF, and OS-ELM on different data sets. Note {{that the number of}} SGBP, KM-RBF, KMRBF-BP, ELM, and OS-ELM <b>hidden</b> <b>neurons</b> is selected manually. When changing the number of <b>hidden</b> <b>neurons</b> several times, the one with the lowest overall testing error is selected as the suitable number of <b>hidden</b> <b>neurons.</b> For multi-class problems, the method of adjusting output saturation problems is used. All simulations in each algorithm are performed ten times and are conducted in the MATLAB 2013 environment on an Intel(R) Core(TM) i 5, 3.2 GHZ CPU with 4 G of RAM. The simulations for the SVM are carried out using the popular LIBSVM package in C [35].|$|R
