3|7674|Public
40|$|Introduction In this {{communication}} {{we apply}} an algorithm, {{based on the}} local trigonometric orthonormal basis and the adapted local trigonometric transform, to decompose digitized speech signals into orthogonal elementary waveforms. This algorithm leads to a local time-frequency representation which is well adapted to analysis-synthesis, compression and segmentation. We present some applications and experimental results for signal compression and automatic voiced-unvoiced segmentation. Furthermore, compression provides a simplified decomposition {{which appears to be}} useful for detecting fundamental frequencies and characterizing formants. We begin with a clean, <b>digitized</b> <b>speech</b> <b>signal.</b> The signal is decomposed into a local trigonometric orthonormal basis which consists of cosines or sines multiplied by smooth cutoff functions. This basis is described by R. Coifman and Y. Meyer [3] and by H. Malvar [7]. We describe and then apply to speech processing an adapted version of this l...|$|E
40|$|Speech coding is a {{representation}} of a <b>digitized</b> <b>speech</b> <b>signal</b> using as few bits as possible, while maintaining reasonable level of speech quality. Due to growing need for bandwidth conservation in wireless communication, the research in speech coding has increased. Recently, Compressive Sensing (CS) is gaining a great interest because {{of its ability to}} recover original signals by taking only few measurements. CS is a new approach that goes against the common data acquisition methods. In this research, a new system of speech encoding system is developed using compressive sensing. Since CS performs well in sparse signals, different sparsifying transforms are analyzed and compared using Gini coefficient. The quality of the speech coder is evaluated using Perceptual Evaluation of Speech Quality (PESQ), Signal-to-Noise Ratio (SNR) and subjective listening tests. Results show that the speech coders have achieved a PESQ score of 3. 16 at 4 kbps which is a good quality as confirmed by listening tests. Furthermore, the coder is also compared with Code Excited Linear Prediction (CELP) coder...|$|E
30|$|Speech coding is {{a process}} to {{transform}} a <b>digitized</b> <b>speech</b> <b>signal</b> into a bit-efficient representation that keeps reasonable speech quality so as to facilitate speech transmission over a band-limited channel or speech storage in a memory-limited media. In general, speech coding techniques can be classified into three categories, including waveform coding, parametric coding, and hybrid coding. The waveform coding technique attempts to maintain the waveform shape of the original speech signal in sample level without any knowledge about the speech generation process. Famous standard speech coders of this category are G. 711 A-law and μ-law Pulse Code Modulation (PCM) coders [1], and G. 726 and G. 727 Adaptive Differential PCM coders [2]. Generally, a waveform coder works well at a high bit rate of 32  kbps or above. The parametric coding technique represents a speech signal by parameters of a speech-generating model. Among various speech-generating models, the most successful one is the linear predictive coding (LPC) model that assumes speech signal is the output of an all-pole model (autoregressive model) fed with an excitation input signal. The parameters of the all-pole filter conceptually represent the vocal tract shape that is highly correlated with the spectral envelope of the speech, while the excitation signal uses a quasiperiodic impulse train to represent information of fundamental frequency (or F 0) for voiced speech, pseudorandom noise for unvoiced speech, {{or a combination of}} the two (i.e., mixed excitation). Coders of this type encode speech signal in a frame-based processing manner and could operate at low bit rates ranging from 2 to 5  kbps. Differing from waveform coders, parametric coders make no attempt to preserve the original waveform shape but to keep the perceptual quality of the reconstructed speech. Famous standard LPC-based speech coders are FS 1015 LPC of LPC- 10 e algorithm [3, 4] and MELP (mixed excitation linear prediction) [5]. The hybrid coding technique tries to combine the advantages of both waveform coding and parametric coding. Coders of this type are similar to parametric coders in utilizing speech-generating models, but also similar to waveform coders in keeping encoded speech waveforms close to the original ones by more detailed modeling of the excitation signal. They generally adopt the code-excited linear prediction (CELP) algorithm [6] to minimize perceptually weighted error. Representative standard hybrid coders are FS 1016 CELP [7, 8], ITU-T G. 728 LD-CELP [9], ETSI AMR-ACELP [10], etc. A hybrid coder generally operates at a medium bit rate of 5 to 15  kbps.|$|E
40|$|In {{the scope}} of <b>digitized</b> <b>speech</b> <b>signals</b> with severe pathologies, as the esophageal voices are, it is {{impossible}} the objective assessment of the voice based {{on a set of}} parameters accepted by the scientific community like pitch, jitter, shimmer or HNR. This impossibility is due to healthy voices or with slight pathologies can be only evaluated with commercial applications. However, this paper presents a robust and precise algorithm that allows the automated calculation of pitch and jitter on very noisy and low quality voice signals. Therefore, both the improvement achieved due to medical treatment and the evaluation of digital signal processing algorithms will be measurable based on objective criteria. ...|$|R
40|$|Abstract — The {{keystream}} reuse {{problem in}} case of textual data {{has been the focus}} of cryptanalysts for quite some time now. This paper presents the use of hidden markov models based speech recognition approach to cryptanalysis of stream ciphered <b>digitized</b> <b>speech</b> in a keystream reuse situation. In this paper, we show that how an adversary can automatically recover the <b>digitized</b> <b>speech</b> <b>signals</b> encrypted under the same keystream. The technique is flexible enough to incorporate all modern speech coding schemes and all languages for which the speech recognition techniques exist. The technique is simple and efficient and can be practically employed with the existing HMM based probabilistic speech recognition techniques with some modification in the training (pre-computation) and/or the maximum likelihood decoding procedure. The simulation experiments, though preliminary, showed promising initial results by recognizing about 80 percent correct phoneme pairs encrypted by the same keystream. Index Terms — cryptanalysis, hidden markov model, keystream reuse, speech recognition, stream cipher. I...|$|R
40|$|Abstract — Keystream reuse {{also known}} as the “two time pad” problem in case of stream ciphered data {{has been the focus of}} cryptanalysts for several decades. All {{heuristics}} presented so far assume the underlying plaintext to be uncompressed text based data encoded through conventional encoding mechanisms such as ASCII Coding. This paper presents the use of hidden Markov model (HMM) based automatic speech recognition (ASR) approach to cryptanalysis of stream-ciphered waveform-encoded speech in a keystream reuse situation. We present that an adversary can automatically recover the <b>digitized</b> <b>speech</b> <b>signals</b> from their plaintext XORs obtained from two different <b>speech</b> <b>signals</b> stream ciphered with the same keystream. The proposed technique can be practically employed with the existing HMM based probabilistic speech recognition techniques with some modification in the selection of HMMs, their training and the maximum likelihood decoding procedures. Simulation experiments using such modified speech recognition tools have been presented. Index Terms — cryptanalysis, keystream reuse, speech coding, stream cipher, two time pad. I...|$|R
40|$|Abstract-This paper {{presents}} an adaptive algorithm for the resto-ration of lost sample values in discrete-time signals that can locally be {{are that the}} positions of the unknown samples should be known and that they should be embedded in a sufficiently large neighborhood of known samples. The estimates of the unknown samples are obtained by minimizing the sum of squares of the residual errors that involve estimates of the autoregressive parameters. A statistical analysis shows that, for a burst of lost samples, the expected quadratic interpolation error per sample converges to the signal variance when the burst length tends to infinity. The method is in fact the first step of an iterative algorithm, in which in each iteration step the current estimates of the missing samples are used to compute the new estimates. Furthermore, the feasibility of implementation in hardware for real-time use is es-tablished. The method has been tested on artificially generated auto-regressive processes as well as on <b>digitized</b> music and <b>speech</b> <b>signals...</b>|$|R
40|$|This paper {{presents}} an adaptive algorithm {{for the restoration}} of lost sample values in discrete-time signals that can locally be described by means of autoregressive processes. The only restrictions are that the positions of the unknown samples should be known and that they should be embedded in a sufficiently large neighborhood of known samples. The estimates of the unknown samples are obtained by minimizing the sum of squares of the residual errors that involve estimates of the autoregressive parameters. A statistical analysis shows that, for a burst of lost samples, the expected quadratic interpolation error per sample converges to the signal variance when the burst length tends to infinity. The method is in fact the first step of an iterative algorithm, in which in each iteration step the current estimates of the missing samples are used to compute the new estimates. Furthermore, the feasibility of implementation in hardware for real-time use is established. The method has been tested on artificially generated auto-regressive processes as well as on <b>digitized</b> music and <b>speech</b> <b>signals...</b>|$|R
5000|$|... #Caption: [...] A static <b>digitized</b> <b>speech</b> {{communication}} device {{with a book}} reading (activity-based) overlay ...|$|R
5000|$|... #Caption: [...] A speech-generating {{device with}} dynamic display, capable of {{outputting}} both synthesized and <b>digitized</b> <b>speech</b> ...|$|R
50|$|Augmentative {{communication}} devices are an alternate treatment method for aphasia. Augmentative communication utilizes computers to aid communication through <b>digitized</b> <b>speech,</b> pictures, animation and/or text (Raymond, 2004, p. 255).|$|R
5000|$|The game {{features}} <b>digitized</b> <b>speech</b> for {{the judge}} to call out such phrases as [...] "Fight!" [...] or [...] "Winner!" [...] The speech is in Japanese in the Japanese version.|$|R
50|$|In 1941, the Allies {{developed}} a voice encryption system called SIGSALY which used a vocoder to <b>digitize</b> <b>speech,</b> then encrypted the speech with one-time pad and encoded the digital data as tones using frequency shift keying.|$|R
50|$|The game {{features}} <b>digitized</b> <b>speech</b> {{effects for}} {{the introduction of}} fighters, knockdown counts, and {{the announcement of the}} outcome of a fight. It {{is also one of the}} few boxing games to depict an in-game referee.|$|R
40|$|In mobile devices, {{perceived}} <b>speech</b> <b>signal</b> degrades {{significantly in}} the presence of background noise as it reaches directly at the listener‟s ears. There is a need to improve the intelligibility and quality of the received <b>speech</b> <b>signal</b> in noisy environments by incorporating speech enhancement algorithms. This paper focuses on speech enhancement method including auditory masking properties of the human ear to improve the intelligibility and quality of the <b>speech</b> <b>signal</b> {{in the presence of}} near-end noise. Implemented by dynamically enhancing the <b>speech</b> <b>signal</b> when the near-end noise dominates. Intelligibility and quality of enhanced <b>speech</b> <b>signal</b> are measured using SII and PESQ. Experimental results show improvement in the intelligibility and quality of the enhanced <b>speech</b> <b>signal</b> with the proposed approach over the unprocessed <b>speech</b> <b>signal.</b> This particular approach is far more efficient in overcoming the degradation of <b>speech</b> <b>signals</b> in noisy environments...|$|R
30|$|Speech {{interferences}} (N 7 : <b>Speech</b> <b>signal</b> {{uttered by}} a woman 1, N 8 : <b>Speech</b> <b>signal</b> uttered by a man 2 and N 9 : <b>Speech</b> <b>signal</b> uttered {{by a woman}} 2).|$|R
5000|$|Computer Gaming World in 1993 {{described}} Victor Vector & Yondo as [...] "Heavy on {{the flash}} and {{light on the}} substance, this product {{is more of a}} talking comic book than a graphic adventure" [...] and criticized the quality of the <b>digitized</b> <b>speech.</b>|$|R
40|$|Abstract: The {{adaptive}} noise cancellation {{system by}} LMS algorithm need {{not to know}} the prior knowledge of input <b>speech</b> <b>signal</b> and noise, and can carry out denoise. In this paper, we present a general approach to using Simulink to build adaptive filter which may denoise for noise added <b>speech</b> <b>signal.</b> Simulation results show that this method has the good suppression ability for the noise of collection <b>speech</b> <b>signal.</b> Because <b>speech</b> <b>signal</b> influenced inevitably by surrounding environment, communications equipment internal electrical noise and so on makes the receiver receive <b>speech</b> <b>signal</b> pollute...|$|R
40|$|This paper {{discusses}} a {{new kind}} of <b>Speech</b> <b>Signal</b> Compression Coding Algorithm based on Half-waveform. According to different characteristics of different <b>speech</b> <b>signal</b> parts, before we encode the <b>Speech</b> <b>Signal,</b> we segment <b>Speech</b> <b>Signal</b> into three kinds of segments: Silence segment, Unvoiced sound segment, Voiced sound segment. As such we encode each kind of speech segment and allocate different bit rate to each kind of speech segment to save the channel sources by different principles. Then we can get these advantages: low bit rate, high compression ratio, high quality of reestablished <b>speech</b> <b>signal...</b>|$|R
40|$|A {{new method}} for {{encoding}} <b>speech</b> <b>signals</b> in automatic voice recognition is proposed. To represent <b>speech</b> <b>signals</b> with minimum redundancy we use independent component analysis to adapt features (basis functions) that efficiently encode the <b>speech</b> <b>signals.</b> The learned basis functions are localized {{in time and}} frequency and resemble Gabor-like filters. In encoding the <b>speech</b> <b>signals,</b> the ICA features capture the statistical essence of <b>speech</b> <b>signals</b> with fewer basis functions than traditional methods such as Gabor filters and the Fourier basis. A speech recognizer can be trained based on those features and the recognition rate is improved and better than the recognition rates obtained by the conventional mel-frequency cepstral features. Our {{results suggest that the}} obtained higher-order structure of <b>speech</b> <b>signals</b> {{plays an important role in}} efficient speech coding. Index Terms Gabor filter, independent component analysis, <b>speech</b> <b>signal</b> processing, feature extraction. I. Introd [...] ...|$|R
5000|$|If {{measurement}} matrix [...] {{satisfies the}} restricted isometric property (RIP) and is incoherent with dictionary matrix [...] then the reconstructed signal is {{much closer to}} the original <b>speech</b> <b>signal.</b> Different types of measurement matrices like random matrices can be used for speech signals.Estimating the sparsity of <b>speech</b> <b>signal</b> is a problem since <b>speech</b> <b>signal</b> highly varies over time and thus sparsity of <b>speech</b> <b>signal</b> also varies highly over time. If sparsity of <b>speech</b> <b>signal</b> can be calculated over time without much complexity that will be best. If this is not possible then worst-case scenario for sparsity can be considered for a given <b>speech</b> <b>signal.</b> Sparse vector (...) for a given <b>speech</b> <b>signals</b> is reconstructed from less number of measurements (...) using [...] minimization. Then original <b>speech</b> <b>signal</b> is reconstructed form the calculated sparse vector [...] using the fixed dictionary matrix as [...] as [...] = [...] [...] Estimation of both the dictionary matrix and sparse vector from just random measurements only has been done iteratively in.The <b>speech</b> <b>signal</b> reconstructed from estimated sparse vector and dictionary matrix is {{much closer to the}} original signal.Some more iterative approaches to calculate both dictionary matrix and <b>speech</b> <b>signal</b> from just random measurements of <b>speech</b> <b>signal</b> are shown in.Th application of structured sparsity for joint speech localization-separation in reverberant acoustics has been investigated for multiparty speech recognition. Further applications of the concept of sparsity are yet to be studied in the field of speech processing. The idea behind CS for <b>speech</b> <b>signals</b> is that can we come up with some algorithms or methods where we only use those random measurements (...) to do some application-based processing like speaker recognition, speech enhancement, etc.|$|R
40|$|Abstract — A {{humanoid}} robot must recognize a target <b>speech</b> <b>signal</b> while {{people around the}} robot chat with them in realworld. To recognize the target <b>speech</b> <b>signal,</b> robot has to separate the target <b>speech</b> <b>signal</b> among other <b>speech</b> <b>signals</b> and recognize the separated <b>speech</b> <b>signal.</b> As separated signal includes distortion, automatic speech recognition (ASR) performance degrades. To avoid the degradation, we trained an acoustic model from non-clean <b>speech</b> <b>signals</b> to adapt acoustic feature of distorted signal and adding white noise to separated <b>speech</b> <b>signal</b> before extracting acoustic feature. The issues are (1) To determine optimal noise level to add the training <b>speech</b> <b>signals,</b> and (2) To determine optimal noise level to add the separated signal. In this paper, we investigate how much noises should be added to clean speech data for training and how speech recognition performance improves for different positions of three talkers with soft masking. Experimental {{results show that the}} best performance is obtained by adding white noises of 30 dB. The ASR with the acoustic model outperforms with ASR with the clean acoustic model by 4 points. I...|$|R
40|$|Noise {{estimation}} and suppression is {{very important}} for improving the quality of <b>speech</b> <b>signal.</b> Noises exist in almost all places. In reality, more than one noise degrades the <b>speech</b> <b>signal.</b> It is hard to find and supress various types of noise that affect the speech quality. This paper proposed a method for noise estimation of mixed non-stationary noisy <b>speech</b> <b>signal.</b> This method uses Spectral properties of the noisy <b>speech</b> <b>signal</b> to detect the frequency regions of noise signal. Highest frequency of <b>speech</b> <b>signal</b> is calculated and it is considered as the threshold value for separating noise <b>signal</b> and clean <b>speech</b> <b>signal.</b> Using Spectral subtraction, Standard deviation of noise spectrum is subtracted with noisy spectrum to acquire enhanced <b>speech</b> <b>signal.</b> Performance of the method is evaluated using SNR and Spectrogram. The main focus {{of this paper is to}} propose an independent method which estimates the noise of any type and nature...|$|R
5000|$|Completing a {{level of}} the game {{triggers}} a voice saying [...] "Yippee!". On the ZX Spectrum release this was remarkable due to it overcoming the Spectrum's rudimentary sound capabilities. It was a very early use of <b>digitized</b> <b>speech</b> sound effects in home computer games.|$|R
50|$|Spawn is an action-adventure {{video game}} {{developed}} {{and published by}} Konami for the Game Boy Color, based on the Spawn comic book character. The game was noted for its extensive use of <b>digitized</b> <b>speech</b> in cutscenes, a largely uncommon feature in games for the system.|$|R
40|$|Abstract. The paper {{addresses}} {{the problem of}} discrimination of homographs when a lengthy segment of an uttered word is missing. The considered discrimination procedure is done by recognizer that operates on cepstrum coefficients extracted from the <b>speech</b> <b>signal.</b> For restoration of the missing speech segment rather than use of the known <b>speech</b> <b>signal,</b> it has been proposed to calculate <b>speech</b> <b>signal</b> characteristics: the period of fundamental frequency and intensity. By experimentation {{it has been shown}} that the polynomial approximation of <b>speech</b> <b>signal</b> characteristics improves homograph discrimination results. An extra computational burden associated with the proposed method is not high because it involves recalculation of the already extracted Fourier coefficients. Key words: homograph discrimination, <b>speech</b> recognition, <b>speech</b> <b>signal</b> processing, <b>speech</b> <b>signal</b> characteristics, restoration, approximation...|$|R
30|$|The {{transform}} coefficients {{based on}} the spectral characteristics of unvoiced <b>speech</b> <b>signals</b> are nearly uniformly distributed in the frequency domain with no obvious decay. Consequently, the sparsity of unvoiced <b>speech</b> <b>signal</b> {{with respect to the}} DCT basis is undesirable. Furthermore, we have not found a satisfactory sparsifying matrix for unvoiced <b>speech</b> <b>signals.</b> Therefore, the usual practice in the framework of CS is to apply the scheme to entire <b>speech</b> <b>signals</b> and not to distinguish voiced <b>speech</b> <b>signals</b> and unvoiced <b>speech</b> <b>signals</b> in advance. Moreover, we find that the overall performance has not been greatly influenced, which can be verified by the simulation results in the following subsection. The reason is that the proportion of voiced speech is more than seventy percent and voiced speech bears dominating information of speech. Certainly, it is of great significance for us to seek to construct a basis or a redundant dictionary for unvoiced <b>speech</b> <b>signals,</b> which is the focus of our future work.|$|R
50|$|The Voice Funnel was an {{experimental}} high-speed interface between <b>digitized</b> <b>speech</b> streams and a packet switching communications network, {{in particular the}} ARPANET. It {{was built in the}} time frame from 1979 to 1981. It may be viewed as an early Voice over IP voice and video telephone.|$|R
40|$|Abstract. A new {{efficient}} {{code for}} <b>speech</b> <b>signals</b> is proposed. To represent <b>speech</b> <b>signals</b> with minimum redundancy we use independent component analysis to adapt features (basis vec-tors) that efficiently encode the <b>speech</b> <b>signals.</b> The learned basis vectors are sparsely distrib-uted and localized in both time and frequency. Time-frequency analysis of basis vectors shows the property similar with the critical bandwidth of human auditory system. Our {{results suggest that}} the obtained codes of <b>speech</b> <b>signals</b> are sparse and biologically plausible...|$|R
40|$|This paper {{presents}} the denoising technique based on spectral subtraction for speech {{synthesis of the}} Marathi numerals. Numerals are recorded through mice and normalized the signals with PRAAT tools. Different form of <b>speech</b> <b>signals</b> were analyzed by added noise in original <b>speech</b> <b>signals.</b> The voice activity detection (VAD) algorithm estimate the noise spectrum of original <b>speech</b> <b>signal.</b> Spectral Subtraction technique is adopted to reduce the noise. It exploits the ability of actively unwanted <b>signals</b> of <b>speech.</b> Spectral Subtraction method has reduced noise and improved the quality of <b>speech</b> <b>signals.</b> This paper concentrates on the application of quality <b>speech</b> <b>signals</b> for <b>speech</b> synthesis and the results found to be satisfactory...|$|R
5000|$|Consider a <b>speech</b> <b>signal</b> , {{which can}} be {{represented}} in a domain [...] such that , where <b>speech</b> <b>signal</b> [...] , dictionary matrix [...] and the sparse coefficient vector [...] This <b>speech</b> <b>signal</b> {{is said to be}} sparse in domain , if number of significant (non zero) coefficients in sparse vector [...] are , where [...]|$|R
30|$|As expected, the time-frequency {{characteristics}} of the watermark follow those components of the <b>speech</b> <b>signal.</b> Consequently, the watermark is inaudible within the <b>speech</b> <b>signal.</b>|$|R
40|$|In this work, <b>speech</b> <b>signals</b> are modeled {{by means}} of the {{so-called}} pre-defined "signature functions". The pre-defined signature functions are generated using the statistical properties of the <b>speech</b> <b>signals.</b> It has been exhibited that, with a few basic signature functions, any <b>speech</b> <b>signal</b> can be generated within a tolerable error. Publisher's Versio...|$|R
50|$|Games are {{permitted}} to end in draws because of this rule. Like in real football, the game {{is divided into two}} halves where the player gets to make about three to four saves per half. The Commodore 64 version has some extra sound effects and some limited <b>digitized</b> <b>speech.</b>|$|R
5000|$|The Commodore 64 version {{features}} {{early use}} of <b>digitized</b> <b>speech.</b> The <b>digitized</b> <b>speech</b> {{was provided by}} the company Electronic Speech Systems, who drastically raised their prices after Impossible Mission became a successful test case. Epyx did not deal with ESS again as a result. Caswell recounted: [...] I never met the performer but, when I supplied the script to the representative from ESS, I told him I had in mind a [...] "50-ish English guy", thinking of the sort of arch-villain James Bond might encounter. I was told that they happened to have just such a person on their staff. When I was given the initial recordings, the ESS guy was apologetic about them being a touch hammy, but I thought the over-acting was amusing and appropriate, and they were left as is...|$|R
40|$|Abstract:- Enhancement of <b>speech</b> <b>signals</b> {{recorded}} using signal channel {{devices such}} as mobile phones is of prime interest. It is because for these devices, {{it is not possible}} to record noise signals separately, and the surrounding background noises are picked up by their microphone simultaneously with the <b>speech</b> <b>signal.</b> This may even completely fade-in the <b>speech</b> <b>signal,</b> depending upon the signal-to-noise ratio (SNR). Therefore to address this problem, number of algorithms and techniques has been developed. However, the existing methods are not able to perform homogenously across all noise types. The auto-correlation function of a noisy <b>speech</b> <b>signal</b> is usually confined to lower time lag and is very small or zero for higher time lag. Therefore, the higher-lag auto-correlation coefficients are relatively robust to additive noise distortion. This paper is focused on enhancing the noisy <b>speech</b> <b>signal</b> from single channel devices by using only the higher-lag auto-correlation coefficients. The efficiency of the algorithm is evaluated in terms of energy, zero crossings and intelligibility of <b>speech</b> <b>signal.</b> Keywords:- Single channel speech enhancement; speech processing; spectral subtraction autocorrelation and <b>Speech</b> <b>signals</b> SNR I...|$|R
