26|305|Public
2500|$|A WinHelp file has a [...] ".hlp" [...] suffix. It can be {{accompanied}} by an optional table of contents (.cnt) file if the help developer created one. When Windows opens a WinHelp file, it creates a [...]gid file in the same directory or in [...] "%LOCALAPPDATA%\Help", containing information about the [...]hlp file such as the window size and location. If the user clicks the [...] "Find" [...] tab and enables <b>keyword</b> <b>indexing,</b> Windows creates an index file with a [...]fts (full text search) extension. Annotations and bookmarks for each Windows help file have the extension [...] ".ann" [...] and [...] ".bmk".|$|E
5000|$|... eXist allows {{software}} developers to persist XML/JSON/Binary documents without writing extensive middleware. eXist follows and extends many W3C XML standards such as XQuery. eXist also supports REST interfaces for interfacing with AJAX-type web forms. Applications such as XForms may save their data by using {{just a few}} lines of code. The WebDAV interface to eXist allows users to [...] "drag and drop" [...] XML files directly into the eXist database. eXist automatically indexes documents using a <b>keyword</b> <b>indexing</b> system.|$|E
5000|$|A WinHelp file has a [...] ".hlp" [...] suffix. It can be {{accompanied}} by an optional table of contents (.cnt) file if the help developer created one. When Windows opens a WinHelp file, it creates a [...]gid file in the same directory or in [...] "", containing information about the [...]hlp file such as the window size and location. If the user clicks the [...] "Find" [...] tab and enables <b>keyword</b> <b>indexing,</b> Windows creates an index file with a [...]fts (full text search) extension. Annotations and bookmarks for each Windows help file have the extension [...] ".ann" [...] and [...] ".bmk".|$|E
30|$|In DB encryption, {{previous}} {{researchers have}} conducted the <b>keyword</b> <b>index</b> search over encrypted documents with various scenarios; however, the <b>keyword</b> <b>index</b> search scheme is inefficient and impractical aspects {{in a real}} world. The <b>keyword</b> <b>index</b> search enables a legitimate queries to search the encrypted documents with an encrypted keyword over the encrypted indexes without revealing any information on the query and documents, even to the server.|$|R
30|$|We made {{definitions}} {{on group}} search secrecy and <b>keyword</b> <b>index</b> search privacy and analyzed them.|$|R
50|$|The site {{includes}} a <b>keyword</b> <b>index</b> and a glossary, {{with links to}} full text versions of Decisions, Directives, Regulations and Commission documents, and to other relevant sites.|$|R
40|$|In {{many areas}} of commerce, government, academia, and hospitals, large {{collections}} of digital im-ages are being created. Many of these collections {{are the product of}} digitizing existing collections of analogue photographs, diagrams, drawings, paintings, and prints. Usually, the only way of search-ing these collections was by <b>keyword</b> <b>indexing,</b> or simply by browsing. Digital images database...|$|E
40|$|Despite {{the success}} of general Internet search engines, {{information}} retrieval remains an incompletely solved problem. Our research focuses on supporting domain experts when they search domain-specific libraries to satisfy targeted information needs. The semantic components model introduces a schema specific to a particular document collection. A semantic component schema consists of a two-level hierarchy, document classes and semantic components. A document class represents a document grouping, such as topic type or document purpose. A semantic component is a characteristic type of information that occurs in a particular document class and represents {{an important aspect of}} the document’s main topic. Semantic component indexing identifies the location and extent of semantic component instances within a document and can supplement traditional full text and <b>keyword</b> <b>indexing</b> techniques. Semantic component searching allows a user to refine a topical search by indicating a preference for documents containing specific semantic components or by indicating terms that should appear in specific semantic components. We investigate four aspects of semantic components in this research. First, we describe lessons learned from using two methods for developing schemas in two domains. Second, we demonstrate use of semantic components to express domainspecific concepts and relationships by mapping a published taxonomy of questions asked by family practice physicians to the semantic component schemas for two document collections about medical care. Third, we report the results of a user study, showing that manual semantic component indexing is comparable to manual <b>keyword</b> <b>indexing</b> with respect to time and perceived difficulty and suggesting that semantic component indexing may be more accurate and consistent than manual <b>keyword</b> <b>indexing.</b> Fourth, we report the results of an interactive searching study, demonstrating the ability of semantic components to enhance search results compared to a baseline system without semantic components. In addition, we contribute a formal description of the semantic components model, a prototype implementation of semantic component indexing software, and a prototype implementation adding semantic components to an existing commercial search engine. Finally, we analyze metrics for evaluating instances of semantic component indexing and <b>keyword</b> <b>indexing</b> and illustrate use of a session-based metric for evaluating multiple-query search sessions...|$|E
40|$|In {{many areas}} of commerce, government, academia, and hospitals, large {{collections}} of digital im- ages are being created. Many of these collections {{are the product of}} digitizing existing collections of analogue photographs, diagrams, drawings, paintings, and prints. Usually, the only way of search- ing these collections was by <b>keyword</b> <b>indexing,</b> or simply by browsing. Digital images databases however, open the way to content-based searching. In this paper we survey some technical aspects of current content-based image retrieval systems...|$|E
30|$|Unlinkability {{means that}} when {{resources}} and services are used by someone, the others cannot link these being correlated or used together. In <b>keyword</b> <b>index</b> search system, it {{can be regarded as}} index indistinguishability.|$|R
30|$|To {{provide a}} secure and {{efficient}} retrieval of data, {{one needs to}} ensure that the user can perform a search over the encrypted data without revealing the contents and the searched keyword to the server. The cryptographic primitive that provides this feature is widely known as searchable encryption (SE). This research aims to study the searchable encryption schemes in detail and implements a solution that enables privacy preserving data storage and retrieval system in cloud computing (aka PrivCloud). For our implementation, we have chosen an existing searchable symmetric key encryption algorithm. To enable the privacy preserving keyword search, this scheme will generate an encrypted <b>keyword</b> <b>index</b> which will be outsourced to the cloud server along with the encrypted data set. The encrypted <b>keyword</b> <b>index</b> lists out the encrypted keyword and pointer to the corresponding document containing that keyword. To search a keyword, client can simply encrypt the keyword to generate the search token and send it to the remote cloud server. The server can retrieve pointer to the corresponding document by matching the search token with the encrypted <b>keyword</b> <b>index</b> table. For our proposed solution, we have chosen to index all the words from the document instead of a specific keyword set. This will allow the client to search for any word in the document rather than any specific <b>keyword.</b> Also, <b>indexing</b> all the words of the document make sure that the user does not need to maintain a <b>keyword</b> <b>index</b> table at the client side since it can search for any words. However, indexing all the words will come with the trade-off of a slightly larger index size. One of the limitations of the proposed solution {{is that it does not}} support the addition of new files since the index update is static.|$|R
5000|$|The Internet DatabaseSo far it {{contains}} more than 4500 selected Africa-related Internet links. Built up by the ilissAfrica staff this pool of websites is searchable as the sites are classified, categorised, <b>keyword</b> <b>indexed</b> and provided with an abstract.|$|R
30|$|Tagging is a {{mechanism}} for linking to relevant resources. Tagging is implemented in internet forums, blogs, collaboration systems (e.g., Wikipedia), and social networks (e.g., Flickr). The tag can be in-text keyword (e.g., [1], Wikipedia) or out-of-text keyword labeled by word or phrase. The in-text keyword tagging methodology focuses on some keywords in the content that may link to other resources. In contrast, the out-of-text keyword maintains tags out of the content body. Tagging in article sharing system is similar to <b>keyword</b> <b>indexing</b> of web search system. However, the tagging is focused on on-site retrieval particularly. The user specifies keyword and later the articles that are tagged with such keyword are retrieved.|$|E
40|$|This report {{presents}} the work involved in developing an adaptive web site, JointZone that personalises student learning on the web. The work combined user modeling, domain modeling and adaptive hypermedia techniques {{to deliver a}} personalised web-based learning environment. The idea of <b>keyword</b> <b>indexing</b> and the site layout structure was used to model the domain giving a conceptual and structural representation of the content. The student model involves the novel idea of using effective reading speed to better gauge if a student has read a page. The project applied the combination of adaptive link hiding and link annotation on a fully functional web site to present an adaptive web-based learning environment...|$|E
40|$|International audienceA {{comparative}} study of the three main chemical information systems (Scifinder, Web of Science and Scopus) was performed by studying the indexing policies of titles, abstracts and keywords within selected literature articles. Various chemical expressions were introduced as topic searches to illustrate the different search tools related to term indexing. The resulting article lists were compared two-by-two {{by means of a}} script designed to identify common reference lists and specific ones to each editor. Analyzing these specific reference lists reveals that only partial coverage areas of references should be expected when querying a single platform. The discussion covers the term and <b>keyword</b> <b>indexing</b> policies, their influence on the retrievability of references and on the retrievability of the highly cited papers...|$|E
30|$|Our schemes are in {{the line}} of the <b>keyword</b> <b>index</b> search area, and this paper focuses on more {{practical}} approaches over the encrypted database to resolve the problems--the efficiency and group search of the encrypted database in the cloud datacenter service.|$|R
5000|$|A Microsoft Help 2.x file has a [...] ".hxs" [...] extension. A {{compressed}} [...]HxS help file (help title) is {{compiled from}} {{a set of}} topic pages written in a subset of HTML, a [...]HxC main project file, an [...]HxF include file, a [...]HxT table of contents, a [...]HxA attribute definition file, {{and a number of}} [...]HxK <b>indexes</b> (<b>keyword</b> <b>Index,</b> NamedURL index, optional associated and context links indexes).|$|R
40|$|Abstract. Combining {{evidence}} of relevance coming from two sources — a <b>keyword</b> <b>index</b> and a keyphrase index — {{has been a}} fundamental part of our INEX-related experiments on XML Retrieval over the past years. In 2008, we focused on {{improving the quality of}} the keyphrase index and finding better ways to use it together with the <b>keyword</b> <b>index</b> even when processing non-phrase queries. We also updated our implementation of the word index which now uses a state-of-the-art scoring function for estimating the relevance of XML elements. Compared to the results from previous years, the improvements turned out to be successful in the INEX 2008 ad hoc track evaluation of the focused retrieval task. ...|$|R
40|$|Abstract. Managing {{and distributing}} {{published}} information is traditionally {{the mission of}} libraries. But in times of digital information provisioning and personalized content delivery, the processes to fulfill this mission have to be reconsidered. Beyond simple <b>keyword</b> <b>indexing</b> using library categorization systems, digital corpora need to be preprocessed for later access directly by the end user. Thus, major functions of the classical librarian like assessing the actual information need and mediating between the library categorization and the end user are to some degree bypassed {{and have to be}} compensated for. Moreover, also the quality control of a digital library’s metadata annotations used for subsequent querying of collections has to be guaranteed. In this paper we discuss the importance of metadata quality control for large eBook collections...|$|E
40|$|However, many {{text mining}} {{applications}} {{do not have}} adequate natural language processing ability beyond simple <b>keyword</b> <b>indexing,</b> and as a result, {{there are too many}} textual elements (words) included in the analysis. We argue that noun phrases as textual elements are better suited for text mining and could provide more discriminating power, than single words. Discourse representation theory (Kamp, 1981) and language learning of children (Snow & Ferguson, 1997) show that a document’s primary concepts are carried by noun phrases. Because noun phrase in a document are not equally important, we propose using them as candidates and identify-ing keyphrases from them. Document keyphrases are the most important topical phrases for a given document, and they address the main topics of that document. Our study pro-poses a Keyphrase Identification Program (KIP) to approac...|$|E
40|$|Dynomite is a {{portable}} electronic notebook that merges the benifits of paper note-taking with the organizational capabilities of computers. Dynomite incorporates four complementary features which combine {{to form an}} easy to use system for the capture and retrieval of handwritten and audio notes. First, Dynomite has a paper-like user interface. Second, Dynomite uses ink properties and keywords for content indexing of notes. Third, Dynomite's properties and keywords allows retrieval of specific ink and notes. The user is shown a view, or {{a subset of the}} notebook content, that dynamically changes as new information is added. Finally, Dynomite continuously records audio, but only permanently stores highlighted portions so {{that it is possible to}} augment handwritten notes with audio on devices with limited storage. Keywords Electronic notebook, note-taking, audio, handwriting, <b>keyword</b> <b>indexing,</b> ink properties, retrieval, paper-like interfaces, pen computing. INTRODUCTION Dynomite is a por [...] ...|$|E
40|$|The International Rice Research Institute (IRRI) has {{published}} the latest edition of this useful and comprehensive catalogue, a compilation in 730 pages of titles on agricultural science for development. It includes a 182 -page <b>keyword</b> <b>index</b> {{to help the}} reader locate publications in certain subject areas. The 332 -page 1990 supplement, which includes only new titles {{that are not in}} the 1989 catalogue, has recently been published. Information Center, IRRI, PO Box 933, Manila PHILIPPINESThe International Rice Research Institute (IRRI) {{has published}} the latest edition of this useful and comprehensive catalogue, a compilation in 730 pages of titles on agricultural science for development. It includes a 182 -page <b>keyword</b> <b>index</b> to help [...] ...|$|R
2500|$|... {{some sort}} of <b>keyword</b> <b>index</b> for [...]" [...] In early May, Mr. Martelli posted a summary of his {{responses}} to Usenet, noting that the [...] "most popular suggestion award must definitely go to 'lq-text' package, by Liam Quin, recently posted in alt.sources." ...|$|R
50|$|Searchable Encryption is a {{cryptographic}} primitive {{which offers}} secure search functions over encrypted data. In {{order to improve}} search efficiency, SE generally builds <b>keyword</b> <b>indexes</b> to securely perform user queries. SE schemes can be classified into two categories: SE based on secret-key cryptography and SE based on public-key cryptography.|$|R
40|$|Abstract: Images and {{graphics}} {{are among the}} most important media formats for human communication and they provide a rich amount of information for people to understand the world. In many areas of commerce, government, academia, and hospitals, large collections of digital images are being created. Many of these collections are the product of digitizing existing collections of analogue photographs, diagrams, drawings, paintings, and prints. Usually, the only way of searching these collections was by <b>keyword</b> <b>indexing,</b> or simply by browsing. Traditional text based methods are proven to be insufficient for retrieval of images from the large image data base. To overcome the drawbacks of text-based image retrieval, images are retrieved with the help of contents present in that image (i. e using the low-level features of an image such as Color, Shape and Texture). Among these three features texture {{plays an important role in}} many image retrieval systems such as surface inspection...|$|E
40|$|Abstract—CBIR {{operates}} on {{a totally different}} principle from <b>keyword</b> <b>indexing.</b> Primitive features characterizing image content, such as color, texture, and shape are computed for both stored and query images, and used to identify the images most closely matching the query. There have been many approaches to decide and extract the features of images in the database. Towards this goal we propose a technique by which the color content of images is automatically extracted to form a class of meta-data that is easily indexed. The color indexing algorithm uses the back-projection of binary color sets to extract color regions from images. This technique use without histogram of image histogram bins of red, green and blue color. The feature vector is composed of mean, standard deviation and variance of 16 histogram bins of each color space. The new proposed methods are tested on the database of 600 images {{and the results are}} in the form of precision and recall...|$|E
40|$|OBJECTIVE: Medical {{information}} is increasingly being {{presented in a}} web-enabled format. Medical journals, guidelines, and textbooks are all accessible in a web-based format. It would be desirable to link these reference sources to the electronic medical record to provide education, to facilitate guideline implementation and usage and for decision support. In order for these rich information sources to be accessed via the medical record {{they will need to}} be indexed by a single comparable underlying reference terminology. METHODS: We took a random sample of 100 web pages out of the 6, 000 web pages on the Mayo Clinic's Health Oasis web site. The web pages were divided into four datasets each containing 25 pages. These were humanly reviewed by four clinicians to identify all of the health concepts present (R 1 DA, R 2 DB, R 3 DC, R 4 DD). The web pages were simultaneously indexed using the SNOMED-RT beta release. The indexing engine has been previously described and validated. A new clinician reviewed the indexed web pages to determine the accuracy of the automated mappings as compared with the human identified concepts (R 4 DA, R 3 DB, R 2 DC, R 1 DD). RESULTS: This review found 13, 220 health concepts. Of these 10, 383 concepts were identified by the initial human review (78. 5 % +/- 3. 6 %). The automated process identified 10, 083 concepts correctly (76. 3 % +/- 4. 0 %) from within this corpus. The computer identified 2, 420 concepts, which were not identified by the clinician's review but were upon further consideration important to include as health concepts. There was on average a 17. 1 % +/- 3. 5 % variability in the human reviewers ability to identify the important health concepts within web page content. Concept Based Indexing provided a positive predictive value (PPV) of finding a health concept of 79. 3 % as compared with <b>keyword</b> <b>indexing</b> which only has a PPV of 33. 7 % (p < 0. 001). CONCLUSION: SNOMED-RT is a reasonable ontology for web page indexing. Concept based indexing provides a significantly greater accuracy in identifying health concepts when compared with <b>keyword</b> <b>indexing...</b>|$|E
40|$|This {{document}} {{presents a}} bibliography of NIOSH communication and research {{products for the}} year 2012. Product types include journal articles, book chapters, numbered publications, abstracts/proceedings, control technology reports, fatality assessment and control evaluation reports, fire fighter fatality investigation and prevention reports, an author <b>index,</b> a <b>keyword</b> <b>Index,</b> and the National Occupational Research Agenda (NORA) IndexI. Journal articles [...] II. Books or book chapter [...] III. NIOSH numbered publications [...] IV. Proceedings [...] V. Abstracts [...] VI. Control technology reports [...] VII. Fire fighter fatality investigation and prevention reports [...] VIII. Health hazard evaluation reports [...] IX. Author <b>index</b> [...] X. <b>Keyword</b> <b>index</b> [...] XI. National Occupational Research Agenda (NORA) index. "April 2013. "Also available via the World Wide Web as an Acrobat. pdf file (4. 87 MB, 129 p.) ...|$|R
40|$|Gaudry {{present a}} {{variation}} of index calculus attack for solving the DLP in the Jacobian of hyperelliptic curves. Harley and Thérialut improve these kind of algorithm. Here, we will present {{a variation of}} these kind of algorithm, which is faster than previous ones. <b>Keywords</b> <b>Index</b> calculus attack, Jacobian, Hyperelliptic curve, DLP,...|$|R
40|$|Table of Contents includes: OCLC's Symposium on Reconceptualizing Cataloging Will Be Webcast; MARC Code List Changes; MARC Code List Changes; Dewey Decimal Classification News; CatExpress Tutorial Revised; CORC Enhancements for December 2001; The Music OCLC Users Group Annual Meeting; Cataloging Electronic Resources: OCLC-MARC Coding Guidelines Revised; <b>Keyword</b> Searching <b>Index</b> Changes Coming in Early 2002; OCLC Interlibrary Loan Service Non-Referral Days for 2002; Users Are Talking about ILL Web; OCLC ILL Web Interface Enhancements: November - December 2001; FirstSearch Enhances Library Holdings Options; Z 39. 50 Access: <b>Keyword</b> <b>Index</b> Changes; ArticleFirst Z 39. 50 Functionality Replaces ContentsFirst Database...|$|R
40|$|With the {{popularity}} of the network and developmentof multimedia technology, the traditional information retrievaltechniques do not meet the users’ demand. Recently, thecontent-based image retrieval has become the hot topic and thetechniques of content-based image retrieval have been achievedgreat development. In this paper, the basic components ofcontent-based image retrieval system are introduced. Imageretrieval methods based on color, texture, shape and semanticimage are discussed, analyzed and compared. The semantic-basedimage retrieval is a better way to solve the “semantic gap”problem, so the semantic-based image retrieval method is stressedin this paper. Other related techniques such as relevance feedbackand performance evaluation also discussed. In the end of paperthe problems and challenges are proposed. In many areas ofcommerce, government, academia, and hospitals, largecollections of digital images are being created. Many ofthese collections are the product of digitizing existingcollections of analogue photographs, diagrams, drawings,paintings, and prints. Usually, the only way of searchingthese collections was by <b>keyword</b> <b>indexing,</b> or simply bybrowsing. Digital images databases however, open the wayto content-based searching. In this paper we survey sometechnical aspects of current content-based image retrievalsystems...|$|E
40|$|The Village Voice {{project is}} {{a study of the}} {{efficacy}} of a localized ontology in the dissemination of narrative. It seeks to understand how community members can articulate their lives in ways that allow each other to reflect on the makeup of their overall community, and how they represent their community's needs to those outside of the group. I utilize a knowledge model, or ontology, created by community members as a foundation for representing and retrieving story fragments (video clips). The focus of this thesis will be to study the methodology by which such a knowledge model can be elicited, and the relative benefits of representing stories by this mechanism versus the standard database technique of <b>keyword</b> <b>indexing.</b> I evaluate {{the strengths and weaknesses of}} this ontology-driven narrative system within the real-world context of a local community of Somali refugees (Jamaica Plain, MA). by Ramesh Srinivasan. Thesis (S. M.) [...] Massachusetts Institute of Technology, School of Architecture and Planning, Program in Media Arts and Sciences, 2002. Includes bibliographical references (leaves 75 - 77) ...|$|E
40|$|Abstract — In {{many areas}} of commerce, government, academia, and hospitals, large {{collections}} of digital images are being created. Many of these collections {{are the product of}} digitizing existing collections of analogue photographs, diagrams, drawings, paintings, and prints. Usually, the only way of searching these collections was by <b>keyword</b> <b>indexing,</b> or simply by browsing. Digital images databases however, open the way to content based searching. In this paper a review on {{the current state of the}} art in content based image retrieval (CBIR), a technique for retrieving images on the basis of automatically derived features such as color, texture and shape is presented. Our findings are based both on a review of the relevant literature and on discussions with researchers and practitioners in the field. Here we discuss a new content based image retrieval approach based on three well known algorithms Mean Shift Segmentation, Wavelet Transform and PCA. The evaluation of the proposed approach is carried out using the standard precision and recall measures. Experimental results for the Caltech and Corel databases showed the effectiveness of our proposed method to be very promising and performance to be better than other existing approaches...|$|E
40|$|This {{bibliography}} {{consists of}} citations {{pertinent to the}} subjects of combustion and detonation of energetic materials, especially, but not exclusively, of secondary solid high explosives. These references were selected from abstracting sources, conference proceedings, reviews, and also individual works. The entries are arranged alphabetically by first author and numbered sequentially. A <b>keyword</b> <b>index</b> is appended...|$|R
5000|$|An online Index to The Fibonacci Quarterly {{covering}} Volumes 1-48 (1963-2010) {{includes a}} Title Index, Author Index, Elementary Problem Index, Advanced Problem Index, Miscellaneous Problem Index, and Quick Reference <b>Keyword</b> <b>Index.</b> The Fibonacci Quarterly is available online to subscribers; on June 29, 2010, online volumes {{ranged from the}} current issue back to volume 41 (2003).|$|R
30|$|Users {{of group}} members are {{the owners of}} documents, and they are {{registered}} in their organization. GM plays a similar role of a client server, {{and it is a}} trusted party in our scheme. In our scheme, the GM manages the group session keys and the search keys of all groups, for secure communication and secure <b>keyword</b> <b>index</b> search.|$|R
