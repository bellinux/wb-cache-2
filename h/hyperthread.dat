1|99|Public
40|$|Recently news {{technologies}} in molecular biology enormously improved the sequencing data production, {{making it possible}} to generate billions of short reads totalizing gibabases of data per experiment. Prices for sequencing are decreasing rapidly and experiments that were impossible in the past because of costs are now being executed. Computational methodologies that were successfully used to solve the genome assembler problem with data obtained by the shotgun strategy, are now inefficient. Efforts are under way to develop new programs. At this moment, a stabilized condition for producing quality assembles is to use paired-end reads to virtually increase the length of reads, but {{there is a lot of}} controversy in other points. The works described in literature basically use two strategies: one is based in a high coverage[1] and the other is based in an incremental assembly, using the made pairs with shorter inserts first[2]. Independently of the strategy used the computational resources demanded are actually very high. Basically the present computational solution for the de novo genome assembly involves the generation of a graph of some kind [3], and one because those graphs use as node whole reads or k-mers, and considering that the amount of reads is very expressive; it is possible to infer that the memory resource of the computational system will be very important. Works in literature corroborate this idea showing that multiprocessors computational systems with at least 512 Gb of principal memory were used in de novo projects of eukaryotes [1, 2, 3]. As an example and benchmark source it is possible use the Panda project, which was executed by a research group consortium at China and generated de novo genome of the giant Panda (Ailuropoda melanoleura). The project initially produced 231 Gb of raw data, which was reduced to 176 Gb after removing low-quality and duplicated reads. In the de novo assembly process just 134 Gb were used. Those bases were distributed in approximately 3 billions short reads. After the assembly, 200604 contigs were generated and 5701 multicontig scaffolds were obtained using 124336 contigs. The N 50 was respectively. 36728 bp and 1. 22 Mb for contigs and scaffolds. The present work investigated the computational demands of de novo assembly of eukaryotes genomes, reproducing the results of the Panda project. The strategy used was incremental as implemented in the SOAPdenovo software, which basically divides the assembly process in four steps: pre-graph to construction of kmer-graph; contig to eliminate errors and output contigs, map to map reads in the contigs and scaff to scaffold contigs. It used a NUMA (non-uniform memory access) computational system with 8 six-core processors with <b>hyperthread</b> tecnology and 512 Gb of RAM (random access memory), and the consumption of resources as memory and processor time were pointed for every steps in the process. The incremental strategy to solve the problem seems practical and can produce effective results. At this moment a work is in progress which is investigating a new methodology to group the short reads together using the entropy concept. It is possible that assemblies with better quality will be generated, because this methodology initially uses more informative reads. References [1] Gnerre et. al.; High-quality draft assemblies of mammalian genomes from massively parallel sequence data, Proceedings of the National Academy of Sciences USA, v. 108, n. 4, p. 1513 - 1518, 2010 [2] Li et. al.; The sequence and de novo assembly of the giant panda genome, Nature, v. 463, p. 311 - 317, 2010 [3] Schatz et. al.; Assembly of large genomes using second-generation sequencing, Genome Research, v. 20, p. 1165 - 1173, 20102011 X-MEETING 2011...|$|E
5000|$|What is <b>hyperthreading?</b> <b>Hyperthreading</b> is {{something}} that makes a single CPU core emulate two [...] "cores", otherwise named threads. For example, the Intel Core i3 and i7 line of now offer <b>hyperthreading.</b>|$|R
5000|$|Re-implemented Hyper-threading. <b>Hyperthreading</b> was {{introduced}} in the older NetBurst microarchitecture, but omitted from the subsequent Core, which was a descendant of the Pentium III family. With <b>hyperthreading</b> enabled, each of the four physical cores can process up to two threads simultaneously, so the processor appears to the OS as eight logical CPUs.|$|R
5000|$|Resource {{allocations}} {{optimized for}} network topology and on-node topology (sockets, cores and <b>hyperthreads)</b> ...|$|R
5000|$|The Virtualization Spectrum from <b>Hyperthreads</b> to GRIDs, Proc. CMG Conf., Reno, Nevada, Dec. (2006) ...|$|R
30|$|This section {{looks at}} a third implementation, {{implemented}} with 16 processors. The idea is based on maximizing the use of resources available in the machine. As already mentioned, a cluster of eight dual processors with <b>hyperthreading</b> nodes is used. However, due to the bad results obtained with <b>hyperthreading</b> shown in Section 7, only the real processors have been used instead of the virtual ones.|$|R
50|$|In 1999, Intel {{introduced}} the Streaming SIMD Extensions (SSE) instruction set, following in 2000 with SSE2. The first addition allowed offloading of basic floating-point operations from the x87 stack {{and the second}} made MMX almost obsolete and allowed the instructions to be realistically targeted by conventional compilers. Introduced in 2004 along with the Prescott revision of the Pentium 4 processor, SSE3 added specific memory and thread-handling instructions to boost the performance of Intel's <b>HyperThreading</b> technology. AMD licensed the SSE3 instruction set and implemented most of the SSE3 instructions for its revision E and later Athlon 64 processors. The Athlon 64 does not support <b>HyperThreading</b> and lacks those SSE3 instructions used only for <b>HyperThreading.</b>|$|R
40|$|Installation {{requirements}} [...] . 2 Integrity VM installation [...] 2 Check for <b>hyperthreads</b> [...] 3 Installing Integrity VM [...] . ...|$|R
50|$|Windows XP {{includes}} simultaneous multithreading (<b>hyperthreading)</b> support. Simultaneous multithreading is a processor's {{ability to}} process {{more than one}} data thread at a time.|$|R
50|$|Skylake has a {{critical}} flaw (<b>hyperthreading</b> issue) where some short loops may cause unpredictable system behavior. A BIOS update was issued {{to fix the}} issue.|$|R
50|$|Processor: Intel Atom {{single-core}} (with <b>hyperthreading)</b> Z520, Z530, Z540, Z550 or Z560, at 1.33, 1.6, 1.87, 2, or 2.13 GHz. Z560 @ 2.13 GHz is non-US.|$|R
40|$|The paper {{gives an}} {{overview}} on the architectural implementation of <b>Hyperthreading</b> technology on Intel Pentium 4 processor. Given {{below is a}} short critique on the paper, presenting its strengths and weaknesses: Strengths 1. The paper gives a good description on the architectural level changes made on the Intel Pentium 4 processor to implement <b>hyperthreading.</b> It also gives a crisp and easy to understand account on the pipelining methodology and resource sharing policies. 2. The paper tries to be fair in showing the negative aspects of <b>hyperthreading</b> by detailing on the design complexity issues and the brief comparisons with chip multiprocessors. Weaknesses 1. The author writes about implementing a Full Sharing resource policy on the processor caches. Despite this method being advantageous to multi-threaded applications, there could be scenarios such as a cache-cleaning thread sharing the memory with a worker thread, when the policy could result in CPU thrashing. 2. The performance evaluation discussed in the paper does not cover the results of testing nonmultithreade...|$|R
3000|$|Three {{different}} versions of the proposed scheme have been implemented and executed on a cluster of dual <b>hyperthreading</b> processor nodes to evaluate its flexibility and scalability. These versions are as follows: [...]...|$|R
3000|$|The first {{implementation}} described used {{a shared}} memory biprocessor with <b>hyperthreading</b> (i.e., four virtual processors). Taking {{into consideration the}} parallel algorithm proposed in Section 6.1 for four processors, the following grouping of tasks has been made: [...]...|$|R
5000|$|These two {{leaves are}} used for {{processor}} topology (thread, core, package) and cache hierarchy enumeration in Intel multi-core (and <b>hyperthreaded)</b> processors. [...] AMD does not use these leaves but has alternate ways of doing the core enumeration.|$|R
3000|$|A hybrid version, using threads and MPI to take {{advantage}} of the fact that each node is a multiprocessor - a dual <b>hyperthreading</b> processor. In an eight-node system, with two processors per node, the speedup reached is 8.41.|$|R
5000|$|Each compute core in the Cell runs {{only one}} thread at a time, in-order. A core in Larrabee {{ran up to}} four threads, but only one at a time. Larrabee's <b>hyperthreading</b> helped hide the latencies {{inherent}} to in-order execution.|$|R
50|$|The modular {{architecture}} consists of multithreaded shared L2 cache and FlexFPU, which uses simultaneous multithreading. Each physical integer core, two per module, is single threaded, {{in contrast with}} Intel's <b>Hyperthreading,</b> where two virtual simultaneous threads share the resources of a single physical core.|$|R
50|$|There {{is also a}} {{security}} concern with certain simultaneous multithreading implementations. Intel's <b>hyperthreading</b> implementation has a vulnerability through which {{it is possible for}} one application to steal a cryptographic key from another application running in the same processor by monitoring its cache use.|$|R
50|$|The modular {{architecture}} of Bulldozer microarchitecture uses a special FPU named FlexFPU, which uses simultaneous multithreading. Each physical integer core, two per module, is single threaded, {{in contrast with}} Intel's <b>Hyperthreading,</b> where two virtual simultaneous threads share the resources of a single physical core.|$|R
50|$|When {{applying}} the map pattern, one formulates an elemental function that captures the operation {{to be performed}} on a data item that represents {{a part of the}} problem, then applies this elemental function in one or more threads of execution, <b>hyperthreads,</b> SIMD lanes or on multiple computers.|$|R
40|$|Nehalem is the codename for the Intel {{processor}} microarchitecture {{successor to}} the Core microarchitecture. The first processor released with the Nehalem architecture is the desktop Core i 7, which was released in November 2008 Intel, Nehalem, Bloomfield, Lynnfield, CPU, Turbo Boost, <b>HyperThreading,</b> PCIe, transistor, QPI, core, thread, overclock...|$|R
40|$|<b>Hyperthreading</b> is {{a design}} that makes {{everybody}} concerned {{believe that they are}} actually using a dual processor system, except for the licensing software, which recognizes it as a single processor. How? It starts with the P 4 engine, called NetBurst Architecture, and then adds the hardware to provide two processor environments in the one chip...|$|R
50|$|The device uses an Intel Atom SoC Atom Z2460 (codename Medfield), which {{contains}} a single-core Intel Atom CPU with <b>HyperThreading</b> running at 1.6 GHz.The device has 1 GB of dedicated RAM, 16 GB of internal storage and a 4.03 inch display with {{a resolution of}} WSVGA (1024x600) supporting 16 Million colors covered by Gorilla Glass.|$|R
30|$|Since a {{potentially}} {{large number of}} block projections p^k_p,b must be added up during the calculation of the barycentre, a shared-memory architecture was chosen for the parallelisation of the ACIM code. A high-performance computing system with 12 <b>hyperthreaded</b> 3.4 GHz cores and 296 GB of fully shared memory is available for {{the execution of the}} ACIM code.|$|R
30|$|Mainstream desktop {{processors}} from Intel and AMD {{include two}} or four out-of-order executing processors. These processors are replications of the original, complex cores that share a level 2 cache {{and the memory}} bus. Cache coherence protocols on the chip keep the level 1 caches coherent and consistent. Furthermore, these cores also support SMT, sometimes also called <b>hyperthreading.</b>|$|R
5000|$|In Windows NT {{operating}} systems, the System Idle Process contains {{one or more}} kernel threads which {{run when}} no other runnable thread can be scheduled on a CPU. In a multiprocessor system, there is one idle thread associated with each CPU core. For a system with <b>hyperthreading</b> enabled, there is an idle thread for each logical processor.|$|R
50|$|Intel Xeon Phi has 4-way SMT (with Time-multiplexed multithreading) with {{hardware}} based threads which can't be disabled unlike regular <b>Hyperthreading.</b> The Intel Atom, {{released in}} 2008, {{is the first}} Intel product to feature 2-way SMT (marketed as Hyper-Threading) without supporting instruction reordering, speculative execution, or register renaming. Intel reintroduced Hyper-Threading with the Nehalem microarchitecture, after its absence on the Core microarchitecture.|$|R
50|$|Intel vPro {{technology}} is an umbrella marketing term used by Intel {{for a large}} collection of computer hardware technologies, including <b>Hyperthreading,</b> Turbo Boost 3.0, VT-x, VT-d, Trusted Execution Technology (TXT), and Intel Active Management Technology (AMT). When the vPro brand was launched (circa 2007), it was identified primarily with AMT, thus some journalists still consider AMT to be the essence of vPro.|$|R
40|$|Job {{scheduling}} typically {{focuses on}} the CPU with little work existing to include I/O or memory. Time-shared execution provides the chance to hide I/O and long-communication latencies though potentially creating a memory conflict. We consider two different cases: standard local CPU scheduling and coscheduling on <b>hyperthreaded</b> CPUs. The latter supports coscheduling without any context switches and provides additional options for CPU-internal resource sharing. We present an approach that includes all possible resources into the schedule optimization and improves utilization by coscheduling two jobs if feasible. Our LOMARC approach partially reorders the queue by lookahead to increase the potential to find good matches. In simulations based on the workload model of [Lublin 2003], we have obtained improvements of about 50 % in both response times and relative bounded response times on <b>hyperthreaded</b> CPUs (i. e. cut times by half) and of about 25 % on standard CPUs for our LOMARC scheduling approach. 1...|$|R
30|$|In {{the first}} run (conventional computers), the process took 28  days to {{complete}} the three libraries on three machines with identical configuration: Intel Core i 7 3.4  GHz quad-core processor and <b>HyperThreading</b> technology (up to eight simultaneous tasks) and 4 GB of memory. As described in the experimental plan, these results were discarded, and we carried out the second execution of the automatic experiment using LOBOC.|$|R
30|$|Another {{cause for}} the low speedup {{is the use of}} <b>hyperthreading</b> technology. While every node {{apparently}} has four processors (dual-hyperthreading processor), there are just two actual processors. Analyzing the application code, it can be noted that the calculations basically consist of floating point operations (floating point square roots, multiplications, etc.). Considering the usual structure of current processors, {{it is clear that the}} floating point functional units are not sufficiently replicated.|$|R
40|$|Abstract ◊ As {{information}} {{acquisition and}} processing applications take greater roles in our everyday life, database management systems are growing in importance. Database management systems have traditionally exhibited poor cache performance and large memory footprints, therefore performing {{only at a}} fraction of their ideal execution and exhibiting low processor utilization. Previous research has studied the memory system of database management systems (DBMSs) on research-based simultaneous multithreading (SMT) processors. Recently, several differences have been noted between the real <b>hyperthreaded</b> architecture implemented by the Intel Pentium 4 and the earlier SMT research architectures. Therefore, it is important to study and analyze the performance of modern DBMSs on real SMT processors. This paper characterizes the performance of a prototype open-source DBMS running TPC-Cequivalent benchmark queries on an Intel Pentium 4 Hyper-Threading processor. We use the performance hardware counters provided by the Pentium 4 to evaluate the micro-architecture and study the memory system behavior of each query running on the data management system. Our results show a performance improvement of up to 1. 16 due to <b>hyperthreading.</b> 1...|$|R
40|$|Abstract. Array {{regrouping}} enhances program spatial locality by interleaving {{elements of}} multiple arrays {{that tend to}} be accessed closely. Its effectiveness has been systematically studied for sequential programs running on unicore processors, but not for multithreading programs on modern Chip Multiprocessor (CMP) machines. On one hand, the processor-level parallelism on CMP intensifies memory bandwidth pressure, suggesting {{the potential benefits of}} array regrouping for CMP computing. On the other hand, CMP architectures exhibit extra complexities— especially the hierarchical, heterogeneous cache sharing among <b>hyperthreads,</b> cores, and processors—that impose new challenges to array regrouping. In this work, we initiate an exploration to the new opportunities and challenges. We propose cache-sharing-aware reference affinity analysis for identifying data affinity in multithreading applications. The analysis consists of affinity-guided thread scheduling and hierarchical reference-vector merging, handles cache sharing among both <b>hyperthreads</b> and cores, and offers hints for array regrouping and the avoidance of false sharing. Preliminary experiments demonstrate the potential of the techniques in improving locality of multithreading applications on CMP with various pitfalls avoided. ...|$|R
50|$|Kernels patched {{with the}} ck1 patch set {{including}} the BFS outperformed the vanilla kernel using the CFS at {{nearly all the}} performance-based benchmarks tested. Further study with a larger test set could be conducted, but based on the small test set of 7 PCs evaluated, these increases in process queuing, efficiency/speed are, on the whole, independent of CPU type (mono, dual, quad, <b>hyperthreaded,</b> etc.), CPU architecture (32-bit and 64-bit) and of CPU multiplicity (mono or dual socket).|$|R
50|$|With the {{introduction}} of the Desktop Core i3 and Core i5 processor code named Clarkdale in January 2010, Intel also added a new Celeron line, starting with the Celeron G1101. This is the first Celeron to come with on-chip PCI Express and integrated graphics. Despite using the same Clarkdale chip as the Core i5-6xx line, it does not support Turbo Boost, <b>HyperThreading,</b> VT-d, SMT, Trusted Execution Technology or AES new instructions, and it comes with only 2 MB of third-level cache enabled.|$|R
