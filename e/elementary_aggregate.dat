10|33|Public
50|$|Most <b>elementary</b> <b>aggregate</b> {{indices are}} {{necessarily}} 'unweighted' averages {{for the sample}} of products within the sampled outlets. However, in cases where {{it is possible to}} select the sample of outlets from which prices are collected so as to reflect the shares of sales to consumers of the different outlet types covered, self-weighted <b>elementary</b> <b>aggregate</b> indices may be computed. Similarly, if the market shares of the different types of product represented by product types are known, even only approximately, the number of observed products to be priced for each of them can be made proportional to those shares.|$|E
50|$|Some of the <b>elementary</b> <b>aggregate</b> indices, {{and some}} of the sub-indices can be defined simply in terms of the types of goods and/or {{services}} they cover, {{as in the case of}} such products as newspapers in some countries and postal services, which have nationally uniform prices. But where price movements do differ or might differ between regions or between outlet types, separate regional and/or outlet-type elementary aggregates are ideally required for each detailed category of goods and services, each with its own weight. An example might be an <b>elementary</b> <b>aggregate</b> for sliced bread sold in supermarkets in the Northern region.|$|E
50|$|For some {{of these}} lower level indices {{detailed}} reweighing to make them be available, allowing computations where the individual price observations can all be weighted. This may be the case, for example, where all selling {{is in the hands}} of a single national organisation which makes its data available to the index compilers. For most lower level indices, however, the weight will consist of the sum of the weights of a number of <b>elementary</b> <b>aggregate</b> indices, each weight corresponding to its fraction of the total annual expenditure covered by the index. An 'elementary aggregate' is a lowest-level component of expenditure, one which has a weight but within which, weights of its sub-components are usually lacking. Thus, for example: Weighted averages of <b>elementary</b> <b>aggregate</b> indices (e.g. for men's shirts, raincoats, women's dresses, etc.) make up low level indices (e.g. Outer garments).|$|E
40|$|This paper {{discusses}} {{the properties of}} price- and Cost-of-Living indexes that follow from specific assumptions about the structure of consumer preferences. Of particular interest are indexes for <b>elementary</b> <b>aggregates.</b> In {{the first part of}} the paper we show how particular indexes for <b>elementary</b> <b>aggregates</b> emerge from a micro model with heterogeneous consumers and unobservable choice sets of product variants. Subsequently, we demonstrate that these indexes also follow from a particular preference structure of a representative consumer. Indexes that are currently used in many countries emerge as special cases of the ones proposed in this paper. Download Inf...|$|R
40|$|Choosing the {{appropriate}} method {{to calculate the}} exact price index is important. Use the proportional method of nature of data can reduce the error. At {{the lowest level of}} aggregation of price index quantity information is usually unavailable and matched samples of prices must be used for the index computation. Familiar indexes at this level of aggregation are those of Dutot, Carli, and Jevons. In this paper, the properties of these indices for calculating <b>elementary</b> <b>aggregates</b> have been studied of point of view various sampling methods. Key words: elementary price index; sampling survey; Aggregation, quantit...|$|R
40|$|The paper {{reviews the}} {{different}} methods {{that can be}} used to calculate elementary PPPs for basic headings. Comparisons are made with the methods used to calculate elementary indices for <b>elementary</b> <b>aggregates</b> in temporal price indices. The selection of the common list of products to be used for price collection in different countries can have a considerable impact on the results. Representative products are defined and their role in the process of drawing up the product list explained. Information about representativity can also be used to correct potential biases in the estimated elementary PPPs. The choice of index formula can have a significant effect on the results even when a complete set of prices can be collected for the products on the common list...|$|R
50|$|Infrequent reweighing saves {{costs for}} the {{national}} statistical office but delays the introduction into the index of new types of expenditure. For example, subscriptions for Internet Service entered index compilation with a considerable time lag in some countries, and account could be taken of digital camera prices between re-weightings only by including some digital cameras in the same <b>elementary</b> <b>aggregate</b> as film cameras.|$|E
50|$|The {{index is}} usually {{computed}} monthly, or quarterly in some countries, as a weighted average of sub-indices for different components of consumer expenditure, such as food, housing, shoes, clothing, {{each of which}} is in turn a weighted average of sub-sub-indices. At the most detailed level, the <b>elementary</b> <b>aggregate</b> level, (for example, men's shirts sold in department stores in San Francisco), detailed weighting information is unavailable, so indices are computed using an unweighted arithmetic or geometric mean of the prices of the sampled product offers. (However, the growing use of scanner data is gradually making weighting information available even at the most detailed level.) These indices compare prices each month with prices in the price-reference month. The weights used to combine them into the higher-level aggregates, and then into the overall index, relate to the estimated expenditures during a preceding whole year of the consumers covered by the index on the products within its scope in the area covered. Thus the index is a fixed-weight index, but rarely a true Laspeyres index, since the weight-reference period of a year and the price-reference period, usually a more recent single month, do not coincide. It takes time to assemble and process the information used for weighting which, in addition to household expenditure surveys, may include trade and tax data.|$|E
40|$|Indexes often {{incorporate}} various biases due {{to their}} methods of construction. The Constant Elasticity of Substitution (CES) index can potentially eliminate substitution bias without needing current period expenditure data. The CES index requires an elasticity parameter. We derive a system of equations from which this parameter is estimated. We find that consumers are highly responsive to price changes at the elementary aggregation level. The results {{support the use of}} a geometric rather than arithmetic mean index at the <b>elementary</b> <b>aggregate</b> level. However, we find that even the use of a geometric mean index at the <b>elementary</b> <b>aggregate</b> level may not sufficiently account for the observed level of consumer substitution. Price indexes; elasticity of substitution; scanner data...|$|E
40|$|The {{database}} means {{a collection}} of many types of occurrences of logical records containing relationships between records and data <b>elementary</b> <b>aggregates.</b> Management System database (DBMS) - a set of programs for creating and operation of a database. Theoretically, any relational DBMS {{can be used to}} store data needed by a Web server. Basically, it was observed that the simple DBMS such as Fox Pro or Access is not suitable for Web sites that are used intensively. For large-scale Web applications need high performance DBMS's able to run multiple applications simultaneously. Hyper Text Markup Language (HTML) is used to create hypertext documents for web pages. The purpose of HTML is rather the presentation of information – paragraphs, fonts, tables, than semantics description document. internet, informations, web, dates...|$|R
40|$|Conventionally, {{consumer}} price indices are constructed {{on the assumption}} that we are observing a stable system of consumer demand, and that all price movements are, therefore, the result of supply-side changes. This often leads to an emphasis on {{consumer price}} substitution and to a recommendation that we should allow for it by using the geometric mean for first-stage aggregation. This paper argues, on the basis of economic theory and from observations on the UK clothing sub-index, that demand-side changes are also important in generating price movements. For most items we are unable to solve the resulting identification problem of whether supply-side or demand-side influences predominate: in these circumstances, the appropriate formula to use for first-stage aggregation is one that makes no assumptions about the cause of price changes – i. e. one that uses an arithmetic rather than a geometric average. Allowing for both sources of price movements also affects the way in which <b>elementary</b> <b>aggregates</b> should be defined: this should be on the basis of both demand and supply characteristics, in order to minimise problems that arise when aggregating disparate products. ...|$|R
40|$|International audienceThis study {{deals with}} the use of the MACBETH {{methodology}} as a global framework for multi-criteria industrial performance expressions. This methodology satisfies the measurement theory requirements, and thus ensures the coherence of the <b>elementary</b> and <b>aggregated</b> performance expressions. The MACBETH procedure allows to express commensurate elementary performances and the weights of the weighted mean from decision-maker knowledge, and then to <b>aggregate</b> the <b>elementary</b> performances. Then, in order to take the criteria interactions into account, we extend MACBETH to the Choquet integral operators. Finally, an industrial case study is presented to illustrate the implementation of the methodology in the field...|$|R
40|$|Hedonic {{methods are}} {{currently}} considered state-of-the-art for handling quality changes when compiling consumer price indices. The present article proposes first a mathematical {{description of characteristics}} and of elementary aggregates. In a following step, a hedonic econometric model is formulated and hedonic elementary population indices are defined. These indices extend from simple indices based on some average quality to universal formulae that incorporate the full quality spectrum of the respective <b>elementary</b> <b>aggregate.</b> We emphasise that population indices are unobservable economic parameters {{that need to be}} estimated by suitable sample indices. It is shown that most of the hedonic elementary index formulae used in practice are sample versions of particular hedonic elementary population indices. Consumer price index; hedonic regression; elementary aggregate; hedonic econometric model; hedonic elementary price index...|$|E
40|$|This paper {{considers}} {{the case for}} replacing the Carli index in the Retail Prices Index for calculating price changes at the <b>elementary</b> <b>aggregate</b> level. Following Diewert (2012), we go through {{each of the three}} approaches used to select appropriate index numbers: the test, stochastic and economic approaches. In each case, we find a few areas where our conclusions differ from Diewert's. Unlike Diewert, we are not as concerned that the Carli fails the time reversibility test, but note that it fails a revised price bouncing test. We find that the stochastic approach is inapplicable at the level of elementary aggregates, where by definition quantity weights for goods are unknown. However, we argue using insights from information theory, that the economic approach can be applied at this level and moreover that it favours the use of the Jevons index. Retail Prices Inde...|$|E
40|$|The {{price level}} in the {{aggregate}} economy and, more concretely, controlling its changes, {{has become one of}} the high-priority objectives within the framework of the regional macroeconomic analysis. Its different evolution could modify the interregional capital and commercial flows, being able to cause strong shocks, and of asymmetric nature, in each economy. The first step to reach this objective is obtaining a trustworthy and comparable measurement of the inflation in the different regions to be compared. The Index Number Theory is then used to calculate Consumer Price Indexes (CPI) the regional level. The calculation of CPI is made, at least, in two phases. In the first one, Elementary Price Index is considered (EPI). In the second and later phases, these EPI are combined, along with weighting information based on household’s expenditure, to obtain CPI for different aggregation levels to the country level. As previous step to the calculation of the IPE and CPI, the set of goods and services has to be defined based on households’ consumption behaviour. These sets are grouped in layers, named elementary aggregates, based on their homogeneity of satisfying consumer’s necessities. The COICOP (Classification Of Individual Consumption by Purpose) has important implications at the time of analyzing the behaviour of the consumer within each <b>elementary</b> <b>aggregate,</b> because of a high possibility of substitution between products. Nevertheless, this possibility diminishes and can get to be null when the goods and services satisfy necessities with very different nature. Whether what is wanted it is to calculate an EPI that correctly reflects the consumer behaviour, the described homogenous character cannot be forgotten, especially if, in addition, we take into account that National Statistics Agencies have no expenditure information available for weighting purposes, only data of prices to calculate EPI. This paper is focussed on analysis of the formula used to obtain the IPE, with the limitations of available information just commented. The election of the formula for the IPE has not been widely studied in the economic literature, being the proposal by Carli in 1764 and Dutot in 1738 [extracted Reference of OIT (2003), chapter 20, pages 12 - 13] the most often used for practical purposes. Nevertheless, Fisher (1922) had already recommended not using the Carli’s formula because of the bias to the rise that it introduces [Fisher (1922), pages 29 - 30]. Throughout the 20 th century different authors has continued looking for the ideal formula extending possible approaches to the subject: the approach of Divisia, the stochastic approach, the economic approach and the axiomatic approach. The final summary of these studies can be synthesized in "Toward to Dwells Accurate Measure of The Cost of Living” by the Advisory Commission To The Study The Consumer Price Index presented in 1996. This report, also known as Boskin’s Report, suggests the use of geometric mean price indices at the <b>elementary</b> <b>aggregate</b> for the EPI, this formula is attributed to Jevons in 1983 [OIT (2003), chapter 20, pages 12 - 13]. In the present paper, we demonstrate that all usually formulas for the calculation of the IPE are incoherent with the theory of consumer behaviour, in an aggregate characterized by the high level of substitution caused by homogeneity in the consumption purpose. In addition, the formula proposed by Rodriguez, González and Rodriguez (2004), is not only superior from the axiomatic point of view, but also from the economic approach, is the only one that is able to reflect the expected consumer behaviour. ...|$|E
40|$|Demand {{estimation}} problem arises {{each time}} when there is need for forecasting of the sales volume, optimal price settlement for profit maximization, or for empirical studies {{of the market for}} demand. Because there is a relationship between price and quantity de-manded, {{it is important to understand}} the impact of pricing on sales by estimating the demand curve for the product. The current paper introduces new method of analysis of demand internal structure and compound nature depended on the contribution of various groups of customers. Direct and inverse problem of estimation of <b>elementary</b> and <b>aggregate</b> demands parameters are defined. Aggregate demand structure is represented as a multidimensional dummy variables regression model. The theoretical results are verified by means of cor-responding numerical example...|$|R
40|$|The paper {{argues for}} the use of scanner data from EPOS systems for use in the {{compilation}} of consumer price indices. A number of methods of calculating micro-indices from such data are outlined. Scanner data for colour television sets in the U. K. are used as an example. The Tornqvist chained index is used as a benchmark against which alternative formulations, including those based on representative products, can be judged, the errors often being substantial. The paper argues {{for the use}} of scanner data, illustrates methods of compiling micro-indices and points to the potential for serious errors from conventional methods. This paper is concerned with the computing of <b>elementary</b> <b>aggregates</b> for Consumer Price Indices (CPls). Its purpose is to address two neglected points in the literature. First, it will argue {{for the use of}} EPOS (Scanner) data in the compilation of CPIs. Second, it will outline and illustrate via such data how rigidities in the weighting systems can lead to serious errors. The latter has only been addressed in the compilation of CPls at a more aggregated level where weights from a, (by necessity) previous and out-of-date, Household Expenditure Survey are applied at least annually, the weights being held constant for the monthly indices between the annual rebase in December or January. The errors involved in using out-of-date annual weights have been considered in Fry and Pasharides (1986) and Silver and Ioannidis (1994). Studies relating to the use of fixed weights in the months (or in some countries, years) between rebasing are problematic due to the very lack of weights for these periods, the author being unaware of any such studies. The use of scanner data provides an opportunity to remedy this...|$|R
40|$|International audienceNowadays, despite {{widespread}} {{recognition of}} the importance of performance measurement systems (PMSs), there are some issues that require further investigations if PMSs are to be e. ective in their role of control support. This article seeks to illustrate that the Choquet integral aggregation operators can address the problem of taking interactions between performance criteria into account. But the use of this quite complex approach requires to explicitly de. ne pieces of information aimed at aiding decision makers by {{a better understanding of the}} contribution of the elementary performances to the overall one, and at better assessing the di. erent ways of improving the overall performance. In this view, indexes of e. cacy and e. ciency of the <b>elementary</b> performances <b>aggregated</b> by a 2 -additive Choquet integral are proposed and applied to a case submitted by a small and medium-sized company (SME) ...|$|R
40|$|This thesis explores whether scanner {{data can}} be used to inform Consumer Price Index (CPI) construction, with {{particular}} reference to the issues of substitution bias and choice of aggregation dimensions. The potential costs and benefits of using scanner data are reviewed. Existing estimates of substitution bias are found to show considerable variation. An Australian scanner data set is used to estimate substitution bias for six different aggregation methods and for fixed base and superlative indexes. Direct and chained indexes are also calculated. Estimates of substitution bias are found to be highly sensitive to both the method of aggregation used and whether direct or chained indexes were used. The ILO (2004) recommends the use of dissimilarity indexes to determine the issue of when to chain. This thesis provides the first empirical study of dissimilarity indexes in this context. The results indicate that dissimilarity indexes may not be sufficient to resolve the issue. A Constant Elasticity of Substitution (CES) index provides an approximate estimate of substitution-bias-free price change, without the need for current period expenditure weights. However, an elasticity parameter is needed. Two methods, referred to as the algebraic and econometric methods, were used to estimate the elasticity parameter. The econometric approach involved the estimation of a system of equations proposed by Diewert (2002 a). This system has not been estimated previously. The results show a relatively high level of substitution at the <b>elementary</b> <b>aggregate</b> level, which supports the use a Jevons index, rather than Carli or Dutot indexes, at this level. Elasticity parameter estimates were found to vary considerably across time, and statistical testing showed that elasticity parameter estimates were significantly different across estimation methods. Aggregation is an extremely important issue in the compilation of the CPI. However, little information exists about 'appropriate' aggregation methods. Aggregation is typically recommended over 'homogenous' units. An hedonic framework is used to test for item homogeneity across four supermarket chains and across all stores within each chain. This is a novel approach. The results show that treating the same good as homogenous across stores which belong to the same chain may be recommended...|$|E
40|$|International audienceYet, despite wide spread {{recognition}} {{of the importance of}} Performance Measurement Systems (PMSs), there are some issues that require further investigations if PMSs are to be effective in their role of support to control. This article seeks to illustrate that the Choquet integral aggregation operators can address the problem of taking interactions between performance criteria into account. But the use of this quite complex approach requires to explicitly define pieces of information aimed at aiding decision-maker by {{a better understanding of the}} contribution of the elementary performances into the global one, and at a better assessing of the different ways of improving the global performance. In this view, indexes of efficacy and efficiency of the <b>elementary</b> performances <b>aggregated</b> by a 2 -additive Choquet integral are proposed and applied to a case submitted by a SME company...|$|R
40|$|In this paper, {{the authors}} {{investigate}} two methodologies for synthesizing compact wind speed profiles {{by means of}} evolutionary algorithms. Such profile {{can be considered as}} input parameter in a prospective design process by optimization of a passive wind system with storage. Compact profiles are obtained by <b>aggregating</b> <b>elementary</b> patterns in order to fulfil some target indicators. The main difference between both methods presented in the paper is related to the choice of these indicators. In the first method, they are related to the storage system features while they only depend on wind features in the second...|$|R
40|$|The {{paper is}} based on the {{synthesis}} of the economic systems and processes management. It analysis the cybernetic characteristics of the economy, it defines in a system manner the market and analysis its characteristic traits and synthesizes the cybernetic characteristics of the economic agent. In {{the first part of the}} paper the systemic and cybernetic traits of the market that determines the behavior and functionality of the economic systems are identified.   In the second part are presented the economic agents, categorized in <b>elementary</b> agents and <b>aggregate</b> agents.   At the end the economic agents are shown from the systemic, cybernetic perspective...|$|R
40|$|The thesis uses {{empirical}} {{tools to}} investigate causal effects in labor economics and politics. The first chapter analyzes {{the effects of}} commercial television in Norway. Matching data on cable television networks with individual-level administrative register data, {{we find that the}} expansion of commercial television reduced ability test scores as well as high school graduation rates. We find stronger effects on sons of low-income parents and particularly large effect for children in <b>elementary</b> school. <b>Aggregate</b> data show a substantial drop in time spent reading by young people in the same period, suggesting that television watching may have crowded out more cognitively stimulating activities such as reading. This chapter is joint work with Simen Markussen and Knut Røed. The second chapter investigates how sickness absence behavior in Norwegian municipalities was affected by the terrorist attack in Norway on July 22, 2011. Using register data covering the complete Norwegian population, I find that sickness absence rates declined substantially in municipalities affected more intensely by the attack. In municipalities from which a resident was killed in the attack, sickness absence rates declined by 4 % compared t...|$|R
40|$|Of 60 TRIC agents {{isolated}} from Gambian children with trachoma, 25 were serotype 1 {{and the remainder}} type 2. There was a pronounced difference in the proportions of these types in the two villages studied. In the village with a predominance of type 2 strains, TRIC agents remained confined to 2 adjacent compounds over a 14 month observation period. All 19 type 1 strains examined were characterized by the appearance in yolk sac smears of compact <b>aggregates</b> of <b>elementary</b> bodies; such <b>aggregates</b> were seen in only 2 of 35 type 2 strains, and may reflect a chemical difference in {{the surface of the}} elementary bodies or in a substance elaborated during their replication...|$|R
40|$|Time is a {{key factor}} in management. Along a project execution, keeping the best {{completion}} rates, not too slow and not too fast, is a central objective but such a best rate cannot be suitably anticipated in a precise schedule. It must be determined on the job, on a comparative basis. This paper develops an evaluation system involving the measurement of schedule fitting indicators designed to deal with such conditions. This evaluation system is based on a transformation of the data into probabilities of reaching the frontier of best performances that permits precisely composing measurements on correlated attributes. This feature of the system allows for combining criteria evaluated on <b>elementary</b> and on <b>aggregate</b> levels...|$|R
40|$|In {{advanced}} {{stages of}} soil formation on volcanic ash, when more clay is formed, {{the combination of}} high surface area and variable charge causes strong soil aggregation, and very large porosity, water retention capacity and shrinkage (e. g. Nanzyo et al. 1993, Poulenard et al. 2002, 2003). Despite previous works, physical properties of Andosols have not been fully explored and explained. Maeda and Soma (1985), Warkentin et al. (1988), Mizota and van Reeuwijk (1993), Nanzyo et al. (1993), Pinheiro et al. (2001) and Poulenard et al. (2003) pointed out that, besides allophanes, organic colloids also influence bulk density and soil water retention. However, the interpretation by these authors are constituent-based and not physical-based. On another hand, most of the published results on hydraulic properties {{are related to the}} water retention at 1500 kPa and sometimes at 33 kPa (see the review Nanzyo et al. 1993). Rare are the published results on the effect on pedogenesis or/and land use the entire retention curve or on hydraulic conductivity as a function of soil saturation (Warkentin and Maeda 1980, Basile and De Mascellis 1999, Poulenard et al. 2003, Fernandez et al. 2004) or on the hydraulic conductivity and solute dispersivity (Katou et al. 1996, Basile and De Mascellis 1999, Fontes et al. 2004) Aggregation in many Andosols (WRB 2001) is also so strong that they fail to disperse completely when dispersion agents are used that are successful in other soils (e. g. Nanzyo et al. 1993, Mizota and van Reeuwijk 1993). Research in the areas of hydraulic properties, solute transport, aggregation and clay dispersion is therefore still required. Finally, relationships between properties at different scales, such as organo- mineral clay, physico-chemical properties, <b>elementary</b> <b>aggregates</b> and physical macro-properties (water retention, water and solute transport, soil hydrophobicity, aggregate stability, shrinkage etc.) are still unclear. Some expected relationships will be reported in the present synthesis, which is based both on studies by the authors and co-workers in the frame of the COST- 622 Action (Bartoli and Burtin, Basile et al., Buurman andvan Doesburg this book section, other published or unpublished works of co-authors of this chapter) and on a detailed analysis of literature. This chapter is organised as follows. First, micro-aggregation and clay dispersibility of the European reference pedon soil samples are reported. Second, we will highlight the key role played by capillary porosity in both soil water retention and shrinkage for the same soils. Third, irreversible drying effect on aggregate stability, hydraulic properties and solute transport parameters will be presented, followed by preliminary results on pore size distribution of the reference soil samples. Finally, recent results on (i) dielectric behaviour and water mobility, (ii) water and solute transport and (iii) soil hydrophobicity will be reported for European and other Andosols...|$|R
40|$|In this work, {{we present}} a {{methodology}} for the quantitative evaluation and comparison of Web site quality called Web-site Quality Evaluation Method (QEM). The core models and procedures for artifact evaluation are supported by the Logic Scoring of Preference (LSP) model and continuous preference logic as mathematical background. We discuss the process steps that the evaluators should follow by applying the Web-site QEM, namely: (a) Selecting a site or a set of competitive sites specific to a domain, (b) Specifying goals and the user view, (c) Specifying in a standard-compliant way, Web-site quality characteristics and attributes, (d) Defining the evaluation criterion for each attribute, and applying attribute measurement, (f) <b>Aggregating</b> <b>elementary</b> attributes to yield the global quality preference, and (g) Analyzing, assessing, and comparing partial and global outcomes. In order to illustrate the methodology we focus on a case study on typical museum sites where more than ninety compon [...] ...|$|R
40|$|International audienceSeismic {{tomography}} enables {{to model}} the internal structure of the Earth. In order to improve the precision of existing models, {{a huge amount of}} acquired seismic data must be analyzed. The analysis of such massive data require a considerable computing power which can only be delivered by parallel computational equipments. Yet, parallel computation is not sufficient for the task: we also need algorithms to automatically concentrate the computations on the most relevant data parts. The objective of the paper is to present such an algorithm. From an initial regular mesh in which cells carry data with varying relevance, we present a method to <b>aggregate</b> <b>elementary</b> cells so as to homogenize the relevance of data. The result is an irregular mesh which has the ad- vantage over the initial mesh of having orders of magnitude less cells while preserving the geophysical meaning of data. We present both a sequential and a parallel algorithm to solve this problem under the hypotheses and constraints inherited from the geophysical context...|$|R
40|$|In this report, a {{framework}} for the Policing function in the Scalable Resource Reservation Protocol (SRP) is proposed. SRP {{is characterized by a}} high scalability, soft guarantees and no a priori specifications of the traffic profile. Resources are reserved for the whole set of <b>elementary</b> stream, the <b>aggregate</b> flow. The policing function monitors the aggregate flow and performs a conformance test to detect packets that belong to conformant or non-conformant flows. If the packets in the aggregate flow exceed the expected number, the policing function has to make a selection to discard some of them. We adopt a Statistical Pattern Recognition (SPR) for the packet classification. Aim of the policing function is to limit the impact of non-conformant traffic on conformant traffic, and also to reduce usefulness of the former. We first describe the approach, then we trace a theoretical analysis and, finally, we describe the simulation carried out and the experimental results...|$|R
40|$|Seismic {{tomography}} enables {{to model}} the internal structure of the Earth. In order to improve the precision of existing models, {{a huge amount of}} acquired seismic data must be analyzed. The analysis of such massive data require a considerable computing power which can only be delivered by parallel computational equipments. Yet, parallel computation is not sufficient for the task: we also need algorithms to automatically concentrate the computations on the most relevant data parts. The objective of the paper is to present such an algorithm. From an initial regular mesh in which cells carry data with varying relevance, we present a method to <b>aggregate</b> <b>elementary</b> cells so as to homogenize the relevance of data. The result is an irregular mesh which has the advantage over the initial mesh of having orders of magnitude less cells while preserving the geophysical meaning of data. We present both a sequential and a parallel algorithm to solve this problem under the hypotheses and constraints inherited from the geophysical context. 1...|$|R
40|$|Abstract – The {{building}} block of Service Oriented Architecture is web services. Web services are self contained, self describing, modular applications {{that can be}} published, located and invoked across the web. Composite services are built by <b>aggregating</b> <b>elementary</b> services by a process called web service composition. Web service composition can be classified into manual and automatic types. The first type involves design time composition when the architecture and design of the software system is planned. The second type automatic service composition involves composing web services automatically to facilitate ease of composition process for engineers. Automatic composition is generally guided by dynamic composition to handle non determinism at run time. Automatic composition can be materialized as an artificial intelligence planning problem. Planning represents {{a part of the}} world as states and changes in those states. Business agility is the ability of business to adapt efficiently and cost effectively in response to the changes in the business environment. The changes in business environment are temporal. In this paper we have devised a new algorithm called Opus deviser which targets to achieve business agility through web service composition that is temporally planned using PERT Network analysis...|$|R
40|$|International audienceIndustrial {{companies}} have to continuously improve their performance, which is {{defined in terms of}} numerous and multi-level criteria. According to the Deming wheel principle, decision-makers need to be provided with sound and pragmatic improvement methodologies, supplying operational tools for the objectives' definition, the action choice and the performance expression. Based on the generic PETRA methodology and using an established performance model that expresses overall performance by <b>aggregating</b> <b>elementary</b> ones, this article is a contribution to the decision-makers' information needs. By considering performance as a satisfaction degree of a given objective, we particularly focus, here, on the improvement of an overall performance, and thus on the choice of the right opportunity viewed as the best subset of relevant actions among the set of potential actions. Therefore the notion of action impact on the elementary performances is defined. Then the problem of the impact of a subset of actions on the elementary performances is posed and several ways are proposed to deal with it. Lastly the subset of actions impact on the overall performance is defined by a performance aggregation model. The proposed approach is applied to a case study submitted by an automation component manufacturer...|$|R
40|$|The aim of {{this paper}} is to propose a class of {{composite}} indicators for measuring well-being at the local level, which takes into account the variability between and within the local units. Although we believe that well-being is a multidimensional concept and cannot be reduced to a single measure, we also stress the importance of aggregating the information of several well-being indicators into a reduced number of composite indicators, one for each well-being domain, which play a crucial role in policymaking and benchmarking. As an application we focus on the Equitable and Sustainable Well-being of the Italian Provinces. In particular, based on a dataset containing 41 <b>elementary</b> indicators, we <b>aggregate</b> them by domain, comparing different aggregative approaches and illustrating the difference in the rankings of the Italian Provinces they produce. Finally, as an illustrative example, we focus on a member of the class of composite indicators that accounts both for vertical variability, which is the between component and horizontal variability, which is the within component. The construction of a composite index for each domain allows us to evaluate and compare multidimensional well-being among the Italian Provinces...|$|R
40|$|Defence date: 28 May 2015 Examining Board: Professor Andrea Mattozzi, Supervisor, EUI; Professor Andrea Ichino, EUI; Professor Peter Fredriksson, Stockholm University; Professor Tarjei Havnes, University of Oslo. Abstract	The {{first chapter}} {{analyzes}} {{the effects of}} commercial television in Norway. Matching data on cable television networks with individual-level administrative register data, {{we find that the}} expansion of commercial television reduced ability test scores as well as high school graduation rates. We find stronger effects on sons of low-income parents and particularly large effect for children in <b>elementary</b> school. <b>Aggregate</b> data show a substantial drop in time spent reading by young people in the same period, suggesting that television watching may have crowded out more cognitively stimulating activities such as reading. This chapter is joint work with Simen Markussen and Knut Røed. The second chapter investigates how sickness absence behaviour in Norwegian municipalities was affected by the terrorist attack in Norway on July 22, 2011. Using register data covering the complete Norwegian population, I find that sickness absence rates declined substantially in municipalities affected more intensely by the attack. In municipalities from which a resident was killed in the attack, sickness absence rates declined by 4 % compared to municipalities without victims. The effect is precisely estimated, stable across several challenging specifications, and persists for {{as long as there is}} available data. The effect for people in their 20 's is more than twice that for the population at large – for this group, local exposure to the attack decreased absence rates by around 10 %. The third chapter exploits a natural experiment in the Norwegian political system. I find that obtaining the right to vote at a lower age is associated with substantially higher turnout among first-time voters, and that this is driven by parental influence. Counter to conventional wisdom about the habitual nature of voting, this difference in political participation does not persist for subsequent elections...|$|R
40|$|Oxidation of the B 880 antenna holochrome {{gives rise}} to a 3. 8 -G {{linewidth}} electron paramagnetic resonance (EPR) signal that is considerably narrower than the 13 -G signal of monomeric bacteriochlorophyll (Bchl) cation. Radiation inactivation was used to verify a model according to which this linewidth narrowing is due to delocalization over several Bchl molecules. Chromatophores of the photoreaction centerless mutant F 24 of Rhodospirillum rubrum were subjected to different doses of gamma-radiation. This induced not only a decay of the EPR signal amplitude but also its linewidth broadening. According to target theory, the induced amplitude decay of the EPR signal had a target size of 10. 5 kDa. This is attributed to an elementary structure (alpha 1 beta 1 Bchl 2), whose number in the membrane would limit the rate of encounter with ferricyanide and thus the formation of unpaired spins. We applied Bernoulli statistics to predict, for a given survival probability of the signal, the number of surviving <b>elementary</b> structures in <b>aggregates</b> of (alpha 1 beta 1 Bchl 2) n where n was varied from 4 to 7. Using an equation that predicted the Bchl special pair in the photo-reaction center, we were able to simulate the observed relationship between the EPR linewidth and the dose of radiation. The best fit was obtained with a hexameric structure alpha 1 beta 1 Bchl 2) 6...|$|R
40|$|Software system {{grows in}} size and {{complexity}} as it evolves over time. The fact that object-oriented software is increasingly developed using an evolutionary development process makes the situation even worse. The developers face increasing difficulties in comprehending the system design and its rapid evolution, since {{the amount of information}} is overwhelming. Traditional top-down approach to software evolution understanding does not work very well to precisely capture the changes and their underlying motivations. In this paper, we present our bottom-up design-evolution analysis approach, implemented in the JDEvAn tool. The JDEvAn tool has been equipped with a suite of longitudinal and datamining analysis methods and a set of change-pattern detection queries to automatically recover the interesting core evolution concerns, such as sets of co-evolving classes or instances of refactorings, by <b>aggregating</b> <b>elementary</b> design changes into composite concerns. Given the key participants of an evolution concern, the JDEvAn Viewer allows developers to interactively explore the relevant elements, relations, and their changes over time so that they can incrementally build up their knowledge about what has been changed, how and why. We evaluate the effectiveness of JDEvAn with two case studies on realistic open-source objectoriented software, in the context of which we show how JDEvAn help us capture the completely different rationale for two pairs of seemingly similar evolution concerns. ...|$|R
