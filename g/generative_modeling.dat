184|5077|Public
5000|$|Grasshopper 3d, a <b>generative</b> <b>modeling</b> {{interface}} for Rhinoceros 3D ...|$|E
50|$|Most speech {{recognition}} researchers {{moved away from}} neural nets to pursue <b>generative</b> <b>modeling.</b> An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI conducted research on deep neural networks in speech and speaker recognition. Heck's speaker recognition team achieved the first significant success with deep neural networks in speech processing as demonstrated in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation and later {{published in the journal}} of Speech Communication. While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in {{speech recognition}}. One decade later, Hinton and Deng collaborated with each other and then with colleagues across groups at University of Toronto, Microsoft, Google and IBM, igniting a renaissance of deep feedforward neural networks in speech recognition.|$|E
50|$|In {{the long}} history of speech recognition, both shallow form and deep form (e.g. {{recurrent}} nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue <b>generative</b> <b>modeling</b> approaches until the recent resurgence of deep learning starting around 2009-2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.|$|E
40|$|Deep <b>generative</b> <b>models</b> {{and their}} {{associated}} top-down architecture are gaining popularity in neuroscience and computer vision. In this paper we link our previous work with regulatory feedback networks to <b>generative</b> <b>models.</b> We show that <b>generative</b> <b>model’s</b> and regulatory feedback model’s equations can {{share the same}} fixed points. Thus, phenomena observed using regulatory feedback can also apply to <b>generative</b> <b>models.</b> This suggests that <b>generative</b> <b>models</b> can also be developed to identify mixtures of patterns, address problems associated with binding, and display the ability to estimate numerosity...|$|R
30|$|Similar to the LNFA model, the {{configuration}} model {{belongs to the}} wider class of network <b>generative</b> <b>models.</b> A <b>generative</b> <b>model</b> allows us to choose parameters and draw a single instance of a network. Since a single <b>generative</b> <b>model</b> can generate many instances of networks, the model itself corresponds to an ensemble of networks.|$|R
30|$|<b>Generative</b> <b>models</b> – the aim {{of these}} models is to {{generate}} a snapshot of a topology. Among <b>generative</b> <b>models,</b> some are static (time independent topologies) while some use growth (and other mechanisms). Furthermore, some <b>generative</b> <b>models</b> include predefined global properties (such as degree distribution, hierarchy and modularity) while others predefine a local property (such as the attachment probability).|$|R
50|$|Procedural {{modeling}} is {{an umbrella}} {{term for a}} number of techniques in computer graphics to create 3D models and textures from sets of rules. L-Systems, fractals, and <b>generative</b> <b>modeling</b> are procedural modeling techniques since they apply algorithms for producing scenes. The set of rules may either be embedded into the algorithm, configurable by parameters, or the set of rules is separate from the evaluation engine. The output is called procedural content, which can be used in computer games, films, be uploaded to the internet, or the user may edit the content manually. Procedural models often exhibit database amplification, meaning that large scenes can be generated from a much smaller amount of rules. If the employed algorithm produces the same output every time, the output need not be stored. Often, it suffices to start the algorithm with the same random seed to achieve this. Although all modeling techniques on a computer require algorithms to manage and store data at some point, procedural modeling focuses on creating a model from a rule set, rather than editing the model via user input. Procedural modeling is often applied when it would be too cumbersome to create a 3D model using generic 3D modelers, or when more specialized tools are required. This is often the case for plants, architecture or landscapes.|$|E
40|$|One {{approach}} {{to perform the}} design for production of masonry with BIM tools is to explicitly represent the wall elements. However, this strategy decreases the application performance because of model complexity. <b>Generative</b> <b>Modeling</b> is a modeling paradigm aimed at defining rules and applying them for generating models. The masonry block placement {{is associated with a}} set of rules, making the use of <b>generative</b> <b>modeling</b> a viable solution to implicitly represent the blocks. This paper presents some preliminary results of our research on investigating if <b>Generative</b> <b>Modeling</b> can help in the representation of masonry modulation and for the proposal of a shape grammar for representing its basic elements...|$|E
40|$|Abstract. Automated {{evaluation}} {{is crucial in}} the context of automated text summaries, {{as is the case with}} evaluation of any of the language technologies. In this paper we present a <b>Generative</b> <b>Modeling</b> framework for evaluation of content of summaries. We used two simple alternatives to identifying signature-terms from the reference summaries based on model consistency and Parts-Of-Speech (POS) features. By using a <b>Generative</b> <b>Modeling</b> approach we capture the sentence level presence of these signature-terms in peer summaries. We show that parts-of-speech such as noun and verb, give simple and robust method to signatureterm identification for the <b>Generative</b> <b>Modeling</b> approach. We also show that having a large set of ‘significant signature-terms ’ is better than a small set of ‘strong signature-terms ’ for our approach. Our results show that the <b>generative</b> <b>modeling</b> approach is indeed promising — providing high correlations with manual evaluations — and further investigation of signature-term identification methods would obtain further better results. The efficacy of the approach can be seen from its ability to capture ‘overall responsiveness ’ much better than the state-of-the-art in distinguishing a human from a system. ...|$|E
50|$|<b>Generative</b> <b>models</b> {{are used}} in machine {{learning}} for either modeling data directly (i.e., modeling observations drawn from a probability density function), or as an intermediate step to forming a conditional probability density function. <b>Generative</b> <b>models</b> are typically probabilistic, specifying a joint probability distribution over observation and target (label) values. A conditional distribution can be formed from a <b>generative</b> <b>model</b> through Bayes' rule.|$|R
40|$|We {{present a}} general {{framework}} for dependency parsing of Italian sentences {{based on a}} combination of discriminative and <b>generative</b> <b>models.</b> We use a state-of-the-art discriminative model to obtain a k-best list of candidate structures for the test sentences, and use the <b>generative</b> <b>model</b> to compute the probability of each candidate, and select the most probable one. We present the details of the specific <b>generative</b> <b>model</b> we have employed for the EVALITA' 09 task. Results show that by using the <b>generative</b> <b>model</b> we gain around 1 % in labeled accuracy (around 7 % error reduction) over the discriminative model...|$|R
40|$|This paper {{describes}} an incremental parsing approach where parameters are estimated using {{a variant of}} the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing <b>generative</b> <b>model</b> (Roark, 2001 a), and experimental results show that it gives competitive performance to the <b>generative</b> <b>model</b> on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the <b>generative</b> <b>model</b> during search provides a 2. 1 percent F-measure improvement over the <b>generative</b> <b>model</b> alone, to 88. 8 percent. ...|$|R
40|$|<b>Generative</b> <b>modeling</b> {{techniques}} are being rapidly {{developed in the}} field of deep learning, and they have been applied to topology optimization. The variational autoencoder (VAE) is a <b>generative</b> <b>modeling</b> technology that extends the autoencoder to generate new images with a limited latent space. We modified the basic VAE structure to encode optimization conditions and decode latent variables for topology optimization design. The modified VAE could efficiently predict the optimized structure after topology optimization. However, it was difficult to train the neural network to predict a very detailed structure. The generative adversarial network (GAN) is another <b>generative</b> <b>modeling</b> technique for generating images and is implemented by a system of two neural networks competing with each other to make realistic synthetic data. We applied GAN to obtain more detailed optimized results from the original design. The proposed methodology of using two <b>generative</b> <b>modeling</b> techniques for deep learning is expected to significantly increase the efficiency of topology optimization. Comment: 21 page, 12 figures, The paper is under review to be published in the Structural and Multidisciplinary Optimization journal, Springe...|$|E
40|$|We study Bayesian discriminative {{inference}} given a model family p(c, x, θ) that {{is assumed}} to contain all our prior information but still known to be incorrect. This falls in between “standard ” Bayesian <b>generative</b> <b>modeling</b> and Bayesian regression, where the margin p(x, θ) {{is known to be}} uninformative about p(c|x, θ). We give an axiomatic proof that discriminative posterior is consistent for conditional inference; using the discriminative posterior is standard practice in classical Bayesian regression, but we show that it is theoretically justified for model families of joint densities as well. A practical benefit compared to Bayesian regression is that the standard methods of handling missing values in <b>generative</b> <b>modeling</b> can be extended into discriminative inference, which is useful if the amount of data is small. Compared to standard <b>generative</b> <b>modeling,</b> discriminative posterior results in better conditional inference if the model family is incorrect. If the model family contains also the true model, the discriminative posterior gives the same result as standard Bayesian <b>generative</b> <b>modeling.</b> Practical computation is done with Markov chain Monte Carlo. ...|$|E
40|$|Within {{the last}} few years <b>generative</b> <b>modeling</b> {{techniques}} have gained attention especially in the context of cultural heritage. As a generative model describes a rather ideal object than a real one, generative techniques are a basis for object description and classification. This procedural knowledge differs from other kinds of knowledge, such as declarative knowledge, in a significant way. It can be applied to a task. This similarity to algorithms is reflected in the way generative models are designed: they are programmed. In order to make <b>generative</b> <b>modeling</b> accessible to cultural heritage experts, we created a <b>generative</b> <b>modeling</b> framework which accounts for their special needs. The result is a generative modeler ([URL] based on an easy-to-use scripting language (JavaScript). The generative model meets the demands on documentation standards and fulfils sustainability conditions. Its integrated meta-modeler approach makes it independent from hardware, software and platform...|$|E
40|$|Predictive Processing (PP) [1], [2], [3] is {{becoming}} an influential account in cognitive neuroscience, including developmental neuroscience [4]. According to PP, human brains interpret their sensory inputs by predicting them, based on a hierarchy of <b>generative</b> <b>models.</b> These predictions are then compared to the actual, observed inputs, {{and the difference between}} predictions and observations (so-called prediction error) is used to update the agent's <b>generative</b> <b>model</b> about the world, to minimize future prediction errors. A key question for PP is how situated agents can learn these <b>generative</b> <b>models.</b> This question is especially important from a developmental perspective on PP. That is, the theory needs to specify how such <b>generative</b> <b>models</b> can be 'developable' at all, given that infants must somehow build these <b>generative</b> <b>models</b> from embodied, situated interaction with their environments...|$|R
40|$|Competitive <b>generative</b> <b>models</b> with {{structure}} {{learning for}} NLP classification tasks In this paper {{we show that}} <b>generative</b> <b>models</b> are competitive with and sometimes superior to discriminative models, when both kinds of models are allowed to learn structures that are optimal for discrimination. In particular, we compare Bayesian Networks and Conditional loglinear models on two NLP tasks. We observe that when {{the structure of the}} <b>generative</b> <b>model</b> encodes very strong independence assumptions (a la Naive Bayes), a discriminative model is superior, but when the <b>generative</b> <b>model</b> is allowed to weaken these independence assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for <b>generative</b> <b>models</b> is far more efficient, they may be preferable for some tasks. ...|$|R
40|$|We {{propose a}} {{framework}} for dependency parsing based {{on a combination of}} discriminative and <b>generative</b> <b>models.</b> We use a discriminative model to obtain a k-best list of candidate parses, and subsequently rerank those candidates using a <b>generative</b> <b>model.</b> We show how this approach allows us to evaluate a variety of <b>generative</b> <b>models,</b> without needing different parser implementations. Moreover, we present empirical results that show a small improvement over state-of-the-art dependency parsing of English sentences. ...|$|R
40|$|Model Integrated Computing employs domainspecific {{modeling}} languages for {{the design}} of Computer Based Systems and automatically generates their implementation. These system models are declarative in nature. However, for complex systems with regular structure, as well as for adaptive systems, a more algorithmic approach is better suited. <b>Generative</b> <b>modeling</b> employs architectural parameters and generator scripts to specify model structure. This paper describes an approach that enables the addition of <b>generative</b> <b>modeling</b> capabilities to any domain-specific modeling language using metamodel composition. The approach is illustrated through an image processing application using the Generic Modeling Environment (GME). 1...|$|E
40|$|The paper {{shows how}} {{mechanisms}} of existing modeling languages (exemplified by UML 2. 0) support the direct modeling of variability in software product lines/system families, and identifies where <b>generative</b> <b>modeling</b> (similar to generative programming) should be applied. Existing mechanisms {{are not only}} well-known mechanisms like composition, specialization, and generics/templates, but also less known (but in fact existing) mechanisms like subsetting and constraining parts of a system. Such mechanisms are useful for configuring specific system models based upon a system family model. <b>Generative</b> <b>modeling</b> {{turns out to be}} useful in cases where there are several orthogonal dimensions of variability. 1...|$|E
40|$|The idea of <b>generative</b> <b>modeling</b> is {{to allow}} the {{generation}} of highly complex objects based {{on a set of}} formal construction rules. Using these construction rules, a shape is described by a sequence of processing steps, rather than just by the result of all applied operations: Shape design becomes rule design. Due to its very general nature, this approach can be applied to any domain and to any shape representation that provides a set of generating functions. The aim of this report is to give an overview of the concepts and techniques of procedural and <b>generative</b> <b>modeling</b> as well as their applications with a special focus on Archaeology and Architecture...|$|E
30|$|Here we {{demonstrate}} {{the possibility of}} using deep <b>generative</b> <b>models</b> to synthesize samples of the cosmic web. Deep <b>generative</b> <b>models</b> (Kingma and Welling 2014; Goodfellow et al. 2014) are able to learn complex distributions from a given set of data, and then generate new, statistically consistent data samples. Such a deep <b>generative</b> <b>model</b> can be trained on a set of N-body simulations. Once the training is complete, the <b>generative</b> <b>model</b> can create new, random dark matter distributions that are uncorrelated to the training examples. A practical advantage of using a <b>generative</b> <b>model</b> is that the generation process is extremely fast, thus giving us the ability to generate a virtually unlimited number of samples of the cosmic web. Having access to such a large amount of simulations can potentially enable more reliable scientific studies and would therefore enhance our ability to understand the physics of the Universe.|$|R
5000|$|... is used. A <b>generative</b> <b>model,</b> as {{the name}} suggests, allows one to {{generate}} new data points [...] The joint distribution is described as [...] To train a <b>generative</b> <b>model,</b> the following equation is used to maximize the joint probability [...]|$|R
40|$|<b>Generative</b> <b>models,</b> {{which can}} {{generate}} bursty error sequences with similar burst error statistics {{to those of}} descriptive models, have an immense impact on the wireless communications industry as they can significantly reduce the computational time of simulating wireless communication links. Adaptive <b>generative</b> <b>models</b> aim to produce any error sequences with any given signal-to-noise ratios (SNRs) by using only two reference error sequences obtained from a reference transmission system with two different SNRs. Compared with traditional <b>generative</b> <b>models,</b> this adaptive technique can further considerably reduce the computational load of generating new error sequences {{as there is no}} need to simulate the whole reference transmission system again. In this paper, reference error sequences are provided by computer simulations of a long term evolution (LTE) system. Adaptive <b>generative</b> <b>models</b> are developed from three widely used <b>generative</b> <b>models,</b> namely, the simplified Fritchman model (SFM), the Baum-Welch based hidden Markov model (BWHMM), and the deterministic process based <b>generative</b> <b>model</b> (DPBGM). It is demonstrated that the adaptive DPBGM can provide accurate burst error statistics and bit error rate (BER) performance of the LTE system, while the adaptive SFM and adaptive BWHMM fail to do so...|$|R
40|$|We outline a {{structured}} speech model, {{as a special}} and perhaps extreme form of probabilistic <b>generative</b> <b>modeling.</b> The model is equipped with long-contextual-span capabilities that are missing in the HMM approach. Compact (and physically meaningful) parameterization of the model is {{made possible by the}} continuity constraint in the hidden vocal tract resonance (VTR) domain. The target-directed VTR dynamics jointly characterize coarticulation and incomplete articulation (reduction). Preliminary evaluation results are presented on the standard TIMIT phonetic recognition task, showing the best result in this task reported in the literature without using many heterogeneous classifier combinations. The pros and cons of our structured <b>generative</b> <b>modeling</b> approach, in comparison with the structured discriminative classification approach, are discussed. ...|$|E
40|$|Procedural {{modeling}} is {{a technique}} to describe 3 D objects by a constructive, generative description. In order to tap the full potential of this technique the content creator needs {{to be familiar with}} two worlds - procedural modeling techniques and computer graphics on the one hand as well as domain-specific expertise and specialized knowledge on the other hand. This article presents a JavaScript-based approach to combine both worlds. It describes a modeling tool for <b>generative</b> <b>modeling</b> whose target audience consists of beginners and intermediate learners of procedural modeling techniques. Our approach will be beneficial in various contexts. JavaScript is a wide-spread, easy-to-use language. With our tool procedural models can be translated from JavaScript to various <b>generative</b> <b>modeling</b> and rendering systems...|$|E
40|$|In {{the context}} of {{computer-aided}} design, computer graphics and geometry processing, the idea of <b>generative</b> <b>modeling</b> is to allow the generation of highly complex objects based {{on a set of}} formal construction rules. Using these construction rules, a shape is described by a sequence of processing steps, rather than just by the result of all applied operations: shape design becomes rule design. Due to its very general nature, this approach can be applied to any domain and to any shape representation that provides a set of generating functions. The aim of this survey is to give an overview of the concepts and techniques of procedural and <b>generative</b> <b>modeling,</b> as well as their applications with a special focus on archeology and architecture...|$|E
40|$|We {{propose a}} novel hybrid model that {{exploits}} {{the strength of}} discriminative classifiers along with the rep-resentational power of <b>generative</b> <b>models.</b> Our focus is on detecting multimodal events in time varying sequences. Discriminative classifiers {{have been shown to}} achieve higher performances than the corresponding generative likelihood-based classifiers. On the other hand, <b>generative</b> <b>models</b> learn a rich informative space which allows for data generation and joint feature representation that discrimina-tive models lack. We employ a deep temporal <b>generative</b> <b>model</b> for unsupervised learning of a shared representation across multiple modalities with time varying data. The tem-poral <b>generative</b> <b>model</b> takes into account short term tem-poral phenomena and allows for filling in missing data by generating data within or across modalities. The hybrid model involves augmenting the temporal <b>generative</b> <b>model</b> with a temporal discriminative model for event detection, and classification, which enables modeling long range tem-poral dynamics. We evaluate our approach on audio-visual datasets (AVEC, AVLetters, and CUAVE) and demonstrate its superiority compared to the state-of-the-art. 1...|$|R
40|$|<b>Generative</b> <b>models</b> {{for deep}} {{learning}} are promising both to improve {{understanding of the}} model, and yield training methods requiring fewer labeled samples. Recent works use <b>generative</b> <b>model</b> approaches to produce the deep net's input given {{the value of a}} hidden layer several levels above. However, there is no accompanying "proof of correctness" for the <b>generative</b> <b>model,</b> showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated. The current paper takes a more theoretical tack. It presents a very simple <b>generative</b> <b>model</b> for RELU deep nets, with the following characteristics: (i) The <b>generative</b> <b>model</b> is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.) (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption [...] -which is experimentally tested on real-life nets like AlexNet [...] - it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. The <b>generative</b> <b>model</b> suggests a simple modification for training: use the <b>generative</b> <b>model</b> to produce synthetic data with labels and include it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training...|$|R
40|$|Abstract—This paper {{presents}} a <b>generative</b> <b>model</b> for burst {{characterization of the}} underlying error profiles obtained from the Narrowband Indoor Powerline Channel. Using error sequences measured from the transmission link, a <b>generative</b> <b>model</b> that produces error sequences of any length, with similar relevant statistics, is obtained...|$|R
40|$|This {{tutorial}} {{introduces the}} concepts and techniques of <b>generative</b> <b>modeling.</b> It starts with some introductory {{examples in the}} first learning unit to motivate the main idea: to describe a shape using an algorithm. After the explanation of technical terms, the second unit focuses on technical details of algorithm descriptions, programming languages, grammars and compiler construction, which {{play an important role}} in <b>generative</b> <b>modeling.</b> The purely geometric aspects are covered by the third learning unit. It comprehends the concepts of geometric building blocks and advanced modeling operations. Notes on semantic modeling aspects i. e. the meaning of a shape complete this unit and introduce the inverse problem. What is the perfect generative description for a real object? The answer to this question is discussed in the fourth learning unit while its application is shown (among other applications of generative and inverse-generative modeling) in the fifth unit. The discussion of open research questions concludes this tutorial. The assumed background knowledge of the audience comprehends basics of computer science (including algorithm design and the principles of programming languages) as well as a general knowledge of computer graphics. The tutorial takes approximately 120 min. and enables the attendees to take an active part in future research on <b>generative</b> <b>modeling...</b>|$|E
40|$|Abstract—In {{the context}} of {{computer}} graphics, a generative model is the description of a three-dimensional shape: Each class of objects is represented by one algorithm M. Further-more, each described object {{is a set of}} high-level parameters x, which reproduces the object, if an interpreter evaluates M(x). This procedural knowledge differs from other kinds of knowledge, such as declarative knowledge, in a significant way. Generative models are designed by programming. In order to make <b>generative</b> <b>modeling</b> accessible to non-computer scien-tists, we created a <b>generative</b> <b>modeling</b> framework based on the easy-to-use scripting language JavaScript (JS). Furthermore, we did not implement yet another interpreter, but a JS-translator and compiler. As a consequence, our framework can translate generative models from JavaScript to various platforms. In this paper we present an overview of Euclides and quintessentia...|$|E
40|$|PRISM, a symbolic-statistical {{modeling}} language we {{have been}} developing since 2 ̆ 797, recently incorporated a program transformation technique to handle failure in <b>generative</b> <b>modeling.</b> I 2 ̆ 7 ll show this feature opens a way to new breeds of symbolic models, including EM learning from negative observations, constrained HMMs and finite PCFGs...|$|E
40|$|In {{this paper}} {{we show that}} <b>generative</b> <b>models</b> are {{competitive}} with and sometimes superior to discriminative models, when both kinds of models are allowed to learn structures that are optimal for discrimination. In particular, we compare Bayesian Networks and Conditional loglinear models on two NLP tasks. We observe that when {{the structure of the}} <b>generative</b> <b>model</b> encodes very strong independence assumptions (a la Naive Bayes), a discriminative model is superior, but when the <b>generative</b> <b>model</b> is allowed to weaken these independence assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for <b>generative</b> <b>models</b> is far more efficient, they may be preferable for some tasks...|$|R
5000|$|So, {{the number}} of {{parameters}} is equal to [...] The number of parameters grows linearly with {{the number of}} documents. In addition, although PLSA is a <b>generative</b> <b>model</b> of the documents in the collection it is estimated on, it is not a <b>generative</b> <b>model</b> of new documents.|$|R
5000|$|... #Subtitle level 2: <b>Generative</b> <b>models</b> vs. discriminative models ...|$|R
