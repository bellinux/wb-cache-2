883|176|Public
25|$|Inverse {{transform}} sampling (also {{known as}} inversion sampling, the <b>inverse</b> <b>probability</b> integral transform, the inverse transformation method, Smirnov transform, golden rule) {{is a basic}} method for pseudo-random number sampling, i.e. for generating sample numbers at random from any probability distribution given its cumulative distribution function.|$|E
25|$|In words, {{you want}} to protect 1 units of {{inventory}} for the higher valued segment where 1 {{is equal to the}} <b>inverse</b> <b>probability</b> of demand of the revenue ratio of the lower valued segment to the higher valued segment. This equation defines the EMSRa algorithm which handles the two segment case. EMSRb is smarter and handles multiple segments by comparing the revenue of the lower segment to a demand weighted average of the revenues of the higher segments. Neither of these heuristics produces the exact right answer and increasingly implementations make use of Monte Carlo simulation to find optimal protection levels.|$|E
2500|$|The {{probability}} integral transform states that if [...] is a {{continuous random variable}} with cumulative distribution function , then the random variable [...] has a uniform distribution on [...] The <b>inverse</b> <b>probability</b> integral transform is just the inverse of this: specifically, if [...] has a uniform distribution on [...] and if [...] has a cumulative distribution [...] , then the random variable [...] has the same distribution as [...]|$|E
5000|$|The {{use of the}} {{representativeness}} heuristic {{will likely}} lead to violations of Bayes' Theorem.Bayes' Theorem states:However, judgments by representativeness only look at the resemblance between the hypothesis and the data, thus <b>inverse</b> <b>probabilities</b> are equated: ...|$|R
40|$|This paper derives a Murthy’s {{unbiased}} estimator of population total under unequal <b>probability</b> <b>inverse</b> sampling. A general unequal <b>probability</b> <b>inverse</b> sampling {{is combined with}} adaptive cluster sampling. An {{unbiased estimator}} of population total and its variance estimator are given using Murthy’s approach. The general unequal <b>probability</b> <b>inverse</b> adaptive cluster sampling and general equal <b>probability</b> <b>inverse</b> adaptive cluster sampling are compared using simulation study based on real life data. The {{results indicate that the}} general unequal <b>probability</b> <b>inverse</b> adaptive cluster sampling has a small coefficient of variation for estimates compared to equal <b>probability</b> <b>inverse</b> adaptive cluster sampling. When the coefficients of correlation between study variable and probability of selection units increase, the coefficient of variation decreases. </p...|$|R
25|$|The role of Bayes’ theorem is best visualized with tree {{diagrams}}, {{as shown}} to the right. The two diagrams partition the same outcomes by A and B in opposite orders, to obtain the <b>inverse</b> <b>probabilities.</b> Bayes’ theorem serves as the link between these different partitionings.|$|R
2500|$|In {{probability}} theory and applications, Bayes' rule relates {{the odds of}} event [...] to event , before (prior to) and after (posterior to) conditioning on another event [...] The odds on [...] to event [...] is simply {{the ratio of the}} probabilities of the two events. When arbitrarily many events [...] are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, [...] where the proportionality symbol means that the left hand side is proportional to [...] (i.e., equals a constant times) the right hand side as [...] varies, for fixed or given [...] (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005). See <b>Inverse</b> <b>probability</b> and Bayes' rule.|$|E
50|$|An {{alternative}} estimator is the augmented <b>inverse</b> <b>probability</b> weighted estimator (AIPWE) combines {{both the}} properties of the regression based estimator and the <b>inverse</b> <b>probability</b> weighted estimator. It is therefore a 'doubly robust' method in that it only requires either the propensity or outcome model to be correctly specified but not both. This method augments the IPWE to reduce variability and improve estimate efficiency. This model holds the same assumptions as the <b>Inverse</b> <b>Probability</b> Weighted Estimator (IPWE).|$|E
5000|$|... #Subtitle level 2: <b>Inverse</b> <b>Probability</b> Weighted Estimator (IPWE) ...|$|E
40|$|People have {{difficulty}} reasoning with diagnostic information in uncertain situations, especially when an understanding and calculation of <b>inverse</b> conditional <b>probabilities</b> (Bayes theorem) is required. While natural frequency representations of inference tasks improve matters, they suffer from three problems: (1) calculation errors persist {{with a majority}} of subjects; (2) the representation suffers from an illusion of certainty that ignores ambiguity; and (3) the costs of repeatedly applying the representation to deal with imprecision and ambiguity in inference are prohibitive. We describe a user friendly, interactive, graphical software tool for calculating, visualizing, and communicating accurate inferences about uncertain states when relevant diagnostic test information (sensitivity, specificity, and base rate) is both imperfect and ambiguous in its application to a specific patient. The software is free, open-source, and runs on all popular PC operating systems (Windows, Mac, Linux). ambiguity, subjective expected utility, certainty illusion, <b>inverse</b> <b>probabilities,</b> choice under uncertainty, natural frequencies,...|$|R
40|$|A {{partially}} {{linear model}} is considered when the responses are missing at random. Imputation, semiparametric regression surrogate and <b>inverse</b> marginal <b>probability</b> weighted approaches are developed {{to estimate the}} regression coefficients and the nonparametric function, respectively. All the proposed estimators for the regression coefficients are shown to be asymptotically normal, and the estimators for the nonparametric function are proved to converge at an optimal rate. A simulation study is conducted to compare the finite sample behavior of the proposed estimators. Imputation estimator Regression surrogate estimator <b>Inverse</b> marginal <b>probability</b> weighted estimator Asymptotic normality...|$|R
40|$|In judging {{posterior}} probabilities, {{people often}} answer with the inverse conditional probabilityï¿½a tendency named the inverse fallacy. Participants (N = 45) {{were given a}} series of probability problems that entailed estimating both p(H|D) and p(~H|D). The findings revealed that deviations of participants' estimates from Bayesian calculations and from the additivity principle could be predicted by the corresponding deviations of the <b>inverse</b> <b>probabilities</b> from these relevant normative benchmarks. Methodological and theoretical implications {{of the distinction between}} inverse fallacy and base-rate neglect and the generalization of the study of additivity to conditional probabilities are discussed...|$|R
50|$|<b>Inverse</b> <b>probability</b> {{weighting}} is {{also used}} to account for missing data when subjects with missing data cannot be included in the primary analysis.With an estimate of the sampling probability, or the probability that the factor would be measured in another measurement, <b>inverse</b> <b>probability</b> weighting can be used to inflate the weight for subjects who are under-represented due to a large degree of missing data.|$|E
50|$|In {{probability}} theory, <b>inverse</b> <b>probability</b> is an obsolete {{term for}} the probability distribution of an unobserved variable.|$|E
5000|$|A two {{parameter}} Benini variable can {{be generated}} by the <b>inverse</b> <b>probability</b> transform method. For the two parameter model, the quantile function (inverse cdf) is ...|$|E
30|$|Second, {{this paper}} {{estimates}} Eq. (1) applying the <b>inverse</b> of <b>probability</b> weight {{because the number}} of control group is smaller than that of treatment group. The results shown in column (2) in Table 8 indicate the similar results to Tables 3 and 4. The coefficients are very slightly smaller than the coefficients in Tables 3 and 4. Even if this paper apply the <b>inverse</b> of <b>probability</b> weight, this paper finds that the Japanese place-based job creation program increases the number of workers who work in the treated municipalities, workers in the agricultural sector, and the number of households.|$|R
5000|$|The <b>inverse</b> gamma distribution's <b>probability</b> density {{function}} is defined over the support ...|$|R
40|$|A two-state master {{equation}} based {{decision making}} {{model has been}} shown to generate phase transitions, to be topologically complex and to manifest temporal complexity through an <b>inverse</b> power-law <b>probability</b> distribution function in the switching times between the two critical states of consensus. These properties are entailed by the fundamental assumption that the network elements in the decision making model imperfectly imitate one another. The process of subordination establishes that a single network element can be described by a fractional master equation whose analytic solution yields the observed <b>inverse</b> power-law <b>probability</b> distribution obtained by numerical integration of the two-state master equation to a high degree of accuracy...|$|R
50|$|The <b>Inverse</b> <b>Probability</b> Weighted Estimator (IPWE) can be {{unstable}} if estimated propensities are small. If {{the probability}} of either treatment assignment is small, then the logistic regression model can become unstable around the tails causing the IPWE to also be less stable.|$|E
50|$|Inverse {{transform}} sampling (also {{known as}} inversion sampling, the <b>inverse</b> <b>probability</b> integral transform, the inverse transformation method, Smirnov transform, golden rule) {{is a basic}} method for pseudo-random number sampling, i.e. for generating sample numbers at random from any probability distribution given its cumulative distribution function.|$|E
50|$|Bayesian {{inference}} {{refers to}} a probabilistic method developed by Reverend Thomas Bayes based on Bayes' theorem. Published posthumously in 1763 {{it was the first}} expression of <b>inverse</b> <b>probability</b> and the basis of Bayesian inference. Independently, unaware of Bayes work, Pierre-Simon Laplace developed Bayes' theorem in 1774.|$|E
40|$|The {{existence}} of an inverse limit of an <b>inverse</b> system of (<b>probability)</b> measure spaces has been investigated since {{the very beginning of}} the birth of the modern probability theory. Results from Kolmogorov [10], Bochner [2], Choksi [5], Metivier [14], Bourbaki [3] among others have paved the way of the deep understanding of the problem under consideration. All the above results, however, call for some topological concepts, or at least ones which are closely related topological ones. In this paper we investigate purely measurable <b>inverse</b> systems of (<b>probability)</b> measure spaces, and give a sucient condition for the {{existence of}} a unique inverse limit. An example for the considered purely measurable <b>inverse</b> systems of (<b>probability)</b> measure spaces is also given...|$|R
40|$|We use {{the common}} {{consequence}} effect {{to investigate whether}} rank-dependent theory with an <b>inverse</b> S-shaped <b>probability</b> weighting function describes the risky choices of rural Ethiopians, Indians and Ugandans. We find that behaviour is better described by an S-shaped weighting function. Keywords: JEL classification...|$|R
3000|$|... − 1 (S,δ,p) is the <b>inverse</b> outage <b>probability</b> {{function}} which {{yields the}} required number of transmitted symbols N {{as a function}} of the number of source symbols S, the reception coefficient δ, and the designated outage probability constraint p. A convenient closed form expression of P [...]...|$|R
5000|$|<b>Inverse</b> <b>probability,</b> a term Fisher used in 1922, {{referring}} to [...] "the fundamental paradox of inverse probability" [...] {{as the source}} of the confusion between statistical terms which refer to the true value to be estimated, with the actual value arrived at by estimation, which is subject to error.|$|E
50|$|In 2009 the GII {{models were}} {{produced}} {{along with a}} corresponding OS update for the original 9860G, with new functions gcd/lcm/mod, random integer, units conversion, string functions, and new probability and <b>inverse</b> <b>probability</b> distributions available within programs. The 9860G OS update was not absolutely identical to an actual 9860GII model.|$|E
50|$|The <b>inverse</b> <b>probability</b> {{weighting}} estimator {{can be used}} {{to demonstrate}} causality when the researcher cannot conduct a controlled experiment but has observed data to model. Because it is assumed that the treatment is not randomly assigned, the goal is to estimate the counterfactual or potential outcome if all subjects in population were assigned either treatment.|$|E
40|$|We {{represent}} {{knowledge by}} using probability distributions of mixed continuous and discrete variables. In {{the case of}} complete knowledge of the joint distribution of all items, one can compute arbitrary conditional distributions, which {{may be used for}} prediction. However, in many cases only some marginal distributions, <b>inverse</b> <b>probabilities,</b> or moments are known. Under these conditions, a principle is needed {{in order to determine the}} full joint distribution of all variables. The principle of maximum entropy (Jaynes; 1957, 2003; Haken; 1977; Guiasu and Shenitzer; 1985) ensures an unbiased estimation of the full multivariate relationships using only known facts. In the case of discrete variables, the expert shell SPIRIT implements this approach (cf. Rödder; 2006; Rödder and Meyer; 2006; Rödder et al.; 2006). In this paper the approach is generalized to continuous and mixed continuous-discrete distributions and applied to the problem of credit scoring...|$|R
40|$|This paper reviews <b>inverse</b> {{selection}} <b>probability</b> weighting {{to estimate}} dynamic causal effects. A distribution theory based on sequential GMM estimation is proposed and {{the method is}} applied to a re-evaluation of {{some parts of the}} Swiss active labor market policy to obtain new results and discuss several issues about the implementation of the estimation procedure...|$|R
40|$|In this paper, {{we propose}} a texture {{classification}} method using local texture features BDIP (block difference of <b>inverse</b> <b>probabilities)</b> and BVLC (block variation of local correlation coefficients) in wavelet domain. BDIP and BVLC {{are known to}} be good texture features which are bounded and well normalized to reduce the effect of illumination and catch the own properties of textures effectively. In the method, a target image is first decomposed into wavelet subbands. BDIPs and BVLCs are then computed in wavelet subbands. The {{means and standard deviations of}} subband BDIPs and BVLCs and the subband standard deviations are fused into a texture feature vector. Finally, the Bayesian distance between the feature vector of a query image and that of each class is stably measured and it is classified into the class of minimum distance. Experimental results for three test databases (DBs) show the proposed method yields excellent performances. 1...|$|R
50|$|In Ars Conjectandi (1713), Jacob Bernoulli {{considered}} {{the problem of}} determining, given a number of pebbles drawn from an urn, the proportions of different colored pebbles within the urn. This problem {{was known as the}} <b>inverse</b> <b>probability</b> problem, and was a topic of research in the eighteenth century, attracting the attention of Abraham de Moivre and Thomas Bayes.|$|E
5000|$|After the 1920s, <b>inverse</b> <b>probability</b> {{was largely}} supplanted by a {{collection}} of methods that were developed by Ronald A. Fisher, Jerzy Neyman and Egon Pearson. Their methods {{came to be called}} frequentist statistics. Fisher rejected the Bayesian view, writing that [...] "the theory of <b>inverse</b> <b>probability</b> is founded upon an error, and must be wholly rejected". At the end of his life, however, Fisher expressed greater respect for the essay of Bayes, which Fisher believed to have anticipated his own, fiducial approach to probability; Fisher still maintained that Laplace's views on probability were [...] "fallacious rubbish". Neyman started out as a [...] "quasi-Bayesian", but subsequently developed confidence intervals (a key method in frequentist statistics) because [...] "the whole theory would look nicer if it were built from the start without reference to Bayesianism and priors".The word Bayesian appeared around 1950, and by the 1960s it became the term preferred by those dissatisfied with the limitations of frequentist statistics.|$|E
50|$|In statistics, the Horvitz-Thompson estimator, {{named after}} Daniel G. Horvitz and Donovan J. Thompson, {{is a method}} for {{estimating}} the total and mean of a superpopulation in a stratified sample. <b>Inverse</b> <b>probability</b> weighting is applied to account for different proportions of observations within strata in a target population. The Horvitz-Thompson estimator is frequently applied in survey analyses {{and can be used}} to account for missing data.|$|E
40|$|It is {{well known}} that if X is a random {{variable}} with distribution function G, then G- 1 (U) has the same distribution as X, where U is uniform on (0, 1); moreover, if G is a continuous function, then G(X) has a uniform distribution on (0, 1). In this paper we consider some related results. Distribution function <b>Inverse</b> function <b>Probability</b> integral transformation...|$|R
40|$|Risk and {{ambiguity}} are {{pervasive in}} farming activities. Although agricultural economists {{have a long}} tradition of analyzing risk, there is still alack of understanding of farmers' risk and ambiguity preferences. We aim at structurally estimating these preferences. We use a model that combines a second order model for ambiguity and a model that allows for differences in utility in the gain and loss domains and probability distortion. Moreover, we allow for an endogenous reference point that we estimate. We collect responses from 197 farmers. We find (i) farmers are slightly risk averse in the gain and loss domains and havean <b>inverse</b> s-shaped <b>probability</b> weighing function for risk; (ii) farmers are slightly ambiguity averse in the gain domain and ambiguity neutral in the loss domain and have an <b>inverse</b> s-shaped <b>probability</b> weighing function in the gain domain but do not distort probabilities in the loss domain; (iii) farmers have a positive reference point...|$|R
5000|$|As can be seen, {{the base}} rate P(H) is ignored in this equation, {{leading to the}} base rate fallacy. A base rate is a phenomenon’s basic rate of incidence. The base rate fallacy {{describes}} how people do not take the base rate of an event into account when solving probability problems. This was explicitly tested by Dawes, Mirels, Gold and Donahue (1993) who had people judge both the base rate {{of people who had}} a particular personality trait and the probability that a person who had a given personality trait had another one. For example, participants were asked how many people out of 100 answered true to the question [...] "I am a conscientious person" [...] and also, given that a person answered true to this question, how many would answer true to a different personality question. They found that participants equated <b>inverse</b> <b>probabilities</b> (e.g., [...] ) even when it was obvious that they were not the same (the two questions were answered immediately after each other).|$|R
