3|25|Public
5000|$|The general {{entities}} {{from the}} example above might be referenced in a document as follows: [...] '&greeting1;' {{is a common}} test string. [...] The content of hello.txt is: &greeting2; [...] In Spanish, &greeting4;When parsed, this document would be reported to the downstream application the same {{as if it has}} been written as follows, assuming the hello.txt file contains the text [...] "Salutations": [...] 'Hello world' is a common test string. [...] The content of hello.txt is: Salutations [...] In Spanish, ¡Hola! means Hello!A reference to an undeclared entity is an error unless a <b>default</b> <b>entity</b> has been defined. For example: Additional markup constructs and processor options may affect whether and how entities are processed. For example, a processor may optionally ignore external entities.|$|E
40|$|We {{present an}} {{approach}} for the disambiguation of textual mentions of ambiguous names: disambiguation means here the identi?cation {{of the true}} entity denoted by a name phrase appearing in a query context through its as- signment to the corresponding Wikipedia article. If this article does not exist, we assign this query to a <b>default</b> <b>entity.</b> Ambiguity of names {{is a major problem}} in information retrieval and causes uncertainty in the assignment of name phrases to existing knowledge base entries. We propose a kernel classi?er to approach this problem and compare two Wikipedia structures to construct a rich feature space. The ?rst approach relies on Wikipedia categories, the second on relations constructed from Wikipedia’s hyper link structure. We evaluate both approaches on the German version of Wikipedia and show that both outperform a baseline approach using simple cosine similarity...|$|E
40|$|Never {{before has}} more been asked of State Education Agencies (SEAs), {{commonly}} known as state departments of education. In recent years, policymakers {{at the state and}} federal level have viewed the SEA as the <b>default</b> <b>entity</b> for implementing new and sweeping K [...] 12 initiatives [...] everything from Race to the Top grants and ESEA waivers to teacher evaluation reform and digital learning. But SEAs were designed [...] and evolved over decades [...] to address a relatively narrow set of tasks: distributing state and federal dollars, monitoring the use of these funds, and overseeing the implementation of federal and state education programs. They were not created [...] nor have they developed the core competencies [...] to drive crucial reforms. Accordingly, we argue that despite the best efforts of talented, energetic leaders, SEAs {{will never be able to}} deliver the reform results we need. But there is an alternative. We should view the SEA through the lens of Reinventing Government (1993), the path-breaking book by David Osborne and Ted Gaebler. In short, Osborne and Gaebler call for state agencies to "steer" more and "row" less. Here, we call for federal and state leaders to apply their thesis to SEAs, scaling back the tasks SEAs perform and empowering nongovernmental organizations to take up the slack. We offer the " 4 Cs" model (control, contract, cleave, and create) for rethinking state-level K [...] 12 reform work. In practice, this means pursuing activities on two parallel tracks. On one, we should make the SEA a far leaner organization, able to execute a narrow set of activities. On the other, we should foster the growth of a new state-level reform ecosystem composed of a range of entities [...] primarily independent public entities or nonprofits [...] able to carry out key reforms...|$|E
50|$|Contagion default {{modeling}} can {{be viewed}} as a variation of CID modeling. As discussed in section 2.3, in the CID framework, correlation is modeled by conditioning on a common market factor M, which impacts all entities to the same degree. The lower the random drawing for M, the higher is the default intensity of all entities (unless ρ = 0). Hence CID modeling can elucidate default clustering. In contrast, contagion approaches model the default intensity of an entity as a function of the <b>default</b> of another <b>entity.</b> Hence contagion <b>default</b> modeling incorporates counterparty risk, i.e. the direct impact of a <b>defaulting</b> <b>entity</b> on the <b>default</b> intensity of another entity. In particular, after a default of a particular <b>entity,</b> the <b>default</b> intensity of all assets in the portfolio increases. This default contagion then typically fades exponentially to non-contagious default intensity levels. See the papers of Davis and Lo (2001) and Jarrow and Yu (2001), who pioneered contagion default modeling.|$|R
5000|$|Credit default {{options on}} single credits are {{extinguished}} upon default without any cashflows, {{other than the}} upfront premium paid by the buyer of the option, of course. Therefore, buying a payer option {{is not a good}} protection against an actual default, only against a rise in the credit spread. This may explain why such options are very illiquid. They may also feature quite high implied volatilities, as shown by Damiano Brigo (2005). However options on credit indices such as iTraxx and CDX include any <b>defaulted</b> <b>entities</b> in the intrinsic value of the option when exercised. This is expressed at times by stating that the options offer [...] "front-end protection". Proper inclusion of front end protection complicates index options valuation, see for example Claus M. Pedersen (2003), or Brigo and Morini (2008).|$|R
50|$|The Common Tailorable Template (CTT) {{datafile}} of this ISO Standard is {{aligned with}} the <b>Default</b> Unicode Collation <b>Entity</b> Table (DUCET) datafile of the Unicode Collation Algorithm (UCA) specified in Unicode Technical Standard #10.|$|R
5000|$|Bank credit deflation: a {{decrease}} in the bank credit supply due to bank failures or increased perceived risk of <b>defaults</b> by private <b>entities</b> or a contraction of the money supply by the central bank.|$|R
5000|$|It is a {{structured}} note issued {{by a special}} purpose company or trust, designed to offer investors par value at maturity unless the referenced <b>entity</b> <b>defaults.</b> In the case of default, the investors receive a recovery rate.|$|R
5000|$|A further {{financial}} correlation measure, [...] is the binomial correlation {{approach of}} Lucas (1995). We define the binomial events [...] and [...] where [...] is the <b>default</b> time of <b>entity</b> [...] and [...] is the <b>default</b> time of <b>entity</b> [...] Hence if <b>entity</b> [...] <b>defaults</b> before or at time , the random indicator variable [...] {{will take the}} value in 1, and 0 otherwise. The same applies to [...] Furthermore, [...] and [...] is the default probability of [...] and [...] respectively, and [...] is the joint probability of default. The standard deviation of a one-trial binomial event is , where P is the probability of outcome X. Hence, we derive the joint default dependence coefficient of the binomial events and [...] as ...|$|R
30|$|A Credit Default Swap (CDS) is {{a credit}} {{derivative}} via which one party buys {{protection against the}} default of a reference entity from another party. In particular, the protection buyer pays a fee to the protection seller who, in turn, promises to pay a notional amount to the protection buyer {{in the event that}} the reference <b>entity</b> <b>defaults.</b>|$|R
5000|$|Attributes are {{analogous}} to the columns within a database table (Entity) e.g. Surname. Attributes exist within entities and help describe members (the records within the table). Name and Code attributes are created by <b>default</b> for each <b>entity</b> and serve to describe and uniquely identify leaf members. Attributes {{can be related to}} other attributes from other entities which are called 'domain-based' attributes. This is similar to the concept of a foreign key.|$|R
2500|$|A CDS {{is linked}} to a [...] "reference entity" [...] or [...] "reference obligor", usually a {{corporation}} or government. The reference entity is not {{a party to the}} contract. The buyer makes regular premium payments to the seller, the premium amounts constituting the [...] "spread" [...] charged by the seller to insure against a credit event. If the reference <b>entity</b> <b>defaults,</b> the protection seller pays the buyer the par value of the bond in exchange for physical delivery of the bond, although settlement may also be by cash or auction.|$|R
40|$|We {{investigate}} the systemic {{risk of the}} European sovereign and banking system during 2008 – 2013. We utilize a conditional measure of systemic risk that reflects market perceptions and can be intuitively interpreted as an entity’s conditional joint probability of default, given the hypothetical <b>default</b> of other <b>entities.</b> The measure of systemic risk is applicable to high dimensions and not only incorporates individual default risk characteristics but also captures the underlying interdependent relations between sovereigns and banks in a multivariate setting. In empirical applications, our results reveal significant time variation in systemic risk spillover effects for the sovereign and banking system. We find that systemic risk is mainly driven by risk premiums coupled with a steady increase in physical default risk. Full Tex...|$|R
40|$|In {{the current}} paper we analyze several methods for {{generation}} of loss distribution for credit portfolios. Loss distributions {{play an important}} role in pricing of credit derivatives and in credit portfolio optimization. A loss distribution is a function of the number of entities in the portfolio, their credit ratings, the notional amount and recovery of each <b>entity,</b> <b>default</b> probabilities, loss given defaults, and the correlation/dependence structure between entities incorporated in the portfolio. Direct generation of loss distribution may require Monte Carlo simulation which is time consuming and is not effective when applied for credit portfolio optimization. To overcome computational complexity a number of approaches were undertaken based on assumptions imposed on the input parameters, goals of loss distributions generation, and the accepted level of tolerance and computational errors...|$|R
40|$|International audienceTo develop Net Shape parts by forging, process planing {{must take}} design, production, heat {{treatment}} and control difficulties {{into account the}} earliest it's possible. In this perspective, we propose to present an aspect of our work, made in collaboration with an industry, which deals with the dimensional control of the functionality of forgedstraight bevel gear for car differentials. To study the functionality of the gear, we have split it up into entities which could be looked from different views (designer, manufacturer, metallurgist, …). The metrologist {{is interested in the}} geometry of the entities. After the measurement on coordinate measuring machine, the development of a software enable us to obtain the standard deviations of gears by analysing the position defaults or the intrinsic <b>defaults</b> of all <b>entities.</b> This new approach enables to qualify the whole process planning. Then, the development reported in this paperdeals with the eccentricity between the Net Shape teeth and the assembly surfaces manufactured before heat treatment...|$|R
40|$|This article {{investigates the}} main issues {{pertaining}} {{to a central}} bank’s concerns relative to financial stability, {{as seen from the}} perspective of risks, sources and key ways of action. At macroeconomic level, central banks need to identify and monitor the risk that market operators face payment <b>default</b> towards local <b>entities</b> (companies, households, government) and/or foreign creditors. Moreover, central banks entrusted with supervisory and licensing tasks are also carefully examining the various risks threatening banks, be they credit risk, liquidity risk, market risk, or risks associated with exposure concentration and distribution. On financial markets, prices, profit margins, interest rates, net exposure to certain financial instruments must be monitored to be able to identify build - ups of risks, stress and significant drawbacks to its adequate functioning. Adding to these is the Contingent Claims Analysis (CCA 1), which involves oversight and assessment of risks in various sectors {{as well as of the}} needs to counter them, by drawing on the real market data to identify the probability of default in one sector or another and the impact on other sectors in an integrated manner...|$|R
40|$|AbstractCzech {{companies}} discover alternative non-banking {{financing of}} their business. Social lending known as {{peer to peer}} (P 2 P) financing has started {{to appear on the}} Czech market. P 2 P lending is a new platform of financial transactions that bypasses traditional intermediaries by directly connecting borrower and lenders. But there is an information asymmetry between lenders and borrowers to which online P 2 P lending platforms have to face. As many loans are not secured by collateral, the assessment of the borrower's creditworthiness is very important. The aim {{of this article is to}} define risks of investments to P 2 P loans and propose the elimination of these risks. We went through studies focused on <b>default</b> of corporate <b>entities</b> which were done in different countries and selected some of them. We have chosen these studies because their research is focused not only on financial indicators assessment but also on non-financial indicators assessment and use econometric models to measure probability of default. These studies also confirmed well known basic relations which should be taken into consideration by lenders – higher profitability, higher liquidity and higher volume of assets means lower risk of default, while higher indebtedness and higher leverage means higher risk of default...|$|R
40|$|Now {{that the}} market for credit default swaps is well established, trading is {{increasing}} in forward credit default swaps and European credit default swap options. This article develops models for valuing these instruments. The model for valuing European credit default swap options {{is very similar to}} the standard market model for valuing European swaptions. Once default probabilities and expected recovery rates have been estimated, it enables traders to calculate option prices from credit default swap spread volatilities and vice versa. The article concludes by presenting numerical results illustrating the properties of the models and estimating spread volatilities from historical data. Credit default swaps (CDSs) have proved {{to be one of the}} most successful financial innovations of the 1990 s. They are instruments that provide insurance against a particular company (or sovereign <b>entity)</b> <b>defaulting</b> on its debt. The company is known as the reference <b>entity</b> and a <b>default</b> by the company is known as a credit event. The buyer of the protection makes periodic payments to the seller of protection at a predetermined fixed rate per year. The payments continue until the end of the life of the contract or until a credit event, whichever is earlier. If a credit event occurs, the buyer of protection has the right to deliver a bond issued by the reference entity (the reference bond) to the seller of protection in exchange for its face value...|$|R
5000|$|QuickDB, {{like other}} tools for object-relational mapping, seeks {{to resolve the}} {{differences}} between the two co-existing data models: Object-Oriented Model and the Relational Model. To address this problem, it takes a fully object-oriented approach, where structures like [...] "Objects composed of other objects", [...] "Inheritance", [...] "Collections" [...] (one-to-many and many-to-many) are recognized by <b>default</b> as common <b>entities,</b> and also other features such as automatic table creation and modification of tables dynamically if the structure of the object change over time (addition of new attributes) are included. QuickDB not require the implementation of any interface, or the use of inheritance by the data model to be persistent, it is based simply on certain naming conventions for attributes to infer relevant information about the Object. However, it is possible to use annotations to set certain characteristics of the object, which gives the assurance that everything that QuickDB recognize by default, can be also managed completely by the developer. For queries, QuickDB pretends to maintain this approach where the developer works with the data model in a fully object-oriented way, and therefore the SQL statements (although they are permitted) are not necessary, and can be used by default a Query system where the condition to be evaluated is specified with a simple reference to the attributes in the objects from the data model.|$|R
40|$|In {{this paper}} we apply a copula {{function}} pricing technique to the eval-uation of credit derivatives, namely a vulnerable default put option and a credit switch. Also in this case, copulas enable {{to separate the}} specifi-cation of marginal default probabilities from their dependence structure. Their use is based here on no-arbitrage arguments, which provide pricing bounds and easy-to-implement super-replication strategies. At a second stage, we specify the copula function to be a mixture one. In this case, we obtain closed form prices and hedges, which we calibrate on real market data. For the sake of comparison, we add a Clayton calibration. Copula functions have been suggested {{for the evaluation of}} credit derivatives by Li (2000). Li uses the notion of survival time or time until default and studies the joint distribution of times to <b>default</b> for different <b>entities.</b> He exploits the possibility of writing any joint distribution function as a copula, taking as arguments the marginal distributions, in order to specify the joint default probabilities required for credit derivative pricing. Copulas allow him to split the specification of the joint default arrival into two steps: first model (and calibrate) the marginal default probabilities, then specify their dependence. In particular, he shows that the Riskmetrics approach is equivalent to adopting a very specific copula, namely the Gaussian one. Schönbucher and Schubert (2001) have expanded Li’s idea in the context of reduced form models. They have used copulas also in order to represen...|$|R
3000|$|This paper {{discusses}} a LCDS contract which guarantees {{a basket}} of housing mortgage loans. The borrower B_i (i = 1, 2 [...]...,n) applies for a housing mortgage loan from bank A and repays both principal and regular monthly interest. Once a market interest rate is below the interest rate stipulated in the mortgage, the borrower may prepay the loan (such risk {{is defined as the}} risk of prepayment); when the house value falls and is lower than the amount remaining on the mortgage, the borrower will have reason to default. This presents the bank with a problem. To transfer these credit risks, bank A and investor C will sign a LCDS contract with the reference entity {{a basket of}} mortgages. In the contract, bank A is the credit protection buyer who pays a regular premium to C. Investor C is the credit protection seller who will reimburse A for the loss when a stated credit incident occurs during the period of the contract. If the reference entity is a mortgage, the loan repayment comes from two main sources: the first is the cash flow used for the repayment of the loan; the second one is the value of the mortgaged house when the reference <b>entity</b> <b>defaults.</b> Therefore, the compensation of the credit protection seller is related to the value of the mortgaged house in the first default. However, the financial market is changing. New financial policies, natural disasters, the current international and domestic political and economic outlook, and other factors, may affect house values. In addition, changes in house values are related to each other. Thus, it is more reasonable to describe the dynamic features using a one-factor copula function. 1 [...]...|$|R
40|$|In Poland, {{more than}} 90 % of {{electricity}} production {{is based on}} coal fuel. Meanwhile, the financial situation of the mining industry is quite challenging. Companies in this sector are in debt, generating losses caused by {{a sharp drop in}} coal prices and a simultaneous increase in extraction costs that result from descending into lower levels of coal deposits. At the same time, banks are reluctant to loan money because of the risk of a borrowing <b>entity's</b> <b>default.</b> Increasingly, companies are turning to bond issue to maintain their liquidity and finance development projects. However, bondholders impose conditions in the form of covenants that are often difficult to satisfy, and the strictest relate {{to the level of a}} company's indebtedness and ability-to-repay-debt financial ratios. This article discusses bond issue costs. The authors analyze the bond issue programs of three of the four mining companies operating in Poland. The fourth company did not issue any bonds. Bond issue costs are composed of interest payable to investors, issue preparation and support costs, collateralization costs, and the cost of recording and organizing the sale. The main cost involved in bond issuance is the coupon cost, which depends on the company's financial health and its level of indebtedness, the purpose of the issue, its volume, and the type and quality of the safeguards against the risk of loss of funds invested by bondholders. Bonds issued by coal mining companies are assumed mainly by banks, which demand high interest due to the poor financial condition of the issuers. In addition to interest, companies also pay a capital commitment fee, an arrangement fee and a fee for early redemption. Altogether, in relation to the costs of capital raised through a bank loan, the bond issue results in significantly higher costs of raising capital...|$|R
40|$|Beginning in 1994, the Federal Communication Commission (FCC) {{started to}} use auctions for the {{allocation}} of scarce radio spectrum licenses. The spectrum auctions have drawn widespread attention as policy makers and economists are interested in how effectively these auctions can raise revenues while promoting efficient allocations. The first chapter provides a broad survey of the PCS auctions and of the developments in Broadband PCS and cellular industries {{that relate to the}} PCS auctions. The survey discuss how technology, geography, policy and the firm's industry circumstances affect firms' valuations of the spectrum and are important determinants of the efficient design of the spectrum auctions, as well. The presence of different types of communications providers, cellular providers, wireline providers, and new PCS providers, in the PCS auctions lead to asymmetries in valuations and information. In theory, this structure has a qualitative affect on common value auction outcomes. In laboratory experiments, bidders are found to overbid, much like the findings in previous common value auction experiments, however, the less informed bidders suffer from much stronger overbidding than the informed bidders, and this overbidding can persist over many periods. In the presence of overbidding, additional public information reduced the seller's average revenues. Experience and feedback diminishes but does not eliminate overbidding. In Chapter 3, bidding data from the Interactive Video and Data Service auction, one of the FCC's earliest spectrum auctions are analyzed. Bidding behavior confirms the hypotheses that (1) the larger the area, given the population, the lower the valuation; (2) the larger the population, the higher the income, and the faster the population grows, the higher the valuation; and (3) the number of bidders, the availability of discounts to some bidders, the earlier in the sequence that an auction occurs all positively influence the amount of the winning bid. In addition, licenses in areas with greater population or in auctions with more participating bidders {{are more likely to be}} <b>defaulted.</b> Finally, designated <b>entities</b> who receive a bidding discount are significantly more likely to default...|$|R
40|$|Our {{research}} {{focuses on}} pricing credit derivatives, including single-name credit default swaps (CDSs), Bermudan CDS options, and basket CDSs with and without counterparty risk adjustment. Unlike European CDS option pricing, which admits closed-form solutions, Bermudan CDS option pricing {{does not have a}} closed-form solution. In this work, approximate dynamic programming (ADP) and duality methods are applied to find lower and upper bounds of Bermudan CDS option prices where the default intensity of the underlying reference company is modeled with the Cox, Ingersoll and Ross (CIR) process. While lower bounds of the option prices are obtained from the primal ADP problem with the exercise strategy we approximate, upper bounds of the option prices are computed using a martingale in the duality method. Unlike European CDS option values, Bermudan CDS option values are less sensitive to the default risk of the reference firm. ^ To model the <b>default</b> dependence among <b>entities,</b> we introduce interacting intensities with exogenous jump diffusion processes. For a single-name CDS with bilateral counterparty risk adjustment, the default intensity of each entity (investor, counterparty or reference firm) is modeled as a two-state Markov chain where a jump to the other state is driven by external economy shock following jump diffusion stochastic processes. With transition probabilities obtained from the Kolmogorov equation, bilateral counterparty risk adjusted CDS and CDS option prices can be computed. Our result shows swap rates with bilateral counterparty risk adjustment are sensitive not only to the default risk of the reference firm, but also to the default risks of the investor and the counterparty. ^ We extend our default contagion model with interacting intensities to price a basket CDS with a homogeneous group of reference firms where risks are exchangeable among firms. We implement three different intensity models: a simple linear model, a time dependent exponential decay model, and a two-state Markov chain model with jump diffusion processes. We investigate swap rates of a basket CDS with the three intensity models, and show the sensitivity of swap rates to the default intensities of the investor and the counterparty, and to external shock. ...|$|R
40|$|In {{an article}} from 2000, an investigative {{journalist}} from The Banker warned against the hidden dangers of credit default swaps (CDS). Although CDSs {{can be a}} useful financial instrument for the banking industry, the article warned of the anonymity of credit derivatives, lack of transparency, {{and the potential for}} disaster. In an unfortunately accurate conclusion, the journalist opined that a crisis might occur because banks may not put in place the proper risk control systems in time to avert a disaster. Fast forward eight years and the financial meltdown of 2008 developed into one of the largest economic disasters in history. Banks and other large market actors had taken risk and, in many cases, reckless financial positions that put them at the brink of bankruptcy. The ensuing bailout targeted some of the largest financial entities, but the damage to the financial markets had already occurred. While many individuals debate about what factors caused the financial meltdown, regulators and Congress pointed to CDSs as a contributing factor to the financial meltdown. At one point, American International Group, Inc. (AIG), owed in excess of $ 400 billion to counterparties in CDS contracts and this was money that AIG simply did not have. CDSs, as financial instruments, are both beneficial and detrimental. CDSs do not necessarily create instability, but the contracts can be conduits of instability by shifting the risk of <b>default</b> onto another <b>entity.</b> This receiving entity, on the other hand, can be ill-equipped to deal with this new risk, even though it views itself as capable. Regardless of the benefits of CDS transactions, the public viewed the financial derivative as dangerous. Due to the public’s anger, there was substantial impetuousness in Congress to create new legislation to prevent another market wide failure. Both the U. S. House of Representatives and U. S. Senate proposed their own pieces of legislation, and, in June of 2010, President Barack Obama signed the financial reform bill. Part I of this paper will discuss CDSs and the swap market. Part II will review the prior law, which governed CDSs, and the changes being made to this law by the financial reform bill. Part III will discuss, in a qualitative manner, how these changes will affect CDS contracts and the market for these contracts and whether clearing CDSs will adequately address the issues within the financial markets. Part IV will discuss the governance issues that still underlie the CDS markets and the impact of the new legislation on governance and trading...|$|R
40|$|Whether it {{concerns}} a private individual, a corporate, {{a financial institution}} or even a government, we all obey the rule that he who lends money to a counterparty is exposed {{to the risk of}} financial loss, in case the borrower fails to honor the terms and conditions specified in the credit agreement. One speaks of default by the obligor and, broadly, refers to the associated risk as credit risk. As in the traditional life and non-life insurance industry,bilateral contracts are developed, by financial institutions, that allow for a risk-transfer between the lender (the protection buyer) and a third party (the protection seller). The protection seller agrees to cover(part of) the loss suffered by the lender in the (uncertain) event of adefault by the borrower (the reference entity). In return for this assurance, the protection buyer pays a periodic premium to the protection seller. Like insurance products, the value of these so-called credit derivatives depends entirely on the underlying (credit) risk. Moreover, thesecontracts can generally be entered into cost-free. No initial payment is required by either of the parties in order to close the deal. Such instruments are called unfunded. However, unlike traditional insurance contracts, credit derivatives can be traded. The corresponding market has seen an exponential growth over the last fifteenyears. From almost nothing in 1995, the total market notional now exceeds 30 trillion (1012) U. S. dollar, according to recent estimates (mid 2009). Moreover, by now this market is well-established, strictly-regulated and (highly) liquid. No doubt, the major catalyst for this immense succes is that, by mitigating risk exposures, credit derivatives can be used to offset and reduce the regulatory capital requirements imposed by the Basel II Capital Accord (2004). The latter agreement provides a unified framework for minimal capital requirements, supervisory review and market discipline, but fails to adopt a standardized approach with regard to assessing credit risk. On the contrary, under the internal ratings-based approach institutions are encouraged to usetheir own internal measures for the key drivers of credit risk, powering the demand for models to price credit derivatives. As key drivers, Basel II identifies the reference <b>entity's</b> <b>default</b> probability and the loss-given-default, i. e. the percentage of the exposure-at-default that willnot be recovered following default. However, with regard to funded credit derivatives and securitization in general, a third driver comes to play, namely the prepayment risk, i. e. the potential loss due to the full or partial early redemption of the outstandingdebt by borrowers. In case of funded derivatives, protection is bought by a so-called special purpose vehicle and payments under the credit derivative are funded using securitization techniques. Securitization is a transaction where the cash flows from an underlying pool of exposures (e. g. loans) are used to service at least two different series (tranches) of bonds (called securities). These tranches reflect different degrees of credit risk. This allows investors to select those securities that match their personal risk appetite. In this dissertation, building on the extensive literature on structural models for credit risk, we develop an integrated mathematical framework for default risk, loss-given-default and prepayment risk; with a focus towards securitization and a clear preference for Lévy processes as the stochastic mechanism to describe the underlying price dynamics. In the last decade, research departments of major banks started to accept jump processes as avaluable tool in modeling day-to-day market fluctuations. This increasing interest in Lévy models in finance {{can be attributed to the}} flexibility of the underlying probability distributions and to the presence of jumps in the sample paths. The distributions allow for skewness and excesskurtosis, granting a sufficiently high probability to the risk of extreme price movements over short time intervals. Moreover, the sample pathscan adequately represent the sudden jumps in the observed prices. It isof key importance to deal with these shocks if the market risk is to bemeasured and managed correctly. The text is structured as follows. Chapter 1 is intended as an accessible introduction to credit risk and securitization. We introduce the former concept based on several real-life examples. Next, we elaborate on credit derivatives. We explain the notion of a derivative instrument, look at the development of the market, discuss the different counterparties involved in the contract and overview the most important products. Subsequently, we clarify the idea of securitization. We show how this originally as a financing transaction intended technique can also be used to mitigate credit risk. We end the chapter with a preamble on credit risk modeling. Chapter 2 provides the necessary mathematical background. We introduce the basics of probability theory from a measure-theoretic point of view. Subsequently, we define the important concepts of a probability space, a random variable and a stochastic process. A second part is devoted to mathematical finance in continuous time. We discuss filtrations and clarify the important no-arbitrage theory and the relatedrisk-neutral valuation principle. The chapter ends with a remark about homogeneous portfolios. Chapter 3 is dedicatedto Lévy processes. In this dissertation, the latter are the elected tool to describe the price mechanism of the reference asset and the corresponding derivative instrument. We formally define a Lévy process and debate its structure and properties. Next, we discuss a technique that allows to generate sample paths of almost any Lévy process. Then, we provide several examples of Lévy processes that will be used in the subsequent chapters. Thereafter, we introduce the so-called Ornstein-Uhlenbeck (OU) processes. We grant special attention to the particular case of the Gamma-OU process, that will appear later on. A justification as to why use Lévy processes in finance concludes the chapter. Chapter 4 deals with modeling the risks of default and prepayment within a securitization framework. We review the two most popular approachesto assess credit risk, i. e. the firm-value and the reduced-form approach. With regard to the probability of default, we discuss several well-acknowledged, several recently developed and some new (Lévy-based) models under both approaches. We examine how these can be adjusted and tailoredto fit within the securitization frame. All models are either based on the Normal distribution or on the Gamma distribution. Next, we focus on describing the risk of prepayment. Thereby, we enlist on the default risk models and extend the latter to be able to cope with prepayment. A major challenge, in this respect, is taking the correlation between these two risks into account. At the end of the chapter we discuss a case studywhere the developed models are used in an algorithm to determine the rating of two tranches of loan-backed securities. In Chapter 5 we discuss a generic framework to obtain stochastic loss-given-default in an arbitrary firm-value model for credit risk. We implement the technique into the Normal one-factor and the Gamma one-factor models. Next, we examine how stochastic loss-given-default influences thecapital charges under the Basel II Accord (2004). We also compare our approach to two models recently developed in the banking industry. Finally, we look at the impact of stochastic loss-given-default on the rating of the securities in the case study of the previous chapter. Chapter 6 is a stand-alone chapter. Here, we investigate the practical implementation of a particular member of a new ten-parameter family of Lévy processes, referred to as the Beta-family, introduced by Kuznetsov (2009). The process that we concentrate on is parameterizedin order to resemble the popular Variance-Gamma process due to Carr, Chang and Madan (1998). We therefore have baptized the new model as the Beta-Variance Gamma model. We discuss and examine its applications in pricing equity derivatives and credit derivatives. A summary of the main results and an outlook to future research conclude the study. nrpages: 260 status: publishe...|$|R

