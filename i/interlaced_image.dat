30|53|Public
25|$|Baird's {{comments}} at {{the time}} of introduction generally spoke of a two-color, 1,000-line resolution signal. This would have required considerable radio bandwidth and would be incompatible with the pre-war 405-line system introduced by EMI and the BBC. However, in the patent application he also considered the possibility of compatible color systems. One would use either frame of the existing <b>interlaced</b> <b>image</b> for each color, although this would reduce the effective resolution from 405 to 202 lines, and the frame rate from 50 to 25fps which would increase flicker in areas of mixed color. He also mentions a system broadcasting full 405-line images, but at even greater reduction in frame rate to 12.5fps, which is noted would cause considerable flicker.|$|E
2500|$|An <b>interlaced</b> <b>image</b> {{is divided}} {{from top to}} bottom into strips 8 pixels high, and the rows of the image are {{presented}} in the following order: ...|$|E
5000|$|... #Caption: A 4096-color HAM <b>interlaced</b> <b>image</b> {{produced}} by an Amiga (1989) ...|$|E
5000|$|... #Caption: A {{screenshot}} from HandBrake, {{demonstrating the}} difference between deinterlaced and <b>interlaced</b> <b>images.</b>|$|R
40|$|Abstract: Most video-compression methods {{require that}} the source image {{sequence}} has to be filtered to be freed from grain, noise and scratches to significantly improve compression efficiency. A software module based on motion-estimation algorithms is introduced for noise-cleaning and scratch removal. Video-specific features such as <b>interlaced</b> <b>images</b> and tape dropouts are also addressed. Note: Publication language:russia...|$|R
50|$|Interlacing {{has been}} criticized because {{it may not be}} clear to viewers when the image has {{finished}} rendering, unlike non-interlaced rendering, where progress is apparent (remaining data appears as blank). Also, the benefits of interlacing to those on low-speed connections may be outweighed by having to download a larger file, as <b>interlaced</b> <b>images</b> typically do not compress as well.|$|R
5000|$|An <b>interlaced</b> <b>image</b> {{is divided}} {{from top to}} bottom into strips 8 pixels high, and the rows of the image are {{presented}} in the following order: ...|$|E
50|$|Postwar {{broadcast}} {{coverage was}} extended to Birmingham in 1949 {{with the opening of}} the Sutton Coldfield transmitting station, and by the mid-1950s most of the country was covered, transmitting a 405-line <b>interlaced</b> <b>image</b> on VHF.|$|E
50|$|Adam7 is an {{interlacing}} algorithm for raster images, {{best known}} as the interlacing scheme optionally used in PNG images. An Adam7 <b>interlaced</b> <b>image</b> is broken into seven subimages, which are defined by replicating this 8×8 pattern across the full image.|$|E
5000|$|Matsushita Electric (Panasonic) prototyped the [...] "3D Full-HD Plasma Theater System" [...] on CES 2008. The {{system is}} a {{combination}} of a 103-inch PDP TV, a Blu-ray Disc player and shutter glasses. The new system transmits 1080i60 <b>interlaced</b> <b>images</b> for both right and left eyes, and the video is stored on 50-gigabyte Blu-ray using the MPEG-4 AVC/H.264 compression Multiview Video Coding extension.|$|R
50|$|Another {{application}} is in 3D imaging and displays. In 1902 Frederic E. Ives proposed {{the use of}} an array of alternately transmitting and opaque strips to define the viewing directions for a pair of <b>interlaced</b> <b>images</b> and hence enable the observer to see a 3D stereoscopic image. The strips were later replaced by Hess with an array of cylindrical lenses known as a lenticular screen, to make more efficient use of the illumination.|$|R
5000|$|Third, analog video {{signals are}} <b>interlaced</b> - each <b>image</b> (frame) is sent as two [...] "fields", each {{with half the}} lines. Thus pixels are either twice as tall as they would be without <b>interlacing,</b> or the <b>image</b> is deinterlaced.|$|R
50|$|The FX1 offers Cineframe {{shooting}} modes at 30 and 24 frames per second. The camera uses an <b>interlaced</b> <b>image</b> but extracts progressive {{images from}} individual fields by doubling them. The 30fps and 24fps {{do not offer}} the same resolution as true progressive scanning. The 24fps Cineframe shooting mode does not offer the same resolution, or motion cadence as true 24fps progressive scanning.|$|E
5000|$|Barrier-grid {{animation}} (also {{known as}} [...] "picket fence" [...] animation and {{often referred to}} by the genericized trademark [...] "Scanimation") is an animation effect created by moving a striped transparent overlay across an <b>interlaced</b> <b>image.</b> The barrier-grid technique and its history overlap with parallax stereography (also known as [...] "Relièphographie") for 3D autostereograms. The technique has also been used for color-changing pictures, but to a much lesser extent.|$|E
50|$|In May 1896 Auguste Berthier {{published}} {{an article about the}} history of stereoscopic images in French scientific magazine Le Cosmos, which included his method of creating an autostereogram. Alternating strips from the left and right image of a traditional stereoscopic negative had to be recomposed as an <b>interlaced</b> <b>image,</b> preferably during the printing of the image on paper. A glass plate with opaque lines had to be fixed in front of the interlaced print with a few millimeters in between, so the lines on the screen formed a parallax barrier: from the right distance and angle each eye could only see the photographic strips shot from the corresponding angle. The article was illustrated with a diagram of the principle, an image of the two parts of a stereoscopic photograph divided into exaggerated wide bands, and the same strips recomposed as an <b>interlaced</b> <b>image.</b> Berthier's idea was hardly noticed. After Frederic Ives' similar autostereograms were presented at the French Academy of Sciences in 1904, Berthier reminded the institute about his autostereograms that he in the meantime had also managed to create in color.|$|E
40|$|The paper shows {{possibility}} of usage of high-penetrating introscopy systems operating in dual energy mode in 4 - 10 MeV energy range for recognition of groups of materials {{according to their}} atomic number. The recognition method itself, its effectiveness and limits of applicability are discussed. The results of recognition processing of <b>interlaced</b> <b>images</b> scanned out in dual energy mode are presented. Some improvement tools of the method and {{possibility of}} its implementation in customs inspection systems for vehicles and large-scale containers are briefly discussed...|$|R
40|$|Series of {{experiments}} on material recognition {{were carried out}} on a full-scale prototype of customs inspection system for vehicles and large-scale containers developed by Efremov Research Institute. The 8 MeV linear electron accelerator operating in 4 - 8 MeV range dual energy mode {{was used as an}} X-ray source. Experimental <b>interlaced</b> <b>images</b> of tested samples and real cargo container were processed and visualized by advanced software. The special palettes representing integral absorption and Z-characteristics of cargo material were used for visualization. Some images are given. ...|$|R
50|$|The {{newest and}} most {{convenient}} displays, in {{products such as}} the Nintendo 3DS, HTC Evo 3D, and LG Optimus 3D, {{do not have the}} parallax barrier in front of the pixels, but behind the pixels and in front of the backlight. The entire LCD matrix is therefore exposed to both eyes, but as seen from each eye's position only one of the <b>interlaced</b> <b>images</b> in it is backlit. Glare from the visibly lit pixel columns tends to make the adjacent unlit columns less noticeable.|$|R
5000|$|From {{there the}} <b>interlaced</b> <b>image</b> can be printed {{directly}} to the back (smooth side) of the lens, {{or it can be}} printed to a substrate (ideally a synthetic paper) and laminated to the lens. When printing to the backside of the lens, the critical registration of the fine [...] "slices" [...] of interlaced images must be absolutely correct during the lithographic or screen printing process or else [...] "ghosting" [...] and poor imagery might result. Ghosting also occurs on choosing the wrong set of images for flip.|$|E
50|$|Ives first {{exhibited}} such {{an image}} in 1901, {{at which time}} he stated that the basic concept had occurred to him about sixteen years earlier while working with line screens for the halftone process. In 1904, Auguste Berthier came forward to claim due credit for the first publication of this concept. He had included it {{near the end of}} an 1896 article about large-format stereoscopic images. Berthier had also created an extremely coarse and nonfunctional <b>interlaced</b> <b>image</b> for purposes of illustration, but he never reduced the idea to practice or attempted to patent it.|$|E
50|$|Each {{image is}} {{arranged}} (slicing) into strips, {{which are then}} interlaced {{with one or more}} similarly arranged images (splicing). These are printed {{on the back of a}} piece of plastic, with a series of thin lenses molded into the opposite side. Alternatively, the images can be printed on paper, which is then bonded to the plastic. With the new technology, lenses are printed in the same printing operation as the <b>interlaced</b> <b>image,</b> either on both sides of a flat sheet of transparent material, or on the same side of a sheet of paper, the image being covered with a transparent sheet of plastic or with a layer of transparent, which in turn is printed with several layers of varnish to create the lenses.|$|E
50|$|Deinterlacing {{algorithms}} temporarily store a few {{frames of}} <b>interlaced</b> <b>images</b> and then extrapolate extra frame data {{to make a}} smooth flicker-free image. This frame storage and processing results in a slight display lag that is visible in business showrooms {{with a large number}} of different models on display. Unlike the old unprocessed NTSC signal, the screens do not all follow motion in perfect synchrony. Some models appear to update slightly faster or slower than others. Similarly, the audio can have an echo effect due to different processing delays.|$|R
50|$|Note: Because the {{refresh rate}} has been slowed {{down by a}} factor of three, and the {{resolution}} is less than half a resolution of a typical interlaced video, the flicker in the simulated interlaced portions and also the visibility of the black lines in these examples are exaggerated. Also, the images above are based on what it would look like on a monitor that does not support interlaced scan, such as a PC monitor or an LCD or plasma-based television set, with the <b>interlaced</b> <b>images</b> displayed using the same mode as the progressive images.|$|R
40|$|Recently, the human-machine {{interface}} using information of user 2 ̆ 7 s eye blink was reported. Eye blinks {{can be classified}} into voluntary (conscious) blinks and involuntary (unconscious) blinks. If the voluntary blinks can be distinguished in automatic, an input decision can be made when user 2 ̆ 7 s voluntary blinks occur. By using this system, the usability of input is increased. We have proposed a new eye blink detection method that uses a Hi-Vision video camera. This method utilizes split <b>interlace</b> <b>images</b> of the eye. These split images are odd- and even- field images in the 1080 i Hi-Vision format and are generated from <b>interlaced</b> <b>images.</b> The proposed method yields a time resolution that is double that in the 1080 i Hi-Vision format. We refer to this approach as a <frame-splitting method<. We also proposed a method for automatic method for classifying eye blink types. This method can classify into voluntary and involuntary blink using the duration of eye blink. The durations of eye blinks depend on individuals. In other words, the input interface using our classification method requires a calibration. To design the calibration, we confirmed the intervals of involuntary blinks by an experiment. In this paper, we present {{the result of this}} experiment and discuss the designing for the calibration of input interface using information of voluntary blinks...|$|R
50|$|Baird's {{comments}} at {{the time}} of introduction generally spoke of a two-color, 1,000-line resolution signal. This would have required considerable radio bandwidth and would be incompatible with the pre-war 405-line system introduced by EMI and the BBC. However, in the patent application he also considered the possibility of compatible color systems. One would use either frame of the existing <b>interlaced</b> <b>image</b> for each color, although this would reduce the effective resolution from 405 to 202 lines, and the frame rate from 50 to 25 fps which would increase flicker in areas of mixed color. He also mentions a system broadcasting full 405-line images, but at even greater reduction in frame rate to 12.5 fps, which is noted would cause considerable flicker.|$|E
50|$|This rough {{animation}} compares {{progressive scan}} with interlace scan, also demonstrating the interline twitter effect associated with interlacing. On the left {{there are two}} progressive scan images. In the middle there are two interlaced images and on the right there are two images with line doublers. The original resolutions are above and the ones with spatial anti-aliasing are below. The interlaced images use half the bandwidth of the progressive ones. The images in the center column precisely duplicate the pixels of {{the ones on the}} left, but interlacing causes details to twitter. Real interlaced video blurs such details to prevent twittering, but as seen in the pictures of the lower row, such softening (or anti-aliasing) comes at the cost of image clarity. A line doubler shown in the bottom center picture cannot restore the previously <b>interlaced</b> <b>image</b> to the full quality of the progressive image shown in the top left one.|$|E
50|$|One {{example of}} a {{computer}} that was capable of producing an <b>interlaced</b> <b>image</b> is the Commodore Amiga. The Amiga's default video mode is PAL or NTSC. NTSC and PAL interlaced screens have two fields called odd and even. The fields switch every 1/60th of a second on NTSC, or 1/50th of a second on PAL, which allows for more higher resolution while using a narrower signal bandwidth than full 50 or 60 FPS progressive video would require, {{but it can also}} produce an alarming jittering effect for graphics with high contrast details between fields. This NTSC/PAL compatibility gave the Amiga a distinct edge in uses such as television production or gaming, however since the original Amigas were unable to produce vertically high resolutioned displays without flickering this was unsuitable for other, office-like uses, where there's a need to work with clear high resolution image often for several hours in order to perform typical tasks. Flicker fixers were devised to remedy this.|$|E
5000|$|... #Caption: Simulated <b>interlaced</b> ZX Spectrum <b>image</b> on a CRT screen ...|$|R
40|$|The {{necessity}} to process <b>interlaced</b> <b>images</b> in surveillance, reconnaissance, or further computer vision areas {{should be a}} topic of the past. But, for different reasons, it is not. So, there are situations in practice, in which <b>interlaced</b> <b>images</b> have to be processed. Since a lot of algorithms strongly degrade when working with such images directly, a usual method is to double or interpolate image lines in order to discard {{one of the two}} enclosed image frames. This is efficient but leads to weak results, in which half of the original information is lost. Alternatively, a lot of valuable computation time has to be spent to solve the highly complex motion compensation task in order to improve the results significantly. In this paper, an efficient algorithm is presented to solve this dilemma. First, the algorithm solves the complex 2 -D mapping problem using the best state-of-the art optical flow method that could be found for this purpose. But, of course, for different physical reasons there are regions which cannot properly be handled by optical flow by itself. Therefore, an efficient post-processing method detects and removes remaining artifacts afterwards, which is the main contribution of this paper. This method is based on color interpolation incorporating local image structure. The presented results document the overall performance of the approach with respect to obtained image quality and calculation time. The method is easy to implement and offers a valuable pre-processing for a lot of computer vision tasks...|$|R
5000|$|... 4:2:0 {{interlaced}} sampling {{applied to}} moving <b>interlaced</b> material. *This <b>image</b> shows a single field.|$|R
5000|$|MUSE's [...] "1125 lines" [...] are {{an analog}} measurement, which {{includes}} non-video [...] "scan lines" [...] during which a CRT's electron beam {{returns to the}} top of the screen to begin scanning the next field. Only 1035 lines have picture information. Digital signals count only the lines (rows of pixels) that have actual detail, so NTSC's 525 lines become 486i (rounded to 480 to be MPEG compatible), PAL's 625 lines become 576i, and MUSE would be 1035i. To convert the bandwidth of Hi-Vision MUSE into 'conventional' lines-of-horizontal resolution (as is used in the NTSC world), multiply 29.9 lines per MHz of bandwidth. (NTSC and PAL/SECAM are 79.9 lines per MHz) - this calculation of 29.9 lines works for all current HD systems including Blu-ray and HD-DVD. So, for MUSE, during a still picture, the lines of resolution would be: 598-lines of luminance resolution per-picture-height. The chroma resolution is: 209-lines. The horizontal luminance measurement approximately matches the vertical resolution of a 1080 <b>interlaced</b> <b>image</b> when the Kell factor and interlace factor are taken into account.|$|E
5000|$|The {{principle}} of the parallax barrier was independently invented by Auguste Berthier, who published an article on stereoscopic pictures including his new idea illustrated with a diagram and pictures with purposely exaggerated dimensions of the <b>interlaced</b> <b>image</b> strips, and by Frederic E. Ives, who made and exhibited a functional autostereoscopic image in 1901. About two years later, Ives began selling specimen images as novelties, the first known commercial use. Nearly a century later, Sharp developed the electronic flat-panel application of this old technology to commercialization, briefly selling two laptops with the world's only 3D LCD screens. These displays are no longer available from Sharp but still being manufactured and further developed from other companies like Tridelity and SpatialView. Similarly, Hitachi has released the first 3D mobile phone for the Japanese market under distribution by KDDI. In 2009, Fujifilm released the Fujifilm FinePix Real 3D W1 digital camera, which features a built-in autostereoscopic LCD measuring 2.8" [...] diagonal. Nintendo has also implemented this technology on its latest portable gaming consoles, the New Nintendo 3DS and the New Nintendo 3DS XL, after first including it on the previous console, the Nintendo 3DS.|$|E
50|$|While {{there are}} simple methods to produce {{somewhat}} satisfactory progressive frames from the <b>interlaced</b> <b>image,</b> for example by doubling {{the lines of}} one field and omitting the other (halving vertical resolution), or anti-aliasing the image in the vertical axis to hide some of the combing, there are sometimes methods of producing results far superior to these. If there is only sideways (X axis) motion between the two fields and this motion is even throughout the full frame, {{it is possible to}} align the scanlines and crop the left and right ends that exceed the frame area to produce a visually satisfactory image. Minor Y axis motion can be corrected similarly by aligning the scanlines in a different sequence and cropping the excess at the top and bottom. Often the middle of the picture is the most necessary area to put into check, and whether there is only X or Y axis alignment correction, or both are applied, most artifacts will occur towards the edges of the picture. However, even these simple procedures require motion tracking between the fields, and a rotating or tilting object, or one that moves in the Z axis (away from or towards the camera) will still produce combing, possibly even looking worse than if the fields were joined in a simpler method. Some deinterlacing processes can analyze each frame individually and decide the best method. The best and only perfect conversion in these cases is to treat each frame as a separate image, but that may not always be possible. For framerate conversions and zooming it would mostly be ideal to line-double each field to produce a double rate of progressive frames, resample the frames to the desired resolution and then re-scan the stream at the desired rate, either in progressive or interlaced mode.|$|E
40|$|Abstract. We {{present a}} novel deinterlacing scheme that makes {{consequent}} use of discontinuity-preserving partial differential equations (PDEs). It combines {{the accuracy of}} recent variational motion estimation techniques with the directional interpolation qualities of anisotropic diffusion filters. Our algorithm proceeds in three steps: First, we interpolate the <b>interlaced</b> <b>images</b> {{by means of a}} spatial edge enhancing diffusion process (EED). Then we apply the variational optic flow technique of Brox et al. (2004) in order to obtain a precise interframe registration. Finally we use a spatiotemporal generalisation of EED for motion-compensated inpainting of the missing data in the original sequence. Experiments demonstrate that the proposed method outperforms not only classical deinterlacing schemes, but also a recent PDE-based approach. ...|$|R
40|$|Perceptual user {{interfaces}} {{will require the}} detection, tracking, and recognition of faces and other body and facial features. This paper introduces a robust, accurate, and low cost real-time solution for the eye and face detection problem. The method uses two infra-red illumination sources to generate bright and dark pupil images, which are combined to robustly detect pupils. Once the pupils are detected, the inter-ocular distance is {{used to determine the}} size and position of the bounding box around the face. The position of other facial features such as eye brows, nose, and mouth can be estimated once the face is detected. A real-time implementation of the system, which process 30 frames per second using <b>interlaced</b> <b>images</b> of resolution 640 480 pixels, is also presented. ...|$|R
30|$|In this paper, we extend {{methods in}} [17, 18] to {{separated}} BI-RL and <b>interlaced</b> BI-RL, for <b>image</b> deblurring applications.|$|R
