494|776|Public
25|$|This {{procedure}} {{was designed to}} prevent <b>hardware</b> <b>failure</b> so that the lunar landing mission could be continued. The Mission Operations Report Apollo 13 recounts how the master caution and warning alarm had been turned off for a previous low-pressure reading on hydrogen tank 2, and so it did not trigger {{to call attention to}} the high oxygen pressure reading.|$|E
25|$|Backup and Restore Center {{also known}} as Windows Backup Status and Configuration replaces NTBackup {{although}} Windows NT Backup Restore Utility is available for download. Backup and Restore Center operates in two modes: (a) Backup or restore selected files or (b) Complete PC Backup. If using Complete PC Backup, incremental snapshots are stored on external hard disk or optical media, and the complete system can be restored to protect against <b>hardware</b> <b>failure</b> or severe software damage. Automatic scheduling of file backups is not available in Vista Home Basic. Complete PC Backup is not available in Vista Home Basic and Home Premium.|$|E
500|$|The final script removed this explanation, {{but it is}} {{hinted at}} when HAL asks David Bowman if Bowman is bothered by the [...] "oddities" [...] and [...] "tight security" [...] {{surrounding}} the mission. After Bowman concludes that HAL is dutifully drawing up the [...] "crew psychology report", the computer makes his false prediction of <b>hardware</b> <b>failure.</b> Another hint occurs {{at the moment of}} HAL's deactivation when a video reveals the purpose of the mission.|$|E
25|$|Erupt was {{generated}} by various <b>hardware</b> <b>failures</b> or alarms.|$|R
30|$|Avoidance of {{inconsistencies}} {{that are}} caused by communication errors and <b>hardware</b> <b>failures.</b>|$|R
5000|$|Business Disruption and Systems Failures - utility disruptions, {{software}} <b>failures,</b> <b>hardware</b> <b>failures</b> ...|$|R
2500|$|What {{trends are}} we seeing for <b>hardware</b> <b>failure</b> and life expectancy? ...|$|E
2500|$|Risks include {{infection}} (5% overall, with 1% {{of those}} requiring pin removal and the bone becoming infected in [...]5%), failure for bone {{to grow in}} the desired direction (between 7 and 9%), [...] <b>hardware</b> <b>failure</b> (between 3 and 4.5%), failure to follow the distraction protocol (4.5% overall; too slow 2% and too fast [...]5%), 1% pain due to distraction ending the procedure; damage to the inferior alveolar nerve occurs in 3.5% of mandibular distraction, tooth bud injury in 2%, and facial nerve injury in 0.5% of cases.|$|E
2500|$|In December 2014, NASA {{reported}} that Opportunity {{was suffering from}} [...] "amnesia" [...] events in which the rover fails to write data, e.g. telemetry information, to non-volatile memory. The <b>hardware</b> <b>failure</b> {{is believed to be}} due to an age-related fault in one of the rover's seven memory banks. As a result, NASA had aimed to force the rover's software to ignore the failed memory bank, however amnesia events continued to occur which eventually resulted in vehicle resets. [...] In light of this, on Sol 4027 (May 23, 2015), the rover was configured to operate in RAM-only mode, completely avoiding the use of non-volatile memory for storage.|$|E
50|$|In March 2017, {{a class-action}} lawsuit was filed against LG {{in regards to}} their {{handling}} of these <b>hardware</b> <b>failures.</b>|$|R
30|$|Physical problems, as <b>hardware</b> <b>failures</b> (disk, memory, network failures, etc.), {{depend on}} human {{intervention}} for correction and {{are harder to}} address.|$|R
50|$|In March 2017, LG Electronics was {{sued for}} its {{handling}} of <b>hardware</b> <b>failures</b> with recent smartphones {{such as the}} LG G4.|$|R
2500|$|On January 21, 2004 (sol [...] ), Spirit {{abruptly}} ceased {{communicating with}} mission control. The {{next day the}} rover radioed a 7.8 bit/s beep, confirming that it had received a transmission from Earth but indicating that the craft believed {{it was in a}} fault mode. Commands would only be responded to intermittently. This was described as a very serious anomaly, but potentially recoverable if it were a software or memory corruption issue rather than a serious <b>hardware</b> <b>failure.</b> Spirit was commanded to transmit engineering data, and on January 23 sent several short low-bitrate messages before finally transmitting 73 megabits via X band to Mars Odyssey. The readings from the engineering data suggested that the rover was not staying in sleep mode. As such, it was wasting its battery energy and overheating – risk factors that could potentially destroy the rover if not fixed soon. On sol 20, the command team sent it the command SHUTDWN_DMT_TIL ("Shutdown Dammit Until") to try to cause it to suspend itself until a given time. It seemingly ignored the command.|$|E
2500|$|In December 1997, NASA {{approved}} {{an extended}} mission for Galileo {{known as the}} Galileo Europa Mission, which ran for two years {{following the end of}} the primary mission. The focus of this extended mission was to follow up on the discoveries made at Europa with seven additional flybys to search for new evidence of a possible sub-surface water ocean. Starting in May 1999, Galileo used four flybys (20 to 23) with Callisto to lower its periapse, setting up a chance for it to fly by Io twice in late 1999. During Galileos 21st orbit, it acquired a [...] of the anti-Jovian hemisphere (the [...] "far" [...] side of Io), its highest resolution observations of Io to date. This mosaic complemented the coverage obtained by Voyager 1, whose highest resolution observations covered Io's sub-Jovian hemisphere. Galileos two flybys in late 1999, on October 11 and November 26, provided high-resolution images and spectra of various volcanoes and mountains on Io's anti-Jovian hemisphere. The camera suffered a problem with an image mode used extensively during the first encounter, causing the majority of images taken to be highly degraded (though a software algorithm was developed to partially recover some of these images). NIMS also had problems due to the high-radiation environment near Io, suffering a <b>hardware</b> <b>failure</b> that limited the number of near-infrared wavelengths it sampled. Finally, the imaging coverage was limited by the low-data rate playback (forcing Galileo to transmit data from each encounter days to weeks later on the apoapse leg of each orbit), and by an incident when radiation forced a reset of the spacecraft's computer putting it into safe mode during the November 1999 encounter. Even so, Galileo fortuitously imaged an outburst eruption at Tvashtar Paterae during the November flyby, observing a curtain of lava fountains [...] long and [...] high. An additional encounter was performed on February 22, 2000. With no new errors with Galileo's remote sensing instruments, no safing events, and more time after the flyby before the next satellite encounter, Galileo was able to acquire and send back more data. This included information on the lava flow rate at Prometheus, Amirani, and Tvashtar, very high resolution imaging of Chaac Patera and layered terrain in Bulicame Regio, and mapping of the mountains and topography around Camaxtli Patera, Zal Patera, and Shamshu Patera.|$|E
50|$|Amazon's {{websites}} were temporarily offline on December 12, 2010, {{although it}} was initially unclear if {{this was due to}} attacks or a <b>hardware</b> <b>failure.</b> An Amazon official later stated that it was due to a <b>hardware</b> <b>failure.</b>|$|E
50|$|In {{software}} testing, recovery {{testing is}} {{the activity of}} testing how well an application is able to recover from crashes, <b>hardware</b> <b>failures</b> and other similar problems.|$|R
50|$|Unplanned outages are {{unexpected}} outages {{that are}} caused {{by the failure of}} any system component. They include <b>hardware</b> <b>failures,</b> software issues, or people and process issues.|$|R
50|$|Hardware problems: Sporadic <b>hardware</b> <b>failures</b> start {{showing up}} in the field due to the new effect. Postmanufacturing {{redesign}} and hardware re-spins are required to get the chip to function.|$|R
50|$|The {{project has}} had to shut down several times to change over to new {{databases}} capable of handling larger datasets. <b>Hardware</b> <b>failure</b> {{has proven to be}} a substantial source of project shutdowns—as <b>hardware</b> <b>failure</b> is often coupled with database corruption.|$|E
5000|$|Minimal {{damage to}} file data and system {{consistency}} on <b>hardware</b> <b>failure</b> ...|$|E
5000|$|What {{trends are}} we seeing for <b>hardware</b> <b>failure</b> and life expectancy? ...|$|E
50|$|Initially {{an average}} runtime of 5 minutes was {{achieved}} before hardware problems appeared. In 1954 the system became more stable. Breakpoints {{were introduced to}} allow software restart after <b>hardware</b> <b>failures.</b>|$|R
40|$|A {{major issue}} in many {{applications}} is how to preserve the consistency of data {{in the presence of}} concurrency and <b>hardware</b> <b>failures.</b> We suggest addressing this problem by implementing applications in terms of abstract data types with two properties: Their objects are atomic (they provide serializability and recoverability for activities using them) and resilient (they survive <b>hardware</b> <b>failures</b> with acceptably high probability). We define what it means for abstract data types to be atomic and resilient. We also discuss issues that arise in implementing such types, and describe a particular linguistic mechanism provided in the Argus programming language. 1...|$|R
30|$|For systems without {{failures}} {{usage of}} Petri nets (PN) is very useful for modelling, analysing and control synthesis. However, practically any {{system is not}} failure-free. Failures can emerge in any device or software. It is gratifying that PN can be used [3, 7, 10, 11, 15, 16, 17, 19, 21, 22, 23, 26] also for systems where failures occur. There failures can be categorized into <b>hardware</b> <b>failures</b> and software ones. To minimize the <b>hardware</b> <b>failures</b> of devices, {{it is necessary to}} timely execute maintenance of devices, test and/or check as well as timely replace their components.|$|R
50|$|The {{operating}} {{system is designed}} to avoid crashes due to a simplexed <b>hardware</b> <b>failure.</b>|$|E
50|$|Touch disease, {{a similar}} form of {{eventual}} <b>hardware</b> <b>failure</b> {{experienced by the}} iPhone 6 Plus.|$|E
5000|$|... #Caption: The three {{red lights}} on the Ring of Light {{indicate}} that a General <b>Hardware</b> <b>Failure</b> error occurred.|$|E
40|$|Abstract. We present our formal {{verification}} of the persistent mem-ory manager in IBM’s 4765 secure coprocessor. Its {{task is to}} achieve a transactional semantics of memory updates {{in the face of}} restarts and <b>hardware</b> <b>failures</b> and to provide resilience against the latter. The inclu-sion of <b>hardware</b> <b>failures</b> is novel in this area and incurs a significant jump in system complexity. We tackle the resulting verification challenge by a combination of a monad-based model, an abstraction that reduces the system’s non-determinism, and stepwise refinement. We propose novel proof rules for handling repeated restarts and nested metadata transac-tions. Our entire development is formalized in Isabelle/HOL. ...|$|R
40|$|The {{integrity}} of system hardware {{is an important}} requirement for providing dependable services. Understanding the <b>hardware’s</b> <b>failure</b> mechanisms and the error rate is therefore an important step towards devising an effective overall protection mechanism to prevent service failure. In this paper we discuss an on-going case study of memory <b>hardware</b> <b>failures</b> of production systems in a server-farm environment. We present some preliminary results collected from 212 machines. Our observations under a normal, non-accelerated condition validate the existence of all failure modes modeled in the previous literature: single-cell, row, column, and whole-chip failures. We also provide a quantitative analysis of the error rates. ...|$|R
30|$|In our experiments, {{we use a}} known buggy POX load {{balancer}} implementation, and manually inject synthetic trigger events and <b>hardware</b> <b>failures</b> to check the viability of MPI {{in terms of its}} detection accuracy and detection time.|$|R
5000|$|Replicated storage {{to ensure}} that <b>hardware</b> <b>failure</b> or a site {{disaster}} {{does not lead to}} loss of data.|$|E
5000|$|If {{there is}} no {{fault-tolerant}} setup and <b>hardware</b> <b>failure</b> occurs, all the data within the database will be lost.|$|E
5000|$|Disaster Recovery- Instantly {{run your}} systems in our cloud {{when they go}} down due to <b>hardware</b> <b>failure</b> or natural disaster.|$|E
40|$|International audienceMassively {{parallel}} processors provide high computing {{performance by}} {{increasing the number of}} concurrent execution units. Moreover, the transistor technology evolves to higher density, higher frequency and lower voltage. The combination of these factors increases significantly the probability of <b>hardware</b> <b>failures.</b> In this paper, we present a methodology to locate and mitigate <b>hardware</b> <b>failures</b> of NVidia GPUs. Results show that intermittent errors can be precisely localized and have a limited impact to a well defined architecture tile. Therefore, we propose, and demonstrate on a software prototype, a rescheduling strategy to quarantine the defective hardware and ensure correct execution. Our approach significantly improve the GPU fault-tolerance capability and GPU’s lifespan, at a reasonable overhead...|$|R
40|$|Collaborating users {{need to move}} terabytes of data {{among their}} sites, often {{involving}} multiple protocols. This process is very fragile and involves considerable human involvement to deal with failures. In this work, we propose data pipelines, an automated system for transferring data among collaborating sites. It speaks multiple protocols, has sophisticated flow control and recovers automatically from network, storage system, software and <b>hardware</b> <b>failures.</b> We successfully used data pipelines to transfer three terabytes of DPOSS data from SRB mass storage server at San Diego Supercomputing Center to UniTree mass storage at NCSA. The whole process did not require any human intervention and the data pipeline recovered automatically from various network, storage system, software and <b>hardware</b> <b>failures...</b>|$|R
50|$|Functional {{safety is}} the part of the overall safety of a system or piece of {{equipment}} that depends on the system or equipment operating correctly in response to its inputs, including the safe management of likely operator errors, <b>hardware</b> <b>failures</b> and environmental changes.|$|R
