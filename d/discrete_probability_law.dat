0|1895|Public
40|$|We propose an integer-valued {{stochastic}} process with conditional marginal distribution {{belonging to the}} class of infinitely divisible <b>discrete</b> <b>probability</b> <b>laws.</b> With this proposal, we introduce a wide class of models for count time series that includes the Poisson integer-valued generalized autoregressive conditional heteroscedastic (INGARCH) model (Ferland et al., 2006) and the negative binomial and generalized Poisson INGARCH models (Zhu, 2011, 2012 a). The main probabilistic analysis {{of this process is}} developed stating, in particular, first-order and second-order stationarity conditions. The existence of a strictly stationary and ergodic solution is established in a subclass including the Poisson and generalized Poisson INGARCH models...|$|R
2500|$|Zipf's law (...) is an {{empirical}} law formulated using mathematical statistics {{that refers to}} the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related <b>discrete</b> power <b>law</b> <b>probability</b> distributions. The <b>law</b> is named after the American linguist George Kingsley Zipf (1902–1950), who popularized it and sought to explain it (Zipf 1935, 1949), though he did not claim to have originated it. The French stenographer Jean-Baptiste Estoup (1868–1950) appears to have noticed the regularity before Zipf. It was also noted in 1913 by German physicist Felix Auerbach (1856–1933).|$|R
5000|$|The Kirkwood {{superposition}} approximation {{was introduced}} in 1935 by John G. Kirkwood {{as a means of}} representing a <b>discrete</b> <b>probability</b> distribution. [...] The Kirkwood approximation for a <b>discrete</b> <b>probability</b> density function [...] is given by ...|$|R
5000|$|Hypergeometric {{distribution}}, a <b>discrete</b> <b>probability</b> distribution ...|$|R
50|$|Variance-to-mean ratio: The variance-to-mean ratio classifies several {{important}} families of <b>discrete</b> <b>probability</b> distributions: the constant distribution as circular (eccentricity 0), binomial distributions as elliptical, Poisson distributions as parabolic, and negative binomial distributions as hyperbolic. This is elaborated at cumulants of some <b>discrete</b> <b>probability</b> distributions.|$|R
5000|$|... #Caption: The cdf of a <b>discrete</b> <b>probability</b> distribution, ...|$|R
5000|$|... #Caption: The Poisson distribution, a <b>discrete</b> <b>probability</b> distribution.|$|R
5000|$|... #Subtitle level 2: Cumulants of some <b>discrete</b> <b>probability</b> {{distributions}} ...|$|R
5000|$|The perplexity of a <b>discrete</b> <b>probability</b> {{distribution}} p {{is defined}} as ...|$|R
25|$|<b>Discrete</b> <b>probability</b> theory {{deals with}} events {{that occur in}} {{countable}} sample spaces.|$|R
30|$|The {{first case}} {{corresponds}} to the entropy of a <b>discrete</b> <b>probability</b> distribution.|$|R
50|$|<b>Discrete</b> <b>probability</b> theory {{deals with}} events {{that occur in}} {{countable}} sample spaces.|$|R
5000|$|For two <b>discrete</b> <b>probability</b> {{distributions}} [...] and ,their Hellinger {{distance is}} defined as ...|$|R
5000|$|Unary coding is an optimally {{efficient}} encoding for {{the following}} <b>discrete</b> <b>probability</b> distribution ...|$|R
30|$|Calculate the <b>discrete</b> <b>probability</b> density {{function}} of the energy levels on the reconstructed image.|$|R
5000|$|Probability mass, Probability mass function, p.m.f., <b>Discrete</b> <b>probability</b> {{distribution}} function: for discrete random variables.|$|R
5000|$|In {{probability}} theory and statistics, the geometric distribution is either of two <b>discrete</b> <b>probability</b> distributions: ...|$|R
50|$|Classification of {{discrete}} distributions by variance-to-mean ratio; see cumulants of some <b>discrete</b> <b>probability</b> distributions for details.|$|R
3000|$|... {{points in}} an 8 -dimensional space. Each meadow can be characterised by a <b>discrete</b> <b>probability</b> {{function}} [...]...|$|R
50|$|The Borel {{distribution}} is a <b>discrete</b> <b>probability</b> distribution, arising in contexts including branching processes and queueing theory.|$|R
5000|$|For <b>discrete</b> <b>probability</b> {{distributions}} P and Q,the Kullback-Leibler divergence from Q to P {{is defined}} to be ...|$|R
5000|$|Here, the <b>discrete</b> <b>probability</b> {{distribution}} [...] {{is interpreted}} as a vector in [...] with [...] and [...]|$|R
5000|$|Let X be {{a random}} {{variable}} with a <b>discrete</b> <b>probability</b> distribution p depending on a parameter θ. Then the function ...|$|R
40|$|We propose two least-squares estimators of a <b>discrete</b> <b>probability</b> {{under the}} {{constraint}} of k-monotonicity and study their statistical properties. We give a characterization of these estimators {{based on the}} decomposition on a spline basis of k-monotone sequences. We develop an algorithm derived from the Support Reduction Algorithm and we finally present a simulation study to illustrate their properties. [br/] MSC 2010 subject classifications: Least squares, non-parametric estimation, k-monotone <b>discrete</b> <b>probability,</b> shape constraint, Support Reduction Algorithm...|$|R
40|$|Abstract: To {{assure the}} privacy of RFID Tags, {{symmetric}} challenge-response identification protocols have been considered. Due to the low cost requirements for tags, it has been assumed that the tampering of RFID tags is possible. In this paper, we estimate the privacy leakage of challenge-response RFID authentication protocols based on symmetric key using <b>discrete</b> <b>probability</b> {{under the assumption that}} tampering the RFID tags are possible. Key–Words: Challenge-Response Authentication protocol, RFID tag, <b>discrete</b> <b>probability...</b>|$|R
5000|$|In {{case the}} random {{variables}} X and Y {{are characterized by}} <b>discrete</b> <b>probability</b> distribution the Łukaszyk-Karmowski metric D is defined as: ...|$|R
5000|$|... if Q has a <b>discrete</b> <b>probability</b> {{function}} f(y), where yi, i = 1 to n, are {{the values}} with nonzero probabilities: ...|$|R
25|$|In statistics, the hypergeometric {{distribution}} is the <b>discrete</b> <b>probability</b> distribution generated by picking colored balls at random from an urn without replacement.|$|R
2500|$|... may be {{considered}} to represent a <b>discrete</b> <b>probability</b> mass function of , with an associated probability mass function constructed from the transformed variable, ...|$|R
2500|$|We use Lagrange {{multipliers}} to {{find the}} point of maximum entropy, , across all <b>discrete</b> <b>probability</b> distributions [...] on [...] We require that: ...|$|R
30|$|Now, we {{will apply}} our results for {{distributions}} given in Theorem  3 to the Zipf–Mandelbrot law, {{a sort of}} <b>discrete</b> <b>probability</b> distributions.|$|R
50|$|In statistics, the hypergeometric {{distribution}} is the <b>discrete</b> <b>probability</b> distribution generated by picking colored balls at random from an urn without replacement.|$|R
5000|$|... may be {{considered}} to represent a <b>discrete</b> <b>probability</b> mass function of , with an associated probability mass function constructed from the transformed variable, ...|$|R
5000|$|We use Lagrange {{multipliers}} to {{find the}} point of maximum entropy, , across all <b>discrete</b> <b>probability</b> distributions [...] on [...] We require that: ...|$|R
40|$|AbstractWe {{apply the}} theory of metric-divergences between {{probability}} distributions and a variational approach {{in order to obtain}} a new model for probabilistic image segmentation. We study a specific model based on a very general measure between <b>discrete</b> <b>probability</b> distributions. We show experimentally that this model is competitive with some other models {{of the state of the}} art. In this work we use a particular case of the the measure of kind (αβγδ) between two <b>discrete</b> <b>probability</b> distributions...|$|R
30|$|For <b>discrete</b> <b>probability</b> distribution, all {{the above}} results and notations are the same, just {{replacing}} the integral sign by the summation sign (∑).|$|R
