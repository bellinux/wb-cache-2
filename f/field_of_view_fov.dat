1149|10000|Public
5|$|The lens is {{equipped}} with a full-frame digital SLR-compatible mount allowing the usage of both APS-C sized cameras and formats larger than APS-C – the build in lens hood however visibly restricts the <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> on formats larger than APS-C – see example above. The FOV in APS-C sized cameras is further restricted at shorter focal lengths if the front cap adaptor ring is not removed – see example below.|$|E
25|$|Early {{infrared}} homing missiles had two {{limitations that}} made them difficult to use in combat situations. The first was that the seeker was relatively insensitive and required large, hot sources to reliably track a target. In practice, this meant the engine of the enemy aircraft had to remain visible to the missile though the shot. The other was that the seeker had a limited <b>field</b> <b>of</b> <b>view</b> (<b>FOV),</b> meaning it could only see the target if it {{was in front of}} the missile. This meant it was possible for the target to escape by flying at right angles to the missile, maximizing its angular velocity relative to the seeker.|$|E
500|$|Some of {{the graphic}} {{capabilities}} of BioShock were also criticised by players. The initial {{release of the}} game was found to use a <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> in widescreen that presented a smaller view compared to the game running at a 4:3 screen resolution, conflicting with original reports from a developer on how widescreen would have been handled. Though the choice of FOV was a design decision made during development, Irrational included an option for [...] "Horizontal FOV Lock" [...] in the December 2007 patch that allows widescreen users a wider field of view, without cutting anything off the image vertically. BioShock was also criticized for not supporting pixel shader 2.0b video cards (such as the Radeon X800/X850), which were considered high-end graphics cards in 2004–2005, and accounted for about 24% of surveyed hardware collected through Valve's Steam platform {{at the time of}} BioShock release.|$|E
40|$|Introduction In spin warp imaging, imaging smaller <b>fields</b> <b>of</b> <b>view</b> (<b>FOV)</b> {{permits a}} {{reduction}} in scan time for a fixed resolution, or an increase in resolution for a fixed scan time. However, objects outside the FOV {{will appear in the}} small FOV at the wrong location. Methods for reduced FOV imaging have been reported [1, 2]. Her...|$|R
30|$|Different CCTV {{cameras in}} a {{building}} have different <b>fields</b> <b>of</b> <b>view</b> (<b>FOV).</b> Based on the optical properties of camera lenses, different FOVs {{can be achieved by}} changing the focal lengths of the lenses. Thus, {{the first step in the}} simulation of CCTV systems in BIM models is to come up with an approach to simulate the varifocal lenses of CCTV cameras.|$|R
40|$|There is {{a strong}} belief that the {{improvement}} of preventive safety applications and the extension of their operative range are achieved by the deployment of multiple sensors with wide <b>fields</b> <b>of</b> <b>view</b> (<b>FOV).</b> The paper contributes to {{the solution of the}} problem and introduces distributed sensor data fusion architectures and algorithms for an efficient deployment of multiple sensors that giv...|$|R
50|$|The {{equivalent}} {{concept for}} optical instruments and sensors is the <b>field</b> <b>of</b> <b>view</b> (<b>FOV).</b>|$|E
50|$|On board OSO 7 {{was a hard}} X-ray telescope. Its {{effective}} energy range: 7-550 keV, <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> 6.5°, effective area ~64 cm2.|$|E
50|$|The highest <b>Field</b> <b>of</b> <b>View</b> (<b>FOV)</b> for a flat {{phased array}} antenna is {{currently}} 120°, although this can be combined with mechanical steering as noted above.|$|E
30|$|In general, the {{performance}} of the tracking algorithms suffers from different adverse effects such as distance or orientation of the camera, and occlusions. However, a VSN with overlapping <b>field</b> <b>of</b> <b>views</b> (<b>FOVs)</b> is capable <b>of</b> providing multiple observations of the same object simultaneously. The authors in [25] presented a distributed and collaborative sensing mechanism to improve the observability of the objects by dynamically changing the camera’s pan, tilt, and zoom. Other examples of distributed object tracking methods are presented in [26] and [27].|$|R
40|$|Objective To compare {{automated}} interactive screening {{using the}} ThinPrep Imaging System with independent manual primary screening of 12, 000 routine ThinPrep slides. Study Design With the first 6, 000 cases, the Review Scopes (RS) screening {{results from the}} 22 <b>fields</b> <b>of</b> <b>view</b> (<b>FOV)</b> only were compared to independent manual primary screening. In the next 6, 000 cases, any abnormality detected in the 22 FOV resulted in full manual screening on the cytotechnologist 9 ̆ 2 s own microscope. Sensitivity and specificity together with their 95...|$|R
5000|$|The IAC's OSIRIS (Optical System for Imaging and low Resolution Integrated Spectroscopy), is an [...] "imaging and low {{resolution}} spectrograph with longslit and multiobject spectroscopic modes. It covers the wavelength range from 0.365 to 1.05 µm with a <b>field</b> <b>of</b> <b>views</b> (<b>FOV)</b> <b>of</b> 7 × 7 arcmin, and 8 arcmin × 5.2 arcmin, for direct imaging and {{low resolution}} spectroscopy respectively." [...] It [...] "provides {{a new generation}} of instrumental observation techniques such as the tunable filters, the charge-shuffling capability in the CCD detectors, etc." ...|$|R
50|$|Focal length (f) and <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> of a lens are {{inversely}} proportional. For {{a standard}} rectilinear lens, FOV = 2 arctan , where x is the diagonal of the film.|$|E
50|$|Because of this crop, the {{effective}} <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> {{is reduced by}} a factor proportional to the ratio between the smaller sensor size and the 35 mm film format (reference) size.|$|E
50|$|Sura-K helmet mounted system (HMS): The <b>field</b> <b>of</b> <b>view</b> (<b>FoV)</b> {{is greatly}} {{increased}} to +/- 60 degrees {{in comparison to}} the +/- 8 degrees of the ASP-PVD-21 HMS originally used on Su-27.|$|E
40|$|We {{consider}} {{the problem of}} super-resolution reconstruction (SRR) in MRI. Subpixel-shifted MR images were taken in several <b>fields</b> <b>of</b> <b>view</b> (<b>FOVs)</b> to reconstruct a high-resolution image. A novel algorithm is presented. The algorithm can be applied locally and guarantees perfect reconstruction {{in the absence of}} noise. Results that demonstrate resolution improvement are given for phantom studies (mathematical model) as well as for MRI studies of a phantom carried out with a GE clinical scanner. The method raises questions that are discussed in the last section of the paper. Open questions should be answered in order to apply this method for clinical purposes...|$|R
40|$|International audienceThe Probability Hypothesis Density (PHD) is a {{well-known}} method for single-sensor multi-target tracking problems in a Bayesian framework, but the extension to the multi-sensor case seems to remain a challenge. In this paper, an extension of Mahler's work to the multi-sensor case provides {{an expression of the}} true PHD multi-sensor data update equation. Then, based on the configuration <b>of</b> the sensors' <b>fields</b> <b>of</b> <b>view</b> (<b>FOVs),</b> a joint partitioning of both the sensors and the state space provides an equivalent yet more practical expression of the data update equation, allowing a more effective implementation in specific FOV configurations...|$|R
50|$|During {{the imaging}} process, four lasers {{illuminate}} 1100 <b>Fields</b> <b>of</b> <b>View</b> (<b>FOV)</b> per channel with pictures taken by four CCD (Charge-coupled device) cameras via a confocal microscope. Though single molecules are visualized, multiple photon emissions are registered for each molecule, {{with the time}} spent at each FOV dependent on {{the brightness of the}} dye in the particular nucleotide as well as camera speed and detection efficiency. At the present time, the imaging process is the rate-determining step, and run time could be reduced at the expense of throughput by reducing the number of FOV per channel.|$|R
50|$|The {{focal length}} of the camera is 43 mm with a <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> of 16° x 16°. The two cameras are {{separated}} by 30 cm and are mounted upside-down relative to each other.|$|E
5000|$|Criteria for project includes: soldier portable, sensory-data collect for a 120 degree <b>field</b> <b>of</b> <b>view</b> (<b>FOV),</b> {{artificial}} {{analysis of}} data, threat analysis and prioritizing [...] "brain-in-the-loop" [...] integration, and real-time processing of neural and artificial cognitive data.|$|E
5000|$|... 2008: The company {{started selling}} the PD-18: a top-down, {{transparent}} monocular display with SVGA resolution 32 degree <b>field</b> <b>of</b> <b>view</b> (<b>FoV),</b> and full color. The PD-18 and its derivative, the PD-14, {{were aimed at}} professional and military markets.|$|E
40|$|In this paper, we {{describe}} a virtual microscope system, based on JPEG 2000, which utilizes extended depth <b>of</b> <b>field</b> (EDF) imaging. Through {{a series of}} observer trials we show that EDF imaging improves both the local image quality <b>of</b> individual <b>fields</b> <b>of</b> <b>view</b> (<b>FOV)</b> and the accuracy with which the FOVs can be mosaiced (stitched) together. In addition, we estimate the required bit rate to adequately render a set of histology and cytology specimens at a quality suitable for on-line learning and collaboration. We show that, using JPEG 2000, we can efficiently represent high-quality, high-resolution colour images of microscopic specimens with less than 1 bit per pixel...|$|R
40|$|We have {{developed}} a modified optical frequency domain imaging (OFDI) system that performs parallel imaging of three-dimensional (3 D) surface profiles by using the space division multiplexing (SDM) method with dual-area swept sourced beams. We have also demonstrated that 3 D surface information for two different areas could be well obtained in a same time with only one camera by our method. In this study, double <b>field</b> <b>of</b> <b>views</b> (<b>FOVs)</b> <b>of</b> 11. 16 mm × 5. 92 mm were achieved within 0. 5 s. Height range for each FOV was 460 µm and axial and transverse resolutions were 3. 6 and 5. 52 µm, respectively...|$|R
40|$|A {{new concept}} for a flat digital image {{acquisition}} device for large <b>field</b> <b>of</b> <b>views</b> (<b>FOV)</b> has been developed. Antetypes for the optical system are compound eyes of small insects and the Gabor-Superlens. A paraxial 3 x 3 matrix formalism {{is used to}} describe the arrangement of three microlens arrays (MLA) with different pitches to find the first order parameters of the system. These considerations are extended to arrays of anamorphic lenses with variable parameters to achieve homogeneous optical performance over the whole FOV. The model is validated by implementation of different systems into commercial raytracing software. A trade-off between system length, sensitivity and diffraction limited resolution as well as aberrations is discussed...|$|R
50|$|When {{some points}} on an image {{receives}} no light at all due to mechanical vignetting (the paths {{of light to}} these image points is completely blocked), then {{this results in a}} restriction of the <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> - parts of the image are then completely black.|$|E
50|$|With {{a typical}} 2 {{megapixel}} CCD, a 1600×1200 pixels image is generated. The {{resolution of the}} image depends {{on the field of}} view of the lens used with the camera. The approximate pixel resolution can be determined by dividing the horizontal <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> by 1600.|$|E
50|$|In {{the optical}} {{instrumentation}} industry the term <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> {{is most often}} used, though the measurements are still expressed as angles. Optical tests are commonly used for measuring the FOV of UV, visible, and infrared (wavelengths about 0.1-20 µm in the electromagnetic spectrum) sensors and cameras.|$|E
40|$|A novel {{human face}} {{tracking}} system with multiple cameras is proposed in this article. Unlike most surveillance approaches requiring overlapping <b>field</b> <b>of</b> <b>views</b> (<b>FOVs)</b> for tracking targets across multiple cameras, our proposed system utilizes face recognition as cue to build correspondence between cameras, {{and is therefore}} compatible with application with non-overlapping FOVs. Face recognition achieves great success in recent years, however it has an intrinsic limitation of low tolerance to face pose changes. In the proposed system, face reconstruction technology is adopted to transform non-frontal human face to a frontal one to substantially enhance the usability of face recognition, so that correspondence of humans across multiple trajectories can be reliably established. © 2011 AICIT. link_to_subscribed_fulltex...|$|R
40|$|AIM: To {{study the}} {{techniques}} of MR diffusion-weighed imaging (DWI) for normal rabbit liver. METHODS: After 15 normal New Zealand white rabbits and one New Zealand white rabbit implanted with VX- 2 tumor were anesthetized with 3 % soluble pentobarbitone, DWI was performed respectively for different b values, repetition times (TR) or thicknesses, when other parameters were the same and magnetic resonance imaging (MRI) was performed respectively, or with different <b>field</b> <b>of</b> <b>views</b> (<b>FOV)</b> or coil when other parameters were the same. The distinction between groups was analyzed by SPSS 10. 0 with apparent diffusion coefficient (ADC), quality index (QI) or signal-noise ratio (SNR). RESULTS: As b value increased, liver ADC, QI and SN...|$|R
40|$|It is {{interesting}} to ask what {{fraction of the total}} available RR Lyrae (RRL) sample that falls in the $Kepler$ and $K 2 $ <b>Fields</b> <b>of</b> <b>View</b> (<b>FoV)</b> is known or discovered. In order assess the completeness of our sample we compared the known RRL sample in the $Kepler$ and $K 2 $ fields with synthetic Galactic models. The Catalina Sky Survey RRL sample was used to calibrate our method. We found {{that a large number of}} faint RRL stars is missing from $Kepler$ and $K 2 $ fields. Comment: 2 pages, 1 figure, proceedings of the RRL 2015 - High-Precision Studies of RR Lyrae Stars conference, to appear in the Communications from the Konkoly Observator...|$|R
5000|$|The {{apparent}} {{shape of}} objects varies with {{distance from the}} center of the <b>field</b> <b>of</b> <b>view</b> (<b>FOV).</b> Objects appearing close to the edges are viewed from an angle, while objects near the centre of the FOV are viewed frontally (circles near the centre of the FOV become egg-shaped when moved towards the periphery).|$|E
50|$|The Compton {{telescope}} uses {{an array}} of twelve Germanium detectors with high spectral resolution to detect gamma rays. On its bottom half the detector {{is surrounded by a}} Bismuth germanate scintillator to shield it from atmospheric gamma rays. The telescope has an overall <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> of 25% of the sky.|$|E
50|$|To {{extend our}} view of the high-energy Universe to the hard X-rays and find the most {{obscured}} black holes, the wide field imaging & hard X-ray imaging detectors (WFI/HXI) together will image the sky up to 18 arcmin <b>field</b> <b>of</b> <b>view</b> (<b>FOV)</b> with a moderate resolution (<150 eV up to 6 keV and <1 keV (FWHM) at 40 keV).|$|E
30|$|In this paper, {{we propose}} a novel {{geospatial}} image and video filtering tool (GIFT) {{to select the}} most relevant input images and videos for computer vision applications with geo-tagged mobile videos. GIFT tightly couples mobile media content and their geospatial metadata for fine granularity video manipulation in the spatial and temporal domain and intelligently indexes <b>field</b> <b>of</b> <b>views</b> (<b>FOVs)</b> to deal with large volumes of data. To demonstrate the effectiveness of GIFT, we introduce an end-to-end application that utilizes mobile videos to achieve persistent target tracking over large space and time. Our experimental results show promising performance of vision applications with GIFT in terms of lower communication load, improved efficiency, accuracy, and scalability when compared with baseline approaches which do not fully utilize geospatial metadata.|$|R
40|$|In Wireless {{multimedia}} sensor networks (WMSNs), {{the camera}} nodes connected in the vision graph share overlapped <b>field</b> <b>of</b> <b>views</b> (<b>FOVs)</b> and {{they depend on}} the densely deployed relay nodes in the communication network graph {{to communicate with each}} other. Given a uniformly deployed camera sensor network with relay nodes, the problem is to find the number of hops for the vision-graph-neighbor-searching messages to construct the vision graph in an energy efficient way. In this paper, mathematical models are developed to analyze the FOV overlap of the camera nodes and the multi-hop communications in two dimensional topologies, which are utilized to analyze the relation between vision graph construction and maximum hop count. In addition, simulations are conducted to verify the models...|$|R
40|$|We {{propose a}} new order {{preserving}} bilinear framework that exploits low-resolution video for person detection in a multi-modal setting using deep neural networks. In this setting cameras are strategically placed such that less robust sensors, e. g. geophones that monitor seismic activity, are located within the <b>field</b> <b>of</b> <b>views</b> (<b>FOVs)</b> <b>of</b> cameras. The primary challenge {{is being able to}} leverage sufficient information from videos where there are less than 40 pixels on targets, while also taking advantage of less discriminative information from other modalities, e. g. seismic. Unlike state-of-the-art methods, our bilinear framework retains spatio-temporal order when computing the vector outer products between pairs of features. Despite the high dimensionality of these outer products, we demonstrate that our order preserving bilinear framework yields better performance than recent orderless bilinear models and alternative fusion methods...|$|R
