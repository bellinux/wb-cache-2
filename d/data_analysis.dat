10000|10000|Public
5|$|Forensic <b>Data</b> <b>Analysis</b> is {{a branch}} of digital forensics. It {{examines}} structured data with the aim to discover and analyse patterns of fraudulent activities resulting from financial crime.|$|E
5|$|A recent {{published}} observation {{during the}} 1997 solar eclipse by Wang et al. suggested a possible gravitational shielding effect, which generated debate. Later in 2002, Yang and Wang published detailed <b>data</b> <b>analysis,</b> which {{suggested that the}} phenomenon still remains unexplained.|$|E
5|$|Content analysis: The {{content of}} {{interviews}} and other texts is systematically analysed. Often data is 'coded' {{as a part}} of the 'grounded theory' approach using qualitative <b>data</b> <b>analysis</b> (QDA) software, such as Atlas.ti, MAXQDA, NVivo, or QDA Miner.|$|E
30|$|KG {{contributed to}} the study design, <b>data</b> collection, <b>analysis,</b> and {{manuscript}} writing. SM {{contributed to the}} <b>data</b> collection, <b>analysis,</b> and manuscript writing. AM contributed to the study design, <b>data</b> collection, and <b>analysis.</b> All authors read and approved the final manuscript.|$|R
30|$|DJS {{contributed to}} the {{conception}} and design of the study; supervised execution; contributed to <b>data</b> collection, <b>analysis,</b> and interpretation; and led writing and revising of the manuscript. SS led the initial draft of the data codebook; contributed to <b>data</b> collection, <b>analysis,</b> and interpretation; and {{contributed to the}} writing and revising of the manuscript. DMB contributed to <b>data</b> collection, <b>analysis,</b> and interpretation {{and contributed to the}} writing and revising of the manuscript. LG contributed to <b>data</b> collection, <b>analysis,</b> and interpretation and contributed to the writing and revising of the manuscript. All authors read and approved the final manuscript.|$|R
40|$|<b>Data</b> ow <b>analysis</b> {{is a well}} studied {{family of}} static program {{analyses}} A rich theoretical basis for <b>data</b> ow <b>analysis</b> has been developed Central to this theory {{is the concept of}} data ow framework These frameworks are a semantically wellfounded formalism for specifying a <b>data</b> ow <b>analysis</b> A variety of solution algorithms for problems specied as data ow frameworks have been developed The bulk of the research on <b>data</b> ow <b>analysis</b> has been done in the context of analysis of sequential programs Unfortunately a number of important assumptions of this work are violated when we apply <b>data</b> ow <b>analysis</b> results to concurrent programs In this paper we extend data ow frameworks for sequential program analysis to accommodate concurrent programs We present solution algorithms for these extended frameworks and reason about their convergence and complexit...|$|R
5|$|Hubble {{data can}} be {{analyzed}} using many different packages. STScI maintains the custom-made Space Telescope Science <b>Data</b> <b>Analysis</b> System (STSDAS) software, which contains all the programs needed to run pipeline reduction on raw data files, {{as well as many}} other astronomical image processing tools, tailored to the requirements of Hubble data. The software runs as a module of IRAF, a popular astronomical data reduction program.|$|E
5|$|JMP {{software}} is partly focused on exploratory <b>data</b> <b>analysis</b> and visualization. It {{is designed for}} users to investigate data to learn something unexpected, as opposed to confirming a hypothesis. JMP links statistical data to graphics representing them, so users can drill down or up to explore the data and various visual representations of it. Its primary applications are for designed experiments and analyzing statistical data from industrial processes.|$|E
5|$|The {{first step}} of metagenomic <b>data</b> <b>analysis</b> {{requires}} the execution of certain pre-filtering steps, including the removal of redundant, low-quality sequences and sequences of probable eukaryotic origin (especially in metagenomes of human origin). The methods available {{for the removal of}} contaminating eukaryotic genomic DNA sequences include Eu-Detect and DeConseq.|$|E
50|$|His early {{research}} {{focused on}} productivity, economies of scale, operational efficiency, and <b>data</b> envelopment <b>analysis.</b> His research on <b>data</b> envelopment <b>analysis</b> includes 15 articles co-authored with William W. Cooper from 1984 to 2011.|$|R
40|$|Abstract—This article {{combines}} two techniques: <b>data</b> envelopment <b>analysis</b> (DEA) and Factor <b>analysis</b> (FA) to <b>data</b> {{reduction in}} decision making units (DMU). <b>Data</b> envelopment <b>analysis</b> (DEA), a popular linear programming technique is useful to rate comparatively operational efficiency of decision making units (DMU) based on their deterministic (not necessarily stochastic) input–output <b>data</b> and factor <b>analysis</b> techniques, have been proposed as data reduction and classification technique, which can be applied in <b>data</b> envelopment <b>analysis</b> (DEA) technique for reduction input – output data. Numerical results reveal that the new approach shows a good consistency in ranking with DEA...|$|R
40|$|We {{present a}} {{comprehensive}} approach to performing <b>data</b> flow <b>analysis</b> in parallel. We identify three types of parallelism inherent in the data flow solution process: independent-problem parallelism, separate-unit parallelism and algorithmic parallelism; and describe a unified framework to exploit them. Our investigations of typical Fortran programs reveal an abundance {{of the last two}} types of parallelism. In particular, we illustrate the exploitation of algorithmic parallelism in the design of our parallel hybrid <b>data</b> flow <b>analysis</b> algorithms. We report on the empirical performance of the parallel hybrid algorithm for the Reaching Definitions problem and the structural characteristics of the program flow graphs that affect algorithm performance. Keywords. <b>Data</b> flow <b>analysis,</b> parallel algorithms, parallel <b>data</b> flow <b>analysis.</b> 1 Introduction 1. 1 Motivation <b>Data</b> flow <b>analysis</b> is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow i [...] ...|$|R
5|$|The {{technical}} aspect {{of an investigation}} is divided into several sub-branches, relating {{to the type of}} digital devices involved; computer forensics, network forensics, forensic <b>data</b> <b>analysis</b> and mobile device forensics. The typical forensic process encompasses the seizure, forensic imaging (acquisition) and analysis of digital media and the production of a report into collected evidence.|$|E
5|$|Sixteen {{isotopes}} of americium {{are known}} with mass numbers from 232 to 248. The most important are 241Am and 243Am, which are alpha-emitters and also emit soft, but intense γ-rays; {{both of them}} can be obtained in an isotopically pure form. Chemical properties of americium were first studied with 241Am, but later shifted to 243Am, which is almost 20 times less radioactive. The disadvantage of 243Am is production of the short-lived daughter isotope 239Np, which has to be considered in the <b>data</b> <b>analysis.</b>|$|E
25|$|Confirmatory <b>data</b> <b>analysis</b> can be {{contrasted with}} {{exploratory}} <b>data</b> <b>analysis,</b> {{which may not}} have pre-specified hypotheses.|$|E
40|$|For <b>data</b> flow <b>analysis</b> of Java {{program to}} be correct and precise, the ows induced by {{exceptions}} must be properly analysed. In our <b>data</b> ow <b>analysis,</b> the implicit control flow for a raised exception is represented explicitly. Exception branches, exception plateaus, and exception exits for methods and method calls are introduced as additional control flow structures for analysis of exception handling. These structures are constructed dynamically under control of <b>data</b> flow <b>analysis...</b>|$|R
40|$|Rough <b>data</b> envelopment <b>analysis</b> (RDEA) {{evaluates the}} {{performance}} of the decision making units (DMUs) under rough uncertainty assumption. In this paper, new discussion regarding RDEA is extended. The RSBM model is proposed by integrating SBM model and rough set theory. The process of reaching solution is presented and this model is applied to efficiency evaluation of the DMUs with uncertain information. Keywords: <b>Data</b> Envelopment <b>Analysis,</b> Uncertainty, Rough <b>Data</b> Envelopment <b>Analysis,</b> Performance Evaluatio...|$|R
30|$|Calibrate well-log <b>data</b> with core <b>analysis</b> <b>data</b> to {{interpret}} the sedimentary facies along the well.|$|R
25|$|He {{also contributed}} to {{statistical}} practice and articulated the important distinction between exploratory <b>data</b> <b>analysis</b> and confirmatory <b>data</b> <b>analysis,</b> believing that much statistical methodology placed too great {{an emphasis on the}} latter.|$|E
25|$|The {{information}} exchange, <b>data</b> <b>analysis,</b> {{and content}} delivery {{capabilities of the}} system.|$|E
25|$|Others {{sometimes}} {{divide the}} bands into sub-bands {{for the purposes}} of <b>data</b> <b>analysis.</b>|$|E
30|$|Network <b>data</b> envelopment <b>analysis.</b>|$|R
40|$|<b>Data</b> flow <b>analysis</b> {{is a well}} studied {{family of}} static program analyses. A rich {{theoretical}} basis for <b>data</b> flow <b>analysis</b> has been developed. Central to this theory {{is the concept of}} data flow framework. These frameworks are a semantically well-founded formalism for specifying a <b>data</b> flow <b>analysis.</b> A variety of solution algorithms for problems specified as data flow frameworks have been developed. The bulk of the research on <b>data</b> flow <b>analysis</b> has been done in the context of analysis of sequential programs. Unfortunately, a number of important assumptions of this work are violated when we apply <b>data</b> flow <b>analysis</b> results to concurrent programs. In this paper, we extend data flow frameworks for sequential program analysis to accommodate concurrent programs. We present solution algorithms for these extended frameworks and reason about their convergence and complexity. 1 This work was supported by the Advanced Research Projects Agency under Grant F 30602 - 94 -C- 0137. 1 Introduction Data f [...] ...|$|R
40|$|Using the non {{parametric}} {{approach of}} <b>Data</b> Envelopment <b>Analysis</b> (DEA) this paper examines the {{input and output}} efficiencies of the Indian pharmaceutical firms for the period 1991 to 2005. Indian, pharmaceutical, firms, non-radial, patents,non-parametric approach, <b>data</b> envelopment <b>analysis,</b> DEA, input, output efficency,...|$|R
25|$|Business Intelligence (BI) {{comprises}} the strategies and technologies used by enterprises for the <b>data</b> <b>analysis</b> of business information.|$|E
25|$|Due to {{its size}} PwC {{is able to}} {{contribute}} <b>data</b> <b>analysis</b> {{to a wide range}} of areas.|$|E
25|$|Yang, J. and Honavar, V. (1999). DistAl: An Inter-Pattern Distance Based Constructive Neural Network Learning Algorithm.. Intelligent <b>Data</b> <b>Analysis.</b> Vol. 3. pp.55–73.|$|E
40|$|With Android {{applications}} processing {{not only}} personal but also business-critical data, efficient and precise <b>data</b> flow <b>analysis</b> {{has become a}} major technique to detect apps handling critical data in unwanted ways. Although <b>data</b> flow <b>analysis</b> in general is a thoroughly researched topic, the event-driven lifecycle model of Android has its own challenges and practical application requires for reliable and efficient analysis techniques. In this paper we present Apparecium, a tool to reveal data flows in Android applications. Apparecium has conceptual differences to other techniques, and can be used to find arbitrary data flows inside Android applications. Details about the used techniques and the differences to existing <b>data</b> flow <b>analysis</b> tools are presented, as well as an evaluation against the <b>data</b> flow <b>analysis</b> framework Flow Droid...|$|R
40|$|Tennenbaum's <b>data</b> flow <b>analysis</b> based {{formulation}} of type inferencing is termed bidirectional in the “Dragon Book”; however, {{it fails to}} qualify as a formal data flow framework and is not amenable to complexity analysis. Further, the types discovered are imprecise. Here, we define a formal data flow framework (based on bidirectional <b>data</b> flow <b>analysis)</b> which discovers more precise type information and is amenable to complexity <b>analysis.</b> We compare <b>data</b> flow analyses with the more general constraint-based analyses and observe that data flow analyses represent program analyses without unbounded auxiliary store. We show that if unlimited auxiliary store is allowed then no <b>data</b> flow <b>analysis</b> would need more than two passes; if auxiliary store is disallowed then type inferencing requires bidirectional <b>data</b> flow <b>analysis.</b> © Elsevie...|$|R
30|$|Organize {{and prepare}} the <b>data</b> for <b>analysis.</b>|$|R
25|$|In genomics, the gamma {{distribution}} was applied in peak calling step (i.e. {{in recognition of}} signal) in ChIP-chip and ChIP-seq <b>data</b> <b>analysis.</b>|$|E
25|$|A central {{computing}} facility together with individually assigned personal computers and workstations for computation, control and monitoring of experiments and <b>data</b> <b>analysis.</b>|$|E
25|$|Anduril is an {{open source}} component-based {{workflow}} framework for scientific <b>data</b> <b>analysis</b> developed at the Systems Biology Laboratory, University of Helsinki.|$|E
40|$|In {{this paper}} {{we present a}} {{quantitative}} model for comparing university departments concerned with the same discipline. This model is based upon ideas drawn from <b>data</b> envelopment <b>analysis.</b> Computational results are given for chemistry and physics departments in the United Kingdom. <b>data</b> envelopment <b>analysis</b> education efficiency...|$|R
50|$|Depending on the organization's {{expectations}} for software development, Development Testing might include static code <b>analysis,</b> <b>data</b> flow <b>analysis,</b> metrics analysis, peer code reviews, unit testing, code coverage analysis, traceability, and other software verification practices.|$|R
5000|$|... iDASH: {{integrating}} <b>Data</b> for <b>Analysis,</b> Anonymization, and SHaring ...|$|R
