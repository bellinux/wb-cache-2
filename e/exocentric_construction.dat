1|4|Public
5000|$|An <b>exocentric</b> <b>construction</b> {{consists}} {{of two or more}} parts, whereby the {{one or the other of}} the parts cannot be viewed as providing the bulk of the semantic content of the whole. Further, the syntactic distribution of the whole cannot be viewed as being determined by the one or the other of the parts. The classic instance of an <b>exocentric</b> <b>construction</b> is the sentence (in a phrase structure grammar). The traditional binary division of the sentence (S) into a subject noun phrase (NP) and a predicate verb phrase (VP) was exocentric: ...|$|E
5000|$|In {{theoretical}} linguistics, {{a distinction}} is made between endocentric and <b>exocentric</b> <b>constructions.</b> A grammatical construction (e.g. a phrase or compound word) {{is said to}} be endocentric if it fulfils the same linguistic function as one of its parts, and exocentric if it does not. The distinction reaches back at least to Bloomfield's work of the 1930s. Such a distinction is possible only in phrase structure grammars (constituency grammars), since in dependency grammars all constructions are necessarily endocentric.|$|R
40|$|<b>Exocentric</b> (aredundant) <b>constructions</b> {{to which}} belong e. g. prepositional phrases periphrastic forms within the verb phrase, phraseologisms, idiomatic expressions, {{are the result}} of {{analysis}} of syntactic constructions according to the method of immediate constituents. The article contains their survey together with the examples of derivation in Polish and German...|$|R
40|$|This thesis aims {{to develop}} a Relation Extraction {{algorithm}} to extract knowledge out of automotive data. While most approaches to Relation Extraction are only evaluated on newspaper data dealing with general relations from the business world their applicability to other data sets is not well studied. Part I of this thesis deals with theoretical foundations of Information Extraction algorithms. Text mining cannot {{be seen as the}} simple application of data mining methods to textual data. Instead, sophisticated methods have to be employed to accurately extract knowledge from text which then can be mined using statistical methods from the field of data mining. Information Extraction itself can be divided into two subtasks: Entity Detection and Relation Extraction. The detection of entities is very domain-dependent due to terminology, abbreviations and general language use within the given domain. Thus, this task has to be solved for each domain employing thesauri or another type of lexicon. Supervised approaches to Named Entity Recognition will not achieve reasonable results unless they have been trained for the given type of data. The task of Relation Extraction can be basically approached by pattern-based and kernel-based algorithms. The latter achieve state-of-the-art results on newspaper data and point out the importance of linguistic features. In order to analyze relations contained in textual data, syntactic features like part-of-speech tags and syntactic parses are essential. Chapter 4 presents machine learning approaches and linguistic foundations being essential for syntactic annotation of textual data and Relation Extraction. Chapter 6 analyzes the performance of state-of-the-art algorithms of POS tagging, syntactic parsing and Relation Extraction on automotive data. The findings are: supervised methods trained on newspaper corpora do not achieve accurate results when being applied on automotive data. This is grounded in various reasons. Besides low-quality text, the nature of automotive relations states the main challenge. Automotive relation types of interest (e. g. component – symptom) are rather arbitrary compared to well-studied relation types like is-a or is-head-of. In order to achieve acceptable results, algorithms have to be trained directly on this kind of data. As the manual annotation of data for each language and data type is too costly and inflexible, unsupervised methods are the ones to rely on. Part II deals with the development of dedicated algorithms for all three essential tasks. Unsupervised POS tagging (Chapter 7) is a well-studied task and algorithms achieving accurate tagging exist. All of them do not disambiguate high frequency words, only out-of-lexicon words are disambiguated. Most high frequency words bear syntactic information and thus, {{it is very important to}} differentiate between their different functions. Especially domain languages contain ambiguous and high frequent words bearing semantic information (e. g. pump). In order to improve POS tagging, an algorithm for disambiguation is developed and used to enhance an existing state-of-the-art tagger. This approach is based on context clustering which is used to detect a word type’s different syntactic functions. Evaluation shows that tagging accuracy is raised significantly. An approach to unsupervised syntactic parsing (Chapter 8) is developed in order to suffice the requirements of Relation Extraction. These requirements include high precision results on nominal and prepositional phrases as they contain the entities being relevant for Relation Extraction. Furthermore, accurate shallow parsing is more desirable than deep binary parsing as it facilitates Relation Extraction more than deep parsing. Endocentric and <b>exocentric</b> <b>constructions</b> can be distinguished and improve proper phrase labeling. unsuParse is based on preferred positions of word types within phrases to detect phrase candidates. Iterating the detection of simple phrases successively induces deeper structures. The proposed algorithm fulfills all demanded criteria and achieves competitive results on standard evaluation setups. Syntactic Relation Extraction (Chapter 9) is an approach exploiting syntactic statistics and text characteristics to extract relations between previously annotated entities. The approach is based on entity distributions given in a corpus and thus, provides a possibility to extend text mining processes to new data in an unsupervised manner. Evaluation on two different languages and two different text types of the automotive domain shows that it achieves accurate results on repair order data. Results are less accurate on internet data, but the task of sentiment analysis and extraction of the opinion target can be mastered. Thus, the incorporation of internet data is possible and important as it provides useful insight into the customerś thoughts. To conclude, this thesis presents a complete unsupervised workflow for Relation Extraction – except for the highly domain-dependent Entity Detection task – improving performance of each of the involved subtasks compared to state-of-the-art approaches. Furthermore, this work applies Natural Language Processing methods and Relation Extraction approaches to real world data unveiling challenges that do not occur in high quality newspaper corpora...|$|R
40|$|Eugene A. Nida's A Synopsis of English Syntax (abr. Synopsis) was {{completed}} as a doctoral dissertation in 1943, but not formally published until 1960, though it circulated {{in a limited}} way in several dittoed or mimeographed editions. In this book he tried to give a comprehensive treatment of the syntax of the English language {{on the basis of}} immediate constituents, and in its second chapter "Criticism of Former Treatments of English Syntax" criticized such grammarians as O. Jespersen, H. Sweet, H. Poutsma, E. Kruisinga, G. O. Curme, E. A. Sonnenschein, and so forth. In the second and revised edition of Synopsis, published in 1966, Eugene A. Nida retracted some parts of his criticism of former treatments of English syntax. Apart from the minor changes of the expressions, the parts of criticism he withdrew in the second edition are as follows: 1) The making of an attributive barking in the barking dog of the same rank as barks in the dog barks because of the parallelism in meaning value does not account for the difference in endocentric and <b>exocentric</b> <b>constructions</b> which Bloomfield so adequately demonstrates. (p. 14) 2) Jespersen's adherence to his basic assumption as to the fundamental logic within language has led him into rather serious distortion and complication of the formal and functional values. For example, the setting up of "nexus" substantives, and dividing expression such as the doctor's arrival from the man's house, because the first is equivalent in meaning to a "nexus" construction, the doctor arrives, is largely unwarranted since there are no paralleling formal or functional differences. (pp. 27 - 28) 3) Often {{there is a tendency to}} explain some particular linguistic item by broad generalizations, as for example in Jespersen's statement, "But there is in all languages a tendency to place a weakly stressed pronoun as near to the verb as possible. " This statement may be true for many languages, but there are certainly many other languages of which this may not be said. But more fundamental than a misstatement is the implication that with the present meagre knowledge which is possessed concerning "all languages" one is able to lay down such generalized principles. (p. 34) What tempted him to withdraw the above criticism of O. Jespersen? It is said that the rise and development of what is called "Generative Transformational Grammar" goes a long way toward his withdrawing it. Especially decisive in this point is the following sharp criticism of Eugene A. Nida offered by N. Chomsky. A) Thus Nida, in his valuable study (1943) of English syntax within the immediate constituent framework, criticizes Jespersen sharply for his "serious distortion and complication of the formal and functional values" in assigning to "the doctor's arrival," but not "the doctor's house," a structural description that indicates that the Subject-Verb relation appears in the former but not in the latter phrase. But clearly Jespersen's account is correct on the level of descriptive adequacy, and the fact that the data-processing operations of modern linguistics fail to provide the correct information indicates only that they are based on an erroneous conception of linguistic structure, or that observational adequacy is being taken as the only relevant concern. ("Current Issues in Linguistic Theory" in Jerry A. Fodor/Jerrold J. Katz (ed.) : The Structure of Language pp. 63 - 64) B) Nida also criticizes Jespersen, on essentially the same grounds, for describing "barking" in "the barking dogs" (sic) as an attributive of the same rank as "barks" in "the dog barks. " Again Jespersen's decision seems to me unquestionably correct from the point of view of descriptive adequacy, though internally unmotivated (i. e., deficient from the point of view of explanatory adequacy). (op. cit. p. 64 Footnote 10) This is why Eugene A. Nida retracted his criticism of O. Jespersen. In A), N. Chomsky is correct, and it is natural that Eugene A. Nida should have withdrawn his criticism. In B), however, N. Chomsky overestimates the Rank theory of O. Jespersen. In his statement that barking in "the barking dog" and barks in "the dog barks" are attributives of the same rank, O. Jespersen didn't perceive the deep structure that N. Chomsky can recognize, but kept in mind those rules of "concord" which play such a large rôle in the older languages of the Indo-European family. This is shown in his own remarks. "This formal inseparability of subordinate elements is at the root of those rules of concord which play such a large rôle in the older languages of our Aryan family, but which tend to disappear in the more recent stages. By concord we mean the fact that a secondary word (adjective or verb) is made to agree with the primary word (substantive or subject) to which it belongs. " (Language p. 335) In O. Jespersen's Rank theory the notion of Rank was at first devised with regard to Junction and then transferred to Nexus. Though he never explicitly abandoned the application of the principle of Rank to Nexus that has the form of sentences, he was not successful in this point...|$|R

