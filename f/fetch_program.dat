0|27|Public
50|$|On 1 February 2015, HGTV {{launched}} in Australia on Australian IPTV service <b>Fetch</b> TV. <b>Programming</b> from the network also airs free-to-air on Nine Network's digital network 9Life.|$|R
50|$|The 8051 is {{designed}} as a strict Harvard architecture; it can only execute code <b>fetched</b> from <b>program</b> memory, and has no instructions to write to program memory.|$|R
40|$|Trace cache, an {{instruction}} fetch technique that reduces taken branch penalties by storing and <b>fetching</b> <b>program</b> instructions in dynamic execution order, dramatically improves instruction <b>fetch</b> bandwidth. Similarly, <b>program</b> transformations like loop unrolling, procedure inlining, feedback-directed program restructuring, and profiledirected feedback can improve instruction fetch bandwidth {{by changing the}} static structure and ordering of a program's basic blocks. We examine the interaction of these compiletime and run-time techniques {{in the context of}} a high-quality production compiler that implements such transformations and a cycle-accurate simulation model of a wide issue superscalar processor. Not surprisingly, we find that the relative benefit of adding trace cache declines with increasing optimization level, and vice versa. Furthermore, we find that certain optimizations that improve performance on a processor model without trace cache can actually degrade performance on a processor [...] ...|$|R
5000|$|... #Caption: Labeling of the workpiece, {{connection}} to work schedule <b>program</b> <b>fetch</b> per barcode ...|$|R
5000|$|For {{the purpose}} of <b>fetching</b> {{constant}} data, <b>program</b> memory is addressed bytewise through the Z pointer register, prepended if necessary by RAMPZ.|$|R
30|$|The {{engine is}} {{programmed}} via a script (contain the entire program) enters a primitive compiler. The compiler outputs a (.hex and.mif) memory initialization file {{for the program}} memory. The output file containing the binary vector is downloaded into the program memory to begin the <b>program</b> <b>fetching.</b>|$|R
5000|$|The Linkage Editor organizes a {{load module}} in a {{specialized}} format consisting of alternating [...] "text records" [...] and [...] "control/relocation dictionary records". This organization allows a load module {{to be completely}} loaded and relocated with one input/output operation by <b>Program</b> <b>Fetch</b> (EXCP on pre-MVS systems, or STARTIO on MVS/370 and later systems).|$|R
40|$|The P 89 C 51 RA 2 /RB 2 /RC 2 /RD 2 xx {{contains}} a non-volatile 8 KB/ 16 KB/ 32 KB/ 64 KB Flash program memory {{that is both}} parallel programmable and serial In-System and In-Application Programmable. In-System Programming (ISP) allows the user to download new code while the microcontroller sits in the application. In-Application Programming (IAP) means that the microcontroller <b>fetches</b> new <b>program</b> code and reprograms itself while in the system. This allows for remote programming over a modem link. A default serial loader (boot loader) program in ROM allows serial In-System programming of the Flash memory via the UART {{without the need for}} a loader in the Flash code. For In-Application Programming, the user program erases and reprograms the Flash memory by use of standard routines contained in ROM. The device supports 6 -clock/ 12 -clock mode selection by programming a Flash bit using parallel programming o...|$|R
5000|$|The OPR {{instruction}} {{was said to}} be [...] "microcoded." [...] This did not mean what the word means today (that a lower-level <b>program</b> <b>fetched</b> and interpreted the OPR instruction), but meant that each bit of the instruction word specified a certain action, and the programmer could achieve several actions in a single instruction cycle by setting multiple bits. In use, a programmer would write several instruction mnemonics alongside one another, and the assembler would combine them with OR to devise the actual instruction word. Many I/O devices supported [...] "microcoded" [...] IOT instructions.|$|R
5000|$|The {{operating}} system requires all executable programs {{to be stored}} in libraries because the member's directory entry contains additional attribute information specific to load modules. When used for storing load modules, directories also contain, among other data, {{the size of the}} load module and the address of the first [...] "text record", which is different from the address of the first member data. Executable programs are written to libraries by the linkage editor and loaded into user-acquired storage by the Loader (an application program) or into system-acquired storage by <b>Program</b> <b>Fetch</b> (a component of the OS supervisor).|$|R
5000|$|Unrolling - duplicates {{the body}} of the loop {{multiple}} times, in order to decrease the number of times the loop condition is tested and the number of jumps, which may degrade performance by impairing the instruction pipeline. Completely unrolling a loop eliminates all overhead (except multiple instruction <b>fetches</b> and increased <b>program</b> load time), but requires that the number of iterations be known at compile time (except in the case of Just-in-time compilation). Care must also be taken to ensure that multiple re-calculation of indexed variables is not a greater overhead than advancing pointers within the original loop.|$|R
2500|$|A RASP or Random access {{stored program}} machine {{begins as a}} counter machine with its [...] "program of instruction" [...] placed in its [...] "registers". Analogous to, but {{independent}} of, the finite state machine's [...] "Instruction Register", {{at least one of}} the registers (nicknamed the [...] "program counter" [...] (PC)) and one or more [...] "temporary" [...] registers maintain a record of, and operate on, the current instruction's number. The finite state machine's TABLE of instructions is responsible for (i) <b>fetching</b> the current <b>program</b> instruction from the proper register, (ii) parsing the <b>program</b> instruction, (iii) <b>fetching</b> operands specified by the program [...] instruction, and (iv) executing the program instruction.|$|R
50|$|A few Harvard {{architecture}} processors, {{such as the}} MAXQ, can execute instructions fetched {{from any}} memory segment—unlike the original Harvard processor, which can only execute instructions <b>fetched</b> from the <b>program</b> memory segment.Such processors, like other Harvard architecture processors—and unlike pure Von Neumann architecture—can read an instruction and read a data value simultaneously, if they're in separate memory segments, since the processor has (at least) two separate memory segments with independent data buses.The most obvious programmer-visible difference between this kind of modified Harvard architecture and a pure Von Neumann architecture is that—when executing an instruction from one memory segment—the same memory segment cannot be simultaneously accessed as data.|$|R
5000|$|A RASP or Random access {{stored program}} machine {{begins as a}} counter machine with its [...] "program of instruction" [...] placed in its [...] "registers". Analogous to, but {{independent}} of, the finite state machine's [...] "Instruction Register", {{at least one of}} the registers (nicknamed the [...] "program counter" [...] (PC)) and one or more [...] "temporary" [...] registers maintain a record of, and operate on, the current instruction's number. The finite state machine's TABLE of instructions is responsible for (i) <b>fetching</b> the current <b>program</b> instruction from the proper register, (ii) parsing the <b>program</b> instruction, (iii) <b>fetching</b> operands specified by the program [...] instruction, and (iv) executing the program instruction.|$|R
40|$|Cloud {{computing}} {{has become}} a hot topic in the IT industry. Great efforts {{have been made to}} establish cloud computing platforms for enterprise users, mostly small businesses. However, there are few researches about the impact of cloud computing over individual users. In this paper we focus on how to provide personalized services for individual users in the cloud environment. We argue that a personalized cloud service shall compose of two parts. The client side program records user activities on personal devices such as PC. Besides that, the user model is also computed on the client side to avoid server overhead. The cloud side <b>program</b> <b>fetches</b> the user model periodically and adjusts its results accordingly. We build a personalized cloud data search engine prototype to prove our idea...|$|R
40|$|Delivering the {{instruction}} stream {{can be the}} largest source of energy consumption in a processor, yet loosely-encoded RISC instruction sets are wasteful of instruction bandwidth. Aiming to improve the performance and energy efficiency of the RISC-V ISA, this thesis proposes RISC-V Compressed (RVC), a variable-length instruction set extension. RVC is a superset of the RISC-V ISA, encoding the most frequent instructions in {{half the size of}} a RISC-V instruction; the remaining functionality is still accessible with full-length instructions. RVC programs are 25 % smaller than RISC-V <b>programs,</b> <b>fetch</b> 25 % fewer instruction bits than RISC-V programs, and incur fewer instruction cache misses. Its code size is competitive with other compressed RISCs. RVC is expected to improve the performance and energy per operation of RISC-V. 1...|$|R
40|$|To boost {{clock rate}} for {{performance}} goals, RISC cores are widely adopted in designing embedded systems. However the fixed-length instruction sets of RISC architecture have poor code density thus burden memory bus contention. For an embedded system, it’s common to have multiple bus masters connected to system bus and contend for bandwidth. This paper proposes code compression architecture to mitigate such conditions. The scheme we posed can effectively alleviate {{the stress on}} bus contention by reducing traffic due to <b>program</b> <b>fetch.</b> Meanwhile, the instruction cache can be virtually expanded to increase performance. Our results show that memory traffic can be significantly reduced without performance degradation. The proposed scheme achieves 47 % reduction on memory traffic and provides 8 % performance gain over the baseline system...|$|R
40|$|In {{this paper}} {{we present a}} {{straightforward}} technique for compressing the instruction stream for programs that overcomes some {{of the limitations of}} earlier proposals. After code generation, the instruction stream is analysed for frequently used sequences of instructions from within the program's basic blocks. These patterns of multiple instructions are then mapped into single byte opcodes. This constitutes a compression of multiple, multi-byte operations onto a single byte. When compressed opcodes are detected during the instruction <b>fetch</b> cycle of <b>program</b> execution, they are expanded within the CPU into the original (multi-cycle) sequence of instructions. We only examine patterns within a program's basic block, so branch instructions and their targets are unaffected by this technique allowing compression to be decoupled from compilation. 1. Introduction Compilers are universally used for program development, due to the complexity of managing the large applications developed today. How [...] ...|$|R
40|$|A {{slipstream}} processor {{reduces the}} length of a running program by dynamically skipping computation non-essential for correct forward progress. The shortened program runs faster as a result, but it is speculative. So a second, unreduced copy of the program is run concurrently with and slightly behind the reduced copy [...] - leveraging a chip multiprocessor (CMP) or simultaneous multithreading (SMT). The short program passes its control and data flow outcomes to the full program for checking. And as it checks the short program, the full <b>program</b> <b>fetches</b> and executes more efficiently due to having an accurate picture of the future. Both programs are sped up: combined, they outperform conventional non-redundant execution. We study slipstreaming with the following key results. 1. A 12 % average performance improvement is achieved by harnessing an otherwise unused, additional processor in a CMP. Slipstreaming using two small superscalar cores often achieves similar instructions-per-cycle as one [...] ...|$|R
40|$|The {{performance}} of instruction memory {{is a critical}} factor for both large, high performance applications and for embedded systems. With high performance systems, the bandwidth to the instruction cache can be the limiting factor for execution speed. Code density is often the critical factor for embedded systems. In this report we demonstrate a straightforward technique for compressing the instruction stream for programs. After code generation, the instruction stream is analysed for often reused sequences of instructions from within the program's basic blocks. These patterns of multiple instructions are then mapped into single byte opcodes. This constitutes a compression of multiple, multi-byte operations onto a single byte. When compressed opcodes are detected during the instruction <b>fetch</b> cycle of <b>program</b> execution, they are expanded within the CPU into the original (multi-cycle) set of instructions. Because we only operate within a program's basic block, branch instructions and their t [...] ...|$|R
50|$|TPF {{also had}} its {{programs}} allocated as 381, 1055 and 4K bytes {{in size and}} each program consisted of a single record (aka segment). Therefore, a comprehensive application needed many segments. With the advent of C support, application programs were no longer limited to 4K sizes, much larger C programs could be created, loaded to the TPF system as multiple 4K records and read into memory during a fetch operation and correctly reassembled. Since core memory was at a premium in the past, only highly used programs ran 100% of the time as core resident, most ran as file resident. Given the limitations of older hardware, and even today's relative limitations, a <b>fetch</b> of a <b>program,</b> be it a single 4K record or many, is expensive. Since core memory is monetarily cheap and physically much much larger, greater numbers of programs could be allocated to reside in core. With the advent of z/TPF, all programs will reside in core — eventually — {{the only question is}} when they get fetched the first time.|$|R
40|$|A good {{random number}} {{generator}} is essential for many graphics applications. As more such applications move onto parallel processing, {{it is vital that}} a good parallel {{random number generator}} be used. Unfortunately, most random number generators today are still sequential, exposing performance bottlenecks and denying random accessibility for parallel computations. Furthermore, popular parallel random number generators are still based off sequential methods and can exhibit statistical bias. In this paper, we propose a random number generator that maps well onto a parallel processor while possessing white noise distribution. Our generator is based on cryptographic hash functions whose statistical robustness has been examined under heavy scrutiny by cryptologists. We implement our generator as a GPU pixel program, allowing us to compute random numbers in parallel just like ordinary texture fetches: given a texture coordinate per pixel, instead of returning a texel as in ordinary texture <b>fetches,</b> our pixel <b>program</b> computes a random noise value based on this given texture coordinate. We demonstrate that our approach features the best quality, speed, and random accessibility for graphics applications. Copyright © 2008 by the Association for Computing Machinery, Inc. link_to_subscribed_fulltex...|$|R
40|$|This chapter {{describes}} the Grayson Electronics boot program {{that came with}} the prototype decoder board, and the application program that was integrated with it. Before the programs are discussed, the DS 80 C 320 architecture is described first. 5. 1 DS 80 C 320 Architecture The DS 80 C 320 has three basic memory address spaces: 64 KB Program Memory, 64 KB External Data Memory, and 384 bytes Internal Data Memory. This differs from the Motorola 68 HC 11 microcontroller which does not distinguish between program memory space and data memory space. Figure 5. 1 shows the DS 80 C 320 memory map. The program memory space has an internal and an external memory portion. If the EA pin on the microcontroller is held high, the DS 80 C 320 executes out of internal program memory unless the address exceeds 1 FFFh. Locations 2000 - FFFFh are <b>fetched</b> from external <b>program</b> memory. If the EA pin is held low, the DS 80 C 320 fetches all instructions from external program memory. The EA pin is held low on the decoder board. The data memory space also has an internal and an external memory portion. The interna...|$|R
40|$|Today, {{integrated}} circuits are fabricated {{with more and}} more devices existent on a unit area. Consequently, the inspection of such a complicated circuit poses a critical problem. A detailed visual inspection is the most effective method to screen out unreliable IC 2 ̆ 7 s. ^ But such a labor-intensive task is not suitable to be done by human operators. Also, the alignment and wire-bonding of integrated circuit are not yet fully automated. Consequently, the visual inspection process and the wire-bonding process constitute the bottle-neck of IC production. To automate these processes by means of pattern recognition and image processing techniques is a challenge. ^ A formulation of an automatic visual inspection and final packaging system is presented in this thesis. Such a system is composed of three subsystems: (1) the image segmentation and registration subsystem, (2) the visual inspection subsystem, and (3) the final packaging subsystem. The information within the visual inspection subsystem flows among three parts: (1) the visual inspection controller, (2) the design and inspection specification data base, and (3) the library of algorithms. The inspection controller decomposes the inspection job into smaller tasks. For each task, the controller activates a <b>program</b> <b>fetched</b> from the <b>program</b> library to operate on the sensed data by consulting the appropriate reference data. At the end of a task, the controller makes a decision of what to do next, depending on the result of the task. At last, the controller generates a report of the inspection and stops. ^ An integrated system for automatic visual inspection and wirebonding of {{integrated circuits}} is proposed in this thesis. The structure of such an integrated system follows the above general formulation. The functions of the image segmentation and registration subsystem include image segmentation, gross registration, and the microregistration of mask subpatterns. The input image S is first decomposed into the image W of the IC chip and the background. The gross registration of the IC chip is achieved by locating the mask frame. But since an IC image is a multi-layer image, each mask subpattern has to be micro-registered independently. The functions of the visual inspection subsystem include image transformation, detection and identification of IC defects, and classification. The idea of image transformation is to emphasize relevant information and to suppress irrelevant information. Furthermore, contextual information is made use of to remove ambiguity. Each IC defect is to be detected by an individual defect detector. Finally, the IC under inspection is classified into three classes: (1) accept, (2) reject, and (3) to be reworked. The final packaging subsystem solders an acceptable IC chip to the package substrate, detects the misregistration introduced during the soldering process, and then carries out the wire-bonding. ^ Such an integrated system proposed in this thesis is proved to be successful by experimental results. ...|$|R
40|$|Abstract. There is {{a growing}} {{interest}} {{in the use of}} speculative multithreading to speed up the execution of sequential programs. In this execution model, threads are extracted from sequential code and are speculatively executed in parallel. This makes it possible to use parallel processing to speed up ordinary applications, which are typically written as sequential programs. This paper has two objectives. The first is to highlight the problems involved in performing accurate return address predictions in speculative multithreaded processors, where many of the subroutine call instructions and return instructions are <b>fetched</b> out of <b>program</b> order. A straightforward application of a return address stack popular scheme for predicting return addresses in single-threaded environments does not work well in such a situation. With out-of-order fetching of call instructions as well as return instructions, pushing and popping of return addresses onto and from the return address stack happen in a somewhat random fashion. This phenomena corrupts the return address stack, resulting in poor prediction accuracy for return addresses. The second objective of the paper is to propose a fixup technique for using the return address stack in speculative multithreaded processors. Our technique involves the use of a distributed return address stack, with facilities for repair when out-of-order pushes and pops happen. Detailed simulation results of the proposed schemes show significant improvements in the predictability of return addresses in a speculative multithreaded environment. ...|$|R
40|$|The energy cost of {{asymmetric}} cryptography, a vital {{component of}} modern secure communications, inhibits its wide spread adoption within the ultra-low energy regimes such as Implantable Medical Devices (IMDs), Wireless Sensor Networks (WSNs), and Radio Frequency Identification tags (RFIDs). In literature, {{a plethora of}} hardware and software acceleration techniques exists for improving the performance of asymmetric cryptography. However, very {{little attention has been}} focused on the energy efficiency. Therefore, in this dissertation, I explore the design space thoroughly, evaluating proposed hardware acceleration techniques in terms of energy cost and showing how effective they are at reducing the energy per cryptographic operation. To do so, I estimate the energy consumption for six different hardware/software configurations across five levels of security, including both GF(p) and GF(2 ^m) computation. First, we design and evaluate an efficient baseline architecture for pure software-based cryptography, which is centered around a pipelined RISC processor with 256 KB of program ROM and 16 KB of RAM. Then, we augment our processor design with simple, yet beneficial instruction set extensions for GF(p) computation and evaluate the improvement in terms of energy per cryptographic operation compared to the baseline microarchitecture. While examining the energy breakdown of the system, it became clear that <b>fetching</b> instructions from <b>program</b> memory was contributing significantly to the overall energy consumption. Thus, we implement a parameterizable instruction cache and simulate various configurations. We determine that for our working set, the energy-optimal instruction cache is 4 KB, providing a 25 % energy improvement over the baseline architecture for a 192 -bit key-size. Next, we introduce a reconfigurable GF(p) accelerator to our microarchitecture and mea sure the energy per operation against the baseline and the ISA extensions. For ISA extensions, we show between 1. 32 and 1. 45 factor improvement in energy efficiency over baseline, while for full acceleration we demonstrate a 5. 17 to 6. 34 factor improvement. Continuing towards greater efficiency, we investigate the energy efficiency of different arithmetic by first adding GF(2 ^m) instruction set extensions to our processor architecture and comparing them to their GF(p) counterpart. Finally, we design a non-configurable 163 -bit GF(2 ^m) accelerator and perform some initial energy estimates, comparing them with our prior work. In the end, we discuss our ongoing research and make suggestions for future work. The work presented here, along with proposed future work, will aid in bringing asymmetric cryptography within reach of ultra-low energy devices...|$|R

