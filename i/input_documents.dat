125|461|Public
50|$|The {{original}} {{document is}} not changed; rather, a new document is created {{based on the}} content of an existing one. Typically, <b>input</b> <b>documents</b> are XML files, but anything from which the processor can build an XQuery and XPath Data Model can be used, such as relational database tables or geographical information systems.|$|E
50|$|SystemT {{comprises}} {{the following three}} main components: (1) AQL, a declarative rule language with a similar syntax to SQL; (2) Optimizer, which accepts AQL statements as input and generates high-performance algebraic execution plans; and (3) Executing engine, which executes the plan generated by the Optimizer and performs information extraction over <b>input</b> <b>documents.</b>|$|E
50|$|GC Mail {{can be used}} now {{to replace}} the {{existing}} less efficient and less secure methods of exchanging data between local authorities and the Police. Local authorities that deliver Housing and Council Tax benefits are {{taking part in the}} e-Transfers programme, which is e-enabling the process for delivery of Local Authority <b>Input</b> <b>Documents</b> (LAIDs) and Local Authority Claim Information (LACIs).|$|E
50|$|For instance, {{given two}} transformations T1 and T2, the two can be {{connected}} so that an <b>input</b> XML <b>document</b> is transformed by T1 {{and then the}} output of T1 is fed as <b>input</b> <b>document</b> to T2. Simple pipelines like the one described above are called linear; a single <b>input</b> <b>document</b> always goes through the same sequence of transformations to produce a single output document.|$|R
3000|$|Information Overload [...] The statistic-based methods exploit {{external}} text corpus {{to enrich}} the <b>input</b> <b>document.</b> Nevertheless, the real meanings of words in the document may be overwhelmed by {{the large amount of}} introduced external texts. Furthermore, they can only acquire very limited useful knowledge about the words in the <b>input</b> <b>document</b> since they just use the statistical information of two words in the external texts actually.|$|R
30|$|After {{incorporating}} the knowledge graph, we can add some relations that {{are derived from}} the <b>input</b> <b>document</b> to the keyterm graph. Consider two anchor nodes v_a_i and v_a_j. If their corresponding keyterms kt_i and kt_j occur in the same window, an edge is added between v_a_i and v_a_j. For each cluster, we build a keyterm graph. Then we combine all the keyterm graphs together and obtain the keyterm graph for the <b>input</b> <b>document,</b> which is the so-called document keyterm graph.|$|R
50|$|The XSLT Mapper {{displays}} <b>input</b> <b>documents</b> on the left, and {{the target}} on the right. To map data, simply drag source nodes and drop them on the target, connecting the data sources to the desired data output. On the XSLT Source tab, the XSLT is displayed composed, based on the source-target relationship defined in the mapping operation. The code being generated is standard W3C XSLT and XPath code.|$|E
5000|$|The first {{proposal}} which {{initiated the}} development of SMS was made by a contribution of Germany and France into the GSM group meeting in February 1985 in Oslo. This proposal was further elaborated in GSM subgroup WP1 Services (Chairman Martine Alvernhe, France Telecom) based on a contribution from Germany. There were also initial discussions in the subgroup WP3 network aspects chaired by Jan Audestad (Telenor). The result {{was approved by the}} main GSM group in a June '85 document which was distributed to industry. The <b>input</b> <b>documents</b> on SMS had been prepared by Friedhelm Hillebrand (Deutsche Telekom) with contributions from Bernard Ghillebaert (France Télécom). The definition that Friedhelm Hillebrand and Bernard Ghillebaert brought into GSM called for the provision of a message transmission service of alphanumeric messages to mobile users [...] "with acknowledgement capabilities". The last three words transformed SMS into something much more useful than the prevailing messaging paging that some in GSM might have had in mind.|$|E
40|$|International audienceThis paper {{presents}} an XML partitioning technique that allows main- memory query engines to process {{a class of}} XQuery queries, that we dub iterative queries, on arbitrarily large <b>input</b> <b>documents.</b> We provide a static analysis technique to recognize these queries. The static analysis is based on paths extracted from queries and does not need additional schema information. We then provide an algorithm using path information for partitioning the <b>input</b> <b>documents</b> of iter- ative queries. This algorithm admits a streaming implementation, whose effectiveness is experimentally validated...|$|E
40|$|Document {{normalization}} is {{an interactive}} process that transforms raw legacy documents into semantically well-formed and linguistically controlled documents {{with the same}} communicative intention content. A paradigm for content analysis has been implemented to select candidate semantic representations of the communicative content of an <b>input</b> <b>document.</b> This implementation reuses the formal content specification of a multilingual controlled authoring system. As a consequence, a candidate semantic representation can not only {{be associated with a}} text {{in the language of the}} <b>input</b> <b>document,</b> but also in all the languages supported by the system. This paper presents how multilingual versions of an <b>input</b> legacy <b>document</b> can be obtained interactively with a proposed implementation, and discusses the advantages and limitations of this kind of normalizing translation. ...|$|R
40|$|XML path queries {{form the}} basis of complex {{filtering}} of XML data. Most current XML path query processing techniques can be divided in two groups. Navigation-based algorithms compute results by analyzing an <b>input</b> <b>document</b> one tag at a time. In contrast, index-based algorithms take advantage of precomputed numbering schemes over the <b>input</b> XML <b>document.</b> In this paper we introduce a new indexbased technique, Index-Filter, to answer multiple XML path queries. Index-Filter uses indexes built over the document tags to avoid processing large portions of the <b>input</b> <b>document</b> that are guaranteed not to be part of any match. We analyze Index-Filter and compare it against Y-Filter, a stateof -the-art navigation-based technique. We show that both techniques have their advantages, and we discuss the scenarios under which each technique is superior to the other one. In particular, we show that while most XML path query processing techniques work off SAX events, in some cases it pays off to preprocess the <b>input</b> <b>document,</b> augmenting it with auxiliary information {{that can be used to}} evaluate the queries faster. We present experimental results over real and synthetic XML documents that validate our claims...|$|R
40|$|This paper {{presents}} {{a system that}} performs skill extraction from text documents. It outputs a list of professional skills {{that are relevant to}} a given input text. We argue that the system can be practical for hiring and management of personnel in an organization. We make use of the texts and the hyperlink graph of Wikipedia, as well as a list of professional skills obtained from the LinkedIn social network. The system is based on first computing similarities between an <b>input</b> <b>document</b> and the texts of Wikipedia pages and then using a biased, hub-avoiding version of the Spreading Activation algorithm on the Wikipedia graph in order to associate the <b>input</b> <b>document</b> with skills. ...|$|R
40|$|In this paper, {{we use the}} {{information}} redundancy in multilingual input to correct errors in machine translation and thus {{improve the quality of}} multilingual summaries. We consider the case of multidocument summarization, where the <b>input</b> <b>documents</b> are in Arabic, and the output summary is in English. Typically, information that makes it to a summary appears in many different lexical-syntactic forms in the <b>input</b> <b>documents.</b> Further, the use of multiple machine translation systems provides yet more redundancy, yielding different ways to realize that information in English. We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy, focusing on noun phrases. ...|$|E
40|$|The eld {{of digital}} {{document}} content analysis includes many important tasks, for example page segmentation or zone classi cation. It {{is impossible to}} build eective solutions for such problems and evaluate their performance without a reliable test set, that contains both <b>input</b> <b>documents</b> and expected results of segmentation and classication. In this paper we present GROTOAP | a test set useful for train-ing and performance evaluation of page segmentation and zone classication tasks. The test set contains input articles in a digital form and corresponding ground truth les. All <b>input</b> <b>documents</b> included in the test set have been selected from DOAJ database, which indexes articles published un-der CC-BY license. The whole test set is available under the same license...|$|E
40|$|The {{field of}} digital {{document}} content analysis includes many important tasks, for example page segmentation or zone classification. It {{is impossible to}} build effective solutions for such problems and evaluate their performance without a reliable test set, that contains both <b>input</b> <b>documents</b> and expected results of segmentation and classification. In this paper we present GROTOAP — a test set useful for training and performance evaluation of page segmentation and zone classification tasks. The test set contains input articles in a digital form and corresponding ground truth files. All <b>input</b> <b>documents</b> included in the test set have been selected from DOAJ database, which indexes articles published under CC-BY license. The whole test set is available under the same license. National Centre for Research and Development (NCBiR) Grant No. SP/I/ 1 / 77065 / 10 Łukasz Bolikowsk...|$|E
40|$|The {{need for}} text {{summarization}} is crucial {{as we enter}} the era of information overload. In this paper we present an automatic summarization system, which generates a summary for a given <b>input</b> <b>document.</b> Our system is based on identification and extraction of important sentences in the <b>input</b> <b>document.</b> We listed a set of features that we collect as part of summary generation process. These features were stored using vector representation model. We defined a ranking function which ranks each sentence as a linear combination of the sentence features. We also discussed about discourse coherence in summaries and techniques to achieve coherent and readable summaries. The experiments showed that the summary generated is coherent the selected features are really helpful in extracting the important information in the document. ...|$|R
40|$|Term {{extraction}} {{is one of}} {{the layers}} in the ontology development process which has the task to extract all the terms contained in the <b>input</b> <b>document</b> automatically. The purpose of this process is to generate list of terms that are relevant to the domain of the <b>input</b> <b>document.</b> In the literature there are many approaches, techniques and algorithms used for term extraction. In this paper we propose a new approach using particle swarm optimization techniques in order to improve the accuracy of term extraction results. We choose five features to represent the term score. The approach has been applied to the domain of religious document. We compare our term extraction method precision with TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental results show that our propose approach achieve better precision than those four algorithm...|$|R
40|$|In recent times, the {{necessity}} of generating single document summary has gained popularity among the researchers due to its extensive applicability. Most of the automatic text summarization systems utilize extraction-based techniques for selecting the most significant portions of text to generate coherent summaries. In this paper we will analyze the performance of fuzzified neural network approach with the graph theory approach. In the proposed system, we have developed an efficient automatic text summarization system based neural network and fuzzy logic. In the training phase at first, the feature vector is computed {{for a set of}} sentences using the feature extraction technique. After that, the feature vector and their corresponding fuzzy score are used to train the neural network optimally. Later in the testing phase, the <b>input</b> <b>document</b> is subjected to preprocessing and feature extraction techniques. In order to obtain the sentence score for every sentence in the <b>input</b> <b>document,</b> the feature vector is fed to the trained neural network that returns the sentence score for every sentence. Finally, by making use of sentence score, the most important sentences are extracted from the <b>input</b> <b>document.</b> The experimentation is performed with the DUC 2002 dataset and the generated summary is evaluated with the measures such as Precision, recall and f-measure. The comparative results of our proposed approach with the graph theory approach produces better results by means of different compression rates...|$|R
40|$|Theoretical thesis. Spine title: Topic {{coherence}} after document restructuring. Bibliography: leaves 52 - 56. 1. Introduction [...] 2. Background [...] 3. Literature review [...] 4. Method [...] 5. Results [...] 6. discussion [...] 7. Conclusion. This thesis examined whether simple preprocessing {{of documents}} such as lemmatising text, or removing or weighting {{certain parts of}} speech, could generate better quality topics, faster, using Latent Dirichlet Allocation (LDA) topic modelling. Past work has generally attempted to improve topic modelling performance by making changes to the topic modelling algorithm itself. This study examines the simpler option of transforming the <b>input</b> <b>documents</b> to the LDA algorithm. Topic quality was assessed {{on a range of}} measures that examined both topic interpretability, and how well the topics represented the source documents. The results indicate that topic quality was improved, and the time to generate the topics was less, if the <b>input</b> <b>documents</b> were reduced to only nouns, or nouns and adjectives, when the numbers of topics to be generated was 200 or 500 topics. This study also found that even when the number of topics to generate was not large, <b>input</b> <b>documents</b> could be reduced to select parts of speech to speed the generation of topics, with no loss of topic quality. The implications of these results are that very large data sets may benefit from being lemmatised and reduced to simply the nouns prior to topic modelling. Mode of access: World wide web 1 online resource (x, 56 leaves) table...|$|E
40|$|We {{present a}} method for {{automated}} topic suggestion. Given a plain-text input document, our algorithm produces a ranking of novel topics that could enrich the input document in a meaningful way. It can thus be used to assist human authors, who often fail to identify important topics relevant to {{the context of the}} documents they are writing. Our approach marries two algorithms originally designed for linking documents to Wikipedia articles, proposed by Milne and Witten [15] and West et al. [22]. While neither of them can suggest novel topics by itself, their combination does have this capability. The key step towards finding missing topics consists in generalizing from a large background corpus using principal component analysis. In a quantitative evaluation we conclude that our method achieves the precision of human editors when <b>input</b> <b>documents</b> are Wikipedia articles, and we complement this result with a qualitative analysis showing that the approach also works well on other types of <b>input</b> <b>documents...</b>|$|E
40|$|We discuss query {{evaluation}} for XML-based server systems {{where the}} same query is evaluated on every incoming XML message. In a typical scenario, {{many of the}} incoming messages will be highly similar to each other. Current XML query evaluators reevaluate the query from scratch on every message. We call substructures that occur in many <b>input</b> <b>documents</b> template fragments, and introduce a novel template folding method that allows to move the work of evaluating the query on recurring document substructures from the query execution engine into the query compiler. Similar to constant folding, our method avoids run-time evaluation of intermediate results whose value only depends on information that is already available at compile time. For XPath location paths, we propose a representation for such invariant intermediate results, and show {{how it can be}} incorporated into query execution plans. Such augmented execution plans improve query performance when evaluating the same query on subsequent <b>input</b> <b>documents...</b>|$|E
50|$|FIPS 140-2, {{issued on}} 25 May 2001, takes account {{of changes in}} {{available}} technology and official standards since 1994, and of comments received from the vendor, tester, and user communities. It was the main <b>input</b> <b>document</b> to the international standard ISO/IEC 19790:2006 Security requirements for cryptographic modules issued on 1 March 2006.|$|R
50|$|Some {{might argue}} that, for SRS, the input is {{the words of}} {{stakeholders}} and, therefore, SRS validation {{is the same as}} SRS verification. Thinking this way is not advisable as it only causes more confusion. It is better to think of verification as a process involving a formal and technical <b>input</b> <b>document.</b>|$|R
40|$|This paper {{discusses}} how {{to automatically}} generate slide shows. The reported presentation system <b>inputs</b> <b>documents</b> annotated with the GDA tagset, an XML tagset which allows machines to automatically infer the semantic structure underlying the raw documents. The system picks up important topics in the <b>input</b> <b>document</b> {{on the basis}} of the semantic dependencies and coreferences identified from the tags. This topic selection depends also on interactions with the audience, leading to dynamic adaptation of the presentation. A slide is composed for each topic by extracting relevant sentences and paraphrasing them to an itemized summary. Some heuristics are employed here for paraphrasing and layout. Since the GDA tagset is independent of the domain and style of documents and applicable to diverse natural languages, the reported system is also domain /style independent and easy to adapt to different languages...|$|R
40|$|Abstract. This is {{a summary}} of the {{mathematical}} aspects of the design of dbacl, a digramic Bayesian classifier for text documents. dbacl computes maximum (relative) entropy models for text corpora and can compute the Bayesian posterior distribution for a given document in terms of any number of previously computed models. Both learning and classifying is O(n) in the number of lines of the <b>input</b> <b>documents.</b> 1...|$|E
40|$|This {{appendix}} {{is divided}} into three sections. The first section contains abstracts of each of the eight computer programs in the system, instructions for keypunching the three <b>input</b> <b>documents,</b> and computer operating instructions pertaining to each program. The second section contains system flowcharts for the entire system as well as program flowcharts for each program. The last section contains PL/l program listings of each program...|$|E
40|$|Distributed {{representation}} learned with {{neural networks}} has recently {{shown to be}} effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e. g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of <b>input</b> <b>documents</b> with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when <b>input</b> <b>documents</b> are long. Comment: Published in IJCAI- 2016 : the 25 th International Joint Conference on Artificial Intelligenc...|$|E
40|$|This paper proposes XML Equivalent Transformation (XET) as an XML-based rule langauge for the Web, which {{seamlessly}} integrates human-readable {{documents and}} computer-interpretable programs by considering XML documents and XML expressions [...] -an extension of ordinary XML elements with variables [...] -as its first class programming entities. With XET, arbitrary XML documents, representing application data, information or knowledge on the Web possibly encoded in certain XML applications, become immediately a program's input data. Manipulation and computation {{of such an}} <b>input</b> <b>document</b> (data) is performed by semantically-equivalently transforming the document successively until a desirable one is obtained. The <b>input</b> <b>document</b> could be, for example, an XML database query, and thus the output or the desirable document {{is a set of}} XML elements yielding the answer to the query. The paper presents the syntax and the computation mechanism of XET and also demonstrates its application to e-business systems...|$|R
40|$|A {{method is}} {{presented}} to filter the output of a word recognition algorithm, which may contain errors, to locate decisions that should be correct {{with a high degree}} of certainty. The algorithm uses the output of a word recognition system and a vector space model for information retrieval to locate a set of documents that have topics which are similar to that of the <b>input</b> <b>document.</b> The vocabulary from these similar documents is then used to locate the correct word recognition decisions. Experimental results show that a subset of the word recognitionder;i-. sions for an <b>input</b> <b>document</b> can be,located that are between 90 and 99 percent correct. This peljormance was obtained on word recognition results for a sample of text into which a 13, 24, and 30 percent word recognition error rate had been introduced The subset located by this method can be used to drive other recognition processes applied to the rest of the text. 1...|$|R
40|$|When a multidatabase system {{contains}} textual database systems (i. e., {{information retrieval}} systems), queries against the global schema of the multidatabase system may contain {{a new type}} of joins [...] - joins between attributes of textual type. Three algorithms for processing such type of joins are presented and their I/O costs are analyzed in this paper. Since such type of joins often involve document collections of very large size, {{it is very important to}} find efficient algorithms to process them. The three algorithms differ on whether the documents themselves or the inverted files on the documents are used to process the join. Our analysis and the simulation results indicate that the relative performance of these algorithms depends on the <b>input</b> <b>document</b> collections, system characteristics and the input query. For each algorithm, the type of <b>input</b> <b>document</b> collections with which the algorithm is likely to perform well is identified. An integrated algorithm that automatically selects the [...] ...|$|R
40|$|This note {{describes}} how the Greenstone digital library system uses "plugins" to import documents and metadata in different formats, and associate metadata with the appropriate documents. Plugins that import documents can perform their own format conversion internally, or take advantage of existing conversion programs. Metadata can be read from the <b>input</b> <b>documents,</b> or from separate metadata files, or are computed from the documents themselves. New plugins can be written for novel situations...|$|E
40|$|Industry 4. 0 standards, such as AutomationML, {{are used}} to specify {{properties}} of mechatronic elements in terms of views, such as electrical and mechanical views of a motor engine. These views have to be integrated {{in order to obtain}} a complete model of the artifact. Currently, the integration requires user knowledge to manually identify elements in the views that refer to the same element in the integrated model. Existing approaches are not able to scale up to large models where a potentially large number of conflicts may exist across the different views of an element. To overcome this limitation, we developed Alligator, a deductive rule-based system able to identify conflicts between AutomationML documents. We define a Datalog-based representation of the AutomationML <b>input</b> <b>documents,</b> and a set of rules for identifying conflicts. A deductive engine is used to resolve the conflicts, to merge the <b>input</b> <b>documents</b> and produce an integrated AutomationML document. Our empirica l evaluation of the quality of Alligator against a benchmark of AutomationML documents suggest that Alligator accurately identifies various types of conflicts between AutomationML documents, and thus helps increasing the scalability, efficiency, and coherence of models for Industry 4. 0 manufacturing environments...|$|E
40|$|Abstract:Clustering {{is one of}} {{the most}} {{traditional}} data mining techniques for knowledge extraction of mass data storages and high dimensional dataset in the years of research, Un preprocessed data (Articles, prepositions,conjunctions,adverbs… [...] etc.) may leads to irrelevant clusters. In this paper we are proposing feature set extraction of preprocessing of <b>input</b> <b>documents</b> and document weights can be computed based on term frequency and inverse document frequencies then computes the mutation based centroid for each iteration, except initial iteration during clustering of documents. I...|$|E
40|$|Problem statement: Term {{extraction}} {{is one of}} {{the layers}} in the ontology development process which has the task to extract all the terms contained in the <b>input</b> <b>document</b> automatically. The purpose of this process is to generate list of terms that are relevant to the domain of the <b>input</b> <b>document.</b> In the literature there are many approaches, techniques and algorithms used for term extraction where each of approaches, techniques and algorithms has the objective to improve the precision of the extracted terms. Approach: We proposed a new approach using particle swarm optimization techniques in order to improve the precision of term extraction results. We choose five features to represent the term score. Results: The approach had been applied to the domain of Islamic documents. We compare our term extraction method with TFIDF, Weirdness, GlossaryExtraction and TermExtractor. Conclusion: The experimental results showed that our proposed approach achieves better precision than those four algorithms...|$|R
40|$|Query {{formulation}} {{is one of}} {{the most}} difficult aspects of search, especially for a novice user. We propose a new search interaction where the user searches with a reference document and the system learns from the user inputs over a period of time to “push” relevant and new content without additional user interaction. Our method is based on identifying key phrases from the <b>input</b> <b>document.</b> The key phrases are used to query a search engine and the results are evaluated for similarity to the original document. By caching documents received from a user over a period of time, a user profile is built. The profile is then used to provide recommendations to the user. Evaluations show that this method has a good precision in finding documents of interest to the user. Also our key phrase extraction method has good recall in retrieving the <b>input</b> <b>document.</b> Additional experiments reveal that our recommendation system is of help in exploring documents of interest to the user. </p...|$|R
40|$|Abstract: Problem statement: Term {{extraction}} {{is one of}} {{the layers}} in the ontology development process which has the task to extract all the terms contained in the <b>input</b> <b>document</b> automatically. The purpose of this process is to generate list of terms that are relevant to the domain of the <b>input</b> <b>document.</b> In the literature there are many approaches, techniques and algorithms used for term extraction where each of approaches, techniques and algorithms has the objective to improve the precision of the extracted terms. Approach: We proposed a new approach using particle swarm optimization techniques in order to improve the precision of term extraction results. We choose five features to represent the term score. Results: The approach had been applied to the domain of Islamic documents. We compare our term extraction method with TFIDF, Weirdness, GlossaryExtraction and TermExtractor. Conclusion: The experimental results showed that our proposed approach achieves better precision than those four algorithms. Key words: Term extraction, particle swarm optimization, feature selection, text minin...|$|R
