14|95|Public
50|$|Refreshing is the {{transfer}} of data between two types of the same storage medium {{so there are no}} bitrot changes or alteration of data. For example, transferring census data from an old preservation CD to a new one. This strategy may need to be combined with migration when the software or hardware required to read the data is no longer available or is unable to understand the format of the <b>data.</b> <b>Refreshing</b> will likely always be necessary due to the deterioration of physical media.|$|E
5000|$|The idea {{of digital}} {{dashboards}} followed {{the study of}} decision support systems in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s {{in the form of}} Executive Information Systems (EISs). Due to problems primarily with <b>data</b> <b>refreshing</b> and handling, it was soon realized that the approach wasn’t practical as information was often incomplete, unreliable, and spread across too many disparate sources. Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and online analytical processing (OLAP) allowed dashboards to function adequately. Despite the availability of enabling technologies, the dashboard use didn't become popular until later in that decade, with the rise of key performance indicators (KPIs), and the introduction of Robert S. Kaplan and David P. Norton's Balanced Scorecard. In the late 1990s, Microsoft promoted a concept known as the Digital Nervous System and [...] "digital dashboards" [...] were described as being one leg of that concept. Today, the use of dashboards forms an important part of Business Performance Management (BPM).|$|E
40|$|Most {{template}} {{detection methods}} process web pages in batches that a newly crawled page {{can not be}} processed until enough pages have been collected. This results in large storage consumption and a huge delay of <b>data</b> <b>refreshing.</b> In this paper, we present an incremental framework to detect templates in which a page is processed {{as soon as it}} has been crawled. In this framework, we don’t need to cache any web page. Experiments show that our framework consumes less than 7 % storage than traditional methods. And also the speed of <b>data</b> <b>refreshing</b> is accelerated because of the incremental manner...|$|E
5000|$|Enable {{constant}} <b>data</b> <b>refresh</b> for real-time updates {{through a}} persistent cache.|$|R
50|$|ExoClick's {{proprietary}} software offers 20+ different ad formats, optimized targeting and behavioural retargeting, access to big data statistics and strategic analysis tools to further improve ROI, <b>data</b> <b>refresh</b> every 60 seconds, day parting and 24/7 customer service. Our platform API allows developers {{to create their}} own bespoke plug ins and software add-ons for automating processes on our platform.|$|R
50|$|Current <b>data</b> is <b>refreshed</b> {{every ten}} minutes; {{it can be}} searched by city or by {{geographic}} coordinates on Earth.|$|R
40|$|This {{research}} defines Time-Locked Embargo, {{a framework}} designed {{to mitigate the}} Preservation Risk Interval: the preservation risk associated with embargoed scholarly material. Due to temporary access restrictions, embargoed data cannot be distributed freely and thus preserved via <b>data</b> <b>refreshing</b> during the embargo time interval. A solution to mitigate the risk of data loss has been developed by suggesting a data dissemination framework that allows <b>data</b> <b>refreshing</b> of encrypted instances of embargoed content in an open, unrestricted scholarly community. This framework has been developed by exploiting implementations of existing technologies to “time-lock ” data using Timed-Release Cryptology (TRC) {{so that it can}} be deployed as digital resources encoded in the MPEG- 21 Digital Item Description Language (DIDL) complex object format to harvesters interested in harvesting a local copy of content by utilizing The Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), a widely accepted interoperability standard for the exchange of metadata. The framework successfully demonstrates dynamic record identification, time-lock puzzle (TLP) encryption, encapsulation and dissemination as XML documents...|$|E
40|$|Abstract. Data {{warehousing}} is {{a booming}} industry with many interesting research problems. The database research community has concentrated on {{only a few}} aspects. In this paper, We summarize {{the state of the}} art, suggest architectural extensions and identify research problems in the areas of warehouse modeling and design, data cleansing and loading, <b>data</b> <b>refreshing</b> and purging, metadata management, extensions to relational operators, alternative implementations of traditional relational operators, special index structures and query optimization with aggregates. ...|$|E
40|$|Systems biology {{research}} {{demands the}} ability to construct custom-designed bio-entities from heterogeneous data sources. EDIT is an ongoing project aimed at building a system to allow bio-researchers with minimum computer proficiency to define {{the structure of a}} desired bio-entity, and identify the data sources for its data fields. Then, the system will automatically extract and output the data in the predefined structure and format. The system consists of a host site, which maintains and updates the configuration files specified for source databases, and user sites, which perform data extraction and integration by using the configuration files. An Event-Trigger-Rule Server is used to support <b>data</b> <b>refreshing</b> and to notify biologists about changes made to source databases. 1...|$|E
40|$|This paper {{analyses}} {{the master}} station redundancy {{technology in the}} POWERLINK network. Through the monitoring network and <b>data</b> <b>refresh</b> to achieve the master station redundancy function. The Managing Node redundancy ensures the POWERLINK cycle production continuance after {{the failure of the}} current master station, the switch-over time (recovery time) of the POWERLINK system is in the two POWERLINK cycle time at least. That ensures a very fast restoring of normal operation without any downtime for the control system. © (2013) Trans Tech Publications, Switzerland. Korea Maritime University; Hong Kong Industrial Technology Research Centre; Inha Universit...|$|R
50|$|Presents {{the latest}} SOTAWatch Alerts - <b>data</b> are <b>refreshed</b> once every minute. Each Alert as it arrives is {{presented}} in the listview on the right-hand side of the main map.|$|R
40|$|Due to {{restrictions}} {{within the}} current {{architecture of the}} global observing system (GOS), space-based remote sensing of Earth suffers from an acute data-deficit over the critical polar-regions. Currently, observation of high-latitude regions is conducted using composite images from spacecraft in geostationary (GEO) and low-Earth orbits (LEOs) [1]. However, the oblique viewing geometry from GEO-based systems to latitudes above around 55 deg [2] and the insufficient temporal resolution of spacecraft in LEO means there is currently no source of continuous imagery for polar-regions obtained with a <b>data</b> <b>refresh</b> rate of less than 15 minutes, as is typically available elsewhere for meteorological observations...|$|R
40|$|Automotive {{architectures}} {{consist of}} multiple {{electronic control units}} (ECUs) which run distributed control applications. Such ECUs are connected to sensors and actuators and communicate via shared buses. Resource arbitration at the ECUs {{and also in the}} communication medium, coupled with variabilities in execution requirements of tasks results in jitter in the signal/data streams existing in the system. As a result, buffers are required at the ECUs and bus controllers. However, these buffers often implement different semantics – FIFO queuing, which is the most straightforward buffering scheme, and <b>data</b> <b>refreshing,</b> where stale data is overwritten by freshly sampled data. Traditional timing and schedulability analysis that are used to compute, e. g., end-to-end delays, in such automotive architectures can only model FIFO buffering. As a result, they return pessimistic delay and resource estimates because in realit...|$|E
40|$|Due to {{temporary}} access restrictions, embargoed data {{cannot be}} refreshed to unlimited parties during the embargo time interval. A solution {{to mitigate the}} risk of data loss has been developed that uses a data dissemination framework, the Timed-Locked Embargo Framework (TLEF), that allows <b>data</b> <b>refreshing</b> of encrypted instances of embargoed content in an open, unrestricted scholarly community. TLEF exploits implementations of existing technologies to “timelock” data using timed-release cryptology so that TLEF can be deployed as digital resources encoded in a complex object format suitable for metadata harvesting. The framework successfully demonstrates dynamic record identification, time-lock puzzle encryption, encapsulation and dissemination as XML documents. We implement TLEF and provide a quantitative analysis of its successful data harvest of time-locked embargoed data with minimum time overhead without compromising data security and integrity...|$|E
40|$|The Department of Energy (DOE) {{is moving}} towards Long-Term Stewardship (LTS) of many {{environmental}} restoration sites that cannot be released for unrestricted use. One aspect of information management for LTS is geospatial data archiving. This report discusses {{the challenges facing}} the DOE LTS program concerning the data management and archiving of geospatial data. It discusses challenges in using electronic media for archiving, overcoming technological obsolescence, <b>data</b> <b>refreshing,</b> data migration, and emulation. It gives an overview of existing guidance and policy and discusses what the United States Geological Service (USGS), National Oceanic and Atmospheric Administration (NOAA) and the Federal Emergency Management Agency (FEMA) are doing to archive the geospatial data that their agencies are responsible for. In the conclusion, this report provides issues for further discussion around long-term spatial data archiving...|$|E
40|$|In view of device scaling issues, {{embedded}} DRAM (eDRAM) {{technology is}} being {{considered as a}} strong alternative to conventional SRAM for use in on-chip memories. Memory cells designed using eDRAM technology {{in addition to being}} logic-compatible, are variation tolerant and immune to noise present at low supply voltages. However, two major causes of concern are the data retention capability which is worsened by parameter variations leading to frequent <b>data</b> <b>refreshes</b> (resulting in large dynamic power overhead) and the transient reduction of stored charge increasing soft-error (SE) susceptibility. In this paper, we present a novel variation-tolerant 4 T-DRAM cell whose power consumption is 20. 4...|$|R
50|$|Sense {{amplifier}} {{is required}} during the <b>data</b> read and <b>refresh</b> operation from the memory concerned.|$|R
40|$|For newly {{designed}} or transformed business processes, accurately predicting business performances such as costs and customer services before actual deployment is very important. We have successfully developed {{and used a}} simulation model for the IBM’s Personal Computer Division by modeling multiple, discrete events such as customer order arrival, replenishment planning and availability <b>data</b> <b>refresh,</b> and uncertainty of demand forecast, order size and customer preference of product feature. Using the model {{we were able to}} predict dynamics of availability, ship dates and accuracy of ship date, and identified other opportunities for improvement. We have also studied how different inventory policies, supply planning policies and sourcing policies affect business performance metrics such as inventory and customer services...|$|R
40|$|S Beam {{intensity}} and lifetime measurement system {{have been implemented}} at SRRC. The average beam intensity is measured by direct current current transformer. The magnet shielding of DCCT is used to reduce stray magnet field interference, then output signal is digitized by high resolution {{analog to digital converter}} on VME crate. In order to be compatible with the <b>data</b> <b>refreshing</b> rate of main control system, which is 10 samples per second, all digitized data are filtered and decimated by digital filter to reduce noise level. The adaptive lifetime calculation algorithm are applied to provide fast time response. I. INTRODUCTION Beam intensity is a basic machine parameter should be measured at any time. A commercially available high precision DC parametric current transformer (PCT) has been chosen for measuring the beam current. Beam lifetime is also a significant machine parameter of the storage ring. The beam lifetime is calculated by the beam current measured using PC [...] ...|$|E
40|$|International audienceModern {{scientific}} repositories {{are growing}} rapidly in size. Scientists are increasingly interested in viewing the latest data {{as part of}} query results. Current scientific middleware cache systems, however, assume repositories are static. Thus, they cannot answer scientific queries with the latest data. The queries, instead, are routed to the repository until data at the cache is refreshed. In data-intensive scientific disciplines, such as astronomy, indiscriminate query routing or <b>data</b> <b>refreshing</b> often results in runaway network costs. This severely affects the performance and scalability of the repositories and makes poor use of the cache system. We present Delta a dynamic data middleware cache system for rapidly-growing scientific repositories. Delta's key component is a decision framework that adaptively decouples data objects [...] choosing to keep some data object at the cache, when they are heavily queried, and keeping some data objects at the repository, when they are heavily updated. Our algorithm profiles incoming workload to search for optimal data decoupling that reduces network costs. It leverages formal concepts from the network flow problem, and is robust to evolving scientific workloads. We evaluate the efficacy of Delta, through a prototype implementation, by running query traces collected from a real astronomy survey...|$|E
40|$|Large-scale {{distributed}} {{systems such as}} Dynamo at Amazon, PNUTS at Yahoo!, and Cassandra at Facebook, are rapidly becoming the data management platform of choice for most web applications. Those key-value data stores rely on data partitioning and replication to achieve higher levels of availability and scalability. Such design choices typically exhibit a trade-off in which data freshness is sacrificed in favor of reduced access latencies. Hence, it is indispensable to optimize resource allocation in order to minimize: 1) query tardiness, i. e., maximize Quality of Service (QoS), and 2) data staleness, i. e., maximize Quality of Data (QoD). That trade-off between QoS and QoD is further manifested at the local-level (i. e., replica-level) and is primarily shaped by the resource allocation strategies deployed for managing the processing of foreground user queries and background system updates. To this end, we propose the AFIT scheduling strategy, which allows for selective <b>data</b> <b>refreshing</b> and integrates the benefits of SJF-based scheduling with an EDF-like policy. Our experiments demonstrate the effectiveness of our method, which does not only strike a fine trade-off between QoS and QoD but also automatically adapts to workload settings...|$|E
40|$|In {{order to}} achieve data {{delivery}} in an opportunistic network, data is replicated when it is transmitted to nodes within communication reach and {{that are likely to}} be able to forward it closer to the destination. This replication and the unpredictable contact times due to mobility necessitate buffer management strategies to avoid buffer overflow on nodes. In this paper, we investigate buffer management strategies based on local forwarding statistics and relevance of the data for other nodes. The results obtained on our emulation platform for opportunistic networks show that strategies with a high <b>data</b> <b>refresh</b> rate achieve the most efficient delivery and generate the smallest overhead on our community and mobility scenarios. Categories andSubject Descriptor...|$|R
5000|$|READOUT key: {{reads the}} data from the {{specified}} address and displays the <b>refreshed</b> <b>data</b> every half second ...|$|R
40|$|A 256 word by 8 -bit {{random access}} memory chip was {{developed}} utilizing p channel, metal gate metal-nitride-oxide-silicon (MNOS) technology; with operational characteristics of a 2. 5 microsecond read cycle, a 6. 0 microsecond write cycle, 800 milliwatts of power dissipation; and retention characteristics of 10 to the 8 th power read cycles before <b>data</b> <b>refresh</b> and 5000 hours of no power retention. Design changes were implemented to reduce switching currents that caused parasitic bipolar transistors inherent in the MNOS structure to turn on. Final wafer runs exhibited acceptable yields for a die 250 mils on a side. Evaluation testing {{was performed on the}} device {{in order to determine the}} maturity of the device. A fixed gate breakdown mechanism was found when operated continuously at high temperature...|$|R
40|$|Applications that monitor {{functions}} over {{rapidly and}} unpredictably changing data, express their needs as continuous queries. Our {{focus is on}} a rich class of queries, expressed as polynomials over multiple data items. Given a set of polynomial queries at a coordinator C, and a user-specified accuracy bound (tolerable imprecision) for each query, we {{address the problem of}} assigning data accuracy bounds or filters to the source of each data item. Assigning data accuracy bounds for non-linear queries poses special challenges. Unlike linear queries, data accuracy bounds for non-linear queries depend on the current values of data items and hence need to be recomputed frequently. So, we seek an assignment such that a) if the value of each data item at C is within its data accuracy bound then the value of each query is also within its accuracy bound, b) the number of <b>data</b> <b>refreshes</b> sent by sources to C to meet the query accuracy bounds, is as low as possible, and c) the number of times the data accuracy bounds need to be recomputed is as low as possible. In this paper, we couple novel ideas with existing optimization techniques to derive such an assignment. Specifically, we make the following contributions: (i) Propose a novel technique that significantly reduces the number of times data accuracy bounds must be recomputed; (ii) Show that a small {{increase in the number of}} <b>data</b> <b>refreshes</b> can lead to a large reduction in the number of recomputations; we introduce this as a tradeoff in our approach; (iii) Give principled heuristics for addressing negative coefficient polynomial queries where no known optimization techniques can be used; we also prove that under many practically encountered conditions our heuristics can be close to optimal; and (iv) Present a detailed experimental evaluation demonstrating the efficacy of our techniques in handling large number of polynomial queries. © IEE...|$|R
40|$|NHN Corporation‘s portals gain {{speed and}} {{accelerate}} response {{times with a}} <b>data</b> center <b>refresh</b> and applications optimized for the Intel ® Xeon ® processor 5500 series CHALLENGES • Increase server utilization, reduce costs. Enable higher performance with faster response times at the data center in order to deliver superior portal services to their customers...|$|R
40|$|Monitoring {{continuous}} queries over distributed {{data sources}} typically requires replicating data continuously at a central location for query monitoring, incurring significant communication overhead when source data is updated. We propose {{a new technique}} that reduces communication overhead by using approximate caching. Our approach enables users to register continuous queries with precision constraints. The system caches approximate data values with just enough accuracy to meet the precision constraints of all registered continuous queries at all times, while dynamically and adaptively allocating imprecision among cached values using an algorithm that minimizes <b>data</b> <b>refreshes.</b> Through experimental simulation over synthetic and real-world data, we demonstrate the effectiveness of our approach in reducing communication costs significantly compared with other approaches. Most importantly, we show that our algorithm enables users to trade precision for communication cost at a fine granularity by individually adjusting the precision constraints of continuous queries in a large multiquery workload, a feature that no known previous algorithm can provide. ...|$|R
40|$|On-line {{decision}} making often involves query processing over time-varying data which {{arrives in the}} form of data streams from distributed locations. In such environments typically, a user application is interested in the value of some function defined over the data items. For example, the traffic management system can make control decisions based on the observed traffic at major intersections; stock investors can manage their investments based on the value of their portfolios. In this paper we present a system that supports pull based <b>data</b> <b>refresh</b> and query processing techniques where such queries access data from multiple distributed sources. Key challenges in supporting such Continuous Multi-Data Incoherency Bounded Queries lie in minimizing network and source overheads, without loss of fidelity in the query responses provided to users. We address these challenges by using mathematically sound approaches based on Gradient Descent and Constraint Optimization which allow us to adapt the refresh frequencies of the dynamically changing data and adjust the quality of service provided to different users. 1...|$|R
50|$|Presents {{the latest}} SOTAWatch Spots - <b>data</b> are <b>refreshed</b> once every minute. Each Spot as it arrives is {{presented}} in the listview on the right-hand side of the main map. Clicking on an entry in the list will cause the map to navigate to the summit mentioned in the Spot and an information window will open, listing all Spots reported for that summit within the time-frame of the query.|$|R
40|$|International audienceIncreasing {{the number}} of bits per cell and {{technology}} scaling are {{ways to reduce the}} cost per gigabyte of Flash memo- ries and solid-state drives (SSDs). Unfortunately, this trend has a negative impact on data retention time and cycling endurance. Periodic <b>data</b> <b>refresh</b> allows dealing with a reduced retention time and, implicitly, improves cycling endurance. A fixed data- refresh frequency is not optimal {{in the presence of a}} variable operating temperature since it may become unnecessarily large and alter the SSD response latency and energy consumption. Here, a flexible data-refresh methodology is proposed based on timestamps and approximations of the Arrhenius-curves em- ployed to describe the temperature impact on Flash data reten- tion. The proposed approximations can be easily implemented with the help of a processor-based solution or a proposed small module called A-timer. For an asymmetric temperature distri- bution between 30 °C and 70 °C, it is estimated that the refresh frequency can be reduced by more than 50 × and almost 3 × for failure mechanisms with activation energies of 1. 1 eV and 0. 3 eV, respectively...|$|R
40|$|Abstract. In the paper, an {{attitude}} and heading reference {{system based on}} MIMU/magnetometers with moderate accuracy is presented. To {{meet the requirements of}} the real-time measurement, a master/slave CPU structure is proposed in order to improve the <b>data</b> <b>refresh</b> rate effectively. In the algorithm part, an adaptive extended Kalman filter equation is applied in the system, where the filter equation uses three tilt angles of attitude and three bias errors for the gyroscopes as state vectors, the measurements of three accelerometers and magnetometers are used to drive the state update. When the system is in dynamic mode, the measured values of the accelerometers consist of the gravity vector and the dynamic accelerations, an adaptive extended Kalman filter tunes its gain automatically based on the system dynamics sensed by the accelerometers to yield optimal performance. The experiment result shows that the attitude and heading angle errors are within 0. 2 deg and 0. 5 deg respectively in stationary mode, and the result can reflect the attitude angles reasonably in dynamic mode...|$|R
40|$|Abstract. Data mining is an {{iterative}} process. Users issue {{series of}} similar data mining queries, in each consecutive run slightly modifying either {{the definition of}} the mined dataset, or the parameters of the mining algorithm. This model of processing is most suitable for incremental mining algorithms that reuse the results of previous queries when answering a given query. Incremental mining algorithms require the results of previous queries to be available. One way to preserve those results is to use materialized data mining views. Materialized data mining views store the mined patterns and refresh them as the underlying data change. Data mining and knowledge discovery often take place in a data warehouse environment. There can be many relatively small materialized data mining views defined over the <b>data</b> warehouse. Separate <b>refresh</b> of each materialized view can be expensive, if the refresh process has to re-discover patterns in the original database. In this paper we present a novel approach to materialized <b>data</b> mining view <b>refresh</b> process. We show that the concurrent on-line refresh of a set of materialized data mining views is more efficient than the sequential refresh of individual views. We present the framework for the integration of <b>data</b> warehouse <b>refresh</b> process with the maintenance of materialized data mining views. Finally, we prove the feasibility of our approach by conducting several experiments on synthetic data sets. ...|$|R
40|$|Energy {{has become}} a {{first-class}} design constraint in computer systems. Memory is a significant contributor to total system power. This paper introduces Flikker, an application-level technique to reduce refresh power in DRAM memories. Flikker enables developers to specify critical and non-critical data in programs and the runtime system allocates this data in separate parts of memory. The portion of memory containing critical <b>data</b> is <b>refreshed</b> at the regular refresh-rate, while the portion containing non-critical <b>data</b> is <b>refreshed</b> at substantially lower rates. This partitioning saves energy {{at the cost of}} a modest increase in data corruption in the non-critical data. Flikker thus exposes and leverages an interesting trade-off between energy consumption and hardware correctness. We show that many applications are naturally tolerant to errors in the non-critical data, and {{in the vast majority of}} cases, the errors have little or no impact on the application’s final outcome. We also find that Flikker can save between 20 - 25 % of the power consumed by the memory sub-system in a mobile device, with negligible impact on application performance. Flikker is implemented almost entirely in software, and requires only modest changes to the hardware...|$|R
40|$|Abstract. The {{characteristics}} of AJAX technology can {{read and write}} <b>data</b> without <b>refreshing</b> the page, so as to relieve the burden of server and speed up the response and shorten the waiting time for customers. The SSH framework is the organic integration of Struts, Spring and Hibernate three technologies. The e-commerce system has good hierarchical coding design thought of MVC system on the Struts 2 framework. The paper presents application of AJAX and SSH framework in development of electronic commerce system. 1...|$|R
40|$|The Fluxnet {{synthesis}} dataset originally compiled for the La Thuile workshop contained approximately 600 site years. Since the workshop, several additional site {{years have}} been added and the dataset now contains over 920 site years from over 240 sites. A <b>data</b> <b>refresh</b> update {{is expected to increase}} those numbers in the next few months. The ancillary data describing the sites continues to evolve as well. There are on the order of 120 site contacts and 60 proposals have been approved to use the data. These proposals involve around 120 researchers. The size and complexity of the dataset and collaboration has led to a new approach to providing access to the data and collaboration support and the support team attended the workshop and worked closely with the attendees and the Fluxnet project office to define the requirements for the support infrastructure. As a result of this effort, a new website ([URL] has been created to provide access to the Fluxnet synthesis dataset. This new web site is based on a scientific data server which enables browsing of the data on-line, data download, and version tracking. We leverage database and data analysis tools such as OLAP data cubes and web reports to enable browser and Excel pivot table access to the data...|$|R
40|$|Mobile {{devices are}} left in sleep mode {{for long periods of}} time. But even while in sleep mode, the {{contents}} of DRAM memory need to be periodically refreshed, which consumes a significant fraction of power in mobile devices. This paper introduces Flicker, an application-level technique to reduce refresh power in DRAM memories. Flicker enables developers to specify critical and non-critical data in programs and the runtime system allocates this data in separate parts of memory. The portion of memory containing critical <b>data</b> is <b>refreshed</b> at the regular refresh-rate, while the portion containing non-critical <b>data</b> is <b>refreshed</b> at substantially lower rates. This saves energy at the cost of a modest increase in data corruption in the non-critical data. Flicker thus explores a novel and interesting trade-off between energy consumption and hardware correctness. We show that many mobile applications are naturally tolerant to errors in the non-critical data, and {{in the vast majority of}} cases, the errors have little or no impact on the application’s final outcome. We also find that Flicker can save between 20 - 25 % of the power consumed by the memory subsystem in a mobile device, with negligible impact on application performance. Flicker is implemented almost entirely in software, and requires only modest changes to the application, operating system and hardware. ...|$|R
