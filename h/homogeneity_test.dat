529|493|Public
5000|$|... #Subtitle level 3: Incongruence length {{difference}} test (or partition <b>homogeneity</b> <b>test)</b> ...|$|E
5000|$|... #Caption: Schematic of a {{balanced}} nested design for a CRM <b>homogeneity</b> <b>test.</b> Large bottles show packaged individual CRM units; small vials show subsamples prepared for measurement.|$|E
40|$|A {{density ratio}} {{is defined by}} the ratio of two {{probability}} densities. We study the inference problem of density ratios and apply a semi-parametric density-ratio estimator to the two-sample <b>homogeneity</b> <b>test.</b> In the proposed test procedure, the f-divergence between two probability densities is estimated using a density-ratio estimator. The f-divergence estimator is then exploited for the two-sample <b>homogeneity</b> <b>test.</b> We derive an optimal estimator of f-divergence {{in the sense of the}} asymptotic variance in a semiparametric setting, and provide a statistic for two-sample <b>homogeneity</b> <b>test</b> based on the optimal estimator. We prove that the proposed test dominates the existing empirical likelihood score test. Through numerical studies, we illustrate the adequacy of the asymptotic theory for finite-sample inference...|$|E
50|$|<b>Homogeneity</b> <b>testing</b> for a {{candidate}} reference material typically involves replicated measurements on multiple units or subsamples of the material.|$|R
40|$|Nonparametric order <b>tests</b> for <b>homogeneity</b> and {{component}} independence are proposed, {{which are}} based on data compressors. For <b>homogeneity</b> <b>testing</b> the idea is to compress the word obtained by ordering the combined samples and writing the number of the sample in place of each element. H 0 should be rejected if the string is compressed to a certain degree and accepted otherwise. We show that such a test obtained from an ideal data compressor is valid against all alternatives. Component independence is reduced to <b>homogeneity</b> <b>testing...</b>|$|R
40|$|Classical {{variance}} <b>homogeneity</b> <b>tests</b> (Fisher's, Bartlett's, Cochran's, Hart- ley's and Levene's tests) and nonparametric {{tests for}} dispersion characteristics homogeneity (Ansari-Bradley's, Mood's, Siegel-Tukey's tests) have been consid- ered. Distributions of classical tests statistics {{have been investigated}} under vio- lation of assumption that samples belong to the normal law. The comparative analysis of power of classical variance <b>homogeneity</b> <b>tests</b> with power of nonpara- metric tests has been carried out. Tables of percentage points for Cochran's test have been obtained in case of distributions which are di®erent from normal...|$|R
3000|$|Variables OB and NB groups, {{separated}} by age (n 1, n 2, n 3), {{were evaluated by}} the <b>homogeneity</b> <b>test</b> (chi-square-x [...]...|$|E
3000|$|..., {{we compare}} {{two sets of}} data which are drawn from the populations. The <b>homogeneity</b> <b>test</b> becomes a problem of testing whether two samples of random {{variables}} are generated from the same distribution.|$|E
40|$|Seagrass (Syringodium isoetifolium) is a {{plant that}} lives {{submerged}} at sea, this floweringplant, leaves, and stems that are plugged into a powerful in water. The chemical content ofthe leaves of the Seagrass is a flavonoid, phenol, hydroquinone, and antioxidants. This studyaims to determine {{the quality of the}} cream of seagrass leaf extracts using a concentration of 5 %, 10 %, 20 % dan 40 %. on organoleptic testing conducted cream, <b>homogeneity</b> <b>test,</b> testdispersive power, pH and absorbance test. results obtained in the organoleptic seen from thecolor green, semi-solid dosage forms and distinctive smell seagrass cream. <b>homogeneity</b> <b>test.</b> <b>homogeneity</b> <b>test</b> cream seagrass leaf extract with a concentration of 5 % 10 % 20 % and 40 %did not experience any clumping or phase separation dispersive power test creams rangedfrom 2. 5 cm - 3. 5 cm. pH ranged from 4. 93 - 5. 96 and test absorption ranged from 3. 7 ml - 5 ml. seagrass leaf extract cream with type W/O creams that meet the test of homogeneity testquality, dispersive power test, test and test pH absorptio...|$|E
40|$|Homogenization of monthly averages of air {{temperature}} and relative humidity {{has been carried}} out for the area of the Czech and Slovak Republics for the period 1961 - 2005. Because of presence of a noise in the series, statistical <b>homogeneity</b> <b>tests</b> give their results with some portion of uncertainty. Using various statistical tests along with various types of reference series made it possible to considerably increase the number of <b>homogeneity</b> <b>tests</b> results for each tested series and thus to assess homogeneity more reliably. Homogenization was performed on individual hourly observations and comparison demonstrating the improvement of results compared to the homogenization of daily averages was made. Air temperature and relative humidity series were compared to help identify to what extent multi-element processing can help improve the homogenization of individual elements. All data processing and analysis were carried out using AnClim and ProcClimDB (software developed for automatic processing, analyzing, <b>homogeneity</b> <b>testing</b> and adjusting of climatological data) ...|$|R
40|$|In {{this paper}} we will check the homogeneity/heterogeneity of Levy {{processes}} using some non-parametric <b>homogeneity</b> <b>tests.</b> First we create two samples from two Levy processes {{starting from the}} definition of the Levy process, and next we test if the two samples have the same distribution. Using the Levy—Ito decomposition we will perform the <b>homogeneity</b> <b>tests</b> for given parts of the Levi processes. The study of the homogeneity of stock markets shocks is usefull because the eventualy homogeneity can produce a phenomenon analogue to the resonance that can be observed in mechanics. This resonance increase the idiosyncratic risk. ...|$|R
40|$|An {{international}} {{interlaboratory trial}} {{was conducted to}} validate thermoluminescence methods for detecting irradiated fruits and vegetables. Five products were used in this study. This paper presents the results from prestudy material, <b>homogeneity</b> <b>testing,</b> details of sample preparation, and participants= results. Prestudy results provided a basis for cross comparison of instruments in different laboratories. A wide range of sensitivities, reproducibilities, and signal?to?background ratios were observed. <b>Homogeneity</b> <b>testing</b> showed that the method can distinguish between nonirradiated and irradiated products, including those bleached with 100 J/cm 2 artificial daylight, provided that sensitivity rejection criteria are rigorously applied. Blind results were returned by 9 participants {{in the form of}} first and second glow integrals and glow ratios for all samples and a qualitative classification for each product. Of the 387 results reported, 327 valid results were obtained from participants. Where valid data were obtained, correct qualitative identifications were made by participants in all cases. Participants= results and <b>homogeneity</b> <b>testing</b> both confirm the validity of the thermoluminescence method for detecting irradiated fruits and vegetables...|$|R
40|$|Incongruence between {{different}} data sets {{remains one of}} the central issues in systematics. Numerous tree- based and character-based tests to identify incongruence have been developed, and each has strengths and weaknesses (e. g., for reviews, see Cunningham 1997 a; Johnson and Soltis 1998). Currently, {{one of the most widely}} used methods for evaluating incongruence within a parsimony framework is the <b>homogeneity</b> <b>test</b> of Farris et al. (1995), usually termed the incongruence length difference (ILD) test (Cunningham 1997 a; but see Johnson and Soltis 1998) or the partition <b>homogeneity</b> <b>Test</b> (Swofford 2000). The test has been argued to produce more accurate results than other tests (Cunningham 1997 a) and is also easy to implement using PAUP* (Swofford 2000) ...|$|E
40|$|The test of {{homogeneity}} {{developed by}} L. V. Hedges (1982) for the fixed effects model is frequently used in quantitative meta-analyses {{to test whether}} effect sizes are equal. Despite its widespread use, evidence {{of the behavior of}} this test for the less-than-ideal case of small study iample sizes paired with large numbers of studies is contradictory, and its behavior for nonnormal score distributions in primary studies is an open question. The results of a Monte Carlo study indicated that the Type I error rate and power of the <b>homogeneity</b> <b>test</b> were insensitive to skewed score distributions, but were very sensitive to smaller study sample sizes paired with larger numbers of studies. These findings extend earlier results and help to clarify the statistical behavior of the <b>homogeneity</b> <b>test.</b> Specifically, the pairing of small study sample sizes with large numbers of studies tends to produce conservative Type I error rates for the <b>homogeneity</b> <b>test</b> and underestimates its power, increasing the likelihood of Type II errors. (Contains 2 tables and 23 references.) (Author/SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|E
30|$|Pre-treatment <b>homogeneity</b> <b>test.</b> A Student’s t-test {{was used}} to compare the means for the {{experimental}} and control groups because the distribution of assessments for both groups was normalised (non-significant Kolmogorov-Smirnov test results). Fisher’s least significant difference (LSD) method was applied to determine which means were significantly different from others.|$|E
5000|$|The {{quality control}} {{procedures}} lead to each data being flagged as either [...] "OK", [...] "suspect" [...] or [...] "missing" [...] and <b>homogeneity</b> <b>testing</b> {{results in the}} classification of series as [...] "useful", [...] "doubtful" [...] or [...] "suspect". It is recommended to use {{the results of the}} <b>homogeneity</b> <b>tests</b> for selecting appropriate series and time intervals since the [...] "useful", [...] "doubtful" [...] and [...] "suspect" [...] categories only hold for the particular time interval for which the tests are applied. Information on the specifics of quality control and homogeneity procedures {{can be found in the}} Algorithm Theoretical Basis Document [...] and elsewhere.|$|R
40|$|Mixtures of {{binomial}} distributions {{are used}} to model linkage between genes and markers. In several classical models, we study the likelihood ratio test for linkage, which are <b>homogeneity</b> <b>tests</b> in mixture models. Using ad hoc reparametrizations, we prove that the corresponding test statistics converge to the supremum of squared truncated Gaussian processes under the null hypothesis. Binomial mixture <b>Homogeneity</b> Likelihood ratio <b>test</b> Linkage...|$|R
40|$|Aims: To {{develop a}} {{technique}} for <b>homogeneity</b> <b>testing</b> of serum aliquot samples {{suitable for use}} in the Quality Assurance Program in Clinical Immunology (QAP Pty Ltd). Methods: Albumin was selected as the surrogate protein marker for the product to be tested and the coefficient of dispersion (COD) calculated as the measure of homogeneity. To detect changes in the average level of homogeneity, cumulative sum control (cusum) charts were used. Results: The COD(%) for each triplicate reading of albumin obtained from 34 specimens was normally distributed with a mean of 0. 49 % and a standard deviation of 0. 25 %. In industrial quality control schemes the action line is generally set at the upper 99 % confidence limits, hence any triplicate sample would be considered to have acceptable homogeneity if the COD was ≤ 1. 08 %. Cusum charts were created to monitor albumin homogeneity over time. Conclusions: The use of albumin measurement as the surrogate appears statistically suitable for <b>homogeneity</b> <b>testing</b> in QAP programs for immunodiagnostic testing. CUSUM charts are particularly useful to monitor such <b>homogeneity</b> <b>testing.</b> Adrian Esterman, Sue Javanovich, Robert McEvoy and Peter Roberts-Thomson...|$|R
40|$|Meta-analysis {{seeks to}} combine the results of several {{experiments}} {{in order to improve}} the accuracy of decisions. It is common to use a test for homogeneity to determine if the results of the several experiments are sufficiently similar to warrant their combination into an overall result. Cochran'sQstatistic is frequently used for this <b>homogeneity</b> <b>test.</b> It is often assumed thatQfollows a chi-square distribution under the null hypothesis of homogeneity, but it has long been known that this asymptotic distribution forQis not accurate for moderate sample sizes. Here, we present an expansion for the mean ofQunder the null hypothesis that is valid when the effect and the weight for each study depend on a single parameter, but for which neither normality nor independence of the effect and weight estimators is needed. This expansion represents an orderO(1 /n) correction to the usual chi-square moment in the one-parameter case. We apply the result to the <b>homogeneity</b> <b>test</b> for meta-analyses in which the effects are measured by the standardized mean difference (Cohen'sd-statistic). In this situation, we recommend approximating the null distribution ofQby a chi-square distribution with fractional degrees of freedom that are estimated from the data using our expansion for the mean ofQ. The resulting <b>homogeneity</b> <b>test</b> is substantially more accurate than the currently used test. We provide a program available at the Paper Information link at theBiometricswebsitefor making the necessary calculations...|$|E
40|$|The <b>homogeneity</b> <b>test</b> {{of glass}} plates in a Fizeau {{interferometer}} is {{hampered by the}} superposition of multiple interference signals coming from the surfaces of the glass plate {{as well as the}} empty Fizeau cavity. To evaluate interferograms resulting from such nested cavities, various approaches {{such as the use of}} broadband light sources have been applied. In this paper, we propose an adaptive frequency comb interferometer to accomplish the cavity selection. An adjustable Fabry-Perot resonator is used to generate a variable frequency comb that can be matched to the length of the desired cavity. Owing to its flexibility, the number of measurements needed for the <b>homogeneity</b> <b>test</b> can be reduced to four. Furthermore, compared to approaches using a two-beam interferometer as a filter for the broadband light source, the visibility of the fringe system is considerably higher if a Fabry-Perot filter is applied. (C) 2011 Optical Society of Americ...|$|E
40|$|The {{widely used}} χ 2 <b>homogeneity</b> <b>test</b> for {{comparing}} histograms(unweighted) is modified for cases involving {{unweighted and weighted}} histograms. Numerical examples illustrate an application of the method for the case of histograms with a small statistics of events and also for large statistics of events. This method {{can be used for}} the comparison of simulated data histograms against experimental data histograms. ...|$|E
40|$|International audienceNonparametric rank <b>tests</b> for <b>homogeneity</b> and {{component}} independence are proposed, {{which are}} based on data compressors. For <b>homogeneity</b> <b>testing</b> the idea is to compress the binary string obtained by ordering the two joint samples and writing 0 if the element is from the first sample and 1 if it is from the second sample and breaking ties by randomization (extension to the case of multiple samples is straightforward). $H_ 0 $ should be rejected if the string is compressed (to a certain degree) and accepted otherwise. We show that such a test obtained from an ideal data compressor is valid against all alternatives. Component independence is reduced to <b>homogeneity</b> <b>testing</b> by constructing two samples, one of which is {{the first half of the}} original and the other is the second half with one of the components randomly permuted...|$|R
40|$|Here is an ado {{file and}} a help file for calculating {{asymptotic}} and exact tests of symmetry for NxN contingency tables from dependent samples. This test {{is known in}} genetics as the TDT test. "symmetry" performs symmetry and marginal <b>homogeneity</b> <b>tests</b> on square NxN tables {{where there is a}} 1 to 1 matching of cases and controls (non-independence). ...|$|R
40|$|This paper {{describes}} the studies {{performed with the}} candidate Certified Reference Material (CRM) of captopril, the first CRM of an active pharmaceutical ingredient (API) in Brazil, including determination of impurities (organic, inorganic and volatiles), <b>homogeneity</b> <b>testing,</b> short- and long-term stability studies, calculation of captopril content using the mass balance approach, and estimation of the associated measurement uncertainty...|$|R
40|$|The aim of {{this paper}} is the {{aggregation}} of AR (1) processes. We determine the dynamic models satisfied by the aggregated series and we characterize all the series which may be interpreted as such an aggregate. We study more carefully the case of a bêta heterogeneity distribution. In particular we propose an <b>homogeneity</b> <b>test</b> and we discuss the sign of the heterogeneity bias. ...|$|E
40|$|Stress-annealed {{amorphous}} Co 69 Fe 2 Cr 7 Si 8 B 14 ribbons {{are suitable}} materials for strain sensors required in civil engineering applications. The equipments for the continuous stress-annealing of ribbons and the automatic <b>homogeneity</b> <b>test</b> of magnetic properties are described. The magnetic properties of stress-annealed ribbons are discussed. The use of ribbon in an inductive strain sensor is also illustrated...|$|E
40|$|The {{objective}} {{of this study is}} to establish a regional relationship between mean annual peak flood and the catchments area based on the frequency analysis for available annual peak flood for various gauging sites of hydro logically homogeneous region of Krishna basin, and to use the same for estimating the floods for various recurrence intervals for the catchments which are not used for analysis. This paper describes a study carried out for the Krishna basin with annual peak flood series data available for 24 sites for varying number of years. The Index flood method was used for analysis. Out of 24 sites, 4 sites were omitted after the USGS <b>homogeneity</b> <b>test</b> since they fall outside the envelope curves of <b>homogeneity</b> <b>test.</b> From the remaining 20 sites only 18 sites were considered for the analysis and data of other 2 sites were used as test sites for judging the performance of the developed regional formulae...|$|E
40|$|Abstract — Regularized Maximum Mean Discrepancy (RMMD), our novel {{measure for}} kernel-based {{hypothesis}} testing, excels at hypothesis tests involving multiple comparisons with power control even when sample sizes are small. We derive asymptotic distributions under the null and alternative hypotheses, and assess power control. Outstanding results are obtained on challenging benchmark datasets. Keywords- kernel-based hypothesis <b>testing,</b> <b>Homogeneity</b> <b>testing,</b> Multiple comparisons, Power I...|$|R
40|$|In {{their recent}} comment, {{published}} in Nature, Jeffrey T. Leek and Roger D. Peng discuss how P-values are widely abused in null hypothesis significance testing. We agree completely {{with them and}} in this short comment we discuss the importance of sample <b>homogeneity</b> <b>tests.</b> No matter with how much scrutiny data are gathered if <b>homogeneity</b> <b>tests</b> are not performed the significance tests suffer from sample homogeneity loophole and the results may not be trusted. For example sample homogeneity loophole was not closed in the experiment testing local realism in which a significant violation of Eberhard inequality was found. We are not surprised that Bell type inequalities are violated since if the contextual character of quantum observables is properly taken into account these inequalities cannot be proven. However in order to trust {{the significance of the}} violation sample homogeneity loophole must be closed. Therefore we repeat after Jeffrey T. Leek and Roger D. Peng that sample homogeneity loophole is probably {{just the tip of the}} iceberg. Comment: 4 page...|$|R
40|$|This paper proposes several {{statistical}} {{tests for}} finite state Markov games {{to examine whether}} data from distinct markets can be pooled. We formulate <b>homogeneity</b> <b>tests</b> of (i) the conditional choice and state transition probabilities, (ii) the steady-state distribution, and (iii) the conditional state distribution given an initial state. The null hypotheses of these <b>homogeneity</b> <b>tests</b> are necessary conditions (or maintained assumptions) for poolability of the data. Thus rejections of these null imply that the data cannot be pooled across markets. Acceptances of these null are considered as supporting evidences for the maintained assumptions of estimation using pooled data. In a Monte Carlo study {{we find that the}} test based on the steady-state distribution performs well and has high power even with small numbers of markets and time periods. We apply the tests to the empirical study of Ryan (2012) that analyzes dynamics of the U. S. Portland cement industry and assess if the data across markets can be pooled...|$|R
30|$|Therefore, three {{absolute}} homogeneity {{tests are}} used to detect any variation in the time series to extract meaningful statistics and characteristics from these data, using AnClim software which has been specially created for climatologic purposes (Stepanek 2008). The tests {{that are used in}} this research are; Pettitt’s test, standard normal <b>homogeneity</b> <b>test</b> (SNHT), and von Neumann’s test (Taxak et al. 2014; Carvalho et al. 2014).|$|E
40|$|A {{simulation}} {{study was}} used to evaluate multiple imputation (MI) to handle MCAR correlations in the first step of meta-analytic structural equation modeling: the synthesis of the correlation matrix and the test of homogeneity. No substantial parameter bias resulted from using MI. Although some SE bias was found for meta-analyses involving smaller numbers of studies, the <b>homogeneity</b> <b>test</b> was never rejected when using MI...|$|E
40|$|We {{provide a}} new general theorem for multivariate normal {{approximation}} on convex sets. The theorem is formulated {{in terms of}} a multivariate extension of Stein couplings. We apply the results to a <b>homogeneity</b> <b>test</b> in dense random graphs and to prove multivariate asymptotic normality for certain doubly indexed permutation statistics. Comment: Published at [URL] in the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|E
40|$|Abstract Background Mixed effects {{logistic}} {{models have}} become a popular method for analyzing multicenter clinical trials with binomial data. However, the statistical properties of these models for <b>testing</b> <b>homogeneity</b> of odds ratios under various conditions, such as within-center and among-centers inequality, are still unknown and not yet {{compared with those of}} commonly used <b>tests</b> of <b>homogeneity.</b> Methods We evaluated the effect of within-center and among-centers inequality on the empirical power and type I error rate of the three <b>homogeneity</b> <b>tests</b> of odds ratios including likelihood ratio (LR) test of a mixed logistic model, DerSimonian-Laird (DL) statistic and Breslow-Day (BD) test by simulation study. Moreover, the impacts of number of centers (K), number of observations in each center and amount of heterogeneity were investigated by simulation. Results As compared with the equal sample size design, the power of the three <b>tests</b> of <b>homogeneity</b> will decrease if the same total sample size, which can be allocated equally within one center or among centers, is allocated unequally. The average reduction in the power of these tests was up to 11 % and 16 % for within-center and among-centers inequality, respectively. Moreover, in this situation, the ranking {{of the power of the}} <b>homogeneity</b> <b>tests</b> was BD≥DL≥LR and the power of these tests increased with increasing K. Conclusions This study shows that the adverse effect of among-centers inequality on the power of the <b>homogeneity</b> <b>tests</b> was stronger than that of within-center inequality. However, the financial limitations make the use of unequal sample size designs inevitable in multicenter trials. Moreover, although the power of the BD is higher than that of the LR when K ≤ 6, the proposed mixed logistic model is recommended when K ≥ 8 due to its practical advantages. </p...|$|R
40|$|The {{first part}} of the paper {{develops}} a novel, sortally-based approach to the problem of aspectual composition. The account is argued to be superior on both empirical and computational grounds to previous semantic approaches relying on referentiai <b>homogeneity</b> <b>tests.</b> While the account is restricted to manner-of-motion verbs, it does cover their interaction with mass terms, amount phrases, locative PPs, and distance, frequency, and temporal modifiers...|$|R
40|$|Mushroom {{reference}} material has been prepared and characterized {{for use in}} proficiency test exercises within the frame of an IAEA Interregional Technical Cooperation Project. Laboratories from 14 countries provided results for <b>homogeneity</b> <b>testing</b> and the assignment of property values. The contents of 11 elements have been assigned. The material was used for conducting a proficiency test in Poland and the results obtained by Polish laboratories are presented and discussed...|$|R
