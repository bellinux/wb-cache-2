0|4278|Public
40|$|A {{conventional}} Voice-over-Internet Protocol (VoIP) conference bridge {{reduces the}} speech quality due to tandeming the mixed multi-speaker signal with <b>high</b> <b>compression</b> speech <b>codecs.</b> One {{solution is to}} select and forward the compressed signal(s) to the endpoints, where they are decoded and mixed. In such arrange-ments, speaker selection is usually accomplished with an order-based approach which prevents listeners from interrupting the cur-rent speaker(s). This paper presents an alternative in which talking privileges are assigned based on order of activity and signal power. Subjective evaluations indicate that speaker switching is smooth, nearly transparent, and unanimously preferred over a VoIP confer-ence with tandemed connections. 1...|$|R
40|$|Traditional {{teleconferencing}} uses a select-and-mix func-tion at {{a centralized}} conferencing bridge. In VoIP en-vironments, this mixing operation {{can lead to}} speech degradation when using <b>high</b> <b>compression</b> speech <b>codecs</b> due to tandem encodings and coding of multi-talker sig-nals. A tandem-free architecture can eliminate tandem encodings and preserve speech quality. VoIP conference bridges must also consider the variable network delays experienced by different packetized voice streams. A syn-chronized speaker selection algorithm at the bridge can smooth out network delay variations and synchronize in-coming voice streams. This provides a clean mapping of the N input packet streams to the M output streams representing selected speakers. This paper presents a synchronized speaker selection algorithm and evaluates its performance using a conference simulator. The syn-chronization process is shown to account for {{only a small part}} of the overall delay experienced by selected packets. ...|$|R
50|$|OptimFROG is a {{proprietary}} lossless audio data <b>compression</b> <b>codec</b> developed by Florin Ghido. OptimFROG is optimized for very <b>high</b> <b>compression</b> ratios {{at the expense}} of encoding and decoding speed.|$|R
40|$|Traditional {{telephone}} conferencing {{has been}} accomplished {{by way of a}} centralized conference bridge. An Internet Protocol (IP) -based conference bridge is subject to speech distortions and substantial computational demands due to the tandem arrangement of <b>high</b> <b>compression</b> speech <b>codecs.</b> Decentralized architectures avoid the speech distortions and delay, but lack strong control and have a key dependence on silence suppression for endpoint scalability. One solution is to use centralized speaker selection and forwarding, and decentralized decoding and mixing. This approach eliminates the problem of tandem encodings and maintains tight control, thereby improving the speech quality and scalability of the conference. This thesis considers design options and solutions for this model, and evaluates performance through live conferences with real conferees. Conferees found the speaker selection of the new conference model to be transparent, and strongly preferred the resulting speech quality to that of a centralized IP-based conference bridge...|$|R
40|$|Abstract — Traditional {{telephone}} conferencing {{has been}} accom-plished {{by way of}} a centralized conference bridge. The tandem arrangement of <b>high</b> <b>compression</b> speech <b>codecs</b> in conventional VoIP conference bridges lead to speech distortions and require a substantial number of computations. Decentralized architectures avoid the speech degradations and delay, but lack strong control and depend on silence suppression to make the endpoint band-width and processing requirements scalable. One solution is to use centralized speaker selection and forwarding, and decentralized decoding and mixing. This approach eliminates the problem of tandem encodings but maintains centralized control, thereby improving the speech quality and scalability of the conference. This paper considers design options and solutions for this model in the context of modern IP telephony networks. Performance was evaluated with real conferees over live conferences using a PC-based conferencing test bed, built using a custom software-based bridge and a third-party endpoint. Conferees strongly preferred the speech quality of the new arrangement to that of a conventional VoIP conference bridge. I...|$|R
40|$|Schmaltz et al. (2009) {{have shown}} that for {{reasonably}} <b>high</b> <b>compression</b> rates, diffusion-based <b>codecs</b> can exceed the quality of transformation-based methods such as JPEG 2000. They store only data at a few optimised pixel locations and in-paint missing data with edge-enhancing anisotropic diffusion (EED). However, research on compression with diffusion methods has mainly focussed on grey-value images, and colour images have been compressed in a straightforward way using anisotropic diffusion in RGB space. So far, there is no sophisticated diffusion-based counterpart to the colour mode of JPEG 2000. To address this shortcoming we in-troduce an advanced colour <b>compression</b> <b>codec</b> that exploits properties of the human visual system in YCbCr space. Since details in the luma channel Y are perceptually relevant, we invest a large fraction of our bit budget in its encoding with high fidelity. For the chroma channels Cb and Cr, the stored information can be very sparse, if we guide the EED-based inpainting with the high quality diffusion tensor from the luma reconstruction. Experiments demonstrate that our novel codec outperforms JPEG 2000 and compression with RGB-diffusion, both visually and quantitatively. Index Terms — colour, edge-enhancing anisotropic diffu-sion, compression, YCbCr space, luma preference 1...|$|R
40|$|H. 264 /AVC is the {{state-of-the-art}} video coding standard, {{which has}} various functions to realize <b>high</b> <b>compression</b> performance. The <b>codec</b> prepares several modes in both intra- and inter-prediction, and chooses {{the best one}} by some criterion. Therefore, the encoder requires a heavy burden. This paper describes a fast mode decision method on Sum of Absolute Transformed Differences (SATD) criterion. The proposed method prunes candidates by projecting difference blocks onto the canonical bases without calculating transformed differences, and guarantees to choose the best mode. Experimental {{results show that the}} proposed method reduces computational time by 16 % compared with the exhaustive calculation performed by Joint Model (JM) 14. 0. APSIPA ASC 2009 : Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference. 4 - 7 October 2009. Sapporo, Japan. Poster session: Image, Video, and Multimedia Signal Processing 2 (6 October 2009) ...|$|R
40|$|H. 264 /AVC is {{expected}} to become an essential component {{in the delivery of}} wireless multimedia content. While achieving <b>high</b> <b>compression</b> ratios, this <b>codec</b> is extremely vulnerable to transmission errors. These errors generally result in spatio-temporal propagation of distorted macroblocks (MBs) which significantly degrade the perceptual quality of the reconstructed video sequences. This paper presents a scheme for resilient transmission of H. 264 /AVC streams in noisy environments. The proposed algorithm exploits the redundant information which is inherent in the neighboring MBs and applies a Probabilistic Neural Network (PNN) classifier to detect visually impaired MBs. This algorithm achieves Peak Signal-to-Noise Ratio (PSNR) gains of up to 14. 29 dB when compared to the standard decoder. Moreover, this significant gain in quality is achieved with minimal overheads and no additional bandwidth requirement, thus making it suitable for conversational and multicast/ broadcast services where feedback-based transport protocols cannot be applied. peer-reviewe...|$|R
40|$|Traditionally, the {{performance}} of a image <b>compression</b> <b>codec</b> is measured by its rate-distortion curve, where a distortion metric measures the fitness of the reconstructed image for a particular purpose. However, metrics employed here address the needs of professional photography only partially: Images are typically not published as taken, but are post-process to express the intent of the photographer. <b>Compression</b> <b>codecs</b> might degrade the image visibly by an interaction of the codec-specific loss with the editing tools. In this work, we present the idea of application specific metrics and measure the robustness of state of the art <b>compression</b> <b>codecs</b> under prototypical image manipulations...|$|R
50|$|NRK uses H.264 as the <b>compression</b> <b>codec</b> for {{the digital}} {{terrestrial}} network {{also known as}} MPEG4.|$|R
50|$|The non-capitalized term {{is used to}} {{describe}} lossy <b>compression</b> <b>codecs</b> that utilize perceptual coding techniques for audio.|$|R
50|$|The cook codec is a lossy audio <b>compression</b> <b>codec</b> {{developed}} by RealNetworks. It {{is also known}} as Cooker, Gecko, RealAudio G2, and RealAudio 8 low bitrate (RA8LBR).|$|R
50|$|On January 29, 2003, the Xiph.Org Foundation officially {{announced}} {{the incorporation of}} FLAC as a lossless audio <b>compression</b> <b>codec</b> under Xiph.org banner. Ogg Squish is no longer maintained.|$|R
50|$|An early {{version of}} the HP RGS video <b>compression</b> <b>codec,</b> is derived from a {{patented}} system developed by HP Labs and used in the NASA Mars Rover program.|$|R
50|$|In {{addition}} to encoding a signal, a codec may also compress {{the data to}} reduce transmission bandwidth or storage space. <b>Compression</b> <b>codecs</b> are classified primarily into lossy codecs and lossless codecs.|$|R
50|$|PSNR is most {{commonly}} {{used to measure the}} quality of reconstruction of lossy <b>compression</b> <b>codecs</b> (e.g., for image compression). The signal in this case is the original data, and the noise is the error introduced by compression. When comparing <b>compression</b> <b>codecs,</b> PSNR is an approximation to human perception of reconstruction quality. Although a higher PSNR generally indicates that the reconstruction is of higher quality, in some cases it may not. One has to be extremely careful with the range of validity of this metric; it is only conclusively valid when it is used to compare results from the same codec (or codec type) and same content.|$|R
40|$|Developed an overcomplete {{dictionary}} {{training method}} (patent filed, I have an 85 % share). • Outperformed state-of-the-art dictionary-based image denoising. • Developed a facial image <b>compression</b> <b>codec</b> that outperforms JPEG 2000. • Developed a low-complexity image matching method using sparse representations...|$|R
50|$|OggSquish {{was one of}} {{the first}} names used for the Ogg project {{developed}} from 1994 by the Xiphophorus company (now Xiph.Org Foundation). Ogg Squish was also an attempt from the Xiphophorus company to create a royalty-free lossless audio <b>compression</b> <b>codec.</b>|$|R
50|$|MPEG-1 Audio Layer III HD more {{commonly}} known and advertised by its abbreviation mp3HD is an audio <b>compression</b> <b>codec</b> developed by Technicolor {{formerly known as}} Thomson. It achieves lossless data compression, and is backwards compatible with the MP3 format by storing two data streams in one file.|$|R
5000|$|In 1998, after Fraunhofer Society Integrated Circuits Institute {{intended}} to sue MPEG-1 Audio Layer 3 development projects because of license issues, the Xiphophorus company's focus {{was moved to}} a royalty-free lossy audio <b>compression</b> <b>codec,</b> named OggSquish Vorbis, or only [...] "Ogg Vorbis" [...] or [...] "Vorbis".|$|R
50|$|SVOPC (Sinusoidal Voice Over Packet Coder) is a {{compression}} method for audio {{which is used}} by VOIP applications. It is a lossy speech <b>compression</b> <b>codec</b> designed specifically towards communication channels suffering from packet loss. It uses more bandwidth than best bandwidth-optimised codecs, but it is packet loss resistant instead.|$|R
5000|$|HDC (Hybrid Digital Coding) with SBR (spectral band replication) is a {{proprietary}} lossy audio <b>compression</b> <b>codec</b> developed by iBiquity {{for use with}} HD Radio. It replaced the earlier PAC codec in 2003. [...] In June 2017, the format was reverse engineered and determined to be a variant of HE-AACv1.|$|R
25|$|Since the {{quantization}} stage always {{results in}} a loss of information, JPEG standard is always a lossy <b>compression</b> <b>codec.</b> (Information is lost both in quantizing and rounding of the floating-point numbers.) Even if the quantization matrix is a matrix of ones, information will still be lost in the rounding step.|$|R
30|$|Common [12] A set {{of common}} {{utilities}} {{needed by the}} other Hadoop modules. It has native shared libraries that include Java implementations for <b>compression</b> <b>codecs,</b> I/O utilities, and error detection. Also included are interfaces and tools for configuration of rack awareness, authorization of proxy users, authentication, service-level authorization, data confidentiality, and the Hadoop Key Management Server (KMS).|$|R
40|$|To {{meet the}} demand for higher data rates and energy {{efficiency}} in medical implants, a real-time <b>compression</b> <b>codec</b> for biosignals was developed. The designed Adaptive Vector-Quantization Codec was successfully evaluated in a hardware implementation with ECG and EMG signals. The proposed method is shown to open perspectives for new high data rate applications in actual and future developments...|$|R
50|$|In 1994, Radius {{acquired}} rival SuperMac {{and shifted}} headquarters into the latter's building. The SuperMac acquisition netted Radius the Cinepak video <b>compression</b> <b>CODEC,</b> {{which was still}} supported by most encoders and almost all media players by the early 2000s. The acquisitions continued with Pipeline Digital and its professional time code and video tape deck control software.|$|R
50|$|Nero Digital is a {{brand name}} applied to a suite of MPEG-4-compatible video and audio <b>compression</b> <b>codecs</b> {{developed}} by Nero AG of Germany and Ateme of France. The audio codecs are integrated into the Nero Digital Audio+ audio encoding tool for Microsoft Windows, and the audio & video codecs are integrated into Nero's Recode DVD ripping software.|$|R
50|$|SENSIO Technologies Inc. {{was founded}} in 1999 by Nicholas Routhier and Richard LaBerge, {{under the name of}} Technologies Sensorielles TEG, to {{manufacture}} technology capable of allowing 3D to be viewed in the home. Their research and development resulted, in 2002, in the first iteration of what was to become the SENSIO® Hi-Fi 3D technology - a spatial <b>compression</b> <b>codec.</b>|$|R
40|$|High Efficiency Video Coding (HEVC) {{demonstrates}} {{a significant improvement}} in compression efficiency compared to H. 264 /MPEG- 4 AVC, especially for video with resolution beyond HD, such as 4 K UHDTV. One advantage of HEVC is the improved intra coding of video frames. Hence, it is natural to question how such intra coding compares to {{state of the art}} <b>compression</b> <b>codecs</b> for still images. This paper attempts to answer this question by providing a detailed analysis and performance comparison of HEVC intra coding with JPEG and JPEG 2000 (both 4 : 2 : 0 and 4 : 4 : 4 configurations) via a series of subjective and objective evaluations. The evaluation results demonstrate that HEVC intra coding outperforms standard codecs for still images with the average bit rate reduction ranging from 16 % (compared to JPEG 2000 4 : 4 : 4) up to 43 % (compared to JPEG). These findings imply that both still images and moving pictures can be efficiently compressed by the same coding algorithm with <b>higher</b> <b>compression</b> efficiency. 1...|$|R
40|$|With the {{emergence}} of software delivery platforms such as Microsoft's. NET, reduced size of transmitted binaries has become a very important system parameter strongly affecting system performance. In this paper, we present two novel pre-processing steps for code compression that explore program binaries' syntax and semantics to achieve superior compression ratios. The first preprocessing step involves heuristic partitioning of a program binary into streams with high auto-correlation. The second preprocessing step uses code optimization via instruction rescheduling {{in order to improve}} prediction probabilities for a given compression engine. We have developed three heuristics for instruction rescheduling that explore tradeoffs of the solution quality versus algorithm run-time. The pre-processing steps are integrated with the generic paradigm of prediction by partial matching (PPM) which is the fundament of our <b>compression</b> <b>codec.</b> The <b>compression</b> algorithm is implemented for x 86 binaries and tested on several large Microsoft applications. Binaries compressed using our <b>compression</b> <b>codec</b> are 18 - 24 % smaller than those compressed using the best available off-the-shelf compressor...|$|R
40|$|Till now, some Joint Fingerprinting and Decryption (JFD) schemes {{have been}} reported, while few works {{have been done}} to analyze their performances. In this paper, the {{security}} of the JFD scheme proposed by Kundur et al. is analyzed and improved. The analyses include the compliance with general <b>compression</b> <b>codecs,</b> the security against cryptographic attacks, the security in perception, the relationship between security and robustness, and the relationship between security and imperceptibility. Additionally, some means are proposed to improve the scheme's performances. The encryption in block based DCT is introduced to make it compliant with such <b>compression</b> <b>codec</b> as JPEG or MPEG 2. The multi-key encryption is presented to improve the security against cryptographic attacks. The DC encryption is proposed to strengthen the perceptual security against ciphertext-only attacks. These analysis methods {{can also be used to}} evaluate some other JFD schemes, and are expected to provide valuable information to design JFD schemes. </p...|$|R
2500|$|... mp3PRO is an unmaintained {{proprietary}} audio <b>compression</b> <b>codec</b> {{that combines}} the MP3 audio format with the spectral band replication (SBR) compression method. At {{the time it}} was developed it could reduce the size of a stereo MP3 by as much as 50% while maintaining the same relative quality. This works, fundamentally, by discarding the higher half of the frequency range and algorithmically replicating that information while decoding.|$|R
40|$|Abstract. An image <b>compression</b> <b>codec</b> {{based on}} WHT is {{proposed}} and im-plemented in wireless handset. Considering the low processing power of wireless handset, fast decoding codec is proposed and implemented. The proposed codec consists of RCT, WHT transform, quantization using R-D optimization and lossless coding. The test results for wireless handsets {{show that the}} proposed codec has a better performance than the IJG JPEG codec. ...|$|R
50|$|The {{audio data}} in most AIFF files is {{uncompressed}} pulse-code modulation (PCM). This type of AIFF files uses much more disk space than lossy formats like MP3—about 10 MB {{for one minute}} of stereo audio at a sample rate of 44.1 kHz and a bit depth of 16 bits. There is also a compressed variant of AIFF known as AIFF-C or AIFC, with various defined <b>compression</b> <b>codecs.</b>|$|R
50|$|Sending faxes over VoIP {{networks}} {{is sometimes}} referred to as Fax over IP (FoIP). Transmission of fax documents was problematic in early VoIP implementations, as most voice digitization and <b>compression</b> <b>codecs</b> are optimized for the representation of the human voice and the proper timing of the modem signals cannot be guaranteed in a packet-based, connection-less network. A standards-based solution for reliably delivering fax-over-IP is the T.38 protocol.|$|R
5000|$|... mp3PRO is an unmaintained {{proprietary}} audio <b>compression</b> <b>codec</b> {{that combines}} the MP3 audio format with the spectral band replication (SBR) compression method. At {{the time it}} was developed it could reduce the size of a stereo MP3 by as much as 50% while maintaining the same relative quality. This works, fundamentally, by discarding the higher half of the frequency range and algorithmically replicating that information while decoding.|$|R
