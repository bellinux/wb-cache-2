4|20|Public
40|$|As {{more and}} more content is {{published}} and consumed online, {{it is imperative to}} know if an <b>information</b> <b>nugget</b> found on the Web is trustworthy or not. This is especially important for online medical information as it affects the most vulnerable group of users looking for medical help online. In this paper, we study the feasibility of automatically assessing the trustworthiness of a medical claim based on community knowledge, and propose techniques to assign a reliability score for an <b>information</b> <b>nugget</b> based on support over a community-generated collection. Specifically, we model the trustworthiness of a medical claim based on experiences shared by users in health forums and mailing lists. The proposed claim scores can be used to rank related claims on their relative trustworthiness. We further extend the notion of trustworthiness to a site (or equivalently, a database of claims from the site) and propose a scheme to rank sites based on aggregating the trust scores of claims from the site. Our experiments show that community knowledge can be exploited to help users distinguish reliable medical claims from unreliable ones. The proposed techniques can be applied to other domains where similar corpora are available...|$|E
40|$|Abstract. When the {{objective}} of an information retrieval task is to return a nugget rather than a document, query terms that exist in a document will often {{not be used in}} the most relevant <b>information</b> <b>nugget</b> in the document. In this paper, a new method of query expansion is proposed based on the Wikipedia link structure surrounding the most relevant articles selected automatically. Evaluated with the Nuggeteer automatic scoring software, an increase in the F-scores is found from the TREC Complex Interactive Question Answering task when integrating this expansion into an already high-performing baseline system. ...|$|E
40|$|Following recent {{developments}} in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called POURPRE, for automatically evaluating answers to definition questions. Until now, {{the only way to}} assess the correctness of answers to such questions involves manual determination of whether an <b>information</b> <b>nugget</b> appears in a system’s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that POURPRE outperforms direct application of existing metrics. ...|$|E
40|$|We {{propose a}} method for ranking short <b>information</b> <b>nuggets</b> {{extracted}} from a text corpus, using another, reliable reference corpus as a user model. We argue that the availability and usage of such additional corpora is common {{in a number of}} IR tasks, and apply the method to answering a form of definition questions. The proposed ranking method makes a substantial improvement in the performance of our system. Categories and Subject Descriptor...|$|R
40|$|We {{describe}} a methodology {{for evaluating the}} statistical performance of information distillation systems {{and apply it to}} a sim-ple illustrative example. (An information distiller provides written English responses to English queries based on automated searches/transcriptions/translations of English and foreign-language sources. The sources include written documents and sound tracks.) The evaluation methodology extracts <b>information</b> <b>nuggets</b> from the distiller response texts and gathers them into fuzzy equivalence classes called nugs. The methodology supports the usual performance metrics, such as recall and precision, as well as a new information-theoretic metric called proficiency, which measures how much information a distiller provides relative to all of the information provided by a col-lection of distillers working on a common query and corpora. Unlike previous evaluation techniques, the methodology evaluates the relevance, granularity, and redundancy of <b>information</b> <b>nuggets</b> explicitly. 1. <b>Information</b> Distillers An autonomous information distiller takeswritten queries as inputs, and in response, automatically gathers, transcribes, translates (if necessary), and distills relevant information frommultilingual text and speech sources. The distiller out-puts the distilled information in a readable document writte...|$|R
40|$|ABSTRACT: We {{propose a}} method for ranking short <b>information</b> <b>nuggets</b> {{extracted}} from a text corpus, using another, reliable reference corpus as a user model. We argue that the availability and usage of such additional corpora is common {{in a number of}} IR tasks, and apply the method to answering a form of definition questions. The proposed ranking method makes a substantial improvement in the performance of our system. Categories and Subject Descriptor...|$|R
40|$|NuggetMine is an {{intelligent}} groupware application that collaborates with a workgroup to increase <b>information</b> <b>nugget</b> sharing among the group. Information nuggets are {{small amounts of}} self-contained information, such as the URL of an interesting news article, a book title, or the time and location of a local art event. NuggetMine and the workgroup work together to build, maintain, and utilize a repository—or “mine”—of information nuggets. Group members submit nuggets to NuggetMine, which organizes and augments the submitted nuggets and provides a desktop interface to each group member. This interface {{makes it easy for}} group members to submit nuggets, view nuggets, and explore the mine. NuggetMine distributes the tasks necessary to share nuggets between it and the workgroup so as to best utilize the skills of each collaborator. In this paper, we describe the NuggetMine application and interface and present a pilot study of the application...|$|E
40|$|Clustering {{separates}} unrelated {{documents and}} groups related documents, and {{is useful for}} discrimination, disambiguation, summarization, organization, and navigation of unstructured collections of hypertext documents. We propose a novel clustering algorithm that clusters hypertext documents using words (contained in the document), out-links (from the document), and in-links (to the document). The algorithm automatically determines {{the relative importance of}} words, out-links, and in-links for a given collection of hypertext documents. We annotate each cluster using six <b>information</b> <b>nuggets...</b>|$|R
40|$|This paper {{describes}} a novel text comparison environment that facilities text comparison administered through assessing and aggregating <b>information</b> <b>nuggets</b> automatically created and {{extracted from the}} texts in question. Our goal in designing such a tool is to enable and improve automatic nugget creation and present its application for evaluations of various natural language processing tasks. During our demonstration at HLT, new users will able to experience first hand text analysis can be fun, enjoyable, and interesting using system-created nuggets. ...|$|R
40|$|Information {{disparity}} is a {{major challenge}} with multi-lingual document collections. When documents are dynamically updated in a distributed fashion, information content among different language editions may gradually diverge. We propose a framework for assisting human editors to manage this information dispar-ity, using tools from machine translation and machine learning. Given source and target documents in two different languages, our system automatically identifies <b>information</b> <b>nuggets</b> that are new with respect to target and suggests positions to place their translations. We perform both real-world experiments and large-scale simulations on Wikipedia documents and conclude our system is effective {{in a variety of}} scenarios...|$|R
40|$|TREC Definition and Relationship {{questions}} are evaluated {{on the basis}} of <b>information</b> <b>nuggets</b> that may be contained in system responses. Human evaluators provide informal descriptions of each nugget, and judgements (assignments of nuggets to responses) for each response submitted by participants. The current best automatic evaluation for these kinds of questions is Pourpre. Pourpre uses a stemmed unigram similarity of responses with nugget descriptions, yielding an aggregate result that is difficult to interpret, but is useful for relative comparison. Nuggeteer, by contrast, uses both the human descriptions and the human judgements, and makes binary decisions about each response, so that the end result is as interpretable as the official score. I explore n-gram length, use of judgements, stemming, and term weighting, and provide a new algorithm quantitatively comparable to, and qualitatively better than, the state of the art. ...|$|R
40|$|The TREC Definition and Relationship {{questions}} are evaluated {{on the basis}} of <b>information</b> <b>nuggets</b> that may be contained in system responses. Human evaluators provide informal descriptions of each nugget, and judgements (assignments of nuggets to responses) for each response submitted by participants. While human evaluation is the most accurate way to compare systems, approximate automatic evaluation becomes critical during system development. We present Nuggeteer, a new automatic evaluation tool for nugget-based tasks. Like the first such tool, Pourpre, Nuggeteer uses words in common between candidate answer and answer key to approximate human judgements. Unlike Pourpre, but like human assessors, Nuggeteer creates a judgement for each candidatenugget pair, and can use existing judgements instead of guessing. This creates a more readily interpretable aggregate score, and allows developers to track individual nuggets through the variants of their system. Nuggeteer is quantitatively comparable in performance to Pourpre, and provides qualitatively better feedback to developers...|$|R
40|$|The TREC Question Answering Track {{presented}} several distinct {{challenges to}} participants in 2007. Participants {{were asked to}} create a system which discovers the answers to factoid and list questions about people, entities, organizations and events, given both blog and newswire text data sources. In addition, {{participants were asked to}} expose interesting <b>information</b> <b>nuggets</b> which exist in the data collection, which were not uncovered by the factoid or list questions. This year is the first time the Intelligent Information Processing group at Drexel has participated in the TREC Question Answering Track. As such, our goal was the development of a Question Answering system framework to which future enhancements could be made, and the construction of simple components to populate the framework. The results of our system this year were not significant; our primary accomplishment was the establishment of a baseline system which can be improved upon in 2008 and going forward...|$|R
40|$|Interoperability in BIM {{is low and}} {{the focus}} is on 3 D coordination. Despite the {{available}} standards including IFC and IDM, there is still no clear guidance how such standards can be effectively used for performance based design. Thus, early collaboration is discouraged and performance analysis is conducted as late as possible to minimize the number of information exchanges, leading to difficulties and costly changes in design that is almost completed. The aim is to propose an interoperability specification development approach for performance based design through the Design 4 Energy case study project. Findings show that the design process had increased flexibility, shared understanding between stakeholders about what <b>information</b> <b>nuggets</b> should be provided from whom to whom, at what stage, using which tool and data model. It can guide for the integrated BIM practice and help developing BIM execution plans for Level 2 BIM while paving the way for Level 3 BIM...|$|R
40|$|The TREC 2006 {{question}} answering (QA) track contained two tasks: the main task and the complex, interactive {{question answering}} (ciQA) task. As in 2005, the main task consisted of series of factoid, list, and “Other ” questions organized around {{a set of}} targets; in contrast to previous years, the evaluation of factoid and list responses distinguished between answers that were globally correct (with respect to the document collection), and those that were only locally correct (with respect to the supporting document). The ciQA task provided a framework for participants to investigate interaction {{in the context of}} complex information needs, and was a blend of the TREC 2005 QA relationship task and the TREC 2005 HARD track. Multiple assessors were used to judge the importance of <b>information</b> <b>nuggets</b> used to evaluate the responses to ciQA and “Other ” questions, resulting in an evaluation that is more stable and discriminative than one that uses only a single assessor to judge nugget importance. ...|$|R
40|$|This thesis proposes the {{creation}} of a scalable architecture to support the access and creation of Location Linked Information [...] the coupling of physical location with arbitrary virtual <b>information</b> <b>nuggets.</b> The proposed system dynamically links a physical space/time moment with a distributed database containing information that describes that moment's surroundings. This hybrid virtual/physical space, called glean space, is owned, managed, and rated by the public, {{for the benefit of the}} populace. Imagined as initially being embodied by an interactive, dynamic map viewed on a handheld computer, the system provides two functions for its urban users: 1) the retrieval of information about their surroundings, and 2) the optional annotation of location for communal benefit. It is our hypothesis that Location Linked Information will enhance the urban experience, just as access to transportation dramatically altered the sensation and form of the city. By making inhabitants hyper-aware of their surroundings, they get the benefits of a small town citizen (omniscience of space and society) while possibly being situated in a much larger megalopolis with the social mobility and features that go with it...|$|R
40|$|Abstract There exist {{large amounts}} of {{heterogeneous}} digital data. This phenomenon is called Big Data which will be examined. The examination of Big Data has been launched as Big Data analytics. In this paper, we present the literature review of definitions for Big Data analytics. The objective of this review is to describe current reported knowledge {{in terms of what}} kind of Big Data analytics is defined in the articles that can be found in ACM and IEEE Xplore databases in June 2013. We found 19 defining parts of the articles for Big Data analytics. Our review shows that Big Data analytics is verbosely explained, and the explanations have been meant for professionals. Furthermore, the findings show that the concept of Big Data analytics is unestablished. Big Data analytics is ambiguous to the professionals- how would we explain it to laypeople (e. g. leaders) ? Therefore, we launch the term data-milling to illustrate an effort to uncover the <b>information</b> <b>nuggets.</b> Data-milling can be seen as an examination of heterogeneous data or as part of competitive advantage. Our example concerns investments of coal power plants in Europe...|$|R
40|$|The {{information}} overload created by social media messages in emergency situations challenges response organizations to find targeted content and users. We aim to select useful messages by detecting {{the presence of}} conversation {{as an indicator of}} coordinated citizen action. Using simple linguistic indicators associated with conversation analysis in social science, we model the presence of conversation in the communication landscape of Twitter in a large corpus of 1. 5 M tweets for various disaster and non-disaster events spanning different periods, lengths of time and varied social significance. Within Replies, Retweets and tweets that mention other Twitter users, we found that domain-independent, linguistic cues distinguish likely conversation from non-conversation in this online (mediated) communication. We demonstrate that conversation subsets within Replies, Retweets and tweets that mention other Twitter users potentially contain more information than non-conversation subsets. Information density also increases for tweets that are not Replies, Retweets or mentioning other Twitter users, as long as they reflect conversational properties. From a practical perspective, we have developed a model for trimming the candidate tweet corpus to identify a much smaller subset of data for submission to deeper, domain-dependent semantic analyses for the identification of actionable <b>information</b> <b>nuggets</b> for coordinated emergency response...|$|R
40|$|Abstract In the following, {{we present}} an {{approach}} using interactive topic graph extraction for {{the exploration of}} web content. The initial information request, {{in the form of}} a query topic description, is issued online by a user to the system. The topic graph is then constructed from N web snippets that are produced by a standard search engine. We consider the extraction of a topic graph to be a specific empirical collocation extraction task, where collocations are extracted between chunks. Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. This topic graph can then be further analyzed by users so that they can request additional background information with the help of interesting nodes and pairs of nodes in the topic graph, e. g., explicit relationships extracted from Wikipedia or those automatically extracted from additional Web content as well as conceptual information of the topic in form of semantically oriented clusters of descriptive phrases. This information is presented to the users, who can investigate the identified <b>information</b> <b>nuggets</b> to refine their information search. An initial user evaluation shows that our approach is especially helpful for finding new interesting information on topics about which the user has only a vague idea or no idea, at all. ...|$|R
40|$|This paper {{presents}} an on-line quality assessment model based on Adaptive Neuro-Fuzzy Inference System (ANFIS). The ANFIS model is realized for identifying the RSW dynamical {{system based on}} given input output data. As a special neural network, ANFIS can approximate all nonlinear systems with less training data, quicker learning speed and higher precision. In this study, a system for monitoring various signals which provide real-time <b>information</b> of <b>nugget</b> formation and growth for RSW is established, {{and a series of}} experiments are conducted to research the correlation between these signals and weld quality. These signals include welding current, welding time and dynamic resistance. A set of dynamic resistance patterns are grouped based on their corresponding weld nugget quality, and were selected as the input data to train the proposed ANFIS model. Once the monitoring system had been trained, it was then tested to evaluate its efficiency and validity. The classifier based on ANFIS algorithm indicates the fast classification, showing a total success rate of 82. 1 per cent for test data...|$|R
40|$|Despite {{advances}} {{in science and}} technology of prediction and simulation of natural hazards, losses incurred due to natural disasters keep growing every year. Natural disasters cause more economic losses as compared to anthropogenic disasters. Economic losses due to natural hazards are estimated to be around $ 6 -$ 10 billion dollars annually for the U. S. and this number keeps increasing every year. This increase has been attributed to population growth and migration to more hazard prone locations such as coasts. As this trend continues, in concert with shifts in weather patterns caused by climate change, it is anticipated that losses associated with natural disasters will keep growing substantially. One of challenges disaster response and recovery analysts face is to quickly find, access and utilize a vast variety of relevant geospatial data collected by different federal agencies such as DoD, NASA, NOAA, EPA, USGS etc. Some examples of these data sets include high spatio-temporal resolution multi/hyperspectral satellite imagery, model prediction outputs from weather models, latest radar scans, measurements from an array of sensor networks such as Integrated Ocean Observing System etc. More often analysts may be familiar with limited, but specific datasets and are often unaware of or unfamiliar with a large quantity of other useful resources. Finding airborne or satellite data useful to a natural disaster event often requires a time consuming search through web pages and data archives. Additional information related to damages, deaths, and injuries requires extensive online searches for news reports and official report summaries. An analyst must also sift through vast amounts of potentially useful digital information captured by the general public such as geo-tagged photos, videos and real time damage updates within twitter feeds. Collecting and aggregating these information fragments can provide useful information in assessing damage in real time and help direct recovery efforts. The search process for the analyst could be made much more efficient and productive if a tool could go beyond a typical search engine and provide not just links to web sites but actual links to specific data relevant to the natural disaster, parse unstructured reports for useful <b>information</b> <b>nuggets,</b> as well as gather other related reports, summaries, news stories, and images. This presentation will describe a semantic aggregation tool developed to address similar problem for Earth Science researchers. This tool provides automated curation, and creates "Data Albums" to support case studies. The generated "Data Albums" are compiled collections of information related to a specific science topic or event, containing links to relevant data files (granules) from different instruments; tools and services for visualization and analysis; information about the event contained in news reports, and images or videos to supplement research analysis. An ontology-based relevancy-ranking algorithm drives the curation of relevant data sets for a given event. This tool is now being used to generate a catalog of Hurricane Case Studies at Global Hydrology Resource Center (GHRC), one of NASA's Distribute Active Archive Centers. Another instance of the Data Albums tool is currently being created in collaboration with NASA/MSFC's SPoRT Center, which conducts research on unique NASA products and capabilities that can be transitioned to the operational community to solve forecast problems. This new instance focuses on severe weather to support SPoRT researchers in their model evaluation studie...|$|R
40|$|This paper {{presents}} {{an approach to}} annotation that BAE Systems has employed in the DARPA GALE Phase 2 Distillation evaluation. The purpose of the GALE Distillation evaluation is to quantify the amount of relevant and non-redundant information a distillation engine is able to produce {{in response to a}} specific, formatted query; and to compare that amount of information to the amount of information gathered by a bilingual human using commonly available state-of-the-art tools. As part of the evaluation, following NIST evaluation methodology of complex question answering (Voorhees, 2003), human annotators were asked to establish the relevancy of responses as well as the presence of atomic facts or <b>information</b> units, called <b>nuggets</b> of <b>information.</b> This paper discusses various challenges to the annotation of nuggets, called nuggetization, which include interaction between the granularity of nuggets and relevancy of these nuggets to the query in question. The approach proposed in the paper views nuggetization as a procedural task and allows annotators to revisit nuggetization based on the requirements imposed by the relevancy guidelines defined with a specific end-user in mind. This approach is shown in the paper to produce consistent annotations with high inter-annotator agreement scores...|$|R
40|$|Geschat wordt dat 80 % van alle bedrijfsinformatie bestaat uit documenten waar informatie is beschreven in de vorm van vrije tekst, terwijl maar 20 % bestaat uit gestructureerde informatie. Verborgen in deze documenten is kostbare informatie die van strategisch belang kan zijn voor ondernemingen, zoals klachten van klanten en beschrijvingen van reparaties die door onderhoudspersoneel zijn uitgevoerd. Bestaande 'Business Intelligence' {{software}} is vooral gericht op gestructureerde data, zoals databases, en is niet in staat om informatie te ontdekken in tekstdocumenten. Als gevolg hiervan kunnen ondernemingen onvoldoende profiteren van belangrijke inzichten die aanwezig zijn in de documenten die het bedrijf produceert. Dit leidt tot gemiste kansen, risico's, en onvermogen om bijvoorbeeld op onvrede en niet vervulde verwachtingen van klanten te reageren. Binnen de Natuurlijke-taalverwerking (Natural Language Processing, NLP) zijn verschillende technieken ontwikkeld om informatie te ontdekken in teksten. Dit zijn onder andere technieken voor informatie-extractie (IE) en tekst-clustering (TC). Deze technieken zijn succesvol toegepast op grote collecties van goed geschreven tekst, zoals verzamelingen krantenartikelen en (medisch-) wetenschappelijke artikelen. Zulke documenten bevatten betrouwbare taalkundige en statistische patronen, die het ontdekken van informatie mogelijk maken. Documenten binnen ondernemingen, zoals die bijvoorbeeld gebruikt worden tijdens productontwikkeling en klantcontacten, vormen een uitdaging voor bestaande NLP-technieken. Zulke documenten zijn vaak zeer informeel, en bevatten regelmatig ongrammaticale zinsconstructies. Ze zijn meestal ook veel beknopter dan documenten waarop NLP-technieken getest zijn, en bevatten daarom ook minder betrouwbare taalkundige en statistische signalen. Bestaande NLP-technieken zijn daarom vaak niet in staat om accuraat informatie aan zulke teksten te onttrekken. Deze tekortkoming in bestaande NLP-technieken was de motivatie voor ons onderzoek. De onderzoeksvraag die centraal staat in dit werk is: “Hoe kan automatisch informatie gevonden worden in beknopte, domein-specifieke, en informele teksten?” Meer in het bijzonder onderzochten we de volgende drie deelvragen: RQ 1 : Hoe kunnen termen en concepten accuraat worden geïdentificeerd in domeinspecifieke, beknopte, en informele teksten binnen ondernemingen? RQ 2 : Hoe kunnen semantische relaties binnen domein-specifieke, beknopte, en informale teksten automatisch gevonden worden? RQ 3 : Hoe kunnen coherente en homogene clusters van vergelijkbare documenten worden gevonden. The main research question (RQ) that we {{addressed in this}} dissertation, as formulated in Chapter 1 was “How to automatically extract meaningful information from sparse, domain-specific and informally-written corporate texts? This question was motivated by two main factors. The first factor, with an industrial/corporate underpinning, pertained to the proliferation of unstructured text data, which {{has resulted in a}} drastic shift in the information space of business organizations. According to various studies, the traditional structured data, such as sales transactions in databases and data warehouses, currently account for a (relatively) insignificant 20 % of all corporate data. Conversely, an overwhelming 80 % of corporate data is in unstructured text format, such as customer complaints and repair notes of service engineers. Corporate organizations have also become increasingly aware that buried within these massive amounts of unstructured texts are meaningful <b>information</b> <b>nuggets,</b> which are valuable for supporting a wide-range of Business Intelligence activities, leading to products of better quality and more satisfied customers. The second factor that motivated our main RQ had an academic underpinning, and was the main focus of this thesis. It pertained to the challenges involved in identifying meaningful information from corporate texts. Several Natural Language Processing (NLP) algorithms have been developed for discovering meaningful information from texts. Most of the existing NLP techniques developed to date have been applied to large collections of wellwritten texts. These texts provide reliable linguistic and statistical evidence, which makes it easier for NLP algorithms to identify meaningful information items from their contents. However, texts generated from specialized domains, such as Product Development-Customer Service, pose a set of new challenges that extant NLP algorithms fail to adequately address. For example, these domain-specific texts are informally-written, and reliable linguistic features cannot be obtained from their contents. Also, they are sparse, and do not offer substantial statistical evidence. Due to the lack of reliable linguistic/statistical features, most NLP algorithms exhibit various shortcomings when applied to these domain-specific texts, and are inaccurate for extracting meaningful information from their contents. The lack of reliable features is further compounded by the scarcity of domain-specific knowledge resources, such as ontologies, which could have been exploited to facilitate the detection of meaningful information from texts. The above two factors highlighted the need for novel NLP techniques that successfully overcome the challenges posed by domain-specific corporate documents. They established our objectives with this research. We had two main objectives. Our first objective, with an industrial underpinning, was to provide business organizations with our algorithms and corresponding prototypes so that they could use them to unlock the valuable corporate information, buried in the massive amount of texts. Our second objective, which was the main aim of this thesis, had an academic underpinning. It was to fill the lacuna in extant NLP research by developing state-of-the-art techniques, which successfully alleviated the limitations of existing ones and addressed the challenges involved in extracting meaningful information from sparse, informally-written domain-specific texts. ...|$|R
40|$|During {{the design}} process for an {{aerospace}} vehicle, decision-makers must have an accurate understanding of how each choice will affect the vehicle and its performance. This understanding is based on experiments and, increasingly often, computer models. In general, as a computer model captures {{a greater number of}} phenomena, its results become more accurate for a broader range of problems. This improved accuracy typically comes at the cost of significantly increased computational expense per analysis. Although rapid analysis tools have been developed that are sufficient for many design efforts, those tools may not be accurate enough for revolutionary concepts subject to grueling flight conditions such as transonic or supersonic flight and extreme angles of attack. At such conditions, the simplifying assumptions of the rapid tools no longer hold. Accurate analysis of such concepts would require models that do not make those simplifying assumptions, with the corresponding increases in computational effort per analysis. As computational costs rise, exploration of the design space can become exceedingly expensive. If this expense cannot be reduced, decision-makers would be forced to choose between a thorough exploration of the design space using inaccurate models, or the analysis of a sparse set of options using accurate models. This problem is exacerbated as the number of free parameters increases, limiting the number of trades that can be investigated in a given time. In the face of limited resources, it can become critically important that only the most useful experiments be performed, which raises multiple questions: how can the most useful experiments be identified, and how can experimental results be used in the most effective manner? This research effort focuses on identifying and applying techniques which could address these questions. The demonstration problem for this effort was the modeling of a reusable booster vehicle, which would be subject {{to a wide range of}} flight conditions while returning to its launch site after staging. Contour-based sampling, an adaptive sampling technique, seeks cases that will improve the prediction accuracy of surrogate models for particular ranges of the responses of interest. In the case of the reusable booster, contour-based sampling was used to emphasize configurations with small pitching moments; the broad design space included many configurations which produced uncontrollable aerodynamic moments for at least one flight condition. By emphasizing designs that were likely to trim over the entire trajectory, contour-based sampling improves the predictive accuracy of surrogate models for such designs while minimizing the number of analyses required. The simplified models mentioned above, although less accurate for extreme flight conditions, can still be useful for analyzing performance at more common flight conditions. The simplified models may also offer insight into trends in the response behavior. Data from these simplified models can be combined with more accurate results to produce useful surrogate models with better accuracy than the simplified models but at less cost than if only expensive analyses were used. Of the data fusion techniques evaluated, Ghoreyshi cokriging was found to be the most effective for the problem at hand. Lastly, uncertainty present in the data was found to negatively affect predictive accuracy of surrogate models. Most surrogate modeling techniques neglect uncertainty in the data and treat all cases as deterministic. This is plausible, especially for data produced by computer analyses which are assumed to be perfectly repeatable and thus truly deterministic. However, a number of sources of uncertainty, such as solver iteration or surrogate model prediction accuracy, can introduce noise to the data. If these sources of uncertainty could be captured and incorporated when surrogate models are trained, the resulting surrogate models would be less susceptible to that noise and correspondingly have better predictive accuracy. This was accomplished in the present effort by capturing the uncertainty <b>information</b> via <b>nuggets</b> added to the Kriging model. By combining these techniques, surrogate models could be created which exhibited better predictive accuracy while selecting the most informative experiments possible. This significantly reduced the computational effort expended compared to a more standard approach using space-filling samples and data from a single source. The relative contributions of each technique were identified, and observations were made pertaining to the most effective way to apply the separate and combined methods. Ph. D...|$|R

