7|25|Public
50|$|Researchers publish {{data that}} they get from participants. To {{preserve}} participants' privacy, the data goes through a process to de-identify it. The goal of such a process would be to remove protected health information {{which could be used}} to connect a study participant to their contribution to a research project so that the participants will not suffer from <b>data</b> <b>re-identification.</b>|$|E
5000|$|<b>Data</b> <b>Re-Identification</b> is the {{practice}} of matching de-identified data with publicly available information, or auxiliary data, in order to discover the individual to which the data belongs to. This is a concern because companies with privacy policies, health care providers, and financial institutions may release the data they collect after the data has gone through the de-identification process. The de-identification process involves masking, generalizing or deleting both direct and indirect identifiers; the definition {{of this process is}} not universal, however. Information in the public domain, even seemingly anonymized, may thus be re-identified in combination with other pieces of available data and basic computer science techniques. The Common Rule Agencies, a collection of multiple U.S. federal agencies and departments including the U.S. Department of Health and Human Services, speculate that re-identification is becoming gradually easier because of [...] "big data" [...] - the abundance and constant collection and analysis of information along the evolution of technologies and the advances of algorithms. However, others have claimed that de-identification is a safe and effective data liberation tool and do not view re-identification as a concern.|$|E
40|$|The {{analysis}} of consumer-related and consumer-generated data {{for measuring the}} success of online retailing is gaining increasing importance. Software packages for data analysis have become commonplace. However, two major shortcomings exist. First, most software solutions are not offered as a service reachable by standard procedures over the Internet, but as isolated stand-alone applications or ERP system modules. Second, privacy restrictions need to be integrated into a framework of business analytics for Web retailers. Whereas the first aspect can be addressed with standardized developer software for Web services, the second must consider privacy legislation, privacy specifications on Web sites (P 3 P), and <b>data</b> <b>re-identification</b> problems. To address these shortcomings, we propose a formal model of these problems and an implementation of this model as a declarative specification of privacy constraints, expressed {{as an extension of}} P 3 P. The constraints are complemented by a logic that identifies, in a given set of Web analytics, those that might lead to <b>data</b> <b>re-identification</b> and would therefore violate implicit privacy constraints. We present a Webbased service that uses these components to automatically adapt the set of available Web analytics to an online retailer’s P 3 P policy. The system was tested on a large data set from a major European multi-channel retailer...|$|E
5000|$|Protecting {{statistically}} useful pseudonymized <b>data</b> from <b>re-identification</b> requires: ...|$|R
40|$|This paper {{provides}} {{examples that}} illustrate the severe analytic distortions of many widely used masking methods {{that have been}} in use for a number of years. The masking methods are intended to reduce or eliminate re-identification risk in public-use files. Although the masking methods yield files that do not allow reproduction of the analytic properties of original, confidential files, in a number of situations they sometimes allow small amounts of re-identification using elementary methods and widely available software. Keywords: <b>Data</b> Quality, <b>Re-identification</b> 1...|$|R
40|$|Abstract. This paper {{analyses}} the re-identifiability of Dutch citizens {{by various}} demographics. Our analysis {{is based on}} registry office data of 2. 7 million Dutch citizens, ∼ 16 % of the total population. We pro-vide overall statistics on re-identifiability {{for a range of}} quasi-identifiers, and present an in-depth analysis of quasi-identifiers found in two de-identified data sets. We found that 67. 0 % of the sampled population is unambiguously identifiable by date of birth and four-digit postal code alone, and that 99. 4 % is unambiguously identifiable if date of birth, full postal code and gender are known. Furthermore, two quasi-identifiers we examined from real-life data sets turn out to unambiguously iden-tify {{a small fraction of the}} sampled population. As far as we are aware, this is the first re-identifiability assessment of Dutch citizens that uses authoritative registry office <b>data.</b> Key words: <b>re-identification,</b> <b>data</b> anonymity...|$|R
40|$|This paper {{provides}} algorithms {{for learning}} {{the identities of}} individuals from the trails of seemingly anonymous information they leave behind. Consider online consumers, who have the IP addresses of their computers logged at each website visited. Many falsely believe they cannot be identified. The term “re-identification ” refers to correctly relating seemingly anonymous data to explicitly identifying information (such as the name or address) {{of the person who}} is the subject of those <b>data.</b> <b>Re-identification</b> has historically been associated with data released from a single data holder. This paper extends the concept to “trail re-identification ” in which a person is related to a trail of seemingly anonymous and homogenous data left across different locations. The 3 novel algorithms presented in this paper perform trail re-identifications by exploiting the fact that some location...|$|E
30|$|Anonymization methods, {{based on}} k-Anonymity, {{have been widely}} {{employed}} to prevent <b>data</b> <b>re-identification</b> [7]. Anonymization methods fall into two broad categories. The first category constitutes of techniques that generalize data {{from the bottom of}} the taxonomy tree towards its top and are referred to as the Bottom–Up Generalization (BUG) [8]. The second one is based on walking through the taxonomy tree from the top towards the bottom, known as the Top–Down Specialization (TDS) [9]. TDS and BUG methods were mainly developed for traditional data. Therefore, researchers upgraded the old methods to suit the new operations of big data. The operations should consider the parallel and distributed processing steps. Various methods of anonymization were specifically designed for parallel distributed processing. However, most resolutions fall short of a proper parallelization capability. The reason for this is further explained in this review.|$|E
40|$|Consumer metrics for {{analyzing}} {{the success of}} customer relationship management (CRM) are gaining increasing importance. CRM software packages have become commonplace. However, two major shortcomings exist. First, most software solutions are not offered as a web service on the Internet. Second, privacy restrictions need to be integrated into an overall framework of success indicators for consumer analysis. Whereas the first aspect can be addressed with standardized developer software for web services, the second must consider privacy legislation, privacy specifications on web sites (P 3 P), and <b>data</b> <b>re-identification</b> problems. To address these problems, we have developed a web service [...] called SIMT [...] that automatically adapts CRM indicators to an online retailer's P 3 P policy. It {{is based on a}} declarative specification of privacy constraints, and syntactically extends P 3 P. This paper presents the prototype and describes how inference problems and legal restrictions have been addressed. The system has been tested on data from a large multi-channel retailer...|$|E
40|$|Record linkage {{methods are}} methods for {{identifying}} {{the presence of}} the same individual in di#erent <b>data</b> files (<b>re-identification).</b> This paper studies and compares the two main existing approaches for record linkage: probabilistic and distance-based. The performance of both approaches is compared when data are categorical. To that end, a distance over ordinal and nominal scales is defined. The paper shows that, for categorical data, distance-based and probabilistic-based record linkage lead to similar results. This is parallel to comparisons in the literature for numerical data, which also showed a similar behaviour between both record-linkage approaches. As a consequence, the distance proposed for ordinal and nominal scales is implicitly validated...|$|R
40|$|In this paper, {{we focus}} on the problem of {{preserving}} the privacy of sensitive relationships in graph data. We refer to the problem of inferring sensitive relationships from anonymized graph <b>data</b> as link <b>re-identification.</b> We propose five different privacy preservation strategies, which vary in terms of the amount of data removed (and hence their utility) and the amount of privacy preserved. We assume the adversary has an accurate predictive model for links, and we show experimentally the success of different link re-identification strategies under varying structural characteristics of the data...|$|R
40|$|Synthetic {{generators}} {{are increasingly}} used to replace sensitive data with artificial data preserving to a predetermined extent {{the utility of}} the original data. When using synthetic <b>data</b> generators, <b>re-identification</b> analysis is usually disregarded on the grounds that, the released data being artificial, no real re-identification is possible. While this may be reasonable if synthetic generation is performed on the confidential outcome attributes, it is an unrealistic assumption if synthetic data generation is performed on the quasi-identifier attributes. In the latter case, reidentification can indeed happen if a snooper is able to link an external identified data source with some record in the released dataset using the quasi-identifier attributes: coming up with a correct pair (identifier, confidential attributes) is indeed a re-identification. This paper is a case study of re-identification risk for three synthetic data generators under different worst-case disclosure scenarios. The results give some insight on how synthetic data generators can be tuned to minimize re-identification risk...|$|R
40|$|There {{has been}} a growing {{interest}} in sharing and mining social network data {{for a wide variety}} of applications. In this paper, we address the problem of privacy disclosure risks that arise from publishing social network data. Specifically, we look at the vertex re-identification attack that aims to link specific vertex in social network data to specific individual in the real world. We show that even when identifiable attributes such as names are removed from released social network <b>data,</b> <b>re-identification</b> attack is still possible by manipulating abstract information. We present a new type of vertex re-identification attack model called neighbourhood-pair attack. This attack utilizes the information about the local communities of two connected vertices to identify the target individual. We show both theoretically and empirically that the proposed attack provides higher re-identification rate compared with the existing re-identification attacks that also manipulate network structure properties. The experiments conducted also show that the proposed attack is still possible even on anonymised social network data...|$|E
5000|$|While a {{complete}} ban on re-identification has been urged, {{the enforcement of}} this is impossible. However, there are ways for lawmakers to combat and punish re-identification efforts, if {{and when they are}} exposed:pair a ban with harsher penalties and stronger enforcement by the Federal Trade Commission and the Federal Bureau of Investigation, grant victims of re-identification a right of action against those who re-identify them, mandate software audit trails for people who utilize and analyze anonymized <b>data.</b> A small-scale <b>re-identification</b> ban may also be imposed on trusted recipients of particular databases, such as government data miners. This ban would be much easier to enforce and may discourage re-identification in other spheres and in the future.|$|R
40|$|Social {{scientists}} increasingly {{expect to}} have access to detailed data for research purposes. As the level of detail increases, data providers worry about “spontaneous recognition”, the likelihood that a microdata user believes that he or she has accidentally identified one of the data subjects in the dataset, and may share that information. This concern, particularly in respect of microdata on businesses, leads to excessive restrictions on data use. We argue that spontaneous recognition presents no meaningful risk to confidentiality. The standard models of deliberate attack on the <b>data</b> cover <b>re-identification</b> risk to an acceptable standard under most current legislation. If spontaneous recognition did occur, the user is very unlikely to be in breach of any law or condition of access. Any breach would only occur as a result of further actions by the user to confirm or assert identity, and these should be seen as a managerial problem. Nevertheless, a consideration of spontaneous recognition does highlight some of the implicit assumptions made in data access decisions. It also shows the importance of the data provider’s culture and attitude. For data providers focused on users, spontaneous recognition is a useful check on whether all relevant risks have been addressed. For data providers primarily concerned with the risks of release, it provides a way to place insurmountable barriers in front of those wanting to increase data access. We present a case study on a business dataset to show how rejecting the concep...|$|R
40|$|International audienceIn many {{surveillance}} systems {{there is}} a requirement {{to determine whether a}} given person of interest has already been observed over a network of cameras. This is the person re-identification problem. The human appearance obtained in one camera is usually different from the ones obtained in another camera. In order to re-identify people the human signature should handle difference in illumination, pose and camera parameters. We propose a new appearance model based on spatial covariance regions extracted from human body parts. The new spatial pyramid scheme is applied to capture the correlation between human body parts in order to obtain a discriminative human signature. The human body parts are automatically detected using Histograms of Oriented Gradients (HOG). The method is evaluated using benchmark video sequences from i-LIDS Multiple-Camera Tracking Scenario <b>data</b> set. The <b>re-identification</b> performance is presented using the cumulative matching characteristic (CMC) curve. Finally, we show that the proposed approach outperforms state of the art methods...|$|R
40|$|Person re-identification {{consists}} {{of searching for}} an individual of interest in video sequences acquired by a camera network, using an image of that individual as a query. Here we consider a related task, named appearance-based people search, which {{consists of}} searching images of individuals using a textual description of clothing appearance as a query, given by a Boolean combination of predefined attributes. People search {{can be useful in}} applications like forensic video analysis, where the query can be obtained from a eyewitness report. We propose a general method for implementing people search as an extension of a given reidentification system that uses any multiple part-multiple component appearance descriptor. To this aim, we chose a predefined set of attributes by taking into account the information provided by the original descriptor, transform such a descriptor into a dissimilarity one, and use the resulting dissimilarity values as input features for training attribute detectors. We experimentally evaluate our method on a benchmark <b>re-identification</b> <b>data</b> set. JRC. G. 7 -Digital Citizen Securit...|$|R
40|$|Person re-identification in a multi-camera {{environment}} {{is an important}} part of modern surveillance systems. Person re-identification from color images has been the focus of much active research, due to the numerous challenges posed with such analysis tasks, such as variations in illumination, pose and viewpoints. In this paper, we suggest that hyperspectral imagery has the potential to provide unique information that is expected to be beneficial for the re-identification task. Specifically, we assert that by accurately characterizing the unique spectral signature for each person's skin, hyperspectral imagery can provide very useful descriptors (e. g. spectral signatures from skin pixels) for re-identification. Towards this end, we acquired proof-of-concept hyperspectral <b>re-identification</b> <b>data</b> under challenging (practical) conditions from 15 people. Our results indicate that hyperspectral data result in a substantially enhanced re-identification performance compared to color (RGB) images, when using spectral signatures over skin as the feature descriptor. Comment: Accepted for presentation at the 8 'th Workshop on Hyperspectral Image and Signal Processing, UCLA, August 201...|$|R
40|$|In many {{surveillance}} systems {{there is}} a requirement {{to determine whether a}} given person of interest has already been observed over a network of cameras. This is the person re-identification problem. The human appearance obtained in one camera is usually different from the ones obtained in another camera. In order to re-identify people the human signature should handle difference in illumination, pose and camera parameters. We propose a new appearance model based on spatial covariance regions extracted from human body parts. The new spatial pyramid scheme is applied to capture the correlation between human body parts in order to obtain a discriminative human signature. The human body parts are automatically detected using Histograms of Oriented Gradients (HOG). The method is evaluated using benchmark video sequences from i-LIDS Multiple-Camera Tracking Scenario <b>data</b> set. The <b>re-identification</b> performance is presented using the cumulative matching characteristic (CMC) curve. Finally, we show that the proposed approach outperforms state of the art methods. 1...|$|R
40|$|Person re-identification is {{the task}} of {{correctly}} matching visual appearances of the same person in image or video data while distinguishing appearances of different persons. The traditional setup for re-identification is a network of fixed cameras. However, in recent years mobile aerial cameras mounted on unmanned aerial vehicles (UAV) have become increasingly useful for security and surveillance tasks. Aerial data has many characteristics different from typical camera network <b>data.</b> Thus, <b>re-identification</b> approaches designed for a camera network scenario {{can be expected to}} suffer a drop in accuracy when applied to aerial data. In this work, we investigate the suitability of features, which were shown to give robust results for re- identification in camera networks, for the task of re-identifying persons between a camera network and a mobile aerial camera. Specifically, we apply hand-crafted region covariance features and features extracted by convolutional neural networks which were learned on separate data. We evaluate their suitability for this new and as yet unexplored scenario. We investigate common fusion methods to combine the hand-crafted and learned features and propose our own deep fusion approach which is already applied during training of the deep network. We evaluate features and fusion methods on our own dataset. The dataset consists of fourteen people moving through a scene recorded by four fixed ground-based cameras and one mobile camera mounted on a small UAV. We discuss strengths and weaknesses of the features in the new scenario and show that our fusion approach successfully leverages the strengths of each feature and outperforms all single features significantly...|$|R
40|$|Person re-identification {{consists}} {{of searching for}} an individual of interest in video sequences acquired by a camera network, using animage of that individual as a query. Here we consider a related task, named people search with textual queries, which {{consists of}} searching images of individuals that match a textual description of clothing appearance, given by a Boolean combination of predefined attributes. People search {{can be useful in}} applications like forensic video analysis, where the query can be obtained from a eyewitness report. We propose a general method for implementing people search as an extension of any given re-identification system that uses any multiple part-multiple component appearance descriptor. In our method the same descriptor of the re-identification system at hand is used, and attributes are chosen by taking into account the information it provides. The original descriptor is then transformed into a dissimilarity one. Attribute detectors are finally constructed as supervised classifiers, using dissimilarity descriptors as the input feature vectors. We experimentally evaluate our method on a benchmark <b>re-identification</b> <b>data</b> set...|$|R
40|$|Abstract. Visual {{identification}} {{of an individual}} in a crowded environ-ment observed by a distributed camera network is critical {{to a variety of}} tasks including commercial space management, border control, and crime prevention. Automatic re-{{identification of}} a human from public space CCTV video is challenging due to spatiotemporal visual feature varia-tions and strong visual similarity in people’s appearance, compounded by low-resolution and poor quality video <b>data.</b> Relying on <b>re-identification</b> using a probe image is limiting, as a linguistic description of an individ-ual’s profile may often be the only available cues. In this work, we show how mid-level semantic attributes can be used synergistically with low-level features for both identification and re-identification. Specifically, we learn an attribute-centric representation to describe people, and a met-ric for comparing attribute profiles to disambiguate individuals. This differs from existing approaches to re-identification which rely purely on bottom-up statistics of low-level features: it allows improved robustness to view and lighting; and can be used for identification as well as re-identification. Experiments demonstrate the flexibility and effectiveness of our approach compared to existing feature representations when ap-plied to benchmark datasets. ...|$|R
40|$|We {{present a}} method for {{generating}} synthetic versions of Twitter data using neural generative models. The goal is to protect individuals in the source <b>data</b> from stylometric <b>re-identification</b> attacks while still releasing data that carries research value. To generate tweet corpora that maintain user-level word distributions, our proposed approach augments powerful neural language models with local parameters that weight user-specific inputs. We compare our work to two standard text data protection methods: redaction and iterative translation. We evaluate the three methods on risk and utility. We define risk following the stylometric models of re-identification, and we define utility based on two general language measures and two common text analysis tasks. We find that neural models are able to significantly lower risk over previous methods {{at the cost of}} some utility. More importantly, we show that the risk utility trade-off depends on how the neural model's logits (or the unscaled pre-activation values of the output layer) are scaled. This work presents promising results for a new tool addressing the problem of privacy for free text and sharing social media data in a way that respects privacy and is ethically responsible...|$|R
40|$|Abstract—Companies and {{government}} agencies frequently own data sets containing personal information about clients, survey responders, or users of a product. Sometimes these organizations are required or wish to release anonymized versions of this information to the public. Prior to releasing these data, they use established privacy preservation methods such as binning, data perturbation, and data suppression to maintain the anonymity of clients, customers, or survey participants. However, existing work has shown that common privacy preserving measures fail when anonymized data are combined with data from online social networks, social media sites, and data aggregation sites. This paper introduces a methodology for determining the vulnerability of individuals in a pre-released <b>data</b> set to <b>re-identification</b> using public <b>data.</b> As part of this methodology, we propose novel metrics to quantify {{the amount of information}} that can be gained from combining pre-released data with publicly available online data. We then investigate how to utilize our metrics to identify individuals in the data set who may be particularly vulnerable to this form of data combination. We demonstrate the effectiveness of our methodology on a real world data set using public data from both social networking and data aggregation sites. I...|$|R
40|$|Crowdsourced GPS {{probe data}} {{has been gaining}} {{popularity}} {{in recent years as}} a source for real-time traffic information. Efforts have been made to evaluate the quality of such data from different perspectives. A quality indicator of any traffic data source is latency that describes the punctuality of data, which is critical for real-time operations, emergency response, and traveler information systems. This paper offers a methodology for measuring the probe data latency, with respect to a selected reference source. Although Bluetooth <b>re-identification</b> <b>data</b> is used as the reference source, the methodology can be applied to any other ground-truth data source of choice (i. e. Automatic License Plate Readers, Electronic Toll Tag). The core of the methodology is a maximum pattern matching algorithm that works with three different fitness objectives. To test the methodology, sample field reference data were collected on multiple freeways segments for a two-week period using portable Bluetooth sensors as ground-truth. Equivalent GPS probe data was obtained from a private vendor, and its latency was evaluated. Latency at different times of the day, the impact of road segmentation scheme on latency, and sensitivity of the latency to both speed slowdown, and recovery from slowdown episodes are also discussed...|$|R
40|$|Matching {{individuals}} across non-overlapping camera networks, {{known as}} person re-identification, is a fundamentally challenging problem {{due to the}} large visual appearance changes caused by variations of viewpoints, lighting, and occlusion. Approaches in literature can be categoried into two streams: The first stream is to develop reliable features against realistic conditions by combining several visual features in a pre-defined way; the second stream is to learn a metric from training data to ensure strong inter-class differences and intra-class similarities. However, seeking an optimal combination of visual features which is generic yet adaptive to different benchmarks is a unsoved problem, and metric learning models easily get over-fitted due to the scarcity of training <b>data</b> in person <b>re-identification.</b> In this paper, we propose two effective structured learning based approaches which explore the adaptive effects of visual features in recognizing persons in different benchmark data sets. Our framework is built {{on the basis of}} multiple low-level visual features with an optimal ensemble of their metrics. We formulate two optimization algorithms, CMCtriplet and CMCstruct, which directly optimize evaluation measures commonly used in person re-identification, also known as the Cumulative Matching Characteristic (CMC) curve. Comment: 16 pages. Extended version of "Learning to Rank in Person Re-Identification With Metric Ensembles", at [URL] arXiv admin note: text overlap with arXiv: 1503. 0154...|$|R
40|$|Background: With {{the growing}} {{adoption}} of electronic medical records, there are increasing demands {{for the use}} of this electronic clinical data in observational research. A frequent ethics board requirement for such secondary use of personal health information in observational research is that the data be de-identified. De-identification heuristics are provided in the Health Insurance Portability and Accountability Act Privacy Rule, funding agency and professional association privacy guidelines, and common practice. Objective: The aim {{of the study was to}} evaluate whether the re-identification risks due to record linkage are sufficiently low when following common de-identification heuristics and whether the risk is stable across sample sizes and data sets. Methods: Two methods were followed to construct identification <b>data</b> sets. <b>Re-identification</b> attacks were simulated on these. For each data set we varied the sample size down to 30 individuals, and for each sample size evaluated the risk of re-identification for all combinations of quasi-identifiers. The combinations of quasi-identifiers that were low risk more than 50 % of the time were considered stable. Results: The identification data sets we were able to construct were the list of all physicians and the list of all lawyers registered in Ontario, using 1 % sampling fractions. The quasi-identifiers of region, gender, and year of birth were found to be low risk more than 50 % of the time across both data sets. The combination of gender and region was also found to be low risk more than 50 % of the time. We were not able to create an identification data set for the whole population. Conclusions: Existing Canadian federal and provincial privacy laws help explain why it is difficult to create an identification data set for the whole population. That such examples of high re-identification risk exist for mainstream professions makes a strong case for not disclosing the high-risk variables and their combinations identified here. For professional subpopulations with published membership lists, many variables often needed by researchers would have to be excluded or generalized to ensure consistently low <b>re-identification</b> risk. <b>Data</b> custodians and researchers need to consider other statistical disclosure techniques for protecting privacy...|$|R
40|$|Abstract Background Large-scale {{genetic data}} sets are {{frequently}} shared with other research groups and even released on the Internet {{to allow for}} secondary analysis. Study participants are usually not informed about such data sharing because data sets {{are assumed to be}} anonymous after stripping off personal identifiers. Discussion The assumption of anonymity of genetic data sets, however, is tenuous because genetic data are intrinsically self-identifying. Two types of re-identification are possible: the "Netflix" type and the "profiling" type. The "Netflix" type needs another small genetic data set, usually with less than 100 SNPs but including a personal identifier. This second data set might originate from another clinical examination, a study of leftover samples or forensic testing. When merged to the primary, unidentified set it will re-identify all samples of that individual. Even with no second data set at hand, a "profiling" strategy can be developed to extract as much information as possible from a sample collection. Starting with the identification of ethnic subgroups along with predictions of body characteristics and diseases, the asthma kids case as a real-life example is used to illustrate that approach. Summary Depending on the degree of supplemental information, {{there is a good chance}} that at least a few individuals can be identified from an anonymized <b>data</b> set. Any <b>re-identification,</b> however, may potentially harm study participants because it will release individual genetic disease risks to the public. </p...|$|R
40|$|Crowdsourced GPS {{probe data}} {{has become a}} major source of {{real-time}} traffic information applications. In addition to traditional traveler advisory systems such as dynamic message signs (DMS) and 511 systems, probe data is being used for automatic incident detection, Integrated Corridor Management (ICM), end of queue warning systems, and mobility-related smartphone applications. Several private sector vendors offer minute by minute network-wide travel time and speed probe data. The quality of such data in terms of deviation of the reported travel time and speeds from ground-truth has been extensively studied in recent years, and as a result concerns over the accuracy of probe data has mostly faded away. However, the latency of probe data, defined as the lag between the time that disturbance in traffic speed is reported in the outsourced data feed, and the time that the traffic is perturbed, has become a subject of interest. The extent of latency of probe data for real-time applications is critical, so {{it is important to have}} a good understanding of the amount of latency and its influencing factors. This paper uses high-quality independent Bluetooth/Wi-Fi <b>re-identification</b> <b>data</b> collected on multiple freeway segments in three different states, to measure the latency of the vehicle probe data provided by three major vendors. The statistical distribution of the latency and its sensitivity to speed slowdown and recovery periods are discussed. Comment: This paper was submitted to TRB annual meeting 201...|$|R
40|$|Abstract Background Vast {{quantities}} of electronic data are collected about patients and service users {{as they pass}} through health service and other public sector organisations, and these data present enormous potential for research and policy evaluation. The Health Information Research Unit (HIRU) aims to realise the potential of electronically-held, person-based, routinely-collected data to conduct and support health-related studies. However, there are considerable challenges {{that must be addressed}} before such data can be used for these purposes, to ensure compliance with the legislation and guidelines generally known as Information Governance. Methods A set of objectives was identified to address the challenges and establish the Secure Anonymised Information Linkage (SAIL) system in accordance with Information Governance. These were to: 1) ensure data transportation is secure; 2) operate a reliable record matching technique to enable accurate record linkage across datasets; 3) anonymise and encrypt the <b>data</b> to prevent <b>re-identification</b> of individuals; 4) apply measures to address disclosure risk in data views created for researchers; 5) ensure data access is controlled and authorised; 6) establish methods for scrutinising proposals for data utilisation and approving output; and 7) gain external verification of compliance with Information Governance. Results The SAIL databank has been established and it operates on a DB 2 platform (Data Warehouse Edition on AIX) running on an IBM 'P' series Supercomputer: Blue-C. The findings of an independent internal audit were favourable and concluded that the systems in place provide adequate assurance of compliance with Information Governance. This expanding databank already holds over 500 million anonymised and encrypted individual-level records from a range of sources relevant to health and well-being. This includes national datasets covering the whole of Wales (approximately 3 million population) and local provider-level datasets, with further growth in progress. The utility of the databank is demonstrated by increasing engagement in high quality research studies. Conclusion Through the pragmatic approach that has been adopted, {{we have been able to}} address the key challenges in establishing a national databank of anonymised person-based records, so that the data are available for research and evaluation whilst meeting the requirements of Information Governance. </p...|$|R
40|$|Research Doctorate - Doctor of Philosophy (PhD) Due to {{advances}} in information processing technology and storage capacity, nowadays {{huge amount of}} data is being collected for various data analyses. Data mining techniques, such as classification, are often applied on these data to extract hidden information. During {{the whole process of}} data mining the data get exposed to several parties and such an exposure potentially leads to breaches of individual privacy. This thesis presents a comprehensive noise addition technique for protecting individual privacy in a data set used for classification, while maintaining the data quality. We add noise to all attributes, both numerical and categorical, and both to class and non-class, in such a way so that the original patterns are preserved in a perturbed data set. Our technique is also capable of incorporating previously proposed noise addition techniques that maintain the statistical parameters of the data set, including correlations among attributes. Thus the perturbed data set may be used not only for classification but also for statistical analysis. Our proposal has two main advantages. Firstly, as also suggested by our experimental results the perturbed data set maintains the same or very similar patterns as the original data set, as well as the correlations among attributes. While there are some noise addition techniques that maintain the statistical parameters of the data set, {{to the best of our}} knowledge this is the first comprehensive technique that preserves the patterns and thus removes the so called Data Mining Bias from the perturbed <b>data</b> set. Secondly, <b>re-identification</b> of the original records directly depends on the amount of noise added, and in general can be made arbitrarily hard, while still preserving the original patterns in the data set. The only exception to this is the case when an intruder knows enough about the record to learn the confidential class value by applying the classifier. However, this is always possible, even when the original record has not been used in the training data set. In other words, providing that enough noise is added, our technique makes the records from the training set as safe as any other previously unseen records of the same kind. In addition to the above contribution, this thesis also explores the suitability of pre-diction accuracy as a sole indicator of data quality, and proposes technique for clustering both categorical values and records containing such values...|$|R
40|$|Intelligent Transportation Systems (ITS) are {{cost-effective}} {{measures to}} manage congestion due to increasing demand by improving {{the efficiency of}} existing transportation infrastructure. Traffic detection and surveillance play {{a pivotal role in}} deploying these technologies in the field. This dissertation continues the work that has been done in recent years in relation to the use of wireless magnetic sensor networks in transportation systems. As part of the effort to improve vehicle detection system technologies so that better management strategies can be implemented in the field, the work presented here focuses on advancing the use of wireless magnetic sensors in Intelligent Transportation Systems. This dissertation addresses improvements in algorithmic tools that advance the use of wireless magnetic sensors for both freeways and arterials. The applications addressed here include on-ramp queue estimation, arterial link vehicle-count, travel time estimation on heavily congested arterial streets, travel time and link vehicle-count in freeways, truck re-identification along long freeway segments, as well as cost-effective vehicle classification. The overall goal of this dissertation is to advance the use of these basic detection technologies to roles that extend beyond basic vehicle detection. A vehicle re-identification system, which relies on matching vehicle signatures from wireless magnetic sensors is modified to improve its performance for stop-and-go traffic conditions and is extended {{so that it can be}} used for truck re-identification along long freeway segments. The modifications to the algorithm address problems observed when vehicles stop or accelerate/decelerate as they go through the sensors. The modified system was tested to ensure that it overcame the deficiencies imposed by the original system. The extension of the vehicle re-identification system, presented as the iterative vehicle re-identification system, addresses traffic dynamics observed when vehicles travel along long road segments, in particular, vehicle overtaking. The system was tested extensively to ensure that it can be deployed for truck re-identification along long freeway segments, e. g., in between weigh-in-motion (WIM) stations. A link vehicle-count and a travel time estimator based on flow-measurements and vehicle <b>re-identification</b> <b>data</b> were studied at a freeway on-ramp, arterial segments as well as at freeway segments. The results show that the estimators are reliable and accurate, and are suitable for real-time traffic responsive management strategies that require precise link vehicle-count and/or vehicle travel time information, such as ramp metering, speed control and traffic intersection control. Vehicle classification, which utilizes a single wireless magnetic sensor installed in the middle of a freeway lane is also presented. The approach uses a two stage binary support vector machine (SVM) classifier based on features extracted from vehicle signatures. This is a cost effective classification system that uses a small subset of data efficiently extracted from the magnetic signal measured by the sensor. The results showed that vehicles can be reliably and accurately classified into passenger vehicles and trucks, and once trucks are extracted, this group can be further divided, with lower accuracy and consistency, into two groups: small trucks and large trucks. Finally, this dissertation presents a systematic tool for tuning vehicle re-identification parameters and evaluating performance. This tool uses different plots, metrics and algorithms to evaluate the output of the vehicle re-identification algorithm as well as estimates based on it, i. e., link vehicle-count and vehicle travel time...|$|R

