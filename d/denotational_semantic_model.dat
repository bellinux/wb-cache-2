8|4316|Public
40|$|This paper {{presents}} the derivation of an operational semantics from a denotational semantics for {{a subset of}} the widely used hardware description language Verilog. Our aim is to build equivalence between the operational and denotational semantics. We propose a discrete <b>denotational</b> <b>semantic</b> <b>model</b> for Verilog. A phase semantics is provided for each type of transition in order to derive the operational semantics...|$|E
40|$|A <b>denotational</b> <b>semantic</b> <b>model</b> of an Algol-like {{programming}} language with local variables, providing fully functional dynamic variable manipulation is presented. Along {{with the other}} usual language features, the standard operations with pointers, that is reattachement and dereferencing, and dynamic variables, that is creation and assignment, are explicated using a possible worlds, functor category, location oriented model. It is shown that the model used to explicate local variables in Algol-like languages can be extended to dynamic variables and pointers. Such a model allows for an analytic comparison of the properties of local and dynamic variables and, at the same time, validates several equivalences that, by common computational and operational intuition, are expected to hold. Two fundamental types of equivalences for linked data structures created using pointers are defined, observational equivalence and ae-isomorphism, and it is contended that they are both the extensional respect [...] ...|$|E
40|$|In this paper, {{we report}} on the {{development}} of a language which is especially tailored to the specification and simulation of microprocessor operations and parallel instructions. The approach is rigorous, and it combines the naturalness and readability of the traditional pseudocode with the formality and rigour of instruction specifications in the programming language C (but without the disadvantages of the latter). The underlying semantic model has been formalized by the equations of an appropriate <b>denotational</b> <b>semantic</b> <b>model.</b> The specifications can be used for a variety of purposes, such as the generation of a data book and other on-line documentation, the generation of a simulator that allows functional testing of programs even before the hardware has been designed and implemented, and the generation of a test suite to perform functional tests of a given design or real chip...|$|E
40|$|AbstractWe define for {{a number}} of {{concurrent}} imperative languages both operational and <b>denotational</b> <b>semantic</b> <b>models</b> as fixed points of contractions on complete metric spaces. Next, we develop a general method for comparing different <b>semantic</b> <b>models</b> by relating their defining contractions and exploiting the fact that contractions have a unique fixed point...|$|R
40|$|We {{consider}} {{the language of}} CSP extended with a construct that allows processes to test whether a particular event is available (without actually performing the event). We present an operational semantics for this language, together with two congruent <b>denotational</b> <b>semantic</b> <b>models.</b> We also show how this extended language can be simulated using standard CSP, {{so as to be}} able to analyse systems using the model checker FDR. Copyright © 2009 The authors and IOS Press...|$|R
40|$|We {{investigate}} {{some aspects}} of interexpressiveness of languages and their (<b>denotational)</b> <b>semantic</b> <b>models</b> by viewing <b>semantic</b> functions from a complexity-theory viewpoint. We classify semantic functions as polynomial or finite if a language term of size n produces a meta-language object respectively polynomially bounded in n or finite. Languages involving concurrency manifest most interest which we associate {{to the fact that}} their <b>semantic</b> <b>models</b> in general lack -style abstraction. The paper provides a quantifiable reason why labelled event structures form a more reasonable model for the choice and concurrency operators of CCS than do synchronisation trees. Similarly we show the representation of conflict by places within (at least occurrence forms of) Petri-nets is exponentially larger than the relational representation within corresponding event structures. An application is a criterion for selection of <b>semantic</b> <b>models</b> for real-world algorithmic purposes; for example, `model checki [...] ...|$|R
40|$|We {{provide a}} new <b>denotational</b> <b>semantic</b> <b>model,</b> based on “footstep traces”, for {{parallel}} programs which share mutable state. The model embodies a classic principle proposed by Dijkstra: processes {{should be treated}} independently, with interference occurring only at synchronization points. The result is a model which makes fewer distinctions between programs than traditional trace models, helping to mitigate the combinatorial explosion triggered by interleaving. Indeed, for a sequential or synchronization-free program the footstep trace semantics is equivalent to a nondeterministic state transformation. The new model {{can be used to}} validate the soundness of concurrent separation logic, replacing the action trace semantic model used in previous work for that purpose and yielding a conceptually simpler proof. We include some example programs to facilitate comparison with earlier models, and we discuss briefly the relationship with a recent model by John Reynolds in which actions have discernable starts and finishes...|$|E
40|$|An {{operational}} and a <b>denotational</b> <b>semantic</b> <b>model</b> {{are presented}} for a real-time programming language incorporating {{the concept of}} integration. This concept of integration, which has been introduced by Baeten and Bergstra [4], enables us to specify a restricted form of unbounded non-determinism. For example, the execution of an action at an arbitrary moment in a time interval can be specified using integration. The operational and the denotational model are proved to be equivalent using a general method based on higher-order transformations and complete metric spaces. In this context, Banach's fixed point theorem and Michael's theorem {{will turn out to}} be the most important aspects of complete metric spaces. Banach's theorem, which states that a contraction on a complete metric space has a unique fixed point, will be used to define semantic models and to compare semantic models. Michael's theorem, which roughly states that a compact union of compact sets is compact, will be used for the d [...] ...|$|E
40|$|AbstractWe {{provide a}} new <b>denotational</b> <b>semantic</b> <b>model,</b> based on “footstep traces”, for {{parallel}} programs which share mutable state. The structure {{of this model}} embodies a classic principle proposed by Dijkstra: processes should be treated independently, with interference occurring only at synchronization points. As a consequence the model makes fewer distinctions between programs than traditional trace models, which may help to mitigate the combinatorial explosion triggered by interleaving. For a sequential or synchronization-free program the footstep trace semantics is equivalent to a non-deterministic state transformation, so the new model supports “sequential” reasoning about synchronization-free code fragments. We show that footstep trace semantics is strictly more abstract than action trace semantics and suitable for compositional reasoning about race-freedom and partial correctness. The new model {{can be used to}} establish the soundness of concurrent separation logic. We include some example programs to facilitate comparison with earlier models, and we discuss briefly the relationship with a recent model by John Reynolds in which actions have discernible starts and finishes...|$|E
40|$|Wrappers are a {{mechanism}} in <b>denotational</b> <b>semantic</b> that <b>model</b> class inheritance of object oriented programming. In this paper {{we try to}} give evidence that {{the unusual step of}} reintroducing a semantic mechanism into the language being described can be sensible. With wrappers now being explicit, a disciplined variant of multiple inheritance can be formulated as single inheritance and a better reusability of code is gained...|$|R
40|$|Abstract. CSP was {{originally}} introduced as a parallel programming {{language in which}} sequential imperative processes execute concurrently and communicate by synchronized input and output. The influence of CSP and the closely related process algebra TCSP is widespread. Over the years CSP has been equipped {{with a series of}} <b>denotational</b> <b>semantic</b> <b>models,</b> involving notions such as communication traces, failure sets, and divergence traces, suitable for compositional reasoning about safety properties and deadlock analysis. We revisit these notions (and review some of the underlying philosophy) with the benefit of hindsight, and we introduce a semantic framework based on action traces that permits a unified account of shared memory parallelism, asynchronous communication, and synchronous communication. The framework also allows a relatively straightforward account of (a weak form of) fairness, so that we obtain models suitable for compositional reasoning about liveness properties as well as about safety properties and deadlock. We show how to incorporate race detection into this semantic framework, leading to models more independent of hardware assumptions about the granularity of atomic actions. ...|$|R
40|$|AbstractOperational and <b>denotational</b> <b>semantic</b> <b>models</b> are {{designed}} for languages with process creation, and {{the relationships between the}} two semantics are investigated. The presentation is organized in four sections dealing with a uniform and static, a uniform and dynamic, a nonuniform and static, and a nonuniform and dynamic language respectively. Here uniform/nonuniform refers to a language with uninterpreted/interpreted elementary actions, and static/dynamic to the distinction between languages with a fixed/growing number of parallel processes. The contrast between uniform and nonuniform is reflected in the use of linear time versus branching time models, the latter employing a version of Plotkin's resumptions. The operational semantics make use of Hennessy's and Plotkin's transition systems. All models are built on metric structures, and involve continuations in an essential way. The languages studied are abstractions of the parallel object-oriented language POOL for which we have designed seperate operational and denotational semantics in earlier work. The paper provides a full analysis {{of the relationship between the}} two semantics for these abstractions. Technically, a key role is played by a new operator which is able to decide dynamically whether it should act as sequential or parallel composition...|$|R
40|$|A {{refinement}} {{method for}} embedded software development: A based events and data approach Abstract — In this work, a new method of refinement of embedded systems specifications {{based on the}} graphical specification language UML-RT and the formal specification CSP-OZ is introduced. The UML-RT is used to model real time distributed architecture systems and these are mapped onto formal specifications using CSP-OZ. The CSP-OZ formal specification language {{is a combination of}} the state-based object oriented language Object-Z and the CSP language that describes behavioral models of concurrent processes (event-based semantics). Usually, the adopted CSP-OZ semantics is based on the <b>denotational</b> <b>semantic</b> <b>model</b> of failures-divergences for both CSP and Object-Z parts. When using failures-divergences semantics it is possible to use the commercial tool FDR 2. However, as the CSPM, the language of FDR 2, does not have mechanisms to represent object-oriented data, it is necessary to translate specifications into standard data and function calls, a process that can be highly error prone. The rationale of the proposed refinement method is twofold, the use of bisimulation to refine the behavioral part and the specification matching algorithm to refine the state-based part. The development of the proposed refinement method is rigorous, including a formal definition for a UML-RT metamodel...|$|E
40|$|Formal {{verification}} of a compiler is a long-standing problem {{in computer science}} and, although recent years have seen substantial achievements in the area, most of the proposed solutions do not scale very well with the complexity of modern software development environments. In this thesis, I present a formal semantic model of the popular C programming language described in the ANSI/ISO/IEC Standard 9899 : 1990, {{in the form of}} a mapping of C programs to computable functions expressed in a suitable variant of lambda calculus. The specification is formulated in a highly readable functional style and is accompanied by a complete Haskell implementation of the compiler, covering all aspects of the translation from a parse tree of a C program down to the actual sequence of executable machine instructions, resolving issues of separate compilation, allowing for optimising program transformations and providing rigorous guarantees of the implementation's conformance to a formal definition of the language. In order to achieve these goals, I revisit the challenge of compiler verification from its very philosophical foundations. Beginning with the basic epistemic notions of knowledge, correctness and proof, I show that a fully rigorous solution of the problem requires a constructive formulation of the correctness criteria in terms of the translation process itself, in contrast with the more popular extensional approaches to compiler verification, in which correctness is generally defined as commutativity of the system with respect to a formal semantics of the source and target languages, effectively formalising various aspects of the compiler independently of each other and separately from its eventual implementation. I argue that a satisfactory judgement of correctness should always constitute a direct formal description of the job performed by the software being judged, instead of an axiomatic definition of some abstract property such as commutativity of the translation system, since the later approach fails to establish a crucial causal connection between a judgement of correctness and a knowledge of it. The primary contribution of this thesis is the new notion of linear correctness, which strives to provide a constructive formulation of a compiler's validity criteria by deriving its judgement directly from a formal description of the language translation process itself. The approach relies crucially on a <b>denotational</b> <b>semantic</b> <b>model</b> of the source and target languages, in which the domains of program meanings are unified with the actual intermediate program representation of the underlying compiler implementation. By defining the concepts of a programming language, compiler and compiler correctness in category theoretic terms, I show that every linearly correct compiler is also valid in the more traditional sense of the word. Further, by presenting a complete verified translation of the standard C programming language within this framework, I demonstrate that linear correctness is a highly effective approach to the problem of compiler verification and that it scales particularly well with the complexity of modern software development environments...|$|E
40|$|This {{dissertation}} {{addresses the}} formal semantics of Handel-C: a C-based language with true parallelism and priority-based channel communication, {{which can be}} compiled to hardware. It describes an implementation in the Haskell functional programming language of a denotational semantics for Handel-C, as described in (Butterfield & Woodcock, 2005 a). In particular, the Typed Assertion Trace trace model is used, and difficulties in creating a concrete implementation of the abstract model are discussed. An existing toolset supporting a operational semantics for the language is renovated, in part to support arbitrary semantic ?modes,? and to add support for the denotational semantics using this feature. A comparison module is written to compare the traces of two programs in any semantic mode supported by the simulator. Random testing support is implemented via the QuickCheck testing tool for Haskell. This tool is incorporated into the comparison module, allowing testing of various properties of Handel-C, {{as well as its}} traditional use of testing the Haskell implementation for errors. This support is used to search for discrepancies between the operational and <b>denotational</b> <b>semantic</b> <b>models.</b> Finally, several proposed ?Laws of Handel-C? are implemented and tested using the QuickCheck module. Some errors in the specification of the laws are discovered and corrected. Once the specifications are corrected, all of the proposed laws pass, paving the way for future formal verification...|$|R
40|$|AbstractFair {{interleaving}} plays {{a fundamental}} rôle in <b>denotational</b> <b>semantic</b> <b>models</b> for shared-memory parallel programs, beginning with Park's trace semantics, {{based on a}} fairmerge relation designed so that (α,β,γ) ∈fairmerge {{if and only if}} γ can be obtained by interleaving α and β. Park's formulation of fairmerge used nested greatest and least fixed points of monotone functions over traces, but he remarked that fixed point induction principles seem unsuitable for proving natural algebraic properties such as associativity. Such properties are needed to validate intuitive laws of program equivalence and to support hierarchical analysis of programs. Recent models and logics for shared-memory programs with mutable state and pointers build on and extend Park's foundations, with emphasis on resources and logical rules that embody separation principles. For example, concurrent separation logic is based on a race-detecting, resource-sensitive variant of fairmerge. For the kinds of interleaving employed in these models, and other more sophisticated variants of fairmerge, the algebraic difficulties are exacerbated. Rather than search for ad hoc techniques, we develop here a general framework for defining k-ary fairmerge operators, parameterized first by a choice of a resource model and then refined by a choice of a conflict or interference relation. Our formulation avoids nested fixed points, and supports inductive reasoning based on the length of finite prefixes of a trace. We prove a generalized associativity property, and obtain associativity proofs for prior models as a by-product...|$|R
40|$|AbstractOur {{focus is}} on the {{semantics}} of programming and specification languages. Over the years, different approaches to give semantics to these languages have been put forward. We restrict ourselves to the operational and the denotational approach, two main streams in the field of semantics. Two notions which {{play an important role in}} this paper are (non) determinism and (non) termination. Nondeterminism arises naturally in concurrent languages and it is a key concept in specification languages. Nontermination is usually caused by recursive constructs which are crucial in programming. The operational models are based on labelled transition systems. The definition of these systems is guided by the structure of the language. Metric spaces are an essential ingredient of our denotational models. We exploit the metric structure to model recursive constructs and to define operators on infinite entities. Furthermore, we also employ the metric structure to relate operational and denotational models for a given language. On the basis of four toy languages, we develop some general theory for defining operational and <b>denotational</b> <b>semantic</b> <b>models</b> and for relating them. This theory is applicable to a wide variety of languages. We start with a very simple deterministic and terminating imperative programming language. By adding the recursive while statement, we obtain a deterministic and nonterminating language. Next, we augment the language with the parallel composition resulting in a bounded nondeterministic and nonterminating language. Finally, we add some timed constructs. We obtain an unbounded nondeterministic and nonterminating specification language...|$|R
40|$|Operational (O) and <b>denotational</b> (D) <b>semantic</b> <b>models</b> are {{designed}} for a language incorporating {{a version of the}} UNIX fork and pipe commands. Taking a simple while language as starting point, a number of programming constructs are added which achieve that a program can generate a dynamically evolving linear array of processes connected by channels. Over these channels sequences of values (`streams') are transmitted. Both O and D are defined as (unique) fixed point of a contractive higher order operator. This allows a smooth proof that O and D are equivalent. Additional features are the use of hiatons, and of the closely related syntactic resumptions and semantic continuations. AMS Subject Classification (1991) : 68 Q 55 CR Subject Classification (1991) : D. 3. 1, F. 3. 2 Keywords & Phrases: operational semantics, denotational semantics, complete metric space, fork statement, hiaton, resumption, continuation Note: The work of F. van Breugel was partially supported by the Netherlands Nationale Faciliteit Informatica programme, project Research and Education in Concurrent Systems (REX). This paper will appear in Proceedings of the Eighteenth International Symposium on Mathematical Foundations of Computer Science, Gdansk, August 30 - September 3, 1993...|$|R
40|$|The Lambek Calculus was {{introduced}} by Lambek [2] {{in order to obtain}} effective rules to allow forming sentences and distinguishing sentences from nonsentences. Two primitives types are defined in this calculus and from these types is allowed to construct compound types to the words of the sentence. We construct a -calculus for the Lambek Calculus, Curry-Howard isomorphic to the sequent calculus. Some proof-theoretical results and a <b>denotational</b> <b>semantic</b> for this calculus, showing its adequacy with regard to the calculus, are shown. A phase semantics, based on Girard's phase semantics for the Linear Logic, searching to implement a verificationistic approach for meaning in construtivism, is also shown for this calculus. Keywords: Lambek Sintactic Calculus, -Calculus, <b>Denotational</b> <b>Semantic,</b> Phase Semantic, Linear Logic. Resumo: O Lambek Calculus foi introduzido por Lambek [2] com o objetivo de obter regras efetivas que permitam formar sentencas e distinguir sentencas de n~ao-sentencas [...] ...|$|R
40|$|The {{increased}} {{acceptance of}} Prolog has motivated widespread {{interest in the}} semanticsbased dataflow analysis of logic programs {{and a number of}} different approaches have been suggested. However, the relationships between these approaches are not clear. The present paper provides a unifying introduction to the approaches by giving novel <b>denotational</b> <b>semantic</b> definitions which capture their essence. In addition, the wide range of analysis tools supported by semantics-based dataflow analysis are discussed. ...|$|R
40|$|We {{present a}} generic <b>denotational</b> <b>semantic</b> {{framework}} for protocols for dialogs between rational and autonomous agents over action {{which allows for}} retraction and revocation of proposals for action. The semantic framework views participants in a deliberation dialog as jointly and incrementally manipulating the contents of shared spaces of action-intention tokens. The framework extends prior work by decoupling the identity of an agent who first articulates a proposal for action from the identity of any agent then empowered to retract or revoke the proposal, thereby permittin...|$|R
40|$|Abstract: Denotational {{semantics}} is {{a powerful}} technique to formally define programming languages. However, language constructs are not always orthogonal, so many semantic equations in a definition {{may have to be}} aware of unrelated constructs semantics. Current approaches for modularity in this formalism do not address this problem, providing, for this reason, tangled semantic definitions. This paper proposes an incremental approach for <b>denotational</b> <b>semantic</b> specifications, in which each step can either add new features or adapt existing equations, by means of a formal language based on function transformation and aspect weaving...|$|R
50|$|During the 1990s the {{application}} of <b>semantic</b> <b>modelling</b> techniques resulted in the <b>semantic</b> data <b>models</b> of the second kind. An example of such is the <b>semantic</b> data <b>model</b> that is standardised as ISO 15926-2 (2002), which is further developed into the <b>semantic</b> <b>modelling</b> language Gellish (2005). The definition of the Gellish language is documented {{in the form of}} a <b>semantic</b> data <b>model.</b> Gellish itself is a <b>semantic</b> <b>modelling</b> language, that can be used to create other <b>semantic</b> <b>models.</b> Those <b>semantic</b> <b>models</b> can be stored in Gellish Databases, being semantic databases.|$|R
40|$|With the {{development}} of video technology and appearance of new video-related applications, the amount of video data has increased dramatically which demands support in <b>semantic</b> <b>models</b> to facilitate information representation and query. The video <b>semantic</b> <b>models</b> surveyed in this paper are classified into two categories: annotation-based <b>models</b> and rich <b>semantic</b> <b>models.</b> However, currently there are no criteria for a good <b>semantic</b> <b>model</b> so people lack the rules for evaluating an existing model and the guidelines for designing a new model when necessary. To address this issue, this paper proposes twenty one properties as the criteria for video <b>semantic</b> <b>models,</b> and evaluates eleven existing rich <b>semantic</b> <b>models</b> according to these properties. The results show that these models mostly concentrate on aspects of basic expressive power and query ability. But for some advanced features such as user-defined constraints, assistance for acquisition of semantic information, query evolution etc., there are rooms for further enhancement. The paper concludes by analyzing the evaluation results and indicating research directions for future video <b>semantic</b> <b>models.</b> Key words: Video <b>semantic</b> <b>model,</b> evaluatio...|$|R
40|$|Threat {{detection}} {{in computer}} vision {{can be achieved}} by extraction of behavioural cues. To achieve recognition of such cues, we propose to work with <b>Semantic</b> <b>Models</b> of behaviours. <b>Semantic</b> <b>Models</b> correspond to the translation of Low-Level information (tracking information) into High-Level <b>semantic</b> description. The <b>model</b> is then similar to a naturally spoken description of the event. We have built <b>semantic</b> <b>models</b> for the behaviours and threats addressed in the PETS 2016 IPATCH dataset. <b>Semantic</b> <b>models</b> can trigger a threat alarm by themselves or give situation awareness. We describe in this paper how <b>semantic</b> <b>models</b> are built from Low-Level trajectory features and how they are recognised. The current results are promising...|$|R
5000|$|Denotational semantics, whereby each {{phrase in}} the {{language}} is interpreted as a denotation, i.e. a conceptual meaning that {{can be thought of}} abstractly. Such denotations are often mathematical objects inhabiting a mathematical space, {{but it is not a}} requirement that they should be so. As a practical necessity, denotations are described using some form of mathematical notation, which can in turn be formalized as a denotational metalanguage. For example, denotational semantics of functional languages often translate the language into domain theory. <b>Denotational</b> <b>semantic</b> descriptions can also serve as compositional translations from a programming language into the denotational metalanguage and used as a basis for designing compilers.|$|R
40|$|Information {{sources such}} as {{relational}} databases, spreadsheets, XML, JSON, and Web APIs contain {{a tremendous amount of}} structured data that can be leveraged to build and augment knowledge graphs. However, they rarely provide a <b>semantic</b> <b>model</b> to describe their contents. <b>Semantic</b> <b>models</b> of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data. Such models are the key ingredients to automatically publish the data into knowledge graphs. Manually modeling the semantics of data sources requires significant effort and expertise, and although desirable, building these models automatically is a challenging problem. Most of the related work focuses on semantic annotation of the data fields (source attributes). However, constructing a <b>semantic</b> <b>model</b> that explicitly describes the relationships between the attributes in addition to their semantic types is critical. We present a novel approach that exploits the knowledge from a domain ontology and the <b>semantic</b> <b>models</b> of previously modeled sources to automatically learn a rich <b>semantic</b> <b>model</b> for a new source. This model represents the semantics of the new source in terms of the concepts and relationships defined by the domain ontology. Given some sample data from the new source, we leverage the knowledge in the domain ontology and the known <b>semantic</b> <b>models</b> to construct a weighted graph that represents the space of plausible <b>semantic</b> <b>models</b> for the new source. Then, we compute the top k candidate <b>semantic</b> <b>models</b> and suggest to the user a ranked list of the <b>semantic</b> <b>models</b> for the new source. The approach takes into account user corrections to learn more accurate <b>semantic</b> <b>models</b> on future data sources. Our evaluation shows that our method generates expressive <b>semantic</b> <b>models</b> for data sources and services with minimal user input. [...] . Comment: Web Semantics: Science, Services and Agents on the World Wide Web, 201...|$|R
40|$|AbstractThe {{question}} of whether a <b>semantic</b> <b>model</b> is suitable {{for the construction of a}} modular proof system is studied in detail. The notion of one <b>semantic</b> <b>model</b> being a (full) abstraction of another <b>semantic</b> <b>model</b> with respect to a given class of properties is introduced, and is used in analyzing different <b>semantic</b> <b>models</b> for communicating processes. A trace model for communicating processes is described and shown to be suitable for the construction of a modular proof system in which partial correctness assertions about communicating processes can be expressed...|$|R
40|$|AbstractNondeterminism is {{introduced}} into an ordinary iterative programming language by treating procedure calls as nondeterministic assignment statements. The effect of such assignment statements {{is assumed to}} be determined solely by the entry-exit specifications of the corresponding procedures. The nondeterminism which this approach yields is not necessarily bounded. The paper discusses the problem of defining a <b>denotational</b> <b>semantic</b> for programming languages with this kind of, possibly unbounded, nondeterminism. As an additional constraint, the semantics is required to be continuous, {{in the sense that the}} underlying semantic algebra is continuous. It is shown how such a continuous semantics for unbounded nondeterminism can be derived from a simple operational semantics based on program execution trees...|$|R
40|$|TAMPR is a fully {{automatic}} transformation {{system based on}} syntactic rewrites. Our approach in a correctness proof is to map the transformation into an axiomatized mathematical domain where formal (and automated) reasoning can be performed. This mapping is accomplished via an extended <b>denotational</b> <b>semantic</b> paradigm. In this approach, the abstract notion of a program state is distributed between an environment function and a store function. Such a distribution introduces properties that go beyond the abstract state that is being modeled. The reasoning framework needs {{to be aware of}} these properties in order to successfully complete a correctness proof. This paper discusses some of our experiences in proving the correctness of TAMPR transformations...|$|R
40|$|In web search, latent <b>semantic</b> <b>models</b> {{have been}} {{proposed}} to bridge the lexical gap between queries and documents that {{is due to the}} fact that searchers and content creators often use different vocabularies and language styles to express the same concept. Modern search engines simply use the outputs of latent <b>semantic</b> <b>models</b> as features for a so-called global ranker. We argue that this is not optimal, because a single value output by a latent <b>semantic</b> <b>model</b> may be insufficient to describe all aspects of the model's prediction, and thus some information captured by the model is not used effectively by the search engine. To increase the effectiveness of latent <b>semantic</b> <b>models</b> in web search, we propose to create metafeatures-feature vectors that describe the structure of the model's prediction for a given query-document pair and pass them to the global ranker along with the models? scores. We provide simple guidelines to represent the latent <b>semantic</b> <b>model's</b> prediction with more than a single number, and illustrate these guidelines using several latent <b>semantic</b> <b>models.</b> We test the impact of the proposed metafeatures on a web document ranking task using four latent <b>semantic</b> <b>models.</b> Our experiments show that (1) through the use of metafeatures, the performance of each individual latent <b>semantic</b> <b>model</b> can be improved by 10. 2 % and 4. 2 % in NDCG scores at truncation levels 1 and 10; and (2) through the use of metafeatures, the performance of a combination of latent <b>semantic</b> <b>models</b> can be improved by 7. 6 % and 3. 8 % in NDCG scores at truncation levels 1 and 10, respectively...|$|R
40|$|Most common {{database}} management systems represent {{information in a}} simple record-based format. <b>Semantic</b> <b>modeling</b> provides richer data structuring capabilities for database applications. In particular, {{research in this area}} has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, <b>semantic</b> <b>modeling</b> complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages. This paper presents an in-depth discussion of <b>semantic</b> data <b>modeling.</b> It reviews the philosophical motivations of <b>semantic</b> <b>models,</b> including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of <b>semantic</b> <b>models,</b> which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent <b>semantic</b> <b>models</b> in the literature is presented. Further, since a broad area of research has developed around <b>semantic</b> <b>modeling,</b> a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies...|$|R
30|$|Define shared {{semantics}} for {{guidelines and}} interactions: {{we need to}} define a <b>semantic</b> <b>model</b> for interactions and guidelines actions. Such <b>semantic</b> <b>model</b> must be shared and “understood” both by information systems and by human users (Gruber 2009; Rubin et al. 2006).|$|R
40|$|Abstract. In this paper, {{we present}} an {{approach}} for the retrieval of natural scenes {{based on a}} <b>semantic</b> <b>modeling</b> step. <b>Semantic</b> <b>modeling</b> stands for the classification of local image regions into semantic classes such as grass, rocks or foliage and the subsequent summary of this information in so-called conceptoccurrence vectors. Using this semantic representation, images from the scene categories coasts,rivers/lakes,forests,plains,mountains and sky/clouds are retrieved. We compare two implementations of the method quantitatively on a visually diverse database of natural scenes. In addition, the <b>semantic</b> <b>modeling</b> approach is compared to retrieval based on low-level features computed directly on the image. The experiments show that <b>semantic</b> <b>modeling</b> leads in fact to better retrieval performance. ...|$|R
40|$|The goal of our {{research}} {{is to develop a}} grammar induction system that can assign descriptive sentences to ontology models. The initial task is to learn the rules of association between free text sentences and ontology representations. Within the frames of our project an important stage is to find an appropriate formalism for the <b>semantic</b> <b>model</b> that represents the knowledge-base to be converted into sentences. After analyzing the existing <b>semantic</b> <b>models,</b> we concluded that all of them have some shortcomings from the aspects of our requirements. Therefore we proposed a novel, concept-based <b>semantic</b> <b>model</b> the distinguishing features of which are 1) fine distinction between different categories of concept and relation types; 2) predicate-centered schema language; and 3) generality and reusability of model fragments. In this paper we examine the applicability of the existing <b>semantic</b> <b>models</b> to our specific task, and give an objective assessment of them on a logical basis in comparison with our <b>semantic</b> <b>model...</b>|$|R
40|$|Distributional <b>semantic</b> <b>models</b> (DSMs) are <b>semantic</b> <b>models</b> {{which are}} based on the {{statistical}} analysis of co-occurrences of words in large corpora. DSMs {{can be used in a}} wide spectrum of semantic applications including semantic search, question answering, paraphrase detection, word sense disambiguation...|$|R
