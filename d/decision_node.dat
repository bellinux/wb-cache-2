117|434|Public
25|$|It {{may be that}} {{a player}} has an {{infinite}} number of possible actions to choose from at a particular <b>decision</b> <b>node.</b> The device used to represent this is an arc joining two edges protruding from the <b>decision</b> <b>node</b> in question. If the action space is a continuum between two numbers, the lower and upper delimiting numbers are placed at the bottom and top of the arc respectively, usually with a variable that is used to express the payoffs. The infinite number of decision nodes that could result are represented by a single node placed in the centre of the arc. A similar device is used to represent action spaces that, whilst not infinite, are large enough to prove impractical to represent with an edge for each action.|$|E
25|$|The game on {{the right}} has two players: 1 and 2. The numbers by every non-terminal node {{indicate}} to which player that <b>decision</b> <b>node</b> belongs. The numbers by every terminal node represent the payoffs to the players (e.g. 2,1 represents a payoff of 2 to player 1 and a payoff of 1 to player 2). The labels by every edge of the graph are {{the name of the}} action that edge represents.|$|E
50|$|Create a <b>decision</b> <b>node</b> that splits on a_best.|$|E
50|$|An {{alternating}} {{decision tree}} consists of <b>decision</b> <b>nodes</b> and prediction <b>nodes.</b> <b>Decision</b> <b>nodes</b> specify a predicate condition. Prediction nodes contain a single number. ADTrees always have prediction nodes as both root and leaves. An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true and summing any prediction nodes that are traversed. This {{is different from}} binary classification trees such as CART (Classification and regression tree) or C4.5 in which an instance follows only one path through the tree.|$|R
2500|$|An ADTree {{consists}} of an alternation of <b>decision</b> <b>nodes,</b> which specify a predicate condition, and prediction nodes, which contain a single number. [...] An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true, and summing any prediction nodes that are traversed.|$|R
2500|$|An {{alternating}} {{decision tree}} consists of <b>decision</b> <b>nodes</b> and prediction <b>nodes.</b> [...] <b>Decision</b> <b>nodes</b> specify a predicate condition. [...] Prediction nodes contain a single number. [...] ADTrees always have prediction nodes as both root and leaves. [...] An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true and summing any prediction nodes that are traversed. [...] This {{is different from}} binary classification trees such as CART (Classification and regression tree) or C4.5 in which an instance follows only one path through the tree.|$|R
50|$|<b>Decision</b> <b>node</b> (corresponding to each {{decision}} to be made) is drawn as a rectangle.|$|E
50|$|Instance of previously-unseen class encountered. Again, C4.5 {{creates a}} <b>decision</b> <b>node</b> higher {{up the tree}} using the {{expected}} value.|$|E
50|$|There is 1 <b>decision</b> <b>node</b> (Vacation Activity), 2 {{uncertainty}} nodes (Weather Condition, Weather Forecast), and 1 value node (Satisfaction).|$|E
40|$|An {{information}} {{completion of}} an extensive game is obtained by extending the information partition of every player {{from the set}} of her <b>decision</b> <b>nodes</b> to the set of all nodes. The extended partition satisfies Memory of Past Knowledge (MPK) if at any node a player remembers what she knew at earlier no des. It is shown that MPK c an b e satisfi ed in a game i f an d only if the game is von Neumann (vN) and satisfies memory at <b>decision</b> <b>nodes</b> (the restriction of MPK to a playerâ€™s own <b>decision</b> <b>nodes).</b> A game is vN if any two <b>decision</b> <b>nodes</b> that {{belong to the same}} information set of a player have the same number of predecessors. By providing an axiom for MPK we also obtain a syntactic characterization of the said class of vN games. ...|$|R
5000|$|More specifically, in the {{extensive}} form, an information set {{is a set}} of <b>decision</b> <b>nodes</b> such that: ...|$|R
40|$|This paper {{presents}} a new decision tree learning algorithm called CSNL that induces Cost-Sensitive Non-Linear decision trees. The algorithm {{is based on}} the hypothesis that non-linear <b>decision</b> <b>nodes</b> provide a better basis than axis-parallel <b>decision</b> <b>nodes</b> and utilizes discriminant analysis to construct non-linear decision trees that take account of costs of misclassification. The performance of the algorithm is evaluated by applying it to seventeen data sets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the data sets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using non-linear <b>decision</b> <b>nodes...</b>|$|R
50|$|Informational arcs (ending in <b>decision</b> <b>node)</b> {{indicate}} that the decision at their heads is made with the outcome of all the nodes at their tails known beforehand.|$|E
50|$|None of the {{features}} provide any information gain. In this case, C4.5 creates a <b>decision</b> <b>node</b> higher up the tree using the expected value of the class.|$|E
50|$|It {{may be that}} {{a player}} has an {{infinite}} number of possible actions to choose from at a particular <b>decision</b> <b>node.</b> The device used to represent this is an arc joining two edges protruding from the <b>decision</b> <b>node</b> in question. If the action space is a continuum between two numbers, the lower and upper delimiting numbers are placed at the bottom and top of the arc respectively, usually with a variable that is used to express the payoffs. The infinite number of decision nodes that could result are represented by a single node placed in the centre of the arc. A similar device is used to represent action spaces that, whilst not infinite, are large enough to prove impractical to represent with an edge for each action.|$|E
5000|$|<b>Decision</b> <b>nodes</b> and {{incoming}} information arcs collectively {{state the}} alternatives (what {{can be done}} when the outcome of certain decisions and/or uncertainties are known beforehand) ...|$|R
40|$|This paper {{presents}} a new decision tree learning algorithm that takes account of costs of misclassification. The algorithm {{is based on}} the hypothesis that non-linear <b>decision</b> <b>nodes</b> provide a better basis for cost-sensitive induction than axis-parallel <b>decision</b> <b>nodes</b> and utilizes discriminant analysis to construct non-linear cost-sensitive decision trees. The performance of the algorithm is evaluated by applying it to seven data sets and the results compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost. MetaCost is applied both with a base learner that uses axis-parallel spits and a base learner that uses non-linear splits, thereby enabling an evaluation of the value of non-linear <b>decision</b> <b>nodes.</b> The results show that the new algorithm displays a better profile than ICET as the ratio of costs of misclassification departs from unity and that it provides a better base learner for MetaCost than an axis-parallel learner. ...|$|R
40|$|Information {{systems that}} provide {{decision}} support in an efficient and timely manner can prove beneficial for emergency responseoperations. Inthispaperweproposetheuseofasystem that provides movement decision support to evacuees by directing {{them through the}} shortest or less hazardous routes to the exit and evaluate it with a specialised software platform that we have developed for simulation of disasters in buildings. The system operates in a distributed manner, and computesthebestevacuationroutesinreal-timewhileahazard is spreading inside the building. It is composed {{of a network of}} <b>decision</b> <b>nodes</b> and sensor nodes, positioned in specific locations inside the building. The recommendations of the <b>decision</b> <b>nodes</b> are computed in a distributed manner, at each of the <b>decision</b> <b>nodes,</b> which then communicate them to evacuees or rescue personnel located in their vicinity. We use a multi-agent simulation platform for Building Evacuationthatwedeveloped,inordertoevaluateourproposedsystem in various emergency scenarios. Our simulation results show that the overall outcome of the evacuation procedure is improved when the decision supportsystemisinoperation...|$|R
50|$|For dynamic (extensive form) games, McKelvey and Palfrey defined agent quantal {{response}} equilibrium (AQRE). AQRE is somewhat analogous to subgame perfection. In an AQRE, each player plays with some error as in QRE. At a given <b>decision</b> <b>node,</b> the player determines the expected payoff of each action by treating their future self {{as an independent}} player with a known probability distribution over actions.|$|E
50|$|The game on {{the right}} has two players: 1 and 2. The numbers by every non-terminal node {{indicate}} to which player that <b>decision</b> <b>node</b> belongs. The numbers by every terminal node represent the payoffs to the players (e.g. 2,1 represents a payoff of 2 to player 1 and a payoff of 1 to player 2). The labels by every edge of the graph are {{the name of the}} action that edge represents.|$|E
5000|$|A Boolean {{function}} can {{be represented}} as a rooted, directed, acyclic graph, which consists of several decision nodes and terminal nodes. There {{are two types of}} terminal nodes called 0-terminal and 1-terminal. Each <b>decision</b> <b>node</b> [...] is labeled by Boolean variable [...] and has two child nodes called low child and high child. The edge from node [...] to a low (or high) child represents an assignment of [...] to 0 (resp. 1).Such a BDD is called 'ordered' if different variables appear in the same order on all paths from the root. A BDD is said to be 'reduced' if the following two rules have been applied to its graph: ...|$|E
40|$|This article {{presents}} a new decision tree learning algorithm called CSNL that induces Cost-Sensitive Non-Linear decision trees. The algorithm {{is based on}} the hypothesis that nonlinear <b>decision</b> <b>nodes</b> provide a better basis than axis-parallel <b>decision</b> <b>nodes</b> and utilizes discriminant analysis to construct nonlinear decision trees that take account of costs of misclassification. The performance of the algorithm is evaluated by applying it to seventeen datasets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the datasets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using nonlinear <b>decision</b> <b>nodes.</b> The performance of the algorithm is evaluated by applying it to seventeen data sets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the data sets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using non-linear <b>decision</b> <b>nodes...</b>|$|R
40|$|Abstractâ€”The {{evacuation}} {{of a building}} is a challenging problem, since the evacuees {{most of the times}} do not know or do not follow the optimal evacuation route. Especially during an ongoing hazard present in the building, finding the best evacuation route becomes harder as the conditions along the paths change {{in the course of the}} evacuation procedure. In this paper we propose a distributed system that will compute the best evacuation routes in real-time, while a hazard is spreading inside the building. The system is composed of a network of <b>decision</b> <b>nodes</b> and sensor nodes, positioned in specific locations inside the building. The recommendations of the <b>decision</b> <b>nodes</b> are computed in a distributed manner, at each of the <b>decision</b> <b>nodes,</b> which then communicate them to evacuees or rescue personnel located in their vicinity. We evaluate our proposed system in various emergency scenarios, using a multiagent simulation platform for Building Evacuation. Our results indicate that the presence of the system improves the outcome of the evacuation with respect to the evacuation time and the injury level of the evacuees. I...|$|R
5000|$|<b>Decision</b> Support <b>node.</b> This node {{allows the}} Program to invoke {{business}} rules {{that run on}} a component of IBM Decision Server that is provided with the Program. Use of this component is supported only via <b>Decision</b> Service <b>nodes.</b> The Program license provides entitlement for the Licensee {{to make use of}} <b>Decision</b> Service <b>nodes</b> for development and functional test uses. Refer to the IBM Integration Bus License Information text for details about the program-unique terms.|$|R
50|$|A {{decision}} tree {{is a simple}} representation for classifying examples. For this section, assume {{that all of the}} input features have finite discrete domains, and there is a single target feature called the classification. Each element of the domain of the classification is called a class.A {{decision tree}} or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target or output feature or the arc leads to a subordinate <b>decision</b> <b>node</b> on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes.|$|E
50|$|Sometimes subgame {{perfection}} {{does not}} impose {{a large enough}} restriction on unreasonable outcomes. For example, since subgames cannot cut through information sets, a game of imperfect information may have only one subgame - itself - and hence subgame perfection cannot be used to eliminate any Nash equilibria. A perfect Bayesian equilibrium (PBE) is a specification of playersâ€™ strategies and beliefs about which node in the information set has been reached {{by the play of}} the game. A belief about a <b>decision</b> <b>node</b> is the probability that a particular player thinks that node is or will be in play (on the equilibrium path). In particular, the intuition of PBE is that it specifies player strategies that are rational given the player beliefs it specifies and the beliefs it specifies are consistent with the strategies it specifies.|$|E
50|$|If the entrant enters, {{the best}} {{response}} of the incumbent is to accommodate. If the incumbent accommodates, the best {{response of the}} entrant is to enter (and gain profit). Hence the strategy profile in which the incumbent accommodates if the entrant enters and the entrant enters if the incumbent accommodates is a Nash equilibrium. However, if the incumbent {{is going to play}} fight, the best response of the entrant is to not enter. If the entrant does not enter, it does not matter what the incumbent chooses to do (since there is no other firm to do it to - note that if the entrant does not enter, fight and accommodate yield the same payoffs to both players; the incumbent will not lower its prices if the entrant does not enter). Hence fight can be considered as a best response of the incumbent if the entrant does not enter. Hence the strategy profile in which the incumbent fights if the entrant does not enter and the entrant does not enter if the incumbent fights is a Nash equilibrium. Since the game is dynamic, any claim by the incumbent that it will fight is an incredible threat because by the time the <b>decision</b> <b>node</b> is reached where it can decide to fight (i.e. the entrant has entered), it would be irrational to do so. Therefore this Nash equilibrium can be eliminated by backward induction.|$|E
40|$|Abstract Finding {{the best}} {{evacuation}} path during an emergency situation inside a {{building is a}} challenging task, due to the dynamically changing conditions and the strict time constraints. Information systems can benefit the evacuation process by providing directions to the evacuees in an efficient and timely manner. In this paper we propose {{the use of such}} a system and evaluate it with a specialised software platform that we have developed for simulation of disasters in buildings. The system provides movement decision support to evacuees by directing them through the less hazardous routes to an exit. It is composed of a network of <b>Decision</b> <b>Nodes</b> and sensor nodes, positioned in specific locations inside the building. The recommendations of the <b>Decision</b> <b>Nodes</b> are computed in a distributed manner, at each of the <b>Decision</b> <b>Nodes,</b> which then communicate them to evacuees or rescue personnel located in their vicinity. The system computes the best evacuation routes in real-time, while a hazard is spreading inside the building. It also takes into account the spatial characteristics of hazard propagation inside a confined space. Our simulation results show that the outcome of the evacuation procedure is improved by the use of the decision support system. ...|$|R
40|$|Abstract. In {{the context}} of {{classification}} problems, algorithms that generate multivariate trees are able to explore multiple representation languages by using decision tests based {{on a combination of}} attributes. In the regression setting, model trees algorithms explore multiple representation languages but using linear models at leaf nodes. In this work we study the effects of using combinations of attributes at <b>decision</b> <b>nodes,</b> leaf nodes, or both nodes and leaves in regression and classification tree learning. In order to study the use of functional nodes at different places and for different types of modeling, we introduce a simple unifying framework for multivariate tree learning. This framework combines a univariate decision tree with a linear function by means of constructive induction. Decision trees derived from the framework are able to use <b>decision</b> <b>nodes</b> with multivariate tests, and leaf nodes that make predictions using linear functions. Multivariate <b>decision</b> <b>nodes</b> are built when growing the tree, while functional leaves are built when pruning the tree. We experimentally evaluate a univariate tree, a multivariate tree using linear combinations at inner and leaf nodes, and two simplified versions restricting linear combinations to inner nodes and leaves. The experimental evaluation shows that all functional trees variants exhibit similar performance, with advantages in different datasets. In this study there is a marginal advantage of the full model. These results lead us to study the role of functional leaves and nodes. We use the bias-variance decomposition of the error, cluster analysis, and learning curves as tools for analysis. We observe that in the datasets under study and for classification and regression, the use of multivariate <b>decision</b> <b>nodes</b> has more impact in the bias component of the error, while the use of multivariate decision leaves has more impact in the variance component...|$|R
2500|$|... is {{a finite}} tree {{with a set}} of nodes , a unique initial node , a set of {{terminal}} nodes [...] (let [...] be a set of <b>decision</b> <b>nodes)</b> and an immediate predecessor function [...] on which {{the rules of the game}} are represented, ...|$|R
40|$|Abstract. Univariate {{decision}} {{trees at}} each <b>decision</b> <b>node</b> consider {{the value of}} only one feature leading to axis-aligned splits. In a linear multivariate decision tree, each <b>decision</b> <b>node</b> divides the input space into two with an arbitrary hyperplane leading to oblique splits. In a nonlinear one, a multilayer perceptron at each node divides the input space arbitrarily, {{at the expense of}} increased complexity. In this paper, we detail and compare using a set of simulations linear and non-linear neural based decision tree methods with the univariate decision tree induction method ID 3 and the linear multivariate decision tree induction method CART. We also propose hybrid trees where the <b>decision</b> <b>node</b> may be linear or nonlinear depending on the outcome of a statistical test on accuracy. ...|$|E
40|$|Univariate {{decision}} {{trees at}} each <b>decision</b> <b>node</b> consider {{the value of}} only one feature leading to axis-aligned splits. In a linear multivariate decision tree, each <b>decision</b> <b>node</b> divides the input space into two with a hyperplane. In a nonlinear multivariate tree, a multilayer perceptron at each node divides the input space arbitrarily, {{at the expense of}} increased complexity and higher risk of overfitting. We propose omnivariate trees where the <b>decision</b> <b>node</b> may be univariate, linear, or nonlinear depending on the outcome of comparative statistical tests on accuracy thus matching automatically the complexity of the node with the subproblem defined by the data reaching that node. Such an architecture frees the designer from choosing the appropriate node type, doing model selection automatically at each node. Our simulation results indicate that such a decision tree induction method generalizes better than trees with the same types of nodes everywhere and induces small trees...|$|E
30|$|The Bayesian Network model {{structure}} is also {{dependent on the}} type of nodes involved in it. Three commonly used BN nodes are the Nature node, Utility node and <b>Decision</b> <b>node</b> (Ticehurst etÂ al. 2007). The nature node describes possible states of a variable and the probability of each state. This type of node could be qualitative or quantitative (discrete or continuous). A utility node is a continuous variable describing the desirability of the consequences of a set of outcomes. The <b>decision</b> <b>node</b> represents a controllable variable providing choice to the decision maker (Robertson 2004).|$|E
40|$|When {{planning}} and designing a policy intervention and evaluation, the policy maker {{will have to}} define a strategy which will define the (conditional independence) structure of the available data. Here, Dawid's extended influence diagrams are augmented by including 'experimental design' <b>decisions</b> <b>nodes</b> within the set of intervention strategies to provide semantics to discuss how a 'design' decision strategy (such as randomisation) might assist the systematic identification of intervention causal effects. By introducing design <b>decision</b> <b>nodes</b> into the framework, the experimental design underlying the data available is made explicit. We show how influence diagrams {{might be used to}} discuss the efficacy of different designs and conditions under which one can identify 'causal' effects of a future policy intervention. The approach of this paper lies primarily within probabilistic decision theory...|$|R
40|$|Abstract. We {{present an}} {{abstract}} domain functor whose elements are binary decision trees. It is parameterized by <b>decision</b> <b>nodes</b> {{which are a}} set of boolean tests appearing in the programs and by a numerical or symbolic abstract domain whose elements are the leaves. We first define the branch condition path abstraction which forms the <b>decision</b> <b>nodes</b> of the binary decision trees. It also provides a new prospective on parti-tioning the trace semantics of {{programs as well as}} separating properties in the leaves. We then discuss our binary decision tree abstract domain functor by giving algorithms for inclusion test, meet and join, trans-fer functions and extrapolation operators. We think the binary decision tree abstract domain functor may provide a flexible way of adjusting the cost/precision ratio in path-dependent static analysis. ...|$|R
40|$|Perceptron Decision Trees (also {{known as}} Linear Machine DTs, etc.) are {{analysed}} {{in order that}} data-dependent Structural Risk Minimization can be applied. Data-dependent analysis is performed which indicates that choosing the maximal margin hyperplanes at the <b>decision</b> <b>nodes</b> will improve the generalization. The analysis uses a novel technique to bound the generalization error {{in terms of the}} margins at individual nodes. Experiments performed on real data sets confirm the validity of the approach. 1 Introduction Neural network researchers have traditionally tackled classification problems by assembling perceptron or sigmoid nodes into feedforward neural networks. In this paper we consider a less common approach where the perceptrons are used as <b>decision</b> <b>nodes</b> in a <b>decision</b> tree structure. The approach has the advantage that more efficient heuristic algorithms exist for these structures, while the advantages of inherent parallelism are if anything greater as all the perceptrons can be eva [...] ...|$|R
