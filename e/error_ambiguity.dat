3|161|Public
5000|$|This is {{probably}} the most complex stage in SSADM. Using the requirements developed in stage 1 and working {{within the framework of the}} selected business option, the analyst must develop a full logical specification of what the new system must do. The specification must be free from <b>error,</b> <b>ambiguity</b> and inconsistency. By logical, we mean that the specification does not say how the system will be implemented but rather describes what the system will do.|$|E
40|$|We {{present a}} {{framework}} for annotating dynamic scenes involving occlusion and other uncertainties. Our system comprises an object tracker, an object classifier and an algorithm for reasoning about spatio-temporal continuity. The principle behind the object tracking and classifier modules is to reduce error by increasing ambiguity (by merging objects in close proximity and presenting multiple hypotheses). The reasoning engine resolves <b>error,</b> <b>ambiguity</b> and occlusion to produce a most likely hypothesis, {{which is consistent with}} global spatio-temporal continuity constraints. The system results in improved annotation over frame-by-frame methods. It has been implemented and applied to the analysis of a team sports video...|$|E
40|$|A {{framework}} for the logical and statistical analysis and annotation of dynamic scenes containing occlusion and other uncertainties is presented. This framework consists of three elements; an object tracker module, an object recognition/classification module and a logical consistency, ambiguity and error reasoning engine. The principle behind the object tracker and object recognition modules is to reduce error by increasing ambiguity (by merging objects in close proximity and presenting multiple hypotheses). The reasoning engine deals with <b>error,</b> <b>ambiguity</b> and occlusion in a unified framework to produce a hypothesis that satisfies fundamental constraints on the spatio-temporal continuity of objects. Our algorithm finds a globally consistent model of an extended video sequence that is maximally supported by a voting function based on the output of a statistical classifier. The system results in an annotation that is significantly more accurate than what would be obtained by frame-by-frame evaluation of the classifier output. The framework has been implemented and applied successfully {{to the analysis of}} team sports with a single camera. Key words: Visua...|$|E
25|$|The HRG is {{periodically}} updated {{to correct}} <b>errors</b> and <b>ambiguities.</b>|$|R
25|$|Oscar Tiegs in {{this work}} {{corrected}} <b>errors</b> and <b>ambiguities</b> {{in the work of}} others, and showed the validity of some traditional interpretations.|$|R
40|$|Incompleteness of {{financial}} markets {{has been widely}} questioned in the literature, but traditional research has been mainly focused {{on the role of}} transaction costs and asymmetric information in determining such incompleteness. This paper, instead, focuses on agents' preferences, showing that the introduction of ambiguity and ambiguity aversion may induce investors to restrict their trading to a simpler set of assets, relative to which {{they are less likely to}} make <b>errors.</b> <b>Ambiguity</b> Variational preferences Idiosyncratic risk Risk sharing Trading...|$|R
5000|$|IEEE 802.15.3b-2005 {{amendment}} {{was released on}} May 5, 2006. It enhanced 802.15.3 to improve implementation and interoperability of the MAC. This amendment include many optimizations, corrected <b>errors,</b> clarified <b>ambiguities,</b> and added editorial clarifications while preserving backward compatibility. Among other changes, the amendment defined the following new features: ...|$|R
40|$|Because {{metadata}} {{that underlies}} semantic web applications is gathered from distributed and heterogeneous data sources, {{it is important}} to ensure its quality (i. e., reduce duplicates, spelling <b>errors,</b> <b>ambiguities).</b> However, current infrastructures that acquire and integrate semantic data have only marginally addressed the issue of metadata quality. In this paper we present our metadata acquisition infrastructure, ASDI, which pays special attention to ensuring that high quality metadata is derived. Central to the architecture of ASDI is a erification engine that relies on several semantic web tools to check the quality of the derived data. We tested our prototype in the context of building a semantic web portal for our lab, KMi. An experimental evaluation omparing the automatically extracted data against manual annotations indicates that the verification engine enhances the quality of the extracted semantic metadata...|$|R
40|$|<b>Errors</b> and <b>ambiguities</b> in the {{nomenclature}} of dyes and fluorescent probes are surveyed {{and illustrated}} by case examples. The unfortunate practical consequences of such mistakes are summarized. Origins and causes of such errors are discussed. Recommendations are appended for end-users, vendors, and editors of learned journals, and those who referee their manuscripts...|$|R
40|$|Abstract. Because {{metadata}} {{that underlies}} semantic web applications is gathered from distributed and heterogeneous data sources, {{it is important}} to ensure its quality (i. e., reduce duplicates, spelling <b>errors,</b> <b>ambiguities).</b> However, current infrastructures that acquire and integrate semantic data have only marginally addressed the issue of metadata quality. In this paper we present our metadata acquisition infrastructure, ASDI, which pays special attention to ensuring that high quality metadata is derived. Central to the architecture of ASDI is a verification engine that relies on several semantic web tools to check the quality of the derived data. We tested our prototype in the context of building a semantic web portal for our lab, KMi. An experimental evaluation comparing the automatically extracted data against manual annotations indicates that the verification engine enhances the quality of the extracted semantic metadata. ...|$|R
40|$|The {{combination}} of photo annotation tasks and with instant messaging fun offers great potentials to both end-users and researchers. In this paper, we first describe our prototype {{system that allows}} users to share and annotate digital photos over the Internet while they are chatting online. In addition to manual annotation, our system can extract information from conversations to generate extra annotations. The advantage of using our system is that the boring and tedious task of annotating photos is turned into {{an essential part of}} an attractive fun activity, viz. online chatting. Extracting meaningful information from instant messages is challenged by abbreviations, pronouns, jargon, ellipsis, grammatical <b>errors,</b> <b>ambiguities</b> and asynchrony, all of which frequently appear in message conversations. In the paper we provide a roadmap, i. e. a systematic analysis of linguistic aspects of automated interpretation of message conversations...|$|R
50|$|Organizational self-structuring is a political, {{subjective}} {{process that}} can be affected by systems, individuals, interests, and traditions in which it takes place. It is not necessarily free of <b>error</b> or <b>ambiguity.</b> To constitute an organization, the communication must imply the formation and governance of a differentiated whole with its own reflexive response cycle and mechanisms.|$|R
40|$|This paper {{presents}} a new design of an Arabic keyboard layout for effective text entry on touch screen mobile phones. Our approach {{is based on}} Pareto front optimization using three metrics: minimizing finger travel distance {{in order to maximize}} speed, minimizing neighboring key <b>error</b> <b>ambiguities</b> in order to maximize the quality of spell correction, and maximizing familiarity for less technologically literature users through approximate alphabetic sorting. In our short user studies, the new layout showed an observed improvement in typing speed in comparison to a common Arabic layout. We believe the opportunity is now ripe to research new optimized keyboard designs where there is less usage experience than Qwerty has in mainstream Western European languages. Pareto optimisation can produce high quality keyboards for alphabet based laguages that could give real benefits where there is less reluctance to change from Qwerty. Author Keywords Touch-screen keyboard design optimization...|$|R
40|$|Abstract This paper {{looks at}} methods for {{predicting}} how {{likely it is}} that an n-version software system will suffer from common-mode failures. Common-mode failures are frequently caused by specification <b>errors,</b> specification <b>ambiguities,</b> and programmer faults. Since common-mode failures are detrimental to n-version systems, we have developed a method and a tool that observes the impact of simulated specification <b>errors</b> and specification <b>ambiguities.</b> These observations are made possible by a new family of fault injection algorithms designed to simulate specification anomalies. As a side-benefit, this analysis also provides clues concerning which portions of the specification, if even slightly wrong or misinterpreted, will lead to identical failures by two or more versions. This suggests which specification directives have the most impact on the system's functionality...|$|R
40|$|It {{is shown}} that in {{asymptotic}} transition from Fourier series to integrals an <b>error</b> and <b>ambiguity</b> may arise. Ambiguity reduces to {{a possibility of}} addition of some distribution to the result. Properties of such distributions are studied and conditions are established under which ambiguity doesn’t arise. Method for correction computation is suggested and conditions for correction turning to zero are specified. ...|$|R
40|$|We correct certain <b>errors</b> and <b>ambiguities</b> in {{the recent}} {{pedagogical}} article by Hopkins and de Bruyn. The early-time asymptotics of {{the solution to the}} transient version of Stokes' second problem for an Oldroyd-B fluid in a half-space is presented, as an Appendix, to complement the late-time asymptotics given by Hopkins and de Bruyn. Comment: 3 pages, no figures, NRC Research Press templat...|$|R
40|$|Our {{ability to}} acquire and analyze DNA {{sequence}} data has increased phenomenally {{in the past}} 12 years. The acquisition of both cDNA and genomic DNA sequence has exerted {{a major influence on}} the direction of biological and medical research {{and will continue to do}} so. However, the DNA sequencing field has progressed so rapidly that technical differences between various sequencing approaches have resulted in large datasets of differing quality. Although all of these datasets are valuable in their own right, they are composed of experimental data; therefore they are subject to <b>errors,</b> <b>ambiguities,</b> and incompleteness at a level related to the experimental strategy that created them. The picture is further complicated by the lack of a community-accepted nomenclature that clearly defines levels of sequence completeness. Because of the small number of people producing this resource relative to the large number using it, the nature of the data is, unfortunately, not commonly appreciated [...] ...|$|R
40|$|Heavy use of {{spreadsheets}} by organisations bears {{many potential}} risks such as <b>errors,</b> <b>ambiguity,</b> data loss, duplication, and fraud. In this paper these risks are briefly outlined {{along with their}} available mitigation methods such as: documentation, centralisation, auditing and user training. However, {{because of the large}} quantities of spreadsheets used in organisations, applying these methods on all spreadsheets is impossible. This fact is considered as a deficiency in these methods, a gap which is addressed in this paper. In this paper a new software tool for managing spreadsheets and identifying the risk levels they include is proposed, developed and tested. As an add-in for Microsoft Excel application, "Risk Calculator" can automatically collect and record spreadsheet properties in an inventory database and assign risk scores based on their importance, use and complexity. Consequently, auditing processes can be targeted to high risk spreadsheets. Such a method saves time, effort, and money. Comment: 22 pages, 11 Colour Figures, 4 Table...|$|R
40|$|We {{present a}} new {{approach}} to multi-dimensional parsing using relational grammars and fuzzy sets. A fast, incremental parsing algorithm is developed, motivated by the two-dimensional structure of written mathematics. Our approach makes no hard decisions; recognized math expressions are reported to the user in ranked order. A flexible correction mechanism enables users to quickly choose the correct math expression in case of recognition <b>errors</b> or <b>ambiguity.</b> ...|$|R
40|$|The {{derivation}} of {{the quantum}} retrodictive probability formula involves an <b>error,</b> an <b>ambiguity.</b> The {{end result is}} correct because this error appears twice, {{in such a way}} as to cancel itself. In addition, however, the usual expression for the probability itself contains the same ambiguity; this may lead to errors in its application. A generally applicable method is given to avoid such ambiguities altogether. Comment: 3 page...|$|R
5000|$|A {{reference}} implementation is, in general, an {{implementation of a}} specification {{to be used as}} a definitive interpretation for that specification (This definition is a bit grandiloquent but works). During the development of the ... conformance test suite, at least one relatively trusted implementation of each interface is necessary to (1) discover <b>errors</b> or <b>ambiguities</b> in the specification, and (2) validate the correct functioning of the test suite.|$|R
50|$|Anyone may {{demand the}} {{commissioner}} of the patent office a trial for invalidation of a patent against the patentee (Article 123). A group of three or five trial examiners (Article 136) conduct the trial, gathering {{the parties to the}} patent office (Article 145, paragraph 1 and 3). The patentee may demand restriction of claims, or correction of <b>errors</b> or <b>ambiguity</b> (Article 134bis, added in 2003) to avoid the invalidation.|$|R
40|$|The Unified Modeling Language (UML) is a {{standard}} language adopted by the Object Management Group (OMG) for writing object-oriented (OO) descriptions of software systems. UML allows the analyst to add class-level and system-level constraints. However, UML does not describe how to check the correctness of these constraints. Recent {{studies have shown that}} Symbolic Model Checking can effectively verify large software specifications. In this thesis, we investigate how to use model checking to verify constraints of UML specifications. We describe the process of specifying, translating and verifying UML specifications for an elevator example. We use the Cadence Symbolic Model Verifier (SMV) to verify the system properties. We demonstrate how to write a UML specification that can be easily translated to SMV. We propose a set of rules and guidelines to translate UML specifications to SMV, and then use these to translate a non-trivial UML elevator specification to SMV. We look at errors detected throughout the specification, translation and verification process, to see how well they reveal <b>errors,</b> <b>ambiguities</b> and omissions in the user requirements...|$|R
40|$|A {{wind field}} {{model can be}} used to {{evaluate}} the accuracy of pointwise ambiguity removal for NASA Scatterometer (NSCAT) data. <b>Errors</b> in pointwise <b>ambiguity</b> removal result in large model-fit errors when the pointwise wind estimates are assimilated into the model. By thresholding the <b>error,</b> regions containing <b>ambiguity</b> removal <b>error</b> can be identified. For these regions, the ambiguity selection can be improved using the model-fit field. I have developed a new automated algorithm for evaluating the quality of the pointwise ambiguity selection and for correcting the ambiguity selection. This paper presents this correction algorithm, which is generally applicable to other scatterometers, and the results for NSCAT data...|$|R
50|$|RAM is {{designed}} and shaped to absorb incident RF radiation (also known as non-ionising radiation) {{as effectively as}} possible, from as many incident directions as possible. The more effective the RAM, the lower the resulting level of reflected RF radiation. Many measurements in electromagnetic compatibility (EMC) and antenna radiation patterns require that spurious signals arising from the test setup, including reflections, are negligible to avoid the risk of causing measurement <b>errors</b> and <b>ambiguities.</b>|$|R
2500|$|... polyphenols, {{a fairly}} broad {{structural}} {{class with a}} formal definition, but where mistranslations and general misuse of the term relative to the formal definition has led to serious usage <b>errors,</b> and so <b>ambiguity</b> {{in the relationship between}} structure and activity (SAR).|$|R
40|$|For {{clarification}} of several points during preliminary {{work on this}} paper, we are indebted to Jonathan Ingersoll. We are also grateful to seminar participants at Otago for useful comments, and to Tim Crack, Scott Chaput, Martin Lally and Tony van Zijl for extremely helpful discussions on various aspects of this paper. Any remaining <b>errors</b> or <b>ambiguities</b> are solely {{the responsibility of the}} authors. The views expressed in this paper are not necessarily those of Deutsche Bank AG...|$|R
50|$|The current {{version of}} BGP is version 4 (BGP4), which was {{published}} as RFC 4271 in 2006, after progressing through 20 drafts from documents based on RFC 1771 version 4. RFC 4271 corrected <b>errors,</b> clarified <b>ambiguities</b> and updated the specification with common industry practices. The major enhancement was the support for Classless Inter-Domain Routing and use of route aggregation to decrease the size of routing tables. BGP4 has been in use on the Internet since 1994.|$|R
40|$|We {{present a}} formal {{model for the}} optimal choice of legal {{standards}} which takes into account decision-theoretic considerations and relates them to the underlying quality (in terms of discriminating between benign and harmful actions undertaken by firms) of economic models and information available to regulatory authorities. The model also accounts for the Indirect Effects (or deterrence effects) caused by alternative legal standards (Joskow, 2002) {{as well as for}} Systemic Effects – delays in reaching decisions and (imperfect) detection by regulators of the actions taking place. After deriving necessary and sufficient conditions for adopting discriminating rules (such as Rule of Reason) we then apply our framework to two recent landmark competition cases – Microsoft vs. EU Commission and Leegn Vs. PSKS – in which a change in legal standards has been proposed. ∗ We would like to thank for their useful comments the participants of seminars at CERGE-EI (Prague), Cyprus University, the University of St. Andrews and at the Conference on “Innovation and Competition in the New Economy”, University of Milan, Bicocca (May 4 - 5 2007). Of course all <b>errors,</b> <b>ambiguities</b> and inaccuracies remain ou...|$|R
40|$|Abstract In the STS-based mapping, we are {{requested}} {{to obtain the}} correct order of probes in a DNA sequence from a given set of fragments or equivalently a hybridization matrix A. It is well-known {{that the problem is}} formulated as the combinatorial problem of obtaining a permutation of A’s columns so that the resulting matrix has the consecutive-one property. If the data (the hybridization matrix) is error free and includes enough information, then the above column order determines the correct order of the probes uniquely. Unfortunately this is no longer true if the data include errors, which {{has been one of the}} popular research targets in computational biology. Even if there is no <b>error,</b> <b>ambiguities</b> in the probe order may still remain. This in fact happens by the lack of some information of the data, but almost no further investigation was made previously. In this paper, we define a measure of such imperfectness of the data as a minimum amount of additional fragments which are needed to fix the probe order uniquely. Several polynomial-time algorithms to compute such additional fragments of minimum cost are presented. Keywords: DNA, hybridization, probe, fragment and PQ-tre...|$|R
40|$|Wavelet {{denoising}} by singularity detection {{was proposed}} as an algorithm that combines Mallat and Donoho’s denoising approaches. With wavelet transform modulus sum, {{we can avoid}} the <b>error</b> and <b>ambiguities</b> of tracing the modulus maxima across scales and the complicated and computationally demanding reconstruction process. We can also avoid the visual artifacts produced by shrinkage. In this paper, we investigate a multiwavelet denoising algorithm based on a modified singularity detection approach. Improved signal denoising results are obtained {{in comparison to the}} single wavelet case...|$|R
30|$|One of {{the early}} studies {{focusing}} on robust distributed localization of noisy sensor networks using globally rigid robust quadrilaterals with certain distance measurement <b>errors</b> and <b>ambiguities</b> caused by these errors is presented in [15]. In that article, assuming that the measurement noise can be modeled as a random process, the proposed algorithm uses robust quadrilaterals as a building block for localization, adding an additional constraint beyond graph rigidity. This constraint permits localization of only those nodes which have a high likelihood of unambiguous realization.|$|R
40|$|We {{explore the}} idea of using a "possibilistic {{graphical}} model" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user's family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution <b>errors</b> and <b>ambiguities,</b> and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model. Comment: 10 page...|$|R
40|$|Simultaneous Geosynchronous Operational Environmental Satellite (GOES) 1 km {{resolution}} {{visible image}} pairs can provide quantitative three dimensional measurements of clouds. These data have {{great potential for}} severe storms research and as a basic parameter measurement source for other areas of meteorology (e. g. climate). These stereo cloud height measurements {{are not subject to}} the <b>errors</b> and <b>ambiguities</b> caused by unknown cloud emissivity and temperature profiles that are associated with infrared techniques. This effort describes the display and measurement of stereo data using digital processing techniques...|$|R
50|$|Radiation-absorbent {{material}}, usually {{known as}} RAM, is a material {{which has been}} specially designed and shaped to absorb incident RF radiation (also known as non-ionising radiation), as effectively as possible, from as many incident directions as possible. The more effective the RAM, the lower the resulting level of reflected RF radiation. Many measurements in electromagnetic compatibility (EMC) and antenna radiation patterns require that spurious signals arising from the test setup, including reflections, are negligible to avoid the risk of causing measurement <b>errors</b> and <b>ambiguities.</b>|$|R
40|$|Although {{environmental}} policy decisions frequently {{are based on}} other criteria, cost-benefit analysis {{plays an important role}} in the decision-making process. Questions concerning the costs and benefits of alternative courses of action are being explored in several of REN's studies of natural resource and environmental management problems. This paper explores a general methodological problem that has arisen in applications of cost-benefit analysis of environmental resource decisions. Several investigators claim that the usual methods of estimating the benefits of environment preservation understate systematically those benefits by an amount called "option value". Loosely put., option value is the benefit that potential (but uncertain) users of environmental services derive from avoiding the risk that these services would be unavailable. A proof has been offered that these option value benefits are always nonnegative. Other investigators have offered proofs that option value benefits may be either positive or negative and that, on a priori grounds, there is no way to determine whether or not usual benefits valuation methods systematically understate benefits of environmental preservation. This paper attempts to reconcile these contradictory results. Helpful comments and suggestions from Jesse Ausubel, Donald Erlenkotter, and Mark Pauly are acknowledged gratefully. None of these kind individuals is to be held responsible for any <b>errors,</b> <b>ambiguities,</b> or other faults that may remain...|$|R
