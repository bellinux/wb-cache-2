47|61|Public
5|$|David A. Wheeler's paper How to Prevent {{the next}} Heartbleed {{analyzes}} why Heartbleed wasn't discovered earlier, and suggests several techniques {{which could have}} led to a faster identification, as well as techniques which could have reduced its impact. According to Wheeler, the most efficient technique which could have prevented Heartbleed is an atypical test suite thoroughly performing what he calls negative testing, i.e. testing that <b>invalid</b> <b>inputs</b> cause failures rather than successes. Wheeler highlights that a single general-purpose test suite could serve as a base for all TLS implementations.|$|E
50|$|ANSI and IEEE {{have defined}} {{robustness}} as {{the degree to}} which a system or component can function correctly in the presence of <b>invalid</b> <b>inputs</b> or stressful environmental conditions.|$|E
50|$|Assertions are {{executable}} predicates {{which are}} placed in a program that allow runtime checks of the program. Design by contract is a development approach in which preconditions and postconditions are included for each routine. Defensive programming is the protection a routine from being broken by <b>invalid</b> <b>inputs.</b>|$|E
5000|$|Input kludge: Failing {{to specify}} and {{implement}} {{the handling of}} possibly <b>invalid</b> <b>input</b> ...|$|R
5000|$|Fuzz testing - a kind {{of random}} testing which {{provides}} <b>invalid</b> <b>input</b> to the tested program ...|$|R
5000|$|Negative testing {{together}} with positive testing {{allows you to}} test your application with any (valid or <b>invalid)</b> <b>input</b> data.|$|R
5000|$|False match rate (FMR, {{also called}} FAR = False Accept Rate): the {{probability}} that the system incorrectly matches the input pattern to a non-matching template in the database. It measures the percent of <b>invalid</b> <b>inputs</b> that are incorrectly accepted. In case of similarity scale, if the person is an imposter in reality, but the matching score is higher than the threshold, then he is treated as genuine. This increases the FMR, which thus also depends upon the threshold value.|$|E
50|$|David A. Wheeler's paper How to Prevent {{the next}} Heartbleed {{analyzes}} why Heartbleed wasn't discovered earlier, and suggests several techniques {{which could have}} led to a faster identification, as well as techniques which could have reduced its impact. According to Wheeler, the most efficient technique which could have prevented Heartbleed is an atypical test suite thoroughly performing what he calls negative testing, i.e. testing that <b>invalid</b> <b>inputs</b> cause failures rather than successes. Wheeler highlights that a single general-purpose test suite could serve as a base for all TLS implementations.|$|E
50|$|Test {{cases are}} built around {{specifications}} and requirements, i.e., what the application {{is supposed to}} do. Test cases are generally derived from external descriptions of the software, including specifications, requirements and design parameters. Although the tests used are primarily functional in nature, non-functional tests may also be used. The test designer selects both valid and <b>invalid</b> <b>inputs</b> and determines the correct output, often {{with the help of}} an oracle or a previous result that is known to be good, without any knowledge of the test object's internal structure.|$|E
40|$|During Bounded Model Checking (BMC) {{blocks of}} a design are often {{considered}} separately due to complexity issues. Because {{the environment of}} a block is not avail-able for the proof, <b>invalid</b> <b>input</b> sequences frequently lead to false negatives, i. e. counter-examples that can not occur in the complete design. Finding and understanding such false negatives is currently a time-consuming manual task. Here, we propose a method to automatically avoid false negatives which are caused by <b>invalid</b> <b>input</b> sequences for blocks connected by standard communication protocols. 1...|$|R
5000|$|Robustness - The {{software}} {{is able to}} operate under stress or tolerate unpredictable or <b>invalid</b> <b>input.</b> For example, it can be designed with resilience to low memory conditions.|$|R
25|$|They are all {{the same}} in their general mechanics, with the main {{differences}} being on issues such as allowed range of code point values and safe handling of <b>invalid</b> <b>input.</b>|$|R
5000|$|Complex {{software}} systems, especially multi-vendor {{distributed systems}} based on open standards, perform input/output operations to exchange data via stateful, structured exchanges known as [...] "protocols." [...] One kind of fault injection {{that is particularly}} useful to test protocol implementations (a type of software code that has the unusual characteristic in that it cannot predict or control its input) is fuzzing. Fuzzing is an especially useful form of Black-box testing since the various <b>invalid</b> <b>inputs</b> that are submitted to the software system do not depend on, and are not created based on knowledge of, {{the details of the}} code running inside the system.|$|E
50|$|A smart (model-based, grammar-based, or protocol-based) fuzzer leverages {{the input}} model to {{generate}} {{a greater proportion of}} valid inputs. For instance, if the input can be modelled as an abstract syntax tree, then a smart mutation-based fuzzer would employ random transformations to move complete subtrees from one node to another. If the input can be modelled by a formal grammar, a smart generation-based fuzzer would instantiate the production rules to generate inputs that are valid w.r.t. the grammar. However, generally the input model must be explicitly provided which is difficult when it is proprietary, unknown, or very complex. If a large corpus of valid and <b>invalid</b> <b>inputs</b> are available, a grammar induction technique, such as Angluin's L* algorithm would be able {{to generate a}}n input model.|$|E
5000|$|Locally {{testable}} codes {{have a lot}} {{in common}} with probabilistically checkable proofs (PCPs). This should be apparent from the similarities of their construction. In both, we are given [...] random nonadaptive queries into a large string and if we want to accept, we must with probability 1, and if not, we must accept no more than half the time. The major difference is that PCPs are interested in accepting [...] if there exists a [...] so that [...] Locally testable codes, on the other hand, accept [...] if it is part of the code. Many things can go wrong in assuming a PCP proof encodes a locally testable code. For example, the PCP definition says nothing about invalid proofs, only <b>invalid</b> <b>inputs.</b>|$|E
30|$|In the {{login page}} that we consider, {{responses}} to valid requests include welcome messages and <b>invalid</b> <b>input</b> error messages. Responses to invalid requests include PHP or SQL error messages. The essential {{point is the}} existence of differences between execution pages and rejection pages, and, more precisely, between welcome messages and <b>invalid</b> <b>input</b> error messages, and between PHP and SQL error messages. Our approach focuses on the analysis of these differences. The objective is to identify, among several responses, those which correspond to execution pages generated through syntactically valid requests. In other words, we learn {{the behavior of the}} application based on the clustering of Web server response pages that are similar enough.|$|R
40|$|Model-based {{testing is}} a {{recognized}} method for testing the functionality {{of a system}} under test. However, {{it is not only}} the functionality of a system that has to be assessed. Also the security aspect has to be tested, especially for systems that provide interfaces to the Internet. In order to find vulnerabilities that could be exploited to break into or to crash a system, fuzzing is an established technique in industry. Model-based fuzzing complements model-based testing of functionality in order to find vulnerabilities by injecting <b>invalid</b> <b>input</b> data into the system. While it focuses on <b>invalid</b> <b>input</b> data, we present a complementary approach called behavioral fuzzing. Behavioral fuzzing does not inject <b>invalid</b> <b>input</b> data but sends an invalid sequence of messages to the system under test. We start with existing UML sequence diagrams - e. g. functional test cases - and modify them by applying fuzzing operators in order to generate invalid sequences of messages. We present the identified fuzzing operators and propose a classification for them. A description of a case study from the ITEA- 2 research project DIAMONDS as well as preliminary results are presented...|$|R
50|$|The NSS {{software}} crypto module {{has been}} validated five times (1997, 1999, 2002, 2007, and 2010) for conformance to FIPS 140 at Security Levels 1 and 2. NSS {{was the first}} open source cryptographic library to receive FIPS 140 validation. The NSS libraries passed the NISCC TLS/SSL and S/MIME test suites (1.6 million test cases of <b>invalid</b> <b>input</b> data).|$|R
5000|$|The Mu Service Analyzer is a {{commercial}} service testing tool developed by Mu Dynamics. The Mu Service Analyzer performs black box and white box {{testing of services}} based on their exposed software interfaces, using denial-of-service simulations, service-level traffic variations (to generate <b>invalid</b> <b>inputs)</b> and the replay of known vulnerability triggers. All these techniques exercise input validation and error handling and are {{used in conjunction with}} valid protocol monitors and SNMP to characterize the effects of the test traffic on the software system. The Mu Service Analyzer allows users to establish and track system-level reliability, availability and security metrics for any exposed protocol implementation. The tool has been available in the market since 2005 by customers in the North America, Asia and Europe, especially in the critical markets of network operators (and their vendors) and Industrial control systems (including Critical infrastructure).|$|E
30|$|Evaluating {{validity}} is {{questioning the}} extent to which we do measure a form of robustness. Recalling the IEEE glossary of Software Engineering [15] where robustness is defined as the degree to which, a system or a component can function correctly in the presence of <b>invalid</b> <b>inputs</b> or stressful environmental conditions, we see that our measurement does not account for <b>invalid</b> <b>inputs,</b> but only for stressful environmental conditions. In our approach the notion of environment only reflects other architectural entities on which the system depends, but does not account for network conditions such as work load for instance.|$|E
30|$|Correct The Web service {{response}} {{in the presence of}} an invalid input is correct (i.e., the Web service responds with an expected exception or error code). Although this is not really a failure mode, it allows characterizing a correct service behavior in the presence of <b>invalid</b> <b>inputs.</b>|$|E
3000|$|... empty {{files and}} geometries, <b>invalid</b> <b>input</b> file formats. It should be noticed that val 3 dity does not {{validate}} the schema of the input, {{the main reason}} is that often small errors are not an issue for the validation and val 3 dity can recover from them (in {{the same way that}} a web browser can often display invalid HTML); [...]...|$|R
5000|$|Finding defects using Boundary value {{analysis}} {{test design}} technique {{can be a}} very effective and can be used at all test levels. You can select multiple test cases from valid and <b>invalid</b> <b>input</b> domains based on your needs or previous experience but remember you do have to select at least one test case from each input domain. Boundary value analysis concept: ...|$|R
40|$|Abstract—Fuzz testing or fuzzing is {{interface}} robustness testing {{by stressing}} the interface {{of a system}} under test (SUT) with <b>invalid</b> <b>input</b> data. It aims at finding security-relevant weaknesses in the implementation that {{may result in a}} crash of the system-under-test or anomalous behavior. Fuzzing means sending <b>invalid</b> <b>input</b> data to the SUT, the input space is usually huge. This is also true for behavioral fuzzing where invalid message sequences are submitted to the SUT. Because systems getting more and more complex, testing a single invalid message sequence becomes more and more time consuming due to startup and initialization of the SUT. We present an approach to make the test execution for behavioral fuzz testing more efficient by generating test cases at runtime instead of before execution, focusing on interesting regions of a message sequence based on a previously conducted risk analysis and reducing the test space by integrating already retrieved test results in the test generation process...|$|R
30|$|Interface faults, {{related to}} {{problems}} in the interaction among software components/modules [8], are particularly relevant in service-oriented environments. In fact, services must provide a robust interface to the client applications, even {{in the presence of}} <b>invalid</b> <b>inputs</b> that may occur due to bugs in the client applications, corruptions caused by silent network failures, or even security attacks.|$|E
30|$|In {{order to}} avoid such <b>invalid</b> <b>inputs,</b> one can insert checks that ensure data trustworthiness. But, how can we {{guarantee}} that all paths, in which probably untrusted information flows, pass all required checks? The solution proposed by Ørbæk and Palsberg [38] {{is to use a}} type system to track the flow of untrusted data in a program.|$|E
40|$|International audienceWeb {{services}} {{represent a}} powerful interface for back-end systems that must provide a robust interface to client applications, {{even in the}} presence of <b>invalid</b> <b>inputs.</b> However, developing robust services is a difficult task. In this paper we demonstrate wsrbench, an online tool that facilitates web services robustness testing. Additionally, we present two scenarios to motivate robustness testing and to demonstrate the power of robustness testing in web services environments...|$|E
30|$|Server {{resource}} disclosure {{was also}} a frequently observed issue (in 10  % of the services). In fact it was frequently observed that, when in presence of an <b>invalid</b> <b>input,</b> some services disclose not only development information (e.g., a tacktrace wrapped in an exception thrown at an unexpected point), but also more critical information (e.g., the partial directory structure of a hard-drive which represents a security issue).|$|R
50|$|In {{fault-tolerant}} computer systems, {{programs that}} are considered robust are designed to continue operation despite an error, exception, or <b>invalid</b> <b>input,</b> instead of crashing completely. Software brittleness {{is the opposite of}} robustness. Resilient networks continue to transmit data despite the failure of some links or nodes; resilient buildings and infrastructure are likewise expected to prevent complete failure in situations like earthquakes, floods, or collisions.|$|R
50|$|The {{division}} operation {{yields a}} real number, but fails when the divisor is zero. If {{we were to}} write a function that performs division, we might choose to return 0 on this <b>invalid</b> <b>input.</b> However, if the dividend is 0, the result is 0 too. This means there is no number we can return to uniquely signal attempted division by zero, since all real numbers {{are in the range}} of division.|$|R
40|$|Existing tools require {{end-user}} programmers (EUPs) {{to write}} regular expressions (“regexps”) or even scripts to validate textual inputs, which {{is slow and}} error-prone. We present a new technique enabling EUPs to describe data {{as a series of}} constrained parts. We incorporate this technique into a prototype tool called Toped, which generates validation code for Excel and web forms. This technique enables EUPs to validate data more quickly and accurately than with existing techniques, finding 90 % of <b>invalid</b> <b>inputs</b> in a lab stud...|$|E
40|$|The data {{distributors}} {{work is to}} give {{sensitive data}} {{to a set of}} presumably trusted third party agents. The data i. e., sent to these third parties are available on the unauthorized places like web and or some ones systems, due to data leakage. The distributor must know the way the data was leaked from one or more agents instead of as opposed to having been independently gathered by other means. Our new proposal on data allocation strategies will improve the probability of identifying leakages along with Security attacks typically result from unintended behaviors or <b>invalid</b> <b>inputs.</b>  Due to too many <b>invalid</b> <b>inputs</b> in the real world programs is labor intensive about security testing. The most desirable thing is to automate or partially automate security-testing process. In this paper we represented Predicate/ Transition nets approach for security tests automated generationby using formal threat models to detect the agents using allocation strategies without modifying the original data. The guilty agent is the one who leaks the distributed data. To detect guilty agents more effectively the idea is to distribute the data intelligently to agents based on sample data request and explicit data request. The fake object implementation algorithms will improve the distributor chance of detecting guilty agents. </p...|$|E
30|$|The {{challenge}} of passing the validation. Programs often validate the inputs before parsing and handling. The validation {{works as a}} guard of programs, saving the computing resource and protecting the program against <b>invalid</b> <b>inputs</b> and damage caused by malicious constructed inputs. Invalid testcases are always ignored or discarded. Magic numbers, magic strings, version number check, and checksums are common validations used in programs. Testcases generated by black box and gray box fuzzers are hard to pass the validation for a blind generation strategy, which results in quite low efficient fuzzing. Thus, how to pass the validation is another key challenge.|$|E
40|$|Abstract This article {{presents}} a local LR error repair method that repairs syntax errors quickly by {{adoption of the}} A * algorithm that helps remove unproductive configurations. The new method also enhances the repair quality by adoption of a flexible edit strategy to support shifting symbols unrestrictedly, as well as inserting and deleting symbols, in order to repair <b>invalid</b> <b>input</b> strings. Experimental {{results show that the}} new method excels existing works in repair quality and efficiency. ...|$|R
40|$|Abstract- The {{encryption}} machine {{takes the}} key Value and the input file (XML file) and generate the encrypted text using Symmetric algorithm (Caesar cipher and Vigenere cipher). The decryption Machine takes the encrypted file and key Value {{to generate the}} original XML file. It also checks whether the input (key value) is valid and block the user if <b>invalid</b> <b>input</b> is provided (key value). These algorithms are compared with respect to different granularity levels of XML files with various file sizes, time obtained for encryption and decryption...|$|R
50|$|Another common {{optimization}} is {{employment of}} unary change accumulation and batch propagation. Such a solution can be faster because it reduces communication among involved nodes. Optimization strategies {{can then be}} employed which reason {{about the nature of}} the changes contained within, and make alterations accordingly. e.g. two changes in the batch can cancel each other, and thus, simply be ignored. Yet another available approach, is described as invalidity notification propagation. This approach causes nodes with <b>invalid</b> <b>input</b> to pull updates, thus resulting in the update of their own outputs.|$|R
