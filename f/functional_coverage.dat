145|44|Public
5000|$|Simulation Solution: Intelligent {{stimulus}} generation, {{code and}} <b>functional</b> <b>coverage,</b> temporal assertion checker ...|$|E
50|$|The {{applications}} {{architecture is}} specified {{on the basis}} of business and functional requirements. This involves defining the interaction between application packages, databases, and middleware systems in terms of <b>functional</b> <b>coverage.</b> This helps identify any integration problems or gaps in <b>functional</b> <b>coverage.</b> A migration plan can then be drawn up for systems which are {{at the end of the}} software life cycle or which have inherent technological risks.|$|E
50|$|To help {{gauge the}} {{thoroughness}} of a simulation, tools exist for assessing code coverage, <b>functional</b> <b>coverage</b> and logic coverage tools.|$|E
50|$|The first {{satellite}} of the second-generation system, Compass-M1 {{was launched}} in 2007. It was followed by further nine satellites during 2009-2011, achieving <b>functional</b> regional <b>coverage.</b> A total of 16 satellites were launched during this phase.|$|R
30|$|In [38], we {{presented}} the preliminary experiments which evaluated the proposed algorithms. In the current work, we {{provide a more}} {{detailed description of the}} algorithms as well as a deeper experimental analysis. In order to consider a more diverse set of experiments, we addressed both structural and functional testing, different from [38] which addressed only structural testing. For structural testing, the experiments were performed here using five programs (flex, grep, gzip, sed, and space) from the Software-artifact Infrastructure Repository (SIR) programs [39]. For functional testing, in turn, two suites from the context of a Motorola mobile device were adopted. The proposed algorithms optimized two objectives simultaneously: maximize branch coverage (structural testing) or <b>functional</b> requirement <b>coverage</b> (<b>functional</b> testing) while minimizing execution cost (time). We point out {{that it is not the}} purpose of this work to discuss which objectives are more important for the TC selection problem. Branch <b>coverage</b> and <b>functional</b> requirement <b>coverage</b> are likely good candidates for assessing the quality of a TS, and execution time is one realistic measure of cost.|$|R
30|$|The cost {{to execute}} each test {{case of the}} {{functional}} suite {{was measured by the}} Test Execution Effort Estimation Tool, developed by [49]. The effort represents the cost (in time) needed to manually execute each test case on a particular mobile device. Each TC has annotated which requirements it covers, thus we used this information in order to calculate the <b>functional</b> requirement <b>coverage.</b>|$|R
50|$|Different {{coverage}} metrics {{are defined}} to assess that the design has been adequately exercised. These include <b>functional</b> <b>coverage</b> (has every functionality {{of the design}} been exercised?), statement coverage (has each line of HDL been exercised?), and branch coverage (has each direction of every branch been exercised?).|$|E
5000|$|Riviera-PRO - {{high-end}} HDL simulator targeting ASIC {{and large}} FPGA designs. Riviera-PRO extends Active-HDL's simulation features with support for advanced verification methodologies such as linting, <b>functional</b> <b>coverage,</b> OVM and UVM, hardware acceleration, and prototyping. Riviera-PRO {{is a new}} generation of the tool known as Riviera-Classic and is available in 32-bit and 64-bit on MS Windows and Linux.|$|E
50|$|A {{hardware}} verification language, or HVL, is a {{programming language}} used to verify the designs of electronic circuits written in a hardware description language. HVLs typically include features of a high-level programming language like C++ or Java as well as features for easy bit-level manipulation {{similar to those found}} in HDLs. Many HVLs will provide constrained random stimulus generation, and <b>functional</b> <b>coverage</b> constructs to assist with complex hardware verification.|$|E
40|$|MycoBASE: {{expanding}} the <b>functional</b> annotation <b>coverage</b> of mycobacterial genomes Benjamin J. Garcia 1, 2 *, Gargi Datta 1, 2, Rebecca M. Davidson 2 and Michael Strong 1, 2 * Background: Central to most omic scale experiments is the interpretation and examination of resulting gene lists corresponding to differentially expressed, regulated, or observed gene or protein sets. Complicating interpretation {{is a lack}} of functional annotation assigned to a large percentage of many microbial genomes. This is particularly noticeable in mycobacterial genomes, which are significantly divergent from many of the microbial model species used for gene and protein functional characterization, but which are extremely important clinically. Mycobacterial species, ranging from M. tuberculosis to M. abscessus, are responsible for deadly infectious diseases that kill over 1. 5 million people each year across the world. A better understanding of the coding capacity of mycobacterial genomes is therefore necessary to shed increasing light on putative mechanisms of virulence, pathogenesis, and functional adaptations. Description: Here we describe the improved <b>functional</b> annotation <b>coverage</b> of 11 important mycobacteria...|$|R
40|$|Adaptation of {{the fitness}} {{criteria}} {{can be a}} very powerful tool enhancing the feedback scheme employed in standard evolutionary algorithms. When the problem EA is trying to solve is changing over time, the fitness criteria needs to change to adapt to the new problem. Significant performance improvements are possible with feedback based adaptation schemes. This work outlines the results of an adaptation scheme applied to the maximization of the <b>functional</b> test <b>coverage</b> problem. ...|$|R
40|$|We have {{developed}} a significantly improved understanding of thermal TEOS (tetraethylorthosilicate, Si(OCH{sub 2 }CH{sub 3 }) {sub 4 }) surface chemistry at CVD (chemical vapor deposition) temperatures and pressures. This was accomplished using GCMS (gas chromatography-mass spectroscopy) and FTIR (Fourier transform infrared spectroscopy) to examine how TEOS reaction rates are influenced by factors critical to the heterogeneous reaction. This included determining the TEOS pressure dependence, testing if reaction by-products inhibit TEOS decomposition, evaluating functional groups on the SiO{sub 2 } surface as potential reaction sites, and establishing the <b>functional</b> group <b>coverage</b> dependencies. Our results show that TEOS decomposition rates are first-order in TEOS pressure and independent of the surface reaction by-products and the relative coverages of siloxane bridges (Si-O-Si) and hydroxyls on SiO{sub 2 }. These conclusions suggest that a precise knowledge of <b>functional</b> group <b>coverages</b> on SiO{sub 2 } is not essential for modeling thermal TEOS decomposition rates at 1000 K. In the absence of gas-phase reactions, growth rates should be directly proportional to TEOS pressure. Therefore, {{it is likely that}} non-uniform SiO{sub 2 } depositions observed in thermal TEOS CVD are due to depletion of TEOS in the gas-phase and/or thermal gradients on the surface...|$|R
50|$|Test {{management}} {{most commonly}} {{refers to the}} activity of managing the computer software testing process. A test management tool is software used to manage tests (automated or manual) that have been previously specified by a test procedure. It {{is often associated with}} automation software. Test management tools often include requirement and/or specification management modules that allow automatic generation of the requirement test matrix (RTM), {{which is one of the}} main metrics to indicate <b>functional</b> <b>coverage</b> of a system under test (SUT).|$|E
50|$|Coverage {{as applied}} to {{hardware}} verification languages refers to the collection of statistics based on sampling events within the simulation. Coverage is used to determine when the device under test (DUT) has been exposed to a sufficient variety of stimuli {{that there is a}} high confidence that the DUT is functioning correctly. Note that this differs from code coverage which instruments the design code to ensure that all lines of code in the design have been executed. <b>Functional</b> <b>coverage</b> ensures that all desired corner cases in the design space have been explored.|$|E
5000|$|The {{most common}} bugs {{in a program}} are {{generally}} triggered by either a single input parameter or an interaction between pairs of parameters. [...] Bugs involving interactions between three or more parameters are both progressively less common [...] and also progressively more expensive to find---such testing has as its limit the testing of all possible inputs. Thus, a combinatorial technique for picking test cases like all-pairs testing is a useful cost-benefit compromise that enables {{a significant reduction in}} the number of test cases without drastically compromising <b>functional</b> <b>coverage.</b>|$|E
5000|$|The first {{generation}} of intelligent verification tools optimized {{one part of the}} verification process known as Regression testing with a feature called automated coverage feedback. With automated coverage feedback, the test description is automatically adjusted to target design functionality that has not been previously verified (or [...] "covered") by other tests existing tests. A key property of automated coverage feedback is that, given the same test environment, the software will automatically change the tests to improve <b>functional</b> design <b>coverage</b> in response to changes in the design.|$|R
30|$|It {{is worth}} {{mentioning}} here the test environments which {{must deal with}} restrictions, such as the available time to execute the TS (see [20]). In such cases, the above cited techniques can also be successfully deployed; however they may reflect the search restriction in some way. Our previous work using PSO falls within this case [23, 24]. In those works, we formulated the TC selection problem as a constrained optimization task in which the objective function to be optimized concerns the <b>functional</b> requirements <b>coverage,</b> and the execution effort {{is used as a}} constraint in the search process.|$|R
40|$|International audienceWe obtain {{explicit}} Berry-Esseen bounds in the Kolmogorov dis- tance for {{the normal}} approximation of non-linear functionals of vec- tors of independent random variables. Our results {{are based on}} the use of Stein’s method and of random difference operators, and gen- eralise the bounds obtained by Chatterjee (2008), concerning normal approximations in the Wasserstein distance. In order to obtain lower bounds for variances, we also revisit the classical Hoeffding decom- positions, for which we provide a new proof and a new representa- tion. Several applications are discussed in detail: in particular, new Berry-Esseen bounds are obtained for set approximations with ran- dom tessellations, as well as for <b>functionals</b> of <b>coverage</b> processes...|$|R
50|$|The UKCDR project used a {{methodology}} {{designed to ensure}} maximum generalisability of outcomes across United Kingdom further education and higher education. The project team used Unified Modeling Language (UML) as a starting point, gathering use cases to define raw requirements. Use cases {{can be used to}} develop discrete functional requirements which can be scrutinised via activity diagrams which allow software developers to look at various user interactions with a proposed system. The product of these undertakings is often a series of test cases. Test cases are a useful way of determining objectively whether a system covers the desired functionality. UKCDR went through several drafts of test cases before a working version was arrived at. This version was used to examine the <b>functional</b> <b>coverage</b> of a range of item banking solutions.|$|E
40|$|We {{compare the}} {{benefits}} and costs of using off-the-shelf coverage tools vs. application specific coverage tools. We start with an overview of coverage, its benefits and its risks. We elaborate on relatively unfamiliar <b>functional</b> <b>coverage</b> {{as a way to}} create custom made coverage models. We explain what <b>functional</b> <b>coverage</b> models are, how to create them and their great benefits with an example from our experience. We provide guidelines on how to decide if coverage should be used at all and whether code based or <b>functional</b> <b>coverage</b> (or both) should be used. ...|$|E
40|$|SystemVerilog Assertions (SVA) {{can be used}} to {{implement}} relatively complex <b>functional</b> <b>coverage</b> models under appropriate circumstances. This paper explores the issues and implementation of such a <b>functional</b> <b>coverage</b> model to demonstrate both the capabilities of SVA coverage and illustrate coding techniques which can also be applied to the more typical use of SVA coverage, which is to specify key corner cases for the RTL from the designer’s detailed knowledge of the structural implementation. This paper is related to previous wor...|$|E
40|$|Software {{testing has}} been an {{integral}} part of software development process in order to ensure software quality. We conduct testing in the software verification process to iden-tify and remove faults and bugs in the software under development. For software quality assurance (QA) these tests have to be good and effective. Therefore, a notion to evaluate the tests themselves is necessary. This paper discusses the evaluation of test quality in the process of software verification. Here I discuss different structural and <b>functional</b> test <b>coverage</b> criteria, as well as fault based techniques, which may be applied to measure the completeness and effectiveness of the test cases. Keywords: Software test, software quality, test quality, test evaluation, test <b>coverage,</b> test adequacy, <b>functional</b> test, structural test, mutation, fault injection...|$|R
5000|$|SportsCenter debuted a new {{graphics}} {{package on}} April 6, 2009, with the [...] "rundown" [...] graphic (shown during the daytime editions) {{moved to the}} left side of the screen. A new BottomLine was also released that day on ESPN, ESPN2, ESPN CLASSIC and ESPNU but it was quickly removed and reverted to the old BottomLine (which had been used since April 2003) because of major technical difficulties. The new BottomLine was <b>functional</b> during <b>coverage</b> of the 2009 NFL Draft and the 2009 NBA Draft, but the issues were fixed and the it eventually returned on July 8 of the same year. Yet another redesigned BottomLine was launched at 11 PM ET on June 22, 2014, to coincide with a dramatic redesign of the SportsCenter studio.|$|R
40|$|We have {{fabricated}} a Chip Multiprocessor prototype code-named Merlot {{to proof}} our novel speculative multithreading architecture. On Merlot, multiple threads provide wider issue window beyond ordinal instruction level parallel (ILP) processors like superscalar or VLIW. With the architecture, we estimate 3. 0 times speedup against single processing elements (PE) on speech recognition code and IDCT code with four PEs. Merlot integrates on-chip devices, PCI interface, and SDRAM interfaces. We have encountered design issues of chip multiprocessor and SoC design. We have successfully run parallelized mpeg 3 decoder {{on the first}} silicon with several software workarounds, thanks to functional verification environment including system modeling on RTL. However, bugs found in later stage of design have required larger manpower or delay of project. In this paper, we also discuss the methodology to improve <b>functional</b> verification <b>coverage,</b> and expect the solution in formal approaches...|$|R
40|$|Until now, the {{functional}} verification at RTL is still mostly done by simulating the HDL designs {{with a massive}} amount of test patterns. In a typical design environment, {{the quality of the}} test mainly depends on the designer’s understanding of the design and is not measurable. Therefore, more objective methods, which use some well-defined <b>functional</b> <b>coverage</b> metrics to perform a quantitative analysis of simulation completeness, are proposed and rapidly getting popular. For this purpose, many <b>functional</b> <b>coverage</b> metrics are proposed to verify the designs written in HDL. We will present a survey on several popular <b>functional</b> <b>coverage</b> metrics first in this paper. In order to monitor the coverage during simulation, a dedicated tool is required besides the simulator. Several commercial tools for Verilog and VHDL code coverage are now available. We will then introduce three popular approaches of coverage measurement in this paper. 1...|$|E
40|$|We {{compare the}} {{benefits}} and costs of using off-the-shelf coverage tools vs. application specific coverage tools. We start with an overview of coverage, its benefits and its risks. We elaborate on relatively unfamiliar <b>functional</b> <b>coverage</b> {{as a way to}} create custom made coverage models. We explain what <b>functional</b> <b>coverage</b> models are, how to create them and their great benefits with an example from our experience. We provide guidelines on how to decide if coverage should be used at all and whether code based or <b>functional</b> <b>coverage</b> (or both) should be used. 1 Introduction Testing {{is one of the biggest}} problems of the software industry. The cost of testing is usually between 4080 % of the development process (70 % for Microsoft) as compared with less than 20 % for the coding itself [4]. The practice of letting the users find the bugs and fixing them in the next release is becoming dangerous and costly for three main reasons: reputation and brand-name are harmed, replacing the software [...] ...|$|E
40|$|Motivation: Tools and {{resources}} for translating the remarkable growth witnessed {{in recent years in}} the number of protein structures deter-mined experimentally into actual gain in the <b>functional</b> <b>coverage</b> of the proteome are becoming increasingly necessary. We introduce FCP, a publicly accessible web tool dedicated to analyzing the current state and trends of the population of structures within protein families. FCP offers both graphical and quantitative data on the degree of <b>functional</b> <b>coverage</b> of enzymes and nuclear receptors by existing structures, {{as well as on the}} bias observed in the distribution of structures along their respective functional classification schemes...|$|E
40|$|International audienceTesting {{software}} for defects exhaustively remains a computationally intensive problem. Therefore, deciding {{when to stop}} the test of a software product {{is one of the}} main issues in software engineering. Introducing fewer defects, detecting defects earlier in the development process and reducing the time to delivery are the primary objectives of software organizations. In particular, we have been working to address this problem within the realm of the automotive suppliers of car-embedded electronic modules and have developed case research with Johnson Controls. In this paper, we describe our approach to automatically generate test cases for a software product and develop the details of our objective and constraint functions for optimizing the test generation. It is based on a compromise between the structural and <b>functional</b> formal <b>coverage</b> and the cost of the generated tests. Finally, we propose a plan to validate and monitor the new software testing process...|$|R
40|$|GeMMA (Genome Modelling and Model Annotation) {{is a new}} {{approach}} to automatic functional subfamily classification within families and superfamilies of protein sequences. A major advantage of GeMMA is its ability to subclassify very large and diverse superfamilies {{with tens of thousands of}} members, without the need for an initial multiple sequence alignment. Its performance is shown to be comparable to the established high-performance method SCI-PHY. GeMMA follows an agglomerative clustering protocol that uses existing software for sensitive and accurate multiple sequence alignment and profile–profile comparison. The produced subfamilies are shown to be equivalent in quality whether whole protein sequences are used or just the sequences of component predicted structural domains. A faster, heuristic version of GeMMA that also uses distributed computing is shown to maintain the performance levels of the original implementation. The use of GeMMA to increase the <b>functional</b> annotation <b>coverage</b> of functionally diverse Pfam families is demonstrated. It is further shown how GeMMA clusters can help to predict the impact of experimentally determining a protein domain structure on comparative protein modelling coverage, in the context of structural genomics...|$|R
40|$|In today's fast-paced {{electronics}} market, time {{to first}} pass, fully functional silicon {{is the ultimate}} determinant of financial success. Functional verification typically ends up being the most unbounded problem in the flow. While teams struggle with verification, {{the root cause of}} this struggle is the lack of a comprehensive verification plan and automation of the verification process that leverages the plan. This paper describes the problem and presents an approach to verification planning that leads to a high quality plan. It concludes with an example of verification automation, specifically highlighting the connection to an executable verification plan. Author Biography Andrew Piziali is an industry veteran design verification engineer with 22 years experience verifying mainframes, supercomputers and microprocessors with StorageTek, Amdahl, Evans and Sutherland, Convex Computer, Cyrix, Texas Instruments, Transmeta, Verisity and Cadence Design Systems. Having an avid interest in coverage-driven verification, in 2004 he authored the book <b>Functional</b> Verification <b>Coverage</b> Measurement and Analysis. Andrew is currently employed by Cadence Design Systems as a verification application specialist, focusing on verification planning and coverage analysis. ...|$|R
40|$|Abstract—Functional Verification is well-accepted for Electronic System Level (ESL) based {{designs and}} is sup-ported {{by a variety}} of {{standardized}} Hardware Verifi-cation Languages like PSL, e, and SystemVerilog. In this article, we present the classification tree method for functional verification (CTM/FV) as a novel method to close the gap from the verification plan to the specifica-tion of randomized tests and <b>functional</b> <b>coverage</b> for test configurations. CTM/FV is introduced based on graph-ical means from which we automatically generate Sys-temVerilog code as a testbench for constraint-based ran-domized tests and <b>functional</b> <b>coverage,</b> where concepts are outlined by the automotive example of an adaptive cruise controller. I...|$|E
40|$|Functional {{verification}} of microprocessors {{is one of}} the most complex and expensive tasks in the current system-on-chip design process. A significant bottleneck in the validation of such systems is the lack of a suitable <b>functional</b> <b>coverage</b> metric. This paper presents a <b>functional</b> <b>coverage</b> based test generation technique for pipelined architectures. The proposed methodology makes three important contributions. First, a general graphtheoretic model is developed that can capture the structure and behavior (instruction-set) of a wide variety of pipelined processors. Second, we propose a functional fault model that is used to define the <b>functional</b> <b>coverage</b> for pipelined architectures. Finally, test generation procedures are presented that accept the graph model of the architecture as input and generate test programs to detect all the faults in the functional fault model. Our experimental results on two pipelined processor models demonstrate that the number of test programs generated by our approach to obtain a fault coverage is an order of magnitude less than those generated by traditional random or constrainedrandom test generation techniques. ...|$|E
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceFunctional {{verification}} of microprocessors {{is one of the}} most complex and expensive tasks in the current system-on-chip design process. A significant bottleneck in the validation of such systems is the lack of a suitable <b>functional</b> <b>coverage</b> metric. This paper presents a <b>functional</b> <b>coverage</b> based test generation technique for pipelined architectures. The proposed methodology makes three important contributions. First, a general graph-theoretic model is developed that can capture the structure and behavior (instruction-set) of a wide variety of pipelined processors. Second, we propose a functional fault model that is used to define the <b>functional</b> <b>coverage</b> for pipelined architectures. Finally, test generation procedures are presented that accept the graph model of the architecture as input and generate test programs to detect all the faults in the functional fault model. Our experimental results on two pipelined processor models demonstrate that the number of test programs generated by our approach to obtain a fault coverage is an order of magnitude less than those generated by traditional random or constrained-random test generation techniques...|$|E
40|$|Pd {{clusters}} {{formed by}} {{physical vapor deposition}} at room temperature (RT) on highly oriented pyrolytic graphite were investigated {{by a combination of}} X-ray photoelectron spectroscopy and diffraction, angle-resolved ultraviolet photoelectron spectroscopy, scanning tunneling microscopy and calculations based on the local density <b>functional</b> theory. Different <b>coverages</b> with nominally 3, 5 and 10 Å were studied after deposition at RT and after heat treatment at 600 °C. Local ordering exhibiting growth with an fcc(1 1 1) orientation is already observed at the lowest coverage, but with no preferred azimuthal orientation in accordance with the substrate itself. However electronic structure features characteristic for a Pd(1 1 1) single crystal appear only after heat treatment...|$|R
40|$|Abstract- The GPIO core design {{provides}} a general purpose input/output interface to a 32 -bit On-Chip Peripheral Bus (OPB). This GPIO core requires simple output and/or input software controlled signals and implements the functions {{that are not}} implemented using dedicated controllers in the system. Almost all FPGA boards contain GPIO peripheral. In this project we are atomizing {{the functions of the}} GPIO core by writing the code in VERILOG and simulating it in QUESTASIM. In this project we verify the all functions of GPIO core by writing verification code in UVM with different test cases. The <b>functional</b> and code <b>coverage</b> and <b>functional</b> verification of the GPIO RTL design is carried out for the better optimum design...|$|R
40|$|Abstract: In {{this paper}} we present LFSR {{reseeding}} scheme for BIST. A time-to –market efficient algorithm is introduced for selecting reseeding {{points in the}} test sequence. This algorithm targets complete fault coverage and minimization of the test length. Functional broadside tests that avoid over testing by ensuring that a circuit traverses only reachable states during the functional clock cycles of a test[1]. These consist of the input vectors and the corresponding responses. They check for proper operation of a verified design by testing the internal chip nodes. Functional tests cover a very high percentage of modeled faults in logic circuits and their generation is the main topic of this paper. Function test sequence is generated by LFSR. Often, functional vectors are understood as verification vectors, which are used to verify whether the hardware actually matches its specification. However, in the ATE world, any vectors applied are understood to be <b>functional</b> fault <b>coverage</b> vectors applied during manufacturing test. This paper shows the on chip test Generation for a bench mark circuit using simple fixed hardware design with small no of parameters altered in the design for the generatio...|$|R
