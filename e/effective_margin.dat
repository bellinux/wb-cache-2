7|28|Public
50|$|In {{contrast}} to Malthus's hypothesis of overpopulation, Ricardo explains mass poverty using deductive logic {{by noting that}} {{when there is no}} rent-free land, subsistence becomes the <b>effective</b> <b>margin</b> of production. Landlords will not charge more than this amount because it would entail no production at all, and thus no rent.|$|E
40|$|We extend {{and apply}} the PAC-Bayes theorem to the {{analysis}} of maximum entropy learning by considering maximum entropy classification. The theory introduces a multiple sampling technique that controls an <b>effective</b> <b>margin</b> of the bound. We further develop a dual implementation of the convex optimisation that optimises the bound. This algorithm is tested on some simple datasets and the value of the bound compared with the test error. ...|$|E
40|$|In {{this paper}} we propose a novel {{approach}} based on multi-stage random forests to address problems faced by traditional vessel segmentation algorithms on account of image artifacts such as stitches organ shadows etc [...] Our approach consists of collecting {{a very large number of}} training data consisting of positive and negative examples of valid seed points. The method makes use of a 14 × 14 window around a putative seed point. For this window three types of feature vectors are computed viz. vesselness, eigenvalue and a novel <b>effective</b> <b>margin</b> feature. A random forest RF is trained for each of the feature vectors. At run time the three RFs are applied in succession to a putative seed point generated by a naiive vessel detection algorithm based on vesselness. Our approach will prune this set of putative seed points to correctly identify true seed points thereby avoiding false positives. We demonstrate the effectiveness of our algorithm on a large dataset of angio images...|$|E
30|$|Patients {{initially}} complained with fistulae, {{followed by}} pain and difficulty in chewing. Four patients had undergone preoperative hyperbaric oxygen (HBO) therapy {{which was not}} <b>effective.</b> The <b>margins</b> of the diseased segments were planned by preoperative panoramic radiographs and CT or MR images. The preoperative imaging is important as a guide, {{and it can be}} make a decision of the location of resection. All removed specimens were subjected to histopathological examination to confirm the presence of ORN and to exclude any residual or recurrent tumor.|$|R
40|$|This article {{investigates the}} effect of product market {{liberalisation}} on employment allowing for interactions between policies and institutions in product and labour markets. Using panel data for OECD countries over the period 19802002, we present evidence that product market deregulation is more <b>effective</b> at the <b>margin</b> when labour market regulation is high. The data also suggest that product market liberalisation may promote employment-enhancing labour market reforms...|$|R
40|$|In {{high grade}} glioma (HGG), {{extensive}} tumor cell infiltration of normal brain typically precludes identifying <b>effective</b> <b>margins</b> for surgical resection or irradiation. Pertussis toxin (PT) is a multimeric complex that inactivates diverse Gi/o G-protein coupled receptors (GPCRs). Despite the broad continuum of regulatory events controlled by GPCRs, PT may be applicable as a therapeutic. We {{have shown that}} the urokinase receptor (uPAR) is a major driver of HGG cell migration. uPAR-initiated cell-signaling requires a Gi/o GPCR, N-formyl Peptide Receptor 2 (FPR 2), as an essential co-receptor and is thus, PT-sensitive. Herein, we show that PT robustly inhibits migration of three separate HGG-like cell lines that express a mutated form of the EGF Receptor (EGFR), EGFRvIII, which is constitutively active. PT also almost completely blocked the ability of HGG cells to invade Matrigel. In the equivalent concentration range (0. 01 - 1. 0 μg/mL), PT had no effect on cell survival and only affected proliferation of one cell line. Neutralization of EGFRvIII expression in HGG cells, which is known to activate uPAR-initiated cell-signaling, promoted HGG cell migration. The increase in HGG cell migration, induced by EGFRvIII neutralization, was entirely blocked by silencing FPR 2 gene expression or by treating the cells with PT. When U 87 MG HGG cells were cultured as suspended neurospheres in serum-free, growth factor-supplemented medium, uPAR expression was increased. HGG cells isolated from neurospheres migrated through Transwell membranes without loss of cell contacts; this process was inhibited by PT by > 90 %. PT also inhibited expression of vimentin by HGG cells; vimentin is associated with epithelial-mesenchymal transition and worsened prognosis. We conclude that PT may function as a selective inhibitor of HGG cell migration and invasion...|$|R
40|$|We examine {{academic}} research laboratories {{as examples of}} intractable governance sites. These spaces often elude regulatory warnings and rules because of the professional status of faculty members, the opacity of scientific work to outsiders, and loose coupling of policy and practice in organizations. We describe one university’s efforts to create a system for managing laboratory health, safety, and environmental hazards, thereby constraining conventional faculty habit to ignore administrative and legal procedures. We demonstrate the specific struggles safety managers face in creating system responsiveness, that is, feedback to re-channel noncompliant laboratory practices. We show how faculty members are buffered {{from the consequences of}} their activities, thus impeding the goals of responsibility and accountability. We conclude by asking where such pockets of intractability reside in other organizations and whether the surrounding buffer, if there is one, may nonetheless paradoxically create an <b>effective</b> <b>margin</b> of safety. National Science Foundation (U. S.) (Grant 0216815) National Science Foundation (U. S.) (Grant 0518118) National Science Foundation (U. S.) (Grant 0535870...|$|E
40|$|Abstract: A lidar detects {{atmospheric}} parameters by transmitting {{laser pulse}} {{to the atmosphere}} and receiving the backscattering signals from molecules and aerosol particles. Because of the small backscattering cross section, a lidar usually uses the high sensitive photomultiplier and avalanche photodiode as detector and uses photon counting technology for collection of weak backscatter signals. Photon Counting enables the capturing of extremely weak lidar return from long distance, throughout dark background, by a long time accumulation. Because of the strong solar background, the signal-to-noise ratio of lidar during daytime could be greatly restricted, especially for the lidar operating at visible wavelengths where solar background is prominent. Narrow band-pass filters must therefore be installed in order to isolate solar background noise at wavelengths close {{to that of the}} lidar receiving channel, whereas the background light in superposition with signal spectrum, limits an <b>effective</b> <b>margin</b> for signal-to-noise ratio (SNR) improvement. This work describes a lidar prototype operating at the Fraunhofer lines, the invisible band of solar spectrum, to achieve photon counting under intense solar background. The photon counting lidar prototype in Fraunhofer lines devised was used to observe the atmospheri...|$|E
40|$|A lidar detects {{atmospheric}} parameters by transmitting {{laser pulse}} {{to the atmosphere}} and receiving the backscattering signals from molecules and aerosol particles. Because of the small backscattering cross section, a lidar usually uses the high sensitive photomultiplier and avalanche photodiode as detector and uses photon counting technology for collection of weak backscatter signals. Photon Counting enables the capturing of extremely weak lidar return from long distance, throughout dark background, by a long time accumulation. Because of the strong solar background, the signal-to-noise ratio of lidar during daytime could be greatly restricted, especially for the lidar operating at visible wavelengths where solar background is prominent. Narrow band-pass filters must therefore be installed in order to isolate solar background noise at wavelengths close {{to that of the}} lidar receiving channel, whereas the background light in superposition with signal spectrum, limits an <b>effective</b> <b>margin</b> for signal-to-noise ratio (SNR) improvement. This work describes a lidar prototype operating at the Fraunhofer lines, the invisible band of solar spectrum, to achieve photon counting under intense solar background. The photon counting lidar prototype in Fraunhofer lines devised was used to observe the atmospheric boundary layer. The SNR was improved 2 - 3 times by operating the lidar at the wavelength in solar dark lines. The aerosol extinctions illustrate the vertical structures of aerosol in the atmospheric boundary over Qingdao suburban during summer 2011...|$|E
40|$|Wake vortex {{encounter}} risk {{is a major}} {{safety issue}} in aircraft final approach phase. The evaluation of the risk is crucial when the industry takes the effort to reduce aircraft spacing {{for the purpose of}} capacity gain. This paper proposes a hybrid analytical method to assess the wake vortex encounter risk during the final approach phase. The hybrid method uses probabilistic method to calculate the risk based on the probabilistic distributions of aircraft positions and vortex characteristics obtained from Monte Carlo simulations. The proposed method will be more computational efficient than pure simulations to evaluate the probability of a rare event, such as a serious vortex encounter. In addition, it is able to provide insights about risk distributions with respect to time and location. The estimation given by the model is conservative, which provides <b>effective</b> safety <b>margin</b> to the risk evaluation in a highly stochastic environment...|$|R
40|$|Advances in {{radiation}} therapy for malignant neoplasms have produced {{techniques such as}} Gamma Knife radiosurgery, capable of delivering an ablative dose to a specific, irregular volume of tissue. However, efficient use of these techniques requires the identification of a target volume that will produce the best therapeutic response while sparing surrounding normal brain tissue. Accomplishing this task using conventional computed tomography (CT) and contrast-enhanced magnetic resonance imaging (MRI) techniques has proven difficult because of the difficulties in identifying the <b>effective</b> tumor <b>margin.</b> Magnetic resonance spectroscopic imaging (MRSI) {{has been shown to}} offer a clinically-feasible metabolic assessment of the presence and extent of neoplasm that can complement conventional anatomic imaging. This paper reviews current Gamma Knife protocols and MRSI acquisition, reconstruction, and interpretation techniques, and discusses the motivation for including magnetic resonance spectroscopy findings while planning focal radiation therapies. A treatment selection and planning strategy incorporating MRSI is then proposed, which can be used in the future to assess the efficacy of spectroscopy-based therapy planning...|$|R
40|$|Abstract—This paper {{presents}} a relational framework for studying properties of labeled data points related to proximity and labeling information {{in order to}} improve the performance of the 1 NN rule. Specifically, the class conditional nearest neighbor (ccnn) relation over pairs of points in a labeled training set is introduced. For a given class label c, this relation associates to each point a its nearest neighbor computed among only those points with class label c (excluded a). A characterization of ccnn in terms of two graphs is given. These graphs are used for defining a novel scoring function over instances by means of an informationtheoretic divergence measure applied to the degree distributions of these graphs. The scoring function is employed to develop an <b>effective</b> large <b>margin</b> instance selection method, which is empirically demonstrated to improve storage and accuracy performance of the 1 NN rule on artificial and real-life data sets. Index Terms—Computing methodologies, artificial intelligence, learning, heuristics design, machine learning...|$|R
40|$|Consider a firm {{selling a}} {{configurable}} product (e. g., a computer), {{which is a}} combination of a required component (e. g., processor) and an optional component (e. g., a speaker). Each component's assortment allows the consumer to choose from several variants (e. g., processors with different speeds, speakers in different styles). The demands for the two components are complementary: broadening the assortment of one component, or decreasing the price of one of its variants, would increase the demand for the other component. We find that, at optimality, all variants of a component (e. g., all processors offered by the firm) share the same "effective profit margin," which is a function of not only the selling price and the unit cost, but also the unit underage and overage costs, service level, and demand variability. As for assortment selection, we show the importance of a variant's "surplus," which is the difference between the customers utility from a variant and the costs incurred by the firm for the variant (including inventory-related costs). When choosing from two variants that belong to the same component (e. g., two processors with different speeds), the firm should pick the one with the higher surplus. However, when choosing from two variants that belong to different components (e. g., a new processor versus a new speaker), the firm must rely on a measure that we call the "attraction" of a product configuration, which increases in the surpluses of variants that make up the configuration. When choosing from two variants that belong to different components, the firm must compare the total attraction of new product configurations enabled by the addition of each variant. In addition, we show that if the optional component's assortment and margin influence the customer's decision to purchase from the firm, then the optional component must bear zero <b>effective</b> <b>margin.</b> Pricing Assortment planning Inventory Retailing...|$|E
40|$|This paper investigates {{determinants}} {{of the oil}} cargo spill size of tanker and tank barge vessel accidents, utilizing detailed data of individual vessel accidents in U. S. waters. The effectiveness of various Coast Guard enforcement methods are examined {{as well as the}} effects of various vessel-related factors, the price of oil, and the price of vessel repair. The results suggest that Coast Guard pollution detection activity is <b>effective</b> at the <b>margin</b> in reducing tank barge (but not in reducing tanker) accident spill size and that tanker accident spill size is less for U. S. than for foreign flag tankers. ...|$|R
40|$|This paper {{investigates the}} effect of product market {{liberalization}} on employment and considers possible interactions between policies and institutions in product and labor markets. Using panel data for OECD countries over the period 1980 - 2002, we present evidence that product market deregulation is more <b>effective</b> at the <b>margin</b> when labor market regulation is high. The data also suggest that product market deregulation promotes labor market deregulation. These {{results are consistent with}} the basic predictions of a standard bargaining model, such as Blanchard and Giavazzi (2003), extended to allow for a richer specification of the fall back position of the union...|$|R
40|$|The one nearest {{neighbor}} (1 -NN) rule uses instance proximity followed by class labeling information for classifying new instances. This paper presents {{a framework for}} studying properties of the training set related to proximity and labeling information, {{in order to improve}} the performance of the 1 -NN rule. To this aim, a so-called class conditional {{nearest neighbor}} (c. c. n. n.) relation is introduced, consisting of those pairs of training instances (a, b) such that b is the nearest neighbor of a among those instances (excluded a) in one of the classes of the training set. A graph-based representation of c. c. n. n. is used for a comparative analysis of c. c. n. n. and of other interesting proximity-based concepts. In particular, a scoring function on instances is introduced, which measures the effect of removing one instance on the hypothesis-margin of other instances. This scoring function is employed to develop an <b>effective</b> large <b>margin</b> instance selection algorithm, which is empirically demonstrated to improve storage and accuracy performance of the 1 -NN rule on artificial and real-life data sets...|$|R
40|$|Future {{generations of}} {{cellular}} networks envision unprecedented growth in capacity requirements essential {{to handle the}} tremendous evolution of multimedia services. Code division multiple access (CDMA) {{has been one of}} the premium candidates to lead the charge in supporting packetized multimedia traffic within mobile cellular networks. This dissertation presents an analytical framework to derive system capacity of CDMA-based cellular networks on the basis of the satisfied-user criteria recommended by universal mobile telecommunications systems (UMTS). The proposed criteria stretch beyond system-level outage probability that has been commonly adopted in the CDMA literature and proceed to focus on evaluating the percentage of outage during the lifetime of each individual session. Special emphasis is devoted for the downlink as it is considered to be the limiting direction in future cellular systems. The foundation of the proposed analysis is established by recognizing one of the main features of wireless communication channels. Large-scale path-loss depicts measurable/controllable parameters that exhibit high correlations over the scale of consecutive frames. On the other hand, small-scale fading represents fast fluctuations that cannot be reliably predicted. Accordingly, the devised analysis relates the satisfied-user probability to the correlated outage behavior of small-scale fading channels with respect to an <b>effective</b> protection <b>margin</b> set over the large scale. The analytical platform is expanded for capacity evaluation of multi-carrier CDMA (MCCDMA). Multi-carrier CDMA (MC-CDMA) has recently emerged as an extremely promising technology to support higher system capacities for its combined advantages of multi-carrier modulation techniques and CDMA in a frequency-selective fading environment. The analytical approach leads to the proposal of power allocation and channel assignment strategies that share the objective of diminishing the effects of multiple access interference (MAI) and increasing system capacity. In particular, an effective margin-based power control operating over the large-scale is introduced for speech services. The base stations target an <b>effective</b> protection <b>margin</b> and avoid allocating un-necessary power to compensate for intra-cell interference that shares the same small-scale fading paths as the target signal. Moreover, a channel assignment strategy based on large-scale interference measurements is used to investigate the migration from MC-CDMA to Grouped MC-CDMA systems. Capacity analysis of Grouped MC-CDMA systems triggers the development of an iterative simulation platform to incorporate the effects of coupling and dependencies of channel assignments and power allocations across base stations of the cellular network...|$|R
40|$|This paper {{provides}} a systematic empirical {{investigation of the}} effect of product market liberalization on employment when there are interactions between policies and institutions in product and labor markets. Using panel data for OECD countries over the period 1980 - 2002, we present evidence that product market deregulation is more <b>effective</b> at the <b>margin</b> when labor market regulation is high. Moreover, there is evidence in our sample that product market deregulation promotes labor market deregulation. We show that these results are mostly consistent with the basic predictions of a standard bargaining model (e. g. Blanchard and Giavazzi (2003)), once one allows for a full specification of the fall back position of the unions. Employment, Competition, Deregulation, Liberalization, Unions...|$|R
5|$|On 10 October, the High Court {{ruled that}} the act passed at the joint sitting that gave the Australian Capital Territory (ACT) and the Northern Territory two {{senators}} each was valid. A half-Senate election needed to be held by June 1976; most senators-elect would take their seats on 1 July but the territorial senators, and those filling Field's and Bunton's seats would take their places at once. The ruling meant {{that it was possible}} for the ALP to gain a temporary majority in the Senate, at least until 1 July 1976. To do so, the ALP would have to win Field's and Bunton's seats, and one seat in each territory, and have the second ACT seat fall to either a Labor candidate or an independent, former Liberal Prime Minister John Gorton, now estranged from his party. If this happened, Labor would have an <b>effective</b> 33–31 <b>margin,</b> would be able to pass supply if that was still an issue, and also could pass electoral redistribution laws (which had been passed by the House, though twice defeated by the Senate) that would give it an advantage at the next election.|$|R
40|$|With robots leaving {{factories}} and entering less controlled domains, possibly sharing the space with humans, safety is paramount and multimodal {{awareness of the}} body surface and the surrounding environment is fundamental. Taking inspiration from peripersonal space representations in humans, we present a framework on a humanoid robot that dynamically maintains such a protective safety zone, composed of the following main components: (i) a human 2 D keypoints estimation pipeline employing a deep learning based algorithm, extended here into 3 D using disparity; (ii) a distributed peripersonal space representation around the robot's body parts; (iii) a reaching controller that incorporates all obstacles entering the robot's safety zone on the fly into the task. Pilot experiments demonstrate that an <b>effective</b> safety <b>margin</b> between the robot's and the human's body parts is kept. The proposed solution is flexible and versatile since the safety zone around individual robot and human body parts can be selectively modulated [...] -here we demonstrate stronger avoidance of the human head compared to rest of the body. Our system works in real time and is self-contained, with no external sensory equipment and use of onboard cameras only...|$|R
50|$|On 10 October, the High Court {{ruled that}} the act passed at the joint sitting that gave the Australian Capital Territory (ACT) and the Northern Territory two {{senators}} each was valid. A half-Senate election needed to be held by June 1976; most senators-elect would take their seats on 1 July but the territorial senators, and those filling Field's and Bunton's seats would take their places at once. The ruling meant {{that it was possible}} for the ALP to gain a temporary majority in the Senate, at least until 1 July 1976. To do so, the ALP would have to win Field's and Bunton's seats, and one seat in each territory, and have the second ACT seat fall to either a Labor candidate or an independent, former Liberal Prime Minister John Gorton, now estranged from his party. If this happened, Labor would have an <b>effective</b> 33-31 <b>margin,</b> would be able to pass supply if that was still an issue, and also could pass electoral redistribution laws (which had been passed by the House, though twice defeated by the Senate) that would give it an advantage at the next election.|$|R
40|$|Current U. S. law prohibits {{compensation}} for cadaveric organ donation. The resulting organ shortage causes thousands of deaths per year. The primary tool currently relied {{on by the}} organ procurement industry to increase organ supply is educational spending aimed at both industry professionals and the general public. This article evaluates the effectiveness of such spending across a fairly comprehensive and unique sample of free-standing U. S. organ procurement organizations, controlling for {{the size of the}} organization, population demographics, and geographic region. The authors find no evidence that such spending is <b>effective</b> on the <b>margin</b> and conclude that the organ shortage is unlikely to be resolved by increased educational expenditures. (JEL "I 18 ", "I 11 ") Copyright 2004 Western Economic Association International. ...|$|R
40|$|Following several {{flight and}} ground test {{failures}} of spacecraft systems using single-shot, &quot;normally closed&quot; pyrotechnically actuated valves (pyrovalves), a Government/Industry cooperative program was initiated {{to assess the}} functional performance of five qualified designs. The goal {{of the program was}} to provide information on functional performance of pyrovalves to allow users the opportunity to improve procurement requirements. Specific objectives included the demonstration of performance test methods, the measurement of &quot;blowby &quot; (the passage of gasses from the pyrotechnic energy source around the activating piston into the valve's fluid path), and the quantification of functional margins for each design. Experiments were conducted at NASA Langley Research Center on several units for each of the five valve designs. The test methods used for this program measured the forces and energies required to actuate the valves, as well as the energies and the pressures (where possible) delivered by the pyrotechnic sources. Functional performance ranged widely among the designs. Blowby cannot be prevented by o-ring seals; metal-to-metal seals were <b>effective.</b> Functional <b>margin</b> was determined by dividing the energy delivered by the pyrotechnic sources in excess to that required to accomplish the function by the energy required for that function. Two of the five designs had inadequate functional margins with the pyrotechnic cartridges evaluated...|$|R
40|$|Emphasis on {{building}} and managing brand equity, {{as a primary}} driver of a hospitality firm's success, is of increasing interest. Building a brand with strong equity provides a number of potential benefits to a firm: greater brand loyalty, larger profit <b>margins,</b> <b>effective</b> marketing communication focus, and opportunities for brand-extensions. Although the issue of brand equity has {{emerged as one of}} the most important aspects of branding, little empirical evidence ists as to how to create brand equity and the nature of its antecedents and consequences. jcially in the hospitality industry. Therefore, the thrust of this research is to develop and test a research model of the antecedents and consequences of brand equity in the hospitality industry - in particular, for the hotel and restaurant sectors. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|A {{tendency}} {{seen for}} quite some time in the Swedish railway network is a growing demand for capacity which no longer can be accommodated. This causes congestion and delays, and the relationships between the trains and how they affect eachother are significantly harder to overview and analyse. Railway traffic timetables normally contain margins to make them robust, and enable trains to recover from certain delays. How <b>effective</b> these <b>margins</b> are, depends on their size and location as well as the frequency and magnitude of the disturbances that occur. Hence, it is important to include marigns so, that they can be used operationally to recover from a variety of disturbances and not restricted to a specific part of the line and/or the timetable. In a case study we compare the performance of a selection of passenger train services to the different prerequisites given by the timetable (e. g. available margins and their location, critical train dependencies). The study focuses on the Swedish Southern mainline between Stockholm and Malmö on which a wide variety of train services operate, e. g. freight trains, local and regional commuter train services as well as long-distance trains with different speed profiles. The analysis shows a clear mismatch between where margins are placed and where delays occur. We also believe that the most widely used performance measure, which is related to the delay when arriving at the final destination, might give rise to an unnecessarily high delay rate at intermediate stations...|$|R
40|$|Abstract — As VLSI {{technologies}} scale down, interconnect {{performance is}} greatly affected by crosstalk noise {{due to the}} decreasing wire separation and increased wire aspect ratio, and crosstalk {{has become a major}} bottleneck for design closure. The effectiveness of traditional buffering and spacing techniques for noise reduction is constrained by the limited available resources on chip. In this paper, we present a method for incorporating crosstalk reduction criteria into global routing under a broad power supply network paradigm. This method utilizes power/ground wires as shields between signal wires to reduce capacitive coupling, while considering the constraints imposed by limited routing and buffering resources. An iterative procedure is employed to route signal wires, assign supply shields, and insert buffers so that both buffer/routing capacity and signal integrity goals are met. In each iteration, shield assignment and buffer insertion are considered simultaneously via a dynamic programming-like approach. Our noise calculations are based on Devgan’s metric, and our work demonstrates, for the first time, that this metric shows good fidelity on average. An <b>effective</b> noise <b>margin</b> inflation technique is also proposed to compensate for the pessimism of Devgan’s metric. Experimental results on testcases with up to about 10, 000 nets point towards an asymptotic runtime that increases linearly with the number of nets. Our algorithm achieves noise reduction improvements of up to 53 % and 28 %, respectively, compared to methods considering only buffer insertion, or only shield insertion after buffer planning. I...|$|R
40|$|Indentation in {{the upper}} brittle crust where one plate is stiffer than the other {{produces}} vertical extrusion of a doubly vergent orogenic wedge. Sandbox models of this process show that erosion with or without deposition of the eroded material onto one or both margins significantly changes the internal patterns of orogenic shear and compaction within the orogens. Erosion decreases the vertical stress and changes the criticality of the orogenic wedge, whereas redeposition increases the vertical stress on its <b>margins.</b> <b>Effective</b> indenters of accreted sand, which develop in models without erosion if the rigid indenter face dip is 75 ° (e. g., > 45 °) or ≤ 15 are strongly affected by erosion. Rapid erosion favors thrusting over compaction and decreases both the size and {{the relevance of the}} effective indenters. Redeposition of eroded material on the margins also expands the lifetime of the active shear as the additional load delays initiation of underlying new shears. © 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|High-speed serial {{communication}} (i. e., Gigabit Ethernet) requires differential transmission and controlled impedances. Impedance control is essential throughout cabling, connector, and circuit board construction. An impedance discontinuity arises at the interface of a high-speed quadrax and twinax connectors and the attached {{printed circuit board}} (PCB). This discontinuity usually is lower impedance since the relative dielectric constant of the board is higher (i. e., polyimide approx. = 4) than the connector (Teflon approx. = 2. 25). The discontinuity {{can be observed in}} transmit or receive eye diagrams, and can reduce the <b>effective</b> link <b>margin</b> of serial data networks. High-speed serial data network transmission improvements can be made at the connector-to-board interfaces as well as improving differential via hole impedances. The impedance discontinuity was improved by 10 percent by drilling a 20 -mil (approx. = 0. 5 -mm) hole in between the pin of a differential connector spaced 55 mils (approx. = 1. 4 mm) apart as it is attached to the PCB. The effective dielectric constant of the board can be lowered by drilling holes into the board material between the differential lines in a quadrax or twinax connector attachment points. The differential impedance is inversely proportional to the square root of the relative dielectric constant. This increases the differential impedance and thus reduces the above described impedance discontinuity. The differential via hole impedance can also be increased in the same manner. This technique can be extended to multiple smaller drilled holes as well as tapered holes (i. e., big in the middle followed by smaller ones diagonally) ...|$|R
40|$|This article {{investigates the}} effect of product market {{liberalisation}} on employment allowing for interactions between policies and institutions in product and labour markets. Using panel data for OECD countries over the period 1980 – 2002, we present evidence that product market deregulation is more <b>effective</b> at the <b>margin</b> when labour market regulation is high. The data also suggest that product market liberalisation may promote employment-enhancing labour market reforms. Over the past two decades, many OECD countries have sought to promote productivity and long-term growth by improving the efficiency {{of goods and services}} markets through liberalisation and privatisation programmes. There is a growing body of evi-dence suggesting that these programmes have indeed boosted productivity perform-ances in the sectors concerned, 1 but there is less evidence on their impacts on employment. A few recent theoretical and empirical studies suggest that product market deregulation may stimulate aggregate employment, yet firm conclusions are still lacking. 2 In assessing {{the effect of}} product market regulatory reforms, it is crucial to take into account that these reforms have been implemented in countries with very different labour market settings. This raises two related questions. First, do th...|$|R
40|$|The {{concept of}} {{capacity}} credit {{is widely used}} to quantify the contribution of renewable technologies to securing demand. This may be quantified {{in a number of}} ways; this paper recommends the use of Effective Load Carrying Capability (ELCC, the additional demand which the new generation can support without increasing system risk), with system risk being measured using Loss of Load Expectation (LOLE, this is calculated through direct use of historic time series for demand and wind load factor). The key benefit of this approach is that it automatically incorporates the available statistical information on the relationship between wind availability and demand during the hours of very high demand which are most relevant in assessing system adequacy risk. The underlying assumptions are discussed in detail, and a comparison is made with alternative calculation approaches; a theme running through the paper is the need to consider the assumptions carefully when presenting or interpreting risk assessment results. A range of applications of capacity credits from Great Britain and Ireland are presented; this includes presentation of <b>effective</b> plant <b>margin,</b> ensuring that the optimal plant mix secures peak demand in economic projection models, and the Irish capacity payments system. Finally, new results comparing capacity credit results from the Great Britain and Irish systems using the same wind data are presented. This allows the various factors which influence capacity credit results to be identified clearly. It is well known that increasing the wind load factor or demand level typically increases the calculated capacity credit, while increasing the installed wind capacity typically decreases its capacity credit (as a percentage of rated capacity). The new results also show that the width of the probability distribution for available conventional generating capacity, relative to the peak demand level, also has a strong influence on the results. This emphasises further that detailed understanding of risk model structures is vitally important in practical application...|$|R
40|$|Assortment {{planning}} and pricing {{are among the}} most important strategic questions for many firms. These decisions are particularly challenging when inventory considerations need to be taken into account. Unfortunately, the trade-offs created by the assortment, pricing and inventory decisions are complicated enough to push many firms to make these decisions separately, ignoring their synergy. This dissertation targets this gap by presenting joint assortment and pricing models with inventory considerations. The first setting studied in this dissertation is a single firm selling a configurable product (e. g. a laptop computer), formed by putting together two components: one required (e. g. processor) and one optional (e. g. DVD writer). This dissertation finds that the optimal prices are such that all variants of a component share the same <b>effective</b> profit <b>margin,</b> defined as the unit gross margin net of unit inventory-related cost, which itself depends on unit underage and overage costs, service level and demand variability. As for assortment selection, the importance of a variant's surplus is shown. When variants are put together to form a product configuration, their surpluses combine to yield the attractiveness of the product configuration, whose role in selecting the assortment is highlighted in this dissertation. Furthermore, this dissertation finds that if the optional component's assortment and margin influence the customer's decision to purchase from the firm, then the component must be priced at effective cost. This is no longer true if only the required component affects the customer's decision to purchase from the firm. The second setting studied here involves a dual-channel supply chain, where a manufacturer sells substitutable products directly to the customer and also through an independent retailer. This dissertation finds that the wholesale prices in such a supply chain exhibit a structure in which the wholesale margins weighted by a function of service levels and demand variability must be common across all variants. In addition, this work characterizes scenarios where the manufacturer's and retailer's assortment preferences are in conflict. In particular, this work shows that the manufacturer may prefer the retailer to carry items with high demand variability while the retailer prefers items with low demand variability...|$|R
40|$|In stark {{contrast}} to other agency-based consumer service industries, real estate agents have been remarkably <b>effective</b> at maintaining <b>margins</b> and market share. One {{possible explanation for the}} success of real estate agents is collusion, and consequently the industry is facing regulator scrutiny. In this paper we first consider the mechanisms through which collusion might be sustained. Although the industry’s low concentration makes collusion harder to support, the involvement of both a seller’s agent and a buyer’s agent on a given home sale helps facilitate collusion. Selling agents need buyer’s agents to deliver customers. Not only can fullcommission agents collude against discount/flat-fee agents by steering their buyers away from such listings, they also can use the same punishment on other full-commission agents who cooperate with discount agents. Using data from three markets, we find that houses listed using flat-fee agents have longer expected times-to-sale than observably similar houses sold by fullcommission agents, but ultimately sell for similar prices. These results are consistent with allegations that traditional agents steer clients away from flat-fee-listed homes, although we consider other possible explanations. We calculate that, even taking into account the longer expected time to sale and the increased effort of the homeowner, sellers who use flat-fee agents saved an average of over $ 5, 000 compared to hiring a full-commission agent. This casts doubt on the necessity of minimum-service laws that exist in several states and are being considered in others. I...|$|R
40|$|Suggested Bibliographic Reference: Challenging New Frontiers in the Global Seafood Sector: Proceedings of the Eighteenth Biennial Conference of the International Institute of Fisheries Economics and Trade, July 11 - 15, 2016. Compiled by Stefani J. Evers and Ann L. Shriver. International Institute of Fisheries Economics and Trade (IIFET), Corvallis, 2016. Proceedings of the Eighteenth Biennial Conference of the International Institute of Fisheries Economics and Trade, held July 11 - 15, 2016 at Aberdeen Exhibition and Conference Center (AECC), Aberdeen, Scotland, UK. Economic {{assessment}} data {{in agricultural}} systems at farm level require a substantial methodology {{in order to}} assure result reliability. This paper aims {{to evaluate the effectiveness}} of a participatory approach which is being used by the Brazilian Agricultural Research Corporation (Embrapa) in aquaculture. This methodology consists in gathering technical parameters, production costs and other economic information by means of panel method with producers. Variables are obtained through consensus among participants of the panel by using the criteria of the most frequent features practiced in farms (modal producer). Later, data is updated in a regular basis by calling input suppliers, fish farmers and wholesalers in order to analyze price variation. The information resulting is spread via newsletters on website and consists of: costs of production, analysis of economic viability (e. g. net <b>margins,</b> <b>effective</b> operative cost), inputs and fish price index. As positive aspects, the methodology shows a strong reliability of data because information is directly provided by a representative sample of producers. Moreover, data collection presents a low cost compared to individual visits to producers or to surveys method. Continuous updating of database and the high level of participation of producers are other assets of this methodology. Despite their effectiveness, the method presents some challenges as: (a) Heterogeneity of producer's profile and consequent difficulty in standardizing data; (b) Logistic requirements related to team travel and organization of panels; (c) Producer's mobilization in order to assure their participation on the panels...|$|R
40|$|Correlation {{techniques}} for {{the identification of}} nonlinear systems are discussed in Chapter 1. The Volterra series expansion of the response of a nonlinear system is described, together with its counterpart in the frequency domain. Qosscorrelation methods for identifying the kernel functions which occur in this expansion are reviewed, with particular emphasis on techniques or obtaining the linear approximant to a nonlinear system. A crosscorrelation method is also discussed {{which appears to be}} unrelated to the Volterra approach. This technique uses a 4 -level test signal and is the subject of detailed analysis in later chapters. The 4 -level test signal is discussed in detail in Chapter 2 and it is shown that the linear channel of a nonlinear system may be identified. The concept of the almost periodic function is presented and it is concluded that by means of almost periodic fi. ncti&ns it is possible to identify the linear portion of the nonlinear channel in the system. The technique is essentially based on destroying the ynchronisation of the two 2 -level signals originally used to produce the 4 -level signal. A generalised technique is developed to calculate the parameters of the 4 -level aperiodic signal in order to app],y the method to any single-valued nonlinearity, assuming that 'a priori' knowledge of the nonlinearity is available. The technique is extended to permit identification of the impulse response of the linear elements when the nonlinearity is single valued but contains only even components. This is achieved by modifying the nonlinear characteristic to provide an odd component. A further extension of the technique shows that identification of certain nonlinear channels containing elements with memory is possible. Several situations are analysed in detail. An extension is also considered where the nonlinear channel is composed of two linear transfer functions separated by a nonlinearity. Experimental results are included to show the accuracy of the techniques developed. In Chapter 4 the application of high frequency signals (dither) as a technique {{to be used in the}} identification of certain open and pl,osed loop nonlinear systems is discussed. It is shown that certin types of dither permit linearization or elimination of nonlinear channels in certain open and closed systems. It is therefore possible, under these conditions, to identify the linear portions by impulse response or frequency testing techniques, and experimental results show the accuracy achieved. The concept of the equivalent nonlinearity provides a simple interpretation of the action of the dither. Dither is next considered in its role as a means of stabilizing nonlinear control systems, and the particular case of a third order system containing a hysteresis type relay characteristic is analysed in detail. The concepts of equivalent nonlinearity and describing function are used to derive an expression for the minimum amplitude of dither required to quench limit cycle oscillations in the system. The system is shown to hAve an <b>effective</b> gain <b>margin</b> once oscillations have been quenched. A typical point in this region is investigated and the three dimensional domain of stability for the system is presented,together with typical trajectories. The system is also shown to exhibit subharmonic resonance apci jump phenomena. In chapter 6 the use of dither in adaptive control systems is discussed, and the specific case of dither adaptive control system proposed as the solution to a qonstant fuel-rate problem is analysed in detail. The dynamic analysis presented divides the system into two loops via an extension of the equivalent nonlinearity concept. The final chapter reviews the work described in the body of the thesis, and explores several avenues for future research...|$|R
40|$|One of the {{challenges}} facing clinical practice today is intra-operative margin detection in breast conserving surgeries (BCS) or lumpectomy procedures. When a surgeon removes a breast tumor from a patient during a BCS procedure, the surgically excised tissue specimen is examined to see whether it contains a margin of healthy tissue around the tumor. A healthy margin of tissue around the tumor would indicate that the tumor in its entirety has been removed. On the other hand, if cancerous tissue is at {{the surface of the}} specimen, that would indicate that the tumor may have been transected during the procedure, leaving some residual cancerous tissue inside the patient. The most <b>effective</b> intra-operative real-time <b>margin</b> detection techniques currently used in clinical practice are frozen section analysis (FSA) and touch-prep cytology. These methods have been shown to possess inconsistent accuracy, which result in 20 % to 30 % of BCS patients being called back for a repeat BCS procedure to remove the residual tumor tissue. In addition these techniques {{have been shown to be}} time-consuming [...] requiring the operating room team to have to wait at least 20 minutes for the results. Therefore, there is a need for accurate and faster technology for intra-operative margin detection. In this dissertation, we describe an x-ray coherent scatter imaging technique for intra-operative margin detection with greater accuracy and speed than currently available techniques. The method is based on cross-sectional imaging of the differential coherent scatter cross section in the sample. We first develop and validate a Monte Carlo simulation of coherent scattering. Then we use that simulation to design and test coherent scatter computed tomography (CSCT) and coded aperture coherent scatter spectral imaging (CACSSI) for cancerous voxel detection and for intra-operative margin detection using (virtual) clinical trials. Finally, we experimentally implement a CACSSI system and determine its accuracy in cancer detection using tissue histology. We find that CSCT and CACSSI are able to accurately detect cancerous voxels inside of breast tissue specimens and accurately perform intra-operative margin detection. Specifically, for the task of individual cancerous voxel detection, we show that CSCT and CACSSI have AUC values of 0. 97 and 0. 94, respectively. Whereas for the task of intra-operative margin detection, the results of our virtual clinical trials show that CSCT and CACSSI have AUC values of 0. 975 and 0. 741, respectively. The gap in spatial resolution between CSCT and CACSSI affects the results of intra-operative margin detection much more than it does the task of individual cancerous voxel detection. Finally, we also show that CSCT would require on the order of 30 minutes to create a 3 D image of a breast cancer specimen, whereas CACSSI would require on the order of 3 minutes. These results of this work show that coherent scatter imaging has the potential to provide more accurate intra-operative margin detection than currently used clinical techniques. In addition, the speed (and therefore low scan duration: 3 min) of CACSSI, along with its ability to automatically classify cancerous tissue for margin detection means that coherent scatter imaging would be much more cost-effective than the clinical techniques that require up to 20 minutes and a trained pathologist. With the cancerous voxel detection accuracy of a 0. 94 AUC and scan time of on the order of 3 minutes demonstrated for coherent scatter imaging in this work, coherent scatter imaging has the potential to reduce healthcare costs for BCS procedures and rates of repeat BCS surgeries. The accuracy for CACSSI can be considerably improved to match CSCT accuracy by improving its spatial resolution through a number of techniques: incorporating into the CACSSI reconstruction algorithm the ability to differentiate noise from high frequency signal so that we can image with higher frequency coded aperture masks; implementing a 2 D coded aperture mask with a 2 D detector; or acquiring additional angles of projection data. Dissertatio...|$|R

