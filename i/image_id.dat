14|46|Public
5000|$|... 0-255The {{number of}} bytes that the <b>image</b> <b>ID</b> field {{consists}} of.The <b>image</b> <b>ID</b> field can contain any information, but {{it is common for}} it to contain the date and time the image was created or a serial number.|$|E
5000|$|... #Caption: Portrait of Myles Horton, {{founder of}} Highlander Folk School. Photographer Unknown. WHS <b>Image</b> <b>ID</b> 52275 http://www.wisconsinhistory.org/whi/fullRecord.asp?id=52275 ...|$|E
50|$|Zetes {{implements}} {{automatic identification}} and data capture systems for different {{steps in the}} supply chain: manufacturing, transport, logistics, warehousing en retail. Zetes’ implementations are based on auto id technologies: bar codes, RFID, printing, print & apply, voice recognition, <b>image</b> <b>ID.</b>|$|E
40|$|Abstract. Let W be a Weyl group, {{presented}} as a reflection group on a Euclidean vector space V, and C ⊂ V an open Weyl chamber. In a recent paper, Waldspurger proved that the <b>images</b> (<b>id</b> −w) (C) for w ∈ W are all disjoint, with union the closed cone spanned by the positive roots. We prove that similarly, the <b>images</b> (<b>id</b> −w) (A) of the open Weyl alcove A, for w ∈ W a in the affine Weyl group, are disjoint and their union is V. 1...|$|R
40|$|New {{features}} Add pairwise speedups, including {{plots and}} tables Add option to remove slowest and fastest observations Fixes Fix tests for MATLAB >= 2016 Improve Octave compatibility Other changes Add startup script for initializing toolbox Time parsing function {{is defined in}} global variable Update documentation and tests according to new features and changes Move user manual to separate page Functions set <b>image</b> <b>IDs</b> to NaN when plots not requeste...|$|R
40|$|Digital <b>ID</b> <b>images</b> {{are getting}} wide use in our social life. The {{automatic}} {{evaluation of the}} images' quality is an important technique in the applications. An evaluation method on the content-related quality of <b>ID</b> <b>images</b> is introduced in this paper. The important attributes and their evaluating methods are analyzed {{in accordance with the}} ISO/IEC and ICAO standards. Considering the special features and restrictions of <b>ID</b> <b>images,</b> a simplified fast facial features extraction algorithm is proposed. The attributes related with the content of face image are then evaluated based on the information yielded from feature extraction results...|$|R
50|$|Upon {{searching for}} images (with a gene name, PubMed ID, <b>image</b> <b>ID</b> or certain keywords), {{a series of}} GEMs are {{displayed}} from the two databases of the gene’s inclusion in descending stages and views of the embryo. Various details, such as the source and experimental protocol of the embryo, are also shown. All expression patterns reflect the wild-type allele for the gene.|$|E
30|$|At {{the cloud}} service level, SIR often relies on {{collecting}} all available cloud service’s settings and instances (e.g. get available regions and list instances). At the CSI level, {{it is usually}} to retrieve the diverse real-time information of a certain service instance (e.g. get instance status). At the PSSA level, it tends to acquire the real-time information of the specific CSP entities (e.g. get VM <b>image</b> <b>ID).</b> Generally, SIR operations would not alter any cloud service, CSI or PSSA after execution.|$|E
30|$|We {{found that}} the ability to {{discriminate}} material was based on the degree of neuronal excitation. For example, a neuron might have been excited for metal but not for plastic. To judge the discrimination ability of the D-CNN model, we observed the distributions of the different neurons’ excitations. The higher the value of a certain neuron’s excitation compared to others, the stronger the ability to discriminate a certain material. For example, according to the red circles in Fig.  10, the values of Neuron 3 were better than the values of Neuron 1 and Neuron 2 for concrete images (<b>image</b> <b>ID</b> 341 – 440). Thus, the AFM model had stronger ability to discriminate concrete. According to the green circles in Fig.  10, compared with the AM model and the AMF model, Neuron 1 of the IAMF model had better excitation values than the other neurons for metal images (<b>image</b> <b>ID</b> 1 – 105). As a result, the IAMF model had stronger discrimination ability for metal than other models. Besides, the IAMF model had a more concentrated neuron excitation distribution, indicating that the IAMF model had a more stable discrimination ability for FOD material. Therefore, the discrimination abilities of the three D-CNN models for FOD gradually increased from the AM model to the IAMF model. The result also confirmed the effectiveness of the IAMF model.|$|E
5000|$|Multiple laser <b>image</b> of photo, <b>ID</b> number, sex {{and date}} of birth) ...|$|R
5000|$|George Weller was {{actually}} photographed with a prominent naval figure Hiram Cassidysic by the Associated Press on March 25, 1943 AP <b>Images</b> <b>ID</b> 4303250106. The {{picture was taken}} just before Weller's letter to the Foreign Editor of the Chicago Daily News quoted in Weller's War pp355-6. At left is Commander Hiram Cassedyspelling, then Captain of Searaven, hero of several submarine engagements in South-west Pacific (Submarine Operations in World War II US Naval Institute Eight Printing December 1965 pages 104,193-4,233,463). At right is George Weller wearing British-type tropical uniform of shorts and knee socks. Cassedy (later Admiral) commanding Tigrone went on to lead a submarine group [...] "Hiram's Hecklers" [...] and also to hold the Pacific record for [...] "lifeguard" [...] duty (with 31 rescues; Ibid. pp 471-3).|$|R
50|$|The {{following}} 08 04 81 00 07 e5 {{means that}} the boot <b>image</b> with the <b>ID</b> 2164262885 is selected.|$|R
40|$|Given a {{pairwise}} dissimilarity matrix D {{of a set}} of n objects, visual methods (such as VAT) for cluster tendency assessment generally represent D as an n x n <b>image</b> <b>I((D)</b> over tilde) {{where the}} objects are reordered to reveal hidden cluster structure as dark blocks along the diagonal of the image. A major limitation of such methods is the inability to high-light cluster structure in I((D) over tilde) when D contains highly complex clusters. To address this problem, this paper proposes an improved VAT (iVAT) method by combining a path-based distance transform with VAT. In addition, an automated VAT (aVAT) method is also proposed to automatically determine the number of clusters from I((D) over tilde). Experimental results on several synthetic and real-world data sets have demonstrated the effectiveness of our methods...|$|E
40|$|Inspired by multi-resolution histogram, {{we propose}} a multi-scale SIFT {{descriptor}} {{to improve the}} discriminability. A series of SIFT descriptions with different scale are ﬁrst acquired by varying the actual size of each spatial bin. Then principle component analysis (PCA) is employed to reduce them to low dimensional vectors, which are further combined into one 128 -dimension multi-scale SIFT description. Next, an entropy maximization based binarization is employed to encode the descriptions into binary codes called ﬁngerprints for indexing the local features. Furthermore, an efﬁcient search architecture consisting of lookup tables and inverted <b>image</b> <b>ID</b> list is designed to improve the query speed. Since the ﬁngerprint building is of low-complexity, this method is very efﬁcient and scalable to very large databases. In addition, the multi-scale ﬁngerprints are very discriminative such that the copies can be effectively distinguished from similar objects, which leads to an improved performance in the detection of copies. The experimental evaluation shows that our approach outperforms {{the state of the}} art methods. Inspired by multi-resolution histogram, we propose a multi-scale SIFT descriptor to improve the discriminability. A series of SIFT descriptions with different scale are first acquired by varying the actual size of each spatial bin. Then principle component analysis (PCA) is employed to reduce them to low dimensional vectors, which are further combined into one 128 -dimension multi-scale SIFT description. Next, an entropy maximization based binarization is employed to encode the descriptions into binary codes called fingerprints for indexing the local features. Furthermore, an efficient search architecture consisting of lookup tables and inverted <b>image</b> <b>ID</b> list is designed to improve the query speed. Since the fingerprint building is of low-complexity, this method is very efficient and scalable to very large databases. In addition, the multi-scale fingerprints are very discriminative such that the copies can be effectively distinguished from similar objects, which leads to an improved performance in the detection of copies. The experimental evaluation shows that our approach outperforms {{the state of the art}} methods...|$|E
40|$|Detailed {{experimental}} procedures Constructs containing putative human mitochondrial release factors Clones for the 2 putative mitochondrial release {{factors were}} obtained from MGC; mtRF 1 (<b>IMAGE</b> <b>id</b> 5198308, accession number BC 042196) and mtRF 1 a (3029407, accession number BC 011873). These were used as templates in PCR amplifications to generate the constructs below. Production of GFP- and GST-fusion constructs and cloning into yeast expression vectors All primers used below included BamH 1 or Not 1 cleavage sites (underlined). An amplicon incorporating nt 89 to 972 of the cDNA encoding mtRF 1 was formed using primers RF 1 -GFPfor- 5 ’-CTCTCTGGATCCTTAGATGCTGAGATGAATCG- 3 ’ and RF 1 -GFPrev 5 ’-CTCTCTGGATCCTCATCTGGCTGAGGAAG- 3 ’ in a standard PCR reaction with a 55 ˚Cannealing temperature. Similarly, an amplicon incorporating nt 12 to 1099 of the cDNA encoding mtRF 1 a was PCR generated using primers RF 1 a-GFPfor- 5 ’CTCTCTGGATCCGATCTCGGACTAAGGATG- 3 ’ and RF 1 a-GFPrev 5 ’...|$|E
5000|$|Introduce station <b>ID</b> <b>image</b> songs - station {{identification}} jingles {{of more than}} :60 seconds that sounded like the hits songs that the stations played, in 1972-1973. Later known in the industry as the “mini-song” station ID - a widely imitated concept; and ...|$|R
30|$|Industrial systems: There exist several {{tools and}} systems offered by modern cloud {{providers}} to their users that aim at providing fully automated application deployment. In the Openstack ecosystem, Heat [12] is an orchestration system which aims at managing an application throughout its lifecycle. The users submit application descriptions (named HOT, i.e., Heat Orchestration Template), where they define: The application modules, the resources each module will occupy, the dependencies between different modules (if any) {{and any other}} runtime parameter (e.g., name of the SSH key to use, flavor id, etc.). In order to maximize reusability, Heat templates can be parameterized, abstracting the installation-specific details (e.g., image/flavor IDs, key names, etc.) from the application description. Sahara [13] is a different deployment tool for Openstack that specializes in provisioning Data Processing clusters, i.e., Hadoop and Spark clusters. Sahara uses Heat as the deployment engine and differs from it as it enables the users to provide Hadoop-specific deployment parameters (e.g., HDFS replication size, number of slaves, etc.) and applies them to the cluster. The AWS counterpart of Openstack Heat is the AWS CloudFormation [15]. Similar to Heat, CloudFormation receives templates that describe what resources will be utilized and by which components. The cloud user can then define a set of parameters (keys, <b>image</b> <b>ids,</b> etc.) and launch the deployment. AWS Elastic Beanstalk [23] specializes in deploying web applications to the AWS cloud, hiding the infrastructure internals and automating the provisioning of load balancers, application-level monitoring, etc.|$|R
40|$|Security {{checking}} can {{be improved}} by watermarking identity (<b>ID)</b> <b>images</b> printed on smart cards plastic supports. The major challenge is resistance to attacks: printing the images on the plastic cards, durability and other attacks then scanning the image from the plastic card. In this work, a robust watermarking technique is presented in this context. It is composed of three main mechanisms. The first is a watermarking algorithm based on the Fourier transform to cope with global geometric distortions. The second comprises a filter that reduces image blurring. The third attenuates color degradations. Experiments on 400 <b>ID</b> <b>images</b> show that the Wiener filter strongly improves the detection rate and outperforms competitive algorithms (blind deconvolution and unsharp filter). Color corrections also enhance the watermarking score. The whole scheme has a high efficiency and a low computational cost. It makes {{it compatible with the}} desired industrial constraints, i. e. the watermark is to be invisible, the error rate must be lower than 1 %, and the detection of the mark should be fast and simple for the user...|$|R
40|$|Image Authentication {{technique}} {{is useful for}} user protection from fraud login. Authentication will find the best matching image from a database and return respective <b>image</b> <b>ID</b> with respect to Login ID used in single sign on. In this paper we will discussed feature extraction of fingerprint image using canny edge detection and perwit edge detection. Feature Similarity Indexing of image algorithm is used to generate the matching score between the original image in database and the input test image. The experimental results achieve recognition accuracy using canny and perwit FSIM of 96. 77 % and 97. 16 %, respectively, on the publicly available database of Hong Kong Polytechnic University. Totally 50 images of 10 individuals, 4 samples for each palm are randomly selected to train in this research. Then we get every person each palm image as a template (total 10). Experimental evaluation using palmprint image database clearly demonstrates the efficient recognition performance of the proposed algorithm using Perwit FSIM gives best result {{when compared with the}} Canny FSIM algorithm...|$|E
40|$|In this study, {{we propose}} a novel scene {{descriptor}} for visual place recognition. Unlike popular bag-of-words scene descriptors which {{rely on a}} library of vector quantized visual features, our proposed descriptor {{is based on a}} library of raw image data, such as publicly available photo collections from Google StreetView and Flickr. The library images need not to be associated with spatial information regarding the viewpoint and orientation of the scene. As a result, these images are cheaper than the database images; in addition, they are readily available. Our proposed descriptor directly mines the image library to discover landmarks (i. e., image patches) that suitably match an input query/database image. The discovered landmarks are then compactly described by their pose and shape (i. e., library <b>image</b> <b>ID,</b> bounding boxes) and used as a compact discriminative scene descriptor for the input image. We evaluate the effectiveness of our scene description framework by comparing its performance to that of previous approaches. Comment: 8 pages, 6 figures, preprint. Accepted for publication in MVA 2015 (oral presentation...|$|E
40|$|The People in Social Context (PISC) dataset {{is a new}} dataset {{that focuses}} on social relationships. It {{consists}} of 22, 670 images of 9 types of social relationships. We provide annotation of the bounding boxes of all people, {{as well as the}} social relationship between all pairs of people in the images. In addition, we provide occupation annotation. For more details on the collection process and statistics of the dataset, please see our paper. 	annotation_image_info. json contains bounding box annotation and information about image source, image size, <b>image</b> <b>id.</b> 	domain. json contains annotation of the 3 types of coarse relationships: {intimate, not intimate, no relation}. 	relationship. json contains annotation of the 6 types of fine relationships: {friends, family, couple, professional, commercial, no relation}. 	occupation. json contains annotation of the occupation. 	domain_split and relation_split contain train/val/test split. 	after downloading all image parts (images-*), extract using: cat images-* | tar zx The dataset can be applied, but not limited to the following research areas: 	social relationship study 	people detection 	occupation recognition Please cite the following paper if you use the PISC dataset in your work (papers, articles, reports, books, software, etc) : 	J. Li, Y. Wong, Q. Zhao, M. Kankanhalli 	Dual-Glance Model for Deciphering Social Relationships 	ICCV, 2017. 	[URL]...|$|E
40|$|This paper proposes {{an image}} {{watermarking}} scheme {{that cater to}} multiple purposes of copyright protection and fingerprinting. For fingerprinting <b>images,</b> an <b>ID</b> number or a unique code pertaining to the buyer is hidden with in the digital resources {{at the time of}} resource transfer. For copyright protection of the resource, a unique signature such as a logo of the owner creator is integrated imperceptibly. In the proposed method these embedding operations are performed in the transform domain using discrete wavelet transform. The embedding algorithm works in two stages; hides the fingerprint using DWT inside the logo in the first stage and in second stage, integrates this secondary watermark in the base image to be watermarked. Keywords Digital images; watermarking; copyright protection; fingerprinting; discrete wavelet transforms (DWT) 1...|$|R
50|$|When it {{has decided}} on a correct address, it prints the {{appropriate}} bar code onto the envelopes, similarly to the MLOCR system. RBCS also has facilities in place, called Remote Encoding Centers, that have humans look at images of mail pieces and enter the address data. The address data {{is associated with the}} <b>image</b> via an <b>ID</b> Tag, a fluorescent barcode printed by mail processing equipment on the back of mail pieces.|$|R
40|$|Abstract—Medical Imaging is {{currently}} a hot area of bio-medical engineers, researchers and medical doctors as it is extensively used in diagnosing of human health and by health care institutes. The imaging equipment is the device, which is used for better image processing and highlighting the important features. These images are affected by random noise during acquisition, analyzing and transmission process. This condition results in the blurry image visible in low contrast. The <b>Image</b> De-noising System (<b>IDs)</b> {{is used as a}} tool for removing image noise and preserving important data. Image de-noising {{is one of the most}} interesting research areas among researchers of technology-giants and academic institutions. For Criminal Identification Systems (CIS) & Magnetic Resonance <b>Imaging</b> (MRI), <b>IDs</b> is more beneficial in the field of medical imaging. This paper proposes an algorithm for de-noising medical images using different types of wavelet transform, such as Haar, Daubechies, Symlets and Bi-orthogonal. In this paper noise image quality has been evaluated using filter assessment parameters like Peak Signal to Noise Ratio (PSNR), Mean Square Error (MSE) and Variance, It has been observed to form the numerical results that, the presentation of proposed algorithm reduced the mean square error and achieved best value of peak signal to noise ratio (PSNR). In this paper, the wavelet based de-noising algorithm has been investigated on medical images along with threshold. Keywords—Image De-noising System; GUI De-noised image; Code De-noised image; Wavelet transform; Soft and Har...|$|R
40|$|Abstract A photobleached-fluorescence imaging tech-nique for {{visualizing}} microscale flow {{fields and}} obtaining molecular diffusion and advection {{information has been}} developed. The technique tracks fluorophores in the re-gion of a photobleached line in a planar microdevice and yields quantitative diffusive and advective transport data. Visualizations of two- and weakly three-dimensional electroosmotically and pressure-driven fluid flow fields are demonstrated using the photobleaching of fluorescein and fluorescein-dextran conjugates. Photobleached-fluores-cence imaging tracks undisturbed fluorophores, functions in polymer and glass microfluidic devices, can take ad-vantage of fluorescent conjugates present in biochemical assays, and has a photobleached region that is flow inde-pendent. List of symbols az capillary depth ay capillary width bx beam width in x at the beam waist by beam width in y at the beam waist c fluorophore concentration dz collimated beam dimension e electron charge E electric field f focal length I pixel intensity Icorr corrected image Idf darkfield image Iff flatfield image (uniform illumination on a uniformly seeded field) IL laser intensity of beam exiting the microscope objective Iraw raw <b>image</b> <b>ID</b> inner diameter k thermal conductivity of water M molarity (moles/liter) NA Avogadro’s number NA numerical aperture OD outer diameter _q 000 rate of heat generation per unit volume QE quantum efficiency R universal gas constant T temperature tb duration of photobleaching pulse u electroosmotic velocity UAPP apparent velocity of photobleached zone UEOF electroosmotic velocity of tracer UEP electrophoretic velocity of tracer z valence number Greek symbols a thermal diffusivity of water permittivity of water k argon ion laser wavelength l dynamic viscosity of water m ionic mobility x molar absorptivity rL laser half-width v area fraction of laser sheet incident on fluid f zeta potential...|$|E
40|$|This {{document}} {{describes the}} WEBUPV dataset compiled for the ImageCLEF 2013 Scalable Concept Image Annotation task. The data mentioned here indicates what {{is ready for}} download. However, upon request or depending on feedback from the participants, additional data may be released. The following is the directory structure of the collection, and bellow there is {{a brief description of}} what each compressed file contains. The corresponding MD 5 checksums of the files shown (for verifying a correct download) can be found in md 5 sums. txt. Any publication in which this data has been used is required to cite the following paper: @inproceedingsVillegas 13 _CLEF, author = Mauricio Villegas and Roberto Paredes and Bart Thomee, title = Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask, booktitle = CLEF 2013 Evaluation Labs and Workshop, Online Working Notes, year = 2013, month = September 23 - 26, address = Valencia, Spain, isbn = 978 - 88 - 904810 - 5 - 5, issn = 2038 - 4963, If the 'hsvcolorhist' and/or the 'lbpcenter' visual features are used, then it is also required to cite: @inproceedingsSanchezOro 13 _CLEF, author = Jesús Sánchez-Oro and Soto Montalvo and Antonio S. Montemayor and Juan J. Pant rigo and Abraham Duarte and Víctor Fresno and Raquel Martínez, title = URJC&UNED at ImageCLEF 2013 Photo Annotation Task, booktitle = CLEF 2013 Evaluation Labs and Workshop, Online Working Notes, year = 2013, month = September 23 - 26, address = Valencia, Spain, isbn = 978 - 88 - 904810 - 5 - 5, issn = 2038 - 4963, Directory structure [...] -. | | [...] - README. txt | [...] - md 5 sums. txt | [...] - webupv 13 _train_lists. zip | [...] - webupv 13 _devel_lists. zip | [...] - webupv 13 _test_lists. zip | [...] - webupv 13 _baseline. zip | | [...] - feats_textual/ | | | | [...] - webupv 13 _train_textual_pages. zip | | [...] - webupv 13 _train_textual. scofeat. gz | | [...] - webupv 13 _train_textual. keywords. gz | | [...] - feats_visual/ | | [...] - webupv 13 _train|devel|test_visual_images. zip | [...] - webupv 13 _train|devel|test_visual_gist. feat. gz | [...] - webupv 13 _train|devel|test_visual_sift_ 1000. feat. gz | [...] - webupv 13 _train|devel|test_visual_csift_ 1000. feat. gz | [...] - webupv 13 _train|devel|test_visual_rgbsift_ 1000. feat. gz | [...] - webupv 13 _train|devel|test_visual_opponentsift_ 1000. feat. gz | [...] - webupv 13 _train|devel|test_visual_colorhist. feat. gz | [...] - webupv 13 _train|devel|test_visual_getlf. feat. gz | [...] - webupv 13 _train|devel|test_visual_hsvcolorhist. feat. gz | [...] - webupv 13 _train|devel|test_visual_lbpcenter. feat. gz Contents of files [...] - * webupv 13 _train_lists. zip -> train_iids. txt : IDs of the images (IIDs) in the training set (250000). -> train_rids. txt : IDs of the webpages (RIDs) in the training set (262526). -> train_*urls. txt : The original URLs from where the images (iurls) and the webpages (rurls) were downloaded. Each line in the file corresponds to an image, starting with the IID and is followed by one or more URLs. -> train_rimgsrc. txt : The URLs of the images as referenced in each of the webpages. Each line of the file is of the form: IID RID URL 1 [URL 2 [...] . ]. This information is necessary to locate the images within the webpages and it can also be useful as a textual feature. * webupv 13 _devel_lists. zip -> devel_iids. txt : IDs of the images in the development set (1000). -> devel_*urls. txt : The original URLs from where the images (iurls) and the webpages (rurls) were downloaded. Each line in the file corresponds to an image, starting with the IID and is followed by one or more URLs. Note: These are included only to acknowledge the source of the data, not be used as input to the annotation systems. -> devel_concepts. txt : List concepts for the development set. -> devel_gnd. txt : Ground truth concepts for the development set images. The concepts are defined by one or more WordNet synsets, which is intended to make it possible to easily obtain more information about the concepts, e. g. synonyms. In the concept list, the first column (which {{is the name of the}} concept) indicates the word to search in WordNet, the second column the synset type (either noun or adjective), the third column is the sense number and the fourth column is the WordNet offset (although this cannot be trusted since it changes between WordNet versions). For most of the concepts there is a fifth column which is a Wikipedia article related to the concept. * webupv 13 _test_lists. zip -> test_iids. txt : IDs of the images in the test set (2000). -> test_*urls. txt : The original URLs from where the images (iurls) and the webpages (rurls) were downloaded. Each line in the file corresponds to an image, starting with the IID and is followed by one or more URLs. Note: These are included only to acknowledge the source of the data, not be used as input to the annotation systems. -> test_concepts. txt : List concepts for the test set. -> test_gnd. txt : Ground truth concepts for the test set images. The definition of the concepts is the same as for devel_concepts. txt. Note that the concepts are not the same as for the development set. * webupv 13 _baseline. zip An archive that includes code for computing the evaluation measures for two baseline techniques. See the included README. txt for details. * feats_textual/webupv 13 _train_textual_pages. zip Contains all of the webpages which referenced the images in the training set after being converted to valid xml. In total there are 262588 files, since each image can appear in more than one page, and there can be several versions of same page which differ by the method of conversion to xml. To avoid having too many files in a single directory (which is an issue for some types of partitions), the files are found in subdirectories named using the first two characters of the RID, thus the paths of the files after extraction are of the form:. /WEBUPV/pages/RID: 0 : 2 /RID. CONVM. xml. gz To be able to locate the training images withing the webpages, the URLs of the images as referenced are provided in the file train_rimgsrc. txt. * feats_textual/webupv 13 _train_textual. scofeat. gz The processed text extracted from the webpages near where the images appeared. Each line corresponds to one image, having the same order as the train_iids. txt list. The lines start with the <b>image</b> <b>ID,</b> followed by the number of extracted unique words and the corresponding word-score pairs. The scores were derived taking into account 1) the term frequency (TF), 2) the document object model (DOM) attributes, and 3) the word distance to the image. The scores are all integers and for each image the sum of scores is always x 240 >' IMGOUT. jpg Then the SIFT features where extracted using the ColorDescriptor software from Koen van de Sande ([URL] As configuration we used, 'densesampling' detector with default parameters, and a hard assignment codebook using a spatial pyramid as 'pyramid- 1 x 1 - 2 x 2 '. The number in the file name indicates the size of the codebook. All of the vectors of the spatial pyramid are given in the same line, thus keeping only the first 1 / 5 th of the dimensions would be like not using the spatial pyramid. The codebook was generated using 1. 25 million randomly selected features and the k-means algorithm. The GIST features were extracted using the LabelMe Toolbox. The images where first resized to 256 x 256 ignoring original aspect ratio, using 5 scales, 6 orientations and 4 blocks. The other features colorhist and getlf, are both color histogram based extracted using our own implementation...|$|E
30|$|Tampere Image Database 2013 (TID 2013) [61]: The {{database}} has 25 reference {{images and}} 3, 000 distorted images (25 reference images ×[*] 24 types of distortions ×[*] 5 levels of distortions). These images are in 512 ×[*] 384 ×[*] 24 uncompressed BMP format. The distortions include additive Gaussian noise (AGN), additive noise in color components (ANCC), spatially correlated noise (SCN), masked noise (MN), {{high frequency noise}} (HFN), impulse noise (IN), quantization noise (QN), Gaussian blur (GB), <b>image</b> denoising (<b>ID),</b> JPEG, JPEG 2 k, JPEG transmission errors (JPEG+TE), JPEG 2 k transmission errors (JPEG 2 k+TE), non eccentricity pattern noise (NEPN), local block-wise distortions (LBD), intensity shift (IS), contrast change (CC), change of color saturation (CCS), multiplicative Gaussian noise (MGN), comfort noise (CN), lossy compression (LC), image color quantization with dither (ICQ), chromatic aberration (CA), and sparse sampling and reconstruction (SSR).|$|R
40|$|Besides the {{ownership}} verification, tracking the <b>image</b> copy <b>ID</b> is another requirement in many watermarking applications. Furthermore, keeping the original image from public accessibility during the watermark extraction process is another common demand. In this paper, we propose a new watermarking method called WMica that bases on the independent component analysis (ICA) technique. The proposed method employs a two-watermark embedding scheme; one watermark {{is to identify}} {{the ownership}} and the other serves as the ID for each copy of the original image. In the extraction scheme, an ICA algorithm is applied together with a down-sizing technique {{so that we can}} estimate all the watermarks without accessing the original image and any prior information about the watermarks. The new method, undergoing a variety of experiments, has shown its robustness against many salient attacks. It also exhibits a capability in image authentication...|$|R
40|$|Three {{dimensional}} Identification Card, {{with its}} three-dimensional personal image displayed and stored for personal identification, is supposed be the advanced {{version of the}} present two-dimensional identification card in the future [1]. Three dimensional Identification Card means that there are three-dimensional optical techniques are used, the personal <b>image</b> on <b>ID</b> card is displayed to be three-dimensional, {{so we can see}} three dimensional personal face. The ID card also stores the three-dimensional face information in its inside electronics chip, which might be recorded by using two-channel cameras, and it can be displayed in computer as three-dimensional images for personal identification. Three-dimensional ID card might be one interesting direction to update the present two-dimensional card in the future. Three-dimension ID card might be widely used in airport custom, entrance of hotel, school, university, as passport for on-line banking, registration of on-line game, etc [...] ...|$|R
5000|$|AU10TIX {{has been}} founded in 2006 as the {{technology}} arm of ICTS International and later diversified into commercial and civil markets. AU10TIX is the technology subsidiary of ICTS international, a Dutch firm that provides solutions and services {{in the field of}} aviation and general security. ICTS International shares are traded on OTCQB under the symbol ICTSF. AU10TIX online ID authentication technology platform has been first to introduce a completely automated, machine-learning enhanced process [...] that includes <b>ID</b> <b>image</b> auto-classification, multi-factor image manipulation detection and collateral risk factor analytics.|$|R
40|$|In this {{research}} work different facial expressions and poses of individual person faces are detected {{and stored in}} voter database by giving appropriate aadhar card id number. If a person comes for a vote then his or her face is detected and this detected face image is compared with images in voter database and aadhar card id number. If the face <b>image</b> and <b>id</b> number are recognized then person is allowed to cast the vote. If it is not recognized then person {{is not allowed to}} vote. After the successful voting process, number of votes to the particular candidate and the party will be counted. This paper illustrates the Haar like features for face detection and eigenface algorithm for face recognition. eigenface method {{is one of the most}} basic and efficient methods for face recognition. This paper also shows that if the minimum Euclidian distance from other images of the test image is zero, then the test image absolutely matches the existing image in the database...|$|R
40|$|This paper {{deals with}} the study of {{analysis}} and accuracy prediction with regard to multi-temporal satellite image classification process (MTSICP). The main objective {{of this paper is}} to assess classification accuracy of temporal images of two different seasons (dry & wet Season) using LISS III IRS <b>ID</b> <b>images</b> of H D Kote taluk. It is found that Maximum Likelihood Classifier (MLC) provides better accuracy when compared to Minimum Distance Classifier (MDC) and MaHalanobis Distance Classifier (MHDC) in all the cases. The Kappa and Overall accuracy of MLC is much better compared to MHDC and MDC. Â© 2010 ACM...|$|R
40|$|Many land-cover change {{detection}} {{techniques have}} been developed; however, different {{conclusions about the}} value or appropriateness of each exist. This difference of opinion is often influenced by the landscape complexity of study areas and data used for analysis. Which method is most suitable for land-cover change detection in Amazon tropical regions remains unclear. ln this paper, 10 binary change detection methods were implemented and compared {{with respect to their}} capability to detect land-cover change and no change conditions in moist tropical regions. They are <b>image</b> differencing (<b>ID),</b> modified <b>image</b> differencing (MID), a combination of image differencing and principal component analysis (IDPCA), principal component differencing (PCD), multitemporal PCA (MPCA), change vector analysis (CVA), vegetation index differencing (VID), image ratioing (IR), modified image ratioing (MIR), and a combination of image ratioing and PCA (lRPCA). Multi-temporal Thematic Mapper (TM) data were used to conduct land-cover binary change detection. Research results indicate that MID, PCD and ID using TM band 5 are significant 1 y better than other binary change detection methods and they are recommended specifically for implementation in the Amazon basin. 200...|$|R
40|$|AbstractPurposeThe normal disc {{configuration}} is biconcave; {{however, the}} deformed disc {{can be found}} in internal derangement (ID) of the temporomandibular joint (TMJ). The {{purpose of this study was}} to evaluate the relationship between the transformation patterns of TMJ disc configuration during mouth opening and disc displacement status, disc reduction ability and TMJ clinical symptoms. MethodsTMJ MR images from volunteers and ID patients were analyzed for closed and open mouth positions. MR <b>images</b> of <b>ID</b> patients were classified into 4 groups: symptomatic joint with anterior disc displacement with or without reduction (ADW or ADWo) and asymptomatic joint with ADW or ADWo. The disc configurations in both positions were categorized as biconcave, biplanar, convex or folded; then the patterns of transformation were observed. ResultsFor the ADW group, the most common disc configuration for closed and open mouth positions were biplanar (66. 7 %) and biconcave (86. 3 %). The transformation pattern was significantly related to disc reduction ability (p< 0. 05). ConclusionsThe transformation patterns of disc configuration during opening were related to anterior disc displacement and disc reduction ability...|$|R
40|$|Several flights {{have been}} {{undertaken}} with PAMS (Photogrammetric Aerial Mapping System) by Germap, Germany, which is briefly introduced. This system {{is based on}} the SmartPlane fixed-wing UAV and a CANON IXUS camera system. The plane is equipped with GPS and has an infrared sensor system to estimate attitude values. A software has been developed to link the PAMS output to a standard photogrammetric processing chain built on Trimble INPHO. The linking of the image files and <b>image</b> <b>IDs</b> and the handling of different cases with partly corrupted output have to be solved to generate an INPHO project file. Based on this project file the software packages MATCH-AT, MATCH-T DSM, OrthoMaster and OrthoVista for digital aerial triangulation, DTM/DSM generation and finally digital orthomosaik generation are applied. The focus has been on investigations on how to adapt the "usual" parameters for the digital aerial triangulation and other software to the UAV flight conditions, which are showing high overlaps, large kappa angles and a certain image blur in case of turbulences. It was found, that the selected parameter setup shows a quite stable behaviour and can be applied to other flights. A comparison is made to results from other open source multi-ray matching software to handle the issue of the described flight conditions. Flights over the same area at different times have been compared to each other. The major objective was here to see, on how far differences occur relative to each other, without having access to ground control data, which would have a potential for applications with low requirements on the absolute accuracy. The results show, that there are influences of weather and illumination visible. The "unusual" flight pattern, which shows big time differences for neighbouring strips has an influence on the AT and DTM/DSM generation. The results obtained so far do indicate problems in the stability of the camera calibration. This clearly requests a usage of GCPs for all projects, independent on the application. The effort is estimated to be even higher as expected, as also self-calibration will be an issue to handle a possibly instable camera calibration. To overcome some of the encountered problems with the very specific features of UAV flights a software UAVision was developed based on Open Source libraries to produce input data for bundle adjustment of UAV images by PAMS. The empirical test results show a considerable improvement in the matching of tie points. The results do, however, show that the Open Source bundle adjustment was not applicable to this type of imagery. This still leaves the possibility to use the improved tie point correspondences in the commercial AT package...|$|R
40|$|MeshFace photos {{have been}} widely used in many Chinese {{business}} organizations to protect ID face photos from being misused. The occlusions incurred by random meshes severely degenerate the performance of face verification systems, which raises the MeshFace verification problem between MeshFace and daily photos. Previous methods cast this problem as a typical low-level vision problem, i. e. blind inpainting. They recover perceptually pleasing clear ID photos from MeshFaces by enforcing pixel level similarity between the recovered <b>ID</b> <b>images</b> and the ground-truth clear <b>ID</b> <b>images</b> and then perform face verification on them. Essentially, face verification is conducted on a compact feature space rather than the image pixel space. Therefore, this paper argues that pixel level similarity and feature level similarity jointly offer the key to improve the verification performance. Based on this insight, we offer a novel feature oriented blind face inpainting framework. Specifically, we implement this by establishing a novel DeMeshNet, which consists of three parts. The first part addresses blind inpainting of the MeshFaces by implicitly exploiting extra supervision from the occlusion position to enforce pixel level similarity. The second part explicitly enforces a feature level similarity in the compact feature space, which can explore informative supervision from the feature space to produce better inpainting results for verification. The last part copes with face alignment within the net via a customized spatial transformer module when extracting deep facial features. All the three parts are implemented within an end-to-end network that facilitates efficient optimization. Extensive experiments on two MeshFace datasets demonstrate the effectiveness of the proposed DeMeshNet as well as the insight of this paper. Comment: 10 pages, submitted to CVPR 1...|$|R
