1|11|Public
50|$|Peripheral {{processor}} (PP or PPU) {{instructions are}} {{completely different from}} CPU instructions. Peripheral processor hardware is simpler; it has an 18-bit A (accumulator register, a 12-bit Program Address register, a 12-bit Q register (not programmer-visible), and a 22-bit R register (used to accomplish address relocation during central memory read and write instructions on Cyber 180 systems). No special <b>job</b> <b>validation</b> was required to assemble peripheral processor programs, but to be executed, such programs were required to installed into the operating system via special system editing commands.|$|E
5000|$|... after {{successful}} JCL <b>validation,</b> <b>job</b> {{is moved}} to input queue, waiting for job execution ...|$|R
50|$|He {{manages a}} small {{consulting}} firm (Campion Consulting Services) specializing in human resources topics including personnel selection, <b>job</b> analysis, <b>validation,</b> performance management, and litigation support. He also manages a small recruiting firm (Campion Recruiting Services) placing industrial-organizational psychologists in corporate and consulting positions.|$|R
40|$|AbstractInventory {{decisions}} in supply chain are crucial for its success. These decisions {{become more important}} for the products with expiration date. Making these {{decisions in}} inventory systems with multiple products is a challenging task for man-agers. Most approaches in the literature for optimizing decisions in such an environment consider only a single item inventory. This paper presents a multi item inventory model to optimize the unit time profit of inventory management for the products having an expiration date after which the product can not be sold. As on one side the shortage costs are significant, on the other side, to maintain appropriate inventory levels for such type of products and avoid shortages is a very problematic <b>job.</b> For <b>validation,</b> the model is simulated and the results are compared. This article offers an approach for optimization and thus has business significance...|$|R
40|$|Often, {{organizational}} psychologists use {{methods of}} classification to group individual jobs into larger {{groups that are}} then used for personnel-related functions. In classifying jobs, researchers and practitioners make several important considerations, such as deciding {{on the type of}} data collected and choosing the appropriate quantitative clustering method. We highlight some of these considerations with a dataset comprising three different types of ability data. Complementing previous research, these three ways of measuring ability requirements resulted in substantially different job clusters. Although these differences did not have clear implications for the practical purposes examined – test <b>validation,</b> <b>job</b> evaluation, and career exploration – it was evident that the differences influenced the final results obtained and therefore would influence personnel decisions made from them...|$|R
40|$|It {{is beyond}} dispute that employment-related {{lawsuits}} have proliferated {{in recent years}} (Weisenfeld, 2003). Although {{some have argued that}} even high-profile class action litigation has done little to remediate past discrimination or to deter it in the future (Selmi, 2003), others continue to maintain that the quality of management practices can and should relate directly to {{the success or failure of}} discrimination claims against employers (see, e. g., Schwartz & Moayed, 2001; Thrasher, 2003). It remains an open question, however, whether core personnel functions such as <b>job</b> analysis, <b>validation,</b> or performance appraisal have discernable relationships with the results of employment-related lawsuits. When I last undertook a systematic review of the law in this area (Malos, 1998), I focused just on performance appraisals. Even then, I remarked upon the daunting number of cases in which these and related management practices had become central to the outcome of employment litigation. This past experience and ongoing professional attention to the area should have prepared me for the enormity of the current round of research, but it did not; the exponential explosion in both number and magnitude of recent discrimination cases [...] not to mention some highly publicized multi-million dollar settlements involving the likes of Coca-Cola, Home Depot...|$|R
40|$|Although most {{job shop}} {{scheduling}} problems are concerning dynamic demand and stochastic processing time, {{the majority of}} existing scheduling techniques are based on static demand and deterministic processing time. As a consequence, the solutions obtained from the traditional scheduling technique are ineffective wherever changes occur to the system. The Decision Support Tool (DST) for dynamic job shop scheduling was developed. The components of DST, namely, the knowledge-base, data-base, rule base, and graphical user interface were integrated. It provides alternative recommended schedules to be selected by practitioners with minimum knowledge in <b>job</b> shop scheduling. <b>Validation</b> of DST shows that the performance of DST is promising and able to provide comparable results as when using discrete event simulation. This paper demonstrates the capability of the DST to handling changes in demand, i. e. job cancellation. The demonstration suggests that the proposed DST can effectively handle job cancellation during early, middle or late time instances...|$|R
40|$|The Jobs and Economic Development Impacts (JEDI) models, {{developed}} by the National Renewable Energy Laboratory (NREL) for the U. S. Department of Energy (DOE) Office of Energy Efficiency and Renewable Energy (EERE), use input-output methodology to estimate gross (not net) jobs and economic impacts of building and operating selected types of renewable electricity generation and fuel plants. This analysis provides the DOE with {{an assessment of the}} value, impact, and validity of the JEDI suite of models. While the models produce estimates of jobs, earnings, and economic output, this analysis focuses only on <b>jobs</b> estimates. This <b>validation</b> report includes an introduction to JEDI models, an analysis of the value and impact of the JEDI models, and an analysis of the validity of job estimates generated by JEDI model through comparison to other modeled estimates and comparison to empirical, observed jobs data as reported or estimated for a commercial project, a state, or a region...|$|R
50|$|Post-silicon {{validation}} encompasses {{all that}} validation effort that is poured onto a system {{after the first}} few silicon prototypes become available, but before product release. While in the past most of this effort was dedicated to validating electrical aspects of the design, or diagnosing systematic manufacturing defects, today a growing portion of the effort focuses on functional system validation. This trend is for the most part due to the increasing complexity of digital systems, which limits the verification coverage provided by traditional pre-silicon methodologies. As a result, a number of functional bugs survive into manufactured silicon, and it is the <b>job</b> of post-silicon <b>validation</b> to detect and diagnose them so that they do not escape into the released system. The bugs in this category are often system-level bugs and rare corner-case situations buried deep in the design state space: since these problems encompass many design modules, they are difficult to identify with pre-silicon tools, characterized by limited scalability and performance.|$|R
40|$|Volunteer Computing (VC) {{projects}} {{harness the}} power of computers owned by volunteers across the Internet to perform {{hundreds of thousands of}} independent jobs. In VC projects, the path leading from the generation of <b>jobs</b> to the <b>validation</b> of the <b>job</b> results is characterized by delays hid-den in the job lifespan, i. e., distribution delay, in-progress delay, and validation delay. These delays are difficult to es-timate because of the dynamic behavior and heterogeneity of VC resources. A wrong estimation of these delays can cause the loss of project throughput and job latency in VC projects. In this paper, we evaluate the accuracy of several prob-abilistic methods to model the upper time bounds of these delays. We show how our selected models predict up-and-down trends in traces from existing VC projects. The use of our models provides valuable insights on selecting project deadlines and taking scheduling decisions. By accurately predicting job lifespan delays, our models lead to more ef-ficient resource use, higher project throughput, and lower job latency in VC projects. 1...|$|R
40|$|The ATLAS Installation System v 2 is the {{evolution}} of the original system, used since 2003. The original tool has been completely re-designed in terms of database backend and components, adding support for submission to multiple backends, including the original WMS and the new Panda modules. The database engine has been changed from plain MySQL to Galera/Percona and the table structure has been optimized to allow a full High-Availability (HA) solution over WAN. The servlets, running on each frontend, have been also decoupled from local settings, to allow an easy scalability of the system, including the possibility of an HA system with multiple sites. The clients can also be run in multiple copies and in different geographical locations, and take care of sending the installation and <b>validation</b> <b>jobs</b> to the target Grid or Cloud sites. Moreover, the Installation DB is used as source of parameters by the automatic agents running in CVMFS, in order to install the software and distribute it to the sites. The system is in production for ATLAS since 2013, having as main sites in HA the INFN Roma T 2 and CERN AI. The LJSFi 2 engine is directly interfacing with Panda for the Job Management, AGIS for the site parameter configurations, and CVMFS for both core components and the installation of the software itself. LJSFi 2 is also able to use other plugins, and is essentially VO-agnostic, so can be directly used and extended to cope with the requirements of any Grid or Cloud enabled VO. In this work we'll present the architecture, performance, status and possible evolutions to the system for the LHC Run 2 and beyond...|$|R
40|$|This {{research}} {{is based on}} the premises that teams can be designed to optimize its performance, and appropriate team coordination is a significant factor to team outcome performance. Contingency theory argues that the effectiveness of a team depends on the right fit of the team design factors to the particular job at hand. Therefore, organizations need computational tools capable of predict the performance of different configurations of teams. ^ This research created an agent-based model of teams called the Team Coordination Model (TCM). The TCM estimates the coordination load and performance of a team, based on its composition, coordination mechanisms, and job’s structural characteristics. The TCM can be used to determine the team’s design characteristics that most likely lead the team to achieve optimal performance. ^ The TCM is implemented as an agent-based discrete-event simulation application built using JAVA and Cybele Pro agent architecture. The model implements the effect of individual team design factors on team processes, but the resulting performance emerges from the behavior of the agents. These team member agents use decision making, and explicit and implicit mechanisms to coordinate the <b>job.</b> The model <b>validation</b> included the comparison of the TCM’s results with statistics from a real team and with the results predicted by the team performance literature. An illustrative 26 - 1 fractional factorial experimental design demonstrates the application of the simulation model to the design of a team. The results from the ANOVA analysis have been used to recommend the combination of levels of the experimental factors that optimize the completion time for a team that runs sailboats races. ^ This research main contribution to the team modeling literature is a model capable of simulating teams working on complex job environments. The TCM implements a stochastic job structure model capable of capturing some of the complexity not capture by current models. In a stochastic job structure, the tasks required to complete the job change during the team execution of the job. This research proposed three new types of dependencies between tasks required to model a job as a stochastic structure. These dependencies are conditional sequential, single-conditional sequential, and the merge dependencies. ...|$|R

