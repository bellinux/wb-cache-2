1382|772|Public
5|$|Another {{approach}} ignores {{energy and}} deals {{only with the}} molecular complexity estimated by the <b>information</b> <b>entropy</b> index. It speculates that the points of several natural compounds (urea, pyrimidine, dihydroxyacetone, uracil, cytosine, glycine, and alanine) fall into {{the range of the}} values typical for the known interstellar molecules that indicates high probability of their detection in interstellar environment. Additionally the molecules with maximal <b>information</b> <b>entropy,</b> i.e. the most complex compounds, make up approximately a half of the interstellar set and their percentage is decreased with the size. This trend may be associated with the different stabilities of the molecules with uniform (usually more stable) and diversified (usually less stable) chemical structures, so the detectable molecules with a large size must possess symmetric structure more probably than non-symmetric. The remarkable detection of low-entropy (highly symmetric) fullerene molecules supports this assumption. It is also noted that <b>information</b> <b>entropy</b> reflects the depth of hydrogenation of interstellar entities: the molecules with maximal <b>information</b> <b>entropy</b> are hydrogen-poor whereas the others are mainly hydrogen-rich.|$|E
5|$|In {{information}} theory, {{the definition}} of the amount of self-information and <b>information</b> <b>entropy</b> is often expressed with the binary logarithm, corresponding to making the bit the fundamental unit of information. However, the natural logarithm and the nat are also used in alternative notations for these definitions.|$|E
25|$|Indices of {{qualitative}} variation are then analogous to <b>information</b> <b>entropy,</b> which is minimized when all cases {{belong to a}} single category and maximized in a uniform distribution. Indeed, <b>information</b> <b>entropy</b> {{can be used as}} an index {{of qualitative}} variation.|$|E
5000|$|... #Caption: Venn diagram {{illustrating}} {{the relation between}} <b>information</b> <b>entropies,</b> mutual <b>information</b> and variation of information.|$|R
40|$|Wave packet {{fractional}} revivals is {{a relevant}} {{feature in the}} long time scale evolution {{of a wide range}} of physical systems, including atoms, molecules and nonlinear systems. We show that the sum of <b>information</b> <b>entropies</b> in both position and momentum conjugate spaces is an indicator of fractional revivals by analyzing three different model systems: $(i) $ the infinite square well, $(ii) $ a particle bouncing vertically against a wall in a gravitational field, and $(iii) $ the vibrational dynamics of hydrogen iodide molecules. This description in terms of <b>information</b> <b>entropies</b> complements the usual one in terms of the autocorrelation function...|$|R
40|$|There is {{a renewed}} {{interest}} in the uncertainty principle, reformulated from the information theoretic point of view, called the entropic uncertainty relations. They have been studied for various integrable systems {{as a function of the}}ir quantum numbers. In this work, focussing on the ground state of a nonlinear, coupled Hamiltonian system, we show that approximate eigenstates can be can be constructed within the framework of adiabatic theory. Using the adiabatic eigenstates, we estimate the <b>information</b> <b>entropies</b> and their sum as a function of the the nonlinearity parameter. We also briefly look at the <b>information</b> <b>entropies</b> for the highly excited states in the system. Comment: 8 pages, 6 figs., revte...|$|R
25|$|This is a {{more natural}} form and this {{rescaled}} entropy exactly corresponds to Shannon's subsequent <b>information</b> <b>entropy.</b>|$|E
25|$|<b>Information</b> <b>entropy</b> {{is defined}} as the average amount of {{information}} produced by a probabilistic stochastic source of data.|$|E
25|$|The <b>information</b> <b>entropy</b> of the Weibull and Lévy {{distribution}}s, and, implicitly, of the {{chi-squared distribution}} {{for one or}} two degrees of freedom.|$|E
40|$|The {{position}} and momentum space <b>information</b> <b>entropies</b> of weakly interacting trapped atomic Bose-Einstein condensates and spin-polarized trapped atomic Fermi gases at absolute zero temperature are evaluated. We find that {{sum of the}} {{position and}} momentum space <b>information</b> <b>entropies</b> of these quantum systems contain N atoms confined in a D(< 3) -dimensional harmonic trap has a universal form as S_t^(D) = N(a D - b N), where a ≃ 2. 332 and b = 2 for interacting bosonic systems and a ≃ 1. 982 and b = 1 for ideal fermionic systems. These results obey the entropic uncertainty relation given by the Beckner, Bialynicki-Birula and Myceilski...|$|R
40|$|AbstractWe derive asymptotics for the Lp-norms and <b>information</b> <b>entropies</b> of Charlier polynomials. The results differ to {{some extent}} from {{previously}} studied orthogonal polynomials, for example, the Lp-norms show a peculiar behaviour with two thresholds. Some complications arise because the measure involved is discrete...|$|R
40|$|The {{position}} and momentum space <b>information</b> <b>entropies</b> for the Morse potential are numerically obtained for different {{strengths of the}} potential. It is found to satisfy the bound obtained by Beckner, Bialynicki-Birula, and Mycielski. Interesting features of the entropy densities are graphically demonstrated. Comment: Paper has been improve...|$|R
25|$|Entropy, if {{considered}} as information (see <b>information</b> <b>entropy),</b> {{is measured in}} bits. The total quantity of bits {{is related to the}} total degrees of freedom of matter/energy.|$|E
25|$|Maximum {{information}} entropy: A {{more elaborate}} {{version of the}} principle of indifference states that the correct ensemble is the ensemble that is compatible with the known information and that has the largest Gibbs entropy (<b>information</b> <b>entropy).</b>|$|E
25|$|The {{question}} {{of the link between}} <b>information</b> <b>entropy</b> and thermodynamic entropy is a debated topic. While most authors argue that there is a link between the two, a few argue that they {{have nothing to do with}} each other.|$|E
40|$|The {{explicit}} form of {{the lower}} bound on {{the sum of the}} <b>information</b> <b>entropies</b> given by the Maassen-Uffink entropic uncertainty relation for two arbitrary angular momentum components Jz and Jz′ is obtained, which provides a rigorous quantitative formulation of the uncertainty principle for these observables. Publicad...|$|R
3000|$|... [...]. The {{position}} and momentum <b>information</b> <b>entropies</b> of D-dimensional quantum systems with central potentials, {{such as the}} isotropic harmonic oscillator and the hydrogen atom, depend on the entropies of the (hyper)spherical harmonics (see [2]). In turn, these entropies are {{expressed in terms of}} the entropies of the Gegenbauer (ultraspherical) polynomials [...]...|$|R
40|$|The exact {{lower bound}} on {{the sum of}} the <b>information</b> <b>entropies</b> is {{obtained}} for arbitrary pairs of observables in two-dimensional Hilbert space. The result coincides with that given by Garrett and Gull for the particular case of real transformation matrices and state vectors. A weaker analytical bound is also obtained. Publicad...|$|R
25|$|An {{index of}} {{qualitative}} variation (IQV) {{is a measure}} of statistical dispersion in nominal distributions. There are a variety of these, but they have been relatively little-studied in the statistics literature. The simplest is the variation ratio, while more complex indices include the <b>information</b> <b>entropy.</b>|$|E
25|$|Coding {{theory is}} one of the most {{important}} and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the <b>information</b> <b>entropy</b> of the source.|$|E
25|$|For example, {{passwords}} of {{the following}} case-insensitive form: consonant, vowel, consonant, consonant, vowel, consonant, number, number (for example pinray45) are called Environ passwords. The pattern of alternating vowel and consonant characters was intended to make passwords {{more likely to be}} pronounceable and thus more memorable. Unfortunately, such patterns severely reduce the password's <b>information</b> <b>entropy,</b> making brute force password attacks considerably more efficient. In the UK in October 2005, employees of the British government were advised to use passwords in this form.|$|E
30|$|Linearization {{problems}} {{appear in}} several applications. For example, the special {{case in which}} i=j in the standard linearization formula (2) is very useful when evaluating the logarithmic potentials of orthogonal polynomials appearing in the calculation of the position and momentum <b>information</b> <b>entropies</b> of quantum systems (see Dehesa et al. [1] and Tcheutia [2]).|$|R
40|$|KEY WORDS: entropy, position-momentum {{uncertainty}} relation, pöschlteller potential well. ABSTRACT: We {{calculated the}} position and momentum dispersions and <b>information</b> <b>entropies,</b> <b>information</b> energies and zero-contingent entropies for the Pöschl-Teller potential well {{as a function of}} its quantum states. We determined also the corresponding uncertainty relations for the symmetric and asymmetric Pöschl-Teller well. 1...|$|R
40|$|A {{family of}} {{replicator}}-like dynamics, called the escort replicator equation, is constructed using information-geometric concepts and generalized <b>information</b> <b>entropies</b> and diverenges from statistical thermodynamics. Lyapunov functions and escort generalizations of basic concepts and constructions in evolutionary game theory are given, {{such as an}} escorted Fisher's Fundamental theorem and generalizations of the Shahshahani geometry. Comment: Minor typo correctio...|$|R
2500|$|In the k, θ parameterization, the <b>information</b> <b>entropy</b> {{is given}} by ...|$|E
2500|$|By definition, the <b>information</b> <b>entropy</b> of the Von Mises {{distribution}} is ...|$|E
2500|$|... (Note: the log-likelihood {{is closely}} related to <b>information</b> <b>entropy</b> and Fisher information.) ...|$|E
40|$|Abstract. Quantum Information Theory is a {{new field}} with {{potential}} implications for the conceptual foundations of Quantum Mechanics through density matrices. In particular, <b>information</b> <b>entropies</b> in Hilbert space representation are highly advantageous {{in contrast with the}} ones in phase space representation since they can be eas-ily calculated for large systems. In this work, novel von Neumann conditional, mutual, and joint entropies are employed to analyze the dissociation process of small molecules, Cl 2 and HCl, by using the spectral decomposition of the first reduced density matrix in natural atomic orbital-based representation which allows us to assure rota-tional invariance, N- and v-representability in the Atoms-in-Molecules (AIM) scheme. Quantum <b>information</b> <b>entropies</b> permit to analyze the dissociation process through quantum mechanics concepts such as electron correlation and entanglement, showing interesting critical points which are not present in the energy profile, such as charge depletion and accumulation, along with bond breaking regions...|$|R
5000|$|E.g. [...] The {{possibility}} of AGTCGTAGATGCTG {{is lower than}} ACGT, and as such, holds a greater amount of <b>information</b> (refer to <b>entropy</b> (<b>information</b> theory) for more information).|$|R
40|$|In this paper, {{we develop}} {{the notion of}} the {{marginal}} and density atomic Wehrl entropies for two-level atom interacting with the single mode field, i. e. Jaynes-Cummings model. For this system we show that there are relationships between these quantities and both of the <b>information</b> <b>entropies</b> and the von Neumann entropy. Comment: 13 pages, 3 figures, this is the final versio...|$|R
2500|$|Another {{measure of}} {{uncertainty}} in momentum is the <b>information</b> <b>entropy</b> of the probability distribution Hp: ...|$|E
2500|$|... the <b>information</b> <b>entropy</b> and {{redundancy}} of a source, and {{its relevance}} through the source coding theorem; ...|$|E
2500|$|The {{measure of}} <b>information</b> <b>entropy</b> {{associated}} with each possible data value is the negative logarithm of the probability mass function for the value. Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more [...] "information" [...] ("surprisal") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the <b>information</b> <b>entropy.</b> Generally, entropy refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the definition used in statistical thermodynamics. The concept of <b>information</b> <b>entropy</b> was introduced by Claude Shannon in his 1948 paper [...] "A Mathematical Theory of Communication".|$|E
40|$|Several D-dimensional uncertainty-like {{relationships}} for N-body {{systems are}} obtained {{by means of}} the Fisher’s <b>information</b> <b>entropies</b> in position and momentum spaces and the Stam’s uncertainty principle. In addition, these relationships, the Fisher’s entropies and the Stam’s inequality are analysed numerically for all ground state neutral atoms from hydrogen (Z ˆ 1) to lawrencium (Z ˆ 103) using highly accurate Roothaan±Hartree±Fock wavefunctions. 1...|$|R
40|$|We {{make use}} of a Hylleraas-type wave {{function}} to derive an exact analytical model to quantify correlation in two-electron atomic/ionic systems and subsequently employ it to examine the role of inter-electronic repulsion in affecting (i) the bare (uncorrelated) single-particle position- and momentum-space charge distributions and (ii) corresponding Shannon's <b>information</b> <b>entropies.</b> The results presented {{for the first five}} members in the helium iso-electronic sequence, on the one hand, correctly demonstrate the effect of correlation on bare charge distributions and, on the other hand, lead us to some important results for the correlated and uncorrelated values of the entropies. These include the limiting behavior of the correlated entropy sum (sum of position- and momentum-space entropies) and geometrical realization for the variation of <b>information</b> <b>entropies</b> as a function of Z. We suggest that, rather than the entropy sum, individual entropies should be regarded as better candidates for the measure of correlation. Comment: 10 pages, 8 figures, 2 table...|$|R
2500|$|... Short {{introduction}} to the axioms of <b>information</b> theory, <b>entropy,</b> mutual <b>information,</b> Kullback–Liebler divergence, and Jensen–Shannon distance.|$|R
