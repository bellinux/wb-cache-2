1|14|Public
40|$|Web Rendezvous is {{the meeting}} of {{information}} pub-lisher and subscriber, allowing the subscriber to cre-ate and maintain a list of links (indices) to publish-ers ' pages. Rendezvous is diÆcult because pages may move and unauthorized publishers may seek to add their pages to the <b>index</b> (<b>spamming).</b> Current ren-dezvous approaches are work-intensive or restricted to very limited domains. We provide a taxonomy of web rendezvous methods. We identify automated methods which allow a subscriber to maintain an index of many pages with only a xed amount of eort. Our methods leverage existing web search engines to be robust to page movement, and use encryption to prevent <b>index</b> <b>spamming.</b> We have designed and implemented two dierent rendezvous methods; we summarize our expe-rience using them, nding that we can securely track index changes {{in a few days}} with publisher support, and eventually even without publisher support. ...|$|E
50|$|The {{purpose of}} a splog can be to {{increase}} the PageRank or backlink portfolio of affiliate websites, to artificially inflate paid ad impressions from visitors (see made for AdSense or MFA-blogs), and/or use the blog as a link outlet to sell links or get new sites <b>indexed.</b> <b>Spam</b> blogs are usually a type of scraper site, where content is often either inauthentic text or merely stolen (see blog scraping) from other websites. These blogs usually contain {{a high number of}} links to sites associated with the splog creator which are often disreputable or otherwise useless websites.|$|R
50|$|BTDigg was {{the first}} BitTorrent DHT search engine. It participated in the BitTorrent DHT network, {{supporting}} the network and making correspondence between magnet links and a few torrent attributes (name, size, list of files) which are indexed and inserted into a database. For end users, BTDigg provides a full text database search via Web interface. The web part of its search system retrieved proper information by a user's text query. The Web search supported queries in European and Asian languages. The project name was an acronym of BitTorrent Digger (in this context digger means a treasure-hunter). It went offline in June 2016, reportedly due to <b>index</b> <b>spam.</b> The site returned later in 2016 at a dot-com domain, but has since shut down again.|$|R
40|$|Abstract- This paper aims {{to improve}} the {{accuracy}} of the original detection model on the product review spammers. The original detection model has three activities of review spammers. Now, we add the new two activities {{to improve the}} accuracy. In this paper, we first introduce the three existing models. Then, we give our new models. At last, we propose scoring methods to ensure that the new activity models have the same effects on the prediction model based on an Amazon review dataset. Our results show that our proposed two models have achieved the desired result. <b>Index</b> Terms- <b>Spam</b> reviewers, activity. I...|$|R
40|$|Abstract—This paper {{gives an}} {{overview}} and analyzes the different existing anti spamming techniques both content based, non content based methods and {{a combination of}} the duo. It also suggests that a small change in one of the parameters in the fuzzy string matching method could be useful to produce better results. <b>Index</b> Terms —email, <b>spam,</b> computer forensics, fuzzy string match...|$|R
40|$|Abstract — Short Message Service (SMS) is a {{protocol}} that allows mobile phone users to send short text messages to servers, other users and telecommunication service providers. SMS spamming can create annoyance to end users, disrupt availability of SMS services, and create delay or denial of services. In this report, we analyze different ways malicious parties can execute SMS spamming attacks {{at both the}} user and the server level. We study the existing solutions and propose a more economical and platform-independent solution using open-sourced software package and scripting language. A prototype Perl script is developed to prove our concept. With advancement in technology and increasing number of GPS-enabled phones, there could be further improvements to our current solution. <b>Index</b> Terms—SMS, <b>spamming,</b> gatewa...|$|R
40|$|Abstract:––Web spam is the {{deliberate}} manipulation of search engine <b>indexes.</b> web <b>spam</b> involves {{a number of}} methods, such as repeating unrelated phrases, to manipulate the relevance or prominence of resources indexed in a manner inconsistent {{with the purpose of}} the indexing system. Search engine includes determining whether the search term appears in the content or URL of a webpage. We presents a spam host detection approach. The content and link features are extracted from hosts to train a learning model based on ant colony optimization (ACO) and bee colony optimization (BCO) algorithm. The dataset has been collected from WEBSPAM-UK 2007 and implemented by java Environment. The optimal solution is compared with the ant colony and bee colony optimization. Finally, it provides which optimization algorithm is better in detecting spam. Keywords:––Ant colony optimization, Bee colony optimization, content and link features and spam host detection I...|$|R
40|$|Text Classification is {{the process}} of {{classifying}} documents into predefined classes based on its content. Text classification is important in many web applications like document <b>indexing,</b> document organization, <b>spam</b> filtering etc. In this paper we analyze the concept of a new classification model which will classify Mobile SMS into predefined classes such as jokes, shayri, festival etc. All sms are converted into text documents. After preprocessing vector space model is prepared and weight is assigned to each term. In the proposed model we have used entropy term weighting scheme and then PCA is used for reparameterization. Artificial Neural Network is used for classification...|$|R
40|$|Abstract—The {{research}} {{community and the}} IT industry have invested significant effort in fighting spam emails. There are many different approaches, ranging from white listing, black listing, reputation ranking, postage, legislation, and content scanning etc. Until every ISP obeys the same rules, content scanning based spam email filters still have a significant {{role to play in}} fighting spam emails. There are many content scanning based spam email filters available and also in operation. Yet we are still inundated with spam emails everyday. This is not because the filters are not powerful enough, but because the filtering systems are not flexible enough to adapt the new development of spam techniques, such as HTML tagging, image based spam, and keyword obfuscating etc. In this paper, we propose to use dynamic multiple normalizers as the preprocessors for spam filters. The normalizers convert an email to its plain text format, called normalization. With the help of the normalizers, spam filters only need to deal with plain text format, which is what the filters are good at. The flexibility of the proposed architecture does not only make the adoption to the new creations of spammers easier but also makes the integration to the other spam fighting technologies easier. <b>Index</b> Terms—spam, <b>spam</b> filters, spam detection, normalization. I...|$|R
40|$|In {{this age}} of {{information}} overload, one experiences a rapidly growing over-abundance of written text. To assist with handling this bounty, this plethora of texts is now widely used to develop and optimize statistical natural language processing (NLP) systems. Surprisingly, the use of more fragments of text to train these statistical NLP systems may not necessarily lead to improved performance. We hypothesize that those fragments that help the most with training are those that contain the desired information. Therefore, determining informativeness in text has become a central issue in our view of NLP. Recent developments in this field have spawned a number of solutions to identify informativeness in text. Nevertheless, a shortfall of most of these solutions is their dependency on the genre and domain of the text. In addition, {{most of them are}} not efficient regardless of the natural language processing problem areas. Therefore, we attempt to provide a more general solution to this NLP problem. This thesis takes a different approach to this problem by considering the underlying theme of a linguistic theory known as the Code Quantity Principle. This theory suggests that humans codify information in text so that readers can retrieve this information more efficiently. During the codification process, humans usually change elements of their writing ranging from characters to sentences. Examples of such elements are the use of simple words, complex words, function words, content words, syllables, and so on. This theory suggests that these elements have reasonable discriminating strength and can {{play a key role in}} distinguishing informativeness in natural language text. In another vein, Stylometry is a modern method to analyze literary style and deals largely with the aforementioned elements of writing. With this as background, we model text using a set of stylometric attributes to characterize variations in writing style present in it. We explore their effectiveness to determine informativeness in text. To the best of our knowledge, this is the first use of stylometric attributes to determine informativeness in statistical NLP. In doing so, we use texts of different genres, viz., scientific papers, technical reports, emails and newspaper articles, that are selected from assorted domains like agriculture, physics, and biomedical science. The variety of NLP systems that have benefitted from incorporating these stylometric attributes somewhere in their computational realm dealing with this set of multifarious texts suggests that these attributes can be regarded as an effective solution to identify informativeness in text. In addition to the variety of text genres and domains, the potential of stylometric attributes is also explored in some NLP application areas [...] -including biomedical relation mining, automatic keyphrase <b>indexing,</b> <b>spam</b> classification, and text summarization [...] -where performance improvement is both important and challenging. The success of the attributes in all these areas further highlights their usefulness...|$|R
40|$|Abstract—The {{effectiveness}} of synthetic coordinate systems against DoS and spam {{stems from the}} fact that, while changing or hiding a logical address is easier, changing the location of the spammer inside the network should be harder. But synthetic coordinate systems are limited by the fact that malicious nodes can easily lie about their position or introduce additional delays which have an immediate impact on it. For this purpose, we enhance the synthetic coordinate system provided by Vivaldi with secure nonces that are periodically broadcasted by trusted servers to achieve a provable location claim within the overlay network. As we advocate in this work, secure localization can help in choosing the hardness of PoWs since locations from which more malicious traffic originates can receive PoWs with higher difficulties. <b>Index</b> Terms—DoS, localization, <b>spam,</b> Vivaldi I...|$|R
40|$|Abstract — Email {{has become}} one of the fastest and most {{economical}} forms of communication. However, the increase of email users has resulted in the dramatic increase of spam emails during the past few years. As spammers always try to find a way to evade existing filters, new filters need to be developed to catch spam. Ontologies allow for machine-understandable semantics of data. It is important to share information with each other for more effective spam filtering. Thus, it is necessary to build ontology and a framework for efficient email filtering. Using ontology that is specially designed to filter spam, bunch of unsolicited bulk email could be filtered out on the system. Similar to other filters, the ontology evolves with the user requests. Hence the ontology would be customized for the user. This paper proposes to find an efficient spam email filtering method using adaptive ontology <b>Index</b> Terms — <b>spam</b> filter, ontology, data mining, text classification, feature extraction I...|$|R
40|$|Spammers are {{constantly}} creating sophisticated new weapons {{in their arms}} race with anti-spam technology, the latest of which is image-based spam. The newest image-based spam uses simple image processing technologies to vary the content of individual messages, e. g. by changing foreground colors, backgrounds, font types, or even rotating and adding artifacts to the images. Thus, they pose great challenges to conventional spam filters. In this paper, we propose a system using a probabilistic boosting tree to determine whether an incoming image is a spam or not based on global image features, i. e. color and gradient orientation histograms. The system identifies spam {{without the need for}} OCR and is robust {{in the face of the}} kinds of variation found in current spam images. Evaluation results show the system correctly classifies 90 % of spam images while mislabeling only 0. 86 % of non-spam images as <b>spam.</b> <b>Index</b> Terms — Image spam, probabilistic boosting tree 1...|$|R
40|$|Abstract—This paper {{addresses}} {{the problem of}} rumor source detection with multiple observations, from a statistical {{point of view of}} a spreading over a network, based on the susceptible-infectious model. For tree networks, multiple independent obser-vations can dramatically improve the detection probability. For the case of a single rumor source, we propose a unified inference framework based on the joint rumor centrality, and provide explicit detection performance for degree-regular tree networks. Surprisingly, even with merely two observations, the detection probability at least doubles that of a single observation, and further approaches one, i. e., reliable detection, with increasing degree. This indicates that a richer diversity enhances detectabil-ity. Furthermore, we consider the case of multiple connected sources and investigate the effect of diversity. For general graphs, a detection algorithm using a breadth-first search strategy is also proposed and evaluated. Besides rumor source detection, our results can be used in network forensics to combat recurring epidemic-like information spreading such as online anomaly and fraudulent email <b>spams.</b> <b>Index</b> Terms—Graph networks, inference algorithms, maxi-mum likelihood detection, multiple observations, rumor spread-ing I...|$|R

