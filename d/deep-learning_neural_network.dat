3|10000|Public
40|$|Humans take {{advantage}} of real world symmetries for various tasks, yet capturing their superb symmetry perception mechanism with a computational model remains elusive. Motivated by a new study demonstrating the extremely high inter-person accuracy of human perceived symmetries in the wild, we have constructed the first <b>deep-learning</b> <b>neural</b> <b>network</b> for reflection and rotation symmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common Object in COntext) dataset with nearly 11 K consistent symmetry-labels from more than 400 human observers. We employ novel methods to convert discrete human labels into symmetry heatmaps, capture symmetry densely in an image and quantitatively evaluate Sym-NET against multiple existing computer vision algorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO photos, Sym-NET significantly outperforms all other competitors. Beyond mathematically well-defined symmetries on a plane, Sym-NET demonstrates abilities to identify viewpoint-varied 3 D symmetries, partially occluded symmetrical objects, and symmetries at a semantic level. Comment: To appear in the International Conference on Computer Vision (ICCV) 201...|$|E
40|$|Complex {{systems may}} have billion {{components}} making consensus formation slow and difficult. Recently several overlapping stories emerged from various disciplines, including protein structures, neuroscience and social networks, showing that fast responses to known stimuli involve a network core of few, strongly connected nodes. In unexpected situations the core {{may fail to}} provide a coherent response, thus the stimulus propagates to {{the periphery of the}} network. Here the final response is determined by a large number of weakly connected nodes mobilizing the collective memory and opinion, i. e. the slow democracy exercising the 'wisdom of crowds'. This mechanism resembles to Kahneman's "Thinking, Fast and Slow" discriminating fast, pattern-based and slow, contemplative decision making. The generality of the response also shows that democracy is neither only a moral stance nor only a decision making technique, but a very efficient general learning strategy developed by complex systems during evolution. The duality of fast core and slow majority may increase our understanding of metabolic, signaling, ecosystem, swarming or market processes, as well as may help to construct novel methods to explore unusual network responses, <b>deep-learning</b> <b>neural</b> <b>network</b> structures and core-periphery targeting drug design strategies. (Illustrative videos can be downloaded from here: [URL] This a preprint of a future paper and book chapter, please find its illustrative videos here: [URL]...|$|E
40|$|Using deep {{learning}} {{to improve the}} capabilities of high-resolution satellite images has emerged recently as an important topic in automatic classification. Deep networks track hierarchical high-level features to identify objects; however, enhancing the classification accuracy from low-level features is often disregarded. We therefore proposed a two-stream <b>deep-learning</b> <b>neural</b> <b>network</b> strategy, with a main stream utilizing fine spatial-resolution panchromatic images to retain low-level information under a supervised residual network structure. An auxiliary line employed an unsupervised net to extract high-level abstract and discriminative features from multispectral images to supplement the spectral information in the main stream. Various feature extraction types from the neural network were selected and jointed in the novel net, as the combined high- and low-level features could provide a superior solution to image classification. In traditional convolutional neural networks, increased network depth might not influence the network performance perceptibly; however, we introduced a residual neural network to develop the expressive ability of the deeper net, increasing the role of net depth in feature extraction. To enhance feature robustness, we proposed a novel consolidation part in feature extraction. An adversarial net improved the feature extraction capabilities and aided digging the inherent and discriminative features from data, with increased extraction efficacy. Tests on satellite images indicated the high overall accuracy of our novel net, verifying that net depth or number of convolution kernels affected the classification capability. Various comparative tests proved the structural rationality for our two-stream structure...|$|E
30|$|Although {{blockchain}} technology prevents fraudulent behaviors, it cannot detect fraud by itself (Ngo 2016). Malefactors may find unforeseen ways to steal funds and commit fraud. Although blockchain developers are strengthening its technology, innovative techniques and methods {{are needed to}} detect attacks. Existing techniques using machine learning and data-mining algorithms may find new applications in detecting fraud and intrusions in blockchain-based transactions. By profiling, monitoring, and detecting behavioral patterns based on people’s transaction histories, supervised machine learning approaches, such as <b>deep-learning</b> <b>neural</b> <b>networks,</b> support vector machines, and Bayesian belief networks may help detect outlier behaviors.|$|R
50|$|Advances in {{hardware}} {{enabled the}} {{renewed interest in}} deep learning. In 2009, Nvidia was involved in {{what was called the}} “big bang” of deep learning, “as <b>deep-learning</b> <b>neural</b> <b>networks</b> were combined with Nvidia graphics processing units (GPUs).” That year, Google Brain used Nvidia GPUs to create Deep <b>Neural</b> <b>Networks</b> capable of machine learning. While there, Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, powerful graphics processing units (GPUs) are well-suited for the matrix/vector math involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Specialized hardware and algorithm optimizations can be used for efficient D processing.|$|R
50|$|Nvidia GPUs {{are used}} in deep learning, {{artificial}} intelligence, and accelerated analytics. The company developed GPU-based deep learning {{in order to use}} artificial intelligence to approach problems like cancer detection, weather prediction, and self-driving vehicles. They are included in all Tesla vehicles. The purpose is to help networks learn to “think”. According to TechRepublic, Nvidia GPUs “work well for deep learning tasks because they are designed for parallel computing, and do well to handle the vector and matrix operations that are prevalent in deep learning.” These GPUs are used by researchers, laboratories, tech companies and enterprise companies. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as <b>deep-learning</b> <b>neural</b> <b>networks</b> were combined with Nvidia graphics processing units (GPUs).” That year, the Google Brain used Nvidia GPUs to create Deep <b>Neural</b> <b>Networks</b> capable of machine learning, where Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.|$|R
40|$|We {{reformulate}} {{the projected}} imaginary-time evolution of Full Configuration Interaction Quantum Monte Carlo {{in terms of}} a Lagrangian minimization. This naturally leads to the admission of polynomial complex wavefunction parameterizations, circumventing the exponential scaling of the approach. While previously these functions have traditionally inhabited the domain of Variational Monte Carlo, we consider recently developments for the identification of <b>deep-learning</b> <b>neural</b> <b>networks</b> to optimize this Lagrangian, which can be written as a modification of the propagator for the wavefunction dynamics. We demonstrate this approach with a form of Tensor Network State, and use it to find solutions to the strongly-correlated Hubbard model, as well as its application to a fully periodic ab-initio Graphene sheet. The number of variables which can be simultaneously optimized greatly exceeds alternative formulations of Variational Monte Carlo, allowing for systematic improvability of the wavefunction flexibility towards exactness for a number of different forms, whilst blurring the line between traditional Variational and Projector quantum Monte Carlo approaches...|$|R
40|$|We {{reformulate}} {{the projected}} imaginary-time {{evolution of the}} full configuration interaction quantum Monte Carlo method {{in terms of a}} Lagrangian minimization. This naturally leads to the admission of polynomial complex wave function parametrizations, circumventing the exponential scaling of the approach. While previously these functions have traditionally inhabited the domain of variational Monte Carlo approaches, we consider recent developments for the identification of <b>deep-learning</b> <b>neural</b> <b>networks</b> to optimize this Lagrangian, which can be written as a modification of the propagator for the wave function dynamics. We demonstrate this approach with a form of tensor network state, and use it to find solutions to the strongly correlated Hubbard model, as well as its application to a fully periodic ab initio graphene sheet. The number of variables which can be simultaneously optimized greatly exceeds alternative formulations of variational Monte Carlo methods, allowing for systematic improvability of the wave function flexibility towards exactness for a number of different forms, while blurring the line between traditional variational and projector quantum Monte Carlo approaches. G. [*]H. [*]B. gratefully acknowledges funding from the Royal Society via a University Research Fellowship, as well as support from the Air Force Office of Scientific Research via Grant No. FA 9550 - 16 - 1 - 0256. A. [*]A. acknowledges support from the EPSRC, Grant No. EP/J 003867 / 1. L. [*]R. [*]S. is supported by an EPSRC studentship...|$|R
40|$|This {{thesis is}} {{concerned}} with the development of a Projector Quantum Monte Carlo method for non-linear wavefunction ansatzes and its application to strongly correlated materials. This new approach is partially inspired by a prior application of the Full Configuration Interaction Quantum Monte Carlo (FCIQMC) method to the three-band (p-d) Hubbard model. Through repeated stochastic application of a projector FCIQMC projects out a stochastic description of the Full Configuration Interaction (FCI) ground state wavefunction, a linear combination of Slater determinants spanning the full Hilbert space. The study of the p-d Hubbard model demonstrates that the nature of this FCI expansion is profoundly affected by the choice of single-particle basis. In a counterintuitive manner, the effectiveness of a one-particle basis to produce a sparse, compact and rapidly converging FCI expansion is not necessarily paralleled by its ability to describe the physics of the system within a single determinant. The results suggest that with an appropriate basis, single-reference quantum chemical approaches may be able to describe many-body wavefunctions of strongly correlated materials. Furthermore, this thesis presents a reformulation of the projected imaginary time evolution of FCIQMC as a Lagrangian minimisation. This naturally allows for the optimisation of polynomial complex wavefunction ansatzes with a polynomial rather than exponential scaling with system size. The proposed approach blurs the line between traditional Variational and Projector Quantum Monte Carlo approaches whilst involving developments from the field of <b>deep-learning</b> <b>neural</b> <b>networks</b> which can be expressed as a modification of the projector. The ability of the developed approach to sample and optimise arbitrary non-linear wavefunctions is demonstrated with several classes of Tensor Network States all of which involve controlled approximations but still retain systematic improvability towards exactness. Thus, by applying the method to strongly-correlated Hubbard models, as well as ab-initio systems, including a fully periodic ab-initio graphene sheet, many-body wavefunctions and their one- and two-body static properties are obtained. The proposed approach can handle and simultaneously optimise large numbers of variational parameters, greatly exceeding those of alternative Variational Monte Carlo approaches. EPSRC studentshi...|$|R
40|$|In our {{preliminary}} study, the reflectance signatures {{obtained from}} hyperspectral imaging (HSI) of normal and abnormal corneal epithelium tissues of porcine show similar morphology with subtle differences. Here we present image enhancement algorithms {{that can be}} used to improve the interpretability of data into clinically relevant information to facilitate diagnostics. A total of 25 corneal epithelium images without the application of eye staining were used. Three image feature extraction approaches were applied for image classification: (i) image feature classification from histogram using a support vector machine with a Gaussian radial basis function (SVM-GRBF); (ii) physical image feature classification using <b>deep-learning</b> Convolutional <b>Neural</b> <b>Networks</b> (CNNs) only, and (iii) the combined classification of CNNs and SVM-Linear. The performance results indicate that our chosen image features from the histogram and length-scale parameter were able to classify with up to 100 % accuracy; particularly, at CNNs and CNNs-SVM, by employing 80 % of the data sample for training and 20 % for testing. Thus, in the assessment of corneal epithelium injuries, HSI has high potential as a method that could surpass current technologies regarding speed, objectivity, and reliability...|$|R
40|$|As {{a typical}} <b>deep-learning</b> model, Convolutional <b>Neural</b> <b>Networks</b> (CNNs) can be {{exploited}} to automatically extract features from images using the hierarchical structure inspired by mammalian visual system. For image classification tasks, traditional CNN models employ the softmax function for classification. However, {{owing to the}} limited capacity of the softmax function, there are some shortcomings of traditional CNN models in image classification. To deal with this problem, a new method combining Biomimetic Pattern Recognition (BPR) with CNNs is proposed for image classification. BPR performs class recognition by a union of geometrical cover sets in a high-dimensional feature space and therefore can overcome some disadvantages of traditional pattern recognition. The proposed method is evaluated on three famous image classification benchmarks, that is, MNIST, AR, and CIFAR- 10. The classification accuracies of the proposed method for the three datasets are 99. 01 %, 98. 40 %, and 87. 11 %, respectively, which are much higher {{in comparison with the}} other four methods in most cases...|$|R
40|$|One of {{the main}} {{problems}} in current artificial <b>neural</b> <b>network</b> engineering {{is the lack of}} design rules for layered <b>neural</b> <b>network</b> topologies, namely how many hidden layers and how many neurons per hidden layer to choose for a <b>neural</b> <b>network.</b> This paper offers a theoretical basis for approaching this problem. Formally proven theories are developed which maximize the interconnection topology of layered <b>neural</b> <b>networks,</b> which use supervised learning, to obtain a maximum number of interconnections and therefore allow a maximum potential storage capacity. The results presented here depend only on the <b>neural</b> <b>network</b> statics and are therefore learning rule independent. Keywords: (artificial) <b>neural</b> <b>network,</b> connectionism, <b>neural</b> <b>network</b> topology, <b>neural</b> <b>network</b> statics, <b>neural</b> <b>network</b> connectivity, <b>neural</b> <b>network</b> architecture, <b>neural</b> <b>network</b> capacity, <b>neural</b> <b>network</b> taxonomy, high(er) order <b>neural</b> <b>network,</b> Sigma-Pi <b>neural</b> <b>network</b> 1 Introduction Although the field of artificial <b>neural</b> <b>network</b> [...] ...|$|R
30|$|The {{top layer}} is {{designed}} to analyze the facial muscle activity collected in the AU activity map, in a pattern recognition task, and output a score from 1 to 10 {{for each of the}} 16 PF traits, in accordance with the 16 PF framework. To accomplish this, we have defined 16 FFNNs denoted as follows: warmth (A) - <b>neural</b> <b>network</b> (A-NN), reasoning (B) - <b>neural</b> <b>network</b> (B-NN), emotional stability (C) - <b>neural</b> <b>network</b> (C-NN), dominance (E) - <b>neural</b> <b>network</b> (E-NN), liveliness (F) - <b>neural</b> <b>network</b> (F-NN), rule-consciousness (G) - <b>neural</b> <b>network</b> (G-NN), social boldness (H) - <b>neural</b> <b>network</b> (H-NN), sensitivity (I) - <b>neural</b> <b>network</b> (I-NN), vigilance (L) - <b>neural</b> <b>network</b> (L-NN), abstractedness (M) - <b>neural</b> <b>network</b> (M-NN), privateness (N) - <b>neural</b> <b>network</b> (P-NN), apprehension (O) - <b>neural</b> <b>network</b> (O-NN), openness to change (Q 1) - <b>neural</b> <b>network</b> (Q 1 -NN), self-reliance (Q 2) - <b>neural</b> <b>network</b> (Q 2 -NN), perfectionism (Q 3) - <b>neural</b> <b>network</b> (Q 3 -NN), and tension (Q 4) - <b>neural</b> <b>network</b> (Q 4 -NN).|$|R
40|$|One of {{the main}} {{problems}} in current (artificial) <b>neural</b> <b>network</b> engineering {{is the lack of}} design rules for <b>neural</b> <b>networks,</b> i. e. how many layers and how many neurons per layer to choose for a fully connected layered <b>neural</b> <b>network</b> with bidirectional weights. A theory is developed which optimizes the topology of the <b>neural</b> <b>network</b> to allow a maximum potential storage capacity with a minimum amount of neurons. Keywords: (artificial) <b>neural</b> <b>networks,</b> connectionism, <b>neural</b> <b>network</b> topology, <b>neural</b> <b>network</b> statics, <b>neural</b> <b>network</b> connectivity, <b>neural</b> <b>network</b> capacity Introduction Although the field of artificial <b>neural</b> <b>networks,</b> hereafter called <b>neural</b> <b>networks,</b> is a rapidly growing one, some basic questions remain unanswered. One of the most important problems is how to configure a <b>neural</b> <b>network.</b> Many <b>neural</b> <b>network</b> learning rules apply to (fully connected) layered (first order) <b>neural</b> <b>networks</b> with bidirectional weights (or interconnection strengths). A bidirectional connection i [...] ...|$|R
30|$|<b>Neural</b> <b>networks</b> can {{be modeled}} {{either as a}} static <b>neural</b> <b>network</b> model or as a local field <b>neural</b> <b>network</b> model {{according}} to the modeling approaches [1, 2]. The typical static <b>neural</b> <b>networks</b> are the recurrent back-propagation networks and the projection <b>networks.</b> The Hopfield <b>neural</b> <b>network</b> is a typical example of the local field <b>neural</b> <b>networks.</b> The two types of <b>neural</b> <b>networks</b> have attained broad applications in knowledge acquisition, combinatorial optimization, pattern recognition, and other areas [3]. But {{these two types of}} <b>neural</b> <b>networks</b> are not equivalent in general. Static <b>neural</b> <b>networks</b> can be transferred into local field <b>neural</b> <b>networks</b> only under some preconditions. However, the preconditions are usually not satisfied. Hence, it is necessary to study static <b>neural</b> <b>networks.</b>|$|R
40|$|Abstract. In this study, a new multi-layered Group Method of Data Handling (GMDH) -type <b>neural</b> <b>network</b> self-selecting optimum <b>neural</b> <b>network</b> {{architecture}} is proposed. We call this algorithm as revised GMDH-type <b>neural</b> <b>network</b> algorithm self-selecting optimum <b>neural</b> <b>network</b> architecture. Revised GMDH-type <b>neural</b> <b>network</b> algorithm has an ability of self-selecting optimum <b>neural</b> <b>network</b> architecture from three <b>neural</b> <b>network</b> architectures such as sigmoid function <b>neural</b> <b>network,</b> radial basis function (RBF) <b>neural</b> <b>network</b> and polynomial <b>neural</b> <b>network.</b> Revised GMDH-type <b>neural</b> <b>network</b> also has abilities of self-selecting {{the number of}} layers, the number of neurons in hidden layers and useful input variables. This algorithm {{is applied to the}} nonlinear system identification problem and it is shown that this algorithm is useful for the nonlinear system identification because optimum <b>neural</b> <b>network</b> {{architecture is}} automatically organized...|$|R
30|$|With the {{development}} of artificial intelligence and computer technology, more and more researchers have developed {{a keen interest in}} <b>neural</b> <b>network</b> methods. <b>Neural</b> <b>networks</b> have been used in many fields such as pattern recognition [11], graphics processing [12], risk assessment [13], control systems [14], forecasting [15 – 18], and classification [19], showing wide application prospects. Based on the advantages of <b>neural</b> <b>network</b> methods, the use of <b>neural</b> <b>network</b> function approximation capabilities [20 – 22] has led to {{the development}} of a number of adopted <b>neural</b> <b>network</b> model for solving differential equations. The <b>neural</b> <b>network</b> methods for solving differential equations mainly include the following categories: multilayer perceptron <b>neural</b> <b>network</b> [23 – 28], radial basis function <b>neural</b> <b>network</b> [29 – 31], multi-scale radial basis function <b>neural</b> <b>network</b> [32 – 35], cellular <b>neural</b> <b>network</b> [36, 37], finite element <b>neural</b> <b>network</b> [38 – 46] and wavelet <b>neural</b> <b>network</b> [28]. The main research focuses on two parts: the construction of the approximate solution and the weights training algorithm.|$|R
30|$|Over {{the past}} decades, {{considerable}} {{attention has been}} devoted to the study of artificial <b>neural</b> <b>networks</b> due to their extensive application in signal and image processing, pattern recognition, and combinatorial optimization [1, 2]. Numerous models of <b>neural</b> <b>networks</b> such as cellular <b>neural</b> <b>networks,</b> Hopfield-type <b>neural</b> <b>networks,</b> Cohen-Grossberg <b>neural</b> <b>networks,</b> and bidirectional associative memory <b>neural</b> <b>networks</b> have been extensively investigated in the literature [3 – 7].|$|R
40|$|AbstractCharacteristics and {{precision}} of BP <b>neural</b> <b>network,</b> genetic <b>neural</b> <b>network</b> and annealing <b>neural</b> <b>network</b> applied in GPS height fitting were hereby compared and analyzed {{to improve the}} precision of transforming GPS geodetic height into normal height. The results indicated that Genetic <b>Neural</b> <b>Network</b> method is superior to BP <b>neural</b> <b>network</b> method as well as Annealing <b>Neural</b> <b>Network</b> 1 method with good precision through several instances...|$|R
30|$|According to {{different}} transfer functions, the <b>neural</b> <b>network</b> {{can be divided}} into single-layer perceptron <b>neural</b> <b>network,</b> linear <b>neural</b> <b>network,</b> multi-node BP <b>neural</b> <b>network,</b> RBF/GRNN <b>network,</b> Hopfield <b>neural</b> <b>network,</b> random <b>neural</b> <b>network,</b> and so on. Single-layer perceptron <b>neural</b> <b>networks</b> are mainly utilized to solve binary pattern recognition problems. Linear <b>neural</b> <b>networks</b> can be used for simple linear separable stress values and linear fitting. BP networks can be employed for linear non-separable pattern recognition, function fitting, and optimization. RBF/GRNN networks can be used for the function approximation of small samples. Hopfield networks can be employed to solve complex pattern recognition problems and achieve prediction and optimization in time domain [22].|$|R
40|$|Probabilistic <b>neural</b> <b>network</b> has {{successfully}} solved {{all kinds of}} engineering problems in various fields since it is proposed. In probabilistic <b>neural</b> <b>network,</b> Spread has great influence on its performance, and probabilistic <b>neural</b> <b>network</b> will generate bad prediction results if it is improperly selected. It is difficult to select the optimal manually. In this article, a variant of probabilistic <b>neural</b> <b>network</b> with self-adaptive strategy, called self-adaptive probabilistic <b>neural</b> <b>network,</b> is proposed. In self-adaptive probabilistic <b>neural</b> <b>network,</b> Spread can be self-adaptively adjusted and selected and then the best selected Spread is used to guide the self-adaptive probabilistic <b>neural</b> <b>network</b> train and test. In addition, two simplified strategies are incorporated into the proposed self-adaptive probabilistic <b>neural</b> <b>network</b> {{with the aim of}} further improving its performance and then two versions of simplified self-adaptive probabilistic <b>neural</b> <b>network</b> (simplified self-adaptive probabilistic <b>neural</b> <b>networks</b> 1 and 2) are proposed. The variants of self-adaptive probabilistic <b>neural</b> <b>networks</b> are further applied to solve the transformer fault diagnosis problem. By comparing them with basic probabilistic <b>neural</b> <b>network,</b> and the traditional back propagation, extreme learning machine, general regression <b>neural</b> <b>network,</b> and self-adaptive extreme learning machine, the results have experimentally proven that self-adaptive probabilistic <b>neural</b> <b>networks</b> have a more accurate prediction and better generalization performance when addressing the transformer fault diagnosis problem...|$|R
40|$|Abstract—In this paper, a novel {{methodology}} {{called a}} reference model approach to stability analysis of <b>neural</b> <b>networks</b> is proposed. The {{core of the}} new approach is to study a <b>neural</b> <b>network</b> model with reference to other related models, so that different modeling approaches can be combinatively used and powerfully cross-fertilized. Focused on two representative <b>neural</b> <b>network</b> modeling approaches (the neuron state modeling approach and the local field modeling approach), we establish a rigorous theoretical basis on the feasibility and efficiency of the reference model approach. The new approach {{has been used to}} develop a series of new, generic stability theories for various <b>neural</b> <b>network</b> models. These results have been applied to several typical <b>neural</b> <b>network</b> systems including the Hopfield-type <b>neural</b> <b>networks,</b> the recurrent back-propagation <b>neural</b> <b>networks,</b> the BSB-type <b>neural</b> <b>networks,</b> the bound-constraints optimization <b>neural</b> <b>networks,</b> and the cellular <b>neural</b> <b>networks.</b> The results obtained unify, sharpen or generalize most of the existing stability assertions, and illustrate the feasibility and power of the new method. Index Terms—Local field <b>neural</b> <b>network</b> model, reference model approach, stability analysis, static <b>neural</b> <b>network</b> model. I...|$|R
40|$|The paper {{deals with}} text {{document}} retrieval from the given document collection by using <b>neural</b> <b>networks,</b> namely cascade <b>neural</b> <b>network,</b> linear and nonlinear Hebbian <b>neural</b> <b>networks</b> and linear autoassociative <b>neural</b> <b>network.</b> With using <b>neural</b> <b>networks</b> {{it is possible}} to reduce the dimension of the document search space with preserving the highest retrieval accuracy...|$|R
40|$|The Monte Carlo {{adaptation}} {{rule has}} been proposed to design asymmetric <b>neural</b> <b>network.</b> By adjusting the degree of the symmetry of the networks designed by this rule, the spurious memories or unwanted attractors of the networks can be suppressed completely. We have extended this rule to design full-connected <b>neural</b> <b>networks</b> and diluted <b>neural</b> <b>networks.</b> Comparing the dynamics of these two <b>neural</b> <b>networks,</b> the simulation {{results indicated that the}} performance of diluted <b>neural</b> <b>network</b> was poorer than the performance of full-connected <b>neural</b> <b>network.</b> As to this point, further research is needed. In this paper, we use the annealed dilution method to design a diluted <b>neural</b> <b>network</b> with fixed degree of dilution. By analyzing the dynamics of the diluted <b>neural</b> <b>network,</b> it is verified that asymmetric full-connected <b>neural</b> <b>network</b> do have significant advantages over the asymmetric diluted <b>neural</b> <b>network...</b>|$|R
50|$|A {{physical}} <b>neural</b> <b>network</b> includes electrically {{adjustable resistance}} material to simulate artificial synapses. Examples include the ADALINE memristor-based <b>neural</b> <b>network.</b> An optical <b>neural</b> <b>network</b> {{is a physical}} implementation of an artificial <b>neural</b> <b>network</b> with optical components.|$|R
40|$|Abstract. This paper {{introduces}} the system structure of <b>neural</b> <b>network</b> in fault diagnosis, and summarizes some applications of <b>neural</b> <b>network</b> in fault diagnosis. The {{most commonly used}} <b>neural</b> <b>network</b> in fault diagnosis is BP network. The second is RBF network {{and the third is}} ART. For each <b>neural</b> <b>network,</b> the paper will discuss the <b>neural</b> <b>network,</b> and the introduce some applications. It also {{introduces the}} combination of <b>neural</b> <b>networks</b> and other techniques. In the last part, this paper points out the development trend of the <b>neural</b> <b>network</b> in fault diagnosis...|$|R
40|$|Abstract- Unlike {{feedforward}} <b>neural</b> <b>networks</b> (FFNN) {{which can}} act as universal function approximaters, recursive <b>neural</b> <b>networks</b> {{have the potential to}} act as both universal function approximaters and universal system approximaters. In this paper, a globally recursive <b>neural</b> <b>network</b> least mean square (GRNNLMS) gradient descent or a real time recursive backpropagation (RTRBP) algorithm is developed for a single layer globally recursive <b>neural</b> <b>network</b> that has multiple delays in its feedback path. Keywords- Least mean square, real-time recursive backpropagation, recurrent <b>neural</b> <b>network,</b> <b>neural</b> <b>network</b> training, globally recursive <b>neural</b> <b>network</b> A I...|$|R
40|$|This article {{presents}} {{an overview of}} artificial <b>neural</b> <b>network</b> (ANN) applications in forecasting and possible forecasting accuracy improvements. Artificial <b>neural</b> <b>networks</b> are computational models and universal approximators, which {{can be applied to}} the time series forecasting with a high accuracy. A great rise in research activities was observed in using artificial <b>neural</b> <b>networks</b> for forecasting. This paper examines multi-layer perceptrons (MLPs) – back-propagation <b>neural</b> <b>network</b> (BPNN), Elman recurrent <b>neural</b> <b>network</b> (ERNN), grey relational artificial <b>neural</b> <b>network</b> (GRANN) and hybrid systems – models that fuse artificial <b>neural</b> <b>network</b> with wavelets and autoregressive integrated moving average (ARIMA) ...|$|R
40|$|Abstract. The paper {{deals with}} text {{document}} retrieval from the given document collection by using <b>neural</b> <b>networks,</b> namely cascade <b>neural</b> <b>network</b> model, linear and nonlinear Hebbian <b>neural</b> <b>networks</b> and linear autoassociative <b>neural</b> <b>network.</b> With using <b>neural</b> <b>networks</b> {{it is possible}} to reduce the dimension of the search space with preserving the highest retrieval accuracy...|$|R
5000|$|<b>Neural</b> <b>Networks</b> - A <b>neural</b> <b>network</b> is {{a family}} of {{learning}} models inspired by biological <b>neural</b> <b>networks</b> (the nervous systems of animals, in particular the brain) and are used to estimate user preferences. <b>Neural</b> <b>networks</b> have classification abilities, including pattern recognition. Netflix, for example, uses a <b>neural</b> <b>network</b> to see what genre of movies you prefer to watch. <b>Neural</b> <b>networks</b> also do data processing, including data filtering, similar {{to the purpose of}} a filtering system.|$|R
50|$|Extensions to graphs include Graph <b>Neural</b> <b>Network</b> (GNN), <b>Neural</b> <b>Network</b> for Graphs (NN4G), {{and more}} {{recently}} convolutional <b>neural</b> <b>networks</b> for graphs.|$|R
30|$|<b>Neural</b> <b>networks</b> with {{deviating}} argument, {{which are}} {{proposed in the}} model of recurrent <b>neural</b> <b>networks</b> by Akhmet et al. [41], are suitable for modeling situations in physics, economy, and biology. In these situations, not only past but also future events are critical for the current properties. The deviating argument changes its type from advanced to retarded alternately and it can link past and future events [42 – 49]. <b>Neural</b> <b>networks</b> with deviating argument conjugate continuous <b>neural</b> <b>networks</b> and discrete <b>neural</b> <b>networks.</b> Hence, this type of <b>neural</b> <b>networks</b> has the properties of both continuous <b>neural</b> <b>networks</b> and discrete <b>neural</b> <b>networks.</b> From a mathematical perspective, these <b>neural</b> <b>networks</b> are of a mixed type. With {{the evolution of the}} process, the deviating state can be advanced and retarded, commutatively. The dynamic behavior of this type of <b>neural</b> <b>networks</b> is studied extensively [50 – 52] and deserves further investigation.|$|R
30|$|The {{generalized}} <b>neural</b> <b>networks</b> (GNNs) model, {{which is}} a combination of local field <b>neural</b> <b>networks</b> (LFNNs) and static <b>neural</b> <b>networks</b> (SNNs), has received increasing attention in recent years, {{due to the fact that}} it provides an unified frame for stability analysis of both SNNs and LFNNs [1 – 6]. It should be mentioned that back-propagation <b>neural</b> <b>networks</b> and optimization type <b>neural</b> <b>networks</b> can be modeled as SNNs, whereas Hopfield <b>neural</b> <b>networks,</b> bidirectional associative memory <b>neural</b> <b>networks,</b> and cellular <b>neural</b> <b>networks</b> can be modeled as LFNNs [6]. Therefore, it is enough to study the stability of GNNs instead of both LFNNs and SNNs. On the other hand, as a source of instability and poor performance, time-delays [7 – 9] always appear in many <b>neural</b> <b>networks.</b> Thus, stability analysis for delayed <b>neural</b> <b>networks</b> has received considerable attention over the past few decades [2 – 6, 10 – 35].|$|R
40|$|Agile Software {{development}} has been increasing popularity and replacing the traditional methods of software develop-ment. This paper presents the all <b>neural</b> <b>network</b> techniques including General Regression <b>Neural</b> <b>Networks</b> (GRNN), Prob-abilistic <b>Neural</b> <b>Network</b> (PNN), GMDH Polynomial <b>Neural</b> <b>Network,</b> Cascade correlation <b>neural</b> <b>network</b> and a Machine Learning Technique Random Forest. To achieve better prediction, effort estimation of agile projects we will use Random Forest with Story Points Approach (SPA) in place of <b>neural</b> <b>network</b> because Random Forest is easy to implement and better than decision tree. In this paper <b>Neural</b> <b>Network</b> is the existing model and the proposed model is Random Forest. Random Forest performs better as compare to General Regression <b>Neural</b> <b>Network</b> (GRNN). The researchers will perform comparison between Random Forest and all types (GRNN, PNN, GMDH, and CCNN) of <b>Neural</b> <b>Network...</b>|$|R
40|$|For {{nonlinear}} {{and adaptive}} control of smart structures {{direct and indirect}} <b>neural</b> <b>network</b> control strategies have been suggested. In indirect <b>neural</b> <b>network</b> control the identified plant models are usually implemented as black-box <b>neural</b> <b>networks</b> using no a priori knowledge. Designing a <b>neural</b> <b>network</b> for system identification using dimensional analysis results in <b>neural</b> <b>networks,</b> where in contrary to black-box solutions no dimensionally inhomogeneous states can occur. Furthermore, the generalization and learning properties of <b>neural</b> <b>networks</b> designed using dimensional analysis are usually improved compared to conventional black-box networks. This work describes a technique of using <b>neural</b> <b>networks</b> for system identification and control, where the <b>neural</b> <b>network</b> has been constructed according to a dimensional analysis of the governing equations. Keywords: Neural Control, Similarity Theory, Pi-Theorem, Dimensional Analysis 1. INTRODUCTION <b>Neural</b> <b>networks</b> have been proposed in the liter [...] ...|$|R
30|$|<b>Neural</b> <b>networks,</b> {{including}} Hopfield <b>neural</b> <b>networks</b> {{and cellular}} <b>neural</b> <b>networks,</b> {{have been widely}} investigated in past decades [1 – 23]. Synchronization, as a typical collective dynamical behavior of <b>neural</b> <b>networks,</b> has attracted more and more attention in various fields. For achieving the synchronization of <b>neural</b> <b>networks,</b> especially of chaotic <b>neural</b> <b>networks,</b> many control methods and techniques have been adopted to design proper and effective controllers, such as feedback control, intermittent control, adaptive control, impulsive control, and so on.|$|R
