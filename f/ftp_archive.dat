13|27|Public
5000|$|The Guide and its {{associated}} <b>FTP</b> <b>archive</b> used to be located at hyperion.com before Grimm lost the domain. In the first-season episode [...] "A Voice in the Wilderness", Straczynski named the EAS Hyperion cruiser after the site.|$|E
50|$|Binaries are {{available}} for FreeBSD, Linux systems, Microsoft Windows, and FreeDOS. A number of Linux distributions, including Fedora, Slackware, Ubuntu, Debian, and Arch Linux, use xz for compressing their software packages. The GNU <b>FTP</b> <b>archive</b> also uses xz.|$|E
50|$|SUNET also {{operates}} one of Sweden's largest <b>FTP</b> <b>archive,</b> mirroring a lot {{of useful}} material of 'academic' interest, similar to the UK Mirror Service hosted by the University of Kent. The archive was upgraded {{in recent years and}} is nowadays dimensioned to handle more than 10 000 concurrent users.|$|E
50|$|Mimic also {{maintains}} the Christ ASCII Archive, {{one of the}} largest <b>FTP</b> <b>archives</b> of artpacks and collies in existence.|$|R
5000|$|Alternatively, the {{scientific}} data is {{made available to}} the public via several World Wide Web sites and <b>FTP</b> <b>archives,</b> such as: ...|$|R
40|$|A {{large part}} of the {{preliminary}} and not inconsiderable work of making the <b>ftp</b> <b>archives</b> of the "subversive discussion" was done by Colleen Wirth, Department of Mathematics, Princeton University, Princeton NJ 08540 (wirth@clarity. princeton. edu). Wirth {{is also one of the}} Assistant Editors of Psycoloquy, the Internet peer-reviewed journal edited by Stevan Harnad. The book originated from the <b>ftp</b> <b>archives.</b> The glossary was prepared by Ethan Starr, a graduate student at the Catholic University of America. Other locations: [URL]...|$|R
50|$|On July 12, 1998, Gossamer Germany (Germany.gossamer.org) {{was opened}} on the FU Berlin network that had hosted the Gossamer <b>FTP</b> <b>archive</b> since 1996. The name choice {{proved to be a}} bad decision, as many US-based fans refuse to use a site {{obviously}} located in Europe due to unfounded concerns about speed and access.|$|E
5000|$|Other grass-roots {{initiatives}} exist, like archivist Jason Scott's Textfiles.com project, the Code Archive (which {{attempts to}} archive GitHub), {{as well as}} the Internet Archive Wayback Machine. Software Heritage is gathering software that has free licenses from sources that include GitHub, Debian package archive, and GNU Project <b>FTP</b> <b>archive</b> and from entities like Gitorious and Google Code, projects that no longer exist.|$|E
50|$|As the Internet grew {{through the}} 1980s and early 1990s, many people {{realized}} the increasing {{need to be}} able to find and organize files and information. Projects such as Archie, Gopher, WAIS, and the <b>FTP</b> <b>Archive</b> list attempted to create ways to organize distributed data. In the early 1990s, Gopher, invented by Mark P. McCahill offered a viable alternative to the World Wide Web. However, in 1993 the World Wide Web saw many advances to indexing and ease of access through search engines, which often neglected Gopher and Gopherspace. As popularity increased through ease of use, investment incentives also grew until in the middle of 1994 the WWW's popularity gained the upper hand. Then it became clear that Gopher and the other projects were doomed fall short.|$|E
50|$|Alan Emtage (born November 27, 1964) {{conceived}} and implemented {{the first version}} of Archie, a pre-Web internet search engine for locating material in public <b>FTP</b> <b>archives.</b>|$|R
50|$|Widely {{cited as}} one of the first web based magazines, the Trincoll Journal began {{published}} on the web in November 1993. Prior to 1993 the Journal was published in Hypercard format and distributed via <b>FTP</b> <b>archives.</b>|$|R
50|$|In 1992 Jorge {{collected}} and widely disseminated (through the historic DEC gatekeeper <b>ftp</b> <b>archives</b> and Prime Time Freeware) {{a set of}} wordlists that later {{formed the basis of}} the ispell resources (later myspell, currently part of OpenOffice.org and Mozilla as hunspell).|$|R
50|$|Around 1994 and 1995, WADs were {{primarily}} distributed online over bulletin board systems or sold in collections on compact discs in computer shops, sometimes bundled with editing guide books. FTP servers became the primary method in later years. A few WADs {{have been released}} commercially, including the Master Levels for Doom II, which was released in 1995 along with Maximum Doom, a CD containing 1,830 WADs that had been downloaded from the Internet. Several thousand WADs have been created in total: the idgames <b>FTP</b> <b>archive</b> contains over 18,000 files, and this represents {{only a fraction of}} the complete output of Doom fans. Third party programs were also written to handle the loading of various WADs, since the game is a DOS game and all commands had to be entered on the command line to run. A typical launcher would allow the player to select which files to load from a menu, making it much easier to start. In 1995, WizardWorks released the D!Zone pack featuring hundreds of levels for Doom and Doom II. D!Zone was reviewed in Dragon by Jay & Dee; Jay gave the pack 1 out of 5 stars, while Dee gave the pack 1Â½ stars.|$|E
40|$|This memo {{provides}} {{information for the}} Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. This document {{provides information}} for the novice Internet user about using the File Transfer Protocol (FTP). It explains what FTP is, what anonymous FTP is, and what an anonymous <b>FTP</b> <b>archive</b> site is. It shows a sample anonymous FTP session. It also discusses common ways files are packaged for efficient storage and transmission. Acknowledgements This document {{is the result of}} work done in the Internet Anonymous FTP Archives (IAFA) working group of the IETF. Special thanks ar...|$|E
40|$|Accurate {{descriptions}} of the solar cycle variations of ionospheric parameters are an important goal of ionospheric modeling. Reliable predictions of these variations are of essential importance for almost all applications of ionospheric models. Unfortunately {{there are very few}} global data sources that cover a solar cycle or more. In an effort to expand the solar cycle coverage of data readily available for ionospheric modeling, we have processed a large number of satellite data sets from the sixties, seventies, and early eighties and have made them online accessible as part of NSSDC's <b>ftp</b> <b>archive</b> (ftv://nssdcftp. gsfc. nasa. aov/spacecraft data/) and it's ATMOWeb retrieval and plotting syste...|$|E
5000|$|The LNH {{moved to}} {{their very own}} newsgroup, alt.comics.lnh, before finding their {{permanent}} home on rec.arts.comics.creative. RACC also hosts {{a number of other}} original fiction shared universes, such as ASH, OMEGA and 8FOLD. Occasionally RACC receives cross-posts from Superguy. It is moderated by Russ [...] "Eagle" [...] Allbery, who also maintains the eyrie <b>ftp</b> <b>archives.</b>|$|R
50|$|Archie is a {{tool for}} {{indexing}} <b>FTP</b> <b>archives,</b> allowing people to find specific files. It {{is considered to be}} the first Internet search engine. The original implementation was written in 1990 by Alan Emtage, then a postgraduate student at McGill University in Montreal, and Bill Heelan, who studied at Concordia University in Montreal and worked at McGill University at the same time.|$|R
5000|$|Self-archiving {{was first}} {{explicitly}} proposed {{as a universal}} practice by Stevan Harnad in his 1994 online posting [...] "Subversive Proposal" [...] (later published in Association of Research Libraries) although computer scientists had been practicing self-archiving in anonymous <b>FTP</b> <b>archives</b> {{since at least the}} 1980s (see CiteSeer) and physicists had been doing it since the early 1990s on the web (see arXiv).|$|R
40|$|On the {{background}} of the constantly growing size and complexity of international networks the organization of public or file archives and archive arrays is getting more and more important. Based on statistical data of a selected Internet anonymous <b>FTP</b> <b>archive,</b> this study deals with some current problems of file archives in wide area networks. It presents mechanisms and solutions to reduce network traffic, costs, and overhead by improving transparency, consistency, and ease of use. Other topics being discussed in this paper are file replication, synchronization of distributed archives. Furthermore, it describes and comments on the protocols and tools currently in use and proposes requirements for possible new yet to be implemented protocols. [Page 1] 1. Introduction The need for file archives arises when individuals want to share resources, e. g. source code, binaries, text, graphics, or music over a certain distance. There may be several users in a research institute or a large comp [...] ...|$|E
40|$|The Protein Data Bank (PDB) is {{the single}} global {{repository}} for three-dimensional struc-tures of biological macromolecules and their complexes, and its more than 100 000 struc-tures contain more than 20000 distinct ligands or small molecules bound to proteins and nucleic acids. Information about these small molecules and their interactions with pro-teins and nucleic acids is crucial for our understanding of biochemical processes and vital for structure-based drug design. Small molecules present in a deposited structure may be attached to a polymer or may occur as a separate, non-covalently linked ligand. During curation of a newly deposited structure by wwPDB annotation staff, each mol-ecule is cross-referenced to the PDB Chemical Component Dictionary (CCD). If the mol-ecule is new to the PDB, a dictionary description is created for it. The information about all small molecule components found in the PDB is distributed via the <b>ftp</b> <b>archive</b> as an external reference file. Small molecule annotation in the PDB also includes information about ligand-binding sites and about covalent and other linkages between ligands and macromolecules. During the remediation of the peptide-like antibiotics and inhibitor...|$|E
30|$|DIUP reads all DICOM {{files from}} a CD on any {{workstation}} in the hospital. After loading the CD, the patient {{name and the}} available series are displayed. The physician can choose the relevant series and push these to a file transfer protocol (ftp) server after providing the in-house patient ID and checking whether the selected data are correct. This forces the physician to check whether the patient is registered at our hospital, and if not, to register the patient first. Replacement of the patient ID with the in-house patient ID is performed during the transfer to the central ftp server. At the radiology department, the data are retrieved from the <b>ftp</b> <b>archive</b> and stored at the appropriate DICOM node. During the storage process, the information regarding a patient having external data is recorded. The uploaded images will be updated with patient identifiers from our institutionâs radiology information system (RIS) by overwriting the original patient ID and accession number with the correct patient ID and accession number in the DICOM header. Image-related materials such as radiology reports or informed consents are obtained from other institutions via email or fax. These image-related materials were not stored in the system but delivered separately to the physicians. Later, the stored data are subsequently removed from the ftp site if the transfer is successfully finished. Using DIUP, physicians can also track whether their CD has already been stored into the DICOM node, thus avoiding resending the same CD over and over.|$|E
5000|$|Despite these {{statements}} {{emerging in the}} 2000s, the idea and practise of providing free online access to journal articles began {{at least a decade}} before the term [...] "open access" [...] was formally coined. Computer scientists had been self-archiving in anonymous <b>ftp</b> <b>archives</b> since the 1970s and physicists had been self-archiving in arxiv since the 1990s. The Subversive Proposal to generalize the practice was posted in 1994.|$|R
5000|$|Green {{open access}} journal {{publishers}} endorse immediate open access self-archiving by their authors. Open access self-archiving was first formally proposed in 1994 by Stevan Harnad in his [...] "Subversive Proposal". However, self-archiving was already {{being done by}} computer scientists in their local <b>FTP</b> <b>archives</b> in the 1980s, later harvested into CiteSeer. What is deposited can be either a preprint, or the peer-reviewed postprint - either the author's refereed, revised final draft or the publisher's version of record.|$|R
50|$|The archie service {{began as}} a project for {{students}} and volunteer staff at the McGill University School of Computer Science in 1987, when Peter Deutsch (systems manager for the School), Emtage, and Heelan were asked to connect the School of Computer Science to the Internet. The earliest versions of Archie, written by Alan Emtage, simply contacted a list of <b>FTP</b> <b>archives</b> {{on a regular basis}} (contacting each roughly once a month, so as not to waste too many resources of the remote servers) and requested a listing. These listings were stored in local files to be searched using the Unix grep command.|$|R
40|$|Modeling of {{the topside}} {{ionosphere}} {{has for the}} most part relied on just a few years of data from topside sounder satellites. The widely used Bent et al. (1972) model, for example, is based on only 50, 000 Alouette 1 profiles. The International Reference Ionosphere (IRI) (Bilitza, 1990, 2001) uses an analytical description of the graphs and tables provided by Bent et al. (1972). The Alouette 1, 2 and ISIS 1, 2 topside sounder satellites of the sixties and seventies were ahead of their times in terms of the sheer volume of data obtained {{and in terms of the}} computer and software requirements for data analysis. As a result, only a small percentage of the collected topside ionograms was converted into electron density profiles. Recently, a NASA-funded data restoration project has undertaken and is continuing the process of digitizing the Alouette/ISIS ionograms from the analog 7 -track tapes. Our project involves the automated processing of these digital ionograms into electron density profiles. The project accomplished a set of important goals that will have a major impact on understanding and modeling of the topside ionosphere: (1) The TOPside Ionogram Scaling and True height inversion (TOPIST) software was developed for the automated scaling and inversion of topside ionograms. (2) The TOPIST software was applied to the over 300, 000 ISIS- 2 topside ionograms that had been digitized in the fkamework of a separate AISRP project (PI: R. F. Benson). (3) The new TOPIST-produced database of global electron density profiles for the topside ionosphere were made publicly available through NASA s National Space Science Data Center (NSSDC) <b>ftp</b> <b>archive</b> at. (4) Earlier Alouette 1, 2 and ISIS 1, 2 data sets of electron density profiles from manual scaling of selected sets of ionograms were converted fiom a highly-compressed binary format into a user-friendly ASCII format and made publicly available through nssdcftp. gsfc. nasa. gov. The new database for the topside ionosphere established as a result of this project, has stimulated a multitude of new studies directed towards a better description and prediction of the topside ionosphere. Marinov et al. (2004) developed a new model for the upper ion transition height (Oxygen to Hydrogen and Helium) and Bilitza (2004) deduced a correction term for the I N topside electron density model. Kutiev et al. (2005) used this data to develop a new model for the topside ionosphere scale height (TISH) as a function of month, local time, latitude, longitude and solar flux F 10. 7. Comparisons by Belehaki et al. (2005) show that TISH is in general agreement with scale heights deduced from ground ionosondes but the model predicts post-midnight and afternoon maxima whereas the ionosonde data show a noon maximum. Webb and Benson (2005) reported on their effort to deduce changes in the plasma temperature and ion composition from changes in the topside electron density profile as recorded by topside sounders. Limitations and possible improvements of the IRI topside model were discussed by Coisson et al. (2005) including also the possible use of the NeQuick model, Our project progressed in close collaboration and coordination with the GSFC team involved in the ISIS digitization effort. The digitization project was highly successful producing a large amount of digital topside ionograms. Several no-cost extensions of the TOPIST project were necessary to keep up with the pace and volume of the digitization effort...|$|E
40|$|The {{information}} retrieval task {{is larger than}} the problem of searching for documents on the Web. In this paper we broaden our analysis to include search logs of many Web search engines, peer-to-peer query logs, newsgroup search logs, and queries to <b>FTP</b> <b>archives.</b> We calculate and compare the characteristics of each of these query logs from 2003 to find commonalities and differences across a wide spectrum of online query workloads. We found Boolean operator usage to be rare; much longer queries in peer-to-peer traffic than Web; that searchers click on slightly more than two results per query; that peer-to-peer and FTP logs are more likely to include file-type extensions; and, that caching of query results is likely to be of value for both WWW and peer-to-peer traffic. ...|$|R
40|$|Despite {{a variety}} of problems, {{electronic}} publishing on BITNET, Internet, and other networks is experiencing vigorous growth as scholars experiment with <b>FTP</b> <b>archives,</b> list servers, WAIS servers, and other technological tools in order to reinvent scholarly publishing. This bibliography presents selected sources, in both paper and electronic form, that are useful in understanding network-based electronic publishing. A limited number of sources that deal with broader electronic publishing topics, such as intellectual property rights, multimedia systems, standards, and virtual libraries, are also included; however, this bibliography does not provide an in-depth treatment of the large and diverse body of literature that deals with electronic publishing as a whole. In order to focus on recent developments, it does not cover sources published prior to 1989 (most sources are from 1990 to the present) ...|$|R
40|$|This memo defines an Experimental Protocol for the Internet community. It {{does not}} specify an Internet {{standard}} of any kind. Discussion {{and suggestions for}} improvement are requested. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (1999). All Rights Reserved. The Summary Object Interchange Format [Ref. 1] was first defined by the Harvest Project [Ref 2. ] in January 1994. SOIF was derived {{from a combination of}} the Internet Anonymous <b>FTP</b> <b>Archives</b> IETF Working Group (IAFA) templates [Ref 3. ] and the BibTeX bibliography format [Ref 4. ]. The combination was originally noted for its advantages of providing a convenient and intuitive way for delimiting objects within a stream, and setting apart the URL for easy object access or invocation, while still preserving compatibility with IAFA templates...|$|R
40|$|Because {{of their}} {{increasing}} popularity, Internet information {{services such as}} the Web, Internet <b>FTP</b> <b>archives,</b> and Network News, replicate their servers to improve availability, response time, and fault tolerance. Traditional replication algorithms do not address the scale and administrative decentralization of today's internetworks. They manage a single group of replicas and rely on system administrators to hand configure the topologies over which updates travel. While this is appropriate for applications with {{a small number of}} replicas that operate within single administrative boundaries, it does not scale in wide-area, highly replicated services whose replicas spread throughout the Internet's thousands of autonomously administered domains. We have proposed and implemented a scalable and efficient tool to replicate widearea, autonomously managed services. We target replication degrees of thousands of weakly consistent replicas. The main goal of our replication tool is to make traditio [...] ...|$|R
5000|$|Most of the {{well-known}} or [...] "elite" [...] cracking groups make software cracks entirely for respect in the [...] "Scene", not profit. From there, the cracks are eventually leaked onto public Internet sites by people/crackers who use well-protected/secure <b>FTP</b> release <b>archives,</b> {{which are made}} into full copies and sometimes sold illegally by other parties.|$|R
50|$|Nomad.NET is a {{freeware}} orthodox {{file manager}} (OFM) for Microsoft Windows. Some features include a built-in <b>FTP</b> client, <b>archive</b> file navigation, folder comparison and synchronization, and a multi-file renaming tool. In {{addition to being}} an orthodox file manager, Nomad.NET features two-windowed mode, tree window for each panel, horizontal and vertical splitting of the windows, tab browsing etc.|$|R
5000|$|The Juggling Information Service or JIS is {{a website}} {{with the goal}} of being, [...] "the primary informational {{resource}} on the subject of juggling." [...] Launched in 1994, the free information service is a successor to the <b>FTP</b> juggling <b>archive</b> at Indiana University. The website is maintained by five people in various locations, primarily Barry Bakalor.|$|R
5000|$|The [...] "Subversive Proposal" [...] was an Internet posting by Stevan Harnad on June 27 1994 (presented at the 1994 Network Services Conference in London [...] ) {{calling on}} all authors of [...] "esoteric" [...] {{research}} writings to archive their articles for free for everyone online (in anonymous <b>FTP</b> <b>archives</b> or websites). It initiated {{a series of}} online exchanges, {{many of which were}} collected and published as a book in 1995. This led to the creation in 1997 of Cogprints, an open access archive for self-archived articles in the cognitive sciences and in 1998 {{to the creation of the}} American Scientist Open Access Forum (initially called the [...] "September98 Forum" [...] until the founding of the Budapest Open Access Initiative which first coined the term [...] "Open Access"). The Subversive Proposal also led to the development of the GNU EPrints software used for creating OAI-compliant open access institutional repositories.|$|R
5000|$|GameFAQs {{was started}} as the Video Game FAQ Archive on November 5, 1995, by gamer and {{programmer}} Jeff Veasey, {{who says he}} wanted to collect the numerous online guides and FAQs into one centralized location. [...] Hosted on America Online, it originally served as a mirror of Andy Eddy's <b>FTP</b> FAQ <b>archive.</b> The initial version of the site had approximately 10 pages and 100 FAQs. In 1996, the site moved to its current domain at gamefaqs.com and {{changed its name to}} GameFAQs. At this time, GameFAQs listed fewer than 1000 FAQs and guides and was updated on an irregular basis.|$|R
50|$|Lisa (Gossamer Birdfeeder/story cleanup), Adam (Gossamer Australia), Deirdre (Gossamer Simplenet/story cleanup), Chael (Gossamer X-Philes/technical), Harri (Gossamer Finland), Vera (Gossamer <b>FTP),</b> Amy (Specialty <b>Archive),</b> Michelle (Unfinished and Serial Archive), and Gem (Database Administrator) {{took over}} maintenance. The {{archives}} were fully integrated, {{using the same}} database of files, containing the same story files, and posting story updates at around the same time.|$|R
50|$|The site {{grew out}} of a network of <b>FTP</b> {{software}} and <b>archived</b> data which was put together by students at Munich University of Technology and Ludwig Maximilians University of Munich even before HTML and HTTP existed. The original aim was to create a single, huge archive by linking up archives run by the different research groups (hence its name LEO - Link Everything Online. The archive was sorted thematically and the different sections organised and kept up to date by archivists.|$|R
