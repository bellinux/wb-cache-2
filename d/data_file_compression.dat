2|10000|Public
50|$|<b>Data</b> <b>file</b> <b>compression</b> is {{employed}} {{in order to}} reduce bandwidth, file transfer time, or storage requirements. A digital recording (such as a CD) encoded to the Monkey's Audio format can be decompressed into an identical copy of the original audio data. As with the FLAC and Apple Lossless format, files encoded to Monkey's Audio are typically reduced to about half of the original size, with data transfer rates and bandwidth requirements being reduced accordingly.|$|E
40|$|Part 2 : The 2014 Asian Conference on Availability, Reliability and Security, AsiaARES 2014 International audienceIn Network Anomaly and Botnet Detection {{the main}} source of input for {{analysis}} is the network traffic, which has to be transmitted from its capture source to the analysis system. High-volume data sources often generate traffic volumes prohibiting direct pass-through of bulk data into researchers hands. In this paper we achieve a reduction in volume of transmitted test data from network flow captures by aggregating raw data using extraction of protocol semantics. This is orthogonal to classic bulk compression algorithms. We propose a formalization for this concept called Descriptors and extend it to network flow data. A comparison with common bulk <b>data</b> <b>file</b> <b>compression</b> formats will be given for full Packet Capture (PCAP) files, giving 4 to 5 orders of magnitude in size reduction using Descriptors. Our approach aims to be compatible with Internet Protocol Flow Information Export (IPFIX) and other standardized network flow data formats as possible inputs...|$|E
40|$|Data {{compression}} {{has been}} {{claimed to be}} an attractive solution to save energy consumption in high-end servers and data centers. However, {{there has not been}} a study to explore this. In this paper, we present a comprehensive evaluation of energy consumption for various <b>file</b> <b>compression</b> techniques implemented in software. We apply various compression tools available on Linux to a variety of <b>data</b> <b>files,</b> and we try them on server class and workstation class systems. We compare their energy and performance results against raw reads and writes. Our results reveal that software based data compression cannot be considered as a universal solution to reduce energy consumption. Various factors like the type of the <b>data</b> <b>file,</b> the <b>compression</b> tool being used, the read-to-write ratio of the workload, and the hardware configuration of the system impact the efficacy of this technique. In some cases, however, we found compression to save substantial energy and improve performance...|$|R
50|$|The {{integrated}} Archive Manager plugin supports numerous <b>data</b> <b>compression</b> <b>file</b> formats such as ZIP files. Furthermore, {{a support}} for metadata {{systems such as}} Exif, IPTC and ID3-Tags for audio and video files, and other documents (e.g. pdf) is integrated. This metadata {{can be used for}} example by means of an extended tool for renaming files. It is possible to define actions depending on certain file extensions and to start these actions via a pop-up menu by right-clicking on the file.|$|R
40|$|Data {{captured}} by flight data recorders are generally stored on the system’s embedded hard disk. A common {{problem is the}} lack of storage space on the disk. This lack of space for data storage leads to either a constant effort to reduce the space used by data, or to increased costs due to acquisition of additional space, which is not always possible. <b>File</b> <b>compression</b> can solve the problem, but carries with it the potential drawback of the increased overhead required when writing the data to the disk, putting an excessive load on the system and degrading system performance. The author suggests the use of an efficient compressed file system that both compresses data in real time and ensures that there will be minimal impact on the performance of other tasks. Keywords: flight <b>data</b> recorder, <b>data</b> <b>compression,</b> <b>file</b> syste...|$|R
5000|$|The {{advantage}} of lossy methods over lossless methods {{is that in}} some cases a lossy method can produce a much smaller compressed file than any lossless method, while still meeting the requirements of the application. Lossy methods are most often used for compressing sound, images or videos. This is because these types of data are intended for human interpretation where the mind can easily [...] "fill in the blanks" [...] or see past very minor errors or inconsistencies - ideally lossy compression is transparent (imperceptible), which can be verified via an ABX test. <b>Data</b> <b>files</b> using lossy <b>compression</b> are smaller in size and thus cost less to store and to transmit over the Internet, a crucial consideration for streaming video services such as Netflix and streaming audio services such as Spotify.|$|R
40|$|Data {{in volume}} form consumes an {{extraordinary}} amount of storage space. For efficient storage and transmission of such data, compression algorithms are imperative. However, most volumetric datasets are used in biomedicine and other scientific applications where lossy compression is unacceptable. We present a lossless data-compression algorithm which, being oriented specifically for volume data, achieves greater compression performance than generic compression algorithms that are typically available on modern computer systems. Our algorithm is a combination of differential pulse-code modulation (DPCM) and Huffman coding and results in compression of around 50 % for a set of volume <b>data</b> <b>files.</b> I. Introduction <b>Compression</b> for efficient storage and transmission of digital data has become routine as the application of such data has grown. Several common datacompression programs are readily available on many computers to fight the burgeoning demand for storage space. These programs are typica [...] ...|$|R
40|$|International audienceIn {{this article}} we propose a parameter-free method for Remote Sensing (RS) image {{databases}} Data Mining (DM). DM of RS images requires methodologies robust to the diversity of context found in such large datasets, as well as methodologies with low computational costs and low memory requirements. The methodology that we propose {{is based on the}} Normalized Compression Distance (NCD) over lossless compressed data. Normalized Compression Distance is a measure of similarity between two <b>data</b> <b>ﬁles</b> using the <b>compression</b> factor as an approximation to the Kolmogorov complexity. This approach allows to directly compare information from two images using the lossless compressed original ﬁles, and avoiding the feature extraction/selection process commonly used in pattern recognition techniques. This shortcut makes the proposed methodology suitable for DM applications in RS. We provided a classiﬁcation experiment with hyperspectral data exemplarizing our methodology and comparing it with common methodologies found on the literature...|$|R
5000|$|FCX <b>file</b> <b>compression</b> is a <b>file</b> <b>compression</b> {{utility and}} <b>file</b> format. [...] It is {{supported}} {{on a large}} number of platforms. [...] It is published by Compact Data Works and was originally released in 1988 for VAX/VMS.|$|R
50|$|See Lossless {{compression}} benchmarks {{for a list}} of <b>file</b> <b>compression</b> benchmarks.|$|R
50|$|There are {{two types}} of image <b>file</b> <b>compression</b> algorithms: {{lossless}} and lossy.|$|R
5000|$|Audio <b>file</b> <b>compression</b> to {{save time}} and space (30 to 60% savings).|$|R
5000|$|PKZIP, the {{compression}} utility that {{quickly became the}} standard in <b>file</b> <b>compression.</b>|$|R
50|$|Transparent <b>file</b> <b>compression</b> {{was also}} supported, {{although}} this {{had a significant}} impact on the performance of file serving.|$|R
5000|$|... 6 {{red book}} audio discs {{for the price}} of one Yellow Book (CD-ROM), {{depending}} on <b>file</b> <b>compression</b> rates ...|$|R
5000|$|Version 9 enables {{support for}} <b>file</b> <b>compression,</b> deduplication, and {{partition}} directories. Version 9 {{was introduced in}} VxFS 6.0.|$|R
40|$|This {{paper will}} discuss about the {{evolution}} of Geography Markup Language (compression model. GML {{is a type of}} XML files normally used to store spatial data from database. However due to the huge size processing and transferring this type of file will cost performance and storage issue. Throughout the years several GML <b>file</b> <b>compression</b> model has been developed to help in addressing this problem. Four GML <b>file</b> <b>compression</b> model which are GPress, Delta Sp Compression and Extrapolation Model, GMill, and GPress++ has been selected to be discussed in this paper. In addition a comparison and the enhancement done in each model will be discussed in here. From the assessment GPress++ compression model h significant <b>file</b> <b>compression</b> rate on synthetic dataset with 77. 10...|$|R
50|$|WinZip 16.5 {{supports}} OpenCL acceleration to <b>file</b> <b>compression</b> {{engine for}} AMD Fusion processors and AMD Radeon users, streamlined interface.|$|R
5000|$|... lzop {{is a free}} {{software}} <b>file</b> <b>compression</b> tool which implements the LZO algorithm and is licensed under the GPL.|$|R
50|$|There is no <b>file</b> <b>compression,</b> and {{therefore}} these load {{very quickly and}} without much programming when displayed in native mode.|$|R
5000|$|G.722 is a freely {{available}} {{file format}} for audio <b>file</b> <b>compression.</b> The <b>files</b> are often named with the extension [...] "722".|$|R
5000|$|CNET rated it 4/5 {{stars and}} wrote, [...] "Easy program {{operation}} sets this freeware <b>file</b> <b>compression</b> tool {{apart from the}} crowded genre." ...|$|R
50|$|Solid PDF Creator {{provides}} {{a variety of}} file conversion options including password protection, encryption, permission definition, ISO 19005-1 archiving standards, and <b>file</b> <b>compression</b> capabilities.|$|R
5000|$|The Maximum Compression benchmark, {{started in}} 2003 and updated until November 2011, {{includes}} over 150 programs. Maintained by Werner Bergmans, it tests {{on a variety}} of data sets, including text, images, and executable code. Two types of results are reported: single <b>file</b> <b>compression</b> (SFC) and multiple <b>file</b> <b>compression</b> (MFC). Not surprisingly, context mixing programs often win here; programs from the PAQ series and WinRK often are in the top. The site also has a list of pointers to other benchmarks.|$|R
50|$|All three formats have {{support for}} compression, however only the PHAR format {{supports}} both per-file and whole archive compression. Zip and Tar formats only support per-file and whole <b>file</b> <b>compression</b> respectively.|$|R
40|$|Combining {{techniques}} used in image processing, digital photogrammetry {{has been proven}} to significantly increase productivity over analytical methods. In certain areas, digital photogrammetry has shown itself to be far superior to conventional methods of data collection. Softcopy (digital) photogrammetry raster files take up large amounts of storage space. Larger pixel sizes may help to reduce file size, {{but it has been}} shown in studies that larger pixel sizes may lead to less accurate results. What is the user to do? The importance of <b>file</b> <b>compression</b> should not be underestimated. File sizes affect almost every step of the digital photogrammetric workflow. Without some type of significant <b>file</b> <b>compression,</b> the entire process becomes complicated with excessive <b>data</b> <b>file</b> management. Depending on the specific workflow, file sizes are also critical in desktop environments where data storage may be small (e. g. heads-up digitising on digital orthophotos in a PC environment). The JPEG image compression/decompression algorithm is a compression method which utilises a variable compression factor or Q-factor. Files can be significantly reduced in size without any visual loss of image quality. Typical reduction is one-third to one-fourth the uncompressed size. The {{purpose of this paper is}} to compare the accuracy and speed of processing uncompressed digital imagery with that of compressed imagery throughout a digital photogrammetric workflow. Each process was timed, including; film scanning, file transfer, epipolar resampling, automatic DTM collection, orthophoto production, creating image pyramids and image backup and restore. By using only hardware and software to perform the above tasks, human operator bias will be removed from the process completely...|$|R
50|$|The EGG {{file format}} is a {{compressed}} archive file format that supports Unicode and intelligent compression algorithms. EGG format {{is created by}} ESTsoft and was first applied in their <b>file</b> <b>compression</b> software ALZip.|$|R
25|$|Other {{specifications}} {{that were}} commonly used provided for echomail, different transfer protocols and handshake methods (e.g.: Yoohoo/Yoohoo2u2, EMSI), <b>file</b> <b>compression,</b> nodelist format, transfer over reliable connections such as the Internet (Binkp), and other aspects.|$|R
50|$|As {{with many}} {{commercial}} file systems, Fusion File System {{is accompanied by}} a full suite of supporting software modules which include: error correction, <b>file</b> <b>compression,</b> Fusion Standard C lib, driver model and an I/O manager.|$|R
50|$|The StuffIt and StuffIt X formats remain, {{unlike some}} other <b>file</b> <b>compression</b> formats, proprietary, and Smith Micro Software charge license fees {{for its use}} in other programs. Given this, few {{alternative}} programs support the format.|$|R
5000|$|Integrated {{support for}} “deflate”, bzip2, and 7-Zip LZMA <b>file</b> <b>compression.</b> The {{installer}} {{has the ability}} to compare file version information, replace in-use files, use shared file counting, register DLL/OCXs and type libraries, and install fonts.|$|R
30|$|For <b>data</b> <b>file</b> preservation, a {{standardized}} <b>data</b> <b>file</b> containing {{all types of}} necessary information can contribute to this issue because a single <b>data</b> <b>file</b> carries all the necessary information for scientific analysis. The practical information contained in <b>data</b> <b>files,</b> such as file names with version numbers of the source <b>data</b> <b>files,</b> and computer codes to generate <b>data</b> <b>files,</b> enables automatic processing of <b>data</b> <b>files</b> in many tasks for maintenance of the data archive. In addition to these efforts, the <b>data</b> <b>files</b> are synchronized between the ERG-SC at Nagoya University and ISAS/JAXA for redundancy.|$|R
50|$|An Apple {{disk image}} allows secure {{password}} protection {{as well as}} <b>file</b> <b>compression,</b> and hence serves both security and file distribution functions; such a disk image is most commonly used to distribute software over the Internet.|$|R
50|$|Synex Systems {{products}} were diverse and targeted accounting, civil engineering, minicomputer thin client, and <b>file</b> <b>compression</b> utility markets. By 2001 the concentration was {{only on the}} accounting reporting product F9 and all other {{products were}} discontinued or sold.|$|R
40|$|Transparent <b>file</b> <b>compression</b> systems handle <b>file</b> <b>compression</b> and {{decompression}} {{of files}} automatically; they are thus transparent to users. The cost of many transparent <b>file</b> system <b>compression</b> schemes, however, is that file accesses require substantial processor time for compression and decompression. One approach to alleviating {{this problem is}} to cache recently accessed files (active files) in uncompressed format. This paper examines the design and implementation of a system built on Linux, a free UNIX-like operating system, and e 2 compr, an existing transparent compression patch for Linux, to {{provide a framework for}} experimenting with the exact heuristics underlying such caching. 1 Introduction It is a commonly recognized problem that no matter how quickly disk space becomes available, it rapidly fills up; colloquially, you can never have enough disk space. On modern systems, users can use compression utilities such as gzip to reduce the disk space consumed by large files; however, f [...] ...|$|R
50|$|Before macOS, AppleSingle and Double {{had little}} {{presence}} in the Mac market, due largely to the small market share of A/UX. Nevertheless, they did force various <b>file</b> <b>compression</b> vendors to add support for the formats, and confuse future MacBinary versions.|$|R
