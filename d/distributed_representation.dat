371|799|Public
25|$|A neural mapping connects two {{cortical}} neural maps. Neural mappings (in {{contrast to}} neural pathways) store training information by adjusting their neural link weights (see artificial neuron, artificial neural networks). Neural mappings {{are capable of}} generating or activating a <b>distributed</b> <b>representation</b> (see above) of a sensory or motor state within a sensory or motor map from a punctual or local activation within the other map (see for example the synaptic projection from speech sound map to motor map, to auditory target region map, or to somatosensory target region map in the DIVA model, explained below; or see for example the neural mapping from phonetic map to auditory state map and motor plan state map in the ACT model, explained below and Fig. 3).|$|E
2500|$|On the {{one hand}} the articulatory model generates sensory information, i.e. an {{auditory}} state for each speech unit which is neurally represented within the auditory state map (<b>distributed</b> <b>representation),</b> and a somatosensory state for each speech unit which is neurally represented within the somatosensory state map (<b>distributed</b> <b>representation</b> as well). The auditory state map {{is assumed to be}} located in the superior temporal cortex while the somatosensory state map is assumed to [...] be located in the inferior parietal cortex.|$|E
5000|$|Temporal memory using sparse <b>distributed</b> <b>representation</b> US 20110225108 A1 Numenta, 2011 ...|$|E
40|$|Syntactic parsing {{is a key}} task {{in natural}} {{language}} processing which {{has been dominated by}} symbolic, grammar-based syntactic parsers. Neural networks, with their <b>distributed</b> <b>representations,</b> are challenging these methods. In this paper, we want to show that existing parsing algorithms can cross the border and be defined over <b>distributed</b> <b>representations.</b> We then define D-CYK: a version of the traditional CYK algorithm defined over <b>distributed</b> <b>representations.</b> Our D-CYK operates as the original CYK but uses matrix multiplications. These operations are compatible with traditional neural networks. Experiments show that D-CYK approximates the original CYK. By showing that CYK can be performed on <b>distributed</b> <b>representations,</b> our D-CYK opens the possibility of defining recurrent layers of CYK-informed neural networks...|$|R
5000|$|... #Subtitle level 3: Quantum {{computing}} via Sparse <b>Distributed</b> <b>Representations</b> ...|$|R
50|$|Hierarchical {{temporal}} memory utilizes SDM {{for storing}} sparse <b>distributed</b> <b>representations</b> of the data.|$|R
50|$|On the {{one hand}} the articulatory model generates sensory information, i.e. an {{auditory}} state for each speech unit which is neurally represented within the auditory state map (<b>distributed</b> <b>representation),</b> and a somatosensory state for each speech unit which is neurally represented within the somatosensory state map (<b>distributed</b> <b>representation</b> as well). The auditory state map {{is assumed to be}} located in the superior temporal cortex while the somatosensory state map is assumed to be located in the inferior parietal cortex.|$|E
50|$|Van Gelder's {{research}} has had three main phases, corresponding to his PhD research on <b>distributed</b> <b>representation,</b> his subsequent research on dynamics & cognition, and his current phase, research into reasoning skills.|$|E
5000|$|In his PhD thesis, {{completed}} {{under the}} supervision of John Haugeland and entitled [...] "Distributed Representation" [...] (1989) van Gelder gave the first sustained exploration of the general concept of <b>distributed</b> <b>representation,</b> and argued that it was a third fundamental kind of representation alongside language and imagery.|$|E
40|$|Abstract:- In this paper, we {{describe}} a fault-tolerant Neuro-Fuzzy inference system for performing fuzzy reasoning using coarse-coded <b>distributed</b> <b>representations.</b> The system implements the fuzzy membership functions {{in a novel}} way using coarse-coded <b>distributed</b> <b>representations</b> for the inputs and outputs of neural networks. <b>Distributed</b> <b>representations</b> are known to give advantages of fault tolerance, generalization and graceful degradation of performance under noise conditions. Performance of the Neuro-Fuzzy inference system with regard to its ability to exhibit fault tolerance under noise conditions is studied. The system offered very good results of fault tolerance under noise conditions. It has also exhibited good generalization ability on unseen test inputs...|$|R
25|$|By contrast, brains use sparse <b>distributed</b> <b>representations</b> (SDR). The human neocortex has roughly 100 billion neurons, but at {{any given}} time only a small percent are active. The {{activity}} of neurons are like bits in a computer, and therefore the representation is sparse. Similarly to SDM developed by NASA in the 80s and vector space models used in Latent semantic analysis, HTM also uses Sparse <b>Distributed</b> <b>Representations.</b>|$|R
40|$|Definition: <b>Distributed</b> <b>representations</b> {{are a way}} of {{representing}} information in a pattern of activation over a set of neurons, in which each concept is represented by activation over multiple neurons, and each neuron participates in the representation of multiple concepts. Contents The importance of representation [...] . 2 Properties of <b>distributed</b> <b>representations</b> [...] 2 Similarity and generalization [...] . 2 Superposition, multiple concepts, interference & ghosting [...] . ...|$|R
50|$|The {{grandmother}} cell hypothesis, {{is an extreme}} version {{of the idea of}} sparseness, and is not without critics. The opposite of the {{grandmother cell}} theory is the <b>distributed</b> <b>representation</b> theory, that states that a specific stimulus is coded by its unique pattern of activity over a large group of neurons widely distributed in the brain.|$|E
5000|$|The {{result of}} this process will be a word-vector {{containing}} all the contexts in which the word Y appears and will therefore {{be representative of the}} semantics of that word in the semantic space. It can be seen that the resulting word-vector is also in a sparse <b>distributed</b> <b>representation</b> (SDR) format 1993 & 2006. Some properties of word-SDRs that are of particular interest with respect to computational semantics are: ...|$|E
50|$|Brown v. The Board of Commissioners of the City of Chattanooga (1989) was the {{restructuring}} of the election process of Chattanooga's Board of Commissioners due to its unconstitutionality as it contradicted Section 2 of the Federal Voting Rights Act of 1965. Filed by twelve citizens in November 1987 under the United States District Court for the Eastern District of Tennessee, Southern Division, the case provided for a more equally <b>distributed</b> <b>representation</b> of the citizens, particularly the city's minority groups, of Chattanooga, TN.|$|E
40|$|In this paper, we {{describe}} {{a model for}} reasoning using forward chaining for predicate logic rules and facts with coarse-coded <b>distributed</b> <b>representations</b> for instantiated predicates in a connectionist frame work. <b>Distributed</b> <b>representations</b> are known to give advantages of good generalization, error correction and graceful degradation of performance under noise conditions. The system supports usage of complex rules which involve multiple conjunctions and disjunctions, The system solves the variable binding problem {{in a new way}} using coarse-coded <b>distributed</b> <b>representations</b> of instantiated predicates without the need to decode them into localist representations. Its performance with regard to generalization on unseen inputs and its ability to exhibit fault tolerance under noise conditions is studied and has been found to give good results...|$|R
50|$|The {{assumption}} underlying <b>distributed</b> <b>representations</b> is that {{observed data}} are {{generated by the}} interactions of layered factors.|$|R
50|$|Layer or unit-group level {{inhibition}} can be computed directly using a k-winners-take-all (KWTA) function, producing sparse <b>distributed</b> <b>representations.</b>|$|R
50|$|Word2vec can utilize {{either of}} two model {{architectures}} {{to produce a}} <b>distributed</b> <b>representation</b> of words: continuous bag-of-words (CBOW) or continuous skip-gram. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note, CBOW is faster while skip-gram is slower but does a better job for infrequent words.|$|E
5000|$|The word {{embedding}} technique began development in 2000. Bengio et al. provided {{in a series}} of papers the [...] "Neural probabilistic language models" [...] to reduce the high dimensionality of words representations in contexts by [...] "learning a <b>distributed</b> <b>representation</b> for words". (Bengio et al, 2003). Roweis and Saul published in Science how to use [...] "locally linear embedding" [...] (LLE) to discover representations of high dimensional data structure. The area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model.|$|E
5000|$|Instead {{of using}} neural net {{language}} models to produce actual probabilities, {{it is common}} to instead use the <b>distributed</b> <b>representation</b> encoded in the networks' [...] "hidden" [...] layers as representations of words; each word is then mapped onto an -dimensional real vector called the word embedding, where [...] is the size of the layer just before the output layer. The representations in skip-gram models have the distinct characteristic that they model semantic relations between words as linear combinations, capturing a form of compositionality. For example, in some such models, if [...] is the function that maps a word [...] to its -d vector representation, then ...|$|E
40|$|Abstract. It is {{a widely}} held view in {{contemporary}} computational neuroscience that the brain responds to sensory input by producing sparse <b>distributed</b> <b>representations.</b> In this paper we investigate a brain-inspired spatial pooling algorithm that produces such sparse <b>distributed</b> <b>representations</b> by modelling the formation of proximal dendrites associated with neocortical minicolumns. In this approach, <b>distributed</b> <b>representations</b> are formed out of a competitive process of inter-column inhibition and subsequent learning. Specifically, we evaluate {{the performance of a}} recently proposed binary spatial pooling algorithm on a well-known benchmark of greyscale natural images. Our main contribution is to augment the algorithm to handle greyscale images, and to produce better quality encodings of binary images. We also show that the augmented algorithm produces superior population and lifetime kurtosis measures in comparison to a number of other well-known coding schemes. ...|$|R
40|$|Learning <b>distributed</b> <b>representations</b> for {{relation}} instances is {{a central}} technique in downstream NLP applications. In order to address semantic modeling of relational patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset. In addition, we conduct a comparative study of different encoders including additive composition, RNN, LSTM, and GRU for composing <b>distributed</b> <b>representations</b> of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of <b>distributed</b> <b>representations</b> of relational patterns in the relation classification task. Comment: Published as a conference paper at ACL 201...|$|R
40|$|There {{has been}} much {{interest}} {{in the possibility of}} connectionist models whose representations can be endowed with compositional structure, and a variety of such models have been proposed. These models typically use <b>distributed</b> <b>representations</b> that arise from the functional composition of constituent parts. Functional composition and decomposition alone, however, yield only an implementation of classical symbolic theories. This paper explores the possibility of moving beyond implementation by exploiting holistic structure-sensitive operations on <b>distributed</b> <b>representations.</b> An experiment is performed using Pollack’s Recursive Auto-Associative Memory. RAAM is used to construct <b>distributed</b> <b>representations</b> of syntactically structured sentences. A feed-forward network is then trained to operate directly on these representations, modeling syntactic transformations of the represented sentences. Successful training and generalization is obtained, demonstrating that the implicit structure present in these representations can be used for a kind of structure-sensitive processing unique to the connectionist domain. ...|$|R
5000|$|Rinkus [...] {{proposes that}} <b>distributed</b> <b>representation,</b> {{specifically}} sparse <b>distributed</b> <b>representation</b> (SDR), provides a classical implementation of quantum computing. Specifically, {{the set of}} SDR codes stored in an SDR coding field will generally intersect {{with each other to}} varying degrees. In other work, Rinkus describes a fixed time learning (and inference) algorithm that preserves similarity in the input space into similarity (intersection size) in the SDR code space. Assuming that input similarity correlates with probability, this means that any single active SDR code is also a probability distribution over all stored inputs, with the probability of each input measured by the fraction of its SDR code that is active (i.e., the size of its intersection with the active SDR code). The learning/inference algorithm can also be viewed as a state update operator and because any single active SDR simultaneously represents both the probability of the single input, X, to which it was assigned during learning and the probabilities of all other stored inputs, the same physical process that updates the probability of X also updates all stored probabilities. By 'fixed time', it is meant that the number of computational steps comprising this process (the update algorithm) remains constant as the number of stored codes increases. This theory departs radically from the standard view of quantum computing and quantum physical theory more generally: rather than assuming that the states of the lowest level entities in the system, i.e., single binary neurons, exist in superposition, it assumes only that higher-level, i.e., composite entities, i.e., whole SDR codes (which are sets of binary neurons), exist in superposition.|$|E
50|$|A neural mapping connects two {{cortical}} neural maps. Neural mappings (in {{contrast to}} neural pathways) store training information by adjusting their neural link weights (see artificial neuron, artificial neural networks). Neural mappings {{are capable of}} generating or activating a <b>distributed</b> <b>representation</b> (see above) of a sensory or motor state within a sensory or motor map from a punctual or local activation within the other map (see for example the synaptic projection from speech sound map to motor map, to auditory target region map, or to somatosensory target region map in the DIVA model, explained below; or see for example the neural mapping from phonetic map to auditory state map and motor plan state map in the ACT model, explained below and Fig. 3).|$|E
5000|$|Many {{researchers}} {{have suggested that}} the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks. In a <b>distributed</b> <b>representation</b> any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights, where 'knowledge is stored, are changed it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input. Another way to conceptualize this is through visualizing learning as movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen. However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern. To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One {{way to do this is}} by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference. [...] Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.|$|E
40|$|Many {{representations}} {{in early}} vision {{can be constructed}} by performing orientation analysis along several sampling dimensions. Texture is often oriented in space, motion is oriented in space-time, and stereo is oriented in spacedisparity. In these modalities, we can construct <b>distributed</b> <b>representations</b> with oriented energy measures used in models of biological vision. Surface models of orientation, velocity, and disparity can easily be fit to <b>distributed</b> <b>representations</b> of texture, motion, and stereo by combining tools of orientation analysis and regularization. We describe base representation construction and model fitting processe...|$|R
40|$|Over {{the last}} few years a number of schemes for {{encoding}} compositional structure in <b>distributed</b> <b>representations</b> have been proposed, e. g., Smolensky's tensor products, Pollack's RAAMs, Plate's HRRs, Halford et al's STAR model, and Kanerva's binary spatter codes. All of these schemes can placed in a general framework involving superposition and binding of patterns. Viewed in this way, it is often simple to decide whether what can be achieved within one scheme will be able to be achieved in another. Furthermore, placing these schemes in a general framework reveals unexplored regions in which other related representation schemes with interesting properties. 1 Introduction <b>Distributed</b> <b>representations</b> {{are one of the most}} compelling ideas in connectionism. The use of <b>distributed</b> <b>representations</b> has endowed many connectionist models with their intriguing properties: ability to learn, parallel processing, soft capacity limits, and fault tolerance. However the difficulty of representing composit [...] ...|$|R
40|$|Abstract:- In this paper, we {{describe}} a fault-tolerant model for reasoning using forward chaining for predicate logic rules and facts with coarse-coded <b>distributed</b> <b>representations</b> of instantiated predicates in a connectionist frame work. <b>Distributed</b> <b>representations</b> {{are known to}} give advantages of fault tolerance and graceful degradation of performance under noise conditions. The system supports usage of complex rules which involve multiple conjunctions and disjunctions. The system solves the variable binding problem in a novel way using coarse-coded <b>distributed</b> <b>representations</b> of instantiated predicates without the need to decode them into localist representations. System’s performance with regard to its ability to exhibit fault tolerance under noise conditions is studied. The system offers better results of fault tolerance under noise conditions as compared to a connectionist reasoning system which uses localist representations. It has also exhibited better fault tolerance as compared to a Hopfield net for error correcting tasks of same magnitude...|$|R
40|$|Global {{coefficients}} for {{unsteady aerodynamic forces}} are {{a powerful tool for}} the assessment of the aeroelastic response of bridges in turbulent winds. However, global coefficients do not provide any information about the phenomenology of the fluid–structure interaction. This paper introduces a <b>distributed</b> <b>representation</b> of global coefficients that, exploiting the knowledge of the unsteady pressure field along the contour of the deck, allows to understand more in detail the aerodynamics of the phenomenon. We use the <b>distributed</b> <b>representation</b> to analyze the unsteady forces acting on a simple closed-box single girder deck, and we focus the study on two aspects. First, we analyze the unsteady aerodynamic forces near the stall of the moment coefficient, highlighting that a specific area of the deck governs the phenomenon; then we focus the attention on the comparison between the phenomenology of self-excited and wind induced unsteady forces. Results show the effectiveness of the <b>distributed</b> <b>representation</b> in highlighting similarities and discrepancies between different wind–bridge interactions. The <b>distributed</b> <b>representation</b> results to be an interesting tool to perform the optimization of the aeroelastic performances of decks and to improve nonlinear models for the dynamic response of bridges in turbulent wind...|$|E
40|$|International audienceThis paper {{proposes to}} use <b>distributed</b> <b>representation</b> of words (word embeddings) in cross-language textual {{similarity}} detection. The main contributions {{of this paper}} are the following: (a) we introduce new cross-language similarity detection methods based on <b>distributed</b> <b>representation</b> of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F 1 score of 89. 15 % for English-French similarity detection at chunk level (88. 5 % at sentence level) on a very challenging corpus...|$|E
40|$|Word {{representation}} {{is the basic}} research content of natural language processing. At present, <b>distributed</b> <b>representation</b> of monolingual words has shown satisfactory application effect in some Neural Probabilistic Language (NPL) research, while as for <b>distributed</b> <b>representation</b> of cross-lingual words, there is little research {{both at home and}} abroad. Aiming at this problem given distribution similarity of nouns and verbs in these two languages, we embed mutual translated words, synonyms, super-ordinates into Chinese corpus by the weakly supervised learning extension approach and other methods, thus Laos word distribution in cross-lingual environment of Chinese and Laos is learned. We applied the <b>distributed</b> <b>representation</b> of the cross-lingual words learned before to compute similarities of bilingual texts and classify the mixed text corpus of Chinese and Laos, Experimental results show that the proposal has a satisfactory effect on the two tasks...|$|E
40|$|This article evaluates purported {{progress}} {{over the}} past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51 % on all metrics that authors attribute to the introduction of <b>distributed</b> <b>representations</b> of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which use <b>distributed</b> <b>representations,</b> from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to <b>distributed</b> <b>representations</b> represent at most a 16 % relative error reduction on fully-labelled structures...|$|R
40|$|There exist {{all kinds}} of {{problems}} where both input and output data for neural networks are continuous and vector-valued. From our previous works we know that <b>distributed</b> <b>representations</b> of the input data are extremely useful for neural networks to embody good generalization skills and also to model forbidden regions in data space. Units are only located in those regions where data {{has occurred in the}} learning process. In this paper, we analyze Kohonen's self-organizing feature maps with respect to <b>distributed</b> <b>representations</b> and in this respect come up with a comparison to Radial Basis Functions. Concerning the output data, we give two interpretations for <b>distributed</b> <b>representations.</b> First, the center of gravity interpretation for which we explain some severe problems. Second, the pseudo-inverse of the matrix defined by the positions of the representation units. Keywords: self-organizing feature maps; function approximation; pseudo-inverse; normalization of input vectors; topologicall [...] ...|$|R
40|$|International audienceThis article evaluates purported {{progress}} {{over the}} past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51 % on all metrics that authors attribute to the introduction of <b>distributed</b> <b>representations</b> of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which use <b>distributed</b> <b>representations,</b> from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to <b>distributed</b> <b>representations</b> represent at most a 16 % relative error reduction on fully-labelled structures...|$|R
