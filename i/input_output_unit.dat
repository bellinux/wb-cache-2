1|10000|Public
40|$|A {{conceptual}} {{design of a}} MIMO Phased antenna array on Liquid Crystal Polymer (LCP) is introduced. Because of {{the properties of the}} substrate, the overall design is comparably light weight and semi-flexible to previous MIMO designs. Each <b>input</b> <b>output</b> <b>unit</b> array is capable of steering the beam ± 6 ° in a 1 -D space. This beam steer is enabled {{with the use of a}} 2 -bit reconfigurable phase shifter using RF MEMS. The building blocks of a single input and output has been simulated to prove the concept. In addition, the array can be expanded to beam steer in a 2 -D space for each individual unit. Overall, the entire system will be capable of having the beam steering function in multiple directions. I...|$|E
5000|$|Involve three-layer neural {{networks}} with <b>input,</b> hidden and <b>output</b> <b>units</b> ...|$|R
40|$|In this paper, the {{interpolation}} {{mechanism of}} functional networks is discussed. A kind of fourlayer (with 1 <b>input</b> and 1 <b>output</b> <b>unit)</b> and a five-layer (with double <b>input</b> and single <b>output</b> <b>unit)</b> functional network are designed. Meanwhile, a learning algorithm based on Minimizing a least squares error function {{with a unique}} minimum has been proposed {{for the purpose of}} approximating function. Experimental results indicate that the interpolation method is completely correct. ...|$|R
50|$|A {{recurrent}} {{neural network}} for this algorithm consists of some <b>input</b> <b>units,</b> some <b>output</b> <b>units</b> and eventually some hidden units.|$|R
40|$|An {{iterative}} method {{has been developed}} for analyzing dynamic loads in a light weight basic planetary gear system. The effects of fixed, semi-floating, and fully-floating sun gear conditions have been emphasized. The load dependent variable gear mesh stiffness were incorporated into a practical torsional dynamic model of a planetary gear system. The dynamic model consists of <b>input</b> and <b>output</b> <b>units,</b> shafts, and a planetary train. In this model, the sun gear has three degrees of freedom; two transverse and one rotational. The planets, ring gear, and the <b>input</b> and <b>output</b> <b>units</b> have one degree of freedom, (rotation) thus giving a total of nine degrees of freedoms for the basic system. The ring gear has a continuous radial support. The {{results indicate that the}} fixed sun gear arrangement with accurate or errorless gearing offers in general better performance than the floating sun gear system...|$|R
40|$|Conditional {{restricted}} Boltzmann {{machines are}} undirected stochastic neural networks {{with a layer}} of <b>input</b> and <b>output</b> <b>units</b> connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the <b>output</b> <b>units</b> given the states of the input units, parametrized by interaction weights and biases. We address the representational power of these models, proving results on the minimal size of universal approximators of condi...|$|R
40|$|In this study, {{we propose}} a new method for {{determine}} maximally productive units based on input-output orientation data envelopment analysis. In this method, {{we find that}} reduce <b>inputs</b> and improve <b>outputs</b> <b>units</b> without regard to any factor weights is possible. The new method is a linear mathematical programming technique that determines the intensities of units. No assumptions are required on the internal transformation processes of the units. Decision making depends critically on the way excellent units are frequently described. Present findings have implications for the monitoring and financing of units. Some units with the maximal productivity {{should be considered as}} a guide for the other units to reduce <b>inputs</b> and improve <b>outputs</b> <b>units.</b> Numerical illustrations are provided for 15 hospitals dataset of Sherman and 12 hospitals in Tehran, Iran...|$|R
50|$|In {{reinforcement}} learning settings, no teacher provides target signals. Instead a fitness function or reward function or utility function is occasionally {{used to evaluate}} performance, which influences its <b>input</b> stream through <b>output</b> <b>units</b> connected to actuators that affect the environment. Variants of evolutionary computation are often used to optimize the weight matrix.|$|R
40|$|The {{generalization}} {{of the problem}} of adaptive competition, known as the minority game, to the case of $K$ possible choices for each player is addressed, and applied to a system of interacting perceptrons with <b>input</b> and <b>output</b> <b>units</b> of the type of $K$-states Potts-spins. An optimal solution of this minority game as well as the dynamic evolution of the adaptive strategies of the players are solved analytically for a general $K$ and compared with numerical simulations. Comment: 5 pages, 2 figures, reorganized and clarifie...|$|R
40|$|STAR {{computer}} has five redundant modular function units, fixed store, arithmetic, memory, <b>input,</b> and <b>output.</b> Each <b>unit</b> {{is connected to}} a diagnostic control unit, each is coded for error detection and error correction. Separation into function units permits assembly of many different systems from the set of units...|$|R
40|$|A new {{algorithm}} for {{feature selection}} {{based on information}} maximization is derived. This algorithm performs subspace mapping from multi-channel signals, where Network Modules (NM) are used to perform the mapping {{for each of the}} channels. The algorithm is based on maximizing the Mutual Information (MI) between <b>input</b> and <b>output</b> <b>units</b> of each NM and between <b>output</b> <b>units</b> of different NMs. Such formulation leads to substantial redundancy reduction in <b>output</b> <b>units,</b> in addition to extraction of higher order features from input units that exhibit coherence across time and/or space useful in classification problems. We discuss the performance of the proposed algorithm using two scenarios, one dealing with the classification of EEG data while, the second is a speech application dealing with digit classification...|$|R
50|$|The D-37C {{computer}} consists {{four main}} sections: the memory, the {{central processing unit}} (CPU), and the <b>input</b> and <b>output</b> <b>units.</b> These sections are enclosed in one case. The memory is a two-sided, fixed head disk which rotatesat 6000 rpm. It contains 7222 words of 27 bits. Each word contains 24 data bits and three spacer bits not available to the programmer. The memory is arranged in 56 channels of 128 words each plus ten rapid access channels of one to sixteen words. The memory also includes the accumulators and instruction register.|$|R
50|$|In {{reinforcement}} learning settings, no teacher provides target signals. Instead a fitness function or reward function is occasionally {{used to evaluate}} the RNN's performance, which influences its <b>input</b> stream through <b>output</b> <b>units</b> connected to actuators that affect the environment. This might be used to play a game in which progress is measured with the number of points won.|$|R
40|$|A {{recurrent}} neural network, {{consisting of}} a small ensemble of eight processing units. In addition to external <b>inputs</b> and <b>outputs,</b> each <b>unit</b> has feedback connections to other units in the network, {{as indicated by the}} branches of the output lines that loop back onto the input lines leading into each unit (reproduced b...|$|R
40|$|We {{introduce}} causal neural networks, a generaliza-tion of {{the usual}} feedforward neural networks which allows input features and target outputs to be rep-resented as <b>input</b> or <b>output</b> <b>units.</b> For inferring the values of target outputs which are represented as in-put units, we developed a forward-backward propa-gation algorithm which uses gradient descent to min-imize the error of the predicted output features. To deal with {{the large number of}} possible structures and feature selection, we use a genetic algorithm. Exper-iments on a regression problem and 5 classication problems show that the causal neural networks can outperform the usual feedforward architectures for particular problems. ...|$|R
40|$|AbstractParallel program {{design of}} Yellow River 2 D water-sand model was {{realized}} based on MPI(Message Passing Interface). Load balance of computing quantity was realized through regional division according to distributive torage of data. Between whole grids and part-region, the mapping relation was established. Also especial treatments were made through protocols at critical <b>units,</b> <b>input</b> & <b>output</b> <b>units</b> and shared nodes, which, {{on the one}} hand can decrease communication as much as possible, on the other hand, can avoid information-jam. The Parallel program can meet the demands of computing capacity and speed in Yellow River mathematical model...|$|R
5000|$|To {{understand}} the mathematical derivation of the backpropagation algorithm, {{it helps to}} first develop some intuitions {{about the relationship between}} the actual output of a neuron and the correct output for a particular training case. Consider a simple neural network with two <b>input</b> <b>units,</b> one <b>output</b> <b>unit</b> and no hidden units. Each neuron uses a linear output that is the weighted sum of its input.|$|R
40|$|Conditional {{restricted}} Boltzmann {{machines are}} undirected stochastic neural networks {{with a layer}} of <b>input</b> and <b>output</b> <b>units</b> connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the <b>output</b> <b>units</b> given the states of the input units, parametrized by interaction weights and biases. We address the representational power of these models, proving results their ability to represent conditional Markov random fields and conditional distributions with restricted supports, the minimal size of universal approximators, the maximal model approximation errors, and on the dimension of the set of representable conditional distributions. We contribute new tools for investigating conditional probability models, which allow us to improve the results that can be derived from existing work on restricted Boltzmann machine probability models. Comment: 30 pages, 5 figures, 1 algorith...|$|R
30|$|NN [13] {{are those}} systems modeled {{based on the}} human brain working. As the human brain {{consists}} of millions of neurons that are interconnected by synapses, a neural network {{is a set of}} connected <b>input</b> or <b>output</b> <b>units</b> in which each connection has a weight associated with it. The network learns in the learning phase by adjusting the weights so as to be able to predict the correct class label of the input. An artificial neural network consists of connected set of processing units. The connections have weights that determine how one unit will affect another. Subsets of such units act as <b>input</b> and <b>output</b> nodes, and the remaining nodes constitute the hidden layer. By assigning activation to each of the input node and allowing them to propagate through the hidden layer nodes to the output nodes, neural network performs a functional mapping from <b>input</b> values to <b>output</b> values.|$|R
50|$|The Model 37 {{terminal}} utilizes {{a serial}} <b>input</b> / <b>output</b> 10 <b>unit</b> code signal {{consisting of a}} start bit, seven information bits, an even parity bit and a stop bit. It was produced in ASR (Automatic Send and Receive){{also known as the}} Model 37/300, KSR (Keyboard Send and Receive) also known as the Model 37/200 and RO (Receive Only) also known as the Model 37/100.|$|R
40|$|An autoassociator is a {{feedforward}} {{neural network}} that has {{the same number of}} <b>input</b> and <b>output</b> <b>units.</b> The goal of the autoassociator is very simple; to reconstruct its <b>input</b> at the <b>output</b> layer. Despite their simplicity, autoassociators have previously been shown to be quite successful on the task of Novelty Detection applied to industrial and military domains. The {{purpose of this paper is}} to test their utility on the more general task of clustering. In particular, we apply a clustering version of the autoassociator to the domain of Network Event Correlation. The results suggest that autoassociators are indeed useful as clustering systems. They were able to successfully correlate similar types of network alerts and have the added advantage of being fast once trained, a crucial feature when used for Network Event Correlation. ...|$|R
40|$|Many {{problems}} {{impede the}} design of multiagent systems, {{not the least of}} which is the passing of information between agents. While others hand implement communication routes and semantics, we explore a method by which communication can evolve. In the experiments described here, we model agents as connectionist networks. We supply each agent with a number of communications channels implemented by the addition of both <b>input</b> and <b>output</b> <b>units</b> for each channel. The <b>output</b> <b>units</b> initiate environmental signals whose amplitude decay over distance and are perturbed by environmental noise. An agent does not receive input from other individuals, rather the agent's input reflects the summation of all other agents' output signals along that channel. Because we use real-valued activations, the agents communicate using real-valued vectors. Under our evolutionary program, GNARL, the agents coevolve a communication scheme over continuous channels which conveys task-specific information. 1. INTRODUCTION [...] ...|$|R
50|$|The 24 {{channels}} of the input tape {{were divided into}} three fields of eight channels. Each accumulator, each set of switches, and the registers associated with the <b>input,</b> <b>output,</b> and arithmetic <b>units</b> were assigned a unique identifying index number. These numbers were represented in binary on the control tape. The first field was the binary index of {{the result of the}} operation and the second, the source datum for the operation. The third field was a code for the operation to be performed.|$|R
40|$|By {{frame of}} {{reference}} transformations, an input variable in one coordinate system is transformed into an output variable in a different coordinate system depending on another input variable. If the variables are represented as neural population codes, then a sigma–pi network is a natural way of coding this transformation. By multiplying two inputs it detects coactivations of input units, and by summing over the multiplied <b>inputs,</b> one <b>output</b> <b>unit</b> can respond invariantly to different combinations of coactivated input units. Here, we present a sigma–pi network and a learning algorithm by which the output representation self-organizes to form a topographic map. This network solves the {{frame of reference}} transformation problem by unsupervised learning. r 2006 Elsevier B. V. All rights reserved...|$|R
40|$|Usually weight {{changes in}} neural {{networks}} are exclusively caused by some hard-wired learning algorithm with many specific limitations. I {{show that it}} is in principle possible to let the network run and improve its own weight change algorithm (without significant theoretical limits). I derive an initial gradientbased supervised sequence learning algorithm for an `introspective' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses special subsets of its <b>input</b> and <b>output</b> <b>units</b> for observing its own errors and for explicitly analyzing and manipulating all of its own weights, including those weights responsible for analyzing and manipulating weights. The result is the first `self-referential' neural network with explicit potential control over all adaptive parameters governing its behavior...|$|R
40|$|In nonparametric methods, if {{the number}} of {{observations}} is relatively small as compared to the sum of number of <b>inputs</b> and <b>outputs,</b> many <b>units</b> are evaluated as efficient. Several methods for prioritizing these efficient units are reported in literature. Andersen et al. and Mehrabian et al. proposed two methods for ranking efficient units, but both methods break down in some cases. This paper describes a new DEA ranking approach that uses L 2 -norm. 1...|$|R
40|$|DE 102008013136 A 1 UPAB: 20090918 NOVELTY - The device (1) has an {{accommodation}} area (3) for goods {{to be transported}} and/or packed. A data processing unit is provided with a memory device and a touch display (4) for user interaction for storing, processing, <b>inputting</b> and <b>outputting</b> charge-specific information about the goods. The information is exchanged with an external data processing system by a communication interface e. g. optical interface. The disfplay {{is attached to a}} side wall of a box-shaped housing (2). The unit is supplied with power by a power supply and is connected with an active radio frequency identification transmitter. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of picking goods by a charging device. USE - Charging device for a decentralized logistic system. ADVANTAGE - The data processing unit is provided with the memory device, and the <b>input</b> and <b>output</b> <b>unit</b> for user interaction for storing, processing, <b>inputting</b> and <b>outputting</b> the charge-specific information about the goods, thus achieving high flexibility and reliability of the charging device against technical disturbances...|$|R
40|$|A {{method for}} {{learning}} phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from <b>input</b> to <b>output</b> <b>units.</b> The network uses hidden units with recurrent links to capture spectral/temporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation {{of the desired}} output as an evaluation function. A simple connectionist network with recurrent links was trained on a single instance of the word pair &quot;no &quot; and &quot;go&quot;, and successful learned a discriminatory mechanism. The trained network also correctly discriminated 98 % of 25 other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items. ...|$|R
40|$|Some of the methodological. {{considerations}} in school effectiveness studies are outlined {{and a state}} of the art presented. Two general theoretical models are given which provide the researcher with an overall strategy for handling such study of school effectiveness: the Dyer Model and Production Process Model. Six statistical models which provide possible methods for the computation of effectiveness indices are proposed and critiqued: (1) analysis of covariance (ANCOVA), (2) nonstandard ANCOVA, (3) corrected nonstandard ANCOVA, (4) mean differences scores, (5) individual regression residuals, and (6) school regression tzsiduals. Finally, several other technical {{considerations in}}volving sources of error, identification of predictors, choice of <b>input</b> and <b>output,</b> <b>unit</b> of analysis, type of samples, and the kind of analysis to be performed are briefly discussed. Major emphasis is on models used to rank schools in *eras of effectiveness. (Author/RC) OU S DEPARTMENT OF HEALTH...|$|R
40|$|A major {{difference}} between human learning and machine learning is that humans can reflect about their own learning behavior and adapt it to typical learning tasks in a given environment. To make some initial theoretical steps toward `introspective' machine learning, I present [...] as a thought experiment [...] a `self-referential' recurrent neural network which can run and actively modify its own weight change algorithm. The network has special input units for observing its own failures and successes. Each of its connections has an address. The network has additional special <b>input</b> and <b>output</b> <b>units</b> for sequentially addressing, analyzing and manipulating all of its own adaptive components (weights), including those weights responsible for addressing, analyzing and manipulating weights. Due to the generality of the architecture, there are no theoretical limits to the sophistication of the modified weight change algorithms running on the network (except for unavoidable pre-wired time a [...] ...|$|R
40|$|Weight {{modifications}} in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many specific limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets {{to run and}} improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is `yes'. I derive an initial gradient-based sequence learning algorithm for a `self-referential' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses some of its <b>input</b> and <b>output</b> <b>units</b> for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including {{those parts of the}} weight matrix responsible for analyzing and modifying the weight matrix. The result is the first `introspective' neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high c [...] ...|$|R
40|$|Described here is sparse {{distributed}} memory (SDM) as a neural-net associative memory. It {{is characterized by}} two weight matrices and by a large internal dimension - the number of hidden units is {{much larger than the}} number of <b>input</b> or <b>output</b> <b>units.</b> The first matrix, A, is fixed and possibly random, and the second matrix, C, is modifiable. The SDM is compared and contrasted to (1) computer memory, (2) correlation-matrix memory, (3) feet-forward artificial neural network, (4) cortex of the cerebellum, (5) Marr and Albus models of the cerebellum, and (6) Albus' cerebellar model arithmetic computer (CMAC). Several variations of the basic SDM design are discussed: the selected-coordinate and hyperplane designs of Jaeckel, the pseudorandom associative neural memory of Hassoun, and SDM with real-valued input variables by Prager and Fallside. SDM research conducted mainly at the Research Institute for Advanced Computer Science (RIACS) in 1986 - 1991 is highlighted...|$|R
40|$|The {{programs}} described {{were developed}} to process GEODYN-formatted satellite altimeter data, and to apply the processed results to predict geoid undulations and gravity anomalies of inland sea areas. These programs are written in standard FORTRAN 77 and are designed to run on the NSESCC IBM 3081 (MVS) computer. Because of the experimental nature of these programs they are tailored to the geographical area analyzed. The attached program listings are customized for processing the altimeter data over the Black Sea. Users interested in the Caspian Sea data are expected to modify each program, although the required modifications are generally minor. Program control parameters are defined in the programs via PARAMETER statements and/or DATA statements. Other auxiliary parameters, such as labels, are hard-wired into the programs. Large data files are read in or written out through different <b>input</b> or <b>output</b> <b>units.</b> The program listings of these programs are accompanied by sample IBM job control language (JCL) images. Familiarity with IBM JCL and the TEMPLATE graphic package is assumed...|$|R
40|$|Abstract: If a DEA {{model has}} {{a mix of}} {{categorical}} and continuous variables a standard LP formulation can still be used by entering all combinations of categorical and continuous variables as different types of <b>inputs</b> and/or <b>outputs.</b> Most <b>units</b> will then not have positive levels of all variables. The implications for selection of peers are investigated. Peers can have the same or fewer types of inputs than the unit under investigation, but either fewer or more types of outputs. There is a basic asymmetry between number of positive <b>inputs</b> and <b>outputs</b> of the peer units due to more of inputs reducing efficiency while more of outputs improving efficiency. The special cases of imposing a hierarchical structure on the categorical variables {{dealt with in the}} literature can easily be incorporated...|$|R
40|$|If a DEA {{model has}} {{a mix of}} {{categorical}} and continuous variables a standard LP formulation can still be used by entering all combinations of categorical and continuous variables as different types of <b>inputs</b> and/or <b>outputs.</b> Most <b>units</b> will then not have positive levels of all variables. The implications for selection of peers are investigated. Peers can have the same or fewer types of inputs than the unit under investigation, but either fewer or more types of outputs. There is a basic asymmetry between number of positive <b>inputs</b> and <b>outputs</b> of the peer units due to more of inputs reducing efficiency while more of outputs improving efficiency. The special cases of imposing a hierarchical structure on the categorical variables {{dealt with in the}} literature can easily be incorporated. Categorical variable; DEA; efficiency; linear programming; peer...|$|R
40|$|Abstract: This paper aims that {{analysing}} {{neural network}} method in pattern recognition. A neural network is a processing device, whose design {{was inspired by}} the design and functioning of human brain and their components. The proposed solutions focus on applying Neocognitron Algorithm model for pattern recognition. The primary function of which is to retrieve in a pattern stored in memory, when an incomplete or noisy version of that pattern is presented. An associative memory is a storehouse of associated patterns that are encoded in some form. In auto-association, an input pattern is associated with itself and the states of <b>input</b> and <b>output</b> <b>units</b> coincide. When the storehouse is incited with a given distorted or partial pattern, the associated pattern pair stored in its perfect form is recalled. Pattern recognition techniques are associated a symbolic identity with the image of the pattern. This problem of replication of patterns by machines (computers) involves the machine printed patterns. There is no idle memory containing data and programmed, but each neuron is programmed and continuously active...|$|R
