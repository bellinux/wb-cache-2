0|43|Public
40|$|The {{evaluation}} method of stray field <b>loss</b> of <b>engineering</b> oriented <b>loss</b> model (TEAM Workshop Problem 21) is investigated. It is {{shown that the}} nonlinear eddy current analysis is obligatory in order to investigate the eddy current loss in the steel plate, because the flux and eddy current in steel {{are affected by the}} permeability of the plate. The hysteresis loss in such a steel plate having a substantial skin effect is not negligible, even if the flux density in air is small</p...|$|R
50|$|The {{liquidation}} of {{a corporation}} is generally {{treated as an}} exchange of a capital asset under the Internal Revenue Code. If a shareholder bought stock for $300 and receives $500 worth of property from a corporation in a liquidation, that shareholder would recognize a capital gain of $200. An exception is when a parent corporation liquidates a subsidiary, which is tax-free {{so long as the}} parent owns more than 80% of the subsidiary. There are certain anti-abuse rules to avoid the <b>engineering</b> of <b>losses</b> in corporate liquidations.|$|R
40|$|Results of {{numerical}} analysis of loss components in a conducting magnetic hysteresis medium are given. They explain inaccuracies of the widespread formula {{for the total}} loss evaluation and {{provide a basis for}} an <b>engineering</b> approach to <b>loss</b> prediction over a wide range of magnetization frequencies and flux densities...|$|R
40|$|Recent {{efforts on}} {{manufacturing}} high temperature superconductors (HTS) {{have resulted in}} the production of coils which promise significant benefits for power <b>engineering</b> applications. <b>Losses</b> in two HTS pancake coils carrying AC current are measured and the results are analysed. It is shown that a thin pancake coil with relatively large diameter can be treated as straight tapes arranged in a face-to-face stack with the spacing between the tapes chosen to be relatively small. Losses in the tapes become those predicted using Norris 2 ̆ 7 s equation for an infinite slab...|$|R
50|$|The {{company has}} {{expertise}} in structural engineering, façade engineering, forensics, renewal, construction <b>engineering,</b> property <b>loss</b> consulting, sustainability, applied science, protective design and transportation. The engineering firm provided the structural design for {{several of the}} world's tallest building structures, including the Petronas Towers in Kuala Lumpur, Malaysia; and Taipei 101 in Taiwan. Other structures designed by the firm include Soldier Field in Chicago, Petco Park in San Diego, and the Minneapolis Public Library in Minneapolis. Among other architects of note, Thornton Tomasetti has collaborated with Cesar Pelli, Santiago Calatrava, Renzo Piano, and Rafael Vinoly.|$|R
40|$|The Mid-America Earthquake Center {{has been}} {{recommended}} for a five-year renewal {{based on a}} newly directed research plan that shall ultimately reduce earthquake losses and thus risk. Intrinsic to the new plan is {{development of a new}} <b>engineering</b> approach to <b>loss</b> reduction, termed “Consequence-Based Engineering, ” which is a systems-based methodology fo...|$|R
50|$|A. C. Wells & Co. {{began in}} Cheetham, Manchester. Their first product was {{a range of}} engineer's lamps, simple {{cast-iron}} wick lamps that were widely used before electric battery torches. This type of lamp was not very bright and their limited light has been blamed for several <b>engineering</b> failures and <b>losses</b> of life, where an inspection was poorly carried out, owing to poor light.|$|R
30|$|The {{empirical}} research by experiment {{has often been}} employed to test of BIM-performance and capabilities. Plume and Mitchell (2007) conducted in 2004 an experiment with 23 students in a design studio setting, testing the IFC-model performance in multi-disciplinary collaboration (architecture, landscape architecture, MEP, statutory planning, sustainability and construction management.) They focused primarily on operational issues, such as building model (representation of a building model in different tools) and IFC –server data sharing issues. They conclude that the original architectural model needs significant adaptation {{for the use of}} other disciplines or their tools. Further issue needing closer attention is model management – tracing of the changes and updates carried out on the common model. Sacks et al. (2010) carried out the “Rosewood experiment”, comparing the BIM-supported versus the traditional 2 D CAD the planning and fabrication process of the pre-cast façade. BIM proved to be more efficient by 57 %, however IFC proved not mature enough causing data inconsistency in transfer between architectural and <b>engineering</b> system. <b>Losses</b> in translation can be assigned to object-semantic, a similar problem addressed by the Plume and Mitchell (2007).|$|R
40|$|Current use of {{microbes}} for metabolic <b>engineering</b> {{suffers from}} <b>loss</b> of metabolic output due to natural selection. Rather than combat {{the evolution of}} bacterial populations, we chose to embrace what makes biological engineering unique among engineering fields – evolving materials. We harnessed bacteria to compute solutions to the biological problem of meta-bolic pathway optimization. Our approach is called Programmed Evolution to capture two concepts. First, a population of cells is programmed with DNA code to enable it to compute solutions to a chosen optimization problem. As analog computers, bacteria process known and unknown inputs and direct the output of their biochemical hardware. Second, the sys...|$|R
40|$|Using a {{simulation}} {{model for a}} benchmark VoIP access architecture, we investigate the performance issues associ-ated with mixing real-time voice and congestion-sensitive data traffic. Arbitration of shared facility is accomplished via First Come First Serve (FCFS), Strictly Priority (SP), and Weighted Fair Queuing (WFQ) disciplines. The per-formance metrics used are facility utilization, transmission delay, queuing delay and packet <b>loss.</b> <b>Engineering</b> rules for sizing the network are provided. More specifically, our results indicate that proper engineering of the queue size under the SP discipline can prevent any packet loss of voice traffic; proper setting the weights of the WFQ scheduler can control the delay for voice traffic; and increasing the queu-ing delay of the WFQ scheduler can improve the packet loss rate of data traffic. 1...|$|R
40|$|ABSTRACT: The State of Florida is in {{the process}} of {{developing}} an open, public model for the purpose of probabilistic assessment of risk to insured residential property associated with wind damage from hurricanes. The model comprises atmospheric science, engineering, and financial/actuarial components and is planned for 2004 submission to the Florida Commission on Hurricane Loss Projection Methodology. The atmospheric component includes the modeling of the complete track and intensity life cycle of each simulated hurricane. When a storm approaches within 200 km of the Florida coastline, the wind field is computed by a slab model of the hurricane boundary layer coupled with a surface layer model based on the results of recent GPS sonde research. A time series of open terrain surface winds is then computed for each zip code in the threatened area. Depending on wind direction, an effective roughness length is assigned to each zip code based on the upstream roughness as determined from land cover / land use products. Thousands of storms are simulated allowing determination of the wind risk for all zip codes in Florida. The wind risk information is then provided to the <b>engineering</b> and <b>loss</b> models to assess damage and average annual loss, respectively...|$|R
40|$|The {{problem of}} {{consistency}} of smoothed particle hydrodynamics (SPH) has demanded considerable {{attention in the}} past few years due to the ever increasing number of applications of the method in many areas of science and <b>engineering.</b> A <b>loss</b> of consistency leads to an inevitable loss of approximation accuracy. In this paper, we revisit the issue of SPH kernel and particle consistency and demonstrate that SPH has a limiting second-order convergence rate. Numerical experiments with suitably chosen test functions validate this conclusion. In particular, we find that when using the root mean square error as a model evaluation statistics, well-known corrective SPH schemes, which were thought to converge to second, or even higher order, are actually first-order accurate, or at best close to second order. We also find that observing the joint limit when N→∞, h→ 0, and n→∞, as was recently proposed by Zhu et al., where N is the total number of particles, h is the smoothing length, and n is the number of neighbor particles, standard SPH restores full C^ 0 particle consistency for both the estimates of the function and its derivatives and becomes insensitive to particle disorder. Comment: 27 pages, 10 figures. Submitted to Journal of Applied Numerical Mathematic...|$|R
40|$|Various {{resonators}} for surface emission {{are reviewed}} that {{have recently been}} developed to improve radiative- and collection-efficiencies of terahertz quantum cascade lasers (THz QCL). While the fabrication of waveguides for long wavelengths is challenging in terms of molecular beam epitaxy, long wavelengths also provide a wonderful testbed for new photonics structure concepts, since these can be easily produced by conventional optical lithography because of the typically large size of the required features. This led to novel geometries, like one-and two-dimensional non-periodic photonic crystals, or circular gratings for microdisk- and ring-lasers, which are all implemented by simply patterning the top metal cladding of a metal-metal wave-guide. The modeling of such resonators with the finite element method is also described, highlighting {{the importance of this}} tool for the <b>engineering</b> of surface <b>losses</b> and far-field patterns...|$|R
40|$|Thesis (M. S.) [...] Wichita State University, College of Engineering, Dept. of Mechanical <b>Engineering.</b> The <b>loss</b> of {{properties}} {{due to the}} ultra violet (UV) light exposure {{has been a long}} standing problem in composite materials. To overcome this issue, the composite materials are often coated with a polymeric material. Even though the polymers offer improved resistance against UV degradation, they are not a comprehensive solution to overcome the problem, because over a prolonged period of time, the coatings themselves are susceptible to UV degradation. This research was aimed to improve the resistance of the coatings against the UV degradation. In order to achieve that, nanoscale additive, namely graphene, was uniformly dispersed into the coatings, and then applied onto the surface of the glass fiber reinforced composite materials. It was hypothesized that graphene, being a very good nano reinforcement material, would act as a strong binding agent and increase the resistance of the coating against the UV degradation. The effects of the addition of the nano-additives were tested by performing atomic force microscopy, water contact angle measurement, coating thickness measurement and the FTIR studies. The experimental results confirmed that addition of nano-additives does in fact improve the resistance of the coatings against UV degradation. As a result, this study will prove a number of advantages for different industrial applications...|$|R
5000|$|Often {{interventions}} to prevent noise-induced hearing loss have many components. A 2017 Cochrane review found that stricter legislation might reduce noise levels. [...] Providing workers {{with information on}} their noise exposure levels was not shown to decrease exposure to noise. Ear protection, if used correctly, can reduce noise to safer levels, but often, providing them {{is not sufficient to}} prevent hearing <b>loss.</b> <b>Engineering</b> noise out and other solutions such as proper maintenance of equipment can lead to noise reduction, but further field studies on resulting noise exposures following such interventions are needed. Other possible solutions include improved enforcement of existing legislation and better implementation of well-designed prevention programmes, which have not yet been proven conclusively to be effective. The conclusion of the Cochrane Review was that further research could modify what is now regarding the effectiveness of the evaluated interventions [...]|$|R
40|$|Output {{power of}} {{thermoelectric}} generators depends on device <b>engineering</b> minimizing heat <b>loss</b> {{as well as}} inherent material properties. However, the device engineering has been largely neglected due to the limited flat or angular shape of devices. Considering that the surface of most heat sources where these planar devices are attached is curved, {{a considerable amount of}} heat loss is inevitable. To address this issue, here, we present the shape-engineerable thermoelectric painting, geometrically compatible to surfaces of any shape. We prepared Bi 2 Te 3 -based inorganic paints using the molecular Sb 2 Te 3 chalcogenidometalate as a sintering aid for thermoelectric particles, with ZT values of 0. 67 for n-type and 1. 21 for p-type painted materials that compete the bulk values. Devices directly brush-painted onto curved surfaces produced the high output power of 4. 0 &# 8201;mW&# 8201;cm&# 8722; 2. This approach paves the way to designing materials and devices that can be easily transferred to other applications. ope...|$|R
40|$|T {{his paper}} shows that no simple, {{common-sense}} {{rule of thumb}} {{can be used to}} identify a most-vital arc, even in a simple maximum-flow problem. The correct answer requires analysis equiv-alent in difficulty to completely solving the maximum-flow problem, perhaps repeat-edly. This insight generalizes to finding a most-vital component, or set of components, in a system whose operation is described by amore general model. Our paper shows how to evaluate the criticality of sets of components, how to assess the worst-case set of components that might be lost to a given number of simultaneous hostile at-tacks (or <b>engineering</b> failures, or <b>losses</b> to Mother Nature), and how to allocate limited defensive resources to minimize the max-imum damage from a subsequent attack. Collateral insights include {{the fact that there is}} no way to prioritize individual components by criticality, and that the analysis that determines critical component sets also yields objective assessments of operational system resilience and can provide con-structive advice on how to increase it...|$|R
50|$|In 2003, Nortel {{made a big}} {{contribution}} to this list of scandals by incorrectly reporting a one cent per share earnings directly after their massive layoff period. They used this {{money to pay the}} top 43 managers of the company. The SEC and the Ontario securities commission eventually settled civil action with Nortel. However, a separate civil action will be taken up against top Nortel executives including former CEO Frank A. Dunn, Douglas C. Beatty, Michael J. Gollogly and MaryAnne E. Pahapill and Hamilton. These proceedings have been postponed pending criminal proceedings in Canada, which opened in Toronto on January 12, 2012. Crown lawyers at this fraud trial of three former Nortel Networks executives say the men defrauded the shareholders of Nortel of more than $5 million. According to the prosecutor this was accomplished by <b>engineering</b> a financial <b>loss</b> in 2002, and a profit in 2003 thereby triggering Return to Profit bonuses of $70 million for top executives.|$|R
40|$|Military Operations Research, 18 (1), pp. 21 - 37. The {{article of}} record as {{published}} may be located at [URL] paper shows that no simple, common-sense {{rule of thumb}} {{can be used to}} identify a most-vital arc, even in a simple maximum-flow problem. The correct answer requires analysis equivalent in difficulty to completely solving the maximum-flow problem, perhaps repeatedly. This insight generalizes to finding a most-vital component, or set of components, in a system whose operations is described by a more general model. Our paper shows how to evaluate the criticality of sets of components, how to assess the worst-case set of components that might be lost to a given number of simultaneous hostile attacks (or <b>engineering</b> failures, or <b>losses</b> to Mother Nature), and how to allocate limited defensive resources to minimize the maximum damage from subsequent attack. Collateral insights include {{the fact that there is}} no way to prioritize individual components by critically, and the the analysis that determines critical component sets also yields objective assessments of operations system resilience and can provide constructive advice on how to increase it...|$|R
40|$|Description of {{the thermal}} {{environment}} and the livestock response can be complex, and {{has been the subject}} of extensive research for over fiver decades inspired in part by a joint report sponsored by ASAE (now ASABE) and ASHRAE. This 1959 report presented the 2 ̆ 2 State of the Art 2 ̆ 2 of the thermal environmental requirements of poultry (Stewart and Hinkle, 1959), dairy cattle (Yeck, 1959), beef cattle (Nelson, 1959), swine (Bond, 1959) and sheep (Kelly, 1959). Even though the report was comprehensive, data were noted as being incomplete for understanding the biophysical interactions between the animal and its thermal environment as required for effective management and <b>engineering</b> design. Heat <b>loss</b> for poultry was primarily based on basal (fasted) conditions, for example, and the role of the skin and hair in heat dissipation from cattle was inadequate. Comprehensive studies have been conducted in the intervening 50 years to evaluate the effects of nutrition, acclimation or conditioning, dynamic changes in the environment, physiological state, and social interactions on livestock productivity responses to the thermal environment: temperature, humidity, radiation, and air velocity...|$|R
40|$|M. Ing. (Electrical & Electronic <b>Engineering)</b> Switching <b>losses</b> {{in power}} {{electronic}} converters {{can be reduced}} by using snubbers, or resonant circuits. Simple snubbers can be implemented without much effort, but then energy is still dissipated, which reduces {{the efficiency of the}} converter. Regenerative snubbers are known, but these usually require complicated additional circuits. The use of nonlinear capacitors as turn-off snubbers has been investigated, and proved profitable. The main reason for this is the much smaller amount of energy which is stored in the the nonlinear capacitors, compared to linear capacitors. Resonant circuits reduce switching losses by allowing switchings to occur when the voltage across the switching device, or the current through the switching device is zero. This effect is called soft-switching and to obtain this, resonance between an inductor and capacitor is usually involved. Resonant circuits have the disadvantage that the installed switching power cannot be used optimally. This drawback has been reduced by using a saturable inductor in the resonant circuit. This thesis considers the use of both nonlinear capacitors and nonlinear inductors in a resonant phase arm. It was found that the nonlinear resonant elements lead to very low overdimensioning of the switching devices, and minimal switching losses in such a phase arm. Very few additional components are used, which keeps the cost of the converter down. A 1. 5 kW converter was constructed in which the double nonlinear resonant phase arm was implemented. This converter was investigated both experimentaly, and by means of computer simulations. The converter is also compared with {{the current state of the}} art. It is concluded that resonant phase arms with both. nonlinear inductors and nonlinear capacitors can function effectively in power electronic converters. Some important advantages are also assosiated with the use of the double nonlinear resonant circuit...|$|R
40|$|The {{life-cycle}} {{cost analysis}} of buildings prone to seismic risk {{is a critical}} issue in structural <b>engineering.</b> Expected <b>loss,</b> including damage and repair costs, is an important parameter for structural design. The combination of economic theory and computer technology allows for a more developed approach to the design and construction of structures than ever before. In this study, a simplified method based on a semi-probabilistic methodology is developed to evaluate the economic performance of a building prone to seismic risk. The proposed approach aims to identify the most cost-effective strengthening strategies and strengthening levels for existing structures during their structural lifetime. To achieve this, the method identifies the optimal strengthening level, computing {{on the one hand}} the costs of strengthening the structure at different performance levels for each strategy, and, on the other, the expected seismic loss during the structure’s lifetime. To assess the expected loss, the building is divided into several components, both structural and non-structural. A set of fragility curves is assigned for each component. Then, once the structural model and the various components of the building, with the corresponding fragility curves, are defined, a loss assessment is performed using a static non-linear analysis. The summation of the strengthening costs and the discounted expected losses produces a relationship between the total costs and the strengthening level. The minimum of this relationship identifies the most cost-effective strengthening intervention. As a case study, this method is applied to an existing reinforced concrete (RC) structure severely damaged by the 2009 earthquake in L’Aquila. Different strategies are analyzed, namely the FRP (fiber reinforced polymer) strengthening of elements, the RC jacketing of columns, RC exterior shear wall insertions, and the base isolation of the building...|$|R
50|$|Engineering colleges {{began to}} feel slighted because doctors, lawyers, and {{business}} executives were viewed as having more prestige and professional status than their engineering graduates. Intellectual elites viewed engineering colleges as trade schools, and graduate engineers {{were said to be}} nothing more than mechanics or glorified shop hands. In response, engineering schools began to drop courses that lacked academic rigor or had the slightest blue-collar aura. The launch of Sputnik in 1957 again changed the perception of design <b>engineering.</b> The perceived <b>loss</b> of world leadership in air and space technology by the people of the United States {{set the stage for a}} considerable renewal of prestige to the engineering discipline. After more than a decade into the Cold War, the public realized science and engineering could play a key role in keeping the Communists at bay. The government unloaded almost limitless supplies of money on high-tech defense industries, and engineering became the career of choice. High salaries and generous perks were lavished on engineers and scientists. Unfortunately, Sputnik also accelerated the movement to delete courses on manufacturing and shop practice from the curricula of top schools. The idea was to portray engineers as being more scientist than mechanic. The rocket scientist working on the space program became the image to which most engineers aspired.|$|R
40|$|Engineering {{work in the}} German {{automotive}} industry is currently being dictated by corporate reorganization processes. These are driven by processes of informatization, financialization, and globalization, and evince {{an increase in the}} (global) division of labor in product development, as well as an increase in competitive pressures. Being able to deal with information and knowledge is important for coping with these processes at work. In terms of engineers' work, this means that engineers must {{be able to deal with}} the increasing amounts of information available in the form of benchmark figures, data, etc., as well as with the challenges presented by work shared in global networks. This paper presents the results of a series of qualitative interviews. The main trends that can be pointed out are the 'enucleation' (removing the core) of <b>engineering</b> work, the <b>loss</b> of autonomy, increasing insecurity, and the changing materiality of the objects worked on. Enucleation processes are increasingly changing older engineers' fields of work. More and more, they are being called upon to take over communication and coordination practices. In connection with increasing insecurity, employees' perception of their own position in the company is changing and, in the end, their strategies of action, as well. " (author's abstract...|$|R
40|$|In {{any given}} {{earthquake}} event scenario, be it a first shock or an aftershock, {{the decision to}} limit traffic or to completely close a bridge in a highway network system hinges primarily on reconnaissance data on visible damage to the bridge. A more quantitative and systematic method for decision making by engineers, owners, and operators alike involves determination of the loss in the bridge load carrying capacity. In a performance-based earthquake <b>engineering</b> context, the <b>loss</b> of load carrying capacity of a bridge {{can be described as}} a earthquake demand parameter. In previously formulated probabilistic seismic demand models, this demand parameter was predicted by a measure descriptive of the earthquake intensity. This method, however, is not the most accurate approach of predicting loss of load carrying capacity. It has been shown that post-earthquake residual displacement is a better proxy for capacity loss than measures of earthquake intensity. The initial load carrying capacity is defined in terms of a static pushover analysis. A comparison of loss in load carrying capacity is made between time history analysis of first shocks and aftershocks, and static analysis including residual displacements and degradation of material stiffness. These demand models are then integrated with estimates of capacity or damage in order to produce traditional fragility curves...|$|R
40|$|The {{conversion}} of a waste heat energy to electricity is now {{becoming one of}} the key points to improve the energy efficiency in a process <b>engineering.</b> However, large <b>losses</b> of a low-temperature thermal energy are also present in power engineering. One of such sources of waste heat in power plants are exhaust gases at the outlet of boilers. Through usage of a waste heat regeneration system it is possible to attain a heat rate of approximately 200 MWth, under about 90 °C, for a supercritical power block of 900 MWel fuelled by a lignite. In the article, we propose to use the waste heat to improve thermal efficiency of the Szewalski binary vapour cycle. The Szewalski binary vapour cycle provides steam as the working fluid in a high temperature part of the cycle, while another fluid – organic working fluid – as the working substance substituting conventional steam over the temperature range represented by the low pressure steam expansion. In order to define in detail the efficiency of energy conversion at various stages of the proposed cycle the exergy analysis was performed. The steam cycle for reference conditions, the Szewalski binary vapour cycle as well as the Szewalski hierarchic vapour cycle cooperating with a system of waste heat recovery have been comprised...|$|R
40|$|The Centre for International Co-operation in Agricultural Research for Development (CIRAD) is {{offering}} seminars for 'on-the-spot' training aimed at {{groups of people}} interested in the study, the application or dissemination of techniques connected {{with the use of}} pest-control products. The length of each seminar varies according to the demand, but generally it is less than three weeks. The courses in French and/or English can be held in any centre for agricultural training university, research station, etc. Education level required: an agricultural certificate or a degree in agriculture, entomology, chemistry, biology or <b>engineering.</b> Subjects covered: <b>losses</b> from crop pests; factors affecting pre or post-harvest delays; weed, insect and disease control; protection of stored crops: pest control, crop residues, regulations, etc. At the end of the course, the student can take a CIRAD examination and obtain a cer tificate if successful. Requests for the organisation of seminars should be sent to CIRAD through the nearest French embassy at least three months before the beginning of the seminar requested. For further information, contact CIRAD 42. rue Scheffer. 75016 Paris FRANCE Tel: 47 - 04 - 32 - 15 The Centre for International Co-operation in Agricultural Research for Development (CIRAD) {{is offering}} seminars for 'on-the-spot' training aimed at groups of people interested in the study, the application or dissemination of techniques connected [...] ...|$|R
30|$|Many on-site {{dedicated}} systems operated {{at individual}} facilities also detected {{the event and}} alarmed the facilities to take necessary actions including stopping trains, etc. JMA issued warnings for large tsunamis 3  min after the earthquake origin time (Ozaki 2011), which should have provided sufficient time for most people to escape. However, complications in the information transfer, and emergency response, {{partly due to the}} underestimated tsunami heights in the beginning, caused problems of miscommunication so that many people were killed by the huge onrush of water. Tsunamis completely destroyed many coastal cities and towns along the Pacific coast of northeast Japan, causing a large number of casualties and the much-publicized failure of a nuclear power plant in Fukushima. The estimated damage was over US$ 300 billion. Nonetheless, without the big-net EEW and on-site monitoring systems and intensive <b>engineering</b> preparations, the <b>loss</b> of lives and damage of facilities could have been far worse (see Tajima et al. 2013). The casualties of this earthquake (Mw 9) were less than those caused by other smaller but destructive earthquakes in the Mw range of 6.6 to 7.9 (see Table 1). The relatively few casualties from the Mw 9 earthquake are a testament to Japan’s emergency systems that include not only the big-net and on-site EEW systems but also the rigorous building codes and advanced engineering technologies for quake resistance (National Institute for Land and Infrastructure Management, and Building Research Institute 2012).|$|R
40|$|The project {{focuses on}} the seismic design of static steel pallet racks, which are widely adopted in warehouses. Despite their lightness, racks can be loaded with tons of {{valuable}} goods, a live load by far higher than the self-weight, opposite to what happens in usual civil <b>engineering</b> structures. The <b>loss</b> of these goods during an earthquake may represent, for the owner, a very large economic loss, {{much larger than the}} cost of the whole rack on which the goods are stored or of the cost for its seismic upgrade. Hence, solution of the problems connected with safe and reliable design of steel storage racks in seismic areas has a very large economic impact. The objective of the SEISRACKS 2 project is to increase knowledge on actual structural behaviour and ductility of steel pallet racks, and to assess design rules for earthquake conditions by full-scale testing and numerical simulation. Main outcomes of the research are: 1. Detailed reports on the different aspects investigated; 2. Validation or invalidation of the rules in the current version of FEM 10. 2. 08; 3. Improvements and extension of the current rules in order to optimize the seismic behaviour of structures designed according to European rules; 4. Definition of standardized experimental procedures to qualify structural elements of rack structures to be used in seismic areas; 5. A new software tool for the design of rack structures under seismic load...|$|R
40|$|The {{reconstitution}} of a fully {{organized and}} functional hair follicle from dissociated cells propagated under defined tissue culture conditions {{is a challenge}} still pending in tissue <b>engineering.</b> The <b>loss</b> of hair follicles caused by injuries or pathologies such as alopecias not only affects the patients´ psychological well-being, but also endangers certain inherent functions of the skin. It is then {{of great interest to}} find different strategies aiming to regenerate or neogenerate the hair follicle under conditions proper of an adult individual. Based upon current knowledge of the epithelial and dermal cells involved in embryonic hair generation and adult hair cycling, and of the epithelial-mesenchymal interactions among them, many researchers have tried to obtain mature hair follicles using different strategies and approaches depending on the causes of hair loss. This review summarizes current advances in the different experimental strategies to regenerate or neogenerate hair follicles, with emphasis on those involving neogenesis of hair follicles in adults from isolated cells and tissue engineering. Most of these experiments were performed using rodent cells, particularly from embryonic or newborn origin. However, no successful strategy to generate human hair follicles from adult cells has yet been reported. This review identifies several issues that should be considered to achieve this objective. Perhaps the most important challenge is to provide the cells with three-dimensional culture conditions mimicking the structure of living tissue. Improving culture conditions that allow the expansion of specific cells without losing their inductive properties, as well as methods of selecting populations of epithelial stem cells should give us the necessary tools to overcome the difficulties that constrain human hair follicle neogenesis. An analysis of patents trends shows that the number of patent applications aiming to hair follicle regeneration and neogenesis has been growing during the last decade, and this field is attractive not only to academic researchers but also to the companies that own almost half of the patents in this field. Fil: Balaña, Maria Eugenia. Consejo Nacional de Investigaciones Científicas y Técnicas. Oficina de Coordinación Administrativa Parque Centenario. Instituto de Ciencias y Tecnología "Dr. Cesar Milstein"; ArgentinaFil: Charreau, Eduardo Hernan. Consejo Nacional de Investigaciones Científicas y Técnicas. Instituto de Biología y Medicina Experimental (i); Argentina. Clarke, Modet & C°. Technology Intelligence Unit; ArgentinaFil: Leiros, Gustavo Jose. Consejo Nacional de Investigaciones Científicas y Técnicas. Oficina de Coordinación Administrativa Parque Centenario. Instituto de Ciencias y Tecnología "Dr. Cesar Milstein"; Argentin...|$|R
40|$|The seismological {{community}} is currently developing operational earthquake forecasting (OEF) systems that aim to estimate, based on continuous ground motion recording by seismic networks, {{the rates of}} events exceeding a certain magnitude threshold {{in an area of}} interest and in a short-period of time (days to weeks); i. e., the seismicity. OEF may be possibly used for short-term seismic risk management in regions affected by seismic swarms only if its results may be the input to compute, in a probabilistically sound manner, consequence-based risk metrics. The present paper reports the investigation about feasibility of short-term risk assessment, or operational earthquake loss forecasting (OELF), in Italy. The approach is that of performance-based earthquake <b>engineering,</b> where the <b>loss</b> rates are computed by means of hazard, vulnerability, and exposure. The risk is expressed in terms of individual and regional measures, which are based on short-term macroseismic intensity, or ground motion intensity, hazard. The vulnerability of the built environment relies on damage probability matrices empirically calibrated for Italian structural classes, and exposure data in terms of buildings per vulnerability class and occupants per building typology. All vulnerability and exposure data are at the municipality scale. The procedure set-up, which is virtually independent on the seismological model used, is implemented in an experimental OELF system, which continuously process OEF information to produce weekly nationwide risk maps. This is illustrated by a retrospective application to the 2012 Pollino (southern Italy) seismic sequence, which provides insights on the capabilities of the system and on the impact, on short-term risk assessment, of the methodology currently used for OEF in Italy...|$|R
40|$|The {{construction}} industry needs to develop methodologies and techniques to better promote quality <b>engineering</b> and minimize <b>losses.</b> This study aims {{to identify the}} Critical Success Factors (CFSs) {{that contribute to the}} successful development of infrastructure projects in Malaysia and examine their possible impact on project objectives in scope, time, cost and quality. Main contractors are usually intensively involved in the development of infrastructure projects. However, there is limited research touching on main contractors’ view on CSFs for infrastructure projects in Malaysia. Therefore, this study is carried out to explore this topic from the main contractors’ perspective. Based on an extensive literature review, 33 candidate CSFs were identified and examined. Practitioners in selected companies that had been intensively involved in infrastructure development in Malaysia were invited to participate in a questionnaire survey. The questionnaire survey was designed to elicit professional opinions of those practitioners on the significance level and the impact on project objectives of the identified candidate CSFs. A significance index (SI) was calculated to show the significance level of the candidate CSFs. This explorative study has found {{that the majority of the}} 33 identified candidate CSFs were perceived to be of critical significance by the respondents. Some of them hold particular importance to achieving project management objectives in terms of scope, time, cost, and quality. The findings may be used as a checklist so as to increase the quality and success rate of future infrastructure projects in Malaysia. Research limitations and future directions are also discussed. Xiao-Hua Jin, Hai Chen Tan, Jian Zuo, and Yingbin Fen...|$|R
40|$|Myoblast {{transfer}} {{therapy has}} been extensively studied {{for a wide range}} of clinical applications, such as tissue <b>engineering</b> for muscular <b>loss,</b> cardiac surgery or Duchenne Muscular Dystrophy treatment. However, this approach has been hindered by numerous limitations, including early myoblast death after injection and specific immune response after transplantation with allogenic cells. Different cell sources have been analyzed to overcome some of these limitations. The object of our study was to investigate the growth potential, characterization and integration in vivo of human primary fetal skeletal muscle cells. These data together show the potential for the creation of a cell bank to be used as a cell source for muscle cell therapy and tissue engineering. For this purpose, we developed primary muscular cell cultures from biopsies of human male thigh muscle from a 16 -week-old fetus and from donors of 13 and 30 years old. We show that fetal myogenic cells can be successfully isolated and expanded in vitro from human fetal muscle biopsies, and that fetal cells have higher growth capacities when compared to young and adult cells. We confirm lineage specificity by comparing fetal muscle cells to fetal skin and bone cells in vitro by immunohistochemistry with desmin and 5. 1 H 11 antibodies. For the feasibility of the cell bank, we ensured that fetal muscle cells retained intrinsic characteristics after 5 years cryopreservation. Finally, human fetal muscle cells marked with PKH 26 were injected in normal C 57 BL/ 6 mice and were found to be present up to 4 days. In conclusion we estimate that a human fetal skeletal muscle cell bank can be created for potential muscle cell therapy and tissue engineering...|$|R
40|$|Current use of {{microbes}} for metabolic <b>engineering</b> {{suffers from}} <b>loss</b> of metabolic output due to natural selection. Rather than combat {{the evolution of}} bacterial populations, we chose to embrace what makes biological engineering unique among engineering fields - evolving materials. We harnessed bacteria to compute solutions to the biological problem of metabolic pathway optimization. Our approach is called Programmed Evolution to capture two concepts. First, a population of cells is programmed with DNA code to enable it to compute solutions to a chosen optimization problem. As analog computers, bacteria process known and unknown inputs and direct the output of their biochemical hardware. Second, the system employs the evolution of bacteria toward an optimal metabolic solution by imposing fitness defined by metabolic output. The current study is a proof-of-concept for Programmed Evolution applied to the optimization of a metabolic pathway for the conversion of caffeine to theophylline in E. coli. Introduced genotype variations included strength of the promoter and ribosome binding site, plasmid copy number, and chaperone proteins. We constructed 24 strains using all combinations of the genetic variables. We used a theophylline riboswitch and a tetracycline resistance gene to link theophylline production to fitness. After subjecting the mixed population to selection, we measured {{a change in the}} distribution of genotypes in the population and an increased conversion of caffeine to theophylline among the most fit strains, demonstrating Programmed Evolution. Programmed Evolution inverts the standard paradigm in metabolic engineering by harnessing evolution instead of fighting it. Our modular system enables researchers to program bacteria and use evolution to determine the combination of genetic control elements that optimizes catabolic or anabolic output and to maintain it in a population of cells. Programmed Evolution could be used for applications in energy, pharmaceuticals, chemical commodities, biomining, and bioremediation...|$|R
40|$|The {{research}} {{sponsored by}} this project has greatly expanded the ASSET corrosion prediction software system {{to produce a}} world-class technology to assess and predict engineering corrosion of metals and alloys corroding by exposure to hot gases. The effort included corrosion data compilation from numerous industrial sources and data generation at Shell Oak Ridge National Laboratory and several other companies for selected conditions. These data were organized into groupings representing various combinations of commercially available alloys and corrosion by various mechanisms after acceptance via a critical screening process to ensure the data were for alloys and conditions, which were adequately well defined, and of sufficient repeatability. ASSET is {{the largest and most}} capable, publicly-available technology in the field of corrosion assessment and prediction for alloys corroding by high temperature processes in chemical plants, hydrogen production, energy conversion processes, petroleum refining, power generation, fuels production and pulp/paper processes. The problems addressed by ASSET are: determination of the likely dominant corrosion mechanism based upon information available to the chemical engineers designing and/or operating various processes and prediction of <b>engineering</b> metal <b>losses</b> and lifetimes of commercial alloys used to build structural components. These assessments consider exposure conditions (metal temperatures, gas compositions and pressures), alloy compositions and exposure times. Results of the assessments are determination of the likely dominant corrosion mechanism and prediction of the loss of metal/alloy thickness as a function of time, temperature, gas composition and gas pressure. The uses of these corrosion mechanism assessments and metal loss predictions are that the degradation of processing equipment can be managed {{for the first time in}} a way which supports efforts to reduce energy consumption, ensure structural integrity of equipment with the goals to avoid premature failure, to quantitatively manage corrosion over the entire life of high temperature process equipment, to select alloys for equipment and to assist in equipment maintenance programs. ASSET software operates on typical Windows-based (Trademark of Microsoft Corporation) personal computers using operating systems such as Windows 2000, Windows NT and Vista. The software is user friendly and contains the background information needed to make productive use of the software in various help-screens in the ASSET software. A graduate from a university-level curriculum producing a B. S. in mechanical/chemical/materials science/engineering, chemistry or physics typically possesses the background required to make appropriate use of ASSET technology. A training/orientation workshop, which requires about 3 hours of class time was developed and has been provided multiple times to various user groups of ASSET technology. Approximately 100 persons have been trained in use of the technology. ASSET technology is available to about 65 companies representing industries in petroleum/gas production and processing, metals/alloys production, power generation, and equipment design...|$|R
40|$|ENGLISH: Social Engineers {{attack the}} weakest link in an organization’s barrier - it’s human users. They {{do this by}} {{manipulating}} the users into performing actions they wouldn’t normally perform. This can have devastating consequences for an organization. The goal may be to get unauthorized access to sensitive information, or gain access to restricted areas, like server rooms. While crackers use their technical skills to break into a computer system and retrieve a password, the Social Engineer use his social skills to make an individual reveal the password themselves. While there has been written books and papers on different attack vectors, and even some methods for defending against this threat, they are not considered scientific - they are {{in many cases the}} experience and views of one particular individual. The amount of scientific work on Social Engineering {{do not appear to be}} comprehensive. This Thesis has gathered the essence of what different authors has conveyed about Social Engineering attacks and defenses, as well as why it actually works. Further it has investigated how popular Social Engineering is in Norway, what vector of attacks are most common and effective, as well as what defense mechanisms one should implement to stand strong against these threats. This has primarily been done by the development of a Questionnaire targeting Norwegian Organizations, a review of existing literature and research, as well as some preliminary interviews with Information Security Professionals. The results suggest that: (i) Social Engineering by E-Mail is by far the most heavily used vector of attack, followed by attacks originating from websites (ii) most Organizations have mechanisms to defend against Social Engineering, (iii) Organizations conceived Security Risk of Social Engineering is leaning towards medium-high and (iv) the ultimate economic consequences due to Social <b>Engineering</b> attacks are <b>loss</b> of millions of NOK. Further, not surprising, the review of earlier literature and research, as well as data gathered from our Questionnaire, suggest that Security Awareness is a very important factor for defending against Social Engineering. We end the Thesis by discussing important steps when developing Security Awareness programs...|$|R
