8|0|Public
50|$|The PX-8 did {{not have}} any {{internal}} disk drive, and instead allowed either memory to be partitioned into application memory and a RAM disk, or an external 60 KB or 120 KB <b>intelligent</b> <b>RAM</b> disk module to be attached (64K and 128K internally but some used for the processor). The <b>intelligent</b> <b>RAM</b> disk module had its own Z80 processor with a backup battery.|$|E
40|$|The goal of <b>Intelligent</b> <b>RAM</b> (IRAM) is {{to design}} a {{cost-effective}} computer by designing a processor in a memory fabrication process, instead of in a conventional logic fabrication process, and include memory on-chip. To design a processor in a DRAM process one must learn about the business {{and culture of the}} DRAMs, which is quite different from microprocessors. We describe some of those differences, and then our current vision of IRAM applications, architectures, and implementations. 1. Potential and Challenges of IRAM <b>Intelligent</b> <b>RAM</b> (IRAM) may lead to a different style of computer than those based on conventional microprocessors. IRAM technology offers the following potential:. Improve memory latency by factors of 5 to 10 and memory bandwidth by factors of 50 to 100, by redesigning the memory interface and exploiting the proximity of on-chip memory [1][2];. Improve energy efficiency of memory by factors of 2 to 4, primarily by going off-chip less frequently [3][4];. Reduce de [...] ...|$|E
40|$|Conventional {{architectures}} {{have been}} developed with a transistor budget {{of a few hundred}} thousand and have evolved to designs of about 10 million transistors, achieving impressive performance. However, we believe that these architectures will not scale efficiently another hundredfold to utilize billion-transistor chips effectively. Here we introduce an alternative way of using the huge amount of real estate available on such a chip: integrating the processor and the main memory on the same die. We call this architecture IRAM, for <b>Intelligent</b> <b>RAM.</b> We claim that a vector microprocessor is an ideal architecture for an IRAM chip. We discuss some of its merits and present a potential IRAM implementation fabricated in a Gigabit DRAM process. Keywords: billion transistor chips, scalable performance, IRAM, processor-memory integration, system-on-a-chip, vector architecture Limitations of Conventional Architectures The importance of an efficient memory system is increasing as fabrication proce [...] ...|$|E
40|$|Computer memory {{systems are}} {{increasingly}} a bottleneck limiting application performance. IRAM architectures, which integrate a CPU with DRAM main memory {{on a single}} chip, promise to remove this limitation by providing tremendous main memory bandwidth and significant reductions in memory latency. To determine whether existing microarchitectures can tap the potential performance advantages of IRAM systems, we examined both execution time analyses of existing microprocessors and system simulation of hypothetical processors. Our results indicate that, for current benchmarks, existing architectures, whether simple, superscalar or out-of-order, are unable to exploit IRAM's increased memory bandwidth and decreased memory latency to achieve significant performance benefits. 1 Introduction One proposed solution to the growing gap between microprocessor performance and main memory latency is to integrate a processor and DRAM on the same die, an organization we refer to as <b>Intelligent</b> <b>RAM</b> (IRAM) [ [...] ...|$|E
40|$|Abstract: Two trends {{call into}} {{question}} the current practice of microprocessors and DRAMs being fabricated as different chips on different fab lines: 1) the gap between processor and DRAM speed is growing at 50 % per year; and 2) the size and organization of memory on a single DRAM chip is becoming awkward to use in a system, yet size is growing at 60 % per year. <b>Intelligent</b> <b>RAM,</b> or IRAM, merges processing and memory into a single chip to lower memory latency, increase memory bandwidth, and improve energy efficiency as well as to allow more flexible selection of memory size and organization. In addition, IRAM promises savings in power and board area. This paper reviews the state of microprocessors and DRAMs today, explores some of the opportunities and challenges for IRAMs, and finally estimates performance and energy efficiency of three IRAM designs. 1. Introduction and Wh...|$|E
40|$|Two trends {{call into}} {{question}} the current practice of microprocessors and DRAMs being fabricated as different chips on different fab lines: 1) the gap between processor and DRAM speed is growing at 50 % per year; and 2) the size and organization of memory on a single DRAM chip is becoming awkward to use in a system, yet size is growing at 60 % per year. <b>Intelligent</b> <b>RAM,</b> or IRAM, merges processing and memory into a single chip to lower memory latency, increase memory bandwidth, and improve energy efficiency as well as to allow more flexible selection of memory size and organization. In addition, IRAM promises savings in power and board area. This paper reviews the state of microprocessors and DRAMs today, explores some of the opportunities and challenges for IRAMs, and finally estimates performance and energy efficiency of three IRAM designs. 1. Introduction and Why there is a Problem The division of the semiconductor industry into microprocessor and memory camps provides many advantag [...] ...|$|E
40|$|For many decades, dynamic {{random access}} memory, or DRAM, {{has been the}} {{technology}} of choice for use as core processor memory storage. Although the functionality and general access characteristics of DRAM have not changed dramatically since its inception, the technology has evolved by continually improving in overall bit density. However, the success of DRAM has also enabled {{the emergence of the}} processormemory latency gap. Much effort has been spent improving processor functionality and redesigning memory hierarchies to limit the effects of this growing gap in performance. In recent years, there has been a rapid onset of different designs proposed to attack the problem at the memory itself. A summary of current DRAM and SRAM technologies focused on this problem is given, enumerating the specific design characteristics that differ among the proposals. Next, we present two technologies, <b>intelligent</b> <b>RAM</b> and magnetic RAM, which provide a fundamentally different perspective from conventional DRAM architectures for bridging the processor-memory gap. Our study is concluded by a discussion on the future of memory architectures. 1...|$|E
40|$|In {{this paper}} we {{investigate}} various algorithms for performing Fast Fourier Transformation (FFT) /Inverse Fast Fourier Transformation (IFFT), and proper techniques for maximizing the FFT/IFFT execution speed, such as pipelining or parallel processing, {{and use of}} memory structures with pre-computed values (look up tables -LUT) or other dedicated hardware components (usually multipliers). Furthermore, we discuss the optimal hardware architectures that best apply to various FFT/IFFT algorithms, along with their abilities to exploit parallel processing with minimal data dependences of the FFT/IFFT calculations. An interesting approach that is also considered in this paper is {{the application of the}} integrated processing-in-memory <b>Intelligent</b> <b>RAM</b> (IRAM) chip to high speed FFT/IFFT computing. The results of the assessment study emphasize that the execution speed of the FFT/IFFT algorithms is tightly connected to the capabilities of the FFT/IFFT hardware to support the provided parallelism of the given algorithm. Therefore, we suggest that the basic Discrete Fourier Transform (DFT) /Inverse Discrete Fourier Transform (IDFT) can also provide high performances, by utilizing a specialized FFT/IFFT hardware architecture that can exploit the provided parallelism of the DFT/IDF operations. The proposed improvements include simplified multiplications over symbols given in polar coordinate system, using sin–µ and cosine look up tables, and an approach for performing parallel addition of N input symbols...|$|E

