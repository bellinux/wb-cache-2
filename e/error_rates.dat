8490|10000|Public
25|$|By {{enabling}} direct {{sequencing of}} single DNA molecules, third generation sequencing technologies {{have the capability}} to produce substantially longer reads than second generation sequencing. Such advantage has critical implications for both genome science and the study of biology in general. However, due to various technical challenges, third generation sequencing has <b>error</b> <b>rates</b> at almost unrepairable levels, rendering the technologies impractical for certain applications such as de novo genome assembly. These technologies are undergoing active development and therefore it is expected that there will be further improvements to the high <b>error</b> <b>rates.</b> For applications that are more tolerant to <b>error</b> <b>rates</b> such as metagenomics or larger structural variant calling, third generation sequencing has been found to outperform existing methods.|$|E
25|$|The {{integrity}} {{provisions of}} the new program included fraud disqualifications, enhanced Federal funding for States' anti-fraud activities, and financial incentives for low <b>error</b> <b>rates.</b>|$|E
25|$|Only a tiny {{fraction}} of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 1016 bits, and another SAS enterprise disk from 2013 specifies similar <b>error</b> <b>rates.</b> Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 1016 bits. An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the Data Integrity Field standard to combat data corruption, specifies similar <b>error</b> <b>rates</b> in 2005.|$|E
40|$|A bit <b>error</b> <b>rate</b> test on a {{transceiver}} is {{accelerated by}} adding a phase offset to data phase encoding and decoding in the transceiver and by mapping bit <b>error</b> <b>rate</b> test results from an elevated <b>error</b> <b>rate</b> condition to a normal <b>error</b> <b>rate</b> condition for the transceiver. The elevated <b>error</b> <b>rate</b> is accomplished by adjusting the phase of the phase encoder and decoder with {{the value of the}} phase offset so that the encoded data transmission signal is not as robust against noise as it normally would be. Noise {{in the form of an}} interference signal is introduced during the transmission, and the bit <b>error</b> <b>rate</b> is measured after the receiver has decoded the signal. The bit <b>error</b> <b>rate</b> (BER) data with an elevated propensity for error is mapped against bit <b>error</b> <b>rate</b> data for normal operations. A mapping function is built to map BERE (bit <b>error</b> <b>rate</b> elevated) data�data from the elevated <b>error</b> <b>rate</b> condition for data encoding, to BERN (normal bit <b>error</b> <b>rate)</b> data�data from the normal <b>error</b> <b>rate</b> condition for data encoding. The BERE data is mapped to BERN data using the mapping function so that the BER performance of the transceiver may be measured using far fewer test sequences of digital bits. Georgia Institute Of Technolog...|$|R
40|$|In {{spite of}} the {{excellent}} bit <b>error</b> <b>rate</b> performance of the original turbo-code, simulations indicate that this code yields relatively poor frame <b>error</b> <b>rate</b> performance. In this paper we show that "primitive" turbo-codes and "asymmetric" turbocodes provides both good bit <b>error</b> <b>rate</b> and frame <b>error</b> <b>rate</b> performance...|$|R
5000|$|... {{where is}} the Bayes <b>error</b> <b>rate</b> (which is the minimal <b>error</b> <b>rate</b> possible), [...] is the k-NN <b>error</b> <b>rate,</b> and [...] {{is the number of}} classes in the problem. For [...] and as the Bayesian <b>error</b> <b>rate</b> [...] {{approaches}} zero, this limit reduces to [...] "not more than twice the Bayesian error rate".|$|R
25|$|Third {{generation}} sequencing {{technologies have}} demonstrated promising prospects {{in solving the}} problem of transcript detection as well as mRNA abundance estimation {{at the level of}} transcripts. While <b>error</b> <b>rates</b> remain high, third generation sequencing technologies have the capability to produce much longer read lengths. Pacific Bioscience has introduced the iso-seq platform, proposing to sequence mRNA molecules at their full lengths. It is anticipated that Oxford Nanopore will put forth similar technologies. The trouble with higher <b>error</b> <b>rates</b> may be alleviated by supplementary high quality short reads. This approach has been previously tested and reported to reduce the error rate by more than 3 folds.|$|E
25|$|All {{statistical}} hypothesis tests have a probability of making type I and type II errors. For example, all blood tests for a disease will falsely detect {{the disease in}} some proportion {{of people who do}}n't have it, and will fail to detect the disease in some proportion of people who do have it. A test's probability of making a type I error is denoted by α. A test's probability of making a type II error is denoted by β. These <b>error</b> <b>rates</b> are traded off against each other: for any given sample set, the effort to reduce one type of error generally results in increasing the other type of error. For a given test, the only way to reduce both <b>error</b> <b>rates</b> is to increase the sample size, and this may not be feasible.|$|E
25|$|In 1998 Chris Phoenix informally {{outlined}} a {{design for a}} hydraulically powered replicator a few cubic feet in volume that used ultraviolet light to cure soft plastic feedstock and a fluidic logic control system, but didn't address most {{of the details of}} assembly procedures, <b>error</b> <b>rates,</b> or machining tolerances.|$|E
5000|$|Total <b>Error</b> <b>Rate</b> = ((INF + IF)/ (C + INF + IF)) * 100%Not Corrected <b>Error</b> <b>Rate</b> = (INF/ (C + INF + IF)) * 100%Corrected <b>Error</b> <b>Rate</b> = (IF/ (C + INF + IF)) * 100% ...|$|R
40|$|The Student-Newman-Kuels {{procedure}} {{is a well-known}} step-down multiple comparisons procedure with critical values based on the Studentized range distribution. The False Discovery Rate is a Type I <b>error</b> <b>rate</b> for multiple comparisons, intermediate in stringency to the weak familywise <b>error</b> <b>rate</b> (experimentwise <b>error</b> <b>rate)</b> and the strong familywise <b>error</b> <b>rate.</b> We show that SNK controls FDR. Multiple comparisons Simultaneous inference SNK FDR...|$|R
40|$|Abstract—Due to high complexity, the low-complexity Gaussian-assumption bit <b>error</b> <b>rate</b> or {{the upper}} bound and lower bound of BER is usually used in CDMA systems in stead {{of the real}} bit <b>error</b> <b>rate.</b> In this paper, we propose a low-complexity {{approximate}} bit <b>error</b> <b>rate</b> formula which is closer to real bit <b>error</b> <b>rate</b> than the Gaussian-assumption bit <b>error</b> <b>rate</b> in CDMA systems. At {{the end of the}} paper, we apply our formula to the MC-CDMA system with a slowly fading flat channel to verify our formula...|$|R
25|$|These {{issues are}} more {{difficult}} for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical pulse shaping. <b>Error</b> <b>rates</b> are typically proportional to the ratio of operating time to decoherence time, hence any operation must be completed {{much more quickly than}} the decoherence time.|$|E
25|$|On average, {{different}} {{individuals of}} the human population share about 99.9% of their genes. In other words, approximately {{only one out of}} every thousand bases would differ between any two person. The high <b>error</b> <b>rates</b> involved with third generation sequencing are inevitably problematic for the purpose of characterizing individual differences that exist between members of the same species.|$|E
25|$|Phthalocyanine dye CD-Rs {{are usually}} silver, gold or light green. The patents on {{phthalocyanine}} CD-Rs {{are held by}} Mitsui and Ciba Specialty Chemicals. Phthalocyanine is a natively stable dye (has no need for stabilizers) and CD-Rs based on this are often given a rated lifetime of hundreds of years. Unlike cyanine, phthalocyanine is less resistant to UV rays and CD-Rs based on this dye show signs of degradation only {{after two weeks of}} direct sunlight exposure. However, phthalocyanine is more sensitive than cyanine to writing laser power calibration, meaning that the power level used by the writing laser has to be more accurately adjusted for the disc {{in order to get a}} good recording; this may erode the benefits of dye stability, as marginally written discs (with higher correctable <b>error</b> <b>rates)</b> will lose data (i.e. have uncorrectable errors) after less dye degradation than well written discs (with lower correctable <b>error</b> <b>rates).</b>|$|E
50|$|As {{the size}} of {{training}} data set approaches infinity, the one nearest neighbour classifier guarantees an <b>error</b> <b>rate</b> of no worse than twice the Bayes <b>error</b> <b>rate</b> (the minimum achievable <b>error</b> <b>rate</b> given {{the distribution of the}} data).|$|R
30|$|In case of [2], {{the average}} {{estimation}} <b>error</b> <b>rate</b> was 3 %, so our assumed estimation <b>error</b> <b>rate</b> {{was higher than}} 3 %. Similarly, for [10, 12], the estimation rate was assumed based on their average estimation <b>error</b> <b>rate.</b>|$|R
50|$|Two {{performance}} measures give quality characteristics of an ARQ-M link. These are <b>error</b> <b>rate</b> and throughput. Residual errors can {{be due to}} transpositions of the symbol elements or double errors. The chances that this happens is about 100 to 1000 times less than for a working unprotected link. A log graph of residual <b>error</b> <b>rate</b> against raw <b>error</b> <b>rate</b> shows a steeper line with slope 2 intercepting at 100% errors. If the unprotected 5 unit code had an <b>error</b> <b>rate</b> of 1%, the ARQ-M protected code <b>error</b> <b>rate</b> is 0.0025%.|$|R
25|$|Reverse {{transcriptase}} {{has a high}} {{error rate}} when transcribing RNA into DNA since, unlike most other DNA polymerases, it has no proofreading ability. This high error rate allows mutations to accumulate at an accelerated rate relative to proofread forms of replication. The commercially available reverse transcriptases produced by Promega are quoted by their manuals as having <b>error</b> <b>rates</b> {{in the range of}} 1 in 17,000 bases for AMV and 1 in 30,000 bases for M-MLV.|$|E
25|$|Long read lengths {{offered by}} third {{generation}} sequencing may alleviate {{many of the}} challenged currently faced by de novo genome assemblies. For example, if a entire repetitive region can be sequenced unambiguously in a single read, no approximative computation inference would be required. Computational methods have been proposed to alleviate the issue of high <b>error</b> <b>rates.</b> For example, in one study, it was demonstrated that de novo assembly of a microbial genome using PacBio sequencing alone performed superior to that of second generation sequencing.|$|E
25|$|Because {{the ballot}} marking is more complex, {{there can be}} an {{increase}} in spoiled ballots. In Australia, voters are required to write a number beside every candidate, and <b>error</b> <b>rates</b> can be five times higher than plurality voting elections Since Australia has compulsory voting, however, it is difficult to tell how many ballots are deliberately spoiled. Most jurisdictions with IRV do not require complete rankings and may use columns to indicate preference instead of numbers. In American elections with IRV, more than 99% of voters typically cast a valid ballot.|$|E
40|$|Abstract. This paper {{proposes a}} method for {{reducing}} the Bit <b>Error</b> <b>Rate</b> (BER), Frame <b>Error</b> <b>Rate</b> (FER) in OFDM system using Spatial Multiplexing. Spatial multiplexing is a transmission technique in MIMO wireless communication to transmit independent and separately encoded data signals {{from each of the}} multiple transmits antennas. For different Modulation techniques to calculate the <b>Error</b> <b>rate</b> for OFDM system. Finally compare the performance using Data <b>Rate</b> and <b>Error</b> <b>Rate...</b>|$|R
30|$|One of the {{effective}} techniques {{to reduce the}} <b>error</b> <b>rate</b> in memories is through ECC. As described in “PRAM reliability” and “STT-RAM reliability” sections, raw <b>error</b> <b>rate</b> of MLC PRAM and STT-RAM can significantly be reduced using circuit-level techniques. For instance, the <b>error</b> <b>rate</b> of MLC PRAM {{can be reduced to}} 10 – 4 by adjusting Rth(10, 00) and the <b>error</b> <b>rate</b> of STT-RAM can be reduced to 10 – 5 by voltage boosting and/or write pulse width adjustment.|$|R
50|$|In 2011, an <b>error</b> <b>rate</b> of 0.27 percent, {{improving}} on {{the previous}} best result, was reported by researchers using a similar system of neural networks. In 2013, an approach based on regularization of neural networks using DropConnect has been claimed to achieve a 0.21 percent <b>error</b> <b>rate.</b> Recently, the single convolutional neural network best performance was 0.31 percent <b>error</b> <b>rate.</b> Currently, the best performance of a single convolutional neural network trained in 74 epochs on the expanded training data is 0.27 percent <b>error</b> <b>rate.</b> Also, the Parallel Computing Center (Khmelnitskiy, Ukraine) obtained an ensemble of only 5 convolutional neural networks which performs on MNIST at 0.21 percent <b>error</b> <b>rate.</b>|$|R
25|$|The working {{world of}} the 21st century is mainly based on Total Quality Management. This is derived from quality control. In {{contrast}} to Taylorism, by which products are produced in the shortest possible time without any form of quality control and delivered to the end customer, the focus {{in the 21st century}} is on quality control at TQM. In order to avoid <b>error</b> <b>rates,</b> it is necessary to hire specialists to check all the products which have been manufactured before they are delivered to the end customer. The quality controls have improved over time, and incorrect partial processes can be detected in time and removed from the production process.|$|E
25|$|As {{the feature}} size of flash memory cells reaches the 15-16 nm minimum limit, further flash density {{increases}} will {{be driven by}} TLC (3bits/cell) combined with vertical stacking of NAND memory planes. The decrease in endurance and increase in uncorrectable bit <b>error</b> <b>rates</b> that accompany feature size shrinking can be compensated by improved error correction mechanisms. Even with these advances, it may be impossible to economically scale flash to smaller and smaller dimensions {{as the number of}} electron holding capacity reduces. Many promising new technologies (such as FeRAM, MRAM, PMC, PCM, ReRAM, and others) are under investigation and development as possible more scalable replacements for flash.|$|E
25|$|There are {{a variety}} of {{theories}} about the cause of this gap. However, it has been well established that one of the main issues is that the questions studied by restoration ecologists are frequently not found useful or easily applicable by land managers. For instance, many publications in restoration ecology characterize the scope of a problem in depth, without providing concrete solutions. Additionally many restoration ecology studies are carried out under controlled conditions and frequently at scales much smaller than actual restorations. Whether or not these patterns hold true in an applied context is often unknown. There is evidence that these small scale experiments inflate type II <b>error</b> <b>rates</b> and differ from ecological patterns in actual restorations.|$|E
50|$|In statistics, per-comparison <b>error</b> <b>rate</b> (PCER) is the {{probability}} of a Type I error {{in the absence of any}} multiple hypothesis testing correction. This is a liberal <b>error</b> <b>rate</b> relative to the false discovery <b>rate</b> and familywise <b>error</b> <b>rate,</b> in that it is always less than or equal to those rates.|$|R
5000|$|Since the {{carriers}} are independent, the overall bit <b>error</b> <b>rate</b> {{is the same}} as the per-carrier <b>error</b> <b>rate,</b> just like BPSK and QPSK: ...|$|R
5000|$|... #Caption: Progress in machine {{classification}} of images--------------------------The <b>error</b> <b>rate</b> of AI by year. Red line - the <b>error</b> <b>rate</b> of a trained human ...|$|R
25|$|Third {{generation}} sequencing, as {{it currently}} stands, faces important challenges mainly surrounding accurate identification of nucleotide bases; <b>error</b> <b>rates</b> are still much higher compared to second generation sequencing. This is generally due to instability of the molecular machinery involved. For example, in PacBio’s single molecular and real time sequencing technology, the DNA polymerase molecule becomes increasingly damaged as the sequencing process occurs. Additionally, since the process happens quickly, the signals given off by individual bases may be blurred by signals from neighbouring bases. This poses a new computational challenge for deciphering the signals and consequently inferring the sequence. Methods such as Hidden Markov Models, for example, have been leveraged {{for this purpose}} with some success.|$|E
25|$|On January 7, 1972, the FCC amended Part 97 {{to allow}} faster RTTY speeds. Four {{standard}} RTTY speeds were authorized, namely, 60 (45 baud), 67 (50 baud), 75 (56.25 baud) and 100 (75 baud) words per minute. Many Amateur Radio operators had equipment that {{was capable of}} being upgraded to 75 and 100 words per minute by changing teleprinter gears. While there was an initial interest in 100 words per minute operation, many Amateur Radio operators moved back to 60 words per minute. Some {{of the reasons for}} the failure of 100 words per minute HF RTTY included poor operation of improperly maintained mechanical teleprinters, narrow bandwidth terminal units, continued use of 170Hz shift at 100 words per minute and excessive <b>error</b> <b>rates</b> due to multipath distortion and the nature of ionospheric propagation.|$|E
25|$|In the {{psychology}} of visual perception and motor control, the term response priming denotes a special form of visuomotor priming effect. The distinctive feature of response priming is that prime and target are presented in quick succession (typically, less than 100 milliseconds apart) and are coupled to identical or alternative motor responses. When a speeded motor response is performed to classify the target stimulus, a prime immediately preceding the target can thus induce response conflicts when assigned to a different response as the target. These response conflicts have observable effects on motor behavior, leading to priming effects, e.g., in response times and <b>error</b> <b>rates.</b> A special property of response priming is its independence from visual awareness of the prime: For example, response priming effects can increase under conditions where visual awareness of the prime is decreasing.|$|E
40|$|Abstract—In this paper, we derive exact {{closed form}} bit <b>error</b> <b>rate</b> (BER) or symbol <b>error</b> <b>rate</b> (SER) {{expressions}} for orthogonal frequency division multiplexing (OFDM) systems with carrier frequency offset (CFO). We consider {{the performance of}} an OFDM system subject to CFO error in additive white Gaussian noise (AWGN), frequency flat and frequency selective Rayleigh fading channels. The BER / SER performances of BPSK and QPSK modulation schemes are analyzed for AWGN and frequency-flat Rayleigh fading channels while BPSK is considered for frequency-selective Rayleigh fading channels. Our results can easily be reduced to the respective analytical <b>error</b> <b>rate</b> expressions for the OFDM systems without CFO error. Furthermore, the simulation results are provided to verify {{the accuracy of the}} new <b>error</b> <b>rate</b> expressions. Index Terms—Bit <b>error</b> <b>rate</b> (BER), frequency offset, frequency selective fading, orthogonal frequency division multiplexing (OFDM), Rayleigh fading, symbol <b>error</b> <b>rate</b> (SER). I...|$|R
50|$|Although it is {{sometimes}} claimed that least squares (or classical statistical methods in general) are robust, they are only robust {{in the sense that}} the type I <b>error</b> <b>rate</b> does not increase under violations of the model. In fact, the type I <b>error</b> <b>rate</b> tends to be lower than the nominal level when outliers are present, and there is often a dramatic increase in the type II <b>error</b> <b>rate.</b> The reduction of the type I <b>error</b> <b>rate</b> has been labelled as the conservatism of classical methods.|$|R
5000|$|... k-NN {{has some}} strong {{consistency}} results. As {{the amount of}} data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an <b>error</b> <b>rate</b> no worse than twice the Bayes <b>error</b> <b>rate</b> (the minimum achievable <b>error</b> <b>rate</b> given {{the distribution of the}} data). [...] Various improvements to the k-NN speed are possible by using proximity graphs.|$|R
