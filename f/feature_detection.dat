1788|1730|Public
25|$|HSL, HSV, HSI, {{or related}} models {{are often used}} in {{computer}} vision and image analysis for <b>feature</b> <b>detection</b> or image segmentation. The applications of such tools include object detection, for instance in robot vision; object recognition, for instance of faces, text, or license plates; content-based image retrieval; and analysis of medical images.|$|E
25|$|VIL {{can be used}} to {{estimate}} the potential for downburst, too. A convective downdraft is linked to three forces in the vertical, namely perturbation pressure gradient force, buoyancy force and precipitation loading. The pressure gradient force was neglected as it has significant effect only on the updraft in supercells. With this assumption and other simplifications (e.g. requiring the environment of the air parcel to be static on the time scale of the downdraft). The resulting momentum equation is integrated over height to yield the kinetic energy of the parcel on descending to the surface and is found to be the negative CAPE of a dry air parcel injected into the storm, plus de motion of the convective cell. S. R. Stewart, from NWS, has published in 1991 an equation relating VIL and the echo tops that give the potential for surface gust using this concept. This is a predictive result that gives a certain lead time. With the Doppler velocity data, the meteorologist can see the downdraft and gust fronts happening, but since this a small scale <b>feature,</b> <b>detection</b> algorithms have been developed to point convergence and divergence areas under a thunderstorm on the radar display.|$|E
25|$|Transcriptomics {{methods are}} highly {{parallel}} and require significant computation to produce meaningful data for both microarray and RNA-Seq experiments. Microarray data is recorded as high-resolution images, requiring <b>feature</b> <b>detection</b> and spectral analysis. Microarray raw image files are each about 750 MB in size, while the processed intensities are around 60 MB in size. Multiple short probes matching a single transcript can reveal {{details about the}} intron-exon structure, requiring statistical models to determine {{the authenticity of the}} resulting signal. RNA-Seq studies produce billions of short DNA sequences, which must be aligned to reference genomes composed of millions to billions of base pairs. De novo assembly of reads within a dataset requires the construction of highly complex sequence graphs. RNA-Seq operations are highly repetitious and benefit from parallelised computation but modern algorithms mean consumer computing hardware is sufficient for simple transcriptomics experiments that do not require de novo assembly of reads. A human transcriptome could be accurately captured using RNA-Seq with 30 million 100 bp sequences per sample. This example would require approximately 1.8 gigabytes of disk space per sample when stored in a compressed fastq format. Processed count data for each gene would be much smaller, equivalent to processed microarray intensities. Sequence data may be stored in public repositories, such as the Sequence Read Archive (SRA). RNA-Seq datasets can be uploaded via the Gene Expression Omnibus.|$|E
40|$|Problem statement: Template {{matching}} {{had been}} a conventional method for object <b>detection</b> especially facial <b>features</b> <b>detection</b> at the early stage of face recognition research. The appearance of moustache and beard had affected the performance of <b>features</b> <b>detection</b> and face recognition system since ages ago. Approach: The proposed algorithm aimed to reduce the effect of beard and moustache for facial <b>features</b> <b>detection</b> and introduce facial features based template matching as the classification method. An automated algorithm for face recognition system based on detected facial features, iris and mouth had been developed. First, the face region was located using skin color information. Next, the algorithm computed the costs for each pair of iris candidates from intensity valleys as references for iris selection. As for mouth detection, color space method was used to allocate lips region, image processing methods to eliminate unwanted noises and corner detection technique to refine {{the exact location of}} mouth. Finally, template matching was used to classify faces based on the extracted features. Results: The proposed method had shown a better <b>features</b> <b>detection</b> rate (iris = 93. 06 %, mouth = 95. 83 %) than conventional method. Template matching had achieved a recognition rate of 86. 11 % with acceptable processing time (0. 36 sec). Conclusion: The results indicate that the elimination of moustache and beard has not affected the performance of facial <b>features</b> <b>detection.</b> The proposed <b>features</b> based template matching has significantly improved the processing time of this method in face recognition research...|$|R
40|$|<b>Feature</b> point <b>detection</b> is {{generally}} {{the first step}} in model-based approaches to sketch recognition. <b>Feature</b> point <b>detection</b> in free-hand strokes is a hard problem because the input has noise from digitization, from natural hand tremor, and from lack of perfect motor control during drawing. Existing <b>feature</b> point <b>detection</b> methods for free-hand strokes require hand-tuned thresholds for filtering out the false positives. In this paper, we present a threshold-free <b>feature</b> point <b>detection</b> method using ideas from the scale-space theory...|$|R
40|$|Automatic <b>detection</b> {{of facial}} <b>feature</b> points plays an {{important}} role in applications such as facial feature tracking, human-machine interaction and face recognition. The majority of facial <b>feature</b> points <b>detection</b> methods using two-dimensional or three-dimensional data are covered in existing survey papers. In this article chosen approaches to the facial <b>features</b> <b>detection</b> have been gathered and described. This overview focuses on the class of researches exploiting facial <b>feature</b> points <b>detection</b> to represent facial surface for two-dimensional or three-dimensional face. In the conclusion, we discusses advantages and disadvantages of the presented algorithms...|$|R
2500|$|The basic {{neuronal}} {{function of}} sending signals to other cells includes a capability for neurons to exchange signals with each other. [...] Networks formed by interconnected groups of neurons {{are capable of}} a wide variety of functions, including <b>feature</b> <b>detection,</b> pattern generation and timing, and there are seen to be countless types of information processing possible. Warren McCulloch and Walter Pitts showed in 1943 that even artificial neural networks formed from a greatly simplified mathematical abstraction of a neuron are capable of universal computation.|$|E
2500|$|Although the {{simplest}} reflexes may be mediated by circuits lying entirely within the spinal cord, more complex responses rely on signal processing in the brain. For example, when {{an object in}} {{the periphery of the}} visual field moves, and a person looks toward it many stages of signal processing are initiated. [...] The initial sensory response, in the retina of the eye, and the final motor response, in the oculomotor nuclei of the brain stem, are not all that different from those in a simple reflex, but the intermediate stages are completely different. [...] Instead of a one or two step chain of processing, the visual signals pass through perhaps a dozen stages of integration, involving the thalamus, cerebral cortex, basal ganglia, superior colliculus, cerebellum, and several brainstem nuclei. [...] These areas perform signal-processing functions that include <b>feature</b> <b>detection,</b> perceptual analysis, memory recall, decision-making, and motor planning.|$|E
2500|$|<b>Feature</b> <b>detection</b> is {{the ability}} to extract biologically {{relevant}} information from combinations of sensory signals. [...] In the visual system, for example, sensory receptors in the retina of the eye are only individually capable of detecting [...] "points of light" [...] in the outside world. [...] Second-level visual neurons receive input from groups of primary receptors, higher-level neurons receive input from groups of second-level neurons, and so on, forming a hierarchy of processing stages. [...] At each stage, important information is extracted from the signal ensemble and unimportant information is discarded. [...] By the end of the process, input signals representing [...] "points of light" [...] have been transformed into a neural representation of objects in the surrounding world and their properties. [...] The most sophisticated sensory processing occurs inside the brain, but complex feature extraction also takes place in the spinal cord and in peripheral sensory organs such as the retina.|$|E
5000|$|In {{terms of}} {{computational}} complexity, the Constellation Model is very expensive. If [...] {{is the number}} of <b>feature</b> <b>detections</b> in the image, and [...] the number of parts in the object model, then the hypothesis space [...] is [...] Because the computation of sufficient statistics in the E-step of expectation maximization necessitates evaluating the likelihood for every hypothesis, learning becomes a major bottleneck operation. For this reason, only values of [...] have been used in practical applications, and the number of <b>feature</b> <b>detections</b> [...] is usually kept within the range of about 20-30 per image.|$|R
40|$|Precise fundus image <b>features</b> <b>detection</b> is an {{important}} factor for screening diabetic retinopathy. Some noises in fundus image features extraction need to be solved. This paper studies using phase information to attempt get better effect. We use and compare four phase-based approaches and get some instructive results...|$|R
40|$|This MSc Thesis {{deals with}} face {{detection}} in image. In this approach, facial features (eyes, nose, mouth corners) are detected {{first and then}} joined to the whole face. For the facial <b>features</b> <b>detection,</b> classifiers trained with AdaBoost algorithm are used. Haar wavelets are used as features for classification...|$|R
50|$|<b>Feature</b> <b>detection</b> is {{the process}} of finding the {{transformation}} which aligns one image with another. There are two main approaches for <b>feature</b> <b>detection.</b>|$|E
50|$|Occasionally, when <b>feature</b> <b>detection</b> is {{computationally}} {{expensive and}} there are time constraints, a higher level algorithm {{may be used to}} guide the <b>feature</b> <b>detection</b> stage, so that only certain parts of the image are searched for features.|$|E
5000|$|Multi-scale <b>feature</b> <b>detection</b> {{within the}} scale-space framework: ...|$|E
40|$|Abstract. In some Robocup leagues, {{specially}} in the four-legged league, robots {{make use of}} coloured landmarks for localisation. Because these landmarks have no correlation with real soccer, it seems a natural approach to remove them. But {{for this to be}} a reality, there are some difficulties that need to be solved, mainly an efficient and robust field <b>features</b> <b>detection</b> and an efficient localisation technique to manage such type of information. In this paper we deal with an approach for field <b>features</b> <b>detection</b> based on finding intersections between field lines which runs at frame rate in the AIBO robots. We also present some experimental results of the vision system and a comparison of the traditional coloured landmark localisation and the field features only localisation, both using a fuzzy-Markov localisation technique...|$|R
40|$|Session Initiation Protocol (SIP) and Servlet {{technology}} {{introduce new}} ways of delivering telephony services over IP networks. The richness and flexibility of these new protocols make it faster and easier for service providers to develop and deploy new services. This flexibility, however, is offset by the challenge of managing the feature interaction problem, which can prove to be quite severe. This thesis proposes a modified SIP Servlet architecture and introduces a logical entity, the Feature Interaction Handler (FIH), to address the feature interaction problem for telephony services. The approach addresses offline and online <b>feature</b> interaction <b>detection,</b> the former occurring when the user registers to a feature and the latter occurring during feature runtime execution. For offline <b>feature</b> interaction <b>detection,</b> a behaviour mapping approach is introduced to reduce the interaction matrix table. For online <b>feature</b> interaction <b>detection,</b> two mechanisms are proposed [...] -'forward detection' and 'backward detection'. Forward detection extends the originating side user profile when sending the message, such that the terminating side can use it for detection. In contrast, backward detection correlates the SIP session 'request' message with the 'response' message belonging to the same session {{in order to determine}} if the resulting service behaviour is acceptable. To validate the new <b>feature</b> interaction <b>detection</b> approach, an offline <b>feature</b> interaction <b>detection</b> tool and online <b>feature</b> interaction <b>detection</b> unit FIH have been implemented. The feature interaction benchmark is applied on both the tool and the FIH, the result proves to be successful. The <b>feature</b> interaction <b>detection</b> approach proposed in this thesis proves to be a viable solution in the context of SIP servlet service environment...|$|R
30|$|This paper {{proposes a}} novel {{approach}} for visual <b>features</b> <b>detection,</b> {{which is based on}} the presence of objects whose shape can be modelled using cylinders or generalized cylinders. These specific structures are commonly found on indoor and outdoor scenarios, and their image representations, the so-called curvilinear regions, automatically deform with changing viewpoint as to keep on covering identical physical parts of a scene. The method is based on Marr's visual theory that proposes that visual objects can be decomposed in generalized cylinders. Also, part of the method can be compared to the behavior of AOS neurons, placed in the caudal intraparietal sulcus, that respond when an elongated object is visualized. Our detector reliably finds the same curvilinear regions under different viewing conditions. Evaluation results are given to demonstrate the performance of the approach and its ability to be applied for visual <b>features</b> <b>detection</b> in a mobile robot navigation framework.|$|R
5000|$|... #Subtitle level 3: Automatic facial <b>feature</b> <b>detection</b> and {{analyses}} ...|$|E
5000|$|... #Subtitle level 3: Occlusion and {{statistics}} of <b>feature</b> <b>detection</b> ...|$|E
5000|$|<b>Feature</b> <b>detection</b> (computer vision) {{for other}} {{low-level}} feature detectors ...|$|E
50|$|It {{possesses}} a 13.0-megapixel rear camera with a F2.2 aperture <b>feature,</b> HDR, face <b>detection</b> <b>feature</b> and continuous shot function, {{as well as}} a 5.0 MP front-facing camera.|$|R
40|$|Automatic {{transcription}} of prosody {{is necessary}} for spoken language understanding. Prominence and intonational boundaries are routinely used to convey meaning beyond that expressed in the lexical content of speech. Using a classiÞcation rule learning algorithm and computationally light acoustic and syntactic <b>features,</b> <b>detection</b> of pitch accent at 87 % on spontaneous elicited speech were attained along with 94 % accurate detection of full intonational phrase boundaries. 1...|$|R
40|$|Abstract Feature {{models are}} {{commonly}} used to describe software product lines in terms of features. Features are linked by relations, which may introduce errors in the model. This paper gives a description of isolated features and states the detection of them, {{as the first step}} in their treatment. Two implementations are given to automatically support isolated <b>features</b> <b>detection</b> and a third one that uses both and improves the performance...|$|R
5000|$|Modernizr: A <b>feature</b> <b>detection</b> {{library for}} HTML5 and CSS3 features.|$|E
50|$|In {{the fields}} of {{computer}} vision and image analysis, the Harris affine region detector belongs to the category of <b>feature</b> <b>detection.</b> <b>Feature</b> <b>detection</b> is a preprocessing step of several algorithms that rely on identifying characteristic points or interest points so to make correspondences between images, recognize textures, categorize objects or build panoramas.|$|E
5000|$|More general {{articles}} on <b>feature</b> <b>detection,</b> computer vision and image processing: ...|$|E
40|$|This bachelor's thesis {{deals with}} facial <b>features</b> <b>{{detection}}</b> in images. Especially, a skin color detection algorithm is addressed. This method find all pixels {{corresponding to the}} skin color and clusters them into candidate face regions by the help of morfological operations. Afterwards, using the Sobel operator, all facial features are located accurately. Implementation of the proposed facial feature detector based on the well known OpenCV library is presented too...|$|R
40|$|This paper {{studies the}} issue of which filters {{should be used for}} <b>feature</b> point <b>detection.</b> Classical <b>feature</b> point <b>detection</b> meth-ods, e. g., SIFT, are based on the scale-space theory in which Gaussian filters are proven to be optimal under the scale-space axiom. However, the recent method SURF {{demonstrates}} em-pirically that a box filter can also achieve good performance even though it violates the scale-space axiom. This leads to the question: Is Gaussian filters necessary for feature point de-tection? Based on the analysis using filter bank and detection theory, we show that theoretically it is possible for a box fil-ter to perform better than the Gaussian filter. Additionally, we show that a new filter, pyramid filter, performs better than both box and Gaussian filters in some situations. Index Terms — <b>feature</b> point <b>detection,</b> SIFT, SUR...|$|R
40|$|International audienceIn this paper, an {{application}} to seismic {{images of a}} recently proposed algorithm of salient <b>features</b> <b>detection</b> is presented. We compute entropy in each pixel location within a neighborhood using two entropy measures: the Shannon entropy and the generalized cumulative residual entropy. The saliency measure is computed for both fixed and variable scale and {{differences between the two}} entropies are highlighted. The effect of noise is also studied. Results are beneficial for horizon picking in seismic images...|$|R
5000|$|<b>Feature</b> <b>detection</b> (web development) ("Browser sniffing" [...] synonym in some contexts) ...|$|E
5000|$|Automatic <b>feature</b> <b>detection</b> by Pedro Alonso (Spain), mentored by Herbert Bay (Switzerland) ...|$|E
5000|$|... #Caption: <b>Feature</b> <b>detection</b> on 1D {{time domain}} data using Phase Stretch Transform.|$|E
40|$|Key words: image processing；gaussian filtering；dichotomy；linear scanning；thread {{features}} Abstract. Thread {{features of}} the traditional measuring method mainly adopts working gauge measurement, due to limitations in the traditional thread features measurement accuracy is relatively low, the efficiency is low, the cost is high. The thread <b>features</b> <b>detection</b> method based on digital image processing techniques using CCD to obtain basic image of thread, processing the thread image, extracting thread outline, calculating thread features through the computer, improves the efficiency, saves the cost...|$|R
40|$|Abstract. Wavelet {{transforms}} {{originated in}} geophysics {{in the early}} 1980 s {{for the analysis of}} seismic signals. Since then, significant mathematical advances in wavelet theory have enabled a suite of applications in diverse fields. In geophysics the power of wavelets for analysis of nonstationary processes that contain multiscale <b>features,</b> <b>detection</b> of singularities, analysis of transient phenomena, fractal and multifractal processes, and signal compression is now being exploited for the study of several processes including space-time precipitation, remotely CONTENT...|$|R
40|$|This thesis {{focuses on}} {{automatic}} image labelling to semantic categories. It describes {{the theory of}} classif cation and local <b>features</b> <b>detection.</b> It explains fundamental machine learning models used for image tagging, and how such models can be learned with Gradient descent. It propose solution with hierarchy for ImageNet and tagging images with attributes. MapReduce computing model is considered for learning on big data sets. In the last part it is described implementation, experimental and test results...|$|R
