83|1397|Public
25|$|Orthogonal array {{testing is}} a black box testing {{technique}} which is a systematic, statistical way of software testing. It is used {{when the number of}} inputs to the system is relatively small, but too large to allow for exhaustive testing of every possible input to the systems. It is particularly effective in finding errors associated with faulty logic within computer software systems. Orthogonal arrays can be applied in user <b>interface</b> <b>testing,</b> system testing, regression testing and performance testing.|$|E
2500|$|Chrome is {{internally}} {{tested with}} unit testing, [...] "automated user <b>interface</b> <b>testing</b> of scripted user actions", fuzz testing, {{as well as}} WebKit's layout tests (99% of which Chrome is claimed to have passed), and against commonly accessed websites inside the Google index within 20–30 minutes.|$|E
2500|$|On 16 October 2014, the Navy {{announced}} that Coronado conducted dynamic <b>interface</b> <b>testing</b> with the MQ-8B Fire Scout unmanned helicopter. [...] The tests familiarized the crew with operating the unmanned aircraft, verified and expanded launch and recovery envelopes, and identified opportunities for envelope expansion to demonstrate future concepts {{of operations for}} the aircraft aboard an LCS, which will use the Fire Scout in all three mission packages. Final Contract Trials (FCT) for the ship were completed in June 2014, and Coronado is scheduled to begin Post Shakedown Availability in October 2014.|$|E
40|$|STS- 121 /Discovery Flight Crew; Steve Lindsey(Commander), Mark Kelley(pilot), Mike Fossum, Lisa Nowak, Stephanie Nowak, Pierce Sellers, and Thomas Ryder {{performed}} the following activities: 1) Crew equipment <b>interface</b> <b>test</b> at SSPF; 2) Crew equipment <b>interface</b> <b>test</b> at Kennedy Space Center; and Payload Crew equipment <b>interface</b> <b>test</b> in SSPF...|$|R
5000|$|... #Caption: John D. Olivas {{checking}} equipment during crew equipment <b>interface</b> <b>test</b> for STS-117.|$|R
5000|$|Supports both {{native and}} virtual (tunnel) <b>interfaces</b> (<b>tested</b> IPv6-IPv4, IPv6-IPv6 and TUN/TAP tunnels) ...|$|R
5000|$|Automated {{functional}} GUI(Graphical User <b>Interface)</b> <b>testing</b> {{tools are}} used to repeat system-level tests through the GUI ...|$|E
5000|$|The {{practice}} of component <b>interface</b> <b>testing</b> {{can be used}} to check the handling of data passed between various units, or subsystem components, beyond full integration testing between those units. The data being passed can be considered as [...] "message packets" [...] and the range or data types can be checked, for data generated from one unit, and tested for validity before being passed into another unit. One option for <b>interface</b> <b>testing</b> is to keep a separate log file of data items being passed, often with a timestamp logged to allow analysis of thousands of cases of data passed between units for days or weeks. Tests can include checking the handling of some extreme data values while other interface variables are passed as normal values. Unusual data values in an interface can help explain unexpected performance in the next unit. Component <b>interface</b> <b>testing</b> is a variation of black-box testing, with the focus on the data values beyond just the related actions of a subsystem component.|$|E
50|$|In {{software}} engineering, graphical user <b>interface</b> <b>testing</b> is {{the process}} of testing a product's graphical user interface to ensure it meets its specifications. This is normally done {{through the use of a}} variety of test cases.|$|E
40|$|The main {{objectives}} {{of this study}} are to explore the possibility of simulating sand-steel interface behaviour using the Discrete Element Method, to investigate numerically the movements of particles in sand-steel <b>interface</b> <b>tests,</b> and to simulate experimentally the liquefaction phenomenon. In the first part, sand-steel <b>interface</b> <b>tests</b> were conducted using an automated 3 -D apparatus C 3 DSSI (Cyclic 3 -Dimensional Simple Shear <b>Testing</b> of <b>Interfaces).</b> A dense sand-rough steel plate <b>interface</b> <b>test</b> was carried out under constant normal stress conditions and the results were used in the discrete element simulations. The tests on the loose sand-smooth steel plate interface samples were carried out to investigate the effects of the initial state of the sample (i. e., relative density and normal stress) and the boundary conditions on the sand-steel interface behaviour. In the second part, an attempt was made to explore the advantages and limitations of using the discrete element method to simulate the behaviour of an interface between a granular soil and a steel plate. A two-dimensional discrete element program PFC 2 D was used in the simulations. (Abstract shortened by UMI. ...|$|R
40|$|An {{apparatus}} in {{the direct}} shear mode {{has been developed}} to conduct soil-soil and soil-solid material <b>interface</b> <b>tests</b> in the undrained condition. Evaluation of the apparatus showed that all the requirements for simulating the undrained condition of shear are satisfied. The <b>interface</b> <b>test</b> {{results show that the}} adhesion factor a increases with the surface roughness of the solid material. In the case of the normally consolidated state, alpha is practically independent of the undrained shear strength of the clay for a given surface. For the overconsolidated state, alpha depends on the undrained shear strength and the overconsolidation ratio for smooth surfaces but for rough surfaces; alpha is independent of both undrained shear strength and overconsolidation ratio...|$|R
25|$|On 13 June 2014, the Montford Point {{successfully}} completed LCAC <b>interface</b> <b>tests</b> {{off the coast}} of Camp Pendleton. LCACs moved Amphibious Assault Vehicles from the base and offloaded them onto the ship, demonstrating the ESD's ability to facilitate at-sea transfers to serve as a mobile seabase.|$|R
50|$|In 2010 RTTS {{began work}} on an {{enterprise}} test tool for data testing. QuerySurge was released in early 2012. QuerySurge automates big data testing, data warehouse and ETL testing, data <b>interface</b> <b>testing,</b> data migration testing and database upgrade testing.|$|E
5000|$|Graphical user <b>interface</b> <b>testing.</b> A testing {{framework}} that generates user interface {{events such as}} keystrokes and mouse clicks, and observes the changes that result in the user interface, to validate that the observable behavior {{of the program is}} correct.|$|E
50|$|Processes.Any {{phase of}} {{software}} development can be crowdsourced, and that phase can be requirements (functional, user interface, performance), design (algorithm, architecture), coding (modules and components), testing (including security testing, user <b>interface</b> <b>testing,</b> user experience testing), maintenance, user experience, or {{any combination of}} these.|$|E
50|$|The company sells {{proprietary}} {{solutions and}} breakthrough technology in power management, protection, advanced communications, human <b>interface,</b> <b>test</b> & measurement, {{as well as}} wireless and sensing products. The Company's integrated circuits (ICs) are employed in communications, computer and computer-peripheral, automated test equipment, industrial and other commercial applications.|$|R
5000|$|MIN {{provides}} {{three different}} <b>interfaces</b> for <b>test</b> case execution: ...|$|R
50|$|As of 2008, {{mainframe}} structure {{had been}} realised. Preliminary design review of all mainframe systems completed. OCM camera test and evaluation had been completed. Scatterometer qualification model hardware had been realized. ROSA interfaces with spacecraft systems had been finalised. Qualification model of scatterometer scan mechanism and the <b>interface</b> <b>test</b> with electronics {{had also been}} completed.|$|R
5000|$|Chrome is {{internally}} {{tested with}} unit testing, [...] "automated user <b>interface</b> <b>testing</b> of scripted user actions", fuzz testing, {{as well as}} WebKit's layout tests (99% of which Chrome is claimed to have passed), and against commonly accessed websites inside the Google index within 20-30 minutes.|$|E
5000|$|This is {{the stage}} where unit testing, <b>interface</b> <b>testing</b> is performed. Quality {{assurance}} team {{make sure that the}} new code will not have any impact on the existing functionality and they test major functionalities of the system once after deploying the new code in their respective environment(i.e. QA environment) ...|$|E
5000|$|End User Experience: We plan {{to conduct}} formal User <b>Interface</b> <b>testing.</b> This {{involves}} defining {{a set of}} tasks and use cases, asking users with various levels of JAUS experience to accomplish those tasks, and measuring performance and collecting feedback, to look for areas where the overall user experience can be improved.|$|E
40|$|Abstract. This paper {{introduces}} {{an automated}} heuristic process {{able to achieve}} high accuracy when matching graphical user interface widgets across multiple versions of a target application. The proposed implemen-tation is flexible as it allows full customization {{of the process and}} easy integration with existing tools for long term graphical user <b>interface</b> <b>test</b> case maintenance, software visualization and analysis. 1...|$|R
40|$|The {{test results}} for the onboard {{navigation}} (ONAV) Ground Based Expert System Trainer System for an aircraft/space shuttle navigation entry phase system are described. A summary of the test methods and analysis results are included. Functional inspection and execution, <b>interface</b> <b>tests,</b> default data sources, function call returns, status light indicators, and user interface command acceptance are covered...|$|R
40|$|This {{document}} {{describes the}} results of the formal qualification test (FQT) / Demonstration conducted on September 10, and 14, 1998 for the EOS AMSU-A 2 instrument. The purpose of the report is to relate {{the results of}} the functional performance and <b>interface</b> <b>tests</b> of the software. This is the final submittal of the EOS/AMSU-A Software Test report...|$|R
50|$|There are {{generally}} four recognized levels of tests: unit testing, integration testing, component <b>interface</b> <b>testing,</b> and system testing. Tests are frequently grouped by {{where they are}} added in the software development process, or by the level of specificity of the test. The main levels during the development process {{as defined by the}} SWEBOK guide are unit-, integration-, and system testing that are distinguished by the test target without implying a specific process model. Other test levels are classified by the testing objective.|$|E
5000|$|A {{heuristic}} {{evaluation is}} a usability inspection method for computer software {{that helps to}} identify usability problems in the user interface (UI) design. It specifically involves evaluators examining the interface and judging its compliance with recognized usability principles (the [...] "heuristics"). These evaluation methods are now widely taught and practiced in the new media sector, where UIs are often designed in a short space of time on a budget that may restrict {{the amount of money}} available to provide for other types of <b>interface</b> <b>testing.</b>|$|E
50|$|On 16 October 2014, the Navy {{announced}} that Coronado conducted dynamic <b>interface</b> <b>testing</b> with the MQ-8B Fire Scout unmanned helicopter. The tests familiarized the crew with operating the unmanned aircraft, verified and expanded launch and recovery envelopes, and identified opportunities for envelope expansion to demonstrate future concepts {{of operations for}} the aircraft aboard an LCS, which will use the Fire Scout in all three mission packages. Final Contract Trials (FCT) for the ship were completed in June 2014, and Coronado is scheduled to begin Post Shakedown Availability in October 2014.|$|E
40|$|This {{essay is}} based on a {{previous}} “regression” testing knowledge, black box testing and user graphic <b>interfaces</b> <b>testing.</b> Besides, {{due to the fact that}} the testing life cycle is inside the software life cycle, the suggested focus promotes existing test cases rework, as a result of traceability of the use cases to the distributed system, the unique tests automation, integration and “regression”...|$|R
40|$|This book {{constitutes}} the refereed {{proceedings of the}} 7 th International Conference on Haptic and Audio Interaction Design, HAID 2012, held in Lund, Sweden, in August 2012. The 15 full papers presented were carefully reviewed and selected from numerous submissions. The papers are organized in topical sections on haptics and audio in navigation, supporting experiences and activities, object and <b>interface,</b> <b>test</b> and evaluation. NanoSi...|$|R
50|$|In August 2007, STS-123 {{crew members}} participated in crew {{equipment}} <b>interface</b> <b>tests</b> for the ELM-PS at Kennedy Space Center. Processing continued on schedule for Endeavours launch in early 2008. NASA engineers applied the same ECO sensor modifications used on STS-122s external tank, to Endeavours tank. In January, a HEPA filter contamination issue was discovered, but was resolved {{and with no}} impact to the mission.|$|R
50|$|Radio channel emulators {{or radio}} channel {{simulators}} (also called fading simulators) are tools for air <b>interface</b> <b>testing</b> in wireless communication. In a test environment, radio channel emulators replace the real-world radio channel between a radio transmitter and a receiver {{by providing a}} faded representation of a transmitted signal to the receiver inputs. As technology moves forward {{to take advantage of}} more complex channel characteristics such as MIMO, the channel modeling needed to accurately emulate the radio environment becomes even more critical to a test setup. Radio channel emulators enable creation of mathematical models representing the physical radio signal transmission medium.|$|E
50|$|WorldBench uses popular desktop {{applications}} to perform realistic tasks, {{making it easier}} to accurately gauge how fast a computer runs applications routinely used, rather than just listing abstract measures of individual components. The application-based tests tell exactly what is needed—how fast the computer runs real applications. WorldBench installs special versions of each application tested, enabling the automated test scripts to perform every test. It also ensures that the software configuration and version are identical for every computer evaluated. The setup program installs the user <b>interface,</b> <b>testing</b> scripts, and all documents necessary to complete the tasks. It also unpacks and installs each application as needed during testing.|$|E
50|$|Orthogonal array {{testing is}} a black box testing {{technique}} that is a systematic, statistical way of software testing. It is used {{when the number of}} inputs to the system is relatively small, but too large to allow for exhaustive testing of every possible input to the systems. It is particularly effective in finding errors associated with faulty logic within computer software systems. Orthogonal arrays can be applied in user <b>interface</b> <b>testing,</b> system testing, regression testing, configuration testing and performance testing.The permutations of factor levels comprising a single treatment are so chosen that their responses are uncorrelated and therefore each treatment gives a unique piece of information. The net effects of organizing the experiment in such treatments is that the same piece of information is gathered in the minimum number of experiments.|$|E
5000|$|In the test, comprehensible {{patterns}} are {{used instead of}} XPath expressions. User <b>interface</b> <b>tests</b> should support software development and quality assurance. Furthermore, automated tests have to be stable against changes {{and be able to}} “grow” with an application. It has to be avoided that a change in CSS class means a subsequent adjustment of all tests. The pattern approach was chosen in order to achieve this.|$|R
40|$|The {{objective}} of this research project {{is to develop a}} user interface “test bed” to measure the role of touchscreen kiosks with community connectivity. This <b>interface</b> <b>test</b> bed will apply the knowledge of interface design to the domain of public transit kiosks, with a particular focus on bike share stations. The value of this research is the ability to generate user feedback between community members using urban experiences. Safety, effectiveness and community connectivity of public transit is enhanced with this information interface. This report explains the test bed kiosk’s design, physical build, and hardware and software testing. User interaction studies for standards, accessibility and design criteria were done; 2 D and 3 D modeling, material sourcing and mockups were built. Hardware sensors for proximity included passive infrared (PIR), ultrasonic range finding and RGB-D technology using Microsoft Kinect. Tests measured interaction with user movement type, including the differentiation of pedestrians, cyclists and vehicles. Observed interaction tests were performed with human subjects review approval. The <b>interface</b> <b>test</b> bed will allow for future tests using more advanced sensors and include user intercept surveys. The prototype test bed is a tool to <b>test</b> <b>interface</b> design for connecting urban design theory classifying urban experience information to enhance transit, firstly through the specific application of public bike share...|$|R
30|$|The {{first series}} of tests was {{conducted}} on dry steel–shale <b>interfaces</b> <b>tested</b> at three different normal stresses (2, 5 and 12  MPa). These stress levels were chosen to avoid damage to the rock matrix during the shear tests to obtain reliable results. During this test, the normal stress was kept constant. Figure  3 shows the typical behavior of this interface for the three levels of normal stress (TD-CNL- 12).|$|R
