6|55|Public
40|$|We analyse {{different}} forms of debt mutualisation in a union of countries. One country suffers from a political distortion and may resort to (partial) debt default. We consider a debt repayment guarantee, which can be "unlimited" or "limited", i. e. only be invoked when the <b>guarantee</b> <b>threshold</b> is not exceeded. We also explore the "blue-red" bonds proposal, under which blue debt is guaranteed, while red debt is not guaranteed. Only a suitably chosen limited guarantee induces the government to reduce debt and raises union welfare. This result is upheld under the time-consistent solution when there are costs {{to the rest of}} the union of not providing financial rescue. Making the guarantee also conditional on sufficient structural reform may in addition stimulate reform effort, thereby raising union welfare...|$|E
40|$|We analyse {{different}} forms of international debt mutualisation in a simple framework with a political distortion and (partial) default under adverse economic circumstances. One form is a debt repayment guarantee, which can be "unlimited" or "limited", i. e. only be invoked when the <b>guarantee</b> <b>threshold</b> is not exceeded. We also explore the "blue-red" bonds proposal, under which blue debt is guaranteed by the other countries in a union, while red debt is not guaranteed. Only a suitably chosen limited guarantee induces the government to reduce debt and raises social welfare. Making the guarantee also conditional on sufficient structural reform may stimulate reform effort. However, now a trade-off exists between extracting more reform and inducing the government to limit debt issuance...|$|E
40|$|ABSTRACT We {{consider}} {{the problem of}} user agents selecting processor agents to processor tasks. We assume that processor agents are drawn from two populations: high and low-performing processors with different averages but similar variance in performance. For selecting a processor, a user agent queries other user agents for their high/low rating of different processors. We assume that a known percentage of &quot;liar &quot; users, who give inverse estimates of processors. We develop a trust mechanism that determines the number of users to query given a target <b>guarantee</b> <b>threshold</b> likelihood of choosing high-performance processors {{in the face of}} such &quot;noisy &quot; reputation mechanisms. We evaluate the robustness of this reputation-based trusting mechanism over varying environmental parameters like percentage of liars, performance difference and variances for high and low-performing agents, learning rates, etc. 1...|$|E
40|$|Creation {{in tobacco}} {{production}} {{development and the}} total complex of activities related to it {{must be based on}} standards preferred by the EU and modern world achievements in that field. The preferred standards of future development pay increasing attention to the production itself, covering a variety of aspects, such as quantitative limitation qualitative suitability, environment protection, social security, economic effectiveness, healthy growth and development of plants, fulfillment of requirements of non - government organizations and WHO, etc. Qualitative limitation is expressed through decreasing of <b>guarantee</b> <b>thresholds</b> by tobacco varieties in member states of EU. Starting from 1999, 345. 508 tons of leaf tobacco were successively limited to 334, 064 tons in 2004, which is a reduction of 4 %. <b>Guarantee</b> <b>thresholds</b> are also determined by the EU for its new tobacco producing member states. The aim this paper is to emphasize some more important aspects of development policy of tobacco production in EU and in our country, and in the same time to initiate activities and targets for tobacco production in future...|$|R
3000|$|Unlike {{most of the}} {{previous}} studies on power control [22 - 26] which only guarantee the SINR of CUEs, the proposed power control scheme not only <b>guarantees</b> a <b>threshold</b> SINR of CUEs γ [...]...|$|R
3000|$|This {{restriction}} imposes {{the condition}} that an agent must provide a minimal number of tourist services (considered as a minimal <b>threshold</b> <b>guarantee)</b> in order {{to play a role}} [...] ri [...]. In this experimental threshold is fixed at 10 services. “touristinfo[*]>[*] 10 ”.|$|R
40|$|Index trees created using {{distance}} based indexing {{are difficult}} to maintain online since the distance function involved is often costly to compute. This problem is intensified when the database we are dealing with, is frequently updated, as only limited time is available to perform the maintenance. In this paper, we propose a novel tree maintenance mechanism for the problem of answering approximate k-Nearest Neighbor queries with a probabilistic guarantee on timeseries streams. When the underlying data change, we may choose to defer updating the tree {{as long as the}} probabilistic guarantee of answering queries is high. To prolong such deferment, we present innovative techniques that maintain the utility of the tree by migrating its pivots and by partially reconstructing it. As the probabilistic guarantee decays with time and crosses the minimum <b>guarantee</b> <b>threshold,</b> all of the deferred updates are performed. In essence, our work offers an elegant compromise between the accuracy guarantee of query results and the cost of providing them. With extensive empirical studies, we also show the flexibility and efficiency of our approach. 1...|$|E
40|$|This paper reports an {{investigation}} into the importance of basic skills in literacy and numeracy in the promotion of success on intermediate vocational courses at age 16 +. Two measures of attainment in literacy and numeracy are examined; GCSE passes in English and Mathematics analysed by grade awarded and the Adult Literacy and Basic Skills Unit (ALBSU) tests in communication and numeracy. The study examines the relationship between prior attainment as attested by GCSE grades and ALBSU scores and course outcome. The extent to which a consistent relationship is found between GCSE grades and ALBSU scores is also examined. The study uses a random sample of 142 students drawn from a population of all first year 16 / 7 year old students who enrolled at a London Further Education college in 1994. A sub-sample of students on GNVQ Intermediate and NVQ level 2 courses is examined in greater depth. Data on course outcomes was collected at three points in time, 1995, one year after enrolment and on two occasions in 1996. It was therefore possible to chart the progress of students in the sample who took more than one year to complete an Intermediate (G) NVQ. Initial analysis found that at the GCSE middle grade range (Grades C,D,E,F) there was a wide range of literacy and numeracy outcomes as measured by the ALBSU literacy and numeracy tests. GCSE Maths and English passes at these grades do not appear to <b>guarantee</b> <b>threshold</b> attainment levels in basic numeracy and literacy. No significant relationship is found between prior attainment as measured by GCSE Maths and English grades and course outcomes. The ALBSU test scores proved to be more helpful in predicting student outcomes on the Intermediate GNVQ but were still fairly weak predictors. The high proportion of leaves from the sample, probably influenced by 'pull' factors from the labour market, gives cause for concern. There is no evidence to indicate that weaker students leave Intermediate GNVQ courses early, if anything, the reverse is true. A significant proportion of Intermediate GNVQ courses early, if anything, the reverse is true. A significant proportion of Intermediate GNVQ students gained their awards after the prescribed one year study period. Commitment and motivation to succeed appear to be as - if not more - important than academic qualifications for success on Intermediate GNVQ. The study offers evidence that, for those motivated to persist with the studies, GNVQ can offer a valuable 'bridge' to further and higher education opportunities to students who have performed poorly on 'academic' GCSEs. ...|$|E
40|$|Femto cells a. k. a. Low Power Nodes (LPNs) {{are used}} to improve indoor data rates {{as well as to}} reduce traffic load on macro Base Stations (BSs) in LTE {{cellular}} networks. These LPNs are deployed inside office buildings and residential apartment complexes to provide high data rates to indoor Users. With high SINR (Signal-to-Interference plus Noise Ratio) the users experience good throughput, but the SINR decreases significantly because of interference and obstacles such as building walls, present in the communication path. So, efficient placement of Femtos in buildings while considering Macro-Femto interference is very crucial for attaining desirable SINR. At the same time, minimizing the power leakage in order to improve the signal strength of outdoor users in a high interference (HIZone) around the building area is important. In our work, we have considered obstacles (walls, floors) and interference between Macro and Femto BSs. To be fair to both indoor and outdoor users, we designed an efficient placement and power control SON (Self organizing Network) algorithm which optimally places Femtos and dynamically adjusts the transmission power of Femtos based on the occupancy of Macro users in the HIZone. To do this, we solve two Mixed Integer Programming (MIP) methods namely: Minimize number of Femtos (MinNF) method which <b>guarantees</b> <b>threshold</b> SINR (SINRTh) - 2 dB for all indoor users and optimal Femto power (OptFP) allocation method which guarantees SINRTh (- 4 dB) for indoor users with the Macro users SINR degradation as lesser than 2 dB...|$|R
40|$|Abstract — We {{consider}} {{the problem of}} providing absolute QoS guarantees to multiple classes of users of an OBS network {{in terms of the}} end-to-end burst loss. We employ Markov decision process (MDP) theory to develop wavelength sharing policies that maximize throughput while meeting the QoS <b>guarantees.</b> The randomized <b>threshold</b> policies we obtain are simple to implement and operate, and make effective use of statistical multiplexing. I...|$|R
3000|$|The role [...] ri /i = { 1, 2, 3 } is {{delegated}} from {{an initial}} agent source ‘s’[*]=[*]Bx to another agent By neighbor of Bx if that agent By provides a minimal number of tourist services. This restriction imposes {{the condition that}} an agent must provide a minimal number of tourist services (considered as a minimal <b>threshold</b> <b>guarantee)</b> in order {{to play a role}} ri.|$|R
50|$|A {{sample of}} 1,000 {{independent}} bits {{is enough to}} ensure an absolute error of at most 0.081 on the estimation of the parameter p of the underlying Bernoulli variable with a confidence of at least 0.99. The same size cannot <b>guarantee</b> a <b>threshold</b> less than 0.088 with the same confidence 0.99 when the error is identified with {{the probability that a}} 20-year-old man living in New York does not fit the ranges of height, weight and waistline observed on 1,000 Big Apple inhabitants. The accuracy shortage occurs because both the VC dimension and the detail of the class of parallelepipeds, among which the one observed from the 1,000 inhabitants' ranges falls, are equal to 6.|$|R
40|$|Mapping a pipelined {{application}} onto a distributed {{and parallel}} platform is a challenging problem. The problem {{becomes even more}} difficult when multiple optimization criteria are involved, and when the target resources are heterogeneous (processors and communication links) and subject to failures. This report investigates the problem of mapping pipelined applications, consisting of a linear chain of stages executed in a pipeline way, onto such platforms. The objective is to optimize the reliability under a performance constraint, i. e., while <b>guaranteeing</b> a <b>threshold</b> throughput. In order to increase reliability, we replicate the execution of stages on multiple processors. We present complexity results, proving that this bi-criteria optimization problem is NP-hard. We then propose some heuristics, and present extensive experiments evaluating their performance...|$|R
40|$|Clustering means {{partitioning}} nodes {{into groups}} called clusters, providing the network with a hierarchical organization. A self-stabilizing protocol, {{regardless of the}} initial system state, automatically converges {{to a set of}} states that satisfy the problem specification without external intervention. Due to this property, self-stabilizing protocols are adapted to highly dynamic networks as ad hoc or sensors networks. In this paper, we propose a self-stabilizing clustering protocol. Our protocol <b>guarantees</b> a <b>threshold</b> (SizeBound) on the number of nodes that a clusterhead handle. Therefore, none of the clusterheads are overloaded at any time. The criterion of the clusterheads election is based on their weight value, a general parameter that can be computed according to several node parameters as transmission power, battery power, [...] 1...|$|R
40|$|In {{this paper}} we {{consider}} a variable annuity which provides guarantees at death and maturity financed {{through the application}} of a state-dependent fee structure, as defined first in Bae and Ko (2013) and extensively analysed in Bernard et al. (2014) and MacKay et al. (2017). We propose a quite general valuation model for such guarantees, along the lines of Bacinello et al. (2011). We then analyse numerically the interaction between fee rates, death/maturity <b>guarantees,</b> fee <b>thresholds</b> and surrender penalties under alternative model assumptions and policy holder behaviours. This allows us to get also some interesting insights into the model risk. Since the assumptions adopted in the numerical analysis are not at all trivial, we resort to Monte Carlo and Least Squares Monte Carlo methods of LSMC valuation algorithm...|$|R
40|$|We offer a {{guide to}} {{dimensional}} reduction (DRED) in theories with anomaly mediated supersymmetry breaking. Evanescent operators proportional to epsilon arise in the bare Lagrangian when it is reduced from d= 4 to d= (4 - 2 epsilon) dimensions. In {{the course of a}} detailed diagrammatic calculation, we show that inclusion of these operators is crucial. The evanescent operators conspire to drive the supersymmetry-breaking parameters along anomaly-mediation trajectories across heavy particle <b>thresholds,</b> <b>guaranteeing</b> the ultraviolet insensitivity. Comment: 24 pages. 10 figures. Uses Axodraw. Reference adde...|$|R
40|$|We {{introduce}} a novel algorithm for decoding binary linear codes by linear programming (LP). We {{build on the}} LP decoding algorithm of Feldman and {{introduce a}} postprocessing step that solves a second linear program that reweights the objective function based {{on the outcome of}} the original LP decoder output. Our analysis shows that for some LDPC ensembles we can improve the provable <b>threshold</b> <b>guarantees</b> compared to standard LP decoding. We also show significant empirical performance gains for the reweighted LP decoding algorithm with very small additional computational complexity...|$|R
40|$|A {{critical}} {{reflection on}} the democratic relationship between corporeality, recognition, representation and democratic togetherness, this doctoral thesis follows {{in the tradition of}} practical philosophy (Iris Marion Young, Bonnie Honig, Philip Pettit, James Tully) to explore and deconstruct the contemporary challenges of late modern democracy in the face of demands by indigenous peoples, minority nations and various intra-national (nonterritorial) groups for effective enjoyment of democratic self-government. An underlying assumption of this thesis is that the emancipatory potential of liberal equality as a political ideal, although rhetorically onmipresent within Anglo-American democracies, has been exhausted and in fact redeployed in ways that diminish the capacity of minorized members of the political community to advocate for their liberty and to protect themselves against domination and oppression from within democratic institutions themselves. Cross-pollinating the insights of diversity feminism and republicanism, this thesis proposes alternative epistemological foundations of representative democracy to expose the political nature of various struggles of/over the interlocking conditions of late modern political corporealization. Specifically, it re-constructs a late modern convention of self-representation as co-authority that is grounded in an anti-essentialist and performative conception of political subjectivity (diversity feminism), and an agonic and institutional conception of political liberty and self-government (republican liberty). These alternative foundations (diversity, liberty, and co-authority) lay the nonnative groundwork for democratic institutional reforms aimed at the re-construction of a late modern praxis of radical representative democratic corporealpolitiks that presumes the necessity of heterogeneous institutional outlets of contestation that performatively politicize and democratize asymmetrical power relations between historically minorized/dominant corporealities. Positing the critical interdependence of personal freedom and collective freedom as realized democratically through free institutions and practices, the thesis advances a democratic defense of <b>guaranteed</b> <b>thresholds</b> of institutional self-representation for a broader diversity of late modern struggles of/over corporealization that liberalism has often excluded as apolitical or pre-political. Albeit in nonnative solidarity with inter-national struggles for democratic self-determination, it probes the specificities of intra-national minorities inflected by the corporealpolitiks of sex, gender, race, religion, ethnicity, mother tongue, and disability, given that their "right to exit" the political association is both epistemologically and practically absent...|$|R
40|$|In this paper, {{we propose}} two new support vector {{approaches}} for ordinal regression, which optimize multiple thresholds to define parallel discriminant hyperplanes for the ordinal scales. Both approaches <b>guarantee</b> that the <b>thresholds</b> are properly ordered at the optimal solution. The size of these optimization problems is linear {{in the number}} of training samples. The SMO algorithm is adapted for the resulting optimization problems; it is extremely easy to implement and scales efficiently as a quadratic function of the number of examples. The results of numerical experiments on benchmark datasets verify the usefulness of these approaches. 1...|$|R
40|$|AbstractGiven a {{connected}} graph G, and a distribution of t pebbles to the vertices of G, a pebbling step consists of removing two pebbles from a vertex v and placing one pebble on {{a neighbor of}} v. For a particular vertex r, the distribution is r-solvable if {{it is possible to}} place a pebble on r after a finite number of pebbling steps. The distribution is solvable if it is r-solvable for every r. The pebbling number of G is the least number t, so that every distribution of t pebbles is solvable. In this paper we are not concerned with such an absolute guarantee but rather an almost sure <b>guarantee.</b> A <b>threshold</b> function for a sequence of graphs G=(G 1,G 2,…,Gn,…), where Gn has n vertices, is any function t 0 (n) such that almost all distributions of t pebbles are solvable when t⪢t 0, and such that almost none are solvable when t⪡t 0. We give bounds on pebbling threshold functions for the sequences of cliques, stars, wheels, cubes, cycles and paths...|$|R
40|$|This paper proposes Dynamic Thermal Management (DTM) {{based on}} a dynamic voltage and {{frequency}} scaling (DVFS) technique for MPEG- 4 decoding to guarantee thermal safety while maintaining a quality of service (QoS) constraint. Although many low-power and low-temperature multimedia playback techniques have been proposed, {{most of them are}} impractical in real-time and have several restricting assumptions. Multimedia data consists of several frames requiring different decoding efforts. Since both temperature and performance of a multimedia system are affected by the complexity of scenes, our main idea is to use the information on scene complexity to find an appropriate frequency. In order to predict the complexity of the current scene, we extract information from the previous group of pictures (GOP) using feedback control with a display buffer. Experimental results with twelve movies show that our DTM scheme <b>guarantees</b> the <b>threshold</b> of temperature (70 ◦ C) while maintaining 0 % frame miss ratio. Also, our DTM scheme decreases the average temperature by up to 13 % without any additional hardware and playback latency. ...|$|R
40|$|Given a {{connected}} graph G, and a distribution of t pebbles to the vertices of G, a pebbling step consists of removing 2 pebbles from a vertex v and placing one pebble on {{a neighbor of}} v. For a particular vertex r, the distribution is r-solvable if {{it is possible to}} place a pebble on r after a finite number of pebbling steps. The distribution is solvable if it is r-solvable for every r. The pebbling number of G is the least number t so that every distribution of t pebbles is solvable. In this paper we are not concerned with such an absolute guarantee but rather an almost sure <b>guarantee.</b> A <b>threshold</b> function for a sequence of graphs G = fG 1; G 2; : : :; G n; : : :g, where G n has n vertices, is any function t 0 (n) such that almost all distributions of t pebbles are solvable when t t 0, and such that almost none are solvable when t t 0. We give bounds on pebbling threshold functions for the sequences of cliques, stars, wheels, cubes, cycles and paths...|$|R
30|$|In this paper, we {{have studied}} robust RA {{problem in the}} {{underlay}} OFDM-based cognitive relay network with joint channel uncertainty and interference uncertainty. Based on worst-case approach and Lagrange dual decomposition method, closed form analytical solutions to robust relay selection and power allocation have been derived. Numerical results demonstrate the robustness of the proposed robust relay selection scheme. The robust algorithm is superior to the non-robust algorithms in terms of <b>guaranteeing</b> the interference <b>thresholds</b> of different PU-RXs, but {{the capacity of the}} robust algorithm is little lower than that of non-robust algorithm for overcoming the uncertainties and also decreases with the increase of the uncertainties. In our future works, we will extend this frame work to two-way cognitive radio networks with multiple SUs or multiple antennas.|$|R
40|$|International audienceThis paper {{addresses}} {{the issue of}} wireless sensor network (WSN) deployment. We investigate this problem in the case where the monitored area {{is characterized by a}} geographical irregularity of the sensed events. Precisely, we consider that each point of the deployment area requires a minimum <b>threshold</b> <b>guarantee</b> on the event detection probability. Our proposed scalable deployment method, named potential field-based deployment algorithm (PFDA), is based on the potential field and the virtual force approaches. Our proposal is able to (1) satisfy the required event detection probability threshold for each point, in a large-scale area, while minimizing the number of deployed sensors and (2) to ensure the network connectivity. The results and evaluation analysis show that PFDA outperforms the other strategies proposed in literatur...|$|R
30|$|A robust {{resource}} allocation (RA) algorithm for cognitive relay networks with multiple primary users considering joint channel uncertainty and interference uncertainty is proposed {{to maximize the}} capacity of the networks subject to the interference threshold limitations of primary users’ receivers (PU-RXs) and the total power constraint of secondary user’s transmitter and relays. Ellipsoid set and interval set are adopted to describe the uncertainty parameters. The robust relay selection and power allocation problems are separately formulated as semi-infinite programming (SIP) problems. With the worst-case approach, the SIP problems are transformed into equivalent convex optimization problems and solved by Lagrange dual decomposition method. Numerical results show the impact of channel uncertainties and validation of the proposed robust algorithm for strict <b>guarantee</b> the interference <b>threshold</b> requirements at different PU-RXs.|$|R
40|$|The {{problem of}} {{estimating}} {{a signal that}} is corrupted by additive noise has been of interest to many researchers for practical as well as theoretical reasons. Many of the traditional denoising methods have been using methods such as the Wiener filtering. Recently, nonlinear methods, especially those based on wavelets have become increasingly popular, due {{to a number of}} advantages over the linear methods. It has been shown that wavelet and multiwavelet <b>thresholding</b> <b>guarantees</b> better rate of convergence, despite its simplicity. This paper demonstrates the work of combining Parametric multiwavelet and Sureshrink to remove noise from the signal. Experimental results shows that the proposed work is 4 % efficient in terms of SNR values and image quality when compared to other wavelet familie...|$|R
40|$|Abstract. A {{new type}} of {{tachometer}} carbon brush wear detection technology is proposed based on data acquisition and signal processing technology, the key {{of which is that}} the tachometer carbon brush output information is acquired and processed, moreover, the real time conditions of the carbon brush wear are detected and analyzed based on the time domain Eigen value analysis method. During the data processing process, the results of the dimensional parameter processing method are compared with the results of the dimensionless parameter processing method at the same time. Then, we find that the detecting precision is greatly promoted and measurement probability is <b>guaranteed</b> by reasonable <b>threshold</b> selection. Experimental results indicate that the technology proposed here has high stability, high reliability, and high detecting probability, which has great practical value and deserves being extended widely...|$|R
40|$|Design of {{safe and}} survivable {{structures}} requires {{the availability of}} <b>guaranteed</b> minimum strength <b>thresholds</b> for structural materials to enable a meaningful comparison of strength requirement and available strength. This paper develops a procedure for determining such a threshold with a desired degree of confidence, for structural materials with none or minimal industrial experience. The problem arose in attempting to use a new, highly weight-efficient structural load tendon material to achieve a lightweight super-pressure balloon. The developed procedure applies to lineal (one dimensional) structural elements. One {{important aspect of the}} formulation is that it extrapolates to expected probability distributions for long length specimen samples from some hypothesized probability distribution that has been obtained from a shorter length specimen sample. The use of the developed procedure is illustrated using both real and simulated data...|$|R
40|$|To detect {{changes in}} gene {{expression}} data from microarrays, a fixed threshold for fold difference is used widely. However, {{it is not}} always <b>guaranteed</b> that a <b>threshold</b> value which is appropriate for highly expressed genes is suitable for lowly expressed genes. In this study, aiming at detecting truly differentially expressed genes from a wide expression range, we proposed an adaptive threshold method (AT). The adaptive thresholds, which have different values for different expression levels, are calculated based on two measurements under the same condition. The sensitivity, specificity and false discovery rate (FDR) of AT were investigated by simulations. The sensitivity and specificity under various noise conditions were greater than 89. 7 % and 99. 32 %, respectively. The FDR was smaller than 0. 27. These results demonstrated the reliability of the method...|$|R
30|$|To {{mitigate}} {{the implication of}} channel uncertainty, some power control methods model channel gain as a combination of deterministic and uncertain components[10]. In general, there are two approaches to model channel uncertainty, i.e., stochastic approach that assumes statistical knowledge of uncertainty (e.g., a given stochastic distribution) and formulates the problem in a probabilistic manner, and worst-case robust optimization approach where fluctuations of the uncertain component are restricted to be within a bounded and convex set[11]. Due to the stochastic variations in channel gains, the worst-cast approach is more appealing to <b>guarantee</b> MUEs’ interference <b>threshold</b> when CSI errors are bounded[12]. Plus the tractability of the worst-case formulation owing to ellipsoid model’s neat form, we employ worst-case robust optimization theory to deal with channel uncertainty. More information about the two models can be found in[13] for further study.|$|R
40|$|Abstract — We {{consider}} the general problem of optimizing {{the performance of}} OBS networks with multiple traffic classes subject to strict (absolute) QoS constraints {{in terms of the}} end-toend burst loss rate of each guaranteed class of traffic. We employ Markov decision process (MDP) theory to obtain optimal wavelength sharing policies for two performance objectives, namely, maximization of weighted network throughput and minimization of the loss rate of best-effort traffic, while meeting the QoS <b>guarantees.</b> The randomized <b>threshold</b> policies we obtain are simple to implement and operate, and make effective use of statistical multiplexing. In particular, the threshold randomization feature enables the policies to allocate bandwidth at arbitrarily fine sub-wavelength granularity, hence making effective use of the available network capacity. Index Terms — Optical burst switching (OBS) networks, wavelength reservations, quality of service, Markov decision process, randomized threshold policies. I...|$|R
40|$|Abstract—This paper {{presents}} results {{pertaining to}} sequential methods for support recovery of sparse signals in noise. Specifically, {{we show that}} any sequential measurement procedure fails provided {{the average number of}} measurements per dimension grows slower then D(f 0 ||f 1) − 1 logs where s is the level of sparsity, and D(f 0 ||f 1) the Kullback-Leibler divergence between the underlying distributions. For comparison, we show any nonsequential procedure fails provided the number of measurements grows at a rate less thanD(f 1 ||f 0) − 1 logn, wherenis the total dimension of the problem. Lastly, we show that a simple procedure termed sequential <b>thresholding</b> <b>guarantees</b> exact support recovery provided the average number of measurements per dimension grows faster than D(f 0 ||f 1) − 1 (logs+loglogn), a mere additive factor more than the lower bound. I...|$|R
40|$|This study {{proposes a}} new {{bandwidth}} reservation strategy (Two Level Guarantee) in wireless environment {{based on the}} user mobility specification which {{is assumed to be}} given at call set up time. This approach is very useful in that 1) it doesn't use prediction method which might require rather heavy computation, 2) it reserves wireless bandwidth as much as the user requests, 3) it is flexible to control the new call blocking rates and handoff dropping rates with the number of EG(Exclusive Guarantee) s and PG(Pseudo <b>Guarantee)</b> s and <b>threshold</b> values. Simulation results based on random walk model, with dynamic traffic and operating conditions show that a TLG system can admit new calls roughly three times larger than the APR (All Path Reservation) system, and can have roughly 2. 5 times larger naturally terminated calls than the APR system...|$|R
40|$|The paper {{deals with}} the initial-value problem for the {{degenerate}} reaction-diffusion-convection equationu_t + h(u) u_x = (u^m) _xx + f(u), x Є R, t> 0, m> 1,with f, h continuous and f of Fisher-type. By means of comparison type techniques, we prove that the equilibrium u ≡ 1 is an attractor for all solutions with a continuous, bounded, non-negative initial condition u_ 0 (x) = u(x, 0) ≠ 0. Whenu_ 0 is also compactly supported and satisfies 0 ≤ u 0 ≤ 1, the convergence is such that an asymptotic estimate of the interface can be obtained. The employed techniques involve the theory of travelling-wave solutions that we improve in thiscontext. The assumptions on f and h <b>guarantee</b> that the <b>threshold</b> speed wavefront is not stationary and we show that the asymptotic speed of the interface equals this minimal speed...|$|R
40|$|This paper {{presents}} results {{pertaining to}} sequential methods for support recovery of sparse signals in noise. Specifically, {{we show that}} any sequential measurement procedure fails provided {{the average number of}} measurements per dimension grows slower then log s / D(f 0 ||f 1) where s is the level of sparsity, and D(f 0 ||f 1) the Kullback-Leibler divergence between the underlying distributions. For comparison, we show any non-sequential procedure fails provided the number of measurements grows at a rate less than log n / D(f 1 ||f 0), where n is the total dimension of the problem. Lastly, we show that a simple procedure termed sequential <b>thresholding</b> <b>guarantees</b> exact support recovery provided the average number of measurements per dimension grows faster than (log s + log log n) / D(f 0 ||f 1), a mere additive factor more than the lower bound. Comment: Asilomar 201...|$|R
40|$|Abstract—In {{downlink}} beamforming in a multiple-input multiple-output (MIMO) {{wireless communication}} system, we design beamformers that minimize the power subject to guaranteeing given signal-to-interference noise ratio (SINR) threshold levels for the users, {{assuming that the}} channel responses {{between the base station}} and the users are known exactly. In robust downlink beamforming, we take into account uncertainties in the channel vectors, by designing beamformers that minimize the power subject to <b>guaranteeing</b> given SINR <b>threshold</b> levels over the given set of possible channel vectors. When the uncertainties in channel vectors are described by complex uncertainty ellipsoids, we show that the associated worst-case robust beamforming problem can be solved efficiently using an iterative method. The method uses an alternating sequence of optimization and worstcase analysis steps, where at each step we solve a convex optimization problem using efficient interior-point methods. Typically, the method provides a fairly robust beamformer design within 5 – 10 iterations. The robust downlink beamforming method is demonstrated with a numerical example. I...|$|R
