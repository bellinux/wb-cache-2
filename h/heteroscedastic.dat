1678|3|Public
5|$|The {{nature of}} Earth's {{magnetic}} field {{is one of}} <b>heteroscedastic</b> fluctuation. An instantaneous measurement of it, or several measurements of it across the span of decades or centuries, are not sufficient to extrapolate an overall trend in the field strength. It has gone {{up and down in}} the past for unknown reasons. Also, noting the local intensity of the dipole field (or its fluctuation) is insufficient to characterize Earth's magnetic field as a whole, as it is not strictly a dipole field. The dipole component of Earth's field can diminish even while the total magnetic field remains the same or increases.|$|E
2500|$|Various {{models have}} been created that allow for heteroscedasticity, i.e. the errors for {{different}} response variables may have different variances. [...] For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially <b>heteroscedastic</b> errors.|$|E
2500|$|In a {{stationary}} Gaussian time series model, the likelihood function is (as usual in Gaussian models) {{a function of}} the associated mean and covariance parameters. With a large number (...) of observations, the (...) covariance matrix may become very large, making computations very costly in practice. However, due to stationarity, the covariance matrix has a rather simple structure, and by using an approximation, computations may be simplified considerably (from [...] to [...] ). The idea effectively boils down to assuming a <b>heteroscedastic</b> zero-mean Gaussian model in Fourier domain; the model formulation is based on the time series' discrete Fourier transform and its power spectral density.|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are <b>heteroscedastic)</b> if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
5000|$|... #Caption: Absolute {{value of}} {{residuals}} for simulated first order <b>heteroscedastic</b> data.|$|E
5000|$|Models {{that are}} {{analyzed}} using the LPM approach will have <b>heteroscedastic</b> disturbances.|$|E
50|$|NLOGIT {{is a full}} {{information}} {{maximum likelihood}} estimator {{for a variety of}} multinomial choice models. NLOGIT includes the discrete estimators in LIMDEP plus model extensions for multinomial logit (many specifications), random parameters mixed logit, random regret logit, WTP space specifications in mixed logit, scaled multinomial logit, nested logit, multinomial probit, <b>heteroscedastic</b> extreme value, error components, <b>heteroscedastic</b> logit and latent class models.|$|E
5000|$|In {{regression}} and time-series modelling, basic {{forms of}} models {{make use of}} the assumption that the errors or disturbances ui have the same variance across all observation points. When this is not the case, the errors are said to be <b>heteroscedastic,</b> or to have heteroscedasticity, and this behaviour will be reflected in the residuals [...] estimated from a fitted model. Heteroscedasticity-consistent standard errors are used to allow the fitting of a model that does contain <b>heteroscedastic</b> residuals. The first such approach was proposed by Huber (1967), and further improved procedures have been produced since for cross-sectional data, time-series data and [...] GARCH estimation.|$|E
50|$|In econometrics, the Park test is a {{test for}} heteroscedasticity. The test {{is based on the}} method {{proposed}} by Rolla Edward Park for estimating linear regression parameters in the presence of <b>heteroscedastic</b> error terms.|$|E
5000|$|Gaussian {{processes}} (also {{known as}} kriging), where the {{any combination of}} output points {{is assumed to be}} distributed as a multivariate Gaussian distribution. Recently, [...] "treed" [...] Gaussian processes have been used to deal with <b>heteroscedastic</b> and discontinuous responses.|$|E
5000|$|The {{assumption}} of homoscedasticity simplifies mathematical and computational treatment. Serious violations in homoscedasticity (assuming a distribution of data is homoscedastic when {{in reality it}} is <b>heteroscedastic</b> [...] ) may result in overestimating the goodness of fit {{as measured by the}} Pearson coefficient.|$|E
50|$|In statistics, the Glejser {{test for}} heteroscedasticity, {{developed}} by Herbert Glejser, regresses the residuals on the explanatory variable that {{is thought to}} be related to the <b>heteroscedastic</b> variance. After it was found not to be asymptotically valid under asymmetric disturbances, similar improvements have been independently suggested by Im, and Machado and Santos Silva.|$|E
5000|$|In statistics, a {{collection}} of random variables is <b>heteroscedastic</b> (or heteroskedastic; from Ancient Greek [...] “different” and [...] “dispersion”) if there are sub-populations that have different variabilities from others. Here [...] "variability" [...] could be quantified by the variance or any other measure of statistical dispersion. Thus heteroscedasticity {{is the absence of}} homoscedasticity.|$|E
50|$|Various {{models have}} been created that allow for heteroscedasticity, i.e. the errors for {{different}} response variables may have different variances. For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially <b>heteroscedastic</b> errors.|$|E
50|$|One of the {{assumptions}} of the classical linear regression model {{is that there is}} no heteroscedasticity. Breaking this assumption means that the Gauss-Markov theorem does not apply, meaning that OLS estimators are not the Best Linear Unbiased Estimators (BLUE) and their variance is not the lowest of all other unbiased estimators.Heteroscedasticity does not cause ordinary least squares coefficient estimates to be biased, although it can cause ordinary least squares estimates of the variance (and, thus, standard errors) of the coefficients to be biased, possibly above or below the true or population variance. Thus, regression analysis using <b>heteroscedastic</b> data will still provide an unbiased estimate for the relationship between the predictor variable and the outcome, but standard errors and therefore inferences obtained from data analysis are suspect. Biased standard errors lead to biased inference, so results of hypothesis tests are possibly wrong. For example, if OLS is performed on a <b>heteroscedastic</b> data set, yielding biased standard error estimation, a researcher might fail to reject a null hypothesis at a given significance level, when that null hypothesis was actually uncharacteristic of the actual population (making a type II error).|$|E
50|$|Described {{above are}} the core {{elements}} of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations {{of a number of}} standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use <b>heteroscedastic</b> linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by <b>heteroscedastic</b> linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).|$|E
50|$|The {{nature of}} Earth's {{magnetic}} field {{is one of}} <b>heteroscedastic</b> fluctuation. An instantaneous measurement of it, or several measurements of it across the span of decades or centuries, are not sufficient to extrapolate an overall trend in the field strength. It has gone {{up and down in}} the past for unknown reasons. Also, noting the local intensity of the dipole field (or its fluctuation) is insufficient to characterize Earth's magnetic field as a whole, as it is not strictly a dipole field. The dipole component of Earth's field can diminish even while the total magnetic field remains the same or increases.|$|E
5000|$|In a {{stationary}} Gaussian time series model, the likelihood function is (as usual in Gaussian models) {{a function of}} the associated mean and covariance parameters. With a large number (...) of observations, the (...) covariance matrix may become very large, making computations very costly in practice. However, due to stationarity, the covariance matrix has a rather simple structure, and by using an approximation, computations may be simplified considerably (from [...] to [...] ). The idea effectively boils down to assuming a <b>heteroscedastic</b> zero-mean Gaussian model in Fourier domain; the model formulation is based on the time series' discrete Fourier transform and its power spectral density.|$|E
50|$|A word {{of caution}} is in order when {{interpreting}} pseudo-R2 statistics. The reason these indices of fit {{are referred to as}} pseudo R2 is that they do not represent the proportionate reduction in error as the R2 in linear regression does. Linear regression assumes homoscedasticity, that the error variance is the same for all values of the criterion. Logistic regression will always be <b>heteroscedastic</b> - the error variances differ for each value of the predicted score. For each value of the predicted score there would be a different value of the proportionate reduction in error. Therefore, it is inappropriate to think of R2 as a proportionate reduction in error in a universal sense in logistic regression.|$|E
5000|$|When {{viewed in}} the {{frequency}} domain, {{it is evident that}} the matched filter applies the greatest weighting to spectral components exhibiting the greatest signal-to-noise ratio (i.e., large weight where noise is relatively low, and vice versa). In general this requires a non-flat frequency response, but the associated [...] "distortion" [...] is no cause for concern in situations such as radar and digital communications, where the original waveform is known and the objective is the detection of this signal against the background noise. On the technical side, the matched filter is a weighted least squares method based on the (<b>heteroscedastic)</b> frequency-domain data (where the [...] "weights" [...] are determined via the noise spectrum, see also previous section), or equivalently, a least squares method applied to the whitened data.|$|E
50|$|One {{instance}} in which robust estimation should be considered is {{when there is a}} strong suspicion of heteroscedasticity. In the homoscedastic model, it is assumed that the variance of the error term is constant for all values of x. Heteroscedasticity allows the variance to be dependent on x, which is more accurate for many real scenarios. For example, the variance of expenditure is often larger for individuals with higher income than for individuals with lower incomes. Software packages usually default to a homoscedastic model, even though such a model may be less accurate than a <b>heteroscedastic</b> model. One simple approach (Tofallis, 2008) is to apply least squares to percentage errors as this reduces the influence of the larger values of the dependent variable compared to ordinary least squares.|$|E
5000|$|Suppose {{there is}} a {{sequence}} of random variables [...] and a sequence of vectors of random variables, [...] In dealing with conditional expectations of Yt given Xt, the sequence {Yt}t=1n {{is said to be}} <b>heteroscedastic</b> if the conditional variance of Yt given Xt, changes with t. Some authors refer to this as conditional heteroscedasticity to emphasize {{the fact that it is}} the sequence of conditional variances that changes and not the unconditional variance. In fact, it is possible to observe conditional heteroscedasticity even when dealing with a sequence of unconditional homoscedastic random variables; however, the opposite does not hold. If the variance changes only because of changes in value of X and not because of a dependence on the index t, the changing variance might be described using a scedastic function.|$|E
5000|$|Constant {{variance}} (a.k.a. homoscedasticity). This {{means that}} different response variables {{have the same}} variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are <b>heteroscedastic)</b> if the response variables can vary over a wide scale. In order to determine for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predicting outcome. Error will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
40|$|Abstract. In this paper, we {{introduce}} {{the extension of}} local polynomial fitting to the linear <b>heteroscedastic</b> regression model and its applications in digital image <b>heteroscedastic</b> noise removal. For better image noise removal with <b>heteroscedastic</b> energy, firstly, the local polynomial regression is applied to estimate <b>heteroscedastic</b> function, then the coefficients of regression model are obtained by using generalized least squares method. Due to non-parametric technique of local polynomial estimation, {{we do not need}} to know the <b>heteroscedastic</b> noise function. Therefore, we improve the estimation precision, when the <b>heteroscedastic</b> noise function is unknown. Numerical simulations results show that the proposed method can improve the image quality of <b>heteroscedastic</b> noise energy...|$|E
40|$|In this study, the Robust <b>Heteroscedastic</b> Consistent Covariance Matrix (RHCCM) was {{proposed}} {{in order to}} estimate standard errors of regression coefficients {{in the presence of}} high leverage points and <b>heteroscedastic</b> errors in multiple linear regression. Robust <b>Heteroscedastic</b> Consistent Covariance Matrix (RHCCM) is the combination of a robust method and Heteroscedasticit Consistent Covariance Matrix (HCCM). The robust method is used to eliminate the effect of high leverage points while HCCM is mainly used to eliminate the effect of <b>heteroscedastic</b> errors. The performance of RHCCM was assessed through an empirical study and compared with results obtained when the original <b>Heteroscedastic</b> Consistent Covariance Matrix was used...|$|E
40|$|Abstract: This article {{highlights}} {{a comprehensive}} and approachable perspective to stochastic volatility models for financial time series analysis. Financial time series represent a distinctive category {{in the economic}} field, with highly dynamic characteristics, especially in times of financial crisis. Beyond its highly empirical behavior, modeling volatility of financial asset returns aims to improve forecast accuracy. The stochastic volatility models analyzed in this article include the autoregressive conditional <b>heteroscedastic</b> model (ARCH), the generalized autoregressive conditional <b>heteroscedastic</b> (GARCH) model and the exponential generalized autoregressive conditional <b>heteroscedastic</b> (EGARCH) model. Key words: stochastic volatility class of models, high-frequency data, stationary time series, autoregressive conditional <b>heteroscedastic</b> model, financial asset return...|$|E
40|$|Keywords: S-estimators, robust regression, <b>heteroscedastic</b> errors This talk is {{concerned}} with robust estimators for linear regression when the errors may not be homoscedastic. Although many regression estimators derived under the assumption of constant error variance remain consistent when the errors are <b>heteroscedastic,</b> they typically experience a loss in efficiency. Several estimators for <b>heteroscedastic</b> linear regression models have been proposed in the literature. Maximum likelihood and generalized least squares estimators were studied b...|$|E
40|$|In {{this thesis}} we study robust {{variable}} selection in (<b>heteroscedastic)</b> linear regression using the nonnegative garrote method. Many variable selection methods are available, {{but very few}} methods are designed to avoid sensitivity to outliers in the response and in the covariates. The nonnegative garrote method is a powerful variable selection method, developed originally for linear regression (see Chapter 1). This method starts from an initial estimator, the ordinary least-squares regression estimator, and then it shrinks or puts some coefficients of this initial estimator to zero using the nonnegative garrote shrinkage factors. Since this method {{is based on the}} ordinary least-squares regression estimator, it is not robust to outliers. In Chapter 2 we robustify the nonnegative garrote method for outliers in the response and in the covariates by using robust alternatives to the ordinary least-squares regression. We present three robust versions of the nonnegative garrote, namely the M-, S-, and LTS-nonnegative garrote and propose an extra reweighting step to improve the results of the S-nonnegative garrote. The performances of the methods are investigated via simulations and their use is illustrated on a real data example. In Chapter 3 some theoretical properties of the S-nonnegative garrote method are proved. We show that this method is consistent in variable selection and in estimation. We also provide a lower bound for its breakdown point and derive its influence function. Some illustrations regarding the influence function are given. The original nonnegative garrote method tends to select too many variables, when the error terms do not have a constant variance. The aim in Chapter 4 is to select and estimate the variables that explain the mean response and/or the variance in a <b>heteroscedastic</b> linear regression model. First, different (robust) <b>heteroscedastic</b> initial estimators are introduced and the original nonnegative garrote method for variable selection in a linear regression model is extended to a variable selection method in a <b>heteroscedastic</b> linear regression model. Then, three approaches to robustify this <b>heteroscedastic</b> nonnegative garrote method for outliers in the response and in the covariates are discussed, namely the <b>heteroscedastic</b> M-, S-, and T-nonnegative garrote. We further compare the performances of the different <b>heteroscedastic</b> nonnegative garrote methods in a simulation study. In Chapter 5 we provide the influence functions of the <b>heteroscedastic</b> nonnegative garrote method and the <b>heteroscedastic</b> S-nonnegative garrote method. Since the influence functions of initial estimators are needed in the influence functions of the <b>heteroscedastic</b> (S-) nonnegative garrote methods, we also derive the influence functions of the maximum likelihood estimator and the <b>heteroscedastic</b> S-estimator. With these influence functions we can see that the <b>heteroscedastic</b> nonnegative garrote method is not robust for outliers and that the influence function of the <b>heteroscedastic</b> S-nonnegative garrote method is bounded for outliers. Illustrations regarding these influence functions are provided and we illustrate the use of the different <b>heteroscedastic</b> nonnegative garrote methods of Chapter 4 in a real data example. Finally, in Chapter 6 we draw some conclusions and discuss some possible topics for further research. nrpages: 254 status: publishe...|$|E
40|$|In his seminal 1982 paper, Robert F. Engle {{described}} a time series {{model with a}} time-varying volatility. Engle showed that this model, which he called ARCH (autoregressive conditionally <b>heteroscedastic),</b> is well-suited for the description of economic and financial price. Nowadays ARCH {{has been replaced by}} more general and more sophisticated models, such as GARCH (generalized autoregressive <b>heteroscedastic).</b> This monograph concentrates on mathematical statistical problems associated with fitting conditionally <b>heteroscedastic</b> time series models to data. This includes the classical statistical i...|$|E
40|$|SUMMARY. The {{universal}} optimality of some block designs with unequal block sizes is studied under {{a certain}} <b>heteroscedastic</b> model. The C-matrix of a block design under the <b>heteroscedastic</b> model is obtained by using generalised least squares. Some methods of con-structing universally optimal designs are given. 1...|$|E
40|$|It {{is shown}} that for linear {{multiple}} model of <b>heteroscedastic</b> observations for which variances changing linearly D-optimal designs take place when all points of spectrum of these designs lay in vertices of unite cube and not only. The structure such D- and A-optimal designs for <b>heteroscedastic</b> observations is investigated...|$|E
40|$|We {{propose a}} new {{approach}} for analyzing skewed and <b>heteroscedastic</b> health care cost data through regression of the conditional quantiles of the transformed cost. Using the appealing equivariance property of quantiles to monotone transformations, we propose a distribution-free estimator of the conditional mean cost on the original scale. The proposed method is extended to a two-part <b>heteroscedastic</b> model to account for zero costs commonly seen in health care cost studies. Simulation studies indicate that the proposed estimator has competitive and more robust performance than existing estimators in various <b>heteroscedastic</b> models. Copyright 2010, Oxford University Press. ...|$|E
40|$|Tsay (1987) {{developed}} the conditional <b>heteroscedastic</b> autoregressive moving-average model, {{which includes the}} conditional <b>heteroscedastic</b> autoregressive and random coefficient autoregressive models as special cases. This paper establishes the multivariate conditional <b>heteroscedastic</b> autoregressive moving-average model, and considers its theoretical properties and applications. Maximum likelihood estimation of the model is discussed in detail. A representation of the information matrix is obtained using the star product. This enhances estimation and statistical inferences procedures. Some simulation results and an application to the volatility of the Standard & Poor's 500 and Sydney's All Ordinaries indices are also considered. link_to_subscribed_fulltex...|$|E
40|$|We {{introduce}} {{the extension of}} local polynomial fitting to the linear <b>heteroscedastic</b> regression model. Firstly, the local polynomial fitting is applied to estimate <b>heteroscedastic</b> function, then the coefficients of regression model are obtained by using generalized least squares method. One noteworthy feature of our approach is that we avoid the testing for heteroscedasticity by improving the traditional two-stage method. Due to nonparametric technique of local polynomial estimation, {{we do not need}} to know the <b>heteroscedastic</b> function. Therefore, we can improve the estimation precision, when the <b>heteroscedastic</b> function is unknown. Furthermore, we focus on comparison of parameters and reach an optimal fitting. Besides, we verify the asymptotic normality of parameters based on numerical simulations. Finally, this approach is applied to a case of economics, and it indicates that our method is surely effective in finite-sample situations...|$|E
40|$|Multivariate local {{polynomial}} fitting {{is applied}} to the multivariate linear <b>heteroscedastic</b> regression model. Firstly, the local polynomial fitting {{is applied to}} estimate <b>heteroscedastic</b> function, then the coefficients of regression model are obtained by using generalized least squares method. One noteworthy feature of our approach is that we avoid the testing for heteroscedasticity by improving the traditional two-stage method. Due to non-parametric technique of local polynomial estimation, it is unnecessary to know the form of <b>heteroscedastic</b> function. Therefore, we can improve the estimation precision, when the <b>heteroscedastic</b> function is unknown. Furthermore, we verify that the regression coefficients is asymptotic normal based on numerical simulations and normal Q-Q plots of residuals. Finally, the simulation results and the local polynomial estimation of real data indicate that our approach is surely effective in finite-sample situations...|$|E
40|$|Finding {{a precise}} {{estimate}} for the smoothness parameter of LSTAR models is notoriously difficult. This paper introduces a robust estimation method {{for the transition}} and autoregressive parameters of STAR models, comprising gradient descent and singular value decomposition to account for <b>heteroscedastic</b> noise. STAR models Gradient descent Singular value decomposition <b>Heteroscedastic</b> noise...|$|E
