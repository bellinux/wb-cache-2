1|206|Public
40|$|This thesis {{describes}} {{the design and}} implementation of a Digital Audio Broadcasting (DAB) System developed using C++ Language and SystemC libraries. The main aspects covered within this report are the data structure of DAB system, and some interesting points of SystemC Library very useful {{for the implementation of}} the final system. It starts with a introduction of DAB system and his principals advantages. Next it goes further into the definition of data structures of DAB, they are FIC, MSC, and <b>DAB</b> <b>audio</b> <b>frame,</b> explained with MPEG and PAD packets. Later on this chapter there is an explanation of the SystemC library with special attention on the features that I used to implement the system. This features are the events used in the communication between processes and the interfaces needed for sending and receiving the data. With all these points covered is quite easy for a reader to understand the implementation of the system, despite this point is covered in the last chapter of the thesis. The implementation is here explained in two different steps. The first one explain how is formed the <b>DAB</b> <b>audio</b> <b>frame</b> by means of MPEG frames that are wrote in channel by producer interface, this frames are readed by consumer interface. For this purpose I have created some classes and structures that are explained in this part. The second part explain how I obtain the DAB transmission frame which is obtained creating MSC frames, that are big data structures formed by groups of DAB audio frames, therefore there are some functions that act like a buffer and add audio frames to the MSC data structure. Of independent way there is the FIC frame that is generated of random way and its added to the transmission frame...|$|E
40|$|A new {{approach}} to design a <b>DAB</b> <b>audio</b> decoder is introduced {{to improve the quality}} of audio at the receiver end. Integrating an MPEG- 1 Layer II (MP 2) decoder and Advanced Audio Coding Low Complexity (AAC LC) decoder provides basic <b>audio</b> decoding for <b>DAB</b> in FPGA. The <b>audio</b> <b>frames</b> data are generated from DAB channel decoder are stored in RAM. The bit stream Demultiplexer parses the quantized spectrum data in the <b>audio</b> <b>frame</b> and stores them in to the audio RAM. The inverse quantization block reads the quantized spectra from the audio RAM, performs the inverse quantization computations, and writes back the result to the audio RAM. The synthesis filter reads the inverse quantized spectra from the audio RAM, generates the time domain Pulse Code Modulation (PCM) samples, and writes them back to the audio RAM. The whole projects run by using XILINX 12. 1 ISE. Finally analyze the power consumption and area occupied by the designed architecture...|$|R
50|$|There {{have been}} at least two pieces of audio {{hardware}} produced with the MoS branding which have been capable of receiving <b>DAB</b> <b>audio</b> broadcasts (despite the station coming off DAB in 2002).|$|R
50|$|A {{total of}} 18 <b>DAB</b> <b>audio</b> {{services}} are on air. Population coverage is currently 20%. The Spanish DAB Association (Asociación Foro de la Radio Digital) comprising both national {{private and public}} broadcasters, is responsible for DAB/DAB+ in Spain.|$|R
40|$|Audio decoder {{device for}} {{decoding}} a bitstream, the audio decoder device comprising: a predictive decoder for producing a decoded <b>audio</b> <b>frame</b> from the bitstream, wherein the predictive decoder comprises a parameter decoder for producing {{one or more}} audio parameters for the decoded <b>audio</b> <b>frame</b> from the bitstream and wherein the predictive decoder comprises a synthesis filter device for producing the decoded <b>audio</b> <b>frame</b> by synthesizing {{the one or more}} audio parameters for the decoded audio frame; a memory device comprising one or more memories, wherein each of the memories is configured to store a memory state for the decoded <b>audio</b> <b>frame,</b> wherein the memory state for the decoded <b>audio</b> <b>frame</b> of the one or more memories is used by the synthesis filter device for synthesizing the one or more audio parameters for the decoded audio frame; and a memory state resampling device configured to determine the memory state for synthesizing the one or more audio parameters for the decoded <b>audio</b> <b>frame,</b> which has a sampling rate, for one or more of said memories by resampling a preceding memory state for synthesizing one or more audio parameters for a preceding decoded <b>audio</b> <b>frame,</b> which has a preceding sampling rate being different from the sampling rate of the decoded <b>audio</b> <b>frame,</b> for one or more of said memories and to store the memory state for synthesizing of the one or more audio parameters for the decoded <b>audio</b> <b>frame</b> for one or more of said memories into the respective memory...|$|R
40|$|In this paper, {{we propose}} an {{adaptive}} audio watermarking scheme {{according to local}} audio features. Firstly, the original audio signal is partitioned into <b>audio</b> <b>frames</b> and these <b>audio</b> <b>frames</b> are transformed into DWT domain respectively. Next, the local features of each <b>audio</b> <b>frame</b> are extracted respectively, and these features are used to train kernel fuzzy c-means (KFCM) clustering algorithm. According to well-trained KFCM, the <b>audio</b> <b>frames</b> to embed the watermark are selected and their embedding strengths are determined adaptively. The experimental results show the proposed method is robust to common signal processing operations such as lossy compression (MP 3), filtering, re-sampling, re-quantizing, etc...|$|R
40|$|Techniques for {{watermarking}} digital representations such as MPEG <b>audio</b> <b>frames</b> {{that spread}} the watermark information {{across the entire}} <b>audio</b> <b>frame.</b> The techniques work in conjunction with lossy compression techniques and are compatible with the perception models that are often used with lossy compression techniques. The watermark information is spread by means of transformations between the space/time domain and the frequency domain. When a MPEG <b>audio</b> <b>frame</b> is being watermarked, the compressed <b>audio</b> <b>frame</b> as it is produced by the quantizer is transformed from the frequency domain to the time domain; the time domain transformation is then randomized using a key and the randomized time domain transformation is transformed into the frequency domain. The watermark information is added at a predetermined frequency in the frequency domain transformation and the sequence of transformations is done in reverse order, with the randomization and derandomization serving to distribute the watermark information across the frequency domain representation of the watermarked <b>audio</b> <b>frame...</b>|$|R
30|$|Low-level audio MPEG- 7 {{descriptors}} {{for every}} <b>audio</b> <b>frame.</b>|$|R
25|$|Digital {{multimedia}} broadcasting (DMB) and DAB-IP {{are suitable}} for mobile radio and TV both because they support MPEG 4 AVC and WMV9 respectively as video codecs. However, a DMB video subchannel can easily be added to any DAB transmission, as {{it was designed to}} be carried on a DAB subchannel. DMB broadcasts in Korea carry conventional MPEG 1 Layer II <b>DAB</b> <b>audio</b> services alongside their DMB video services.|$|R
3000|$|... {{will contain}} the same {{harmonic}} frequency components. If the envelope modeling was perfect, then {{it follows that}} they would also be equal (differences in total gain are of no interest for this application), since they would have flat magnitude with exactly the same frequency components. In that case, {{it would be possible}} to resynthesize each of the two <b>audio</b> <b>frames</b> using only the AR filter that corresponds to that <b>audio</b> <b>frame</b> and the residual signal of the other microphone. The final signal is resynthesized from the <b>audio</b> <b>frames</b> using the overlap-add procedure. If, similarly, the source/filter model was used for all the spot microphone signals of a single performance, {{it would be possible to}} completely resynthesize these signals using their AR vector sequences (one vector for each <b>audio</b> <b>frame)</b> and the residual error of only one microphone signal. This would result in a great reduction of the datarate of the multiple microphone signals.|$|R
40|$|An {{audio decoder}} for {{providing}} a decoded representation of an audio content {{on the basis}} of an encoded representation of the audio content comprises a linear-prediction-domain decoder core configured to provide a time-domain representation of an <b>audio</b> <b>frame</b> {{on the basis of}} a set of linear-prediction domain parameters associated with the <b>audio</b> <b>frame</b> and a frequency-domain decoder core configured to provide a time-domain representation of an <b>audio</b> <b>frame</b> {{on the basis of a}} set of frequency-domain parameters, taking into account a transform window out of a set comprising a plurality of different transform windows. The audio decoder comprises a signal combiner configured to overlap-and-add-time-domain representations of subsequent <b>audio</b> <b>frames</b> encoded in different domains, in order to smoothen a transition between the time-domain representations of the subsequent frames. The set of transform windows comprises one or more windows specifically adapted for a transition between a frequency-domain core mode and a linear-prediction-domain core mode...|$|R
3000|$|... : {{the quality}} impact of video- or audio-transmission errors, that is, video packet or <b>audio</b> <b>frame</b> loss.|$|R
50|$|This {{stream of}} <b>audio</b> <b>frames,</b> as a whole, is then {{subjected}} to CIRC encoding, which segments and rearranges {{the data and}} expands it with parity bits {{in a way that}} allows occasional read errors to be detected and corrected. CIRC encoding also interleaves the <b>audio</b> <b>frames</b> throughout the disc over several consecutive frames so that the information will be more resistant to burst errors. Therefore, a physical frame on the disc will actually contain information from multiple logical <b>audio</b> <b>frames.</b> This process adds 64 bits of error correction data to each frame. After this, 8 bits of subcode or subchannel data are added to each of these encoded frames, which is used for control and addressing when playing the CD.|$|R
5000|$|Michael Wehr - arranger, keyboards, {{background}} vocals, producer, engineer, mixing, digital engineer, <b>audio</b> <b>frame</b> systems operator, {{digital system}} operator ...|$|R
50|$|WorldDAB is {{a global}} {{industry}} organisation responsible for defining {{the standards of the}} Eureka-147 family which includes <b>DAB</b> (Digital <b>Audio</b> Broadcasting) and DAB+ for digital radio Digital radio.|$|R
5000|$|In {{the early}} 1990s he was {{involved}} in trials of <b>DAB</b> (Digital <b>Audio</b> Broadcasting [...] - [...] i.e. Digital Radio), but was made redundant on 2 May 1997.|$|R
50|$|DAB and DAB+ {{cannot be}} used for mobile TV {{because they do not}} include any video codecs. DAB related {{standards}} Digital Multimedia Broadcasting (DMB) and DAB-IP are suitable for mobile radio and TV both because they have MPEG 4 AVC and WMV9 respectively as video coding formats. However a DMB video sub-channel can easily be added to any DAB transmission - as DMB was designed from the outset to be carried on a DAB subchannel. DMB broadcasts in Korea carry conventional MPEG 1 Layer II <b>DAB</b> <b>audio</b> services alongside their DMB video services.|$|R
5000|$|An <b>Audio</b> <b>Frame</b> section, which {{contains}} decoding information {{common to all}} audio blocks within the syncframe, including the necessary information to determine how exponents and mantissas are packed.|$|R
50|$|The {{comparison}} with hidden stations shows that RTS/CTS packages in each traffic class are profitable (even with short <b>audio</b> <b>frames,</b> which cause a high overhead on RTS/CTS frames).|$|R
3000|$|... {{are stored}} {{for later use}} in the {{extraction}} process. This makes the proposed watermarking algorithm semi-blind, as the whole original <b>audio</b> <b>frame</b> is not required in the extraction process.|$|R
3000|$|... {{completely}} {{characterizes the}} signal spectral properties. In the general case, the error signal (or residual signal) {{will not have}} white noise statistics and thus cannot be ignored. In this general case, the all-pole model that results from the LP analysis gives only an approximation of the signal spectrum, and more specifically the spectral envelope. For the particular case of audio signals, the spectrum contains only the frequency components that correspond to the fundamental frequencies of the recorded instruments and all their harmonics. (For simplicity, {{at this point we}} consider only harmonic sounds. The proposed model is tested for complex music signals in Section 5.) The AR filter for an <b>audio</b> <b>frame</b> will capture its spectral envelope. The error signal {{is the result of the}} <b>audio</b> <b>frame</b> filtered with the inverse of its spectral envelope. Thus we conclude that the error signal will contain the same harmonics as the <b>audio</b> <b>frame,</b> but their amplitudes will now have significantly flatter shape in the frequency spectrum.|$|R
40|$|One {{challenge}} in score following (i. e., mapping <b>audio</b> <b>frames</b> to score positions in real time) for piano performances is the mismatch between audio and score {{caused by the}} us-age of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration nat-urally ceases. This makes the notes longer than their no-tated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each <b>audio</b> <b>frame</b> are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of “new notes ” in the audio repre-sentation. This operation reduces sustain-pedal effects by weakening the match between the <b>audio</b> <b>frame</b> and previ-ous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Ex-periments on synthetic and real piano performances from the MAPS dataset show significant improvements on both alignment accuracy and robustness...|$|R
30|$|In the {{proposed}} method the <b>audio</b> <b>frame</b> is marked as voiced frame when the STE {{is high and}} ZCC is low. In contrast, when the STE is low and ZCC is high, the frame is marked as unvoiced frame [35].|$|R
50|$|MPEG-4 SL, the MPEG-4 {{synchronization}} layer {{manages the}} identification of access units like video or <b>audio</b> <b>frames,</b> and scene description commands and the time stamping of them independent of the media type within elementary streams to enable synchronization among them.|$|R
40|$|A new {{low level}} audio {{descriptor}} {{that represents the}} psycho acoustic noise floor shape of an <b>audio</b> <b>frame</b> is proposed. Results presented indicate that the proposed descriptor is far more resilient to compression noise {{than any of the}} MPEG- 7 low level audio descriptors. In fact, across a wide range of files, on average the proposed scheme fails to uniquely identify only five frames in every ten thousand. In addition, the proposed descriptor maintains a high resilience to compression noise even when decimated to use only one quarter of the values per frame to represent the noise floor. This characteristic indicates the proposed descriptor presents a truly scalable mechanism for transparently describing the characteristics of an <b>audio</b> <b>frame...</b>|$|R
40|$|In this paper, {{we propose}} a {{statistical}} optimization framework for transmitting audio sequences over wireless links. Our proposed framework protects <b>audio</b> <b>frames</b> against both temporally cor-related random bit errors introduced by a fading channel and packet erasures caused by network buffering. Forming a two-dimensional grid of symbols, our framework forms horizontal packets that are compensated only vertically against {{both types of}} errors. The utilized one-dimensional error correction coding scheme of our framework assigns parity bits according to the perceptual importance of frames such that the Segmented SNR of a received audio sequence is maximized. In addition, the proposed framework suggests an effective way of reducing the packetization over-head of small <b>audio</b> <b>frames.</b> I...|$|R
50|$|On 9 January 1995 {{the station}} split its services/frequencies to become Tay AM and Tay FM. Both {{stations}} now also broadcast on <b>DAB</b> (Digital <b>Audio</b> Broadcasting - i.e. Digital Radio) {{and over the}} internet via their respective websites.|$|R
50|$|Smacker audio {{is one of}} {{the audio}} formats {{that can be used in}} the Smacker container. For compression, Differential pulse code {{modulation}} (DPCM) is used. The difference between two successive samples is compressed using Huffman coding. The Huffman tables are adapted once per <b>audio</b> <b>frame.</b>|$|R
40|$|We {{present an}} {{integrated}} approach of full-band audio time scale modification for Voice over IP communication. The concept {{is based on}} a low complexity adaptive playout method that uses <b>frame</b> dropping and <b>audio</b> concealment for time shrinking and stretching, respectively. The existing version of this method is improved using a classifier that assists in choosing which <b>audio</b> <b>frames</b> can be dropped with the least subjective impact on audio quality. To maintain low complexity, we exclusively use audio signal features that are available in the audio codec. The classification of <b>audio</b> <b>frames</b> improves <b>audio</b> quality of the existing method without classification by 0 : 5 Mean Opinion Score points while requiring significantly less computational complexity by a factor of ca 104...|$|R
40|$|Abstract—In this paper, {{we present}} an {{optimization}} framework for transmitting high quality audio sequences over error-prone wireless links. Our framework introduces apparatus and technique to optimally protect a stored audio sequence transmitted over a wireless link while considering the packetization overhead of <b>audio</b> <b>frames.</b> Utilizing rate compatible punctured RS codes and dynamic program-ming, it identifies the optimal assignment of parity to <b>audio</b> <b>frames</b> {{according to their}} perceptual importance such that the Segmented SNR of the received audio sequence is maximized. Our framework covers two cases. In the first case, a frame grouping technique is proposed to packetize <b>audio</b> <b>frames</b> and protect them against temporarily corre-lated bit errors introduced by a fading wireless channel. In this case, each packet is treated as a channel coding codeword. In the second case, a one-dimensional RS coder is applied vertically to a sequence of horizontally formed packets associated with an audio sequence {{in order to protect}} the sequence against both bit errors introduced by fading wireless channels and packet erasures introduced by network buffering. Our numerical results capture the performance advantage of our framework compared to existing techniques proposed in the literature of audio transmission. We also note that our framework can be generically applied to a variety of audio coders making it attractive in terms of implementation...|$|R
40|$|This paper {{describes}} a novel high capacity steganography algorithm for embedding {{data in the}} inactive frames of low bit rate audio streams encoded by G. 723. 1 source codec, which is used extensively in Voice over Internet Protocol (VoIP). This study reveals that, contrary to existing thoughts, the inactive frames of VoIP streams {{are more suitable for}} data embedding than the active frames of the streams, that is, steganography in the inactive <b>audio</b> <b>frames</b> attains a larger data embedding capacity than that in the active <b>audio</b> <b>frames</b> under the same imperceptibility. By analysing the concealment of steganography in the inactive frames of low bit rate audio streams encoded by G. 723. 1 codec with 6. 3 kbps, the authors propose a new algorithm for steganography in different speech parameters of the inactive frames. Performance evaluation shows embedding data in various speech parameters led to different levels of concealment. An improved voice activity detection algorithm is suggested for detecting inactive <b>audio</b> <b>frames</b> taking into packet loss account. Experimental results show our proposed steganography algorithm not only achieved perfect imperceptibility but also gained a high data embedding rate up to 101 bits/frame, indicating that the data embedding capacity of the proposed algorithm is very much larger than those of previously suggested algorithms...|$|R
50|$|An old-fashioned CD player reading subcode {{correctly}} sees {{a missing}} <b>audio</b> <b>frame</b> and interpolates any missing information that it cannot correct using information from neighbouring frames. Because these missing frames occur at points where the waveform was nearly {{a straight line}} anyway, this interpolation is very accurate and generally transparent to the user.|$|R
30|$|The {{next stage}} {{estimates}} the energy {{contained in the}} interest band of an <b>audio</b> <b>frame.</b> Once this energy is estimated, a threshold is applied to detect the whistle. Although there exists high correlation between energy peaks and whistles, in some difficult scenarios thresholding produce an unacceptable number of false alarms due {{to the presence of}} noise in the band of interest. Then, the last two stages of the whistle detector system try {{to reduce the number of}} false detections by exploding the fact that the whistle spectrum is made of three tones at very close frequencies. However, as these frequencies are not stable and may vary slightly with the specific excitation, the direction of blowing and the whistle model, in order to discriminate the tonal vs non-tonal nature of the <b>audio</b> <b>frame,</b> an entropy-based stage is introduced.|$|R
30|$|The {{frequency}} analysis block computes {{a set of}} Discrete Fourier Transform (DFT) samples in the band of interest for every <b>audio</b> <b>frame.</b> That band is selected between 3.5 and 4.5  kHz as broadly include the frequencies produced by professional whistles. The reduced set of samples of the DFT is computed using the Goertzel algorithm (Oppenheim and Schafer 2010).|$|R
40|$|International audienceEndpoints or conference servers {{of current}} audio-conferencing {{solutions}} {{use all the}} <b>audio</b> <b>frames</b> they receive in order to mix them into one final aggregate stream. However, at each time-instant, some of this content may not be audible due to auditory masking. Hence, sending corresponding frames through the network leads {{to a loss of}} bandwidth, while decoding them for mixing or spatial audio processing leads to increased processor load. In this paper, we propose a solution based on an efficient on-the-fly auditory masking evaluation. Our technique allows prioritizing <b>audio</b> <b>frames</b> in order to select only those audible for each connected client. We present results of quality tests showing the transparency of the algorithm. We describe its integration in a France Telecom audio conference server. Tests in a 3 D game environment with spatialized chat capabilities show a 70 % average reduction in required bandwidth, demonstrating the efficiency of our method...|$|R
40|$|In {{this paper}} {{we present a}} new method to compute <b>frame</b> based <b>audio</b> similarities, based on nearest {{neighbour}} density estimation. We do not recommend it is as a practical method for large collections {{because of the high}} runtime. Rather, we use this new method for a detailed analysis to get a deeper insight on how a bag of frames approach (BOF) determines similarities among songs, and in particular, to identify those <b>audio</b> <b>frames</b> that make two songs similar from a machine’s point of view. Our analysis reveals that <b>audio</b> <b>frames</b> of very low energy, which are of course not the most salient with respect to human perception, have a surprisingly big influence on current similarity measures. Based on this observation we propose to remove these low-energy frames before computing song models and show, via classification experiments, that the proposed frame selection strategy improves the audio similarity measure. 1...|$|R
