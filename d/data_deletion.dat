78|253|Public
25|$|Btrieve {{is not a}} {{relational}} database management system (RDBMS). Early descriptions of Btrieve {{referred to it as}} a record manager (though Pervasive initially used the term navigational database but later changed this to transactional database) because it only deals with the underlying record creation, data retrieval, record updating and <b>data</b> <b>deletion</b> primitives. It uses ISAM as its underlying indexing and storage mechanism. A key part of Pervasive's architecture is the use of a MicroKernel Database Engine, which allows different database backends to be modularised and integrated easily into their DBMS package, Pervasive.SQL. This has enabled them to support both their Btrieve navigational database engine and an SQL-based engine, Scalable SQL.|$|E
5000|$|... "A {{technical}} {{analysis of the}} <b>data</b> <b>deletion</b> malware used in this attack revealed links to other malware that the FBI knows North Korea previously developed. For example, there were similarities in specific lines of code, encryption algorithms, <b>data</b> <b>deletion</b> methods, and compromised networks.|$|E
50|$|Note: {{a factory}} reset may only hide {{data from the}} {{operating}} system so it appears it no longer exists. This {{is not the same}} effect as <b>data</b> <b>deletion,</b> and may not therefore be wholly suitable in situations where the device changes ownership.|$|E
30|$|Our {{main focus}} for future {{work is to}} study the effect of updates on <b>data</b> (<b>deletions</b> and insertions) in the {{fragments}}: it must be studied in detail how fragments can be reconfigured and probably migrated to other server without incurring too much transfer cost.|$|R
50|$|The company {{develops}} {{advertising and}} marketplace software {{that does not}} require collecting and tracking user <b>data,</b> offers <b>deletion</b> functionality and doesn't utilize behavioral targeting or data brokering on GotChosen.|$|R
50|$|A {{database}} (.DBF) file {{is composed}} of a header, <b>data</b> records, <b>deletion</b> flags, and an end-of-file marker. The header contains information about the file structure, and the records contain the actual data. One byte of each record is reserved for the deletion flag.|$|R
5000|$|Malware infections causing {{incidents}} such as unauthorized access, leakage or {{disclosure of}} personal or proprietary <b>data,</b> <b>deletion</b> of or {{damage to the}} data or programs, interruption or denial of authorized access to the database, attacks on other systems and the unanticipated failure of database services; ...|$|E
50|$|Btrieve is a {{database}} developed by Pervasive Software. The architecture of Btrieve {{has been designed}} with record management in mind. This means that Btrieve only deals with the underlying record creation, data retrieval, record updating and <b>data</b> <b>deletion</b> primitives. Together with the MicroKernel Database Engine it uses ISAM, Indexed Sequential Access Method, as its underlying storage mechanism.|$|E
50|$|PFIF 1.3, {{released}} in March 2011, addressed {{the privacy of}} personal information by adding a field to specify an expiry date on each person record and setting out requirements for <b>data</b> <b>deletion.</b> PFIF 1.3 also {{moved away from the}} US-specific assumption of a first and last name by adding one field for a person's full name.|$|E
40|$|This paper {{contains}} {{a discussion on}} the role of ATM adaptation for video, and an evaluation of AAL 1 and 5 in light of the conclusions. A proposal for a new AAL that may support video (and audio) generically is presented. Most attention is given to the detection of <b>data</b> [...] <b>deletions</b> caused by cell loss...|$|R
50|$|Over-expression of VMAT2 {{results in}} {{increased}} secretion of neurotransmitter upon cell stimulation. <b>Data</b> suggests that <b>deletion</b> of the VMAT2 genes {{does not affect}} the size of small clear-core vesicles.|$|R
40|$|We {{performed}} computational {{reconstruction of}} the in silico gene regulatory networks in the DREAM 3 Challenges. Our task was to learn the networks from two types of data, namely gene expression profiles in deletion strains (the ‘deletion data’) and time series trajectories of gene expression after some initial perturbation (the ‘perturbation data’). In the course of developing the prediction method, we observed that {{the two types of}} data contained different and complementary information about the underlying network. In particular, <b>deletion</b> <b>data</b> allow for the detection of direct regulatory activities with strong responses upon the deletion of the regulator while perturbation data provide richer information for the identification of weaker and more complex types of regulation. We applied different techniques to learn the regulation from the two types of <b>data.</b> For <b>deletion</b> <b>data,</b> we learned a noise model to distinguish real signals from random fluctuations using an iterative method. For perturbation data, we used differential equations to model the change of expression levels of a gene along the trajectories due to the regulation of other genes. We tried different models, and combined their predictions. The final predictions were obtained by merging the results from the two types of data. A comparison with the actual regulatory networks suggests that our approach is effective for networks with a range of different sizes. The succes...|$|R
5000|$|... "The FBI also {{observed}} significant overlap between the infrastructure {{used in this}} attack and other malicious cyber activity the U.S. government has previously linked directly to North Korea. For example, the FBI discovered that several Internet protocol (IP) addresses associated with known North Korean infrastructure communicated with IP addresses that were hardcoded into the <b>data</b> <b>deletion</b> malware used in this attack. The FBI later clarified that the source IP addresses were associated {{with a group of}} North Korean businesses located in Shenyang in northeastern China.|$|E
50|$|This {{component}} is the control point for product scheduling functions, configuration, event information, reporting, and {{graphical user interface}} (GUI) support. It coordinates communication with and data collection from agents that scan file systems and databases to gather storage demographics and populate the database with results. Automated actions can be defined to perform file system extension, <b>data</b> <b>deletion,</b> and Tivoli Storage Productivity Center backup or archiving, or event reporting when defined thresholds are encountered. The Data server is the primary contact point for GUI user interface functions. It also includes functions that schedule data collection and discovery for the Device server.|$|E
50|$|Btrieve {{is not a}} {{relational}} database management system (RDBMS). Early descriptions of Btrieve {{referred to it as}} a record manager (though Pervasive initially used the term navigational database but later changed this to transactional database) because it only deals with the underlying record creation, data retrieval, record updating and <b>data</b> <b>deletion</b> primitives. It uses ISAM as its underlying indexing and storage mechanism. A key part of Pervasive's architecture is the use of a MicroKernel Database Engine, which allows different database backends to be modularised and integrated easily into their DBMS package, Pervasive.SQL. This has enabled them to support both their Btrieve navigational database engine and an SQL-based engine, Scalable SQL.|$|E
40|$|AbstractTo {{understand}} a biological process {{it is clear}} that a single approach will not be sufficient, just like a single measurement on a protein – such as its expression level – does not describe protein function. Using reference sets of proteins as benchmarks different approaches can be scaled and integrated. Here, we demonstrate the power of data re-analysis and integration by applying it in a case study to <b>data</b> from <b>deletion</b> phenotype screens and mRNA expression profiling...|$|R
40|$|The {{current study}} answers the question: What are the {{relationships}} between teachers' assessment practices and students' mathematics achievement in large-scale assessment tests? The investigated teachers' assessment practices and students' achievement were defined according to the School Achievement Indicator Program (SAIP) 2001 mathematics achievement test and its accompanying teachers' questionnaire. The study focused on the problem solving component of that test. The assessment practices were investigated within three dimensions: assessment tools (e. g. teacher made multiple-choice tests, projects, and portfolios), assessment purpose (e. g. feedback, and diagnosis), and homework purpose (e. g. feedback and grading). Correlational analyses were employed to investigate whether there are relations between the assessment variables and students' mathematical achievements. Research findings proposed a few relations between students' achievement and the investigated assessment practices. The directions and the strengths of these relations varied based on a few {{factors such as the}} distribution of teachers' assessment practices, the amount of missing data, and the <b>data</b> <b>deletions</b> during analyses. The research findings could contribute to the literature on mathematical assessment, and also propose potential relations from which teachers, administrators, and policy makers may predict possible impacts of assessment practices on students' achievement...|$|R
50|$|The Guerrilla Archiving Event: Saving Environmental Data from Trump was {{a meeting}} {{arranged}} by two professors at the University of Toronto in December 2016, {{in an effort to}} pre-emptively preserve US government climate <b>data</b> from possible <b>deletion</b> by the Administration of Donald Trump.|$|R
5000|$|Evidence {{suggests}} that our visual processing system engages in bottom-up selection. For example, inattentional blindness {{suggests that}} there must be <b>data</b> <b>deletion</b> early on in the visual pathway. [...] This bottom-up approach allows us to respond to unexpected and salient events more quickly and is often directed by attentional selection. This also gives our visual system the property of being goal-directed. Many have suggested that the visual system is able to work efficiently by breaking images down into distinct components. Additionally, {{it has been argued that}} the visual system takes advantage of redundancies in inputs in order to transmit as much information as possible while using the fewest resources.|$|E
50|$|MozyEnterprise is {{for larger}} {{organizations}} and includes support for Active Directory, user groups (in a company), and more administrator control over user rights. For Mozy administrators, the company detailed its keyless activation and pooled storage features in a 2013 EMC blog post. Mozy {{has more than}} 800 enterprise customers. With all Mozy products, after the initial backup, Mozy only backs up new or changed portions of files. Backups can take place when the computer is not in use, either automatically, and at scheduled times. All user data is encrypted locally with strong encryption prior to transfer via a 128-bit SSL connection. Users can choose a managed encryption key or choose a personal key for added security. Mozy's security does not let end-users shortcut the <b>data</b> <b>deletion</b> process. Mozy is both SSAE 16 Type 2 and ISO 27001 certified.|$|E
50|$|Backups {{have two}} {{distinct}} purposes. The primary {{purpose is to}} recover data after its loss, be it by <b>data</b> <b>deletion</b> or corruption. Data loss can be a common experience of computer users; a 2008 survey found that 66% of respondents had lost files on their home PC. The secondary purpose of backups is to recover data from an earlier time, according to a user-defined data retention policy, typically configured within a backup application for how long copies of data are required. Though backups represent a simple form of disaster recovery, and {{should be part of}} any disaster recovery plan, backups by themselves should not be considered a complete disaster recovery plan. One {{reason for this is that}} not all backup systems are able to reconstitute a computer system or other complex configuration such as a computer cluster, active directory server, or database server by simply restoring data from a backup.|$|E
5000|$|Backup4all is {{a backup}} {{software}} for Microsoft Windows developed by Softland. It allows files {{to be backed}} up to any local or network drive, FTP or SFTP server, CD/DVD/Blu-ray, or other removable media. The term [...] "backing up" [...] means to save data on an external hard drive in case something happens to you personal computer. This is useful to safeguard sensitive <b>data</b> from <b>deletion.</b> Four different types of backups are supported: incremental backup, differential backup, full backup and mirror backup.|$|R
40|$|Many {{procedures}} {{exist for}} identifying sets of sites that collectively represent regional biodiversity. Whereas the mechanics and suitability of these procedures have received considerable attention, little {{effort has been}} directed towards assessing and quantifying the effects of varying data inputs on their outcomes. In the present paper, we use sensitivity analysis to evaluate the impacts of varying degrees of (i) survey intensity, (ii) survey extent and (iii) taxonomic diversity on iterative reserve selection procedures. A comprehensive distribution database of the mammalian fauna from the Transvaal region of South Africa is systematically perturbed before implementation of a site selection algorithm. The resulting networks of sites are then compared to quantitatively {{assess the impact of}} database variations on algorithm performance. Systematic <b>data</b> <b>deletions</b> result in increased network variability (identity of selected sites), decreased numbers of frequently selected sites, decreased spatial congruence among successive runs and a rapid {{increase in the number of}} additional sites required to represent all species present in the region. These effects become particularly evident once data sets are reduced to below 20 % of the original data. Consequently, a mixed survey strategy that balances survey effort with survey extent and maximizes taxonomic knowledge is more likely to ensure appropriate planning outcomes...|$|R
40|$|Becker muscular {{dystrophy}} (BMD) often results from in-frame mutations of the dystrophin gene that allow production of an altered but partially functional protein. To address potential structure-function relationships {{for the various}} domains of dystrophin, we examined both the dystrophin gene and protein in 68 patients with abnormal dystrophin. Eighty-six percent of BMD patients with dystrophin of altered size have deletions or duplications, and the observed sizes of dystrophin fit well with predictions based on DNA <b>data.</b> <b>Deletions</b> within the amino-terminal domain I tended to result in low levels of dystrophin and a more severe phenotype. The phenotypes of patients with deletions or duplications in the central rod domain were more variable. This region {{can be divided into}} three portions based on differences in clinical presentations of patients. Deletions around exons 4553 were most common and generally caused typical BMD; however, phenotypic variability among patients with similar mutations suggests that epigenetic and/or environmental factors {{play an important role in}} determining the clinical progression. In contrast, deletions or duplications in the proximal portion of this domain tended to cause severe cramps and myalgia. Finally, loss of the middle of this region probably causes a very mild phenotype, as only one such patient was found and his only symptom was elevated serum creatine phosphokinase levels...|$|R
5000|$|Peter Gutmann is a {{computer}} scientist in the Department of Computer Science at the University of Auckland, Auckland, New Zealand. He has a Ph.D. in computer science from the University of Auckland. His Ph.D. thesis and a book based on the thesis were about a cryptographic security architecture. He is interested in computer security issues, including security architecture, security usability (or more precisely the lack thereof), and hardware security, he has discovered assorted flaws in publicly released cryptosystems and protocols. He is the developer of the cryptlib open source software security library and contributed to PGP version 2. He is also known for his analysis of <b>data</b> <b>deletion</b> on electronic memory media, magnetic and otherwise, and devised the Gutmann method for erasing data from a hard drive more or less securely. Having lived in New Zealand for some time, he has written on such subjects as wetas, which are peculiar to New Zealand, and the Auckland power crisis of 1998, during which the electrical power system failed completely in the central city for five weeks. See, for instance, Auckland: Your Y2K beta test site on Gutmann's Homepage. He has also written on his career as an [...] "arms courier" [...] for New Zealand, detailing the difficulty faced in complying with customs control regulations with respect to cryptographic products, which were once classed as [...] "munitions" [...] by various jurisdictions including the United States.|$|E
40|$|Approaches to data {{protection}} have traditionally focused on physical security and/or cryptographic security, but the automatic {{deletion of data}} {{as a form of}} protection has not been widely employed. However, a vast amount of data is stored that has only a limited useful life and presents a risk to privacy and security after that time period. Approaches to <b>data</b> <b>deletion</b> can be broadly classified as physical destruction, data overwrite and cryptographic deletion. Cryptographic <b>data</b> <b>deletion</b> does not require any physical destruction and can be done quickly without any need to alter the data that is stored. Previous approaches to cryptographic <b>data</b> <b>deletion</b> have focused on bulk sanitization of storage devices. In this paper we introduce the notion of “forgetful ” storage that employs cryptographic <b>data</b> <b>deletion</b> using a time-based approach to key management. By periodically erasing old keys and creating new keys, old data becomes automatically and instantly inaccessible, or virtually deleted. This <b>data</b> <b>deletion</b> can be accomplished without depending on any actions {{on the part of the}} user, application software or operating system. A refresh policy can be utilized to cause information that is accessed to be re-encrypted using a newer key, thereby extending the time before it will expire...|$|E
40|$|This paper {{analyses}} how DNA {{sampling and}} genetic data retention {{can affect the}} fundamental rights of the individual. In particular, the author tries {{to focus on the}} <b>data</b> <b>deletion</b> in the discipline of the DNA database. In fact, the Italian legal discipline of the acquittal interacts with the regulation of <b>data</b> <b>deletion,</b> with the consequence that individual fundamental rights (i. e. the presumption of innocence, the principle of criminal responsibility based on illicit facts and not on personal qualities and the principle of equality) can be jeopardized by the retention of the genetic profiles in the DNA database...|$|E
40|$|Missing data is {{a serious}} problem in {{cross-cultural}} survey research, especially for regression models employing data sets such as the Ethnographic Atlas (EA) or the Standard Cross-Cultural Sample (SCCS). 1 In comparative survey research the most common procedure for handling missing <b>data</b> is listwise <b>deletion,</b> where one simply drop...|$|R
40|$|Background: Deletions in {{chromosome}} 3 occur {{frequently in}} uterine cervical carcinoma (CA-CX). The common consensus regions deleted during CA-CX development {{are not well}} defined, and have not been correlated with tumour progression. Aims: To define specific regions of chromosome 3 deleted during development of CA-CX and to correlate these with clinicopathological <b>data.</b> Methods: <b>Deletion</b> mapping of chromosome 3 was done in seven cervical intraepithelial neoplasia (CIN) and 43 primary CA-CX samples using 20 highly polymorphic microsatellite markers. Results: Deletions of chromosome 3 {{were significantly associated with}} tumour progression. High frequencies (33 – 53...|$|R
50|$|Radix trees support insertion, deletion, {{and searching}} operations. Insertion adds a new string to the trie {{while trying to}} {{minimize}} the amount of <b>data</b> stored. <b>Deletion</b> removes a string from the trie. Searching operations include (but are not necessarily limited to) exact lookup, find predecessor, find successor, and find all strings with a prefix. All of these operations are O(k) where k is the maximum length of all strings in the set, where length is measured in the quantity of bits equal to the radix of the radix trie.|$|R
40|$|With global {{personal}} information flows increasing, {{efforts have been}} made to develop principles to standardize data protection regulations. However, no set of principles has yet achieved universal adoption. This note proposes a principle mandating that personal data be securely destroyed when it is no longer necessary for the purpose for which it was collected. Including a <b>data</b> <b>deletion</b> principle in future data protection standards will increase respect for individual autonomy and decrease the risk of abuse of personal data. Though <b>data</b> <b>deletion</b> is already practiced by many data controllers, including it in legal data protection mandates will further the goal of establishing an effective global data protection regime...|$|E
40|$|The {{purpose of}} this {{document}} {{is to establish a}} policy on records retention and disposition, for records in both electronic and hardcopy formats. This policy will contain instructions on: electronic document management, a policy on data retention and <b>data</b> <b>deletion,</b> and a process for instituting a litigation hold. These policies and procedures are necessary to comply with all applicable Federal and Stat...|$|E
40|$|We {{address the}} problem of secure <b>data</b> <b>deletion</b> on log-structured file systems. We focus on the YAFFS file sys-tem, widely used on Android smartphones. We show that these systems provide no {{temporal}} guarantees on data dele-tion and that deleted data still persists for nearly 44 hours with average phone use and indefinitely if the phone is not used after the deletion. Furthermore, we show that file over-writing and encryption, methods commonly used for secure deletion on block-structured file systems, do not ensure <b>data</b> <b>deletion</b> in log-structured file systems. We propose three mechanisms for secure deletion on log-structured file systems. Purging is a user-level mechanism that guarantees secure deletion at the cost of negligible de-vice wear. Ballooning is a user-level mechanism that runs continuously and gives probabilistic improvements to se-cure deletion. Zero overwriting is a kernel-level mecha-nism that guarantees immediate secure deletion without de-vice wear. We implement these mechanisms on Nexus One smartphones and show that they succeed in secure deletion and neither prohibitively reduce the longevity of the flash memory nor noticeably reduce the device’s battery lifetime. These techniques provide mobile phone users more confi-dence that data they delete from their phones are indeed deleted. 1...|$|E
40|$|The {{effective}} use of targeted therapy is highly dependent upon the identification of responder patient populations. Loss of the Fbw 7 tumor suppressor is frequently found in various types of human cancers including breast cancer, colon cancer 1 and T-cell acute lymphoblastic leukemia (T-ALL) 2. In line with these genomic <b>data,</b> engineered <b>deletion</b> of Fbw 7 in mouse T cells results in T-ALL 3 – 5, validating Fbw 7 as a T-ALL tumor suppressor. The precise molecular mechanisms Users may view, print, copy, download and text and data- mine the content in such documents, {{for the purposes of}} academic research...|$|R
30|$|Block device layer. Reardon et al. (2013) {{proposed}} a secure deletion approach targeting persistent storage. Their approach relies on encryption and key wrapping. They use a key disclosure graph {{to model the}} adversarial knowledge about key generation and wrapping history. In addition, a small securely-deleting key-value map is used to discard encryption key of the <b>data,</b> achieving secure <b>deletion.</b>|$|R
30|$|As this {{research}} involved comparison among different models, {{the sample size}} had to be identical across them. After restricting the research population as wage earners, the sample size of the 1996 and 2006 data are 1654 and 2257, respectively. However, the missing data problem, {{for all of the}} variables introduced above, inevitably exists for both two datasets. In the 1996 <b>data,</b> the case-wise <b>deletion</b> procedure loses nearly 10  % of the legitimate respondents, yielding a net sample of 1501 cases. In the 2006 <b>data,</b> the case-wise <b>deletion</b> procedure loses nearly 30  % of the legitimate respondents, yielding a net sample of 1639 cases. In the descriptive statistics and formal regressions, the 1501 cases for 1996 and 1639 cases for 2006 are used, but the multiple imputation procedure is also applied to both datasets to remedy the missing data problem and check the robustness of analysis results.|$|R
