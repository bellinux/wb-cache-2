81|905|Public
50|$|Example motions {{are often}} created through <b>keyframing</b> or motion capture. However, <b>keyframing</b> is labor-intensive and lacks {{varieties}} of motion, and both processes result in motions that are time-consuming to alter.Motion interpolation provides a much faster alternative to creating new motions {{through the same}} means.|$|E
5000|$|Two keying modes — set key {{and auto}} key — offer support for {{different}} <b>keyframing</b> workflows.|$|E
50|$|There {{are several}} methods for {{generating}} the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly. Rather than set Avars for every frame, they usually set Avars at strategic points (frames) {{in time and}} let the computer interpolate or tween between them in a process called <b>keyframing.</b> <b>Keyframing</b> puts control {{in the hands of}} the animator and has roots in hand-drawn traditional animation.|$|E
30|$|Ideally, {{different}} <b>keyframes</b> should represent distinct visual elements. The <b>keyframe</b> {{extraction process}} must take geometric information into consideration. In addition, the high volume of video collections requires the <b>keyframe</b> selection algorithm to be computationally efficient. With such goals in mind, we choose a GPU-accelerated KLT tracker [25] to select <b>keyframes.</b>|$|R
40|$|A {{convenient}} {{representation of}} a video segment is a single “keyframe. ” <b>Keyframes</b> are widely used in applications such as non-linear browsing and video editing. With existing methods of <b>keyframe</b> selection, similar video segments result in very similar <b>keyframes,</b> with the drawback that actual differences between the segments may be obscured. We present methods for <b>keyframe</b> selection based on two criteria: capturing the similarity to the represented segment, and preserving the differences from other segment <b>keyframes,</b> so that different segments will have visually distinct representations. We present two discriminative <b>keyframe</b> selection methods, and an example of experimental results. 1...|$|R
30|$|As seen in Table  2, <b>keyframe</b> {{extraction}} {{takes the}} majority of the video processing time. But the quality of selected video <b>keyframes</b> is critical for extracting meaningful BoI representations and controlling the <b>keyframe</b> collection sizes.|$|R
50|$|Studio 11 Plus added native HDV and AVCHD editing, and HD DVD from {{standard}} discs. <b>Keyframing</b> became possible on most effects. Blu-ray AVCHD to standard DVDs was added in S11.1.1.|$|E
5000|$|Fast and {{intuitive}} {{controls for}} <b>keyframing</b> — including cut, copy, and paste — let the user create animations with ease. Animation trajectories {{may be viewed}} and edited directly in the viewport.|$|E
50|$|There are {{two types}} of digital {{armatures}}: <b>Keyframing</b> (stop-motion) armatures and real-time (puppeteering) armatures. <b>Keyframing</b> armatures were initially developed to assist in animating digital characters without basing the movement on a live performance. The animator poses a device manually for each keyframe, while the character in the animation is set up with a mechanical structure equivalent to the armature. The device is connected to the animation software through a driver program and each move is recorded for a particular frame in time. Real-time armatures are similar, but they are puppeteered by one or more people and captured in real-time.|$|E
3000|$|... is found, {{the closest}} intra-frame {{image from the}} MPEG video {{sequence}} stream in that time instant is extracted and labeled as a candidate <b>keyframe.</b> After finding a maximum of 10 candidate <b>keyframes</b> the iterative algorithm stops. Finally, the color resemblance among the candidate <b>keyframes</b> is assessed. The <b>keyframes</b> that are similar in terms of color are discarded by doing a histogram comparison in the HSV colorspace using the Chi Squared distance.|$|R
40|$|Although {{people are}} {{capturing}} more video with their mobile phones, digital cameras, and other devices, they rarely watch all that video. More commonly, users extract a still {{image from the}} video to print or a short clip to share with others. We created a novel interface for browsing through a video <b>keyframe</b> hierarchy to find frames or clips. The interface is shown to be more efficient than scrolling linearly through all <b>keyframes.</b> We developed algorithms for selecting quality <b>keyframes</b> and for clustering <b>keyframes</b> hierarchically. At each level of the hierarchy, a single representative <b>keyframe</b> from each cluster is shown. Users can drill down into the most promising cluster and view representative <b>keyframes</b> for the sub-clusters. Our clustering algorithms optimize for short navigation paths to the desired <b>keyframe.</b> A single <b>keyframe</b> is located using a non-temporal clustering algorithm. A video clip is located using one of two temporal clustering algorithms. We evaluated the clustering algorithms using a simulated search task. User feedback provided us with valuable suggestions for improvements to our system...|$|R
40|$|This paper {{presents}} an approach for efficient <b>keyframe</b> extraction, using local semantics {{in form of}} a region thesaurus. More specifically, certain MPEG- 7 color and texture features are locally extracted from <b>keyframe</b> regions. Then, using a hierarchical clustering approach a local region thesaurus is constructed to facilitate the description of each frame in terms of higher semantic features. The feature is consisted by the most common region types that are encountered within the video shot, along with their synonyms. These region types carry semantic information. Each <b>keyframe</b> is represented by a vector consisting of the degrees of confidence {{of the existence of}} all region types within this shot. Using this <b>keyframe</b> representation, the most representative <b>keyframe</b> is then selected for each shot. Where a single <b>keyframe</b> is not adequate, using the same algorithm and exploiting the coverage of the visual thesaurus, more <b>keyframes</b> are extracted. ...|$|R
50|$|Though {{the final}} version of AutoShade {{supported}} RenderMan extensions, Autodesk never developed it into a full rendering application. Even with AutoFlix, it did not support <b>keyframing</b> or hierarchical kinematics. When Autodesk released 3D Studio, the need for AutoShade ended, since 3D Studio was far superior in technology and in ease of use.|$|E
50|$|Before {{rendering}} into an image, objects must be {{laid out}} (place) in a scene. This defines spatial relationships between objects, including location and size. Animation {{refers to the}} temporal description of an object (i.e., how it moves and deforms over time. Popular methods include <b>keyframing,</b> inverse kinematics, and motion capture). These techniques are often used in combination. As with animation, physical simulation also specifies motion.|$|E
50|$|ProShow Producer is professional-grade {{slideshow}} {{software for}} creating photo and video slideshows. Features including <b>keyframing,</b> masking and adjustment {{effects can be}} used for further customization. The software has built-in CD, DVD and Blu-ray burning capabilities. It also supports output to an Executable file (.exe) for playing on most computers (excluding Apple) as well as to other formats including MPEG video, HTML5 video, QuickTime video, Flash Video, Facebook, Vimeo and YouTube.|$|E
40|$|Two {{methods for}} <b>keyframe</b> {{reduction}} of motion capture data are presented. <b>Keyframe</b> reduction of motion capture data enables animators to easily edit motion data with {{smaller number of}} <b>keyframes.</b> One of the approaches achieves <b>keyframe</b> reduction and noise removal simultaneously by fitting a curve to the motion information using dynamic programming. The other approach uses curve simplification algorithms on the motion capture data until a predefined threshold of number of <b>keyframes</b> is reached. Although the error rate varies with different motions, {{the results show that}} curve fitting with dynamic programming performs as good as curve simplification methods. © 2008 IEEE...|$|R
40|$|We propose {{an event}} based {{approach}} for locating <b>keyframes</b> in natural video through detection of locally correlated spectral targets. Temporal Decomposition (TD) {{is used to}} describe a set of spectral parameters of the video as a linear combination {{of a set of}} temporally overlapping event functions. This process provides preliminary information about <b>keyframes,</b> by selecting the frames located at event centroids as the <b>keyframe</b> candidates. No shot or shot cluster boundary detection is needed and <b>keyframes</b> are extracted directly from among event centroids that are much smaller in number than the number of frames. Generalized Gaussian Density (GGD) parameters, extracted from 2 D wavelet transform subbands of the frames, are used as the spectral parameters in the event detection process and Kullback-Leibler distance (KLD) is employed as a measure to select salient <b>keyframes.</b> Experimental results confirm superiority of the proposed scheme over the conventional <b>keyframe</b> selection approaches. Index Terms — Video scene analysis, <b>keyframe</b> selection, temporal decomposition, event function...|$|R
40|$|AbstractVideo {{partitioning}} and <b>keyframe</b> extraction (KFE) {{are the key}} {{foundations of}} video analysis and Content based video retrieval. The use of <b>keyframes</b> reduces the amount of data that is necessary in video indexing and provides the outline {{for dealing with the}} video content. In the last few years, many algorithms of <b>keyframe</b> extraction concentrated on the original compressed video stream. This can increase computational complexity when decompression is required before video processing. <b>Keyframe</b> is the frame, which can be a prototype of the salient content and information of the shot. The <b>keyframes</b> extracted must summarize the significant features of the video in time sequence. Therefore, there is a commensurate need of an efficient and secured <b>keyframe</b> selection technique in an efficient CBVR system. We propose an algorithm for <b>keyframe</b> extraction of compressed video shots using adaptive threshold method. Extensive computation on 200 plus video clips is performed and results are accurate and satisfactory...|$|R
50|$|An {{entry-level}} version, Adobe Premiere Elements {{is aimed}} at home users available on Microsoft Windows and macOS. With Premiere Pro aimed at the professional market, it has advantages over Premiere Elements including multiple sequence support, high bit-depth rendering, multicamera editing, time remapping, scopes, color correction tools, advanced audio mixer interface, and bezier <b>keyframing.</b> Premiere Pro also has Encore, for more elaborate DVD and Blu-ray Disc authoring options, and OnLocation for direct-to-disk recording. Encore was discontinued {{with the release of}} Adobe Creative Cloud.|$|E
50|$|Elements of {{a motion}} {{graphics}} project can be animated by various means, {{depending on the}} capabilities of the software. These elements may be in the form of art, text, photos, and video clips, to name a few. The most popular form of animation is <b>keyframing,</b> in which properties of an object can be specified at certain points in time by setting a series of keyframes so that the properties of the object can be automatically altered (or tweened) in the frames between keyframes. Another method involves a behavior system such as is found in Apple Motion that controls these changes by simulating natural forces without requiring the more rigid but precise <b>keyframing</b> method. Yet another method involves the use of formulas or scripts, such as the expressions function in Adobe After Effects or the creation of ActionScripts within Adobe Flash. Computers are capable of calculating and randomizing changes in imagery to create the illusion of motion and transformation. Computer animations can use less information space (computer memory) by automatically tweening, a process of rendering the key changes of an image at a specified or calculated time. These key poses or frames are commonly referred to as keyframes or low CP. Adobe Flash uses computer animation tweening as well as frame-by-frame animation and video.|$|E
5000|$|Character Studio was a plugin which since version 4 of Max is now {{integrated}} in 3D Studio Max; it helps users to animate virtual characters. The system works using a character rig or [...] "Biped" [...] skeleton which has stock settings {{that can be}} modified and customized to fit the character meshes and animation needs. This tool also includes robust editing tools for IK/FK switching, Pose manipulation, Layers and <b>Keyframing</b> workflows, and sharing of animation data across different Biped skeletons. These [...] "Biped" [...] objects have other useful features that help accelerate the production of walk cycles and movement paths, as well as secondary motion.|$|E
40|$|The {{acquisition}} of surround-view panoramas using a single handheld or head-worn camera relies on robust real-time camera orientation tracking. In absence of robust tracking recovery methods, the complete acquisition process {{has to be}} re-started when tracking fails. This paper presents methodology for camera orientation relocalization, using virtual <b>keyframes</b> for online environment map construction. Instead of relying on real <b>keyframes</b> from incoming video, the proposed approach enables camera orientation relocalization by employing virtual <b>keyframes</b> which are distributed strategically within an environment map. We discuss our insights about a suitable number and distribution of virtual <b>keyframes,</b> as suggested by our experiments on virtual <b>keyframe</b> generation and orientation relocalization. After a shading correction step, we relocalize camera orientation in real-time by comparing the current camera frame to virtual <b>keyframes.</b> While expanding the captured environment map, we continue to simultaneously generate virtual <b>keyframes</b> within the completed portion of the map, as descriptors to estimate camera orientation. We implemented our camera orientation relocalizer {{with the help of}} a GPU fragment shader for real-time application, and evaluated the speed and accuracy of the proposed approach...|$|R
30|$|To further justify {{our choice}} of KLT tracking, we compare our GPU-based KLT tracker with two {{different}} <b>keyframe</b> selection strategies. One is a fast frame intensity based <b>keyframe</b> selection algorithm: where each frame is represented as the concatenation of the integrated {{row and column}} pixel intensities; frame vectors are normalized to unit length; subsequent frame vector is compared against the previous <b>keyframe</b> vector, whenever significant changes are detected (Euclidean distance larger than 0.2) the current subsequent frame is selected as a new <b>keyframe.</b> The other method [18] explicitly evaluates frame-to-frame point correspondence sets as well as frame-to-frame epipolar geometries (homography and fundamental matrix), thus avoiding motion and structure degeneracy to select more robust <b>keyframes</b> for 3 D reconstruction purposes.|$|R
40|$|Near-duplicate <b>keyframe</b> {{retrieval}} is {{a critical}} task for video similarity measure, video threading and tracking. In this paper, instead of using expensive point-to-point matching on keypoints, we investigate the visual language models built on visual keywords {{to speed up the}} near-duplicate <b>keyframe</b> retrieval. The main idea is to estimate a visual language model on visual keywords for each <b>keyframe</b> and compare <b>keyframes</b> by the likelihood of their visual language models. Experiments on a subset of TRECVID- 2004 video corpus show that visual language models built on visual keywords demonstrate promising performance for near-duplicate <b>keyframe</b> retrieval, which greatly speed up the retrieval speed although sacrifice a little performance compared to expensive point-to-point matching. 1...|$|R
5000|$|On {{the bottom}} floor is an exhibit room showing {{the history and}} science of animation, {{including}} a three-dimensional zoetrope named [...] "Bouncing Totoro", with models of characters from My Neighbor Totoro (1988). On the first floor is a mock-up of an animation studio. Called [...] "Where a Film is Born," [...] the five-room exhibit is meant to showcase the creative process of an animation filmmaker such as illustration techniques. Packed with books and toys, the room also displays drawings and illustrations that cover the walls. Another exhibit demonstrates {{the process of creating}} an animated film, with sketches, storyboarding, <b>keyframing,</b> cleanup, coloring and background painting.|$|E
5000|$|... {{where the}} {{technology}} let us down {{was in the}} time and energy needed to process the motion-capture. We were capturing at 60 frames per second, and the characters in voxel space took up {{so much that we}} had to go through them, removing those frames by hand using <b>keyframing.</b> The original voxel objects weighed in at around seven megabytes in size while hardware at the time could manage only about 100K. Automatic optimisation would leave them unbearably blocky, leaving Westwood to do it by hand. Naturally, the studio didn't have time for 20,000 sequences, which is why some of the game's objects look plain bizarre. It just took too long.|$|E
5000|$|Node-based {{compositing}} {{represents an}} entire composite as a tree graph, linking media objects and effects in a procedural map, intuitively {{laying out the}} progression from source input to final output, and {{is in fact the}} way all compositing applications internally handle composites. This type of compositing interface allows great flexibility, including the ability to modify the parameters of an earlier image processing step [...] "in context" [...] (while viewing the final composite). Node-based compositing packages often handle <b>keyframing</b> and time effects poorly, as their workflow does not stem directly from a timeline, as do layer-based compositing packages. Software which incorporates a node based interface include Apple Shake, Blender, Blackmagic Fusion, and The Foundry's Nuke.|$|E
30|$|To {{build the}} {{proposed}} Bag-of-Iconic representation for videos, we first need to distill the temporal redundancy in the videos by selecting only <b>keyframes</b> (Section 3.1). Visually similar <b>keyframes</b> are then grouped together and each <b>keyframe</b> cluster represents some commonly captured visual entities or structures. An iconic image is selected to represent each <b>keyframe</b> cluster (Section 3.2). The set of representing iconic images, when viewed in aggregation, forms a “visual codebook” describing the captured visual contents. At individual video sequence level, it encodes how frequently each visual element {{occurs in a}} video, and it characterizes and summarizes the video’s content. To utilize the visual codebook, we perform geometric verification between the video <b>keyframes</b> and the iconic images to accumulate the histogram of iconic image occurrences (Section 3.3).|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe explosive growth of user-generated video presents opportunities and challenges. The videos may possess valuable {{information that was}} once unavailable. On the other hand, the information may be buried or difficult to access with traditional methods. Automatic <b>keyframe</b> video summarization technologies exist that attempt to address this problem. A <b>keyframe</b> summary can often be viewed quicker than the underlying video. However, a theoretical framework for objectively assessing <b>keyframe</b> summary quality has been absent. The work presented here bridges this gap by presenting a semantically high-level, stakeholder-centered evaluation frame-work. The framework can capture common stakeholder concerns and quantitatively measure {{the extent to which}} they are satisfied by <b>keyframe</b> summaries. With this framework, <b>keyframe</b> summary stakeholders and algorithm developers can now identify when success has been achieved. This work also develops a number of novel <b>keyframe</b> summarization algorithms and shows, using the evaluation framework, that they outperform baseline methods. Lieutenant, United States Nav...|$|R
40|$|Our {{experiments}} in TRECVID 2010 include {{participation in the}} semantic indexing and known-item search tasks. In the semantic indexing task we implemented SVM-based classifiers on five different low-level visual features extracted from the <b>keyframes.</b> In addition to the main <b>keyframes</b> provided by NIST, we also extracted and analysed additional frames from longer shots. The feature-wise classifiers were fused using standard and weighted geometric mean. We submitted the following four runs: • PicSOM_geom: Geometric mean of five features, all <b>keyframes.</b> • PicSOM_wgeom: Weighted geometric mean of five features, all <b>keyframes.</b> • PicSOM_ 2 geom-mkf: Geometric mean of two “best ” features, main <b>keyframe</b> only. • PicSOM_ 2 geom-max: Geometric mean of two “best ” features, all <b>keyframes.</b> The runs 2 geom-max and wgeom obtained the highest MIAP scores (with essentially the same score, 0. 0697 vs. 0. 0694). Overall, using more <b>keyframes</b> always improved the results substantially. Our weighting approach improved the result over the standard geometric mean. However, by using only two features in fusion without weighting we achieved a similar result. In the known-item search task we submitted two automatic and two interactive runs: • PicSOM_ 1 : Text search + concept detectors with distributio...|$|R
50|$|The body of {{work around}} {{computer}} facial animation can be divided in two main areas. Techniques to generate animation data and methods to apply such data to a character. Techniques such as motion capture and <b>keyframing</b> belong to the first group, while morph targets animation (more commonly known as blendshape animation) and skeletal animation belong to the second. Facial animation has become well-known and popular through animated feature films and computer games but its applications include many more areas such as communication, education, scientific simulation, and agent-based systems (for example online customer service representatives). With the recent advancements in computational power in personal and mobile devices, facial animation has transitioned from appearing in pre-rendered content to being created at runtime.|$|E
5000|$|Gmax is an {{application}} based on Autodesk's 3ds Max application used by professional computer graphics artists. 3ds Max {{is a comprehensive}} modeling, animation and rendering package with some secondary post-production and compositing features. Gmax is much more limited due to its singular intended use - game content creation. Infrequently used tools and features, or the ones completely unrelated to creating 3D game models, were removed (these include most, {{if not all of}} the more complex rendering, materials, shaders, physics simulation, some of the more advanced geometry tools, in addition to the rendering engine), leaving the core modeling, texturing, and basic animation rigging and <b>keyframing</b> capabilities. In 2005, the promotional freeware software was discontinued after version 1.2, but a licensed version is still available from [...]|$|E
5000|$|... 3D {{computer}} animation combines 3D models {{of objects and}} programmed or hand [...] "keyframed" [...] movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model {{is intended to be}} a solid color, it must be painted with [...] "textures" [...] for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as rigging, the virtual marionette is given various controllers and handles for controlling movement. Animation data can be created using motion capture, or <b>keyframing</b> by a human animator, or a combination of the two.|$|E
40|$|This paper {{describes}} a <b>keyframe</b> summarization method for client-server applications. This technique {{is designed for}} applications where a camera is collecting content on a continuous basis that must be transmitted in a summarized form to a remote database server over wireless network. The system combines three <b>keyframe</b> selection methods including a novel fast motion-based selection method, <b>keyframe</b> pooling and clustering for bandwidth control, and network bandwidth estimation...|$|R
40|$|We {{present a}} novel method to measure {{saliency}} in {{molecular dynamics simulation}} data. This saliency measure {{is based on a}} multiscale center-surround mechanism, which is fast and efficient to compute. We explore the use of the saliency function to guide the selection of representative and anomalous timesteps for summarization of simulations. To this end, we also introduce a multiscale <b>keyframe</b> selection procedure which automatically provides <b>keyframes</b> representing the simulation at varying levels of coarseness. We compare our saliency guided <b>keyframe</b> approach against other methods, and show that it consistently selects superior <b>keyframes</b> as measured by their predictive power in reconstructing the simulation...|$|R
40|$|<b>Keyframe</b> {{selection}} {{has been}} crucial for {{effective and efficient}} video content analysis. While most of the existing approaches represent individual frames with global features, we, for the first time, propose a keypoint-based framework to address the <b>keyframe</b> selection problem so that local features can be employed in selecting <b>keyframes.</b> In general, the selected <b>keyframes</b> should both be representative of video content and containing minimum redundancy. Therefore, we introduce two criteria, coverage and redundancy, based on keypoint matching in the selection process. Comprehensive experiments demonstrate that our approach outperforms {{the state of the}} art. Department of Electronic and Information Engineerin...|$|R
