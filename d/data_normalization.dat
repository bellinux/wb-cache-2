607|324|Public
2500|$|... data {{transformation}} (incorporating aspects such as <b>data</b> <b>normalization</b> and data analysis) – for example {{principal components analysis}} dimensionality reduction, mean calculation ...|$|E
2500|$|Microarray {{data sets}} are {{commonly}} very large, and analytical precision {{is influenced by}} a number of variables. Statistical challenges include taking into account effects of background noise and appropriate normalization of the <b>data.</b> <b>Normalization</b> methods may be suited to specific platforms and, in the case of commercial platforms, the analysis may be proprietary. Algorithms that affect statistical analysis include: ...|$|E
50|$|The {{formula is}} useful for correct {{approximation}} of samples data before <b>data</b> <b>normalization</b> procedure.|$|E
5000|$|... #Subtitle level 3: <b>Data</b> acquisition, <b>normalization</b> and {{cleansing}} ...|$|R
3000|$|... min are {{the maximum}} and minimum of {{original}} data, respectively. T {{is the target}} <b>data</b> after <b>normalization.</b>|$|R
5000|$|Method and Architecture for Method and {{architecture}} for <b>data</b> transformation, <b>normalization,</b> profiling, cleansing and validation [...] "Patent Application No. 10/635891 Pub. No. 20040083199." ...|$|R
50|$|In computing, the {{reduction}} of data {{to any kind of}} canonical form is commonly called <b>data</b> <b>normalization.</b>|$|E
5000|$|Clinical Intelligence - Identifies {{opportunities}} for better efficiency in companies’ clinical data by utilizing <b>data</b> <b>normalization</b> and cleansing ...|$|E
5000|$|... data {{transformation}} (incorporating aspects such as <b>data</b> <b>normalization</b> and data analysis) - for example {{principal components analysis}} dimensionality reduction, mean calculation ...|$|E
40|$|In this paper, {{we explore}} how US {{financial}} firms trade {{relative to their}} own equity analyst recommendations. In the quarter-of and that immediately follow a recommendation, firm trades are significantly positively related to recommendation changes. This relation is robust to controls for sub-sample effects, return/momentum related phenomena, size effects, controls for consensus recommendation, and other <b>data</b> <b>normalizations.</b> Our results show that financial firm trades are consistent with their analysts' research and recommendations, despite recent conjecture to the contrary. Institutional trading Equity analyst Recommendation...|$|R
50|$|Effective EDM usually {{includes}} the creation, documentation {{and enforcement of}} operating policies and procedures associated with change management, (i.e. data model, business glossary, master data shared domains, <b>data</b> cleansing and <b>normalization),</b> <b>data</b> stewardship, security constraints and dependency rules. In many cases, these policies and procedures are documented {{for the first time}} as part of the EDM initiative.|$|R
30|$|In Equation 2, I {{demonstrates}} the target function (amount of rotation in normalization space), c represents column matrix of input <b>data</b> in <b>normalization</b> space, w shows {{the amount of}} weight, and b exhibits the amount of bias.|$|R
50|$|Apply the {{normalization}} rules - Apply the <b>data</b> <b>normalization</b> {{rules to}} see if tables are structured correctly. Make adjustments to the tables, as needed.|$|E
5000|$|McGoveran, D. (2002). <b>Data</b> <b>Normalization.</b> In Riccardi, G. Database Management with Web Site Development Applications. (p. 155). Englewood Cliffs, NJ: Prentice-Hall. [...] & [...]|$|E
5000|$|Requires little data preparation. Other {{techniques}} {{often require}} <b>data</b> <b>normalization.</b> Since trees can handle qualitative predictors, {{there is no}} need to create dummy variables.|$|E
50|$|Finally, the <b>data</b> acquisition, <b>normalization,</b> and {{cleansing}} {{portion of}} SHM process {{should not be}} static. Insight gained from the feature selection process and the statistical model development process will provide information regarding changes that can improve the data acquisition process.|$|R
40|$|Description The main {{function}} biclust provides several algorithms to find biclusters in two-dimensional data: Cheng and Church, Spectral, Plaid Model, Xmotifs and Bimax. In addition, the package provides methods for <b>data</b> preprocessing (<b>normalization</b> and discretisation), visualisation, and validation of bicluster solutions...|$|R
50|$|Processes {{commonly}} seen in {{master data}} management include source identification, data collection, <b>data</b> transformation, <b>normalization,</b> rule administration, error detection and correction, data consolidation, data storage, data distribution, data classification, taxonomy services, item master creation, schema mapping, product codification, data enrichment and data governance.|$|R
5000|$|Remote Sensing and Photogrammetry, {{including}} ground, airborne and/or {{satellite observation}} systems, sensor and acquisition systems' technology, geometric and radiometric <b>data</b> <b>normalization,</b> and information extraction and fusion from data, ...|$|E
50|$|Feature scaling is {{a method}} used to {{standardize}} the range of independent variables or features of data. In data processing, it {{is also known as}} <b>data</b> <b>normalization</b> and is generally performed during the data preprocessing step.|$|E
5000|$|Microarray {{data sets}} are {{commonly}} very large, and analytical precision {{is influenced by}} a number of variables. Statistical challenges include taking into account effects of background noise and appropriate normalization of the <b>data.</b> <b>Normalization</b> methods may be suited to specific platforms and, in the case of commercial platforms, the analysis may be proprietary. Algorithms that affect statistical analysis include: ...|$|E
30|$|In general, {{there are}} many {{differences}} between the MIDI data and user's humming <b>data,</b> and <b>normalization</b> is required. As the first step, all zero value components obtained from the silent (muted) samples are removed. All the zero values in the humming pitch and MIDI data are removed [13 – 15, 17, 18].|$|R
40|$|Computer {{programs}} {{for the analysis of}} data from techniques frequently used in nucleic acids research are described. In addition to calculating non-linear least-squares solutions to equations describing these systems, the programs allow for <b>data</b> editing, <b>normalization,</b> plotting and storage, and are flexible and simple to use. Typical applications of the programs are described...|$|R
40|$|To my parents, To my family, And to my friends. ii ACKNOWLEDGEMENTS I {{would like}} to thank many of the {{researchers}} who worked alongside me through the long and painful process of <b>data</b> cleansing, <b>normalization,</b> and analyses. These individuals not only pushed me through the hardships but also enlightened me to fin...|$|R
50|$|Data {{processing}} includes <b>data</b> <b>normalization,</b> flagging of the data, averaging {{the intensity}} ratio for replicates, clustering of similarly expressed genes, etc. Data {{also must be}} normalized before further analysis. Normalization removes non-biological variation between the samples. After normalization, the intensity ratio is calculated for each gene in the replicate. Based on the ratio, the level of gene expression is determined. Quality control can then be performed.|$|E
50|$|Sapio {{has made}} {{increasing}} inroads into Biomarker discovery at large Biotechnology and Pharmaceutical firms {{with the introduction}} of its Exemplar Biomarker Discovery solution. This offering enables a single solution for managing clinical study data, treatment groups data, subject data, sample data and assay data from diverse assays such as Immunohistochemistry, Western Blot, PCR, etc. This collective data warehouse can then be interrogated using Exemplar's built-in data mining tools to get answers to complex questions regarding the Pharmacodynamics effects of a drug. Exemplar then integrates statistical analysis tools for doing common data processing such as <b>data</b> <b>normalization,</b> but also for doing exploratory analysis such as logistic regression, ANOVA and genetic algorithms for model building. The results of these analysis can lead to proper patient profiling for personalized medicine, and {{can mean the difference between}} success or failure for new or existing drugs.|$|E
50|$|Because {{data can}} be {{measured}} under varying conditions, the ability to normalize the data becomes {{very important to the}} damage identification process. As it applies to SHM, <b>data</b> <b>normalization</b> is the process of separating changes in sensor reading caused by damage from those caused by varying operational and environmental conditions. One of the most common procedures is to normalize the measured responses by the measured inputs. When environmental or operational variability is an issue, the need can arise to normalize the data in some temporal fashion to facilitate the comparison of data measured at similar times of an environmental or operational cycle. Sources of variability in the data acquisition process and with the system being monitored need to be identified and minimized to the extent possible. In general, not all sources of variability can be eliminated. Therefore, it is necessary to make the appropriate measurements such that these sources can be statistically quantified. Variability can arise from changing environmental and test conditions, changes in the data reduction process, and unit-to-unit inconsistencies.|$|E
40|$|Some 512 by 512 pixel subwindows for {{simultaneously}} acquired scene pairs {{obtained by}} LANDSAT 2, 3 and 4 multispectral band scanners were coregistered using LANDSAT 4 scenes {{as the base}} to which the other images were registered. Scattergrams between the coregistered scenes (a form of contingency analysis) were used to radiometrically compare data from the various sensors. Mode values were derived and used to visually fit a linear regression. Root mean square errors of the registration varied between. 1 and 1. 5 pixels. There appear to be no major problem preventing the use of LANDSAT 4 MSS with previous MSS sensors for change detection, provided the noise interference can be removed or minimized. <b>Data</b> <b>normalizations</b> for change detection {{should be based on}} the data rather than solely on calibration information. This allows simultaneous normalization of the atmosphere as well as the radiometry...|$|R
40|$|Numerical and {{graphical}} {{summaries of}} RNA-Seq read <b>data.</b> Within-lane <b>normalization</b> procedures {{to adjust for}} GC-content effect (or other gene-level effects) on read counts: loess robust local regression, global-scaling, and full-quantile normalization (Risso et al., 2011). Betweenlane normalization procedures to adjust for distributional differences between lanes (e. g., sequencing depth) : global-scaling and full-quantile normalization (Bullard et al., 2010) ...|$|R
40|$|Summary: arrayMagic is a {{software}} package for quality control and preprocessing of two-colour cDNA microarray data. The automated analysis pipeline comprises <b>data</b> import, <b>normalization,</b> replica merging, quality diagnostics and data export. The script-based processing combines reproducibility and flexibility at high-throughput and provides quality-assured and preprocessed microarray data to high-level follow-up analysis. Availability: The R package arrayMagic is available with BSD license a...|$|R
5000|$|METAGENassist is {{designed}} to support {{a wide range of}} statistical comparisons across metagenomic samples. METAGENassist accepts a wide range of bacterial census data or taxonomic profile data derived from 16S rRNA data, classical DNA sequencing, NextGen shotgun sequencing or even classical microbial culturing techniques. These taxonomic profile data can be in different formats including standard comma-separated value (CSV) formats or in program-specific formats generated by tools such as mothur [...] and QIIME. [...] Once the data are uploaded to the website, METAGENassist offers users a large selection of data pre-processing and data quality checking tools such as: 1) taxonomic name normalization; 2) taxonomic-to-phenotypic mapping; 3) data integrity/quality checks and 4) <b>data</b> <b>normalization.</b> METAGENassist also supports an extensive collection of classical univariate and multivariate analyses, such as fold-change analysis, t-tests, one-way ANOVA, partial least-squares discriminant analysis (PLS-DA) and principal component analysis (PCA). Each of these analyses generates colorful, informative graphs and tables in PNG or PDF formats. All of the processed data and images are also available for download. These data analysis and visualization tools can be used to visualize key features that distinguish or characterize microbial populations in different environments or in different conditions. METAGENassist distinguishes itself from most other metagenomics data analysis tools through its extensive use of automated taxonomic-to-phenotypic mapping and its ability to support sophisticated data analyses with the resulting phenotypic data. METAGENassist’s phenotype database covers more than 11,000 microbial species annotated with 20 different phenotypic categories, including oxygen requirements, energy source(s), metabolism, and GC content. This gives users substantially more features with which to compare and analyze different samples. The phenotype database is regularly updated with information retrieved from several resources including BacMap, GOLD, and other NCBI taxonomy resources.|$|E
5000|$|In an October 2004 Technology Review article, Muller {{discussed}} blog postings by McIntyre and McKitrick {{alleging that}} Mann, Bradley and Hughes {{did not do}} proper principal component analysis (PCA). In the article, Richard Muller stated:McIntyre and McKitrick obtained {{part of the program}} that Mann used, and they found serious problems. Not only does the program not do conventional PCA, but it handles <b>data</b> <b>normalization</b> in a way that {{can only be described as}} mistaken.Now comes the real shocker. This improper normalization procedure tends to emphasize any data that do have the hockey stick shape, and to suppress all data that do not. To demonstrate this effect, McIntyre and McKitrick created some meaningless test data that had, on average, no trends. This method of generating random data is called [...] "Monte Carlo" [...] analysis, after the famous casino, and it is widely used in statistical analysis to test procedures. When McIntyre and McKitrick fed these random data into the Mann procedure, out popped a hockey stick shape!That discovery hit me like a bombshell, and I suspect it is having the same effect on many others. Suddenly the hockey stick, the poster-child of the global warming community, turns out to be an artifact of poor mathematics. How could it happen?He went on to state [...] "If you are concerned about global warming (as I am) and think that human-created carbon dioxide may contribute (as I do), then you still should agree that we are much better off having broken the hockey stick. Misinformation can do real harm, because it distorts predictions." [...] In an article on the RealClimate blog on various myths about the graph, Mann mentioned Muller's article as parroting the claims of McIntyre and McKitrick. Muller's opinion piece in the reputable MIT journal helped to spread the idea that the hockey stick shape was a statistical artifact, but several peer reviewed studies showed that the PCA methodology had little effect on the shape of the graph. By 2006 there was general acceptance of the conclusion of the graph that recent warming was unprecedented in 1,000 years.|$|E
40|$|Abstract * In this paper, {{we discuss}} {{the design of a}} <b>data</b> <b>normalization</b> system that we term {{commonality}} factoring. A real-world implementation of a storage system based upon <b>data</b> <b>normalization</b> requires design of the <b>data</b> <b>normalization</b> itself, of the storage repository for the data, and of the protocols to be used between applications performing <b>data</b> <b>normalization</b> and the server software of the repository. Each of these areas is discussed and potential applications are presented. Building on research begun in 1999, Avamar Technologies has implemented an initial application of this technology to provide a nearline, disk-based system for backup of primary storage. 1...|$|E
40|$|Microarray {{experiments}} {{are part of}} a new class of biotechnologies which allow the monitoring of expression levels for thousands of genes simultaneously. The measurements generated by these studies are subject to multiple sources of experimental variation. Some of these variations are considered systematic and may be explicitly corrected through <b>data</b> <b>normalizations</b> with the objective of cleaning and improving the quality of the measures of gene expression. Usual approaches consider a limited class of non-parametric techniques such as the lowess adjustments. However, there are more adaptive non-parametric methodologies that may be adopted for the problem. In this article we apply splines smoothing for normalization of gene expression data from cDNA microarray experiments. The union/intersection metric is adopted for comparison of the different methods, which explores the measures under dye-swap slides. The analysis uses a small study, consisting of RNA samples, from four rat groups generated from two rat strains, hypertensive SHR and congenic SHR-BN rat strains, evaluated under controlled condition and after 2 week NaCl treatment. Collectively...|$|R
40|$|Motivation: LC-MS {{allows for}} the {{identification}} and quantification of proteins from biological samples. As with any high-throughput technology, systematic biases are often observed in LC-MS <b>data,</b> making <b>normalization</b> an important preprocessing step. Normalization models need to be flexible enough to capture biases of arbitrary complexity, while avoiding overfitting that would invalidate downstream statistical inference. Careful normalization of MS peak intensities would enable greater accuracy and precision in quantitative comparisons of protein abundance levels...|$|R
40|$|The {{measurements}} of coordinated patterns of protein abundance using antibody microarrays {{could be used}} to gain insight into disease biology and to probe the use of combinations of proteins for disease classification. The correct use and interpretation of antibody microarray <b>data</b> requires proper <b>normalization</b> of the <b>data,</b> which has not yet been systematically studied. Therefore we undertook a study to determine the optimal <b>normalization</b> of <b>data</b> from antibody microarray profiling of proteins in human serum specimens. Forty-three serum samples collected from patients with pancreatic cancer and from control subjects were probed in triplicate on microarrays containing 48 different antibodies, using a direct labeling, twocolor comparative fluorescence detection format. Seven different normalization methods representing major classes of normalization for antibody microarray data were compared by their effects on reproducibility, accuracy, and trends in the <b>data</b> set. <b>Normalization</b> with ELISAdetermined concentrations of IgM resulted in the most accurate, reproducible, and reliable <b>data.</b> The other <b>normalization</b> methods were deficient in {{at least one of the}} criteria. Multiparametric classification of the samples based on the combined measurement of seven of the proteins demonstrated the potential for increased classification accuracy compared with the use of individual measurements. This study establishes reliable normalization for antibody microarray data, criteria for assessing normalization performance, and the capability of antibody microarrays for serum-protein profiling and multiparametric sample classification. Molecular & Cellular Proteomic...|$|R
