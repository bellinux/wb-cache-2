8745|4149|Public
5|$|A new {{character}} in the film, Grawp, Hagrid's giant half-brother, came to life by a new technology called Soul Capturing, developed by Image Metrics. Instead of building the character from scratch, the movements and <b>facial</b> <b>expressions</b> of actor Tony Maudsley were used to model Grawp's actions.|$|E
5|$|Dyadic joint {{attention}} is a conversation-like behavior that individuals engage in. This {{is especially true}} for human adults and infants, who engage in this behavior starting at two months of age. Adults and infants take turns exchanging <b>facial</b> <b>expressions,</b> noises, {{and in the case of}} the adult, speech.|$|E
5|$|Lemur {{communication}} can {{be transmitted}} through sound, sight, and smell (olfaction). The ring-tailed lemur, for instance, uses complex though highly stereotyped behaviors such as scent-marking and vocalizations. Visual signals are probably the least used by lemurs, since they lack many of the muscles used in common primate <b>facial</b> <b>expressions.</b> Given their poor vision, whole-body postures are probably more noticeable. However, the ring-tailed lemur has demonstrated distinct <b>facial</b> <b>expressions</b> including a threat stare, pulled back lips for submission, and pulled back ears along with flared nostrils during scent-marking. This species has also been observed using yawns as threats. Their ringed tails also communicate distance, warn off neighboring troops, and help locate troop members. Sifakas are known to exhibit an open-mouth play face {{as well as a}} submissive teeth-baring grimace used in agonistic interactions.|$|E
40|$|The {{existing}} {{methods of}} <b>facial</b> <b>expression</b> recognition are typically {{based on the}} near-frontal face data. The analysis of non-frontal-view <b>facial</b> <b>expression</b> is a largely unexplored research. The accessibility to a recent 3 D <b>facial</b> <b>expression</b> database (BU- 3 DFE database) motivates us to explore an interesting question: whether non-frontal-view <b>facial</b> <b>expression</b> analysis can achieve the same as or better performance than the existing frontal-view <b>facial</b> <b>expression</b> method. Our extensive recognition experiments on data of 100 subjects with 5 yaw rotation view angles suggests that the non-frontal-view <b>facial</b> <b>expression</b> classification can outperform frontalview <b>facial</b> <b>expression</b> recognition, given the manually labeled facial key points. 1...|$|R
40|$|This paper {{introduces}} a new video-based <b>facial</b> <b>expression</b> recognition system. <b>Facial</b> <b>expression</b> analysis encounters two major problems: non-rigid shape deformation and person-specific <b>facial</b> <b>expression</b> appearance. Our method analyzes the video sequence to recognize <b>facial</b> <b>expression</b> and locate the temporal {{apex of the}} <b>facial</b> <b>expression</b> by using modified Hough forest and minimizing the influence of person-specific <b>facial</b> <b>expression</b> appearance. Our contributions are (1) random sampling 3 -D accumulated spatial-temporal motion map to generate video patches, (2) proposing the correlation filtering for more effective Hough voting, and (3) recognizing and locating {{the apex of the}} <b>facial</b> <b>expression.</b> The experimental results show that the performance of our method is better than the other face expression recognition methods...|$|R
40|$|<b>Facial</b> <b>expression</b> interpretation, {{recognition}} and analysis {{is a key}} issue in visual communication and man to machine interaction. In this paper, we present a factorization technique which decomposes the appearance parameters coding a natural image. This technique is then used to perform <b>facial</b> <b>expression</b> synthesis on unseen faces showing any undetermined <b>facial</b> <b>expression,</b> as well as <b>facial</b> <b>expression</b> recognition...|$|R
5|$|The pons {{lies in the}} {{brainstem}} {{directly above}} the medulla. Among other things, it contains nuclei that control often voluntary but simple acts such as sleep, respiration, swallowing, bladder function, equilibrium, eye movement, <b>facial</b> <b>expressions,</b> and posture.|$|E
5|$|Hiroyuki Okiura, the {{character}} designer and key animation supervisor, designed a more mature and serious Motoko than Masamune Shirow's original portrayal of {{the character}} in the manga. Okiura chose to depict a physically mature person to match Motoko's mental age, instead of the youthful twenty-something appearance in the manga. Motoko's demeanor lacks the comedic <b>facial</b> <b>expressions</b> and rebellious nature depicted in the manga.|$|E
5|$|American Sign Language {{is growing}} in {{popularity}} among many states. Many people {{in high school and}} colleges wanting to take it as a foreign language, but until recently, it was not a creditable foreign language elective. The issue was that many didn't consider it a foreign language. ASL users, however, have a very distinct culture and way they interact when talking. Their <b>facial</b> <b>expressions</b> and hand movements reflect what they are conveying. They also have their own sentence structure which sets the language apart.|$|E
40|$|The {{performance}} of an automatic <b>facial</b> <b>expression</b> recognition {{system can be}} significantly improved by modeling the reliability of different streams of <b>facial</b> <b>expression</b> information utilizing multi-stream hidden Markov models (HMMs). In this paper, we present an automatic multi-stream HMM <b>facial</b> <b>expression</b> recognition system and analyze its performance. The proposed system utilizes facial animation parameters (FAPs), supported by the MPEG- 4 standard, as features for <b>facial</b> <b>expression</b> classification. Specifically, the FAPs describing {{the movement of the}} outer-lips and eyebrows are used as observations. Experiments are first performed employing single-stream HMMs under several different scenarios, utilizing outer-lip and eyebrow FAPs individually and jointly. A multi-stream HMM approach is proposed for introducing <b>facial</b> <b>expression</b> and FAP group dependent stream reliability weights. The stream weights are determined based on the <b>facial</b> <b>expression</b> recognition results obtained when FAP streams are utilized individually. The proposed multi-stream HMM <b>facial</b> <b>expression</b> system, which utilizes stream reliability weights, achieves relative reduction of the <b>facial</b> <b>expression</b> recognition error of 44 % compared to the single-stream HMM system. Index Terms- <b>facial</b> <b>expression</b> recognition, multi-stream HMMs, facial animation parameter...|$|R
40|$|Abstract. In this paper, <b>facial</b> <b>expression</b> {{recognition}} {{as the starting}} point, it is extracted the mixing characteristics of the <b>facial</b> <b>expression,</b> including geometric and texture features. It can {{solve the problem of}} the collection contains the degree based on Variable Precision Fuzzy Rough Set, It could improved the accuracy of <b>facial</b> <b>expression</b> recognition using the method, thus making the <b>facial</b> <b>expression</b> recognition process more accurate and efficient...|$|R
40|$|Keywords:facial {{expression}} recognition; LBP; SVM decision tree; histogram Abstract. In {{order to}} improve the recognition of <b>facial</b> <b>expression</b> recognition rate, <b>facial</b> <b>expression</b> recognition algorithm {{is based on a}} LBP and SVM decision tree. First will <b>facial</b> <b>expression</b> image is converted to LBP characteristic spectrum using LBP algorithm, and then the LBP characteristic spectrum into LBP histogram feature sequence, finally completes the classification and recognition of <b>facial</b> <b>expression</b> by SVM decision tree algorithm, and prove the effectiveness of the proposed method in the recognition of <b>facial</b> <b>expression</b> database in JAFFE...|$|R
5|$|On 18 March 2015, {{the first}} image capture {{from the film}} was released, {{featuring}} Andrea, Ranjith and Mohanlal on location in Calicut. The first-look poster featuring Mohanlal was released on 29 June 2015. More detailed posters were released {{during the first week}} of August. The first poster showed an ensemble cast with Mohanlal in the centre; their <b>facial</b> <b>expressions</b> indicative of the film's thriller genre. Other posters showed Mohanlal wearing army-style camouflage pants.|$|E
5|$|Square's aim {{with the}} title was to create {{some of the most}} {{realistic}} wrestlers seen in video games, with accurate details such as a faithful replication of body-muscle physics and <b>facial</b> <b>expressions.</b> Screenshots of the game were shown during the Square Millennium Event in Japan in 2000; and a battle was played in demonstration by Yusuke Hirata and a young Japanese idol later in the same year at the Tokyo Game Show.|$|E
5|$|The patient's own {{description}} {{is the best}} measure of pain; they will usually be asked to estimate intensity {{on a scale of}} 0–10 (with 0 being no pain and 10 being the worst pain they have ever felt). Some patients, however, may be unable to give verbal feedback about their pain. In these cases you must rely on physiological indicators such as <b>facial</b> <b>expressions,</b> body movements, and vocalizations such as moaning.|$|E
40|$|In this paper, {{we propose}} a {{visualization}} method of music impression in <b>facial</b> <b>expression</b> to represent emotion. We apply <b>facial</b> <b>expression</b> {{to represent the}} complicated and mixed emotions. This method can generate <b>facial</b> <b>expression</b> corresponding to impres-sions of music data by measurement of relationship between each basic emotion for <b>facial</b> <b>expression</b> and impressions extracted from music data. The feature of this method is a realization of an integration be-tween music data and the <b>facial</b> <b>expression</b> that con-vey various emotions effectively. One of the important issues is a realization of communication media corre-sponding to human Kansei with less difficulty for a user. <b>Facial</b> <b>expression</b> can express complicated emo-tions with which various emotions are mixed. Assum-ing that an integration between existing mediadata and <b>facial</b> <b>expression</b> is possible, visualization corre-sponding to human Kansei with less difficulty realized for a user...|$|R
40|$|Abstract—The {{performance}} of an automatic <b>facial</b> <b>expression</b> recognition {{system can be}} significantly improved by modeling the reliability of different streams of <b>facial</b> <b>expression</b> information utilizing multistream hidden Markov models (HMMs). In this paper, we present an automatic multistream HMM <b>facial</b> <b>expression</b> recognition system and analyze its performance. The proposed system utilizes facial animation parameters (FAPs), supported by the MPEG- 4 standard, as features for <b>facial</b> <b>expression</b> classification. Specifically, the FAPs describing {{the movement of the}} outer-lip contours and eyebrows are used as observations. Experiments are first performed employing single-stream HMMs under several different scenarios, utilizing outer-lip and eyebrow FAPs individually and jointly. A multistream HMM approach is proposed for introducing <b>facial</b> <b>expression</b> and FAP group dependent stream reliability weights. The stream weights are determined based on the <b>facial</b> <b>expression</b> recognition results obtained when FAP streams are utilized individually. The proposed multistream HMM <b>facial</b> <b>expression</b> system, which utilizes stream reliability weights, achieves relative reduction of the <b>facial</b> <b>expression</b> recognition error of 44 % compared to the single-stream HMM system. Index Terms—Facial expression recognition, multistream HMMs, facial animation parameters. I...|$|R
40|$|We {{propose a}} {{framework}} for estimation and analysis of temporal <b>facial</b> <b>expression</b> patterns of a speaker. The proposed system aims to learn personalized elementary dynamic <b>facial</b> <b>expression</b> patterns for a particular speaker. We use head-and-shoulder stereo video sequences to track lip, eye, eyebrow, and eyelid motion of a speaker in 3 D. MPEG- 4 Facial Definition Parameters (FDPs) are used as the feature set, and temporal <b>facial</b> <b>expression</b> patterns are represented by the MPEG- 4 Facial Animation Parameters (FAPs). We perform Hidden Markov Model (HMM) based unsupervised temporal segmentation of upper and lower <b>facial</b> <b>expression</b> features separately to determine recurrent elementary <b>facial</b> <b>expression</b> patterns for a particular speaker. These <b>facial</b> <b>expression</b> patterns coded by FAP sequences, {{which may not be}} tied with prespecified emotions, can be used for personalized emotion estimation and synthesis of a speaker. Experimental results are presented. Index Terms — dynamic <b>facial</b> <b>expression</b> analysis, temporal patterns 1...|$|R
5|$|Cave 2 {{is above}} {{and to the}} east of Cave1 and faces north. It was created in late 6th or early 7th century. It is smaller than Cave 1, {{somewhat}} similar in terms of its floor plan, but it is dedicated primarily to Vishnu. Cave2 is reached by climbing 64 steps from the first cave. The cave entrance is a verandah divided by four square pillars with ends as half pillars, all carved out of the monolithic stone face. The pillars have decorative carvings with frieze of ganas (mythical dwarfs) with various <b>facial</b> <b>expressions.</b> On {{the two sides of the}} entrance are standing dvarapalas (guardians) holding flowers, not weapons. Like Cave1, Cave 2 art reflects Hindu theology and arts.|$|E
5|$|Glasgow based {{animation}} studio, Axis Animation, assisted 343 Industries on {{the creation}} of CGI cinematics for the Spartan Ops game mode. Axis have produced animations for numerous video games, including the award-winning announcement trailer for Dead Island. To make each Spartan Ops episode as engaging as possible the team at Axis shot and edited episodes in live action using performance capture. The team then used the reference cameras at the live action shoot to create a performance edit, before shooting with CG cameras to provide more coverage of all scenes and a greater selection of shots for the editorial team. The shading team at Axis made use of the 3D animation package, Houdini, to procedurally generate the environment in the cinematics. Axis worked with Glasgow-based audio post production company Savalas on sound design and the final mix for the Spartan Ops cinematics. Facial motion capture was also utilized to take the movements and <b>facial</b> <b>expressions</b> from actors and apply them to the in-game cinematics for both Spartan Ops and campaign. Performance capture for both campaign and Spartan Ops cutscenes was directed and recorded at Giant Studios. Axis worked in conjunction with Giant Studios and Cubic Motion to develop a special facial motion capture solution that would retain <b>facial</b> <b>expressions</b> from the actors when creating the animation.|$|E
5|$|Like New World monkeys, strepsirrhines rely on scent marking {{for much}} of their communication. This {{involves}} smearing secretions from epidermal scent glands on tree branches, along with urine and feces. In some cases, strepsirrhines may anoint themselves with urine (urine washing). Body postures and gestures may be used, although the long snout, non-mobile lips, and reduced facial enervation restrict the use of <b>facial</b> <b>expressions</b> in strepsirrhines. Short-range calls, long-range calls, and alarm calls are also used. Nocturnal species are more constrained {{by the lack of}} light, so their communication systems differ from those of diurnal species, often using long-range calls to claim their territory.|$|E
40|$|Automatic <b>facial</b> <b>expression</b> {{recognition}} {{has attracted}} significant attention {{over the past}} decades. Although substantial progress has been achieved for certain scenarios (such as frontal faces in strictly controlled laboratory settings), accurate recognition of <b>facial</b> <b>expression</b> in realistic environments remains unsolved for the most part. The main objective of this thesis is to investigate <b>facial</b> <b>expression</b> recognition in unconstrained environments. As one major problem faced by the literature {{is the lack of}} realistic training and testing data, this thesis presents a web search based framework to collect realistic <b>facial</b> <b>expression</b> dataset from the Web. By adopting an active learning based method to remove noisy images from text based image search results, the proposed approach minimizes the human efforts during the dataset construction and maximizes the scalability for future research. Various novel <b>facial</b> <b>expression</b> features are then proposed to address the challenges imposed by the newly collected dataset. Finally, a spectral embedding based feature fusion framework is presented to combine the proposed <b>facial</b> <b>expression</b> features to form a more descriptive representation. This thesis also systematically investigates how the number of frames of a <b>facial</b> <b>expression</b> sequence can affect the performance of <b>facial</b> <b>expression</b> recognition algorithms, since <b>facial</b> <b>expression</b> sequences may be captured under different frame rates in realistic scenarios. A <b>facial</b> <b>expression</b> keyframe selection method is proposed based on keypoint based frame representation. Comprehensive experiments have been performed to demonstrate the effectiveness of the presented methods...|$|R
40|$|Abstract: In social interaction, the <b>facial</b> <b>expression</b> of an {{opponent}} contains {{information that may}} influence the interaction. We asked whether <b>facial</b> <b>expression</b> affects decision-making in the ultimatum game. In this two-person game, the proposer divides a sum of money into two parts, one for each player, and then the responder decides whether to accept the or reject it. Rejection means that neither player gets any money. Results of a large-sample study support our hypothesis that offers from proposers with a smiling <b>facial</b> <b>expression</b> are more often accepted, compared to a neutral <b>facial</b> <b>expression.</b> Moreover, we found lower acceptance rates for offers from proposers with an angry <b>facial</b> <b>expression...</b>|$|R
5000|$|Facial muscle {{weakness}} (eyelid drooping, inability to whistle, decreased <b>facial</b> <b>expression,</b> depressed or angry <b>facial</b> <b>expression,</b> difficulty pronouncing the letters M, B, and P) ...|$|R
5|$|On December 9, 2010, Revolution Software {{announced}} {{the release of}} Broken Sword: The Smoking Mirror - Remastered on iOS devices, and was released on December 16, 2010. The new features include an exclusive interactive digital comic from Dave Gibbons, fully animated <b>facial</b> <b>expressions,</b> enhanced graphics, high quality music, a context-sensitive hint system, diary, and a Dropbox integration which facilitates a unique cross-platform save-game feature, enabling players to enjoy the same adventure simultaneously on multiple devices. It also featured full Game Center integration – including in-game achievements. The Mac and PC versions followed in early 2011.|$|E
5|$|Preliminary work on L.A. Noire {{began after}} the {{founding}} of developer Team Bondi in 2003. Though Team Bondi oversaw development, work was shared between the core team and multiple other studios owned by publisher Rockstar Games. Unlike other games published by Rockstar, L.A. Noire uses a custom engine, which includes a combination of facial motion capture and animation software. The game also uses MotionScan to capture actor's <b>facial</b> <b>expressions.</b> BBC News reporter Kev Geoghegan estimated that the development budget for the game exceeded US$50 million, making L.A. Noire {{one of the most}} expensive video games ever made.|$|E
5|$|The advancements in portraying {{realistic}} emotions {{achieved with}} Final Fantasy X through voice-overs and detailed <b>facial</b> <b>expressions</b> have since become {{a staple of}} the series, with Final Fantasy X-2 and other subsequent titles (such as , Final Fantasy XII, XIII and its sequels, and XV) also featuring this development. Traversing real-time 3D environments instead of an overworld map has also become a standard of the series, as demonstrated in Final Fantasy XI, XII, XIII and its sequels, XIV and XV. Final Fantasy X can be considered a pioneer in 3-D RPG maps.|$|E
40|$|This paper proposes the {{architecture}} of learning companion agent with <b>facial</b> <b>expression</b> of emotion. Based on ABC and ToK architecture, the emotion agent architecture contains five modules to realize the interaction in the world. A particular part {{of this research is}} the transition between emotion space in emotion module and <b>facial</b> <b>expression</b> space in <b>facial</b> <b>expression</b> module. Using this transition and those five modules, a small experiment website with emotion agent demonstrates the <b>facial</b> <b>expression</b> of the agent and its emotion tracks. 1...|$|R
40|$|Human <b>facial</b> <b>expression</b> {{is one of}} {{the most}} powerful, natural and {{immediate}} means for communication between each other. Automatic human <b>facial</b> <b>expression</b> recognition is challenging, interesting problem in many areas such as human computer interaction and data driven animation etc. In this paper, <b>Facial</b> <b>expression</b> based on Local Binary Pattern (LBP) is evaluated, “curse of dimensionality ” for real world scenarios problem solved by dimensionality reduction using Local Fisher Discriminant Analysis (LFDA) and Sparse representation classifier (SRC) used for efficient <b>facial</b> <b>expression</b> classification. The experiment is performed in both person-independent and person-dependent <b>facial</b> <b>expression</b> recognition cases, on Japanese Female <b>Facial</b> <b>Expression</b> (JAFFE) and observed that LBP features perform stably and robustly over useful range of low resolutions of face images (150 by 110 pixel and 64 by 64 pixel size). Proposed method shows better result than traditional algorithms such a...|$|R
40|$|Research into <b>facial</b> <b>expression</b> {{recognition}} has predominantly {{been based}} upon near frontal view data. However, a recent 3 D <b>facial</b> <b>expression</b> database (BU- 3 DFE database) has allowed empirical investigation of <b>facial</b> <b>expression</b> recognition across pose. In this paper, we investigate {{the effects of}} pose from frontal to profile view on <b>facial</b> <b>expression</b> recognition. Experiments are carried out on 100 subjects with 5 yaw angles over 6 prototypical expressions. Expressions have 4 levels of intensity from subtle to exaggerated. We evaluate features such as local binary patterns (LBPs) as well as various extensions of LBPs. In addition, a novel approach to <b>facial</b> <b>expression</b> recognition is proposed using local gabor binary patterns (LGBPs). Multi class support vector machines (SVMs) are used for classification. We investigate the effects of image resolution and pose on <b>facial</b> <b>expression</b> classification {{using a variety of}} different features. ...|$|R
5|$|The game {{is notable}} {{for being the}} first to use the newly {{developed}} MotionScan technology developed by Depth Analysis. MotionScan uses 32 surrounding cameras to capture actors' <b>facial</b> <b>expressions</b> from every angle, resulting in a highly realistic recreation of a human face. The technology is central to the game's interrogation mechanic, as players are required to use the suspects' reactions to questioning to judge {{whether or not they are}} lying. The game uses full motion capture actors to record the voices and movements of the characters. Over twenty hours of voice work was recorded for the game.|$|E
5|$|The {{game was}} {{developed}} on an in-house 3D game engine under construction for multiple years. Each character {{is made of}} over 7000 polygons and 55 bones, making for players with <b>facial</b> <b>expressions,</b> over 800 animations, and a capacity to blink. Character faces can additionally express emotions like happiness or anger, and feelings of pain. The move to Xbox led to greater variation in the team personalities. The artists drew many options for each team and the developers chose from the lot. Deathrow was designed for the Action camera view, but Sports view was introduced to expand the game's appeal.|$|E
5|$|The {{gameplay}} for Anachronox is turn-based; {{the player}} controls {{a party of}} up to three characters as they explore a 3D environment of futuristic cities, space vessels, and outdoor areas. Inspirations for the game include older role-playing video games such as Chrono Trigger and the Final Fantasy series, animator Chuck Jones and the novel Ender's Game. The game was built with a heavily {{modified version of the}} Quake II engine, rewritten chiefly to allow a wider color palette, emotive animations and <b>facial</b> <b>expressions,</b> better lighting, particle, and camera effects.|$|E
40|$|This paper {{presents}} a new method for <b>facial</b> <b>expression</b> modelling and recognition based on diffeomorphic image registration parameterised via stationary velocity fields in Log-Euclidean framework. The validation and comparison are done using different statistical shape models (SSM) built using the Point Distribution Model (PDM), velocity fields, and deformation fields. The obtained {{results show that}} the <b>facial</b> <b>expression</b> representation based on stationary velocity field can be successfully utilised in <b>facial</b> <b>expression</b> recognition, and this parameterisation produces higher recognition rate than the <b>facial</b> <b>expression</b> representation based on deformation fields...|$|R
40|$|This paper {{proposes a}} novel natural <b>facial</b> <b>expression</b> {{recognition}} method that recognizes {{a sequence of}} dynamic <b>facial</b> <b>expression</b> images using the differential active ap-pearance model (AAM) and k-NNS as follows. First, we use the differential-AAM features (DAFs) that are computed from the difference of the AAM parameters between an in-put face image and a reference face image. Second, we per-form the manifold learning. Third, we recognize the <b>facial</b> <b>expression</b> of the input face image in the embedded fea-ture space using sequence based k-NN, k-NNS. Since we use DAFs, we also propose an effective way of finding the neutral <b>facial</b> <b>expression</b> as kernel density approximation. Experimental results show that (1) the DAFs improves the <b>facial</b> <b>expression</b> recognition performance than the conven-tional AAM features by 20 % and (2) the sequence-based k-nearest neighbors classifier provides a 95 % of facial ex-pression recognition performance on the <b>facial</b> <b>expression</b> database (FED 06). 1...|$|R
50|$|The muscles {{acting on}} the lips are {{considered}} part of the muscles of <b>facial</b> <b>expression.</b> All muscles of <b>facial</b> <b>expression</b> are derived from the mesoderm of the second pharyngeal arch, and are therefore supplied (motor supply) by the nerve of the second pharyngeal arch, the facial nerve (7th cranial nerve). The muscles of <b>facial</b> <b>expression</b> are all specialized members of the panniculus carnosus, which attach to the dermis and so wrinkle, or dimple the overlying skin. Functionally, the muscles of <b>facial</b> <b>expression</b> are arranged in groups around the orbits, nose and mouth.|$|R
