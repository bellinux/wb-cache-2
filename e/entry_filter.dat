0|54|Public
50|$|Artificial Reading Intelligence is an {{automatic}} extraction of meaningful text from a webpage. Meaningful text is read while skipping over superfluous {{information such as}} long lists of menu items, photo captions on advertisements and other data points that {{are not part of}} the story or blog <b>entry.</b> <b>Filtered</b> text is then voiced by NeoSpeech Text to Speech engine.|$|R
2500|$|I've {{noticed a}} {{tendency}} for acronyms to proliferate within DoD. Is there a DoD clearinghouse/arbitrator through whom these <b>entries</b> are <b>filtered?</b> ...|$|R
50|$|A VPN Server {{can have}} several Virtual Hubs and Virtual Layer-3 Switches. A Virtual Hub has full layer-2 Ethernet packet-switching {{functions}} like a physical Ethernet switch. Additionally, a Virtual Hub can be configured to define IP packet <b>filter</b> <b>entries</b> to <b>filter</b> the packets through the Virtual Hub. A Virtual Layer-3 Switch has layer-3 IP static routing functions like a physical router.|$|R
5000|$|EMC Products, {{including}} <b>filtered</b> Power <b>Entry</b> Modules, EMI/RFI <b>Filters,</b> Choke Inductors and Pulse Transformers ...|$|R
30|$|In {{this example}} {{there is a}} sensor, LogSensor and two actors: join and report. LogSensor {{periodically}} checks for new log <b>entries,</b> <b>filter</b> the relevant log type and passes it to join. join extracts the ID from the new log and makes an API call to a document store to ascertain if another log entry with the same ID exits. If it does, it concatenates the two entries and pushes the result to the document store. If not, it commits the partial entry to the document store and awaits the next entry. report is called {{in conjunction with the}} aggregate preprocessor to enumerate and report to a service the number of log entries concatenated within the 10 min window.|$|R
5000|$|To demonstrate, using {{a window}} size of three with one entry {{immediately}} preceding and following each <b>entry,</b> a median <b>filter</b> {{will be applied}} to the following simple 1D signal: ...|$|R
40|$|In {{this paper}} we present the {{language}} modeling approach to information retrieval as a toolbox to systematically combine information from dierent sources. Four TREC subtasks (Ad Hoc, <b>Entry</b> Page, Adaptive <b>Filtering</b> and Cross-language) {{are used to}} illustrate the application of language models to dierent information retrieval problems...|$|R
40|$|International audienceAs genomes, transcriptomes and meta-genomes {{are being}} sequenced {{at a faster}} pace than ever, there is a {{pressing}} need for efficient genome assembly methods. Two practical issues in assembly are heavy memory usage and long execution time during the read indexing phase. In this article, a parallel and memory-efficient method is proposed for reads indexing prior to assembly. Specifically, a hash-based structure that stores a reduced amount of read information is designed. Erroneous <b>entries</b> are <b>filtered</b> on the fly during index construction. A prototype implementation has been designed and applied to actual Illumina short reads. Benchmark evaluation shows that this indexing method requires significantly less memory than those from popular assemblers...|$|R
40|$|Word-level {{translational}} equivalences can {{be extracted}} from parallel texts by surprisingly simple statistical techniques. However, these techniques are easily fooled by indirect associations [...] - pairs of unrelated words whose statistical properties {{resemble those of}} mutual translations. Indirect associations pollute the resulting translation lexicons, drastically reducing their precision. This paper presents an iterative lexicon cleaning method. On each iteration, {{most of the remaining}} incorrect lexicon <b>entries</b> are <b>filtered</b> out, without significant degradation in recall. This lexicon cleaning technique can produce translation lexicons with recall and precision both exceeding 90 %, as well as dictionary-sized translation lexicons that are over 99 % correct. 1 Introduction Translation lexicons are explicit representations of translational equivalence at the word level. They are central to any machine translation system, and {{play a vital role in}} other multilingual applications, including [...] ...|$|R
40|$|AbstractMicrosomal {{prostaglandin}} E 2 synthase- 1 (mPGES- 1) inhibitors {{are considered}} as potential therapeutic agents {{for the treatment}} of inflammatory pain and certain types of cancer. So far, several series of acidic as well as non-acidic inhibitors of mPGES- 1 have been discovered. Acidic inhibitors, however, may have issues, such as loss of potency in human whole blood and in vivo, stressing the importance of the design and identification of novel, non-acidic chemical scaffolds of mPGES- 1 inhibitors. Using a multistep virtual screening protocol, the Vitas-M compound library (∼ 1. 3 million <b>entries)</b> was <b>filtered</b> and 16 predicted compounds were experimentally evaluated in a biological assay in vitro. This approach yielded two molecules active in the low micromolar range (IC 50 values: 4. 5 and 3. 8 μM, respectively) ...|$|R
40|$|To analyze {{large amounts}} of data, visual {{analysis}} tools offer filter mechanisms for drilling down into multi-dimensional information spaces, or slicing and dicing them according to given criteria. This paper introduces an analysis approach for navigating multi-dimensional process instance execution logs based on business process models. By visually selecting parts of a business process model, a set of available log <b>entries</b> is <b>filtered</b> to include only those entries that result from execution instances of the selected process branches. Using this approach allows to exploratively navigate through process execution logs and analyze them according to the causal-temporal relationships encoded in the underlying business process model. The business process models used by the approach can either be created using model editors, or be statistically derived using process mining techniques. We exemplify our approach with a prototypical implementation...|$|R
40|$|Abstract Parameter {{estimation}} plays {{a dominant}} {{role in a}} wide number of image processing and computer vision tasks. In these settings, parameterizations can be {{as diverse as the}} application areas. Examples of such parameters are the <b>entries</b> of <b>filter</b> kernels optimized for a certain criterion, image features such as the velocity field, or part descriptors or compositions thereof. Subsequently, approaches for estimating these parameters encompass a wide range of techniques, often tuned to the application, the underlying data and viable assumptions. Here, an overview of parameter estimation in image processing and computer vision will be given. Due to the wide and diverse areas in which parameter estimation is applicable, this review does not claim completeness. Based on selected key topics in image processing and computer vision we will discuss parameter estimation, its relevance, and give an overview over the techniques involved. ...|$|R
40|$|Here we {{incorporate}} a four-dimensional filter line search method into an infeasible primal-dual interior point framework for nonlinear programming. Each <b>entry</b> in the <b>filter</b> has four components measuring dual feasibility, complementarity, primal feasibility and optimality. Three measures arise {{directly from the}} first order optimality conditions {{of the problem and}} the fourth is the objective function, so that convergence to a stationary point that is a minimizer is guaranteed. The primary assessment of the method has been done with a well-known collection of small problems...|$|R
40|$|Here {{we present}} a primal-dual {{interior}} point method that relies on a filter line search method to promote global convergence of nonlinear optimization problems. Each <b>entry</b> in the <b>filter</b> includes two components directly taken from the first-order optimality conditions of the problem. Primal feasibility, complementarity and dual feasibility measures are used to compose the components. The logarithmic barrier function {{is also used to}} promote convergence to minimizers. The algorithm is tested with a benchmark set of nonlinear problems. Fundação para a Ciência e a Tecnologi...|$|R
40|$|International audienceOne of {{the main}} {{resources}} used for the task of bilingual lexicon extraction from comparable corpora is : the bilingual dictionary, which is considered as a bridge between two languages. However, no particular {{attention has been given}} to this lexicon, except its coverage, {{and the fact that it}} can be issued from the general language, the specialized one, or a mix of both. In this paper, we want to highlight the idea that a better consideration of the bilingual dictionary by studying its <b>entries</b> and <b>filtering</b> the non-useful ones, leads to a better lexicon extraction and thus, reach a higher precision. The experiments are conducted on a medical domain corpora. The French-English specialized corpus 'breast cancer' of 1 million words. We show that the empirical results obtained with our filtering process improve the standard approach traditionally dedicated to this task and are promising for future work...|$|R
40|$|Digitarium {{is a joint}} {{initiative}} of the Finnish Museum of Natural History and the University of Eastern Finland. It was established in 2010 as a dedicated shop for the large-scale digitisation of natural history collections. Digitarium offers service packages based on the digitisation process, including tagging, imaging, data <b>entry,</b> georeferencing, <b>filtering,</b> and validation. During the process, all specimens are imaged, and distance workers {{take care of the}} data entry from the images. The customer receives the data in Darwin Core Archive format, as well as images of the specimens and their labels. Digitarium also offers the option of publishing images through Morphbank, sharing data through GBIF, and archiving data for long-term storage. Service packages can also be designed on demand to respond to the specific needs of the customer. The paper also discusses logistics, costs, and intellectual property rights (IPR) issues related to the work that Digitarium undertakes...|$|R
40|$|Abstract: Here {{we present}} a {{performance}} evaluation of three versions of a primal-dual interior point filter line search method for nonlinear programming. Each <b>entry</b> in the <b>filter</b> relies on three components, the feasibility, centrality and optimality, that {{are present in the}} first-order optimality conditions. The versions differ in a set of acceptance conditions that are used to consider a trial iterate to be acceptable to the filter. Performance profiles are used to compare the obtained numerical results {{in terms of the number}} of iterations and the number of the optimality measure evaluations...|$|R
40|$|Abstract: The {{authentication}} {{based on}} biometric information has several advantages compared to solely password-based systems, {{which has led}} to a growing interest of in-dustry and of public authorities in biometric-based systems. To meet the high security standards concerning biometric data, template protection systems such as the fuzzy vault are indispensable to maintain the secrecy of the critical information. Several publications have discussed the application of fuzzy vault to fingerprint authentica-tion systems. However, for identification purposes in large databases the fuzzy vault protection of the biometric reference data poses severe efficiency challenges. In this work, we examine and compare the performance of three different ap-proaches to enable the identification based on protected fingerprint minutiae templates also for large databases. All three approaches calculate a prioritization of the database <b>entries</b> exploiting <b>filtering</b> techniques and indexing structures. Based on this prior-itization a search performing the complex exact comparison of database and query template is steered and thus will faster find a match. ...|$|R
40|$|We {{present a}} primal-dual {{interior}} point method for nonlinear optimization {{that relies on}} a line search filter strategy to allow convergence from poor starting points. The filter technique has already been adapted to interior point methods in different ways. Our filter relies on three components. Each <b>entry</b> in the <b>filter</b> includes the feasibility measure, the centrality measure and the barrier objective function value as the optimality measure. Numerical experiments carried out {{with a set of}} engineering design problems show that our filter approach is effective in reaching the solution. A comparison with other well-known methods is also reported...|$|R
40|$|In this paper, {{we propose}} a {{structured}} trust-region algorithm combining with filter technique {{to minimize the}} sum of two general functions with general constraints. Specifically, the new iterates are generated in the Gauss-Seidel type iterative procedure, whose sizes are controlled by a trust-region type parameter. The <b>entries</b> in the <b>filter</b> are a pair: one resulting from feasibility; the other resulting from optimality. The global convergence of the proposed algorithm is proved under some suitable assumptions. Some preliminary numerical results show that our algorithm is potentially efficient for solving general nonconvex optimization problems with separable structure. Department of Applied Mathematic...|$|R
40|$|Directories {{have become}} an {{important}} component of the enterprise security and identity management middleware. This paper describes a novel filter based replication model for Lightweight Directory Access Protocol (LDAP) directories. Instead of replicating entire subtrees from a Directory Information Tree (DIT), only <b>entries</b> matching a <b>filter</b> specification are replicated. Efficient algorithms for selecting such filters, keeping them synchronized with the master copy and for using them to answer directory queries have been proposed. Advantages of the filter based replication framework over existing subtree based mechanisms have been demonstrated for a real enterprise directory using real workloads. 1...|$|R
40|$|The {{topic of}} this bachelor's thesis is {{development}} of a script for downloading and <b>filtering</b> <b>entries</b> from public registry of contracts of Ministry {{of the interior}} of the Czech Republic. This thesis is part of a project to create price-list of ICT work for state institutions. The goals of this thesis include introduction of the price-list project, draft of download script's functionality, description of follow-up data processing, trying out two different programming languages of the script and final deployment of chosen script in production. My thesis contributes to creation of price-list of ICT work, which is important for evaluating offers in open tenders...|$|R
40|$|There {{has been}} {{virtually}} {{little in the}} way of user interfaces designed for the exploration and information gathering from large weblog datasets to allow for an integrated and aggregated knowledge collection and information analysis tool. Users have to rely on their own capability to find, select or <b>filter</b> <b>entries</b> and navigate through a blog archive. For weblogs with a large collection of entries this task easily becomes tedious, since current blog interfaces lack fundamental support for facilitating the exploration of their archives. A solution to this problem could be POSTCONNECT, a mature blog-archive visualization tool presented in this paper. 1...|$|R
40|$|The linear filters {{characterized}} by a state-variable realization given by matrices with nonnegative <b>entries</b> (called positive <b>filters)</b> are heavily restricted in their achievable performance. Nevertheless, such filters are the only choice when dealing with the charged coupled device MOS technology of charge routing networks (CRN's), since nonnegativity {{is a consequence of}} the underlying physical mechanism. In order to exploit the advantages offered by this technology, the authors try to overcome the above-mentioned limitation by realizing an arbitrary transfer function as a difference of two positive filters. Index Terms [...] -Charge routing network, discrete-time filtering, positive systems. I...|$|R
50|$|The {{convolutional}} {{layer is}} the core building block of a CNN. The layer's parameters consist {{of a set of}} learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the <b>entries</b> of the <b>filter</b> and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.|$|R
40|$|Abstract. In {{this paper}} we modify the {{original}} primal-dual interior-point filter method proposed in [18] for {{the solution of}} nonlinear programming problems. We introduce two new optimality <b>filter</b> <b>entries</b> based on the objective function, and thus better suited {{for the purposes of}} minimization, and propose conditions for using inexact Hessians. We show that the global convergence properties of the method remain true under such modifications. We also introduce a new optimization solver for the solution of nonlinear programming problems, called ipfilter, based on our primal-dual interior-point filter approach. The numerical results reported show that ipfilter is competitive both in efficiency and robustness and can handle large instances...|$|R
40|$|In this paper, {{the filter}} {{technique}} of Fletcher and Leyffer (1997) {{is used to}} globalize the primal-dual interior-point algorithm for nonlinear programming, avoiding the use of merit functions and the updating of penalty parameters. The new algorithm decomposes the primal-dual step obtained from the perturbed first-order necessary conditions into a normal and a tangential step, whose sizes are controlled by a trust-region type parameter. Each <b>entry</b> in the <b>filter</b> {{is a pair of}} coordinates: one resulting from feasibility and centrality, and associated with the normal step; the other resulting from optimality (complementarity and duality), and related with the tangential step. Global convergence to first-order critical points is proved for the new primal-dual interior-point filter algorithm. [URL]...|$|R
40|$|The filter flow {{problem is}} to compute a space-variant linear filter that {{transforms}} one image into another. This framework encompasses {{a broad range of}} transformations including stereo, optical flow, lighting changes, blur, and combinations of these effects. Parametric models such as affine motion, vignetting, and radial distortion can also be modeled within the same framework. All such transformations are modeled by selecting a number of constraints and objectives on the <b>filter</b> <b>entries</b> from a catalog which we enumerate. Most of the constraints are linear, leading to globally optimal solutions (via linear programming) for affine transformations, depth-from-defocus, and other problems. Adding a (non-convex) compactness objective enables solutions for optical flow with illumination changes, spacevariant defocus, and higher-order smoothness. 1...|$|R
3000|$|..., Equation  12 a {{corresponds}} to filter the input data and then deliver the heterodyned output, whereas Equation  12 b maps to first heterodyne the input signal and then apply the filtering. Clearly, the arrangement in Equation  12 b matches {{the requirement of}} a loop control system. Therefore, the NMDFB implementation requires the analysis LPPF to be a low-pass filter and synthesis LPPF to be any Nyquist pulse, i.e., Figure  7. Another detail which is worth noting is that the conventional BE design produces an oversampled filter (see Figure  5). The oversampling ratio {{is related to the}} SRRC filter's transition bandwidth or the roll-off factor. Mapping to the NMDFB implementation, this means only a certain group of <b>entries</b> of BE <b>filter's</b> IPE matrix [...]...|$|R
40|$|In {{practical}} implementations of estimation algorithms, designers {{usually have}} {{information about the}} range in which the unknown variables must lie, either due to physical constraints (such as power always being nonnegative) or due to hardware constraints (such as in implementations using fixedpoint arithmetic). In this {{paper we propose a}} fast (that is, whose complexity grows linearly with the filter length) version of the dichotomous coordinate descent recursive least-squares adaptive filter which can incorporate constraints on the variables. The constraints can be in the form of lower and upper bounds on each <b>entry</b> of the <b>filter,</b> or norm bounds. We compare the proposed algorithm with the recently proposed normalized non-negative least mean squares (LMS) and projected-gradient normalized LMS filters, which also include inequality constraints in the variables...|$|R
5000|$|This cost {{function}} (...) is the mean square error, {{and it is}} minimized by the LMS. This is where the LMS gets its name. Applying steepest descent means to take the partial derivatives {{with respect to the}} individual <b>entries</b> of the <b>filter</b> coefficient (weight) vectorwhere [...] is the gradient operatorNow, [...] is a vector which points towards the steepest ascent of the {{cost function}}. To find the minimum of the cost function {{we need to take a}} step in the opposite direction of [...] To express that in mathematical terms where [...] is the step size(adaptation constant). That means we have found a sequential update algorithm which minimizes the cost function. Unfortunately, this algorithm is not realizable until we know [...]|$|R
5000|$|Typically, {{approaches}} to automatic term extraction {{make use of}} linguistic processors (part of speech tagging, phrase chunking) to extract terminological candidates, i.e. syntactically plausible terminological noun phrases, NPs (e.g. compounds [...] "credit card", adjective-NPs [...] "local tourist information office", and prepositional-NPs [...] "board of directors" [...] - in English, the first two constructs are the most frequent). Terminological <b>entries</b> are then <b>filtered</b> from the candidate list using statistical and machine learning methods. Once filtered, because of their low ambiguity and high specificity, these terms are particularly useful for conceptualizing a knowledge domain or for supporting {{the creation of a}} domain ontology or a terminology base. Furthermore, terminology extraction is a very useful starting point for semantic similarity, knowledge management, human translation and machine translation, etc.|$|R
30|$|Joseph Chee Ming Teo et al. [14] {{proposes a}} group key {{agreement}} protocol to protect heterogeneous networks against DoS attacks. But it causes more communication overhead in heterogeneous networks. Wei Chen et al. [23] proposes a storage-efficient data structure and a change-point detection method to distinguish complete three-way TCP handshakes from incomplete ones. This mechanism leads to large memory consumption. Sungwon Yi et al. [15] introduced a two-level cache Content Addressable Memory (CAM) to dynamically detect and quarantine the unresponsive TCP flows [18]. But {{it leads to}} large memory comsumption. Dimitris Geneiata et al. [7] proposed a two-part bloom filter based monitor to detect and filter flooding attacks against proxy servers. The monitor's main task is to record the state of any incoming session in 3 different filters and the filter is indexed through a hash function. This mechanism uses an alarming system to trigger an alarm and report if any <b>entries</b> in the <b>filter</b> exceed the threshold value. This mechanism is very efficient and cost-effective and causes reduced time delay to detect an attack. However, hashing of <b>entries</b> in the <b>filters</b> leads to computation overhead and more CPU utilization. Dimitris Geneiatakis et al. [6] proposes a new header to overcome signaling DoS attacks in SIP servers. But the scheme uses a pre-shared key which when explored leads to password-based attacks and also it is vulnerable to man-in-the-middle attacks. It is observed in [9] that collaborative flooding attacks (DDoS) depend heavily on IP spoofing; therefore clever IP spoof detection might contribute to solving the problem. A common way for preventing IP spoofing is by using ingress and egress filters on firewalls [19]. But it fails in wireless networks where legitimate packets could have topologically incorrect addresses. In this paper, we have introduced a spoof-based collaborative detection of collaborative flooding attack (DDoS).|$|R
40|$|A hybrid multi-objective {{evolutionary}} algorithm (MOEA) for solving nonlinear multi-objective opti- mization problems {{that relies on}} a pattern search filter method is proposed. The aim {{is to reduce the}} computational time involved in solving expensive multi-objective problems by improving a subset of Pareto points. The proposed pattern search filter method relies on two components. Each <b>entry</b> in the <b>filter</b> aims to measure feasibility and optimality. The feasibility and optimality come directly from each single-objective nonlinear program problem that is associated to the multi-objective problem. Experi- ments carried out with a set of nonlinear multi-objective problems show that our pattern search filter approach is effective in reaching improved Pareto points. A comparison with other techniques known in the literature is presented. Fundação para a Ciência e a Tecnologia - Pluriannual Funding Program...|$|R
40|$|Abstract — In {{this paper}} an {{efficient}} method for removing noise {{as well as}} preserving the edge from corrupted image is presented here. In the signal transmission, the image signal can be corrupted by noise and the blurred image be the result. Impulse noise is caused by malfunctioning pixels in camera sensors, faulty memory locations in hardware, or transmission in a noisy channel. For instance two common types of impulse noise are the salt-and-pepper noise and the random-valued noise. For images corrupted by salt-and-pepper noise, the noisy pixels can take only the maximum and the minimum. The proposed idea is to run through the image pixel entry by entry and replacing each entry with the median of neighbouring <b>entries.</b> This Median <b>filter</b> has the capability to remove the noise without damaging the edges. The simulation design gives better image and it is algorithmically simple. Our scheme can remove salt-and-pepper-noise effectively...|$|R
40|$|Potassium {{channels}} allow K + ions {{to diffuse}} through their pores while preventing smaller Na + ions from permeating. Discrimination between these similar, abundant ions enables these proteins to control electrical and chemical activity in all organisms. Selection {{occurs at the}} narrow selectivity filter containing structurally identified K + binding sites. Selectivity is thought to arise because smaller ions such as Na + do not bind to these K + sites in a thermodynamically favorable way. Using the model K + channel KcsA, we examined how intracellular Na + and Li + interact with the pore and the permeant ions using electrophysiology, molecular dynamics simulations and X-ray crystallography. Our results suggest that these small cations have a separate binding site within the K + selectivity filter. We propose that selective permeation from the intracellular side primarily results from a large energy barrier blocking <b>filter</b> <b>entry</b> for Na + and Li + {{in the presence of}} K +, not from a difference of binding affinity between ions...|$|R
