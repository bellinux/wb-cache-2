23|156|Public
50|$|The machine {{running an}} <b>Internet</b> <b>Location</b> Server {{must have a}} public IP address.If the network running an <b>Internet</b> <b>Location</b> Server has a firewall, it is usually {{necessary}} to run the server in the demilitarized zone of the network.|$|E
50|$|There are {{two main}} {{approaches}} to using <b>Internet</b> <b>Location</b> Servers: use a public server on the Internet, or run and use a private server.|$|E
5000|$|Microsoft Windows {{includes}} an <b>Internet</b> <b>Location</b> Server. It {{can be installed}} in the Control Panel using Add/Remove Windows Components, under [...] "Networking Services" [...] (Site Server ILS Services).|$|E
5000|$|... #Subtitle level 3: Blocking <b>Internet</b> <b>locations</b> (sections 17-18) ...|$|R
5000|$|The [...] "Map of the Internet Project" [...] http://mapoftheinternet.com maps over 4 billion <b>internet</b> <b>locations</b> as cubes in 3D cyberspace. Users can add URLs as {{cubes and}} re-arrange objects on the map.|$|R
5000|$|Mega iAdvantage(互聯優勢大廈) (One of {{the major}} <b>Internet</b> housing <b>locations</b> of Asia) ...|$|R
5000|$|The <b>Internet</b> <b>Location</b> Server (ILS) {{included}} in Microsoft Windows 2000 offers service on port 1002, while {{the latest version}} of NetMeeting requests service from port 389. The choice of 1002 was to avoid conflict with Windows 2000's domain controllers, which use LDAP and Active Directory on port 389, as well as Microsoft Exchange Server 2000, which uses port 389. If the server is running neither Active Directory nor Microsoft Exchange Server, the <b>Internet</b> <b>Location</b> Server's port can be changed to 389 using the following command at a command prompt: ...|$|E
50|$|In the 21st Century, the Open Studio (often {{taking the}} form of a virtual or <b>internet</b> <b>location)</b> focuses on the {{creative}} act of making and sharing, in a flexible space equipped with a range of contemporary media and multimedia. Artists and non-artists come together in a social act of collaboration, the only entry requirements being an inquisitive nature, a curiosity about new and traditional media, and a lack of inhibition about creating in a semi-public space.|$|E
5000|$|Gerocentral.org: This website is a {{collaborative}} effort between the American Psychological Association’s (APA) Division 12, Section II: Society of Clinical Geropsychology (12/II) and The Adult Development and Aging Division, along with the Council of Professional Geropsychology Training Programs (CoPGTP), Psychologists in Long Term Care (PLTC), and the APA Committee ON Aging (CONA) to bring together available resources for geropsychology training, service provision, policy, and research, including online assessment of geropsychology competencies, in a central <b>internet</b> <b>location.</b>|$|E
5000|$|The Climatic Research Unit email {{controversy}} (also {{known as}} [...] "Climategate") began in November 2009 with the hacking of a server at the Climatic Research Unit (CRU) at the University of East Anglia (UEA) by an external attacker, copying thousands of emails and computer files, the Climatic Research Unit documents, to various <b>internet</b> <b>locations</b> {{several weeks before}} the Copenhagen Summit on climate change.|$|R
50|$|Stakkato is the alias of Swede Philip Gabriel Pettersson {{the alleged}} {{perpetrator}} of a worldwide cyber attack {{known to have}} occurred from at least December 2003 until May 2005, targeting {{a large number of}} sites on the Internet including the US Military, White Sands Missile Range, NASA, a number of US academic institutions (known to include Caltech, Stanford University, San Diego Supercomputer Center, and UIUC), and a number of non-US academic institutions (known to include Uppsala University in Sweden and University College Cork in Ireland) and several other <b>Internet</b> <b>locations.</b>|$|R
40|$|Agents on the Internet {{sometimes}} need {{to migrate}} between <b>Internet</b> <b>locations</b> {{to perform their}} tasks. However, migration is often complicated by the heterogeneous nature of the Internet. For example, an agent's destination may not offer a compatible agent platform. Different solutions for migration are possible. This thesis concerns one such solution, called generative agent migration. Instead of transporting the agent itself, a blueprint of the agent is sent to its destination. There the agent is regenerated using the blueprint. This thesis continues earlier work by extending available theory and providing a demonstration of generative agent migration...|$|R
50|$|<b>Internet</b> <b>Location</b> Servers do {{not address}} two other issues with using NetMeeting behind a firewall. First, {{although}} a participant can join the directory from an external IP address, the participant cannot join a meeting unless the internal host manually adds the participant to the meeting from the directory. Second, while this approach is fine for data conferencing, audio or video conferencing requires opening {{of a wide range}} of ports on the firewall. In this case, it may be desirable to use a gateway.|$|E
50|$|In {{order to}} arrive at the Holding, the Supreme Court analogized domain names to trade marks. After an {{elaboration}} of sections 2(zb), 2(m), and 2(z), the court stated that a domain name in the contemporary era has evolved from a mere business address to a business identifier. Therefore, a domain name is not merely a portal for internet navigation, but also an instrument which distinguishes and identifies the goods or services of the business, while simultaneously providing the specific <b>internet</b> <b>location.</b> This feature of domain names in the modern era allows them to be likened to trade marks, and concomitantly to be included within the purview of the Trade Marks Act.|$|E
5000|$|This site was, however, fast {{followed}} {{up by the}} DOOM Honorific Titles (also known as the [...] "DHT"), [...] launched in May 1994 by Frank Stajano, which introduced the first serious competition between players. This site, designed around a notion of earning titles by successfully recording {{a particular type of}} demo on one of the pre-determined maps in the [...] "IWADs", would create the basis for all Doom demo sites that would follow. These so-called [...] "exams" [...] became very popular as the player had to earn each title by sending in a demo of the feat to one of the site's judges to justify his application. Doom II: Hell on Earth was released in October 1994, and the DHT conformed to the new additions as well as the new Doom version releases. At the height of its popularity, the DHT had many different categories and playing styles. For example, playing with only the in-game fists and pistol, while killing all monsters on a map, became known as [...] "Tyson" [...] mode, named after the heavyweight boxer and former champion Mike Tyson. [...] "Pacifist mode" [...] was playing without intentionally harming any monsters. Each category had [...] "easy", [...] "medium", and [...] "hard" [...] difficulty maps for players to get randomly chosen for. As an authentication method to prevent players from submitting demos made by other people, it was required that they performed a distinct [...] "dance" [...] during their demo (often at the very beginning). With such varied categories, the DHT was appealing to a diverse group of players. However, the DHT had trouble retaining a permanent <b>Internet</b> <b>location.</b> This, combined with the constantly changing rules and the diminished importance of most of the titles, caused public interest to wane as the years went by.|$|E
40|$|Content {{delivery}} networks (CDNs) {{have become}} {{a crucial part of}} the modern Web infrastructure. This paper studies the performance of the leading content delivery provider – Akamai. It measures the performance of the current Akamai platform and considers a key architectural question faced by both CDN designers and their prospective customers: whether the co-location approach to CDN platforms adopted by Akamai, which tries to deploy servers in numerous <b>Internet</b> <b>locations,</b> brings inherent performance benefits over a more consolidated data center approach pursued by other influential CDNs such as Limelight. We believe the methodology we developed for this study will be useful for other researchers in the CDN arena...|$|R
40|$|There is a {{high demand}} for {{mosaicking}} of digital images. The digital images are of different formats, different resolutions, band combinations, sizes, etc. This increases {{the complexity of the}} mosaicking of images. We propose a dynamic, real-time pproach for mosaicking digital images of different temporal and spatial characteristics into tiles. Also, the source images at the time of mosaicking could be accessed from LAN <b>locations</b> and remote <b>Internet</b> <b>locations.</b> Further any specific source image can be reconstructed from the mosaicked tiles. This dynamic approach reuses digital images upon demand and generates mosaicked tiles only for the required region accord ng to user’s requirements such as resolution, temporal range, target bands, etc...|$|R
50|$|An Internet {{resource}} locator is a locator defined by an <b>Internet</b> resource <b>location</b> standard. A resource location standard {{in conjunction with}} resource description and resource naming standards specifies a comprehensive infrastructure for network-based information dissemination. Mechanisms for mapping between locators, names, and descriptive identifiers.|$|R
40|$|Internet banking {{offers many}} {{benefits}} but {{little research has}} been done about its acceptance in Mauritius. This paper aims at assessing the factors that contribute to the adoption of internet banking in Mauritius. To support our arguments, we use a logistic regression model based on a sample survey to analyze the factors that influence internet banking in Mauritius. We conclude that factors such as age, income, service usefulness, risk factor, checking account frequency and <b>internet</b> <b>location</b> are the main determinants for a person to opt for online banking...|$|E
40|$|This paper jointly {{examines}} the length between successive participations in several activity purposes using a 1999 multi-week travel survey {{conducted in the}} German cities of Halle and Karlsruhe. A multivariate hazard model that accommodates a flexible duration dynamics structure, recognizes the effects of covariates, incorporates the variation in interactivity duration due to unobserved individual-specific factors and variation in interactivity duration within spells of the same individual, and considers the joint nature {{of participation in the}} various activities is proposed and applied. The variables considered in the analysis include demographics, access to the <b>internet,</b> <b>location</b> characteristics, and day of week variables...|$|E
40|$|Purpose: The {{research}} let determine, to {{what extent}} the <b>Internet</b> <b>location</b> of scientific publications influence the visibility of those documents in general and specialist search engines. Methodology: The study involved selected publications on information science, located in repositories, digital libraries and on the websites of scientific journals and institutions. Visibility of selected documents is determined by the possibility of finding them in some general and specialized search engines. The background of the research is the "invisible web" issue. Value: The results should indicate the best methods of publishing scientific papers on the Internet in terms of visibility to potential users. This will help researchers choose the optimal location for their publications. ...|$|E
50|$|One {{consequence}} of the new unacceptability of open relays was an inconvenience for some end users and certain internet service providers. To allow customers to use their e-mail addresses at <b>Internet</b> <b>locations</b> other than the company's systems (such as at school or work), many mail sites explicitly allowed open relaying so that customers could send e-mail via the ISP from any location. Once open relay became unacceptable because of abuse (and unusable because of blocking of open relays), ISPs and other sites had to adopt new protocols to allow remote users to send mail. These include smart hosts, SMTP-AUTH, POP before SMTP, {{and the use of}} virtual private networks (VPNs). The IETF has written a best current practices covering Email Submission Operations in RFC 5068.|$|R
25|$|Google {{launches}} {{a fleet of}} high-altitude balloons capable of beaming wireless <b>internet</b> to remote <b>locations</b> far more cheaply than satellites.|$|R
5000|$|... "Taken together, {{the revelations}} {{have brought to}} light a global {{surveillance}} system that cast off many of its historical restraints after the attacks of Sept. 11, 2001. Secret legal authorities empowered the NSA to sweep in the telephone, <b>Internet</b> and <b>location</b> records of whole populations." [...] The Washington Post ...|$|R
40|$|Abstract—In {{the mobile}} {{wireless}} <b>Internet,</b> <b>location</b> privacy is serious concerns. As {{a response to}} these concerns, many location-privacy protection mechanisms (LPPMs) have been proposed. However, the existing work doesn’t integrate formal models into assessments, which leads a huge gap: after designing a LPPM, we have to select formal methods to formalize them, and select another evaluation metric to measure them. In this paper, we propose a probabilistic process calculus to model the LPPMs and use the relative entropy to measure the degree of location privacy LPPMs can leak. Our work decreases {{the gap between the}} formalization and the measurement for LPPMs. Keywords—Location Privacy; Process Calculus;Measurement; I...|$|E
40|$|Engraved, {{with the}} {{exception}} of 8 p. following general t. p., and pt. [4], p. 1 - 16. Parts [2]-[4] have special t. p.; pt. [2]: Directions for writing shorthand [...] . pt. [3]: A dictionary, or An alphabetical table, containing almost all the words in the English tongue, with the short-hand over-against each word [...] . pt. [4]: Observations, and explications [...] . Mode of access: <b>Internet.</b> <b>Location</b> (Pforzheimer copy) : Press 40. Provenance (Pforzheimer copy) : Bookseller's label on back paste-down: W. & G. Foyle, 5 / 13 / 68. Binding (Pforzheimer copy) : Contemporary brown calf, rebacked. Black morocco label laid down. Calf scratched and torn in places, edges and corners worn. Some foxing...|$|E
40|$|Recent {{advances}} in the Domain Name System (DNS) and the Dynamic Host Configuration Protocol (DHCP) have enabled {{a new approach to}} supporting mobile users: location independent naming. In this approach, machines use the same hostname from any <b>internet</b> <b>location,</b> but use an IP address that corresponds to their current location. We describe a protocol that implements location independent naming for nomadic computers, i. e., machines that do not need transparent mobility. Our protocol allows hosts to move across security domains, uses existing protocols, and preserves existing trust relationships. Therefore, it preserves the performance and security of normal IP for nomadic computers at the expense of not providing the transparent mobility of Mobile IP. We contend that this is a reasonable tradeoff for nomadic computing. ...|$|E
40|$|Abstract—Troubleshooting network outages is {{a complex}} and {{time-consuming}} process. Network administrators are typically overwhelmed with large volumes of monitoring data, like NetFlow data, and are often “left alone”, fighting problems with very basic debugging tools, like ping and traceroute. Distributed network traffic monitoring and intelligent correlation of data from differ-ent <b>Internet</b> <b>locations</b> are highly valuable for analysing {{the root cause of}} network outages. However, correlating measurements across domains is presently largely avoided due to privacy concerns. A possible solution to this problem is secure multi-party computation (MPC). In this work, we propose a distributed mechanism based on MPC for privacy-preserving correlation of traffic measurements from multiple networks, towards network outage diagnosis. We first outline an MPC protocol {{that can be used to}} analyse the scope (local, global, or semi-global) an...|$|R
30|$|While these {{investigations}} into the flow of user trust from physical buildings to <b>Internet</b> <b>locations</b> to mobile services do not exactly mirror our experiments on trust across game contexts, strong parallels exist that support our work. For example, in these related works, the user’s objective to purchase an item or service is consistent, and the details around how the purchase is made changes from physical to web to mobile contexts. Similarly, in our implementations of both Berg’s investment game and the modified Battleship game, the user’s objective of playing a game to accumulate money is consistent, but the contextual details around how the games are played and the structural rules in place to limit betrayal change. Furthermore, our work takes this exploration a step further as we {{have no knowledge of}} any other investigation into the strength of cross-context trust with respect to new objective information.|$|R
50|$|A small torrent file {{is created}} to {{represent}} a file or folder to be shared. The torrent file acts {{as the key to}} initiating downloading of the actual content. Someone interested in receiving the shared file or folder first obtains the corresponding torrent file, either by directly downloading it, or by using a magnet link. The user then opens that file in a BitTorrent client, which automates the rest of the process. In order to learn the <b>Internet</b> <b>locations</b> of peers which may be sharing pieces, the client connects to the trackers named in the torrent file, and/or achieves a similar result through the use of distributed hash tables. Then the client connects directly to the peers in order to request pieces and otherwise participate in a swarm. The client may also report progress to trackers, to help the tracker with its peer recommendations.|$|R
40|$|This project {{laid the}} groundwork for an Internet-delivered Public Participation Geographic Information System to {{facilitate}} exploration and discovery of the past communities of the Mammoth Cave Park area. The emergence of Internet Web 2. 0 design along with distributed GIS services allows for anyone to interact with and add to the information found on central Internet sites. Historical geography often relies upon public participation from individuals outside the academic world to provide narrative descriptions, photographs and manuscripts of past places and events to augment information held by institutions and academia. A public-participation website for the Mammoth Cave Historic GIS (MCHGIS) created a central <b>Internet</b> <b>location</b> for dispersed and disparate data related to pre-park communities to be presented with a geographic context. The MCHGIS project allowed for visualization of the pre-park communities in unique ways and contributed new understandings of this pre-park area...|$|E
40|$|This paper {{presents}} the results from a performance evaluation on a Mobile IPv 6 based test-bed composed of WLANs (Wireless Local Area Networks), MANETs (Mobile Ad-hoc Networks) and the <b>Internet.</b> <b>Location</b> management and handoff efficiency is {{measured in terms of}} session set-up delay and handoff delay. The mobiles, connected as a MANET, employ the OLSR (Optimized Link State Routing) protocol for routing within the MANET. Location management across WLANs and MANETs is achieved through Mobile IPv 6. The impact of OLSR based route discovery and packet propagation, and IPv 6 features such as neighbor discovery and address auto-configuration on the session set-up latency, handoff latency and packet loss are quantified. There are two main contributions reported in this paper. First, a novel approach is proposed to integrate MANETs into the Internet using Mobile IPv 6 and OLSR. Secondly, various performance benchmarks and metrics are presented that are obtained using the test-bed develope...|$|E
40|$|This paper models spatial {{economic}} development, making explicit {{both time}} and space. Locations {{are not just}} points [...] -which would leave unanswered the question, What happens in between? [...] -but instead a continuum. Equilibrium is a law of motion in spatial distribution dynamics, or a transition kernel in measures on geographical space. The paper provides a model of economic geography without transportation costs; {{it is used to}} study the evolution across Earth of financial, Internet, telecommunications, and computer activity. Keywords: distribution dynamics, Fourier, <b>Internet,</b> <b>location,</b> space, time, Toeplitz, weightless economy JEL Classification: C 33, D 30, 018, 033 Communications to: D. Quah, LSE, Houghton Street, London WC 2 A 2 AE. Tel: + 44 / 0 207 955 - 7535, Email: dquah@econ. lse. ac. uk (URL) [URL] Cluster Emergence on a Global Continuum by Danny Quah # LSE Economics Department October 1999 1 Pockets of poverty and clusters of excellence That economic activ [...] ...|$|E
40|$|Probabilistic Safety Analysis (PSA) {{determines the}} {{probability}} {{and consequences of}} accidents, hence, the risk. This subject concerns policy makers, regulators, designers, educators and engineers working to achieve maximum safety with operational efficiency. Risk is analyzed using methods for achieving reliability in the space program. The first major application was to the nuclear power industry, followed by applications to the chemical industry. It has also been applied to space, aviation, defense, ground, and water transportation. This book is unique in its treatment of chemical and nuclear risk. Problems are included {{at the end of}} many chapters, and answers are {{in the back of the}} book. Computer files are provided (via the internet), containing reliability data, a calculator that determines failure rate and uncertainty based on field experience, pipe break calculator, event tree calculator, FTAP and associated programs for fault tree analysis, and a units conversion code. It contains 540 references and many referrals to <b>internet</b> <b>locations</b> for information...|$|R
40|$|Large-scale {{peer-to-peer}} systems span a {{wide range}} of <b>Internet</b> <b>locations.</b> Such diversity can be leveraged to build overlay “detours” to circumvent periods of poor performance on the default path. However, identifying which peers are “good” relay choices in support of such detours is challenging, if one is to avoid incurring an overhead that grows with the size of the peer-to-peer system. This paper proposes and investigates the Earliest Branching Rule (EBR) to perform such a selection. EBR builds on the Earliest Diverging Rule (EDR) that selects relay nodes whose AS path diverges from the default path at the earliest possible point, but calls for monitoring a much smaller number of paths. As a result, it has a much lower overhead. The paper explores the performance and overhead of EBR, and compares them to that of EDR. The results demonstrate that EBR succeeds in selecting good relay nodes with minimum control overhead. Hence, providing a practical solution for dynamically building good overlays in large peer-to-peer systems...|$|R
40|$|This paper {{proposes a}} novel {{protocol}} which uses the Internet Domain Name System (DNS) to partition Web clients into disjoint sets, {{each of which}} is associated with a single DNS server. We define an L-DNS cluster to be a grouping of Web Clients that use the same Local DNS server to resolve Internet host names. We identify such clusters in realtime using data obtained from a Web Server in conjunction with that server’s Authoritative DNS—both instrumented with an implementation of our clustering algorithm. Using these clusters, we perform measurements from four distinct <b>Internet</b> <b>locations.</b> Our results show that L-DNS clustering enables a better estimation of proximity of a Web Client to a Web Server than previously proposed techniques. Thus, in a Content Distribution Network, a DNS-based scheme that redirects a request from a web client to one of many servers based on the client’s name server coordinates (e. g., hops/latency/loss-rates) would perform better with our algorithm. 1...|$|R
