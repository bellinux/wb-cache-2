302|86|Public
50|$|Alternatively, {{flexible}} and mobile tracking systems provide head and face tracking which include eye, eyelid and <b>gaze</b> <b>tracking.</b> These systems now provide real time feedback {{without the use}} of wire, magnets or headgear.|$|E
50|$|The second broad {{category}} uses some non-contact, optical {{method for}} measuring eye motion. Light, typically infrared, is reflected from {{the eye and}} sensed by a video camera or some other specially designed optical sensor. The information is then analyzed to extract eye rotation from changes in reflections. Video-based eye trackers typically use the corneal reflection (the first Purkinje image) {{and the center of}} the pupil as features to track over time. A more sensitive type of eye tracker, the dual-Purkinje eye tracker, uses reflections {{from the front of the}} cornea (first Purkinje image) and the back of the lens (fourth Purkinje image) as features to track. A still more sensitive method of tracking is to image features from inside the eye, such as the retinal blood vessels, and follow these features as the eye rotates. Optical methods, particularly those based on video recording, are widely used for <b>gaze</b> <b>tracking</b> and are favored for being non-invasive and inexpensive.|$|E
5000|$|Gabe Newell {{explained}} that Valve's {{strategy is to}} develop a single hardware unit themselves as the default model, internally named [...] "Bigfoot", and work with other computer manufacturers who want to offer the same user experience but with different hardware configurations not offered by Valve's model; for example, Valve does not expect to include an optical drive due to size and cost, but this can be a feature offered by a manufacturing partner. He also envisions the software to enable screencast capabilities, allowing the single box to work with any monitor or television within the home. Newell stated that they would also likely develop controllers for the unit that integrate biometrics data from the player and options for <b>gaze</b> <b>tracking,</b> citing that the involuntary responses from the player are more useful than other forms of player input such as motion control. Newell also {{explained that}} Valve is also considering the mobile device market in addition to the home console market, specifically considering laptops and tablets with their own hardware nicknamed [...] "Littlefoot". During the Steam Dev Days in January 2014, Valve further explained that the initial target market for Steam Machines is the living room and build a demand for support for Linux versions of games such that they can continue to work away from Windows and OS X operating systems for the future.|$|E
40|$|In {{face-to-face}} collaboration, eye gaze is {{used both}} as a bidirectional signal to monitor and indicate focus of attention and action, {{as well as a}} resource to manage the interaction. In remote interaction supported by Immersive Collaborative Virtual Environments (ICVEs), embodied avatars representing and controlled by each participant share a virtual space. We report on a study designed to evaluate methods of avatar eye gaze control during an object-focused puzzle scenario performed between three networked CAVETM -like systems. We compare <b>tracked</b> <b>gaze,</b> in which avatars’ eyes are controlled by head-mounted mobile eye trackers worn by participants, to a gaze model informed by head orientation for saccade generation, and static gaze featuring non-moving eyes. We analyse task performance, subjective user experience, and interactional behaviour. While not providing statistically significant benefit over static <b>gaze,</b> <b>tracked</b> <b>gaze</b> is observed as the highest performing condition. However, the gaze model resulted in significantly lower task performance and increased error rate...|$|R
50|$|Foveated {{rendering}} - User's <b>gaze</b> is <b>tracked</b> {{and calculated}} so the graphical resources are allocated {{to where he}} or she is looking. The different areas of VR world sharpens and blurs depending on where your eyes are focusing.|$|R
40|$|This paper reports {{experience}} {{in developing a}} parallel reality system which allows its user to observe and move around their real environment whilst wearing a stereoscopic 3 D head mounted display imbued with video-see through capabilities, with their position and <b>gaze</b> <b>tracked</b> by an indoor positioning system and head tracker, allowing them to alternately view their real environment and an immersive virtual reality environment from the equivalent vantage point. In so doing {{the challenge of the}} vacancy problem is addressed by lightening the cognitive load needed to switch between realities and to navigate the virtual environment. Evaluation of the usability, system performance and value of the system are undertaken {{in the context of a}} cultural heritage application; users are able to compare a reconstruction of an important 15 th century chapel with its present day instantiation. Postprin...|$|R
40|$|<b>Gaze</b> <b>tracking</b> is the {{technology}} that identifies a region in space that a user is looking at. Most previous non-wearable <b>gaze</b> <b>tracking</b> systems use a near-infrared (NIR) light camera with an NIR illuminator. Based {{on the kind of}} camera lens used, the viewing angle and depth-of-field (DOF) of a <b>gaze</b> <b>tracking</b> camera can be different, which affects the performance of the <b>gaze</b> <b>tracking</b> system. Nevertheless, to our best knowledge, most previous researches implemented <b>gaze</b> <b>tracking</b> cameras without ground truth information for determining the optimal viewing angle and DOF of the camera lens. Eye-tracker manufacturers might also use ground truth information, but they do not provide this in public. Therefore, researchers and developers of <b>gaze</b> <b>tracking</b> systems cannot refer to such information for implementing <b>gaze</b> <b>tracking</b> system. We address this problem providing an empirical study in which we design an optimal <b>gaze</b> <b>tracking</b> camera based on experimental measurements of the amount and velocity of user’s head movements. Based on our results and analyses, researchers and developers might be able to more easily implement an optimal <b>gaze</b> <b>tracking</b> system. Experimental results show that our <b>gaze</b> <b>tracking</b> system shows high performance in terms of accuracy, user convenience and interest...|$|E
40|$|Eye gaze {{can be a}} {{rich source}} of {{information}} to identify particular interests of human users. Eye <b>gaze</b> <b>tracking</b> has been largely used in different research areas in the last years, as for example in psychology, visual system design and to leverage the user interaction with computer systems. In this paper, we present an IR-based <b>gaze</b> <b>tracking</b> framework that can be easily coupled to common user applications and allows for real-time gaze estimation. Compared to other <b>gaze</b> <b>tracking</b> systems, our system uses only affordable consumer-grade hardware and still achieves fair accuracy. To evaluate the usability of our <b>gaze</b> <b>tracking</b> system, we performed a user study with persons of different genders and ethnicities. Keywords: Eye tracking, <b>gaze</b> <b>tracking,</b> human-computer interaction, interactivity, application...|$|E
40|$|<b>Gaze</b> <b>tracking</b> has {{previously}} been used to evaluate usability, but research using <b>gaze</b> <b>tracking</b> to evaluate user experience has not been conducted or is very limited. The objective of the thesis {{is to examine the}} possibility of using <b>gaze</b> <b>tracking</b> in user experience evaluation and providing results comparable with other forms of user experience evaluations. A convenience sample of ten participants took part in an experiment to evaluate the user experience of an augmented reality application. <b>Gaze</b> <b>tracking</b> was used as a cue to help participants recall their user experience in a retrospective think-aloud. Participants also filled in a user experience questionnaire and were interviewed about their experience of using the application. The results of the experiment suggest that <b>gaze</b> <b>tracking</b> can be used in measuring user experience when combined with the retrospective think-aloud method. The quotes generated can be used to establish which features or qualities of the application affected the user experience of participants. The method establishes a basis for further research for using <b>gaze</b> <b>tracking</b> to evaluate user experience...|$|E
5000|$|Gaze and Macara were {{founding}} members of Ariel, another progressive rock group, which formed in 1973, alongside John Mills on keyboards, Bill Putt on bass guitar, and Mike Rudd on guitar and vocals (all ex-Spectrum). Aside from guitar and vocals, <b>Gaze</b> co-wrote <b>tracks</b> for the group's debut album, A Strange Fantastic Dream (December 1973), including their first single, [...] "Jamaican Farewell" [...] (September). He remained {{with the group}} until April 1974 and then joined the Stevie Wright Band to tour Australia {{until the end of}} that year.|$|R
40|$|Selective {{presentation}} of information to a specific user is one of main display research topics, while selective un-presentation was not fully explored. We focused on the selective un-presentation by detecting his/her eye gaze. To verify the method, we used a morphing image {{as an example of}} changing image. The result showed {{that it is possible to}} un-present information to the user whose eye <b>gaze</b> is <b>tracked.</b> We also made a prototype installation of clocks whose second hands stop when being looked at. Author Keywords Eye tracking; information un-presentation; morphing movie; peripheral vision; visual one-shot leanin...|$|R
40|$|In this paper, we {{describe}} Gaze-Con, a generic framework for writing gaze-controlled applications. Application behaviors are fully customizable {{by the user}} through built-in macros or user-written scripts. The framework provides an interface that abstracts low-level operations such as parsing raw data from the device, calibration, recalibration, and handling of loss of <b>gaze.</b> Keywords: Eye <b>Tracking,</b> Gaze-control, Scripting...|$|R
40|$|A <b>gaze</b> <b>tracking</b> system, {{based on}} a novel neural approach, is proposed. The work {{is part of a}} wider {{research}} project concerning the development of human computer interfaces (HCI) addressed to disabled people, that could overcome the drawbacks of most of the existing methods for <b>gaze</b> <b>tracking</b> that require either intrusive devices or expensive equipment. This work, instead, aims at developing a low cost, completely non-intrusive and self-calibrating system which combines different techniques for three blocks in Eye <b>Gaze</b> <b>Tracking,</b> i. e. blink detection, feature extraction and neural computing. The experimental results show good accuracy in eye <b>gaze</b> <b>tracking</b> (rmse < 1 degree), and adequate generalization performance (rmse < 2 degrees) ...|$|E
40|$|International audienceThe {{problem of}} eye <b>gaze</b> <b>tracking</b> has been {{researched}} and developed for a long time. The most difficult problem in the non-intrusive system of eye <b>gaze</b> <b>tracking</b> {{is the problem of}} head movements. Some of existing methods have to use two cameras and an active infrared (IR) illumination to solve this problem. Otherwise, with a single camera, the user has to hold the head uncomfortably still when performing a session of eye <b>gaze</b> <b>tracking.</b> If the head of the user moves away from original position, the accuracy of these eye gaze-tracking systems drops dramatically. In this paper, we propose a solution using Gaussian Processes for eye <b>gaze</b> <b>tracking</b> that allows free head movements with a single camera...|$|E
40|$|No {{learning}} {{could take}} place without input attention. In order to promote students’ attention in e-learning content, this study focuses on <b>gaze</b> <b>tracking</b> data {{as a kind of}} enhanced input. <b>Gaze</b> <b>tracking</b> integrated video and slide synchronized e-learning contents were developed and evaluated in the study. An eye mark is showed at e-learning contents to indicate where the teacher is explaining. A comparison survey is conducted. As a result, students’ learning attention is promoted with <b>gaze</b> <b>tracking</b> data in e-learning...|$|E
40|$|A company's aim is {{to develop}} {{products}} that engage user attention and evoke positive emotions. Customers base their emotional evaluation on product components that are relevant for their perception. This paper presents findings of both identifying relevant product components and measuring emotions evoked by relevant perceived product components. To validate results, the comparison with self-reporting methods identifies {{similarities and differences between}} explicit expressed and implicit recorded customer requirements. On the one hand, eye tracking is applied to deduce the attention provoked by perceived product components. In order to link the product strategy with product components, the paper presents results considering the fact that the <b>gaze</b> <b>track</b> is affected by current thoughts. (Köhler et al., 2013, 2014 a, b; Köhler and Schmitt, 2012) On the other hand, since self-reporting tools are only useful for obtaining information about the conscious part of customers' emotions, {{there is a need for}} measurement methods that measure the changes in physiological signals (bio-signals). Arousal is similar to emotional intensity and is related to the galvanic skin response. Positive or negative emotions are defined by the valence that is measured by facial electromyography. Findings are presented that relate changes in bio-signals on the aesthetical design to the global product impression as well as to emotions and, subsequently, linking changes in physiological signals to the evaluation of semantic concepts and design parameters. The presented approach provides conclusions and valid information about products as well as product components that provoke certain emotions and about product components linked to a certain product concept, which could be part of a product strategy. Consequently, hard facts and special design rules for emotional product design can be deduced...|$|R
40|$|Using a psychophysical {{approach}} {{coupled with}} eye-tracking measures, we varied length and width of shape stimuli {{to determine the}} objective parameters that corresponded to subjective determination of square/rectangle judgments. Participants viewed a two-dimensional shape stimulus and made a two-alternative forced-choice {{whether it was a}} square or rectangle. Participants’ <b>gaze</b> was <b>tracked</b> throughout the task to explore directed visual attention to the vertical and horizontal axes of space. Behavioral results provide threshold values for two-dimensional square/rectangle perception, and eye-tracking data indicated that participants directed attention to the major and minor principal axes. Results are consistent {{with the use of the}} major and minor principal axis of space for shape perception and may have theoretical and empirical implications for orientation via geometric cues...|$|R
60|$|All the {{way from}} Winnipeg a dingy {{greyness}} had shrouded the apparently interminable levels, which lay parched and white beneath an almost intolerable heat, while the lurching cars swung through a rolling cloud of dust that blurred the dreary prospect. Now, as they were slowing down, grimy faces were thrust from the windows and perspiring men leaned out from the platforms, <b>gazing</b> down the <b>track</b> and inquiring with expletives why they were stopping again.|$|R
40|$|This paper {{addresses}} the accuracy problem {{of an eye}} <b>gaze</b> <b>tracking</b> system. We first analyze the technical barrier for a <b>gaze</b> <b>tracking</b> system to achieve desired accuracy, and then propose a subpixel tracking method to break the barrier. We present new algorithms for detecting the inner eye corner {{and the center of}} an iris in subpixel accuracy, and we apply these new methods in developing a real-time <b>gaze</b> <b>tracking</b> system. The experimental results indicate that the new methods achieve an average accuracy to within 1. 4 û using normal eye image resolutions. 1...|$|E
40|$|Most <b>gaze</b> <b>tracking</b> {{systems are}} based on the pupil center corneal {{reflection}} (PCCR) method using near infrared (NIR) illuminators. One advantage of the PCCR method is the high accuracy it achieves in <b>gaze</b> <b>tracking</b> because it compensates for the pupil center position based on the relative position of corneal specular reflection (SR). However, the PCCR method only works for user head movements within a limited range, and its performance is degraded by the natural movement of the user’s head. To overcome this problem, we propose a <b>gaze</b> <b>tracking</b> method using an ultrasonic sensor that is robust to the natural head movement of users. Experimental results demonstrate that with our compensation method the <b>gaze</b> <b>tracking</b> system is more robust to natural head movements compared to other systems without our method and commercial systems...|$|E
40|$|In {{our effort}} to make human-robot {{interfaces}} more userfriendly, we built an active <b>gaze</b> <b>tracking</b> system that can measure a person's gaze direction in real-time. Gaze normally tells which object in his/her surrounding a person is interested in. Therefore, {{it can be used}} as a medium for human-robot interaction like instructing a robot arm to pick a certain object a user is looking at. In this paper, we discuss how we developed and put together algorithms for zoom camera calibration, low-level control of active head, face and <b>gaze</b> <b>tracking</b> to create an active <b>gaze</b> <b>tracking</b> system...|$|E
40|$|A {{multimodal}} {{combination of}} gaze and foot input is highly promising for supporting manual interactions using, for example, mouse and keyboard. This is particularly interesting for simultaneously performing primary (e. g., object selection or manipulation) and secondary tasks (e. g., pan and zoom) in zoomable information spaces. While our eye gaze is ideal to quickly indicate a user’s current point of interest, foot interaction {{is well suited}} for parallel hand-free input controls, for example, to quickly confirm an action. This allows for using gaze input in a subtle and unobtrusive way, while still maintaining a fast and convenient interaction. Motivated by this, we present several alternatives for multimodal gaze-supported foot interaction to pan and zoom in a desktop computer setup. With this, we contribute the novel approach of seamlessly combining gaze and foot input for a more convenient interaction. Author Keywords Multimodal interaction, foot, <b>gaze,</b> eye <b>tracking...</b>|$|R
40|$|Children {{develop some}} orthographic {{knowledge}} before learning to read. In some contexts phonological knowledge can scaffold orthographic understanding, but in others, phonological knowledge must be ignored {{in favor of}} orthographic knowledge. The current study examines the development of orthographic knowledge as it interacts with phonological knowledge in early readers. Forty-five Kindergarten students were presented with two different nonwords on screen and their <b>gaze</b> was <b>tracked.</b> In the first task, {{they were asked to}} choose the best “word,” and in the second task they were asked to choose the best “word” for a specific pronunciation, thereby requiring phonological decoding of the stimuli. Our findings indicate that early readers show explicit awareness of some orthographic conventions and implicit awareness of others, but they only showed implicit awareness when {{they did not have to}} additionally decode the stimuli. These results suggest that early orthographic knowledge may be fragile and easily masked by phonological knowledge...|$|R
40|$|This paper {{explores the}} {{possible}} relations between Reaction Times (RT) and Index of Productive Syntax (IPSyn) scores in preschool children. Nineteen children participated and were categorized into two groups; {{low and high}} Mean Length of Utterance (MLU). An auditory plus visual looking-while-listening task was conducted and eye <b>gaze</b> was <b>tracked.</b> Language samples from a previous study collected as children played with their mothers and a standard toy set were used to obtain IPSyn scores. Three hypothesis were tested; children with shorter RT will have higher IPSyn scores regardless of MLU; the noun phrase subscale of IPSyn will be related to RT in the low MLU group and not the high; the verb phrase subscale of the IPSyn will not be related to RT in the low MLU group but {{will be in the}} high MLU group. Results revealed no significant relations between RT and IPSyn scores. B. A. (Bachelor of Arts...|$|R
40|$|Nowadays the {{computer}} game industry is developing {{more and more}} innovative interaction and control methods for user inputs. Nevertheless <b>Gaze</b> <b>tracking,</b> that is a fast natural and intuitive input channel, is not exploited in any commercial computer game, yet. In recent years several research groups started to study <b>gaze</b> <b>tracking</b> devices applied to compute...|$|E
40|$|<b>Gaze</b> <b>tracking</b> is a camera-vision based {{technology}} for identifying the location where a user is looking. In general, a calibration process is applied {{at the initial}} stage of most <b>gaze</b> <b>tracking</b> systems. This process is necessary to calibrate for {{the differences in the}} eyeballs and cornea size of the user, as well as the angle kappa, and to find the relationship between the user’s eye and screen coordinates. It is applied {{on the basis of the}} information of the user’s pupil and corneal specular reflection obtained while the user is looking at several predetermined positions on a screen. In previous studies, user calibration was performed using various types of markers and marker display methods. However, studies on estimating the accuracy of gaze detection through the results obtained during the calibration process have yet to be carried out. Therefore, we propose the method for estimating the accuracy of a final <b>gaze</b> <b>tracking</b> system with a near-infrared (NIR) camera by using a fuzzy system based on the user calibration information. Here, the accuracy of the final <b>gaze</b> <b>tracking</b> system ensures the gaze detection accuracy during the testing stage of the <b>gaze</b> <b>tracking</b> system. Experiments were performed using a total of four types of markers and three types of marker display methods. From them, it was found that the proposed method correctly estimated the accuracy of the <b>gaze</b> <b>tracking</b> regardless of the various marker and marker display types applied...|$|E
40|$|DRAFT A novel {{approach}} to 3 -D <b>gaze</b> <b>tracking</b> using 3 -D computer vision techniques is proposed in this paper. This method employs multiple cameras and multiple point light sources {{to estimate the}} optical axis of user’s eye without using any user-dependent parameters. Thus, it renders the inconvenient system calibration process which may produce possible calibration errors unnecessary. A real-time 3 -D <b>gaze</b> <b>tracking</b> system has been developed which can provide 30 gaze measurements per second. Moreover, a simple and accurate calibration method is proposed to calibrate the <b>gaze</b> <b>tracking</b> system. Before using the system, each user only has to stare at a target point for a few (2 – 3) seconds so that the constant angle between the 3 -D line of sight and the optical axis can be estimated. The test results of six subjects showed that the <b>gaze</b> <b>tracking</b> system is very promising achieving an average estimation error of under 1 degree...|$|E
40|$|The CD {{presents}} {{a series of}} sarangi solo executions inspired by a rich repertoire of traditional Nepali and Tibetan melodies, performed by Shyam Nepali: in this work the long-established music of the Gaine of Nepal blends with the very personal experimentation and intimate feelings {{of one of the}} most renowned innovative and sensitive musicians in the panorama of contemporary Nepali music. Track 01 : Morning bliss Track 02 : Himalayan dawn Track 03 : Across the clouds Track 04 : Waiting Track 05 : Footprints in the snow Track 06 : Soul’s vibe Track 07 : The shaman’s flight Track 08 : Melting water Track 09 : Beauty revealed Track 10 : Don’t turn your <b>gaze</b> behind <b>Track</b> 11 : Back from the fields Scientific researches, recording organisation and supervision: Martino Nicoletti; Sound engineering and post-production: Roberto Passuti; Label: Stenopeica – A Buzz Supreme...|$|R
40|$|Rotary cranes {{are used}} in many fields, in which a {{suspended}} load must be transported and positioned without any swing. Therefore we simulated visual tracking of a rotary crane using gazing stereo vision installed on a rotary crane. Rotary crane has broad working space, so we use stereo vision on the crane 2 ̆ 7 s rotary axis that has <b>gazing</b> motion to <b>track</b> the suspended load. In this paper we present simulation results of visual tracking of a rotary crane in state of boom-lifting or boom-rotating with rope-hoisting...|$|R
40|$|This {{research}} {{investigates the}} usefulness of visual flow field information for the robotic navigation task of independent motion detection. The task is considered non-trivial for the case of moving observers. Multiple modules are proposed to attempt to solve the task for a mobile robotic system which is constantly in motion. First, egomotion parameters are computed {{on the basis of}} the flow field computed from image sequences. Second, independent motion is detected after constantly monitoring the flow field and noticing deviation from the expected flow. Third, the target is fixated (<b>gaze</b> control, <b>tracking)</b> by using this attentive information for an automatic initialization of an active contour which locks onto the desired agent's apparent contours for subsequent pursuit. Finally, the object is approached if necessary. The research tries to foster the discussion on cooperative processing of the modules in order to achieve more complex behaviors in a robust way. In addition, the modules a [...] ...|$|R
40|$|This thesis {{presents}} a non-contact, video-based <b>gaze</b> <b>tracking</b> system using novel eye detection and gaze estimation techniques. The {{objective of the}} work is to develop a real-time <b>gaze</b> <b>tracking</b> system that is capable of estimating the gaze accurately under natural head movement. The system contains both hardware and software components. The hardware of the system is responsible for illuminating the scene and capturing facial images for further computer analysis, while the software implements the core technique of <b>gaze</b> <b>tracking</b> which consists of two main modules, i. e., eye detection subsystem and gaze estimation subsystem. The proposed <b>gaze</b> <b>tracking</b> technique uses image plane features, namely, the inter-pupil vector (IPV) and the image center-inter pupil center vector (IC-IPCV) to improve gaze estimation precision under natural head movement. A support vector regression (SVR) based estimation method using image plane features along with traditional pupil center-cornea reflection (PC-CR) vector is also proposed to estimate the gaze. The designed <b>gaze</b> <b>tracking</b> system can work in real-time and achieve an overall estimation accuracy of 0. 84 º with still head and 2. 26 º under natural head movement. By using the SVR method for off-line processing, the estimation accuracy with head movement can be improved to 1. 12 º while providing a tolerance of 10 cm× 8 cm× 5 cm head movement...|$|E
40|$|In {{this paper}} a review is {{presented}} {{of the research}} on eye gaze estimation techniques and applications, that has progressed in diverse ways over the past two decades. Several generic eye gaze use-cases are identified: desktop, TV, head-mounted, automotive and handheld devices. Analysis of the literature leads to the identification of several platform specific factors that influence <b>gaze</b> <b>tracking</b> accuracy. A key outcome from this review is the realization of a need to develop standardized methodologies for performance evaluation of <b>gaze</b> <b>tracking</b> systems and achieve consistency in their specification and comparative evaluation. To address this need, the concept of a methodological framework for practical evaluation of different <b>gaze</b> <b>tracking</b> systems is proposed. Comment: 25 pages, 13 figures, Accepted for publication in IEEE Access in July 201...|$|E
40|$|<b>Gaze</b> <b>tracking</b> systems usually utilize {{near-infrared}} (NIR) {{lights and}} NIR cameras, {{and the performance}} of such systems is mainly affected by external light sources that include NIR components. This is ascribed to the production of additional (imposter) corneal specular reflection (SR) caused by the external light, which makes it difficult to discriminate between the correct SR as caused by the NIR illuminator of the <b>gaze</b> <b>tracking</b> system and the imposter SR. To overcome this problem, a new method is proposed for determining the correct SR in the presence of external light based on the relationship between the corneal SR and the pupil movable area with the relative position of the pupil and the corneal SR. The experimental results showed that the proposed method makes the <b>gaze</b> <b>tracking</b> system robust to the existence of external light...|$|E
40|$|Sexual {{selection}} {{may have}} shaped male visual sensitivity to characteristics that {{provide information about}} female mate quality. Indeed, men judge certain facial and bodily configurations of women to be attractive, possibly because those configurations signal health and fertility. Most of this evidence derives {{from the study of}} women's facial and body photographs. We tested the hypothesis that attractive female dancers receive greater visual attention from men than do unattractive dancers. Twenty-nine men viewed video pairs of pre-categorized high and low attractive female dancers. Their eye <b>gaze</b> was <b>tracked</b> and they also provided ratings of attractiveness, femininity, and dance movement harmony. High attractive dancers received greater visual attention than did low attractive dancers and men's visual attention correlated positively with their judgments of attractiveness, femininity, and dance movement harmony. We discuss our findings {{in the context of the}} ‘beauty captures the mind of the beholder’ hypothesis and the role of dance movements in human mate selection...|$|R
60|$|In the {{meanwhile}} {{the men and}} oxen had conveyed the big log up the slope, and, while Nasmyth drove the beasts back along the skidded track, it swung out over the chasm {{at the end of}} a rope. Men leaning out from fragile stages clutched at and guided it, and when one of them shouted, Nasmyth cast the chain to which the rope was fastened loose from his oxen. Then little lithe figures crawled out along the beams of the trestle, and there was a ringing of hammers. Gordon, who <b>gazed</b> up the <b>track,</b> swung his arm up in warning.|$|R
40|$|Abstract—We {{describe}} the design, implementation {{and use of}} a middleware system, called DiscoRT, to support the development of virtual and robotic conversational agents. The use cases for this system include handling conversational and event-based interrup-tions, and supporting engagement maintenance behaviors, such as turn-taking, backchanneling, directed <b>gaze</b> and face <b>tracking.</b> The multi-threaded architecture of the system includes both “hard” and “soft ” real-time scheduling and integrates with an existing collaborative discourse manager, called Disco. We have used the system to build a substantial conversational agent that is about to undergo long-term field studies. Keywords-engagement; interruption; turn-taking; backchannel; barge-in; schema; BML; arbitration. I...|$|R
