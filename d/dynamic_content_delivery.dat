17|5120|Public
40|$|Web {{sites have}} {{gradually}} shifted from delivering just static html pages and images to customized, user-specific content and plethora of online services. The new features and facilities are {{made possible by}} dynamic content which is produced at request time. Multi-tiered database-driven web sites form the predominant infrastructure for most structured and scalable approaches to <b>dynamic</b> <b>content</b> <b>delivery.</b> However, even with these scalable approaches, the request-time computation and high resource demands for dynamic content generation result in significantly higher latencies and lower throughputs than for sites with just static content. This thesis proposes the caching of dynamic content as the solution for improving the performance of web sites with significant amount of dynamic content. This work shows that there is significant locality in the data accesses and computations for content generation which can be exploited by caching to improve performance. This work introduces a novel multi-tier caching architecture that incorporates multiple, independent caching components to enable easy deployment and effective performance over the prevalent multi-tiered database-driven architecture for <b>dynamic</b> <b>content</b> <b>delivery.</b> The dynamic content infrastructure and the proposed caching strategy is evaluated with e-commerce workloads from the TPC-W benchmark. The evaluation of the system without caching shows that content generation overheads are dominated by the database component for e-commerce workloads. With multi-tier caching, each caching component overcomes specific overheads during content generation while the combination provides overall improvements in performance significantly greater than the individual contributions. The increased peak throughputs with caching range from 1. 58 to 8. 72 times the peak throughputs without caching at similar or significantly reduced average response times. At the same load as for the peak throughputs without caching, the response times were reduced by 90 % to 97 % with caching. The evaluations also establish {{the effectiveness of the}} strategy in relation to variation in platform and site configurations. Overall, the proposed multi-tier caching strategy brings about dramatic improvements in performance for <b>dynamic</b> <b>content</b> <b>delivery...</b>|$|E
40|$|Web service {{deployment}} is {{hampered by}} the possibility of sudden variations in request volumes. Mechanisms exist to enhance scalability in times of heavy load when the delivered content is static. However, web services typically involve dynamic content, delivered through application servers which may have little to no support for adapting to varying loads in order to ensure timely delivery. In this paper we discuss why scaling <b>dynamic</b> <b>content</b> <b>delivery</b> under load is difficult, we present a technique for controlled service degradation to achieve this scalability, and we present experimental results evaluating its benefits...|$|E
40|$|This paper {{investigates the}} roles of {{front-end}} (proxy) servers in improving user-perceived performance of dynamic content distribution. Using Bing and Google search services as two case studies, we perform extensive network measurement and analysis to understand several key factors that affect the overall user-perceived performance. In particular, we develop a simple model-based inference framework to indirectly measure and quantify the (directly unobservable) “frontend-to-backend fetching time ” comprised of the query processing time at back-end data centers and the delivery time between the back-end data centers and front-end servers. We show that this fetching time plays {{a critical role in}} the end-to-end performance of <b>dynamic</b> <b>content</b> <b>delivery...</b>|$|E
40|$|Abstract. <b>Dynamic</b> Web <b>content</b> is {{increasing}} in popularity and, by its nature, {{is harder to}} scale than static content. As a result, <b>dynamic</b> Web <b>content</b> <b>delivery</b> degrades more rapidly than static content under similar client request rates. Many techniques have been explored for effectively handling heavy Web request traffic. In this paper, we concentrate on <b>dynamic</b> <b>content</b> degradation, believing that it offers a good balance between minimising {{total cost of ownership}} and maximising scalability. We describe an algorithm for <b>dynamic</b> <b>content</b> degradation that is easily implemented on top of existing mainstream Web application architectures. The algorithm is based on measuring the elapsed time of content generation. We demonstrate the algorithm’s adaptability against two traffic request patterns, and explore behavioural changes when varying the algorithm’s key parameters. We find our elapsed time based algorithm is better at recognising when the server is unloaded, that the supporting architecture limits the effectiveness of the algorithm and and that the algorithm must be configured pessimistically for best results under load. ...|$|R
40|$|Fragment-based caching {{has been}} {{proposed}} as a promising technique for <b>dynamic</b> Web <b>content</b> <b>delivery</b> and caching [2, 3, 4]. Most of these approaches either assume the fragment-based content is served by Web server automatically, or look at server-side caching only. There is no method of extracting fragments from an existing <b>dynamic</b> Web <b>content,</b> which is of great importance {{to the success of}} fragment-based caching. Also, current technologies for supporting dynamic fragments do not allow to take into account changes in fragment spatiality, which is a popular technique in dynamic and personalized Web site design. This paper describes our effort to address these shortcomings. The first, DyCA, a <b>Dynamic</b> <b>Content</b> Adapter, is a tool for creating fragment-based <b>content</b> from original <b>dynamic</b> <b>content.</b> Our second proposal is an augmentation to the ESI standard that will allow it to support looking up fragment locations in a mapping table that comes attached with the template. This allows the fragments to move across the document without needing to reserve the template...|$|R
40|$|<b>Dynamic</b> Web <b>content</b> is {{increasing}} in popularity, and by its nature, {{is harder to}} scale than static content. As a result, <b>dynamic</b> web <b>content</b> <b>delivery</b> degrades more rapidly than static content under similar client request rates. Many techniques have been explored for effectively handling heavy Web request traffic. In this paper, we concentrate on <b>dynamic</b> <b>content</b> degradation, believing that it offers a good balance between minimizing {{total cost of ownership}} and maximizing scalability. We describe an algorithm for <b>dynamic</b> <b>content</b> degradation that is easily implementable on top of existing, mainstream web application architectures. The algorithm is based on measuring elapsed-time of content generation. We demonstrate the algorithm’s adaptability against two request traffic patterns, and explore behavioural changes when varying the algorithm's key parameters. We find our elapsed-time based algorithm is better at recognizing when the server is unloaded, that the supporting architecture limits the effectiveness of the algorithm and and that the algorithm must be configured pessimistically for best results under load...|$|R
40|$|Response time is {{essential}} to many Web applications. Consequently, many database-driven Web applications rely on data centers that host applications and database contents. Such IT infrastructure enables generation of requested pages at locations much close to the end-users, avoiding network latency. However, it the has additional challenges of database/data center synchronization and data freshness. In this paper, we describe the deployment of NEC’s CachePortal dynamic content caching technology on a database-driven Web site in a data center-based distribution infrastructure. The new system architecture has been experimentally evaluated and {{the results show that}} the deployment of NEC’sCachePortal accelerates the <b>dynamic</b> <b>content</b> <b>delivery</b> up to 7 times while maintaining the same level or better content freshness...|$|E
40|$|We {{explore the}} {{opportunities}} and design options enabled by novel SDN and NFV technologies, by re-designing a <b>dynamic</b> <b>Content</b> <b>Delivery</b> Network (CDN) service. Our system, named MOSTO, provides performance levels {{comparable to that}} of a regular CDN, but does not require the deployment of a large distributed infrastructure. In the process of designing the system, we identify relevant functions that could be integrated in the future Internet infrastructure. Such functions greatly simplify the design and effectiveness of services such as MOSTO. We demonstrate our system using a mixture of simulation, emulation, testbed experiments and by realizing a proof-of-concept deployment in a planet-wide commercial cloud system. Comment: Extended version of the paper accepted for publication in JSAC special issue on Emerging Technologies in Software-Driven Communication - November 201...|$|E
40|$|As Web {{technologies}} advance, the porting {{and adaptation}} of existing Web applications {{to take advantage}} of the advancement has become an issue of increasing importance. Examples of such technology advancement include extensible architectural designs, more efficient caching protocols, and provision for customizable <b>dynamic</b> <b>content</b> <b>delivery.</b> This paper presents an experience report on the migration of legacy IBM ® Net. Data ® based applications to new enterprise Java ™ environments. In this respect, a Net. Data application is refactored into JavaBeans ™ (Model), JavaServer Pages™ (View), and Java Servlet ™ (Controller). To evaluate the effectiveness of the migration methodology, a tool has been developed to support the automatic translation of Net. Data to JavaServer Pages. Using such a tool, a case study is presented to deal with IBM WebSphere® Commerce applications...|$|E
40|$|The Akamai {{platform}} is {{a network of}} over 73, 000 servers supporting numerous web infrastructure services including the distribution of static and <b>dynamic</b> HTTP <b>content,</b> <b>delivery</b> of live and on-demand streaming media, high-availability storage, accelerated web applications, and intelligent routing. The maintenance of such a network requires significant monitoring infrastructure to enable detailed understanding of its state at all times. For that purpose, Akamai has developed and uses Query, a distributed monitoring system in which all Akamai machines participate. Query collects data {{at the edges of}} the Internet and aggregates it at several hundred places to be used to answer SQL queries about the state of the Akamai network. We explain the design of Query, outline some of its critical features, discuss who some of its users are and what Query allows them to do, and explain how Query scales to meet demand as the Akamai network grows. ...|$|R
40|$|The {{bottleneck}} of Web <b>content</b> <b>delivery</b> is {{not serving}} static <b>content</b> but <b>dynamic</b> <b>content</b> which requires data processing. We propose using a database cache to offload the demand on a database-driven Web service. This paper presents {{the design of}} a database cache and preliminary results on the overhead of maintaining such a cache...|$|R
40|$|We {{report on}} a {{high-performance}} in-kernel web server for Linux known as the Threaded linUX http layer, or TUX, for short. TUX uses aggressive network layer data caching to accelerate static <b>content</b> <b>delivery,</b> and invokes CGI scripts directly from the kernel to accelerate <b>dynamic</b> <b>content</b> generation. We describe the TUX web server architecture, modifications included in the patch, and how they affect kernel operation and web server performance...|$|R
30|$|The {{paper is}} {{structured}} as follows: In {{the following section}} we introduce related work on replication strategies for bio-inspired distribution networks. To emphasize the relevance of our problem statement, we elaborate {{the context of the}} targeted problem followed by {{a detailed description of the}} hormone-based delivery and replication approach. A special focus is given on the different replacement mechanisms. We further consider the constraint that a clean-up action must never delete the last instance of a unit. The described strategies are implemented in a simulation model of a network of multimedia servers with limited storage and consumers requesting content according to a preference model. The simulation results allow for a quantitative comparison of the efficiency of the different variants. We evaluated realistic scenarios involving node churn and networks of different size and structure. The conclusion summarizes our recommendation for effective replication and replacement strategies in <b>dynamic</b> <b>content</b> <b>delivery</b> networks.|$|E
40|$|Information {{overload}} {{and content}} chaos are major challenges for organizations, {{as they have}} to deal with a high amount of unstructured content. With enterprise content management (ECM) systems, a technological solution is developed to deal with such challenges; however, these systems can only provide value to an organization if they are implemented in the context of an ECM strategy. In this paper the implementation of a new ECM strategy at a financial service provider is described to illustrate how organizations can on the one side design an ECM strategy that reduces information overload and content chaos and on the other side implement it successfully. The four keys for successfully implementing ECM based on the lessons learnt derived are an ECM team leading the change process, the acceptance of users by meeting the organization�s business needs, a metadata taxonomy enabling <b>dynamic</b> <b>content</b> <b>delivery,</b> and an effective change management from the outset...|$|E
40|$|Telemedicine {{scenarios}} include today in-hospital care management, remote teleconsulting, collaborative {{diagnosis and}} emergency situations handling. Different {{types of information}} need to be accessed by means of etherogeneous client devices in different communication environments in order to enable high quality continuous sanitary assistance delivery wherever and whenever needed. In this paper, a Web-based telemedicine architecture based on Java, XML and XSL technologies is presented. By providing <b>dynamic</b> <b>content</b> <b>delivery</b> services and Java based client applications for medical data consultation and modification, the system enables effective access to an Electronic Patient Record based standard database by means of any device equipped with a Web browser, such as traditional Personal Computers and workstation as well as modern Personal Digital Assistants. The effectiveness of the proposed architecture has been evaluated in different scenarios, experiencing fixed and mobile clinical data transmissions over Local Area Networks, wireless LANs and wide coverage telecommunication network including GSM and GPRS. ...|$|E
40|$|Abstract—Currently, people gain {{easy access}} to an {{increasingly}} diverse range of mobile devices such as personal digital assistants (PDAs), smart phones, and handheld computers. As <b>dynamic</b> <b>content</b> has become dominant on the fast-growing World Wide Web [24], {{it is necessary to}} provide effective ways for the users to access such prevalent Web content in a mobile computing environment. During a course of browsing <b>dynamic</b> <b>content</b> on mobile devices, the requested content is first dynamically generated by remote Web server, then transmitted over a wireless network, and, finally, adapted for display on small screens. This leads to considerable latency and processing load on mobile devices. By integrating a novel Web content adaptation algorithm and an enhanced caching strategy, we propose an adaptive scheme called MobiDNA for serving <b>dynamic</b> <b>content</b> in a mobile computing environment. To validate the feasibility and effectiveness of the proposed MobiDNA system, we construct an experimental testbed to investigate its performance. Experimental results demonstrate that this scheme can effectively improve mobile <b>dynamic</b> <b>content</b> browsing, by improving Web content readability on small displays, decreasing mobile browsing latency, and reducing wireless bandwidth consumption. Index Terms—Mobile computing, adaptive <b>content</b> <b>delivery,</b> <b>dynamic</b> <b>content,</b> small form factors, Web content adaptation, fragment caching. Ç...|$|R
40|$|With {{the wide}} {{availability}} of <b>content</b> <b>delivery</b> networks, many e-commerce Web applications utilize edge cache servers to cache and deliver <b>dynamic</b> <b>contents</b> at loca-tions {{much closer to}} users, avoiding network latency. By caching {{a large number of}} <b>dynamic</b> <b>content</b> pages in the edge cache servers, response time can be reduced, benefit-ing from higher cache hit rates. However, this is achieved at the expense of higher invalidation cost. On the other hand, a higher invalidation cost leads to a longer invali-dation cycle (time to perform the invalidation check on the pages in caches) at the expense of freshness of cached dy-namic content. In this paper, we propose a freshness-driven adaptive <b>dynamic</b> <b>content</b> caching technique, which mon-itors response time and invalidation cycle length and dy-namically adjusts caching policies. We have implemented the proposed technique within NEC’s CachePortal Web ac-celeration solution. The experimental results show that the proposed technique consistently maintains the best content freshness to users. The experimental results also show that even a Web site with <b>dynamic</b> <b>content</b> caching enabled can further benefit from deployment of our solution with im-provement of its content freshness up to 10 times especially during heavy traffic...|$|R
40|$|The Windows Azure Platform has rapidly {{established}} {{itself as}} one of the most sophisticated cloud computing platforms available. With Microsoft working to continually update their product and keep it at the cutting edge, the future looks bright - if you have the skills to harness it. In particular, new features such as remote desktop access, <b>dynamic</b> <b>content</b> caching and secure <b>content</b> <b>delivery</b> using SSL make the latest version of Azure a more powerful solution than ever before. It's widely agreed that cloud computing has produced a paradigm shift in traditional architectural concepts by providi...|$|R
40|$|An {{introduction}} to the Apache source code layout • An {{introduction to}} key concepts such as modules, buckets, brigades, hooks, filters, and MPMs • An introduction to several tools {{that can be used}} to perform post-mortem analysis • An introduction to several tools {{that can be used to}} debug transient server problems Apache layout What is Apache? • Flexible and extensible opensource web server • Supports HTTP versions 0. 9, 1. 0 and 1. 1 • Ships with a variety of modules that can be used to secure communications and transform content • Supports virtual hosting • Supports static and <b>dynamic</b> <b>content</b> <b>delivery</b> • The list goes on and on … How big is Apache? • 150 k+ lines of C code*: find. -name. c-exec egrep-v '(^ []+|^||) | wc-l 155884 • 480 + source code files: find. -name. c-ls | wc-l 488 • 240 + header files: find. -name. h-ls | wc-...|$|E
40|$|As {{web sites}} {{increasingly}} deliver dynamic content, {{the process of}} content generation at request time is becoming a severe limitation to web site throughput. Recent {{studies have shown that}} much of the dynamic content is, however, better characterized as pseudo-dynamic, i. e., a dynamic composition of stored or static data. Consequently, caching the generated web pages may increase the web server's throughput if there is some temporal locality in the request stream. In this paper, we perform a quantitative analysis of the benefits of caching for dynamic content using the e-commerce benchmark, TPC-W,as the workload. We implement caching through a simple and efficient Apache extension module, DCache, that can be easily incorporated into the current infrastructure for <b>dynamic</b> <b>content</b> <b>delivery.</b> Our DCache module uses conventional expiration times and our own request-initiated invalidation scheme as the methods for keeping the cache consistent. It also supports site-specific optimization by providing a mechanism to incorporate the priorities of specific web pages into the caching scheme. Our experiments show that we can obtain over 3 times the non-caching throughput with our caching approach...|$|E
40|$|Abstract: For many {{e-commerce}} applications, {{web pages}} are created dynamically {{based on the}} current state of a business, such as product prices and inventory, stored in database systems. This characteristic requires e-commerce websites to deploy and integrate web servers, application servers, and database systems at the backend. Response time is essential to many e-commerce applications. With increasing availability and advancement of content delivery networks (CDN), many database-driven web applications rely on data centers that host applications and database contents. Such IT infrastructure enables the generation of requested pages at locations much closer to the endusers, thus reducing network latency. However, it incurs additional complexity associated with database–data center synchronization and data freshness. In this paper, the deployment of NEC’s CachePortal dynamic content caching technology on a database-driven website in a data center-based distribution infrastructure is described. The new system architecture has been experimentally evaluated and the results show that the deployment of NEC’s CachePortal accelerates the <b>dynamic</b> <b>content</b> <b>delivery</b> up to seven times while attaining a high leve...|$|E
40|$|Internet {{infrastructure}} {{is a key}} enabler of e-business. The infrastructure consists of backbone networks (such as UUNET and AT&T), access networks (such as AOL and Earthlink), <b>Content</b> <b>Delivery</b> Networks (CDNs- such as Akamai and Digital Island) and other cache operators. Together, all the players make up the digital supply chain for information goods. Caches provisioned by the access networks and the CDNs are the storage centers- the digital equivalent of warehouses. These caches store and deliver information {{from the edge of}} the network, bringing efficiency and responsiveness to <b>content</b> <b>delivery</b> on a global scale. The benefits of caching to content publishers, namely scalable <b>content</b> <b>delivery,</b> reduction in bandwidth costs and improvements in response times, are well recognized. Yet, caching has not been fully embraced by content publishers since its use can interfere with site personalization strategies, <b>dynamic</b> <b>content</b> creation and the potential loss of information about visitors to the site. Recent work on web caching has focused on the technological advances required to address these deficiencies. However, there has been no work on the managerial issues related to the design of incentive compatible caching services, appropriate pricing schemes and associated resourc...|$|R
40|$|Strong replica {{consistency}} models {{ensure that}} the data delivered by a replica always includes the latest updates, although this may result in poor response times. On the other hand, weak replica consistency models provide quicker access to information, but do not usually provide guarantees about the degree of staleness in the data they deliver. In order to support emerging distributed applications that are characterized by high concurrency demands, increasing shift towards <b>dynamic</b> <b>content,</b> and timely <b>delivery,</b> we need quality-of-service models {{that allow us to}} explore the intermediate space between these two extreme approaches to replica consistency. Further, to better support time-sensitive applications that can tolerate relaxed consistency in exchange for better responsiveness, we need to understand how the desired level of consistency affects the timeliness of a response. The QoS model we have developed to realize these objectives considers both timeliness and consistency, and treats consistency along two dimensions: order and staleness. In this paper, we experimentally evaluate the framework we have developed to study the timeliness/consistency tradeoffs for replicated services and present experimental results that compare these tradeoffs in the context of sequential and FIFO ordering...|$|R
40|$|The fast {{development}} of web services, or more broadly, service-oriented architectures (SOAs), has prompted more organizations to move contents and applications {{out to the}} Web. Softwares on the web allow one to enjoy a variety of services, for example translating texts into other languages and converting a document from one format to another. In this paper, we {{address the problem of}} maintaining data integrity and confidentiality in web <b>content</b> <b>delivery</b> when <b>dynamic</b> <b>content</b> modifications are needed. We propose a flexible and scalable model for secure <b>content</b> <b>delivery</b> based on the use of roles and role certificates to manage web intermediaries. The proxies coordinate themselves in order to process and deliver contents, and the integrity of the delivered content is enforced using a decentralized strategy. To achieve this, we utilize a distributed role lookup table and a role-number based routing mechanism. We give an efficient secure protocol, iDeliver, for <b>content</b> processing and <b>delivery,</b> and also describe a method for securely updating role lookup tables. Our solution also applies to the security problem in web-based workflows, for example maintaining the data integrity in automated trading, contract authorization, and supply chain management in large organizations...|$|R
40|$|Large scale {{distributed}} applications typically involve {{a number of}} nodes, which may be spread over a large geographical area. The Client-Server(CS) paradigm has been found useful for designing {{distributed applications}}. However, CS approach does not scale well {{when the number of}} participating nodes increase and build complex relations with one another. Also, increase in geographical spread and unreliable network connectivity pose problems for CS implementations. Mobile Agents (MA) is emerging as a useful paradigm for overcoming the above limitations [6]. We have designed and implemented a large distributed application, viz. distance evaluation, which uses MA as the underlying design approach. Our system provides the infrastructure for conducting computer based testing of students who may be dispersed around the globe. In this paper we present the software engineering lessons learnt from our implementation and put forth the various issues that emerge. We show that MA paradigm can be exploited for effective structuring of large-scale distributed applications. We claim that, compared to other approaches, MA based approach gives us several advantages such as: scalability, flexible structuring, dynamic extensibility, push-pull modes of information dissemination, transparency to varying communication channels, application layer multicasting, and <b>dynamic</b> <b>content</b> <b>delivery...</b>|$|E
40|$|There is {{a growing}} trend of using mobile devices in retail stores. Access to instant product related {{information}} is desirable by consumers in making a purchase decision. This information may include offers or sales discount associated with a product, comparing prices, ingredients or materials composition and many more. Retail business owners have seen this as a big opportunity to increase their in-store sales and they are adapting various technologies to deliver helpful contents and information to consumer. This thesis project aims to design and develop a location based <b>dynamic</b> <b>content</b> <b>delivery</b> system for the consumers of retail stores and supermarkets. A system architecture is proposed here which makes use of iBeacon to capture customer’s location based contextual information and then deliver relevant contents back to the customer. In order to design and develop the system, iBeacon technology and in-store environment were studied in depth. Choice of techniques and technologies {{that can be used}} to develop such a system is demonstrated here along with the important factors to consider while putting the system in use. It is concluded that in the right circumstance this design can be adapted to implement a context aware mobile application for stores and supermarkets...|$|E
40|$|The growing {{ubiquity of}} Internet and cloud {{computing}} is having {{significant impact on}} mediarelated industries. These industries are using the Internet and cloud as a medium to enable creation, search, management, and consumption of their content. Rich web pages, software downloads, interactive communications, and ever-expanding universe of digital media require {{a new approach to}} multimedia (e. g., audio, video, images, etc.) content sharing and management. Size and volume of multimedia content is growing exponentially. Though there are many online videohosting services (YouTube, flickr, dailymotion, etc.) for storing and viewing multimedia content, there is clear lack of software platforms that can help end-users in content authoring and sharing activities in a social setting. This paper introduces the principle concepts of multimedia content sharing in a social setting and presents a novel platform that supports cloud-based collaborative video story authoring and telling. The platform leverages social networks for video search and recommendation for the purpose of story telling. The Social Content Authoring and Sharing Platform (SCASP) leverages MediaWise Cloud Content Orchestrator (MCCO) that supports do-it-yourself creation, search, management, and consumption of multimedia content. MCCO supports <b>dynamic</b> <b>content</b> <b>delivery</b> using CPU and storage services available in the public cloud. The platform exploits flexible content distribution capabilities of MCCO for a group of possibly geographically distributed end-users who collaborate on a specific task such as video story telling...|$|E
40|$|AbstractIn present work, the {{theoretical}} and development {{details of a}} child-friendly mobile application to enhance cognitive skills of preschool students are presented. Cognitive skills are {{a vital part of}} the curriculum at all level of education especially preschool learning. Currently considerable numbers of mobile applications are available for child learning that claim educational goals but they only dispense information without considering pedagogical theories and models. In contrast those applications that were primarily developed to achieve educational goals are only available to a selected audience mainly researchers. Also the nature of existing mobile application is static <b>content</b> <b>delivery</b> in which previously created contents are stored for repetitive use. This static nature jeopardized individualized and cognitive skills learning. The objectives of the present work is to overcome this static nature of <b>content</b> <b>delivery</b> by proposing a knowledge model named OntoCog with mobile application for <b>dynamic</b> <b>content</b> creation that follows constructive pedagogical theories. A mobile application named Cognitive Skills (CogSkills) is developed to evaluate proposed model. As a result the dynamic nature of the CogSkills provides a clear advantage over existing static nature mobile applications...|$|R
40|$|Abstract: Response {{time is a}} key {{differentiation}} point among {{electronic commerce}} (e-commerce) applications. For many e-commerce applications, web pages are created dynamically based on {{the current state of}} a business stored in database systems. To improve the response time, many e-commerce websites deploy caching solutions for acceleration of <b>content</b> <b>delivery.</b> There are multiple tiers in the <b>content</b> <b>delivery</b> infrastructure where cache servers can be deployed, including (1) data caching (in data centers), (2) content page caching (in edge or front end caches), and (3) database query result set caching (between application servers and DBMS). The architecture of database-driven e-commerce websites is more complex than that of typical websites. It requires the integration of web servers, application servers, and back end database systems as well as <b>dynamic</b> <b>content</b> caching solutions. In this paper, the issues associated with the management of content page caching and database query result set caching tiers are studied. It is observed that caching management for these two tiers have their unique characteristics. It is because cached object types and information available for the caching management in th...|$|R
40|$|Abstract—According to {{increasing}} performance of mobile devices, like smart phone, tablet PC and etc, and diffusing network infrastructures, like LTE, WiFi and etc, {{various types of}} <b>content</b> <b>delivery</b> services based on PC services can serve into mobile devices using cloud. In this paper we proposed <b>content</b> <b>delivery</b> framework with SDN (Software Defined Networking) and CCN (Content Centric Networking) to improve <b>content</b> <b>delivery</b> QoS in mobile cloud environment. Additionally to serve autonomic optimal services, we proposed reinforcement learning based context-aware <b>content</b> <b>delivery</b> scheme. Using our framework, we can guarantee QoS to provide context-aware <b>content</b> <b>delivery</b> scheme...|$|R
40|$|Abstract—In recent years, edge {{computing}} {{has emerged as}} a popular mechanism to deliver dynamic Web content to clients. However, many existing edge cache networks {{have not been able to}} harness the full potential of {{edge computing}} technology. In this paper, we argue and experimentally demonstrate that cooperation among the individual edge caches coupled with scalable serverdriven document consistency mechanisms can significantly enhance the capabilities and performance of edge cache networks in delivering fresh dynamic content. However, designing large-scale cooperative edge cache networks presents many research challenges. Toward addressing these challenges, this paper presents cooperative edge cache grid (cooperative EC grid, for short) —a large-scale cooperative edge cache network for efficiently delivering highly dynamic Web content with varying server update frequencies. The design of the cooperative EC grid focuses on the scalability and reliability of <b>dynamic</b> <b>content</b> <b>delivery</b> in addition to cache hit rates, and it incorporates several novel features. We introduce the concept of cache clouds as a generic framework of cooperation in large-scale edge cache networks. The architectural design of the cache clouds includes dynamic hashing-based document lookup and update protocols, which dynamically balance lookup and update loads among the caches in the cloud. We also present cooperative techniques for making the document lookup and update protocols resilient to the failures of individual caches. This paper reports a series of simulation-based experiments which show that the overheads of cooperation in the cooperative EC grid are very low, and our architecture and techniques enhance the performance of the cooperative edge networks. Index Terms—Dynamic content caching, edge computing, cooperative caching, cache clouds. ...|$|E
50|$|Specialist {{networks}} {{known as}} <b>content</b> <b>delivery</b> networks help distribute content over the Internet by ensuring both high availability and high performance. Alternative technologies for <b>content</b> <b>delivery</b> include peer-to-peer file sharing technologies. Alternatively, <b>content</b> <b>delivery</b> platforms create and syndicate content remotely, acting like hosted content management systems.|$|R
40|$|Abstract—With the {{proliferation}} of cloud services, cloud-based systems can become a cost-effective means of on-demand <b>content</b> <b>delivery.</b> In order to make best use of the available cloud bandwidth and storage resources, content distributors {{need to have a}} good understanding of the tradeoffs between various system design choices. In this work we consider a peer-assisted <b>content</b> <b>delivery</b> system that aims to provide guaranteed average download rate to its customers. We show that bandwidth demand peaks for contents with moderate popularity, and identify these contents as candidates for cloud-based service. We then consider <b>dynamic</b> <b>content</b> bundling (inflation) and cross-swarm seeding, which were recently proposed to improve download performance, and evaluate their impact on the optimal choice of cloud service use. We find that much of the benefits from peer seeding can be achieved with careful torrent inflation, and that hybrid policies that combine bundling and peer seeding often reduce the delivery costs by 20 % relative to only using seeding. Furthermore, all these peer-assisted policies reduce the number of files that would need to be pushed to the cloud. Finally, we show that careful system design is needed if locality is an important criterion when choosing cloud-based service provisioning. I...|$|R
40|$|<b>Content</b> <b>delivery</b> {{networks}} (CDNs) {{improve the}} scalability of accessing static and, recently, streaming content. However, proxy caching can improve {{access to these}} types of content as well. A unique value of CDNs is therefore in improving performance of accesses to <b>dynamic</b> <b>content</b> and other computer applications. We describe an architecture, algorithms, and a preliminary performance study of a CDN for applications (ACDN). Our system includes novel algorithms for automatic redeployment of applications on networked servers as required by changing demand and for distributing client requests among application replicas based on their load and proximity. The system also incorporates a mechanism for keeping application replicas consistent {{in the presence of}} developer updates to the content. A prototype of the system has been implemented...|$|R
40|$|In {{the span}} {{of only a few}} years, the Internet has {{experienced}} an astronomical increase in the use of specialized <b>content</b> <b>delivery</b> systems, such as <b>content</b> <b>delivery</b> networks and peer-to-peer file sharing systems. Therefore, an understanding of <b>content</b> <b>delivery</b> on the Internet now requires a detailed understanding of how these systems are used in practice. This paper examines <b>content</b> <b>delivery</b> {{from the point of view}} of four <b>content</b> <b>delivery</b> systems: HTTP web traffic, the Akamai <b>content</b> <b>delivery</b> network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60, 000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new <b>content</b> <b>delivery</b> systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. ...|$|R
40|$|In {{the span}} {{of only a few}} years, the Internet has {{experienced}} an astronomical increase in the use of specialized <b>content</b> <b>delivery</b> systems, such as <b>content</b> <b>delivery</b> networks and peer-to-peer file sharing systems. Therefore, an understanding of <b>content</b> <b>delivery</b> on the Internet now requires a detailed understanding of how these systems are used in practice...|$|R
