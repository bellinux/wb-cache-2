2|1234|Public
40|$|This paper {{presents}} a memory management system offering real-time guarantees for Java object access, along with accurately predictable memory management functions. The {{primary goal of}} the design is to allow precise worst-case execution time prediction for all memory management and object access functions. The system is designed to work with fragmented memory and offers simplified reference checking for scoped memory implementations. The presented linking model uses special runtime data structures which enable deterministic execution of object access instructions. While maintaining predictability, the system offers quick access to object data in memory; thus reducing the complexity required to implement an <b>instruction</b> <b>execution</b> <b>unit.</b> The results presented include time guarantees achieved for memory management and object access functions, size of the runtime image and synthesis estimates for area and speed...|$|E
40|$|While {{the use of}} RNS has {{provided}} groundbreaking theory and progress in this field, the applications still lack viable testing platforms to test and verify the theory. This Thesis outlines the processing of developing an instruction set architecture (ISA) and an <b>instruction</b> <b>execution</b> <b>unit</b> (IEU) {{to help make the}} first residue based general processor a viable testing platform to address the mentioned problems. Consider a 32 -bit ripple adder. The delay on this device will be 32 N where N is the delay for each adder to complete its operation. The delay of this process is due to the need to propagate each carry signal generated by each adder to the next one. This was solved by the creation of the Carry Look Ahead (CLA), which could drastically reduce the delay by 2 / 3. However, like the ripple adder, the CLA is still encumbered by propagation delay. A residue processor in the same situation would have a delay of 1 N regardless of bit size since carry propagation is no longer a concern. The Thesis discusses how prior challenges using residue number systems in computers has been overcome by Digital System Research (DSR) ...|$|E
5000|$|... "Each POWER7 {{processor}} core implements aggressive out-of-order (OoO) instructionexecution to drive high efficiency {{in the use}} of available execution paths. The POWER7processor has an Instruction Sequence Unit that is capable of dispatching up to sixinstructions per cycle to a set of queues. Up to eight instructions per cycle can be issued tothe <b>Instruction</b> <b>Execution</b> <b>units.</b> The POWER7 processor has a set of twelve execution unitsas above" ...|$|R
40|$|Real-time {{systems are}} {{characterized}} {{by the presence of}} timing constraints in which a task must be completed within a specific amount of time. This paper examines the problem of determining the bound on the worst case execution time (WCET) of a given program on a given processor. There are two important issues in solving this problem: (i) program path analysis, which determines what sequence of instructions will be executed in the worst case, and (ii) microarchitecture modeling, which models the hardware system and determines the WCET of a known sequence of instructions. To obtain a tight estimate on the bound, both these issues must be addressed accurately and efficiently. The latter is becoming difficult to model for modern processors due to the presence of pipelined <b>instruction</b> <b>execution</b> <b>units</b> and cached memory systems. Because of the complexity of the problem, all existing methods that we know of focus only on one of above issues. This limits the accuracy of the estimated bound and [...] ...|$|R
50|$|Each of the {{instruction}} queues can accept up to four instructions from the decoder, avoiding any bottlenecks. The instruction queues issue their <b>instructions</b> to their <b>execution</b> <b>units</b> dynamically depending {{on the availability of}} operands and resources. Each of the queues except for the load/store queue can issue up to two instructions every cycle to its <b>execution</b> <b>units.</b> The load/store queue can only issue one instruction. The R10000 can thus issue up to five instructions every cycle.|$|R
2500|$|PPE {{consists}} of three main units: <b>Instruction</b> <b>Unit</b> (IU), <b>Execution</b> <b>Unit</b> (XU) and vector/scalar <b>execution</b> <b>unit</b> (VSU). IU contains L1 instruction cache, branch prediction hardware, instruction buffers and dependency checking login. XU contains integer <b>execution</b> <b>units</b> (FXU) and load-store unit (LSU). VSU contains all of the execution resources for FPU and VMX. [...] Each PPE can complete two double precision operations per clock cycle using a scalar fused-multiply-add instruction, which translates to 6.4GFLOPS at 3.2GHz; or eight single precision operations per clock cycle with a vector fused-multiply-add instruction, which translates to 25.6GFLOPS at 3.2GHz.|$|R
5000|$|Execution {{begins in}} stage four. The {{instruction}} queues dispatch {{up to eight}} <b>instructions</b> to the <b>execution</b> <b>units.</b> Integer <b>instructions</b> are executed in three integer <b>execution</b> <b>units</b> (termed [...] "fixed-point units" [...] by IBM). Two of the units are identical and execute all integer instructions except for multiply and divide. All instructions executed by them have a one-cycle latency. The third unit executes multiply and divide instructions. These instructions are not pipelined and have multi-cycle latencies. 64-bit multiply has a nine-cycle latency and 64-bit divide has a 37-cycle latency.|$|R
40|$|Superscalar {{architecture}} {{resulting in}} aggressive performance is a proven architecture for general purpose computation. The negative {{side effect of}} aggressive performance {{is the need for}} higher number of register read/write ports to supply operands to multiple execution units; the need to resolve false data dependence and true data dependence; the need to dispatch operand ready <b>instructions</b> to <b>execution</b> <b>units</b> and finally retire out of order executed instructions to program order. A processor architecture is proposed in here to address {{at least some of the}} above negative side effects. This processor architecture is call LITERAL QUEUE ARCHITECTURE(LQA). In here, LITERAL has the meaning of immediate data. In LQA, opcode and operands in an instruction are treated as a self contained structured element and forms the necessary and sufficient condition for <b>instruction</b> <b>execution.</b> Sequence of <b>instructions</b> embedded with LITERALS are treated as elements in a QUEUE. The elements are executed with respect to time and rotated out of the QUEUE while new elements are rotated into the QUEUE...|$|R
50|$|The CPU die {{contains}} {{the majority of}} logic, all of the <b>execution</b> <b>units</b> and a level 0 (L0) <b>instruction</b> cache. The <b>execution</b> <b>units</b> consist of two integer units, address units, floating-point units (FPUs), memory units. The FPU hardware consists of a fused multiply add (FMA) unit and a divide unit. But the FMA instructions are really fused (that is, with a single rounding) only as of SPARC64 VI. The FMA unit is pipelined and has a four-cycle latency and a one-cycle-throughput. The divide unit is not pipelined and has significantly longer latencies. The L0 instruction cache has a capacity of 4 KB, is direct-mapped and has a one-cycle latency.|$|R
5000|$|Assume {{that the}} {{scheduling}} logic will issue an <b>instruction</b> to the <b>execution</b> <b>unit</b> {{when all of}} its register operands are ready. Further assume that registers [...] and [...] are ready: the values in [...] and [...] were computed {{a long time ago}} and have not changed. However, assume [...] is not ready: its value is still in the process of being computed by the [...] (integer divide) instruction. Finally, assume that registers [...] and [...] hold the same value, and thus all the loads and stores in the snippet access the same memory word.|$|R
40|$|Superscalar {{architectural}} techniques increase instruction throughput {{by increasing}} resources and using complex control units that perform various functions to minimize stalls {{and to ensure}} a continuous feed of <b>instructions</b> to the <b>execution</b> <b>units.</b> This work proposes a dynamic scheme to increase efficiency of execution (throughput) by a methodology called block slicing. This takes advantage of instruction level parallelism (ILP) available in programs without {{increasing the number of}} <b>execution</b> <b>units.</b> Implementation of this concept in a wide, superscalar pipelined architecture introduces nominal additional hardware and delay, while offering power and area advantages. We present the design of the hardware required for the implementation of the proposed scheme and evaluate it for the metrics of speed-up, throughput and efficiency...|$|R
50|$|A superscalar {{processor}} is a CPU that implements {{a form of}} parallelism called instruction-level parallelism {{within a}} single processor. In contrast to a scalar processor, which can execute at most one single instruction per clock cycle, a superscalar processor can execute more than one instruction during a clock cycle by simultaneously dispatching multiple <b>instructions</b> to different <b>execution</b> <b>units</b> on the processor. It therefore allows for more throughput (the number of instructions that can be executed in a unit of time) than would otherwise be possible at a given clock rate. Each <b>execution</b> <b>unit</b> is not a separate processor (or a core if the processor is a multi-core processor), but an execution resource within a single CPU such as an arithmetic logic unit.|$|R
5000|$|The Jazz DSP, by Improv Systems, is a VLIW {{embedded}} {{digital signal}} processor architecture with a 2-stage instruction pipeline, and single-cycle <b>execution</b> <b>units.</b> The baseline DSP includes one arithmetic logic unit (ALU), dual memory interfaces, and the control unit (instruction decoder, branch control, task control). Most aspects of the architecture, such as the number and sizes of Memory Interface Units (MIU) or the types and number of Computation Units (CU), datapath width (16 or 32-bit), the number of interrupts and priority levels, and debugging support may be independently configured using a proprietary graphical user interface (GUI) tool. A key feature of the architecture allows the user to add custom <b>instructions</b> and/or custom <b>execution</b> <b>units</b> to enhance {{the performance of their}} application.|$|R
25|$|The 80286 was {{designed}} for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: address unit, bus <b>unit,</b> <b>instruction</b> <b>unit</b> and <b>execution</b> <b>unit,</b> organized into a loosely coupled (buffered) pipeline {{just as in the}} 8086. The significantly increased performance over the 8086 was primarily due to the non-multiplexed address and data buses, more address-calculation hardware (most importantly, a dedicated adder) and a faster (more hardware-based) multiplier. It was produced in a 68-pin package, including PLCC (plastic leaded chip carrier), LCC (leadless chip carrier) and PGA (pin grid array) packages.|$|R
40|$|Abstract: The {{principal}} {{requirement for}} the Model 91 floating-point <b>execution</b> <b>unit</b> was that it be designed to support the instruction-issuing rate of the processor. The chosen solution was to develop separate, instruction-oriented algorithms for the add, multiply, and divide functions. Linked together by the floating-point <b>instruction</b> <b>unit,</b> the multiple <b>execution</b> <b>units</b> provide concurrent <b>instruction</b> <b>execution</b> at the burst rate of one instruction per cycle...|$|R
40|$|The TRIPS {{hardware}} prototype is {{the first}} instantiation of an Explicit Data Graph Execution (EDGE) architecture. Building the compiler, toolset, and system software for the prototype required supporting the system’s unique dataflow construction, its banked register and memory configurations, and its novel Instruction Set Architecture. In particular, the TRIPS ISA includes (i) a block atomic execution model, (ii) explicit mappings of <b>instructions</b> to <b>execution</b> <b>units,</b> and (iii) predicated instructions {{which may or may}} not fire, depending on the outcome of preceding instructions. Our primary goal has been to construct tools to consume standard C and Fortran source code and generate binaries both for the TRIPS software simulators and hardware prototype. A secondary goal has been to build the software infrastructure on standard platforms using standard tools. These goals have been met through a combination of off-the-shelf and custom tools. We present a number of design issues and their resolution in enabling end users to exercise the prototype ISA using familiar tools and programming interfaces. Finally, we offer download instructions for those who wish to test-drive the TRIPS tools. ...|$|R
50|$|The SPARC64 V fetches up {{to eight}} {{instructions}} from the instruction cache during the first stage and places them into a 48-entry instruction buffer. In the next stage, four instructions are taken from this buffer, decoded and issued to the appropriate reserve stations. The SPARC64 V has six reserve stations, two that serve the integer units, one for the address generators, two for the floating-point units, and one for branch instructions. Each integer, address generator and floating-point unit has an eight-entry reserve station. Each reserve station can dispatch an <b>instruction</b> to its <b>execution</b> <b>unit.</b> Which <b>instruction</b> is dispatched firstly depends on operand availability and then its age. Older instructions are given higher priority than newer ones. The reserve stations can dispatch instructions speculatively (speculative dispatch). That is, instructions can be dispatched to the <b>execution</b> <b>units</b> even when their operands are not yet available but will be when execution begins. During stage six, up to six instructions are dispatched.|$|R
50|$|Additional {{internal}} parallelism {{has been}} {{the driving force behind}} improvements in conventional CPU designs. Instead of explicit thread-level parallelism (such as that found in the transputer), CPU designs exploited implicit parallelism at the instruction-level, inspecting code sequences for data dependencies and issuing multiple independent <b>instructions</b> to different <b>execution</b> <b>units.</b> This is known as superscalar processing. Superscalar processors are suited for optimising the execution of sequentially constructed fragments of code. The combination of superscalar processing and speculative execution delivered a tangible performance increase on existing bodies of code - which were mostly written in Pascal, Fortran, C and C++. Given these substantial and regular performance improvements to existing code there was little incentive to rewrite software in languages or coding styles which expose more task-level parallelism.|$|R
40|$|International audienceAudio {{processing}} {{applications are}} among the most computeintensive and often rely on additional DSP resources for realtime performance. However, programmable audio DSPs are in general only available to product developers. Professional audio boards with multiple DSPs usually support specific effects and products while consumer "game-audio" hardware still only implements fixed-function pipelines which evolve at a rather slow pace. The widespread availability and increasing processing power of GPUs could offer an alternative solution. GPU features, like multiply-accumulate <b>instructions</b> or multiple <b>execution</b> <b>units,</b> {{are similar to those of}} most DSPs [3]. Besides, 3 D audio rendering applications require a significant number of geometric calculations, which are a perfect fit for the GPU. Our feasibility study investigates the use of GPUs for efficient audio processing...|$|R
40|$|State {{of the art}} {{microprocessors}} achieve {{high performance}} by executing multiple instructions per cycle. In an out-oforder engine, the instruction scheduler is responsible for dispatching <b>instructions</b> to <b>execution</b> <b>units</b> based on dependencies, latencies, and resource availability. Most existing instruction schedulers are doing a less than optimal job of scheduling memory accesses and instructions dependent on them, for the following reasons: • Memory dependencies cannot be resolved prior to execution, so loads are not advanced ahead of preceding stores. • The dynamic latencies of load instructions are unknown, so scheduling dependent instructions is based on either optimistic load-use delay (may cause re-scheduling and re-execution) or pessimistic delay (creating unnecessary delays). • Memory pipelines are more expensive than other <b>execution</b> <b>units,</b> and as such, are a scarce resource. Currently, {{an increase in the}} memory execution bandwidth is usually achieved through multi-banked caches where bank conflicts limit efficiency. In this paper we present three techniques to address these scheduler limitations. One is to improve the scheduling of load instructions by using a simple memory disambiguation mechanism. The second is to improve the scheduling of load dependent instructions by employing a Data Cache Hit-Miss Predictor to predict the dynamic load latencies. And the third is to improve the efficiency of load scheduling in a multi-banked cache through Cache-Bank Prediction. 1...|$|R
40|$|Introduction Audio {{processing}} {{applications are}} among the most computeintensive and often rely on additional DSP resources for realtime performance. However, programmable audio DSPs are in general only available to product developers. Professional audio boards with multiple DSPs usually support specific effects and products while consumer "game-audio" hardware still only implements fixed-function pipelines which evolve at a rather slow pace. The widespread availability and increasing processing power of GPUs could offer an alternative solution. GPU features, like multiply-accumulate <b>instructions</b> or multiple <b>execution</b> <b>units,</b> {{are similar to those of}} most DSPs [3]. Besides, 3 D audio rendering applications require a significant number of geometric calculations, which are a perfect fit for the GPU. Our feasibility study investigates the use of GPUs for efficient audio processing. GPU-accelerated audio rendering We consider a combination of two simple operations commonly used for 3 D audi...|$|R
40|$|This paper {{presents}} a multithreaded superscalar processor that permits several threads to issue <b>instructions</b> to the <b>execution</b> <b>units</b> {{of a wide}} superscalar processor in a single cycle. Instructions can simultaneously be issued from up to 8 threads with a total issue bandwidth of 8 instructions per cycle. Our {{results show that the}} 8 -threaded 8 -issue processor reaches a throughput of 4. 2 instructions per cycle. 1 Introduction Current microprocessors utilize instruction-level parallelism by a deep processor pipeline and by the superscalar technique that issues up to four instructions per cycle from a single thread. VLSI-technology will allow future generations of microprocessors to exploit instruction-level parallelism up to 8 instructions per cycle, or more. However, the instruction-level parallelism found in a conventional instruction stream is limited. The solution is the additional utilization of more coarse-grained parallelism. The main approaches are the multiprocessor chip and the [...] ...|$|R
50|$|CodeAnalyst {{is built}} on OProfile for the Linux {{platform}} and is available as a free download. The GUI assists in various kinds of code profiling including time based profiling, hardware event-based profiling, instruction-based profiling and others. This produces statistics about details such as time spent in each subroutine which can be drilled down to the source code or instruction level. The time taken by the instructions can be indicative of stalls in the pipeline during <b>instruction</b> <b>execution.</b> Optimization could {{be as simple as}} reordering the instructions for maximum utilization of a data line cache or altering/removing the branches and loops so that the maximum number of <b>execution</b> units(Load/Store <b>units,</b> ALU, FP <b>execution</b> <b>unit...)</b> are utilized in parallel.|$|R
5000|$|The PPE is the Power Architecture based, dual issue in-order two-way multithreaded core with 23-stages {{pipeline}} {{acting as}} the controller for the eight SPEs, which handle most of the computational workload. PPE has limited out of order execution capabilities, it can perform loads out of order and has delayed execution pipelines. The PPE will work with conventional operating systems due to its similarity to other 64-bit PowerPC processors, while the SPEs are designed for vectorized floating point code execution. The PPE contains a 64 KiB level 1 cache (32 KiB instruction and a 32 KiB data) and a 512 KiB Level 2 cache. The size of a cache line is 128 bytes. Additionally, IBM has included an AltiVec(VMX) unit which is fully pipelined for single precision floating point (Altivec 1 does not support double precision floating-point vectors.), 32-bit Fixed Point Unit (FXU) with 64-bit register file per thread, Load and Store Unit (LSU), 64-bit Floating-Point Unit (FPU) , Branch Unit (BRU) and Branch Execution Unit(BXU).PPE consists of three main units: <b>Instruction</b> <b>Unit</b> (IU), <b>Execution</b> <b>Unit</b> (XU) and vector/scalar <b>execution</b> <b>unit</b> (VSU). IU contains L1 instruction cache, branch prediction hardware, instruction buffers and dependency checking login. XU contains integer <b>execution</b> <b>units</b> (FXU) and load-store unit (LSU). VSU contains all of the execution resources for FPU and VMX. Each PPE can complete two double precision operations per clock cycle using a scalar fused-multiply-add instruction, which translates to 6.4 GFLOPS at 3.2 GHz; or eight single precision operations per clock cycle with a vector fused-multiply-add instruction, which translates to 25.6 GFLOPS at 3.2 GHz.|$|R
40|$|Abstract. The {{behaviour}} {{produced by}} an <b>instruction</b> sequence under <b>execution</b> is a behaviour {{to be controlled}} by some execution environment: each step performed actuates the processing of an <b>instruction</b> by the <b>execution</b> environment and a reply returned at completion of the processing determines how the behaviour proceeds. In this paper, we are concerned with the case where the processing takes place remotely. We describe a protocol to deal with the case where the behaviour produced by an <b>instruction</b> sequence under <b>execution</b> leads to the generation of a stream of instructions to be processed and a remote <b>execution</b> <b>unit</b> handles the processing of that stream of instructions...|$|R
40|$|The {{behaviour}} {{produced by}} an <b>instruction</b> sequence under <b>execution</b> is a behaviour {{to be controlled}} by some execution environment: each step performed actuates the processing of an <b>instruction</b> by the <b>execution</b> environment and a reply returned at completion of the processing determines how the behaviour proceeds. The increasingly occurring case where the processing takes place remotely involves the generation of a stream of instructions to be processed and a remote <b>execution</b> <b>unit</b> that handles the processing of this stream of instructions. We use process algebra to describe the behaviours produced by <b>instruction</b> sequences under <b>execution</b> and to describe two protocols implementing these behaviours in the case of remote processing. We also show that all finite-state behaviours considered in process algebra can be produced by <b>instruction</b> sequences under <b>execution...</b>|$|R
50|$|Furthermore, modern CPUs have {{instruction}} pipelines that queue <b>instructions</b> for <b>execution.</b> A {{processor with}} multiple <b>execution</b> <b>units</b> can perform {{more than one}} instruction per cycle if more than one instruction {{is available in the}} pipeline. Branching (the use of conditionals like if) makes it harder for the processor to fill its pipeline(s) because the CPU cannot tell what it needs to do in advance. Too much branching makes the pipeline less effective and potentially reduces the number of instructions the processor can execute per cycle. Many bitboard operations require fewer conditionals and therefore increase pipelining and make effective use of multiple <b>execution</b> <b>units</b> on many CPUs.|$|R
40|$|Single-event upsets from {{particle}} strikes {{have become}} a key challenge in microprocessor design. Techniques {{to deal with these}} transient faults exist, but come at a cost. Designers clearly require accurate estimates of processor error rates to make appropriate cost/reliability trade-offs. This paper describes a method for generating these estimates. A key aspect of this analysis is that some single-bit faults (such as those occurring in the branch predictor) will not produce an error in a program's output. We define a structure's architectural vulnerability factor (AVF) as the probability that a fault in that particular structure will result in an error. A structure's error rate is the product of its raw error rate, as determined by process and circuit technology, and the AVF. Unfortunately, computing AVFs of complex structures, such as the instruction queue, can be quite involved. We identify numerous cases, such as prefetches, dynamically dead code, and wrong-path instructions, in which a fault will not affect correct execution. We instrument a detailed IA 64 processor simulator to map bit-level microarchitectural state to these cases, generating per-structure AVF estimates. This analysis shows AVFs of 28 % and 9 % for the <b>instruction</b> queue and <b>execution</b> <b>units,</b> respectively, averaged across dynamic sections of the entire CPU 2000 benchmark suite. 1...|$|R
40|$|Scalability is {{important}} in superscalar processors design. A superscalar processor {{is said to be}} linearly scalable if with linear increase in load or demand, performance remains constant relative to linear increase in resources. In this paper, for evaluating the instruction fetching scalability, an analytical model of a superscalar processor is proposed by defining the fetch unit as the “producer ” of <b>instructions</b> and the <b>execution</b> <b>unit</b> as the “consumer. ” The scalability of the fetch unit relative to its branch predictor – the Bi-Mode Predictor – is then evaluated using SPEC 2000 suite of benchmarks. Our simulation results strongly suggest that reducing branch misprediction penalty is a better alternative solution – compared with increasing prediction accuracy – for improving instruction fetch scalability. 1. Summary and future work In this paper, we composed an analytical model for evaluating instruction fetching scalability relative to branch prediction accuracy. For this purpose, we decoupled the fetch and execution engines using the model proposed in [1] and assumed the instruction cache to be totally perfect and based the fetch engine performance only on branch prediction accuracy. We defined the performance of the fetch unit as the number of instructions it produces in each cycle based on the prediction it makes for control instructions. However, this performance is dependent on the <b>execution</b> <b>unit</b> which informs the fetch unit of the resolved outcomes of branch instructions and keeps it on the correct execution path. To assess the scalability of the fetch unit, we proposed an analytical fetching model and developed formulas for measuring the fetch performance. Our formulas dictated two factors are necessary for evaluating the fetch engine performance...|$|R
40|$|Instruction {{scheduling}} is {{an important}} issue in the compiler optimization for RISC machines. In this paper, we present a fast approximation algorithm for scheduling <b>instructions</b> of <b>unit</b> <b>execution</b> time with precedence constraints, interinstructional latencies and deadline constraints in a basic block on RISC processors, where the interinstructional latencies are at most d. Unlike Palem and Simon's algorithm, our algorithm allows latency of l ij = Γ 1 which denotes that instruction v j cannot run before v i. For the general model where latencies can be any value in fΓ 1; 0; 1; ΔΔΔ; dg, the time complexity of our algorithm is O(ne+nd). For the restricted where there is no latency of Γ 1, the time complexity of our algorithm is O(minfne;n 2 : 376 g + minfne; edg + nd). For the more restricted model where the interinstructional latencies between each instruction and its immediate successors are identical and there is no latency of Γ 1, the time com [...] ...|$|R
5000|$|Program Trace Macrocell and CoreSight Design Kit for non-intrusive tracing of <b>instruction</b> <b>execution.</b>|$|R
5000|$|Program Trace Macrocell and CoreSight Design Kit for {{unobtrusive}} tracing of <b>instruction</b> <b>execution</b> ...|$|R
50|$|The {{main memory}} had a {{capacity}} of 65,536 superwords (SWORDs), which are 512-bit words. The main memory was 32-way interleaved to pipeline memory accesses. It was constructed from core memory with an access time of 1.28 μs. The main memory was accessed via a 512-bit bus, controlled by the storage access controller (SAC), which handled requests from the stream unit. The stream unit accesses the main memory through the SAC via three 128-bit data buses, two for reads, and one for writes. Additionally, there is a 128-bit data bus for instruction fetch, I/O, and control vector access. The stream unit serves as the control unit, fetching and decoding instructions, initiating memory accesses on the behalf of the pipelined functional <b>units,</b> and controlling <b>instruction</b> <b>execution,</b> among other tasks. It also contains two read buffers and one write buffer for streaming data to the <b>execution</b> <b>units.</b>|$|R
50|$|The CPU {{design was}} quite complex - using three way {{interleaving}} of <b>instruction</b> <b>execution</b> (later called <b>instruction</b> pipeline) to improve throughput. Each instruction {{would go through}} an indexing phase, an actual <b>instruction</b> <b>execution</b> phase and an output phase. While an instruction {{was going through the}} indexing phase, the previous instruction was in its execution phase and the instruction before it was in its output phase.|$|R
50|$|While a superscalar CPU is {{typically}} also pipelined, pipelining and superscalar execution are considered different performance enhancement techniques. The former executes multiple instructions {{in the same}} <b>execution</b> <b>unit</b> in parallel by dividing the <b>execution</b> <b>unit</b> into different phases, whereas the latter executes multiple instructions in parallel by using multiple <b>execution</b> <b>units.</b>|$|R
50|$|<b>Instruction</b> <b>execution</b> time 1 or 2 {{instruction}} cycles (10.8 or 21.6 µs), 46300 to 92600 instructions per second.|$|R
