383|9|Public
25|$|Parekh, R. and Honavar, V. (2000). On the Relationships between Models of Learning in Helpful Environments. In: Proceedings of the Fifth International Conference on <b>Grammatical</b> <b>Inference.</b> Lisbon, Portugal.|$|E
25|$|Parekh, R., Nichitiu, C., and Honavar, V. (1998). A Polynomial Time Incremental Algorithm for Learning DFA. In: Proceedings of the Fourth International Colloquium on <b>Grammatical</b> <b>Inference</b> (ICGI'98), Ames, IA. Lecture Notes in Computer Science vol. 1433 pp.37–49. Berlin: Springer-Verlag.|$|E
2500|$|Vasant Honavar and Giora Slutzki (Ed). <b>Grammatical</b> <b>Inference.</b> Berlin: Springer-Verlag. 1998.|$|E
40|$|Artificial neural {{networks}} are proposed {{as a tool}} for machine learning and many results have been obtained regarding their application to practical problems in robotics control, vision, pat-tern recognition, <b>grammatical</b> <b>inferences</b> and other areas. Typically, a neural network is trained during a supervised training session to recognize complex associations between inputs and out...|$|R
40|$|In this article, {{we report}} {{the results of}} a study on the {{relationship}} between individual differences in language learning aptitude and the structural connectivity of language pathways in the adult brain, the first of its kind. We measured four components of language aptitude (vocabulary learning; sound recognition; sound-symbol correspondence; and <b>grammatical</b> <b>inferencing)</b> using the LLAMA language aptitude test. Spatial working memory, verbal working memory and IQ were also measured as control factors. Diffusion Tensor Imaging was employed to investigate the structural connectivity of language pathways in the perisylvian language network. Regression analysis suggested significant correlations between most of these behavioural measures and the structural connectivity of certain language pathways, that is, <b>grammatical</b> <b>inferencing</b> and the BA 45 - and BA 46 -Temporal pathway, sound-symbol correspondence and the interhemispheric BA 45 pathway, vocabulary learning and the BA 47 -Parietal pathway, IQ and the BA 44 - and BA- 47 Parietal pathways, the BA 47 -Temporal pathway and interhemispheric BA 45 pathway, spatial working memory and the interhemispheric BA 6 pathway and the BA 47 -Parietal pathway, and verbal working memory and the BA 47 -Temporal pathway. These findings provide further insights into the neural underpinnings of the variation in language aptitude of human adults and are discussed in relation to relevant findings in the literature...|$|R
40|$|This paper investigates {{methods to}} {{automatically}} infer structural information from large XML documents. Using XML {{as a reference}} format, we approach the schema generation problem by application of inductive inference theory. In doing so, we review and extend results relating to the search spaces of <b>grammatical</b> <b>inferences</b> for large data set. We evaluate {{the result of an}} inference process using the concept of Minimum Message Length. Comprehensive experimentation reveals our new hybrid method to be the most effective for large documents. Finally tractability issues, including scalability analysis, are discussed...|$|R
2500|$|Tu, K., and Honavar, V. (2008). Unsupervised Learning of Probabilistic Context-Free Grammar using Iterative Biclustering[...] In: International Colloquium on <b>Grammatical</b> <b>Inference</b> (ICGI-2008). Springer-Verlag Lecture Notes in Computer Science vol. 5278 pp.224–237.|$|E
5000|$|... #Subtitle level 3: <b>Grammatical</b> <b>inference</b> by genetic {{algorithms}} ...|$|E
5000|$|There is a {{wide variety}} of methods for <b>grammatical</b> <b>inference.</b> Two of the classic sources are [...] and [...] [...] also devote a brief section to the problem, and cite a number of references. The basic {{trial-and-error}} method they present is discussed below. For approaches to infer subclasses of regular languages in particular, see Induction of regular languages. A more recent textbook is de la Higuera (2010), which covers the theory of <b>grammatical</b> <b>inference</b> of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni provide a survey that explores <b>grammatical</b> <b>inference</b> methods for natural languages.|$|E
40|$|Semistructured data {{presents}} many challenges, mainly due to {{its lack}} of a strict schema. These challenges are further magnified when large amounts of data are gathered from heterogeneous sources. We address this by investigation and development of methods to automatically infer structural information from example data. Using XML as a reference format, we approach the schema generation problem by application of inductive inference theory. In doing so, we review and extend results relating to the search spaces of <b>grammatical</b> <b>inferences.</b> We then adapt a method for evaluating {{the result of an}} inference process from computational linguistics. Further, we combine several inference algorithms, including both new techniques introduced by us and those from previous work. Comprehensive experimentation reveals our new hybrid method, based upon recently developed optimisation techniques, to be the most effective...|$|R
40|$|We {{survey the}} {{computational}} power of analog neural networks. 1 Introduction Artificial neural networks are proposed {{as a tool}} for machine learning and many results have been obtained regarding their application to practical problems in robotics control, vision, pattern recognition, <b>grammatical</b> <b>inferences</b> and other areas. Typically, a neural network is trained during a supervised training session to recognize complex associations between inputs and outputs. These associations are incorporated into the weights of the network, which encode a distributed representation of the information contained in the input patterns. Once trained, the network will compute an input/output mapping which, if the training data was representative enough, will closely match the unknown rule producing the original data. The applicability of learning algorithms such as the backpropagation algorithm, the massive parallelism of computation, as well as noise and fault tolerance, are some of the prominent feature [...] ...|$|R
40|$|Introduction Artificial neural {{networks}} are proposed {{as a tool}} for machine learning and many results have been obtained regarding their application to practical problems in robotics control, vision, pattern recognition, <b>grammatical</b> <b>inferences</b> and other areas. Typically, a neural network is trained during a supervised training session to recognize complex associations between inputs and outputs. These associations are incorporated into the weights of the network, which encode a distributed representation of the information contained in the input patterns. Once trained, the network will compute an input/output mapping which, if the training data was representative enough, will closely match the unknown rule producing the original data. The applicability of learning algorithms such as the backpropagation algorithm, the massive parallelism of computation, as well as noise and fault tolerance, are some of the prominent features of neural nets. Feedforward circuits comp...|$|R
5000|$|Vasant Honavar and Giora Slutzki (Ed). <b>Grammatical</b> <b>Inference.</b> Berlin: Springer-Verlag. 1998.|$|E
50|$|Grammar {{induction}} (or <b>grammatical</b> <b>inference)</b> is {{the process}} in machine learning of learning a formal grammar (usually {{as a collection of}} re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, <b>grammatical</b> <b>inference</b> is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.|$|E
5000|$|Parekh, R. and Honavar, V. (2000). On the Relationships between Models of Learning in Helpful Environments. In: Proceedings of the Fifth International Conference on <b>Grammatical</b> <b>Inference.</b> Lisbon, Portugal.|$|E
40|$|This paper {{outlines}} {{two approaches}} {{to the construction of}} computer systems that generate prose in the style of a given author. The first involves using intuitive notions of stylistic trademarks to construct a grammar that characterizes a particular author in this case, Ernest Hemingway. The second uses statistical methods for inferring a grammar from samples of an author's work in this instance, Thomas Hardy. A brief outline of grammar induction principles is included as background material for the latter system. The relative merits of each approach are discussed, and text generated from the resulting grammars is assessed in terms of its parodic quality. Further to its esoteric interest, a discussion of parody generation as a useful technique for measuring the success of <b>grammatical</b> <b>inferencing</b> systems is included, along with suggestions for its practical application in areas of language modeling and text compression...|$|R
40|$|Thesis (Master's) [...] University of Washington, 2016 - 03 This thesis {{describes}} a software system that maps glosses from interlinear glossed text (IGT) to an internally consistent set. This study hypothesizes that mapping glosses supports better <b>inference</b> of <b>grammatical</b> properties. <b>Inference</b> refers to analyzing information from IGT to automatically determine grammatical properties {{of a language}} for which a computational grammar can be constructed. The IGT will likely contain unknown or non-standard glosses. By mapping all glosses to an internally consistent set the non-standard rate should decrease which would provide more precise information for inferring <b>grammatical</b> properties. The <b>inference</b> procedure matches standard grams from a language to tense, aspect, and mood categories. These inferred gram and category pairs called choices are used to create computational grammars. The final results demonstrate that the methodology successfully reduces the non-standard gloss rate...|$|R
40|$|We {{carried out}} the first study on the {{relationship}} between individual language aptitude and structural connectivity of language pathways in the adult brain. We measured four components of language aptitude (vocabulary learning, VocL; sound recognition, SndRec; sound-symbol correspondence, SndSym; and <b>grammatical</b> <b>inferencing,</b> GrInf) using the LLAMA language aptitude test (Meara, 2005). Spatial working memory (SWM), verbal working memory (VWM) and IQ were also measured as control factors. Diffusion Tensor Imaging (DTI) was employed to investigate the structural connectivity of language pathways in the perisylvian language network. Principal Component Analysis (PCA) on behavioural measures suggests that a general ability might be important to the first stages of L 2 acquisition. It also suggested that VocL, SndSy and SWM are more closely related to general IQ than SndRec and VocL, and distinguished the tasks specifically designed to tap into L 2 acquisition (VocL, SndRec,SndSym and GrInf) from more generic measures (IQ, SWM and VWM). Regression analysis suggested significant correlations between most of these behavioural measures and the structural connectivity of certain language pathways, i. e., VocL and BA 47 -Parietal pathway, SndSym and inter-hemispheric BA 45 pathway, GrInf and BA 45 -Temporal pathway and BA 6 -Temporal pathway, IQ and BA 44 -Parietal pathway, BA 47 -Parietal pathway, BA 47 -Temporal pathway and inter-hemispheric BA 45 pathway, SWM and inter-hemispheric BA 6 pathway and BA 47 -Parietal pathway, and VWM and BA 47 -Temporal pathway. These results are discussed in relation to relevant findings in the literature...|$|R
5000|$|Tu, K., and Honavar, V. (2008). Unsupervised Learning of Probabilistic Context-Free Grammar using Iterative Biclustering[...] In: International Colloquium on <b>Grammatical</b> <b>Inference</b> (ICGI-2008). Springer-Verlag Lecture Notes in Computer Science vol. 5278 pp. 224-237.|$|E
5000|$|Parekh, R., Nichitiu, C., and Honavar, V. (1998). A Polynomial Time Incremental Algorithm for Learning DFA. In: Proceedings of the Fourth International Colloquium on <b>Grammatical</b> <b>Inference</b> (ICGI'98), Ames, IA. Lecture Notes in Computer Science vol. 1433 pp. 37-49. Berlin: Springer-Verlag.|$|E
50|$|<b>Grammatical</b> <b>inference</b> {{has often}} been very focused {{on the problem of}} {{learning}} finite state machines of various types (see the article Induction of regular languages for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.|$|E
50|$|Since the {{beginning}} of the century, these approaches have been extended to the problem of inference of context-free grammars and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.Other classes of grammars for which <b>grammatical</b> <b>inference</b> has been studied are contextual grammars and pattern languages.|$|E
50|$|Processes {{related to}} {{functional}} decomposition are prevalent throughout {{the fields of}} knowledge representation and machine learning. Hierarchical model induction techniques such as Logic circuit minimization, decision trees, <b>grammatical</b> <b>inference,</b> hierarchical clustering, and quadtree decomposition are all examples of function decomposition. A review of other applications and function decomposition {{can be found in}} , which also presents methods based on information theory and graph theory.|$|E
50|$|The {{early work}} in grammar {{induction}} (also known as <b>grammatical</b> <b>inference)</b> {{is related to}} inductive programming, as rewriting systems or logic programs {{can be used to}} represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community.|$|E
50|$|Jim Horning {{received}} a PhD {{in computer science}} from Stanford University in 1969 for a thesis entitled A Study of <b>Grammatical</b> <b>Inference.</b> He was a founding member, and later chairman, of the Computer Systems Research Group at the University of Toronto, Canada, from 1969 until 1977. He was then a Research Fellow at the Xerox Palo Alto Research Center (PARC) from 1977 until 1984 and a founding member and senior consultant at DEC Systems Research Center (DEC/SRC) from 1984 until 1996. He was {{founder and director of}} STAR Lab from 1997 until 2001 at InterTrust Technologies Corp. Peter G. Neumann reported on 22 January 2013 in the RISKS Digest, Volume 27, Issue 14, that Horning had died on 18 January 2013.|$|E
40|$|This paper {{describes}} the Omphalos Context-Free Grammar Learning Competition held {{as part of}} the International Colloquium on <b>Grammatical</b> <b>Inference</b> 2004. The competition was created in an e#ort to promote the development of new and better <b>grammatical</b> <b>inference</b> algorithms for context-free languages, to provide a forum for the comparison of di#erent <b>grammatical</b> <b>inference</b> algorithms and to gain insight into the current state-of-the-art of context-free <b>grammatical</b> <b>inference</b> algorithms...|$|E
40|$|Abstract. This paper {{focuses on}} a {{subfield}} of machine learning, the socalled <b>grammatical</b> <b>inference.</b> Roughly speaking, <b>grammatical</b> <b>inference</b> deals {{with the problem of}} inferring a grammar that generates a given set of sample sentences in some manner that is supposed to be realized by some inference algorithm. We discuss how the analysis and formalization of the main features of the process of human natural language acquisition may improve results in the area of <b>grammatical</b> <b>inference.</b> ...|$|E
40|$|In {{syntactic}} pattern recognition, the <b>grammatical</b> <b>inference</b> is {{the process}} that characterizes a pattern class by a grammar. So establishment of <b>grammatical</b> <b>inference</b> procedure is very important. In this paper, {{for the purpose of}} the recognition of hand-written Katakana characters, we suggest a <b>grammatical</b> <b>inference</b> algorithm for context free grammar. Our inference is developed by selecting sample patterns from original set of sample patterns according to a criterion, and classifying the production rules previously...|$|E
40|$|International audienceThis paper {{focuses on}} a subﬁeld of machine learning, the so- called <b>grammatical</b> <b>inference.</b> Roughly speaking, <b>grammatical</b> <b>inference</b> deals {{with the problem of}} {{inferring}} a grammar that generates a given set of sample sentences in some manner that is supposed to be realized by some inference algorithm. We discuss how the analysis and formalization of the main features of the process of human natural language acquisition may improve results in the area of <b>grammatical</b> <b>inference...</b>|$|E
40|$|Abstract—we {{present an}} {{overview}} in the advances {{related to the}} learning of formal languages i. e. development in the <b>grammatical</b> <b>inference</b> research. The problem of learning correct grammars for the unknown languages is known as <b>grammatical</b> <b>inference.</b> It is considered a main subject of inductive inference, and grammars are important representations to be investigated in machine learning from both theoretical and practical points of view. Application area of <b>grammatical</b> <b>inference</b> is increasing day by day, {{and it is still}} required to find a task where <b>grammatical</b> <b>inference</b> models have done much better than other machine learning or pattern recognition programs. However, it is known that making research in this area is a computationally hard problem. This paper mainly explores the area, its applications, various learning paradigms, the case of context-free grammars, challenges, recent trends etc., and cites the important literature on these. Index Terms — machine learning, <b>grammatical</b> <b>inference,</b> learning model, formal language, context-free grammars D I...|$|E
40|$|International audienceThis paper {{describes}} the Omphalos Context-Free Language Learning Competition held {{as part of}} the International Colloquium on <b>Grammatical</b> <b>Inference</b> 2004. After the success of the Abbadingo Competition on the better known task of learning regular languages, the competition was created in an effort to promote the development of new and better <b>grammatical</b> <b>inference</b> algorithms for context-free languages, to provide a forum for the comparison of different <b>grammatical</b> <b>inference</b> algorithms and to gain insight into the current state-of-the-art of context-free <b>grammatical</b> <b>inference</b> algorithms. This paper discusses design issues and decisions made when creating the competition, leading to the introduction of a new complexity measure developed to estimate the difficulty of learning a context-free grammar. It presents also the results of the competition and lessons learned...|$|E
40|$|International audienceThe {{field of}} <b>Grammatical</b> <b>Inference</b> {{provides}} a good theoretical framework for investigating a learning process. Formal results in this field can be relevant {{to the question of}} first language acquisition. However, <b>Grammatical</b> <b>Inference</b> studies have been focused mainly on mathematical aspects, and have not exploited the linguistic relevance of their results. With this paper, we try to enrich <b>Grammatical</b> <b>Inference</b> studies with ideas from Linguistics. We propose a non-classical mechanism that has relevant linguistic and computational properties, and we study its learnability from positive data...|$|E
40|$|Abstract. <b>Grammatical</b> <b>inference</b> – used {{successfully}} {{in a variety}} of fields such as pattern recognition, computational biology and natural language processing – is the process of automatically inferring a grammar by examining the sentences of an unknown language. Software engineering can also benefit from <b>grammatical</b> <b>inference.</b> Unlike the aforementioned fields, which use grammars as a convenient tool to model naturally occuring patterns, software engineering treats grammars as first-class objects typically created and maintained for a specific purpose by human designers. We introduce the theory of <b>grammatical</b> <b>inference</b> and review {{the state of the art}} as it relates to software engineering...|$|E
40|$|International audienceThis paper {{tries to}} bring {{together}} the theory of <b>Grammatical</b> <b>Inference</b> and the studies of natural language acquisition. We discuss how the studies of natural language acquisition can improve results in the ﬁeld of <b>Grammatical</b> <b>Inference,</b> and how a computational model inspired by such studies can help to answer several key questions in natural language learning...|$|E
40|$|This paper {{presents}} {{the application of}} stochastic <b>grammatical</b> <b>inference</b> to speech recognition. In speech recognition, the acoustic signal process produces a set of words which are combinating to build sentences. Language models are then used to lead the speech recognition application to the most pertinent combination. Up to now, statistical language models are used. We suggest to use stochastic formal grammars instead of statistical models. Theses stochastic grammars will be build by machine learning algorithms. We will first show that unaided <b>grammatical</b> <b>inference</b> cannot be used for speech recognition. We will then make manifest that smoothing is necessary and show the gain that one can obtain by using a basic smoothing. We finally put up a smoothing technic dedicates to stochastic formal grammars. 2 THE QUALITY CRITERION 1 Introduction Our aim is to use stochastic <b>grammatical</b> <b>inference</b> for natural speech recognition. The main difference between validations of <b>grammatical</b> <b>inference</b> [...] ...|$|E
40|$|This paper {{describes}} an active learning {{approach to the}} problem of <b>grammatical</b> <b>inference,</b> specifically the inference of deterministic finite automata (DFAs). We refer to the algorithm as the estimation-exploration algorithm (EEA). This approach differs from previous passive and active learning approaches to <b>grammatical</b> <b>inference</b> in that training data is actively proposed by the algorithm, rather than passively receiving training data from some external teacher. Here we show that this algorithm outperforms one version of the most powerful set of algorithms for <b>grammatical</b> <b>inference,</b> evidence driven state merging (EDSM), on randomly-generated DFAs. The performance increase {{is due to the fact}} that the EDSM algorithm only works well for DFAs with specific balances (percentage of positive labelings), while the EEA is more consistent over a wider range of balances. Based on this finding we propose a more general method for generating DFAs to be used in the development of future <b>grammatical</b> <b>inference</b> algorithms...|$|E
