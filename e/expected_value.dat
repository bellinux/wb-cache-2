7877|6631|Public
5|$|Bernoulli {{wrote the}} text between 1684 and 1689, {{including}} {{the work of}} mathematicians such as Christiaan Huygens, Gerolamo Cardano, Pierre de Fermat, and Blaise Pascal. He incorporated fundamental combinatorial topics such as his theory of permutations and combinations (the aforementioned problems from the twelvefold way) {{as well as those}} more distantly connected to the burgeoning subject: the derivation and properties of the eponymous Bernoulli numbers, for instance. Core topics from probability, such as <b>expected</b> <b>value,</b> were also a significant portion of this important work.|$|E
5|$|The {{properties}} of thorium vary widely {{depending on the}} degree of impurities in the sample. The major impurity is usually thorium dioxide (ThO2); even the purest thorium specimens usually contain about a tenth of a percent of the dioxide. Experimental measurements of its density give values between 11.5and11.66g/cm3: these are slightly lower than the theoretically <b>expected</b> <b>value</b> of 11.7g/cm3 calculated from thorium's lattice parameters, perhaps due to microscopic voids forming in the metal when it is cast. These values lie between those of its neighbours actinium (10.1g/cm3) and protactinium (15.4g/cm3), part of a trend across the early actinides.|$|E
5|$|Rhenium and {{technetium}} form a {{range of}} oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MO3Cl, so BhO3Cl should be formed in this reaction. Fluorination results in MO3F and MO2F3 for the heavier elements {{in addition to the}} rhenium compounds ReOF5 and ReF7. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties. Since the oxychlorides are asymmetrical, and they should have increasingly large dipole moments going down the group, they should become less volatile in the order TcO3Cl > ReO3Cl > BhO3Cl: this was experimentally confirmed in 2000 by measuring the enthalpies of adsorption of these three compounds. The values are for TcO3Cl and ReO3Cl are −51kJ/mol and −61kJ/mol respectively; the experimental value for BhO3Cl is −77.8kJ/mol, very close to the theoretically <b>expected</b> <b>value</b> of −78.5kJ/mol.|$|E
40|$|In this paper, {{moments and}} other <b>expected</b> <b>values</b> of {{functions}} of matrix variate elliptically contoured distribution are studied. A relationship is pointed {{out between the}} <b>expected</b> <b>values</b> for the normal case and the general elliptical case. <b>Expected</b> <b>values</b> of concrete functions are derived. An example is given when the underlying distribution is Student's t-distribution...|$|R
3000|$|... s are not independent, {{calculation}} of these <b>expected</b> <b>values</b> is not easy. In {{the following section}} we focus on estimating these desired <b>expected</b> <b>values</b> for the case of thresholding.|$|R
40|$|AbstractSubrahmaniam [1] {{records the}} <b>expected</b> <b>values</b> of certain zonal polynomials for the multivariate Dirichlet distribution. Actually, {{these results are}} <b>expected</b> <b>values</b> of certain zonal polynomials for the multivariate beta distribution. The present paper evaluates the <b>expected</b> <b>values</b> for a single matrix-variate noncentral Dirichlet distribution. Chikuse [2] shows that such {{integrals}} find applications to the distribution of sum of Wishart matrices...|$|R
5|$|If {{a player}} is dealt {{a pair of}} eights, the total of 16 is {{considered}} a troublesome hand. In fact, the value 16 {{is said to be}} the worst hand one can have in blackjack. Since sixteen of the other fifty cards have a value of 10 and four have a value of 11, there is a strong chance of getting at least an 18 with either or both split cards. A hand totaling 18 or 19 is much stronger than having a 16. Splitting eights limits one's losses and improves one's hand. Probabilistic research of <b>expected</b> <b>value</b> scenarios shows that by splitting eights one can convert a hand that presents an expected loss to two hands that may present an expected profit or a reduced loss, depending on what the dealer is showing. A split pair of eights is expected to win against dealer upcards of 2 through 7 and to lose less against dealer upcards of 8 through ace. If a player hits on a pair of eights, he is expected to lose $52 for a $100 bet. If the player splits the eights, he is expected to lose only $43 for a $100 bet.|$|E
5|$|Mimas, the {{innermost}} {{of the round}} moons of Saturn and directly interior to Enceladus, is a geologically dead body, even though it should experience stronger tidal forces than Enceladus. This apparent paradox can be explained in part by temperature-dependent properties of water ice (the main constituent of the interiors of Mimas and Enceladus). The tidal heating per unit mass is given by the formula , where ρ is the (mass) density of the satellite, n is its mean orbital motion, r is the satellite's radius, e is the orbital eccentricity of the satellite, μ is the shear modulus and Q is the dimensionless dissipation factor. For a same-temperature approximation, the <b>expected</b> <b>value</b> of qtid for Mimas is about 40 times that of Enceladus. However, the material parameters μ and Q are temperature dependent. At high temperatures (close to the melting point), μ and Q are low, so tidal heating is high. Modeling suggests that for Enceladus, both a 'basic' low-energy thermal state with little internal temperature gradient, and an 'excited' high-energy thermal state with a significant temperature gradient, and consequent convection (endogenic geologic activity), once established, would be stable. For Mimas, only a low-energy state {{is expected to be}} stable, despite its being closer to Saturn. So the model predicts a low-internal-temperature state for Mimas (values of μ and Q are high) but a possible higher-temperature state for Enceladus (values of μ and Q are low). Additional historical information is needed to explain how Enceladus first entered the high-energy state (e.g. more radiogenic heating or a more eccentric orbit in the past).|$|E
25|$|Law {{of total}} {{expectation}} –the <b>expected</b> <b>value</b> of the conditional <b>expected</b> <b>value</b> of X given Y {{is the same}} as the <b>expected</b> <b>value</b> of X.|$|E
25|$|Classical {{quantities}} {{appear in}} quantum mechanics {{in the form}} of <b>expected</b> <b>values</b> of observables, and as such the Ehrenfest theorem (which predicts the time evolution of the <b>expected</b> <b>values)</b> lends support to the correspondence principle.|$|R
50|$|Statically {{estimation}} is {{to determine}} the <b>expected</b> <b>value(s)</b> of statistical <b>expected</b> <b>values</b> of statistical quantities. Statistical estimation also tries to find the <b>expected</b> <b>values.</b> The <b>expected</b> <b>values</b> are those <b>values</b> that we <b>expect</b> among the random values, derived from samples of population in probability (group of subset). Because we have problem here, and in time series analysis, discrete data obtained as a function of time is usually available rather than samples of population or group of subsets taken simultaneously. The difficulty is commonly avoided by the process which is named ergodic process, that changes with time and probability gets involved with it, and it's not always periodic at all portions of time.|$|R
5000|$|Minimum {{and maximum}} <b>expected</b> <b>values</b> and {{measurement}} precision (...) ...|$|R
25|$|The <b>expected</b> <b>value</b> {{is a key}} {{aspect of}} how one characterizes a {{probability}} distribution; it is one type of location parameter. By contrast, the variance {{is a measure of}} dispersion of the possible values of the random variable around the <b>expected</b> <b>value.</b> The variance itself is defined in terms of two expectations: it is the <b>expected</b> <b>value</b> of the squared deviation of the variable's value from the variable's <b>expected</b> <b>value.</b>|$|E
25|$|This will {{be useful}} when it is {{possible}} to derive formulae for the <b>expected</b> <b>value</b> and for the <b>expected</b> <b>value</b> of the square.|$|E
25|$|In {{probability}} theory, the <b>expected</b> <b>value</b> of {{a random}} variable, intuitively, is the long-run average value of repetitions {{of the experiment}} it represents. For example, the <b>expected</b> <b>value</b> in rolling a six-sided die is 3.5, because the average of all the numbers that come up in an extremely large number of rolls is close to 3.5. Less roughly, the law of large numbers states that the arithmetic mean of the values almost surely converges to the <b>expected</b> <b>value</b> {{as the number of}} repetitions approaches infinity. The <b>expected</b> <b>value</b> is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment.|$|E
25|$|The <b>expected</b> <b>values</b> of {{the powers}} of X are called the moments of X; the moments about the mean of X are <b>expected</b> <b>values</b> of powers of X − E. The moments of some random {{variables}} {{can be used to}} specify their distributions, via their moment generating functions.|$|R
5000|$|... #Subtitle level 4: The {{measurement}} only approximates <b>expected</b> <b>values</b> ...|$|R
2500|$|Rational {{expectations}} are <b>expected</b> <b>values</b> in the mathematical sense. In {{order to be}} able to compute <b>expected</b> <b>values,</b> individuals must know the true economic model, its parameters, and the nature of the stochastic processes that govern its evolution. If these extreme assumptions are violated, individuals simply cannot form rational expectations ...|$|R
25|$|A {{statistical}} error {{is the amount}} by which an observation differs from its <b>expected</b> <b>value,</b> a residual is the amount an observation differs from the value the estimator of the <b>expected</b> <b>value</b> assumes on a given sample (also called prediction).|$|E
25|$|To empirically {{estimate}} the <b>expected</b> <b>value</b> of a random variable, one repeatedly measures {{observations of the}} variable and computes the arithmetic mean of the results. If the <b>expected</b> <b>value</b> exists, this procedure estimates the true <b>expected</b> <b>value</b> in an unbiased manner and has the property of minimizing {{the sum of the}} squares of the residuals (the sum of the squared differences between the observations and the estimate). The law of large numbers demonstrates (under fairly mild conditions) that, as the size of the sample gets larger, the variance of this estimate gets smaller.|$|E
25|$|Between two {{estimator}}s {{of a given}} parameter, the {{one with}} lower mean squared error {{is said to be}} more efficient. Furthermore, an estimator is said to be unbiased if its <b>expected</b> <b>value</b> is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its <b>expected</b> <b>value</b> converges at the limit to the true value of such parameter.|$|E
5000|$|A bit of {{algebraic}} manipulation {{shows that}} the <b>expected</b> <b>values</b> ...|$|R
5000|$|Help {{determine}} worst, {{best and}} <b>expected</b> <b>values</b> for different scenarios.|$|R
5000|$|In {{terms of}} <b>expected</b> <b>values,</b> {{this model is}} {{expressed}} as follows: ...|$|R
25|$|Since {{this does}} not {{converge}} but instead keeps growing, the <b>expected</b> <b>value</b> is infinite.|$|E
25|$|Remark 4. For multidimensional random variables, their <b>expected</b> <b>value</b> {{is defined}} per component, i.e.|$|E
25|$|To contrast, in {{a process}} that is not a martingale, it may still be the case that the <b>expected</b> <b>value</b> of the process at one time is equal to the <b>expected</b> <b>value</b> of the process at the next time. However, {{knowledge}} of the prior outcomes (e.g., all prior cards drawn from a card deck) may be able to reduce the uncertainty of future outcomes. Thus, the <b>expected</b> <b>value</b> of the next outcome given knowledge of the present and all prior outcomes may be higher than the current outcome if a winning strategy is used. Martingales exclude the possibility of winning strategies based on game history, and thus they are a model of fair games.|$|E
5000|$|On the {{distribution}} of the <b>expected</b> <b>values</b> of the order statistics, 1953 ...|$|R
5000|$|... where [...] is a {{diagonal}} weighting matrix, [...] the vector of <b>expected</b> <b>values,</b> ...|$|R
40|$|To {{understand}} {{and describe the}} proton induced spallation reactions, {{a large number of}} computer codes have been proposed. Various quanti- tative tests are used in literature to judge the agreement between model calculations and experimental data. The judgement is based on the magni- tude of the deviation of the tests from their <b>expected</b> <b>values</b> {{in the case of the}} perfect agreement. However, the <b>expected</b> <b>values</b> of the tests and their standard deviations are usually not well known. Thus the conclusions may be ambiguous. It is proposed to calculate the <b>expected</b> <b>values</b> and standard deviations of the tests by Monte Carlo method and to use the tests in their standardized for...|$|R
25|$|The <b>expected</b> <b>value</b> and {{variance}} of a Poisson-distributed random variable are both equal to λ.|$|E
25|$|In {{decision}} theory, and {{in particular}} in choice under uncertainty, an agent is described as making an optimal choice {{in the context of}} incomplete information. For risk neutral agents, the choice involves using the expected values of uncertain quantities, while for risk averse agents it involves maximizing the <b>expected</b> <b>value</b> of some objective function such as a von Neumann–Morgenstern utility function. One example of using <b>expected</b> <b>value</b> in reaching optimal decisions is the Gordon–Loeb model of information security investment. According to the model, one can conclude that the amount a firm spends to protect information should generally be {{only a small fraction of}} the expected loss (i.e., the <b>expected</b> <b>value</b> of the loss resulting from a cyber/information security breach).|$|E
25|$|That is, the bet of $1 {{stands to}} lose $0.0526, so its <b>expected</b> <b>value</b> is -$0.0526.|$|E
40|$|Attention is {{restricted}} to a method called Analysis of variance (ANOVA) {{that is used to}} compare <b>expected</b> <b>values</b> of several independent random samples. The clas- sic ANOVA theory with all its assumptions, including the assumption of normality, is presented at the beginning. Afterwards, an instance when the assumption of nor- mality of input data is violated is exemplified. The asymptotic distribution of test statistic under the hypothesis of the equality of the <b>expected</b> <b>values</b> is derived. The distribution is used to test the equality. Subsequently, it is shown that Tukey's range test and Scheffé's method of multiple comparison in case of non-normality could be used {{in the same way as}} for normal samples. The methods serve for compa- ring <b>expected</b> <b>values</b> of pairs of random samples. Thus, they can determine <b>expected</b> <b>values</b> which are different. Finally, a simulation study is presented which is to verify the proved theoretical results and to describe situations with data from non-normal distributions...|$|R
2500|$|... all the <b>expected</b> <b>values</b> [...] and [...] {{are defined}} (do {{not have the}} form [...] ); ...|$|R
2500|$|... {{where the}} <b>expected</b> <b>values</b> are {{with respect to}} some {{probability}} distribution in the random variable [...]|$|R
