125|410|Public
40|$|This paper {{considers}} {{the problem of}} fitting an <b>error</b> <b>density</b> to the goodness-of-fit test of the errors in a nonlinear autoregressive stationary time series regression model. The test statistic {{is based on the}} integrated squared error of the nonparametric <b>error</b> <b>density</b> estimate and the null <b>error</b> <b>density.</b> Without knowing the nonlinear autoregressive function, we can show that the test statistic behaves asymptotically the same as the one based on the true errors. Residuals <b>Error</b> <b>density</b> estimation Stationary process...|$|E
40|$|This paper {{considers}} {{the problem of}} estimating the <b>error</b> <b>density</b> and distribution function in nonparametric regression models. Sufficient conditions are given under which the histogram <b>error</b> <b>density</b> estimator based on nonparametric residuals is uniformly weakly and strongly consistent, and L 1 -consistent. The uniform consistency with a rate of the nonparametric residual empirical distribution function and the histogram <b>error</b> <b>density</b> estimator is also established. Histogram density estimation Nonparametric residuals Empirical process...|$|E
40|$|This paper {{considers}} the asymptotic distributions of the <b>error</b> <b>density</b> estimators in first-order autoregressive models. At a fixed point, {{the distribution of}} the <b>error</b> <b>density</b> estimator is shown to be normal. Globally, the asymptotic distribution of the maximum of a suitably normalized deviation of the density estimator from the expectation of the kernel <b>error</b> <b>density</b> (based on the true error) is the same {{as in the case of}} the one sample set up, which is given in Bickel and Rosenblatt (1973). AMS (2000) subject classification. Primary 62 G 07; secondary 62 M 10...|$|E
40|$|Abstract: We {{consider}} Bayesian {{estimation of}} restricted conditional moment models with linear regression as a particular example. The standard {{practice in the}} Bayesian literature for semiparametric models is to use flexible families of distributionsfor the errors and assume that the errors are independent from covariates. However, a model with flexible covariate dependent error distributions should be preferred for the following reasons: consistent estimation of the parameters of interest evenif errors and covariates are dependent; possibly superior prediction intervals and more efficient estimation of the parameters under heteroscedasticity. To address these issues, we develop a Bayesian semiparametric model with flexible predictor dependent <b>error</b> <b>densities</b> and with mean restricted by a conditional moment condition. Sufficient conditions to achieve posterior consistency of the regression parameters and conditional <b>error</b> <b>densities</b> are provided. In experiments, the proposed method compares favorably with classical and alternative Bayesian estimation methods for the estimation of the regression coefficients. ...|$|R
40|$|Jurecková and Milhaud (1997) {{recently}} developed some characterization properties {{for a broad}} class of distributions under independent structure {{and in this way}} extended the results of Kagan et al. (1973) to the nonnormal distributions. The present paper further extends the characterization properties to a class of linear models with autoregressive, generally nonnormal errors. Admissibility Equivariance Time-series regression Characterization of <b>error</b> <b>densities...</b>|$|R
40|$|The authors {{determined}} <b>error</b> <b>densities</b> of ephemeris {{predictions for}} 14 LEO satellites. The empirical distributions are not {{inconsistent with the}} hypothesis of a Gaussian distribution. The growth rate of radial errors are most highly correlated with eccentricity ({vert_bar}r{vert_bar} = 0. 63, {alpha} < 0. 05). The growth rate of along-track errors is most highly correlated with the decay rate of the semimajor axis ({vert_bar}r{vert_bar} = 0. 97; {alpha} < 0. 01) ...|$|R
40|$|It {{is quite}} common in the {{statistical}} literature on nonparametric deconvolution {{to assume that the}} <b>error</b> <b>density</b> is perfectly known. Since this seems to be unrealistic in many practical applications, we study the effect of estimating the unknown <b>error</b> <b>density.</b> We derive minimax rates of convergence and propose a modification of the usual kernel-based estimation scheme, which takes the uncertainty about the <b>error</b> <b>density</b> into account. A simulation study quantifies the possible gains by this new method in finite sample situations. (orig.) Available from TIB Hannover: RR 5549 (205) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|In {{this work}} {{we deal with}} the problem of fitting an <b>error</b> <b>density</b> to the goodness-of-fit test of the errors in {{nonlinear}} autoregressive time series models with stationary α-mixing error terms. The test statistic is based on the integrated squared error of the nonparametric <b>error</b> <b>density</b> estimate and the null <b>error</b> <b>density.</b> By deriving the asymptotic normality of test statistics in these models, we extend the result of Cheng and Sun (Statist. Probab. Lett. 78, 1 (2008), 50 - 59) in the model with i. i. d error terms to the more general case. Comment: 9 page...|$|E
40|$|Abstract: This paper aims to {{investigate}} a Bayesian sampling approach to parameter estimation in the GARCH model with an unknown conditional <b>error</b> <b>density,</b> which we approximate by a mixture of Gaussian densities centered at individual errors and scaled by a common standard deviation. This mixture density has {{the form of a}} kernel density estimator of the errors with its bandwidth being the standard deviation. This study is motivated by the lack of robustness in GARCH models with a parametric assumption for the <b>error</b> <b>density</b> when used for error– density based inference such as value–at–risk (VaR) estimation. A contribution of the paper is to construct the likelihood and posterior of the model and bandwidth parameters under the kernel–form <b>error</b> <b>density,</b> and to derive the one–step–ahead posterior predictive density of asset returns. We also investigate the use and benefit of localized bandwidths in the kernel–form <b>error</b> <b>density.</b> A Monte Carlo simulation study reveals that the robustness of the kernel–form <b>error</b> <b>density</b> compensates for the loss of accuracy when using this density. Applying this GARCH model to daily return series of 42 assets in stock, commodity and currency markets, we find that this GARCH model is favored against the GARCH model with a skewed Student t <b>error</b> <b>density</b> for all stock indices, two out of 11 currencies and nearly half of the commodities. This provides an empirical justification for the value of the proposed GARCH model...|$|E
40|$|Verifiable {{conditions}} are {{given for the}} existence of efficient estimation in Smooth Threshold Autoregressive models of order 1. The paper establishes local asymptotic normality in the semi-parametric setting which is then used to discuss adaptive and efficient estimates of the models. It is found that the adaptation is satisfied if the <b>error</b> <b>densities</b> are symmetric. Simulation results are presented to compare the conditional least squares estimate with the adaptive and efficient estimates for the models...|$|R
40|$|The {{classical}} Nadaraya-Watson estimator {{is shown}} to solve a generic sensor fusion problem where the underlying sensor <b>error</b> <b>densities</b> are not known but a sample is available. By employing Haar kernels this estimator {{is shown to}} yield finite sample guarantees and also to be efficiently computable. Two simulation examples, and a robotics example involving the detection of a door using arrays of ultrasonic and infrared sensors, are presented to illustrate the performance...|$|R
40|$|The {{objective}} of this work is to present a simple approach {{to deal with a}} dynamic model without imposing any assumptions on the error distribution. Using a state-space representation the model does not need any optimization procedure to estimate the system parameters because it is optimized during an iterative process of prediction and filtering. The kernel approach obviates the need to estimate the usual unknown paramenters related to <b>error</b> <b>densities.</b> Postprint (published version...|$|R
40|$|We {{investigate}} {{the issue of}} bandwidth estimation in a nonparametric functional regression model with function-valued, continuous real-valued and discrete-valued regressors under the framework of unknown <b>error</b> <b>density.</b> Extending from the recent work of Shang (2013, Computational Statistics & Data Analysis), we approximate the unknown <b>error</b> <b>density</b> by a kernel density estimator of residuals, where the regression function is estimated by the functional Nadaraya-Watson estimator that admits mixed types of regressors. We derive a kernel likelihood and posterior density for the bandwidth parameters under the kernel-form <b>error</b> <b>density,</b> and put forward a Bayesian bandwidth estimation approach that can simultaneously estimate the bandwidths. Simulation studies demonstrated the estimation accuracy of the regression function and <b>error</b> <b>density</b> for the proposed Bayesian approach. Illustrated by a spectroscopy data set in the food quality control, we applied the proposed Bayesian approach to select the optimal bandwidths in a nonparametric functional regression model with mixed types of regressors...|$|E
40|$|Consider the nonparametric {{regression}} model Y=m(X) + ε, where the function m is smooth but unknown, and ε {{is independent of}} X. An estimator of {{the density of the}} error term ε is proposed and its weak consistency is obtained. The strategy used here is based on the kernel estimation of the residuals. Our contribution is twofold. First, we evaluate the impact of the estimation of the regression function m on the <b>error</b> <b>density</b> estimator. Secondly, the optimal choices of the first and second-step bandwidths used for estimating the regression function and the <b>error</b> <b>density</b> respectively, are proposed. Further, we investigate the asymptotic normality of the <b>error</b> <b>density</b> estimator and its rate-optimality. © 2011 Académie des sciences...|$|E
40|$|Consider the nonparametric {{regression}} model Y=m(X) +E, where the function m is smooth but unknown, and E {{is independent of}} X. An estimator of {{the density of the}} error term E is proposed and its weak consistency is obtained. The contribution of this paper is twofold. First, we evaluate the impact of the estimation of the regression function on the <b>error</b> <b>density</b> estimator. Secondly, the optimal choices of the first and second step bandwidths used for estimating the regression function and the <b>error</b> <b>density</b> are proposed. Further, we investigate the asymptotic normality of the <b>error</b> <b>density</b> estimator and evaluate its performances in simulated examples. Comment: arXiv admin note: substantial overlap with arXiv: 1010. 0439 and arXiv: 1011. 067...|$|E
40|$|In {{this report}} we present {{results of an}} {{in-depth}} analysis of the SP <b>error</b> <b>densities</b> for 29 satellites. These satellites were divided into three groups [...] Low Earth Orbit (LEO), Near Circular Orbit (NCO) and Highly Eccentric Orbit (HEO). Included in the first group were those satellites with eccentricities of less than 0. 2 and perigees below 450 km. The second group included satellites in near circular orbits (eccentricities of less than 0. 015) and perigees from 700 km to 1500 km. The third group consisted of those satellites that were in highly eccentric orbits, namely those with eccentricities greater than 0. 2. These satellites have perigees far into the thermosphere. Table 1 contains {{a summary of the}} orbit characteristics for the 29 satellites. In our study we attempted to unravel and elucidate the networks of relationships above. The satellite groupings and the report are organized in a way that reflects these efforts. We begin in Section 2 with a summary of the methods used in our analysis. One objective in this study was to establish a baseline for future work in satellite orbit propagators. Section 2 contains descriptions of the SP, truth orbits, and the satellite observation data used to establish this baseline. In the report we show how satellite <b>error</b> <b>densities</b> evolve in time up to thirty-six hours. We present error profiles, error histograms, rms errors and 95 / 9970 confidence limits for the along-track cross-track, and radial axes of motion for satellites {{in each of the three}} groupings. We present results of a regression analysis that establishes a physical model of the <b>error</b> <b>densities.</b> We also link the errors in the various regimes to the quality and quantity of the observational data...|$|R
40|$|In {{this paper}} robust, rank-based {{inference}} procedures are considered for general linear models with (possibly) asymmetric errors. Approximating standard errors of estimates and testing hypotheses about the model parameters require estimating a scaling functional, and an {{approach is developed}} which does not require symmetry of the underlying error distribution or replicates in the design matrix. In addition {{an estimate of the}} intercept is developed without requiring the assumption of a symmetric error distribution. inference based on ranks linear models asymmetric <b>errors</b> <b>density</b> estimation...|$|R
40|$|Linear {{models with}} stable <b>error</b> <b>densities</b> are considered. The local {{asymptotic}} normality {{of the resulting}} model is established. We use this result, combined with Le Cam’s third lemma, to obtain local powers of various classical rank tests (Wilcoxon’s and van der Waerden’s test, the median test, and their counterparts for regression and analysis of variance) under α-stable laws. The same results are used to construct new rank tests achieving parametric optimality at specified stable densities. A Monte-Carlo study is conducted to compare their relative performances...|$|R
40|$|This paper {{considers}} nonparametric {{instrumental variable}} regression when the endogenous variable is contaminated with classical measurement error. Existing methods are inconsistent {{in the presence}} of measurement error. We propose a wavelet deconvolution estimator for the structural function that modifies the generalized Fourier coefficients of the orthogonal series estimator {{to take into account the}} measurement error. We establish the convergence rates of our estimator for the cases of mildly/severely ill-posed models and ordinary/super smooth measurement errors. We characterize how the presence of measurement error slows down the convergence rates of the estimator. We also study the case where the measurement <b>error</b> <b>density</b> is unknown and needs to be estimated, and show that the estimation error of the measurement <b>error</b> <b>density</b> is negligible under mild conditions as far as the measurement <b>error</b> <b>density</b> is symmetric...|$|E
40|$|We {{approximate}} the <b>error</b> <b>density</b> of a nonparametric regression model by {{a mixture of}} Gaussian densities with means being the individual error realizations and variance a constant parameter. We investigate {{the construction of a}} likelihood and posterior for bandwidth parameters under this Gaussian-component mixture density of errors in a nonparametric regression. A Markov chain Monte Carlo algorithm is presented to sample bandwidths for the kernel estimators of the regression function and <b>error</b> <b>density.</b> A simulation study shows that the proposed Gaussian-component mixture density of errors is clearly favored against wrong assumptions of the <b>error</b> <b>density.</b> We apply our sampling algorithm to a nonparametric regression model of the All Ordinaries daily return on the overnight FTSE and S&P 500 returns, where the <b>error</b> <b>density</b> is approximated by the proposed mixture density. With the estimated bandwidths, we estimate the density of the one-step-ahead point forecast of the All Ordinaries return, and therefore, a distribution-free value-at-risk is obtained. The proposed Gaussian component mixture density of regression errors is also validated through the nonparametric regression involved in the state-price density estimation proposed by Aït-Sahalia and Lo (1998). Bayes factors, Gaussian-component mixture density, Markov chain Monte Carlo, state-price density, value-at-risk. ...|$|E
40|$|This paper aims to {{investigate}} a Bayesian sampling approach to parameter estimation in the semiparametric GARCH model with an unknown conditional <b>error</b> <b>density,</b> which we approximate by a mixture of Gaussian densities centered at individual errors and scaled by a common standard deviation. This mixture density has {{the form of a}} kernel density estimator of the errors with its bandwidth being the standard deviation. The proposed investigation is motivated by the lack of robustness in GARCH models with any parametric assumption of the <b>error</b> <b>density</b> for the purpose of error-density based inference such as value-at-risk (VaR) estimation. The contribution of the paper is to construct the likelihood and posterior of model and bandwidth parameters under the proposed mixture <b>error</b> <b>density,</b> and to forecast the one-step out-of-sample density of asset returns. The resulting VaR measure therefore would be distribution-free. Applying the semiparametric GARCH(1, 1) model to daily stock-index returns in eight stock markets, we find that this semiparametric GARCH model is favoured against the GARCH(1, 1) model with Student t errors for five indices, and that the GARCH model underestimates VaR compared to its semiparametric counterpart. We also investigate the use and benefit of localized bandwidths in the proposed mixture density of the errors. Bayes factors, kernel-form <b>error</b> <b>density,</b> localized bandwidths, Markov chain Monte Carlo, value-at-risk...|$|E
40|$|Linearmodelswith stable <b>error</b> <b>densities</b> are considered. The local {{asymptotic}} normality {{of the resulting}} model is established. We use this result, combined with Le Cam’s third lemma, to obtain local powers of various classical rank tests (Wilcoxon’s and van derWaerden’s test, the median test, and their counterparts for regression and analysis of variance) under α-stable laws. The same results are used to construct new rank tests achieving parametric optimality at specified stable densities. A Monte-Carlo study is conducted to compare their relative performances. Stable distributions; local {{asymptotic normality}}; rank tests; Asymptotic Relative efficiencies...|$|R
40|$|This paper {{discusses}} {{the problem of}} testing the equality of two nonparametric regression curves against one-sided alternatives in a two sample heteroscedastic setting in which design and <b>error</b> <b>densities</b> may dier between the two populations. The paper proposes a class of tests using covariate matching and derives their asymptotic power for local alternatives. Using a semiparametric approach, an upper bound on the asymptotic power of all tests against a given local alternative is obtained. For a given local alternative, {{a member of the}} proposed class of tests is shown to achieve this upper bound...|$|R
2500|$|The {{field of}} [...] values {{resulting}} from {{the solution of the}} differential system ( [...] ) satisfies ( [...] ) an order of magnitude better (on average) than the present instrumentation <b>error</b> in <b>density.</b>|$|R
40|$|We {{propose a}} new semiparametric observation-driven {{volatility}} model where {{the form of}} the <b>error</b> <b>density</b> directly influences the volatility dynamics. This feature distinguishes our model from standard semiparametric GARCH models. The link between the estimated <b>error</b> <b>density</b> and the volatility dynamics follows from the application of the generalized autoregressive score framework of Creal, Koopman, and Lucas (2012). We provide simulated evidence for the estimation efficiency and forecast accuracy of the new model, particularly if errors are fat-tailed and possibly skewed. In an application to equity return data we find that the model also does well in density forecasting...|$|E
40|$|We prove {{asymptotic}} normality of a suitably standardized integrated square {{difference between a}} kernel type <b>error</b> <b>density</b> estimator based on residuals and the expected value of the <b>error</b> <b>density</b> estimator based on innovations in GARCH models. This result {{is similar to that}} of Bickel-Rosenblatt under i. i. d. set up. Consequently the goodness-of-fit test for the innovation density of the GARCH processes based on this statistic is asymptotically distribution free, unlike the tests based on the residual empirical process. A simulation study comparing the finite sample behavior of this test with Kolmogorov-Smirnov test and the test based on integrated square difference between the kernel density estimate and null density shows some superiority of the proposed test. ...|$|E
40|$|We {{consider}} the nonparametric goodness-of-fit {{test of the}} uniform density on the sphere when we have observations whose density is the convolution of an <b>error</b> <b>density</b> and the true underlying density. We will deal specifically with the supersmooth error case which includes the Gaussian distribution. Similar to deconvolution density estimation, the smoother the <b>error</b> <b>density</b> the harder is the rate recovery of the test problem. When considering nonparametric alternatives expressed over analytic classes, we show {{that it is possible}} to obtain original separation rates much faster than any logarithmic power of the sample size according to the ratio of the regularity index of the analytic class and the smoothness degree of the error. Furthermore, we show that our adaptive statistical procedure attains these optimal rates...|$|E
30|$|Adjusting the {{coefficients}} to minimize mean <b>errors</b> from estimated <b>densities.</b>|$|R
40|$|Abstract: This article {{estimates}} generalized ARCH (GARCH) {{models for}} German stock market indices returns, using weekly and monthly data, various GARCH specifications and (non) nonnal <b>error</b> <b>densities,</b> {{and a variety}} of diagnostic checks. German stock return scries exhibit significant levels of second-order dependence. Our results clearly demonstrate that for both weekly as well as monthly return series the Studcnt-r distribution is superior to the standard normal distribution. In particular, the estimated GARCH-t models appear to be reasonably successful in accounting for both observed leptokurtosis and conditional heteroskedasticity from German stock return movements. JEL Classification System-Numbers: C 32, G 1...|$|R
40|$|We {{performed}} {{an analysis of}} <b>error</b> <b>densities</b> for the Special Perturbations orbit propagator using data for 29 satellites in orbits of interest to Space Shuttle and International Space Station collision avoidance. We find that the along-track errors predominate. These errors increase monotonically over each 36 -hour prediction interval. The predicted positions in the along-track direction progressively either leap ahead of or lag behind the actual positions. Unlike the along-track errors the radial and cross-track errors oscillate about their nearly zero mean values. As the number of observations per fit interval decline the along-track prediction errors, and amplitudes of the radial and cross-track errors, increase...|$|R
40|$|Abstract: This paper aims to {{investigate}} a Bayesian sampling approach to parameter estimation in the semiparametric GARCH model with an unknown conditional <b>error</b> <b>density,</b> which we approximate by a mixture of Gaussian densities centered at individual errors and scaled by a common standard deviation. This mixture density has {{the form of a}} kernel density estimator of the errors with its bandwidth being the standard deviation. The proposed investigation is motivated by the lack of robustness in GARCH models with any parametric assumption of the <b>error</b> <b>density</b> for the purpose of error-density based inference such as value-at-risk (VaR) estimation. The contribution of the paper is to construct the likelihood and posterior of model and bandwidth parameters under the proposed mixture <b>error</b> <b>density,</b> and to forecast the one-step out-of-sample density of asset returns. The resulting VaR measure therefore would be distribution-free. Applying the semiparametric GARCH(1, 1) model to daily stock-index returns in eight stock markets, we find that this semiparametric GARCH model is favored against the GARCH(1, 1) model with Student t errors for five indices, and that the GARCH model underestimates VaR compared to its semiparametric counterpart. We also investigate the use and benefit of localized bandwidths in the proposed mixture density of the errors...|$|E
40|$|Abstract. This work {{focuses on}} {{numerical}} solutions of optimal control problems. A time discretization error representation is derived for the approximation {{of the associated}} value function. It concerns Symplectic Euler solutions of the Hamiltonian system connected with the optimal control problem. The error representation has a leading order term consisting of an <b>error</b> <b>density</b> that is computable from Symplectic Euler solutions. Under an assumption of the pathwise convergence of the approximate dual function as the maximum time step goes to zero, we prove that the remainder is of higher order than the leading <b>error</b> <b>density</b> part in the error representation. With the error representation, {{it is possible to}} perform adaptive time stepping. We apply an adaptive algorithm originally developed for ordinary differential equations. The performance is illustrated by numerical tests...|$|E
40|$|This paper {{develops}} a sampling algorithm for bandwidth estimation in a nonparametric regression model with continuous and discrete regressors under an unknown <b>error</b> <b>density.</b> The <b>error</b> <b>density</b> is approximated by the kernel density estimator of the unobserved errors, while the regression function is estimated using the Nadaraya-Watson estimator admitting continuous and discrete regressors. We derive an approximate likelihood and posterior for bandwidth parameters, {{followed by a}} sampling algorithm. Simulation {{results show that the}} proposed approach typically leads to better accuracy of the resulting estimates than cross-validation, particularly for smaller sample sizes. This bandwidth estimation approach is applied to nonparametric regression model of the Australian All Ordinaries returns and the kernel density estimation of gross domestic product (GDP) growth rates among the organisation for economic co-operation and development (OECD) and non-OECD countries...|$|E
40|$|Asymptotic {{equivalence}} in Le Cam’s {{sense for}} nonparametric regression experiments is {{extended to the}} case of non-regular <b>error</b> <b>densities,</b> which have jump discontinuities at their endpoints. We prove asymptotic equivalence of such regression models and the observation of two independent Poisson point processes which contain the target curve as the support boundary of its intensity function. The intensity of the point processes is of order of the sample size n and involves the jump sizes as well as the design density. The statistical model significantly differs from regression problems with Gaussian or regular errors, which are known to be asymptotically equivalent to Gaussian white noise models...|$|R
40|$|The {{impact of}} {{covariate}} measurement error on quantile regression functions is investigated {{using a small}} variance approximation. The approximation shows how the error contaminated and error free quantile regression functions are related, a key factor being {{the distribution of the}} error free covariate. Exact calculations probe the accuracy of the approximation. The order of the approxiamtion error is unchanged if the <b>error</b> free covariate <b>density</b> is replaced by the <b>error</b> contaminated <b>density.</b> It is then possible to use the approximation to investigate the sensitivity of estimates to varying amounts of measurement error. ...|$|R
40|$|Linear {{models with}} stable <b>error</b> <b>densities</b> are considered, and their local {{asymptotic}} normality {{with respect to}} the regression parameter is established. We use this result, combined with Le Cam's third lemma, to obtain local powers and asymptotic relative efficiencies for various classical rank tests (the regression and analysis of variance counterparts of theWilcoxon, van derWaerden and median tests) under α-stable densities with various values of the skewness parameter and tail index. The same results are used to construct new rank tests, based on 'stable scores', achieving parametric optimality at specified stable densities. A Monte Carlo study is conducted to compare their finite-sample relative performances. © American Statistical Association and Taylor & Francis 2011. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
