3|124|Public
40|$|An {{extracellular}} β-glucosidase (EC 3. 2. 1. 21) {{has been}} purified to homogeneity from the culture filtrate of a thermophilic fungus, Humicola lanuginosa (Griffon and Maublanc) Bunce, using <b>duplicating</b> <b>paper</b> as the carbon source. The enzyme was purified 82 -fold with a 43 % yield by ion-exchange chromatography and gel filtration. The molecular {{weight of the}} protein {{was estimated to be}} 135, 000 by gel filtration and 110, 000 by electrophoresis. The sedimentation coefficient was 10. 5 S. It was an acidic protein containing high amounts of acidic amino acid residues. It was poor in sulphur-containing amino acids. It also contained 9 % carbohydrate. The enzyme activity was optimum at pH 4. 5 and at 60 °C. The enzyme was stable in the pH range 6 – 9 for 24 h at 25 °C. The enzyme had similar affinities towards cellobiose and p-nitrophenyl-β-d-glucoside with Km values of 0. 44 mM and 0. 50 mM, respectively. The enzyme was capable of hydrolysing larchwood xylan, xylobiose and p-nitrophenyl-β-d-xyloside, though to a lesser extent. The enzyme was specific for the β-configuration and glucose moiety in the substrate...|$|E
40|$|The {{prevalence}} of pes planus was determined {{in an adult}} Nigerian population in Anambra State, Southeast Nigeria. A total of 649 subjects comprising 325 males and 324 females aged 18 - 27 years {{were used for the}} study. The dynamic footprints of the subjects were obtained using endorsing ink and plain <b>duplicating</b> <b>paper.</b> Based on the objective index developed by Qaura et al, (1980) the contact index II was determined as the ratio of the contact width to the total width of the footprints. Descriptive statistics for each variable included mean and standard deviation (SD). Mean Â± 1 - 2 SD was regarded as normal but greater than that was considered as pes planus. A total of 45 subjects had pes planus comprising 22 males and 23 females. The overall {{prevalence of}} pes planus was 13. 9 % with a prevalence of 6. 8 % among males and 7. 1 % among females. Bilateral pes planus was commoner among males (4. 9 %) than females (4. 0 %). Unilateral pes planus was commoner among females (3. 1 %) compared to males (1. 9 %). The results showed that the prevalence of pes planus was higher (P< 0. 05) among females than males in our study. </p...|$|E
40|$|Introduction: Palm {{prints and}} toe prints are {{epidermal}} ridges that form early in fetal life and {{are unique to}} each individual. This means {{that they can be}} used for personal identification in criminal investigations. They are useful in diagnosis since recurring abnormal patterns are often seen in a variety of genetic syndromes. Materials and Methods: In this study, the palm prints of 170 young Yoruba students consisting of 75 males aged between 8 years and 19 years, and 95 females aged between 7 years and 18 years were used for the measurement of the ADT angle, AB ridge count (ABRC), and total ridge count (TRC) after informed consent. The patterns were obtained using the ink method and these were taken on white <b>duplicating</b> <b>paper.</b> The ridges were counted using hand lens and needle while the ADT angles were measured with a protractor. Results: The finding of higher values of ABRC (70. 77), TRC (123. 07), and the average ADT angle (40. 42) in female Yorubas compared with males were 68. 31, 122. 39, and 39. 29, respectively, and is in conflict with the results of most of the studies documented in other scientific journals. Conclusion: Our study without a doubt confirms that anthropometric dissimilarities subsist based on sex. The data submitted herein can provide useful information to the forensic investigators and scientist in solving cases especially when it involves individuals from the Yoruba ethnic group...|$|E
40|$|The {{issue of}} {{duplicate}} publications {{has received a}} lot of attention in the medical literature, but much less in the information science community. This paper aims at analyzing the prevalence and scientific impact of duplicate publications across all fields of research between 1980 and 2007, using a definition of <b>duplicate</b> <b>papers</b> based on their metadata. It shows that in all fields combined, the prevalence of duplicates is one out of two-thousand papers, but is higher in the natural and medical sciences than in the social sciences and humanities. A very high proportion (> 85 %) of these papers are published the same year or one year apart, which suggest that most <b>duplicate</b> <b>papers</b> were submitted simultaneously. Furthermore, <b>duplicate</b> <b>papers</b> are generally published in journals with impact factors below the average of their field and obtain a lower number of citations. This paper provides clear evidence that the prevalence of <b>duplicate</b> <b>papers</b> is low and, more importantly, that the scientific impact of such papers is below average. Comment: 13 pages, 7 figure...|$|R
30|$|From Step 1 {{we found}} a large number by our search criteria. We believe {{that this can be}} {{attributed}} to the fact that we have used broad search terms. Furthermore, we do not excluded <b>duplicate</b> <b>papers</b> in this step.|$|R
5000|$|Ralph Wedgwood (1766-1837) was an English {{inventor}} {{and member}} of the Wedgwood family of potters. His most notable invention was the earliest form of carbon paper, a method of creating <b>duplicate</b> <b>paper</b> documents, which he called [...] "stylographic writer" [...] or Noctograph. He obtained a patent for the invention in 1806.|$|R
30|$|This paper aims {{to address}} an {{analysis}} of the more considerable research output (papers published in the seven important databases) “Big Data in Healthcare” for achieving a deep and comprehensive trend study and based on it, makes a knowledge discovery from the publications. Using Naïve Bayes, results identified a classification of methodologies used in <b>duplicated</b> <b>papers</b> in journals.|$|R
50|$|Hopkins' {{organizing}} {{scheme was}} based on a series of <b>duplicate</b> <b>paper</b> records that contained collection and identification information about the specimens. Each record was given a unique sequential Hopkins U.S. Number. Specimens, each one bearing its own unique Hopkins US Number, and collection records were stored at Forest Service Research Stations and Laboratories, Regional Offices, and at the ARS-SEL in the U.S. National Museum / Smithsonian Institution.|$|R
40|$|Since the {{publication}} of Robert K. Merton’s theory of cumulative advantage in science (Matthew Effect), several empirical studies have tried to measure its presence {{at the level of}} papers, individual researchers, institutions or countries. However, these studies seldom control for the intrinsic “quality ” of papers or of researchers—“better ” (however defined) papers or researchers could receive higher citation rates because they are indeed of better quality. Using an original method for controlling the intrinsic value of papers— identical <b>duplicate</b> <b>papers</b> published in different journals with different impact factors—this paper shows that the journal in which papers are published have a strong influence on their citation rates, as <b>duplicate</b> <b>papers</b> published in high impact journals obtain, on average, twice as much citations as their identical counterparts published in journals with lower impact factors. The intrinsic value of a paper is thus not the only reason a given paper gets cited or not; there is a specific Matthew effect attached to journals and this gives to paper published there an added value over and above their intrinsic quality...|$|R
40|$|The {{goal of the}} American Association for Agricultural Education (AAAE) Posters {{session is}} to promote the sharing of ideas and {{research}} that support agricultural education. The informal setting of the poster session will enhance sharing of ideas and promote collaboration between conference participants. Poster Submission Requirements Posters are accepted in two categories: innovative ideas and research. Submissions cannot have been previously presented at the regional or national level (e. g. prior year). Posters should NOT <b>duplicate</b> <b>paper</b> presentations at the same conference. At least one author of the poster must {{be a member of}} the AAAE. Posters topics should be of significant interest to the national membership...|$|R
6000|$|... "Well, I mustn't {{tell you}} his name, of course," [...] said {{the little man}} simply. [...] "He was a penitent, you know. He had lived prosperously for about twenty years entirely on <b>duplicate</b> brown <b>paper</b> parcels. And so, you see, when I began to suspect you, I thought of this poor chap's {{way of doing it}} at once." ...|$|R
40|$|In wide-coverage lexicalized grammars {{many of the}} {{elementary}} structures have substructures in common. This means that during parsing some of the computation associated with different structures is <b>duplicated.</b> This <b>paper</b> explores {{ways in which the}} grammar can be precompiled into finite state automata so that some of this shared structure results in shared computation at run-time. 1 Introduction This paper investigates grammar precompilation techniques aimed at improving the parsing performance of lexicalized grammars. In a wide-coverage lexicalized grammar, such as the XTAG grammar (XTAG-Group, 1995), many of {{the elementary}} structures 1 have substructures in common. If such structures are viewed as independent by a parsing algorithm, the computation associated with their shared structure may be <b>duplicated.</b> This <b>paper</b> explores ways in which the grammar can be precompiled so that some of this shared structure results in shared computation at run-time. We assume as a starting point a co [...] ...|$|R
40|$|A hybrid {{intelligent}} gas {{array sensor}} (or electronic nose) has been constructed, which comprises 10 CHEMFET devices, four Taguchi gas sensors (TGS), one infrared CO 2 sensor and a microcomputer {{in order to}} examine the odours from five cardboard papers from commercial manufacturers. Four of the papers came from two different production lines of the same Swedish manufacturer; the <b>duplicate</b> <b>paper</b> from each line was processed further to reduce the odour from this packaging material. The fifth paper came from another manufacturer. The sensor array data was screened using both principal component analysis (PCA) and cluster analysis (CA), and predictively classified using a back-propagation neural network. It was discovered using PCA/CA that only four of the 15 sensors were necessary to discriminate totally between the five classes of paper when air, which was initially classified separately, was used as a reference. Thus we have shown that the olfactory quality of cardboard papers can be recognized using a simple hybrid CHEMFET/TGS electronic nose...|$|R
5000|$|In July 2008, Richard Poynder's {{interview}} series brought {{attention to}} the practices of new publishers who were [...] "better able to exploit the opportunities of the new environment." [...] Doubts about honesty and scams in a subset of open-access journals continued to be raised in 2009. [...] Concerns for spamming practices from the [...] "black sheep among open access journals and publishers" [...] ushered the leading open access publishers to create the Open Access Scholarly Publishers Association in 2008. [...] In another early precedent, in 2009 the Improbable Research blog had found that Scientific Research Publishing's journals <b>duplicated</b> <b>papers</b> already published elsewhere; the case was subsequently reported in Nature. In 2010, Cornell University graduate student Phil Davis (editor of the Scholarly Kitchen blog) submitted a manuscript consisting of computer-generated nonsense (using SCIgen) which was accepted for a fee (but withdrawn by the author). [...] Predatory publishers {{have been reported to}} hold submissions hostage, refusing to allow them to be withdrawn and thereby preventing submission in another journal.|$|R
40|$|Recursively {{designed}} {{internal models}} have been extensively applied in the output regulation problem for lower-triangular systems. In particular, one internal model component is constructed at each step {{to compensate for the}} steady state of one plant state (or input). This design method is simple with the potential expense of high dimension when some components are <b>duplicate.</b> This <b>paper</b> proposes a novel approach of designing a minimal dimension internal model by eliminating any possible duplication...|$|R
40|$|This Organisational Development Project {{focused on}} how admissions through a hospital’s Emergency Department are organized. The primary aim was to {{streamline}} the process of organising these admissions by reducing waste and utilising available technology to improve patient flow. Following the introduction of Governmental targets in Healthcare, individual healthcare institutions are seeking ways to achieve these targets. The current process in place has many steps with <b>duplicate</b> <b>paper</b> records of ward allocation for patients who require admission. The patients 2 ̆ 7 final destination within the hospital was not recorded on the Emergency Departments 2 ̆ 7 Patient Information Management system (Symphony). Through the process of Lean Management the process was mapped and non-value added steps were removed. Symphony was upgraded to allow the Bed Managers to electronically record the patient’s final destination. This in turn also removed further non value steps. This increased efficiency has allowed for a reduction in paper records, a reduced workload for staff {{and a reduction in}} the time patients spend in the Emergency Department. This project has improved an aspect of patient flow and will enhance the delivery of safe and effective healthcare...|$|R
50|$|The next {{development}} was the paper pad system. Diplomats had long used codes and ciphers for confidentiality and to minimize telegraph costs. For the codes, {{words and phrases}} were converted to groups of numbers (typically 4 or 5 digits) using a dictionary-like codebook. For added security, secret numbers could be combined with (usually modular addition) each code group before transmission, with the secret numbers being changed periodically (this was called superencryption). In the early 1920s, three German cryptographers (Werner Kunze, Rudolf Schauffler and Erich Langlotz), {{who were involved in}} breaking such systems, realized that they could never be broken if a separate randomly chosen additive number was used for every code group. They had <b>duplicate</b> <b>paper</b> pads printed with lines of random number groups. Each page had a serial number and eight lines. Each line had six 5-digit numbers. A page would be used as a work sheet to encode a message and then destroyed. The serial number of the page would be sent with the encoded message. The recipient would reverse the procedure and then destroy his copy of the page. The German foreign office put this system into operation by 1923.|$|R
50|$|SCIRP {{generated}} {{controversy in}} 2010 {{when it was}} found that its journals <b>duplicated</b> <b>papers</b> which had already been published elsewhere, without notification of or permission from the original author and of the copyright holder. Several of these publications have subsequently been retracted. Some of the journals had listed academics on their editorial boards without their permission or even knowledge, sometimes in fields very different from their own. In 2012, one of its journals, Advances in Pure Mathematics, accepted a paper written by a random text generator. The paper was not published, but only due to its author's unwillingness to pay the publication fee. The company has also been noted for the many unsolicited bulk emails it sends to academics about its journals. In 2013, the Open Journal of Pediatrics, a SCIRP journal, published a study which concluded that the number of babies born with thyroid problems in the western United States increased by 16 percent in 2011 compared to 2010, after the Fukushima Daiichi nuclear disaster. The study has been criticized for not taking into account the fact that 2010 was a year with an unusually low number of births with thyroid problems. SCIRP refused to print a letter criticizing the study, but offered to publish it as an article for a charge.|$|R
25|$|Sometime in {{the early}} to mid 1960s, the BEP experimented with a new firm, the Gilbert Paper Company, {{to see if they could}} <b>duplicate</b> the usual <b>paper</b> production. The BEP {{selected}} a series of notes printed by the Philadelphia FRB as the test subjects. Serial numbers for this group range from C60800001AC61440000A.|$|R
40|$|Abstract –Duplicate Elimination (DE) is a {{specialized}} data compression technique for eliminating duplicate copies of repeating data {{to optimize the}} use of storage space or bandwidth. The {{most common form of}} DE implementation works by dividing files as chunks and comparing chunks of data to detect <b>duplicates.</b> This <b>paper</b> implements a content-based chunking algorithm to improve duplicate elimination over fixed-sized blocking, and evaluates the methods of chunk comparison, that is, compare-by-hash versus compare-by-value. It indicates that compare-by-hash is efficient and feasible even employed in ultra-large-scale storage systems. Index Terms – Data Storage, Duplicate Elimination, Compare-by-Hash. 1...|$|R
40|$|This {{work is the}} French {{translation}} of articles related to {{the wreck of the}} Jeannette in the New York herald. It is dedicated to the owner/editor of the New York herald. "Il a été tiré 25 exemplaires sur hollande Van Gelder, avec gravures sur papier teinté [...] . Exemplaire no. 9. "Frontispieces <b>duplicated</b> on laid <b>paper...</b>|$|R
40|$|The {{problem of}} {{identifying}} approximately duplicate objects in databases {{is an essential}} step for the information integration process. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential <b>duplicates.</b> In this <b>paper,</b> we present a framework for improving duplicate detection using trainable measures of textual similarity...|$|R
50|$|Sometime in {{the early}} to mid 1960s, the BEP experimented with a new firm, the Gilbert Paper Company, {{to see if they could}} <b>duplicate</b> the usual <b>paper</b> production. The BEP {{selected}} a series of notes printed by the Philadelphia FRB as the test subjects. Serial numbers for this group range from C60800001A - C61440000A.|$|R
30|$|Results {{show the}} most <b>duplicated</b> <b>papers</b> belong to Springer database, and year of 2016 had the high {{frequency}} of publication. Big Data” are the high-frequency words and also key words and results verified the expectation. Results of applying VOSviewer for keywords, title, abstracts and conclusion of papers show eight clusters of words. The clusters are: public health, health informatics, healthcare big data research, data science, association, e-health encryption and things. Journal of medical systems published most {{papers in the}} field. decision Tree was most used techniques in data mining in papers applied data mining. The most number of author is 57. Health data analytics has the first rank among the subject. Males having Ph.D. degree with university affiliations had the dominant rate of authors. Meta-analysis and evidence was the most used Big Data methodology. In addition to descriptive statistics methods, in order to perform scientometrics study, a prediction technique (classification) {{has been done on}} Big Data methodology used in the papers of various databases and knowledge discovered from them. According to the results, the Nature database had the maximum accuracy in the results, and the “Agent-based modeling” had the maximum call in Wiley’s database. It shows applied Big Data methodology in papers of Nature could be better predicted, and the other papers of this database are more consonant in Big Data methodology. Moreover in papers of Wiley database there was no papers with “Agent-based modeling” methodology which its methodologies predicted false.|$|R
40|$|Purpose: to {{establish}} {{what is known}} regarding the psychological and social problems faced by adult cancer survivors (people who are living with and beyond a diagnosis of cancer) and identify areas future research should address. Method: a rapid search of published literature reviews held in electronic data bases was under taken. Inclusion and exclusion criteria, and removal of <b>duplicated</b> <b>papers,</b> reduced the initial number of papers from 4051 to 38. Twenty-two review papers were excluded on grounds of quality and 16 review papers were selected for appraisal. Results: the psychological and social problems for cancer survivors are identified as depression, anxiety, distress, fear of recurrence, social support/function, relationships and impact on family, and quality of life. A substantial minority of people surviving cancer experience depression, anxiety, and distress or fear associated with recurrence or follow up. There is some indication that social support is positively associated with better outcomes. Quality of life for survivors of cancer appears generally good for most people, but an important minority experience a reduction in quality of life, especially those with more advanced disease and reduced social and economic resources. The majority of research knowledge is based on women with breast cancer. The longer term implications of cancer survival have not been adequately explored. Conclusions: focussing well designed research in the identified areas where less is already known about the psychological and social impact of cancer survival {{is likely to have}} the greatest impact on the wellbeing of people surviving cance...|$|R
40|$|Most {{database}} theory {{focused on}} investigating databases containing sets of tuples. In practice databases often implement relations using bags, i. e. sets with <b>duplicates.</b> In this <b>paper</b> we study how database query languages are aected {{by the use}} of duplicates. We consider query languages that are simple extensions of the (nested) relational algebra, and investigate their resulting expressive power and complexity. ...|$|R
40|$|Broadcast in a {{communication}} network is the delivery of copies of messages to all nodes. A broadcast protocol is reliable if all messages reach all nodes in finite time, in the correct order and with no <b>duplicates.</b> The present <b>paper</b> presents an efficient reliable broadcast protocol. The work of A. Segall was performed on a consulting agreement with the Laboratory fo...|$|R
40|$|Objective: The {{arrival of}} {{precision}} medicine plan brings new {{opportunities and challenges}} for patients undergoing precision {{diagnosis and treatment of}} malignant tumors. With the development of medical imaging, information on different modality imaging can be integrated and comprehensively analyzed by imaging fusion system. This review aimed to update the application of multimodality imaging fusion technology in the precise diagnosis and treatment of malignant tumors under the precision medicine plan. We introduced several multimodality imaging fusion technologies and their application to the diagnosis and treatment of malignant tumors in clinical practice. Date Sources: The data cited in this review were obtained mainly from the PubMed database from 1996 to 2016, using the keywords of "precision medicine", "fusion imaging", "multimodality", and "tumor diagnosis and treatment". Study Selection: Original articles, clinical practice, reviews, and other relevant literatures published in English were reviewed. Papers focusing on precision medicine, fusion imaging, multimodality, and tumor diagnosis and treatment were selected. <b>Duplicated</b> <b>papers</b> were excluded. Results: Multimodality imaging fusion technology {{plays an important role in}} tumor diagnosis and treatment under the precision medicine plan, such as accurate location, qualitative diagnosis, tumor staging, treatment plan design, and real-time intraoperative monitoring. Multimodality imaging fusion systems could provide more imaging information of tumors from different dimensions and angles, thereby offing strong technical support for the implementation of precision oncology. Conclusion: Under the precision medicine plan, personalized treatment of tumors is a distinct possibility. We believe that multimodality imaging fusion technology will find an increasingly wide application in clinical practice...|$|R
40|$|M. Balter (“Reviewer's Déjà Vu, French science sleuthing uncover plagiarized papers,” News & Analysis, 9 March, p. 1157) {{describes}} how a scientist recently published {{at least nine}} articles that largely or entirely <b>duplicated</b> <b>papers</b> written by others and was exposed only after we found one of our papers integrally copied in a manuscript that both of us coincidentally received for review. What is remarkable here {{is not only the}} flagrant fraud, but the fact that six of these papers were published in scholarly journals only last year. Publishers can easily prevent publishing plagiarism by systematically running submitted manuscripts through software such as CrossCheck and eBlast (1, 2) or by running strings of words that are unlikely to be repeated by chance through search engines (3). It is evident that not all publishers systematically use these tools, despite the fact that plagiarism is common (1, 2). It is also noteworthy that these six 2011 papers—as well as the manuscript for review—are all from journals of publishers that Beall (4) lists as “predatory open-access scholarly publishers. ” Such publishers “exploit the author-pays, Open-Access model for their own profit” and do not invest in quality control (4, 5). In this light, it is less surprising that papers escape plagiarism detection today. We argue that publishers that do not systematically use anti-plagiarism tools consciously take the risk of copyright infringement and of being accomplices in plagiarism. We encourage copyright holders to sue publishers of plagiarism for these offenses. When fines become a realistic threat, plagiarism prevention will become valuable even for predatory publisher...|$|R
40|$|Purpose– The {{purpose of}} this paper is to provide an {{overview}} of previous studies in the field of stakeholder management, and propose implications for the construction industry. Design/methodology/approach– Three major databases are searched: ABI, EI CompendexWeb, and ISI web of knowledge. Papers are searched on topic by using the keywords of "stakeholder management", "management of stakeholders" and "management of stakeholder". A brief review of the abstracts and conclusions of these papers is conducted to filter out the irrelevant and/or <b>duplicate</b> <b>papers.</b> After filtering, 159 articles with content relevant to stakeholder management are selected for analysis. Findings– An overview of previous studies reveals that research interest in stakeholder management has turned to the descriptive approach. Through a critical review of stakeholder management process, three main problems of previous studies are identified: very few methods and tools are available to identify all stakeholders and their interests; limited studies involve the change management about the stakeholders' influence and relationship; and few studies are capable of reflecting the influence of the entire relationship network in practice. Research limitations/implications– Two implications for the construction industry are suggested: establish a practical framework for managing stakeholders; and apply social network theory (SNT) in developing a stakeholder relationship model. Originality/value– The overview and implications lead to new knowledge and an improved understanding of the management of multiple stakeholders in construction projects. The perspective of SNT avoids the deficiency of Freeman's dyadic ties model, and the project managers can make decisions in response to the stakeholder behaviours according to the entire relationship. Department of Building and Real Estat...|$|R
40|$|Fraud is a {{dramatic}} offense in scientific publishing but other offenses are more frequent and probably far more damaging. The most frequent lesser offenses are irresponsible authorship and wasteful publication. The authorship problems include listing "authors " who made little or no contribution to the work reported and omitting of persons who made major contributions. Wasteful publication includes dividing the results in a single study into two or more papers ("salami science"); republishing the same material in successive papers (which need not have identical format and content); and blending data from one study with additional data to extract yet another paper that could not make its way on {{the second set of}} data alone ("meat extenders"). Wasteful publication may be the most frequent of these offenses and is probably the most damaging because of its economic implications for publishers, readers, libraries, and indexes. F R A U D in scientific publication is {{a dramatic}} abuse and it readily catches the eye of the popular press and the pub-lic. But there are abuses that because of their frequency and ubiquity should concern us more than the extreme of fraud. These are abuses of two kinds: irresponsible au-thorship and wasteful publication. Public evidence of these abuses is scanty and largely anecdotal, although they have been discussed in the liter-ature (1, 2). Aside from May's study (3) of <b>duplicate</b> <b>papers</b> in one small corner of mathematics, however, I do not know of any thorough and quantitative studies of the extent and frequency of these problems...|$|R
40|$|Purpose – The {{purpose of}} this paper is to provide an {{overview}} of previous studies in the field of stakeholder management, and propose implications for the construction industry. Design/methodology/approach – Three major databases are searched: ABI, EI CompendexWeb, and ISI web of knowledge. Papers are searched on topic by using the keywords of “stakeholder management”, “management of stakeholders” and “management of stakeholder”. A brief review of the abstracts and conclusions of these papers is conducted to filter out the irrelevant and/or <b>duplicate</b> <b>papers.</b> After filtering, 159 articles with content relevant to stakeholder management are selected for analysis. Findings – An overview of previous studies reveals that research interest in stakeholder management has turned to the descriptive approach. Through a critical review of stakeholder management process, three main problems of previous studies are identified: very few methods and tools are available to identify all stakeholders and their interests; limited studies involve the change management about the stakeholders 2 ̆ 7 influence and relationship; and few studies are capable of reflecting the influence of the entire relationship network in practice. Research limitations/implications – Two implications for the construction industry are suggested: establish a practical framework for managing stakeholders; and apply social network theory (SNT) in developing a stakeholder relationship model. Originality/value – The overview and implications lead to new knowledge and an improved understanding of the management of multiple stakeholders in construction projects. The perspective of SNT avoids the deficiency of Freeman 2 ̆ 7 s dyadic ties model, and the project managers can make decisions in response to the stakeholder behaviours according to the entire relationship. <br /...|$|R
40|$|Code {{duplication}} {{is considered}} as bad practice that complicates the maintenance {{and evolution of}} software. Detecting duplicated code is a difficult task {{because of the large}} amount of data to be checked and the fact that aprioriit is unknown which code part has been <b>duplicated.</b> In this <b>paper,</b> we present a tool called DUPLOC that supports code duplication detection in a visual and exploratory or an automatic way...|$|R
40|$|Identifying {{approximately}} duplicate records between databases {{requires the}} costly computation of distances between their attributes. Thus duplicate detection is usually performed in two phases, an efficient blocking phase that determines few potential candidate duplicates based on simple criteria, {{followed by a}} second phase performing an in-depth comparison of the candidate <b>duplicates.</b> This <b>paper</b> introduces and evaluates a precise and efficient approach for the blocking phase, which requires only standard indices, but performs {{as well as other}} approaches based on special purpose indices, and outperforms other approaches based on standard indices. The key idea of the approach is to use a comparison window with a size that depends dynamically on a maximum distance, rather than using a window with fixed size...|$|R
40|$|The American Jewish Joint Distribution Committee (also {{known as}} the JDC) was founded on November 27, 1914, in order to {{facilitate}} and centralize the collection and distribution of funds by American Jews for Jews abroad. This collection consists of the German case files of the JDC (dating from 1929 to 1947). The records document various individuals’ immigration attempts. Although the JDC was at first reluctant to aid individuals directly, this became necessary after the outbreak of World War II. Documents concerning many prominent individuals may be found in this collection. The collection also contains a small section of Italian case files of the JDC, 1945 - 1947, for displaced persons in Italy after the war, concerning attempts to contact friends and relatives, conditions in Italy, and attempts to emigrate. The files of the New York central office of the JDC are stored in the JDC archives in New York. Records on the European offices of the JDC are now stored at the Jerusalem office of the organization. A few selected items and <b>duplicate</b> <b>papers</b> can also be found at the YIVO Institute for Jewish Research in New York. For background information on the JDC and its efforts in the 1930 s and 1940 s, the researcher is referred to the studies of Yehuda Bauer, which are based on extensive research in the JDC archives: "My Brother's Keeper: A History of the American Jewish Joint Distribution Committee 1929 - 1939 " (Philadelphia: jewish Publication Society, 1974) and "American Jewry and the Holocaust: the American Jewish Joint Distribution Committee, 1939 - 1945 " (Detroit: Wayne State University Press, 1981);See also annotated archival catalogues for 1933 - 1964 in "Guides to Other Archives Collection" (AR 25054);digitizedPhotographs removed to Photograph Collectio...|$|R
40|$|Aim: To {{investigate}} the procedural aspects in inserting central venous catheters that minimise central line associated bloodstream infections rates in adult intensive care units through a structured literature review. Background: In adult intensive care units (AICU), central line associated bloodstream infections (CLABSI) {{are a major}} cause of high mortality rates and increased in costs due to the consequences of complications. Methods: Eligible articles were identified by combining indexed keywords using Boolean operator of “AND” under databases of Ovid and CINAHL. Titles and abstract of retrieved papers were screened and duplicates removed. An inclusion and exclusion criteria was applied to derive the final papers which contained seminal studies. The quality of papers was assessed using a special data extraction form. Results: The number of papers retrieved from all databases was 337, reduced to 302 after removing <b>duplicates.</b> <b>Papers</b> were scanned for titles and abstract to locate those relevant to the review question. After this, 250 papers were excluded for different reasons and a total of 52 papers were fully accessed to assess for eligibility. The final number of papers included was 10 articles. Conclusion: Many interventions can be implemented in the AICU during the insertion of a central venous catheter (CVC) to minimise CLABSI rates. These include choosing the subclavian site to insert the catheters as the least infectious and decolonising patients’ skin with alcoholic chlorhexidine gluconate (CHG) preparation due to its broad antimicrobial effect and durability. Relevance to clinical practice: Choosing optimal sites for CVC insertion is a complex process that relies on many factors. Furthermore, the introduction of CHG preparations should be accompanied with multifaceted interventions including quality improvement initiatives to improve healthcare workers’ compliance. As a quality marker in AICUs, healthcare sectors should work on establishing benchmarks with other sectors around the world...|$|R
