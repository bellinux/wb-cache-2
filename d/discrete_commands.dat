18|42|Public
2500|$|... {{provide the}} {{capability}} to issue coded and <b>discrete</b> <b>commands</b> to spacecraft users ...|$|E
40|$|In this work, a novel command shaping control {{strategy}} for oscillation reduction of simple harmonic oscillators is proposed, and validated experimentally. A wave-form acceleration command shaper is derived analytically. The {{performance of the}} proposed shaper is simulated numerically, and validated experimentally on a scaled model of an overhead crane. Amplitude modulation is used to enhance the shaper performance, which results in a modulated wave-form command shaper. It is determined that the proposed wave-form and modulated wave-form command shaper profiles are capable of eliminating travel and residual oscillations. Furthermore, unlike traditional impulse and step command shapers, the proposed command shaper has piecewise smoother acceleration, velocity, and displacement profiles. Experimental results using continuous and <b>discrete</b> <b>commands</b> are presented. Experiments with <b>discrete</b> <b>commands</b> involved embedding a saturation model-based feedback in the algorithm of the command shaper...|$|E
40|$|Most {{unmanned}} {{missions in}} space and undersea are commanded by a “script ” that specifies a sequence of <b>discrete</b> <b>commands</b> and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals t...|$|E
5000|$|<b>Discrete</b> <b>commanding,</b> {{monitoring}} and temperature monitoring services ...|$|R
40|$|A {{control system}} and method for {{prosthetic}} devices is provided. The control system comprises a transducer for receiving movement from a body part for generating a sensing signal {{associated with that}} movement. The sensing signal is processed by a linearizer for linearizing the sensing signal to be a linear function {{of the magnitude of}} the distance moved by the body part. The linearized sensing signal is normalized to be a function of the entire range of body part movement from the no-shrug position of the movable body part through the full-shrug position of the movable body part. The normalized signal is divided into a plurality of <b>discrete</b> <b>command</b> signals. The <b>discrete</b> <b>command</b> signals are used by typical converter devices which are in operational association with the prosthetic device. The converter device uses the <b>discrete</b> <b>command</b> signals for driving the movable portions of the prosthetic device and its sub-prosthesis. The method for controlling a prosthetic device associated with the present invention comprises the steps of receiving the movement from the body part, generating a sensing signal in association with the movement of the body part, linearizing the sensing signal to be a linear function of the magnitude of the distance moved by the body part, normalizing the linear signal to be a function of the entire range of the body part movement, dividing the normalized signal into a plurality of <b>discrete</b> <b>command</b> signals, and implementing the plurality of <b>discrete</b> <b>command</b> signals for driving the respective movable prosthesis device and its sub-prosthesis...|$|R
40|$|Abstract: The paper {{deals with}} the pattern {{recognition}} control systems. The concept of command situation is introduced, in order to permit {{the analysis of the}} properties of control systems that determine the appropriate <b>discrete</b> <b>command</b> by classification. A case study, carried out on a biotechnological process, is presented, to show the performances of the pattern recognition controller, trained with data supplied by a MBPC procedure...|$|R
40|$|Hands-free {{control of}} {{unmanned}} ground vehicles {{is essential for}} soldiers, bomb disposal squads, and first responders. Having their hands free for other equipment and tasks allows them to be safer and more mobile. Currently, the most successful hands-free control devices are speech-command based. However, these devices use external microphones, and in field environments, e. g., war zones and fire sites, their performance suffers because of loud ambient noise: typically above 90 dBA. This paper describes the development of technology using the ear as an output source that can provide excellent command recognition accuracy even in noisy environments. Instead of picking up speech radiating from the mouth, this technology detects speech transmitted internally through the ear canal. Discreet tongue movements also create air pressure changes within the ear canal, {{and can be used}} for stealth control. A patented earpiece was developed with a microphone pointed into the ear canal that captures these signals generated by tongue movements and speech. The signals are transmitted from the earpiece to an Ultra-Mobile Personal Computer (UMPC) through a wired connection. The UMPC processes the signals and utilizes them for device control. The processing can include command recognition, ambient noise cancellation, acoustic echo cancellation, and speech equalization. Successful control of an iRobot PackBot has been demonstrated with both speech (13 <b>discrete</b> <b>commands)</b> and tongue (5 <b>discrete</b> <b>commands)</b> signals. In preliminary tests, command recognition accuracy was 95 % with speech control and 85 % with tongue control...|$|E
40|$|Abstract — Despite {{the many}} {{significant}} advances made in robotics research, few works {{have focused on}} the tight integration of task planning and motion control. Most integration works involve the task planner providing <b>discrete</b> <b>commands</b> to the low-level controller, which performs kinematics and control computations to command the motor and joint actuators. This paper presents a framework of the integrated planning and control for mobile robot navigation. Unlike existing integrated approaches, it produces a sequence of checkpoints instead of a complete path at the planning level. At the motion control level, a neural network is trained to perform motor control that moves the robot from one checkpoint to the next. This method allows for a tight integration between high-level planning and low-level control, which permits real-time performance and easy modification of motion path while the robot is enroute to the goal position. I...|$|E
40|$|A {{controlled}} impact demonstration (CID) program using a large, four engine, remotely piloted {{transport airplane}} was conducted. Closed loop {{primary flight control}} was performed from a ground based cockpit and digital computer in conjunction with an up/down telemetry link. Uplink commands were received aboard the airplane and transferred through uplink interface systems to a highly modified Bendix PB- 20 D autopilot. Both proportional and <b>discrete</b> <b>commands</b> were generated by the ground pilot. Prior to flight tests, extensive simulation was conducted during the development of ground based digital control laws. The control laws included primary control, secondary control, and racetrack and final approach guidance. Extensive ground checks were performed on all remotely piloted systems. However, manned flight tests were the primary method of verification and validation of control law concepts developed from simulation. The design development, and flight testing of control laws and the systems required to accomplish the remotely piloted mission are discussed...|$|E
40|$|We propose two {{algorithms}} for {{real-time tracking}} {{of the location}} and dynamic motion of a mobile station in a cellular network using the pilot signal strengths from neighboring base stations. The underlying mobility model {{is based on a}} dynamic linear system driven by a <b>discrete</b> <b>command</b> process that determines the mobile station's acceleration. The command process is modeled as a semi-Markov process over a finite set of acceleration levels. The first algorithm consists of an averaging filter for processing pilot signal strength measurements and two Kalman filters, one to estimate the <b>discrete</b> <b>command</b> process and the other to estimate the mobility state. The second algorithm employs a single Kalman filter without pre-filtering and is able to track a mobile station even when a limited set of pilot signal measurements is available. Both of the proposed tracking algorithms can be used to predict future mobility behavior, which can be useful in resource allocation applications. Our numerical results show that the proposed tracking algorithms perform accurately over a wide range of mobility parameter values...|$|R
40|$|Abstract—We propose two {{algorithms}} for {{real-time tracking}} {{of the location}} and dynamic motion of a mobile station in a cellular network using the pilot signal strengths from neighboring base stations. The underlying mobility model {{is based on a}} dynamic linear system driven by a <b>discrete</b> <b>command</b> process that determines the mobile station’s acceleration. The command process is modeled as a semi-Markov process over a finite set of acceleration levels. The first algorithm consists of an averaging filter for processing pilot signal strength measurements and two Kalman filters, one to estimate the <b>discrete</b> <b>command</b> process and the other to estimate the mobility state. The second algorithm employs a single Kalman filter without prefiltering and is able to track a mobile station even when a limited set of pilot signal measurements is available. Both of the proposed tracking algorithms can be used to predict future mobility behavior, which can be useful in resource allocation applications. Our numerical results show that the proposed tracking algorithms perform accurately over a wide range of mobility parameter values. Index Terms—Cellular networks, mobility model, geolocation, pilot signal strengths, Kalman filter. ...|$|R
40|$|Abstract- This paper {{addresses}} {{the problem of}} position localisation of mobile nodes in ad hoc wireless networks based on received signal strength indicator measurements. Node mobility is modelled as a linear system driven by a <b>discrete</b> <b>command</b> Markov process. Selflocalisation of mobile nodes is performed via an Interacting Multiple Model filter consisting of a bank of Unscented Kalman filters (IMM-UKF). Estimation of the mobility state, which comprises the position, speed and acceleration of the mobile nodes is accomplished. The performance of the IMM-UKF filter is investigated and compared to a multiple model particle filter (MM PF) by Monte Carlo simulation...|$|R
40|$|The work {{presented}} here {{is part of}} a bigger effort to design an automated highway system to improve the capacity and safety of the current highways. Automation of highways and in particular platooning of vehicles raises a number of control issues. In the design proposed in [1] these issues are addressed by a hierarchical structure consisting of both discrete event and continuous controllers. Our work is an attempt to construct a consistent interface between these two types of controllers. The proposed design {{is in the form of}} a set of finite state machines that interact with the discrete controllers through <b>discrete</b> <b>commands</b> and with the continuous controllers by issuing commands that get translated to inputs for the vehicle actuators. The operation of the proposed design is verified using COSPAN and tested in simulation. The interface design provides insight into interesting problems related to the hybrid nature of the automated highway system, as it touches on both the discrete and c [...] ...|$|E
40|$|The {{transformation}} from high level task speci cation to low level motion {{control is a}} fundamental issue in sensorimotor control in animals and robots. This paper describes a control scheme called Virtual Model Control that addresses this issue. Virtual Model Control is a motion control language that uses simulations of imagined mechanical components to create forces, which are applied through real joint torques, thereby creating the illusion that the virtual components {{are connected to the}} robot. Due to the intuitive nature of this technique, designing a Virtual Model Controller requires the same skills as designing the mechanism itself. A high level control system can be cascaded with the low level Virtual Model Controller to modulate the parameters of the virtual mechanisms. <b>Discrete</b> <b>commands</b> from the high level controller would then result in uid motion. Virtual Model Control has been applied to a physical bipedal walking robot. A simple algorithm utilizing a simple set of virtual components has successfully compelled the robot to walk continuously over level terrain. ...|$|E
40|$|A novel control {{strategy}} based on motion description language (MDL) is proposed in this paper, to distribute computational and communication resources {{in a particular}} tele-robotic system. On one hand, limited bandwidth, random time delay and other transmission problems of Internet make it necessary to reduce data transferred between master and slave site. On the other hand, remote robotic system has a fairly good autonomy. As a result, it is preferable to transmit high-level <b>discrete</b> <b>commands</b> between master and slave site. In this paper, a symbolic-based control framework based on MDL is put forward and implemented on a tele-manipulator system. Especially, open-loop and closed-loop component are obtained based on the resolved motion method, termination conditions for atoms are acquired {{from the point of}} view optimization problem. The obvious characteristic of the new method is that control commands have a linguistic flavor, data transferred between operator and remote robot can be reduced accordingly. A prototype tele-robotic system based on MDL is constructed and the experiment results validate the efficiency of the proposed method. © 2011 IEEE. Link_to_subscribed_fulltex...|$|E
40|$|This paper {{presents}} a HMM-based speech recognition engine and its integration into direct manipulation interfaces for Korean document editor. Speech recognition can reduce typical tedious and repetitive actions which are inevitable in standard GUIs (graphic user interfaces). Our system consists of general speech recognition engine called ABrain {Auditory Brain} and speech commandable document editor called SHE {Simple Hearing Editor}. ABrain is a phoneme-based speech recognition engine which shows up to 97 % of <b>discrete</b> <b>command</b> recognition rate. SHE is a EuroBridge widget-based document editor that supports speech commands {{as well as}} direct manipulation interfaces. Comment: 6 pages, ps file, presented at icmi 96 (Bejing...|$|R
40|$|International Telemetering Conference Proceedings / October 21, 2002 / Town & Country Hotel and Conference Center, San Diego, CaliforniaFor {{the last}} 30 years Magnetic Tape Systems {{have been the}} primary means of {{recording}} data from airborne instrumentation systems. Increasing data rates and harsh environmental requirements have often exceeded the ability of tape-based systems {{to keep pace with}} platform technology. This paper examines operational and data reduction benefits when employing the IRIG 106 Chapter 10 Solid State Recorder Standard introduced by the Range Commanders Council (RCC) Telemetry Group (TG). The Standard and this paper address media formatting, data formatting for a variety of different data types, data downloading, and data security, along with serial command and control and <b>discrete</b> <b>command</b> and control of the recorder. This paper also addresses software data processing and raw data reconstruction of Chapter 10 data...|$|R
40|$|During Summer 2013, the Intelligent Robotics Group at NASA Ames Research Center {{conducted}} {{a series of}} tests to examine how astronauts in the International Space Station (ISS) can remotely operate a planetary rover. The tests simulated portions of a proposed lunar mission, in which an astronaut in lunar orbit would remotely operate a planetary rover to deploy a radio telescope on the lunar far side. Over the course of Expedition 36, three ISS astronauts remotely operated the NASA "K 10 " planetary rover in an analogue lunar terrain located at the NASA Ames Research Center in California. The astronauts used a "Space Station Computer" (crew laptop), a combination of supervisory control (command sequencing) and manual control (<b>discrete</b> <b>commanding),</b> and Ku-band data communications to command and monitor K 10 for 11 hours. In this paper, we present and analyze test results, summarize user feedback, and describe directions for future research...|$|R
40|$|Purpose of the S/C “Ocean-O ” is {{operative}} {{reception of}} Earth and World ocean remote probe information. After the launch {{the control group}} came across complexity of S/C attitude control. In condition of atmosphere density difference control system nominal work {{turned out to be}} impossible because of lack of flywheel uncharge by electromagnetic plant in tangage channel. Russian-Ukraine commission suggested and the control group accomplished unique scheme of S/C attitude control. The main idea of this scheme is performing periodic correction of the solar array position during communication session by issuing <b>discrete</b> <b>commands</b> to make necessary aerodynamic and gravitational moments combination that compensates disturbing moment. To support energy balance limits on angle of solar array turning were taken into consideration. Analytic dependences were come out by input a number of simplifications into general system f differential equations, that describe the change dynamics of kinetic moment of flywheel. This dependences help to determine solar array inclination angle to provide necessary aerodynamic and gravitational moments combination. Using such scheme turned out to be effective in S/C control in geomagnetic storms (July, 15 - 16, 2000...|$|E
40|$|The Dryden Flight Research Center Facility of NASA Ames Research Center (Ames-Dryden) and the FAA {{conducted}} the controlled impact demonstration (CID) program using a large, four-engine, remotely piloted jet transport airplane. Closed-loop primary flight was controlled through the existing onboard PB- 20 D autopilot {{which had been}} modified for the CID program. Uplink commands were sent from a ground-based cockpit and digital computer in conjunction with an up-down telemetry link. These uplink commands were received aboard the airplane and transferred through uplink interface systems to the modified PB- 20 D autopilot. Both proportional and <b>discrete</b> <b>commands</b> were produced by the ground system. Prior to flight tests, extensive simulation was conducted during the development of ground-based digital control laws. The control laws included primary control, secondary control, and racetrack and final approach guidance. Extensive ground checks were performed on all remotely piloted systems; however, piloted flight tests were the primary method and validation of control law concepts developed from simulation. The design, development, and flight testing of control laws and systems required to accomplish the remotely piloted mission are discussed...|$|E
40|$|Abstract: This paper {{presents}} an intelligent shared control system {{applied to a}} smart cane. The cane is a mobile platform that aids the visually impaired persons to navigate through obstacle environments. It consists of two independently wheels that can be steered to follow the control commands. The intelligent shared control system consists of three basic control modules and a decision-maker to select which action to be applied. The first basic module is a continuous fuzzy controller to travel the cane from a starting point to end point (goal seeking). The second basic one is a discrete event controller to avoid obstacles while walking {{based on the measured}} data from the sensors (obstacle avoidance). The third basic module is the <b>discrete</b> <b>commands</b> from the user (human intervention), who provides the commands: Go Straight, Turn Left, Turn Right and Stop. The human interacts with cane through a small joystick, which can be pushed in any of the above four directions. All these modules are shared through a decision-maker module to select one action only at any instant of real time. The complete dynamic system is simulated and verified using data measured from the sensors in MATLAB/SIMULINK environment. The obtained results affirmed the potential of the proposed control approach...|$|E
40|$|We {{propose a}} robust {{estimation}} algorithm for tracking {{the location and}} dynamic motion of a mobile unit in a cellular network. The underlying mobility model is a dynamic linear system driven by a <b>discrete</b> <b>command</b> process that determines the mobile unit's acceleration. The command process is modeled as a semi-Markov process over a finite set of acceleration levels. Previous approaches to mobility estimation based on this model have used a modified Kalman filter with a Bayesian estimator for the command process. However, the Bayesian estimator can lead to inaccuracies in estimating the command process from pilot signal strengths, which can result in significant errors in tracking the mobile trajectory. Our proposed tracking algorithm combines the modified Kalman filter with an e#cient hidden semi-Markov model (HSMM) estimation algorithm to estimate {{the parameters of the}} command process. Numerical results show that the proposed tracking algorithm performs accurately over a wide range of mobility parameter values...|$|R
40|$|In {{an ad hoc}} {{wireless}} network, connectivity {{is determined}} by the physical locations and transmission ranges of the mobile units. In e#ect, user mobility causes the topology of an ad hoc network to change dynamically over time, which complicates the important tasks of routing and flow control. In this paper, we propose a novel scheme for tracking the mobility of users in a wireless ad hoc network. Mobile nodes track their positions using pilot signal strength measurements from neighboring nodes. Node mobility is modeled as a linear system driven by a <b>discrete</b> <b>command</b> semi-Markov process. Mobility tracking is performed using an extended Kalman filter preceded by an e#cient averaging filter. We also suggest the schemes to develop a local coordinate system for ad hoc networks using relative distances between the mobile nodes. Di#erent local systems can be related to each other to form a network coordinate system. The mobility tracking model can be used in developing improved algorithms for adaptive cluster-based routing algorithms. Our numerical results show that the mobility tracking scheme performs e#ectively and can enhance routing performance in ad hoc networks...|$|R
40|$|This paper {{introduces}} a master-slave control method of teleoperating a redundant manipulator with double handles. The master handles send motion commands {{in the form}} of increments. The mapping module transforms the commands into homogeneous matrices. And the slave manipulator links <b>discrete</b> motion <b>commands</b> in the mode of PVAT automatically, by inverse kinematics and fifth-order polynomial interpolation. Simulations and experiments are taken to prove the effectiveness of the control method in the paper...|$|R
40|$|Mission Planning {{systems have}} {{traditionally}} {{focused on the}} generation of a simple queue of time-tagged telecommands. As on-board autonomy and use of ground automation increases, schedules are increasingly complex and distributed. Schedules initiate procedures and software functions as well as <b>discrete</b> <b>commands</b> {{and in response to}} orbital position or event as well as time. From an operational perspective it is important that distributed schedules are consistent and coordinated, and that there is an integrated view of the current status of operations that synthesizes the latest available information relating to all executing schedules. A layered model for Mission Planning and Schedule Execution is proposed that is capable of supporting distributed schedules and integration with automation. The focus is on representing plans and schedules as a set of abstract items describing predicted events, spacecraft contacts, planned operations, schedulable activities and the constraints between them. Operations are tagged by a triggering event, which has an abstract ID but may also be elaborated to absolute or predicted position and time. The definition of a common model of the schedule and its dynamic update allows for development of mission independent schedule displays and schedule history. The model is layered to ensure that planning is performed on a generic view of an integrated schedule, with translation of the schedule to mission specific execution formats encapsulated at the lowest layer of integration with the spacecraft and mission control system. I...|$|E
40|$|Most {{unmanned}} {{missions in}} space and undersea are commanded by a "script" that specifies a sequence of <b>discrete</b> <b>commands</b> and continuous actions. Currently such scripts are mostly hand-generated by human operators. This introduces inefficiency, puts a significant cognitive burden on the engineers, and prevents re-planning in response to environment disturbances or plan execution failure. For discrete systems, the field of autonomy has elevated the level of commanding by developing goal-directed systems, to which human operators specify a series of temporally extended goals to be accomplished, and the goal-directed systems automatically output the correct, executable command sequences. Increasingly, the control of autonomous systems involves performing actions {{with a mix of}} discrete and continuous effects. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and take sample, and continuous actions, like descend and ascend, which are influenced by the dynamical model of the vehicle. A hybrid planner generates a sequence of discrete and continuous actions that achieve the mission goals. In this thesis, I present a novel approach to solve the generative planning problem for temporally extended goals for hybrid systems, involving both continuous and discrete actions. The planner, Kongming, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it engages novel reformulation schemes to handle temporally flexible actions and temporally extended goals. I have successfully demonstrated controlling an AUV in the Atlantic ocean using mission scripts solely generated by Kongming. I have also empirically evaluated Kongming on various real-world scenarios in the underwater domain and the air vehicle domain, and found it successfully and efficiently generates valid and optimal plans. by Hui X. Li. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2010. Cataloged from PDF version of thesis. Includes bibliographical references (p. 230 - 237) ...|$|E
40|$|Agile {{autonomous}} {{systems are}} emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness, {{in the context}} of space probes and planetary rovers, modeled as discrete systems. We address the challenge of extending plan execution to systems with continuous dynamics, such as air vehicles and robot manipulators, and that are controlled indirectly through the setting of continuous state variables. Systems with continuous dynamics are more challenging than discrete systems, because they require continuous, low-level control, and cannot be controlled by issuing simple sequences of <b>discrete</b> <b>commands.</b> Hence, manually controlling these systems (or plants) at a low level can become very costly, {{in terms of the number}} of human operators necessary to operate the plant. For example, in the case of a fleet of UAVs performing a search-and-rescue scenario, the traditional approach to controlling the UAVs involves providing series of close waypoints for each aircraft, which incurs a high workload for the human operators, when the fleet consists of a large number of vehicles. (cont.) Our solution is a novel, model-based executive, called Sulu, that takes as input a qualitative state plan, specifying the desired evolution of the state of the system. This approach elevates the interaction between the human operator and the plant, to a more abstract level where the operator is able to "coach" the plant by qualitatively specifying the tasks, or activities, the plant must perform. These activities are described in a qualitative manner, because they specify regions in the plant's state space in which the plant must be at a certain point in time. Time constraints are also described qualitatively, in the form of flexible temporal constraints between activities in the state plan. The design of low-level control inputs in order to meet this abstract goal specification is then delegated to the autonomous controller, hence decreasing the workload per human operator. This approach also provides robustness to the executive, by giving it room to adapt to disturbances and unforeseen events, while satisfying the qualitative constraints on the plant state, specified in the qualitative state plan. Sulu reasons on a model of the plant in order to dynamically generate near-optimal control sequences to fulfill the qualitative state plan. To achieve optimality and safety, Sulu plans into the future, framing the problem as a disjunctive linear programming problem. (cont.) To achieve robustness to disturbances and maintain tractability, planning is folded within a receding horizon, continuous planning and execution framework. The key to performance is a problem reduction method based on constraint pruning. We benchmark performance using multi-UAV firefighting scenarios on a real-time, hardware-in-the-loop testbed. by Thomas Léauté. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2005. Includes bibliographical references (p. 149 - 155) ...|$|E
40|$|A {{new method}} of robust, {{autonomous}} real-time {{diagnosis of a}} time-varying complex system (e. g., a spacecraft, an advanced aircraft, or a process-control system) is presented here. It {{is based upon the}} characterization and comparison of (1) the execution of software, as reported by discrete data, and (2) data from sensors that monitor the physical state of the system, such as performance sensors or similar quantitative time-varying measurements. By taking account of the relationship between execution of, and the responses to, software commands, this method satisfies a key requirement for robust autonomous diagnosis, namely, ensuring that control is maintained and followed. Such monitoring of control software requires that estimates {{of the state of the}} system, as represented within the control software itself, are representative of the physical behavior of the system. In this method, data from sensors and <b>discrete</b> <b>command</b> data are analyzed simultaneously and compared to determine their correlation. If the sensed physical state of the system differs from the software estimate (see figure) or if the system fails to perform a transition as commanded by software, or such a transition occurs without the associated command, the system has experienced a control fault. This method provides a means of detecting such divergent behavior and automatically generating an appropriate warning...|$|R
40|$|I present {{research}} (an experiment {{and the resulting}} prototype) on a new method for interacting with virtual environments. This method involves the capture and use of natural empty-hand gestures. Users are allowed to gesture normal continuous manner, rather than being restricted to a small set of <b>discrete</b> gestural <b>commands.</b> Their gestures are captured and analyzed into a higherlevel description. This description {{can be used by}} an application-specific interpreter to under-stand the gestural input in its proper context. Having a gesture analyzer of this sort enables natural gesture input to any appropriate application...|$|R
40|$|We {{describe}} the technical details behind a novel voicebased human-computer interface designed to enable individuals with motor impairments to use vocal parameters for both discrete and continuous control tasks. Since <b>discrete</b> spoken <b>commands</b> are not {{ideally suited to}} such tasks, our methodology exploits a large set of continuous acoustic-phonetic parameters like pitch, loudness, vowel quality, etc. Their selection is optimized with respect to automatic recognizability, communication bandwidth, learnability, suitability, and ease of use. These parameters are extracted continually in real time, transformed via adaptation and acceleration, and converted into continuous control signals. ...|$|R
40|$|We {{present a}} novel voice-based humancomputer {{interface}} designed to enable individuals with motor impairments to use vocal parameters for continuous control tasks. Since <b>discrete</b> spoken <b>commands</b> are ill-suited to such tasks, our interface exploits a large set of continuous acousticphonetic parameters like pitch, loudness, vowel quality, etc. Their selection is optimized {{with respect to}} automatic recognizability, communication bandwidth, learnability, suitability, and ease of use. Parameters are extracted in real time, transformed via adaptation and acceleration, and converted into continuous control signals. This paper describes the basic engine, prototype applications (in particular, voice-based web browsing and a controlled trajectory-following task), and initial user studies confirming the feasibility of this technology. ...|$|R
40|$|We will {{demonstrate}} a novel voice-based human-computer interface {{we call the}} Vocal Joystick (VJ), designed to enable individuals with motor impairments to use vocal parameters for both discrete and continuous control tasks. Since <b>discrete</b> spoken <b>commands</b> are not ideally suited to such tasks, our methodology exploits a large set of continuous acoustic-phonetic parameters like pitch, loudness, vowel quality, etc. Their selection is optimized with respect to automatic recognizability, communication bandwidth, learnability, suitability, and ease of use. These parameters are extracted continually in real time, transformed via adaptation and acceleration, and converted into continuous control signals. Our UIST’ 2005 demo will allow the user to try out the vocal joystick on arbitrary web-browsing tasks, VJ-enable...|$|R
40|$|To {{generate}} an action sequence, {{it is essential}} to inte-grate information regarding all temporally coordinating motor elements. Spatial information about both immedi-ate and subsequent component reaches is encoded simultaneously in the parietal reach region (PRR) [1]. However, it is still unclear how a cognitive sequence conveying multiple goals in parallel is discomposed into series of <b>discrete</b> movement <b>commands</b> to be executed by the musculoskeletal system. In the present study, we recorded single-neuron activity from dorsal area 5 (area 5 d) in the posterior parietal cortex while monkeys performed a memory-guided double-reach task. Briefly, the monkey was required to touch a fixation center at the trial beginning. Then, the first and second goals were simultaneously displayed for 400 ms with...|$|R
40|$|This article {{presents}} research—an experiment {{and the resulting}} prototype—on a method for treating gestural input {{so that it can}} be used for multimodal applications, such as interacting with virtual environments. This method involves the capture and use of natural, empty-hand gestures that are made during conventional descriptive utterances. Users are allowed to gesture in a normal continuous manner, rather than being restricted to a small set of <b>discrete</b> gestural <b>commands</b> as in most other systems. The gestures are captured and analyzed into a higher-level description. This description can be used by an application-specific interpreter to understand the gestural input in its proper context. Having a gesture analyzer of this sort enables natural gesture input to any appropriate application...|$|R
40|$|Abstract — An {{increasing}} {{requirement for}} satellites, space probes and (unmanned) aircraft {{is that they}} exhibit robust behaviour without direct human intervention. Autonomous oper-ation is required in spite of incomplete knowledge of an uncertain environment. In particular, embedded equipment that processes sensing data must consider uncertain input parameters while managing its own activities. We show how uncertainty may be addressed in constraint-based planning and scheduling functions for aerospace equipment, contrasting with some current practice in Integrated Modular Avionic (IMA) design. We produce a conditional plan that takes account of foreseeable contingencies, so guaranteeing system behaviour in the worst case. Executing {{a branch of the}} plan corresponds to synthesising a deterministic finite state automaton capable of <b>discrete</b> event <b>commanding</b> of an avionic sub-system. Experimental results show the feasibility of the approach for realistic aerospace equipment. I...|$|R
40|$|This paper {{considers}} {{power distribution}} grids with {{distributed energy resources}} (DERs) and flexible controllable loads, and designs an incentive-based algorithm to coordinate the network operator and customers to pursue personal and system-wide operational and economic objectives. Heterogeneous DERs and controllable loads with continuous and <b>discrete</b> control <b>commands</b> and various dynamics are considered. We first relax the discrete feasible sets into their convex hull and formulate a well-defined social-welfare optimization problem with general objective functions and constraints. We then propose an offline distributed stochastic dual algorithm for solving the relaxed problem while recovering original discrete set points at every iteration. We further implement our design in an online setting with time-varying optimal operation points, non-linear power flow equations and device dynamics, and asynchronous updates from devices. Stability and performance characterization of both offline and online algorithms are analytically established and numerically corroborated...|$|R
