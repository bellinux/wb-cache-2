10000|10000|Public
5|$|The {{simplest}} {{method of}} <b>forecasting</b> the weather, persistence, relies upon today's conditions to forecast the conditions tomorrow. This {{can be a}} valid way of <b>forecasting</b> the weather when {{it is in a}} steady state, such as during the summer season in the tropics. This method of <b>forecasting</b> strongly depends upon the presence of a stagnant weather pattern. Therefore, when in a fluctuating weather pattern, this method of <b>forecasting</b> becomes inaccurate. It can be useful in both short range forecasts and long range forecasts.|$|E
5|$|The {{launch of}} the first weather satellite, TIROS-I, in 1960, {{introduced}} new <b>forecasting</b> techniques that remain important to tropical cyclone <b>forecasting</b> to the present. In the 1970s, buoys were introduced to improve the resolution of surface measurements, which until that point, were not available at all over sea surfaces.|$|E
5|$|The two men {{credited}} {{with the birth of}} <b>forecasting</b> as a science were officer of the Royal Navy Francis Beaufort and his protégé Robert FitzRoy. Both were influential men in British naval and governmental circles, and though ridiculed in the press at the time, their work gained scientific credence, was accepted by the Royal Navy, and formed the basis for all of today's weather <b>forecasting</b> knowledge.|$|E
40|$|Inquiries {{about this}} {{document}} {{can be made}} to HARC@hawaii. edu Range <b>forecasts</b> have emerged as the predominant form of management <b>forecasts,</b> but the existing literature fails to capture the information conveyed by these <b>forecast</b> ranges when compared to the range of individual analyst <b>forecasts.</b> This study fills this gap by examining the implications of overlap between management <b>forecasts</b> range and the range of analyst <b>forecasts.</b> Controlling for management <b>forecast</b> width and analyst <b>forecast</b> dispersion, we find stronger market reactions to non-overlapping management <b>forecasts</b> (to each unit of <b>forecast</b> news) than to overlapping <b>forecasts.</b> Using a matched sample, we demonstrate that this effect is distinct from known effects of <b>forecast</b> precision and <b>forecast</b> news magnitude on market reactions. Moreover, we find that, compared with overlapping <b>forecasts,</b> non-overlapping <b>forecasts</b> are more accurate relative to analyst <b>forecasts</b> and result in stronger responses by analysts whose prior <b>forecasts</b> are outside of management <b>forecast</b> ranges. Together our findings suggest that investors and analysts view managers’ non-overlapping <b>forecasts</b> as signals of superior private information...|$|R
40|$|It {{is obvious}} that <b>forecasts</b> are of great {{importance}} and widely used in economics and finance. Quite simply, good <b>forecasts</b> lead to good decisions. The importance of <b>forecast</b> evaluation and combination techniques follows immediately [...] <b>forecast</b> users naturally have {{a keen interest in}} monitoring and improving <b>forecast</b> performance. More generally, <b>forecast</b> evaluation figures prominently in many questions in empirical economics and finance. We provide selective account of <b>forecast</b> evaluation and combination methods. First we discuss evaluation of a single <b>forecast,</b> and in particular, evaluation of whether and how it may be improved. Second, we discuss the evaluation and comparison of the accuracy of competing <b>forecasts.</b> Third, we discuss whether and how a set of <b>forecasts</b> may be combined to produce a superior composite <b>forecast.</b> Fourth, we describe a number of <b>forecast</b> evaluation topics of particular relevance in economics and finance, including methods for evaluating direction-of-change <b>forecasts,</b> probability <b>forecasts</b> and volatility <b>forecasts.</b> ...|$|R
5000|$|Art Porter - Pocket City, (Verve <b>Forecast)</b> Straight to the Point, (Verve <b>Forecast)</b> Undercover, (Verve <b>Forecast)</b> Lay Your Hands on Me, (Verve <b>Forecast)</b> For Art's Sake, (Verve <b>Forecast)</b> ...|$|R
5|$|Weather <b>forecasting</b> of wind, {{precipitations}} {{and humidity}} {{is essential for}} preventing and controlling wildfires. Different indices, like the Forest fire weather index and the Haines Index, {{have been developed to}} predict the areas more at risk to experience fire from natural or human causes. Conditions for the development of harmful insects can be predicted by <b>forecasting</b> the evolution of weather, too.|$|E
5|$|A weather ship, or Ocean Station Vessel, was a ship {{stationed}} in the ocean as a platform for surface and upper air meteorological observations for use in weather <b>forecasting.</b> They were primarily located in the north Atlantic and north Pacific oceans, reporting via radio. In addition to their weather reporting function, these vessels aided in search and rescue operations, supported transatlantic flights, acted as research platforms for oceanographers, monitored marine pollution, and aided weather <b>forecasting</b> both by weather forecasters and within computerized atmospheric models. Research vessels remain heavily used in oceanography, including physical oceanography and the integration of meteorological and climatological data in Earth system science.|$|E
5|$|Animations may {{depict a}} time series of station model {{conditions}} {{which is most}} often used to show recent changes in weather conditions and are useful in nowcasting and <b>forecasting.</b>|$|E
40|$|This article {{introduces}} {{the concept of}} <b>forecast</b> efficiency, in which the <b>forecast</b> contains all information {{available at the time}} of the <b>forecast.</b> Empirical tests investigate weak efficiency, where the information set is all past <b>forecasts</b> and where all <b>forecast</b> revisions and errors should be uncorrelated with past <b>forecast</b> revisions. Tests of macroeconomic, energy-consumption, and oil-price <b>forecasts</b> find a significant autocorrelation of <b>forecast</b> revisions, with fifty of fifty-one tests showing positive correlation of <b>forecast</b> revisions, as opposed to zero correlation consistent with <b>forecast</b> efficiency. Copyright 1987 by MIT Press. ...|$|R
40|$|Experts {{can rely}} on {{statistical}} model <b>forecasts</b> when creating their own <b>forecasts.</b> Usually {{it is not known}} what experts actually do. In this paper we focus on three questions, which we try to answer given the availability of expert <b>forecasts</b> and model <b>forecasts.</b> First, is the expert <b>forecast</b> related to the model <b>forecast</b> and how? Second, how is this potential relation influenced by other factors? Third, how does this relation influence <b>forecast</b> accuracy? We propose a new and innovative two-level Hierarchical Bayes model to answer these questions. We apply our proposed methodology to a large data set of <b>forecasts</b> and realizations of SKU-level sales data from a pharmaceutical company. We find that expert <b>forecasts</b> can depend on model <b>forecasts</b> in a variety of ways. Average sales levels, sales volatility, and the <b>forecast</b> horizon influence this dependence. We also demonstrate that theoretical implications of expert behavior on <b>forecast</b> accuracy are reflected in the empirical data. model <b>forecasts,</b> expert <b>forecasts,</b> <b>forecast</b> adjustment, Bayesian analysis, endogeneity...|$|R
40|$|We {{consider}} {{wind power}} <b>forecasts</b> {{based on a}} number of different meteorological <b>forecasts</b> originating from three different global meteorological models. Wind power <b>forecasts</b> based on these meteorological <b>forecasts</b> have fairly similar performance. However, in the paper we show that the wind power <b>forecast</b> errors are relatively uncorrelated. For this reason we can combine the <b>forecasts</b> and obtain a final <b>forecast</b> which performs better than any of the individual <b>forecasts.</b> Optimal weights are found based on the bias of the individual <b>forecasts</b> and the variance-covariance matrix of the individual <b>forecast</b> errors. In the paper we show that quite significant improvements can be obtained using only a few different meteorological <b>forecasts.</b> ∗Corresponding author...|$|R
5|$|Climate <b>forecasting</b> {{is a way}} by some {{scientists}} are using to predict climate change. In 1997 the prediction division of the International Research Institute for Climate and Society at Columbia University began generating seasonal climate forecasts on a real-time basis. To produce these forecasts an extensive suite of <b>forecasting</b> tools was developed, including a multimodel ensemble approach that required thorough validation of each model's accuracy level in simulating interannual climate variability.|$|E
5|$|Corfidi, S. F., 1998: <b>Forecasting</b> MCS {{mode and}} motion. Preprints 19th Conf. on Severe Local Storms, American Meteorological Society, Minneapolis, Minnesota, pp.626–629.|$|E
5|$|In 904AD, Ibn Wahshiyya's Nabatean Agriculture {{discussed}} the weather <b>forecasting</b> of atmospheric changes and signs from the planetary astral alterations; signs of rain based on {{observation of the}} lunar phases; and weather forecasts based on the movement of winds.|$|E
40|$|<b>Forecasts</b> {{are often}} {{influential}} because a low <b>forecast</b> may cause a firm not {{to launch a}} new product so that actual sales are never observed. This paper considers a dilemma we face as influential forecasters. Our client requests an unbiased <b>forecast</b> but pressures sometimes exist to provide a bias <b>forecast.</b> From theoretical and empirical perspectives, we discuss {{the impact of these}} pressures on the quality of <b>forecasts.</b> We find that: Non-influential <b>forecasts,</b> generally, create no pressure for statistically biased <b>forecasts.</b> As influence increases, the pressures increase. When our <b>forecasts</b> eliminate alternatives, (e. g., product designs, advertising campaigns), not all <b>forecasts</b> are tested. Not validating all <b>forecasts</b> causes two effects: Survivor's Curse and Prophet's Fear. Survivor's Curse makes statistically unbiased <b>forecasts</b> appear optimistic (i. e., overestimate actual sales) because, often, only optimistic <b>forecasts</b> are tested. <b>Forecasts</b> appearing statistically unbiased or pessimistic might cause concern. Perhaps, some failures are justified. Prophet's Fear encourages pessimistic <b>forecasts</b> because these <b>forecasts</b> cause hidden opportunity losses while optimistic <b>forecasts</b> cause observable actual losses. Tested <b>forecasts</b> may appear completely unbiased despite a pessimistic pre-launch bias. Although no perfect solution exists, clients may lessen bias with experimentation and by seeking more accurate <b>forecasts.</b> Forecasters may lessen bias with <b>forecasts</b> conditioned on launching and by seeking more accurate <b>forecasts.</b> Reproduced with permission of the copyright owner. Further reproduction prohibited without permission...|$|R
40|$|The Rice Outlook and Situation (RO&S) <b>forecasts</b> were {{compared}} to the <b>forecasts</b> of a univariate Box-Jenkins (BJ) model. On balance, the RO&S <b>forecasts</b> had lower mean square <b>forecast</b> errors and lower mean absolute <b>forecast</b> errors than the BJ model <b>forecasts.</b> The differences in the squared and absolute <b>forecast</b> errors were not significant, however. Based {{on the concept of}} conditional efficiency as set forth by Granger and Newbold, {{it was found that the}} BJ <b>forecasts</b> did not add any information that might improve <b>forecast</b> accuracy beyond what was already incorporated in the RO&S <b>forecasts.</b> Demand and Price Analysis,...|$|R
40|$|The idea of {{combining}} <b>forecasts</b> {{is of great}} interest to forecasters and a linear combination of different <b>forecasts</b> can be more accurate than any individual <b>forecast.</b> For model builders, <b>forecast</b> encompassing is a way of checking whether any extra important information is contained in <b>forecasts</b> from rival models. Efficient <b>forecasts</b> have errors which are unrelated to any information available when they are formed. Combinations of <b>forecasts,</b> <b>forecast</b> encompassing and efficiency tests can all be achieved by a restricted or unrestricted regression of outcomes on the separate <b>forecasts.</b> This paper links these three approaches and examines the implications for recent UK annual <b>forecasts.</b> ...|$|R
5|$|In June, Canadian Prime Minister Jean Chrétien and President Clinton {{appointed}} the International Red River Basin Task Force containing {{members of both}} countries. The task force's purpose was {{to find ways to}} improve flood <b>forecasting.</b>|$|E
5|$|Most <b>forecasting</b> groups {{predicted}} {{above average}} activity {{in anticipation of}} a dissipating El Niño event {{and the development of a}} La Niña, as well as warmer than normal sea surface temperatures. Overall, the forecasts were fairly accurate.|$|E
5|$|Most major <b>forecasting</b> {{agencies}} predicted below-average {{activity to}} occur this season {{due to an}} expected strong El Niño; the El Nino failed to materialize, though unfavorable conditions still became established across the basin.|$|E
40|$|Based on a {{valuable}} testing venue in China where listed companies {{are required to}} disclose corporate site visit records of financial analyst to the public, this study examines whether analysts will learn from visiting analysts 2 ̆ 7 <b>forecasts</b> that contain superior private information. I find that visiting <b>forecasts</b> tend to attract more analysts to issue <b>forecasts</b> in their aftermath than the prior <b>forecasts</b> issued by the same analysts but without conducting corporate site visit (non-visiting <b>forecasts).</b> The following effect is weaker when the visiting <b>forecasts</b> are more informative. In addition, other analysts’ <b>forecasts</b> following the visiting <b>forecasts</b> tend to move closer to the visiting <b>forecasts</b> than the <b>forecasts</b> following the non-visiting <b>forecasts,</b> with the effects being stronger for more informative visiting <b>forecasts.</b> Furthermore, followers experience a greater improvement in their <b>forecast</b> accuracy than the non-followers. This effect is also stronger when the visiting <b>forecasts</b> are more informative. Last but not the least, I find a decline in analyst <b>forecast</b> dispersion, an increase in common information, and an improvement in <b>forecast</b> accuracy in the period subsequent to the issuance of visiting analysts’ <b>forecasts</b> but no such effect for non-visiting <b>forecasts.</b> Collectively, {{the results suggest that}} analysts have incentive to learn from the <b>forecasts</b> that contain superior information and such learning activities tend to improve the information environment of the visiting firms...|$|R
40|$|Learning {{objectives}} As {{you work}} through this session you will: Become {{familiar with the}} multi-faceted nature of <b>forecast</b> quality Learn the difference between <b>forecast</b> “quality ” and <b>forecast</b> “value”, {{in addition to other}} terms in <b>forecast</b> verification Understand common biases in estimates of <b>forecast</b> quality Become familiar with aspects of communicating <b>forecast</b> confidence: What do (or should) <b>forecast</b> probabilities convey...|$|R
40|$|I {{investigate}} {{the way in}} which Australian managers issue their earnings <b>forecasts,</b> and the impact this has on the reaction of equity investors and security analysts. Using a sample of 233 management earnings <b>forecasts</b> issued from 1994 to 2001, I find that managers are more likely to issue earnings <b>forecasts</b> when they have bad earnings news than good earnings news. I find that a vast majority of <b>forecasts</b> are framed by the use of an accompanying earnings benchmark. <b>Forecasts</b> are issued with varying degrees of specificity (or precision) and also with variation in additional accompanying disclosures. <b>Forecasts</b> issued with negative framing (<b>forecast</b> earnings less than benchmark earnings) {{are more likely to be}} accompanied by statements about factors external to the firm in explaining performance, while <b>forecasts</b> issued with positive framing (<b>forecast</b> earnings greater than benchmark earnings) are more likely to be accompanied by additional verifiable <b>forecasts</b> of components of earnings. I find the market reaction to earnings <b>forecasts</b> released with positive framing is higher than for <b>forecasts</b> released with negative framing, after controlling for <b>forecast</b> news and other <b>forecast</b> properties. I also examine security analysts <b>forecasts</b> around the release of management earnings <b>forecasts</b> and find that after the release of a management earnings <b>forecast,</b> analyst activity increases, but that analysts <b>forecasts</b> become less accurate and more biased. Neither the extent of analyst activity nor changes in analysts <b>forecast</b> accuracy or bias is related to <b>forecast</b> framing...|$|R
5|$|The analog {{technique}} {{is a complex}} way of making a forecast, requiring the forecaster to remember a previous weather event {{that is expected to}} be mimicked by an upcoming event. What makes it a difficult technique to use is that there is rarely a perfect analog for an event in the future. Some call this type of <b>forecasting</b> pattern recognition. It remains a useful method of observing rainfall over data voids such as oceans, as well as the <b>forecasting</b> of precipitation amounts and distribution in the future. A similar {{technique is}} used in medium range <b>forecasting,</b> which is known as teleconnections, when systems in other locations are used to help pin down the location of another system within the surrounding regime. An example of teleconnections are by using El Niño-Southern Oscillation (ENSO) related phenomena.|$|E
5|$|In August 2016, IBM {{announced}} it would be using Watson for weather <b>forecasting.</b> Specifically, the company announced they would use Watson to analyze data from over 200,000 Weather Underground personal weather stations, and data from other sources, {{as a part of}} project Deep Thunder.|$|E
5|$|The National Weather Service {{has since}} revised their method of <b>forecasting</b> spring floods. Increased {{technology}} and funding has {{allowed for the}} mapping of the entire Red River bottom, more flood monitoring gauges, and up-to-date satellite images of the river at various flood stages.|$|E
40|$|We propose an easy direct {{regression}} test to evaluate <b>forecasts</b> {{with respect to}} the naïve most recent observation alternative. The test is applied to thirteen macroeconomic <b>forecasts</b> published by the OECD. Four time horizons are considered: starting with eighteen months and declining to zero for the December current year <b>forecast.</b> Our results show that the longest horizon <b>forecasts</b> are essentially worthless. Unfortunately, the naïve <b>forecasts</b> do not have any value either. In general, naïve <b>forecasts</b> are inferior to OECD <b>forecasts,</b> but frequently the current year <b>forecasts</b> compare in quality to the one-year-ahead <b>forecast.</b> info:eu-repo/semantics/publishe...|$|R
40|$|Iterated’ multiperiod ahead {{time series}} <b>forecasts</b> are made using a one-period ahead model, {{iterated}} {{forward for the}} desired number of periods, whereas ‘direct’ <b>forecasts</b> are made using a horizon-specific estimated model, where the dependent variable is the multi-period ahead value being <b>forecasted.</b> Which approach is better is an empirical matter: in theory, iterated <b>forecasts</b> are more efficient if correctly specified, but direct <b>forecasts</b> are more robust to model misspecification. This paper compares empirical iterated and direct <b>forecasts</b> from linear univariate and bivariate models by applying simulated out-of-sample methods to 171 US monthly macroeconomic time series spanning 1959 - 2002. The iterated <b>forecasts</b> typically outperform the direct <b>forecasts,</b> particularly if the models can select long lag specifications. The relative performance of the iterated <b>forecasts</b> improves with the <b>forecast</b> horizon. <b>forecast</b> comparisons; multistep forecasts; VAR <b>forecasts...</b>|$|R
40|$|<b>Forecasts</b> for peak streamflows from annual {{snow melt}} {{are widely used}} by {{emergency}} management, reservoir operators, and river recreationists in the Colorado and Great Basins. The Colorado Basin River <b>Forecast</b> Center (CBRFC) has produced these <b>forecasts</b> {{for many years in}} the spring {{months leading up to the}} snowmelt. However, only recently has a systematic effort to verify these <b>forecasts</b> been undertaken. Historical <b>forecasts</b> were verified along with reforecasts from the River <b>Forecast</b> System to produce a suite of verification statistics for current <b>forecast</b> points. Preliminary results, although site specific, suggest that both <b>forecasts</b> and reforecast have better skill that climatology and that skill improves with lead time. The resulting analysis will be used to drive changes to the peak flow <b>forecast</b> program at the CBRFC. For example, observed <b>forecast</b> skill may be used together with stakeholder requirements to determine <b>forecast</b> issuance dates and <b>forecast</b> frequency. Results may also be used to validate new and existing <b>forecast</b> procedures, evaluate the validity of current and future <b>forecast</b> points, and provide future tools to stakeholders to evaluate <b>forecast</b> performance...|$|R
5|$|The Quantitative Precipitation Forecast (abbreviated QPF) is the {{expected}} amount of liquid precipitation accumulated over a specified time period over a specified area. A QPF will be specified when a measurable precipitation type reaching a minimum threshold is forecast for any hour during a QPF valid period. Precipitation forecasts {{tend to be}} bound by synoptic hours such as 0000, 0600, 1200 and 1800GMT. Terrain is considered in QPFs by use of topography or based upon climatological precipitation patterns from observations with fine detail. Starting {{in the mid to}} late 1990s, QPFs were used within hydrologic forecast models to simulate impact to rivers throughout the United States. Forecast models show significant sensitivity to humidity levels within the planetary boundary layer, or in the lowest levels of the atmosphere, which decreases with height. QPF can be generated on a quantitative, <b>forecasting</b> amounts, or a qualitative, <b>forecasting</b> the probability of a specific amount, basis. Radar imagery <b>forecasting</b> techniques show higher skill than model forecasts within 6 to 7hours of the time of the radar image. The forecasts can be verified through use of rain gauge measurements, weather radar estimates, or a combination of both. Various skill scores can be determined to measure the value of the rainfall forecast.|$|E
5|$|The {{agency is}} also {{responsible}} for <b>forecasting</b> fire weather (indicating conditions that are favorable for wildfires) in the contiguous U.S., issuing fire weather outlooks for Days1, 2, and 3–8, which detail areas with various levels of risk for fire conditions.|$|E
5|$|With the {{exception}} of Tropical Storm Risk's initial season prediction in December2014, all major <b>forecasting</b> agencies called for a below or near-average season, due to the strong El Niño and colder than average sea surface temperatures. Overall, the forecasts were fairly accurate.|$|E
40|$|This study {{examines}} whether the Taiwanese regulation requiring disclosure of earnings <b>forecasts</b> in the IPOs resulted in disclosure of more optimistic earnings <b>forecasts</b> {{and whether the}} <b>forecast</b> error was reduced more by manipulating the reported earnings rather than revising the earnings <b>forecasts</b> to meet the <b>forecast</b> error threshold. The study is based on 759 <b>forecasts</b> issued by the Taiwanese IPO firms from 1994 to 2001, i. e. 8 -year period after the regulation was modified to increase the <b>forecast</b> error threshold to 20 %. The findings show that the disclosure regulation resulted in more optimistic <b>forecasts</b> than conservative <b>forecasts,</b> especially for firms expecting better performance in the <b>forecast</b> year compared to the previous year. Firms disclosing optimistic earnings <b>forecasts</b> engaged more in manipulation of reported earnings than revision of <b>forecasts</b> to meet the <b>forecast</b> error threshold. These findings thus suggest that the disclosure regulation resulted in earnings manipulation, which reduced the quality of reported earnings. Copyright Springer Science + Business Media, Inc. 2006 Earnings management, Disclosure regulation, Discretionary accruals, Mandatory earnings <b>forecasts,</b> <b>Forecast</b> error threshold,...|$|R
40|$|The {{effectively}} {{mandatory provision}} of management <b>forecasts</b> of earnings {{is a unique}} feature of Japan's financial disclosure system. The first objective {{of this study is}} to identify the determinants of systematic bias in management <b>forecasts</b> using a sample of more than 36 000 one-year-ahead earnings <b>forecasts</b> announced by Japanese firms at the beginning of a fiscal year over the period 1979 to 2005. The examination of ex post management <b>forecast</b> errors shows that financial distress, firm growth, firm size and prior <b>forecast</b> errors are all associated with bias in management <b>forecasts.</b> The second objective {{of this study is to}} investigate whether analysts are aware of these factors that are found to be related to systematic bias in management earnings <b>forecasts.</b> The examination of analysts' <b>forecasts</b> issued subsequent to the announcement of management <b>forecasts</b> reveals that analysts take account of these factors when they issue their own earnings <b>forecasts.</b> The overall findings suggest that analysts are to some extent aware of the determinants of systematic bias in management <b>forecasts.</b> management earnings <b>forecasts,</b> analysts' earnings <b>forecasts,</b> determinants of <b>forecast</b> bias, <b>forecast</b> accuracy,...|$|R
40|$|One-year-ahead <b>forecasts</b> by the OECD and by {{national}} institutes of GDP growth and inflation in 13 European countries are analysed. RMSE was large: 1. 9 % for growth and 1. 6 % for inflation. Six (11) OECD and 10 (7) institute growth <b>forecast</b> records were significantly better than an average growth <b>forecast</b> (the current year <b>forecast).</b> All full record-length inflation <b>forecasts</b> were significantly better than both naive alternatives. There {{was no significant difference}} in accuracy between the <b>forecasts</b> of the OECD and the institutes. Two <b>forecasts</b> were found to be biased and one had autocorrelated errors. Directional <b>forecasts</b> were significantly better than a naive alternative in one-half of the cases. Overall, inflation <b>forecasts</b> were significantly more accurate than growth <b>forecasts,</b> and in contrast to growth <b>forecasts,</b> they generally improved over time. This has implications for economic policy. Positively biased revisions reveal large errors in data. <b>Forecast</b> accuracy; Directional errors; <b>Forecast</b> tests...|$|R
