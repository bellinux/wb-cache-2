4|78|Public
40|$|One of {{the most}} commented upon {{elements}} of the Trans-Pacific Partnership (TPP) is the inclusion of new rules around digital information flows and digital data. In particular, we have seen civil society and technology commentators criticising some of the rules within the agreement – on source code, <b>data</b> <b>localisation</b> and intermediaries – that they suggest will be detrimental to a secure, open and competitive digital sector...|$|E
40|$|Abstract – Currently {{industrial}} robotic systems employ {{almost exclusively}} global maps for navigation purposes, if any. Additional information – intra-process, spatial, current, and persistent sensor data – {{is useful to}} cope with uncertainty, mea-surement errors, and incompleteness of data. We propose to augment robot world models by using local sensors (which pro-vide data from a local ε-environment) and build precise maps from local sensors, with force and audio classification in ortho-pedics applications with a medical robot system (RONAF) as an example. Improving precision of this map-building is presented both for <b>data</b> <b>localisation</b> and data insertion...|$|E
40|$|Purpose 99 mTc-DI-DD 3 B 6 / 22 - 80 B 3 (ThromboView®, {{hereafter}} {{abbreviated to}} 99 mTc-DI- 80 B 3 Fab′) is a radiolabelled humanised monoclonal Fab′ fragment with affinity and specificity for D-dimer domains of cross-linked fibrin. Detection of thromboembolic events {{has been demonstrated}} in canine models. The study objectives were evaluation of safety and characterisation of biodistribution, immunogenicity and pharmacokinetic profile of increasing doses of 99 mTc-DI- 80 B 3 Fab′ in subjects with acute lower-limb DVT. Methods Twenty-six patients with acute lower limb DVT were enrolled. Of these, 21 received a single intravenous dose of 0. 5 mg (n[*]=[*] 6), 1. 0 mg (n[*]=[*] 9) or 2 mg (n[*]=[*] 6) 99 mTc-DI- 80 B 3 Fab′. Blood and urine samples and gamma camera images were collected to 24 h after administration for pharmacokinetic and dosimetry analysis. Vital signs, electrocardiography, hematological and biochemical data and human anti-human antibody (HAHA) levels were monitored for up to 30 days following administration. Patients were assigned to either planar or single photon emission computed tomographic (SPECT) imaging of the thorax at 4 h following injection. Results Thirty-five adverse events were reported in 15 of the 21 subjects. Those deemed possibly related to administration of 99 mTc-DI- 80 B 3 Fab′ included mild hypertension, mild elevation of LD (lactate dehydrogenase) and moderate elevation of ALT (alanine transaminase). HAHA assays remained negative. Pharmacokinetics and organ dosimetry were comparable to prior normal volunteer <b>data.</b> <b>Localisation</b> of Thromboview® to sites of known thrombus was evident as early as 30 min post-injection. Conclusions In subjects with acute DVT, 99 mTc-DI- 80 B 3 Fab′ was well tolerated with favourable characteristics {{for the detection of}} acute venous thrombosis...|$|E
40|$|Key {{issues in}} {{wireless}} sensor networks such as <b>data</b> aggregation, <b>localisation,</b> MAC protocols and routing, all {{have to do with}} communication at some level. At a low level, these are influenced by the link layer performance between two nodes. The lack of accurate sensor network specific radio models, and the limited experimental data on actual link behaviour, warrant additional investigation in this area...|$|R
30|$|As {{well as the}} {{precision}} and sampling limits of resolution, a separate constraint on the effective resolution of a localisation microscope can arise as Nyquist limitation due to fluorescent label density. Also, the effective localisation precision may be worsened by fluorophore width or motion blur correction. Spurious <b>localisation</b> <b>data</b> should be excluded by stringent quality control criteria, as it can otherwise generate artefacts {{that are hard to}} quantify.|$|R
40|$|Background: While {{considerable}} genomic and transcriptomic {{data are}} available for Schistosoma mansoni, many of its genes lack significant annotation. A transcriptomic study of individual tissues and organs of schistosomes could {{play an important role}} in functional annotation of the unknown genes, particularly by providing rapid <b>localisation</b> <b>data</b> and thus giving insight into the potential roles of these molecules in parasite development, reproduction and homeostasis, and in the complex host-parasite interaction...|$|R
40|$|The {{interface}} between trade and Internet governance {{is one of}} the most complex policy challenges in the current-day digital economy. This working paper highlights the following observations and findings on the delicate and complex relationship between international trade and the Internet: Recent preferential trade agreements (PTAs) such as the Trans-Pacific Partnership Agreement (TPP) and the Japan - Mongolia Economic Partnership Agreement (Japan - Mongolia FTA) contain legal provisions on cybersecurity, data protection, <b>data</b> <b>localisation,</b> consumer protection, net neutrality, spam control, and protection of online intellectual property, intended to facilitate electronic commerce and enable cross-border data flows. However, these provisions will also have a lasting impact on important aspects of Internet regulation. Similar provisions are likely to appear in other ongoing trade deals such as the Trade in Services Agreement (TISA) and the renegotiation of North American Free Trade Agreement (NAFTA). International trade law does not contain adequate tools to address all aspects of Internet data flows because: (a) Internet is not just an important platform for trade, but also a site for political, cultural and social engagement - the latter aspects largely relate to the domestic regulatory space of countries and fall outside the scope of international trade law; (b) trade lawyers and policy-makers have insufficient knowledge of the technical and policy aspects of the Internet; and (c) the ideological divide between countries on issues including online censorship and surveillance, cybersecurity and privacy (which deeply impact cross-border data flows) cannot be resolved through international trade agreements. Yet, many issues related to Internet policy are also central to trade in digital economy, and thereby, not entirely avoidable in international trade law. Internet openness, security and trust are fundamental to the governance of Internet data flows. Measures designed to enforce Internet security and Internet trust, when implemented in a well-reasoned and proportionate manner, do not act as impediments to Internet openness - to the contrary, these measures play an essential role in facilitating efficient and secure data flows through the Internet. Thus, issues of cybersecurity, privacy and data protection can not only act as barriers to electronic commerce, but also facilitate electronic commerce - this perspective necessitates a reorientation of legal provisions in trade agreements. International trade institutions should explore both formal and informal means to engage with the Internet policy community in course of dialogues and/or trade negotiations within the World Trade Organization (WTO) as well as bilateral and plurilateral trade agreements, and in multistakeholder platforms such as the Internet Governance Forum. Further, international trade tribunals can rely on the technical and policy expertise of the Internet community to resolve certain complex trade disputes in international trade law...|$|E
40|$|The XML Localisation Interchange File Format (XLIFF) is {{the main}} {{standard}} for the interchange of <b>localisation</b> <b>data</b> during the <b>localisation</b> process and {{the most popular and}} widely used in the industry. Computer Assisted Translation (CAT) tools already support its version 1. 2. However, the most important end users of the format, i. e. translators, still have limited or no knowledge about the standard and the possible advantages of its adoption (Anastasiou 2010). With a view to bridging this knowledge gap, we have been introducing XLIFF as a topic of study in the translation and localisation studies curricula {{for the last four years}} in four different European universities, both at undergraduate and postgraduate levels, thus satisfying one of the missions of the Promotion and Liaison OASIS XLIFF subcommittee. In this paper, we aim at sharing our experience in teaching XLIFF to translation and localisation students: the curriculum design, the topics covered, the practical exercises and the areas that we have improved and modified based on our experience over this period of time...|$|R
40|$|The {{invention}} {{relates to}} a method and a device {{for determining the}} position of defects or damage on rotor blades (4) of a wind turbine (1) in an installed state, comprising the steps: a. a localisation instrument (5) is guided along the rotor blade (4) and the defect or damage is detected; b. the localisation instrument (5) has a GPS module (6) by means of which the GPS <b>data</b> of the <b>localisation</b> instrument (5) is detected at the defect or damage; c. {{the position of the}} investigated wind turbine (1) is detected by means of the GPS module (6); d. using the position data of the wind turbine (1), the hub height (n) of the wind turbine (1) is retrieved from a database and the distance (d) of the defect or damage of the rotor blade (4) from the hub (34) is calculated in an evaluation unit (8) from the difference between the GPS <b>data</b> of the <b>localisation</b> instrument (5) and the hub height (n) of the wind turbine and in accordance with the rotor blade position...|$|R
3000|$|Compensating {{for this}} error {{requires}} {{the availability of}} sound velocity profile <b>data</b> during <b>localisation.</b> Figure 2 shows that in very shallow water environments with relatively benign velocity profiles and short ranges (i.e., under 4000 [*]m), the error caused by sound refraction is relatively small (i.e., 0.02 % of the range) compared to the ambiguity errors that are potentially introduced by {{a large percentage of}} missing pairwise distances (typically in the order of 10 to 20 % of range). Another common compensation approach is by using a set of available anchor points of which the precise coordinates, and hence shortest inter-node distances, are used to predict a best-fit velocity profile by matching against ranging measurements. These anchor points are also used for mapping the relative locations onto geo-coordinates by using m-axis vector translations along with [...]...|$|R
40|$|Key {{issues in}} {{wireless}} sensor networks such as <b>data</b> aggregation, <b>localisation,</b> MAC protocols and routing, all {{have to do with}} communication at some level. At a low level, these are influenced by the link layer performance between two nodes. The lack of accurate sensor network specific radio models, and the limited experimental data on actual link behaviour, warrant additional investigation in this area. In this paper we present the results from extensive experiments, exploring several factors that are relevant for the link layer performance. These include (i) the effect of interference from simultaneous transmissions, which has not been looked into before, (ii) the degree of symmetry in the links between nodes, and (iii) the use of calibrated RSSI measurements. Finally, we present some guidelines on how to use the results for effective protocol design. 1...|$|R
40|$|CHROMSCAN {{implements}} {{a composite}} likelihood {{model for the}} analysis of association <b>data.</b> Disease-gene <b>localisation</b> is on a linkage disequilibrium unit (LDU) map, and locations and standard errors, for putatively causal polymorphisms, are determined by the programme. Distortions of the probability distribution created by auto-correlation are avoided by implementation of a permutation test. We evaluated the relative efficiency of the LDU map by simulating pseudo-phenotypes in real genotype samples. We observed that multi-locus mapping on an underlying LDU map reduces location error by approximately 46 %. Furthermore, there is a small, but significant, increase in power of approximately 5 %. Effective meta-analysis across multiple samples, increasingly important to combine evidence from genome-wide and other association data, is achieved through the weighted combination of location evidence provided by the programme...|$|R
40|$|Muscle LIM Protein (MLP) is small, just 198 {{amino acid}} long protein, which is {{specifically}} expressed in slow skeletal muscle and cardiac tissues. This article {{will focus on}} the cardiac functions of MLP: the current knowledge about <b>localisation</b> <b>data,</b> binding partners and animal models for the protein will be summarised, and the role of MLP in maintaining a healthy heart be discussed. This review will furthermore attempt to identify gaps in our knowledge-and hence future research potential-with a special focus on MLP's role in cardiac mechano-signalling...|$|R
40|$|Knowledge {{of protein}} {{localisation}} contributes towards {{our understanding of}} protein function and of biological inter-relationships. A variety of experimental methods are currently being used to produce <b>localisation</b> <b>data</b> {{that need to be}} made accessible in an integrated manner. Chimeric fluorescent fusion proteins have been used to define subcellular localisations with at least 1100 related experiments completed in Arabidopsis. More recently, many studies have employed mass spectrometry to undertake proteomic surveys of subcellular components in Arabidopsis yielding localisation information for ∼ 2600 proteins. Further protein localisation information may be obtained from other literature references to analysis of locations (AmiGO: ∼ 900 proteins), location information from Swiss-Prot annotations (∼ 2000 proteins); and location inferred from gene descriptions (∼ 2700 proteins). Additionally, an increasing volume of available software provides location prediction information for proteins based on amino acid sequence. We have undertaken to bring these various data sources together to build SUBA, a SUBcellular location database for Arabidopsis proteins. The <b>localisation</b> <b>data</b> in SUBA encompasses 10 distinct subcellular locations, > 6743 non-redundant proteins and represents the proteins encoded in the transcripts responsible for 51 % of Arabidopsis expressed sequence tags. The SUBA database provides a powerful means by which to assess protein subcellular localisation in Arabidopsis () ...|$|R
40|$|Background: While {{considerable}} genomic and transcriptomic {{data are}} available for Schistosoma mansoni, many of its genes lack significant annotation. A transcriptomic study of individual tissues and organs of schistosomes could {{play an important role}} in functional annotation of the unknown genes, particularly by providing rapid <b>localisation</b> <b>data</b> and thus giving insight into the potential roles of these molecules in parasite development, reproduction and homeostasis, and in the complex host-parasite interaction. Methodology/Principal Findings: Quantification of gene expression in tissues of S. mansoni was achieved by a combination of laser microdissection microscopy (LMM) and oligonucleotide microarray analysis. We compared the gene expression profile of the adult female gastrodermis and male and female reproductive tissues with whole worm controls. The results revealed a total of 393 genes (contigs) that were up-regulated two-fold or more in the gastrodermis, 4, 450 in the ovary, 384 in the vitelline tissues of female parasites, and 2, 171 in the testes. We have also supplemented these data with the identification of highly expressed genes in different regions of manually dissected male and female S. mansoni. Though relatively crude, this dissection strategy provides low resolution <b>localisation</b> <b>data</b> for critical regions of the adult parasites that are not amenable to LMM isolation. Conclusions: This is the first detailed transcriptomic study of the reproductive tissues and gastrodermis of S. mansoni. Th...|$|R
40|$|Understanding how sets {{of genes}} are coordinately {{regulated}} {{in space and}} time to generate the diversity of cell types that characterise complex metazoans is a major challenge in modern biology. The use of high-throughput approaches, such as large-scale in situ hybridisation and genome-wide expression profiling via DNA microarrays, is beginning to provide insights into the complexities of development. However, in many organisms the collection and annotation of comprehensive in situ <b>localisation</b> <b>data</b> is a difficult and time-consuming task. Here, we present a widely applicable computational approach, integrating developmental time-course microarray data with annotated in situ hybridisation studies, that facilitates the de novo prediction of tissue-specific expression for genes that have no in vivo gene expression <b>localisation</b> <b>data</b> available. Using a classification approach, trained with data from microarray and in situ hybridisation studies of gene expression during Drosophila embryonic development, we made a set of predictions on the tissue-specific expression of Drosophila genes that have not been systematically characterised by in situ hybridisation experiments. The reliability of our predictions is confirmed by literature-derived annotations in FlyBase, by overrepresentation of Gene Ontology biological process annotations, and, in a selected set, by detailed genespecific studies from the literature. Our novel organism-independent method will be of considerable utility in enriching the annotation of gene function and expression in complex multicellular organisms...|$|R
25|$|This motivates why wavelet {{transforms}} are {{now being}} adopted for {{a vast number of}} applications, often replacing the conventional Fourier transform. Many areas of physics have seen this paradigm shift, including molecular dynamics, chaos theory, ab initio calculations, astrophysics, gravitational wave transient <b>data</b> analysis, density-matrix <b>localisation,</b> seismology, optics, turbulence and quantum mechanics. This change has also occurred in image processing, EEG, EMG, ECG analyses, brain rhythms, DNA analysis, protein analysis, climatology, human sexual response analysis, general signal processing, speech recognition, acoustics, vibration signals, computer graphics, multifractal analysis, and sparse coding. In computer vision and image processing, the notion of scale space representation and Gaussian derivative operators is regarded as a canonical multi-scale representation.|$|R
40|$|Activity {{recognition}} gained {{relevance in}} recent years because of its numerous applications. Despite relevant improvements, current classifiers are still inaccurate in several usage conditions or require time-consuming training. In this paper we show how <b>localisation</b> <b>data</b> and common sense knowledge {{could be used to}} improve activity recognition. More specifically, given the GPS position of the user, we both gather (i) a list of neighbouring commercial activities using a reverse geo-coding service and (ii) classify the satellite image of the area with state-of-the-art techniques. The approach maps classification labels produced by the three classifiers (i. e., activity, reverse geocoding localisation, satellite imagery localisation) to concepts within the ConceptNet network for the sake of improving activity recognition accuracy...|$|R
40|$|Robot programmers {{are faced}} with the {{challenging}} problem of understanding the robot’s view of its world, both when creating and when debugging robot software. As a result tools are created as needed in different laboratories for different robots and different applications. We discuss the requirements for effective interaction under these conditions, and propose an augmented reality approach to visualising robot input, output and state information, including geometric data such as laser range scans, temporal data such as the past robot path, conditional data such as possible future robot paths, and statistical <b>data</b> such as <b>localisation</b> distributions. The visualisation techniques must scale appropriately as robot data and complexity increases. Our current progress in developing a robot visualisation toolkit is presented...|$|R
50|$|This motivates why wavelet {{transforms}} are {{now being}} adopted for {{a vast number of}} applications, often replacing the conventional Fourier transform. Many areas of physics have seen this paradigm shift, including molecular dynamics, chaos theory, ab initio calculations, astrophysics, gravitational wave transient <b>data</b> analysis, density-matrix <b>localisation,</b> seismology, optics, turbulence and quantum mechanics. This change has also occurred in image processing, EEG, EMG, ECG analyses, brain rhythms, DNA analysis, protein analysis, climatology, human sexual response analysis, general signal processing, speech recognition, acoustics, vibration signals, computer graphics, multifractal analysis, and sparse coding. In computer vision and image processing, the notion of scale space representation and Gaussian derivative operators is regarded as a canonical multi-scale representation.|$|R
40|$|The Web of Things (WoT) {{paradigm}} enables {{access to}} physical world things and their data through standard Web protocols. This provides interoperability at the hardware and communication protocol level, {{but does not}} add intelligence to the things or facilitate unambiguous interpretation of their data. The evolution of the WoT towards the semantic WoT offers the promise of meeting the interoperability challenge {{through the use of}} semantic Web technologies. The W 3 C Web of Things initiative encourages the use of common vocabularies to ensure interoperability and a common understanding of the domain knowledge. Ontologies provide a structured, common formalism to the disparate elements of the WoT and can form the basis of a common knowledge base. The research community and standardisation bodies have developed numerous ontologies describing the elements of the WoT and associated domains. A comprehensive review of the various proposed ontologies is needed to facilitate the adoption and reuse of the available models. This survey reviews the current state-of-the-art in WoT ontologies, which are presented from two perspectives: cross-domain ontologies which are classified into device, service, <b>data</b> and <b>localisation</b> models, and domain ontologies, which are presented from an environmental and user-oriented perspective...|$|R
40|$|Single photon {{emission}} computed tomography (SPECT) {{used in conjunction}} with HM-PAO (Ceretec-Amersham International) was used to image regional cerebral blood flow (rCBF) in 28 patients with medically intractable complex partial seizures during or soon after a seizure, and interictally. Changes from interictal rCBF were seen in 26 / 28 (93 %) patients. The main findings were; 1) During the seizure [...] hyperperfusion of the whole temporal lobe; 2) Up to 2 m postically [...] hyperperfusion of the hippocampus with hypoperfusion of lateral temporal structures; 3) From 2 - 15 m postically [...] hypoperfusion of the whole temporal lobe. When compared with EEG and MRI <b>data,</b> correct <b>localisation</b> to one temporal lobe was obtained in 23 patients. In one further patient bilateral temporal foci, and in a further two patients frontal foci, were correctly identified. There were no disagreements between EEG and SPECT localisation. Temporal lobe surgery was successful (by the criterion of at least 90 % reduction in seizure frequency) in {{all but one of the}} 23 patients operated on. It is concluded that ictal/postictal SPECT is a reliable technique for the presurgical localisation of complex partial seizures. The data indicate a likely sequence of changes in rCBF during and after complex partial seizures of temporal lobe origin...|$|R
40|$|Existing Global Navigation Satellite Systems {{offer no}} {{authentication}} {{to the open}} service signals and so stand-alone receivers are vulnerable to meaconing and spoofing attacks. These attacks interfere with the integrity and authenticity of satellite signals: they can delay signals, or re-broadcast signals. Positioning is thus compromised and location-based services are at risk. This paper describes a solution to mitigate this risk. It is a trusted third-party Localisation Assurance service that informs location-based services providers up to which level a location claimed by client can be trusted. It runs several tests over the <b>localisation</b> <b>data</b> of client receivers and certifies the level of assurance of locations. An assurance level expresses the amount of trust the third-party has that a receiver's location is calculated from integral and authentic satellite signals...|$|R
40|$|Cardiac motion {{analysis}} from B-mode ultrasound sequence {{is a key}} task in assessing {{the health of the}} heart. The paper proposes a new methodology for cardiac {{motion analysis}} based on the temporal behaviour of points of interest on the myocardium. We define a new signal called the Temporal Flow Graph (TFG) which depicts the movement of a point of interest over time. It is a graphical representation derived from a flow field and describes the temporal evolution of a point. We prove that TFG for an object undergoing periodic motion is also periodic. This principle can be utilized to derive both global and local information from a given sequence. We demonstrate this for detecting motion irregularities at the sequence, as well as regional levels on real and synthetic <b>data.</b> A coarse <b>localisation</b> of anatomical landmarks such as centres of left/right cavities and valve points is also demonstrated using TFGs...|$|R
40|$|Sound {{localisation}} {{is one of}} the {{key roles}} for listening, and measuring localisation performance is a mainstay of the hearing research laboratory. Such measurements consider both accuracy and, for incorrect trials, the size of the error. In terms of error analysis, localisation studies have frequently used general univariate techniques in conjunction with either mean signed or unsigned error measurements. This approach can make inappropriate distributional assumptions and so more suitable alternatives based on directional statistics (e. g. based on von Mises distributed data) have also been used. However these are not readily computed using most commercially available, commonly used statistical software, and are generally only defined for simple experimental designs. We describe a novel use of a 'centre of mass' approach for describing <b>localisation</b> <b>data</b> jointly in terms of accuracy and size of error. This spatial method offers powerful, yet flexible, statistical analysis using standard multivariate analysis of variance (MANOVA) ...|$|R
40|$|We {{report a}} 24 year old female with growth retardation, microcephaly, and {{congenital}} abnormalities {{who has an}} unbalanced de novo translocation between chromosomes 16 and 6 : 45,XX,t(6; 15) (q 25;q 11. 2). FISH analysis confirmed that the deletion on chromosome 15 is proximal to the Prader-Willi locus. Several genes have been assigned to the 6 q 25 -qter region including the insulin-like growth factor II/mannose- 6 -phosphate (IGF-II/M 6 P) receptor. DNA analysis from our patient documented the loss of one IGF 2 R gene copy. These <b>data</b> confirm the <b>localisation</b> of the IGF 2 R receptor to distal 6 q 25. We also showed reduced expression of the soluble and membrane bound IGF-II receptor, a gene dosage effect incompatible with imprinting. The IGF 2 R gene {{has been shown to}} be imprinted in the mouse but not in humans. Our data provide further evidence for lack of imprinting of this gene in humans...|$|R
40|$|In biology, {{localisation}} is function: {{knowledge of}} the localisation of proteins {{is of paramount importance}} to assess and study their function. This supports the need for reliable protein sub- cellular localisation assignment. Concomitant with recent technological advances in organelle proteomics, there is a requirement for more rigorous experimental and analysis design planning and description. In this review, we present an overview of current experimental designs in qualitative and quantitative organelle proteomics as well as associated data analysis. We also consider the major benefits associated with careful description and dissemination of the experiment and analysis designs, namely (i) comparison and optimisation of experimental designs and analysis pipelines, (ii) data validation, (iii) reproducible research, (iv) efficient repository submission and retrieval and (v) meta analysis. Formalisation of experimental design and analysis work flows is of direct benefit for the organelle proteomics researchers and will result in providing organelle <b>localisation</b> <b>data</b> of highest quality for the wider research community...|$|R
40|$|Water is {{a highly}} {{valuable}} resource so asset management of associated infrastructure is of critical importance. Water distribution pipe networks are usually buried, and so are difficult to access. Robots are therefore appealing for performing inspection and detecting damage to target repairs. However, robot mapping and localisation of buried water pipes has not been widely investigated to date, and is challenging because pipes tend to be relatively featureless. In this {{paper we propose a}} mapping and localisation algorithm for metal water pipes with two key novelties: {{the development of a new}} type of map based on hydrophone induced vibration signals of metal pipes, and a mapping algorithm based on spatial warping and averaging of dead reckoning signals used to calibrate the map (using dynamic time warping). Localisation is performed using both terrain-based extended Kalman filtering and also particle filtering. We successfully demonstrate and evaluate the approach on a combination of experimental and simulation <b>data,</b> showing improved <b>localisation</b> compared to dead reckoning. ...|$|R
40|$|The {{knowledge}} of wind resource {{is essential to}} the sitting of a wind farm. This is crucial especially in coastal areas where installation of windmills is more costly than onshore. More accurate estimation of the resource is necessary to preserve the confidence of investors and reduce the cost of production. Nowadays, the resource is evaluated by interpolation of discrete measurements but offshore measurements are rare and very costly (750, 000 euros for an offshore mast) [1]. In addition, coastal wind distribution is characterised by large spatial variations, which are not represented by local measurements [1]. Several studies demonstrated the potentials of using satellite images to map the wind field at sea surface. However, [3] and [4] show the limitations of the use of only one source of data. These limitations are caused by either low spatial resolution of <b>data</b> or <b>localisation</b> far away from coast or too low numbers of measurements for a given site. One solution is to merge all available data to get a more accurate estimation of wind resource [4]. Satellite data have different spatial resolutions, localisations and temporal sampling. These differences impose constraints to the fusion method. In order to work out a data fusion method and evaluate it, it is necessary to study these constraints and their impacts. This paper gives an overview of various available offshore wind data. They are obtained by an appropriate processing of observations of the sea surface performed by spaceborne radars. We study the characteristics of the various data sets, namely, spatial and temporal coverage, spatial and temporal sampling, data support and accuracy. Finally, we discuss the design of a fusion algorithm...|$|R
40|$|Within {{the field}} of action recognition, {{features}} and descriptors are often engineered to be sparse and invariant to transformation. While sparsity makes the problem tractable, {{it is not necessarily}} optimal in terms of class separability and classification. This paper proposes a novel approach that uses very dense corner features that are spatially and temporally grouped in a hierarchical process to produce an overcomplete compound feature set. Frequently reoccurring patterns of features are then found through data mining, designed for use with large data sets. The novel use of the hierarchical classifier allows real time operation while the approach is demonstrated to handle camera motion, scale, human appearance variations, occlusions and background clutter. The performance of classification, outperforms other state-of-the-art action recognition algorithms on the three datasets; KTH, multi-KTH, and Hollywood. Multiple action localisation is performed, though no groundtruth <b>localisation</b> <b>data</b> is required, using only weak supervision of class labels for each training sequence. The Hollywood dataset contain complex realistic actions from movies, the approach outperforms the published accuracy on this dataset and also achieves real time performance. © 2009 IEEE...|$|R
40|$|Innovations in {{localisation}} {{have focused}} on the collection and leverage of language resources. However, smaller localisation clients and Language Service Providers are poorly positioned to exploit the benefits of language resource reuse in comparison to larger companies. Their low throughput of localised content means they have little opportunity to amass significant resources, such as Translation memories and Terminology databases, to reuse between jobs or to train statistical machine translation engines tailored to their domain specialisms and language pairs. We propose addressing this disadvantage via the sharing and pooling of language resources. However, the current localisation standards do not support multiparty sharing, are not well integrated with emerging language resource standards and do not address key requirements in determining ownership and license terms for resources. We survey standards and {{research in the area of}} Localisation, Language Resources and Language Technologies to leverage existing localisation standards via Linked Data methodologies. This points to the potential of using semantic representation of existing <b>data</b> models for <b>localisation</b> workflow metadata, terminology, parallel text, provenance and access control, which we illustrate with an RDF example...|$|R
40|$|Pyrrhotites (Fe (sub 7) S (sub 8)) {{from three}} {{different}} komatiite hosted massive nickel sulphide deposits have been analysed with electron backscatter diffraction analysis (EBSD) and laser ablation inductively coupled plasma mass spectrometry (LA-ICP-MS) {{in order to understand}} how trace elements behave during deformation. EBSD <b>data</b> reveals strain <b>localisation</b> microstructures in sample from greenschist facies whereas in sample from mid-amphibolite facies, pyrrhotite contains multiple parallel low angle boundaries and crystallographic preferred orientation. A sample from the upper amphibolite facies, is characterised by very large > 2 cm grains that contains numerous deformation twins. Laser ablation ICP-MS data reveal increased concentrations of Pb, Bi and Ag along low angle subgrain boundaries and twin boundaries. The increased concentrations of Pb, Bi and Ag are explained by diffusion of the trace elements along fast diffusion pathways (low angle and twin boundaries). Trace element variations are developed at the low-temperature stage of the tectonic history of the three deposits. Diffusion of Pb, Bi and Ag is triggered by their low solubility in sulphide phase, pyrrhotite...|$|R
40|$|Due to the {{ubiquity of}} {{localisation}} technology, users now {{have the ability to}} keep a record of their own location, as a kind of ‘location diary’. Such a large collection of data can become unmanageable without some way to structure that data to make it useful and searchable. We address this problem of structuring location data by proposing a framework for classifying the data into often-traversed routes. In this work, commonly traversed routes are identified with clusters based on sensed data. Our framework does not rely on any one source of location information, but can fuse <b>data</b> from multimodal <b>localisation</b> sources. We demonstrate the effectiveness of our algorithm by examining the combination of GPS, wireless signal strength readings and image-matching on very challenging data in a variety of environmental conditions. By fusing these three modalities we obtained better performance than any individual or combination of two modalities. As it can be orientated towards the needs and capabilities of the user based on context, this method becomes useful for some ambient assisted living applications...|$|R
40|$|Electron {{paramagnetic}} resonance spin probe {{techniques are}} used for the characterisation of structure and dynamics of selected microdomains of polyacrylate latices and films obtained from them. Analysis of the spectral line shapes provides quantitative <b>data</b> on the <b>localisation</b> and mobility of ionic and polar non-ionic low molecular additives during the drying process. Such additives are highly mobile down to bulk water contents of only 5 %. In the dried dispersions, ionic additives form aggregates, in which their local concentration exceeds their bulk concentration by at least a factor of ten. These aggregates can be "melted" by rewetting and the ionic. additives can be partially washed out if the drying is performed below the minimum film forming temperature (MFT), Surfactants in the initial dispersions are partially anchored with their hydrophobic tails in the polymer particles. The fraction of such anchored surfactant molecules increases continuously with decreasing water content. the drying is performed only slightly below the FT, the, surfactants reorganise to inverse micelles., In films obtained by drying at a temperature significantly below the MFT, somewhat larger, surfactant aggregates are formed during annealing...|$|R
40|$|Indoor {{visualisation}} {{has received}} little attention. Research related to indoor environments have primarily focussed on the <b>data</b> structuring, <b>localisation</b> and navigation components (Zlatanova et al., 2013). Visualisation {{is an integral}} component in addressing the diverse array of indoor environments. In simple words, 'What is {{the most efficient way}} to visualise the surrounding indoor environment so that the user can concisely understand their surroundings as well as facilitating the process of navigation?' This dissertation proposes a holistic approach that consists of two components. The significance of this approach is that it provides a robust and adaptable method in providing a standard to which indoor visualisation can be referenced against. The first component is a theoretical framework focussing on indoor visualisation and it comprises of principles from several disciplines such as geovisualisation, human-perception theory, spatial cognition, dynamic and 3 D environments as well as accommodating emotional processes resulting from human-computer interaction. The second component is based on the theoretical framework and adopts a practical approach towards indoor visualisation. It consists of a set of design properties that can be used for the design of effective indoor visualisations. The framework is referred to as the "Elements of Design" framework. Both these components aim to provide a set of principles and guidelines that can be used as best practices for the design of indoor visualisations. In order to practically demonstrate the holistic indoor visualisation approach, multiple indoor visualisation renderings were developed. The visualisation renderings were represented in a three-dimensional virtual environment from a first-person perspective. Each rendering used the design framework differently. Also, each rendering was graded using a parallel chart that compares how the different visual elements were used per the rendering. The main findings were that the techniques/ renderings that used the visual elements effectively (enhanced human-perception) resulted in better acquisition and construction of knowledge about the surrounding indoor environment...|$|R
40|$|Skin {{and soft}} tissue infections (SSTIs) cause {{significant}} morbidity among intravenous drug users (IVDUs). Information about special features of SSTIs in IVDUs {{is essential for}} treatment. We performed a retrospective cross-sectional study in a Norwegian university hospital. Using the ICD- 10 -classification, we selected admissions for abscess, cellulitis and erysipelas from two subsequent years and excluded non-IVDUs. From the patient charts we collected information about demographic <b>data,</b> anatomical <b>localisations,</b> microbiological findings and treatment. 192 admissions of 144 IVDUs were selected. IVDUs utilized 1 / 5 of hospitalisation days for SSTIs and 1 / 3 for abscesses. Most SSTIs were localized in the lower extremities and 15 % had coexisting bacterial infections other than SSTIs. In abscesses, streptococci (42, 9 %), staphylococci (40, 0 %), gram-negative rods (8, 6 %) and anaerobic bacteria (6, 4 %) were the most commonly isolated strains. 1 / 4 had mixed cultures. In erysipelas and cellulitis, staphylococci (42, 3 %) and streptococci (19, 2 %) dominated. We found two strains of MRSA in one patient. Surprisingly, 8 % of staphylococci were clindamycin resistant and clindamycin was commonly prescribed, raising concern about selection of resistant strains. 85 % received effective antibiotics. There was compliance to guidelines for empirical antimicrobial therapy in 2 / 3 of cases, with most patients receiving dicloxacillin/cloxacillin or penicillin. Among non-compliant cases, {{a larger proportion of}} patients received ineffective and/or too broad-spectrum antibiotics. For abscesses, the use of antibiotics is generally controversial. Today’s guidelines for empirical therapy cover the most common pathogens in IVDUs. Our material was limited regarding erysipelas and cellulitis, but findings suggest that dicloxacillin/cloxacillin may be the better empirical treatment for IVDUs. In general, antimicrobial therapy was adjusted according to microbiological findings in only 50 % of the cases in our study...|$|R
