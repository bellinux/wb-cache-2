52|7|Public
50|$|Snappy {{encoding}} is not bit-oriented, but byte-oriented (only whole bytes are emitted or consumed from a stream). The format uses no <b>entropy</b> <b>encoder,</b> like Huffman tree or arithmetic encoder.|$|E
50|$|HEVC uses a context-adaptive binary {{arithmetic}} coding (CABAC) algorithm that is fundamentally similar to CABAC in H.264/MPEG-4 AVC. CABAC {{is the only}} <b>entropy</b> <b>encoder</b> method that is allowed in HEVC while there are two <b>entropy</b> <b>encoder</b> methods allowed by H.264/MPEG-4 AVC. CABAC and the entropy coding of transform coefficients in HEVC were designed for a higher throughput than H.264/MPEG-4 AVC, while maintaining higher compression efficiency for larger transform block sizes relative to simple extensions. For instance, the number of context coded bins have been reduced by 8× and the CABAC bypass-mode has been improved {{in terms of its}} design to increase throughput. Another improvement with HEVC is that the dependencies between the coded data has been changed to further increase throughput. Context modeling in HEVC has also been improved so that CABAC can better select a context that increases efficiency when compared with H.264/MPEG-4 AVC.|$|E
5000|$|Wavefront {{parallel}} processing (WPP) {{is when a}} slice is divided into rows of CTUs in which the first row is decoded normally but each additional row requires that decisions {{be made in the}} previous row. WPP has the <b>entropy</b> <b>encoder</b> use information from the preceding row of CTUs and allows for a method of {{parallel processing}} that may allow for better compression than tiles.|$|E
3000|$|Sayir [10] {{showed that}} an {{arithmetic}} coder {{can be an}} <b>entropy</b> source <b>encoder</b> when the model is matched with the source and can be a channel encoder when the probability space is properly reserved for error protection and {{can act as a}} convolutional code. After inserting the forbidden symbol to a source with M alphabet, we will have an arithmetic coding with M + 1 alphabet in which one of the symbols never appears. Therefore, adding parity is performed while compression without adding more additional operations to the conventional arithmetic coding. If the source has M alphabet, so this method just adds M multiplication and 1 additional operation to the complexity of conventional arithmetic encoder. But if we want to place a convolutional encoder after arithmetic encoder, according to the amount of redundancy, it needs some shift and XOR operations and increasing memory usage. For example, if the bit rate is 1 / 2 and the code generator polynomial is [...]...|$|R
40|$|A VLSI {{architecture}} for {{an adaptive}} data compression encoder capable of sustaining {{fixed or variable}} bit-rate output has been developed. There are three modes of operation: lossless with variable bit-rate, lossy with fixed bit-rate and lossy with variable bit-rate. For lossless encoding, the implementation {{is identical to the}} USES chip designed for Landsat 7. Obtaining a fixed bit-rate is achieved with a lossy DPCM algorithm using adaptive, nonuniform scalar quantization. In lossy mode, variable bit-rate coding uses the lossless sections of the <b>encoder</b> for post-DPCM <b>entropy</b> coding. The <b>encoder</b> shows excellent compression performance in comparison to other current data compression techniques. No external tables or memory are required for operation...|$|R
5000|$|FFV1, {{which stands}} for [...] "FF video codec 1", is a {{lossless}} intra-frame video codec. It can use either variable length coding or arithmetic coding for <b>entropy</b> coding. The <b>encoder</b> and decoder {{are part of the}} free, open-source library libavcodec in the project FFmpeg since June 2003. FFV1 is also included in ffdshow and LAV Filters, which makes the video codec available to Microsoft Windows application that support system-wide codecs over Video for Windows (VfW) or DirectShow.FFV1 is particularly popular for its performance regarding speed and size, compared to other lossless preservation codecs, such as M-JPEG2000.The European Broadcasting Union (EBU) lists FFV1 under the codec-family index [...] "31" [...] in their combined list of video codec references.|$|R
50|$|Besides using entropy {{encoding}} {{as a way}} to compress digital data, an <b>entropy</b> <b>encoder</b> {{can also be used to}} measure the amount of similarity between streams of data and already existing classes of data. This is done by generating an entropy coder/compressor for each class of data; unknown data is then classified by feeding the uncompressed data to each compressor and seeing which compressor yields the highest compression. The coder with the best compression is probably the coder trained on the data that was most similar to the unknown data.|$|E
50|$|The simple scheme {{described}} above {{focuses on the}} LZW algorithm itself. Many applications apply further encoding to the sequence of output symbols. Some package the coded stream as printable characters using some form of binary-to-text encoding; this will increase the encoded length and decrease the compression rate. Conversely, increased compression can often be achieved with an adaptive <b>entropy</b> <b>encoder.</b> Such a coder estimates the probability distribution {{for the value of}} the next symbol, based on the observed frequencies of values so far. A standard entropy encoding such as Huffman coding or arithmetic coding then uses shorter codes for values with higher probabilities.|$|E
40|$|JPEG 2000 {{is the new}} {{standard}} for the compression of images, which succeeds to JPEG. It’s has a much higher algorithmic complexity. The complexity of the <b>entropy</b> <b>encoder</b> (EBCOT) {{is the most significant}} in JPEG 2000. Which alone constitutes about 70 % of the overall processing time for compression of an image. This {{new standard}} has many features and characteristics (region of interest, several types of decompression). In this paper we are interested on studying the algorithm of JPEG 2000 and the most complex in the JPEG 2000 compression process, the EBCOT <b>entropy</b> <b>encoder,</b> and its performance are presented...|$|E
40|$|We {{consider}} the multi-user lossy source-coding problem for continuous alphabet sources. In a previous work, Ziv proposed a single-user universal coding scheme which uses uniform quantization with dither, {{followed by a}} lossless source <b>encoder</b> (<b>entropy</b> coder). In this paper, we generalize Ziv's scheme to the multi-user setting. For this generalized universal scheme, upper bounds are derived on the redundancies, defined as {{the differences between the}} actual rates and the closest corresponding rates on the boundary of the rate region. It is shown that this scheme can achieve redundancies of no more than 0. 754 bits per sample for each user. These bounds are obtained without knowledge of the multi-user rate region, which is an open problem in general. As a direct consequence of these results, inner and outer bounds on the rate-distortion achievable region are obtained. Comment: 24 pages, 1 figur...|$|R
40|$|Uniform {{quantization}} with dither, or lattice quantization with dither in {{the vector}} case, {{followed by a}} universal lossless source <b>encoder</b> (<b>entropy</b> coder), is a simple procedure for universal coding with distortion of a source that may take continuously many values. The rate of this universal coding scheme is examined, and we derive a general expression for it. An upper bound for the redundancy of this scheme, defined as the difference between its rate and the minimal possible rate, given by the rate distortion function of the source, is derived. This bound holds for all distortion levels. Furthermore, we present a composite upper bound on the redundancy {{as a function of}} the quantizer resolution which leads to a tighter bound in the high rate (low distortion) case. Key Words: Uniform and Lattice Quantization, Randomized Quantization, Universal Coding, Rate-Distortion Performance Meir Feder was also supported by The Andrew W. Mellon Foundation, Woods Hole Oceanographic Institu [...] ...|$|R
40|$|One way to {{save the}} power {{consumption}} in the H. 264 decoder is for the H. 264 encoder to generate decoderfriendly bit streams. By following this idea, a decoding complexity model of context-based adaptive binary arithmetic coding (CABAC) for H. 264 /AVC is investigated in this research. Since different coding modes {{will have an impact}} on the number of quantized transformed coefficients (QTCs) and motion vectors (MVs) and, consequently, the complexity of <b>entropy</b> decoding, the <b>encoder</b> with a complexity model can estimate the complexity of entropy decoding and choose the best coding mode to yield the best tradeoff between the rate, distortion and decoding complexity performance. The complexity model consists of two parts: one for source data (i. e. QTCs) and the other for header data (i. e. the macro-block (MB) type and MVs). Thus, the proposed CABAC decoding complexity model of a MB is a function of QTCs and associated MVs, which is verified experimentally. The proposed CABAC decoding complexity model can provide good estimation results for variant bit streams. Practical applications of this complexity model will also be discussed. 1...|$|R
40|$|Switched-current is an analog, {{discrete}} in time, {{signal processing}} technique that is fully compatible with any digital CMOS technology. This means that analog circuits {{can be realized}} together with digital components on a single chip without any additional technological processes. In designs implemented using the switched-current technique, the individual circuit elements interact by the means of currents, which allows to reduce voltage swings and thus power consumption. This work investigated {{the implementation of a}} low power mixed signal image compression system in TSMC 0. 35 um technology. The major components of this system were two dimensional discrete cosine transform processor, analog to digital converter, quantizer and <b>entropy</b> <b>encoder.</b> The discrete cosine transform section was implemented using switched-current technique. The digital part consisting of the quantizer, <b>entropy</b> <b>encoder</b> and control unit was modelled using VHDL and then synthesized into standard cells...|$|E
30|$|Additionally, {{the labels}} (‘O’, ‘I’, or ‘S’) are set for each frame to {{distinguish}} the original frame, interpolated one, or skipped one, then indexed with odd or even numbers and transmitted over two channels, respectively. Here, the labels are coded by <b>entropy</b> <b>encoder,</b> which nearly can be neglected compared with the total bit rate.|$|E
40|$|Abstract- This paper {{presents}} an efficient algorithm to compress digital images in CFA (Color Filter Array) format. The proposed technique is mainly {{based on a}} vector quantization (VQ) engine followed by an <b>entropy</b> <b>encoder</b> (e. g. DPCM). The VQ strategy adopted exploits both chromatic correlation and psychovisual/perceptive characteristics. Experiments show effectiveness in terms of overhead computation and compression performance...|$|E
40|$|Autonomous marine {{vehicles}} are increasingly used in clusters for {{an array of}} oceanographic tasks. The effectiveness of this collaboration is often limited by communications: throughput, latency, and ease of reconfiguration. This thesis argues that improved communication on intelligent marine robotic agents can be gained from acting on knowledge gained by improved awareness of the physical acoustic link and higher network layers by the AUV's decision making software. This thesis presents a modular acoustic networking framework, realized through a C++ library called goby-acomms, to provide collaborating underwater vehicles with an efficient short-range single-hop network. goby-acomms is comprised of four components that provide: 1) losslessly compressed encoding of short messages; 2) a set of message queues that dynamically prioritize messages based both on overall importance and time sensitivity; 3) Time Division Multiple Access (TDMA) Medium Access Control (MAC) with automatic discovery; and 4) an abstract acoustic modem driver. Building on this networking framework, two approaches that use the vehicle's "intelligence" to improve communications are presented. The first is a "non-disruptive" approach which is a novel technique for using state observers in conjunction with an <b>entropy</b> source <b>encoder</b> to enable highly compressed telemetry of autonomous underwater vehicle (AUV) position vectors. This system was analyzed on experimental data and implemented on a fielded vehicle. Using an adaptive probability distribution in combination with either of two state observer models, greater than 90 % compression, relative to a 32 -bit integer baseline, was achieved. The second approach is "disruptive," as it changes the vehicle's course to effect an improvement in the communications channel. A hybrid data- and model-based autonomous environmental adaptation framework is presented which allows autonomous underwater vehicles (AUVs) with acoustic sensors to follow a path which optimizes their ability to maintain connectivity with an acoustic contact for optimal sensing or communication. by Toby Edwin Schneider. Thesis (Ph. D.) [...] Joint Program in Oceanography/Applied Ocean Science and Engineering (Massachusetts Institute of Technology, Dept. of Mechanical Engineering; and the Woods Hole Oceanographic Institution), 2013. This electronic version was submitted by the student author. The certified thesis {{is available in the}} Institute Archives and Special Collections. Cataloged from student-submitted PDF version of thesis. Includes bibliographical references (p. 151 - 161) ...|$|R
40|$|In {{the present}} contribution, a new fast 3 -D seismic {{adaptive}} compression strategy {{based on the}} Lifted (discrete) Wavelet Transform (LWT) is proposed to improve the overall performance of modem 3 -D seismic interpretation. The compression scheme consists of three units, namely a 3 -D biorthogonal LWT, an adaptive quantizer, and an <b>entropy</b> <b>encoder.</b> The LWT constitutes the most efficient implementation of higher dimension wavelet transforms. It allows for an in-place implementation, which saves considerable auxiliary memory usage. This {{turns out to be}} of the utmost importance when compressing large datasets like seismic data. Moreover, the LWT is immediately invertible, with exactly the same complexity as for the forward transform. The proposed adaptive quantizer involves techniques from non-linear approximation theory, namely the wavelet shrinkage. Indeed the scale and direction dependent dead-zone of the Threshold Uniform Scalar Quantizer (TUSQ) is determined for each subband using the SureShrink principle. The <b>entropy</b> <b>encoder</b> is the classical association run-length and Huffman encoding techniques...|$|E
40|$|Integrated {{multimedia}} systems process text, graphics, {{and other}} discrete media such as digital {{audio and video}} streams. In an uncompressed state, graphics, audio and video data, especially moving pictures, require large transmission and storage capacities which can be very expensive. Hence video compression has become {{a key component of}} any multimedia system or application. The ITU (International Telecommunications Union) and MPEG (Moving Picture Experts Group) have combined efforts to put together the next generation of video compression standard, the H. 264 /MPEG- 4 PartlO/AVC, which was finalized in 2003. The H. 264 /AVC uses significantly improved and computationally intensive compression techniques to maximize performance. H. 264 /AVC compliant encoders achieve the same reproduction quality as encoders that are compliant with the previous standards while requiring 60 % or less of the bit rate [2]. This thesis aims at designing two basic blocks of an ASIC capable of performing the H. 264 video compression. These two blocks, the Quantizer, and <b>Entropy</b> <b>Encoder</b> implement the Baseline Profile of the H. 264 /AVC standard. The architecture is implemented in Register Transfer Level HDL and synthesized with Synopsys Design Compiler using TSMC 0. 25 (xm technology, giving us an estimate of the hardware requirements in real-time implementation. The quantizer block is capable of running at 309 MHz and has a total area of 785 K gates with a power requirement of 88. 59 mW. The <b>entropy</b> <b>encoder</b> unit is capable of running at 250 MHz and has a total area of 49 K gates with a power requirement of 2. 68 mW. The high speed that is achieved in this thesis simply indicates that the two blocks Quantizer and <b>Entropy</b> <b>Encoder</b> can be used as IP embedded in the HDTV systems...|$|E
40|$|International audienceThis paper {{presents}} a reconfigurable <b>entropy</b> <b>encoder</b> for real time adaptation of multi-standard compressed video stream. The proposed architecture {{is based on}} partial and dynamic reconfiguration. Static and dynamic wrappers are defined to encapsulate different types of entropy coders. A partitioning of reconfigurable area is also presented aiming at the optimization of reconfiguration overhead. The obtained results show a significant gain in silicon area compared to a solution without reconfiguration and a significant gain in reconfiguration time...|$|E
30|$|For all codecs under evaluation, {{only the}} {{luminance}} component was coded, {{meaning that the}} SI quality and the RD performance consider only the luminance rate and quality (naturally, for both the key frames and WZ frames). For the DVC-MCFI and DVC-BPSI key frame coding and the H. 264 /AVC Intra and H. 264 /AVC Zero motion codecs, the Main profile was selected (as typically in the DVC literature) since it allows high RD performance, even if with some encoding complexity cost associated to the CABAC <b>entropy</b> <b>encoder</b> and Intra coding modes.|$|E
40|$|A low-complexity circuit for on-sensor {{compression}} is presented. The proposed circuit achieves complexity savings {{by combining}} a single-slope analog-to-digital converter with a Golomb-Rice <b>entropy</b> <b>encoder</b> and by implementing a low-complexity adaptation rule. The adaptation rule monitors the output codewords and minimizes their length by incrementing or decrementing {{the value of}} the Golomb-Rice coding parameter k. Its hardware implementation is one order of magnitude lower than existing adaptive algorithms. The compression circuit has been fabricated using a 0. 35 micrometers CMOS technology and occupies an area of 0. 0918 mm 2. Test measurements confirm the validity of the desig...|$|E
40|$|An {{audio encoder}} (100) for {{encoding}} segments of coefficients, the segments of coefficients representing different time or frequency resolutions of a sampled audio signal, the audio encoder (100) comprising a processor (110) for deriving a coding context for a currently encoded coefficient of a current segment {{based on a}} previously encoded coefficient of a previous segment, the previously encoded coefficient representing a different time or frequency resolution than the currently encoded coefficient. The audio encoder (100) further comprises an <b>entropy</b> <b>encoder</b> (120) for entropy encoding the current coefficient based on the coding context to obtain an encoded audio stream...|$|E
40|$|The {{document}} m 13416 {{presents the}} encoding {{performance of the}} wavelet scalable video codec developed by aceMedia consortium. The codec architecture, concerning the encoding schemes (t+ 2 D, 2 D+t+ 2 D, etc.) that can be realized, are basically the same allowed in the current MSRA VidWav scalable video coding model. The main differences {{can be found in}} the fundamental tools, such as Motion Estimation, <b>Entropy</b> <b>Encoder,</b> etc., used to implement a given encoding architecture. The evaluation of the proposed technology and its performance will be useful for improving the current VidWav reference scalable video codec...|$|E
30|$|There are {{multiple}} ways {{to realize the}} proposed FS-ECVQ. First of all, if the quantizing errors generated from the lattice quantizer are directly discarded, then the FS-ECVQ is equivalent to an ordinary ECVQ. However, if the quantizing errors are taken as the LSBs and encoded by an additional length function, the FS-ECVQ will be equal to an uniform quantizer. In addition, if the quantization steps of all the component ECVQs are separated from the FS-ECVQ, then the FS-ECVQ becomes an <b>entropy</b> <b>encoder.</b> The FS-ECVQ {{can also be used}} in coding the speech, image, and video signals, and even any other source sequence with non-uniform distribution.|$|E
40|$|JPEG 2000 {{is the new}} {{standard}} for the compression of images, which succeeds to JPEG. This standard is motivated primarily by the need for compressed image representations that offer new features increasingly demanded by modern applications, and also offering superior compression performance, especially at low bit-rates. This {{new standard}} has many features and characteristics (region of interest, several types of decompression). But these characteristics are accompanied by a much higher algorithmic complexity than JPEG (about five times more complex). In this paper we are interested on studying the algorithm of JPEG 2000 and the most complex in the JPEG 2000 compression process, the EBCOT <b>entropy</b> <b>encoder,</b> and its performance are presented...|$|E
40|$|In this paper, we {{describe}} the design and implementation of a prototype single chip VLSI architecture for implementing the JPEG baseline image compression standard. The chip exploits the principles of pipelining and parallelism to the maximum extent {{in order to obtain}} high speed and throughput. The architecture for discrete cosine transform and the <b>entropy</b> <b>encoder</b> are based on efficient algorithms designed for high speed VLSI implementation. The chip was implemented using the Cadence tools and based on the prototype implementation the proposed chip architecture can yield a clock rate of about 100 MHz which would allow an input rate of 30 frames per second for 1024 x 1024 color images. 1...|$|E
40|$|In {{this paper}} we {{introduce}} an adaptive local pdf estimation {{strategy for the}} construction of Generalized Lifting (GL) mappings in the wavelet domain. Our approach consists in trying to estimate the local pdf of the wavelet coefficients conditioned to a context formed by neighboring coefficients. To this end, we search in a small causal window for similar contexts. This strategy is independent of the wavelet filters used to transform the image. Experimental results exhibit interesting gains in terms of energy reduction comparable to those obtained in [8]. In order to take benefit from this energy reduction, specific <b>entropy</b> <b>encoder</b> should be designed in the future. Index Terms — Generalized lifting, wavelets, image coding, adaptive local pdf estimation...|$|E
40|$|Efficient {{multimedia}} encryption algorithms play a {{key role}} in multimedia security protection. We introducing multiple Huffman Tables (MHT), which performs both compression and encryption by using multiple statistical models (i. e. Huffman coding tables) in the <b>entropy</b> <b>encoder</b> and multiple Huffman tables are kept secret. A known-plaintext attack is presented to show that the MHTs used for encryption should be carefully selected to avoid the weak keys problem. We then propose chosen-plaintext attacks on the basic MHT algorithm as well as the advanced scheme with random bit insertion. In addition, we suggest two empirical criteria for Huffman table selection, based on which we can simplify the stream cipher integrated scheme, while ensuring a high level of security...|$|E
40|$|In {{this paper}} {{a simple and}} fast image {{compression}} scheme is proposed, {{it is based on}} using wavelet transform to decompose the image signal and then using polynomial approximation to prune the smoothing component of the image band. The architect of proposed coding scheme is high synthetic where the error produced due to polynomial approximation in addition to the detail sub-band data are coded using both quantization and Quadtree spatial coding. As a last stage of the encoding process shift encoding is used as a simple and efficient <b>entropy</b> <b>encoder</b> to compress the outcomes of the previous stage. The test results indicate that the proposed system can produce a promising compression performance while preserving the image quality level...|$|E
40|$|Chaos, lossy {{compression}}, control system, coding pipeline. In {{this paper}} a novel image compression pipeline, by {{making use of}} a controlled chaotic system, is proposed. Chaos is a particular dynamic generated by nonlinear systems. Under certain conditions {{it is possible to}} properly manage the chaotic dynamics obtaining very feasible and powerful working instruments. In the proposed compression pipeline a linear feedback control strategy has been used to stabilize chaotic dynamic used to track the 1 D signal generated by the input image. The pipeline is closed by an <b>entropy</b> <b>encoder.</b> Preliminary experiments and comparison with respect to standard JPEG engine confirm the effectiveness of the proposed chaotic coding system both for natural and graphic images. Also the overall performances in terms of rate-distortion capabilities are promising. ...|$|E
40|$|To {{design an}} {{efficient}} VLSI architecture of a lossless ECG encoding circuit for wireless healthcare monitoring application. This system eventually helps in hospitals for easily to monitoring the patient ECG signal at 24 hour in house or healthcare. This technology {{can also be}} well utilized in all regions. For ECG signal compression andin order to save wireless transmission poweranovel lossless encoding algorithm is proposed. This algorithm containsa novel adaptive predictor {{which is based on}} fuzzy decision control, and a novel hybrid <b>entropy</b> <b>encoder</b> including both a two-stage Huffman and a Golomb-Rice coding. Compared to previous low-complexity and highperformance lossless ECG encoder studies, this design is expected to have a higher compression rate, lower power consumption and lower hardware cost than other VLSI designs...|$|E
40|$|Abstract — The Color Filter Array is {{a mosaic}} of tiny color filters placed over the pixel sensors of an image sensor to capture color information. CFA image is divided into 4 sub images. Each sub image {{contains}} G 1, G 2, R and B color components. G 1 is encoded by using any conventional gray scale encoding technique. G 2 is predicted from encoded G 1 values which produces the prediction error eδG 2. Then, the G pixels are interpolated {{to fill in the}} G values at the positions of the R and B pixels. Fourth, these interpolated G pixels are subtracted from the R and B pixels, producing δR. δR is predicted from encoded G 1 value, predicted G 2 value and already encoded R value produces the prediction error of red. δB is predicted from encoded G 1 value and from both predicted G 2 and B value and also from already encoded B value produces the prediction error of blue. The error signals obtained by the prediction block are fed into an <b>entropy</b> <b>encoder.</b> The choice of predictors and weights is of course based on the direction of edges around the x. We define the edge directivity around x and take smallest two of them and they are used for the calculation of weight and then by using the weight and predictors actual value is estimated. After estimating the value of G 2, R and B, three errors are calculated. These three errors are fed into an <b>entropy</b> <b>encoder</b> like Huffman encoder and they are separately encoded. Then bits per pixel and compression ratio are calculated. It can be decoded by using a Huffman decoder. From images that are outputted by Huffman decoder, mosaic image i...|$|E
40|$|This {{dissertation}} {{studies the}} integration of a video compression system on the focal plane. The integration of a video compression circuit and an image sensor on the same chip is expected to improve {{the performance of the}} sensor chip and lower fabrication costs. Enabling technologies are researched and a circuit is proposed. The approach followed in this research work was a top-to-bottom approach. The problem was first studied at the system and algorithmic level. Once a system solution was found it was implemented on a silicon chip. ^ The proposed chip consists of an 80 x 44 active pixel array and a bank of column-level processors that performs the tasks of image decorrelation, quantization, and entropy encoding. The chip provides at its output a compressed bit stream. The integration of all these building blocks was possible due to two key contributions of this dissertation. First, a quantizer and an <b>entropy</b> <b>encoder</b> were combined by noticing that they have common circuitry. The result of exploiting this observation is an analog-to-digital converter capable of performing a joint quantization/coding operation. The second contribution is a low-complexity algorithm that adapts the <b>entropy</b> <b>encoder</b> to the statistics of the image. The chip was fabricated in a 4 -metal, 2 -poly 0. 35 -μm complementary metal-oxide-semiconductor (CMOS) process. The chip occupies an area of 2. 596 mm x 5. 958 mm. ^ Test and measurement results are presented which verify the validity of the design. The advantages and drawbacks of the proposed implementation are discussed as well. The results of this research work are not limited to the area of focal plane video compression. Other interesting and exciting areas like sensor networks can benefit from this research work. ...|$|E
40|$|Abstract – Claude Shannon’s {{pioneering}} work quantified the performance limits of communications systems operating over classic wireline Gaussian channels. However, his source and channel coding theorems were derived {{for a range}} of idealistic conditions, which may not hold in low-delay, interactive wireless multimedia communications. Firstly, Shannon’s ideal lossless source encoder, namely the <b>entropy</b> <b>encoder</b> may have an excessive codeword length, hence exhibiting a high delay and a high error sensitivity. However, in practice most multimedia source signals are capable of tolerating lossy, rather than lossless delivery to the human eye, ear and other human sensors. The corresponding lossy and preferably low-delay multimedia source codecs however exhibit unequal error sensitivity, which is not the case for Shannon’s ideal entropy codec. There are further numerous differences between the Shannonian lessons originally outlined for Gaussia...|$|E
40|$|Abstract — Conceptual and {{practical}} encoding/decoding, aimed at accurately reproducing remotely collected observations, has been heavily investigated since the pioneering works by Shannon about source coding. However, when {{the goal is}} not to reproduce the observables, but making inference about an embedded parameter and the scenario consists of many unconnected remote nodes, the landscape is less certain. We consider a multiterminal system designed for efficiently estimating a random parameter according to the MMSE crite-rion. The analysis is limited to scalar quantizers followed by a joint <b>entropy</b> <b>encoder,</b> and it is performed in the high-resolution regime where the problem can be easier mathematically tackled. Focus is made on the peculiarities deriving from the estimation task, as opposed to that of reconstruction, {{as well as on the}} mul-titerminal, as opposite to centralized, character of the inference...|$|E
40|$|International audienceThis paper {{presents}} an efficient reconfigurable <b>entropy</b> <b>encoder</b> for real time adaptation of multi-standard compressed video stream. The proposed embedded architecture {{is based on}} partial and dynamic reconfiguration. Static and dynamic wrappers are defined to encapsulate different types of entropy coders with efficient swapping capabilities for context saving and restoring. The wrappers are designed to reduce the area overhead. A partitioning of reconfigurable area is also presented aiming at the optimization of reconfiguration overhead. The obtained results show a significant gain up to 40 % in silicon area compared to a solution without reconfiguration and a significant gain of 23 % in reconfiguration time. Thanks to reconfigurable entropy coder, {{it is possible to}} treat MPEG- 2 and H. 264 video streams meeting its real time constraints using dynamic reconfiguration...|$|E
