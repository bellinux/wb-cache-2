7|28|Public
40|$|Abstract. In this paper, an {{improved}} differential cryptanalysis framework for finding collisions in hash functions is provided. Its principle {{is based on}} linearization of compression functions {{in order to find}} low weight differential characteristics as initiated by Chabaud and Joux. This is formalized and refined however in several ways: for the problem of finding a conforming message pair whose differential trail follows a linear trail, a condition function is introduced so that finding a collision is equivalent to finding a preimage of the zero vector under the condition function. Then, the <b>dependency</b> <b>table</b> concept shows how much influence every input bit of the condition function has on each output bit. Careful analysis of the <b>dependency</b> <b>table</b> reveals degrees of freedom that can be exploited in accelerated preimage reconstruction under the condition function. These concepts are applied to an in-depth collision analysis of reduced-round versions of the two SHA- 3 candidates CubeHash and MD 6, and are demonstrated to give by far the best currently known collision attacks on these SHA- 3 candidates...|$|E
40|$|This paper {{describes}} the incremental weaving implementation of Apostle, an aspect-oriented language extension to Smalltalk modelled on AspectJ. Apostle implements incremental weaving {{in order to}} make aspect-oriented programming (AOP) a natural extension of the incremental edit-run-debug cycle of Smalltalk environments. The paper analyzes build dependencies for aspect declarations, and shows that two simple <b>dependency</b> <b>table</b> structures are sufficient to produce reasonable re-weaving efficiency. The resulting incremental weaver provides re-weaving performance proportional to the change in the program. 1...|$|E
40|$|Tasks may {{be planned}} for {{execution}} {{on a single}} processor or are split up by the designer for execution among a plurality of signal processors. The tasks are modeled using a design aid called a precedence graph, from which a <b>dependency</b> <b>table</b> and a prerequisite table are established for reference within each processor. During execution, {{at the completion of}} a given task, an end of task interrupt is provided from any processor which has completed a task to any and all other processors including itself in which completion of that task is a prerequisite for commencement of any dependent tasks. The relevant updated data may be transferred by the processor either before or after signalling task completion to the processors needing the updated data prior to commencing execution of the dependent tasks. Coherency may be ensured, however, by sending the data before the interrupt. When the end of task interrupt is received in a processor, its <b>dependency</b> <b>table</b> is consulted to determine those tasks dependent upon completion of the task which has just been signalled as completed, and task dependency signals indicative thereof are provided and stored in a current status list of a prerequisite table. The current status of all current prerequisites are compared to the complete prerequisites listed for all affected tasks and those tasks for which the comparison indicates that all prerequisites have been met are queued for execution in a selected order...|$|E
40|$|The main {{objective}} of this thesis is to develop an approaches for the creation and revision of mediated schema ontology. A set of rules for mapping between relational database components and ontological entities,including rules for handling inclusion <b>dependency</b> and <b>table</b> integration were also proposed...|$|R
50|$|The {{phase of}} the design of {{computer}} architecture and software architecture can also {{be referred to as}} high-level design. The baseline in selecting the architecture is that it should realize all which typically consists of the list of modules, brief functionality of each module, their interface relationships, <b>dependencies,</b> database <b>tables,</b> architecture diagrams, technology details etc. The integration testing design is carried out in the particular phase.|$|R
40|$|The {{bachelor}} thesis discribes {{design and}} implementation of database schema documentation generator. It uses Schemagic application to create wellarranged documentation in user-defined output format. Schemagic is used to load information about schema into a XML document. For output documentation HTML, PDF, Postscript, RTF, DocBook and FO (XML file of formating objects) formats are supported. The Gendok application also generates some graphic information, e. g. graphs of <b>dependencies</b> between <b>tables...</b>|$|R
40|$|Software {{testing is}} the process of {{evaluating}} a system or its modules in the intent to find that the software is acquiring the efficient requirements or not. In simple words testing is the execution of the system in order to find their gaps, ambiguity and inconsistency. Software testing comprises into three factors: test case generation, test cases execution and test cases evaluation. This paper implemented a semantic approach for the generation of test cases on UML model i. e., Activity Diagram. In this approach an Activity diagram is created then it automatically generated a Activity <b>Dependency</b> <b>table</b> (ADT) from Activity diagram. From the ADT an Activity Dependency Graph (ADG) is introduced. Finally After the automatic generation of ADG a consistent test case are generated. This approach includes the validation of the test cases by their consistency and efficiency. This approach saves the cost, time, efforts and increases the quality of generated test cases...|$|E
40|$|Volume {{resistivity}} of some insulators at cold temperatures, and electrodless TPC * J. Va’vra, SLAC We {{believe that}} one should control gradients on high voltage insulators using low resistivity plastics. Longterm space charge build-up, especially in a very cold insulator, cannot be excluded if one has a source of charge. Time constants, given extremely large volume resistance values involved, can be extremely long, which can result in quite unusual voltage gradients. Teflon may also shed pollution, which may {{end up on the}} cathode wires as a film, which will increase its resistivity in cold liquid, and may lead to a Malter effect. Measurements of volume resisitivity of various insulators: Figures 1 & 2 show the experimental setup, which allows a measurement of volume resistivity at LN 2 temperature. Typically we would cool the sample to LN 2 temperature and measure its resistance as it warms up (temperature was monitored by a thermocouple). In this way we could measure temperature <b>dependency.</b> <b>Table</b> 1 shows the summary of measurements. Figure 3 shows that many samples, which have a low resistivity at room temperature, show an exponentia...|$|E
40|$|Abstract — Software {{testing is}} an {{essential}} and {{integral part of the}} software development process. The testing effort is divided into three parts: test case generation, test execution, and test evaluation. Test case generation is the core of any testing process and automating it saves much time and effort as well as reduces the number of errors and faults. This paper proposes an automated approach for generating test cases from one of the most famed UML diagrams which is the activity diagram. The proposed model introduces an algorithm that automatically creates a table called Activity <b>Dependency</b> <b>Table</b> (ADT), and then uses it to create a directed graph called Activity Dependency Graph (ADG). The ADT is constructed in a detailed form that makes the generated ADG covers all the functionalities in the activity diagram. Finally the ADG with the ADT are used to generate the final test cases. The proposed model includes validation of the generated test cases during the generation process to ensure their coverage and efficiency. The generated test cases meet a hybrid coverage criterion in addition to their form which enables using them in system, regression as well as integration testing. The proposed model saves time and effort besides, increases the quality of generated test cases. The model is implemented on three different systems and evaluated to show its effectiveness. Index Term — Automatic test case generation, Model-Base...|$|E
40|$|Volume {{tests in}} a {{regression}} context {{were introduced in}} 1939. The concept was revisited in 1985, {{this time in the}} case of con-tingency tables to refine the power of explanation of the χ 2 test. This article considers volume tests as a measure of <b>dependency</b> for <b>tables</b> with fixed margins. Building on earlier contributions, this article suggests the use of the volume test statistic as a mea-sure of dependence to be applied, for example, in the evaluation of linkage disequilibrium between markers...|$|R
40|$|AD-{{formulas}} {{are simple}} formulas representing particular <b>dependencies</b> in data <b>tables.</b> We study AD-formulas from the logical point of view. In particular, we present results regarding {{the relationship of}} AD-formulas to attribute implications, models and entailment of AD-formulas, non-redundant bases, and computation of non-redundant bases...|$|R
30|$|The clauses in a {{sentence}} can be identified {{as the number of}} subjects (nsubj, or nsubjpas) occurred in the Stanford Typed Dependency notations of that sentence. From the <b>dependencies</b> in <b>Table</b> 1, the first sentence (Simple sentence) has one basic clause (Bachchan married), the second sentence (Compound sentence) has two basic clauses (Bachchan joined and Bachchan became) and the third sentence (Complex sentence) has two clauses (Bachchan born and Bachchan actor). Therefore, we can identify the simple sentence easily that has only one clause (one nsubj, or nsubjpass).|$|R
40|$|This {{article was}} {{published}} in the journal, Proceedings of ICE, Structures and Buildings [© Institution of Civil Engineers / Thomas Telford] and the definitive version is available at: [URL] construction process is traditionally planned either directly with bar charts, or with network analysis techniques forming the basis of the bar charts. The success of these approaches in construction planning over the years has led to their extensive use in the planning of design. Network analysis techniques and bar charts were developed specifically to plan production processes, such as construction, that have an easily definable logic and are sequential in nature. Design, however, is an iterative processes requiring assumptions and estimates of information to be made and work to be redone until a satisfactory solution is developed. Network analysis is not therefore an appropriate basis for planning design. They cannot account for this iterative nature, they monitor progress based upon the completion of drawing work and other design deliverables and are inappropriate for monitoring the availability of key pieces of information. The Analytical Design Planning Technique (ADePT), shown schematically in figure 1, offers an approach to planning design that accounts for the necessity to undertake work in an iterative manner, enables work to be monitored {{on the basis of the}} production of information, and allows design to be fully integrated with the overall construction process 1. The first stage of the ADePT methodology is a model of the detailed stage of the building design process, representing design activities and their information requirements. The data in this model is linked via a <b>dependency</b> <b>table</b> to a Dependency Structure Matrix (DSM) analysis tool 2 which is used in the second stage to identify iteration within the design process and arrange the activities with the objective of optimising the task order. The third stage of the methodology produces design programmes based on the optimised process sequence. The technique requires some iteration between the DSM and programming stages. The authors have developed computer tools to enable each stage to be undertaken in an efficient manner and thus, facilitate more effective planning and management of building design 3. This paper reviews current problems in design planning within the construction industry and the use of a Dependency Structure Matrix tool to order the detailed design process. It then describes in detail the representation of the optimal design sequence within a programme and the integration of the optimised design programme with procurement and construction programmes...|$|E
40|$|A package {{written in}} C for identif~’ingvariable <b>dependencies</b> in <b>tables</b> of data is presented. Thekey {{ingredient}} is the 8 -test, which establishes dependency structures by exploiting theproperties of continuous functions using conditional probabilities formed {{out of the}} data. The method estimates most relevantvariables, embedding dimensions and noise levels. The program, which is self-contained, also includes optional graphical output. PROGRAM SUMMARY Memory required to execute with typical data: 170 k words for a data file of 4000 lines lUle ofprogram: Delta 2. 0 No. of bits in a word: 3...|$|R
5000|$|Fourth {{normal form}} (4NF) {{is a normal}} form used in {{database}} normalization. Introduced by Ronald Fagin in 1977, 4NF is {{the next level of}} normalization after Boyce-Codd normal form (BCNF). Whereas the second, third, and Boyce-Codd normal forms are concerned with functional dependencies, 4NF is concerned with a more general type of dependency known as a multivalued <b>dependency.</b> A <b>table</b> is in 4NF if and only if, for every one of its non-trivial multivalued dependencies X [...] Y, X is a superkey—that is, X is either a candidate key or a superset thereof.|$|R
50|$|Fifth {{normal form}} (5NF), {{also known as}} project-join normal form (PJ/NF) is a level of {{database}} normalization designed to reduce redundancy in relational databases recording multi-valued facts by isolating semantically related multiple relationships. A table {{is said to be}} in the 5NF if and only if every non-trivial join <b>dependency</b> in that <b>table</b> is implied by the candidate keys.|$|R
40|$|In {{this paper}} we {{represent}} the relational database as a directed graph, regarding tables as nodes and foreign keys as edges. We also make several definitions {{to describe the}} <b>dependencies</b> between <b>tables.</b> Then algorithms are proposed for identifying all tables dependent upon a certain table and ordering them in a specific way so every table to appear before all tables that depend {{on it in the}} sequence. After that we define dependencies between records and represent a set of records also as a graph. In the end several applications of the algorithms are shown as well as practical problems that could be solved by them...|$|R
40|$|One of {{the main}} {{limitations}} for the functional scalability of automated design systems is the representation used for encoding designs. I argue that generative representations, those which are capable of reusing elements of the encoded design in the translation to the actual artifact, are better suited for automated design because reuse of building blocks captures some design dependencies and improves {{the ability to make}} large changes in design space. To support this argument I compare a generative and a nongenerative representation on a table-design problem and find that designs evolved with the generative representation have higher fitness and a more regular structure. Additionally the generative representation was found to capture better the height <b>dependency</b> between <b>table</b> legs and also produced a wider range of table designs. ...|$|R
5000|$|Extensibility. Dimensional {{models are}} {{scalable}} and easily accommodate unexpected new data. Existing tables {{can be changed}} in place either by simply adding new data rows into the table or executing SQL alter table commands. No queries or applications that sit {{on top of the}} data warehouse need to be reprogrammed to accommodate changes. Old queries and applications continue to run without yielding different results. But in normalized models each modification should be considered carefully, because of the complex <b>dependencies</b> between database <b>tables.</b>|$|R
40|$|Abstract. This paper {{analyzes}} {{issues which}} appear when supporting pruning op-erations in tabled LP. A {{version of the}} once/ 1 control predicate tailored for tabled predicates is presented, and an implementation is discussed and evaluated. The use of once/ 1 with answer-on-demand strategies {{makes it possible to}} avoid computing unneeded solutions for problems which can benefit from tabled LP but in which only a single solution is needed, such as model checking and planning. The pro-posed version of once/ 1 is also directly applicable to the efficient implementation of other optimizations, such as early completion, cut-fail loops (to, e. g., perform pruning at the toplevel), if-then-else, and constraint-based branch-and-bound op-timization. Although once/ 1 still presents open issues such as <b>dependencies</b> of <b>tabled</b> solutions on program history, our experimental evaluation confirms that it provides an arbitrarily large efficiency improvement in several application areas...|$|R
40|$|A package {{written in}} C for {{identifying}} variable <b>dependencies</b> in <b>tables</b> of data is presented. The key ingredient is the ffi -test, which establishes dependency structures by exploiting {{the properties of}} continuous functions using conditional probabilities formed out of the data. The method estimates most relevant variables, embedding dimensions and noise levels. The program, which is self-contained, also includes optional graphical output. 1 pihong@thep. lu. se 2 carsten@thep. lu. se PROGRAM SUMMARY Title of Program: Delta 2. 0 Catalogue number: Program obtainable from: anonymous ftp at thep. lu. se in directory pub/LundPrograms [...] delta. tar. Z and delta. readme respectively. Computer for which the programme is designed: DEC Alpha, DECstation, SUN, NeXT, VAX, IBM, Hewlett-Packard, and others with a C compiler Computer: DEC Alpha 3000; installation: Department of Theoretical Physics, University of Lund, Lund, Sweden Operating system: DEC OSF 1. 3 Program language used: ANSI C High speed [...] ...|$|R
40|$|In {{this paper}} we {{consider}} incomplete tables over some attribute set {{and discuss the}} recovery problem under functional <b>dependencies.</b> An incomplete <b>table</b> is introduced as a table over an attribute set such that each table entry is {{a subset of the}} attribute domain instead of just a single value. The recovery problem we consider is to extend the given incomplete table so that each table entry contains only one value and the resulting table will be consistent with a given set of functional dependencies. We show that the recovery problem for incomplete tables such that each table entry is finite is NP-complete. We also give some observation on the unique recoverability. Furthermore, we consider timevariant tables. The recovery problem for time-variaut tables is shown to be PSPACE-complete...|$|R
40|$|Access and {{exchange}} of building {{information in a}} timely and standard manner between construction project teams and applications during design and construction {{plays a major role}} in assuring efficient building energy simulation during retrofitting. Yet, in spite of several efforts in major technological developments in information management, building data and information remains highly fragmented and frequently inaccessible to project teams. This paper presents a table for categorization of information exchanges items and illustrates their dependencies between internal process activities, and between activities of different processes. The paper uses the HVAC design process as a baseline to map-out the information input-and-output relationships of various activities and shows how an information item produced by an activity is consumed by another, thus creating information exchanges <b>dependency.</b> The <b>table</b> can serve as a reference point for project teams and for software developers to understand interoperability requirements of process activities and applications. It will also be beneficial to analyze impact of changes as change to an information exchanges item can impact on other activities where it is required as an input. The paper concludes by formulating a roadmap to utilize the table in order to support development of Information Delivery Manual (IDM) and other information exchanges initiatives. © 2013 American Society of Civil Engineers. link_to_OA_fulltex...|$|R
40|$|Haskell's type {{system with}} multi-parameter {{constructor}} classes and functional dependencies allows static (compile-time) computations {{to be expressed}} by logic programming {{on the level of}} types. This emergent capability has been exploited for instance to model arbitrary-length tuples (heterogeneous lists), extensible records, functions with variable length argument lists, and (homogenous) lists of statically fixed length (vectors). We explain how type-level programming can be exploited to define a strongly-typed model of relational databases and operations on them. In particular, we present a strongly typed embedding of a significant subset of SQL in Haskell. In this model, meta-data is represented by type-level entities that guard the semantic correctness of database operations at compile time. Apart from the standard relational database operations, such as selection and join, we model functional <b>dependencies</b> (among <b>table</b> attributes), normal forms, and operations for database transformation. We show how functional dependency information can be represented at the type level, and can be transported through operations. This means that type inference statically computes functional dependencies on the result from those on the arguments. Our model shows that Haskell can be used to design and prototype typed languages for designing, programming, and transforming relational databasesFundação para a Ciência e a Tecnologia (FCT) - POSI/ICHS/ 44304 / 2002; SFRH/BPD/ 11609 / 2002...|$|R
40|$|Includes bibliographical {{references}} (leaves 78 - 79). A {{reverse engineering}} method is presented and implemented {{in this work}} to extract a conceptual schema from a relational database. In this work we extend the extraction method presented in Andersson's paper entitled "Extracting an entity relationship schema from a relational database through reverse engineering". Such a method depends on {{the analysis of the}} database extension as well as the analysis of data manipulating statements, SQL statements, in the source code of a database application that uses a Relational Database Management System. Attributes representing references between tables in the relational schema are determined by an analysis of join conditions in all queries of the database. Candidate keys are determined {{with the help of the}} referencing attributes in their corresponding relations. The analysis of the database extension will determine whether the proposed keys are actually primary keys or not. Also, database extension helps in determining the inclusion <b>dependencies</b> between <b>tables.</b> When ambiguity occurs concerning candidate keys, users can interact with the system to clarity such ambiguity. This approach makes it possible to efficiently construct a conceptual schema, ERC+, from only elementary information. 1 bound copy: ix, 81 leaves; ill.; 30 cm. available at RNL...|$|R
40|$|With {{the rapid}} growth in {{wireless}} technologies and the cost effectiveness in deploying wireless networks, wireless computers are quickly becoming the normal front-end devices for accessing enterprise data. In this paper, we are addressing the issue of efficient delivery of business-critical data {{in the form of}} summary tables to wireless clients equipped with OLAP front-end tools. Towards this, we propose a new heuristic, on-demand scheduling algorithm, called STOBS-ff, that aggregates requests and broadcasts the results only once to all clients. STOBS-ff exploits the structural <b>dependencies</b> among summary <b>tables</b> to maximize data sharing based on aggregation and does not assume fixed length or uniform broadcasts. The effectiveness of our proposed heuristic was evaluated experimentally using simulation with respect to both access time and fairness, as well as power consumption in the case of mobile clients...|$|R
40|$|We study {{functional}} and multivalued <b>dependencies</b> over SQL <b>tables</b> with NOT NULL constraints. Under a no-information interpretation of null values we develop tools for reason-ing. We further {{show that in}} the absence of NOT NULL constraints the associated implication problem is equivalent to that in propositional fragments of Priest’s paraconsistent Logic of Paradox. Subsequently, we extend the equivalence to Boolean dependencies and to the presence of NOT NULL constraints using Schaerf and Cadoli’s S- 3 logics where S corresponds to the set of attributes declared NOT NULL. The findings also apply to Codd’s interpretation “value at present unknown”utilizing a weak possible world semantics. Our results establish NOT NULL constraints as an effective mechanism to balance the expressiveness and tractability of consequence relations, and to control the degree by which the existing classical theory of data dependencies can be soundly approximated in practice...|$|R
40|$|A general {{method is}} discussed, the ffi -test, which {{establishes}} functional <b>dependencies</b> given a <b>table</b> of measurements. The approach {{is based on}} calculating conditional probabilities from data densities. Imposing the requirement of continuity of the underlying function the obtained values of the conditional probabilities carry information on the variable dependencies. The power of the method is illustrated on synthetic timeseries with different time-lag dependencies and noise levels. For N data points the computational demand is N 2. Also, the same method is used for estimating nonlinear regression errors and their distributions without performing regression. Comparing the predicted residual errors with those from linear models provides a signal for nonlinearity. The virtue of the method {{in the context of}} feedforward neural networks is stressed with respect to preprocessing data and tracking residual errors. 1 carsten@thep. lu. se Motivation Successful regression of a system given [...] ...|$|R
40|$|Most of the {{probabilistic}} database models proposed so far represent probabilistic {{information by}} attaching the existence probability of each tuple to the tuple. However, these models {{are not able}} to represent dependency among tuples such as "at least one tuple of two or more tuples must exist in a database. " In this paper, we propose a probabilistic database model which has the ability to represent such dependency. As a basic framework of our model, we adopt conditional tables introduced by Imielinski and Lipski, which have been proposed as a model for incomplete information. Furthermore, we consider the problem of computing the existence probability of a given tuple over our proposed model and discuss its computational complexity. Several conditions which make the problem solvable efficiently are also provided. Keywords: probabilistic database, incomplete information, conditional <b>table,</b> <b>dependency,</b> probabilistic query, complexity. 1. INTRODUCTION A conventional relational database [...] ...|$|R
40|$|A tabled {{evaluation}} of a logic program may have states in which answers are consumed {{by more than one}} computation path. Such a situation may arise due to cyclic <b>dependencies</b> among various <b>tabled</b> subgoals. Accordingly, an implementation of tabling must maintain environments for multiple computation paths, and support the suspension and resumption of such computation paths. As noted in [2], the SLG-WAM, accomplishes this by sharing the WAM Local Stack, Global Stack, Trail, and Choice Point Stack. When tabling is used, each of these stacks represents a tree of computations, and freeze registers are added to each stack to protect information in suspended computation paths from being overwritten and lost. In addition the SLG-WAM maintains a forward trail, which contains the values to which conditional variables (i. e. variables created before the youngest choice point) are bound, along with the addresses of these variables. Suspending a node n 1 and resuming a node n 2 is...|$|R
40|$|Abstract. Codd {{tables are}} {{databases}} that can carry Codd’s null “value unknown at present ” in columns that are specified as NULL. Under Lev-ene and Loizou’s possible world semantics we investigate the combined class of uniqueness constraints and functional <b>dependencies</b> over Codd <b>tables.</b> We characterize the implication problem of this class axiomati-cally, logically and algorithmically. Since {{the interaction of}} members in this class is intricate data engineers can benefit from concise sample ta-bles. Therefore, we investigate structural and computational properties of Armstrong tables. These are Codd tables that satisfy {{the consequences of a}} given set of elements in our class and violate all those elements that are not consequences. We characterize when a given Codd table is an Armstrong table for any given set of our class. From this result we es-tablish an algorithm that computes an Armstrong table in time that is at most quadratic in the number of rows in a minimum-sized Armstrong table. Data engineers can use our Armstrong tables to judge, justify, convey and test their understanding of the application domain. ...|$|R
40|$|We {{investigate}} the implication problem for classes of data <b>dependencies</b> over SQL <b>table</b> definitions. Under Zaniolo’s “no information ” interpretation of null markers we establish an axiomatization and algorithms {{to decide the}} implication problem for the combined class of functional and multivalued dependencies {{in the presence of}} NOT NULL constraints. The resulting theory subsumes three previously orthogonal frame-works. We further show that the implication problem of this class is equivalent to that in a propositional fragment of Cadoli and Schaerf ’s family of para-consistent S- 3 logics. In particular, S is the set of variables that correspond to attributes declared NOT NULL. We also show how our equivalences for multivalued de-pendencies can be extended to Delobel’s class of full first-order hierarchical decompositions, and the equiva-lences for functional dependencies can be extended to arbitrary Boolean dependencies. These dualities allow us to transfer several findings from the propositional fragments to the corresponding classes of data depen-dencies, and vice versa. We show that our results also apply to Codd’s null interpretation “value unknown at present”, but not to Imielinski’s or-relations utilizing Levene and Loizou’s weak possible world semantics. Our findings establish NOT NULL constraints as an effective mechanism to balance not only the certainty in database relations but also the expressiveness with the efficiency of entailment relations. They also con-trol the degree by which the implication of data dependencies over total relations is soundly approximate...|$|R
40|$|Machine {{learning}} (ML) over {{relational data}} is a booming {{area of the}} database industry and academia. While several projects aim to build scalable and fast ML systems, little work has addressed the pains of sourcing data and features for ML tasks. Real-world relational databases typically have many tables (often, dozens) and data scientists often struggle to even obtain and join all possible tables that provide features for ML. In this context, Kumar et al. showed recently that key-foreign key <b>dependencies</b> (KFKDs) between <b>tables</b> often lets us avoid such joins without significantly affecting prediction accuracy [...] an idea they called avoiding joins safely. While initially controversial, this idea has since been used by multiple companies to reduce the burden of data sourcing for ML. But their work applied only to linear classifiers. In this work, we verify if their results hold for three popular complex classifiers: decision trees, SVMs, and ANNs. We conduct an extensive experimental study using both real-world datasets and simulations to analyze the effects of avoiding KFK joins on such models. Our results show that these high-capacity classifiers are surprisingly and counter-intuitively more robust to avoiding KFK joins compared to linear classifiers, refuting an intuition from the prior work's analysis. We explain this behavior intuitively and identify open questions {{at the intersection of}} data management and ML theoretical research. All of our code and datasets are available for download from [URL] 14 page...|$|R
40|$|The cross-national empirics of the {{international}} asylum system are in their infancy. While Hatton, 2009, and Neumayer, 2005, 2006 a and 2006 b provided important and valuable cross-national insights on the drivers of the asylum seeking process, as yet little is known in terms of hard-core evidence {{about the effects of}} asylum-driven migration processes on the recipient countries. But such analyses are necessary, since asylum plays such {{an important role in the}} overall South-North migration process, and several international decision makers, especially on the European level, are increasingly stressing the necessity to get asylum seekers into employment, while others – like the Austrian Ministry of the Interior in its long-term strategy, published in 2012 – vehemently argue in favour of a clear separation between legal, employment-related migration and asylum. Will ‘getting asylum seekers into employment’ have any effects on social and economic development, or will it motivate more and more people to emigrate for work as “free riders” of the asylum system? This paper should preliminarily attempt to close this widening and politically highly relevant research gap. The EU's total population was 502. 5 million, with a yearly increase of 0. 5 million due to natural population increase and 0. 9 million due to net migration. While the European Union accepts about 2. 4 million immigrants per year from third countries, among them more than 800. 000 people in the framework of work visas, and more than 750. 000 people under the title of family reunifications, 260. 000 to 300. 000 people apply for asylum each year. All official European Commission data, surveyed in this article, seem to suggest that asylum and illegal migration are part and parcel of the overall migration process. While on average ¾ of the asylum applications in Europe are being rejected by the authorities as unfounded, there was a stock of up to 4. 5 million illegal residents already residing in the entire EU- 27; and in addition, we can assume that around 450. 000 illegal entrants are apprehended each year at the EU external borders. The illegal inflow and shadow economy migration statistics also have to take into account the around 340. 000 persons, denied entry each year, suggesting that the overall shadow migration pressure, resulting from unfounded asylum and illegal or rejected entries amounts to more than 1 million people each year, by far exceeding the 800. 000 work visas granted annually. Thus there is an urgent political need to act. The somewhat surprising, but undisputable net end result of all these European immigration procedures (work visas, family re-unifications, and other migration) up to now was a sharp and clear-cut rise in the total stock of the resident population in Europe from only three countries: Turkey (approx. 2. 4 million), Morocco (approx. 1. 8 million) and Albania (approx. 1 million). They are the absolute winners of the hitherto existing de-facto European migration ‘policy’. The combined size of illegal border crossings, denied entry applications, and rejected asylum applications of more than 1. 0 million persons seems to suggest that indeed there exists a huge migration-related shadow economy (Graphs 1 - 3). Our ensuing data analysis is based on the tradition of cross-national development accounting, using an expanded version of the Tausch, 2012 b data set (“Corvinus University data set”) and UNDP, 2009 and UNHCR, 2012 figures on migration. We start these empirical cross-national analyses by providing some calculations about the societal effects of the well-known Migration Policy Index, which measures the general institutional ease with which migration recipient countries integrate migrants economically in general. Our calculations reconfirm the reservations by the present author (Tausch, 2010, 2012) against the generalized neo-liberal thesis that a free migration process automatically ensures economic prosperity. With the level of development and the overall conditions of the migration process being constant, there are some very serious and significant negative partial correlations of the MIPEX Index with indicators of political participation and the fight against discrimination. Our data also show the significant pull-factors, caused by an open migration regime, as measured by the MIPEX Index, as well as the societal consequences of a high MIPEX Index score - growing xenophobia against the weakest groups in society - such as the Roma and Sintis, an ensuing growing public debt burden, and lower economic growth. One might still argue that, on ethical grounds, one should be still in favour of increasing MIPEX index performance, but in terms of its societal consequences, our results suggest to be pessimistic. We then move on to analyse systematically the effects of the UNDP cross-national migration variables on socio-economic development and vice versa. Our hypothesis is that opening the gates of unlimited access of asylum seekers to the labour market an even more substantial number of people would decide to enter the labour markets in the developed countries in Europe via the asylum procedure, thus thwarting any attempts to arrive at a more education and skill oriented immigration system. We try to corroborate this by first looking into the question of the relationship between access liberalization, measured by the MIPEX Index, and the UNDP documented asylum burden rate (Graph 1 and 2). Although the relationship is not too strong, there are some positive trade-offs between the two variables. In Table 3 of this study, we then provide a very clear-cut argument on how a migration policy, based on asylum influx, is ill-conceived, and several important phenomena are significantly being undermined - internal security, the balance of tolerance in society, gender relations, education, and environmental conditions. Our partial correlation analysis shows that with increasing dependence on the immigration system based on the influx of asylum seekers, there is a significantly larger societal acceptance of the value orientation that men have precedence on the labour market over women when jobs are scarce; and in addition, the import of polluting goods and raw materials; maternal mortality, terrorist attacks, and the violations of civil rights and political rights increase, independent from the development level reached and the general conditions of the migration process being in place. The near bankruptcy of the current de facto existing European asylum-based migration policy is also reflected in Table 4 of this study – documenting the partial correlations of asylum seekers per head of population with processes of socio-economic development. Again, the level of development and general overall conditions of the migration process were held constant. Crime rates, macho values, and the terrorist threat increase significantly, while fiscal freedom, growth prospects in the current crisis and the employment of older workers are being curtailed, and important areas of environmental policy, measured by the Yale-Columbia environment policy data series, are again being negatively affected. In addition, also the World Values Survey data on the work ethics of society are negatively being affected by an asylum-based migration system. Table 5 then documents the positive effects of work permit requirements for asylum seekers, still in place in several European countries and documented by the European Commission/Europäische Kommission (2012), on various socio-economic indicators from the Tausch 2012 b Corvinus data set, including environment data, economic growth, education, and World Values Survey indicators of tolerance and volunteer activities. Social security, growth, environmental policy, education, health, liberal values in society - all these are positively affected by a work permit regime for asylum seekers in Europe, which the European Commission seems to be inclined to abolish. Table 6 shows the sobering results of the determinants of average economic growth rates in the EU- 27 in the era of the current world economic crisis, 2008 to 2011. The crisis hit the poorer EU countries - ceteris paribus - far harder than the richer countries, and immigration rates are a significant negative determinant of growth, while the work permits regime for asylum seekers significantly and positively affects economic growth. Table 7 shows our final estimates of the determinants of asylum burden rates in the world system. In addition to the famous "bell-curve" of the levels of development, private health expenditures and the military personnel rates are significant drivers of asylum burden rates, while we also show that dependency from the large transnational corporations (measured by UNCTAD data on MNC penetration and its rise over time) are conducive to such higher asylum burden rates. Thus, we can show that traditional quantitative approaches to international development, initiated by the Swiss sociologist Volker Bornschier, which are based on UNCTAD data on MNC penetration and its rise over time, explain well contemporary social asylum process realities of the world today. By contrast, an employment policy favouring the employment rates of older workers generally deters higher asylum <b>dependency</b> ratios. In <b>Tables</b> 8 a and 8 b, we finally show bivariate and partial correlations of asylum procedure global recognition rates, as documented by the UNHCR for 2010, and key variables of socio-economic development, as documented in Tausch, 2012 a, 2012 b. Our results again would caution against an asylum-based or asylum-driven immigration policy. We conclude by saying that the European Commission would be well advised to seek to redistribute current asylum inflows from countries like Germany, France, Netherlands, Sweden, and Austria to other EU-member countries, thus providing more fairness in the current Schengen system. Doubling or even tripling the European numbers of legal work permits would also be an advisable strategy, and Europe should seriously consider the new Austrian migration procedure for third-country nationals (Red-White-Red-card) as a best practice model. ...|$|R

