36|70|Public
5000|$|Scripting {{languages}} {{to build}} static reports and for filtering <b>diagnostic</b> <b>output</b> ...|$|E
50|$|Originally, IKE had {{numerous}} configuration options but lacked a general facility for automatic negotiation of a well-known default case that is universally implemented. Consequently, {{both sides of}} an IKE had to exactly agree {{on the type of}} security association they wanted to create - option by option - or a connection could not be established. Further complications arose from the fact that in many implementations the debug output was difficult to interpret, if there was any facility to produce <b>diagnostic</b> <b>output</b> at all.|$|E
40|$|Consumer need is the {{dominant}} force in successful technological innovation. This paper presents a consumer analysis methodology that provides R&D management with diagnostic information to evaluate and improve new technology. Consumer theory, measurement, model estimation, <b>diagnostic</b> <b>output,</b> and managerial actions are all illustrated by application to telecommunications innovation...|$|E
40|$|Qualified for {{automotive}} applications Output overvoltage (short-to-battery) protection up to 18 V Short-to-battery output flag for wire <b>diagnostics</b> <b>Output</b> short-to-ground protection Fifth-order, low-pass video filter 0. 1 dB flatness to 3 MHz − 3 dB bandwidth of 10 MHz 45 dB rejection at 27 MHz Ultralow power-down current: 13. 5 µA typical Low {{quiescent current}} 7. 6 mA typical (ADA 4432 - 1) 13. 2 mA typical (ADA 4433 - 1...|$|R
40|$|The {{subject of}} my final thesis is the {{assessment}} centre {{as a method}} being used in practice of personnel psychology. The theoretical part is aimed at basic methods of assessment centre, it contains the definitions and instructions which limit and edit the assessment centre as regards its content and implementation aspects. It describes the processes of assessment centre creation and choice of its partial methods (dialogue, test methods, model situation etc). In more detail it deals with character of assessment centre partial methods and possibilities of their <b>diagnostic</b> <b>outputs</b> application. In particular chapter special attention is given to model situations and group exercises questions. At {{the same time it}} describes the assessment centre social-psychological aspects and its cultural and intercultural extent. The empirical part contains the description and analysis of particular existing assessment centre, from which a suggestion for its optimalization arises. Key words assessment center group exercises simulation...|$|R
40|$|The article {{describes}} alternative methods of fuel consumption measurement based on model with using the <b>diagnostic</b> <b>outputs</b> of engine control unit. On-board diagnosis (the second level, known as OBD- 2) has been mandated by government regulation because of advanced damage control systems in newer cars. However, its signals {{can be used}} for accurate analyses of power or torque measurement. On-board diagnostics offers many various parameters such a spark advance, intake air temperature, coolant temperature, throttle position, air flow mass and so on. Many of them have been unavailable without using sophisticated and expensive instrumentation. In the article are described two ways of fuel consumption measuring which are based on intake air consumption and knowledge about air-fuel ratio. First of them is founded on voltage output of oxygen sensor, the second on short (long) term fuel trim. As is shown at the end the second way gives more accurately results...|$|R
40|$|We have {{performed}} an initial {{assessment of the}} sensitivity of various expected ignition diagnostic signatures to ignition failure modes using one and two-dimensional hydrodynamics simulations and post-processed simulated <b>diagnostic</b> <b>output.</b> As {{a result of this}} assessment, we recommend several changes to the current requirements for the ignition diagnostic suite. These recommendations are summarized in Table 1...|$|E
40|$|High {{throughput}} screening {{technology has}} the advantage that it enables researchers to perform extensive experiments. A drawback is that complex data sets are generated that require special processing. In this paper we describe a general framework within which this information processing problem can be addressed. We also propose a speci®c implementation for a diagnostic domain. In this implementation, the {{emphasis is placed on}} providing easily understandable explanations of the <b>diagnostic</b> <b>output,</b> without compromising diagnostic accuracy. The approach is illustrate...|$|E
40|$|Measurement (ARM) Program {{research}} at the Goddard Institute for Space Studies (GISS) are 1) to improve and validate the radiation parameterizations in the GISS general circulation model (GCM) through model intercomparisons with line-by-line calculations and through comparisons with ARM observations, 2) to improve the GCM <b>diagnostic</b> <b>output</b> to enable more effective comparisons to global cloud/radiation data sets, and 3) to use ARM Cloud and Radiation Testbed (CART) data to develop improved parameterization of clouds in the GCM and to study the interaction of dynamics and radiation...|$|E
40|$|The NATA code is a {{computer}} program for calculating quasi-one-dimensional gas flow in axisymmetric nozzles and rectangular channels, primarily to describe conditions in electric archeated wind tunnels. The program provides solutions based on frozen chemistry, chemical equilibrium, and nonequilibrium flow with finite reaction rates. The shear and heat flux on the nozzle wall are calculated and boundary layer displacement effects on the inviscid flow are taken into account. The program contains compiled-in thermochemical, chemical kinetic and transport cross section data for high-temperature air, CO 2 -N 2 -Ar mixtures, helium, and argon. It calculates stagnation conditions on axisymmetric or two-dimensional models and conditions on the flat surface of a blunt wedge. Included in the report are: definitions of the inputs and outputs; precoded data on gas models, reactions, thermodynamic and transport properties of species, and nozzle geometries; explanations of <b>diagnostic</b> <b>outputs</b> and code abort conditions; test problems; and a user's manual for an auxiliary program (NOZFIT) used to set up analytical curvefits to nozzle profiles...|$|R
40|$|A {{computer}} program performs level- 1 B processing (the term 1 B is explained below) {{of data from}} observations of the limb of the Earth by the Earth Observing System (EOS) Microwave Limb Sounder (MLS), which is an instrument aboard the Aura spacecraft. This software accepts, as input, the raw EOS MLS scientific and engineering data and the Aura spacecraft ephemeris and attitude data. Its output consists of calibrated instrument radiances and associated engineering and diagnostic data. [This software {{is one of several}} {{computer program}}s, denoted product generation executives (PGEs), for processing EOS MLS data. Starting from level 0 (representing the aforementioned raw data, the PGEs and their data products are denoted by alphanumeric labels (e. g., 1 B and 2) that signify the successive stages of processing. ] At the time of this reporting, this software is at version 2. 2 and incorporates improvements over a prior version that make the code more robust, improve calibration, provide more <b>diagnostic</b> <b>outputs,</b> improve the interface with the Level 2 PGE, and effect a 15 -percent reduction in file sizes by use of data compression...|$|R
40|$|Cryptovirological augmentations {{present an}} immediate, {{incomparable}} threat. Over the last decade, the substantial proliferation of crypto-ransomware has had widespread consequences for consumers and organisations alike. Established preventive measures perform well, however, {{the problem has}} not ceased. Reverse engineering potentially malicious software is a cumbersome task due to platform eccentricities and obfuscated transmutation mechanisms, hence requiring smarter, more efficient detection strategies. The following manuscript presents a novel approach for the classification of cryptographic primitives in compiled binary executables using deep learning. The model blueprint, a DCNN, is fittingly configured to learn from variable-length control flow <b>diagnostics</b> <b>output</b> from a dynamic trace. To rival the size and variability of contemporary data compendiums, hence feeding the model cognition, a methodology for the procedural generation of synthetic cryptographic binaries is defined, utilising core primitives from OpenSSL with multivariate obfuscation, to draw a vastly scalable distribution. The library, CryptoKnight, rendered an algorithmic pool of AES, RC 4, Blowfish, MD 5 and RSA to synthesis combinable variants which are automatically fed in its core model. Converging at 91 % accuracy, CryptoKnight is successfully able to classify the sample algorithms with minimal loss. Comment: 9 Pages, 6 Figure...|$|R
40|$|Implementation of the tridiagonal {{reduction}} {{method for}} real eigenvalue extraction in structural vibration and buckling problems is described. The basic concepts underlying the method are summarized and special features, {{such as the}} computation of error bounds and default modes of operation are discussed. In addition, the new user information and error messages and optional <b>diagnostic</b> <b>output</b> relating to the tridiagonal reduction method are presented. Some numerical results and initial experiences relating to usage in the NASTRAN environment are provided, including comparisons with other existing NASTRAN eigenvalue methods...|$|E
40|$|The {{increasing}} use of high-current brushless dc (BLDC) three-phase motors puts stringent {{requirements on}} the drive electronics for function and safety. A new threephase motor controller has integrated all the circuitry to control the six power NMOS FETs for fractional horsepower motors up to 50 V. The basic techniques of fixed off-time PWM current control and bootstrapped high-side gate drives are enhanced by adding synchronous rectification control, cross-conduction protection and on-chip charge pumps to allow 100 % PWM duty cycles. Safety features and <b>diagnostic</b> <b>output</b> prevent inappropriate switching of the power FETs and allow programmable motor spin-down on power loss. This paper presents the system concepts and describes th...|$|E
40|$|Aim of the study: Fine-needle {{aspiration}} biopsy (FNAB) is {{the most}} accurate and cost-effective method to evaluate the risk of malignancy of thyroid nodules, but approximately 1 – 24 % of FNABs generate a nondiagnostic result (ND-FNAB). The {{aim of this study}} was to determine the predictive factors of a repeated nondiagnostic result of FNAB. Material and methods : A total of 4018 FNABs performed in a territorial referral centre were analysed, of which 288 (7. 17 %) were nondiagnostic. Medical records were available for 245 biopsies performed in 228 patients. The retrospective analysis of factors that may influence a repeat ND-FNAB, including demographic, clinical and ultrasound characteristics, was performed. Results : A repeat FNAB was performed in 159 nodules giving a diagnostic result in 79. 2 % of cases. The time between the biopsies ranged from 1 to 611 days (mean 154. 4, median 119). The timing of a repeat FNAB did not significantly alter the <b>diagnostic</b> <b>output</b> (p = 0. 29). In the univariate analysis, significant predictors of a repeat ND-FNAB were older patient age (p = 0. 02), L-thyroxine supplementation (p = 0. 05), and a history of 131 I therapy (p < 0. 0001). In the multivariate analysis, only a history of 131 I therapy was a statistically significant risk factor for a repeat ND-FNAB (p = 0. 002). Conclusions : Patients with a history of 131 I therapy and ND-FNAB should undergo periodic ultrasonographic assessment rather than a repeat biopsy. The interval between repeated FNABs recommended by guidelines does not affect the <b>diagnostic</b> <b>output...</b>|$|E
40|$|The National Ignition Campaign (NIC) uses non-igniting 'THD' {{capsules}} {{to study}} and optimize the hydrodynamic assembly of the fuel without burn. These capsules are designed to simultaneously reduce DT neutron yield and to maintain hydrodynamic similarity with the DT ignition capsule. We will discuss nominal THD performance and the associated experimental observables. We will show the results of large ensembles of numerical simulations of THD and DT implosions and their simulated <b>diagnostic</b> <b>outputs.</b> These simulations cover {{a broad range of}} both nominal and off nominal implosions. We will focus on the development of an experimental implosion performance metric called the experimental ignition threshold factor (ITFX). We will discuss the relationship between ITFX and other integrated performance metrics, including the ignition threshold factor (ITF), the generalized Lawson criterion (GLC), and the hot spot pressure (HSP). We will then consider the experimental results of the recent NIC THD campaign. We will show that we can observe the key quantities for producing a measured ITFX and for inferring the other performance metrics. We will discuss trends in the experimental data, improvement in ITFX, and briefly the upcoming tuning campaign aimed at taking the next steps in performance improvement on the path to ignition on NIF...|$|R
40|$|This {{research}} concerns automation {{of qualitative}} analysis of human motion in sports, using {{a novel approach}} related to assessment and diagnostics, which is required to provide a general user with coaching experience in next generations of motion capture video games or sport coaching software. The research comprises a framework {{hereinafter referred to as}} augmented coaching systems (ACS) and its critical components. In contrast to formative assessing of knowledge of results, which is based on predefined objective criteria, a qualitative approach to assessing knowledge of performance is linked to the questions: (1) Can qualitative assessment be automated?; (2) If so, how can such assessment be communicated from a machine to a human?; and (3) Can qualitative assessment automation be similar to human implicit, multifaceted, empirical, evolving and subjective criteria? An investigative development approach was used for automating human motion assessment. The assessment of qualitative nature incorporated a mix of objectives – such as subjective, objective, and flexible pre-defined criteria similar to a domain expert or coach. The methods of analysis and machine learning techniques included: learning-by-example from expert’s data; integrative visualisation/replay functionality for qualitative analysis and machine learning modelling; modelling and analysis utilising relatively small and larger unbalanced motion data sets; modular implementation of common-sense descriptive rules mapped to diagnostic outputs; and sub-space modelling and temporal and spatial feature extraction techniques. The introduced ACS framework is generic and it includes a critical analysis applicable to more than one sport discipline. The ACS architecture is modular, extendible and its machine learning system supports global, coaching scenario specific, personalised, evolving, and life-long learning. Using captured motion data sets representing novices towards advanced skill levels in two case studies (golf and tennis), a series of experimental modelling systems integral to ACS were developed for testing and validation using empirical, subjective, and flexible criteria. The results achieved on small and on relatively large unbalanced data sets produced human- intelligible <b>diagnostic</b> <b>outputs</b> in a qualitative fashion. The machine learning <b>diagnostic</b> <b>outputs</b> were similar to those produced by visual assessment of a tennis coach (81 % [...] . 99. 9 %) and to those produced by objective measures from an embedded motion capture system in a golf club, resulting in 89. 5 ± 2. 6 %. Flexible assessment criteria were demonstrated by comparing the two different assessments for tennis swing stances that were based on different subjective criteria operating on the same motion data set using the same assessment system. The ACS framework, and developed software components {{for the next generation of}} intelligent ACS using subjective and flexible criteria, is novel in the field. This thesis has demonstrated that qualitative assessment can be automated, that assessment diagnostics can be communicated from a machine to human and that coaching insights as implicit knowledge can be modelled using connectionist and evolving connectionist approaches...|$|R
50|$|In Workspace menu {{from the}} Main menu the user can create new Workspaces, open an {{existing}} project {{in a new}} window, save the file, activates and deactivates the panels, open workspace recently saved and closes an open project.Tools menu {{is divided into three}} parts: Container for displaying data; Tool Window for charts and data transformation; Options for <b>diagnostic</b> and <b>output</b> options that can be set by user.Window menu offers the following functions: Floating, Tabbed, Tile vertically and Tile horizontally for the type of arrange all windows; Skinning for graphical appearance of Demetra+ and Documents options which offers some additional options for organising windows.|$|R
40|$|The {{principal}} {{objectives of}} the research supported at the Goddard Institute for Space Studies (GISS) by the Atmospheric Radiation Measurement (ARM) Program for a three year period commencing September 1990, were: (1) to improve and validate the radiation parameterizations in the GISS GCM through model intercomparisons with line-by-line calculations and through comparisons with ARM observations; (2) to improve the GISS GCM <b>diagnostic</b> <b>output</b> to enable more effective comparisons to global cloud/radiation data sets; and (3) to use ARM data to develop improved parameterization of clouds in the GCM and to study the interaction of dynamics and radiation. The ARM Program support {{has made it possible}} to establish and support an active and productive research group at GISS specializing in radiative transfer and cloud process modeling in support of improving the performance of a climate GCM...|$|E
40|$|Several {{features}} {{were added to}} COSMIC NASTRAN, along with some enhancements to improve or update existing capabilities. Most of these additions and enhancements were provided by industry users to be incorporated into NASTRAN for wider use. DIAG 48 provides a synopsis of significant developments in past NASTRAN releases (1983 - 1985) and indexes all <b>diagnostic</b> <b>output</b> messages and operation requests (DOMOR). Other features include: volume and surface computation of the 2 -D and 3 -D elements, NOLIN 5 input and; NASTRAN PLOTOPT-N (where N = 2, 3, 4, or 5); shrink element plots; and output scan. A nonprint option on stress and force output request cards was added. Automated find and nofind options on the plot card, fully stressed design, high level plate elements, eigenvalue messages, and upgrading of all FORTRAN source code to the ANSI standard are enhancements made...|$|E
40|$|High {{throughput}} screening {{technology has}} the advantage that it enables researchers to perform extensive experiments. The downside is that complex data sets are generated that require special processing. In this paper we describe a general framework within which this information processing problem can be addressed. We also propose a specific implementation for a diagnostic domain. In this implementation, the {{emphasis is placed on}} providing easily understandable explanations of the <b>diagnostic</b> <b>output</b> without compromising diagnostic accuracy. The approach is illustrated on real and simulated data. 1 Introduction Recently, the Technical University of Delft established an inter-faculty research program to produce an Intelligent Molecular Diagnostic System (IMDS). The IMDS will consist of two basic components: a measurement device and an information processing unit (IPU). The measurement device is a chemical sensor on a chip, which will be capable of rapidly performing vast numbers of measu [...] ...|$|E
40|$|Aircraft engines {{have evolved}} into a highly complex system to meet {{ever-increasing}} demands. The evolution of engine technologies has primarily been driven by fuel efficiency, reliability, as well as engine noise concerns. One {{of the sources of}} engine noise is pressure fluctuations that are induced on the stator vanes. These local pressure fluctuations, once produced, propagate and coalesce with the pressure waves originating elsewhere on the stator to form a spinning pressure pattern. Depending on the duct geometry, air flow, and frequency of fluctuations, these spinning pressure patterns are self-sustaining and result in noise which eventually radiate to the far-field from engine. To investigate the nature of vane pressure fluctuations and the resulting engine noise, unsteady pressure signatures from an array of embedded acoustic sensors are recorded as a part of vane noise source <b>diagnostics.</b> <b>Output</b> time signatures from these sensors are routed to a control and data processing station adding complexity to the system and cable loss to the measured signal. "Smart" wireless sensors have data processing capability at the sensor locations which further increases the potential of wireless sensors. Smart sensors can process measured data locally and transmit only the important information through wireless communication. The aim of this wireless noise telemetry task was to demonstrate a single acoustic sensor wireless link for unsteady pressure measurement, and thus, establish the feasibility of distributed smart sensors scheme for aircraft engine vane surface unsteady pressure data transmission and characterization...|$|R
40|$|Next {{generation}} {{technology of}} integrated health management systems for air-transportation struc-tures will combine different single SHM methods to an overall system with multiple abilities con-sidering {{different stages of}} damage initiation and propagation. The fundamental configuration of the proposed SHM technique will involve {{the idea of an}} integrated passive/active monitoring and di-agnostic system extended by numerical modules for lifetime prediction. The overall system is ca-pable of providing real-time load monitoring and damage estimation on a global structure level as well as precise damage diagnostics on a local level. This robust diagnostic technique provides quantifiable damage location and size estimation that account for the uncertainties induced by the environments or the system itself continuously during flight. Additionally, efficient prediction and prognostic methods are integrated with moni-toring and <b>diagnostic</b> <b>outputs</b> to provide real time estimation of possible damage scenarios, residual strength, and remaining useful life of the dam-aged structure. From this result information is gained which allow appropriate preventative ac-tions on the monitored structure. To achieve those objectives, a built-in sensor/actuator network is employed and numerical simulation methods of damage estimation and propagation are devel-oped and applied. The goal of this work is to inte-grate all these single techniques and subsystems into an integrated structural health management system for composite airframe structures. The system design, data exchange between the dif-ferent subsystems, and the performance of each module is presented. This is an open-access article distributed {{under the terms of the}} Creative Commons Attribution 3. 0 United States Li-cense, which permits unrestricted use, distribution, and re-production in any medium, provided the original author and source are credited. ...|$|R
40|$|This {{software}} is an improvement on Version 2, which {{was described in}} EOS MLS Level 1 B Data Processing, Version 2. 2, NASA Tech Briefs, Vol. 33, No. 5 (May 2009), p. 34. It accepts the EOS MLS Level 0 science/engineering data, and the EOS Aura spacecraft ephemeris/attitude data, and produces calibrated instrument radiances and associated engineering and diagnostic data. This version makes the code more robust, improves calibration, provides more <b>diagnostics</b> <b>outputs,</b> defines the Galactic core more finely, and fixes the equator crossing. The Level 1 processing software manages several different tasks. It qualifies each data quantity using instrument configuration and checksum data, as well as data transmission quality flags. Statistical tests are applied for data quality and reasonableness. The instrument engineering data (e. g., voltages, currents, temperatures, and encoder angles) is calibrated by the software, and the filter channel space reference measurements are interpolated onto the times of each limb measurement with the interpolates being differenced from the measurements. Filter channel calibration target measurements are interpolated onto the times of each limb measurement, and are used to compute radiometric gain. The total signal power is determined and analyzed by each digital autocorrelator spectrometer (DACS) during each data integration. The software converts each DACS data integration from an autocorrelation measurement in the time domain into a spectral measurement in the frequency domain, and estimates separately the spectrally, smoothly varying and spectrally averaged components of the limb port signal arising from antenna emission and scattering effects. Limb radiances are also calibrated...|$|R
40|$|In {{situations}} where the cost/benefit analysis of using physics-based damage propagation algorithms is not favorable and when sufficient test data are available that map out the damage space, one can employ data-driven approaches. In this investigation, we evaluate different algorithms for their suitability in those circumstances. We are interested in assessing the trade-off that arises from the ability to support uncertainty management, and {{the accuracy of the}} predictions. We compare here a Relevance Vector Machine (RVM), Gaussian Process Regression (GPR), and a Neural Network-based approach and employ them on relatively sparse training sets with very high noise content. Results show that while all methods can provide remaining life estimates although different damage estimates of the data (<b>diagnostic</b> <b>output)</b> changes the outcome considerably. In addition, we found {{that there is a need}} for performance metrics that provide a comprehensive and objective assessment of prognostics algorithm performance...|$|E
40|$|We {{discuss the}} use of {{regression}} diagnostics combined with nonlinear least-squares to refine cell parameters from powder diffraction data, presenting a method which minimizes residuals in the experimentally-determined quantity (usually 20 hkt or energy, Ehkt). Regression diagnostics, particularly deletion diagnostics, are invaluable in detection of outliers and influential data which could be deleterious to the regressed results. The usual practice of simple inspection of calculated residuals alone often fails to detect the seriously deleterious outliers in a dataset, because bare residuals provide no information on the leverage (sensitivity) of the datum concerned. The regression diagnostics which predict he change xpected in each cell constant upon deletion of each observation (hkl reflection) are particularly valuable in assessing {{the sensitivity of the}} calculated results to individual reflections. A new computer program, implementing nonlinear egression methods and providing the <b>diagnostic</b> <b>output,</b> is described. I~YWORDS: powder diffraction, regression diagnostics, lattice parameters, computer program...|$|E
40|$|The {{abstraction}} {{of meaningful}} diagnostic information from raw condition monitoring data in domains where diagnostic expertise and knowledge is limited presents a significant research challenge. This paper proposes {{a means of}} abstracting the salient features required to characterize partial discharge (PD) activity detected in oil-filled power transformers. This enables ultra high frequency (UHF) sensor data to be interpreted and translated into a meaningful diagnostic explanation of the observed PD activity. Plant data captured from UHF sensors forms the inputs to a knowledge-based data interpretation system, supporting on-line plant condition assessment and insulation defect diagnosis. The paper describes the functionality of a knowledge-based decision support system, providing engineers with a comprehensive diagnostic explanation of partial discharge activity detected in oil-filled power transformers. The <b>diagnostic</b> <b>output</b> can then be used to advise the engineer in (and potentially automate) the classification and location of partial discharge defect source...|$|E
40|$|Sea surface {{temperature}} (SST) measurements {{are required by}} operational ocean and atmospheric forecasting systems to constrain modeled upper ocean circulation and thermal structure. The Global Ocean Data Assimilation Experiment (GODAE) High Resolution SST Pilot Project (GHRSST-PP) was initiated to address these needs by coordinating the provision of accurate, high-resolution, SST products for the global domain. The pilot project is now complete, but activities continue within the Group for High Resolution SST (GHRSST). The pilot project focused on harmonizing diverse satellite and in situ data streams that were indexed, processed, quality controlled, analyzed, and documented within a Regional/Global Task Sharing (R/GTS) framework implemented in an internationally distributed manner. Data with meaningful error estimates developed within GHRSST are provided by services within R/GTS. Currently, several terabytes of data are processed at international centers daily, creating more than 25 gigabytes of product. Ensemble SST analyses together with anomaly SST outputs are generated each day, providing confidence in SST analyses via <b>diagnostic</b> <b>outputs.</b> <b>Diagnostic</b> data sets are generated and Web interfaces are provided to monitor the quality of observation and analysis products. GHRSST research and development projects continue to tackle problems of instrument calibration, algorithm development, diurnal variability, skin temperature deviation, and validation/verification of GHRSST products. GHRSST also works closely with applications and users, providing a forum for discussion and feedback between SST users and producers on a regular basis. All data within the GHRSST R/GTS framework are freely available. This paper reviews the progress of GHRSST-PP, highlighting achievements that have been fundamental {{to the success of}} the pilot project...|$|R
40|$|A beam of {{electrons}} with E= 10 HeV and protons with E= 8, 4 and 70 HeV and monocrystalline targets have been investigated in the paper. During the investigation experiments on beams of relativistic electrons and protons have been held. As a result theoretical forecastings have been confirmed. New experimental data of the flow dynamics and main characteristics of channeled particles have been obtained. Concrete crystallooptical systems for the formation, division, <b>diagnostics</b> and <b>output</b> of a beam have been developed and created for the first time. The methodologies and systems for the formation and output of a beam have been introduced into operationAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|The Mansell Neocot is a {{portable}} humicrib developed at Mansell Neonatal Transport Equipment (MNTE). When operating normally the Neocot <b>outputs</b> <b>diagnostic</b> data every two seconds. This diagnostic {{data can be}} captured by a Bluetooth module plugged {{into the side of}} the Neocot. This Bluetooth module can transmit the diagnostic data to a local, connected computer. Previously this computer logged the data and emailed the log file to the MNTE Office for diagnostic purposes. This project details the establishment of a connection over the Internet between a computer connected to the Neocot via Bluetooth and a computer at the MNTE Office. This connection allows the <b>diagnostic</b> data <b>output</b> from a remote Neocot to be streamed in real-time directly back to the MNTE Office. Two software applications were developed to allow for the real-time data transmission between the Neocot and a remote computer. 'MNTE Server' is the application developed to receive incoming data at the MNTE Office. 'Neocot Diagnostics is an extended application of the original of the same name that forwards data received from a local Neocot, over the Internet, to the MNTE Office. The development and testing of the software and resulting connection are documented in detail. The goals of this project were achieved, resulting in real-time data transmission from a remote Neocot to a local computer...|$|R
40|$|A {{new program}} in the Daresbury Lane {{software}} suite has been developed for the scaling and normalization of Laue intensity data, to yield fully corrected structure amplitudes. Previously available routines have been improved, and additional options for refinement, control and statistical <b>diagnostic</b> <b>output</b> provided. A new feature, namely a wavelength- and position-dependent absorption correction that models a two-dimensional surface derived from the Laue data alone, is discussed in detail; it is tested on simulated and real data, and the improvement in data quality is demonstrated. The wavelength normalization function is now able, when sufficiently redundant experimental data are available, to model fine details such as the features arising from the modification of the incident intensity spectrum by a platinum mirror in the beamline optics. A full data set for tetragonal lysozyme is processed with the new program, and extensive statistical output is given. © 1999 International Union of Crystallography - all rights reserved. link_to_subscribed_fulltex...|$|E
40|$|As long as {{computers}} remained {{simply an}} auxilary device {{which served as}} a backup or augmentation to the routine clinical testing the lack of quality control was not a serious medical-legal problem. Recently, the computer system has become the primary diagnostic tool with other instrumentation performing the role of backup. It is important to quantify the effects of hardware and software parameters in the clinical study and to implement methods by which routine monitoring can be done. General comments and suggestions can be made concerning the common hardware but {{it is much more}} difficult to make meaningful suggestions for the specialized hardware. Software has {{as much to do with}} the <b>diagnostic</b> <b>output</b> of a patient study as does the hardware used to make the initial measurements. The effects of software reliability or accuracy may not lead to immediate problems but it will eventually have an impact on the confidence of clinicians and research associates...|$|E
40|$|Although medical {{diagnosis}} {{has been given}} some attention by economists in particular contexts, the phenomenon has not received the attention it deserves. This paper {{is an attempt to}} redress this imbalance. As with most real phenomena, there are special issues that characterise diagnosis. It is argued that the output of a diagnostic test is information about a person 2 ̆ 7 s disease status. This information is measured probabilistically in two ways viz. the predictive value of positive and negative tests results. The economic analysis of diagnosis involves a relevant combination of characteristics and Hicksian demand theory. It is shown that the purpose of undertaking diagnostic tests is crucial to the consumer 2 ̆ 7 s judgement {{of the importance of the}} two measures of <b>diagnostic</b> <b>output.</b> The argument is illustrated by reference to three alternative technologies for diagnosis of the upper gastrointestinal tract viz. single and double contrast barium meal radiology and fibre optic endoscopy...|$|E
40|$|The {{principal}} {{elements and}} functions of the Ground Operations Aerospace Language (GOAL) compiler are presented. The technique used to transcribe the syntax diagrams into machine processable format {{for use by the}} parsing routines is described. An explanation of the parsing technique used to process GOAL source statements is included. The compiler <b>diagnostics</b> and the <b>output</b> reports generated during a GOAL compilation are explained. A description of the GOAL program package is provided...|$|R
40|$|Předmětem předložené práce je {{assessment}} centrum jako metoda využívaná v personálně - psychologické praxi. V teoretické části práce je věnována pozornost základním charakteristikám metody assessment centra - je zde uveden přehled definic a směrnic, které vymezují a upravují assessment centrum po jeho obsahové a realizační stránce. Popsán je také postup tvorby assessment centra a proces volby dílčích metod (rozhovor, testové metody, modelové situace atd.), které assessment centrum tvoří. Podrobněji se zabýváme charakterem dílčích metod assessment centra a možností aplikace jejich diagnostických výstupů. V samostatné kapitole věnujeme zvláštní pozornost problematice modelových situací a skupinových úloh. Zmíněny jsou také sociálně - psychologické aspekty assessment centra a dále pak jeho kulturní a interkulturní rozměr. Empirickou část práce tvoří popis a analýza konkrétního realizovaného assessment centra, ze které pak vychází návrh řešení pro jeho optimalizaci. Klíčová slova assessment centrum modelové situace skupinové úlohyThe {{subject of}} my final thesis is the assessment centre {{as a method}} being used in practice of personnel psychology. The theoretical part is aimed at basic methods of assessment centre, it contains the definitions and instructions which limit and edit the assessment centre as regards its content and implementation aspects. It describes the processes of assessment centre creation and choice of its partial methods (dialogue, test methods, model situation etc). In more detail it deals with character of assessment centre partial methods and possibilities of their <b>diagnostic</b> <b>outputs</b> application. In particular chapter special attention is given to model situations and group exercises questions. At {{the same time it}} describes the assessment centre social-psychological aspects and its cultural and intercultural extent. The empirical part contains the description and analysis of particular existing assessment centre, from which a suggestion for its optimalization arises. Key words assessment center group exercises simulationsDepartment of PsychologyKatedra psychologieFaculty of ArtsFilozofická fakult...|$|R
40|$|Background: The Diagnostic Interview for Social and Communication Disorders (DISCO) is an interviewer-based {{schedule}} {{for use with}} parents and carers. In addition to its primary clinical purpose of helping the clinician to obtain a developmental history and description of the child or adult concerned, {{it can also be}} used to assist in providing a formal diagnostic category. Method: In this study we compared two algorithms based on the ninth revision of the schedule (DISCO 9). The algorithm for ICD- 10 childhood autism comprised 91 individual, operationally defined items covering the behaviour outlined in the ICD- 10 research criteria. The algorithm for the autistic spectrum disorder, as defined by Wing and Gould (1979), was based on 5 DISCO items that represented overarching categories of behaviour crucial for the diagnosis of autistic disorders. The aim of the study was to examine the implications for clinical diagnosis of these two different approaches. Parents of 36 children with clinical diagnoses of autistic disorder, 17 children with learning disability and 14 children with language disorders were interviewed by two interviewers. Algorithm diagnoses were applied to interview items in order to analyse the relationship between clinical and algorithm diagnoses and the inter-rater reliability between interviewers. Results: Clinical diagnosis was significantly related to the <b>diagnostic</b> <b>outputs</b> for both algorithms. Inter-rater reliability was also high for both algorithms. The ICD childhood disorder algorithm produced more discrepant diagnoses than the Wing and Gould autistic spectrum algorithm. Analysis of the ICD- 10 algorithm items and combination of items helped to explain the reason for these discrepancies. Conclusions: The results indicate that the DISCO is a reliable instrument for diagnosis when sources of information are used from the whole interview. It is particularly effective for diagnosing disorders of the broader autistic spectrum...|$|R
