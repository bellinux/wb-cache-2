665|1085|Public
5000|$|... #Subtitle level 4: Goodness of <b>fit</b> <b>index</b> and {{adjusted}} goodness of <b>fit</b> <b>index</b> ...|$|E
5000|$|The {{goodness}} of <b>fit</b> <b>index</b> (GFI) {{is a measure}} of fit between the hypothesized model and the observed covariance matrix. The adjusted {{goodness of}} <b>fit</b> <b>index</b> (AGFI) corrects the GFI, which is affected by the number of indicators of each latent variable. The GFI and AGFI range between 0 and 1, with a value of over [...]9 generally indicating acceptable model fit.|$|E
50|$|Relative {{fit indices}} (also called “incremental fit indices” and “comparative fit indices”) compare the {{chi-square}} for the hypothesized model to {{one from a}} “null”, or “baseline” model. This null model almost always contains a model in which all of the variables are uncorrelated, and as a result, has a very large chi-square (indicating poor fit). Relative fit indices include the normed <b>fit</b> <b>index</b> and comparative <b>fit</b> <b>index.</b>|$|E
40|$|This {{dissertation}} {{examines the}} sensitivity of six <b>fit</b> <b>indices</b> in detecting various types of misspecifications {{in the application of}} a linear-linear piecewise multilevel latent growth curve model that uses continuous multivariate normal data. The study results show that all <b>fit</b> <b>indices</b> are more sensitive to misspecifications on the within level than those on the between level structure of the model. On the within level, all <b>fit</b> <b>indices</b> are more sensitive to the misspecification in the covariance structure than that in the residual structure; on the between level, all <b>fit</b> <b>indices</b> are more sensitive to the misspecification in the marginal mean structure than that in the covariance structure. Actually, none of the <b>fit</b> <b>indices</b> are practically significantly sensitive to the misspecification in the between-level covariance structure. Partially-saturated estimation method helps NFI, TLI, and Mc {{to be sensitive to the}} appropriate sample size when evaluating the misspecification in the between-level covariance structure; however, it helps none of the <b>fit</b> <b>indices</b> when detecting models misspecified in the between-level covariance structure. All <b>fit</b> <b>indices</b> are principally influenced by the severity of misfit if it happens on the within level; however, they are primarily affected by group size if the misspecification occurs at the between level. When severity level increases, all <b>fit</b> <b>indices</b> have more power to detect misspecification in the within-level covariance structure. When group size increases, NFI, TLI, CFI, Mc, and RMSEA are more likely to commit Type II errors in detecting misspecifications in the marginal mean structure and in both the marginal mean and the covariance structures. Compared with other <b>fit</b> <b>indices,</b> NFI is most vulnerable to sample size and least sensitive to severity level of misfit. SRMR, however, behaves differentially from all other <b>fit</b> <b>indices</b> in that it is most sensitive to the intraclass correlation coefficient when detecting studied misspecifications on the between level structure. Furthermore, the recommended cutoff values lead to high Type II errors for all <b>fit</b> <b>indices</b> in detecting various types of misspecifications, and it is infeasible to find a substitute new set of criteria based on the current data conditions...|$|R
50|$|Absolute <b>fit</b> <b>indices</b> {{determine}} {{how well the}} a priori model fits, or reproduces the data. Absolute <b>fit</b> <b>indices</b> include, {{but are not limited}} to, the Chi-Squared test, RMSEA, GFI, AGFI, RMR, and SRMR.|$|R
40|$|The {{following}} paper presents current {{thinking and}} research on <b>fit</b> <b>indices</b> for structural equation modelling. The paper presents a selection of <b>fit</b> <b>indices</b> that are {{widely regarded as the}} most informative indices available to researchers. As well as outlining each of these indices, guidelines are presented on their use. The paper also provides reporting strategies of these indices and concludes with a discussion on the future of <b>fit</b> <b>indices...</b>|$|R
5000|$|Formann, A. K. (2006). Testing the Rasch {{model by}} means of the mixture <b>fit</b> <b>index.</b> British Journal of Mathematical and Statistical Psychology, 59, 89-95.|$|E
5000|$|The normed <b>fit</b> <b>index</b> (NFI) {{analyzes}} {{the discrepancy between}} the chi-squared value of the hypothesized model and the chi-squared value of the null model. However, NFI tends to be negatively biased. The non-normed <b>fit</b> <b>index</b> (NNFI; also known as the Tucker-Lewis index, as it was built on an index formed by Tucker and Lewis, in 1973) resolves some of the issues of negative bias, though NNFI values may sometimes fall beyond the 0 to 1 range. Values for both the NFI and NNFI should range between 0 and 1, with a cutoff of [...]95 or greater indicating a good model fit.|$|E
5000|$|<b>Fit</b> <b>index</b> where a {{value of}} zero {{indicates}} the best fit. While the guideline for determining a [...] "close fit" [...] using RMSEA is highly contested, most researchers concur that an RMSEA of [...]1 or more indicates poor fit.|$|E
40|$|In this study, it {{was aimed}} to {{investigate}} type I error and power {{rates of the}} item <b>fit</b> <b>indices</b> through various conditions (sample sizes, different test lengths and different magnitudes of misfit) for dichotomously generated items based on one-, two-, and three-parameter logistic models in Item Response Theory. In this study, the type I error and power rates of these item <b>fit</b> <b>indices</b> were assessed in a simulation study. χ², Q 1 and G 2 indices as traditional item <b>fit</b> <b>indices</b> and S-χ² index as alternative indices were assessed. The performance of four different item <b>fit</b> <b>indices</b> in study were compared by manipulating three different sample size (1000, 2000, 4000), three different test lengths (20, 30, 40) and four different misfit magnitude (% 0, % 10, % 30 and % 50). Item responses were generated using the R 3. 3. 2 software program and analyzed by using “mirt” package in R software. The p value of item <b>fit</b> <b>indices</b> and their degrees of freedom were calculated for both item responses for generating model and analysis model. Type I errors and power rates of item <b>fit</b> <b>indices</b> were examined according to significance levels of 0. 05. All item <b>fit</b> <b>indices</b> {{in this study were}} compared by calculating the type I error and power rates of each item <b>fit</b> <b>indices</b> under all conditions. The findings indicated that S-χ² index has lower type I error to detect misfit than the other indices. It can be concluded that in the case where the sample size was 2000 or more and the number of items in test are 20 and more, S-χ² index has lower type I error rates than traditional indices and has adequate power to detect misfit items...|$|R
30|$|The {{model fit}} of the {{structural}} model was again tested using different <b>fit</b> <b>indices.</b> Except for χ 2, all <b>fit</b> <b>indices</b> (χ 2  =  169.72, p <  0.001; χ 2 /df =  2.93; TLI =  0.99; CFI =  0.99; GFI =  0.97; RMSEA =  0.05; SRMR =  0.02), were acceptable to the recommended levels of fit (see Table 4).|$|R
40|$|Selecting between {{competing}} {{structural equation}} models {{is a common}} problem. Often selection {{is based on the}} chi-square test statistic or other <b>fit</b> <b>indices.</b> In other areas of statistical research Bayesian information criteria are commonly used, but they are less frequently used with structural equation models compared to other <b>fit</b> <b>indices.</b> This article examines several new and old information criteria (IC) that approximate Bayes factors. We compare these IC measures to common <b>fit</b> <b>indices</b> in a simulation that includes the true and false models. In moderate to large samples, the IC measures outperform the <b>fit</b> <b>indices.</b> In a second simulation we only consider the IC measures and do not include the true model. In moderate to large samples the IC measures favor approximate models that only differ from the true model by having extra parameters. Overall, SPBIC, a new IC measure, performs well relative to the other IC measures...|$|R
5000|$|The {{comparative}} <b>fit</b> <b>index</b> (CFI) {{analyzes the}} model fit {{by examining the}} discrepancy between the data and the hypothesized model, while adjusting for the issues of sample size inherent in the chi-squared test of model fit, and the normed <b>fit</b> <b>index.</b> CFI values range from 0 to 1, with larger values indicating better fit. Previously, a CFI value of [...]90 or larger was considered to indicate acceptable model fit. However, recent studies have indicated that a value greater than [...]90 is needed to ensure that misspecified models are not deemed acceptable (Hu & Bentler, 1999). Thus, a CFI value of [...]95 or higher is presently accepted as an indicator of good fit (Hu & Bentler, 1999).|$|E
5000|$|Root {{mean square}} error of {{approximation}} (RMSEA) fit index: RMSEA is {{an estimate of the}} discrepancy between the model and the data per degree of freedom for the model. Values less that [...]05 constitute good fit, values between 0.05 and 0.08 constitute acceptable fit, a values between 0.08 and 0.10 constitute marginal fit and values greater than 0.10 indicate poor fit [...] An advantage of the RMSEA <b>fit</b> <b>index</b> is that it provides confidence intervals which allow researchers to compare a series of models with varying numbers of factors.|$|E
50|$|Most {{statistical}} methods only require one statistical test {{to determine the}} significance of the analyses. However, in CFA, several statistical tests are used to determine how well the model fits to the data. Note that a good fit between the model and the data {{does not mean that the}} model is “correct”, or even that it explains a large proportion of the covariance. A “good model fit” only indicates that the model is plausible. When reporting the results of a confirmatory factor analysis, one is urged to report: a) the proposed models, b) any modifications made, c) which measures identify each latent variable, d) correlations between latent variables, e) any other pertinent information, such as whether constraints are used. With regard to selecting model fit statistics to report, one should not simply report the statistics that estimate the best fit, though this may be tempting. Though several varying opinions exist, Kline (2010) recommends reporting the Chi-squared test, the Root mean square error of approximation (RMSEA), the comparative <b>fit</b> <b>index</b> (CFI), and the standardised root mean square residual (SRMR).|$|E
40|$|A Monte Carlo {{study was}} {{conducted}} to assess the effects of some potential confounding factors on structural equation modeling (SEM) <b>fit</b> <b>indices</b> and parameter estimates for both true and misspecified models. The factors investigated were data nonnormality, SEM estimation method, and sample size. Based on the fully crossed and balanced 3 x 3 x 4 x 2 experimental design with 200 replications in each cell division, a total of 14, 400 samples were generated and fitted to SEM models with different degrees of model misspecification. The major findings are: (1) mild to moderate data nonnormality has little effect on SEM <b>fit</b> <b>indices</b> and parameter estimates; (2) estimation method has considerable influence on some SEM <b>fit</b> <b>indices</b> when the model was misspecified, primarily on those comparative model fit indices; and (3) some <b>fit</b> <b>indices</b> are susceptible to the influence of sample size, and show moderate downward bias under smaller sample size conditions. Previous studies in this area have simulated a correctly-specified true model, and fi...|$|R
40|$|Confirmatory factor {{analytic}} {{tests of}} measurement invariance (MI) {{based on the}} chi-square statistic {{are known to be}} sensitive to sample size. For this reason, Cheung and Rensvold (2002) recommended using alternative <b>fit</b> <b>indices</b> in MI investigations. However, previous studies have not established the power of <b>fit</b> <b>indices</b> to detect data with a lack of invariance. In this study, we investigated the performance of <b>fit</b> <b>indices</b> with simulated data known to not be invariant. Our results indicate that alternative <b>fit</b> <b>indices</b> can be successfully used in MI investigations. Specifically, we suggest reporting McDonald’s noncentrality index along with CFI, and Gamma-hat. Measurement invariance (MI) can be considered the degree to which measurements conducted under different conditions yield measures of the same attributes (Drasgow, 1984; Horn & McArdle, 1992). These different conditions include stability of measurement over time (Chan, 1998; Chan & Schmitt, 2000), across different populations (e. g., cultures, Riordan & Vandenberg, 1994; gender...|$|R
40|$|In {{adjustment}} {{studies of}} scales {{and in terms}} of cross validity at scale development, confirmatory factor analysis is conducted. Confirmatory factor analysis, multivariate statistics, is estimated via various parameter estimation methods and utilizes several <b>fit</b> <b>indexes</b> for evaluating the model fit. In this study, model <b>fit</b> <b>indexes</b> utilized in confirmatory factor analysis are examined with different parameter estimation methods under different sample sizes. For the purpose of this study, answers of 60, 100, 250, 500 and 1000 students who attended PISA 2012 program were pulled from the answers to two dimensional “thoughts on the importance of mathematics” dimension. Estimations were based on methods of maximum likelihood (ML), unweighted least squares (ULS) and generalized least squares (GLS). As a result of the study, it was found that model <b>fit</b> <b>indexes</b> were affected by the conditions, however some <b>fit</b> <b>indexes</b> were affected less than others and vice versa. In order to analyze these, some suggestions were made...|$|R
5000|$|Although {{there is}} {{need for further}} {{research}} on the application of various invariance tests and their respective criteria across diverse testing conditions, two approaches are common among applied researchers. For each model being compared (e.g., Equal form, Equal Intercepts) a χ2 fit statistic is iteratively estimated from the minimization {{of the difference between}} the model implied mean and covariance matrices and the observed mean and covariance matrices. As long as the models under comparison are nested, the difference between the χ2 values and their respective degrees of freedom of any two CFA models of varying levels of invariance follows a χ2 distribution (diff χ2) and as such, can be inspected for significance as an indication of whether increasingly restrictive models produce appreciable changes in model-data fit. However, there is some evidence the diff χ2 is sensitive to factors unrelated to changes in invariance targeted constraints (e.g., sample size). As a result, researchers are also recommended to use the difference between the comparative <b>fit</b> <b>index</b> (ΔCFI) of two models specified to investigate measurement invariance. When the difference between the CFIs of two models of varying levels of measurement invariance (e.g., equal forms versus equal loadings) is greater than 0.01, then invariance in likely untenable. [...] It {{is important to note that}} the CFI values being subtracted are expected to come from nested models as in the case of diff χ2 testing; however, there is indication that applied researchers rarely take this into consideration when applying the CFI test.|$|E
3000|$|... 2), {{degrees of}} freedom, {{root mean square}} error of {{approximation}} (RMSEA), normed <b>fit</b> <b>index</b> (NFI), non-normed <b>fit</b> <b>index</b> (NNFI), comparative <b>fit</b> <b>index</b> (CFI), goodness of <b>fit</b> <b>index</b> (GFI), RMR, and adjusted goodness of <b>fit</b> <b>index</b> (AGFI) (Kline 2010). As a result, each dimension was further reduced to fewer factors. For each of the generated factors, the summated means and standard deviations were computed. It should be noted that factor analysis, which groups related questions into factors, can help validate a summated scale by demonstrating that its questions are related (Spector 1992). In this study, we use factor analysis to select the best questions to include in a summated scale.|$|E
30|$|The {{hypothesized}} {{model was}} tested for goodness-of-fit using AMOS 9. Results suggest that for long-term respondents, goodness of <b>fit</b> <b>Index</b> (GFI) =  0.91; {{root mean square}} error of approximation (RMSEA) =  0.05; Tucker Lewis index (TLI) =  0.95 and comparative <b>fit</b> <b>index</b> (CFI) =  0.96; and for short-term respondents, goodness of <b>fit</b> <b>Index</b> (GFI) =  0.93; {{root mean square error}} of approximation (RMSEA) =  0.04; Tucker Lewis index (TLI) =  0.95 and comparative <b>fit</b> <b>index</b> (CFI) =  0.96; the model was found to achieve adequate fit to the observed data. Thus the proposed structural model satisfies the conditions of unidimensionality.|$|E
40|$|A Monte Carlo {{simulation}} {{study was}} conducted to investigate the effects of sample size, estimation method, and model specification on structural equation modeling (SEM) <b>fit</b> <b>indices.</b> Based on a balanced 3 x 2 x 5 design, a total of 6, 000 samples were generated from a prespecified population covariance matrix, and eight popular SEM <b>fit</b> <b>indices</b> were studied. Two primary conclusions were suggested. First, for misspecified models, some <b>fit</b> <b>indices</b> appear to be noncomparable in terms of the information they provide about model fit; some <b>fit</b> <b>indices</b> also seem to be more sensitive to model misspecification. Second, estimation method strongly influenced almost all the <b>fit</b> <b>indices</b> examined, especially for misspecified models. These two issues do not appear to have been well documented in the previous literature. Perhaps the focus of most previous simulation studies on correctly specified models may have failed to detect these dynamics. It is further suggested that future research should study not only different models relative to model complexity, but also a wider range of model specification conditions, including correctly specified models as well as models specified incorrectly to varying degrees. (Contains 2 figures, 6 tables, and 26 references.) (Author/SLD) *************** * c**************************************************** * Reproductions supplied by EDRS are the best that can be made * * from the original document. * fr...|$|R
30|$|Ho 2 : The {{instrument}} {{does not}} show good <b>fit</b> <b>indices</b> as measure by the Confirmatory factor analysis.|$|R
40|$|This paper {{deals with}} the <b>fit</b> <b>indices</b> used in Structural Equation Modelling (SEM) for testing {{theoretical}} models and the difficulties that can occur during the testing of theoretical models in different fields of psychology. The paper discusses the basic assumptions of SEM and presents the indices used for assessing the fit of theoretical models. This paper also presents the procedures for calculating the basic statistic for assessing the fit of models (χ 2), {{as well as for}} calculating the most commonly used <b>fit</b> <b>indices,</b> in order to gain a better insight into the advantages and potential difficulties that can occur during their usage. We mention the difficulties regarding the assessment of fit of the model based on χ 2 and the discussed <b>fit</b> <b>indices</b> stemming from the sample size, data distribution and assessment methods, wrong specification of model and disturbance of normality and independence of latent variables, as well as the ways in which these difficulties can be overcome. This paper provides a proposal for the approach to presenting the <b>fit</b> <b>indices</b> in reports on studies where theoretical models were tested via SEM...|$|R
30|$|Since the Chi – Square equals 13, the p-value {{is larger}} than 0.05 and RMSEA is less than 0.05, we {{conclude}} that the model is fit. The Goodness of <b>Fit</b> <b>Index</b> (GFI) equals 0.91, Adjusted Goodness of <b>Fit</b> <b>Index</b> (AGFI) equals 0.76 and Parsimony Goodness of <b>Fit</b> <b>Index</b> (PGFI) equals 0.35. These findings also confirm that the data fits the model.|$|E
30|$|The former {{group of}} {{indicators}} {{includes the following}} statistics: the chi-square fit test index (CMIN/DF), the normed <b>fit</b> <b>index</b> (NFI), the comparative <b>fit</b> <b>index</b> (CFI), and the {{root mean square error}} of approximation (RMSEA).|$|E
30|$|Results {{showed that}} the multi-factor model fits the data [χ 2  =  141.587, p < . 05; {{comparative}} <b>fit</b> <b>index</b> (CFI) = . 943; goodness of <b>fit</b> <b>index</b> (GFI) = . 945; route mean square error of approximation (RMSEA) = . 063; non-normed <b>fit</b> <b>index</b> (NNFI) = . 931] considerably better than the one-factor model [χ 2  =  183.628, p < . 05; (CFI) = . 721; (GFI) = . 704; (RMSEA) = . 096; (NNFI) = . 713], indicating that no serious threat of common method bias exists in the research.|$|E
40|$|The {{issue of}} {{perturbations}} in real or simulated data has been substantially neglected {{in evaluating the}} adequacy of <b>fit</b> <b>indices</b> used to test covariance structure modeling. Nevertheless, it is certainly legitimate to wonder whether <b>fit</b> <b>indices</b> are reliably sensitive to data corruption. In particular, we would expect that a good index should approach its maximum under correct model specification and uncorrupted data, but also degrade substantially under massive data perturbation. In this paper we provide a possible methodological {{solution to the problem}} of evaluating the sensitivity of <b>fit</b> <b>indices</b> in structural equation modeling when perturbed data are considered. In particular, in our study the sensitivity of four different <b>fit</b> <b>indices</b> (two absolute fit-indices: GFI and AGFI, and two incremental fit-indices: CFI and NNFI) to perturbed data is examined in three different factorial models. The sensitivity evaluation is carried out by means of a new integrated approach which combines standard Monte Carlo (MC) simulations and a recent data generating procedure called Sample Generation by Replacements (SGR, [Lombardi et al., 2004]) ...|$|R
3000|$|To {{evaluate}} to {{what extent}} the models specified fit the data, absolute and incremental <b>fit</b> <b>indices</b> were used. X [...]...|$|R
5000|$|... 4 Split or <b>fitted</b> <b>indexes</b> {{are allowed}} with a holding system except in extra-thin calibers where the holding {{system is not}} required.|$|R
40|$|This {{study was}} {{designed}} to test validity and reliability of Academic Motivation Scale (AMS) for sports high school students. The research conducted with 357 volunteered girls (n= 117) and boys (n= 240). Confirmatory factor analysis showed that Chi square (χ 2), degrees of freedom (df) and χ 2 /df ratio were 1102. 90, 341 and 3. 234, respectively. Goodness of <b>Fit</b> <b>Index,</b> Comparative <b>Fit</b> <b>Index,</b> Non-normed <b>Fit</b> <b>Index</b> and Incremental <b>Fit</b> <b>Index</b> were between 0. 92 - 0. 95. Additionally, Adjusted Goodness of <b>Fit</b> <b>Index,</b> An Average Errors Square Root and Root Mean Square Error of Approximation were 0. 88, 0. 070 and 0. 079, respectively. Subscale reliability coefficients were between 0. 77 and 0. 86. Test-retest correlations of AMS were found between 0. 79 and 0. 91. Results showed that scale was suitable for determination of sports high school students’ academicals motivation levels...|$|E
3000|$|... 2 /gl[*]=[*] 1.751; Comparative <b>Fit</b> <b>Index</b> – CFI[*]=[*] 0.973; Tucker-Lewis <b>Fit</b> <b>Index</b> – TLI[*]=[*] 0.964; Root Mean Square Error of Approximation – RMSEA[*]=[*] 0.042 (Confidence Interval – CI 90 %: 0.032 – 0.052); Standardized Root Mean Square Residual – SRMR[*]=[*] 0.037.|$|E
40|$|In many {{psychological}} questionnaires {{the need}} to analyze empirical data raises the fundamental problem of possible fake or fraudulent observations in the data. This aspect is particularly relevant for researchers working on sensitive topics such as, for example, risky sexual behaviors and drug addictions. Our contribution presents a new probabilistic approach, called Sample Generation by Replacement (SGR), {{to address the problem}} of evaluating the sensitivity of 8 commonly used SEM-based fit indices (Goodness of <b>Fit</b> <b>Index,</b> GFI; Adjusted Goodness of <b>Fit</b> <b>Index,</b> AGFI; Expected Cross Validation Index, ECVI; Standardized Root-Mean-Square Residual Index, SRMR; Root-Mean-Square Error of Approximation, RMSEA; Comparative <b>Fit</b> <b>Index,</b> CFI; Nonnormed <b>Fit</b> <b>Index,</b> NNFI; and Normed <b>Fit</b> <b>Index,</b> NFI) to fake-good ordinal data. We used SGR to perform a simulation study involving 3 different SEM models, 2 sample size conditions, and 2 estimation methods: maximum likelihood (ML) and weighted least squares (WLS). Our results show that the incremental fit indices (CFI, NNFI, and NFI) are clearly more sensitive to fake perturbation than the absolute fit indices (GFI, AGFI, and ECVI). Overall, NFI turned out to be the best and most reliable <b>fit</b> <b>index.</b> We also applied SGR to real behavioral data on (non) compliance in liver transplant patients...|$|E
50|$|RAM models return {{standardized}} and raw estimates, {{as well as}} a {{range of}} <b>fit</b> <b>indices</b> (AIC, RMSEA, TLI, CFI etc.). Confidence intervals are estimated robustly.|$|R
40|$|Abstract: Bi-factor {{confirmatory factor}} {{models have been}} {{influential}} in research on cognitive abilities because they often better fit the data than correlated factors and higher-order models. They also instantiate a perspective that differs from that offered by other models. Motivated by previous work that hypothesized an inherent statistical bias of <b>fit</b> <b>indices</b> favoring the bi-factor model, we compared the fit of correlated factors, higher-order, and bi-factor models via Monte Carlo methods. When data were sampled from a true bi-factor structure, each of the approximate <b>fit</b> <b>indices</b> was {{more likely than not}} to identify the bi-factor solution as the best fitting. When samples were selected from a true multiple correlated factors structure, approximate <b>fit</b> <b>indices</b> were more likely overall to identify the correlated factors solution as the best fitting. In contrast, when samples were generated from a true higher-order structure, approximate <b>fit</b> <b>indices</b> tended to identify the bi-factor solution as best fitting. There was extensive overlap of fit values across the models regardless of true structure. Although one model may fit a given dataset best relative to the other models, each of the models tended to fit the data well in absolute terms. Given this variability, models must also be judged on substantive and conceptual grounds. J. Intell. 2015, 3...|$|R
40|$|Bi-factor {{confirmatory factor}} {{models have been}} {{influential}} in research on cognitive abilities because they often better fit the data than correlated factors and higher-order models. They also instantiate a perspective that differs from that offered by other models. Motivated by previous work that hypothesized an inherent statistical bias of <b>fit</b> <b>indices</b> favoring the bi-factor model, we compared the fit of correlated factors, higher-order, and bi-factor models via Monte Carlo methods. When data were sampled from a true bi-factor structure, each of the approximate <b>fit</b> <b>indices</b> was {{more likely than not}} to identify the bi-factor solution as the best fitting. When samples were selected from a true multiple correlated factors structure, approximate <b>fit</b> <b>indices</b> were more likely overall to identify the correlated factors solution as the best fitting. In contrast, when samples were generated from a true higher-order structure, approximate <b>fit</b> <b>indices</b> tended to identify the bi-factor solution as best fitting. There was extensive overlap of fit values across the models regardless of true structure. Although one model may fit a given dataset best relative to the other models, each of the models tended to fit the data well in absolute terms. Given this variability, models must also be judged on substantive and conceptual grounds...|$|R
