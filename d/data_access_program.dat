4|10000|Public
50|$|CIHI {{has set up}} a Graduate Student <b>Data</b> <b>Access</b> <b>Program</b> that {{provides}} qualifying graduate students with access to CIHI’s data at no cost whatsoever. Graduate students may only use this program in order to fulfill academic requirements of their program.|$|E
40|$|The {{chances of}} {{enhanced}} short term weather predictions and economic {{benefits from the}} use of GOES satellite data were examined. Results for a meteorological consulting firm before and after the introduction of GOES data were chosen as the method, and monetary benefits were selected as the measure. Services were provided for use by road and street departments, commodities dealers, and marine clients of the consulting firm. The Man-computer Interactive <b>Data</b> <b>Access</b> <b>Program</b> (McIDAS) was employed to furnish 1 / 2 hour visual or IR imagery for remote access. The commodities clients reconnected the GOES real-time imagery once the study was completed, while the consulting firm, which was personnel and not equipment intensive, did not. Further development of the flexibility of access to the GOES data and improvements in the projected grids are indicated...|$|E
40|$|Maritime Situational Awareness is the {{understanding}} of anything associated with the maritime domain that could impact the security, safety, economy, or environment. The European Commission’s Joint Research Centre (JRC) has developed the Blue Hub as an in-house R&D platform for integrated maritime surveillance and maritime situational awareness. It is applied, for example, to support counter-piracy around Africa, and to monitor the growing ship traffic in the Arctic. In order to improve maritime awareness and support risk assessment, the JRC has started to integrate data from the marine science community. In particular the JRC is interested in using forecasts from operational ocean models and weather models. For the Blue Hub, message brokering and data mediation has become an essential tool for the accessing of ocean forecast data {{as quickly as possible}} in easy to use formats. NOAA (National Oceanic and Atmospheric Administration) is making global oceanography data available through the Environmental Research Division's <b>Data</b> <b>Access</b> <b>Program</b> (ERDDAP) data broker. ERDDAP provides RESTful machine to machine communication, data brokering and data mediation by converting data to a number of standard and developer friendly ways, including some Open Geospatial Consortium formats. In this presentation we demonstrate how data brokering and mediation is making complex scientific data efficiently accessible. We show how such data is being integrated into the Blue Hub system to enhance maritime situational awareness. JRC. G. 3 -Maritime affair...|$|E
40|$|In {{scientific}} writing, {{the writer}} explains {{the design of}} application programs on the MediaCellular voucher sales by using Microsoft Visual Basic 6. 0. In this application programconsists of 7 Form. And use the <b>Data</b> <b>Access</b> Object <b>program</b> to connect to the database. And using Microsoft Access 1997 for database...|$|R
5000|$|Remote Data Objects (abbreviated RDO) is an {{obsolete}} <b>data</b> <b>access</b> application <b>programming</b> interface primarily used in Microsoft Visual Basic applications on Windows 95 {{and later}} operating systems. [...] This includes database connection, queries, stored procedures, result manipulation, and change commits. It allowed developers to create interfaces that can directly interact with Open Database Connectivity (ODBC) data sources on remote machines, {{without having to}} deal with the comparatively complex ODBC API.|$|R
40|$|Transaction {{processing}} {{literature is}} usually focusing on SQL transactions (local in a database connection) and distributed transactions {{consisting of a}} set of local transactions building a logical atomic transaction, concurrency problems of these. This paper extends the discussion on transaction concurrency to user transactions built of a sequence of inter-related SQL transactions and to the programming discipline called "Optimistic Locking " The discipline was invented decades ago, before the era of RDBMS systems, but seems to have been forgotten in some modern complicated software architectures. We consider the name “Optimistic Locking ” as misleading, since to be precise the question is on Row Version Verifying (RVV) <b>Data</b> <b>Access</b> Discipline in case of single row / object updates. In Part I of this article present the concept of user transaction for a generic use case, and various <b>data</b> <b>access</b> <b>programming</b> patterns. We compare the classic Lost Update Problem of concurrent transactions with Blind Overwriting Problem of user transactions and discuss the technologies for avoiding it. We will focus on the client-side programming discipline with examples on using SQL for modern RDBMS systems, but the programming discipline is...|$|R
40|$|Previous {{research}} has demonstrated that MODIS data products can be used as inputs into the seagrass productivity model developed by Fong and Harwell (1994). To further explore this use to predict seagrass productivity, Moderate Resolution Imaging Spectroradiometer (MODIS) custom data products, including Sea Surface Temperature, Light Attenuation, and Chlorophyll-a have been created for use as model parameter inputs. Coastal researchers can use these MODIS data products and model results in conjunction with historical and daily assessment of seagrass conditions to assess variables that affect the productivity of the seagrass beds. Current monitoring practices involve manual data collection (typically on a quarterly basis) and the data is often insufficient for evaluating the dynamic events that influence seagrass beds. As part of a NASA-funded research grant, the University of Mississippi, is working with researchers at NASA and Radiance Technologies to develop methods to deliver MODIS derived model output for the northern Gulf of Mexico (GOM) to coastal and environmental managers. The result of the project will be a data portal that provides access to MODIS data products and model results from the past 5 years, that includes an automated process to incorporate new data as it becomes available. All model parameters and final output will be available through the use National Oceanic and Atmospheric Administration?s (NOAA) Environmental Research Divisions <b>Data</b> <b>Access</b> <b>Program</b> (ERDDAP) tools as well as viewable using Thematic Realtime Environmental Distributed Data Services (THREDDS) and the Integrated Data Viewer (IDV). These tools provide the ability to create raster-based time sequences of model output and parameters as well as create graphs of model parameters versus time. This tool will provide researchers and coastal managers the ability to analyze the model inputs so that the factors influencing a change in seagrass productivity can be determined over time...|$|E
50|$|Each 4K {{of memory}} {{was called a}} field. The Memory Extension Controller {{contained}} two three-bit registers: the DF (Data Field) and the IF (Instruction Field). These registers specified a field for each memory reference of the CPU, allowing a total of 15 bits of address. The IF register specified the field for instruction fetches and direct memory references; the DF register specified the field for indirect <b>data</b> <b>accesses.</b> A <b>program</b> running in one field could reference data in the same field by direct addressing, and reference data in another field by indirect addressing.|$|R
5000|$|The {{company has}} also hosted four hackathons and {{maintains}} a [...] "Monzo Developers" [...] Slack channel for developer discussion and support. The API allows developers to view transactions and accounts, receive notifications for events via webhooks and {{create their own}} items {{to appear in the}} app. The responses are encoded using JSON which allows for <b>data</b> <b>access</b> in any <b>programming</b> language with JSON support.|$|R
500|$|VII. [...] The {{idea of a}} {{globally}} interconnected set {{of computers}} through which everyone could quickly <b>access</b> <b>data</b> and <b>programs</b> from any site was first described in 1962 {{in a series of}} memos on the [...] "Galactic Computer Network" [...] by J.C.R. Licklider of DARPA.|$|R
5000|$|Most 8051 systems respect this distinction, and so {{are unable}} to {{download}} and directly execute new programs.The strict Harvard architecture {{has the advantage of}} making such systems immune to most forms of malware, except those that reuse existing program code.Some 8051 systems have (or can be modified to have) some [...] "dual-mapped" [...] RAM, making them act somewhat more like von Neumann architecture, as external ROM and RAM share data and address buses and the mapping can be designed to allow R/W <b>data</b> <b>access</b> to <b>program</b> memory.This (partial) von Neumann architecture has the advantage of making it possible for a boot loader running on the 8051 to write new native code to RAM and then execute it, leading to faster incremental and interactive programming cycles than strict Harvard systems.|$|R
30|$|Extensive {{and rapid}} {{advances}} in cloud computing allow users to <b>access</b> <b>data,</b> <b>programs,</b> and other computing resources from anywhere, at any time, and on any web-enabled device over the Internet. Cloud computing provides an optimal {{solution for the}} real-time <b>data</b> and/or software <b>access</b> problem. It also delivers on-demand service and offers a more flexible and cost-effective way to store data and obtain computing resources. Thus, it enables small operating companies to remotely monitor their operations in real time without a need for the dedicated network attached data servers and expensive RTMC typically required for remote real-time monitoring.|$|R
40|$|The central task of {{managing}} laboratory data is {{keeping track of}} laboratory samples, the experimental steps performed on them, {{and the results of}} these experiments. This task engenders several challenges, namely: ffl The need to accommodate frequent changes to laboratory protocols. ffl The need to provide <b>data</b> <b>access</b> to <b>programs</b> written in multiple languages and running on heterogeneous hardware. ffl The need to represent unusual data types, such as DNA sequences, with specialized behavior. ffl The need to view data in both static and historical perspectives. The static perspective deals with the current state of knowledge about a material such as the "sequence of a DNA fragment" or the "chromosome from which a DNA fragment was obtained". The historical perspective deals with the history of experimental steps, such as "for what percentage of DNA fragments has the sequence of constituent bases been read more than once?" Such historical queries are crucial to understanding and refining l [...] ...|$|R
40|$|This paper {{discusses}} {{the implementation of}} a data base management system designed to enable easy, flexible, <b>data</b> <b>access</b> from FORTRAN <b>programs.</b> The system was created in response to the need for a library of routines for accessing an archival data base whose structure is only tentative and is expected to change frequently and perhaps radically. A FORTRAN library interface between user programs and data bases is described. A methodology for conveniently restructuring data bases without having to change the FORTRAN programs accessing the data bases is presented. This paper also describes a system for restoring and then accessing a data base which has been saved in an obsolete format without having to alter any of the data accessing programs...|$|R
40|$|One of {{the methods}} used to define {{biological}} resources is biological surveys. Results from biological surveys are often used to make comparisons between populations of organisms above and below discharges, or between reference sites and suspected impacted sites. One of the metrics from these biological surveys is count data {{that are based on}} replicate samples taken from sites or stations. These data present challenges for the analyst because they are often clumped, with zero values and potential outliers. In this paper we present some approaches that will increase the ability of the analyst to apply the appropriate models to these <b>data.</b> <b>Access</b> to the <b>programs</b> described are given along with contacts for help in using the compiled programs. KEY TERMS: biological assessments; statistics; clumped distributions; negative binomia...|$|R
40|$|Data {{parallel}} languages {{based on}} usersupplied data distribution directives significantly simplify {{the development of}} the initial version of a parallel application. However, selection of good data distributions leading to efficient computations is often quite difficult. Therefore, performance debuggers are usually employed to yield insights into effects of data distribution. This paper presents design and implementation of a performance debugger that is specifically targeted to meet the performance debugging requirements of a data-parallel programming model based on userspecified data distributions. The visual interface of the performance debugger provides feedback regarding data distributions amongst processor nodes, <b>data</b> <b>access</b> patterns during <b>program</b> execution, as well as statistical performance data. The feedback provided by the performance debugger is explicitly related back to objects in the programmer's source code. INTRODUCTION The recently proposed data parallel languages, suc [...] ...|$|R
40|$|Data-parallel {{languages}} {{based on}} user-supplied data distribution directives significantly simplify {{the development of}} the initial version of a parallel application. However, selection of good data distributions leading to efficient computations is often quite difficult. Therefore, performance debuggers are usually employed to yield insights into effects of data distribution. This paper presents the design and implementation of a performance debugger which meets the requirements of a data-parallel programming model based on user-specified data distributions. Collection of performance data can be performed during an execution of the program on the real or simulated target machine. The visual interface of the performance debugger provides feedback regarding data distributions amongst processor nodes, <b>data</b> <b>access</b> patterns during <b>program</b> execution, as well as statistical performance data. The feedback provided by the performance debugger is explicitly related back to objects in the programm [...] ...|$|R
40|$|Recently, several data-parallel {{programming}} languages, such as High Performance Fortran and High Performance C, {{have been}} proposed whereby a user specifies the distribution of data amongst the processor nodes of a distributed-memory machine. Although this approach significantly simplifies {{the development of the}} initial version of a parallel application, selection of good data distributions leading to efficient computations is often quite difficult. Thus performance debuggers are employed to yield insights of the data distribution effects. In this paper, we describe the design and use of a performance debugger that is specifically targeted to meet the performance debugging requirements of a data-parallel programming model based on user-specified data distributions. The visual interface of the performance debugger provides feedback regarding data distributions amongst processor nodes, <b>data</b> <b>access</b> patterns during <b>program</b> execution, as well as statistical performance data. The feedback pro [...] ...|$|R
50|$|Geodetic Data Services (GDS) program {{provides}} {{services for the}} long-term stewardship of unique data sets. These services organize, manage, and archive data, and develop tools for <b>data</b> <b>access</b> and interpretation. GDS provides a comprehensive suite of services including sensor network data operations, data products and services, data management and archiving, and advanced cyberinfrastructure. Services are provided for GPS/GNSS data, Imaging data, Strain and Seismic data, and Meteorological data. GPS/GNSS data enable millimeter-scale surface motions at discrete points. Data from geodetic imaging instruments {{can be used to}} map topography and delineate deformation with high spatial resolution. InSAR and Terrestrial LiDAR imaging data services are provided. Strain and seismic data from borehole strainmeters, seismometers, thermometers, pore pressure transducers, tiltmeters, and rock samples from drilling, as well as surface-based tiltmeters and laser strainmeters are available. In addition, temperature, relative humidity, and atmospheric pressure data are available from surface measurements of atmospheric conditions from stations. Tropospheric parameters are generated during daily GPS post-processing managed by UNAVCO and are accessible through <b>data</b> <b>access</b> services. The <b>program</b> is optimized to enable access to high-precision geodetic data. The UNAVCO Data Archive includes more than 2,300 continuous GPS stations.|$|R
40|$|In this paper, {{we examine}} the causes and eects of {{contention}} for shared <b>data</b> <b>access</b> in parallel <b>programs</b> running on a software distributed shared memory (DSM) system. Specically, we experiment on two widely-used, page-based protocols, Princeton's home-based lazy release consistency (HLRC) and TreadMarks. For most of our programs, these protocols were equally aected by latency increases caused by contention and achieved similar performance. Where they dier signicantly, HLRC's ability to manually eliminate load imbalance was the largest factor accounting for the dierence. To quantify the eects of contention we either modied the application to eliminate {{the cause of the}} contention or modied the underlying protocol to eciently handle it. Overall, we nd that contention has profound eects on performance: eliminating contention reduced execution time by 64 % in the most extreme case, even at the relatively modest scale of 32 nodes that we consider in this paper. 1 I [...] ...|$|R
40|$|This paper {{describes}} an algorithm for deriving data and computation partitions to improve memory locality on scalable shared memory multiprocessors. The algorithm first determines computation partitions by establishing affinity between where the computations are performed {{and where the}} data is located based on <b>data</b> <b>accesses</b> in the <b>program.</b> In the process of deriving the computation partitions, static data partitions are derived for the arrays. These static partitions {{may not be the}} best choice for some arrays. The partitioning of these arrays is reconsidered by evaluating the costs of all possible partitions for each such array individually using depth-first search with pruning. The cost of a given data partition is computed using knowledge of cache, local and remote memory access costs as well as the costs of contention and synchronization. Experimental results from a prototype implementation of the algorithm demonstrate that it is computationally efficient and that it is effective in [...] ...|$|R
40|$|The system {{efficiency}} and throughput of most architectures are critically {{dependent on the}} ability of the memory subsystem to satisfy <b>data</b> operand <b>accesses.</b> This ability is in turn dependent on the distribution or layout of the data relative to the <b>access</b> of the <b>data</b> by the executing code. Page faults, cache misses, truncated vectors, global communication, for example, are expensive but common symptoms of <b>data</b> and <b>access</b> misalignment. Compiler optimization, traditionally synonymous with code optimization, has addressed the issue of efficient <b>data</b> <b>access</b> by manipulating the code to better <b>access</b> the <b>data</b> under a fixed, default distribution. This approach is restrictive, and often suboptimal. Data optimization, or data-layout optimization, is presented {{as an integral part of}} compiler optimization. For scalar data, a good compile-time approximation of the "reference string," or sequence of <b>data</b> <b>accesses,</b> is advanced for the purpose of distributing the data. However, the optimal distribution of the scalar data for such, or any, reference string is proved NP-complete. A methodology and a polynomial algorithm for an approximate solution are developed. Experiments with representative, but scaled, scientific programs and execution environments display a reduction in cache misses up to two orders in magnitude. For array data, compile-time predictions of the patterns in which the <b>data</b> is <b>accessed</b> by <b>programs</b> in scalar and array languages are examined. For arbitrary computations in an array language, the determination of the optimal layout of the data is proved to be NP-complete. Polynomial techniques for the approximate solutions to the optimal layout of arrays in both languages, scalar and array, are outlined. The general applicability of the techniques, in terms of environments other than hierarchical memories, and in terms of interdependence with code manipulations, is discussed. New code optimizations inspired by the data distribution techniques are motivated. The prudence of compiler- over user-optimized data distribution is argued...|$|R
40|$|The current {{main memory}} (DRAM) access speeds lag far behind CPU speeds. Cache memory, made of static RAM, {{is being used}} in today's {{architectures}} to bridge this gap. It provides access latencies of 2 [...] 4 processor cycles, in contrast to main memory which requires 15 [...] 25 cycles. Therefore, {{the performance of the}} CPU depends upon how well the cache can be utilized. We show that there are significant benefits in redesigning our traditional query processing algorithms so that they can make better use of the cache. The new algorithms run 8 % [...] 200 % faster than the traditional ones. 1 Introduction The DRAM access speeds have not reduced much compared to the CPU cycle time reduction resulting from the improvements in VLSI technology. Cache memories, made of fast static RAM, help alleviate this disparity by exploiting the spatial and temporal locality in the <b>data</b> <b>accesses</b> of a <b>program.</b> However, <b>programs</b> with poor <b>access</b> locality waste significantly many cycles transferring the data to and from th [...] ...|$|R
40|$|This {{document}} {{constitutes the}} first deliverable of MAFTIA work package 1. The {{objective of this}} work package is to define a consistent framework for ensuring the dependability of distributed applications {{in the face of}} a wide class of threats. In particular, the aim is to develop a coherent set of concepts for an architecture that can tolerate deliberately malicious faults, such as intrusions, in applications distributed over the Internet. The intrusions of concern include not only those perpetrated by external penetrators, but also those carried out by corrupt insiders, i. e., users who are authorized to access the system but not authorized for the <b>accessed</b> <b>data,</b> <b>program</b> or resource, and administrators who misuse their rights. Although intrusions are the primary class of targeted faults, the architecture should also be adequately robust towards accidental physical faults and accidental design fault...|$|R
40|$|Due to new United States (U. S.) Federal Regulations and the Council for the Accreditation of Educator Preparation {{standards}} {{focusing on}} K- 12 student outcomes, teacher education providers {{are facing the}} challenge of gaining <b>access</b> to <b>data</b> on their graduates. Our College of Education (COE) has taken strategic steps to secure post-graduation <b>data</b> <b>access</b> for <b>program</b> completers with {{the primary purpose of}} using it for program improvement. To do this, we needed a disciplined process for employing data as the driver for improvement. Improvement science provides a methodology for accelerating the process of learning to improve through disciplined inquiry. Central to this approach are gradual, iterative cycles that focus on evidence related to specific problems of practice and the influence of system factors on the implementation of change. Working within networked communities, practitioners engage in rapid cycles of learning through a plan-do-study-act process that seeks to build shared knowledge and ownership within the improvement process. Using the improvement science model as our guide, we started by focusing our work to be problem specific and user-centered. Specifically, we needed to better align our candidate intake, assessment, and graduation processes across five teacher education programs. We also sought to learn more about variations between program processes. We used our exploration to align around clear action steps serving an overall COE goal. Through this process, we have learned that the tools and processes of improvement science offer a way for teacher education providers to build capacity and drive innovative improvement initiatives...|$|R
40|$|In {{recent years}} the amount of {{biological}} data has exploded {{to the point where}} much useful information can only be extracted by complex computational analyses. Such analyses are greatly facilitated by metadata standards, both in terms of the ability to compare data originating from different sources, and in terms of exchanging data in standard forms, e. g. when running processes on a distributed computing infrastructure. However, standards thrive on stability whereas science tends to constantly move, with new methods being developed and old ones modified. Therefore maintaining both metadata standards, and all the code that is required to make them useful, is a non-trivial problem. Memops is a framework that uses an abstract definition of the metadata (described in UML) to generate internal data structures and subroutine libraries for <b>data</b> <b>access</b> (application <b>programming</b> interfaces - APIs - currently in Python, C and Java) and data storage (in XML files or databases). For the individual project these libraries obviate the need for writing code for input parsing, validity checking or output. Memops also ensures that the code is always internally consistent, massively reducing the need for code reorganisation. Across a scientific domain a Memops-supported data model makes it easier to support complex standards that can capture all the data produced in a scientific area, share them among all programs in a complex software pipeline, and carry them forward to deposition in an archive. The principles behind the Memops generation code will be presented, along with example applications in Nuclear Magnetic Resonance (NMR) spectroscopy and structural biology...|$|R
40|$|Plant Reactome ([URL] is a free, open-source, curated plant pathway {{database}} portal, {{provided as}} part of the Gramene project. The database provides intuitive bioinformatics tools for the visualization, analysis and interpretation of pathway knowledge to support genome annotation, genome analysis, modeling, systems biology, basic research and education. Plant Reactome employs the structural framework of a plant cell to show metabolic, transport, genetic, developmental and signaling pathways. We manually curate molecular details of pathways in these domains for reference species Oryza sativa (rice) supported by published literature and annotation of well-characterized genes. Two hundred twenty-two rice pathways, 1025 reactions associated with 1173 proteins, 907 small molecules and 256 literature references have been curated to date. These reference annotations were used to project pathways for 62 model, crop and evolutionarily significant plant species based on gene homology. Database users can search and browse various components of the database, visualize curated baseline expression of pathway-associated genes provided by the Expression Atlas and upload and analyze their Omics datasets. The database also offers <b>data</b> <b>access</b> via Application <b>Programming</b> Interfaces (APIs) and in various standardized pathway formats, such as SBML and BioPAX...|$|R
40|$|Transparent {{persistence}} {{promises to}} integrate programming languages and databases by allowing procedural <b>programs</b> to <b>access</b> persistent <b>data</b> {{with the same}} ease as non-persistent data. In this work we demonstrate the practical feasibility of a technique for extracting queries from object-oriented programs that use transparent persistence to <b>access</b> <b>data.</b> A <b>program</b> analysis derives query structure and conditions across methods that <b>access</b> persistent <b>data.</b> The system combines static analysis and runtime query composition to handle procedures that return persistent values. Our prototype Java compiler implements the analysis, and handles recursion and parameterized queries. We {{evaluate the effectiveness of}} query extraction on the OO 7 and TORPEDO benchmarks. This work is focused on programs written in the current version of Java, without languages changes. However, the techniques developed here may also be of value in conjunction with object oriented languages extended with highlevel query syntax. 1...|$|R
40|$|INTRODUCTION As {{real-time}} applications {{grow more}} and more complex, so do {{the ways in which}} they maintain and <b>access</b> <b>data.</b> As <b>programs</b> are required to manage larger and large volumes of data, they typically turn away from applicationspecific solutions and seek general, adaptable, modular ways to manage data. Conventional systems use Database Management Systems (DBMS) to achieve these ends, and DBMS technology is well-understood. Despite all of its features, however, a conventional DBMS is not quite capable of meeting the demands of a real-time system. Typically, its goals are to maximize transaction throughput, minimize response time, and provide some degree of fairness. An RTDB, however, must adopt goals which are consistent with any real-time system: providing the best service to the most critical transactions and ensuring some degree of predictability in transaction processing. The StarBase RTDB is an attempt to merge conventional DBMS functionality with real-time technolog...|$|R
40|$|Previous work in {{real-time}} {{database management}} systems (RT-DBMS) has primarily based on simulation. This paper discusses how current real-time {{technology has been}} applied to architect an actual RTDBMS on a real-time microkernel operating system. A real RT-DBMS must confront many practical issues which simulations typically ignore: race conditions, concurrency, and asynchrony. The challenge of constructing a RT-DBMS is divided into three basic problems: dealing with resource contention, dealing with data contention, and enforcing timing constraints. In this paper, we present our approaches to each problem. 1 Introduction As real-time applications grow more and more complex, so do {{the ways in which they}} maintain and <b>access</b> <b>data.</b> As <b>programs</b> are required to manage larger and large volumes of data, they typically turn away from application-specific solutions and seek general, adaptable, modular ways to manage data. Conventional systems use Database Management Systems (DBMS) to achieve th [...] ...|$|R
40|$|An actual {{development}} of a real-time database management system (RT-DBMS) must confront many new and practical problems which have been often ignored in previous work based on simulation. In addition to transaction scheduling, an RT-DBMS must deal with resource and data contention and enforce timing constraints. This paper discusses how current real-time technology {{has been applied to}} architect an RT-DBMS on a real-time microkernel operating system and how the problems have been addressed. We present the current software architecture of our StarBase RT-DBMS and address how it can be extended to support both guaranteed and non-guaranteed transaction processing. keywords: real-time database systems, real-time transaction processing, concurrency control, predictability 1 Introduction As real-time applications grow more and more complex, so do {{the ways in which they}} maintain and <b>access</b> <b>data.</b> As <b>programs</b> are required to manage larger and large volumes of data, they typically turn away fro [...] ...|$|R
40|$|Secular {{trends in}} medical {{knowledge}} generation, infor-mation dissemination, and shared medical decision making will substantially alter {{the structure of}} industry-sponsored clinical research—all for the better. These trends include democratization of medical information and move-ment away from a paternalistic health care model. Both challenge legacy behaviors in industry research (where clin-ical data are typically sequestered and intended solely for internal company, regulatory body, and medical publica-tion uses) and in product marketing to physician providers and patient consumers. The thriving interest in open sci-ence and the inevitable widespread adoption of data shar-ing will be the centerpieces of this positive disruption (1). Concerns of specious multiplicity of secondary data analy-ses, groundless litigation (2), exposure of confidential in-formation for industry (3), {{and the desire for}} proprietary <b>access</b> to <b>data</b> in academia (3, 4) hinder the potential to improve public health and augment patient safety by shar-ing and pooling data sets resident in medical industry, ac-ademia, and regulatory authorities (5), including neutral and negative studies that go unpublished (6, 7). Aware of these concerns, Medtronic (Minneapolis, Minnesota) partnered with the Yale University Open <b>Data</b> <b>Access</b> (YODA) <b>program</b> 2 years ago to gain objective anal-yses of the totality of data for 1 of our products, INFUSE (bone morphogenetic protein- 2), which was approved by the U. S. Food and Drug Administration and available since 2002. In this approach, we transferred all patient-level data in our possession to YODA, from completed randomized trials and nonrandomized studies and all reg-ulatory adverse event reports for YODA-directed system-atic reviews that were done through 2 independent aca-demic systematic review centers. Summary reports of these reviews, which were not available to the company before publication, appear in this issue (8, 9). In addition, we further committed to open <b>access</b> of these <b>data</b> sets to the public. To learn how to apply to <b>access</b> the <b>data,</b> visi...|$|R
40|$|AbstractÐExploiting {{locality}} {{of references}} has become extremely important in realizing the potential performance of modern machines with deep memory hierarchies. The <b>data</b> <b>access</b> patterns of <b>programs</b> {{and the memory}} layouts of the <b>accessed</b> <b>data</b> sets {{play a critical role}} in determining the performance of applications running on these machines. This paper presents a cache locality optimization technique that can optimize a loop nest even if the arrays referenced have different layouts in memory. Such a capability is required for a global locality optimization framework that applies both loop and data transformations to a sequence of loop nests for optimizing locality. Our method uses a single linear algebra framework to represent both data layouts and loop transformations. It computes a nonsingular loop transformation matrix such that, in a given loop nest, data locality is exploited in the innermost loops, where it is most useful. The inverse of a nonsingular transformation matrix is built column-by-column, starting from the rightmost column. In addition, our approach can work in those cases where the data layouts of a subset of the referenced arrays is unknown; this is a key step in optimizing a sequence of loop nests and whole programs for locality. Experimental results on an SGI/Cray Origin 2000 nonuniform memory access multiprocessor machine show that our technique reduces execution times by as much as 70 percent. Index TermsÐData reuse, cache locality, memory layouts, loop transformations, program optimization...|$|R
40|$|Abstract. According to {{different}} node <b>data</b> <b>access</b> objects, {{and based on}} the idea of behavior-driven development, an interface-oriented node <b>data</b> <b>access</b> tool is designed and implemented. The tool mainly implements an executable node <b>data</b> <b>access</b> tool that is based on MFC, and writes a set of Win 32 console application, which improves the node <b>data</b> automation <b>access</b> scripts and encapsulates the module interfaces, thus improving the distributed network <b>data</b> <b>access</b> speed...|$|R
5000|$|Exception {{handling}} - translating <b>data</b> <b>access</b> related {{exception to}} a Spring <b>data</b> <b>access</b> hierarchy ...|$|R
50|$|The Deputy Secretary of Defense {{chairs the}} Special <b>Access</b> <b>Program</b> Oversight Committee (SAPOC), which has {{oversight}} responsibilities and provides recommendations {{with respect to}} changes in status of the Department's Special <b>Access</b> <b>Programs,</b> for either the Deputy Secretary Defense or the Secretary of Defense to make.|$|R
