11|133|Public
3000|$|... [...]. As an application, we compute both minimum numbers {{explicitly}} in four concrete <b>geometric</b> <b>sample</b> situations. The Nielsen {{decomposition of}} a coincidence set is {{induced by the}} decomposition of a certain path space [...]...|$|E
40|$|Surface {{reconstruction}} {{is to find}} {{a surface}} from a given finite set of <b>geometric</b> <b>sample</b> values. In many applications, the sample values are points. Reverse engineering of geometric shapes is the process of converting a large number of measured data points into a concise and consistent computer representation. The “feature points ” techniques are used to create mesh from the extraction points. The present work is to develop a system for image reconstruction from scattered cloud points. Various algorithms like crust algorithm and Delaunay algorithm will be implemented and compared for time taken by the algorithm for surface reconstruction...|$|E
40|$|A total logging residue {{assessment}} {{system is}} proposed specifically for application in the Pinus radiata D. Don. (radiata pine) plantations in South Australia and Western Victoria. The choice of line intersect sampling using pseudo-circular sample lines, believed {{not to have}} been tried before, ensures a robust sampling technique able to overcome any bias that exists in the alignment of residue following harvesting. An analysis is presented defining the bias and precision obtained from a variety of <b>geometric</b> <b>sample</b> line arrangements and sampling intensities. A cost effective residue sampling system of known efficiency can be implemented as an integral component in an overall yield regulation and control system. Restricted Access: University of Melbourne Staff and Students Onl...|$|E
50|$|The {{purpose of}} a 3D scanner is usually to create a point cloud of <b>geometric</b> <b>samples</b> {{on the surface of}} the subject. These points can then be used to {{extrapolate}} the shape of the subject (a process called reconstruction). If colour information is collected at each point, then the colours {{on the surface of the}} subject can also be determined.|$|R
2500|$|... {{where we}} {{recognize}} [...] as the logarithm of the <b>sample</b> <b>geometric</b> mean and [...] as the logarithm of the <b>sample</b> <b>geometric</b> mean based on (1−X), the mirror-image ofX. For , {{it follows that}} [...]|$|R
40|$|ABSTRACT. In signal processing, communications, {{and other}} {{branches}} of information technologies, {{it is often}} desirable to map the higher-dimensional signals on Sn. In this article we introduce a novel method of representing signals on Sn. This approach is based on geometric function theory, in particular on the theory of quasiregular mappings. The importance of sampling is underlined, and new <b>geometric</b> <b>sampling</b> theorems for general manifolds are presented. 1...|$|R
40|$|Abstract. In {{classical}} {{fixed point}} and coincidence theory {{the notion of}} Nielsen numbers {{has proved to be}} extremely fruitful. Here we extend it to pairs (f 1, f 2) of maps between manifolds of arbitrary dimensions. This leads to estimates of the minimum numbers MCC(f 1, f 2) (and MC(f 1, f 2), resp.) of pathcomponents (and of points, resp.) in the coincidence sets of those pairs of maps which are homotopic to (f 1, f 2). Furthermore we deduce finiteness conditions for MC(f 1, f 2). As an application we compute both minimum numbers explicitly in four concrete <b>geometric</b> <b>sample</b> situations. The Nielsen decomposition of a coincidence set is induced by the decomposition of a certain path space E(f 1, f 2) into pathcomponents. Its higher dimensional topology captures further crucial geometric coincidence data. An analoguous approach can be used to define also Nielsen numbers of certain link maps...|$|E
40|$|The thermo-mechanical {{behaviour}} of polycrystalline Ni-Mn-Ga {{depends on}} microstructure and several <b>geometric</b> <b>sample</b> properties like grain diameter and sample size. The interdependence of microstructure and shape is {{more pronounced in}} plate-shaped specimens. A significant increase of the magnetic field induced strain (MFIS) may be expected for coarse grained specimen because of fewer grain boundaries which are hindering twin boundary movement. It was found that larger grains do not result in higher reorientation strain in every case because the elastically stored energy is distributed on less grain boundaries. Thus, {{in this case the}} instability of grain boundaries against cracks is larger than in the case of fine grained samples. The findings were used to design new composites of Ni-Mn-Ga and polyurethane which show up to 1 % MFIS and are stable over one million cycles of strain. The particles in this composite are still oriented in a { 110 } fiber texture and the volume fraction of polyurethane is up to 10 %...|$|E
40|$|Figure 1 : Examples of {{the noisy}} input images {{generated}} by a Monte Carlo-renderer and our filtered results. Our denoising technique allows rendering at interactive frame rates. We present a fast reconstruction filtering method for images generated with Monte Carlo-based rendering tech-niques. Our approach specializes in reducing global illumination noise {{in the presence of}} depth-of-field effects at very low sampling rates and interactive frame rates. We employ edge-aware filtering in the sample space to locally improve outgoing radiance of each sample. The improved samples are then distributed in the image plane using a fast, linear manifold-based approach supporting very large circles of confusion. We evaluate our filter by applying it to several images containing noise caused by Monte Carlo-simulated global illumination, area light sources and depth-of-field. We show that our filter can efficiently denoise such images at interactive frame rates on current GPUs and with as few as four to 16 samples per pixel. Our method operates only on the color and <b>geometric</b> <b>sample</b> information output of the initial rendering process. It does not make any assumptions on the underlying rendering technique and sampling strategy and can therefore be implemented completely as a post-process filter...|$|E
40|$|We {{describe}} {{the design and}} implementation of an autonomous adaptive software agent that addresses the practical problem of observing undersampled, periodic, time-varying phenomena using a network of HTN-compliant robotic telescopes. The algorithm governing the behaviour of the agent uses an optimal <b>geometric</b> <b>sampling</b> technique to cover the period range of interest, but additionally implements proactive behaviour that maximises the optimality of the dataset {{in the face of}} a...|$|R
5000|$|... #Caption: <b>Geometric</b> Kufic <b>sample</b> (Surah 112, al-Ikhlas or [...] "The Surah of Monotheism", of the Qur'an), read clockwise, {{starting}} at bottom left.|$|R
40|$|Abstract. As {{the size}} of the {{databases}} increases, machine learning algorithms face more demanding requirements for efficiency and accuracy. Rather than analyzing all the data, an alternative approach is to use only a small subset trying to achieve competitive results with less resources. Progressive sampling follows this approach, starting with a small sample of data and increasing the size progressively until the accuracy (or other performance measure) cannot be improved. Progressive sampling aims for equivalent accuracies obtained with all the data but with significantly less resources. This paper introduces a new progressive sampling algorithm called NSC which: (i) defines an initial data set with a simple to evaluate measure and close to the sample size where the accuracy no longer improves, (ii) proposes a sampling schedule which is more aggressive than arithmetic sampling but more conservative than the standard <b>geometric</b> <b>sampling</b> with a <b>geometric</b> factor of 2, and (iii) introduces a termination criterion based on an adaptive local linear regression algorithm and on the complexity of the machine learning algorithm used to generate models. It is shown experimentally that NSC consistently outperforms both arithmetic and <b>geometric</b> <b>sampling</b> on several databases. ...|$|R
40|$|Quantitative {{radiological}} analysis {{attempts to}} determine the quantity of activity or concentration of specific radionuclide(s) in a sample. Based upon the certified standards {{that are used to}} calibrate gamma spectral detectors, geometric similarities between sample shape and the calibration standards determine if the analysis results developed are qualitative or quantitative. A sample analyzed that does not mimic a calibrated sample geometry must be reported as a non-standard geometry and thus the results are considered qualitative and not quantitative. MicroShieldR or ISOCSR calibration software can be used to model non-standard <b>geometric</b> <b>sample</b> shapes in an effort to obtain a quantitative analytical result. MicroShieldR and Canberra's ISOCSR software contain several geometry templates that can provide accurate quantitative modeling for a variety of sample configurations. Included in the software are computational algorithms that are used to develop and calculate energy efficiency values for the modeled sample geometry which can then be used with conventional analysis methodology to calculate the result. The response of the analytical method and the sensitivity of the mechanical and electronic equipment to the radionuclide of interest must be calibrated, or standardized, using a calibrated radiological source that contains a known and certified amount of activity...|$|E
40|$|The inner {{structure}} of materials is usually studied through {{the realization of}} suitable <b>geometric</b> <b>sample,</b> which is most often a plane section. Spatial characteristics are then estimated {{with the use of}} standard stereological methods. In the presented thesis a model of Voronoj tessellation generated by an adequate point process is used for ultra-grained materials' structure. Every tessellation cell is given its orientation attribute which represents the position of crystal lattice inside. The developed computer model allows to generate a tessellation with oriented cells according {{to a wide range of}} elective parameters, to perform the section of the structure and to calculate basic shape and size quantities with the purpose of further statistical processing. Its next function is to visualize the orientations in a pole or an inverse pole gure and to nd out the prevailing directions in the structure. Besides examining stereological methods, basics of point processes and theoretical properties of tessellations, a brief introduction into inner {{structure of}} crystal materials is included as well. Furthermore, the thesis involves the explanation of basic principals of their investigation using modern electron microscopes. The colorful picture supplement involves graphical simulation-outputs as well as samples of [...] ...|$|E
40|$|The {{functional}} characteristics of plant cell walls {{depend on the}} composition of the cell wall polymers, as well as on their highly ordered architecture at scales from a few nanometres to several microns. Raman spectra of wood acquired with linear polarized laser light include information about polymer composition as well as the alignment of cellulose microfibrils with respect to the fibre axis (microfibril angle). By changing the laser polarization direction in 3 ° steps, the dependency between cellulose and laser orientation direction was investigated. Orientation-dependent changes of band height ratios and spectra were described by quadratic linear regression and partial least square regressions, respectively. Using the models and regressions with high coefficients of determination (R 2 [*]>[*] 0. 99) microfibril orientation was predicted in the S 1 and S 2 layers distinguished by the Raman imaging approach in cross-sections of spruce normal, opposite, and compression wood. The determined microfibril angle (MFA) in the different S 2 layers ranged from 0 ° to 49. 9 ° and was in coincidence with X-ray diffraction determination. With the prerequisite of <b>geometric</b> <b>sample</b> and laser alignment, exact MFA prediction can complete the picture of the chemical cell wall design gained by the Raman imaging approach at the micron level in all plant tissues...|$|E
40|$|Abstract. We {{proceed with}} our recently-introduced <b>geometric</b> {{approach}} to <b>sampling</b> of manifolds and investigate {{the relationship that}} exists between the classical, i. e. Shannon type, and <b>geometric</b> <b>sampling</b> concepts and formalism. Some aspects of coding and the Gaussian channel problem are considered. A geometric version of Shannon’s Second Theorem is introduced. Applications to Pulse Code Modulation and Vector Quantization of Images are provided. An extension of our sampling scheme to a certain class of infinite dimensional manifolds is also considered. The relationship between real functions of bounded curvature and classical band-limited signals is investigated. 1. General background 1. 1. Introduction. A sampling theorem for differentiable manifolds was recently presented and applied in the context image processing ([22], [23]) : Theorem 1. 1. Let Σn ⊂ Rn+ 1, n ≥ 2 be a connected, not necessarily compact, smooth hypersurface, with finitely many compact boundary components...|$|R
40|$|In this paper, {{we study}} the {{convolutions}} of heterogeneous exponential and geometric random variables {{in terms of}} the weakly majorization order () of parameter vectors and the likelihood ratio order (>=lr). It is proved that order between two parameter vectors implies >=lr order between convolutions of two heterogeneous exponential (<b>geometric)</b> <b>samples.</b> For the two-dimensional case, it is found that there exist stronger equivalent characterizations. These results strengthen the corresponding ones of Boland etÂ al. [Boland, P. J., El-Neweihi, E., Proschan, F., 1994. Schur properties of convolutions of exponential and geometric random variables. Journal of Multivariate Analysis 48, 157 - 167] by relaxing the conditions on parameter vectors from the majorization order () to order. ...|$|R
40|$|We {{describe}} an algorithm for computing {{the intersection of}} n balls of equal radius in R³ which runs in time O(n lg² n). The algorithm can be parallelized so that the comparisons that involve the radius of the balls are performed in O(lg³ n) batches. Using parametric search, these algorithms are used to obtain an algorithm for computing the diameter {{of a set of}} n points in R³ (the maximum distance between any pair) which runs in time O(n lg^ 5 n). The algorithms are deterministic and elementary; this is in contrast with the running time O(n log n) in both cases that can be achieved using randomization [3], and the running times O(n lg n) and O(n lg³ n) using deterministic <b>geometric</b> <b>sampling</b> [2, 1]...|$|R
40|$|Abstract. Existing {{theories}} on shape digitization impose strong constraints on feasible shapes and require error-free measurements. We use Delaunay triangulation and ff-shapes {{to prove that}} topologically correct segmentations can be obtained under much more realistic conditions. Our key assumption is that sampling points represent object boundaries with a certain maximum error. Experiments on real and generated images demonstrate the good performance and correctness of the new method. 1 Introduction A fundamental question of image analysis is how closely a computed imagesegmentation corresponds to the underlying real-world partitioning. Existing <b>geometric</b> <b>sampling</b> theorems are limited to binary partitionings, where the planeis split into (not necessarily connected) fore- and background components. In this case, the topology of the partition is preserved under various discretizationschemes when the original regions ar...|$|R
40|$|Having {{access to}} {{massive amounts of}} data does not {{necessarily}} imply that induction algorithms must use them all. Samples often provide the same accuracy with far less computational cost. However, the correct sample size is rarely obvious. We analyze methods for progressive sampling [...] starting with small samples and progressively increasing them as long as model accuracy improves. We show that a simple, <b>geometric</b> <b>sampling</b> schedule is efficient in an asymptotic sense. We then explore the notion of optimal efficiency: what is the absolute best sampling schedule? We describe the issues involved in instantiating an "optimally efficient" progressive sampler. Finally, we provide empirical results comparing a variety of progressive sampling methods. We conclude that progressive sampling often is preferable to analyzing all data instances...|$|R
40|$|In {{the present}} paper, we {{describe}} new robust methods of estimating cell shape and orientation in 3 D from sections. The descriptors of 3 D cell shape and orientation {{are based on}} volume tensors which are used to construct an ellipsoid, the Miles ellipsoid, approximating the average cell shape and orientation in 3 D. The estimators of volume tensors are based on observations in several optical planes through sampled cells. This type of <b>geometric</b> <b>sampling</b> design {{is known as the}} optical rotator. The statistical behaviour of the estimator of the Miles ellipsoid is studied under a flexible model for 3 D cell shape and orientation. In a simulation study, the lengths of the axes of the Miles ellipsoid can be estimated with CVs of about 2...|$|R
40|$|In {{this paper}} we discuss ł-policy iteration, a method for exact and {{approximate}} dynamic programming. It is intermediate between the classical value iteration (VI) and policy iteration (PI) methods, and it {{is closely related to}} optimistic (also known as modified) PI, whereby each policy evaluation is done approximately, using a finite number of VI. We review the theory of the method and associated questions of bias and exploration arising in simulation-based cost function approximation. We then discuss various implementations, which offer advantages over well-established PI methods that use LSPE(ł), LSTD(ł), or TD(ł) for policy evaluation with cost function approximation. One of these implementations is based on a new simulation scheme, called <b>geometric</b> <b>sampling,</b> which uses multiple short trajectories rather than a single infinitely long trajectory...|$|R
40|$|AbstractWe {{describe}} an algorithm for computing {{the intersection of}} n balls of equal radius in R 3 which runs in time O(nlg 2 n). The algorithm can be parallelized so that the comparisons that involve the radius of the balls are performed in O(lg 3 n) batches. Using parametric search, these algorithms are used to obtain an algorithm for computing the diameter {{of a set of}} n points in R 3 (the maximum distance between any pair) which runs in time O(n lg 5 n). The algorithms are deterministic and elementary; this is in contrast with the running time O(nlgn) in both cases that can be achieved using randomization (Clarkson and Shor, 1989), and the running times O(nlg n) and O(nlg 3 n) using deterministic <b>geometric</b> <b>sampling</b> (Brönnimann et al., 1993; Amato et al., 1994) ...|$|R
500|$|Then {{the maximum}} spacing {{estimator}} of θ0 {{is defined as}} a value that maximizes the logarithm of the <b>geometric</b> mean of <b>sample</b> spacings: ...|$|R
40|$|We {{present a}} data-parallel, High Performance Fortran (HPF) {{implementation}} of the geometric partitioning algorithm. The geometric partitioning algorithm has provably good partitioning quality. To our knowledge, our implementation is the first data [...] parallel {{implementation of the}} algorithm. Our data [...] parallel formulation makes extensive use of segmented prefix sums and parallel selections, and provide a dataparallel procedure for <b>geometric</b> <b>sampling.</b> Experiments in partitioning particles for load [...] balance and data interactions as required in hierarchical N-body algorithms and iterative algorithms for the solution of equilibrium equations on unstructured meshes by the finite element method {{have shown that the}} geometric partitioning algorithm has an efficient data [...] parallel formulation. Moreover, the quality of the generated partitions is competitive with that offered by the spectral bisection technique and better than the quality offered by other partitioning heuristics...|$|R
40|$|Computerized image {{analysis}} makes {{statements about the}} continous world by looking at a discrete representation. Therefore, {{it is important to}} know precisely which information is preserved during digitization. We analyse this question in the context of shape recognition. Existing results in this area are based on very restricted models and thus not applicable to real imaging situations. We present generalizations in several directions: first, we introduce a new shape similarity measure that approximates human perception better. Second, we prove a <b>geometric</b> <b>sampling</b> theorem for arbitrary dimensional spaces. Third, we extend our sampling theorem to 2 -dimensional images that are subjected to blurring by a disk point spread function. Our findings are steps towards a general sampling theory for shapes that shall ultimately describe the behavior of real optical systems. Key words: shape preservation, digitization, discretization, sampling theory...|$|R
40|$|Exactly {{computing}} network {{reliability and}} performability measures are NP-hard problems, precluding their frequent use in design of large networks. Instead, Monte Carlo simulation has been frequently used by network designers to obtain accurate estimates. This paper focuses on simulation estimation of network reliability and performability. First, a literature survey of existing approaches is given. Then, using a heap data structure, efficient implementation of two previous approaches, dagger sampling and Markov model, are proposed. Two new techniques, <b>geometric</b> <b>sampling</b> and block sampling, are developed to efficiently sample states of a network. These techniques are event-driven rather than time-driven, {{and are thus}} efficient for highly reliable networks. To gauge relative performance, computational experiments are carried out on various types of networks using the existing and the new procedures. These networks include up to 400 nodes and both binary and non binary structure functions are used. ...|$|R
2500|$|... {{which leads}} to the {{following}} solution for the initial values (of the estimate shape parameters in terms of the <b>sample</b> <b>geometric</b> means) for an iterative solution: ...|$|R
40|$|We {{investigate}} {{a method of}} dividing an irregular mesh into equal-sized pieces with few interconnecting edges. The method's novel feature is that it exploits the geometric coordinates of the mesh vertices. It is based on theoretical work of Miller, Teng, Thurston, and Vavasis, who showed that certain classes of "well-shaped" finite element meshes have good separators. The geometric method is quite simple to implement: we describe a Matlab code for it in some detail. The method is also quite efficient and effective: we compare it with some other methods, including spectral bisection. Keywords: centerpoints, conformal mapping, data-parallel algorithms, <b>geometric</b> <b>sampling,</b> graph and <b>geometric</b> algorithms, Matlab, mesh partitioning, moment of inertia, parallel processing, scientific computing, separators, sparse matrix computations. AMS(MOS) subject classifications: 65 F 50, 68 Q 20 Computing Reviews descriptors: G. 1. 3 [Numerical Analysis]: Numerical Linear Algebra [...] - Linear systems (di [...] ...|$|R
40|$|We {{describe}} two deterministic algorithms {{for constructing}} the arrangement {{determined by a}} set of (algebraic) curve segments in the plane. They both use a divide-and-conquer approach based on derandomized <b>geometric</b> <b>sampling</b> and achieve the optimal running time O(n log n + k), where n is the number of segments and k is the number of intersections. The first algorithm, a simplified version of one presented in [1], generates a structure of size O(n log log n + k) and its parallel implementation runs in time O(log 2 n). The second algorithm is better in that the decomposition of the arrangement constructed has optimal size O(n + k) and it has a parallel implementation in the EREW PRAM model that runs in time O(log 3 = 2 n). The improvements in the second algorithm are achieved by means of an approach that adds some degree of globality to the divide-and-conque...|$|R
40|$|A laser stripe sensor {{has limited}} {{application}} when a point cloud of <b>geometric</b> <b>samples</b> {{on the surface}} of the object needs to be collected, so a galvanometric laser scanner is designed by using a one-mirror galvanometer element as its mechanical device to drive the laser stripe to sweep along the object. A novel mathematical model is derived for the proposed galvanometer laser scanner without any position assumptions and then a model-driven calibration procedure is proposed. Compared with available model-driven approaches, the influence of machining and assembly errors is considered in the proposed model. Meanwhile, a plane-constraint-based approach is proposed to extract a large number of calibration points effectively and accurately to calibrate the galvanometric laser scanner. Repeatability and accuracy of the galvanometric laser scanner are evaluated on the automobile production line to verify the efficiency and accuracy of the proposed calibration method. Experimental results show that the proposed calibration approach yields similar measurement performance compared with a look-up table calibration method...|$|R
40|$|We {{show that}} several discrepancy-like {{problems}} can be solved in NC nearly achieving the discrepancies guaranteed by a probabilistic analysis and achievable sequentially. For example, we describe an NC algorithm that given a set system (X; S), where X is a ground set and S ` 2 X, computes a set R ` X so that for each S 2 S the discrepancy jjR " Sj Γ jR"Sjj is O(p jSj log jSj). Whereas previous NC algorithms could only achieve discrepancies O(p jSj 1 +ffl log jSj) with ffl ? 0, ours matches the probabilistic bound within a multiplicative factor 1 +o(1). Other problems whose NC solution we improve are lattice approximation, ffl-approximations of range spaces with constant VC-exponent, <b>sampling</b> in <b>geometric</b> configuration spaces, approximation of integer linear programs, and edge coloring of graphs. Key Words: Discrepancy, lattice approximation, parallel algorithms, derandomization, <b>geometric</b> <b>sampling.</b> 1 Introduction Problem and Previous Work. Discrepancy is an important [...] ...|$|R
40|$|Applying certain {{flexible}} <b>geometric</b> <b>sampling</b> of a multi-scale invariant (MSI) field {{we provide}} a multi-dimensional multi-selfsimilar field {{which has a}} one to one correspondence with such sampled MSI field. This sampling enables us to characterize harmonic-like representation and spectral density function of the sampled MSI field. Imposing Markov property for the MSI field, {{we find that the}} covariance function and spectral density matrix of such sampled Markov MSI field are characterized by the covariance functions of samples of the first scale rectangle. We present an example of MSI field as two-dimensional simple fractional Brownian motion. We consider a real data example of the precipitation in some area of Brisbane in Australia for some special period. We show that precipitation on this area has MSI property and estimate time dependent scale and Hurst parameters of this MSI field in three dimension as latitude, longitude and time. Our method enables one to predict precipitation in time and place. Comment: 15 page...|$|R
40|$|In this paper, we {{introduce}} the mobile embedded system implemented for capturing stereo image {{based on two}} CMOS camera module. We use WinCE as an operating system and capture the stereo image by using device driver for CMOS camera interface and Direct Draw API functions. We aslo comments on the GPU hardware and CUDA programming for implementation of 3 D exaggeraion algorithm for ROI by adjusting and synthesizing the disparity value of ROI (region of interest) in real time. We comment on the pattern of aperture for deblurring of CMOS camera module based on the Kirchhoff diffraction formula and clarify {{the reason why we}} can get more sharp and clear image by blocking some portion of aperture or <b>geometric</b> <b>sampling.</b> Synthesized stereo image is real time monitored on the shutter glass type three-dimensional LCD monitor and disparity values of each segment are analyzed to prove the validness of emphasizing effect of ROI...|$|R
40|$|The main {{objective}} of this thesis is the development and exploitation of techniques to generate <b>geometric</b> <b>samples</b> {{for the purpose of}} image segmentation. A sampling-based approach provides a number of benefits over existing optimization-based methods such as robustness to noise and model error, characterization of segmentation uncertainty, natural handling of multi-modal distributions, and incorporation of partial segmentation information. This is important for applications which suffer from, e. g., low signal-to-noise ratio (SNR) or ill-posedness. We create a curve sampling algorithm using the Metropolis-Hastings Markov chain Monte Carlo (MCMC) framework. With this method, samples from a target distribution π (which can be evaluated but not sampled from directly) are generated by creating a Markov chain whose stationary distribution is π and sampling many times from a proposal distribution q. We define a proposal distribution using random Gaussian curve perturbations, and show how to ensure detailed balance and ergodicity of the chain so that iterates of the Markov chain asymptotically converge to samples from π. We visualiz...|$|R
40|$|We {{modify the}} Reyes object-space shading {{approach}} to address two inefficiencies {{that result from}} performing shading calculations at micropolygon grid vertices prior to rasterization. Our system samples shading of surface sub-patches uniformly in the object’s parametric domain, but the location of shading samples need not correspond with the location of mesh vertices. Thus we perform object-space shading that efficiencly supports motion and defocus blur, but do not require micropolygons to achieve a shading rate of one sample per pixel. Second, our system resolves surface visibility prior to shading, then lazily shades 2 x 2 sample blocks that are known {{to contribute to the}} resulting fragments. We find that in comparison to a Reyes micropolygon rendering pipeline, decoupling <b>geometric</b> <b>sampling</b> rate from shading rate permits the use of meshes containing an order of magnitude fewer vertices with minimal loss of image quality in our test scenes. Shading on-demand after rasterization reduces shader invocations by over two times in comparison to pre-visibility object-space shading. Categories and Subject Descriptors (according to ACM CCS) : Generation—Line and curve generation I. 3. 3 [Computer Graphics]: Picture/Image 1...|$|R
