5116|10000|Public
5|$|A {{hurricane}} in {{the middle}} of September affected some of the same areas along the northern coast of the Gulf of Mexico as the August hurricane. It was first detected on the evening of September 10 in the southeastern Gulf, although its track may be far off from the actual storms movement {{due to a lack of}} available information. Its first <b>data</b> <b>point</b> in the hurricane database lists it as possessing sustained winds of , placing it at Category 2 intensity on the Saffir-Simpson Hurricane Scale. The storm followed a broad northward curve through the central Gulf of Mexico, making landfall along the Mississippi River Delta of Louisiana on the night of September 14. It continued toward the north-northeast, dissipating on September 16.|$|E
5|$|Cognitive and emotional: neuropsychiatric {{symptomatology}} {{is common}} {{in the course of}} the disease. Depression and anxiety appear in up to 80% of patients,. Emotional lability leading to uncontrollable crying is also common. These symptoms can be treated with antidepressants and cognitive behavioral therapy; however, high quality studies on efficacy are lacking. For example, in the specific case of antidepressants and depression, only two studies were considered worth considering as of 2011 by the Cochrane collaboration and they only showed a trend towards efficacy. Other neuropsychiatric symptoms are euphoria and disinhibition. Cognitive impairment is a frequent complication of MS even after the introduction of disease-modifying treatments in the last 20 years. Although the disease is usually the primary cause of cognitive problems, other factors such as medications, relapses or depression may be enhancing them so a correct evaluation of the deficits and factors exacerbating them is important. Regarding primary deficits, <b>data</b> <b>point</b> towards administration of L-amphetamine and methylphenidate being useful, whereas memantine and anticholinesterase drugs such as donepezil—commonly used in Alzheimer disease— are not considered effective in improving cognitive functions. Effectiveness of cognitive rehabilitation therapy is more controverted. For those patients with MS who have pseudobulbar affect (PBA), characterized by uncontrollable episodes of crying and/or laughing, or other emotional displays, Dextromethorphan/quinidine can be considered as treatment as it is the only FDA approved drug for treatment for PBA, though other medications such as selective serotonin reuptake inhibitors, tricyclic antidepressants have been used in clinical practice.|$|E
25|$|When each <b>data</b> <b>point</b> {{is itself}} a function, it can be useful to see the {{interpolation}} problem as a partial advection problem between each <b>data</b> <b>point.</b> This idea leads to the displacement interpolation problem used in transportation theory.|$|E
25|$|The term {{extrapolation}} {{is used to}} find <b>data</b> <b>points</b> {{outside the}} range of known <b>data</b> <b>points.</b>|$|R
3000|$|... 25 values {{determined}} by the LNISO methods (six <b>data</b> <b>points</b> vs. 11 <b>data</b> <b>points)</b> demonstrated similar deviations from the results by the conventional isothermal method. The average deviations were[*]±[*] 44 % by the LNISO method (6 <b>data</b> <b>points)</b> versus ± 43 % by the LNISO method (11 <b>data</b> <b>points).</b> For the above analysis, the two outlier compounds (A 11 and A 16) were excluded.|$|R
40|$|The {{existing}} {{surface roughness}} standards comprise only two dimensions. However, the real roughness {{of the surface}} is 3 D (three-dimensional). Roughness parameters of the 3 D surface are also important in analyzing the mechanics of contact surfaces. Problems of mechanics of contact surfaces are related to accuracy of 3 D surface roughness characteristic. One {{of the most important}} factors for 3 D characteristics determination is the number of <b>data</b> <b>points</b> per* mdy axes. With number of <b>data</b> <b>points</b> we understand its number in cut-off length. Number of <b>data</b> <b>points</b> have substantial influence on the accuracy of measurement results, measuring time and size of output data file (especially along they-axis direction, where number of <b>data</b> <b>points</b> are number of parallel profiles). Number of <b>data</b> <b>points</b> must be optimal. Small number of <b>data</b> <b>points</b> lead to incorrect results and increase distribution amplitude, but too large number of <b>data</b> <b>points</b> do not enlarge range of fundamental information, but substantially increase measuring time. Therefore, we must find optimal number of <b>data</b> <b>points</b> per each surface processing method...|$|R
25|$|If δ > Rejection Region, the <b>data</b> <b>point</b> is an outlier.|$|E
25|$|DCTs {{of types}} I-IV treat both {{boundaries}} consistently regarding {{the point of}} symmetry: they are even/odd around either a <b>data</b> <b>point</b> for both boundaries or halfway between two data points for both boundaries. By contrast, DCTs of types V-VIII imply boundaries that are even/odd around a <b>data</b> <b>point</b> for one boundary and halfway between two data points for the other boundary.|$|E
25|$|These {{constraints}} {{state that}} each <b>data</b> <b>point</b> must {{lie on the}} correct side of the margin.|$|E
50|$|An {{admissible}} model must {{be consistent}} with all the <b>data</b> <b>points.</b> Thus, the straight line (heighti = b0 + b1agei) is not {{a model of the}} data. The line cannot be a model, unless it exactly fits all the <b>data</b> <b>points</b> - i.e. all the <b>data</b> <b>points</b> lie perfectly on a straight line. The error term, εi, must be included in the model, so that the model is consistent with all the <b>data</b> <b>points.</b>|$|R
50|$|Relational {{perspective}} map is {{a multidimensional}} scaling algorithm. The algorithm finds a configuration of <b>data</b> <b>points</b> on a manifold by simulating a multi-particle dynamic system on a closed manifold, where <b>data</b> <b>points</b> are mapped to particles and distances (or dissimilarity) between <b>data</b> <b>points</b> represent a repulsive force. As the manifold gradually grows in size the multi-particle system cools down gradually and converges to a configuration {{that reflects the}} distance information of the <b>data</b> <b>points.</b>|$|R
3000|$|... [...])=K×log(m). Where, K is the {{attribute}} {{number of}} <b>data</b> <b>points</b> and m {{is the number}} of <b>data</b> <b>points</b> in cluster C [...]...|$|R
25|$|If a <b>data</b> <b>point</b> (or points) is {{excluded}} from the data analysis, this should be clearly stated on any subsequent report.|$|E
25|$|The prior {{predictive}} {{distribution and}} posterior predictive distribution {{of a new}} normally distributed <b>data</b> <b>point</b> when a series of independent identically distributed normally distributed data points have been observed, with prior mean and variance as in the above model.|$|E
25|$|The {{experimental}} data will comprise {{a set of}} data points. At the i'th <b>data</b> <b>point,</b> the analytical concentrations of the reactants, TA(i), TB(i) etc. will be experimentally known quantities {{and there will be}} one or more measured quantities, yi, that depend in some way on the analytical concentrations and equilibrium constants. A general computational procedure has three main components.|$|E
40|$|A {{method is}} presented, based on linear interpolation, for {{detecting}} and correcting bad <b>data</b> <b>points</b> {{in a set}} of data without contaminating the good <b>data</b> <b>points.</b> The method used is not concerned with the small random errors usually attributed to a noisy system. It assumes that the <b>data</b> <b>points</b> which are in error are relatively isolated from each other and that the number of such points is small compared to the total number of <b>data</b> <b>points...</b>|$|R
40|$|Abstract Recently, local {{discriminant}} embedding (LDE) {{was proposed}} {{as a means}} of addressing manifold learning and pattern classification. In the LDE framework, the neigh-bor and class of <b>data</b> <b>points</b> are used to construct the graph embedding for classification problems. From a high dimensional to a low dimensional subspace, <b>data</b> <b>points</b> of the same class maintain their intrinsic neighbor relations, whereas neighboring <b>data</b> <b>points</b> of different classes no longer stick to one another. But, neighboring <b>data</b> <b>points</b> of different classes are not deemphasized efficiently by LDE and it may degrade the performance of classification. In this paper, we investigate its extension, called class mean embedding (CME), using class mean of <b>data</b> <b>points</b> to enhance its discriminant power in their mapping into a low dimen-sional space. After joined class mean <b>data</b> <b>points,</b> (1) CME may cause each class of <b>data</b> <b>points</b> to be more compact in the high dimension space; (2) CME may increase the quantity of <b>data</b> <b>points,</b> and solves the small sample size (SSS) problem; (3) CME may preserve well the local geometry of the data manifolds in the embedding space. Experimental results on ORL, Yale, AR, and FERET face databases show the effectiveness of the proposed method...|$|R
30|$|Anomalies are {{abnormal}} {{events or}} patterns {{that do not}} conform to expected events or patterns [1]. Identifying anomalies is important in a broad set of disciplines; including, medical diagnosis, insurance and identity fraud, network intrusion, and programming defects. Anomalies are generally categorized into three types: point, or content anomalies; context anomalies, and collective anomalies. Point anomalies occur for <b>data</b> <b>points</b> that are considered abnormal when viewed against the whole dataset. Context anomalies are <b>data</b> <b>points</b> that are considered abnormal when viewed against meta-information associated with the <b>data</b> <b>points.</b> Finally, collective anomalies are <b>data</b> <b>points</b> which are considered anomalies when viewed with other <b>data</b> <b>points,</b> against the rest of the dataset.|$|R
25|$|The surveys, {{which were}} given more {{significance}} by standard marketing {{procedures of the}} era, were less negative and were key in convincing management to change the formula in 1985, {{to coincide with the}} drink's centenary. But the focus groups had provided a clue as to how the change would play out in a public context, a <b>data</b> <b>point</b> the company downplayed but which proved important later.|$|E
25|$|Tufte {{encourages}} {{the use of}} data-rich illustrations that present all available data. When such illustrations are examined closely, every <b>data</b> <b>point</b> has a value, but when they are looked at more generally, only trends and patterns can be observed. Tufte suggests these macro/micro readings be presented {{in the space of}} an eye-span, in the high resolution format of the printed page, and at the unhurried pace of the viewer's leisure.|$|E
25|$|The {{modified}} Thompson Tau test {{is used to}} {{find one}} outlier at a time (largest value of δ is removed if it is an outlier). Meaning, if a <b>data</b> <b>point</b> {{is found to be}} an outlier, it is removed from the data set and the test is applied again with a new average and rejection region. This process is continued until no outliers remain in a data set.|$|E
30|$|The {{number of}} <b>data</b> <b>points</b> {{locating}} {{in the same}} cell is determined using segmented parallel reduction. As described above, after sorting all <b>data</b> <b>points</b> according to cell indices, all <b>data</b> <b>points</b> are stored {{in a group of}} segments; each segment is flagged with the cell index, and contains the indices of <b>data</b> <b>points</b> locating in the same cell. The number of <b>data</b> <b>points</b> locating in the same cell can be achieved by performing a reduction for each segment; see Fig.  3 a. Similarly, the head index of the first point of each segment can be obtained using segmented parallel scan; see Fig.  3 b.|$|R
3000|$|We {{initialize}} {{the model}} parameters using all <b>data</b> <b>points</b> and their labels {{in the training}} set in Algorithm 1. In particular, we use the {{mean and standard deviation}} of the <b>data</b> <b>points</b> in each class to initialize Ω and the ratio of <b>data</b> <b>points</b> in different classes to initialize α [...]...|$|R
50|$|Users {{may have}} {{particular}} <b>data</b> <b>points</b> of interest within a data set, {{as opposed to}} general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding <b>data</b> <b>points,</b> and arranging <b>data</b> <b>points.</b>|$|R
25|$|This hypothesis, {{created to}} explain palaeomagnetic data, {{suggests}} that Earth's {{axis of rotation}} shifted one or more times during the general time-frame attributed to snowball Earth. This could feasibly produce the same distribution of glacial deposits without requiring any of them to have been deposited at equatorial latitude. While the physics behind the proposition is sound, the removal of one flawed <b>data</b> <b>point</b> from the original study rendered {{the application of the}} concept in these circumstances unwarranted.|$|E
25|$|The {{modified}} Thompson Tau test is {{a method}} used to determine if an outlier exists in a data set. The strength of this method {{lies in the fact}} that it takes into account a data set’s standard deviation, average and provides a statistically determined rejection zone; thus providing an objective method to determine if a <b>data</b> <b>point</b> is an outlier. Note: Although intuitively appealing, this method appears to be unpublished (it is not described in Thompson (1985)) and one should use it with caution.|$|E
25|$|The goal of density {{estimation}} {{is to take}} {{a finite}} sample of data and to make inferences about the underlying probability density function everywhere, including where no data are observed. In kernel density estimation, the contribution of each <b>data</b> <b>point</b> is smoothed out from a single point into a region of space surrounding it. Aggregating the individually smoothed contributions gives an overall picture {{of the structure of the}} data and its density function. In the details to follow, we show that this approach leads to a reasonable estimate of the underlying density function.|$|E
50|$|Perpendicular {{regression}} fits {{a line to}} <b>data</b> <b>points</b> by {{minimizing the}} sum of squared perpendicular distances from the <b>data</b> <b>points</b> to the line.|$|R
30|$|The <b>data</b> <b>points</b> being {{far away}} from trend of {{equality}} might be caused by manipulation of the specimens (triangle <b>data</b> <b>points</b> in Fig.  3).|$|R
3000|$|... where N is {{the number}} of <b>data</b> <b>points</b> during each day, xp,i and yM,i are, respectively, the {{predicted}} and measured (observed) <b>data</b> <b>points,</b> [...]...|$|R
25|$|In statistics, {{ordinary}} {{least squares}} (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, {{with the goal of}} minimizing the sum of the squares {{of the differences between the}} observed responses (values of the variable being predicted) in the given dataset and those predicted by a linear function of a set of explanatory variables. Visually this is seen as the sum of the squared vertical distances between each <b>data</b> <b>point</b> in the set and the corresponding point on the regression line – the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side.|$|E
25|$|Projected lattice {{geometries}} can {{be represented}} by so-called ‘lattice-fringe fingerprint plots’ (LFFPs), also called angular covariance plots. The horizontal axis of such a plot is given in reciprocal lattice length and {{is limited by the}} point resolution of the microscope. The vertical axis is defined as acute angle between Fourier transformed lattice fringes or electron diffraction spots. A 2D <b>data</b> <b>point</b> is defined by the length of a reciprocal lattice vector and its (acute) angle with another reciprocal lattice vector. Sets of 2D data points that obey Weiss’s zone law are subsets of the entirety of data points in an LFFP. A suitable search-match algorithm using LFFPs, therefore, tries to find matching zone axis subsets in the database. It is, essentially, a variant of a lattice matching algorithm.|$|E
25|$|In a mixed {{review of}} the book in the Wall Street Journal, William Easterly was {{generally}} supportive of the plausibility of the book's thesis but critiqued the book's failure to cite extant statistics-based {{evidence to support the}} validity of the historical case studies. For example, in the book's example about Congo, the stated reason Congo is impoverished is that Congo is close to slave trade shipping points. The approach of this historical case study only offers one <b>data</b> <b>point.</b> Moreover, Easterly also points out the danger of ex-post rationalization that the book only attributes different levels of development to institutions in a way a bit too neat. For example, to explain the fall of Venice, it could be the extractive regime during the time or it could also be the shift from Mediterranean trade to Atlantic trade. The historical case studies approach might be biased.|$|E
30|$|Here, x and y {{represent}} {{the mean of}} the <b>data</b> <b>points</b> xi and yi respectively. The number of <b>data</b> <b>points</b> is denoted by n.|$|R
25|$|From the {{analysis}} of the case with unknown mean but known variance, we see that the update equations involve sufficient statistics computed from the data consisting of the mean of the <b>data</b> <b>points</b> and the total variance of the <b>data</b> <b>points,</b> computed in turn from the known variance divided by the number of <b>data</b> <b>points.</b>|$|R
50|$|In general, super-sampling is a {{technique}} of collecting <b>data</b> <b>points</b> at a greater resolution (usually by a power of two) than the final data resolution. These <b>data</b> <b>points</b> are then combined (down-sampled) to the desired resolution, often just by a simple average. The combined <b>data</b> <b>points</b> have less visible aliasing artifacts (or moiré patterns).|$|R
