0|3789|Public
30|$|Therefore, we use {{different}} criteria for object labeling. We <b>calculate</b> <b>four</b> <b>evaluation</b> <b>metrics</b> of (1) pixel-wise precision rate per object averaged over all object predictions (Mi-AP), (2) pixel-wise recall rate per object of groundtruth (Mi-AR), (3) pixel-wise precision rate over all pixels (Ma-AP), and (4) pixel-wise recall rate over all pixels (Ma-AR).|$|R
50|$|The <b>four</b> major <b>evaluation</b> <b>metrics</b> for locks {{in general}} are uncontended lock-acquisition latency, bus traffic, fairness, and storage.|$|R
40|$|A deep feature based {{saliency}} model (DeepFeat) {{is developed}} to leverage {{the understanding of}} the prediction of human fixations. Traditional saliency models often predict the human visual attention relying on few level image cues. Although such models predict fixations on a variety of image complexities, their approaches are limited to the incorporated features. In this study, we aim to provide an intuitive interpretation of convolu- tional neural network deep features by combining low and high level visual factors. We exploit <b>four</b> <b>evaluation</b> <b>metrics</b> to evaluate the correspondence between the proposed framework and the ground-truth fixations. The key findings of the results demon- strate that the DeepFeat algorithm, incorporation of bottom up and top down saliency maps, outperforms the individual bottom up and top down approach. Moreover, in comparison to nine 9 state-of-the-art saliency models, our proposed DeepFeat model achieves satisfactory performance based on all <b>four</b> <b>evaluation</b> <b>metrics.</b> Comment: 9 pages, 7 figures, submitted to IEEE transactions on cognitive developmental system...|$|R
30|$|In conclusion, HCR outperforms RIM and HCR {{in terms}} of the <b>four</b> <b>evaluation</b> <b>metrics</b> on all the aspects whether the network is sparse or dense because of its {{proactive}} backup node selection and reactive cascading node motion. Though the proactive backup node selection is time consuming, it is carried out before the node failure. This will improve the response time. During the reactive cascading node motion, the computing of BestPosition is complex and time-consuming. While HCR is a distributed and localized method, the number of each node’s neighbors is very small in all kinds of applications, so the selection of BestPosition will not spend so much time, and the connectivity restoration process will be fast.|$|R
40|$|Abstract — One of {{the main}} design issues for a sensor network is {{conservation}} of the energy available at each sensor node. We propose to deploy multiple, mobile base stations to prolong the lifetime of the sensor network. We split the lifetime of the sensor network into equal periods of time known as rounds. Base stations are relocated {{at the start of}} a round. Our method uses an integer linear program to determine new locations for the base stations and a flow-based routing protocol to ensure energy efficient routing during each round. We propose <b>four</b> <b>evaluation</b> <b>metrics</b> and compare our solution using these metrics. Based on the simulation results we show that employing multiple, mobile base stations in accordance with the solution given by our schemes would significantly increase the lifetime of the sensor network. I...|$|R
40|$|Our goal is {{to develop}} a {{cognitive}} model of how humans acquire skills on complex, sensorimotor tasks. To achieve this goal, we collected data from subjects learning the NRL Navigation task, then used the data to construct a model that reflects the basic, cognitive elements required to learn and thereby succeed at this task (Gordon & Subramanian, 1997). This paper describes a new experiment with human subjects on the task. Data from this experiment not only confirms the key cognitive element of our model, but also helps us better understand individual differences in learning this task. <b>Four</b> <b>evaluation</b> <b>metrics</b> indicate that we are able to model important trends in the evolution of action choice. Introduction Our {{goal is to}} model how humans acquire skills on complex, cognitive tasks. We are pursuing this goal by designing computational architectures for the NRL Navigation task, which requires competent sensorimotor coordination. To achieve this goal, we first constructed a model reflecti [...] ...|$|R
40|$|Validation {{methods used}} in {{literature}} to evaluate vessel segmentation algorithms suffer {{to a great}} extent from objectiveness, reliability and reproducibility. This is because almost each group has its own way to evaluate an algorithms. In this paper, an extendable standardized evaluation framework for quantitative validation of vessel segmentation algorithms is presented. As ground-truth, it uses a physical vascular model to simulate the growth of vessels within organ masks extracted from clinical CT datasets. A set of image- and graph- based <b>evaluation</b> <b>metrics</b> are <b>calculated</b> to analyze various aspects of the algorithms under study. Using the proposed framework helps to meet the aforementioned quality criteria...|$|R
40|$|The Government Performance and Results Act (GPRA) of 1993 was legislated in the United States to, {{among other}} things, hold Federal {{agencies}} accountable for achieving program results, and through systematic program evaluations to improve Congressional decision making. Two technologies {{funded by the}} U. S. Department of Energy (DOE) {{that are related to}} improved fuel efficiency in new heavy-duty diesel trucks are studied in this paper—laser diagnostics and optical engine technologies, and combustion modeling. Based on DOE cost data, and benefit data derived from field-based interviews, economic <b>evaluation</b> <b>metrics</b> are <b>calculated.</b> They suggest that DOE’s investments in these two technologies have been socially valuable. innovation; program evaluation; social rate of return...|$|R
40|$|Needle {{segmentation}} is {{a fundamental}} step for needle reconstruction and image-guided surgery. Although there has been success stories in needle segmentation for non-microsurgeries, the methods cannot be directly extended to ophthalmic surgery due to the challenges bounded to required spatial resolution. As the ophthalmic surgery is performed by finer and smaller surgical instruments in micro-structural anatomies, specifically in retinal domains, difficulties are raised for delicate operation and sensitive perception. To address these challenges, {{in this paper we}} investigate needle segmentation in ophthalmic operation on 60 Optical Coherence Tomography (OCT) cubes captured during needle injection surgeries on ex-vivo pig eyes. Furthermore, we developed two different approaches, a conventional method based on morphological features (MF) and a specifically designed full convolution neural networks (FCN) method, moreover, we evaluate them on the benchmark for needle segmentation in the volumetric OCT images. The experimental results show that FCN method has a better segmentation performance based on <b>four</b> <b>evaluation</b> <b>metrics</b> while MF method has a short inference time, which provides valuable reference for future works...|$|R
40|$|The GaoFen- 2 {{satellite}} (GF- 2) is a self-developed civil optical {{remote sensing}} satellite of China, {{which is also}} the first satellite with the resolution of being superior to 1 meter in China. In this paper, we propose a pan-sharpening method based on guided image filtering, apply it to the GF- 2 images and compare the performance to state-of-the-art methods. Firstly, a simulated low-resolution panchromatic band is yielded; thereafter, the resampled multispectral image is taken as the guidance image to filter the simulated low resolution panchromatic Pan image, and extracting the spatial information from the original Pan image; finally, the pan-sharpened result is synthesized by injecting the spatial details into each band of the resampled MS image according to proper weights. Three groups of GF- 2 images acquired from water body, urban and cropland areas have been selected for assessments. <b>Four</b> <b>evaluation</b> <b>metrics</b> are employed for quantitative assessment. The experimental results show that, for GF- 2 imagery acquired over different scenes, the proposed method can not only achieve high spectral fidelity, but also enhance the spatial detail...|$|R
40|$|Abstract—Class {{imbalance}} is {{a problem}} that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and <b>four</b> <b>evaluation</b> <b>metrics.</b> RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data. Index Terms—Binary classification, boosting, class imbalance, RUSBoost, sampling. I...|$|R
40|$|Subspace {{clustering}} aims {{to group}} data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering methods {{assume that the}} data could be linearly represented {{with each other in}} the input space. In practice, however, this assumption is hard to be satisfied. To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces. The first advantage makes our method efficient in handling large-scale data sets, and the second one enables the proposed method to address the nonlinear subspace clustering challenge. Extensive experiments on five real-world datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding <b>four</b> <b>evaluation</b> <b>metrics.</b> Comment: 12 page...|$|R
40|$|In a {{football}} stadium environment with multiple overhead floodlights, many protruding shadows {{can be observed}} orig-inating {{from each of the}} targets. To successfully track in-dividual targets, it is essential to achieve an accurate rep-resentation of the foreground. Many of the existing tech-niques are sensitive to shadows, falsely classifying shadows as foreground. This work presents four different techniques associated with shadow classification. Three of the classi-fier’s originate from the review material whilst the fourth is a novel application of a real-time implementation of the k-nearest neighbour algorithm to shadow identification. To assess the performance for each of the classifiers <b>four</b> quan-titative <b>evaluation</b> <b>metrics</b> are proposed. Using each of the <b>evaluation</b> <b>metrics,</b> we will discuss the performance of each classifier’s segmentation results as well as assess their im-pact on the tracking performances. 1...|$|R
40|$|Classification {{algorithms}} {{are unable}} to make reliable models on the datasets with huge sizes. These datasets contain many irrelevant and redundant features that mislead the classifiers. Furthermore, many huge datasets have imbalanced class distribution which leads to bias over majority class in the classification process. In this paper combination of unsupervised dimensionality reduction methods with resampling is proposed {{and the results are}} tested on Lung-Cancer dataset. In the first step PCA is applied on Lung-Cancer dataset to compact the dataset and eliminate irrelevant features and in the second step SMOTE resampling is carried out to balance the class distribution and increase the variety of sample domain. Finally, Naïve Bayes classifier is applied on the resulting dataset and the results are compared and <b>evaluation</b> <b>metrics</b> are <b>calculated.</b> The experiments show the effectiveness of the proposed method across four evaluatio...|$|R
40|$|Dust storms are devastating natural {{disasters}} that cost {{billions of dollars}} and many human lives every year. Using the Non-Hydrostatic Mesoscale Dust Model (NMM-dust), this research studies how different spatiotemporal resolutions of two input parameters (soil moisture and greenness vegetation fraction) impact the sensitivity and accuracy of a dust model. Experiments are conducted by simulating dust concentration during July 1 - 7, 2014, for the target area covering part of Arizona and California (31, 37, - 118, - 112), with a resolution of ~ 3 km. Using ground-based and satellite observations, this research validates the temporal evolution and spatial distribution of dust storm output from the NMM-dust, and quantifies model error using measurements of <b>four</b> <b>evaluation</b> <b>metrics</b> (mean bias error, root mean square error, correlation coefficient and fractional gross error). Results showed that the default configuration of NMM-dust (with a low spatiotemporal resolution of both input parameters) generates an overestimation of Aerosol Optical Depth (AOD). Although it is able to qualitatively reproduce the temporal trend of the dust event, the default configuration of NMM-dust cannot fully capture its actual spatial distribution. Adjusting the spatiotemporal resolution of soil moisture and vegetation cover datasets showed that the model is sensitive to both parameters. Increasing the spatiotemporal resolution of soil moisture effectively reduces model's overestimation of AOD, while increasing the spatiotemporal resolution of vegetation cover changes the spatial distribution of reproduced dust storm. The adjustment of both parameters enables NMM-dust to capture the spatial distribution of dust storms, as well as reproducing more accurate dust concentration...|$|R
40|$|Clustering [...] {{automatically}} sorting [...] web {{search results}} {{has been the}} focus of much attention but is by no means a solved problem, and there is little previous work in Swedish. This thesis studies the performance of three clustering algorithms [...] k-means, agglomerative hierarchical clustering, and bisecting k-means [...] on a total of 32 corpora, as well as whether clustering web search previews, called snippets, instead of full texts can achieve reasonably decent results. <b>Four</b> internal <b>evaluation</b> <b>metrics</b> are used to assess the data. Results indicate that k-means performs worse than the other two algorithms, and that snippets may be good enough to use in an actual product, although there is ample opportunity for further research on both issues; however, results are inconclusive regarding bisecting k-means vis-à-vis agglomerative hierarchical clustering. Stop word and stemmer usage results are not significant, and appear to not affect the clustering by any considerable magnitude...|$|R
40|$|Delay Tolerant Networks (DTNs) {{uses the}} store-carry-forward scheme for the {{delivery}} of the messages, with this way data transmission can be successfully done despite {{of the absence of}} continuous end-to-end paths. The opportunities of message forwarding in such types of networks usually are limited due to the absence of contemporaneous paths. In such networks, the “store-carry-forward ” methodology is used for the transmission of the messages to be delivered to their intend destinations in a hop by hop manner. It arises many problems like how to schedule the messages, how to drop the messages in the buffer due to the impulsive nature of the nodes. It also arises many challengeable situations like short contact durations between the two nodes, limited storage capacity of nodes and so on. This paper evaluates the performance of MOFO buffer management technique with three routing protocols i. e. Epidemic, Prophet and MaxProp under variable message buffer sizes (5 MB to 40 MB). Such evaluation can improve the performance of the opportunistic networks by reducing the overhead ratio, enhancing the delivery rate, minimizing latency average and hop count average in a certain degree. So <b>four</b> performance <b>evaluation</b> <b>metrics</b> namely delivery probability, latency average, overhead ratio and hop count average are used in this study. This study uses ONE (Opportunistic Network Environment) simulator for the performance evaluation of the MOFO buffer management technique and routing protocols. The evaluation results shows that the performance of different protocols can benefit to optimizing the performance of delay tolerant networks in terms of deliver...|$|R
40|$|Abstract. The {{concept of}} a {{negative}} class {{does not apply to}} many problems for which classification is increasingly utilized. In this study we investigate the reliability of <b>evaluation</b> <b>metrics</b> when the negative class contains an unknown proportion of mislabeled positive class instances. We examine how <b>evaluation</b> <b>metrics</b> can inform us about potential systematic biases in the data. We provide a motivating case study and a general framework for approaching evaluation when the negative class contains mislabeled positive class instances. We show that the behavior of <b>evaluation</b> <b>metrics</b> is unstable in the presence of uncertainty in class labels and that the stability of <b>evaluation</b> <b>metrics</b> depends on the kind of bias in the data. Finally, we show that the type and amount of bias present in data can {{have a significant effect on}} the ranking of <b>evaluation</b> <b>metrics</b> and the degree to which they over- or underestimate the true performance of classifiers...|$|R
40|$|This paper {{describes}} a feasibility study of n-gram-based <b>evaluation</b> <b>metrics</b> for automatic keyphrase extraction. To account for near-misses currently ignored by standard <b>evaluation</b> <b>metrics,</b> we adapt various <b>evaluation</b> <b>metrics</b> developed for machine translation and summarization, {{and also the}} R-precision evaluation metric from keyphrase evaluation. In evaluation, the R-precision metric is found to achieve the highest correlation with human annotations. We also provide evidence {{that the degree of}} semantic similarity varies with the location of the partially-matching component words. ...|$|R
40|$|Abstract Background The {{performance}} of 3 D-based virtual screening similarity functions {{is affected by}} the applied conformations of compounds. Therefore, the results of 3 D approaches are often less robust than 2 D approaches. The application of 3 D methods on multiple conformer data sets normally reduces this weakness, but entails a significant computational overhead. Therefore, we developed a special conformational space encoding by means of Gaussian mixture models and a similarity function that operates on these models. The application of a model-based encoding allows an efficient comparison of the conformational space of compounds. Results Comparisons of our 4 D flexible atom-pair approach with over 15 state-of-the-art 2 D- and 3 D-based virtual screening similarity functions on the 40 data sets of the Directory of Useful Decoys show a robust {{performance of}} our approach. Even 3 D-based approaches that operate on multiple conformers yield inferior results. The 4 D flexible atom-pair method achieves an averaged AUC value of 0. 78 on the filtered Directory of Useful Decoys data sets. The best 2 D- and 3 D-based approaches of this study yield an AUC value of 0. 74 and 0. 72, respectively. As a result, the 4 D flexible atom-pair approach achieves an average rank of 1. 25 with respect to 15 other state-of-the-art similarity functions and <b>four</b> different <b>evaluation</b> <b>metrics.</b> Conclusions Our 4 D method yields a robust performance on 40 pharmaceutically relevant targets. The conformational space encoding enables an efficient comparison of the conformational space. Therefore, the weakness of the 3 D-based approaches on single conformations is circumvented. With over 100, 000 similarity calculations on a single desktop CPU, the utilization of the 4 D flexible atom-pair in real-world applications is feasible. </p...|$|R
40|$|How good {{is a given}} machine {{translation}} system? • Hard problem, since many different translations acceptable → semantic equivalence / similarity • <b>Evaluation</b> <b>metrics</b> – subjective judgments by human evaluators – automatic <b>evaluation</b> <b>metrics</b> – task-based <b>evaluation,</b> e. g. : – how much post-editing effort? – does information come across...|$|R
5000|$|... 2003. Beyond {{independent}} relevance: {{methods and}} <b>evaluation</b> <b>metrics</b> for subtopic retrieval.|$|R
30|$|The {{following}} <b>evaluation</b> <b>metrics</b> {{were employed}} to determine our method's performance [4].|$|R
30|$|In this section, {{we present}} our <b>evaluation</b> <b>metrics,</b> {{experiment}} methodology and experiment results.|$|R
5000|$|Use certain <b>evaluation</b> <b>metrics</b> for tag clouds {{with respect}} to coverage, overlap and {{selectivity}} ...|$|R
30|$|We {{conducted}} a systematic review of CAD systems and metrics to evaluate segmentation in such systems [7]. From {{a large number}} of papers retrieved, 10 detailed segmentation techniques and the <b>evaluation</b> <b>metrics</b> used in the testing stage. In this context, <b>evaluation</b> <b>metrics</b> refers to metrics that use quantitative data obtained from a system execution to attribute it a performance index.|$|R
3000|$|<b>Evaluation</b> <b>metrics</b> for theories, design processes, implementations, {{and systems}} from a human-centered perspective; and [...]...|$|R
40|$|Heart rate {{variability}} (HRV) {{has been}} used in many studies to assess the effects of autonomic regulation on the heart rate. A 1996 task force specified standards for <b>calculating</b> HRV <b>metrics</b> and reporting results. The standards focused on <b>metrics</b> <b>calculated</b> from short-term (5 min) and long-term (24 h) recordings of the electrocardiogram (ECG). We compared the accuracy of nine HRV <b>metrics</b> <b>calculated</b> from ECG records spanning 10 s to 10 min to that calculated from a 5 min record. We also estimated the reliability of all nine <b>metrics</b> <b>calculated</b> from 5 min records. We found that high frequency power (HF) and the root mean square of successive NN interval differences (RMSSD) were substantially less sensitive to the segment duration and more reliable than the other seven metrics...|$|R
5000|$|User {{navigation}} {{model that}} {{combined with the}} <b>evaluation</b> <b>metrics</b> allows a tag cloud evaluation with respect to navigation ...|$|R
40|$|We {{describe}} a dataset containing 16, 000 translations produced by four machine translation systems and manually annotated for quality by professional translators. This dataset {{can be used}} in a range of tasks assessing machine translation <b>evaluation</b> <b>metrics,</b> from basic correlation analysis to training and test of machine learning-based metrics. By providing a standard dataset for such tasks, we hope to encourage the development of better MT <b>evaluation</b> <b>metrics.</b> 1...|$|R
50|$|While {{all these}} advancements were being made, the {{community}} {{felt the need}} to have standardised datasets and <b>evaluation</b> <b>metrics</b> so the performances can be compared. This led to the emergence of challenges like the Pascal VOC challenge and the ImageNet challenge. The availability of standard <b>evaluation</b> <b>metrics</b> and the open challenges gave directions to the research. Better algorithms were introduced for specific tasks like object detection and classification.|$|R
3000|$|... [40, 41]. The {{scores of}} all five <b>evaluation</b> <b>metrics</b> closer to 1 {{indicate}} a higher {{quality of the}} composite image.|$|R
40|$|In {{this paper}} we give an {{overview}} of the <b>evaluation</b> <b>metrics</b> used to measure the performance of backchannel prediction models. Both objective and subjective <b>evaluation</b> <b>metrics</b> are discussed. The survey shows that almost every backchannel prediction model is evaluated with a different evaluation metric. This makes comparison between developed models unreliable, even beside the other variables in play, such as different corpora, language, conversational setting, amount of data and/or deﬁnition of the term backchannel...|$|R
30|$|An {{appropriate}} set {{of performance}} <b>evaluation</b> <b>metrics</b> are presented, {{related to the}} feasibility/ operational efficiency and scalability of the proposed framework.|$|R
40|$|Automatic <b>evaluation</b> <b>metrics</b> for Machine Translation (MT) systems, such as BLEU and {{the related}} NIST metric, are {{becoming}} increasingly important in MT. This paper reports a novel method of calculating the confidence intervals for BLEU/NIST scores using bootstrapping. With this method, we can determine whether two MT systems are significantly different from each other. We study the effect of test set size and number of reference translations on the confidence intervals for these MT <b>evaluation</b> <b>metrics.</b> 1...|$|R
40|$|As in most {{information}} retrieval (IR) studies, evaluation plays {{an essential part}} in Web search research. Both offline and online <b>evaluation</b> <b>metrics</b> are adopted in measuring the performance of search engines. Offline metrics are usually based on relevance judgments of query-document pairs from assessors while online metrics exploit the user behavior data, such as clicks, collected from search engines to compare search algorithms. Although both types of IR <b>evaluation</b> <b>metrics</b> have achieved success, {{to what extent can}} they predict user satisfaction still remains under-investigated. To shed light on this research question, we meta-evaluate a series of existing online and offline metrics to study how well they infer actual search user satisfaction in different search scenarios. We find that both types of <b>evaluation</b> <b>metrics</b> significantly correlate with user satisfaction while they reflect satisfaction from different perspectives for different search tasks. Offline metrics better align with user satisfaction in homogeneous search (i. e. ten blue links) whereas online metrics outperform when vertical results are federated. Finally, we also propose to incorporate mouse hover information into existing online <b>evaluation</b> <b>metrics,</b> and empirically show that they better align with search user satisfaction than click-based online metrics...|$|R
