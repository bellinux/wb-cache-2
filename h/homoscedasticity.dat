464|0|Public
25|$|If the {{assumption}} of normality is replaced by assumptions of <b>homoscedasticity</b> and uncorrelatedness of errors, and if one still assumes zero mean, then the Gauss–Markov theorem entails that the solution is the minimal unbiased estimator.|$|E
2500|$|Constant {{variance}} (a.k.a. <b>homoscedasticity).</b> [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of <b>homoscedasticity</b> (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
5000|$|The {{assumption}} of <b>homoscedasticity</b> simplifies mathematical and computational treatment. Serious violations in <b>homoscedasticity</b> (assuming a distribution of data is homoscedastic when {{in reality it}} is heteroscedastic [...] ) may result in overestimating the goodness of fit {{as measured by the}} Pearson coefficient.|$|E
50|$|The {{concept of}} <b>homoscedasticity</b> {{can be applied}} to {{distributions}} on spheres.|$|E
50|$|Bartlett {{is known}} for Bartlett's method for {{estimating}} power spectra and Bartlett's test for <b>homoscedasticity.</b>|$|E
50|$|The {{assumption}} of <b>homoscedasticity,</b> {{also known as}} homogeneity of variance, assumes equality of population variances.|$|E
5000|$|It assumes {{independence}} of the observations being tested, as well as equal variation across observations (<b>homoscedasticity).</b>|$|E
50|$|Some of the {{procedures}} typically assuming <b>homoscedasticity,</b> for which one can use Levene's tests, include analysis of variance and t-tests.|$|E
5000|$|Equality (or [...] "homogeneity") of {{variance}}s, called <b>homoscedasticity</b> [...] - [...] {{the variance}} of data in groups should be the same.|$|E
50|$|The {{variance}} of the error is constant across observations (<b>homoscedasticity).</b> If not, weighted least squares or other methods might instead be used.|$|E
50|$|Step 4: Perform a t-test on the {{equation}} selected from step 3 on γ1. If γ1 is statistically significant, reject {{the null hypothesis}} of <b>homoscedasticity.</b>|$|E
50|$|As used in {{describing}} {{simple linear regression}} analysis, one assumption of the fitted model (to ensure that the least-squares estimators are each a best linear unbiased estimator of the respective population parameters, by the Gauss-Markov theorem) is that the standard deviations of the error terms are constant and do {{not depend on the}} x-value. Consequently, each probability distribution for y (response variable) has the same standard deviation regardless of the x-value (predictor). In short, this assumption is <b>homoscedasticity.</b> <b>Homoscedasticity</b> is not required for the estimates to be unbiased, consistent, and asymptotically normal.|$|E
5000|$|LDA instead {{makes the}} {{additional}} simplifying <b>homoscedasticity</b> assumption (i.e. that the class covariances are identical, so [...] ) {{and that the}} covariances have full rank.In this case, several terms cancel: ...|$|E
50|$|Levene's test {{is often}} used before a {{comparison}} of means. When Levene's test shows significance, one should switch to more generalized tests that is free from <b>homoscedasticity</b> assumptions (sometimes even non-parametric tests).|$|E
50|$|If the {{assumption}} of normality is replaced by assumptions of <b>homoscedasticity</b> and uncorrelatedness of errors, and if one still assumes zero mean, then the Gauss-Markov theorem entails that the solution is the minimal unbiased estimator.|$|E
5000|$|Under the {{assumption}} of spherical errors, i.e. <b>homoscedasticity</b> and no serial correlation in , the FE estimator is more efficient than the FD estimator. If [...] follows a random walk, however, the FD estimator is more efficient as [...] are serially uncorrelated.|$|E
50|$|The C test {{assumes a}} {{balanced}} design, i.e. the considered full data set should consist of individual data series that all have equal size. The C test further assumes that each individual data series is normally distributed. Although primarily an outlier test, the C test {{is also in}} use as a simple alternative for regular <b>homoscedasticity</b> tests such as Bartlett's test, Levene's test and the Brown-Forsythe test to check a statistical data set for homogeneity of variances. An even simpler way to check <b>homoscedasticity</b> is provided by Hartley's Fmax test, but Hartley's Fmax test has the disadvantage that it only accounts for the minimum and the maximum of the variance range, while the C test accounts for all variances within the range.|$|E
5000|$|In statistics, a {{collection}} of random variables is heteroscedastic (or heteroskedastic; from Ancient Greek [...] “different” and [...] “dispersion”) if there are sub-populations that have different variabilities from others. Here [...] "variability" [...] could be quantified by the variance or any other measure of statistical dispersion. Thus heteroscedasticity {{is the absence of}} <b>homoscedasticity.</b>|$|E
5000|$|... 1. MLM has Less Stringent Assumptions: MLM {{can be used}} if the {{assumptions}} of constant variances (homogeneity of variance, or <b>homoscedasticity),</b> constant covariances (compound symmetry), or constant variances of differences scores (sphericity) are violated for RM-ANOVA. MLM allows modeling of the variance-covariance matrix from the data; thus, unlike in RM-ANOVA, these assumptions are not necessary.|$|E
5000|$|If, then, in a {{regression}} of ln( [...] εi {{on the natural}} logarithm {{of one or more}} of the regressors Xi, we arrive at statistical significance for non-zero values on {{one or more of the}} γi-hat, we reveal a connection between the residuals and the regressors. We reject the null hypothesis of <b>homoscedasticity</b> and conclude that heteroscedasticity is present.|$|E
5000|$|... again a {{value that}} depends on i - or, more specifically, a value that is {{conditional}} on the values {{of one or more}} of the regressors X. <b>Homoscedasticity,</b> one of the basic Gauss-Markov assumptions of ordinary least squares linear regression modeling, refers to equal variance in the random error terms regardless of the trial or observation, such that ...|$|E
50|$|The F-test is {{sensitive}} to non-normality. In the analysis of variance (ANOVA), alternative tests include Levene's test, Bartlett's test, and the Brown-Forsythe test. However, when any of these tests are conducted to test the underlying assumption of <b>homoscedasticity</b> (i.e. homogeneity of variance), as a preliminary step to testing for mean effects, there {{is an increase in}} the experiment-wise Type I error rate.|$|E
50|$|In statistics, Bartlett's test (see Snedecor and Cochran, 1989) {{is used to}} test if k {{samples are}} from populations with equal variances. Equal variances across populations is called <b>homoscedasticity</b> or {{homogeneity}} of variances. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The Bartlett test {{can be used to}} verify that assumption.|$|E
50|$|It {{is often}} stated in popular {{literature}} {{that none of}} these F-tests are robust when there are severe violations of the assumption that each population follows the normal distribution, particularly for small alpha levels and unbalanced layouts. Furthermore, it is also claimed that if the underlying assumption of <b>homoscedasticity</b> is violated, the Type I error properties degenerate much more severely.|$|E
50|$|The ANOVA on ranks {{has never}} been {{recommended}} when the underlying assumption of homogeneous variances has been violated, either by itself, or {{in conjunction with a}} violation of the assumption of population normality. In general, rank based statistics become nonrobust with respect to Type I errors for departures from <b>homoscedasticity</b> even more quickly than parametric counterparts that share the same assumption.|$|E
5000|$|Once the {{appropriateness}} of the model is determined, the parameters can be estimated by fitting a regression of [...] on [...] However, the basic assumption of <b>homoscedasticity</b> is violated, so a weighted least squares regression must be used. The inverse weight matrix will have the variances of each ratio on the diagonal with the one-step covariances on the first off-diagonal, both given below.|$|E
50|$|The normal-model based ANOVA {{analysis}} {{assumes the}} independence, normality and homogeneity of the variances of the residuals. The randomization-based analysis assumes only the homogeneity of the variances of the residuals (as {{a consequence of}} unit-treatment additivity) and uses the randomization procedure of the experiment. Both these analyses require <b>homoscedasticity,</b> as an assumption for the normal-model analysis {{and as a consequence}} of randomization and additivity for the randomization-based analysis.|$|E
5000|$|The outer {{product of}} the error vector must be spherical.This implies the error term has uniform {{variance}} (<b>homoscedasticity)</b> and no serial dependence. If this assumption is violated, OLS is still unbiased, but inefficient. The term [...] "spherical errors" [...] will describe the multivariate normal distribution: if [...] in the multivariate normal density, then the equation [...] is the formula for a ball centered at μ with radius σ in n-dimensional space.|$|E
50|$|Residuals can {{be tested}} for <b>homoscedasticity</b> using the Breusch-Pagan test, which {{performs}} an auxiliary regression of the squared residuals on the independent variables. From this auxiliary regression, the explained sum of squares is retained, divided by two, and then becomes the test statistic for a chi-squared distribution with the degrees of freedom equal {{to the number of}} independent variables. The null hypothesis of this chi-squared test is <b>homoscedasticity,</b> and the alternative hypothesis would indicate heteroscedasticity. Since the Breusch-Pagan test is sensitive to departures from normality or small sample sizes, the Koenker-Bassett or 'generalized Breusch-Pagan' test is commonly used instead. From the auxiliary regression, it retains the R-squared value which is then multiplied by the sample size, and then becomes the test statistic for a chi-squared distribution (and uses the same degrees of freedom). Although it is not necessary for the Koenker-Bassett test, the Breusch-Pagan test requires that the squared residuals also be divided by the residual sum of squares divided by the sample size. Testing for groupwise heteroscedasticity requires the Goldfeld-Quandt test.|$|E
5000|$|In statistics, the Goldfeld-Quandt test {{checks for}} <b>homoscedasticity</b> in {{regression}} analyses. It does this by dividing a dataset {{into two parts}} or groups, and hence the test is sometimes called a two-group test. The Goldfeld-Quandt test {{is one of two}} tests proposed in a 1965 paper by Stephen Goldfeld and Richard Quandt. Both a parametric and nonparametric test are described in the paper, but the term [...] "Goldfeld-Quandt test" [...] is usually associated only with the former.|$|E
50|$|Homogeneity can {{be studied}} to several degrees of complexity. For example, {{considerations}} of <b>homoscedasticity</b> examine how much the variability of data-values changes throughout a dataset. However, questions of homogeneity apply to {{all aspects of the}} statistical distributions, including the location parameter. Thus, a more detailed study would examine changes to the whole of the marginal distribution. An intermediate-level study might move from looking at the variability to studying changes in the skewness. In addition to these, questions of homogeneity apply also to the joint distributions.|$|E
5000|$|However, it {{has been}} said that {{students}} in econometrics should not overreact to heteroscedasticity. One author wrote, [...] "unequal error variance is worth correcting only when the problem is severe." [...] In addition, another word of caution was in the form, [...] "heteroscedasticity has never been a reason to throw out an otherwise good model." [...] With the advent of heteroscedasticity-consistent standard errors allowing for inference without specifying the conditional second moment of error term, testing conditional <b>homoscedasticity</b> is not as important as in the past.|$|E
50|$|This F-test {{is known}} to be {{extremely}} sensitive to non-normality, so Levene's test, Bartlett's test, or the Brown-Forsythe test are better tests for testing the equality of two variances. (However, all of these tests create experiment-wise type I error inflations when conducted as a test of the assumption of <b>homoscedasticity</b> prior to a test of effects.) F-tests for the equality of variances can be used in practice, with care, particularly where a quick check is required, and subject to associated diagnostic checking: practical text-books suggest both graphical and formal checks of the assumption.|$|E
50|$|Another {{assumption}} of linear regression {{is that the}} variance {{be the same for}} each possible expected value (this is known as <b>homoscedasticity).</b> Univariate normality is not needed for least squares estimates of the regression parameters to be meaningful (see Gauss-Markov theorem). However confidence intervals and hypothesis tests will have better statistical properties if the variables exhibit multivariate normality. This can be assessed empirically by plotting the fitted values against the residuals, and by inspecting the normal quantile plot of the residuals. Note that it is not relevant whether the dependent variable Y is marginally normally distributed.|$|E
5000|$|It is {{prudent to}} verify that the {{assumptions}} of ANOVA have been met. Residuals are examined or analyzed to confirm <b>homoscedasticity</b> and gross normality. [...] Residuals should have the appearance of (zero mean normal distribution) noise when plotted {{as a function of}} anything including time and modeled data values. Trends hint at interactions among factors or among observations. One rule of thumb: [...] "If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results will still be approximately correct." ...|$|E
50|$|A word {{of caution}} is in order when {{interpreting}} pseudo-R2 statistics. The reason these indices of fit {{are referred to as}} pseudo R2 is that they do not represent the proportionate reduction in error as the R2 in linear regression does. Linear regression assumes <b>homoscedasticity,</b> that the error variance is the same for all values of the criterion. Logistic regression will always be heteroscedastic - the error variances differ for each value of the predicted score. For each value of the predicted score there would be a different value of the proportionate reduction in error. Therefore, it is inappropriate to think of R2 as a proportionate reduction in error in a universal sense in logistic regression.|$|E
