64|1269|Public
25|$|If other {{copies of}} the <b>damaged</b> <b>data</b> exist or can be reconstructed from {{checksums}} and parity data, ZFS will use {{a copy of the}} data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update all faulty copies with known-good data and redundancy will be restored.|$|E
2500|$|In {{addition}} to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering [...] "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, {{and if the}} data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the <b>damaged</b> <b>data</b> and returns good data to the requestor.|$|E
2500|$|Self-healing NTFS: In {{previous}} Windows versions, NTFS {{marked the}} volume [...] "dirty" [...] upon detecting file-system corruption and CHKDSK {{was required to}} be run by taking the volume [...] "offline". With self-healing NTFS, an NTFS worker thread is spawned in the background which performs a localized fix-up of <b>damaged</b> <b>data</b> structures, with only the corrupted files/folders remaining unavailable without locking out the entire volume. The self-healing behavior can be turned on for a volume with the fsutil repair set C: 1 command where C presents the volume letter.|$|E
40|$|To {{verify the}} results of seismic microzoning and to improve techniques, the <b>damage</b> <b>data</b> of past {{destructive}} earthquakes is an important key reference. The <b>damage</b> <b>data</b> of the 1923 Kanto, Japan, earthquake in the epicentral region are collected and compiled to produce the most reliable and detailed damage map. The damage map is compared with {{the results from the}} existing damage assessment and is used to discuss revision of the site amplification evaluation...|$|R
40|$|Nowadays, {{flood control}} has been {{replaced}} by flood management concept in terms of living with flood, making benefit of it, and minimizing its losses. The success in flood management in any region depends on the evaluation of different types of flood losses. For the assessment of flood damages, this requires the use of stage–damage functions for different categories of land use. A review is presented of the methods used to construct stage–damage function curves for residential, commercial, agricultural, and industrial category. Two main approaches in constructing stage–damage functions are empirical approach, which is based on <b>damage</b> <b>data</b> of past floods, and synthetic approach, which uses <b>damage</b> <b>data</b> collected by interview survey or questionnaire. For a developing country like Malaysia which has limited history and actual flood <b>damage</b> <b>data,</b> the synthetic method is the preferred approach in constructing stage–damage function curve...|$|R
50|$|The show {{involves}} four characters, Doc <b>Damage,</b> <b>Data</b> Girl and the Petri Twins, who {{experiment on}} themselves {{to uncover the}} secrets of the human body.|$|R
5000|$|Erased, {{fragmented}} or <b>damaged</b> <b>data</b> is {{the least}} accessible form of data. Fragmented data occurs if data is stored in clusters.|$|E
50|$|As for {{the file}} format, special {{emphasis}} {{has been put}} on enabling integrity checks by means of an integrated 32-bit checksum for each compressed stream; this is used in combination with the lziprecover program to detect and reconstruct <b>damaged</b> <b>data.</b>|$|E
5000|$|In {{addition}} to handling whole-disk failures, RAID-Z can also detect and correct silent data corruption, offering [...] "self-healing data": when reading a RAID-Z block, ZFS compares it against its checksum, {{and if the}} data disks did not return the right answer, ZFS reads the parity and then figures out which disk returned bad data. Then, it repairs the <b>damaged</b> <b>data</b> and returns good data to the requestor.|$|E
5000|$|Long life — without {{physical}} <b>damage,</b> <b>data</b> is retained for {{an estimated}} 30 years. For this reason, it is used for archival storage of data.|$|R
40|$|The {{objective}} {{of this research was}} to assess the damage of various structures that were affected during Hurricane Andrew using aerial photographs. Different damage mechanisms were demonstrated. Quantitative <b>damage</b> assessment <b>data</b> was obtained by interpretation of aerial photographs. The <b>damage</b> <b>data</b> have been statistically analyzed. Various types of structures were studied and their typical damages were examined using the statistical analysis with respect to wind speed and zip codes. Illustrations of damages in different communities, damages to different roofs, and their possible failure mechanisms were also discussed. The <b>damage</b> <b>data</b> generated in this study can be used to predict damage during a hurricane after they are statistically correlated with the wind speed...|$|R
40|$|The {{number of}} {{recorded}} ground {{motion in the}} 1995 Hyogoken-Nanbu (Kobe) Earthquake was not large enough to estimate the detailed spatial distribution of ground motion. Yamaguchi and Yamazaki (1999) estimated the PGV distribution using the BRI’s building <b>damage</b> <b>data</b> and the recorded strong motion indices. However the estimated PGV might {{be affected by the}} inventory characteristics of buildings in each district. Hence in this paper, the PGV distribution was re-evaluated using the building <b>damage</b> <b>data</b> surveyed by Kobe City for the purpose of property tax reduction. Using the re-evaluated PGV distribution and the building <b>damage</b> <b>data,</b> fragility curves in terms of the structural type and construction period were constructed. It was also demonstrated that the number of damaged buildings in Nada Ward estimated by the fragility curves fits the actual damage by the earthquake. The fragility curves thus obtained may be useful for damage assessments of buildings in Japan...|$|R
50|$|If other {{copies of}} the <b>damaged</b> <b>data</b> exist or can be reconstructed from {{checksums}} and parity data, ZFS will use {{a copy of the}} data (or recreate it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update all faulty copies with known-good data and redundancy will be restored.|$|E
50|$|In {{computer}} main memory, {{auxiliary storage}} and computer buses, data redundancy is {{the existence of}} data that is additional to the actual data and permits correction of errors in stored or transmitted data. The additional data can simply be a complete copy of the actual data, or only select pieces of data that allow detection of errors and reconstruction of lost or <b>damaged</b> <b>data</b> {{up to a certain}} level.|$|E
50|$|UDP-Lite (Lightweight User Datagram Protocol, {{sometimes}} UDP Lite) is a connectionless protocol {{that allows}} a potentially <b>damaged</b> <b>data</b> payload to be delivered to an application rather than being discarded by the receiving station. This is useful as it allows decisions about {{the integrity of the}} data {{to be made in the}} application layer (application or the codec), where the significance of the bits is understood. UDP-Lite is described in RFC 3828.|$|E
40|$|The 2015 Gorkha Nepal {{earthquake}} caused tremendous {{damage and}} loss. To gain valuable lessons from this tragic event, an earthquake damage investigation team was dispatched to Nepal from 1 May 2015 to 7 May 2015. A unique {{aspect of the}} earthquake damage investigation is that first-hand earthquake <b>damage</b> <b>data</b> were obtained 6 – 11 days after the mainshock. To gain {{deeper understanding of the}} observed earthquake damage in Nepal, the paper reviews the seismotectonic setting and regional seismicity in Nepal and analyzes available aftershock data and ground motion <b>data.</b> The earthquake <b>damage</b> observations indicate {{that the majority of the}} damaged buildings were stone/brick masonry structures with no seismic detailing, whereas the most of RC buildings were undamaged. This indicates that adequate structural design is the key to reduce the earthquake risk in Nepal. To share the gathered <b>damage</b> <b>data</b> widely, the collected <b>damage</b> <b>data</b> (geo-tagged photos and observation comments) are organized using Google Earth and the kmz file is made publicly available...|$|R
5000|$|Once the malware has {{infected}} a phone it {{will also}} seek to accomplish its goal, which is usually one of the following: monetary <b>damage,</b> <b>damage</b> <b>data</b> and/or device, and concealed damage: ...|$|R
40|$|A <b>damage</b> <b>data</b> {{database}} of 131 reinforced concrete (RC) buildings, collected after 2009 L’Aquila (Italy) earthquake, is employed {{for the evaluation}} of observational fragility curves. The specific interpretation of <b>damage</b> <b>data</b> allowed carrying out fragility curves for slight, moderate, and heavy damage, (i. e., DS 1, DS 2, and DS 3), defined according to EMS 98 macroseismic scale. Observational fragility curves are then employed for the calibration of FAST analytical methodology. FAST method is a spectral based approach, meant for the estimate of fragility curves of infilled RC buildings up to DS 3, evaluated, again, according to EMS 98. Kullback–Leibler divergence is employed to check the matching between analytical and observational fragilities. FAST input variables can vary in quite large ranges and the calibration provides a valuable suggestion for the application of the method in other cases in which field <b>damage</b> <b>data</b> are not available. Results showed that optimizing values, for the input variables calibrated, are in good agreement with typical values assumed in literature. Analytical results showed a very satisfactory agreement with observational data for DS 2 and DS 3, while systematical underestimation was found for the case of DS 1...|$|R
5000|$|The {{instruction}} cache keeps copies of 64-byte lines of memory, and fetches 16 bytes each cycle. Each byte in this cache {{is stored in}} ten bits rather than eight, with the extra bits marking the boundaries of instructions (this {{is an example of}} predecoding). The cache has only parity protection rather than ECC, because parity is smaller and any <b>damaged</b> <b>data</b> can be replaced by fresh data fetched from memory (which always has an up-to-date copy of instructions).|$|E
50|$|When a RAID array {{experiences}} {{the failure of}} one or more disks, it can enter degraded mode, a fallback mode that generally allows the continued usage of the array, but either loses the performance boosts of the RAID technique (such as a RAID-1 mirror across two disks when one of them fails; performance will fall back to that of a normal, single drive) or experiences severe performance penalties due to the necessity to reconstruct the <b>damaged</b> <b>data</b> from error correction data.|$|E
5000|$|Disk cloning is {{the process}} of copying the {{contents}} of one computer hard disk to another disk or to an [...] "image" [...] file. This may be done straight from one disk to another, but more often, the contents of the first disk are written to an image file as an intermediate step, then the second disk is loaded with the contents of the image. Typically, this is done for archiving purposes, to restore lost or <b>damaged</b> <b>data,</b> or to move wanted data into a new disk, though other reasons also exist.|$|E
40|$|The 1995 Kobe Earthquake caused {{unprecedented}} {{damage in}} the Hanshin-Awaji area. Several coordinated damage surveys were conducted by groups of researches and engineers after the earthquake. Using these ample <b>data</b> of building <b>damage</b> due to the earthquake, the authors tried to develop fragility curves for damage assessment. First, an analysis of building damage due to the Kobe Earthquake was conducted using the building <b>damage</b> <b>data</b> surveyed by Nishinomiya City Government {{for the purpose of}} property tax reduction. Fragility curves for buildings in Japan which considered the structural type and construction period were created using the building <b>damage</b> <b>data</b> provided by Nishinomiya City and estimated ground motion indices. Note that the ground motion indices were estimated based on another building <b>damage</b> survey <b>data</b> in the Hanshin area compiled by the Building Research Institute and the recorded motions. Since of {{the quality and quantity of}} the data used, improved accuracy is expected for the obtained fragility curves compared with existing ones...|$|R
40|$|Several {{scholars}} {{across the}} globe identified the present lack of high quality <b>damage</b> <b>data</b> as the main constraint to efficient risk mitigation. The need for a systematic collection of <b>damage</b> <b>data</b> {{in the aftermath of}} flood events come into light, thus the aim being the creation of complete and reliable databases. Flood <b>damage</b> <b>data</b> collected {{in the aftermath of a}} disastrous event can support a variety of actions, which include: (i) the identification of priorities for intervention during emergencies, (ii) the creation of complete event scenarios on the basis of which understanding the fragilities of the flooded areas and tailoring risk mitigation strategies, (iii) the definition of victims compensation schemes, and (iv) the validation/definition of damage models to feed cost-benefit analysis of structural and non-structural mitigation actions (including insurance schemes). Volume highlights include: A good compilation of real world case studies elaborating on the survey experiences and best practices associated with flood <b>damage</b> <b>data</b> collection, storage and analysis, that can help strategize flood risk mitigation in an efficient manner Valuable contributions covering different flooding phenomena such as riverine and mountain floods, different spatial level of analysis from local to global scales, and different stakeholders perspectives, e. g. public decision makers, researchers, private companies Contributions from leading experts in the field, researchers and practitioners, including civil protection actors working at different spatial and administrative level, insurers and professionals working in the field of natural hazards mitigation Flood Damage Survey and Assessment: New Insights from Research and Practice will be a valuable resource to all earth scientists, hydrologists, meteorologists, geologists, geographers, civil engineers, insurers and policy decision makers...|$|R
40|$|International audienceForests cover about 56 % of {{the land}} area in Sweden and forest damage due to strong winds has been a {{recurring}} problem. In this paper we analyse recorded storm damage in Swedish forests for the years 1965 ? 2007. During the period 48 individual storm events with a total damage of 164 Mm³ have been reported with the severe storm on 8 to 9 January 2005, as the worst with 70 Mm³ damaged forest. For the analysis, storm <b>damage</b> <b>data</b> has been normalised {{to account for the}} increase in total forest volume over the period. We show that, within the framework of statistical extreme value theory, a Poisson point process model can be used to describe these storm <b>damage</b> events. <b>Damage</b> <b>data</b> supports a heavy-tailed distribution with great variability in damage for the worst storm events. According to the model, and in view of available data, the return period for a storm with damage in size of the severe storm of January 2005 is approximately 80 years, i. e. a storm with damage of this magnitude will happen, on average, once every eighty years. To investigate a possible temporal trend, models with time-dependent parameters have been analysed but give no conclusive evidence of an increasing trend in the normalised storm <b>damage</b> <b>data</b> for the period. Using a non-parametric approach with a kernel based local-likelihood method gives the same result...|$|R
5000|$|Self-healing NTFS: In {{previous}} Windows versions, NTFS {{marked the}} volume [...] "dirty" [...] upon detecting file-system corruption and CHKDSK {{was required to}} be run by taking the volume [...] "offline". With self-healing NTFS, an NTFS worker thread is spawned in the background which performs a localized fix-up of <b>damaged</b> <b>data</b> structures, with only the corrupted files/folders remaining unavailable without locking out the entire volume. The self-healing behavior can be turned on for a volume with the fsutil repair set C: 1 command where C presents the volume letter.|$|E
5000|$|In Windows {{versions}} {{prior to}} Windows Vista, if {{the operating system}} detected corruption in the file system of an NTFS volume, it marked the volume [...] "dirty"; to correct errors on the volume, {{it had to be}} taken offline. With self-healing NTFS, an NTFS worker thread is spawned in the background which performs a localized fix-up of <b>damaged</b> <b>data</b> structures, with only the corrupted files/folders remaining unavailable without locking out the entire volume and needing the server to be taken down. The operating system now features S.M.A.R.T. detection techniques to help determine when a hard disk may fail.|$|E
50|$|A locally {{testable}} code {{is a type}} of error-correcting {{code for}} which it can be determined if a string is a word in that code by looking at a small (frequently constant) number of bits of the string. In some situations, it is useful to know if the data is corrupted without decoding all of it so that appropriate action can be taken in response. For example, in communication, if the receiver encounters a corrupted code, it can request the data be re-sent, which could increase the accuracy of said data. Similarly, in data storage, these codes can allow for <b>damaged</b> <b>data</b> to be recovered and rewritten properly.|$|E
5000|$|Retail - {{inventory}} reports, <b>damage</b> reports, <b>data</b> sheets, manuals, labels ...|$|R
50|$|In contrast, a {{very low}} {{humidity}} level favors the build-up of static electricity, which may result in spontaneous shutdown of computers when discharges occur. Apart from spurious erratic function, electrostatic discharges can cause dielectric breakdown in solid state devices, resulting in irreversible <b>damage.</b> <b>Data</b> centers often monitor relative humidity levels for these reasons.|$|R
40|$|An {{access control}} system regulates the {{operations}} that can be executed on data and resources to be protected • Its goal is to control operations executed by subjects {{in order to prevent}} actions that could <b>damage</b> <b>data</b> and resources • Access control is typically provided as part of the operating system and of the database management system (DBMS...|$|R
5000|$|The defendant, arguing undue {{burden and}} expense, {{requested}} {{the court to}} shift the cost of production to the plaintiff, citing the Rowe decision. The court stated that whether the production of documents is unduly burdensome or expensive [...] "turns primarily on whether it is kept in an accessible or inaccessible format". [...] The court concluded {{that the issue of}} accessibility depends on the media on which data are stored. It described five categories of electronic repositories: (1) online data, including hard disks; (2) near-line data, including optical disks; (3) offline storage, such as magnetic tapes; (4) backup tapes; (5) fragmented, erased and <b>damaged</b> <b>data.</b> The last two were considered inaccessible, that is, not readily available and thus subject to cost-shifting. The court, then discussing the Rowe decision (the balance test), concluded that it needed modification and created a new seven-factor test: ...|$|E
5000|$|During the Opal card {{customer}} trial period, all Opal {{cards were}} required to be registered with the customer's personal information. This allowed for feedback and issues to be recorded against an individual's account. Registered cards offer the ability to protect the balance and transfer it to a new card, if a card is lost, stolen or <b>damaged.</b> <b>Data</b> is made available to other NSW government departments and law enforcement agencies. Concerns about privacy have been repeatedly raised in the mainstream media, with commentators questioning {{the extent to which}} user data can be accessed by authorities. Since July 2014, unregistered adult and child/youth Opal cards have been available. In December 2014, University of Sydney delayed collaboration with the new Opal card system, citing privacy concerns, whereas Macquarie University, University of New South Wales and Australian Catholic University had already agreed to provide the [...] "student data" [...] to the card network.|$|E
5000|$|Data {{recovery}} imaging {{must have}} the ability to pre-configure drives by disabling certain attributes (such as SMART and G-List re-mapping) and the ability to work with unstable drives (drive instability/read instability can be caused by minute mechanical wear and other issues). Data recovery imaging {{must have the}} ability to read data from [...] "bad sectors." [...] Read instability is a major factor when working with drives in operating systems such as Windows. A typical operating system is limited in its ability to deal with drives that {{take a long time to}} read. For these reasons, software that relies on the BIOS and operating system to communicate with the hard drive is often unsuccessful in data recovery imaging; separate hardware control of the source hard drive is required to achieve the full spectrum of data recovery imaging. This is because the operating system (through the BIOS) has a certain set of protocols or rules for communication with the drive that cannot be violated (such as when the hard drive detects a bad sector). A hard drive's protocols may not allow [...] "bad" [...] data to be propagated through to the operating system; firmware on the drive may compensate by rereading sectors until checksums, CRCs, or ECCs pass, or use ECC data to recreate <b>damaged</b> <b>data.</b>|$|E
40|$|A {{solar cell}} {{degradation}} model {{developed for the}} SECKSPOT trajectory optimization code is presented. The model is based on two analytic expressions, one describing solar cell power degradation {{as a function of}} 1 MeV equivalent fluence and cell base resistivity and thickness, and one describing a spatial field of 1 MeV equivalent electron flux. The model extends the latitude range, provides a continuous and smooth representation of the flux field, and provides for changing the cell characteristics. Construction of a 1 MeV electron flux model and of a power loss model are described. It is shown that modeling the 1 MeV flux field as a separate entity allows simple consideration of both front and back shielding, and that the coefficients relating to specific cell <b>damage</b> <b>data</b> can be simply updated using the latest cell <b>damage</b> <b>data</b> once the general analytical characteristics of the model have been established...|$|R
40|$|This {{technical}} note and its attachments are issued {{in response to}} the request made by the States at the last Watershed Planning Meetings held October 25 - 26 and December 19 - 20, 1966, for data related to urban flood damages. The following items are attached for use by the Economist: 3 1. A folder of generalized urban <b>damage</b> <b>data</b> for use in preparing preliminary investigation reports and watershed work plans, This supplements the urban <b>damage</b> <b>data</b> issued at the Northeast Watershed. Planning Party Economists Workshop held at Ocean City, Maryland, in July 17 - 21, 1961, and the Stanford Research Institute data recently issued with TSC Advisory 175 - 7 (UD) dated August 29, 1567. The use of generalized data requires a determination that the data are applicable, and therefore, does not eliminate the need for personal interviews of flood-plain occupiers. Environmental Economics and Policy, Land Economics/Use,...|$|R
40|$|The paper {{presents}} empirical seismic fragility {{models for}} precast RC buildings, fitted using observational <b>damage</b> <b>data</b> gathered after the 2012 Emilia earthquake. <b>Damage</b> <b>data</b> in 1890 buildings was collected, analyzed and classified {{according to a}} six level scale derived from the European Macroseismic Scale. The completeness of the damage database and the spatial distribution of the buildings were analyzed using cadastral data as a reference. The intensity of the ground-motion was quantified by the maximum horizontal peak ground acceleration (PGA), which was obtained from shakemaps. Bayesian regression was then used in order to fit two different classes of models: i) models considering the different damage levels independently and ii) an ordinal logistic model which leads to non-overlapping fragility curves. The fragility curves obtained in the present work, when compared to literature fragilities for cast in place RC frame buildings, indicate that precast industrial buildings are significantly more vulnerable...|$|R
