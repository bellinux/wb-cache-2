573|218|Public
5|$|For instance, {{when one}} cell of a {{spreadsheet}} changes, {{it is necessary}} to recalculate the values of other cells that depend directly or indirectly on the changed cell. For this problem, the tasks to be scheduled are the recalculations of the values of individual cells of the spreadsheet. Dependencies arise when an expression in one cell uses a value from another cell. In such a case, the value that is used must be recalculated earlier than the expression that uses it. Topologically ordering the dependency graph, and using this topological order to schedule the cell updates, allows the whole spreadsheet to be updated with only a single evaluation per cell. Similar problems of task ordering arise in makefiles for program compilation and <b>instruction</b> <b>scheduling</b> for low-level computer program optimization.|$|E
25|$|Some CPU designs {{can perform}} some {{optimizations}} at runtime. Some examples include Out-of-order execution, Speculative execution, Instruction pipelines, and Branch predictors. Compilers {{can help the}} program {{take advantage of these}} CPU features, for example through <b>instruction</b> <b>scheduling.</b>|$|E
25|$|Code {{optimization}} can be also broadly {{categorized as}} platform-dependent and platform-independent techniques. While the latter ones are effective on {{most or all}} platforms, platform-dependent techniques use specific properties of one platform, or rely on parameters depending on the single platform or even on the single processor. Writing or producing {{different versions of the}} same code for different processors might therefore be needed. For instance, in the case of compile-level optimization, platform-independent techniques are generic techniques (such as loop unrolling, reduction in function calls, memory efficient routines, reduction in conditions, etc.), that impact most CPU architectures in a similar way. Generally, these serve to reduce the total instruction path length required to complete the program and/or reduce total memory usage during the process. On the other hand, platform-dependent techniques involve <b>instruction</b> <b>scheduling,</b> instruction-level parallelism, data-level parallelism, cache optimization techniques (i.e., parameters that differ among various platforms) and the optimal <b>instruction</b> <b>scheduling</b> might be different even on different processors of the same architecture.|$|E
50|$|Data hazards {{occur when}} an <b>instruction,</b> <b>scheduled</b> blindly, {{would attempt to}} use data before the data is {{available}} in the register file.|$|R
5000|$|Instruction scheduling: {{in which}} order {{to put those}} <b>instructions.</b> <b>Scheduling</b> is a speed {{optimization}} that can have a critical effect on pipelined machines.|$|R
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceIn this work, we {{experiment}} with complier-directed instruction duplication to detect soft errors in VLIW datapaths. In the proposed approach, the compiler determines the <b>instruction</b> <b>schedule</b> by balancing the permissible performance degradation with the required degree of duplication. Our experimental results show that our algorithms allow the designer to perform tradeoff analysis between performance and reliability...|$|R
25|$|The exact set of GCC {{optimizations}} {{varies from}} release to release as it develops, but includes the standard algorithms, such as loop optimization, jump threading, common subexpression elimination, <b>instruction</b> <b>scheduling,</b> and so forth. The RTL optimizations are of less importance {{with the addition}} of global SSA-based optimizations on GIMPLE trees, as RTL optimizations have a much more limited scope, and have less high-level information.|$|E
25|$|Some {{assemblers}} {{may also}} be able to perform some simple types of instruction set-specific optimizations. One concrete example of this may be the ubiquitous x86 assemblers from various vendors. Most of them are able to perform jump-instruction replacements (long jumps replaced by short or relative jumps) in any number of passes, on request. Others may even do simple rearrangement or insertion of instructions, such as some assemblers for RISC architectures that can help optimize a sensible <b>instruction</b> <b>scheduling</b> to exploit the CPU pipeline as efficiently as possible.|$|E
5000|$|... #Subtitle level 3: Move <b>instruction</b> <b>scheduling</b> to the {{compiler}} ...|$|E
50|$|The {{simplest}} algorithm {{to find a}} topological sort {{is frequently}} used and is known as list scheduling. Conceptually, it repeatedly selects a source of the dependency graph, appends it to the current <b>instruction</b> <b>schedule</b> and removes it from the graph. This may cause other vertices to be sources, which will then also be considered for scheduling. The algorithm terminates if the graph is empty.|$|R
40|$|As part of {{a larger}} project, we have built a {{declarative}} assembly language. This language enables us to specify multiple code paths to compute particular quantities, giving the instruction scheduler more flexibility in balancing execution resources for superscalar execution. The instruction scheduler is also innovative in that it includes aggressive pipelining, and exhaustive (but lazy) search for optimal <b>instruction</b> <b>schedules.</b> We present some examples where our approach has produced very promising results. McMaster Universit...|$|R
50|$|VLIW CPUs {{are usually}} made of {{multiple}} RISC-like execution units that operate independently. Contemporary VLIWs usually have {{four to eight}} main execution units. Compilers generate initial instruction sequences for the VLIW CPU in roughly {{the same manner as}} for traditional CPUs, generating a sequence of RISC-like instructions. The compiler analyzes this code for dependence relationships and resource requirements. It then <b>schedules</b> the <b>instructions</b> according to those constraints. In this process, independent <b>instructions</b> can be <b>scheduled</b> in parallel. Because VLIWs typically represent <b>instructions</b> <b>scheduled</b> in parallel with a longer instruction word that incorporates the individual instructions, this results in a much longer opcode (termed very long) to specify what executes on a given cycle.|$|R
50|$|The compilers {{used with}} the systems were {{expected}} to dedicate more time during compilation to making effective use of the registers and cleaning the instruction stream. By doing <b>instruction</b> <b>scheduling</b> in the compiler, this design avoided the problems and complexity of dynamic <b>instruction</b> <b>scheduling</b> (where instructions for multiple functional units must be selected carefully {{in order to avoid}} interdependencies between intermediate values) encountered in superscalar designs such as Digital Equipment Corporation's Alpha.|$|E
5000|$|<b>Instruction</b> <b>Scheduling.</b> Dependency graphs are {{computed}} for the operands of assembly or intermediate instructions {{and used to}} determine an optimal order for the instructions.|$|E
5000|$|Predicated {{instructions}} {{with different}} predicates can be mixed {{with each other}} and with unconditional code, allowing better <b>instruction</b> <b>scheduling</b> and so even better performance.|$|E
40|$|The {{length of}} a statically created <b>instruction</b> <b>schedule</b> determines {{to a great extent}} the {{performance}} of program executions on VLIW architectures. In this paper we present a simple, yet effective, method to reduce the {{length of a}} static <b>instruction</b> <b>schedule</b> by introducing new hardware operations, referred to as super operations. A super operation replaces a number of operations, while maintaining functionality, hence decreasing the total number of operations to be executed and thereby eliminating the dependencies between them. In order to replace a number of operations, super operations must often process more operands and produce more results than traditional operations. The Philips TM- 1000 is a VLIW based architecture. Its CPU is a 5 -issue machine with 27 functional units, each connected to one issue-slot. To support super operations, we extend the hardware with special functional units which are connected to more than one issue-slot. In this paper we discuss the modifications that were made to the compiler in order to support super operations and we demonstrate the ease with which super operations can be applied by the application programmer. To a lesser extent, we address consequences of super operations concerning the hardware. Furthermore, we demonstrate the benefit of super operations by showing the performance improvement for some multimedia applications...|$|R
3000|$|... (Get patient from waiting area), greet new patient, provide {{orientation}} to patient, educate patient on Warfarin therapy, explain to patient about Warfarin interaction with food, alcohol, and other drugs, explain to patient about over-the-counter medications and vitamin k interactions with Warfarin, print and give patient list of medications that patient should/should not take, explain to patient about lab draw and follow-ups, hand over educational package, update patient’s information on CPRS, provide patient with written dosing <b>instructions,</b> <b>schedule</b> next appt. on Vista, enter note on CPRS, close chart, to: educate and help patient start Warfarin therapy.|$|R
40|$|The main topic {{discussed}} {{is a daily}} modular scheduling system initiated for {{the small}} enrollment at Pahranagat Valley High School in Alamo, Nevada, with specific reference to types of <b>instruction,</b> <b>schedule</b> procedures, and conflict problems. An evaluation of the scheduling system is also included. The report is written in dissertation format, which presents a statement {{of the problem and}} a definition of terms, a review of literature relevant to modular scheduling, and a discussion of the development of a hand-generated modular schedule. (EV) - 1. 4 re cr...|$|R
50|$|One goal of EPIC was to {{move the}} {{complexity}} of <b>instruction</b> <b>scheduling</b> from the CPU hardware to the software compiler, which can do the <b>instruction</b> <b>scheduling</b> statically (with help of trace feedback information). This {{eliminates the need for}} complex scheduling circuitry in the CPU, which frees up space and power for other functions, including additional execution resources. An equally important goal was to further exploit instruction level parallelism (ILP) by using the compiler to find and exploit additional opportunities for parallel execution.|$|E
50|$|Unified Reservation station, {{also known}} as unified scheduler, is a {{decentralized}} feature of the microarchitecture of a CPU that allows for register renaming, and {{is used by the}} Tomasulo algorithm for dynamic <b>instruction</b> <b>scheduling.</b>|$|E
50|$|Instruction scheduling: <b>Instruction</b> <b>scheduling</b> is an {{important}} optimization for modern pipelined processors, which avoids stalls or bubbles in the pipeline by clustering instructions with no dependencies together, while being careful to preserve the original semantics.|$|E
40|$|One {{of the new}} {{extensions}} in EPIC architectures are cache hints. On each memory instruction, {{two kinds}} of hints can be attached: a source cache hint and a target cache hint. The source hint indicates the true latency of the instruction, which {{is used by the}} compiler to improve the <b>instruction</b> <b>schedule.</b> The target hint indicates at which cache levels it is profitable to retain data, allowing to improve cache replacement decisions at run time. A compile-time method is presented which calculates appropriate cache hints. Both kind of hints are based on the locality of the instruction, measured by the reuse distance metric. Tw...|$|R
40|$|Software {{pipelining}} {{can generate}} efficient schedules for loop by overlapping {{the execution of}} operations from different iterations in order to exploit maximum Instruction Level Parallelism (ILP). Code optimization can decrease total number of calculations and memory related operations. As a result, <b>instruction</b> <b>schedules</b> can use freed resources to construct shorter schedules. Particularly, when the data is not presented in cache, the performance will be significantly degraded by memory references. Therefore, elimination of redundant load-store operations is most important for improving overall performance. This paper introduces a method for integrating software pipelining and load-store elimination techniques. Moreover, we demonstrate that integrated algorithm {{is more effective than}} other methods...|$|R
50|$|To {{make sure}} we respect {{the three types of}} dependencies, we {{construct}} a dependency graph, which is a directed graph where each vertex is an instruction and there is an edge from I1 to I2 if I1 must come before I2 due to a dependency. If loop-carried dependencies are left out, the dependency graph is a directed acyclic graph. Then, any topological sort of this graph is a valid <b>instruction</b> <b>schedule.</b> The edges of the graph are usually labelled with the latency of the dependence. This is the number of clock cycles that needs to elapse before the pipeline can proceed with the target instruction without stalling.|$|R
5000|$|In {{computer}} science, <b>instruction</b> <b>scheduling</b> is a compiler optimization used {{to improve}} instruction-level parallelism, which improves performance on machines with instruction pipelines. Put more simply, without changing {{the meaning of the}} code, it tries to ...|$|E
50|$|Some CPU designs {{can perform}} some {{optimizations}} at runtime. Some examples include Out-of-order execution, Speculative execution, Instruction pipelines, and Branch predictors. Compilers {{can help the}} program {{take advantage of these}} CPU features, for example through <b>instruction</b> <b>scheduling.</b>|$|E
50|$|The <b>instruction</b> <b>scheduling</b> {{logic that}} makes a superscalar {{processor}} is boolean logic. In the early 1990s, a significant innovation was {{to realize that the}} coordination of a multi-ALU computer could be moved into the compiler, the software that translates a programmer's instructions into machine-level instructions.|$|E
40|$|International audienceTo {{keep up with}} a {{large degree}} of {{instruction}} level parallelism (ILP), the Itanium 2 cache systems use a complex organization scheme: load/store queues, banking and interleaving. In this paper, we study {{the impact of these}} cache systems on memory <b>instructions</b> <b>scheduling.</b> We demonstrate that, if no care is taken at compile time, the non-precise memory disambiguation mechanism and the banking structure cause severe performance loss, even for very simple regular codes. We also show that grouping the memory operations in a pseudo-vectorized way enables the compiler to generate more effective code for the Itanium 2 processor. The impact of this code optimization technique on register pressure is analyzed for various vectorization schemes...|$|R
40|$|The growing {{complexity}} of modern processors {{has made the}} development of highly efficient code increasingly difficult. Manually developing highly efficient code is usually expensive but necessary due to the limitations of today’s compilers. A promising automatic code generation strategy, implemented by library generators such as ATLAS, FFTW, and SPIRAL, relies on empirical search to identify, for each target machine, the code characteristics, such as the tile size and <b>instruction</b> <b>schedules,</b> that deliver the best performance. This approach has mainly been applied to scientific codes which can be optimized by identifying code characteristics that depend only on the target machine. In this paper, we study the generation of sorting routines whose performance also depends {{on the characteristics of}} the inpu...|$|R
40|$|Special {{issue on}} {{interaction}} between compilers and computer architectures. International audienceSoftware pipelining can generate efficient schedules for loop by overlapping {{the execution of}} operations from different iterations in order to exploit maximum Instruction Level Parallelism (ILP). Code optimization can decrease total number of calculations and memory related operations. As a result, <b>instruction</b> <b>schedules</b> can use freed resources to construct shorter schedules. Particularly, when the data is not presented in cache, the performance will be significantly degraded by memory references. Therefore, elimination of redundant load-store operations is most important for improving overall performance. This paper introduces a method for integrating software pipelining and load-store elimination techniques. Moreover, we demonstrate that integrated algorithm {{is more effective than}} other methods...|$|R
5000|$|<b>Instruction</b> <b>scheduling</b> is {{typically}} {{done on a}} single basic block. In order to determine whether rearranging the block's instructions {{in a certain way}} preserves the behavior of that block, we need the concept of a data dependency. There are three types of dependencies, which also happen to be the three data hazards: ...|$|E
50|$|Most {{register}} files make {{no special}} provision to prevent multiple write ports from writing the same entry simultaneously. Instead, the <b>instruction</b> <b>scheduling</b> hardware ensures {{that only one}} instruction in any particular cycle writes a particular entry. If multiple instructions targeting the same register are issued, all but one have their write enables turned off.|$|E
50|$|In {{computer}} science, {{applications of}} this type arise in <b>instruction</b> <b>scheduling,</b> ordering of formula cell evaluation when recomputing formula values in spreadsheets, logic synthesis, determining the order of compilation tasks to perform in makefiles, data serialization, and resolving symbol dependencies in linkers. It {{is also used to}} decide in which order to load tables with foreign keys in databases.|$|E
500|$|Johnny's part is {{not heard}} on the next track, [...] "I'm Affected", as {{reported}} by Johnny himself. Joey admitted {{that he did not}} favor the song, recalling: [...] "I couldn't believe how awful it sounded. It was horrible." [...] "Danny Says", the third track, was a lyrical depiction of what the band constantly went through while touring--soundchecks, autograph sessions, interviews, etc. The title [...] "Danny Says" [...] refers to the band's tour manager Danny Fields giving the members <b>instructions,</b> <b>schedules,</b> and demands. According to Joey, the ballad was inspired by Lou Reed, who had released the songs [...] "Candy Says" [...] and [...] "Caroline Says". Joey's brother Mickey Leigh called the song a [...] "masterpiece" [...] and said it [...] "remains one of the most captivatingly beautiful songs I've ever heard." ...|$|R
50|$|Most of the <b>instruction</b> <b>schedule</b> or {{nature is}} {{unknown to the}} recruits. That means {{that they must be}} {{constantly}} ready and, to the smallest indication, present themselves on the parade ground or where they are ordered to, and follow whatever the instructors say. It might happen that they stay uninterrupted in instruction for more than a day, or that they have to conduct their daily lives during the night. The unforeseen and surprise are fundamental characteristics of the instruction. Each recruit must also be self-controlled: they have to control reactions that, otherwise, might be normal if they were not future Commandos.All the demands made in the instruction are not obligations: each recruit has the right to refuse to do whatever he is ordered to. Obviously, doing this means that he is off the course.|$|R
5000|$|APD officer Mikal Monette had crisis {{intervention}} training and had successfully resolved hundreds of situations. He testified at trial that an APD sergeant removed {{him from the}} bargaining process, replacing him with Detective Sandy and others shortly before Boyd was mortally wounded. This was even though Monette had made some progress and had succeeded in getting Boyd, at one point, to agree to leave with him. The judge dismissed the charges of involuntary manslaughter on September 28, leaving only the more serious 2nd degree murder charges remaining. Under state law, involuntary manslaughter implies either provocation by the victim or behavior emanating from the [...] "heat of passion." [...] The prosecution and defense testimony concluded on October 5, 2016, with closing arguments and jury <b>instructions</b> <b>scheduled</b> for the next day.|$|R
