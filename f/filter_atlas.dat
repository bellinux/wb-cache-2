0|13|Public
40|$|International audienceThe {{geometric}} median is {{a classic}} robust estimator of centrality for data in Euclidean spaces, {{and it has been}} generalized in analytical manifold in [1]. Recently, an intrinsic Riemannian framework for Orientation Distribution Function (ODF) was proposed for the calculation in ODF field [2]. In this work, we prove the unique existence of the Riemannian median in ODF space. Then we explore its two potential applications, median <b>filtering</b> and <b>atlas</b> estimation...|$|R
40|$|This {{document}} {{describes an}} {{implementation of a}} PC-based Event <b>Filter</b> for <b>ATLAS.</b> Included are discussions of both {{the treatment of the}} dataflow and the monitoring/supervision aspects of the software. The emphasis in this implementation is on simplicity. In the case of the dataflow an effort has been made to simplify the interface between the Event Filter control processes and the Processing Tasks. The implementation of the monitoring/supervision software has been simplified by exploiting the services of the Online Software system as far as possible. Direct comparisons are made with an alternative implementation of the Event Filter...|$|R
40|$|In 2006 the Large Hadron Collider (LHC) with a center-of-mass {{energy of}} 14 will be {{operational}} at CERN near Geneva. It will offer new experimental insights at the TeV-energy scale. The long awaited Higgs particle might be found, filling the last {{gap in the}} Standard Model. New theories like the Supersymmetry or Supergravity can be testified. The unprecedented luminosity of 10 ^ 34 and the large QCD background will pose stringent requirements to the detectors at LHC. The online selection of rare interesting events within the tremendous amount of particles crossing the detectors is challenging. In this thesis, a first study of the high-Pt single electron trigger at the event <b>filter</b> of <b>ATLAS</b> - the third and last online selection step - is presented. An overall trigger efficiency of 70...|$|R
40|$|ABSTRACT: The purpose, design specifications, {{construction}} techniques, {{and testing}} methods are described {{for the high}} voltage feedthrough ports and <b>filters</b> of the <b>ATLAS</b> Liquid Argon calorimeters. These feedthroughs carry about 5000 high voltage wires from a room-temperature environment (300 K) through the cryostat walls to the calorimeters cells (89 K) while maintaining the electrical and cryogenic integrity of the system. The feedthrough wiring and filters operate at a maximum high voltage of 2. 5 kV without danger of degradation by corona discharges or radiatio...|$|R
40|$|Abstract. This paper {{presents}} a local feature vector based method for automated Multiple Sclerosis (MS) lesion segmentation of multi spec-tral MRI data. Twenty datasets from MS patients with FLAIR, T 1,T 2, MD and FA data with expert annotations are available as training set from the MICCAI 2008 challenge on MS, and 24 test datasets. Our lo-cal feature vector method contains neighbourhood voxel intensities, his-togram and MS probability atlas information. Principal Component Anal-ysis(PCA) [8] with log-likelihood ratio {{is used to}} classify each voxel. MRI suffers from intensity inhomogenities. We try to correct this ”bias field” with 3 methods: a genetic algorithm, edge preserving <b>filtering</b> and <b>atlas</b> based correction. A large observer variability exist between expert classifi-cations, but the similarity scores between model and expert classifications are often lower. Our model gives the best classification results with raw data, because bias correction gives artifacts at the edges and flatten large MS lesions...|$|R
40|$|The purpose, design specifications, {{construction}} techniques, {{and testing}} methods are described {{for the high}} voltage feedthrough ports and <b>filters</b> of the <b>ATLAS</b> Liquid Argon calorimeters. These feedthroughs carry about 5000 high voltage wires from a room-temperature environment (300 K) through the cryostat walls to the calorimeters cells (89 K) while maintaining the electrical and cryogenic integrity of the system. The feedthrough wiring and filters operate at a maximum high voltage of 2. 5 kV without danger of degradation by corona discharges or radiation at the Large Hadron Collider...|$|R
40|$|In {{this note}} a {{comprehensive}} study of the high-Pt single electron trigger at the event <b>filter</b> of <b>ATLAS</b> is presented. An overall trigger efficiency of 70 %(66 %) for single electrons with Pt= 30 (20) GeV at design (low) luminosity within the stringent time constraints of the online system can be achieved. The single electron trigger rate is about 80 (25) Hz of which 65 %(50 %) are due to irreducible processes like W -> e nu. The high trigger rate for single electrons compared to the total target rate of 100 Hz is of big concern. Therefore it might be necessary to raise the threshold for single object triggers and to pre-scale the lower threshold trigger items. A case study using the physics channels W -> e nu, Z -> e e, and H -> 4 e will give an indication how double object trigger items or the missing transverse energy {{can be used to}} save the trigger efficiency while reducing the trigger rate...|$|R
40|$|While {{many high}} energy {{experiments}} use track fitting software {{that is based}} solely on the Kalman <b>Filter</b> technique, the <b>ATLAS</b> offline reconstruction also has several global track fitters available. One of these is the GlobalChi 2 Fitter, which is based on the scattering angle formulation of the track fit. One of the advantages of this method over the Kalman fit is that it can provide the scattering angles and related quantities (e. g. the residual derivatives) to the alignment algorithms. The algorithm has been implemented in the new common tracking framework in ATLAS, the philosophy of which is to improve the modularity and flexibility of the tracking software. This flexibility has proven crucial for the understanding of the data from the testbeam and cosmic runs. An overview of recent results will be presented, in particular the results from the combined tracking with the inner detector and the muon spectrometer using the cosmics data...|$|R
40|$|The {{expansion}} and differentiation of hematopoietic progenitors is regulated by cytokine and growth factor signaling. To examine how signal transduction controls the gene expression program required for progenitor expansion, we screened <b>ATLAS</b> <b>filters</b> with polysome-associated mRNA derived from erythroid progenitors stimulated with erythropoietin and/or stem cell factor. The putative proto-oncogene {{nucleoside diphosphate kinase}} B (ndpk-B or nm 23 -M 2) was identified as an erythropoietin and stem cell factor target gene. Factor-induced expression of nm 23 -M 2 was regulated specifically {{at the level of}} polysome association by a phosphoinositide 3 -kinase-dependent mechanism. Identification of the transcription initiation site revealed that nm 23 -M 2 mRNA starts with a terminal oligopyrimidine sequence, which is known to render mRNA translation dependent on mitogenic factors. Recently, the nm 23 -M 2 locus was identified as a common leukemia retrovirus integration site, suggesting that it plays a role in leukemia development. The expression of Nm 23 from a retroviral vector in the absence of its 5 '-untranslated region caused constitutive polysome association of nm 23 -M 2. Polysome-association and protein expression of endogenous nm 23 -M 2 declined during differentiation of erythroid progenitors, suggesting a role for Nm 23 -M 2 in progenitor expansion. Taken together, nm 23 -m 2 exemplifies that cytokine-dependent control of translation initiation is an important mechanism of gene expression regulatio...|$|R
40|$|At LHC the 40 MHz bunch {{crossing}} rate dictates a {{high selectivity}} of the ATLAS Trigger system, {{which has to}} keep the full physics potential of the experiment {{in spite of a}} limited storage capability. The level- 1 trigger, implemented in a custom hardware, will reduce the initial rate to 75 kHz and is followed by the software based level- 2 and Event Filter, usually referred as High Level Triggers (HLT), which further reduce the rate to about 100 Hz. In this paper an overview of the implementation of the offline muon recostruction algortihms MOORE (Muon Object Oriented REconstruction) and MuId (Muon Identification) as Event <b>Filter</b> in the <b>ATLAS</b> online framework is given. The MOORE algorithm performs the reconstruction inside the Muon Spectrometer providing a precise measurement of the muon track parameters outside the calorimeters; MuId combines the measurements of all ATLAS sub-detectors in order to identify muons and provides the best estimate of their momentum at the production vertex. In the HLT implementation the muon reconstruction can be executed in "full scan mode", performing pattern recognition in the whole muon spectrometer, or in the "seeded mode", taking advantage {{of the results of the}} earlier trigger levels. An estimate of the execution time will be presented along with the performances in terms of efficiency, momentum resolution and rejection power for muons coming from hadron decays and for fake muon tracks, due to accidental hit correlations in the high background environment of the experiment...|$|R
40|$|The ATLAS Trigger and Data Acquisition (TDAQ) {{infrastructure}} {{is responsible for}} <b>filtering</b> and transferring <b>ATLAS</b> experimental data from detectors to mass storage systems. It relies on a large, distributed computing environment composed by thousands of software applications running concurrently. In such a complex environment, information sharing is fundamental for controlling applications behavior, error reporting and operational monitoring. During data taking runs, the streams of messages sent by applications and data published via information services are constantly monitored by experts to verify correctness of running operations and to understand problematic situations. To simplify and improve system analysis and errors detection tasks, we developed the TDAQ Analytics Dashboard, a web application that aims to collect, correlate and visualize effectively this real time flow of information. The TDAQ Analytics Dashboard is composed by two main entities, that reflect the twofold scope of the application. The first is the engine, a Java service that performs aggregation, processing and filtering of real time data stream and computes statistical correlation on sliding windows of time. The results are made available to clients via a simple web interface supporting SQL-like query syntax. The second is the visualization, provided by an Ajax-based web application that runs on client's browser. The dashboard approach allow to present in formation in a clear and customizable structure. Several types of interactive graphs are proposed as widget, that can be dynamically added and removed from visualization panels. Each widget acts as a client for the engine, querying the web interface to retrieve data with desired criteria. In this paper we present in details the design, development and evolution of the TDAQ Analytics Dashboard. We also present the statistical analysis computed by the application in this first period of high energy data taking operations for the ATLAS experiment...|$|R
40|$|The Trigger and Data Acquisition (TDAQ) {{system of}} the ATLAS {{experiment}} at CERN is the infrastructure responsible for <b>filtering</b> and transferring <b>ATLAS</b> experimental data from detectors to the mass storage system. It relies on a large, distributed computing environment, including thousands of computing nodes with thousands of application running concurrently. In such a complex environment, information analysis is fundamental for controlling applications behavior, error reporting and operational monitoring. During data taking runs, streams of messages sent by applications via the message reporting system together with data published from applications via information services are the main sources of knowledge about correctness of running operations. The huge flow of data produced (with an average rate of O(1 - 10 KHz)) is constantly monitored by experts to detect problem or misbehavior. This require strong competence and experience in understanding and discovering problems and root causes, and often the meaningful information {{is not in the}} single message or update, but in the aggregated behavior in a certain time-line. The AAL project is meant at reducing the man power needs and at assuring a constant high quality of problem detection by automating most of the monitoring tasks and providing real-time correlation of data-taking and system metrics. This project combines technologies coming from different disciplines, in particular it leverages on an Event Driven Architecture to unify the flow of data from the ATLAS infrastructure, on a Complex Event Processing (CEP) engine for correlation of events and on a machine learning module to detect anomaly and problems that cannot be defined in advance. The project is composed of 3 main components: a core processing engine, responsible for correlation of events through expert-defined queries, a machine learning module to detect anomalies in an unsupervised manner and a web based front-end to present real-time information and interact with the system. All components works in a loose-coupled event based architecture, with a message broker to centralize all communication between modules. The result is an intelligent system able to extract and compute relevant information from the flow of operational data to provide real-time feedback to human experts who can promptly react when needed. The paper presents the design and implementation of the AAL project, together with the results of its usage as automated monitoring assistant for the ATLAS data taking infrastructure...|$|R

