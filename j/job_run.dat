19|590|Public
5|$|The {{associated}} compiler listing generated {{over four}} pages of technical detail and <b>job</b> <b>run</b> information, {{for the single}} line of output from the 14 lines of COBOL.|$|E
50|$|D'Ambrosio, Roberts, & Symons (March 15, 2003). Grand jury subpoenas {{secretary}} - Bennett to keep top Senate <b>job,</b> <b>run</b> again. Asbury Park Press.|$|E
50|$|The {{associated}} compiler listing generated {{over four}} pages of technical detail and <b>job</b> <b>run</b> information, {{for the single}} line of output from the 14 lines of COBOL.|$|E
5000|$|...SBSD: Subsystem {{description}} (used {{when starting}} subsystems; {{this is the}} place where user <b>jobs</b> <b>run).</b>|$|R
50|$|Usually a {{batch job}} {{or group of}} related batch <b>jobs</b> (schedule/stream) <b>runs</b> to {{accomplish}} one or more business functions. These batch <b>jobs</b> <b>run</b> unattended and normally complete without any errors or issues. However, sometimes the batch job can have a break/interruption/abend/abort. There could be several reasons why a job could abend.|$|R
50|$|A batch {{processing}} operating system that controls execution of CMS-2 components and user <b>jobs</b> <b>run</b> on the CP-642 computer. It provides input/output, software library facilities and debugging tools. Job accounting is also provided.|$|R
50|$|In game theory, a job {{scheduling}} {{game is a}} game that models a scenario in which multiple selfish users wish to utilize multiple processing machines. Each user has a single job, {{and he needs to}} choose a single machine to process it. The incentive of each user is to have his <b>job</b> <b>run</b> as fast as possible.|$|E
5000|$|A {{more natural}} {{example is the}} one of job scheduling. There are [...] players {{and each of them}} has a job to run. They can choose one of [...] {{machines}} to run the job. The Price of Anarchy compares the situation where the selection of machines is guided/directed centrally to the situation where each player chooses the machine that will make its <b>job</b> <b>run</b> fastest.|$|E
50|$|Veeam Backup & Replication {{operates}} at the virtualization layer. It backs up VMs at the image-level using a hypervisor’s snapshots to retrieve VM data. Backups can be full (a full copy of VM image) or incremental (saving only the changed {{blocks of data}} since the last backup <b>job</b> <b>run).</b> Backup increments are created using the built-in changed block tracking (CBT) mechanism. The available backup methods include forward incremental-forever backup, forward incremental backup, and reverse incremental backup. Additionally, there’s an option to perform active full and synthetic full backups.Veeam Backup & Replication provides automated recovery verification for both backups and replicas. The program starts a VM directly from a backup or replica in the isolated test environment and runs tests against it. During the verification, the VM image remains in a read-only state. This mechanism {{can also be used}} for troubleshooting or testing patches and upgrades.|$|E
50|$|These {{jobs are}} {{organized}} by a queue manager. Due to the {{characteristic of the}} <b>jobs</b> (<b>runs</b> in hundred of CPUs a few days) its impossible to use more conventional access to the resources. The supercomputer must be running jobs without interrupts all the year.|$|R
40|$|This {{document}} {{covers the}} conceptual {{design of a}} system for users to monitor the status of GRID jobs in real time. The JobMon system allows users to view process status and log files interactively as the <b>job</b> <b>runs.</b> This facilitates debugging and reduces unnecessary resource consumption due to user error. ...|$|R
50|$|Cron {{process for}} {{scheduling}} <b>jobs</b> to <b>run</b> {{at a particular}} time.|$|R
40|$|Abstract—Parallel program {{execution}} on {{multiprocessor system}} {{is affected by}} many factors. Often it is rather difficult to estimate how program would behave when running on some definite number of processors. Most tools designed for performance and other parallel program characteristics evaluation affect source code of a program. In this paper we consider neural network approach to predict <b>job</b> <b>run</b> time (when using a queue-based submission system). All analysis that is needed for the prediction can be provided without any knowledge about program source code. I...|$|E
40|$|This paper {{presents}} a comprehensive {{characterization of the}} DAS- 2 1 workloads using twelve-month scientific traces. Metrics that we characterize include system utilization, job arrival rate and interarrival time, job size (degree of parallelism), <b>job</b> <b>run</b> time, memory usage, and job queue wait time. Differences with previous reported workloads are recognized and statistical distributions are fitted for generating synthetic workloads with the same characteristics. This study provides a realistic basis for experiments in resource management and evaluations of different scheduling strategies in a multi-cluster environment. ...|$|E
40|$|The {{purpose of}} this manual is to assist the DOE- 2 user to run DOE- 2 and its utility {{programs}} at Lawrence Berkeley Laboratory (LBL). It is organized to reflect the facts that every DOE- 2 <b>job</b> <b>run</b> at LBL requires certain steps, {{and that there are}} options related to DOE- 2 job runs available to any DOE- 2 user. The standard steps for running a DOE- 2 job are as follows: 1. Prepare a job deck 2. Process a job deck 3. Obtain standard output reports...|$|E
5000|$|Cron {{process for}} {{scheduling}} <b>jobs</b> to <b>run</b> {{on a particular}} date.|$|R
500|$|What {{makes the}} North country {{valuable}} to conservationists is the seclusion, beauty, isolation, quiet, clear water, and absence of development. [...] The preservationists {{have tried to}} limit or prohibit roads, hydroelectric generators, sawmills and lumbering, resorts, power boats, airplanes, and snowmobiles…. [...] The tensions between development and preservation, restraint and grows, beauty and <b>jobs</b> <b>runs</b> deep and strong.|$|R
50|$|A {{remote access}} capable webcron {{solution}} is typically bundled {{with a pair}} of client and server components. The client runs on a separate computer, such as the user's personal computer. A job schedule is set up on the computer where the client component resides. Then, when the <b>job</b> <b>runs,</b> the client component communicates with the server component.|$|R
40|$|The authors present {{methods to}} reduce {{computer}} energy consumption by a {{better use of}} resources and by maximizing the efficiencies of applications. The processor frequency is adjusted {{to the needs of}} the running job, leading to a power drop in servers and PCs, and increasing battery life time of laptops. It is shown how computer resources can be optimally adapted to application needs, reducing <b>job</b> <b>run</b> time. The job-related data is stored and reused to help computer managers to stop old machines and to choose new ones better adapted to the application community...|$|E
40|$|Abstract. This paper {{presents}} a comprehensive characterization of a multi-cluster supercomputer 3 workload using twelve-month scientific research traces. Metrics that we characterize include system utilization, job arrival rate and interarrival time, job cancellation rate, job size (degree of parallelism), <b>job</b> <b>run</b> time, memory usage, and user/group behavior. Correlations between metrics (job runtime and memory usage, requested and actual runtime, etc) are identified and extensively studied. Differences with previously reported workloads are recognized and statistical distributions are fitted for generating synthetic workloads {{with the same}} characteristics. This study provides a realistic basis for experiments in resource management and evaluations of different scheduling strategies in a multi-cluster research environment. ...|$|E
40|$|In the Mathematics and Computer Science Division at Argonne, {{the demand}} for {{resources}} on the Onyx 2 exceeds the resources available for consumption. To distribute these scarce resources effectively, we need a scheduling and resource management package with multiple capabilities. In particular, it must accept standard interactive user logins, allow batch jobs, backfill the system based on available resources, and permit system activities such as accounting to proceed without interruption. The package must include a mechanism to treat the graphic pipes as a schedulable resource. Also required {{is the ability to}} create advance reservations, offer dedicated system modes for large resource runs and benchmarking, and track the resources consumed for each <b>job</b> <b>run.</b> Furthermore, our users {{want to be able to}} obtain repeatable timing results on job runs. And, of course, package costs must be carefully considered. We explored several options, including NQE and various third-party products, before settling on the PBS scheduler...|$|E
50|$|Usually, with paged memory management, each <b>job</b> <b>runs</b> {{in its own}} address space. However, {{there are}} some single address space {{operating}} systems that run all processes within a single address space, such as IBM i, which runs all processes within a large address space, and IBM OS/VS2 SVS, which <b>ran</b> all <b>jobs</b> in a single 16MiB virtual address space.|$|R
50|$|Students {{typically}} work 8 to 15 {{hours per}} week, <b>jobs</b> <b>run</b> {{the gamut of}} campus need from teaching assistant-ships, to accounts payable, to IT, to food services, to grounds keeping, to plumbing, public relations, campus safety etc. Work colleges share {{the belief that the}} integration of work-learning-service provides a strong, successful, and relevant model for educating students.|$|R
5000|$|What {{makes the}} North country {{valuable}} to conservationists is the seclusion, beauty, isolation, quiet, clear water, and absence of development. The preservationists {{have tried to}} limit or prohibit roads, hydroelectric generators, sawmills and lumbering, resorts, power boats, airplanes, and snowmobiles…. The tensions between development and preservation, restraint and grows, beauty and <b>jobs</b> <b>runs</b> deep and strong.|$|R
40|$|This paper {{presents}} a comprehensive {{statistical analysis of}} workloads collected on data-intensive clusters and Grids. The analysis is conducted at different levels, including Virtual Organization (VO) and user behavior. The aggregation procedure and scaling analysis are applied to job arrival processes, leading to the identification of several basic patterns, namely, pseudo-periodicity, long range dependence (LRD), and (multi) fractals. It is shown that statistical measures based on interarrivals are of limited usefulness and count based measures should be trusted instead {{when it comes to}} correlations. We also study workload characteristics like <b>job</b> <b>run</b> time, memory consumption, and cross correlations between these characteristics. A “bag-of-tasks” behavior is empirically proved, strongly indicating temporal locality. We argue that pseudo-periodicity, LRD, and “bag-of-tasks ” behavior are important workload properties on data-intensive clusters and Grids, which are not present in traditional parallel workloads. This study has important implications on workload modeling and performance predictions in data-intensive Grid environments. 1...|$|E
40|$|Flexible flow {{shops are}} {{becoming}} increasingly common in industry practice because of higher workloads imposed by jobs on one or more stages, thus requiring two or more units of the same machine type. We present a methodology for solving this important problem, namely group scheduling, {{within the context of}} cellular manufacturing systems in order to minimize the total completion time of all groups of jobs considered in the planning horizon. Two different setup options, namely ############ and ###############, are investigated for jobs within the same group. A combined heuristic solution algorithm, comprised of single- and multiple-pass heuristics, is used for solving the problem. An example problem, consisting of three different problem instances of the same structure, is used to not only demonstrate the applicability of the solution algorithm, but also to identify the setup to ####### <b>job</b> <b>run</b> time ratios that should ###############prevail across all stages represented by two or more identical units in order to select ############### over ############ or vice versa. Keywords Group Scheduling; Flexible Flow Shops 1...|$|E
40|$|Unstructured {{storage and}} data {{processing}} using platforms such as MapReduce are increasingly popular for their simplicity, scalability, and flexibility. Using elastic cloud storage and computation makes {{them even more}} attractive. However cloud providers such as Amazon and Windows Azure separate their storage and compute resources even within the same data center. Transferring data from storage to compute thus uses core data center network bandwidth, which is scarce and oversubscribed. As the data is unstructured, the infrastructure cannot automatically apply selection, projection, or other filtering predicates at the storage layer. The problem is even worse if customers want to use compute resources on one provider but use data stored with other provider(s). The bottleneck is now the WAN link which impacts performance but also incurs egress bandwidth charges. This paper presents Rhea, a system to automatically generate and run storage-side data filters for unstructured and semi-structured data. It uses static analysis of application code to generate filters that are safe, stateless, side effect free, best effort, and transparent to both storage and compute layers. Filters never remove data {{that is used by}} the computation. Our evaluation shows that Rhea filters achieve a reduction in data transfer of 2 x– 20, 000 x, which reduces <b>job</b> <b>run</b> times by up to 5 x and dollar costs for cross-cloud computations by up to 13 x. ...|$|E
5000|$|...JOBQ: Job queue (used {{to queue}} up batch <b>jobs</b> to <b>run</b> in a subsystem).|$|R
50|$|The {{government}} executive occupation also covers many jobs. They {{range from}} city council member, mayor, and governor, {{all the way}} up to President of the United States. Some of these positions are elected. Other officials are appointed to their jobs. In smaller communities, many of these jobs may be volunteer positions. Sometimes these <b>jobs</b> <b>run</b> through only part of the year.|$|R
30|$|In the Detroit site, the {{transitional}} <b>jobs</b> program was <b>run</b> by Goodwill Industries of Greater Detroit. In the Minneapolis-St. Paul site, {{the transitional}} <b>jobs</b> program was <b>run</b> by Goodwill/Easter Seals Minnesota.|$|R
40|$|The {{date of receipt}} and {{acceptance}} will be inserted by the editor Abstract This paper presents a comprehensive statistical analysis of workloads collected on production clusters and Grids. The applications we focus on are dataintensive in nature, which dominate the workloads running on current production Grid environments. Trace data obtained on a parallel supercomputer is included for comparison studies. We investigate the statistical properties of workloads at different levels, including Virtual Organization (VO) and user behavior. The spectrum and the scaling analysis are applied to job arrival processes, leading to the identification of several important patterns, namely pseudo-periodity, long range dependent (LRD), and multifractals. It is shown that statistical measures based on interarrivals are of limited usefulness and count based measures should be trusted instead {{when it comes to}} correlations. We also study workload characteristics like <b>job</b> <b>run</b> time, memory consumption, and cross correlations between these characteristics. Based on these results a “bag-of-tasks ” behavior is empirically evidenced for data-intensive workloads, strongly indicating temporal locality. We argue that pseudo-periodicity, LRD, and “bag-of-tasks ” behavior are important workload characteristics on data-intensive clusters and Grids, which are not reported in studies on parallel workloads. This study has important implications on workload modeling and performance predictions, and points out the need of comprehensive performance evaluation studies given the discovered workload patterns. ...|$|E
40|$|Title from PDF {{of title}} page viewed October 30, 2017 Dissertation advisor: Deep MedhiVitaIncludes bibliographical {{references}} (pages 122 - 135) Thesis (Ph. D.) [...] School of Computing and Engineering. University of Missouri [...] Kansas City, 2017 This dissertation investigates improvement in application performance. For applications, we consider two classes: Hadoop MapReduce and video streaming. The Hadoop MapReduce (M/R) framework {{has become the}} de facto standard for Big Data analytics. However, the lack of network-awareness of the default MapReduce resource manager in a traditional IP network can cause unbalanced job scheduling and network bottlenecks; such factors can eventually {{lead to an increase}} in the Hadoop MapReduce job completion time. Dynamic Video streaming over the HTTP (MPEG-DASH) is becoming the defacto dominating transport for today’s video applications. It has been implemented in today’s major media carriers such as Youtube and Netﬂix. It enables new video applications to fully utilize the existing physical IP network infrastructure. For new 3 D immersive medias such as Virtual Reality and 360 -degree videos are drawing great attentions from both consumers and researchers in recent years. One of the biggest challenges in streaming such 3 D media is the high band width demands and video quality. A new Tile-based video is introduced in both video codec and streaming layer to reduce the transferred media size. In this dissertation, we propose a Software-Deﬁned Network (SDN) approach in an Application-Aware Network (AAN) platform. We ﬁrst present an architecture for our approach and then show how this architecture can be applied to two aforementioned application areas. Our approach provides both underlying network functions and application level forwarding logics for Hadoop MapReduce and video streaming. By incorporating a comprehensive view of the network, the SDN controller can optimize MapReduce work loads and DASH ﬂows for videos by application-aware trafﬁc reroute. We quantify the improvement for both Hadoop and MPEG-DASH in terms of job completion time and user’s quality of experience (QoE), respectively. Based on our experiments, we observed that our AAN platform for Hadoop MapReduce job optimization offer a signiﬁcant improvement compared to a static, traditional IP network environment by reducing <b>job</b> <b>run</b> time by 16 % to 300 % for various MapReduce benchmark jobs. As for MPEG-DASH based video streaming, we can increase user perceived video bitrate by 100 %. Introduction [...] Research survey [...] Proposed architecture [...] AAN-SDN for Hadoop [...] Study of User QoE Improvement for Dynamic Adaptive Streaming over HTTP (MPEG-DASH) [...] AAN-SDN For MPEG-DASH [...] Conclusion [...] Appendix A. Mininet Topology Source Code For DASH Setup [...] Appendix B. Hadoop Installation Source Code [...] Appendix C. Openvswitch Installation Source Code [...] Appendix D. HiBench Installation Guid...|$|E
40|$|WELCOME TO Cogstack Introduction CogStack is a {{lightweight}} distributed, fault tolerant database processing architecture, {{intended to make}} NLP processing and preprocessing easier in resource constained environments. It makes use of the Spring Batch framework {{in order to provide}} a fully configurable pipeline with the goal of generating an annotated JSON that can be readily indexed into elasticsearch, or pushed back to a database. In the parlance of the batch processing domain language, it uses the partitioning concept to create 'partition step' metadata for a DB table. This metadata is persisted in the Spring database schema, whereafter each partition can then be executed locally or farmed out remotely via a JMS middleware server (only ActiveMQ is suported at this time). Remote worker JVMs then retrieve metadata descriptions of work units. The outcome of processing is then persisted in the database, allowing robust tracking and simple restart of failed partitions. Why does this project exist/ why is batch processing difficult? The CogStack is a range of technologies designed to to support modern, open source healthcare analytics within the NHS, and is chiefly comprised of the Elastic stack (elasticsearch, kibana etc), GATE, Bioyodie and Biolark (clinical natural language processing for entity extraction), OCR, clinical text de-identification, and Apache Tika for MS Office to text conversion. When processing very large datasets (10 s - 100 s of millions rows of data), it is likely that some rows will present certain difficulties for different processes. These problems are typically hard to predict - for example, some documents may have very long sentences, an unusual sequence of characters, or machine only content. Such circumstances can create a range of problems for NLP algorithms, and thus a fault tolerant batch frameworks are required to ensure robust, consistent processing. Installation We're not quite at a regular release cycle yet, so if you want a stable version, I suggest downloading v 1. 0. 0 from the release page. However, if you want more features and (potentially) fewer bugs, it's best to build from source on the master branch. To build from source: Install Tesseract and Imagemagick (can be installed but apt-get on Ubuntu) Run the following: gradlew clean build Quick Start Guide The absolute easiest way to get up and running with CogStack is to use Docker. Docker can provide lightweight virtualisation of a variety of microservices that CogStack makes use of. When coupled with the microservice orchestration docker compose technology, all of the components required to use CogStack can be set up with a few simple commands. First, ensure you have docker v 1. 13 or above installed. Elasticsearch in docker requires the following to be set on the host: sudo sysctl -w vm. max_map_count= 262144 Now you need to build the required docker containers. Fortunately, the gradle build file can do this for you. From the CogStack top level directory: gradlew buildSimpleContainers Assuming the containers have been built successfully, simply navigate to cd docker-cogstack/compose-ymls/simple/ And type docker-compose up All of the docker containers should be up and communicating with each other. You can view their status with docker ps -a That's it! "But that's what?", I hear you ask? The high level workflow of CogStack is as follows: Read a row of the table into the CogStack software Process the columns of the row with inbuilt Processors Construct a JSON that represents the table row and new data arising from the webservice Index the JSON into an elasticsearch cluster Visualise the results with Kibana To understand what's going on, we need to delve into what each of the components is doing. Let's start with the container called 'some-postgres'. Let's assume this is a database that contains a table that we want to process somehow. In fact this example database already contains some example data. If you have some database browsing software, {{you should be able to}} connect to it with the following JDBC confguration source. JdbcPath = jdbc:postgresql://localhost: 5432 /cogstack source. Driver = org. postgresql. Driver source. username = cogstack source. password = mysecretpassword You should see a table called 'tblinputdocs' in the 'cogstack' database with four lines of dummy data. This table is now constantly being scanned and indexed into elasticsearch. If you know how to use the Kibana tool, you can visualise the data in the cluster. Now bring the compose configuration down with from the same compose directory as before: docker-compose down This is the most basic configuration, and really doesn't do too much other than convert a database table/view into an elasticsearch index. For more advanced use cases/configurations, check out the integration test below. Integration Tests Although cogstack has unit tests where appropriate, the nature of the project is such that the real value fo testing comes from the integration tests. Consequently, cogstack has an extensive suite. To run the integration tests, ensure the required external services are available (which also give a good idea of how cogstack is configured). These services are Postgresql, Biolark, Bioyodie and Elasticsearch. The easiest way to get these going is with Docker. Once you have docker installed, cogstack handily will build the containers you need for you (apart from elasticsearch, where the official image will suffice). To build the containers: From the CogStack top level directory: gradlew buildAllContainers Note, Biolark and Bioyodie are external applications. Building their containers (and subsequently running their integration tests) may require you to meet their licencing conditions. Please check with Tudor Groza (Biolark) and Angus Roberts/Genevieve Gorrell if in doubt. Assuming the containers have been built successfully, navigate to cd docker-cogstack/compose-ymls/nlp/ And type docker-compose up to launch all of the external services. All being well, you should now be able to run the integration tests. Each of these demonstrate a different facet of cogstack's functionality. Each integration test follows the same pattern: Generate some dummy data for processing, by using an integration test execution listener Activate a configuration appropriate for the data and run cogstack Verify results All integration tests for Postgres can be run by using: gradlew postgresIntegTest Although if you're new to cogstack, you might find it more informative to run them individually, and inspect the results after each one. For example, to run a single test: gradlew -DpostgresIntegTest. single= -i postgresIntegTest Available classes for integration tests are in the package src/integration-test/java/uk/ac/kcl/it/postgres For example, to load the postgres database with some dummy word files into a database table called, process them with Tika, and load them into ElasticSearch index called and a postgres table called gradlew -DpostgresIntegTest. single=TikaWithoutScheduling -i postgresIntegTest then point your browser to localhost: 5601 A note on SQL Server Microsoft have recently made SQL Server available on linux, with a docker container available. This is good news, as most NHS Trusts use SQL Server for most of their systems. To run this container docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!) Password' -p 1433 : 1433 -d microsoft/mssql-server-linux [...] . noting their licence conditions. This container will then allow you to run the integration tests for SQL Server: gradlew sqlServergresIntegTest Single tests can be run in the same fashion as Postgres, substituting the syntax as appropriate (e. g.) gradlew -DsqlServerIntegTest. single=TikaWithoutScheduling -i sqlServerIntegTest A note on GATE Applications that require GATE generally need to be configured to point to the GATE installation directory (or they would need to include a rather large amount of plugins on their classpath). To do this in cogstack, set the appropriate properties as detailed in gate. *. Acceptance Tests The accompanying manuscript for this piece describes some artifically generated pseudo-documents containing misspellings and other string mutations in order to validate the de-identification algorithm without requiring access to real world data. These results can be replicated (subject to RNG) by using the acceptance test package. To reproduce the results described in the manuscript, simply run the following command: gradlew -DacceptTest. single=ElasticGazetteerAcceptanceTest -i acceptTest to reconfigure this test class for the different conditions described in the manuscript, you will need to alter the parameters inside the elasticgazetteer_test. properties file, which describes the potential options. For efficiency, it is recommended to do this from inside an IDE. Example usage in real world deployments The entire process is run through the command line, taking a path to a directory as a single argument. This directory should contain configuration files, (one complete one per spring batch job that you want to run simultaneously). These config files selectively activate Spring profiles as required to perform required data selection, processing and output writing steps. Examples of config file are in the exampleConfigs dir. Most are (hopefully) relatively self explanatory, or should be annotated to explain their meaning. example configs can be generated from the gradle task: gradlew writeExampleConfig The behaviour of cogstack is configured by activating a variety of spring profiles (again, in the config files - see examples) as required. Currently. the available profiles are inputs jdbc_in - Spring Batch's JdbcPagingItemReader for reading from a database table or view. Also requires a partitioning profile to be activated, to set a partitioning strategy. If you don't know what you're doing, just use the primaryKeyPartition profile. docmanReader - a custom reader for system that stores files in a file system, but holds their path in a database. Weird [...] . processes tika - process JDBC input with Tika. Extended with a custom PDF preprocessor to perform OCR on scanned PDF document. (requires ImageMagick and Tesseract on the PATH) gate - process JDBC input with a generic GATE app. dBLineFixer - process JDBC input with dBLineFixer (concatenates multi-row documents) basic - a job without a processing step, for simply writing JDBC input to elasticsearch deid - deidentify text with a GATE application (such as the Healtex texscrubber) or using the Cognition algorithm, which queries a database for identifiers and mask them in free text using Levenstein distance. webservice - send a document to a webservice (such as an NLP REST service, like bioyodie/biolark) for annotation. The response should be a JSON, so it can be mapped to Elasticsearch's 'nested' type. scaling localPartitioning - run all processes within the launching JVM remotePartitioning - send partitions to JMS middleware, to be picked up by remote hosts (see below) outputs elasticsearch - write to an elasticsearch cluster jdbc_out - write the generated JSON to a JDBC endpoint. Useful if the selected processes are particularly heavy (e. g. biolark), so that data can be reindexed without the need for reprocessing partitioning primaryKeyPartition - process all records based upon partitioning of the primary key primaryKeyAndTimeStampPartition - process all records based upon partitioning of the primary key and the timestamp, for finer control/ smaller batch sizes per job. Use the processingPeriod property to specify the number of milliseconds to 'scan' ahead for each <b>job</b> <b>run</b> Scheduling CogStack also offers a built in scheduler, to process changes in a database between job runs (requires a timestamp in the source database) useScheduling = true run intervals are handled with the following CRON like syntax scheduler. rate = "*/ 5 * * * * *" Logging support CogStack uses the SLF 4 J abstraction for logging, with logback as the concrete implementation. To name a logfile, simply add the -DLOG_FILE_NAME system flag when launching the JVM e. g. java -DLOG_FILE_NAME=aTestLog -DLOG_LEVEL=debug -jar cogstack- 0. 3. 0. jar /my/path/to/configs CogStack assumes the 'job repository' schema is already in place in the DB implementation of your choice (see spring batch docs for more details). The scripts to set this up for various vendors can be found here Scaling To add additional JVM processes, whether locally or remotely (via the magic of Spring Integration), just launch an instance with the same config files but with useScheduling = slave. You'll need an ActiveMQ server to co-ordinate the nodes (see config example for details) If a job fails, any uncompleted partitions will be picked up by the next run. If a Job ends up in an unknown state (e. g. due to hardware failure), the next run will mark it as abandonded and recommence from the last successful job it can find in the repository. JDBC output/reindexing Using the JDBC output profile, it is possible to generate a column of JSON strings back into a database. This is useful for reindexing large quantities of data without the need to re-process with the more computationally expensive item processors (e. g. OCR, biolark). To reindex, simply use the reindexColumn in the configuration file. Note, if you include other profiles, these will still run, but will not contribute to the final JSON, and are thus pointless. Therefore, only the 'basic' profile should be used when reindexing data. reindex = true #select the column name of jsons in the db table reindexField = sometext History This project is an update of an earlier KHP-Informatics project I was involved with called Cognition. Although Cognition had an excellent implementation of Levenstein distance for string substitution (thanks iemre!), the architecture of the code suffered some design flaws, such as an overly complex domain model and configuration, and lack of fault tolerance/job stop/start/retry logic. As such, it was somewhat difficult to work with in production, and hard to extend with new features. It was clear that there was the need for a proper batch processing framework. Enter Spring Batch and a completely rebuilt codebase, save a couple of classes from the original Cognition project. cogstack is used at King's College Hospital and the South London and Maudsley Hospital to feed Elasticsearch clusters for business intelligence and research use cases Some of the advancements in cogstack: A simple map, with a few pieces of database metadata for its domain model (essentially mapping a database row to a elasticsearch document, with the ability to embed nested types Complete, sensible coverage of stop, start, retry, abandon logic A custom socket timeout factory, to manage network failures, which can cause JDBC driver implementations to lock up, when the standard isn't fully implemented. Check out this blog post for info. The ability to run multiple batch jobs (i. e. process multiple database tables within a single JVM, each having its own Spring container Remote partitioning via an ActiveMQ JMS server, for complete scalability Built in job scheduler to enable near real time synchronisation with a database Questions? Want to help? Drop me a message: richgjackson@gmail. co...|$|E
40|$|We {{observed}} that the Condor batch execution system exposes {{a lot of information}} about the <b>jobs</b> that <b>run</b> in the system. This observation led us to explore whether this system information could be used for provenance. The result of our explorations is Provenance Aware Condor (PAC), a system that transparently gathers provenance while <b>jobs</b> <b>run</b> in Condor. Transparent provenance gathering requires that the application not be altered in order to run in the provenance system. This requirement allows any application that can run in Condor to also run in PAC. Through SQL queries, PAC is able to answer a wide range of questions about the files used by a job and the machines that execute jobs. ...|$|R
5000|$|Various schemes {{are used}} to decide which {{particular}} <b>job</b> to <b>run.</b> Parameters that might be considered include: ...|$|R
5000|$|... {{according}} to Domain and Affinity tags, <b>job</b> is <b>run</b> on an appropriate node {{and moved to}} running queue ...|$|R
