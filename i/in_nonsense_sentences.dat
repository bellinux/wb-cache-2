2|775|Public
40|$|We {{tested the}} {{hypothesis}} that, given {{the consistency of}} tone-syllable alignment found in recent research, accuracy of tone perception {{is dependent on the}} accuracy of syllable perception. In two experiments, subjects either judged the number of sylla-bles or identified the tones <b>in</b> <b>nonsense</b> <b>sentences</b> that were spectrally intact, low-pass filtered at 300 Hz or converted to sustained schwa carrying the original F 0. It was found that removing spectral information affected not only subjects’ ability to judge the number of syllables in a sentence, but also their ability to identify the tones. The results thus confirm the dependence of tone perception on syllable perception. Index Terms: speech perception, tone perception, syllable perception, tone-syllable alignmen...|$|E
40|$|Clear {{speech is}} a form of {{communication}} that talkers naturally use when speaking in difficult listening conditions or with a person who has a hearing loss. Clear speech, on average, provides listeners with hearing impairments an intelligibility benefit of 17 percentage points (Picheny, Durlach, 2 ̆ 6 Braida, 1985) over conversational speech. In addition, it provides increased intelligibility in various listening conditions (Krause 2 ̆ 6 Braida, 2003, among others), with different stimuli (Bradlow 2 ̆ 6 Bent, 2002; Gagne, Rochette, 2 ̆ 6 Charest, 2002; Helfer, 1997, among others) and across listener populations (Bradlow, Kraus, 2 ̆ 6 Hayes, 2003, among others). Recently, researchers have attempted to compare their findings with clear and conversational speech, at slow and normal rates, with results from other investigators 2 ̆ 7 studies in an effort to determine the relative benefits of clear speech across populations and environments. However, relative intelligibility benefits are difficult to determine unless baseline performance levels can be equated, suggesting that listener psychometric functions with clear speech are needed. The {{purpose of this study was}} to determine how speech intelligibility, as measured by percentage key words correct <b>in</b> <b>nonsense</b> <b>sentences</b> by young adults, varies with changes in speaking condition, talker and signal-to-noise ratio (SNR). Forty young, normal hearing adults were presented with grammatically correct nonsense sentences at five SNRs. Each listener heard a total of 800 sentences in four speaking conditions: clear and conversational styles, at slow and normal rates (i. e., clear/slow, clear/normal, conversational/slow, and conversational/normal). Overall results indicate clear/slow and conversational/slow were the most intelligible conditions, followed by clear/normal and then conversational/normal conditions. Moreover, the average intelligibility benefit for clear/slow, clear/normal and conversational/slow conditions (relative to conversational/normal) was maintained across an SNR range of - 4 to 0 dB in the middle, or linear, portion of the psychometric function. However, when results are examined by talker, differences are observed in the benefit provided by each condition and in how the benefit varies across noise levels. In order to counteract talker variability, research with a larger number of talkers is recommended for future studies...|$|E
50|$|The game of {{exquisite}} corpse is {{a method}} for generating <b>nonsense</b> <b>sentences.</b> It was named after the first sentence generated in the game in 1925 Le cadavre exquis boira le vin nouveau (the exquisite corpse will drink the new wine).|$|R
50|$|Bouwsma taught {{philosophy}} at the University of Nebraska from 1928 until 1965 and the University of Texas from 1965 until 1977. His greatest influence came, {{not so much}} through his humorously and finely written essays, but through the many graduate students he trained in his unique style of exploring the borderlands of sense and <b>nonsense</b> <b>in</b> philosophical <b>sentences.</b> Although he wrote incessantly and presented numerous papers, he published only one book {{toward the end of}} his career - a collection of essays titled Philosophical Essays. He died in 1978. His papers and daily notebooks, the latter filling hundreds of legal pads, are housed in the Harry Ransom Humanities Research Center at The University of Texas, Austin. J.L. Craft and Ronald E. Hustwit Sr. co-edited and published two additional volumes of his papers and selections of his commonplace book. His notebooks recording his discussions with Wittgenstein, published with the title, Wittgenstein Conversations, 1949-51, have become a primary source for Wittgenstein studies.|$|R
5000|$|The German {{sentence}} in the figures reads: [...] " [...] of Old Schwabacher: [...] ". This is a <b>nonsense</b> <b>sentence</b> meaning [...] "Victor chases twelve boxers across the great dam of Sylt", but contains all 26 {{letters of the alphabet}} plus the German umlauts and is thus an example of a pangram.|$|R
40|$|This study {{investigated}} how confusability between target and masking utterances affects the masking release achieved through spatial separation. Important distinguishing characteristics between competing voices were removed by processing speech with six-channel envelope vocoding, which simulates {{some aspects of}} listening with a cochlear implant. In the first experiment, vocoded target <b>nonsense</b> <b>sentences</b> were presented against two-talker vocoded maskers in conditions that provide different spatial impressions but not reliable cues that lead to traditional release from masking. Surprisingly, no benefit of spatial separation was found. The absence of spatial release was hypothesized {{to be the result}} of the highly positive target-to-masker ratios necessary to understand vocoded speech, which may have been sufficient to reduce confusability. In experiment 2, words excised from the vocoded <b>nonsense</b> <b>sentences</b> were presented against the same vocoded two-talker masker in a four-alternative forced-choice detection paradigm where threshold performance was achieved at negative target-to-masker ratios. Here, the spatial release from masking was more than 20 dB. The results suggest the importance of signal-to-noise ratio in the observation of “informational” masking and indicate that careful attention should be paid to this type of masking as implant processing improves and listeners begin to achieve success in poorer listening environments...|$|R
5000|$|In {{the opening}} theme, one sound clip is changed {{in each episode}} which, when {{following}} the previous clip, makes up a <b>nonsense</b> <b>sentence.</b> Depending on the season, the preceding clip is either Franklin D. Roosevelt's [...] "The only {{thing we have to}} fear is..." [...] (from his famous inauguration speech)(seasons 1, 3 & 4), a man saying, [...] "I can't believe I ate that whole..." [...] (from a commercial for Alka-Seltzer)(season 2), or an announcer saying [...] "Tonight's episode brought to you by..." [...] (season 5).|$|R
40|$|A {{sentence}} verification {{task was}} developed to investigate semantic memory in schizophrenia. The test consisted of three types of sentence (true, unlikely and nonsense) and seven different types of content (neutral, persecutory, grandiose, political, religious, relationships and somatic) representing common delusional themes present in schizophrenic patients. Sixty-three schizophrenic patients and 66 matched control {{subjects were asked to}} make true/false judgements to 143 sentences. Overall accuracy was similar across the two groups; sentences with some emotional themes and sentences of the unlikely type produced the most violations. Significant differences between the two subject groups were found specifically on <b>nonsense</b> <b>sentences</b> with persecutory and religious themes. Patients made significantly more incorrect responses (acceptance) to <b>nonsense</b> <b>sentences</b> that had an emotional content congruent with their delusional beliefs, past or present, and also on unlikely sentences (incorrect rejections) whose content was not congruent with their delusions. Further analysis of response bias in the patients showed, overall, that there were more incorrect rejections (a reflection of the large number of unlikely sentence errors) and more incorrect responses to sentences congruent with patients delusions. Furthermore, analysis of those patients currently experiencing delusions revealed more incorrect responses to sentences congruent with their delusional ideas compared with patients not currently deluded. These findings are indicative of cognitive bias in schizophrenia towards certain emotional themes that may underlie illogical semantic connections and delusions...|$|R
40|$|We {{evaluated}} {{the effects of}} different electrical parameter settings on the intelligibility of speech in patients with Parkinson's disease (PD) bilaterally treated with deep brain stimulation (DBS) in the subthalamic nucleus (STN). Ten patients treated with DBS for 15 +/- 5 months (mean, SD) with significant (P < 0. 01) symptom reduction (Unified Parkinson's Disease Rating Scale III) were included. In the medication off condition, video laryngostroboscopy was performed and then, in random order, 1 1 DBS parameter settings were tested. Amplitude was increased and decreased by 25 %, frequency was varied in the range 70 to 185 pps, {{and each of the}} contacts was tested separately as a cathode. The patients read a standard running text and five <b>nonsense</b> <b>sentences</b> per setting. A listener panel transcribed the <b>nonsense</b> <b>sentences</b> as perceived and valued the quality of speech on a visual analogue scale. With the patients' normally used settings, there was no significant (P = 0. 058) group difference between DBS OFF and ON, but in four patients the intelligibility deteriorated with DBS ON. The higher frequencies or increased amplitude caused significant (P < 0. 02) impairments of intelligibility, whereas changing the polarity between the separate contacts did not. The settings of amplitude and frequency have a major influence on the intelligibility of speech, emphasizing the importance of meticulous parameter adjustments when programming DBS to minimize side effects related to speech...|$|R
5000|$|These <b>nonsense</b> <b>sentences</b> {{were created}} <b>in</b> order to {{eliminate}} any non-prosodic interference (e.g phonological differences, different number of syllables, etc.) thus babies {{would only be}} able to differentiate between the two languages based on the prominence of prosodic cues in the sentences. The table above depicts the sentences heard by the French babies (translated as [...] "The large orangoutang was nervous"), where the bolded and enlarged letter indicates word stress and prominence (Christophe et al. 2003). As predicted, French babies tended to prefer the modified nonsense French phrases, based solely on prosodic prominence, given by the location of the head direction parameter.|$|R
40|$|Three {{experiments}} investigated {{factors that}} influence the creation of and release from informational masking in speech recognition. The target stimuli were <b>nonsense</b> <b>sentences</b> spoken by a female talker. In experiment 1 the masker {{was a mixture of}} three, four, six, or ten female talkers, all reciting similar <b>nonsense</b> <b>sentences.</b> Listeners’ recognition performance was measured with both target and masker presented from a front loudspeaker ~F–F! or with a masker presented from two loudspeakers, with the right leading the front by 4 ms ~F–RF!. In the latter condition the target and masker appear to be from different locations. This aids recognition performance for one- and two-talker maskers, but not for noise. As the number of masking talkers increased to ten, the improvement in the F–RF condition diminished, but did not disappear. The second experiment investigated whether hearing a preview ~prime! of the target sentence before it was presented in masking improved recognition for the last key word, which {{was not included in the}} prime. Marked improvements occurred only for the F–F condition with two-talker masking, not for continuous noise or F–RF two-talker masking. The third experiment found that the benefit of priming in the F–F condition was maintained if the prime sentence was spoken by a different talker or even if it was printed and read silently. These results suggest that informational masking can be overcome by factors that improve listeners’ auditory attention toward the target...|$|R
30|$|Interestingly, {{semantic}} {{processing of}} words presented between the hands {{appears to be}} impaired, consistent with a decrease in holistic analysis and a greater focus on visual detail. Davoli, Du, Montana, Garverick, and Abrams (2010) presented sentences between participants’ hands {{and asked them to}} judge whether or not they made sense. Detection of <b>nonsense</b> <b>sentences</b> between the hands was impaired. In a follow-up experiment, participants completed a Stroop task (naming the font color of a word while ignoring the word’s meaning). Stroop interference was decreased when the word was presented between participants’ hands. The authors interpreted this as a decrease in semantic processing and an increase is spatial processing.|$|R
40|$|We {{prove that}} an {{integral}} Cauchy-Riemann inequality holds for any pair of smooth functions (f,h) on the 2 -sphere S^ 2, and equality holds iff f and h are related λ_ 1 -eigenfunctions. We extend such inequality to 4 -tuples of functions, only valid on the L^ 2 -orthogonal complement of a suitable nonzero finite dimensional space of functions. As a consequence we prove that 2 -spheres are not Ω-stable surfaces with parallel mean curvature in R^ 7 for the associative calibration Ω. Comment: 24 pages. LaTex 2 e V 2 : we correct some minor misprints. Remove a <b>nonsense</b> <b>sentence</b> <b>in</b> corollary 1. 1, and correct a referenc...|$|R
40|$|The {{present study}} {{examined}} information processing differences {{between good and}} poor and fast and slow college-age readers. Four groups (high comprehension-high speed, high comprehension-low speed, low comprehension-high speed, and low comprehension-low speed) were {{selected on the basis}} of performance on the Nelson-Denny reading comprehension subtest and a timed reading sample. Performance among groups was compared on five information processing tasks, including word verification, sentence verification, letter reordering, word reordering, and reading span tasks. ^ No significant differences were found among groups for raw scores in the verification tasks. Groups did differ significantly on time scores for the verification tasks and on performance on letter reordering, word reordering, and reading span tasks. Follow-up procedures revealed that high comprehenders with either fast or slow reading speed outperformed low comprehenders on verification of words, nonwords, and <b>nonsense</b> <b>sentences,</b> and on letter reordering, word reordering, and reading span tasks. High comprehenders outperformed low comprehenders in verifying real sentences only if they were low speed readers, while high speed readers outperformed low speed readers in verification of nonwords and <b>nonsense</b> <b>sentences</b> only when they were low comprehenders. High speed readers in both low and high comprehension categories outperformed low speed readers in letter reordering. ^ Performance on tasks was discussed in light of speed and comprehension variables and type of information processing task (Palmer, MacLeod, Hunt, 2 ̆ 6 Davidson, 1985), while the Just and Carpenter (1980) model of reading provided the framework for understanding results in relationship to the reading process. Differences in working memory were proposed as a major source of individual differences in reading performance. ...|$|R
5000|$|The vowels [...] {{and short}} [...] occur <b>in</b> <b>nonsense</b> syllables <b>in</b> Haida songs.|$|R
40|$|Many {{languages}} have {{restrictions on}} word-final segments, {{such as a}} requirement that any word- final obstruent be voiceless. There is a phonetic basis for such restrictions {{at the ends of}} utterances, but not the ends of words. Historical linguists have long noted this mismatch, and have attributed it to an analogical generalization of such restrictions from utterance-final to word-final position. To test whether language learners actually generalize in this way, two artificial language learning experiments were conducted. Participants heard <b>nonsense</b> <b>sentences</b> <b>in</b> which there was a restriction on utterance-final obstruents, but in which no information was available about word-final, utterance-medial obstruents. They were then tested on utterances that included obstruents in both positions. They learned the pattern and generalized it to word-final utterance-medial position, confirming that learners are biased toward word-based distributional patterns...|$|R
50|$|The {{stump speech}} was a comic {{monologue}} from blackface minstrelsy (which is an American entertainment consisting of comic skits, variety acts, dancing, and music, performed by {{white people in}} blackface). A typical stump speech consisted of malapropisms (the substitution of a word for a word with a similar sound), <b>nonsense</b> <b>sentences,</b> and puns delivered in a parodied version of Black Vernacular English. The stump speaker wore blackface makeup and moved about like a clown. Topics varied from pure nonsense to parodies of politics, science, and social issues. Although both the topic itself and the black character's inability to comprehend it served as sources of comedy, minstrels used such speeches to deliver social commentary that might be considered taboo in another setting. The stump speech was an important precursor to modern stand-up comedy.|$|R
50|$|Partial loss of {{function}} results <b>in</b> <b>nonsense</b> suppression, <b>in</b> which stop codons are ignored and proteins are abnormally synthesized with carboxyl terminal extensions. Complete loss {{of function}} is fatal.|$|R
40|$|It has {{consistently}} {{been shown that}} among the three mainland Scandinavian languages, Danish is most difficult to understand for fellow Scandinavians. Recent research suggests that Danish is spoken significantly faster than Norwegian and Swedish. This finding might partly explain the asymmetric intelligibility among Scandinavian languages. However, since fast speech {{goes hand in hand}} with a high amount of speech reduction, the question arises whether the high speech rate as such impairs intelligibility, or the high amount of reduction. In this paper we tear apart these two factors by auditorily presenting 168 Norwegian- and Swedish-speaking participants with 50 monotonised <b>nonsense</b> <b>sentences</b> <b>in</b> four conditions (quick and unclear, slow and clear, quick and clear, slow and unclear) in a translation task. Our results suggest that speech rate has a larger impact on the intelligibility of monotonised speech than naturally occurring reduction...|$|R
50|$|Many {{comparisons}} {{have been}} drawn {{to the work of}} key absurdist playwright Eugène Ionesco. However, Simpson denies any link, adding that he had never even heard of the writer when he commenced a career <b>in</b> <b>nonsense.</b> <b>In</b> his own view, the valid literary parallels are with Lewis Carroll, James Thurber and P. G. Wodehouse.|$|R
40|$|This paper reviews past work {{comparing}} modern {{speech recognition}} systems and humans {{to determine how}} far recent dramatic advances in technology have progressed towards the goal of human-like performance. Comparisons use six modern speech corpora with vocabularies ranging from 10 to more than 65, 000 words and content ranging from read isolated words to spontaneous conversations. Error rates of machines are often more than {{an order of magnitude}} greater than those of humans for quiet, wideband, read speech. Machine performance degrades further below that of humans in noise, with channel variability, and for spontaneous speech. Humans can also recognize quiet, clearly spoken nonsense syllables and <b>nonsense</b> <b>sentences</b> with little high-level grammatical information. These comparisons suggest that the human–machine performance gap can be reduced by basic research on improving low-level acoustic-phonetic modeling, on improving robustness with noise and channel variability, and on more accurately modeling spontaneous speech...|$|R
30|$|Participants were {{required}} to read sentences while trying to remember {{the same set of}} unrelated letters as Ospan. For this task, participants read a sentence and determined whether the sentence made sense or not (e.g., “The prosecutor’s dish was lost because it was not based on fact. ?”). Half of the sentences made sense while the other half did not. <b>Nonsense</b> <b>sentences</b> were made by simply changing one word (e.g., “dish” from “case”) from an otherwise normal sentence. Participants {{were required}} to read the sentence and to indicate whether it made sense or not. After participants gave their response they were presented with a letter for 1 s. At recall, letters from the current set were recalled in the correct order by clicking on the appropriate letters (see Unsworth et al., 2009 for more details). There were three trials of each list-length with list-length ranging from 3 – 7 for a total possible of 75. The same scoring procedure as Ospan was used.|$|R
40|$|In adverse {{listening}} conditions, {{large and}} robust increases in intelligibility {{can be achieved}} by speaking clearly. The most striking differences between clear and conver-sational speech are associated with differences in speaking rate. To understand these differences, the intelligibility of speech in a variety of speaking modes was investigated at three different speaking rates. Talkers with significant speaking experience were asked to produce clear and conversational speech at slow, normal, and quick rates. Previous studies show that the speaking rate for clear speech (100 words-per-minute) is roughly one-half that of normal rates for conversational speech. Therefore, during training, the talkers were given feedback on their intelligibility in order to elicit the clearest possible speech at each speaking rate. Talkers also recorded sentences in sev-eral other speaking modes such as soft, loud, and conversational with pauses inserted, as required for input to some automatic speech recognition systems. All speech materials used for intelligibility tests were <b>nonsense</b> <b>sentences</b> whic...|$|R
40|$|There {{are reasons}} to believe that infant-directed (ID) speech may make {{language}} acquisition easier for infants. However, the effects of ID speech on infants 2 ̆ 7 learning remain poorly understood. The experiments reported here assess whether ID speech facilitates word segmentation from fluent speech. One group of infants heard a set of <b>nonsense</b> <b>sentences</b> spoken with intonation contours characteristic of adult-directed (AD) speech, and the other group heard the same sentences spoken with intonation contours characteristic of ID speech. In both cases, the only cue to word boundaries was the statistical structure of the speech. Infants were able to distinguish words from syllable sequences spanning word boundaries after exposure to ID speech but not after hearing AD speech. These results suggest that ID speech facilitates word segmentation and may be useful for other aspects of language acquisition as well. Issues of direction of preference in preferential listening paradigms are also considered...|$|R
40|$|This study {{tested the}} {{hypothesis}} that the previously reported advantage of musicians over non-musicians in understanding speech in noise arises from more efficient or robust coding of periodic voiced speech, particularly in fluctuating backgrounds. Speech intelligibility was measured in listeners with extensive musical training, and in those with very little musical training or experience, using normal (voiced) or whispered (unvoiced) grammatically correct <b>nonsense</b> <b>sentences</b> <b>in</b> noise that was spectrally shaped to match the long-term spectrum of the speech, and was either continuous or gated with a 16 -Hz square wave. Performance was also measured in clinical speech-in-noise tests and in pitch discrimination. Musicians exhibited enhanced pitch discrimination, as expected. However, no systematic or statistically significant advantage for musicians over non-musicians was found in understanding either voiced or whispered sentences in either continuous or gated noise. Musicians also showed no statistically significant advantage in the clinical speech-in-noise tests. Overall, the results provide no evidence for a significant difference between young adult musicians and non-musicians in their ability t...|$|R
5000|$|As Bayesian {{filtering}} {{has become}} popular as a spam-filtering technique, spammers have started using methods to weaken it. To a rough approximation, Bayesian filters rely on word probabilities. If a message contains many {{words that are}} used only in spam, and few that are never used in spam, {{it is likely to}} be spam. To weaken Bayesian filters, some spammers, alongside the sales pitch, now include lines of irrelevant, random words, in a technique known as Bayesian poisoning. A variant on this tactic may be borrowed from the Usenet abuser known as [...] "Hipcrime"—to include passages from books taken from Project Gutenberg, or <b>nonsense</b> <b>sentences</b> generated with [...] "dissociated press" [...] algorithms. Randomly generated phrases can create spoetry (spam poetry) or spam art. The perceived credibility of spam messages by users differs across cultures; for example, Korean unsolicited email frequently uses apologies, likely to be based on Koreans’ modeling behavior and a greater tendency to follow social norms.|$|R
40|$|Three experiments, {{in which}} Japanese {{listeners}} detected Japanese words embedded <b>in</b> <b>nonsense</b> sequences, examined the perceptual consequences of vowel devoicing in that language. Since vowelless sequences disrupt speech segmentation [Norris et al. (1997). Cognit. Psychol. 34, 191 – 243], devoicing is potentially problematic for perception. Words in initial position <b>in</b> <b>nonsense</b> sequences were detected more easily when {{followed by a}} sequence containing a vowel than by a vowelless segment (with or without further context), and vowelless segments that were potential devoicing environments were no easier than those not allowing devoicing. Thus asa, “morning,” was easier in asau or asazu than in all of asap, asapdo, asaf, or asafte, {{despite the fact that}} the /f/ in the latter two is a possible realization of fu, with devoiced [u]. Japanese listeners thus do not treat devoicing contexts as if they always contain vowels. Words in final position <b>in</b> <b>nonsense</b> sequences, however, produced a different pattern: here, preceding vowelless contexts allowing devoicing impeded word detection less strongly (so, sake was detected less accurately, but not less rapidly, in nyaksake—possibly arising from nyakusake—than in nyagusake). This is consistent with listeners treating consonant sequences as potential realizations of parts of existing lexical candidates wherever possible...|$|R
50|$|At ESC in Paris, 22 April, Björn Skifs {{intended}} to cause controversy by singing in English, which {{could make him}} disqualified. Instead, he sang {{a part of the}} song <b>in</b> <b>nonsense</b> words, but fortunately only Swedes knew the words were wrong. He finished 14th (out of 20).|$|R
5000|$|Enrico (2003) uses [...] {{for some}} {{instances}} of [...] based on morphophonemics. Alaskan Haida {{also has a}} diphthong written [...] Enrico & Stuart (1996) use [...] for the vowels [...] that occur <b>in</b> <b>nonsense</b> syllables <b>in</b> songs. The Alaskan Haida orthography was updated in 2010 by Jordan Lachler.|$|R
500|$|On {{the release}} {{date of the}} Potato Sack bundle, players found the games within it had {{recently}} received updates. Most provided an immediate cosmetic change by replacing or adding assets that referred to potatoes. When players started looking deeper into these new assets, they discovered a series of glyphs that referred to other games associated with specific letters, as well as <b>nonsense</b> <b>sentences</b> that lead to specific cyphers. Other hints were less direct, using online services such as Twitter and YouTube to embed clues. [...] In the case of Toki Tori, sections of new levels included braille code that referred to latitude and longitude coordinates of Two Tribes' headquarters. One player, [...] "Jake_R", traveled to Two Tribes, where he discovered the glyphs and cyphers posted outside their headquarters. Several of Two Tribes' developers, upon learning of his presence, began filming him from a barbershop across the street. They would later use this footage of him climbing a pole to find these clues as part of another clue during the second phase.|$|R
5000|$|Other {{than a few}} foreign words, morpheme-initial [...] doesnt occur (even its phonemic {{state is}} highly debated), {{therefore}} {{it is hard to}} find a real example when it induces voicing (even alapdzadzíki is forced and not used colloquially). However, the regressive voice assimilation before [...] does occur even <b>in</b> <b>nonsense</b> sound sequences.|$|R
5000|$|Jeeves and Wooster <b>in</b> Perfect <b>Nonsense</b> as Seppings (Theatre Royal, Nottingham) ...|$|R
50|$|Now the {{sentence}} makes absolutely no sense. In {{the case of}} a translating ribosome, a frameshift can result <b>in</b> <b>nonsense</b> being created after the frameshift or a completely different protein being created after the frameshift. When referring to translational frameshifting, the latter is always implied, the former being usually a result of a point mutation such as a deletion.|$|R
2500|$|Edward Lear makes {{reference}} to Bray <b>in</b> More <b>Nonsense</b> Pictures, Rhymes, Botany, etc: ...|$|R
40|$|This paper {{presents}} the Demo / Kemo corpus of Dutch and Korean emotional speech. The corpus has been specifically {{developed for the}} purpose of cross-linguistic comparison, and is more balanced than any similar corpus available so far: a) it contains expressions by both Dutch and Korean actors as well as judgments by both Dutch and Korean listeners; b) the same elicitation technique and recording procedure was used for recordings of both languages; c) the same <b>nonsense</b> <b>sentence,</b> which was constructed to be permissible in both languages, was used for recordings of both languages; and d) the emotions present in the corpus are balanced in terms of valence, arousal, and dominance. The corpus contains a comparatively large number of emotions (eight) uttered by a large number of speakers (eight Dutch and eight Korean). The counterbalanced nature of the corpus will enable a stricter investigation of language-specific versus universal aspects of emotional expression than was possible so far. Furthermore, given the carefully controlled phonetic content of the expressions, it allows for analysis of the role of specific phonetic features in emotional expression in Dutch and Korean. 1...|$|R
5000|$|Jeeves and Wooster <b>in</b> Perfect <b>Nonsense</b> (2013), {{based on}} The Code of the Woosters.|$|R
