4|150|Public
40|$|Topic Modeling {{algorithms}} {{are rarely}} {{used to support}} the qualitative content analysis process. The main contributing factors {{for the lack of}} mainstream adoption {{can be attributed to the}} perception that Topic Modeling produces topics of poor quality and that content analysts do not trust the <b>derived</b> <b>topics</b> because they are unable to supply domain knowledge and interact with the algorithm. In this paper, interactive Topic Modeling algorithms namely Dirichlet-Forrest Latent Dirichlet Allocation and Penalised Non-negative Matrix Factorisation, are evaluated with respect to their ability to aid qualitative content analysis. More specifically, the relationship between interactivity, interpretation, topic coherence and trust in interactive content analysis is examined. The findings indicate that providing content analysts with the ability to interact with Topic Modeling algorithms produces topics that are directly related to their research questions. However, a number of improvements to these algorithms were also identified which have the potential to influence future algorithm development to better meet the requirements of qualitative content analysts...|$|E
40|$|The aim of {{this thesis}} is to {{introduce}} the popular puppet theatre called Opera dei Pupi, which was developed mainly in Sicily {{in the second half}} of the 19 th century. The first part desrcibes literary influences that nourished this tradition, from which puppeteers <b>derived</b> <b>topics</b> for their repertoire. These stories were primarily about the French king Charlemagne and his knights and were called paladins. I am trying to map the form of an adaptation of this topic for Opera dei Pupi needs and the way of connection with oral tradition. Furthermore I am going to mention the secondary repertoire of the theatre for example farce. Next chapter focuses on a formal side of the Opera dei Pupi including puppeteer's personality, his work and activities, structure of the theatre itself, characteristics of puppets and handling them. Furthermore this section also describes a specific concept of everyday's performances "to be continued" and how were these performaces organized in practice. The analytical part focuses on a process of putting plot on a scene, which includes both improvisation and fixed rules approach. The principal theme of this part is the so-called copione, which are the puppeteer's own handwritten notes, on which the show was based. This section also includes numerous excerpts not only from copione but also [...] ...|$|E
40|$|We extend on McCarthy et al. ’s {{predominant}} sense {{method to}} create an unsupervised method of word sense disambiguation that uses automatically <b>derived</b> <b>topics</b> using Latent Dirichlet allocation. Using topicspecific synset similarity measures, we create predictions for each word in each document using only word frequency information. It is hoped that this procedure can improve upon the method for larger numbers of topics by providing more relevant training corpora for the individual topics. This method is evaluated on SemEval- 2007 Task 1 and Task 17. 1 Generative Model of WSD Word Sense Disambiguation (WSD) {{is the problem of}} labeling text with the appropriate semantic labels automatically. Although WSD is claimed to be an essential step in information retrieval and machine translation, it has not seen effective practical application because the dearth of labeled data has prevented the use of established supervised statistical methods that have been successfully applied to other natural language problems. Unsupervised methods have been developed for WSD, but despite modest success have not always been well understood statistically (Abney, 2004). Unsupervised methods are particularly appealing because they do not require expensive senseannotated data and can use the ever-increasing amount of raw text freely available. This paper expands on an effective unsupervised method for WSD and embeds it into a topic model, thus allowing an algorithm trained on a single, monolithic corpora to instead hand-pick relevant documents in choosin...|$|E
40|$|Much {{research}} has been concerned with <b>deriving</b> <b>topics</b> from Twitter and applying the outcomes {{in a variety of}} real life applications such as emergency management, business advertisements and corporate/ government communication. These activities have used mostly Twitter content to <b>derive</b> <b>topics.</b> More recently, tweet interactions have also been considered, leading to better topics. Given the dynamic aspect of Twitter, we hypothesize that temporal features could further improve topic derivation on a Twitter collection. In this paper, we first perform experiments to characterize the temporal features of the interactions in Twitter. We then propose a time-sensitive topic derivation method. The proposed method incorporates temporal features when it clusters the tweets and identifies the representative terms for each topic. Our experimental results show that the inclusion of temporal features into topic derivation results in a significant improvement for both topic clustering accuracy and topic coherence comparing to existing baseline methods. 15 page(s...|$|R
30|$|We {{defined key}} topic as the topic cluster {{that has a}} {{significant}} volume of posts and authors, is discussed actively at current and is composed of meaningful keywords. We cluster messages in {{a large number of}} classes and select meaningful topic clusters with keywords that reflect user needs. We selected bigrams from extracted keywords to <b>derive</b> <b>topics</b> that are meaningful and have significant volume.|$|R
40|$|The {{visualization}} of large text databases and document collections {{is an important}} step towards more exible and interactive types of information access and retrieval. This paper presents a probabilistic approach which combines a statistical, model{ based analysis of a given set of document with a topological visualization principle. Our method can be utilized to <b>derive</b> <b>topic</b> maps, which represent topical informa-tion by characteristic keyword distributions arranged in a two{dimensional spatial layout. Combined with multi-resolution techniques this provides a three-dimensional space for interactive information navigation in large text collections...|$|R
40|$|Context and setting: With {{the growing}} older {{population}} in both Belgium and The Netherlands, the government urges {{people to stay}} at home {{for an extended period of}} time. To be able to accomplish this, adaptations in their life need to be made. There is already a lot of information on which adaptations can be implemented in their house, but not about what is needed in the neighborhood. The vital living environment for elderly needs to be mapped so that their surroundings can nurture their extended independent living. Whether this includes technology or not is also an important aspect that needs to be looked in to. Objectives and method: The main objective of this research is to map the vital living environment of the elderly in Rilland, a small village in Zeeland, The Netherlands. This was established in cooperation with the Zeeland Living Room, a living laboratory for independent living. This living lab uses an interactive bottom-up approach for user-led innovation in which the residents themselves are given a voice in communicating their needs towards housing providers. The same method was used in this project, where the living lab was extended to the whole village. With the Expertise Management Methodology and Soft Systems Methodology in mind, around 36 indepth interviews were conducted between March 2015 and July 2015. Both the elderly, key figures and organizations in Rilland were questioned and information was gathered. Main outcomes and result: The VLE project resulted in a broader view on what was needed for an extended independent living. Rilland showed a great diversity, respect for one another, good relationships with neighbours, not a lot of deep contacts, low sense of place attachment, not a financially strong village and values like idealism and solidarity. However, the neighborhood could still be improved to achieve an extended independent period of time at home. The results from the interviews are arranged in (problematic) situations. These situations are selected, based on the problems that the interviewees brought in. A situation is seen as a setting in which stakeholders such as organisations and citizens act to achieve common goals or not. The primary objective in Rilland is the welfare of the people and concrete nameable targets are derived, such as accessibility of care or a pleasant and safe environment. In this context, a problematic situation is not the same as a problem because what might seem like a problem to one person can be perceived as not a problem by others. Whenever the people involved felt like there was room for improvement, there is talk of a problematic situation. In total, six situations can be described: contacts & information, care, living environment, safety, selfreliance and solidarity & acceptance. These situations came forward as a common topic in the interviews and are not unrelated. Conclusions: Overall, Rilland seems to be a social village where some improvements could still be made concerning the six <b>derived</b> <b>topics.</b> Remarkably, technology was not mentioned by the interviewees. Feedback was given to the local government so that they could make Rilland more vital for the elderly. Further research is needed to see if these results are similar in other dwellings such as cities. status: publishe...|$|E
40|$|From {{literature}} surveys {{to legal}} document collections, {{people need to}} organize and explore large amounts of documents. During these tasks, students and researchers will search for documents based on particular themes. In this paper, we use a popular topic modeling algorithm, Latent Dirichlet Alloca-tion, to <b>derive</b> <b>topic</b> distributions for articles. We allow users to specify personal topic distribution to contextualize the ex-ploration experience. We introduce three types of exploration: user model re-weighted keyword search, topic-based search, and topic-based exploration. We demonstrate these methods using a scientific citation data set and a Wikipedia article col-lection. We also describe the user interaction model. ...|$|R
40|$|This {{document}} {{is about the}} multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to <b>derive</b> <b>topic</b> embeddings, i. e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis. Comment: 5 page...|$|R
40|$|Latent <b>topics</b> <b>derived</b> by <b>topic</b> models such as Latent Dirichlet Allocation (LDA) are {{the result}} of hidden {{thematic}} structures which provide further insights into the data. The automatic labelling of such <b>topics</b> <b>derived</b> from social media poses however new challenges since topics may characterise novel events happening in the real world. Existing automatic topic la-belling approaches which depend on exter-nal knowledge sources become less appli-cable here since relevant articles/concepts of the extracted topics may not exist in ex-ternal sources. In this paper we propose {{to address the problem of}} automatic la-belling of latent topics learned from Twit-ter as a summarisation problem. We in-troduce a framework which apply sum-marisation algorithms to generate topic la-bels. These algorithms are independent of external sources and only rely on the identification of dominant terms in doc-uments related to the latent topic. We compare the efficiency of existing state of the art summarisation algorithms. Our results suggest that summarisation algo-rithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA. ...|$|R
40|$|Semantic {{visualization}} integrates topic {{modeling and}} visualization, such that every document {{is associated with}} a topic distribution as well as visualization coordinates on a low-dimensional Euclidean space. We address the problem of semantic visualization for short texts. Such documents are increasingly common, including tweets, search snippets, news headlines, or status updates. Due to their short lengths, it is difficult to model semantics as the word co-occurrences in such a corpus are very sparse. Our approach is to incorporate auxiliary information, such as word embeddings from a larger corpus, to supplement the lack of co-occurrences. This requires the development of a novel semantic visualization model that seamlessly integrates visualization coordinates, topic distributions, and word vectors. We propose a model called GaussianSV, which outperforms pipelined baselines that <b>derive</b> <b>topic</b> models and visualization coordinates as disjoint steps, as well as semantic visualization baselines that do not consider word embeddings...|$|R
25|$|Little to no musical {{research}} has been undertaken on Antigua and Barbuda. As a result, much knowledge on the <b>topic</b> <b>derives</b> from novels, essays and other secondary sources.|$|R
40|$|Child {{language}} narratives {{are used}} for language analysis, measurement of language development, and the detection of language impairment. In this paper, we explore the use of Latent Dirichlet Allocation (LDA) for detecting topics from narratives, and use the <b>topics</b> <b>derived</b> from LDA in two classification tasks: automatic prediction of coherence and language impairment. Our experiments show LDA is useful for detecting the topics that correspond to the narrative structure. We also observed improved performance for the automatic prediction of coherence and language impairment when we use features <b>derived</b> from the <b>topic</b> words provided by LDA. ...|$|R
40|$|Discussion forums {{serve as}} a {{platform}} for student discussions in massive open online courses (MOOCs). Analyzing content in these forums can uncover useful informa-tion for improving student retention and help in initiating instructor intervention. In this work, we explore the use of topic models, particularly seeded topic models toward this goal. We demonstrate that fea-tures <b>derived</b> from <b>topic</b> analysis help in predicting student survival. ...|$|R
50|$|Another {{important}} source that feeds into A {{is a group}} of scholia on mythographical and allegorical <b>topics,</b> <b>derived</b> from Porphyrys Homeric Questions. The current standard edition of the Iliads scholia, that of Erbse, omits these scholia.|$|R
40|$|Understanding {{the rapidly}} growing short text is very important. Short text is {{different}} from traditional documents in its shortness and sparsity, which hinders the application of conventional machine learning and text mining algorithms. Two major approaches have been exploited to enrich the representation of short text. One is to fetch contextual information of a short text to directly add more text; {{the other is to}} <b>derive</b> latent <b>topics</b> from existing large corpus, which are used as features to enrich the representation of short text. The latter approach is elegant and efficient in most cases. The major trend along this direction is to <b>derive</b> latent <b>topics</b> of certain granularity through well-known topic models such as latent Dirichlet allocation (LDA). However, topics of certain granularity are usually not sufficient to set up effective feature spaces. In this paper, we move forward along this direction by proposing an method to leverage topics at multiple granularity, which can model the short text more precisely. Taking short text classification as an example, we compared our proposed method with the state-of-the-art baseline over one open data set. Our method reduced the classification error by 20. 25 % and 16. 68 % respectively on two classifiers. ...|$|R
40|$|Twitter {{has become}} one of the most popular social media platforms, widely used for {{discussion}} and information dissemination on all kinds of topics. As a result, both business and academics have researched methods to identify the topics being discussed on Twitter. Those methods can be employed for a number of applications, including emergency management, advertisements, and corporate/government communication. However, <b>deriving</b> <b>topics</b> from this short text based and highly dynamic environment remains a huge challenge. Most current methods use the content of tweets as the only source for topic derivation. Recently, tweet interactions have been considered for improving the quality of topic derivation. In this paper, we propose a method that considers both content and interactions with a temporal aspect to further improve the quality of topic derivation. The impact of the temporal aspect in user/tweet interactions is analyzed based on several Twitter datasets. The proposed method incorporates time when it clusters tweets and identifies representative terms for each topic. Experimental results show that the inclusion of the temporal aspect in the interactions results in a significant improvement in the quality of topic derivation comparing to existing baseline methods. 27 page(s...|$|R
40|$|This {{paper is}} {{concerned}} with the use of conversational agents as an interaction paradigm for accessing open domain encyclopedic knowledge by means of Wikipedia. More precisely, we describe a dialog-based question answering system for German which utilizes Wikipedia-based topic models as a reference point for context detection and answer prediction. We investigate two different perspectives to the task of interfacing virtual agents with collaborative knowledge. First, we exploit the use of Wikipedia categories as a basis for identifying the broader topic of a spoken utterance. Second, we describe how to enhance the conversational behavior of the virtual agent by means of a Wikipedia-based question answering component which incorporates the question topic. At large, our approach identifies topic-related focus terms of a user’s question, which are subsequently mapped onto a category taxonomy. Thus, we utilize the taxonomy as a reference point to <b>derive</b> <b>topic</b> labels for a user’s question. The employed topic model is thereby based on explicitly given concepts as represented by the document and category structure of the Wikipedia knowledge base. Identified topic categories are subsequently combined with different linguistic filtering methods to improve answer candidate retrieval and reranking. Results show that the topic model approach contributes to an enhancement of the conversational behavior of virtual agents. ...|$|R
40|$|We {{investigate}} how a speaker's preference for specific topics {{can be used}} for speaker identification. In domains like broadcast news or parliamentary speeches, speakers have a field of expertise they are associated with. We explore how topic information for a segment of speech, extracted from an automatic speech recognition transcript, can be employed to identify the speaker. Two methods for modelling topic preferences are compared: implicitly, based on speaker-characteristic keywords, and explicitly, by using automatically <b>derived</b> <b>topic</b> models to assign topics to the speech segments. In the keyword-based approach, the segments' tf-idf vectors are classified with Support Vector Machine speaker models. For the topic-model-based approach, a domain-specific topic model is used to represent each segment as a mixture of topics; the speakers' score is derived from the Kullback-Leibler divergence between the topic mixtures of their training data and of the segment. The methods were tested on political speeches given in German parliament by 235 politicians. We found that topic cues do carry speaker information, as the topic-model-based system yielded an equal error rate (EER) of 16. 3 %. The topic-based approach combined well with a spectral baseline system, improving the EER from 8. 6 % for the spectral to 6. 2 % for the fused system...|$|R
40|$|The direct {{measurement}} of the nonlinear term of the gravitational field equations by using very stable clocks is discussed along with measuring the perhelion advance of a planet or satellite. These are considered measurements of the second-order gravitational red shift. The exact expression for the frequency shift of light in a gravitational field is <b>derived.</b> Other <b>topics</b> discussed include: The Doppler-cancelling technique; the second-order red shift in a spherically symmetric gravitational field; finite signal transit time; and the reality and interpretation of coordinates in the second-order red shift experiment...|$|R
40|$|This paper {{presents}} a research agenda on innovation through (international) food supply chains and networks {{in developing countries}}. It <b>derives</b> major <b>topics</b> from a multi-perspective view on international food chains (economic, technology, social/legal and environment) and from different theoretical streams dealing with chains and networks (Supply Chain Management, Industrial Organisation theory and Network Theory). Three agri-supply chain projects in developing countries (Thailand, South-Africa, Ghana) are analysed to identify focus areas in supply chain development projects and important gaps. These projects were collaborative actions between companies and research institutes to initiate international supply chain development. Key words: international supply chains, innovation, developing countries 1...|$|R
30|$|Since we are {{discussing}} the data itself, our future work will extend the set of people to all faces detected in the dataset, {{not limited to the}} tagged individuals. We also put effort in enhancing the recall and the precision of the detection; and the addition of semantic information <b>derived</b> from the <b>topic</b> detection will be a great improvement.|$|R
40|$|In {{order to}} assess the {{advantages}} and risks of emerging technologies an intensive dialogue between scientists and the individual users of these technologies is crucial - especially in a security and military environment. A particular challenge during this exchange is the encounter of specific scientific and military terminologies, which sometimes hamper the communication and hence {{make it difficult to}} reach decisions for the future. This contribution presents a methodological approach that brings scientific expertise and the military or security customers´ competence together. The aim of this approach is twofold, namely to <b>derive</b> hot <b>topics</b> in future technologies as well as new technological perspectives and recommendations in a military or security context...|$|R
40|$|Short text {{differs from}} {{traditional}} documents in its shortness and sparseness. Feature extension can ease {{the problem of}} high sparseness in the vector space model, but it inevitably introduces noise. To resolve this problem, this paper proposes a high-frequency feature expansion method based on a latent Dirichlet allocation (LDA) topic model. High-frequency features are extracted from each category as the feature space, using LDA to <b>derive</b> latent <b>topics</b> from the corpus, and topic words are extended to the short text. Extensive experiments are conducted on Chinese short messages and news titles. The proposed method for classifying Chinese short texts outperforms conventional classification methods. </p...|$|R
40|$|Letters to Anyone {{is written}} as a {{parallel}} text {{to the body}} of work I’ve created during my time in graduate school, culminating in the installation almost‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐release. This thesis deals with my personal narrative as a way to understand the ideas and concepts that my work is <b>derived</b> from. Specific <b>topics</b> include memory, the body, time, process, space, and change...|$|R
40|$|We {{have applied}} topic {{modelling}} to the full-text British HCI and CHI corpora {{in order to}} automatically <b>derive</b> one hundred <b>topics</b> and their trends. We use these to compare the distributions of topics and changing foci of two conferences {{over the last five}} years. These data suggest that that, while the two conferences have significant overlap, they make quite distinct contributions to HCI...|$|R
40|$|The {{analysis}} of some letters written by Augustine {{on the occasion}} of the fight he led, as Bishop of Ippona, against mangones, or slave traders¸ enables us to reconstruct his conception of the purpose of punishment and follow some <b>topics</b> <b>derived</b> from some principia juris (prevention and deterrent functions of the punishment, actual knowledge of the law, terms of un-punishability) remarkably interesting and extraordinarily original...|$|R
40|$|Abstract—Short text {{differs from}} {{traditional}} documents in its shortness and sparseness. Feature extension can ease {{the problem of}} high sparseness in the vector space model, but it inevitably introduces noise. To resolve this problem, this paper proposes a high-frequency feature expansion method based on a latent Dirichlet allocation (LDA) topic model. High-frequency features are extracted from each category as the feature space, using LDA to <b>derive</b> latent <b>topics</b> from the corpus, and topic words are extended to the short text. Extensive experiments are conducted on Chinese short messages and news titles. The proposed method for classifying Chinese short texts outperforms conventional classification methods. Index Terms—Chinese short-text classification, feature extension, topic model, high-frequency featur...|$|R
40|$|We have {{developed}} and implemented a case-based approach for introducing discrete event simulation to {{undergraduate and graduate}} manufacturing engineering students. Students learn only the simulation methods necessary to support the case studies. Case studies are <b>derived</b> from <b>topics</b> of interest to practicing manufacturing engineers. Cases are organized into four modules: basic systems organizations, systems operating strategies, material handling, and supply chain management. Course instruction is based on active learning. Tutorials and laboratories assist students in comprehending the simulation methods. Courses are taught in a computer-aided teaching studio, so that the mix of passive and active learning can be adjusted as appropriate to each class meeting. An industrybased project serves as the course capstone...|$|R
50|$|The only non-controversial {{counter-argument}} {{to using}} user-friendly URLs {{is that they}} would be leaked in the HTTP referer header field when a user clicks on an external link from a post, which is undesirable for private (sub)forums, since a URL <b>derived</b> from the <b>topic</b> title could convey sensitive information. This issue can be resolved by rewriting external links to point to a redirection page that performs referer hiding.|$|R
40|$|A {{literature}} search {{was conducted by}} using the Web of Science® databases component of the ISI Web of KnowledgeSM to identify recent articles that {{would be useful to}} help assess the potential environmental effects of renewable energy development in the ocean, with emphasis on seabirds and fish. Several relatively recent general review articles that included possible effects on seabirds and fish were examined to begin the search process. From these articles, several general topics of potential environmental effects on seabirds and fish were <b>derived.</b> These <b>topics</b> were used as the primary search factors. Additional sources were identified by cross-checking the Web of Science databases for articles that cited the review articles. It also became clear that the potential effects frequently...|$|R
40|$|In this paper, {{we propose}} the CBS topic model, a {{probabilistic}} graphical model, to derive the user communities in microblogging networks {{based on the}} sentiments they express on their generated content and behaviors they adopt. As a topic model, CBS can uncover hidden <b>topics</b> and <b>derive</b> user <b>topic</b> distribution. In addition, our model associates topic-specific sentiments and behaviors with each user community. Notably, CBS has a general framework that accommodates multiple types of behaviors simultaneously. Our experiments on two Twitter datasets show that the CBS model can effectively mine the representative behaviors and emotional topics for each community. We also demonstrate that CBS model perform {{as well as other}} state-of-the-art models in modeling topics, but outperforms the rest in mining user communitie...|$|R
500|$|When Feynman was 15, {{he taught}} himself trigonometry, {{advanced}} algebra, infinite series, analytic geometry, and both differential and integral calculus. Before entering college, he was experimenting with and <b>deriving</b> mathematical <b>topics</b> {{such as the}} half-derivative using his own notation. He created special symbols for logarithm, sine, cosine and tangent functions so they didn't look like three variables multiplied together, and for the derivative, to remove the temptation of canceling out the d's. A member of the Arista Honor Society, in his last {{year in high school}} he won the New York University Math Championship. His habit of direct characterization sometimes rattled more conventional thinkers; for example, one of his questions, when learning feline anatomy, was [...] "Do you have a map of the cat?" [...] (referring to an anatomical chart).|$|R
40|$|Abstract. The present work is {{motivated}} by the problem of ana-lyzing the activities of an organization and representing them in a taxonomy of the domain as its hierarchical ontology. We focus on representing the research activities of a Computer Science research organization {{in terms of the}} ACM-CCS taxonomy. We <b>derive</b> re-search <b>topic</b> clusters according to the similarity derived on the ba-sis of profiling of the topics covered by the researchers working in the organization and then place them onto the hierarchical tree. Each of the steps is performed by using our original algorithms: one-by-one spectral-additive fuzzy clustering SAF and a recursive algorithm PARL for mapping fuzzy clusters to higher ranks of the taxonomy. The latter minimizes the weighted sum of penalties for the chosen “head subjects”, and corresponding “gaps ” and “offshoots”. ...|$|R
40|$|This {{workshop}} provides practical {{information on}} how to design and implement a discrete mathematics course for an information systems program seeking ABET accreditation or al-ready accredited by ABET. A matrix correlates the local ABET-accreditable core curriculum with a standard set of discrete mathematics <b>topics</b> to <b>derive</b> relevant <b>topic</b> coverage. Materials, software resources, and teaching techniques are targeted toward needs and interests of IS students and thus foster motivation and confidence as well as understanding of how the con-cepts presented serve them in learning and will serve them in career settings. The technologi-cal and societal reasons for including discrete mathematics in the IS curriculum are covered. Experiences in the information systems (IS) and information systems management (ISM) pro-grams at Robert Morris University (RMU) guided the design of this workshop...|$|R
5000|$|When Feynman was 15, {{he taught}} himself trigonometry, {{advanced}} algebra, infinite series, analytic geometry, and both differential and integral calculus. Before entering college, he was experimenting with and <b>deriving</b> mathematical <b>topics</b> {{such as the}} half-derivative using his own notation. He created special symbols for logarithm, sine, cosine and tangent functions so they didn't look like three variables multiplied together, and for the derivative, to remove the temptation of canceling out the d's. A member of the Arista Honor Society, in his last {{year in high school}} he won the New York University Math Championship. His habit of direct characterization sometimes rattled more conventional thinkers; for example, one of his questions, when learning feline anatomy, was [...] "Do you have a map of the cat?" [...] (referring to an anatomical chart).|$|R
