108|2573|Public
500|$|... 2,300 ± 160 BCE a large Plinian {{eruption}} {{occurred at}} Cerro Blanco. Rhyolites with {{a volume of}} [...] generated ashfalls and ignimbrites, forming the largest known eruption in the Central Andes during the Holocene. Ash from this eruption covered an area of [...] These Purulla/El Médano ignimbrites (...) cover an area of [...] and are dated to have occurred less than 12,200 {{years ago on the}} basis of radiocarbon analysis of organic material contained between various ignimbrite layers. One layer in La Hoyada is dated between 8,830 and 5,480 BP. Other <b>data</b> <b>attribute</b> the Purulla ignimbrites to 22,000 years ago. The date of the major Holocene eruption is constrained to between 5,500 and 4,000 years ago. Based on stratigraphic relationships, the Cerro Blanco caldera formed during this eruption. The location of the vent did not change {{during the course of the}} eruption, indicating that the annular fractures around the caldera were possibly uninvolved in the process. The low population density in the Fiambalá valley south of the volcano during the 10,000–3,000 B.P interval is ascribed to this eruption.|$|E
5000|$|The Attributes File {{is a new}} B-tree in HFS Plus {{that does}} not have a {{corresponding}} structure in HFS. The Attributes File can store three different types of 4 KB records: Inline <b>Data</b> <b>Attribute</b> records, Fork <b>Data</b> <b>Attribute</b> records and Extension Attribute records. Inline <b>Data</b> <b>Attribute</b> records store small attributes that can fit within the record itself. Fork <b>Data</b> <b>Attribute</b> records contain references to a maximum of eight extents that can hold larger attributes. Extension Attributes are used to extend a Fork <b>Data</b> <b>Attribute</b> record when its eight extent records are already used.|$|E
50|$|GraphStream allows {{to store}} {{any kind of}} <b>data</b> <b>attribute</b> on the graph elements: numbers, strings, or any object.|$|E
5000|$|Table, graph, {{and tree}} data {{structures}} supporting arbitrary <b>data</b> <b>attributes,</b> <b>data</b> indexing, and selection queries, all with an efficient memory footprint.|$|R
5000|$|... 1. A {{memory for}} storing data for access by an {{application}} program being executed on a data processing system, comprising:a data structure stored in said memory, said data structure including information resident in a database used by said application program and including:a plurality of <b>attribute</b> <b>data</b> objects stored in said memory, each of said <b>attribute</b> <b>data</b> objects containing different information from said database;a single holder <b>attribute</b> <b>data</b> object for each of said <b>attribute</b> <b>data</b> objects, each of said holder <b>attribute</b> <b>data</b> objects being one of said plurality of <b>attribute</b> <b>data</b> objects, a being-held relationship existing between each <b>attribute</b> <b>data</b> object and its holder <b>attribute</b> <b>data</b> object, and each of said <b>attribute</b> <b>data</b> objects having a being-held relationship with only a single other <b>attribute</b> <b>data</b> object, thereby establishing a hierarchy of said plurality of <b>attribute</b> <b>data</b> objects;a referent <b>attribute</b> <b>data</b> object {{for at least one}} of said <b>attribute</b> <b>data</b> objects, said referent <b>attribute</b> <b>data</b> object being nonhierarchically related to a holder <b>attribute</b> <b>data</b> object for the same at least one of said <b>attribute</b> <b>data</b> objects and also being one of said plurality of <b>attribute</b> <b>data</b> objects, <b>attribute</b> <b>data</b> objects for which there exist only holder <b>attribute</b> <b>data</b> objects being called element <b>data</b> objects, and <b>attribute</b> <b>data</b> objects for which there also exist referent <b>attribute</b> <b>data</b> objects being called relation data objects; andan apex data object stored in said memory and having no being-held relationship with any of said <b>attribute</b> <b>data</b> objects, however, at least one of said <b>attribute</b> <b>data</b> objects having a being-held relationship with said apex data object.The examiner rejected all of Lowry’s claims as so-called printed matter. That is, they were directed to information stored on a medium ("substrate") without any unobvious cooperation between the medium and the information to provide a useful function. The PTO Board of Appeals sustained the examiner’s rejection of all claims. The claimed data structures were stored into a computer memory, but they did not sufficiently interact with the memory to provide a functional relationship with it. Accordingly, the ADOs were not [...] "patentably distinguished" [...] from the prior art, [...] making them obvious or anticipated, and thus unpatentable.|$|R
5000|$|A brief {{explanation}} of the <b>Data</b> <b>Attributes</b> used in this example: ...|$|R
50|$|A smooth {{parallel}} coordinate plot {{is achieved}} with splines. In the smooth plot, every observation is mapped into a parametric line (or curve), which is smooth, continuous on the axes, and orthogonal to each parallel axis. This design emphasizes the quantization level for each <b>data</b> <b>attribute.</b> If one uses the Fourier interpolation of degree equals {{to the data}} dimensionality, then an Andrews plot is achieved.|$|E
50|$|One of {{the core}} {{challenges}} associated with EDM {{is the ability to}} compare data that is obtained from multiple internal and external sources. In many circumstances, these sources use inconsistent terms and definitions to describe the data content itself - making it hard to compare data, hard to automate business processes, hard to feed complex applications and hard to exchange data. This frequently results in a difficult process of data mapping and cross-referencing. Normalization of all the terms and definitions at the <b>data</b> <b>attribute</b> level {{is referred to as the}} metadata component of EDM and is an essential prerequisite for effective data management.|$|E
5000|$|Within {{the body}} of the SGML document, these {{referenced}} external entities (whose name is specified between [...] "&" [...] and [...] ";") are not replaced like usual named entities (defined with a CDATA value), but are left as distinct unparsed tokens that may be used either as the value of an element attribute (like above) or within the element contents, provided that either the DTD allows such external entities in the declared content type of elements or in the declared type of attributes (here the ENTITY type for the <b>data</b> <b>attribute),</b> or the SGML parser is not validating the content.|$|E
5000|$|Global {{attributes}} (that can {{be applied}} for every element): , , , [...] (custom <b>data</b> <b>attributes)</b> ...|$|R
30|$|The fusion {{planning}} {{agent is}} {{to trigger a}} sequence of algorithms in the algorithm-pool and discover their required <b>data</b> <b>attributes</b> are in datasets in the data-pool. The <b>data</b> <b>attributes</b> discovery is a backward chaining recursive procedure as given below. The resulting fusion plan implicitly indicates a suite of fusion algorithms (all or a part of 7 -type fusion algorithms) triggered.|$|R
40|$|We {{combine the}} {{approaches}} of typographic variation in labels with cartograms to create multivariate labelled cartograms. These multivariate labelled cartograms can scale {{to a large}} number of entities and {{a large number of}} <b>data</b> <b>attributes.</b> We also introduce techniques of positional and proportional encoding which apply attributes to only portions of labels thereby aiding the encoding of many <b>data</b> <b>attributes</b> or quantitative <b>data...</b>|$|R
5000|$|... 2,300 ± 160 BCE a large Plinian {{eruption}} {{occurred at}} Cerro Blanco. Rhyolites with {{a volume of}} [...] generated ashfalls and ignimbrites, forming the largest known eruption in the Central Andes during the Holocene. Ash from this eruption covered an area of 440000 km2. These Purulla/El Médano ignimbrites (...) cover an area of 51.61 km2 and are dated to have occurred less than 12,200 {{years ago on the}} basis of radiocarbon analysis of organic material contained between various ignimbrite layers. One layer in La Hoyada is dated between 8,830 and 5,480 BP. Other <b>data</b> <b>attribute</b> the Purulla ignimbrites to 22,000 years ago. The date of the major Holocene eruption is constrained to between 5,500 and 4,000 years ago. Based on stratigraphic relationships, the Cerro Blanco caldera formed during this eruption. The location of the vent did not change {{during the course of the}} eruption, indicating that the annular fractures around the caldera were possibly uninvolved in the process. The low population density in the Fiambalá valley south of the volcano during the 10,000-3,000 B.P interval is ascribed to this eruption.|$|E
30|$|A {{control command}} {{is applied to}} a {{controllable}} data object. As soon as a command has been received, the device shall activate the <b>data</b> <b>attribute</b> opRcvd. The device shall then process the command. If the command is accepted, the <b>data</b> <b>attribute</b> opOk shall be activated with the same timing (e.g. pulse length) of the wired output. The <b>data</b> <b>attribute</b> tOpOk shall be the time stamp of the wired output and opOk [7].|$|E
40|$|At present, in {{the fault}} {{diagnosis}} database of submarine optical fiber network, the attribute selection of large data is completed by detecting {{the attributes of}} the data, the accuracy of large <b>data</b> <b>attribute</b> selection cannot be guaranteed. In this paper, a large <b>data</b> <b>attribute</b> selection method based on support vector machines (SVM) for fault diagnosis database of submarine optical fiber network is proposed. Mining large data in the database of optical fiber network fault diagnosis, and calculate its attribute weight, attribute classification is completed according to attribute weight, so as to complete attribute selection of large data. Experimental results prove that,the proposed method can improve the accuracy of large <b>data</b> <b>attribute</b> selection in fault diagnosis database of submarine optical fiber network, and has high use value...|$|E
50|$|Features and <b>data</b> <b>attributes</b> are tagged {{utilizing}} the international Feature and Attribute Coding Catalogue (FACC).|$|R
5000|$|Copy {{directory}} {{contents of}} [...] to [...] (including file <b>data,</b> <b>attributes</b> and timestamps), recursively with empty directories (...) : ...|$|R
40|$|Understanding {{the role}} of <b>attribute</b> <b>data</b> in a GIS. <b>Attribute</b> <b>data</b> {{describe}} features. <b>Attribute</b> <b>data</b> can help us to make interesting and informative maps, and perform spatial analysis in a GIS application. In this topic we describe how <b>attribute</b> <b>data</b> are associated with vector features {{and can be used}} to symbolise data. Basic 1 hou...|$|R
40|$|Visualizations of {{tabular data}} are widely used; {{understanding}} their effectiveness in different task and data contexts {{is fundamental to}} scaling their impact. However, {{little is known about}} how basic tabular data visualizations perform across varying data analysis tasks and <b>data</b> <b>attribute</b> types. In this paper, we report results from a crowdsourced experiment {{to evaluate the effectiveness of}} five visualization types [...] - Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart [...] - across ten common data analysis tasks and three <b>data</b> <b>attribute</b> types using two real world datasets. We found the effectiveness of these visualization types significantly varies across task and <b>data</b> <b>attribute</b> types, suggesting that visualization design would benefit from considering context dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different task and data contexts...|$|E
3000|$|... {{represent}} the maximum correlation distance between nodes {{and the space}} distance between two sensor nodes, f represents {{the effect of the}} fusion algorithm on the data correlations, and p is the <b>data</b> <b>attribute.</b>|$|E
3000|$|The element RBRF 1 in the {{multifunctional}} {{transformer protection}} relay (IED 4) is associated to all feeders. When the distribution feeder protection relay (IED 2) operates, {{it sends a}} GOOSE message indicating its operation requiring the tripping of the feeder breaker to clear the fault. This includes the <b>data</b> <b>attribute</b> [...]...|$|E
30|$|Fifth, we {{excluded}} non-useful <b>data</b> <b>attributes.</b> These include, for example, {{the case}} where the attribute is derived from other attributes considered in the models.|$|R
5000|$|Information Management View [...] : The {{information}} {{architecture of the}} enterprise will contain three levels of detail, subject areas, data groups, and <b>data</b> <b>attributes.</b>|$|R
40|$|The {{right choice}} of the set of <b>data</b> <b>attributes</b> is very {{important}} in remote sensing data classification based in the decison tree learning method. Vegetation indexes are an alternative way to increase the set of <b>data</b> <b>attributes</b> because they incorporate important spectral information. The objective of this work were investigate the effect of inclusion of vegetation indexes NDVI, EVI, PVI, SAVI and RVI in the remote sensing data classification process based in decision tree learning methods. Pages: 997 - 100...|$|R
30|$|The {{analysis}} of Data source 2 {{reveals that the}} security scan tools (represented by the <b>data</b> <b>attribute</b> Scan_source) are not a leading factor of issue fix time. It {{is possible that the}} developers rely on their expertise in analyzing security issues and not on the tool features as they get experts in addressing security issues. Further analysis may explain the finding better.|$|E
40|$|Wireless sensor {{networks}} become ubiquitous {{to collect}} people's information in many people-centric applications, such as, health care, smart space and public safety. Because any misusage of these personal data {{might result in}} the leakage of privacy, {{it is expected that}} the data requesters can only access to the data what they are entitled to read. Based on a revised hash chain technique, we proposed a novel time-based privacy protection (TPP) scheme for multi-attribute data in WSNs. In the scheme, all the personal data are divided into 2 -D subspaces representing <b>data</b> <b>attribute</b> and generation time. Data in each subspace is encrypted with a sub-key before its transmission to the sink. Anyone who wants to read <b>data</b> <b>attribute</b> at a particular time must get the corresponding sub-key from the sender node. TPP can generate a sub-key for data in each subspace in an efficient manner in terms of less sub-key generation time and low memory space usage. The simulation results show that the schemes {{can be applied to the}} resource limited WSNs efficiently. Department of ComputingRefereed conference pape...|$|E
40|$|Abstract—Data {{cleaning}} is {{a process}} of maintaining data quality in information systems. Current data cleaning solutions require reference data to identify incorrect or duplicate entries. This article proposes usage of data mining in the area of data cleaning as effective in discovering reference data and validation rules from the data itself. Two algorithms designed by the author for <b>data</b> <b>attribute</b> correction have been presented. Both algorithms utilize data mining methods. Experimental results show that both algorithms can effectively clean text attributes without external reference data. I...|$|E
40|$|In {{this paper}} {{we present a}} novel method to {{visualize}} weather data with multi-layer controllable texture synthesis. Texture possesses multiple principal perceptual channels, which makes it good at encoding multiple <b>data</b> <b>attributes</b> contained in weather data. The natural textures existed {{in the real world}} especially provide plenty of choices to encode the data with visually pleasing images. A controllable texture synthesis method is developed to generate a large amount of textures which change the appearances of their individual perceptual dimensions according to the underlying distribution of <b>data</b> <b>attributes.</b> In order to encode more <b>data</b> <b>attributes</b> we further propose multi-layer texture synthesis. The background and foreground textures are separately synthesized and then combined together for display. In the end, we apply our method to some real-world weather data and demonstrate its effectiveness with a user study. 1...|$|R
40|$|Last year at EDDI 15 we {{introduced}} our work on geocoded survey data {{that have been}} enriched with spatial <b>data</b> <b>attributes</b> at the GESIS Data Archive. This year, we suggest solutions and unveil our metadata scheme reflecting the challenges of documenting geocoded survey data at the archive. By splitting survey <b>data</b> <b>attributes</b> and spatial <b>data</b> <b>attributes</b> logically {{we were able to}} implement a documentation structure that also supports the dissemination of these data to the end-users. Moreover, we showcase present and future directions of our work regarding metadata of geocoded survey data with spatial <b>data</b> <b>attributes.</b> Since our present approach demands a lot of manual work, we intend to propose automatic solutions to the metadata processing. This includes metadata that are directly transferred from spatial datasets as, e. g., ISO 19115 metadata fields to the corresponding DDI metadata fields. Furthermore, we aim to automatically collect metadata that are produced during the processes of geocoding, data linking, and disclosure analyses. These metadata will be included in a future metadata scheme that uses the wide and rich opportunities of DDI-Lifecycle. We complete our presentation with final thoughts on the future infrastructure at the archive: a spatial data infrastructure for social science survey data...|$|R
40|$|Autonomous agents may {{encapsulate}} their principals' personal <b>data</b> <b>attributes.</b> These attributes may {{be disclosed}} to other agents during agent interactions, producing {{a loss of}} privacy. Thus, agents need self-disclosure decision-making mechanisms to autonomously decide whether disclosing personal <b>data</b> <b>attributes</b> to other agents is acceptable or not. Current self-disclosure decision-making mechanisms consider the direct benefit and the privacy loss of disclosing an attribute. However, there are many situations in which the direct benefit of disclosing an attribute is a priori unknown. This {{is the case in}} human relationships, where the disclosure of personal <b>data</b> <b>attributes</b> plays a crucial role in their development. In this paper, we present self-disclosure decision-making mechanisms based on psychological findings regarding how humans disclose personal information in the building of their relationships. We experimentally demonstrate that, in most situations, agents following these decision-making mechanisms lose less privacy than agents that do not use them. (C) 2012 Elsevier Inc. All rights reserved...|$|R
40|$|Over time, {{advances}} in database technology and utilization {{have resulted in}} a rapid increase in the number and types of data sources. Simultaneously, numerous methods of unifying these various data sources have emerged. Research has shown that a more comprehensive set of <b>data</b> <b>attribute</b> matches between multiple schemas can be detected by combining a number of the unification methodologies as opposed to using a single method. In this research project, a unification framework, dbUNiFier, has been proposed as an approach to allow for easy integration of both existing and future unification methods and data sources...|$|E
40|$|Geographic Information System (GIS) is {{a system}} {{which is used to}} {{integrate}} both spatial (geographical picture) and nonspatial <b>data</b> (<b>attribute</b> data) together which helps in interlinking a geographical citation with its information. This feature of GIS makes its way forward into many applied services. This paper proposes an overall view of GIS and one of its present practical implementation of structuring ‘APDRP’ (Accelerated Power Development and Reforms Programme) an APEPDCL (Andhra Pradesh Eastern Power Distribution Company Limited) project. Restructuring implies Digitization of the entire power system distribution sector through IT based applications...|$|E
40|$|In {{this paper}} we present {{experimental}} data supporting {{an alternative approach}} to developing decision support spreadsheets using a Programming by Demonstration (PbD) paradigm. This technique is coined “Example Driven Modelling” and uses example <b>data</b> (<b>attribute</b> classifications) in combination with inductive machine learning to create decision support models {{as an alternative to}} spreadsheet programming. In this experiment we examine whether participants can define attribute classifications (“example-giving”) satisfactorily and describe benefits and limitations this method offers through statistical analysis of the experimental results. We then consider the wider implications of this research in traditional programming...|$|E
50|$|The {{universal}} relation assumption in relational databases states, {{that one}} can place all <b>data</b> <b>attributes</b> into a (possibly very wide) table, which may then be decomposed into smaller tables as needed.|$|R
30|$|The dataset was {{collected}} by using school reports and questionnaires. The collected data approaches students achievement in secondary education of two Portuguese schools. The <b>data</b> <b>attributes</b> include student grades, demographic, social and school related features.|$|R
50|$|Moreover, in addition, GraphStream {{provides}} a way to handle the graph evolution in time. This means handling the way nodes and edges are added and removed, and the way <b>data</b> <b>attributes</b> may appear, disappear and evolve.|$|R
