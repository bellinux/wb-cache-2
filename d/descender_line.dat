4|1|Public
5000|$|... #Caption: Typographic {{parts of}} a glyph: 1) x-height; 2) {{ascender}} line; 3) apex; 4) baseline; 5) ascender; 6) crossbar; 7) stem; 8) serif; 9) leg; 10) bowl; 11) counter; 12) collar; 13) loop; 14) ear; 15) tie; 16) horizontal bar; 17) arm; 18) vertical bar; 19) cap height; 20) <b>descender</b> <b>line.</b>|$|E
40|$|Many {{document}} analysis and OCR systems depend on precise identification of page rotation, {{as well as}} the reliable identification of text lines. This paper presents a new algorithm to address both problems. It uses a branch-and-bound approach to globally optimal line finding and simultaneously models the baseline and the <b>descender</b> <b>line</b> under a Gaussian error/robust least square model. Results of applying the algorithm to documents in the University of Washington Database 2 are presented. Keywords: {{document analysis}}, layout analysis, skew detection, page rotation, text line finding, a#ne transformations 1...|$|E
30|$|Another {{complexity}} {{is introduced}} by multiple baselines in the Nasta’liq script (Naz et al. 2014 b). The baseline {{is a virtual}} line on which characters are combined to form the ligatures and it facilitates both readers and writers. Unlike the Naskh script, character may appear at different <b>descender</b> <b>line</b> depending upon the associated characters. In Nasta’liq writing style, the varying locations of ascenders and descenders leads to errors in the accurate detection of the baseline because of their oblique orientation and long tail. Thus, without prior knowledge of the word and text-line structure it is quite difficult to estimate the baseline.|$|E
40|$|For {{being able}} to {{automatically}} acquire the information recorded in church registers and other historical scriptures, the writing on these documents has to be recognized. This paper describes algorithms for transforming the paper documents into a representation of text apt {{to be used as}} input for an automatic text recognizer. The automatic recognition of old handwritten scriptures is difficult for two main reasons. Lines of text in general are not straight and ascenders and <b>descenders</b> of adjacent <b>lines</b> interfere. The algorithms described in this paper provide ways to reconstruct the path of the lines of text using an approach of gradually constructing line segments until an unique line of text is formed. In addition, the single lines are segmented and an output in form of a raster image is provided. The method was applied to church registers. They were written between the 17 th and 19 th century. Line segmentation was found to be successful in 97 % of all samples...|$|R
30|$|The {{second step}} in {{document}} forensics is document forgery detection. A forgery could involve changing, adding, or deleting some {{information on the}} document or replacing an entire page with a counterfeited page [9]. For pasting and reprinting forgery operations, character location distortion is often introduced. Beusekon et al. [10] presented a technique for extracting text lines and alignment lines for document inspection. For English language characters, most characters align according to the ascender line, <b>descender</b> <b>line,</b> and base line. Tampered characters deviate from these three lines because of location distortion. In [11], the matching quality between all pairs of documents was used to expose tampered documents. When a page is replaced or reprinted, location distortion will occur when comparing the forged page or the tampered region with a genuine document. By computing the matching quality of two page images, the forged page or tampered region will be detected. Farid and Kee [12] established a printer model for characters to detect documents forged by different printers. They used principal component analysis (PCA) and singular value decomposition (SVD) to model the degradation of a page caused by printing, and the resulting printer profile was then used to distinguish between characters generated from different printers.|$|E

