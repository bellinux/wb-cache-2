23|52|Public
5000|$|The Biosafety Clearing-House is {{designed}} to be interoperable with other databases, so governments may register their information with the central Biosafety Clearing-House database, or with another (<b>interoperable)</b> <b>database</b> of their choice. The location of the information makes no difference to the user, who is able to retrieve all information through the Central Portal of the Biosafety Clearing-House.|$|E
40|$|Many <b>interoperable</b> <b>database</b> systems {{offer the}} {{possibility}} of defining integrated schemata on top of heterogeneous databases. A very important challenge for these <b>interoperable</b> <b>database</b> systems is to maintain {{the autonomy of the}} component databases while preserving the correct semantics of the integrated schemata. This paper presents a mechanism that responds automatically to design changes made in component databases which are relevant to one or more integrated schemata. Further, this mechanism provides each component that decides to participate in the interoperable system with the opportunity to choose between assuming the default monitoring provided by the system or customizing it by defining the system responses. 1 Introduction It is widely recognized that many organizations possess their data stored in distributed, heterogeneous, autonomous data repositories. In order to support coordinated access to this data, interoperable systems have been defined. In the literature we can find [...] ...|$|E
40|$|The Australian National Corpus (AusNC) {{provides}} a technical infrastructure for collecting and publishing language resources representing Australian language use. As {{part of the}} project we have ingested a wide range of resource types into the system, bringing together the different meta-data and annotations into a single <b>interoperable</b> <b>database.</b> This paper describes the initial collections in AusNC and the procedures used to parse a variety of data types into a single unified annotation store. ...|$|E
40|$|We {{propose a}} {{software}} architecture, which addresses the interoperability problem in multiple database systems. The architecture avoids integration, centralisation {{of data and}} structures, and use of common database languages, which have already been proposed to support interoperability. We describe our architectural model in terms of interconnected software components that hide their implementation behind their required and provided services A specific architectural style for <b>interoperable</b> <b>databases</b> has been generated, which is deployable within a distributed component platform available from industry...|$|R
2500|$|Neuroinformatics is a {{research}} field concerned with the organization of neuroscience data by the application of computational models and analytical tools. These areas of research are important for the integration and analysis of increasingly large-volume, high-dimensional, and fine-grain experimental data. [...] Neuroinformaticians provide computational tools, mathematical models, and create <b>interoperable</b> <b>databases</b> for clinicians and research scientists. Neuroscience is a heterogeneous field, consisting of many and various sub-disciplines (e.g., cognitive psychology, behavioral neuroscience, and behavioral genetics). In order for {{our understanding of the}} brain to continue to deepen, it is necessary that these sub-disciplines are able to share data and findings in a meaningful way; Neuroinformaticians facilitate this.|$|R
40|$|Abstract. This paper {{presents}} {{techniques for}} reverse engineering of relational database applications. The target {{of such an}} effort {{is the definition of}} a fully equipped object-oriented view of the relational database, including methods and constraints. Such views {{can be seen as a}} full specification of the database semantics, aiding for example in the identification of semantic heterogeneity among <b>interoperable</b> <b>databases.</b> The general problem of translating from an imperative programming language environment to a declarative context such as provided by our target OODML is very hard. However, we show that the specific features of database application software allow for the development of a framework geared towards the particular problems encountered in this context. ...|$|R
40|$|Abstract: Purpose – Aims {{to provide}} an {{overview}} of methods for digitizing maps, the creation and preservation of a digital collection, <b>interoperable</b> <b>database</b> construction, and an examination of the real costs associated with maintaining a digital collection. Design/methodology/approach – This paper is an examination of the tools, metadata choices, work with Open Archives Initiative (OAI) research projects, and sustainability costs, associated with a digital project of 946 dynamic historic cartographic images. Findings – The Early Washington Maps project demonstrates how multiple tools may be integrated and interoperability between databases achieved. Realistic costs associated with digital collections and practical reference techniques are described. OCLC Systems & Services Services International Digital Library Perspectives, v. 22, no. 1 (2006) ...|$|E
40|$|Research in {{database}} interoperability has primarily {{focused on}} circumventing schematic and semantic incompatibility arising from {{autonomy of the}} underlying databases. We argue that, while existing integration strategies might provide satisfactory support for small or static systems, their inadequacies rapidly become evident in large-scale <b>interoperable</b> <b>database</b> systems operating in a dynamic environment. This paper highlights the problem of receiver heterogeneity, scalability, and evolution which have received little attention in the literature, {{provides an overview of}} the Context Inter-change approach to interoperability, illustrates why this is able to better circumvent the problems identified, and forges the connections to other works by suggesting how the context interchange framework differs from other integration approaches in the literature. ...|$|E
40|$|A {{large-scale}} <b>interoperable</b> <b>database</b> system {{operating in}} a dynamic environment should provide a uniform access user interface to its components, scalability to larger networks, evolution of database schema and applications, composability of client and server components, and preserve component autonomy. To address the research issues presented by such systems, we introduce the Distributed Interoperable Object Model (DIOM). DIOM promotes an adaptive approach to interoperation and mediation [34, 35], aimed at improving the robustness and scalability of the interoperation services used for integrating and accessing heterogeneous information resources. DIOM's main features include the explicit representation of semantics in data sources through the DIOM base interfaces, the use of interface composition mechanisms to support the incremental design and construction of compound interoperation interfaces, the decoupling of semantic heterogeneity from the representational mismatch, the deferment [...] ...|$|E
40|$|The President of the European Commission, José Manuel Barroso, has {{launched}} a brainstorming exercise about the future policy priorities {{for the development of}} the Area of Freedom, Security and Justice (AFSJ). Migration and borders will constitute two of the most relevant policies on which the next EU budget covering the AFSJ beyond 2013 will be focused. This Policy Brief seeks to contribute to this brain-storming exercise by putting forward a package of policy recommendations covering the following three themes: I. How to structure Justice and Home Affairs in the EU: to get the best results. II. Mastering the integration debate and comprehensively framing the citizen/foreigner/immigrant relationship. III. The utility of large-scale information systems with <b>interoperable</b> <b>databases</b> including biometric technology for the protection of people living in the EU...|$|R
40|$|This paper {{introduces}} object-oriented {{access controls}} (OOAC) {{as a result}} of consequently applying the object-oriented paradigm for providing access controls in object and <b>interoperable</b> <b>databases.</b> OOAC includes: (1) subjects, like users, roles etc., are regarded as first-class objects, (2) objects are accessed by sending messages, and (3) access controls deal with controlling the flow of messages among objects. OOAC are not intended to replace legacy access control mechanisms which mainly have been designed and applied in non-object environments. Instead, they provide the basis for applying these concepts in true object-oriented environments. An object authorization language (OAL) is proposed for specifying authorizations in a declarative manner. We illustrate the feasibility of the proposed concepts in applying them to IRO-DB II, an extension of the database federation IRO-DB, that provides interoperable access between relational and objectoriented database systems on the world-wide- [...] ...|$|R
30|$|Unfortunately, the {{short-term}} {{nature of the}} funding prohibited this initiative from keeping pace {{with the development of}} informatics tools and the initiative was thus progressively abandoned. Nevertheless, MINE had a very positive impact on European CCs; indeed, a number of them retain all, or part, of the MINE dataset in their catalogues to this day. MINE was a landmark in CC collaboration on database projects, but CCs were unable to capitalize on this initiative to truly harmonize their data handling and use. CCs have generally made their individual catalogues available on line making cross collection searches difficult. Only now are they re-engaging with each other in Europe and moving towards the creation of <b>interoperable</b> <b>databases.</b> This endeavour is greatly facilitated by the creation of the Microbial Resource Research Infrastructure (MIRRI, [URL] a research infrastructure that is being designed {{in the context of the}} European Strategy Forum on Research Infrastructures (ESFRI).|$|R
40|$|MediaWare is a {{distributed}} multimedia environment {{based on}} state-of-the-art distributed object computing standards as speci ed by CORBA. MediaWare enhances interoperability among media objects through the interoperable virtual connection protocol. MediaWare provides an infrastructure for building interactive multimedia applications that support synchronized, time-based media in a heterogeneous, distributed environment. This paper discusses MediaWare's architecture and its object life cycle. MediaWare's architecture includes an application layer; a quality-of-connection layer, {{which is an}} aggregation of the quality-of-service {{in addition to the}} user's interaction and synchronization requirements; an interoperable virtual connection protocol layer; an objects database including the <b>interoperable</b> <b>database</b> layer; an object request broker layer; an interface de nition language (IDL) mapping layer; and nally, an object implementation layer. The paper includes examples to show MediaWare's power to generate several multimedia teleorchestrations and scenarios...|$|E
40|$|A {{large-scale}} <b>interoperable</b> <b>database</b> system {{operating in}} a dynamic environment should provide Uniform access to heterogeneous information sources, Scalability to {{the growing number of}} information sources, Evolution and Composability of software and information sources, and Autonomy of participants, both information consumers and information producers. We refer to these set of properties as the USECA properties [30]. To address the research issues presented by such systems in a systematic manner, we introduce the Distributed Interoperable Object Model (DIOM). DIOM promotes an adaptive approach to interoperation via intelligent mediation [46, 47], aimed at enhancing the robustness and scalability of the services provided for integrating and accessing heterogeneous information sources. DIOM's main features include (1) the recursive construction and organization of information access through a network of application-specific mediators, (2) the explicit use of interface composition meta ope [...] ...|$|E
40|$|A {{large-scale}} <b>interoperable</b> <b>database</b> system {{operating in}} a dynamic environment should provide uniform access user interface to its components, scalability to larger networks, evolution of database schema and applications, composability of client and server components, and preserve component autonomy. To address the research issues presented by such systems, we introduce the Distributed Interoperable Object Model (DIOM). DIOM's main features include the explicit representation of and access to semantics in data sources through the DIOM base interfaces, the use of interface abstraction mechanisms such as import, specialization, generalization, and aggregation. 1 Introduction Many papers on multidatabase systems or federated database systems [BHP: 92, AS: 90, SL: 90], focused on the resolution of schematic and semantic incompatibilities among autonomous and heterogeneous component databases in relatively static system configurations. While this assumption was reasonable for the past, recen [...] ...|$|E
40|$|A {{language}} {{in support of}} semantic integration of deductive databases is proposed. The language allows one to construct mediators by extending logic programming with a suite of operators for composing programs and message passing features. The abstract semantics and implementation techniques of the extensions are discussed, and an example of integration of databases supporting libraries and departments is used to illustrate {{the usefulness of the}} approach. 1 Introduction At present, in the database area, much attention is devoted to studying the possibility of integrating different databases or, in any case, databases which have been developed within other projects, and that may be resident at different sites [19]. This issue has been largely studied in the past decades giving rise to approaches such as federated <b>databases,</b> multidatabases, <b>interoperable</b> <b>databases</b> or mediators. Here, we focus on integration of deductive databases. The problem is twofold: [...] at the lower level, how to [...] ...|$|R
40|$|This paper {{presents}} {{techniques for}} reverse engineering of relational database applications. The target {{of such an}} effort {{is the definition of}} a fully equipped object-oriented view of the relational database, including methods and constraints. Such views {{can be seen as a}} full specification of the database semantics, aiding for example in the identification of semantic heterogeneity among <b>interoperable</b> <b>databases.</b> The general problem of translating from an imperative programming language environment to a declarative context such as provided by our target OODML is very hard. However, we show that the specific features of database application software allow for the development of a framework geared towards the particular problems encountered in this context. 1 Introduction With the success of object-orientation in complex application domains, an interest in reverse engineering of legacy relational databases into object-oriented specifications has been risen. Such a specification can be s [...] ...|$|R
40|$|This paper {{presents}} {{a framework for}} reverse engineering of relational database applications. The target of such an effort {{is the definition of}} a fully equipped object-oriented view of the relational database, including methods and constraints. Such views {{can be seen as a}} full specification of the database semantics, aiding in the identification of semantic heterogeneity among <b>interoperable</b> <b>databases.</b> Other possible use includes support for a gradual migration towards an OODBMS, reducing the maintenance effort, and re-use of application code by new applications accessing the database through the view. The general problem of translating from an imperative programming language environment to a declarative context such as provided by our target OODML is very hard. However, based on our experience in analysing existing applications operational in oil industry, we show that the specific features of database application software allow for the development of a framework geared towards the part [...] ...|$|R
40|$|Purpose – Aims {{to provide}} an {{overview}} of methods for digitizing maps, the creation and preservation of a digital collection, <b>interoperable</b> <b>database</b> construction, and an examination of the real costs associated with maintaining a digital collection. Design/methodology/approach – This paper is an examination of the tools, metadata choices, work with Open Archives Initiative (OAI) research projects, and sustainability costs, associated with a digital project of 946 dynamic historic cartographic images. Findings – The Early Washington Maps project demonstrates how multiple tools may be integrated and interoperability between databases achieved. Realistic costs associated with digital collections and practical reference techniques are described. Practical implications – This is an extremely common-sense discussion of digital library issues for library professionals planning or enhancing digital collections. Originality/value – The paper describes ideas for enhancing access to digital collections during and after a grant award...|$|E
40|$|Negotiation and {{contracting}} {{have been}} traditionally {{used in the}} database context in connection with passing schema information between interconnected database nodes. Several protocols have been proposed to address the transmission of schema information. However, all of the approaches suggested so far are fairly simplistic, restrictive in nature as they do not result in alleviating conflicts or formulating partial solutions in a collective manner. In contrast, Distributed AI (DAI) has taken the view that negotiation is a mechanism used by autonomous systems to resolve inconsistent views and reach agreement on how they can work together in order to cooperate effectively. Although this approach may sound appealing it becomes an intractable problem in an environment as complex as <b>interoperable</b> <b>database</b> systems. In this paper we reassess the concepts of negotiation and contracting in both distributed databases and DAI and propose a flexible framework tailored around the client/server model [...] ...|$|E
40|$|Advances in {{wide area}} {{networking}} {{is making the}} goal of achieving a worldwide <b>interoperable</b> <b>database</b> system closer than ever. Because of the large environment, new problems have surfaced {{that need to be}} addressed. We distinguish several reasons for a new design approach. Traditional approaches like global integration and federated approaches are better suited to small and medium scale heterogeneous databases. In a large network of interoperable autonomous heterogeneous databases, the architecture should be flexible enough to allow negotiation to take place to establish grouping of databases. In such a large environment, it is important that databases be made aware of other participating databases in an incremental fashion. Further, users should be educated about the space of information in an incremental and dynamic fashion. These goals are to be achieved under the major assumptions that participating databases keep their autonomy intact as much as possible and that the overhead in brid [...] ...|$|E
40|$|Relational {{database}} schemas must be semantically enriched {{to reflect}} {{knowledge about the}} data, as needed by many applications. One technique is {{the analysis of the}} database extension, extracting existing data dependencies. In this paper, new algorithms to extract functional and inclusion dependencies are presented; they are designed to minimize the number of disk accesses. 1. Introduction The relational data model has gained lots of popularity during the last years, one key factor for this success being the simplicity of its structure. However, the price for this simplicity is its being almost devoid of semantics. In the relational model, semantics are specified not in the structure, but thru constraints. Data dependencies are one kind of these constraints, where functional and inclusion dependencies [6, 9] are of particular interest. Nowadays relational databases are spread everywhere, and many applications like reverse engineering, integrated access for <b>interoperable</b> <b>databases,</b> know [...] ...|$|R
40|$|More {{effective}} {{environmental pollution}} control and management are needed {{due to the}} increasing environmental impacts {{from a range of}} human activities and the growing public demands for a better living environment. Urban air pollution is a serious environmental issue that poses adverse impacts on the health of people and the environment in most metropolitan areas. In this paper, we propose a geoinformatics augmented framework of environmental modelling and information sharing for supporting effective urban air pollution control and management. This framework is outlined in terms of its key components and processes including: 1) an integrated, adaptive network of sensors for environmental monitoring; 2) a set of distributed, <b>interoperable</b> <b>databases</b> for data management; 3) a set of intelligent, robust algorithms and models for environmental modelling; 4) a set of flexible, efficient user interfaces for data access and information sharing; and 5) a reliable, high capacity, high performance computing and communication infrastructure for integrating and supporting other framework components and processes...|$|R
40|$|We {{develop a}} {{framework}} of characteristics, essential and recommended, that a data model should have to be suitable as canonical model for federated databases. This framework {{is based on the}} two factors of the representation ability of a model: expressiveness and semantic relativism. Several data models are analyzed with respect to the characteristics of the framework, to evaluate their adequacy as canonical models. 1 Introduction When several databases (DBs) are to interoperate, they form a federation, and a data model must be chosen as the canonical data model (CDM) for the federation (we use the terminology of [SL 90]). Work on federated or <b>interoperable</b> <b>databases</b> has often used an Entity Relationship (ER) model, or some extension of it, as the CDM; others have adopted an Object Oriented (OO) model. Is any data model equally adequate as CDM? This paper discusses some characteristics of a data model that make it suitable as the CDM of a federation. Note that we are not trying to une [...] ...|$|R
40|$|A Common Data Model is {{a unifying}} {{structure}} used to allow heterogeneous environments to interoperate. An Object Oriented common model {{is presented in}} this paper, which provides this unifying structure for a Meta-Data Repository Visualisation Tool. The creation of this common model from the Meta-Data held in component databases is described. The role this common model has in interoperable environments is discussed, and the physical architecture created from {{the examination of the}} Meta-Data in the Repository common model is described. 1 Introduction Many <b>Interoperable</b> <b>Database</b> Management systems utilise a Common Data Model as a single structure which all of the components use, allowing application programs to access data transparently i. e. the Data Model in the component databases are transparent. The common model is not only a conflict resolution structure but also a way to describe the complete data resource held in many heterogeneous data sources. This paper focuses on the common mode [...] ...|$|E
40|$|METU <b>INteroperable</b> <b>Database</b> System (MIND) is a multidatabase {{system that}} {{achieves}} interoperability among heterogeneous, federated DBMSs. MIND architecture {{is based on}} OMG distributed object management model. It is implemented {{on top of a}} CORBA compliant ORB, namely, ObjectBroker. MIND provides users a single ODMG- 93 compliant common data model, and a single global query language based on SQL. This makes it possible to incorporate both relational and Object oriented databases into the system. Currently Oracle 7, Sybase and METU OODBMS (MOOD) has been integrated to MIND. The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a graphical interface. In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases {{from the rest of the}} system. The integration of export schemas is currently [...] ...|$|E
40|$|Research in {{database}} interoperability has primarily {{focused on}} circumventing schematic and semantic incompatibility arising from {{autonomy of the}} underlying databases. We ar-gue that, while existing integration strategies might provide satisfactory support for small or static systems, their inadequacies rapidly become evident in large-scale <b>interoperable</b> <b>database</b> systems operating in a dynamic environment. The frequent entry and exit of heterogeneous interoperating agents renders "frozen " interfaces (e. g., shared schemas) im-practical and places an ever increasing burden on the system to accord more flexibility to heterogeneous users. User heterogeneity mandates that disparate users ' conceptual models and preferences must be accommodated, {{and the emergence of}} large-scale networks sug-gests that the integration strategy must be scalable and capable of dealing with evolving semantics. As an alternative to the integration approaches presented in the literature, we propose a strategy based on the notion of context interchange. In the context interchange framework, assumptions underlying the interpretations attributed to data are explicitly represente...|$|E
40|$|The {{concept of}} {{producing}} a prototype of <b>interoperable</b> cartographic <b>database</b> is explored in this paper, including the possibilities of integration of different  geospatial data  into the  database management system  and their visualization on the Internet. The implementation includes vectorization {{of the concept of}} a single map page, creation of the cartographic database in an object-relation database, spatial analysis, definition and visualization of the database content {{in the form of a}} map on the Internet.  </p...|$|R
40|$|In February 2008 {{the first}} {{demonstration}} {{pages of the}} Encyclopedia of Life (EOL) website went online. In less than six hours, it received 11. 5 million hits, and has since become a valuable resource for educators, researchers, {{and the general public}} around the world. Looking at another metric, the EOL has attracted over $ 50 million in funding, and eventually expects to receive twice that amount. Has the time come for a similar project in archaeology, an 'Encyclopedia of Cultural Heritage'? The situation in archaeology resembles that in the life sciences several years ago. A number of valuable online databases exist, and attempts to improve interoperability between databases have been initiated (with limited success). Existing resources provide the raw material for an Encyclopedia of Cultural Heritage, which in turn could accelerate the development of <b>interoperable</b> <b>databases</b> and improve data management practices across the discipline. This paper provides an overview of the EOL and a discussion of existing archaeological databases. We consider the EOL as an example for archaeology, and explore the obstacles to and potential benefits of an Encyclopedia of Cultural Heritage. 8 page(s...|$|R
30|$|<b>Interoperable</b> justice agency <b>databases</b> {{could be}} used to {{identify}} systemic trouble points relate them to case complexity and perhaps develop improvements. For a variety of good reasons ranging from issues of privacy to the requirements of fair trials judicial data systems are rarely made available to researchers interested in understanding system problems.|$|R
40|$|While {{a number}} of {{different}} approaches exist to facilitate the interoperation of distributed, heterogeneous and autonomous databases, a systematic framework to assist with the evolution process of federated systems has yet to be presented. A primary hurdle in the life cycle of an <b>interoperable</b> <b>database</b> system is the effort, and therefore the cost, that is associated with its maintenance following its initial launch. In this paper we present a framework {{that can be used for}} the algorithmic detection of inconsistencies in evolving multidatabase systems, at both structural and semantic levels. Such framework forms the basis of a prototype system. We focus on the detection and reporting of incongruities that occur in the way that local entities are ontologically committed to global ones. Our proposal is founded on the exploitation of intensional sources ’ data in the creation of a construct that represents explicitly the ontological commitment of local entities onto federal concepts, which we refer to as Conceptual Links...|$|E
40|$|A {{large-scale}} <b>interoperable</b> <b>database</b> system {{operating in}} a dynamic environment should provide uniform access user interface to its components, scalability to larger networks, evolution of database schema and applications, flexible composability of client and server components, and preserve component autonomy. To address the research issues presented by such systems, we introduce the Distributed Interoperable Object Model (DIOM). DIOM's main features include the explicit repre- sentation of and access to semantics in data sources through the DIOM base interfaces, the use of interface abstraction mechanisms (such as specialization, generalization, aggregation and import) to support incremental design and construction of compound interoperation interfaces, the deferment of conflict resolution to the query submission time instead of {{at the time of}} schema integration, and a clean interface between distributed interoperable objects that supports the independent evolution and management of such objects. To make DIOM concrete, we outline the Diorama architecture, which includes important auxiliary services such as domain-specific library functions, object linking databases, and query decomposition and packaging strategies. Several practical examples and appli- cation scenarios illustrate the usefulness of DIOM...|$|E
40|$|Purpose: Purpose of {{the paper}} is to present a few {{comments}} about the renewal of the representation techniques, explored in the building and construction areas, as methodological and operational equipments that the digital design tends to upgrade. Method: The foreword above the topic of this work summarizes {{the picture of the}} needs expressed by the construction field in Italy, focusing on the main problems that arise analyzing the current production system. In Italy it has been started a research project in the announcement "Energy Efficiency Industria 2015 ", named InnovANCE, which provides {{for the development of the}} first unified and <b>interoperable</b> <b>database</b> in the construction industry, identifying the strategies for integration in the construction process, from the design phase to construction and management. Result: These methods answer some urgency carried out by data complexity inherent with building construction, relating to each other different professionals in a new system, reforming the assets and the processing mode, requiring explicit coordination of activities and procedures. Discussion & Conclusion: We discuss on the added value of the outcomes of the research in progress, focusing on the use of operational methods that are best suited to manage the complexity information carried on in the project representatio...|$|E
40|$|Abstract: The paper {{deals with}} the {{security}} of <b>interoperable</b> heterogeneous <b>database</b> environments. It contains a general discussion of the issues involved {{as well as a}} description of our experiences gained during {{the development and implementation of}} the security module of IRO-DB- an European ESPRIT III funded project with the goal to develop interoperable access between relational and object-oriented databases. Key words: database federation, security, heterogeneity, authorization, access control Security in homogeneous database systems has been an issue for several years now. Various techniques have been proposed addressing the general security requirements confidentiality (protecting information from unauthorized access), integrity (protectin...|$|R
40|$|Abstract. The {{maintenance}} of global integrity constraints in database federations {{is still a}} challenge since traditional integrity constraint man-agement techniques {{cannot be applied to}} such a distributed manage-ment of data. In this paper we present a concept of global integrity maintenance by migrating the concepts of active database systems to a collection of <b>interoperable</b> relational <b>databases.</b> We introduce Active Component Systems which are able to interact with each other using direct connections established from within their database management systems. Global integrity constraints are decomposed into sets of partial integrity constraints, which are enforced directly by the affected Active Component Systems without the need of a global component. ...|$|R
40|$|The Standard for the Exchange of Product model data (STEP, or ISO 10303) is {{a family}} of {{standards}} aimed to represent a product's information throughout its entire life cycle. STEP comprises several classes of documents organized in a database-like architecture. This paper focuses on STEP Implementation Methods and their application on the implementation of shareable product databases. File exchange, the simplest STEP Implementation Method, is described. SDAI, a language-independent application programming interface for access and manipulation of STEP data, is discussed, together with an outline of its application on the integration of network <b>interoperable</b> product <b>databases.</b> Keywords STEP, SDAI, Engineering Databases, EXPRESS, Product Data Exchange (PDE), Interoperability, CORBA 1...|$|R
