96|56|Public
40|$|In {{data with}} a group structure, <b>incidental</b> <b>parameters</b> are {{included}} to control for missing variables. Applications include longitudinal data and sibling data. In general, the joint maximum likelihood estimator of the structural parameters is not consistent {{as the number of}} groups increases, with a fixed number of observations per group. Instead a conditional likelihood function is maximized, conditional on sufficient statistics for the <b>incidental</b> <b>parameters.</b> In the logit case, a standard conditional logit program can be used. Another solution is a random effects model, in which the distribution of the <b>incidental</b> <b>parameters</b> may depend upon the exogenous variables. ...|$|E
40|$|This paper {{considers}} an {{estimation method}} for a binary panel model with <b>incidental</b> <b>parameters</b> as individual effects. The {{necessary condition for}} the conditional maximum likelihood approach proposed byÂ Andersen (1970) is investigated and we show that unique sufficient statistics exist only for logit models in a two-wave panel. Binary panel model <b>Incidental</b> <b>parameters</b> Conditional maximum likelihood estimator...|$|E
40|$|We {{consider}} {{estimation of}} the parameter B in a multivariate linear functional relationship Xi=[xi]i+[xi] 1 i, Yi=B[xi]i+[xi] 2 i, i= 1, [...] .,n, where the errors ([zeta] 1 i', [zeta] 2 i') are independent standard normal and ([xi]i, i [set membership, variant]) is a sequence of unknown nonrandom vectors (<b>incidental</b> <b>parameters).</b> If there are no substantial a priori restrictions on the infinite sequence of <b>incidental</b> <b>parameters</b> then asymptotically the model is nonparametric but does not fit into common settings presupposing a parameter from a metric function space. A special result of the local asymptotic minimax type for the m. 1. e. of B is proved. The accuracy of the normal approximation for the m. l. e. of order n- 1 / 2 is also established. Functional relationship infinitely many <b>incidental</b> <b>parameters</b> local asymptotic minimax risk bound accuracy of normal approximation...|$|E
40|$|We {{consider}} a panel quantile model with fixed effects. It is {{shown that the}} maximum likelihood estimator is numerically equivalent to the least absolute deviations estimator of the differenced model, and as a consequence, there is no <b>incidental</b> <b>parameter</b> problem. Panel Quantile <b>Incidental</b> <b>parameter</b> problem...|$|R
40|$|Following Lancaster (2002), {{we propose}} a {{strategy}} to solve the <b>incidental</b> <b>parameter</b> problem. The method is demonstrated under a simple panel Poisson count model. We also extend the strategy to accomodate cases when information orthogonality is unavailable, such as the linear AR(p) panel model. For the AR(p) model, there exists a correction function to fix the <b>incidental</b> <b>parameter</b> problem when the model is stationary with strictly exogenous regressors. MCMC algorithms are developed for parameter estimation and model comparison. The results based on the simulated data sets suggest that our method could achieve consistency in both parameter estimation and model selection. dynamic panel data model with fixed effect; <b>incidental</b> <b>parameter</b> problem; consistency in estimation; model selection; Bayesian model averaging; Markov chain Monte Carlo (MCMC) ...|$|R
40|$|We {{propose a}} bias {{correction}} method for nonlinear models with {{both individual and}} time effects. Under {{the presence of the}} <b>incidental</b> <b>parameter</b> problem, the maximum likelihood estimator derived from such models may be severely biased. Our method produces an approximation to an infeasible log-likelihood function that is not exposed to the <b>incidental</b> <b>parameter</b> problem. The maximizer derived from the approximating function serves as a bias-corrected estimator that is asymptotically unbiased when the sequence N=T converges to a constant. The proposed method is general in several perspectives. The method can be extended to models with multiple fixed effects and can be easily modified to accommodate dynamic models. status: publishe...|$|R
40|$|This paper {{considers}} model {{selection of}} nonlinear panel data {{models in the}} presence of <b>incidental</b> <b>parameters</b> (i. e., large-dimensional nuisance parameters). The main interest is in selecting the model that approximates best the structure with the common parameters after concentrating out the <b>incidental</b> <b>parameters.</b> New model selection information criteria are developed that use either the Kullback-Leibler information criterion based on the profile likelihood or the Bayes factor based on the integrated likelihood with the robust prior of Arellano and Bonhomme (2009, Econometrica 77 : 489 - 536). These model selection criteria impose heavier penalties than those of the standard information criteria such as AIC and BIC. The additional penalty, which is data-dependent, properly reflects the model complexity from the <b>incidental</b> <b>parameters.</b> As a particular example, a lag order selection criterion is examined in the context of dynamic panel models with fixed individual effects, and it is illustrated how the over/under-selection probabilities are controlled for...|$|E
40|$|In this paper, {{functional}} {{models with}} not replications are investigated within {{the class of}} the elliptical distributions. Emphasis {{is placed on the}} special case of the Student-t distribution. Main results encompasses consistency and asymptotic normality of the maximum likelihood estimators. Due to the presence of <b>incidental</b> <b>parameters,</b> standard maximum likelihood methodology cannot be used to obtain the main results, which require extensions of some existing results related to elliptical distributions. Asymptotic relative efficiencies are reported which show that the generalized least squares estimator can be highly inefficient when compared with the maximum likelihood estimator under nonnormality. asymptotic relative efficiency, elliptical distributions, <b>incidental</b> <b>parameters,</b> maximum likelihood estimators, Student-t distribution...|$|E
40|$|We study a {{nonlinear}} panel {{data model}} in which the fixed effects are assumed to have finite support. The fixed effects estimator {{is known to have}} the <b>incidental</b> <b>parameters</b> problem. We contribute to the literature by making a qualitative observation that the <b>incidental</b> <b>parameters</b> problem in this model may not be not as severe as in the conventional case. Because fixed effects have finite support, the probability of correctly identifying the fixed effect converges to one even when the cross sectional dimension grows as fast as some exponential function of the time dimension. As a consequence, the finite sample bias of the fixed effects estimator is expected to be small. ...|$|E
30|$|I {{caution to}} {{interpret}} the results of all probit models in this paper that include firm indicators with care, because they inherently suffer from the <b>incidental</b> <b>parameter</b> problem (see Lancaster 2000 for a survey of the problem).|$|R
3000|$|Equations (3) and (4) are {{theoretically}} {{prone to}} the <b>incidental</b> <b>parameter</b> problem, {{because of the}} presence of the cohort specific parameters αj and _j^ 2 [...] (Neyman and Scott 1948). However, in a recent article it has been shown that this bias is negligible (Salinari and De Santis 2014).|$|R
3000|$|... are {{replaced}} by assuming a proper distribution G(θ) in the population (e.g. the normal), requiring only the hyperparameters τ of G(θ) to be determined (i.e. the mean and the variance of G(θ) in our example). Although this solves the <b>incidental</b> <b>parameter</b> problem, the correct choice of G(·) is decisive for obtaining correct estimates (cf. Molenaar 1995, p. 47).|$|R
3000|$|Generally, this {{incidental}} parameter {{problem may be}} overcome by marginalization or conditional inference (cf. Pawitan 2001, p. 274). In the first case (Marginal Maximum Likelihood estimation, MML, cf. Baker and Kim 2004, ch. 6), the <b>incidental</b> <b>parameters</b> θ [...]...|$|E
40|$|In {{this paper}} we {{consider}} estimation of nonlinear panel data models that include multiple individual fixed effects. Estimation {{of these models}} is complicated both by the diffi culty of estimating models with possibly thousands of coeffi cients and also by the <b>incidental</b> <b>parameters</b> problem; that is, noisy estimates of the fi xed effects when the time dimension is short contaminate the estimates of the common parameters due to the nonlinearity of the problem. We propose a simple variation of existing bias-corrected estimators, which can exploit the additivity of the effects for numerical optimization. We exhibit {{the performance of the}} estimators in simulations. Panel data, nonlinear models, multiple fi xed e¤ects, <b>incidental</b> <b>parameters,</b> bias reduction...|$|E
40|$|This paper {{considers}} model {{selection in}} nonlinear panel data models where <b>incidental</b> <b>parameters</b> or large-dimensional nuisance parameters are present. Primary interest typically centres on selecting {{a model that}} best approximates the underlying structure involving parameters that are common within the panel after concentrating out the <b>incidental</b> <b>parameters.</b> It {{is well known that}} conventional model selection procedures are often inconsistent in panel models and this can be so even without nuisance parameters (Han et al, 2012). Modifications are then needed to achieve consistency. New model selection information criteria are developed here that use either the Kullback-Leibler information criterion based on the profile likelihood or the Bayes factor based on the integrated likelihood with the robust prior of Arellano and Bonhomme (2009). These model selection criteria impose heavier penalties than those associatedwithstandardinformationcriteria such as AIC and BIC. The additional penalty, which is data-dependent, properly reflects the model complexity arising from the presence of <b>incidental</b> <b>parameters.</b> A particular example is studied in detail involving lag order selection in dynamic panel models with fixed individual effects. The new criteria are shown to control for over/under-selection probabilities in these models and lead to consistent order selection criteria...|$|E
40|$|In {{this paper}} I study a fixed effects model of dyadic link {{formation}} for directed networks. I discuss inference on structural parameters {{as well as}} a test of model specification. In the model, an agent's linking decisions depend on perceived similarity to potential linking partners(homophily). Agents are endowed with potentially unobserved characteristics that govern their ability to establish links (productivity) and to receive links (popularity). Heterogeneity in productivity and popularity is a structural driver of degree heterogeneity. The unobserved heterogeneity is captured by a fixed effects approach. This allows for arbitrary correlation between an observed homophily component and latent sources of degree heterogeneity. The linking model accounts for link reciprocity by allowing linking decisions within each pair of agents to be correlated. Estimates of structural parameters related to homophily preferences and reciprocity can be obtained by ML but inference is non-standard due to the <b>incidental</b> <b>parameter</b> problem (Neyman and Scott 1948). I study t-statistics constructed from ML estimates via a naive plug-in approach. For these statistics it is not appropriate to compute critical values from a standard normal distribution because of the <b>incidental</b> <b>parameter</b> problem. I suggest modified t-statistics that are justified by an asymptotic approximation that sends the number of agents to infinity. For a t-test based on the modified statistics, critical values can be computed from a standard normal distribution. My model specification test compares observed transitivity to the transitivity predicted by the dyadic linking model. The test statistic corrects for <b>incidental</b> <b>parameter</b> bias that is due to ML estimation of the null model. The implementation of my procedures is illustrated by an application to favor networks in Indianvillages. JEL:C 33, C 3...|$|R
30|$|Finally, column 7 {{estimates}} {{a linear}} probability model. Including fixed effects in a discrete choice model might generate an <b>incidental</b> <b>parameter</b> problem, which, however, {{does not occur}} when using a linear probability model. The results are mostly in line with previous findings, with the only exception being the employment status variable, which now shows a negative effect, indicating that unemployed {{people are more likely}} to desire to leave their country.|$|R
40|$|Abstract. I discuss {{an issue}} arising in {{analyzing}} data from astronomical surveys: accounting for measurement uncertainties in {{the properties of}} individual sources detected in a survey when making inferences about {{the entire population of}} sources. Source uncertainties require the analyst to introduce unknown “incidental ” parameters for each source. The number of parameters thus grows with the size of the sample, and standard theorems guaranteeing asymptotic convergence of maximum likelihood estimates fail in such settings. From the Bayesian point of view, the missing ingredient in such analyses is accounting for the volume in the <b>incidental</b> <b>parameter</b> space via marginalization. I use simple simulations, motivated by modeling the distribution of trans-Neptunian objects surveyed in the outer solar system, to study the effects of source uncertainties on inferences. The simulations show that current non-Bayesian methods for handling source uncertainties (ignoring them, or using an ad hoc <b>incidental</b> <b>parameter</b> integration) produce incorrect inferences, with errors that grow more severe with increasing sample size. In contrast, accounting for source uncertainty via marginalization leads to sound inferences for any sample size...|$|R
30|$|We use linear {{probability}} models rather than, say, probits, {{because our}} most saturated specifications use {{large numbers of}} fixed effects (one for each job ad), raising computational issues as well as concerns with consistency {{in the presence of}} a large number of <b>incidental</b> <b>parameters.</b>|$|E
40|$|AbstractWe {{consider}} {{estimation of}} the parameter B in a multivariate linear functional relationship Xi=ξi+ξ 1 i, Yi=Bξi+ξ 2 i, i= 1,…,n, where the errors (ζ 1 i′, ζ 2 i′) are independent standard normal and (ξi, i ∈ N) is a sequence of unknown nonrandom vectors (<b>incidental</b> <b>parameters).</b> If there are no substantial a priori restrictions on the infinite sequence of <b>incidental</b> <b>parameters</b> then asymptotically the model is nonparametric but does not fit into common settings presupposing a parameter from a metric function space. A special result of the local asymptotic minimax type for the m. 1. e. of B is proved. The accuracy of the normal approximation for the m. l. e. of order n− 12 is also established...|$|E
40|$|This paper {{considers}} dynamic panel {{models with}} a factor error structure that is {{correlated with the}} regressors. Both short panels (small T) and long panels (large T) are considered. With a small T, consistent estimation requires either a suitable formulation of the reduced form or an appropriate conditional equation for the first observation. Also needed is a suitable control for {{the correlation between the}} effects and the regressors. Under the factor error structure, the panel system implies parameter constraints between the mean vector and the covariance matrix. We explore the constraints through a quasi-FIML approach. The factor process is treated as parameters and it can have arbitrary dynamics under both fixed and large T. The large T setting involves <b>incidental</b> <b>parameters</b> because the number of parameters (including the time effects, the factor process, the heteroskedasticity parameters) increases with T. Even though an increasing number of parameters are estimated, we show that there is no <b>incidental</b> <b>parameters</b> bias to affect the limiting distributions; the estimator is centered at zero even scaled by the fast convergence rate of root-NT. We also show that the quasi-FIML approach is efficient under both fixed and large T, despite non-normality, heteroskedasticity, and <b>incidental</b> <b>parameters.</b> Finally we develop a feasible and fast algorithm for computing the quasi-FIML estimators under interactive effects. ...|$|E
40|$|This paper {{analyzes}} {{the determinants of}} annual worker reallocation across disaggregated occupations in western Germany for the period 1985 - 2003. Employing data from the German Socio-Economic Panel, the pattern of average occupational mobility is documented. Worker reallocation {{is found to be}} strongly procyclical. Its determinants at the individual level are then investigated while controlling for unobserved worker heterogeneity. A dynamic probit fixed effects model is estimated to obtain coefficients and marginal effects. The <b>incidental</b> <b>parameter</b> bias is reduced by the method proposed in Hahn and Kuersteiner (2004). An interesting finding is that workers changing occupation are about 8 to 9 percent less inclined to experience occupational mobility in the subsequent year than workers who do not change. Except for workers with only compulsory education, the impact of age on the probability of occupational change is declining in the level of education. The unemployment rate has a negative effect on the probability of occupational changes, especially for female foreigners. Dynamic binary choice models, fixed effects, <b>incidental</b> <b>parameter</b> bias, occupational mobility, Panel data...|$|R
30|$|Therefore, a {{possible}} linear- and quadratic time trend in efficiency {{will be included}} in the model which is used in this research. To analyze our data for fitting results, this study has applied an SFA model, proposed by Bates and Coelli (1992) and moderated by Chen et al. 2014. This model is commonly known as a log transformation frontier model using frontier command Xtfrontier. The transformation method removes the potential issues caused by the <b>incidental</b> <b>parameter</b> problem (Wong, Ho and Singh 2007).|$|R
40|$|This thesis {{consists}} of three chapters on economic and econometric applications of Bayesian parameter estimation and model comparison. The first two chapters study the <b>incidental</b> <b>parameter</b> problem mainly under a linear autoregressive (AR) panel data model with fixed effect. The first chapter investigates the problem from a model comparison perspective. The major finding in the first chapter is that consistency in parameter estimation and model selection are interrelated. The reparameterization of the fixed effect parameter proposed by Lancaster (2002) may not provide a valid solution to the <b>incidental</b> <b>parameter</b> problem if the wrong set of exogenous regressors are included. To estimate the model consistently and to measure its goodness of fit, the Bayes factor {{is found to be}} more preferable for model comparson than the Bayesian information criterion based on the biased maximum likelihood estimates. When the model uncertainty is substantial, Bayesian model averaging is recommended. The method is applied to study the relationship between financial development and economic growth. The second chapter proposes a correction function approach to solve the <b>incidental</b> <b>parameter</b> problem. It is discovered that the correction function exists for the linear AR panel model of order p when the model is stationary with strictly exogenous regressors. MCMC algorithms are developed for parameter estimation and to calculate the Bayes factor for model comparison. The last chapter studies how stock return's predictability and model uncertainty affect a rational buy-and-hold investor's decision to allocate her wealth for different lengths of investment horizons in the UK market. The FTSE All-Share Index is treated as the risky asset, and the UK Treasury bill as the riskless asset in forming the investor's portfolio. Bayesian methods are employed to identify the most powerful predictors by accounting for model uncertainty. It is found that though stock return predictability is weak, it can still affect the investor's optimal portfolio decisions over different investment horizons. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Estimations of {{nonlinear}} panel {{models that}} include individual specific fixed effects are {{complicated by the}} <b>incidental</b> <b>parameters</b> problem, that is, the asymptotic bias in the estimation of typical fixed effects panel models generally results in inconsistent estimates. In this paper, I characterize the leading term of a large-T expansion of the biases in the nonlinear least square estimator (NLSE) and estimators of the average partial effects in panel fractional response models. The resulting estimator after analytical bias correction is robust to the <b>incidental</b> <b>parameters</b> bias and reduces the bias order from O(T− 1) to O(T− 2). I also examine the finite sample performance of the proposed estimator using a new data generating process in which panel fractional response variables are collapsed from repeated, clustered cross-sectional binary probit choices. A proof showing the generated data satisfies the identification assumption at the cluster level has been given. Simulation results suggest that, in the static case, the bias corrected estimator performs comparably to the quasi-maximum likelihood estimator (QMLE), which is the standard approach in the literature, for 8 or more periods, while in the dynamic case, the bias corrected estimators are substantially superior to those QMLE’s. Fractional responses, Panel Data, Unobserved effects, Probit, Partial effects, Bias, <b>Incidental</b> <b>parameters</b> problem, Fixed effects, Bias Correction...|$|E
40|$|Spatial {{effects and}} common-shocks effects are of {{increasing}} empirical importance. Each type of effect has been analyzed separately {{in a growing}} literature. This paper considers a joint modeling of both types. Joint modeling allows one to determine whether {{one or both of}} these effects are present. A large number of <b>incidental</b> <b>parameters</b> exist under the joint modeling. The quasi maximum likelihood method (MLE) is proposed to estimate the model. Heteroskedasticity is explicitly estimated. This paper demonstrates that the quasi-MLE is effective in dealing with the <b>incidental</b> <b>parameters</b> problem. An inferential theory including consistency, rate of convergence and limiting distributions is developed. The quasi-MLE can be easily implemented via the EM algorithm, as confirmed by the Monte Carlo simulations. The simulations further reveal the excellent finite sample properties of the quasi-MLE. Some extensions are discussed. ...|$|E
40|$|Applied econometric {{research}} frequently encounters {{the difficulty}} that {{estimation of the}} parameters of interest is complex owing {{to the presence of}} <b>incidental</b> <b>parameters.</b> It is tempting therefore to try to circumvent the difficulties by proceeding in two stages. In the first, some estimates are made of the <b>incidental</b> <b>parameters.</b> In the second, these estimates are treated as though they were population values, leading to a large reduction in the dimension of the unknown parameter space, possibly even down to that of the parameters of interest only. The properties of such a staged process (particularly as they relate to issues arising from the consistency and efficiency of the estimator and the provision of reliable inference), applications involving the presence of current and future anticipations, an alternative estimator, and diagnostic tests are all discussed in this paper. Two stage estimators, censored data, anticipation...|$|E
40|$|AbstractWe derive fixed effects estimators of {{parameters}} and average partial effects in (possibly dynamic) nonlinear panel data models with individual and time effects. They cover logit, probit, ordered probit, Poisson and Tobit models {{that are important}} for many empirical applications in micro and macroeconomics. Our estimators use analytical and jackknife bias corrections {{to deal with the}} <b>incidental</b> <b>parameter</b> problem, and are asymptotically unbiased under asymptotic sequences where N/T converges to a constant. We develop inference methods and show that they perform well in numerical examples...|$|R
40|$|In {{nonlinear}} panel data models, the <b>incidental</b> <b>parameter</b> problem {{remains a}} challenge to econometricians. Available solutions are often based on ingenious, modelspecific methods. In this paper, we propose a systematic approach to construct moment restrictions on common parameters that are free from the individual fixed effects. This is done by an orthogonal projection that differences out the unknown distribution function of individual effects. Our method applies generally in likelihood models with continuous dependent variables where a condition of non-surjectivity holds. The resulting method-of-moments estimators are root...|$|R
40|$|Neyman and Scott (1948) {{define the}} <b>incidental</b> <b>parameter</b> problem. In panel data with T {{observations}} per individual and unobservable individual- specific effects, the inconsistency {{of the maximum}} likelihood estimator of the common parameters is in general of the order 1 /T. This paper considers the integrated likelihood estimator and develops the integrated moment estimator. It shows that the inconsistency of the integrated likelihood estimator reduces from 1 /T to 1 /T 2 if an information orthogonal parametrization is used. It derives information orthogonal moment functions for the general linear model and the index model with weakly exogenous regressors and thereby offers an approximate solution for the <b>incidental</b> <b>parameter</b> problem {{for a wide range}} of models. It argues that reparametrizations are easier in a Bayesian framework and shows how to use the 1 /T 2 - result to increase the robustness against the choice of mixing distribution. The integrated likelihood estimator is consistent and adaptive for asympototics in which T proportional to N to the power alpha where alpha is larger than 1 / 3. The paper also shows that likelihood methods that use sufficient statistics for the individual-specific effects can be viewed as a special case of the integrated likelihood estimator. ...|$|R
40|$|Traditional panel {{stochastic}} frontier models do {{not distinguish}} between unobserved individual heterogeneity and inefficiency. They thus force all time-invariant individual heterogeneity into the estimated inefficiency. Greene (2005) proposes a true fixed-effect stochastic frontier model which, in theory, may be biased by the <b>incidental</b> <b>parameters</b> problem. The problem usually cannot {{be dealt with by}} model transformations owing to the nonlinearity of the stochastic frontier model. In this paper, we propose a class of panel stochastic frontier models which create an exception. We show that first-difference and within-transformation can be analytically performed on this model to remove the fixed individual effects, and thus the estimator is immune to the <b>incidental</b> <b>parameters</b> problem. Consistency of the estimator is obtained by either N [...] >[infinity] or T [...] >[infinity], which is an attractive property for empirical researchers. Stochastic frontier models Fixed effects Panel data...|$|E
40|$|This note {{argues that}} a Bayesian {{framework}} is almost inescapable when specifying statistical models of the LISREL type, i. e. models involving not only latent and manifest variables but also <b>incidental</b> <b>parameters.</b> Indeed, a careful speciﬁcation, making every hypothesis explicit and interpretable both contextually and statistically, requires a fully probabilistic framework, {{which is one of}} the most attractive features of the Bayesian approach. Such an environment allows one to develop a complete analysis of identiﬁcation distinguishing ﬁve levels of identiﬁcation problems. From this analysis the paper proceeds, on one hand, by giving some sficient conditions for the identiﬁcation of the statistical model, and, on the other hand, by studying the identiﬁcation problem in the predictive modelBayesian Identiﬁcation, Complete Parameters, <b>Incidental</b> <b>Parameters,</b> Hierarchical Model, Minimal Predictive Sufficiency, Mixture Model, Strong Sifting Sequences, Speciﬁcation, Strong Identiﬁcation, Structural Parameters...|$|E
40|$|Bootstrap and Markov chain Monte Carlo {{methods have}} {{received}} much attention in recent years. We study computer intensive methods {{that can be}} used in complex situations where it is not possible to express the likelihood estimates or the posterior analytically. The work is inspired by a set of car crash data from real traffic. We formulate and develop a model for car crash data that aims to estimate and compare the relative collision safety among different car models. This model works sufficiently well, although complications arise due to a growing vector of <b>incidental</b> <b>parameters.</b> The bootstrap is shown to be a useful tool for studying uncertainties of the estimates of the structural parameters. This model is further extended to include driver characteristics. In a Poisson model with similar, but simpler structure, estimates of the structural parameter in the presence of <b>incidental</b> <b>parameters</b> are studied. The profile likelihood, bootstrap and the delta method are compared for deterministic and random <b>incidental</b> <b>parameters.</b> The same asymptotic properties, up to first order, are seen for deterministic as well as random <b>incidental</b> <b>parameters.</b> The search for suitable methods that work in complex model structures leads us to consider Markov chain Monte Carlo (MCMC) methods. In the area of MCMC, we consider particularly the question of how and when to claim convergence of the MCMC run in situations where it is only possible to analyse the output values of the run and also how to compare different MCMC modellings. In Metropolis-Hastings algorithm, different proposal functions lead to different realisations. We develop a new convergence diagnostic, based on the Kullback-Leibler distance, which is shown to be particularly useful when comparing different runs. Comparisons with established methods turn out favourably for the KL. In both models, a Bayesian analysis is made where the posterior distribution is obtained by MCMC methods. The credible intervals are compared to the corresponding confidence intervals from the bootstrap analysis and are shown to give the same qualitative conclusions...|$|E
40|$|I discuss {{an issue}} arising in {{analyzing}} data from astronomical surveys: accounting for measurement uncertainties in {{the properties of}} individual sources detected in a survey when making inferences about {{the entire population of}} sources. Source uncertainties require the analyst to introduce unknown ``incidental'' parameters for each source. The number of parameters thus grows with the size of the sample, and standard theorems guaranteeing asymptotic convergence of maximum likelihood estimates fail in such settings. From the Bayesian point of view, the missing ingredient in such analyses is accounting for the volume in the <b>incidental</b> <b>parameter</b> space via marginalization. I use simple simulations, motivated by modeling the distribution of trans-Neptunian objects surveyed in the outer solar system, to study the effects of source uncertainties on inferences. The simulations show that current non-Bayesian methods for handling source uncertainties (ignoring them, or using an ad hoc <b>incidental</b> <b>parameter</b> integration) produce incorrect inferences, with errors that grow more severe with increasing sample size. In contrast, accounting for source uncertainty via marginalization leads to sound inferences for any sample size. Comment: 12 pages, 5 figures; to appear in Bayesian Inference And Maximum Entropy Methods In Science And Engineering: 24 th International Workshop, Garching, Germany, 2004; ed. Volker Dose et al. (AIP Conference Proceedings Series...|$|R
40|$|We derive fixed effects estimators of {{parameters}} and average partial effects in (possibly dynamic) nonlinear panel data models with individual and time effects. They cover logit, probit, ordered probit, Poisson and Tobit models {{that are important}} for many empirical applications in micro and macroeconomics. Our estimators use analytical and jackknife bias corrections {{to deal with the}} <b>incidental</b> <b>parameter</b> problem, and are asymptotically unbiased under asymptotic sequences where $N/T$ converges to a constant. We develop inference methods and show that they perform well in numerical examples. Comment: 84 pages, 10 tables, includes supplementary appendi...|$|R
40|$|The fixed effects {{estimator}} of panel {{models can}} be severely biased because of well-known <b>incidental</b> <b>parameter</b> problems. It is shown that this bias {{can be reduced}} in nonlinear dynamic panel models. We consider asymptotics where n and T grow {{at the same rate}} as an approximation that facilitates comparison of bias properties. Under these asymptotics, the bias-corrected estimators we propose are centered at the truth, whereas fixed effects estimators are not. We discuss several examples and provide Monte Carlo evidence for the small sample performance of our procedure. ...|$|R
