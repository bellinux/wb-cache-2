22|10000|Public
50|$|Sound is {{measured}} {{based on the}} amplitude and <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> wave. Noise is most commonly {{discussed in terms of}} decibels (dB), the measure of loudness, or intensity of a sound; this measurement describes the amplitude of a sound wave. On the other hand, pitch describes the <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> and {{is measured}} in hertz (Hz).|$|E
50|$|Pitch/frequency effects modify pitch by {{altering}} the <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> wave or sound signal or adding new harmonies.|$|E
50|$|In musical theory, pitch {{represents}} the perceived fundamental <b>frequency</b> <b>of</b> <b>a</b> <b>sound.</b> However the actual fundamental frequency {{may differ from}} the perceived fundamental frequency because of overtones.|$|E
40|$|Pitch {{shifting}} is {{the process}} <b>of</b> changing the <b>frequencies</b> <b>of</b> <b>a</b> <b>sound</b> but keeping the same duration. This change in frequency creates new formants, changing {{the character of the}} original sound. This paper proposes a way to maintain the original formants while pitch shifting the sound. Architecture & Allied Art...|$|R
5000|$|The volley {{theory was}} {{explained}} in depth in Ernest Wever's 1949 book, Theory of Hearing [...] Groups of neurons in the cochlea individually fire at subharmonic <b>frequencies</b> <b>of</b> <b>a</b> <b>sound</b> being heard and collectively phase-lock {{to match the}} total <b>frequencies</b> <b>of</b> the sound. The {{reason for this is}} that neurons can only fire at <b>a</b> maximum <b>of</b> about 500 Hz but other theories of hearing did not explain for hearing sounds below about 5000 Hz.|$|R
40|$|It is {{necessary}} to derive <b>a</b> proper method <b>of</b> estimating the change of angular velocity or the change of crossing angle <b>of</b> <b>a</b> rotating shaft which is driven through a universal joint. From our experiments, a rotating shaft which is driven through a universal joint {{has a tendency to}} produce <b>a</b> metalic <b>sound</b> <b>of</b> high <b>frequencies.</b> when in an abnormal state. Then we investigate the new method of estimating the condition <b>of</b> <b>a</b> rotating shaft by analyzing the distribution <b>of</b> <b>frequencies</b> <b>of</b> <b>a</b> <b>sound</b> due to the angular velocity variation...|$|R
50|$|The <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> {{is defined}} as the number of repetitions of its {{waveform}} per second, and is measured in hertz; frequency is inversely proportional to wavelength (in a medium of uniform propagation velocity, such as sound in air). The wavelength of a sound is the distance between any two consecutive matching points on the waveform. The audible frequency range for young humans is about 20 Hz to 20 kHz. Hearing of higher frequencies decreases with age, limiting to about 16 kHz for adults, and even down to 3 kHz for elders.|$|E
50|$|An {{overtone}} is any frequency {{greater than}} the fundamental <b>frequency</b> <b>of</b> <b>a</b> <b>sound.</b> Using the model of Fourier analysis, the fundamental and the overtones together are called partials. Harmonics, or more precisely, harmonic partials, are partials whose frequencies are integer multiples of the fundamental (including the fundamental which is 1 times itself). These overlapping terms are variously used when discussing the acoustic behavior of musical instruments. (See etymology below.) The model of Fourier analysis provides {{for the inclusion of}} inharmonic partials, which are partials whose frequencies are not whole-number ratios of the fundamental (such as 1.1 or 2.14179).|$|E
5000|$|The N100 is preattentive and {{involved}} in perception because its amplitude is strongly dependent upon {{such things as the}} rise time of the onset of a sound, its loudness, [...] interstimulus interval with other sounds, and the comparative <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> as its amplitude increases in proportion to how much a sound differs in frequency from a preceding one. Neuromagnetic research has linked it further to perception by finding that the auditory cortex has a tonotopic organization to N100. However, it also shows a link to a person's arousal and selective attention. N100 is decreased when a person controls the creation of auditory stimuli, such as their own voice.|$|E
5000|$|The upper <b>frequency</b> limit <b>of</b> <b>a</b> NICAM <b>sound</b> {{channel is}} 15 kHz due to {{anti-aliasing}} filters at the encoder.|$|R
5000|$|... n is <b>frequency</b> <b>of</b> <b>sound,</b> <b>A</b> is the Amplitude {{of sound}} wave, v is {{velocity}} of sound, and œÅ is density of medium in which sound is traveling ...|$|R
2500|$|It is {{possible}} to analyze the <b>frequency</b> components <b>of</b> <b>a</b> recorded <b>sound</b> giving <b>a</b> [...] "sum <b>of</b> sinusoids" [...] representation. This representation can be re-synthesized using additive synthesis. One method <b>of</b> decomposing <b>a</b> <b>sound</b> into time varying sinusoidal partials is short-time Fourier transform (STFT)-based McAulay-Quatieri Analysis.|$|R
5000|$|One {{major goal}} of sensory {{neuroscience}} {{is to try}} to estimate the neuron's receptive field; that is, to try to determine which stimuli cause the neuron to fire in what ways. One common way to find the receptive field is to use linear regression to find which stimulus characteristics typically caused neurons to become excited or depressed. Since the receptive field of a sensory neuron can vary in time (i.e. latency between the stimulus and the effect it has on the neuron) and in some spatial dimension (literally space for vision and somatosensory cells, but other [...] "spatial" [...] dimensions such as the <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> for auditory neurons), the term spatio temporal receptive field or STRF is often used to describe these receptive fields.|$|E
5000|$|In the {{physical}} {{sense of the}} term, the word [...] "pitch" [...] refers to the <b>frequency</b> <b>of</b> <b>a</b> <b>sound.</b> Another term that is frequently used by music neuroscientists is [...] "fine-grained pitch processing" [...] which refers {{to the ability of}} a person to distinguish minor changes or fluctuations in pitch. Processing pitch is an extremely integral part of music cognition. Recent developments in brain scanning techniques have shown neuroscientists that the posterior secondary cortex plays an extremely important part in the processing of pitch in the brain. In music, [...] "pitch relation" [...] is more important than pitch itself. A subset of five to seven pitches creates a scale. The scale tones are [...] "not equivalent and are organized around a central tone, called the tonic" [...] (Peretz 2005).|$|E
5000|$|Acoustic {{information}} {{describing the}} environment is the primary data required in soundscape ecology studies. Technological advances have provided improved methods for the collection of such data. Automated recording systems allow for temporally replicated samples of soundscapes to be gathered with relative ease. Data collected from such equipment can be extracted to generate a visual representation of the soundscape {{in the form of}} a spectrogram. Spectrograms provide information on a number of sound properties that may be subject to quantitative analysis. The vertical axis of a spectrogram indicates the <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> while the horizontal axis displays the time scale over which sounds were recorded. In addition, spectrograms display the amplitude of sound, a measure of sound intensity. Ecological indices traditionally used with species-level data, such as diversity and evenness, have been adapted for use with acoustic metrics. These measures provide a method of comparing soundscapes across time or space. For example, automated recording devices have been used to gather acoustic data in different landscapes across yearlong time scales, and diversity metrics were employed to evaluate daily and seasonal fluctuations in soundscapes across sites. [...] Spatial patterns of sound may also be studied using tools familiar to landscape ecologists such as geographic information systems (GIS). Finally, recorded samples of the soundscape can provide proxy measures for biodiversity inventories in cases where other sampling methods are impractical or inefficient. These techniques may be especially important for the study of rare or elusive species that are especially difficult to monitor in other ways.|$|E
40|$|This work {{presents}} an interactive device to control an adaptive tuning and synthesis system. The gestural controller {{is based on}} the theremin concept in which only an antenna is used as a proximity sensor. This interactive process is guided by sensorial consonance curves and adaptive tuning related to psychoacoustical studies. We used an algorithm to calculate the dissonance values according to amplitudes and <b>frequencies</b> <b>of</b> <b>a</b> given <b>sound</b> spectrum. The theoretical background is presented followed by interactive composition strategies and sound results...|$|R
30|$|A {{spectrogram}} is <b>a</b> visual representation <b>of</b> {{the spectrum}} <b>of</b> <b>frequencies</b> in <b>a</b> <b>sound</b> or other signal as they vary with time {{or some other}} variable. They are used extensively {{in the fields of}} music, sonar, radar, and speech processing and seismology [21].|$|R
50|$|This typical 4000 Hz notch {{is due to}} the {{transfer}} function of the ear. Indeed, as any object facing <b>a</b> <b>sound,</b> the ear acts as a passive filter (-although the inner ear is not an absolute passive filter, as the outer hair cells provide active mechanisms). A passive filter is a low pass : the high frequencies are more absorbed by the object, as high frequencies impose <b>a</b> higher pace <b>of</b> compression-decompression to the object. Thus, the high <b>frequency</b> harmonics <b>of</b> <b>a</b> <b>sound</b> are more harmful to the inner-ear.|$|R
40|$|Participants made speeded {{discrimination}} {{responses to}} unimodal auditory (low-frequency vs. high-frequency sounds) or vibrotactile stimuli (presented to the index finger, upper location vs. to the thumb, lower location). In the compatible blocks of trials, the implicitly related stimuli (i. e. higher-frequency sounds and upper tactile stimuli; {{and the lower}}-frequency sounds and the lower tactile stimuli) {{were associated with the}} same response key; in the incompatible blocks, weakly related stimuli (i. e. high-frequency sounds and lower tactile stimuli; and the low-frequency sounds and the upper tactile stimuli) were associated with the same response key. Better performance was observed in the compatible (vs. incompatible) blocks, thus providing empirical support for the cross-modal association between the relative <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> and the relative elevation of a tactile stimulus...|$|E
40|$|Doppler Effect is {{the change}} in <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> causes by the {{movement}} of the source producing the sound. The distance, velocity of the source movement and speed of sound in medium determines the how the frequency changes. Doppler shift can be reproduced using programming languages like "Matlab". But usually the reproduction quality of the Doppler shift which is produced using the programming language is not anything similar to the Doppler shift experienced in the real life. This is mainly because there are number factors which are ignored while programming the Doppler shift reproduction program. This review focuses on how to reproduce virtual moving source causing Doppler shift more accurately as heard in the natural world. Architecture & Allied Art...|$|E
40|$|The {{equations}} of continuum hydrodynamics can {{be derived}} from the Boltzmann equation, which describes rarefied gas dynamics at the kinetic level, by means of the Chapman-Enskog expansion. This expansion assumes a small Knudsen number, and as a consequence, the hydrodynamics equations are able to successfully describe sound propagation when the <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> wave is much higher than the collision frequency of the particles. When both frequencies become comparable, these equations give a poor account of the experimental measurements. A series of generalized hydrodynamic equations has been introduced in the literature along the years in order to improve the continuous description of small scale properties of fluid flow, as ultrasound propagation. We will describe herein some of the simplified models that has been proposed so far. Comment: Submitted to the Proceedings of the Ninth International Conference on Theoretical and Computational Acoustic...|$|E
40|$|The {{corresponding}} {{study was}} carried out to detect changes in audio file using spectrograph. An audio file format is a file format for storing digital audio data on a computer system. <b>A</b> <b>sound</b> spectrograph is <b>a</b> laboratory instrument that displays <b>a</b> graphical representation <b>of</b> {{the strengths of the}} various component <b>frequencies</b> <b>of</b> <b>a</b> <b>sound</b> as time passes. The objectives of the study were to find the changes in spectrograph of audio after altering them to compare altering changes with spectrograph of original files and to check for similarity and difference in mp 3 and wav. Five different alterations were carried out on each audio file to analyze the differences between the original and the altered file. For altering the audio file MP 3 or WAV by cutcopy the file was opened in Audacity. A different audio was then pasted to the audio file. This new file was analyzed to view the differences. By adjusting the necessary parameters the noise was reduced. The differences between the new file and the original file were analyzed. By adjusting the parameters from the dialog box the necessary changes were made. The edited audio file was opened in the software named spek where after analyzing a graph is obtained of that particular file which is saved for further analysis. The original audio graph received was combined with the edited audio file graph to see the alterations...|$|R
5000|$|Much {{later the}} concept was applied in audio {{engineering}} to adjust the frequency response in recording, reproduction, and live sound reinforcement systems. Sound engineers correct the <b>frequency</b> response <b>of</b> <b>a</b> <b>sound</b> system so that the <b>frequency</b> balance <b>of</b> the music as heard through speakers better matches the original performance {{picked up by a}} microphone. Audio amplifiers have long had filters or controls to modify their frequency response. These are most often in the form of variable bass and treble controls (shelving filters), and switches to apply low-cut or high-cut filters for elimination <b>of</b> low <b>frequency</b> [...] "rumble" [...] and high frequency [...] "hiss" [...] respectively.|$|R
50|$|Pure tones {{have a clear}} pitch, but complex sounds such as {{speech and}} music {{typically}} have intense peaks at many different frequencies. Nevertheless, by establishing a fixed reference point in the <b>frequency</b> function <b>of</b> <b>a</b> complex <b>sound,</b> and then observing the movement of this reference point as the function translates, one can generate a meaningful pitch contour consistent with human experience.|$|R
40|$|Abstract. Fast and {{successful}} {{searching for an}} object in a multime-dia database is a highly desirable functionality. Several approaches to content based retrieval for multimedia databases {{can be found in}} the lit-erature [9, 10, 12, 14, 17]. The approach we consider is feature extraction. A feature can be seen as a way to present simple information like the tex-ture, color and spatial information of an image, or the pitch, <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> etc. In this paper we present a method for feature extraction on texture and spatial similarity, using fractal coding techniques. Our method is based upon the observation that the coefficients describing the fractal code of an image, contain very useful information about the structural content of the image. We apply simple statistics on information produced by fractal image coding. The statistics reveal features and require a small amount of storage. Several invariances are a consequence of the used methods: size, global contrast, orientation. ...|$|E
40|$|Detection of the {{periodicity}} of {{amplitude modulation}} {{is a major}} step in {{the determination of the}} pitch of a sound. In this article we will present a silicon model that uses synchronicity of spiking neurons to extract the fundamental <b>frequency</b> <b>of</b> <b>a</b> <b>sound.</b> It is based on the observation that the so called `Choppers' in the mammalian Cochlear Nucleus synchronize well for certain rates of amplitude modulation, depending on the cell's intrinsic chopping frequency. Our silicon model uses three different circuits, i. e., an artificial cochlea, an Inner Hair Cell circuit, and a spiking neuron circuit. 1. INTRODUCTION Over the last few years, we have developed and implemented several analog VLSI building blocks that allow us to model parts of the auditory pathway [1], [2], [3]. This paper presents one experiment using these building blocks to create a model for the detection of the fundamental frequency of a harmonic complex. The estimation of this fundamental frequency by the model shows some i [...] ...|$|E
40|$|Fast and {{successful}} {{searching for an}} object in a multimedia database is a highly desirable functionality. Several approaches to content based retrieval for multimedia databases {{can be found in}} the literature [9, 10, 12, 14, 17]. The approach we consider is feature extraction. A feature can be seen as a way to present simple information like the texture, color and spatial information of an image, or the pitch, <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> etc. In this paper we present a method for feature extraction on texture and spatial similarity, using fractal coding techniques. Our method is based upon the observation that the coefficients describing the fractal code of an image, contain very useful information about the structural content of the image. We apply simple statistics on information produced by fractal image coding. The statistics reveal features and require a small amount of storage. Several invariances are a consequence of the used methods: size, global contrast, orientation...|$|E
50|$|In its {{simplest}} form, <b>a</b> <b>sound</b> trap consists <b>of</b> <b>an</b> offset in the ductwork {{to reflect}} the sound back to its source. This configuration is often combined {{with the use of}} sound-absorbing material inside the trap. The physical dimensions of the sound trap may be selected to tune the trap to specific <b>frequencies</b> <b>of</b> <b>sound.</b> <b>As</b> such, it is then essentially a Helmholtz resonator used as a passive noise-control device.|$|R
5|$|Consonants and vowel {{segments}} {{combine to}} form syllables, {{which in turn}} combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in <b>a</b> spectrogram <b>of</b> the recorded sound wave (See illustration of Spectrogram of the formant structures of three English vowels). Formants are the amplitude peaks in the <b>frequency</b> spectrum <b>of</b> <b>a</b> specific <b>sound.</b>|$|R
40|$|This paper {{presents}} the 21 -channel sound field reconstruction {{system based on}} the physical reconstruction <b>of</b> <b>a</b> three dimensional target sound field over the pre-defined control volume. According to the virtual sound source position and intensity, each loudspeaker signal is estimated through convolving with appropriate FIR filter to reconstruct <b>a</b> target <b>sound</b> field. In addition, the gain of FIR filter is only applied to the mid <b>frequency</b> band <b>of</b> <b>a</b> <b>sound</b> source signal to prevent aliasing effects and to save the computational complexity at the high frequency bands. Also the whole filter processing is carried out at the frequency domain to adopt a real-time application. Through the subjective listening tests the proposed system showed better performance on the localization in the horizontal plane comparing with conventional panning method. <br/...|$|R
40|$|The {{ability of}} the {{auditory}} system to perceive the fundamental <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> even when this frequency {{is removed from the}} stimulus is an interesting phenomenon related to the pitch of complex sounds. This capability is known as "residue" or "virtual pitch" perception and was first reported last century in the pioneering work of Seebeck. It is residue perception that allows one to listen to music with small transistor radios, which in general have a very poor and sometimes negligible response to low frequencies. The first attempt, due to Helmholtz, to explain the residue as a nonlinear effect in the ear considered it to originate from difference combination tones. However, later experiments have shown that the residue does not coincide with a difference combination tone. These results and the fact that dichotically presented signals also elicit residue perception have led to nonlinear theories being gradually abandoned in favour of central processor models. In this paper we use r [...] ...|$|E
40|$|A compact sound-speed sensor {{based on}} a phase {{difference}} method was developed. The sensor employs a U-shaped stainless steel tube with two holes located on its front and back ends, {{which serves as a}} sound wave guide. The phase difference between the two holes was measured using two mini-microphones by means of a phase-sensitive detection technique. This method offers the advantage of eliminating the influence of signal fluctuations. The <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> source offered by a loudspeaker can be scanned between 1 kHz and 50 kHz. The slope of the phase difference as a function of frequency was obtained by scanning the frequency of the sound source. The speed of sound was retrieved from the rate of change of the phase difference. The performance of the sensor was evaluated over a wide range of speeds of sound from 260 m/s to 1010 m/s in different gas mixtures. The measured speed of sound was found to be in good agreement with the theoretical value for the sound-speed sensor...|$|E
40|$|Abstract. The {{ability of}} the {{auditory}} system to perceive the fundamental <b>frequency</b> <b>of</b> <b>a</b> <b>sound</b> even when this frequency {{is removed from the}} stimulus is an interesting phenomenon related to the pitch of complex sounds. This capability is known as residue or virtual pitch perception and was first reported last century in the pioneering work of Seebeck. It is residue perception that allows one to listen to music with small transistor radios, which in general have a very poor and sometimes negligible response to low frequencies. The first attempt, due to von Helmholtz, to explain the residue as a nonlinear effect in the ear considered it to originate from difference combination tones. But later experiments showed that the residue does not coincide with a difference combination tone, and nonlinear theories were abandoned. However, in this paper we use recent results from the theory of nonlinear dynamical systems to show that physical frequencies produced by generic nonlinear oscillators acted upon by two independent periodic excitations can reproduce with great precision most of the experimental data about the residue. ...|$|E
40|$|In high-risk pregnancies, the {{transport}} of oxygen and nutrients from maternal to fetal blood via the placenta is often impaired. To assess the risk, pulsed Doppler ultrasound (US) is {{used to evaluate the}} flow velocity waveform in the umbilical artery with the pulsatility index (PI), which is derived from the velocity envelope of the Doppler power spectrum. However, simply listening to the Doppler signal can indicate to an experienced sonographer that the type of the blood flow is worse than the PI suggests. This is however dependent on the operator's experience and {{it may be difficult to}} estimate what influences the subjective judgement. Motivated by the description of the Doppler <b>sounds</b> by <b>an</b> experienced operator (AT) as having a "timbre", this study describes <b>an</b> analysis <b>of</b> Doppler <b>sounds</b> in search for an index or method with capacity to better evaluate the blood flow in the umbilical artery in high-risk pregnancies. A test was designed, where synthetically produced Doppler sounds with various spectral contents were played together with <b>a</b> variable sinusoidal <b>sound</b> signal. The task for the five test persons was to match the <b>frequency</b> <b>of</b> the sinusoidal signal to the Doppler sounds. The tests indicated that the human ear is most sensitive to the lower <b>frequencies</b> <b>of</b> Doppler sounds. An analysis <b>of</b> prerecorded <b>sounds</b> showed <b>a</b> difference in the lower <b>frequencies</b> <b>of</b> <b>a</b> <b>sound</b> considered to emanate from the umbilical blood flow of healthy fetuses with normally functioning placenta as compared to a pathological one. This might explain the difference between the <b>sounds</b> experienced by <b>an</b> operator. As a suggestion to extract more information than the maximum envelope, the minimum frequency envelope was extracted from pre-recorded clinical sounds. Based on the pilot tests presented here, this shows to be a promising strategy...|$|R
40|$|The {{fundamental}} <b>frequency</b> <b>of</b> <b>a</b> complex <b>sound</b> modulates {{the perceived}} duration <b>of</b> <b>a</b> <b>sound.</b> Higher pitch sounds are perceived longer compared to lower pitch sounds {{as shown by}} several independent studies since 1973. In this paper, the effect of language background is studied: native speakers of Finnish and German participated in a two alternative forced choice duration discrimination experiment where the duration and <b>frequency</b> <b>of</b> two sounds are randomly varied. The overall duration discrimination sensitivity was similar to both groups but the speakers of Finnish were influenced more by the pitch in their judgements. In addition, {{the difference in the}} two sounds‚Äô pitch period explained the response data better than the difference in pitch frequencies or the pitch interval. As the Finnish quantity system is known to employ both duration and pitch cues, the present results suggest that the speakers are shaped by the language environment even when the task is purely non-linguistic...|$|R
40|$|Using <b>a</b> {{nonlinear}} <b>sound</b> {{wave equation}} for a bubbly liquid {{in conjunction with}} an equation for bubble pulsation, we predict and experimentally demonstrate the appearance <b>of</b> <b>a</b> gap in the <b>frequency</b> spectrum <b>of</b> <b>a</b> <b>sound</b> wave propagating in a cavitation cloud comprising bubbles. For bubbles with <b>an</b> ambient radius <b>of</b> 100 Œºm, the calculations revealed that this gap corresponds to the phenomenon of sound wave localization. For bubbles with <b>an</b> ambient radius <b>of</b> 120 Œºm, this spectral gap relates to <b>a</b> forbidden band <b>of</b> the sound wave. In the experiment, we observed the predicted gap in the frequency spectrum in soda water; however, in tap water, no spectral gap was present because the bubbles were much smaller than 100 Œºm. Comment: It {{is the first time}} to have observed sound wave localization in a cavitation clou...|$|R
