896|1418|Public
5|$|The Nexus7 {{is powered}} by a 4,325mAh {{lithium-ion}} polymer battery that typically lasts 9.5hours and can be charged via micro-USB. In order to maximize the device's battery life, Asus engineers spent one month attempting to reduce electrical leakage by measuring heat and voltage at every point on the printed circuit board (PCB). The tablet includes a highly efficient power management integrated circuit designed by Maxim Integrated Products. Google claims the Nexus7's battery life allows 9hours of <b>HD</b> <b>video</b> playback, 10hours of web browsing or e-reading, and 300hours on standby.|$|E
5|$|The steel {{structure}} was 51 metres (167 feet) tall (doubling {{the size of}} the stadium set for The Rolling Stones' A Bigger Bang Tour, the previous record holder), was able to hold up to 200 tonnes underneath it, and required 120trucks to transport each of the three sets constructed to support the tour. Each leg of the structure contained its own sound system. The cost of each structure was between £15million and £20million ($23 million and $31 million, respectively). As a result, the tour was heavily insured. The size of the stage led to some problems with its construction in certain venues. The band paid $2million to raise the <b>HD</b> <b>video</b> screen in Cowboys Stadium for their concert in Arlington, and paid $3million to expand the Hippodrome de Montréal into a temporary stadium for their concert in Montreal. The 360° tour crew consisted of 137 touring production crew supplemented by over 120 hired locally. Daily costs of the production were approximately $750,000, not including the stage construction; the majority of this came from truck rentals, transportation, and staff wages. The tour was not expected to break even until the conclusion of the second leg.|$|E
25|$|There are two new <b>HD</b> <b>video</b> {{boards in}} the new South Endzone Upper Deck {{situated}} in each corner that were both installed in 2014.|$|E
40|$|The {{emerging}} <b>H.</b> 264 <b>video</b> {{coding standard}} offers a flexible coding algorithm {{for use in}} a range of applications from broadcast TV to low bitrate mobile applications. The flexibility comes with complexity. The <b>H.</b> 264 <b>video</b> coding algorithm is complex and requires substantial amount of resources for encoding and decoding. The resources required to decode an <b>H.</b> 264 <b>video</b> varies depending upon the encoding options used. Understanding the resources required to play the video at the receiver is especially important for mobile devices with limited amount of resources. In this paper, we present an overview of the <b>H.</b> 264 <b>video</b> coding standard, complexity analysis of <b>H.</b> 264 <b>video,</b> and its implications for resource management and adaptive coding for mobile devices. We examine high level bitstream characteristics and its suitability for estimating the bitstream complexity and receiver resource consumption. We show that high level metrics can be used to estimate receiver resource consumption with reasonable accuracy. 1...|$|R
40|$|Texas Instrument's TMS 320 DM 365 <b>H.</b> 264 <b>video</b> encoder is a {{flexible}} {{implementation of the}} <b>H.</b> 264 <b>video</b> encoder standard. It supports various profiles and features, each {{of which can be}} independently selected, set or controlled. This flexibility enables the TMS 320 DM 365 <b>H.</b> 264 <b>video</b> encoder to be used in a variety of different applications. This application report lists the different parameters that can be set to control the operation of the TMS 320 DM 365 <b>H.</b> 264 <b>video</b> encoder. Recommended values of the parameters for several applications scenarios, video conferencing, video security, video surveillance, video broadcast, video streaming, storage, local playback and digital video recorder (DVR) are provided. These recommended values enable the optimal usage of the video encoder for these application scenarios. ...|$|R
40|$|<b>H.</b> 265 <b>video</b> {{compression}} (also {{known as}} High Efficiency Video Encoding (HEVC)) promises to provide double the video quality at half the bandwidth, {{or the same}} quality at half the bandwidth of <b>h.</b> 264 <b>video</b> compression [1]. This study uses a Tektronix PQA 500 to determine the video quality gains by using h. 265 encoding. This study also compares two video encoders to see how different implementations of h. 264 and <b>h.</b> 265 impact <b>video</b> quality at various bandwidths...|$|R
25|$|BBC iPlayer {{was one of}} four service {{available}} at the launch of YouView in July 2012. At launch the BBC iPlayer app contained options to resume watching recent programmes, access favourites, browse the most popular shows, find similar programmes or more episodes, included integrated search and was the only service to feature <b>HD</b> <b>video.</b>|$|E
25|$|In 2013, the Iowa Board of Regents {{approved}} an $8 million upgrade of Kinnick Stadium's {{video and}} sound systems. These upgrades include {{the installation of}} new <b>HD</b> <b>video</b> displays in both {{the north and south}} endzones, as well as a new HD ribbon display above the north endzone. The upgrades were completed by the start of the 2013 football season.|$|E
25|$|The {{emergence}} of high definition players followed {{the entry of}} HDTV televisions into the mainstream market in the mid-2000s. Consumer-grade high definition players required an inexpensive storage medium capable of holding the larger amount of data needed for <b>HD</b> <b>video.</b> The breakthrough came with Shuji Nakamura's invention of the blue laser diode, whose shorter wavelength {{opened the door to}} higher density optical media following a six-year patent dispute.|$|E
40|$|While trivial {{research}} has been conducted in watermarking and authentication of <b>H.</b> 264 <b>video</b> in recent years, most techniques require cascaded operations within a video adaptation scenario. In this paper, we propose an authentication scheme for adapted <b>H.</b> 264 <b>video</b> content to detect integrity at the receiver’s side without the need for cascaded operations. The proposed scheme utilizes MPEG- 21 gBSD for hard authentication of <b>H.</b> 264 <b>video</b> in the compressed domain and does not necessitate any cascaded decompression and recompression. The design uses contentbased authentication which is derived from a hash value. The authentication data is embedded as a fragile watermark, and the marking space is selected during the adaptation process of the <b>H.</b> 264 <b>video</b> by parsing the gBSD. The authentication information is embedded in already encoded videos during adaptation. Proof of concept and performance evaluation is also presented. 1...|$|R
5000|$|PureVideo Feature Set <b>H</b> {{hardware}} <b>video</b> decoding HEVC Main10(10bit), Main12(12bit) and VP9 hardware decoding.|$|R
30|$|This paper {{proposes a}} cross-layer design that is {{comprised}} of mechanisms in the application, transport, and MAC layers. The design is based on mapping of the MAC scheduling services to different partitions and priorities provided by the H. 264 encoding scheme. The scheduling services are provided using our previously published scheduling algorithm, controlled access phase scheduling (CAPS) [12], and its modified version for partitioned <b>H.</b> 264 <b>video.</b> An enhancement based on aggregation of some H. 264 partitions is also proposed. A summary {{of some of the}} mechanisms to deliver partitioned <b>H.</b> 264 <b>video</b> over WLANs is presented in our work in [13]. This paper elaborates on solutions presented in [13]and considers other issues such as multirate operation. It also presents modifications to CAPS for partitioned <b>H.</b> 264 <b>video.</b> In addition to these MAC layer mechanisms, the effect of PHY link adaptation and its possible customization for partitioned <b>H.</b> 264 <b>video</b> are investigated in this article.|$|R
25|$|Windows Movie Maker {{supports}} smooth Direct3D-based {{effects and}} transitions, editing and outputting <b>HD</b> <b>video,</b> importing recorded DVR-MS format videos, {{as well as}} burning the output movie on a CD. In Windows Vista Home Premium and Ultimate editions, it can import HDV video from camcorders and output video to Windows DVD Maker for creating DVD-Video discs. New effects and transitions have been added. All Windows Vista Movie Maker versions require pixel shader hardware support.|$|E
25|$|In 2009, OLPC {{announced}} an updated XO (dubbed XO-1.5) that {{takes advantage of}} the latest component technologies. The XO-1.5 includes a new VIA C7-M processor and a new chipset providing a 3D graphics engine and an <b>HD</b> <b>video</b> decoder. It has 1GB of RAM and built-in storage of 4 GB, with an option for 8 GB. The XO-1.5 uses the same display, and a network wireless interface with half the power dissipation.|$|E
25|$|Photography and {{videography}} are the mainstays of recording, {{which has}} become much more convenient with the advent of reasonably priced digital still and <b>HD</b> <b>video</b> cameras. Cameras, including video cameras can be provided with special underwater housings that enable them to be used for underwater videography. Low visibility underwater and distortion of image due to refraction mean that perspective photographs can be difficult to obtain. However, it is possible to take a series of photographs at adjacent points and then combined into a single photomontage or photomosaic image of the whole site. 3D photogrammetry has also become a very popular way to image underwater cultural materials and shipwreck sites.|$|E
40|$|Abstract—The {{performance}} evaluation of video transport mechanisms becomes increasingly important as encoded video accounts for growing {{portions of the}} network traffic. Compared to the widely studied MPEG- 4 encoded video, the recently adopted <b>H.</b> 264 <b>video</b> coding standards include novel mechanisms, such as hierarchical B frame prediction structures and highly efficient quality scalable coding, that {{have important implications for}} network transport. This tutorial introduces a trace-based evaluation methodology for the network transport of <b>H.</b> 264 encoded <b>video.</b> We first give an overview of <b>H.</b> 264 <b>video</b> coding, and then present the trace structures for capturing the characteristics of <b>H.</b> 264 encoded <b>video.</b> We give an overview of the typical video traffic and quality characteristics of <b>H.</b> 264 encoded <b>video.</b> Finally, we explain how to account for the H. 264 specific coding mechanisms, such as hierarchical B frames, in networking studies. Index Terms—H. 264 encoded video, hierarchical B frames, medium grain scalability (MGS), network transport, simulation, traffic variability, video trace...|$|R
40|$|Abstract [...] -The new <b>video</b> {{compression}} standard, <b>H.</b> 264 /MPEG- 4 AVC, promises better rate-distortion {{performance and}} compression efficiency over all its predecessors. However, the computational {{complexity of the}} <b>H.</b> 264 <b>video</b> codec is increased drastically because of inclusion of many new coding, error-resilience and network friendly tools and techniques, which results in it’s implementation for low-end mobile and hand held devices practically difficult. This paper presents fully optimized H. 264 baseline profile video encoder to show its suitability for mobile and hand-held applications. Various algorithmic and implementation techniques to optimize <b>H.</b> 264 <b>video</b> encoder at low bit-rates are described. The complexity requirements for <b>H.</b> 264 <b>video</b> encoder on various low-end processors are presented...|$|R
40|$|With the {{emergence}} of the International Telecommunication Union’s (ITU) <b>H.</b> 263 <b>video</b> coding standard, real-time visual communications over low-bandwidth, low-error-rate wired telephone networks is now possible. Unfortunately, {{because of the nature of}} the algorithms employed within this compression standard and due to the inherent limitations of the radio environment, <b>H.</b> 263 coded <b>video</b> performs poorly when employed over bandwidth-limited, error-prone wireless channels. In this paper we propose an intuitive, low-complexity, pre-compression scheme that improves the performance of <b>H.</b> 263 coded <b>video</b> over radio channels. In our scheme video frames are spatially segmented into regions of different perceptual importance before being compressed independently with the H. 263 encoder. This allows us to apply unequal error protection and prioritized transmission to achieve at least a minimum temporal resolution at the receiver. Additionally, with this technique, both spatial and temporal error propagation is limited and through intra-frame statistical multiplexing the reserved transmission bandwidth is utilized optimally. Simulation results demonstrate that in the presence of severe error conditions and severe bandwidth constraints, our modified <b>H.</b> 263 <b>video</b> codec exhibits better error concealment, better temporal resolution, and better bandwidth utilization properties than the original <b>H.</b> 263 <b>video</b> codec 1 1...|$|R
25|$|While {{for years}} Dish Network has used {{standard}} MPEG-2 for broadcasting, {{the addition of}} bandwidth-intensive HDTV in a limited-bandwidth world {{has called for a}} change to an H.264/MPEG-4 AVC system. Dish Network announced as of February 1, 2006, that all new HDTV channels would be available in H.264 format only, while maintaining the current lineup as MPEG-2. Dish Network intends to eventually convert the entire platform to H.264 in order to provide more channels to subscribers. In 2007, Dish Network reduced the resolution of 1080-line channels from 1920x1080 to 1440x1080. Reducing horizontal resolution and/or data rate of <b>HD</b> <b>video</b> is known as HD Lite and is practiced by other TV providers as well.|$|E
25|$|As the Sixth Generation gained {{international}} exposure, many subsequent {{movies were}} joint ventures and projects with international backers, but remained quite resolutely low-key and low budget. Jia's Platform (2000) was funded {{in part by}} Takeshi Kitano's production house, while his Still Life was shot on <b>HD</b> <b>video.</b> Still Life was a surprise addition and Golden Lion winner of the 2006 Venice International Film Festival. Still Life, which concerns provincial workers around the Three Gorges region, sharply contrasts with the works of Fifth Generation Chinese directors like Zhang Yimou and Chen Kaige who were at the time producing House of Flying Daggers (2004) and The Promise (2005). It featured no star of international renown and was acted mostly by non-professionals.|$|E
25|$|Major opera {{companies}} have begun presenting their performances in local cinemas throughout the United States {{and many other}} countries. The Metropolitan Opera {{began a series of}} live high-definition video transmissions to cinemas around the world in 2006. In 2007, Met performances were shown in over 424 theaters in 350 U.S. cities. La bohème went out to 671 screens worldwide. San Francisco Opera began prerecorded video transmissions in March 2008. As of June 2008, approximately 125 theaters in 117 U.S. cities carry the showings. The <b>HD</b> <b>video</b> opera transmissions are presented via the same HD digital cinema projectors used for major Hollywood films. European opera houses and festivals including the Royal Opera in London, La Scala in Milan, the Salzburg Festival, La Fenice in Venice, and the Maggio Musicale in Florence have also transmitted their productions to theaters in cities around the world since 2006, including 90 cities in the U.S.|$|E
30|$|One of {{the primary}} issues with <b>H.</b> 264 <b>video</b> {{applications}} lies on how to realize the profiles, levels, tools, and algorithms featured by H. 264 /AVC draft. Thanks to the rapid development of FPGA [8] techniques and embedded software system design and verification tools, the designers can utilize the hardware-software (HW/SW) codesign environment {{which is based on}} the reconfigurable and programmable FPGA infrastructure as a dedicated solution for <b>H.</b> 264 <b>video</b> applications [9, 10].|$|R
30|$|In general, {{the main}} {{bottleneck}} of <b>H.</b> 264 <b>video</b> encoding {{is a combination}} of multiple reference frames and large search ranges.|$|R
40|$|Abstract: The {{recently}} developed <b>H.</b> 264 <b>video</b> standard achieves efficient encoding over a bandwidth {{ranging from a}} few kilobits per second to several megabits per second. Hence, transporting <b>H.</b> 264 <b>video</b> {{is expected to be}} an important component of many networks multimedia services. However, due to lack of QoS support reaction networks, the basic GMPLS network management is not sufficient to deliver real-time traffic. The delivery should be augmented by appropriate mechanisms to adjust the QoS parameters. In this paper we address <b>H.</b> 264 <b>video</b> transmission over GMPLS networks by proposing a robust policy based control architecture that leverages the inherent H. 264 error resilience tools. Keywords: DiffServ; GMPLS; policy; policy based control; MPEG 4, H. 264; video; self-adaptation. Reference to this paper should be made as follows: Moungla, H. and Krief, F. (2008...|$|R
25|$|Chasers heavily utilize still {{photography}} {{since the}} beginning. Videography gained prominence by the 1990s {{into the early}} 2000s but a resurgence of photography occurred {{with the advent of}} affordable and versatile digital SLR (DSLR) cameras. Prior to this, 35 mm SLR print and slide film formats were mostly used, along with some medium format cameras. In the late 2000s, mobile phone 3G data networks became fast enough to allow live streaming video from chasers using webcams. This live imagery is frequently used by the media, as well as NWS meteorologists, emergency managers, and the general public for direct ground truth information, and it provides video sales opportunities to chasers. Also by this time, camcorders using memory cards to record video began to be adopted. Digital video had been around for years but was recorded on tape, whereas solid-state is random access rather than sequential access (linear) and has no moving parts. Late in the 2000s <b>HD</b> <b>video</b> began to overtake SD (which had been NTSC in North America) in usage as prices came down and performance increased (initially there were low-light and sporadic aliasing problems due to chip and sensor limitations). By the mid-2010s 4K cameras were increasingly in use. Tripods are used by those seeking crisp professional photo and video imagery and also enable chasers to tend to other activities.|$|E
500|$|Although often <b>HD</b> <b>video</b> capable cameras {{include an}} HDMI {{interface}} for playback or even live preview, the image processor and the video processor of cameras usable for uncompressed video {{must be able}} to deliver the full image resolution at the specified frame rate in realtime without any missing frames causing jitter. Therefore, usable uncompressed video out of HDMI is often called [...] "clean HDMI".|$|E
500|$|The revised tablet adds front- and {{rear-facing}} cameras, {{which allow}} FaceTime video calls with other iPad 2s, {{the third generation}} iPad, iPhone 4 and 4S, fourth-generation iPod Touch and Macintosh computers (running Mac OS X 10.6.6 or later with a webcam). The 0.3 MP front camera shoots VGA-quality 30 frame/s video and VGA-quality still photos. The 0.7 MP back camera can shoot 720p <b>HD</b> <b>video</b> at 30 frame/s and has a 5× times digital zoom. Both shoot photo in a [...] fullscreen aspect ratio. The rear camera shoots video in 16:9 widescreen to match the 720p standard, although only the central 4:3 part of the recording is shown on the screen during recording. The forward-facing camera shoots in 4:3.|$|E
3000|$|... resolution. The {{experiments}} {{were based on}} the video encoders available in the Intel Integrated Performance Primitives (IPP) SDK. Face detection for the experiments was done manually and face regions are input to the system. As in the compression independence experiments, regions of interest are identified, encrypted and encoded using <b>H.</b> 264 <b>video</b> encoding. The <b>H.</b> 264 <b>video</b> is then transcoded to MPEG- 2 and MPEG- 4 —simulating a scenario for legacy codec support in video surveillance system.|$|R
40|$|ITU-T Rec. <b>H.</b> 263 <b>Video</b> (<b>H.</b> 263 +) This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (1998). All Rights Reserved...|$|R
40|$|As {{the result}} of the work done as a project for the Image Processing and Communications course in the winter of 1997, this report {{describes}} the implementation of a Java <b>H.</b> 263 <b>video</b> decoder. The Decoder allows <b>H.</b> 263 coded <b>video</b> streams to be decoded and displayed in a window on any Java enabled machine. It shows that it's possible to create a conferencing environment just using Java Enabled Browsers...|$|R
500|$|HDMI {{uses the}} Consumer Electronics Association/Electronic Industries Alliance 861 standards. HDMI 1.0 to HDMI 1.2a uses the EIA/CEA-861-B video standard, HDMI 1.3 uses the CEA-861-D video standard, and HDMI 1.4 uses the CEA-861-E video standard. The CEA-861-E {{document}} defines [...] "video formats and waveforms; colorimetry and quantization; transport of compressed and uncompressed, {{as well as}} Linear Pulse Code Modulation (LPCM), audio; carriage of auxiliary data; and implementations of the Video Electronics Standards Association (VESA) Enhanced Extended Display Identification Data Standard (E-EDID)". [...] On July 15, 2013, the CEA announced the publication of CEA-861-F which is a standard {{that can be used}} by interfaces such as DVI, HDMI, and LVDS. CEA-861-F adds the ability to transmit several Ultra <b>HD</b> <b>video</b> formats and additional color spaces.|$|E
500|$|Arrested Development uses several {{elements}} {{which were}} rare {{at the time}} for American live-action sitcoms. It was shot on location and in <b>HD</b> <b>video</b> (at 24 frames per second) with multiple cameras, parodying tactics often employed in documentary film and reality television, straying from the [...] "fixed-set, studio audience, laugh track" [...] style long dominant in comedy production. The show also makes heavy use of cutaway gags, supplementing the narrative with visual punchlines like security camera footage, Bluth family photos, website screenshots, archive films, and flashbacks. An omniscient third-person narrator (producer Ron Howard) ties together the multiple plot threads running through each episode, while humorously undercutting and commenting on the characters. Arrested Development also developed a unique self-referentiality through use of in-jokes that evolved over multiple episodes, which rewarded longtime viewership (and in turn may have discouraged new viewers {{and contributed to the}} show's ratings difficulties).|$|E
500|$|Blu-ray permits {{secondary}} audio decoding, {{whereby the}} disc content {{can tell the}} player to mix multiple audio sources together before final output. Some Blu-ray and HD DVD players can decode all of the audio codecs internally and can output LPCM audio over HDMI. Multichannel LPCM can be transported over an HDMI connection, {{and as long as}} the AV receiver implements multichannel LPCM audio over HDMI and implements HDCP, the audio reproduction is equal in resolution to HDMI 1.3 bitstream output. Some low-cost AV receivers, such as the Onkyo TX-SR506, do not allow audio processing over HDMI and are labelled as [...] "HDMI pass through" [...] devices. [...] Virtually all modern AV Receivers now offer HDMI 1.4 inputs and output(s) with processing for all of the audio formats offered by Blu-ray Discs and other <b>HD</b> <b>video</b> sources. [...] During 2014 several manufacturers introduced premium AV Receivers that include one, or multiple, HDMI 2.0 inputs along with a HDMI 2.0 output(s). However, it was not until 2015 that most major manufacturers of AV receivers also included support for HDCP 2.2 as will be needed to support certain high quality 4K/UHD video sources, such as the upcoming Blu-ray 4K/UHD players.|$|E
40|$|We {{develop a}} {{framework}} that uses visual attention analysis combined with temporal coherence to detect the attended region from a <b>H.</b> 264 <b>video</b> bitstream, and display it on a small screen. A visual attention module based upon Walther and Koch's model gives us the attended region in I-frames. We propose a temporal coherence matching framework that uses the motion information in P-frames to extend the attended region over the <b>H.</b> 264 <b>video</b> sequence. Evaluations show encouraging results with over 80 % successful detection rate for objects of interest, and 85 % respondents claiming satisfactory output...|$|R
40|$|Abstract. A novel {{watermarking}} {{technique to}} authenticate <b>video</b> of <b>H.</b> 264 {{is presented in}} this paper, using Uniform Content Locator (UCL) to semantically indexing video content and dual watermarks to preserve and enhance video content integrity and authentication. UCL index information is firstly extracted from video content and is formatted as semantic watermark to be embedded in video content. The UCL watermark is regarded as robust watermark and is then embedded into medium frequencies of DCT-coefficients of <b>H.</b> 264 <b>video</b> I-frames {{in order to protect}} video attributes property (e. g., video author, copyright, content category). Features information obtained from the previously watermarked DCT-coefficients are treated as fragile watermark and are embedded into the motion vectors of <b>H.</b> 264 <b>video</b> P-frames in order to ensure video secrecies (e. g., video integrity, authentication). Experiments demonstrate that this proposed technique can fulfill the requirements of <b>H.</b> 264 <b>video</b> authentication and has negligible effects on video code rate change and content distortion...|$|R
40|$|Motion Estimation (ME) is {{the most}} {{computationally}} intensive part of video compression systems. Multiple reference frame (MRF) ME used in H. 264 standard increases the video coding efficiency {{at the expense of}} increased computational complexity and power consumption. Therefore, in this paper, we present a reconfigurable baseline <b>H.</b> 264 <b>video</b> encoder hardware in which the number of reference frames used for MRF ME can be configured based on the application requirements in order to trade-off video coding efficiency and power consumption. The proposed <b>H.</b> 264 <b>video</b> encoder hardware is based on an existing low cost H. 264 intra frame coder hardware and it includes new reconfigurable MRF ME, mode decision and motion compensation hardware. The proposed <b>H.</b> 264 <b>video</b> encoder hardware is capable of processing 55 CIF (352 × 288) frames per second and its power consumption ranges between 115 mW and 235 mW depending on the number of reference frames used for MRF ME...|$|R
