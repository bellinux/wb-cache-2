4|1331|Public
40|$|Blast is an {{automatic}} verification tool for checking temporal safety properties of C programs. Blast isbasedonlazypredicate abstraction driven by interpolation-based predicate discovery. In this paper, we present the Blast specification language. The language specifies program properties at {{two levels of}} precision. At the lower level, monitor automata are used to specify temporal safety properties of program <b>executions</b> (<b>traces).</b> At the higher level, relational reachability queries over program locations are used to combine lower-level trace properties. The two-level specification language {{can be used to}} break down a verification task into several independent calls of the model-checking engine. In this way, each call to the model checker may have to analyze only part of the program, or part of the specification, and may thus succeed in a reduction of the number of predicates needed for the analysis. In addition, the two-level specification language provides a means for structuring and maintaining specifications...|$|E
40|$|International audienceExecutable Domain Specific Modeling Languages (xDSML) opens many {{possibilities}} {{in terms of}} early verification and validation (V&V) of systems, {{including the use of}} dynamic V&V approaches. Such approaches rely on the notion of execution trace, i. e. the evolution of a system during a run. To benefit from dynamic V&V approaches, it is therefore necessary to characterize what is the structure of the <b>executions</b> <b>traces</b> of a given xDSML. Our goal is to provide an approach to design trace metamodels for xDSMLs. We identify seven problems that must be considered when modeling execution traces, including concurrency, modularity, and scalability. Then we present our envisioned approach to design scalable multidimensional trace metamodels for xDSMLs. Our work in progress relies on the dimensions of a trace (i. e. subsets of mu- table elements of the traced model) to provide an original structure that faces the identified problems, along with a trace API to manipulate them...|$|E
40|$|Existing {{business}} process drift detection methods {{do not work}} with event streams. As such, {{they are designed to}} detect inter-trace drifts only, i. e. drifts that occur between complete process <b>executions</b> (<b>traces),</b> as recorded in event logs. However, process drift may also occur during the execution of a process, and may impact ongoing executions. Existing methods either do not detect such intra-trace drifts, or detect them with a long delay. Moreover, they do not perform well with unpredictable processes, i. e. processes whose logs exhibit a high number of distinct executions to the total number of executions. We address these two issues by proposing a fully automated and scalable method for online detection of process drift from event streams. We perform statistical tests over distributions of behavioral relations between events, as observed in two adjacent windows of adaptive size, sliding along with the stream. An extensive evaluation on synthetic and real-life logs shows that our method is fast and accurate in the detection of typical change patterns, and performs significantly better than the state of the art...|$|E
40|$|Les rapports de {{recherche}} du LIG - ISSN: 2105 - 0422 Recent technology {{advances have}} {{made possible the}} retrieval of <b>execution</b> <b>traces</b> on microcontrollers. However, even after a short execution time of the embedded program, the collected <b>execution</b> <b>trace</b> contains {{a huge amount of}} data. This is due to the cyclic nature of embedded programs. The huge amount of data makes extremely difficult and time-consuming the understanding of the program behavior. Software engineers need a way to get a quick understanding of <b>execution</b> <b>traces.</b> In this paper, we present an approach based on an improvement of the Sequitur algorithm to compress large <b>execution</b> <b>traces</b> of microcontrollers. By leveraging both cycles and repetitions present in such <b>execution</b> <b>traces,</b> our approach offers a compact and accurate compression of <b>execution</b> <b>traces.</b> This compression may be used by software engineers to understand the behavior of the system, for instance, identifying cycles that appears most often in the trace or comparing different cycles. Our evaluations give two major results. On one hand our approach gives high compression rate on microcontroller <b>execution</b> <b>traces.</b> On the other hand software engineers mostly agree that generated outputs (compressions) may help reviewing and understanding <b>execution</b> <b>traces...</b>|$|R
40|$|Recent {{technology}} {{advances have}} {{made possible the}} retrieval of <b>execution</b> <b>traces</b> on microcontrollers. However, even after a short execution time of the embedded program, the collected <b>execution</b> <b>trace</b> contains {{a huge amount of}} data. This is due to the cyclic nature of embedded programs. The huge amount of data makes extremely difficult and time-consuming the understanding of the program behavior. Software engineers need a way to get a quick understanding of <b>execution</b> <b>traces.</b> In this paper, we present an approach based on an improvement of the Sequitur algorithm to compress large <b>execution</b> <b>traces</b> of microcontrollers. By leveraging both cycles and repetitions present in such <b>execution</b> <b>traces,</b> our approach offers a compact and accurate compression of <b>execution</b> <b>traces.</b> This compression may be used by software engineers to understand the behavior of the system, for instance, identifying cycles that appears most often in the trace or comparing different cycles. Our evaluations give two major results. On one hand our approach gives high compression rate on microcontroller <b>execution</b> <b>traces.</b> On the other hand software engineers mostly agree that generated outputs (compressions) may help reviewing and understanding <b>execution</b> <b>traces...</b>|$|R
40|$|An <b>execution</b> <b>trace</b> {{contains}} {{a description of}} everything that happened during an execution of a program. <b>Execution</b> <b>traces</b> are useful, because they can help software engineers understand code, resulting {{in a variety of}} applications such as debugging software, or more effective software reuse. Unfortunately, <b>execution</b> <b>traces</b> are also complex, typically containing hundreds of thousands of events for medium size computer programs, and more for large scale programs. We have developed an <b>execution</b> <b>trace</b> visualisation tool, called VET, that helps programmers manage the complexity of <b>execution</b> <b>traces.</b> VET is also plugin based. Expert users of VET can add new visualisations and new filters, without changing VET's main code base...|$|R
40|$|For this project, {{we worked}} with Andreas Stuhlmüller to develop and {{implement}} a new inference algorithm for the Church programming language. Church is a programming language that extends Scheme with random primitives. For more details, see the original Church paper [1] Andreas developed the basic algorithm and we did most of the implementation work, also working with Andreas to extend the algorithm. 2 <b>Execution</b> <b>Traces</b> In Church, the evaluation of an expression produces an <b>execution</b> <b>trace.</b> Intuitively, the <b>execution</b> <b>trace</b> is the full tree of evaluations that was necessary to evaluate the expression. An <b>execution</b> <b>trace</b> consists of the following: • The expression that was evaluated • The environment (mapping of variables to values) it was evaluated in • The return value • A list of sub-traces The sub-traces item requires explanation. A typical Scheme/Church interpreter will have a recursive eval function. For example, to evaluate an application like (+ 1 2), it is first necessary to evaluate the expression +, then the expression 1, then the expression 2. All of these evaluations have their own <b>execution</b> <b>traces,</b> which are included in the <b>execution</b> <b>trace</b> for the expression (+ 1 2). In addition to the function and the argument, sometimes additional expressions are evaluated during the evaluation of an application expression. For example, to evaluate the expression (lambda (x) (* x 2)) 1), it is first necessary to evaluate (lambda (x) (* x 2)), then 1, then (* x 2) in the environment {’x’: 1]}. Therefore, the sub-traces for <b>execution</b> <b>trace</b> for (lambda (x) (* x 2)) 1) will consist of the <b>execution</b> <b>traces</b> for these 3 expressions. 3 Sampling Traces Our inference engine samples an <b>execution</b> <b>trace,</b> given an expression, its environment, and its return value. Intuitively, it “imagines ” a sequence of evaluations that would have resulted in the provided return value. For example, if the expression is (+ (gaussian) (gaussian)), and the return value is 1. 32, then in the sampled <b>execution</b> <b>trace,</b> perhaps the first (gaussian) expression returned 0. 96 and the second returned 0. 36. Ideally, the sampling distribution Q(trace|result) would exactly match the posterior P (trace|result). However, due to our approximations, they will not be equal. Therefore, we attach an importance weight to each sample. The im...|$|R
40|$|Abstract. Recent {{technological}} advances have {{made possible the}} re-trieval of <b>execution</b> <b>traces</b> on microcontrollers. However, the huge amount of data in the collected trace makes the trace analysis extremely diffi-cult and time-consuming. In this paper, by leveraging both cycles and repetitions present in an <b>execution</b> <b>trace,</b> we present an approach which offers a compact and accurate trace compression. This compression may be used during the trace analysis without decompression, notably for identifying repeated cycles or comparing different cycles. The evalua-tion demonstrates that our approach reaches high compression ratios on microcontroller <b>execution</b> <b>traces.</b> ...|$|R
40|$|This {{dissertation}} proposes generalized {{techniques to}} support software performance analysis using system <b>execution</b> <b>traces</b> {{in the absence}} of software development artifacts such as source code. The proposed techniques do not require modifications to the source code, or to the software binaries, for the purpose of software analysis (non-intrusive). The proposed techniques are also not tightly coupled to the architecture specific details of the system being analyzed. This dissertation extends the current techniques of using system <b>execution</b> <b>traces</b> to evaluate software performance properties, such as response times, service times. The dissertation also proposes a novel technique to auto-construct a dataflow model from the system <b>execution</b> <b>trace,</b> which will be useful in evaluating software performance properties. Finally, it showcases how we can use <b>execution</b> <b>traces</b> in a novel technique to detect Excessive Dynamic Memory Allocations software performance anti-pattern. This is the first attempt, according to the author 2 ̆ 7 s best knowledge, of a technique to detect automatically the excessive dynamic memory allocations anti-pattern. The contributions from this dissertation will ease the laborious process of software performance analysis and provide a foundation for helping software developers quickly locate the causes for negative performance results via <b>execution</b> <b>traces.</b> ...|$|R
40|$|Error {{explanation}} [GV 03, GCKS 06] is {{a formal}} approach to automate diagnosis of software programs {{with the aid}} of a Satisfiability (SAT) -based model checker. Firstly, the semantics of the program is modeled as a Finite State Machine (FSM) and is encoded into an instance of the SAT problem [CKL 04]. Given a specification expressed in a formal logic which does not hold on the FSM, error explanation utilizes the model checker to produce a pair of similar failing and successful <b>execution</b> <b>traces</b> and highlights the differences of the <b>execution</b> <b>traces</b> as a possible explanation of the error. Thus, an explanation corresponds to a set of locations of the program source. More precisely, in SAT-based model checking <b>execution</b> <b>traces</b> correspond to assignments of logic variables in the SAT instance. The logic variables are used to capture the possible valuations of the program variables. The domains of the logic variables depend on the logic in use. Similarity of <b>execution</b> <b>traces</b> can then be expressed leveraging a distance metric which counts the number of values for which the two <b>execution</b> <b>traces</b> are different. Suppose (A, B) is a pair of a failing and a successful <b>execution</b> <b>trace</b> which correspond to the sequences a = (a 1, [...] ., an) and b = (b 1, [...] ., bn) of logic variables, respectively. The distance metric is then defined as ∑ n i= 1 (1 − δaibi...|$|R
40|$|This report {{presents}} the research {{carried out in}} the area of software product quality modelling. Its main endeavour is to consider software product quality with regard to maintainability. Supporting this aim, <b>execution</b> <b>tracing</b> quality, which is a neglected property of the software product quality at present in the quality frameworks under investigation, needs to be described by a model that offers possibilities to link to the overall software product quality frameworks. The report includes concise description of the research objectives: (1) the thorough investigation of software product quality frameworks {{from the point of view}} of the quality property analysability with regard to execution tracing; (2) moreover, extension possibilities of software product quality frameworks, and (3) a pilot quality model developed for <b>execution</b> <b>tracing</b> quality, which is capable to capture subjective uncertainty associated with the software quality measurement. The report closes with concluding remarks: (1) the present software quality frameworks do not exhibit any property to describe <b>execution</b> <b>tracing</b> quality, (2) <b>execution</b> <b>tracing</b> has a significant impact on the analysability of software systems that increases with the complexity, and (3) the uncertainty associated with <b>execution</b> <b>tracing</b> quality can adequately be expressed by type- 1 fuzzy logic. The section potential future work outlines directions into which the research could be continued. Findings of the research were summarized in two research reports, which were also incorporated in the thesis, and submitted for publication: 1. 	Tamas Galli, Francisco Chiclana, Jenny Carter, Helge Janicke, “Towards Introducing <b>Execution</b> <b>Tracing</b> to Software Product Quality Frameworks,” Acta Polytechnica Hungarica, vol. 11, no. 3, pp. 5 - 24, 2014. doi: 10. 12700 /APH. 11. 03. 2014. 03. 1 2. 	Tamas Galli, Francisco Chiclana, Jenny Carter, Helge Janicke “Modelling <b>Execution</b> <b>Tracing</b> Quality by Means of Type- 1 Fuzzy Logic,” Acta Polytechnica Hungarica, vol. 10, no. 8, pp. 49 - 67, 2013. doi: 10. 12700 /APH. 10. 08. 2013. 8. ...|$|R
40|$|White-box testing allows {{developers}} {{to determine whether}} or not a program is partially consistent with its specified behavior and design through the examination of intermediate values of variables during program execution. These intermediate values are often recorded as an <b>execution</b> <b>trace</b> produced by monitoring code inserted into the program. After program execution, the values in an <b>execution</b> <b>trace</b> are compared to values predicted by the specified behavior and design. Inconsistencies between predicted and actual values can lead to the discovery of errors in the specification and its implementation. This paper describes an approach to (1) verify the <b>execution</b> <b>traces</b> created by monitoring statements during white-box testing using a model checker as a semantic tableau; (2) organize multiple <b>execution</b> <b>traces</b> into distinct equivalence partitions based on requirements specifications written in linear temporal logic (LTL); and (3) use the counter-example generation mechanisms found in most model-checker tools to generate new testcases for unpopulated equivalence partitions...|$|R
40|$|This paper studies, for {{the first}} time, the {{management}} of type information for an important class of semi-structured data: nested DAGs (Directed Acyclic Graphs) that describe <b>execution</b> <b>traces</b> of business processes (BPs for short). Specifically, we consider here type inference and type checking for queries over BP <b>execution</b> <b>traces.</b> The queries that we consider select portions of the traces that are {{of interest to the}} user; the types describe the possible shape of the <b>execution</b> <b>traces</b> in the input/output of the query. We formally define and characterize here three common classes of BP <b>execution</b> <b>traces</b> and their respective notions of type inference and type checking. We study the complexity of the two problems for query languages of varying expressive power and present efficient type inference/checking algorithms whenever possible. Our analysis offers a nearly complete picture of which combinations of trace classes and query features lead to PTIME algorithms and which to NP-complete or undecidable problems. 1...|$|R
40|$|As {{planners}} and their environments become increasingly complex, planner behavior becomes increasingly di cult to understand. We {{often do not}} understand what causes them to fail, {{so that we can}} debug their failures, and we may not understand what allows them to succeed, so that we can design the next generation. This paper describes a partially automated methodology for understanding planner behavior over long periods of time. The methodology, called Dependency Interpretation, uses statistical dependency detection to identify interesting patterns of behavior in <b>execution</b> <b>traces</b> and interprets the patterns using a weak model of the planner's interaction with its environment to explain how the patterns might be caused by the planner. Dependency Interpretation has been applied to identify possible causes of plan failures in the Phoenix planner. By analyzing four sets of <b>execution</b> <b>traces</b> gathered from about 400 runs of the Phoenix planner, we showed that the statistical dependencies describe patterns of behavior that are sensitive totheversion of the planner and to increasing temporal separation between events, and that dependency detection degrades predictably as the number of available <b>execution</b> <b>traces</b> decreases and as noise is introduced in the <b>execution</b> <b>traces.</b> Dependency Interpretation is appropriate when a complete and correct model of the planner and environment is not available, but <b>execution</b> <b>traces</b> are available...|$|R
40|$|Callbacks are {{essential}} in many programming environments, but drastically complicate program understanding and reasoning because they allow to mutate object's local states by external objects in unexpected fashions, thus breaking modularity. The famous DAO bug in the cryptocurrency framework Ethereum, employed callbacks to steal $ 150 M. We define {{the notion of}} Effectively Callback Free (ECF) objects {{in order to allow}} callbacks without preventing modular reasoning. An object is ECF in a given <b>execution</b> <b>trace</b> if there exists an equivalent <b>execution</b> <b>trace</b> without callbacks to this object. An object is ECF if it is ECF in every possible <b>execution</b> <b>trace.</b> We study the decidability of dynamically checking ECF in a given <b>execution</b> <b>trace</b> and statically checking if an object is ECF. We also show that dynamically checking ECF in Ethereum is feasible and can be done online. By running the history of all <b>execution</b> <b>traces</b> in Ethereum, we were able to verify that virtually all existing contracts, excluding the DAO or contracts with similar known vulnerabilities, are ECF. Finally, we show that ECF, whether it is verified dynamically or statically, enables modular reasoning about objects with encapsulated state. Comment: 31 pages. Technical Report for the paper presented in POPL' 18 with the same titl...|$|R
5000|$|A {{workflow}} <b>trace</b> or <b>execution</b> <b>trace</b> is {{a string}} over an alphabet [...] of tasks.|$|R
5000|$|A round is the {{shortest}} <b>execution</b> <b>trace</b> in which each processor executes at least one step.|$|R
40|$|We {{illustrate}} {{the effectiveness of}} our techniques in POTA on test cases such as the General Inter-ORB Protocol (GIOP) [18]. POTA also contains a module that translates <b>execution</b> <b>traces</b> to Promela [16] (input language SPIN). This module enables us to compare our results on <b>execution</b> <b>traces</b> with SPIN. In some cases, {{we were able to}} verify traces with 250 processes compared to only 10 processes using SPIN...|$|R
40|$|Stochastic {{simulations}} frequently exhibit {{behaviors that}} are difficult to recreate and analyze, owing largely to the stochastics themselves, and consequent program dependency chains that can defy human reasoning capabilities. We present a novel approach called Markov Chain <b>Execution</b> <b>Traces</b> (MCETs) for efficiently representing sampled stochastic simulation <b>execution</b> <b>traces</b> and ultimately driving semiautomated analysis methods that require accurate, efficiently generated candidate <b>execution</b> <b>traces.</b> The MCET approach is evaluated, using new and established measures, against both additional novel and existing approaches for computing dynamic program slices in stochastic simulations. ������������������rformance is established. Finally, a description of how users can apply MCETs to their own stochastic simulations and a discussion of the new analyses MCETs can enable are presented. ...|$|R
5000|$|Simulator: {{following}} {{one possible}} execution {{path through the}} system and presenting the resulting <b>execution</b> <b>trace</b> to the user.|$|R
5000|$|We {{can check}} the {{correctness}} {{of the method}} for n=4 by presenting the <b>execution</b> <b>trace</b> of the program: ...|$|R
40|$|Abstract—Identifying {{concepts}} in <b>execution</b> <b>traces</b> {{is a task}} often necessary to support program comprehension or maintenance activities. Several approaches—static, dynamic or hybrid—have been proposed to identify cohesive, meaningful sequence of methods in <b>execution</b> <b>traces.</b> However, none of the proposed approaches is able to label such segments and to identify relations between segments of the same trace. This paper present SCAN (Segment Concept AssigNer) an approach to assign labels to sequences of methods in <b>execution</b> <b>traces,</b> and to identify relations between such segments. SCAN uses information retrieval methods and formal concept analysis to produce sets of words helping the developer to understand the concept implemented by a segment. Specifically, formal concept analysis allows SCAN to discover commonalities between segments in different trace areas, as well as terms more specific to a given segment and high level relations between segments. The paper describes SCAN along with a preliminary manual validation—upon <b>execution</b> <b>traces</b> collected from usage scenarios of JHotDraw and ArgoUML—of SCAN accuracy in assigning labels representative of concepts implemented by trace segments. Keywords-Concept identification, dynamic analysis, information retrieval, formal concept analysis. I...|$|R
5000|$|System {{structure}} and run-time <b>execution</b> <b>traces</b> are correlated to facilitate program comprehension through dynamic analysis in software maintenance tasks.|$|R
5000|$|The {{following}} are three step-by-step <b>execution</b> <b>traces</b> annotated with the sentence numbers applied {{at each step}} to produce the next ...|$|R
40|$|Abstract Program <b>execution</b> <b>traces</b> can be {{so large}} in {{practical}} testing and monitoring applications {{that it would be}} very expensive, if not impossible, to store them for detailed analysis. Monitoring <b>execution</b> <b>traces</b> without storing them, can be a nontrivial matter for many specification formalisms, because complex formulae may require a considerable amount of information about the past. Metric temporal logic (MTL) is an extension of propositional linear temporal logic with discrete-time-bounded temporal operators. In MTL, one can specify time limits within which certain temporal properties must hold, thus making it very suitable to express real-time monitoring requirements. In this paper, we present monitoring algorithms for checking timestamped <b>execution</b> <b>traces</b> against formulae in MTL or certain important sublogics of it. We also present lower bounds for the monitoring problem, showing that the presented algorithms are asymptotically optimal. 1 Introduction Runtime verification and monitoring have been proposed as lightweight formal verification methods [13] with the explicit goal of checking systems against their formal requirements while they execute. In most monitoring applications, <b>execution</b> <b>traces</b> are available only incrementally and they are much larger than the formulae against which they are checked. Storing an entire <b>execution</b> <b>trace</b> and then performing the formal analysis by having random access to the trace is very expensive and sometimes even impossible. For example, the monitor may lack resources, e. g., if it runs within an embedded system, or the monitor may be expected to react promptly when its requirements are violated, in order for the system to safely take a recovery or a shutdown action. In this paper, we adopt the position that a monitoring algorithm does not store <b>execution</b> <b>traces,</b> but rather consumes the events as they are reThis is a preliminary version. The final version will be published inElectronic Notes in Theoretical Computer Scienc...|$|R
40|$|When {{trying to}} reverse {{engineer}} software, <b>execution</b> <b>trace</b> analysis is increasingly used. Though, by using this technique we are quickly faced with {{an enormous amount of}} data that we must process. While many solutions have been proposed that consist of summarizing, filtering or compressing the trace, the lossless techniques are seldom able to cope with millions of events. Then, we developed a dynamic clustering technique, based on the segmentation of the <b>execution</b> <b>trace</b> that can losslessly process such a large quantity of data. In order to compute the clusters of classes we use a maximal clique computing algorithm. After having presented our technology we show experimental results highlighting that it is robust with respect to the segmentation parameters. Finally we present the tool we developed to compute dynamic clusters from <b>execution</b> <b>traces...</b>|$|R
40|$|We need {{tools and}} {{new ways to}} {{visualize}} our software <b>execution</b> <b>traces</b> and to deploy and integrate the visualizations into users’ environments. We are currently implementing existing information visualization 3 D metaphors using X 3 D to visualize our <b>execution</b> <b>traces,</b> however most of these metaphors are principly node-link diagrams or graph structures and {{are not easy to}} interpret nor understand. We want to use real world metaphors which users can easily relate to, such as a 3 D city metaphor. One tool for implementing the 3 D city metaphor could be the open source X 3 D-Earth application. In this paper we list the requirements for an X 3 D-Earth application which could be used for visualizations of static and dynamic information of software. Keywords Software Visualization, <b>Execution</b> <b>Traces,</b> X 3 D-Earth 1...|$|R
40|$|AbstractProgram <b>execution</b> <b>traces</b> can be {{so large}} in {{practical}} testing and monitoring applications {{that it would be}} very expensive, if not impossible, to store them for detailed analysis. Monitoring <b>execution</b> <b>traces</b> without storing them, can be a nontrivial matter for many specification formalisms, because complex formulae may require a considerable amount of information about the past. Metric temporal logic (MTL) is an extension of propositional linear temporal logic with discrete-timebounded temporal operators. In MTL, one can specify time limits within which certain temporal properties must hold, thus making it very suitable to express real-time monitoring requirements. In this paper, we present monitoring algorithms for checking timestamped <b>execution</b> <b>traces</b> against formulae in MTL or certain important sublogics of it. We also present lower bounds for the monitoring problem, showing that the presented algorithms are asymptotically optimal...|$|R
5000|$|VB Watch injects dynamic {{analysis}} code into Visual Basic programs {{to monitor their}} performance, call stack, <b>execution</b> <b>trace,</b> instantiated objects, variables and code coverage.|$|R
40|$|The paper {{describes}} {{a system which}} is able, given <b>execution</b> <b>traces</b> of formal program specifications, to surmise relevant program properties. The system is embedded {{in an environment of}} software development by levels of abstractions. A surmising algorithm is given which is based on generalization, instantiation by symbolic execution and symbolic <b>execution</b> <b>trace</b> analysis. Examples are given which illustrate the behaviour and the capabilities of the system. Finally, conditions are given on program specification schemata which guarantee that the surmised properties actually hold...|$|R
40|$|The {{analysis}} of <b>execution</b> <b>traces</b> {{is a common}} practice {{in the context of}} software understanding. A major issue during this task is scalability, as the massive amounts of data often make the comprehension process difficult. A significant portion of this data overload can be attributed to repetitions that are caused by, for example, iterations in the software’s source code. In this position paper, we elaborate on a novel approach to visualize such repetitions. The idea is to compare an <b>execution</b> <b>trace</b> against itself and to visualize the matching events in a two-dimensional matrix, similar to related work in the field of code duplication detection. By revealing these similarities we hope to gain new insights into <b>execution</b> <b>traces.</b> We identify the potential purposes in facilitating the software understanding process and report on our findings so far. 1...|$|R
40|$|Abstract. Runtime {{verification}} is {{an alternative}} to static verification, applicable when the latter is impossible (e. g., for lack of the specification of the system to be verified) or computationally too costly. Runtime verification usually requires observability of the system to be verified, that is, complete knowledge of the <b>execution</b> <b>trace</b> being discussed. However, such a requirement might not be met. Even when it is, observation or sampling can be costly, {{so as to make}} it worth reasoning about incomplete <b>execution</b> <b>traces.</b> In this paper, we consider the problem of reasoning on incomplete <b>execution</b> <b>traces</b> to verify properties expressed in linear temporal logic. We show how, with suitable hypotheses on the properties to be checked and on the structure of the partial trace, it is possible to reason about the complete trace by observing only the partial trace. ...|$|R
5000|$|Similarly, a {{cycle is}} the {{shortest}} <b>execution</b> <b>trace</b> in which each processor executes at least one complete iteration of its repeatedly executed list of commands.|$|R
40|$|We {{describe}} {{a tool that}} enables users to record and visualise runtime behaviour of software applications developed in Java. The <b>execution</b> <b>trace,</b> stored {{in the form of}} an XML file is visualized using 3 D call graphs that are an extension of the Cone Tree information visualisation technique. This tool gives the user the ability to create several call graph views from a program’s <b>execution</b> <b>trace,</b> providing additional representations of the program execution to both novice and expert programmers for the purposes of program execution analysis. ...|$|R
40|$|Understanding {{the dynamic}} {{behavior}} of a software system {{is one of the}} most important and time-consuming tasks for today’s software maintainers. In practice, understanding the inner workings of software requires studying the source code and documentation and inserting logging code to map high-level descriptions of the program behavior with low-level implementation, i. e., the source code. Unfortunately, for large codebases and large log files, such cognitive mapping can be quite challenging. To bridge the cognitive gap between the source code and detailed models of program behavior, prior software-execution mining research primarily focused on reducing the size of the low-level instruction <b>execution</b> <b>traces.</b> In contrast, in this thesis we propose a generic approach to present a semantic abstraction with different levels of functional granularity from full <b>execution</b> <b>traces.</b> Our approach mines multiple <b>execution</b> <b>traces</b> to identify frequent behaviors at multiple levels of abstraction, and then analyzes and labels individual <b>execution</b> <b>traces</b> according to the identified major functional behaviors of the system. To validate our technique, we conducted a case study on a large-scale subject program, Javac, to demonstrate the effectiveness of the mining result. Furthermore, the results of a user study demonstrate that our technique is capable of presenting users with a high-level comprehensible abstraction of execution behavior...|$|R
50|$|The {{capability}} of a runtime verifier to detect errors strictly {{depends on its}} capability to analyze <b>execution</b> <b>traces.</b> When the monitors are deployed with the system, instrumentation is typically minimal and the <b>execution</b> <b>traces</b> are {{as simple as possible}} to keep the runtime overhead low. When runtime verification is used for testing, one can afford more comprehensive instrumentations that augment events with important system information that can be used by the monitors to construct and therefore analyze more refined models of the executing system. For example, augmenting events with vector-clock information and with data and control flow information allows the monitors to construct a causal model of the running system in which the observed execution was only one possible instance. Any other permutation of events which is consistent with the model is a feasible execution of the system, which could happen under a different thread interleaving. Detecting property violations in such inferred executions (by monitoring them) makes the monitor predict errors which did not happen in the observed execution, but which can happen in another execution of the same system. An important research challenge is to extract models from <b>execution</b> <b>traces</b> which comprise as many other <b>execution</b> <b>traces</b> as possible.|$|R
