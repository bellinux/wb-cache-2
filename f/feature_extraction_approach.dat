160|10000|Public
30|$|In the {{following}} paper, authors K. Jaya Priya and R.S. Rajesh handle face recognition {{with a local}} selective <b>feature</b> <b>extraction</b> <b>approach</b> based on Gabor filters and the Local Binary Pattern (LBP) method. Experiments using well-known face databases show that their approach is time effective and obtains promising results in the scenario of one training sample per person with significant facial variation.|$|E
40|$|We {{present a}} new batch {{learning}} algorithm for text classification in the vector space of document representations. The algorithm uses ellipsoid separation [3] in the feature space {{which leads to}} a semidefinite program. An approximation of the latent semantic <b>feature</b> <b>extraction</b> <b>approach</b> using Gram-Schmidt orthogonalization [2] is used for the feature extraction. Preliminary results demonstrate some potential for the presented approach. ...|$|E
40|$|We {{present a}} novel face <b>feature</b> <b>extraction</b> <b>approach</b> using {{localized}} Principal Component Analysis (PCA) learning in face recognition tasks. The localized PCA approach produces {{a set of}} fine-tuned feature specific masks from a constrained subset of the input distribution. This method is a guided-learning based {{on a set of}} pre-defined feature points over a short training sequence. The result is the set of eigenfeatures specifically tailored for face recognition. The procedure and result of our <b>feature</b> <b>extraction</b> <b>approach</b> and face recognition are illustrated and discussed. 1 Introduction Face recognition is a specialized image processing task. The recognition typically is performed in two steps: (1) feature extraction and (2) classification or matching based on the features extracted. One of the main goals of the feature extraction procedure is to seek a reduced representation of the source data that best describe each face. What is the best representation used often depends on the input [...] ...|$|E
40|$|Signals are {{a popular}} mean of {{representing}} information, and signal processing {{is of great}} importance. <b>Feature</b> <b>extraction</b> plays {{a critical role in}} signal processing, and many approaches have been studies. This article critically reviews these signal <b>feature</b> <b>extraction</b> <b>approaches</b> to provide an overview of <b>feature</b> <b>extraction</b> <b>approaches,</b> to critically address important issues, and compare and contract reported techniques...|$|R
5000|$|<b>Feature</b> <b>Extraction</b> <b>Approaches</b> for Optical Character Recognition. Briviba Scientific Press, 2007, ...|$|R
40|$|The {{identification}} of pedestrians {{is an important}} problem {{in a wide range}} of computer vision applications. Previous work on pedestrian identification mainly focuses on <b>feature</b> <b>extraction</b> <b>approaches.</b> In this study, we investigate four new features for pedestrian identification that aim to overcome the limitations of features that are employed in previous studies. In particular, we focus on features that are to some extent robust to the presence of occlusions in the pedestrian images. The four new <b>feature</b> <b>extraction</b> <b>approaches</b> that are investigated are based on: (1) nonlinear dimensionality reduction, (2) texton frequency histograms, (3) Latent Dirichlet Allocation, and (4) local receptive fields networks. We test the performance of our <b>feature</b> <b>extraction</b> <b>approaches</b> on the NiSIS competition dataset. From the obtained results, we conclude that Support Vector Machines trained on PCA features perform best, most likely due to the exploitation of biases in the NiSIS dataset. ...|$|R
40|$|Pain is {{a highly}} {{subjective}} experience, {{and the availability of}} an objective assessment of pain perception would be of great importance for both basic and clinical applications. The objective of the present study is to develop a novel approach to extract pain-related features from single-trial laser-evoked potentials (LEPs) for classification of pain perception. The single-trial LEP <b>feature</b> <b>extraction</b> <b>approach</b> combines a spatial filtering using common spatial pattern (CSP) and a multiple linear regression (MLR). The CSP method is effective in separating laser-evoked EEG response from ongoing EEG activity, while MLR is capable of automatically estimating the amplitudes and latencies of N 2 and P 2 from single-trial LEP waveforms. The extracted single-trial LEP features are used in a Naïve Bayes classifier to classify different levels of pain perceived by the subjects. The experimental results show that the proposed single-trial LEP <b>feature</b> <b>extraction</b> <b>approach</b> can effectively extract pain-related LEP features for achieving high classification accuracy. published_or_final_versio...|$|E
40|$|This paper investigates a new {{approach}} for the automated human identification using 2 D ear imaging. We present a completely automated approach for the robust segmentation of curved region of interest using morphological operators and Fourier descriptors. We also investigate new <b>feature</b> <b>extraction</b> <b>approach</b> for ear identification using localized orientation information and also examine local gray-level phase information using complex Gabor filters. Our investigation develops a computationally attractive and effective alternative to characterize the automatically segmented ear images using a pair of log-Gabor filters. The experimental results achieve average rank-one recognition accuracy of 96. 27 % and 95. 93 %, respectively, on the publicly available database of 125 and 221 subjects. Our experimental results from the authentication experiments and false positive identification verses false negative identification also suggest {{the superiority of the}} proposed approach over the other popular <b>feature</b> <b>extraction</b> <b>approach</b> considered in this work. (C) 2011 Elsevier Ltd. All rights reserved...|$|E
40|$|The {{objective}} of the present paper is to describe a pattern recognition approach for image segmentation. First, in the introduction, we describe the general aspects of uniformity and texture recognition. Then we provide a mean-based <b>feature</b> <b>extraction</b> <b>approach</b> for uniformity analysis and a moment-based one for texture analysis. In the classification stage we propose both supervised and also unsupervised clustering algorithms. The graphical results of our implementations are also presented...|$|E
40|$|Regularized-Direct Linear Discriminant Analysis (RD-LDA) - based <b>feature</b> <b>extraction</b> <b>approaches</b> {{are tested}} and {{compared}} in an experimental personal recognition system. The system is multimodal and bases on features extracted from nine regions {{of an image}} of the palmar surface of the hand. For testing purposes 10 gray-scale images of right hand of 184 people were acquired. The experiments have shown that the best results are obtained with the RD-LDA- based <b>features</b> <b>extraction</b> <b>approach</b> (100 % correctness for 920 identification tests and EER = 0. 01 % for 64170 verification tests) ...|$|R
30|$|Therefore, {{the goal}} of our paper is to {{evaluate}} quantitatively based on automated classification the utility of novel <b>feature</b> <b>extraction</b> <b>approaches</b> to breast lesion detection such as foci and non-mass enhancing lesions.|$|R
40|$|This paper {{presents}} the detection techniques of anomalous programs {{based on the}} analysis of their system call traces. We collect the API calls for the tested executable programs from Microsoft detour system and extract the features for our classification task using the previously established n-gram technique. We propose three different <b>feature</b> <b>extraction</b> <b>approaches</b> in this paper. These are frequency-based, time-based and a hybrid approach which actually combines the first two approaches. We use the well-known classifier algorithms in our experiments using WEKA interface to classify the malicious programs from the benign programs. Our empirical evidence demonstrates that the proposed <b>feature</b> <b>extraction</b> <b>approaches</b> can detect malicious programs over 88 % which is quite promising for the contemporary similar research. <br /...|$|R
30|$|In this section, {{we compare}} the <b>feature</b> <b>extraction</b> <b>approach</b> {{described}} in Section “Feature extraction” {{with respect to}} Lee et al. [20], here referred as Lea, Castellani et al. [22], Cea, and Bonarrigo et al. [6], Bea. It is worth {{to remember that the}} proposed approach is a variation of Cea, and that both Lea and Cea were originally conceived to work with mesh datasets, therefore they have been adapted here to work with range images.|$|E
40|$|Optical Character Recognition is a {{technique}} by which you can automatically recognize the characters with an optical mechanism. OCR technology allows you the recognition of printed or handwritten text documents. Main aim {{of this research is}} to prepare a recognition system which can be used for the recognition of offline handwritten Hindi characters. For this proposed system Support Vector Machine is used as classifier and Diagonal <b>feature</b> <b>extraction</b> <b>approach</b> is used to extract features...|$|E
40|$|Abstract:- In {{this paper}} we {{consider}} a <b>feature</b> <b>extraction</b> <b>approach</b> for recognition of handwritten electrical symbols. The symbols are {{represented as a}} sequence of points. We apply a feature extraction technique to extract the most important features and then feed them for recognition to a Neural Network. We utilize a Learning Vector Quantization (LVQ) network and show its capability to recognize the symbols. Key-Words:- handwritten symbol recognition, feature extraction, neural networks, Learning Vector Quantization (LVQ) ...|$|E
40|$|In many {{applications}} of machine learning {{a series of}} <b>feature</b> <b>extraction</b> <b>approaches</b> {{and a series of}} classifiers are explored in order to obtain a system with the highest accuracy possible. In the application we discussed here at least two <b>feature</b> <b>extraction</b> <b>approaches</b> have been explored (Fast Fourier Transform) and Discrete Wavelet Transform, and at least two approaches to build classifiers have also been explored (Artificial Neural Networks and Support Vector Machines). If one combination seems superior in terms of accuracy rate, shall we adopt it as the one to use or is there a combination of the approaches that results in some benefit? We show here how we have combined classifiers considering the misclassification cost to obtain a more informative classification for its application in the field...|$|R
40|$|The {{focus of}} this work is {{on the problem of}} <b>feature</b> <b>extraction</b> for vehicle detection. <b>Feature</b> <b>extraction</b> is a key point of pattern recognition. In particular, we propose using {{improved}} wavelet <b>feature</b> <b>extraction</b> <b>approaches</b> based on HSV space for rear-vehicle detection. Wavelet features are attractive for vehicle detection because they form a compact representation, encode edges, capture information from multi-resolution, and can be computed efficiently. Currently, the wavelet features based on coefficients and grayscale space are easily affected by the ����� � surroundings and illumination conditions and cause high intra-class variability. In order to deal with this problem, three improved wavelet <b>feature</b> <b>extraction</b> <b>approaches</b> based on HSV space are proposed. The experimental results indicate that the improved approaches based on HSV show super performance compared with the current methods based on both HSV space and Grayscale space. Furthermore, they also show better results than themselves based on Grayscale space. ...|$|R
30|$|In {{order to}} improve the {{existing}} face recognition techniques, discriminative competence of the invariant features selected to represent the face images should be high because, thereafter, classification is performed {{on the basis of}} these invariant features only. In literature, the approaches used to represent the face images are classified broadly into two categories, namely, the global <b>feature</b> <b>extraction</b> <b>approaches</b> and the local <b>feature</b> <b>extraction</b> <b>approaches</b> [2]. The global <b>feature</b> <b>extraction</b> <b>approaches</b> are based on the statistical methods, wherein features are extracted from the whole face image. In this category, the subspace-based methods, namely, principal component analysis (PCA), Fisher linear discriminant (FLD), two-dimensional PCA (2 DPCA), and two-directional two-dimensional PCA (2 D 2 PCA) [3 – 7], are some of the popular and most frequently employed techniques. Moment invariants, such as Hu's seven moment invariants and orthogonal rotation invariant moments such as Zernike moments (ZMs), pseudo-Zernike moments (PZMs), and orthogonal Fourier-Mellin moments (OFMMs), are observed to be very effective in global image description and recognition [8], and MPEG- 7 uses some of them as region-based shape descriptors for image retrieval [9]. The magnitude of these moments is invariant to image rotation, and after applying some geometric transformations, it becomes invariant to translation and scale [10, 11].|$|R
40|$|In this work, a <b>feature</b> <b>extraction</b> <b>approach</b> {{based on}} {{improved}} Locally Linear Embedding(LLE) is proposed. In the algorithm, tangent space distance is introduced to LLE, which overcomes the shortcoming of original LLE method based on Euclidean distance. It can satisfy {{the requirement of}} locally linear much better and so can express the I/O mapping quality better than classical method. Simulation results are given to demonstrate {{the effectiveness of the}} improved LLE method...|$|E
40|$|In Handwritten {{signatures}} {{analyzed for}} forgery have to undergo feature extraction process, due to varied samples in size rotation and intra-domain changes, invariance {{has to be}} achieved during feature extraction process; circular Hidden Markov Model with discrete radon transform approach of feature extraction provides invariance. On other hand Scale Invariant Feature Transform (SIFT) has inherent invariant <b>feature</b> <b>extraction</b> <b>approach.</b> This paper compares both approaches on common signature databases for False acceptance rate (FAR), False Rejection Rate (FRR) an...|$|E
40|$|This letter {{proposes a}} {{nonlinear}} DCT discriminant <b>feature</b> <b>extraction</b> <b>approach</b> for face recognition. The proposed approach first selects appropriate DCT frequency bands {{according to their}} levels of nonlinear discrimination. Then, this approach extracts nonlinear discriminant features from the selected DCT bands by presenting a new kernel discriminant method, i. e. the improved kernel discriminative common vector (KDCV) method. Experiments on the public FERET database show that this new approach {{is more effective than}} several related methods. Department of Computin...|$|E
40|$|Abstract. In many {{applications}} of machine learning {{a series of}} <b>feature</b> <b>extraction</b> <b>approaches</b> {{and a series of}} classifiers are explored in order to obtain a system with the highest accuracy possible. In the application we discussed here at least two <b>feature</b> <b>extraction</b> <b>approaches</b> have been explored (Fast Fourier Transform) and Discrete Wavelet Transform, and at least two approaches to build classifiers have also been explored (Artificial Neural Networks and Support Vector Machines). If one combination seems superior in terms of accuracy rate, shall we adopt it as the one to use or is there a combination of the approaches that results in some benefit? We show here how we have combined classifiers considering the misclassification cost to obtain a more informative classification for its application in the field...|$|R
30|$|Material {{recognition}} {{is a fundamental}} problem in computer vision. In contrast with the several decades of object recognition research, material recognition has only begun receiving attention in recent years. It is a flourishing and challenging field. The approaches to material recognition can be broadly categorized as hand-crafted or automatic <b>feature</b> <b>extraction.</b> Hand-crafted <b>approaches</b> can be further divided into surface reflectance [7 – 11], 3 D texture [12 – 19], and feature fusion [20 – 22] <b>approaches.</b> Automatic <b>feature</b> <b>extraction</b> <b>approaches</b> refer to those that involve acquiring image features using a {{deep convolutional neural network}} (D-CNN) [23 – 26].|$|R
40|$|Abstract — Recently, {{multimodal}} biometric {{systems have}} been widely accepted, which has shown increased accuracy and population coverage, while reducing vulnerability to spoofing. The main feature to multimodal biometrics is the amalgamation of different biometric modality data at the <b>feature</b> <b>extraction,</b> matching score, or decision levels. Recently, a lot of works are presented in the literature for multi-modal biometric recognition. In this paper, we have presented comparative analysis of four different <b>feature</b> <b>extraction</b> <b>approaches,</b> such as LBP, LGXP, EMD and PCA. The main steps involved in such four <b>approaches</b> are: 1) <b>Feature</b> <b>extraction</b> from face image, 2) <b>Feature</b> <b>extraction</b> from iris image and 3) Fusion of face and iris features. The performance of the <b>feature</b> <b>extraction</b> methods in multi-modal recognition is analyzed using FMR and FNMR to study the recognition behavior of these approaches. Then, an extensive analysis is carried out to find the effectiveness of different approaches using two different databases. The experimental results show the equal error rate of different <b>feature</b> <b>extraction</b> <b>approaches</b> in multi-modal biometric recognition. From the ROC curve plotted, {{the performance of the}} LBP and LGXP method is better compared to PCA-based technique...|$|R
30|$|We propose an {{empirical}} mode decomposition (EMD-) based method to extract features from the multichannel recordings of local field potential (LFP), {{collected from the}} middle temporal (MT) visual cortex in a macaque monkey, for decoding its bistable structure-from-motion (SFM) perception. The <b>feature</b> <b>extraction</b> <b>approach</b> consists of three stages. First, we employ EMD to decompose nonstationary single-trial time series into narrowband components called intrinsic mode functions (IMFs) with time scales dependent on the data. Second, we adopt unsupervised K-means clustering to group the IMFs and residues into several clusters across all trials and channels. Third, we use the supervised common spatial patterns (CSP) approach to design spatial filters for the clustered spatiotemporal signals. We exploit the support vector machine (SVM) classifier on the extracted features to decode the reported perception on a single-trial basis. We demonstrate that the CSP feature of the cluster in the gamma frequency band outperforms the features in other frequency bands and leads to the best decoding performance. We also show that the EMD-based feature extraction can be useful for evoked potential estimation. Our proposed <b>feature</b> <b>extraction</b> <b>approach</b> may have potential for many applications involving nonstationary multivariable time series such as brain-computer interfaces (BCI).|$|E
40|$|Abstract — Optical Character Recognition is a {{technique}} by which you can automatically recognize the characters with an optical mechanism. OCR technology allows you the recognition of printed or handwritten text documents. Main aim {{of this research is}} to prepare a recognition system which can be used for the recognition of offline handwritten Hindi characters. For this proposed system Support Vector Machine is used as classifier and Diagonal <b>feature</b> <b>extraction</b> <b>approach</b> is used to extract features. Keywords- Handwritten Character Recognition, OCR, Feature Extraction, SVM. I...|$|E
40|$|We {{propose a}} new EEG-based {{wireless}} brain computer interface (BCI) with which subjects can “mind-type” text {{on a computer}} screen. The application is based on detecting P 300 event-related potentials in EEG signals recorded on the scalp of the subject. The BCI uses a simple classifier which relies on a linear <b>feature</b> <b>extraction</b> <b>approach.</b> The accuracy of the presented system {{is comparable to the}} state-of-the-art for on-line P 300 detection, but with the additional benefit that its much simpler design supports a power-efficient on-chip implementation. status: publishe...|$|E
30|$|Other {{implementations}} of monocular SLAM {{are also}} based on obtaining abstract regions of an image in each frame that are more useful for reconstruction of location than robots navigation [7, 21, 27]. Some of the algorithms {{for the detection of}} regions are SIFT, BRIEF, FREAK, SURF and ORB [11]. However, all these <b>feature</b> <b>extraction</b> <b>approaches</b> present the problem of data association in SLAM [21].|$|R
40|$|In this paper, {{we propose}} a Multi-stage Neural Network Architecture (MNNA) which {{integrates}} several neural networks and various <b>feature</b> <b>extraction</b> <b>approaches</b> into a unique pattern recognition system. General mechanism for designing the MNNA is presented. A three-stage fully connected feedforward neural networks {{system is designed}} for Handwritten Chinese Character Recognition (HCCR). Different <b>feature</b> <b>extraction</b> methods are employed at each stage. Experiments show that the three-stage neural network HCCR system has achieved impressive performance and the preliminary results are very encouraging. published_or_final_versio...|$|R
30|$|The {{dimensionality}} of {{the gathered}} data has been increasingly large {{due to the}} rapid development of modern sensing systems [1]. However, the high-dimensional data are {{hard to deal with}} since high computational complexity and memory requirements. Meanwhile, some irrelevant, redundant, and noisy features will be incorporated into high-dimensional data, which will adversely affect the performance. Hence, reducing the dimension of the data is an essential step for subsequent processing. <b>Feature</b> <b>extraction</b> [2, 3] and feature selection [4] can be regarded as two main techniques for dimensionality reduction. For <b>feature</b> <b>extraction</b> <b>approaches,</b> they obtain the features by mapping the original data into a new low-dimensional subspace using a transformation matrix or projection. Nevertheless, the obtained features have relatively poor interpretability [3]. In comparison, feature selection approaches aim at selecting several optimal features from the original data by a series of criteria [5]. Therefore, the obtained low-dimensional representation is interpretable [4]. More importantly, feature selection approaches just need to collect these optimal features during data acquisition, and they perform better than <b>feature</b> <b>extraction</b> <b>approaches,</b> which need to utilize all the features for dimensionality reduction. In this paper, we focus on feature selection.|$|R
40|$|Republic of China Principle {{component}} analysis (PCA) and Fisher {{linear discriminant analysis}} (FLDA), as two popular feature extraction approaches in Pattern recognition and data analysis, extract so-needed features directly based on vector patterns, i. e., before applying them, any non-vector pattern such as an image is first vectorized into a vector pattern by some technique like concatenation. However, such a vectorization has been proved not to be beneficial for image recognition due to consequences of both the algebraic <b>feature</b> <b>extraction</b> <b>approach</b> and 2 DPCA. In this paper, inspired by the above two approaches, we try an opposite direction to extract features for any vector pattern by first matrixizing it into a matrix pattern and then applying the matrixized versions of PCA and FLDA, MatPCA and MatFLDA, to the pattern. MatFLDA uses, in essence, the same principle as the algebraic <b>feature</b> <b>extraction</b> <b>approach</b> and is constructed in terms of similar objective function to FLDA while MatPCA uses a minimization of the reconstructed error for the training samples like PCA to obtain a set of projection vectors, which is somewhat different derivation from 2 DPCA despite of equivalence. Finally experiments on 10 publicly obtainable datasets show that both MatPCA and MatFLDA gain performance improvement in different degrees respectively on 7 and...|$|E
40|$|We {{investigate}} {{the performance of}} our audio-visual speech recognition system in both English and Greek {{under the influence of}} audio noise. We present the architecture of our recently built system that utilizes information from three streams including 3 -D distance measurements. The <b>feature</b> <b>extraction</b> <b>approach</b> used is based on the discrete cosine transform and linear discriminant analysis. Data fusion is employed using state-synchronous hidden Markov models. Our experiments were conducted on our recently collected database under a multi-speaker configuration and resulted in higher performance and robustness in comparison to an audio-only recognizer. © 2013 Springer-Verlag...|$|E
40|$|Abstract. Principal Component Analysis (PCA) is a <b>feature</b> <b>extraction</b> <b>approach</b> {{directly}} {{based on}} a whole vector pattern and acquires a set of projections that can realize the best reconstruction for an original data in the mean squared error sense. In this paper, the progressive PCA (PrPCA) is proposed, which could progressively extract features from a set of given data with large dimensionality and the extracted features are subsequently applied to pattern recognition. Experiments on the FERET database show its face recognition performance is better than those based on both E(PC) 2 A and FLDA. ...|$|E
40|$|Finding {{simple and}} {{efficient}} features for offline hand written character recognition {{is still an}} active area of research. In this work, we propose modified view based <b>feature</b> <b>extraction</b> <b>approaches</b> for the recognition of handwritten Tamil characters. In the first approach, the five views of a normalized and binarized character image viz, top, bottom, left, right and front are extracted. Each view is then divided into 16 equal zones and the total numbers of background pixel in each zone are counted. The 80 values so obtained form a feature vector. In the second approach, the normalized and binaraized character images are divided into 16 equal zones. Five views are extracted from each zone and {{the total number of}} background pixel in each view is counted, resulting in 80 feature values. Further the above two approaches are modified by employing thinned images instead of the whole image. The extracted features are classified using SVM, MLP and ELM classifier. The discriminative powers of the proposed approaches are compared with that of four popular <b>feature</b> <b>extraction</b> <b>approaches</b> in character recognition. The <b>feature</b> <b>extraction</b> time and classification performances are also compared. The proposed modified approaches results in high classification performance (95. 26...|$|R
40|$|Two-dimensional similarity-based image {{organizing}} studies how {{to place}} photos within 2 D virtual canvas {{based on their}} visual contents so that the users can easily locate the desired photos. As an extension to our previous work [10], several improvements are made in this paper to allow better photo browsing experiences. For example, the new approach pre-orders all the photos so that a consistent set of photos is selected for display. This solves the photo flickering problem of our previous approach, which uses K-mean algorithm to dynamically select photos. The main focus of this paper however is on {{the evaluation of the}} effectiveness of different feature vectors for 2 D photo organization. A performance metric is proposed to measure how well photos with similar visual contents are grouped together on the 2 D canvas. Feature vectors generated using eight different low-level <b>feature</b> <b>extraction</b> <b>approaches</b> are tested. The evaluation results reveal {{the pros and cons of}} different <b>feature</b> <b>extraction</b> <b>approaches,</b> which can be a useful guide for developing new feature vectors...|$|R
40|$|This paper {{presents}} {{a comparison of}} different <b>feature</b> <b>extraction</b> methods for automatically recognizing soccer ball patterns through a probabilistic analysis. It contributes to investigate different well-known <b>feature</b> <b>extraction</b> <b>approaches</b> applied in a soccer environment, {{in order to measure}} robustness accuracy and detection performances. This work, evaluating different methodologies, permits to select the one which achieves best performances in terms of detection rate and CPU processing time. The effectiveness of the different methodologies is demonstrated by a huge number of experiments on real ball examples under challenging conditions...|$|R
