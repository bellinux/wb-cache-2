9245|329|Public
5|$|Snow science {{addresses}} how snow forms, its distribution, {{and processes}} affecting how snowpacks change over time. Scientists improve storm forecasting, study global snow cover {{and its effect}} on climate, glaciers, and water supplies around the world. The study includes physical properties of the material as it changes, bulk properties of in-place snow packs, and the aggregate properties of regions with snow cover. In doing so, they employ on-the-ground physical measurement techniques to establish <b>ground</b> <b>truth</b> and remote sensing techniques to develop understanding of snow-related processes over large areas.|$|E
5|$|Storm {{spotters}} {{are needed}} because radar {{systems such as}} NEXRAD do not detect a tornado; merely signatures which hint at the presence of tornadoes. Radar may give a warning before there is any visual evidence of a tornado or imminent tornado, but <b>ground</b> <b>truth</b> from an observer can either verify the threat or determine that a tornado is not imminent. The spotter's ability to see what radar cannot is especially important as distance from the radar site increases, because the radar beam becomes progressively higher in altitude further away from the radar, chiefly due to curvature of Earth, and the beam also spreads out.|$|E
25|$|Google {{has also}} {{recruited}} volunteers to check and correct <b>ground</b> <b>truth</b> data.|$|E
40|$|It packs a {{bunch of}} {{features}} since it's a first release, 	Condor reads database entries from ISI, BIB and FROAC files. 	Also creates term document matrices with optional TF-IDF regularisation. 	Furthermore it's capable of creating LSA search engines based on the term document matrices. 	It also loads <b>ground</b> <b>truths</b> with specially formatted sets of BIB files. 	Finally, it's able to test the search engines against the loaded <b>ground</b> <b>truths...</b>|$|R
30|$|We {{divide the}} {{datasets}} into different snapshots and measuring the clustering quality using the <b>ground</b> <b>truths</b> {{of the full}} datasets.|$|R
5000|$|On March 4, 2010, Asali spoke {{before the}} Senate Committee on Foreign Relations on [...] "Middle East Peace: <b>Ground</b> <b>Truths,</b> Challenges Ahead".|$|R
25|$|The <b>Ground</b> <b>Truth,</b> a 2006 {{documentary}} film about {{veterans of the}} Iraq War.|$|E
25|$|Indeed, the Nile Delta {{is an area}} of {{the world}} that lacks the {{detailed}} <b>ground</b> <b>truth</b> data and monitoring stations. Despite the economic importance of the Nile Delta, it could be considered {{as one of the most}} data-poor regions with respect to sea level rise.|$|E
25|$|Although {{scientific}} work is sometimes {{cited as a}} goal, direct participation in such work is almost always impractical except for those collaborating in an organized university or government project. Many chasers also are storm spotters, reporting their observations of hazardous weather to relevant authorities. These reports greatly benefit real-time warnings with <b>ground</b> <b>truth</b> information as well as science by increasing the reliability of severe storm databases used in climatology and other research (which ultimately boosts forecast and warning skill). Additionally, many recreational chasers submit photos and videos to researchers {{as well as to}} the National Weather Service (NWS) for spotter training.|$|E
40|$|<b>Ground</b> <b>truths</b> {{based on}} {{partially}} ordered lists {{have been used}} for some years now {{to evaluate the effectiveness of}} Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these <b>ground</b> <b>truths</b> and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggregation of the relevant results, and show that it is not possible to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Recall and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative <b>ground</b> <b>truths.</b> 1...|$|R
40|$|Hyperspectral unmixing (HU) is a {{very useful}} and {{increasingly}} popular preprocessing step {{for a wide range}} of hyperspectral applications. However, the HU research has been constrained a lot by three factors: (a) the number of hyperspectral images (especially the ones with <b>ground</b> <b>truths)</b> are very limited; (b) the <b>ground</b> <b>truths</b> of most hyperspectral images are not shared on the web, which may cause lots of unnecessary troubles for researchers to evaluate their algorithms; (c) the codes of most state-of-the-art methods are not shared, which may also delay the testing of new methods. Accordingly, this paper deals with the above issues from the following three perspectives: (1) as a profound contribution, we provide a general labeling method for the HU. With it, we labeled up to 15 hyperspectral images, providing 18 versions of <b>ground</b> <b>truths.</b> To the best of our knowledge, this is the first paper to summarize and share up to 15 hyperspectral images and their 18 versions of <b>ground</b> <b>truths</b> for the HU. Observing that the hyperspectral classification (HyC) has much more standard datasets (whose <b>ground</b> <b>truths</b> are generally publicly shared) than the HU, we propose an interesting method to transform the HyC datasets for the HU research. (2) To further facilitate the evaluation of HU methods under different conditions, we reviewed and implemented the algorithm to generate a complex synthetic hyperspectral image. By tuning the hyper-parameters in the code, we may verify the HU methods from four perspectives. The code would also be shared on the web. (3) To provide a standard comparison, we reviewed up to 10 state-of-the-art HU algorithms, then selected the 5 most benchmark HU algorithms, and compared them on the 15 real hyperspectral datasets. The experiment results are surely reproducible; the implemented codes would be shared on the web...|$|R
3000|$|... 3. Similar to the {{evaluation}} of segmentation results from CT images in the previous subsection, we averaged the rates obtained from <b>ground</b> <b>truths</b> {{in order to avoid}} any bias towards a particular observer’s evaluation as complied with the standard evaluation techniques [...]...|$|R
25|$|A good map has to {{compromise}} between portraying the items of interest (or themes) {{in the right}} place on the map, and the need to show that item using text or a symbol, which take up space on the map and might displace some other item of information. The cartographer is thus constantly making judgements about what to include, what to leave out and what to show in a slightly incorrect place. This issue assumes more importance as the scale of the map gets smaller (i.e. the map shows a larger area) because the information shown on the map takes up more space on the ground. A good example from the late 1980s was the Ordnance Survey's first digital maps, where the absolute positions of major roads were sometimes a scale distance of hundreds of metres away from <b>ground</b> <b>truth,</b> when shown on digital maps at scales of 1:250,000 and 1:625,000, because of the overriding need to annotate the features.|$|E
25|$|Storm {{spotters}} {{are needed}} because radar {{systems such as}} NEXRAD, and satellite images, do not detect tornadoes or hail, only indications that the storm has the potential. Radar and satellite data interpretation will usually give a warning before there is any visual evidence of such events, but <b>ground</b> <b>truth</b> from an observer can either verify the threat or determine it is not imminent. The spotter's ability to see what these remote sensing devices cannot is especially important as distance from a radar site increases, because the radar beam becomes progressively higher in altitude further away from the radar, due to curvature of Earth {{and the spread of}} the beam with distance. Therefore, when far from a radar, only precipitations and velocities high in the storm are observed. The important areas might not then be sampled or the resolution of the data might be poor. Also, some meteorological situations leading to tornadogenesis are not readily detectable by radar and on occasion tornado development may occur more quickly than radar can complete a scan and send the batch of data.|$|E
25|$|Chasers heavily utilize still {{photography}} {{since the}} beginning. Videography gained prominence by the 1990s {{into the early}} 2000s but a resurgence of photography occurred {{with the advent of}} affordable and versatile digital SLR (DSLR) cameras. Prior to this, 35 mm SLR print and slide film formats were mostly used, along with some medium format cameras. In the late 2000s, mobile phone 3G data networks became fast enough to allow live streaming video from chasers using webcams. This live imagery is frequently used by the media, as well as NWS meteorologists, emergency managers, and the general public for direct <b>ground</b> <b>truth</b> information, and it provides video sales opportunities to chasers. Also by this time, camcorders using memory cards to record video began to be adopted. Digital video had been around for years but was recorded on tape, whereas solid-state is random access rather than sequential access (linear) and has no moving parts. Late in the 2000s HD video began to overtake SD (which had been NTSC in North America) in usage as prices came down and performance increased (initially there were low-light and sporadic aliasing problems due to chip and sensor limitations). By the mid-2010s 4K cameras were increasingly in use. Tripods are used by those seeking crisp professional photo and video imagery and also enable chasers to tend to other activities.|$|E
30|$|Nevertheless, {{most of the}} {{previous}} researchers measure the improvement of image quality through visual inspection (Schettini and Corchs 2012). A few of them use quantitative estimations and residual error which is computed between <b>ground</b> <b>truths</b> and corrected images (Schettini and Corchs 2012).|$|R
40|$|Abstract. The Arc Segmentation Contest, as {{the fifth}} in the series of {{graphics}} recognition contests organized by IAPR TC 10, was held {{in association with the}} GREC’ 2003 workshop. In this paper we present the report of the contest: the contest rules, performance metrics, test images and their <b>ground</b> <b>truths,</b> and the outcomes...|$|R
40|$|Semantic concept {{detection}} is {{an active}} research topic {{as it can provide}} semantic filters and aid in automatic search of image and video databases. The annual NIST TRECVID video retrieval benchmarking event [1] has greatly contributed to this area by providing benchmark datasets and performing system evaluation. As acquiring <b>ground</b> <b>truths</b> of semantic concepts i...|$|R
500|$|There {{are several}} {{difficulties}} with satellite-based absolute SST measurements. First, in infrared remote sensing methodology the radiation emanates {{from the top}} [...] "skin" [...] of the ocean, approximately the top 0.01 mm or less, which may not represent the bulk temperature of the upper meter of ocean due primarily to effects of solar surface heating during the daytime, reflected radiation, as well as sensible heat loss and surface evaporation. All these factors make it somewhat difficult to compare satellite data to measurements from buoys or shipboard methods, complicating <b>ground</b> <b>truth</b> efforts. [...] Secondly, the satellite cannot look through clouds, creating a cool bias in satellite-derived SSTs within cloudy areas. [...] However, passive microwave techniques can accurately measure SST and penetrate cloud cover. [...] Within atmospheric sounder channels on weather satellites, which peak just above the ocean's surface, knowledge of the sea surface temperature is important to their calibration.|$|E
2500|$|The Moon {{has been}} {{suggested}} as a testbed for Planetary protection for human missions in space. Currently the Moon has no contamination restrictions because it's considered to be [...] "not of interest" [...] for prebiotic chemistry and origins of life. It {{could be used to}} test new technology to protect sites in the Solar System, and astronauts, from forward and backward contamination. Analysis of the contamination left by the Apollo astronauts could also yield useful <b>ground</b> <b>truth</b> for planetary protection models.|$|E
5000|$|The term <b>ground</b> <b>truth</b> {{refers to}} the {{underlying}} absolute state of information; the gold standard strives to represent the <b>ground</b> <b>truth</b> as closely as possible. While the gold standard refers to a best effort to obtain the truth, <b>ground</b> <b>truth</b> is typically collected by direct observations. In machine learning and information retrieval, [...]|$|E
30|$|CNN {{training}} is a computationally demanding process, whereas creating <b>ground</b> <b>truths</b> by expert segmentation is often expensive, tedious, and time consuming. To give an oversight of these expenses, CNN training durations were recorded, and one subject was chosen {{from the group}} A for which the manual segmentation, automatic preprocessing run, and CNN execution times were recorded.|$|R
40|$|Over-segmentation', `under-segmentation', `good results' {{and similar}} {{subjective}} terms appear {{frequently in the}} literature on range image segmentation. However, even though the need for standardized segmentation error metrics has been long recognized, no formal methodology for evaluating a range image segmentation has appeared. This paper describes a framework in which to carry out such an evaluation. With this framework, a more rigorous and objective comparison of range image segmentation techniques can be performed. We have developed many key issues, including a formal definition of the range image segmentation problem, a comprehensive data set to use in evaluation, a method for creating <b>ground</b> <b>truths,</b> and a set of formally defined metrics to classify segmentation results against <b>ground</b> <b>truths.</b> 1 Introduction This paper describes a methodology to compare range image segmentation techniques. As early as 1988, at the NSF Range Image Understanding Workshop, the community recognized the [...] ...|$|R
40|$|Accurate {{estimation}} of tissue volume has important applications in brain diagnostic and morphologic studies. In this paper, we show how partial volume {{estimation of}} brain tissues from 3 D MR brain {{images can be}} performed using the recently proposed adaptive spatial FCM algorithm. The efficacy of the proposed algorithm is demonstrated experimentally using simulated MR images with known <b>ground</b> <b>truths.</b> Full Tex...|$|R
50|$|<b>Ground</b> <b>truth</b> {{also helps}} with {{atmospheric}} correction. Since images from satellites obviously {{have to pass}} through the atmosphere, they can get distorted because of absorption in the atmosphere. So <b>ground</b> <b>truth</b> can help fully identify objects in satellite photos.|$|E
5000|$|Bayesian spam {{filtering}} is {{a common}} example of supervised learning. In this system, the algorithm is manually taught the differences between spam and non-spam. This depends on the <b>ground</b> <b>truth</b> of the messages used to train the algorithm [...] - [...] inaccuracies in the <b>ground</b> <b>truth</b> will correlate to inaccuracies in the resulting spam/non-spam verdicts.|$|E
5000|$|Geographic {{information}} {{systems such as}} GIS, GPS, and GNSS, have become so widespread that the term [...] "ground truth" [...] has taken on special meaning in that context. If the location coordinates returned by a location method such as GPS are an estimate of a location, then the [...] "ground truth" [...] is the actual location on earth. A smart phone might return a set of estimated location coordinates such as 43.87870,-103.45901. The <b>ground</b> <b>truth</b> being estimated by those coordinates is the tip of George Washington's nose on Mt. Rushmore. The accuracy of the estimate is the maximum distance between the location coordinates and the <b>ground</b> <b>truth.</b> We could say {{in this case that}} the estimate accuracy is 10 meters, meaning that the point on earth represented by the location coordinates is thought to be within 10 meters of George's nose—the <b>ground</b> <b>truth.</b> In slang, the coordinates indicate where we think George Washington's nose is located, and the <b>ground</b> <b>truth</b> is where it's really at. In practice a smart phone or hand-held GPS unit is routinely able to estimate the <b>ground</b> <b>truth</b> within 6-10 meters. Specialized instruments can reduce GPS measurement error to under a centimeter.|$|E
40|$|Current {{recommender}} systems largely {{focus on}} static, unstructured content. In many scenarios, {{we would like}} to recommend content that has structure, such as a trajectory of points-of-interests in a city, or a playlist of songs. Dubbed Structured Recommendation, this problem differs from the typical structured prediction problem in that there are multiple correct answers for a given input. Motivated by trajectory recommendation, we focus on sequential structures but in contrast to classical Viterbi decoding we require that valid predictions are sequences with no repeated elements. We propose an approach to sequence recommendation based on the structured support vector machine. For prediction, we modify the inference procedure to avoid predicting loops in the sequence. For training, we modify the objective function to account for the existence of multiple <b>ground</b> <b>truths</b> for a given input. We also modify the loss-augmented inference procedure to exclude the known <b>ground</b> <b>truths.</b> Experiments on real-world trajectory recommendation datasets show the benefits of our approach over existing, non-structured recommendation approaches. Comment: 18 page...|$|R
40|$|In {{document}} {{image analysis}} {{and especially in}} handwritten document image recognition, standard datasets play vital roles for evaluating performances of algorithms and comparing results obtained by different groups of researchers. In this paper, an unconstrained Persian handwritten text dataset (PHTD) is introduced. The PHTD contains 140 handwritten documents of three different categories written by 40 individuals. Total number of text-lines and words/subwords in the dataset are 1787 and 27073, respectively. In most of the PHTD documents either an overlapping or a touching text-lines is present. The average number of text-lines in documents of the PHTD is 13. Two types of <b>ground</b> <b>truths</b> based on pixels information and content information are generated for the dataset. Providing {{these two types of}} <b>ground</b> <b>truths</b> for the PHTD, it can be utilized in many areas of document image processing such as sentence recognition/understanding, text-line segmentation, word segmentation, word recognition, and character segmentation. To provide a framework for other researches, recent text-line segmentation results on this dataset are also reported...|$|R
40|$|The {{ability to}} deploy neural {{networks}} in real-world, safety-critical systems is severely {{limited by the}} presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses. This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks. We propose to address this difficulty through formal verification techniques. We construct ground truths: adversarial examples with provably minimal perturbation. We demonstrate how <b>ground</b> <b>truths</b> can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced to the ground truths; and also of defense techniques, by measuring the increase in distortion to <b>ground</b> <b>truths</b> in the hardened network versus the original. We use this technique to assess recently suggested attack and defense techniques...|$|R
50|$|Google {{has also}} {{recruited}} volunteers to check and correct <b>ground</b> <b>truth</b> data.|$|E
50|$|The <b>Ground</b> <b>Truth,</b> a 2006 {{documentary}} film about {{veterans of the}} Iraq War.|$|E
5000|$|US {{military}} slang uses [...] "ground truth" [...] {{to describe}} {{the reality of a}} tactical situation - as opposed to intelligence reports and mission plans. The term appears in the title of the Iraq War documentary film The <b>Ground</b> <b>Truth</b> (2006), and also in military publications, for example Stars and Stripes saying: [...] "Stripes decided {{to figure out what the}} <b>ground</b> <b>truth</b> was in Iraq." ...|$|E
5000|$|... #Subtitle level 3: Church of the Living God, the Pillar <b>Ground</b> of <b>Truth</b> for All Nations ...|$|R
40|$|Edge {{detection}} {{is one of}} {{the most}} important techniques used for image segmentation. Image segmentation remains a puzzled problem even after four decades of research. In this paper, a soft computing approach based on fuzzy logic is applied on histogram of an image to enhance edge detection technique. We used BSD images for experimentation and their respective <b>ground</b> <b>truths</b> for qualitative evaluation of proposed approach...|$|R
40|$|This {{paper is}} an {{argument}} against Truthmaker Necessitarianism — the doctrine {{that the existence of}} a truthmaker necessitates the truth of the proposition it makes true. Armstrong’s sufficiency argument for necessitarianism is examined and shown to be question begging. It is then argued in detail that truthmaking is a matter of <b>grounding</b> <b>truth</b> and that <b>grounding</b> is a dependency relation that neither entails nor reduces to necessitation...|$|R
