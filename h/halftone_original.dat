1|22|Public
40|$|In {{research}} {{presents an}} analysis of the main modes of DTP?s video displays based on LCD-monitors and determine their impact on the quality of color reproduction at <b>halftone</b> <b>original.</b> Based on the research found the impact of fixed regimes on quality of DTP?s video displays, which consist in the fact that with increasing color gamut there is a increase chances of uneven brightness on the display screen. The optimal period within 3 - 4 days to perform the calibration for a particular class LCD-monitors ensuring stable color reproduction level ?E < 1. The list of recommendations for the stabilization process prepress processing halftone originals, such as the analysis of the properties of a particular color monitor based on matching its color temperature, color gamut and uniformity of brightness on the standards set requirements for video of DTP?s video displays are specified. ????????? ???????? ??????? ???????? ?????????? ???????????? ??? ?? ???????? ??????????????? ????? ??????? ??????????. ?????????? ??????????? ?????? ?????????? ?????????? ??? ??????????? ?????? ????????? ? ???????????? ??????????? ????????????????????. ????????? ???????? ???????????? ??? ???????????? ???????? ??????????...|$|E
3000|$|..., will be {{selected}} to form a pair for data embedding. In the data embedding process, the <b>original</b> <b>halftone</b> image is partitioned into a group of [...]...|$|R
3000|$|The {{proposed}} {{method of}} reversible halftone data hiding technique uses pattern substitution method to embed and extract data into <b>halftone</b> images. The <b>original</b> image is partitioned into {{a set of}} nonoverlapping [...]...|$|R
50|$|Early laser {{printers}} {{from the late}} 1970s onward could also generate <b>halftones</b> but their <b>original</b> 300 dpi resolution limited the screen ruling to about 65 lpi. This was improved as higher resolutions of 600 dpi and above, and dithering techniques, were introduced.|$|R
3000|$|Finally, we can {{reconstruct}} the grayscale images for the <b>original</b> <b>halftone</b> image using the proposed hybrid method. In the image recovery process, we first predict the grayscale {{image from the}} restored halftone image by Gaussian filtering method. Then we rescan the restored halftone image again to find the pattern with the same contents of the embedded LUT template [...]...|$|R
40|$|Characterization of {{halftone}} texture {{is important}} for quantitative assessment of halftone quality. In this paper, we develop a new framework based on directional local sequency analysis and a filter bank structure. We decompose a halftone image into subband images from which we can easily reconstruct the <b>original</b> <b>halftone.</b> Based on these subband images, we define the directional sequency spectrum which {{is analogous to the}} 2 -D Fourier spectrum, and formulate several texture measures. Three test images are used to justify these measures. 1...|$|R
40|$|Photograph of {{photograph}} of Herbert Hoover, Southern California, 1932. "Subject: Herbert Hoover (copy); Client: Palmen-Jollison Co, 1547 Venice B[ou]l[e]v[ar]d; Classifications: Prominent People; Original Print Order: 1 -; Size: 8 x 10; Finish: gl[ossy]; from copy negative,. 50 ¢; Job: 9 - 7 - 28; Year: 1932 " [...] on envelope front. " 1 Original Photographs [...] Size: 11 x 14 positive print on film incl[uding] live neg[ative]; 1 contact 11 x 14 neg[ative]; 1 Copies (Print Extra) of <b>original</b> <b>Halftone</b> print; Amount: $ 5. 50, 2. 50, 1. 00; Extra Charges: Johnson" [...] on envelope back...|$|R
40|$|Abstract. This paper {{proposes a}} {{reversible}} data hiding method for error diffused halftone images. It employs statistics feature of pixel block patterns to embed data, and utilizes the HVS characteristics {{to reduce the}} introduced visual distortion. The watermarked halftone image can be perfectly recovered if it is intact, only a secret key is required. The method is suitable for the applications where the content accuracy of the <b>original</b> <b>halftone</b> image must be guaranteed, and it is easily extended {{to the field of}} halftone image authentication. Key words: look-up table, halftone images, reversible data hiding, human visual system. 1...|$|R
40|$|Different {{advantages}} {{stem from}} the digital restoration of documents of high cultural value. In this paper different techniques to digital restoration of antique documents are reported. In particular, we {{address the problem of}} virtual restoration of photographic prints. We propose a classification of defects related to their origin presenting also some non linear techniques able to restore the more diffused ones. In particular we briefly report some restoration results of water blotches, foxing and creases with some more details for a novel methodology to obtain a continuous tone image starting from an halftoned one, without no knowledge of the <b>original</b> <b>halftoning</b> technique. ...|$|R
40|$|We {{propose the}} Wavelet-based Inverse Halftoning via Deconvolution (WInliD) {{algorithm}} to perform inverse halftoning of error-diffused halftones. WInliD {{is motivated by}} our realization that inverse halftoning can be formulated as a deconvolution problem under Kite et al. 's linear approximation model for error diffusion halftoning. Under the linear model, the error-diffused <b>halftone</b> comprises the <b>original</b> gray-scale image blurred by a convolution operator and colored noise; the convolution operator and noise coloring {{are determined by the}} error diffusion tech- nique. WInliD performs inverse halftoning by first inverting the model-specified convolution operator and then attenuating the residual noise using scalar wavelet-domain shrinkage. Since WInliD is model-based, it is easily adapted to different error diffusion halftoning techniques. Usin...|$|R
40|$|Halftoning {{breaks the}} {{continuous}} tone image into a pattern of black dots and makes an illusion of a continuous image to the observer. Halftoned image is a binary version of the continuous toned image. Halftoning techniques have been widely accepted for the printing of newspapers, magazines as well as fax machines and printers. ^ Inverse halftoning is the reconstruction of continuous toned image from its halftoned version. The need for inverse halftoning arises from the need to manipulate the images, rotation or zooming etc In this thesis we deal with inverse halftoning of images halftoned by error diffusion algorithms. ^ Analysing the spectrum the low frequencies of the <b>halftoned</b> and <b>original</b> image are similar, but the halftoned image contains quantization noise in the higher frequencies. So {{the first step in}} our inverse halftoning method is the removal of high frequency quantization noise by Gaussian lowpass filtering. The second stage of our inverse halftoning algorithm is the Projection Onto Convex Sets (POCS) algorithm which alternatively projects the image onto the two convex sets, repeats and converges in the intersection of the two convex sets. (Abstract shortened by UMI.) ...|$|R
40|$|Abstract- Visual {{cryptography}} is {{a secret}} sharing scheme in which secret image is distributed in n number of shares such that, when this n shares are superimposed together, a hidden secret image is get. In extended visual cryptography, we use the cover image for share images, which will give integrating visual cryptography. In this paper, we propose a method for {{improving the quality of}} processing halftone images of the share images and the recovered secret image. In an extended visual cryptography scheme the size of the share images and the recovered image {{is the same as the}} <b>original</b> <b>halftone</b> secret image. The resulting scheme of extended visual cryptography maintains the quality of original secret image and its security. Index Terms- image processing, visual cryptography, secret sharing, halftone algorithm. I...|$|R
40|$|Visual {{cryptography}} is {{a secret}} sharing scheme which uses images distributed as shares such that, when the shares are superimposed, a hidden secret image is revealed. In extended visual cryptography, the share images are constructed to contain meaningful cover images, thereby providing opportunities for integrating visual cryptography and biometric security techniques. In this paper, we propose a method for processing halftone images that improves {{the quality of the}} share images and the recovered secret image in an extended visual cryptography scheme for which the size of the share images and the recovered image is the same as for the <b>original</b> <b>halftone</b> secret image. The resulting scheme maintains the perfect security of the original extended visual cryptography approach. Index Terms — cryptography, image processing, visual cryptography, secret sharing Table 1. Illustration of a (2, 2) VC Scheme with 4 Subpixels 1...|$|R
30|$|For the DHPS method, the <b>original</b> <b>halftone</b> {{was first}} {{generated}} using regular clustered-dot screening. However, in the embedding process, the image quality was degraded {{by the addition}} of intentional pixel shifts from the unknown hidden data; the addition rendered the halftone texture noisy. For the DHVCED method, even though the ED procedure diffuses the self-togging errors, when the embedded data become too large, the image quality is still affected and has the worm artifacts. By contrast, in the proposed method, the halftone patterns in the embeddable cells were converted into a dispersed-dot texture in the embedding process, and this was followed by the use of the DBS optimization framework, which searched for optimal halftone textures around every embeddable cell (i.e., the vicinity of an embeddable cell). The quality of the entire halftone image was improved as the quality of each local region was improved through DBS. Compared to DHVCED method, the proposed modified DBS optimization produces better image quality.|$|R
40|$|Introduction In this gem we {{introduce}} a technique borrowed and adapted from non-interactive hardcopy to real-time environments. Similar approaches {{comparable to the}} one presented in this gem are great resources for inspiration. Some are less flexible [Lake 00, Praun 01] while others are more complicated [Webb 02]. <b>Halftoning</b> in its <b>original</b> form is the procedure used to print images with gray levels using only black ink. It does so by varying the size of ink dots, and thus, the ratio of paper area to inked area. Viewed from a distance, a certain tone is perceived. A similar effect is used when an artist is drawing a picture with pen and ink, where more lines are put in areas that should appear darker. Yet another variation is employed in engravings or wood cuts, where the width of lines is adjusted to depict variations in shading. All these styles can be recreated by applying real-time halftoning. Similar to traditional halftoning, when the image intensity changes in a given neighborhoo...|$|R
40|$|Journal PaperWe {{propose the}} Wavelet-based Inverse Halftoning via Deconvolution (WInHD) {{algorithm}} to perform inverse halftoning of error-diffused halftones. WInHD {{is motivated by}} our realization that inverse halftoning can be formulated as a deconvolution problem under Kite et al. 's linear approximation model for error diffusion halftoning. Under the linear model, the error-diffused <b>halftone</b> comprises the <b>original</b> gray-scale image blurred by a convolution operator and colored noise; the convolution operator and noise coloring {{are determined by the}} error diffusion technique. WInHD performs inverse halftoning by first inverting the model-specified convolution operator and then attenuating the residual noise using scalar wavelet-domain shrinkage. Since WInHD is model-based, it is easily adapted to different error diffusion halftoning techniques. Using simulations, we verify that WInHD is competitive with state-of-the-art inverse halftoning techniques in the mean-squared-error sense and that it also provides good visual performance. We also derive and analyze bounds on WInHD's mean-squared-error performance as the image resolution increases. National Science FoundationNational Science FoundationAir Force Office of Scientific Researc...|$|R
40|$|Facsimile copying {{of images}} on {{polished}} mineral surfaces {{can be performed}} by rastration (dithering). when the <b>original</b> <b>halftone</b> image is replaced by the microstroke image. Different halftones are reproduced by pulse modulation of 2 -dimensional signal without loss in the visual perception due to the Nyquist theorem. Technological process of rasterised facsimile engraving provides the integral optical density {{on the length of}} raster element step. such that it is approximately equivalent to the optical density of the original on the same length. In the facsimile engraving, the destruction of mineral is considered in "small deviations". Theoretical analysis of the process results in the transfer function of required energy of chisel from the depth of penetration into mineral. Analysis of work in "small deviations" results in the formulation of three technological stages of transfer function, which allow to separate the deformation and destruction of compression kernel into the primary and the secondary with a minimal effect of additional chip...|$|R
30|$|Reversible {{data hiding}} can embed secret {{message in a}} {{reversible}} way. Relatively large amounts of secret data are embedded into a cover image so that the decoder can extract the hidden secret data and restore the original cover image without any distortion. Recently, a boundary-based PWLC method has been presented [8]. This method defines the same continuous 6 edge pixels as an embeddable block through searching for binary image edges. And then one can embed data in the pair of {{the third and fourth}} edge pixels. A reversible data hiding method for error-diffused halftone images is proposed [9]. This method employs statistics feature of pixel block patterns to embed data and utilizes the HVS characteristics to reduce the introduced visual distortion. The method is suitable for the applications, where the content accuracy of the <b>original</b> <b>halftone</b> image must be guaranteed, and it is easily extended to the field of halftone image authentication. However, these two methods have a drawback that the capacity of data hiding is still limited.|$|R
40|$|The {{purpose of}} this {{research}} was to investigate the effects of dry etching techniques on changing halftone dotsizes. Dry etching, or photographic color correction, has emerged in recent years as an alternative to traditional dot etching for making local corrections to color separation halftones. This study examines six factors related to dry etching. First, three types of <b>original</b> <b>halftone</b> dots were considered: laser formed (hard) dots from a Hell DC- 300 B scanner, laser formed (semi-hard) dots from a P. D. I, scanner, and contact screen (soft) dots from a Crosfield 510 Magnascan scanner. Second, the original dot-sizes being treated were fixed {{as close as possible to}} five target dot-sizes (5 $, 25 $, 50 $, 75 $, and 95 $). Third, three types of dry etching treat ments were applied a technique of overexposure; the use of a 4 -mil clear spacer film; and a technique of exposing through the base. Fourth, sixteen levels of corrective treatment were applied ranging from 1 times dot-for-dot exposure to 100 times. Fifth, two types of graphic arts films were test ed lith film and rapid access. Sixth, two generations of contact printing were compared original positive-to-inter mediate negative and intermediate negative-to-duplicate positive...|$|R
40|$|Because of {{its good}} image quality and {{moderate}} computational requirements, error diffusion {{has become a}} popular halftoning solution for desktop printers, especially those that use inkjet technology. In this dissertation, we will develop memory efficient error diffusion algorithms which greatly accelerate halftoning process for very low cost or large format printers, or reduce implementation cost of generic printers. In chapter 1, we first develop a reduced lookup table implementation of tone-dependent error diffusion (TDED). This algorithm relieves the requirement to store the tone-dependent parameters and halftone bitmap in standard TDED. Secondly, we introduce a new serial block-based approach to error diffusion. This depends on a novel intrablock scan path {{and the use of}} different parameter sets at different points along that path. We show that serial block-based error diffusion reduces off-chip memory access by a factor equal to the block height. In chapter 2, we consider fixed-rate scalar quantization of the accumulated diffused error (ADE) in error diffusion. We explain the encoding and decoding procedures which simultaneously operate with error diffusion, and show that the required on-chip random access memory (RAM) for storing the ADE can be reduced by a factor of 2, 3, or 4. We demonstrate 5 methods for designing different quantizers. In chapter 3, we develop a new framework based on local sequency analysis and a quasi filter bank structure in order to characterize halftone textures. Our framework provides a simple mean to decompose a halftone image into subband images, based on which we can easily reconstruct the <b>original</b> <b>halftone</b> and formulate texture characterizations. ...|$|R
40|$|Most halftoning {{algorithms}} assume {{there is}} no interaction between neighboring dots or if there is, it is additive. Without accounting for dot-gain effect, the printed image {{will not have the}} appearance predicted by the halftoning algorithm. Thus, there is need to embed a printer model in the halftoning algorithm which can predict such deviations and develop a halftone accordingly. ^ The direct binary search (DBS) algorithm employs a search heuristic to minimize the mean squared perceptually filtered error between the <b>halftone</b> and continuous-tone <b>original</b> images. We incorporate a measurement-based stochastic model for dot interactions of an electro-photographic printer within the iterative DBS binary halftoning algorithm. The stochastic model developed is based on microscopic absorptance and variance measurements. We present an efficient strategy to estimate the impact of 5 × 5 neighborhood pixels on the central pixel absorptance. By including the impact of 5 × 5 neighborhood pixels, the average relative error between the predicted tone and tone observed is reduced from around 23 % to 4 %. Also, the experimental results show that electro-photography-model based halftoning reduces the mottle and banding artifacts. ^ We also embed our printer model in stochastic clustered-dot halftoning algorithm CLU-DBS. The method CLU-DBS uses different filters in the initialization and update phases, in comparison to the same filters used in both the phases in the conventional DBS method. In this work, we derive a closed form expression for cost metric that is minimized in the CLU-DBS framework. This cost metric analysis not only provides us in-depth understanding of stochastic clustered-dot halftoning but also simplifies and speeds up the screen design algorithm and it also enables us to embed our printer model to generate visually pleasing stochastic clustered-dot textures. ...|$|R
40|$|Rapid access {{processing}} is {{a photographic}} processing method that combines high temperature processing and high energy developing agents to obtain very short induction periods, and thus, reduced processing times. In {{order to obtain}} maximum quality from the process, the rapid access halftone percent dot areas must be correctly evaluated according to established aim points. Traditional evaluation methods, either visual or instrumental, produce errors. These errors are the partial result of the unique dot characteristics of the process. These characteristics include soft, fringed dots; dots with low Dmax, found particularily in the shadow areas; loss of dot area on {{the tips of the}} halftone dots during plate exposure^ and high fog in the highlight areas. The illustration of the differences in dot fringe characteristics of a rapid access and a conventional lith halftone imaged with main exposures only were determined visually by the use of microphotographs illuminated with oblique illumination, and quanitatively by microdensitometer traces and film contacting the <b>original</b> <b>halftone</b> films, In all cases, the rapid access halftone dots: had a more highly fringed area when compared to the conventional lith halftone. The test designed to compare percent dot area of first generation rapid access halftones by zero referencing the dot area meter on the identified ghost dot to the percent dot area of their second generation hard dot contact films produced a poor correlation between the two sets of films. Of the three first generation halftones imaged with a main, main plus flash and main plus hump exposure; the main plus bump exposure produced the poorest correlation, particularly in the midtone area of the halftone scale. The compensation method designed to determine the effective percent dot area of first generation rapid access halftones by applying percent dot area correction factors found under various: halfone exposure conditions effectively reduced percent dot area error...|$|R
40|$|Methods {{are needed}} for {{evaluating}} the quality of augmented visual displays (AVID). Computational quality metrics will help summarize, interpolate, and extrapolate the results of human performance tests with displays. The FLM Vision group at NASA Ames has been developing computational models of visual processing and using them to develop computational metrics for similar problems. For example, display modeling systems use metrics for comparing proposed displays, halftoning optimizing methods use metrics to evaluate the difference between the <b>halftone</b> and the <b>original,</b> and image compression methods minimize the predicted visibility of compression artifacts. The visual discrimination models take as input two arbitrary images A and B and compute an estimate of the probability that a human observer will report that A is different from B. If A is an image that one desires to display and B is the actual displayed image, such an estimate can be regarded as an image quality metric reflecting how well B approximates A. There are additional complexities associated with the problem of {{evaluating the quality of}} radar and IR enhanced displays for AVID tasks. One important problem is the question of whether intruding obstacles are detectable in such displays. Although the discrimination model can handle detection situations by making B the original image A plus the intrusion, this detection model makes the inappropriate assumption that the observer knows where the intrusion will be. Effects of signal uncertainty need to be added to our models. A pilot needs to make decisions rapidly. The models need to predict not just the probability of a correct decision, but the probability of a correct decision by the time the decision needs to be made. That is, the models need to predict latency as well as accuracy. Luce and Green have generated models for auditory detection latencies. Similar models {{are needed for}} visual detection. Most image quality models are designed for static imagery. Watson has been developing a general spatial-temporal vision model to optimize video compression techniques. These models need to be adapted and calibrated for AVID applications...|$|R

