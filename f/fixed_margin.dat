25|82|Public
25|$|The {{securities}} can {{be issued}} {{with either a}} fixed interest rate or a floating rate under currency pegging system. Fixed rate ABS set the “coupon” (rate) {{at the time of}} issuance, in a fashion similar to corporate bonds and T-Bills. Floating rate securities may be backed by both amortizing and non-amortizing assets in the floating market. In contrast to fixed rate securities, the rates on “floaters” will periodically adjust up or down according to a designated index such as a U.S. Treasury rate, or, more typically, the London Interbank Offered Rate (LIBOR). The floating rate usually reflects the movement in the index plus an additional <b>fixed</b> <b>margin</b> to cover the added risk.|$|E
5000|$|But these involve {{risks of}} loss, {{profit-sharing}} modes of financing cannot guarantee banks income. Murabahah, with its <b>fixed</b> <b>margin,</b> offers the seller (i.e. the bank/financier) a more predictable income stream. One {{estimate is that}} 80% of Islamic lending is by murabahah. M. Kabir Hassan reports that murabaha accounts are quite profitable. As of 2005, [...] "the average cost efficiency" [...] for murabaha was [...] "74%, whereas average profit efficiency" [...] even higher at 84%. Hassan states, [...] "although Islamic banks are less efficient in containing cost, they are generally efficient in generating profit." ...|$|E
50|$|The {{securities}} can {{be issued}} {{with either a}} fixed interest rate or a floating rate under currency pegging system. Fixed rate ABS set the “coupon” (rate) {{at the time of}} issuance, in a fashion similar to corporate bonds and T-Bills. Floating rate securities may be backed by both amortizing and non-amortizing assets in the floating market. In contrast to fixed rate securities, the rates on “floaters” will periodically adjust up or down according to a designated index such as a U.S. Treasury rate, or, more typically, the London Interbank Offered Rate (LIBOR). The floating rate usually reflects the movement in the index plus an additional <b>fixed</b> <b>margin</b> to cover the added risk.|$|E
40|$|In {{this paper}} I {{investigate}} {{the problem of}} defining a multivariate dependence ordering. First, I provide a characterization of the concordance dependence ordering between multivariate random vectors with <b>fixed</b> <b>margins.</b> Central to the characterization is a multivariate generalization of a well-known bivariate elementary dependence increasing rearrangement. Second, to order multivariate random vectors with non- <b>fixed</b> <b>margins,</b> I impose a scale invariance principle {{which leads to a}} copula-based concordance dependence ordering. Finally, a wide family of copula-based measures of dependence is characterized to which Spearmanís rank correlation coefficient belongs. copula, concordance ordering, dependence measures, dependence orderings, multivariate stochastic dominance, supermodular ordering...|$|R
25|$|Fisher's noncentral hypergeometric {{distribution}} is used mostly for tests in contingency tables where a conditional distribution for <b>fixed</b> <b>margins</b> is desired. This can be useful, for example, for testing or measuring {{the effect of}} a medicine. See McCullagh and Nelder (1989).|$|R
40|$|International audienceCopulas are a {{relevant}} tool to build multivariate probability laws, from <b>fixed</b> <b>margins</b> and required degree of dependence. In this communication, we propose simple estimation methods {{dedicated to a}} semiparametric family of bivariate copulas. These copulas can be simply estimated through the estimation of their univariate generating function. We take profit of this result to estimate the associated measures of association {{as well as the}} high probability regions of the copula. These procedures are illustrated on simulations and on real data...|$|R
50|$|Starting {{with the}} 2007 season, the NHRA {{implemented}} a playoff system {{to determine the}} champion in each class, billed as the Countdown to The Championship. Each season {{is divided into two}} segments of races, with the bulk of the races making up the first segment, and the final events making up the second segment. After the first segment is complete, the drivers in each class at or above the cutoff point in the standings (8th place up to 2007 and 10th place thereafter) become eligible for the championship, while the drivers below the cutoff point are eliminated from championship contention, though they still participate in the remaining race events. The points for the advancing drivers are readjusted so that they are separated by a <b>fixed</b> <b>margin,</b> with first place receiving bonus points. The drivers then compete for the championship over the final races of the season.|$|E
40|$|There are {{essentially}} {{two kinds of}} non-inferiority hypotheses in an active control trial: <b>fixed</b> <b>margin</b> and ratio hypotheses. In a <b>fixed</b> <b>margin</b> hypothesis, the margin is a prespecified constant and the hypothesis is {{defined in terms of}} a single parameter that represents the effect of the active treatment relative to the control. The statistical inference for a <b>fixed</b> <b>margin</b> hypothesis is straightforward. The outstanding issue for a <b>fixed</b> <b>margin</b> non-inferiority hypothesis is how to select the margin, a task that may not be as simple as it appears. The selection of a fixed non-inferiority margin has been discussed in a few articles (Chi et al., 2003; Hung et al., 2003; Ng, 1993). In a ratio hypothesis, the control effect is also considered as an unknown parameter, and the non-inferiority hypothesis is then formulated as a ratio in terms of these two parameters, the treatment effect and the control effect. This type of non-inferiority hypothesis has also been called the fraction retention hypothesis because the ratio hypothesis can be interpreted as a retention of certain fraction of the control effect. Rothmann et al. (2003) formulated a ratio non-inferiority hypothesis in terms of log hazards in the time-to-event setting. To circumvent the complexity of having to deal with a rati...|$|E
40|$|This paper studies a {{model of}} a {{two-armed}} bandit played in parallel by two or more players. Players observe the actions of all other players, but not the outcome of their experiments. It is shown that if the parameters of the two arms (i. e., their success probabilities) are different by a <b>fixed</b> <b>margin,</b> all players eventually settle on the same arm with probability one in any Nash equilibrium of the game...|$|E
40|$|Volume {{tests in}} a {{regression}} context {{were introduced in}} 1939. The concept was revisited in 1985, {{this time in the}} case of con-tingency tables to refine the power of explanation of the χ 2 test. This article considers volume tests as a measure of dependency for tables with <b>fixed</b> <b>margins.</b> Building on earlier contributions, this article suggests the use of the volume test statistic as a mea-sure of dependence to be applied, for example, in the evaluation of linkage disequilibrium between markers...|$|R
40|$|Conditional {{inference}} on 2 x 2 {{tables with}} <b>fixed</b> <b>margins</b> and unequal probabilities {{is based on}} the extended hypergeometric distribution. If the support of the distribution is large, exact calculation of the conditional mean and variance of the table entry may be computationally demanding. This paper proposes a single-saddlepoint approximation to the mean and variance. While the approximation achieves acceptable accuracy for ordinary practical purposes, an alternative saddlepoint approximation is provided that gives much closer to exact results. It improves the accuracy of current approximations to up to more than four powers of ten...|$|R
40|$|The {{concordance}} {{number in}} the generalized shift test method is in terpreted as the trace of contingency tables with <b>fixed</b> <b>margins.</b> For {{the distribution of the}} concordance number the closeness of the normalap proximation and its improvement by the Gram-Charlier approximation are discussed. The Metropolis walk on the set of contingency tables is constructed. The two approximations are compared with the simula tion by the Metropolis walk, and then the Gram-Charlier approximation is shown to be extremely close to the distribution of the concordance number. Key words and phrases: Lexicostatistics, generalized shift test method, contingenc...|$|R
40|$|Gini {{index is}} a widely used measure of {{economic}} inequality. This article develops a general theory for constructing a confidence interval for Gini index with a specified confidence coefficient and a specified width. Fixed sample size methods cannot simultaneously achieve both the specified confidence coefficient and specified width. We develop a purely sequential procedure for interval estimation of Gini index with a specified confidence coefficient and a <b>fixed</b> <b>margin</b> of error. Optimality properties of the proposed method, namely first order asymptotic efficiency and asymptotic consistency are proved. All theoretical results are derived without assuming any specific distribution of the data...|$|E
40|$|We {{investigate}} whether adapting the transmit power optimally to the time-varying channel compares favorably, {{in terms of}} total energy consumption, to using a fixed link margin in wireless networks over short transmission distances. Over short distances, the circuit energy consumption dominates the transmission energy. For that reason, feeding back channel state information – a requirement for power control – {{may not be a}} power efficient strategy. We investigate both slow and fast power control and conclude, somewhat surprisingly, that using a <b>fixed</b> <b>margin</b> is typically more power-efficient than using power control. Index Terms — Sensor networks, energy efficiency, power control, feedback, fading 1...|$|E
40|$|This study {{investigates the}} {{determining}} factors of international corporate sukuk pricing {{in the primary}} market {{for the period of}} 2004 – 2015. We present novel evidence for a unique data set covering all 63 international corporate sukuk issuances consisting of both a <b>fixed</b> <b>margin</b> rating as well a credit rating score. Our cross-sectional analysis indicates that both credit rating and maturity are significant factors which reduce issue spreads, whereas sukuk margin rating increases issue spreads. More prominently, Shari’ah scholar reputation and the type of sukuk are not statistically significant factors in the explanation of the issue spread. Our results are comparable with determinants of conventional bond pricing, and our findings further confirm existing sukuk market practices...|$|E
40|$|Rapid {{research}} {{progress in}} genotyping techniques have allowed large genome-wide disease association studies. Existing methods often focus on determining associations between single loci and the disease. However, most diseases in-volve complex relationships between multiple loci an the environment. Here we describe {{a method for}} finding interacting loci by combining the traditionally used single-locus search with a search for multiway interactions on contingency tables. First, we develop an extended Fisher’s exact test for multidimensional contingency tables. To do so, we introduce toric ideals and construct a Markov chain on the space of multidimensional contingency tables with <b>fixed</b> <b>margins.</b> Second, we test our methods on simulated data, showing that we can detect interacting loci where single locus methods fail to do so. ...|$|R
40|$|KANN UM ANN (l!W 2 a, b) is oritisized {{on several}} points, {{primarily}} using results {{available in the}} lite-rature prior to his pu pers. In two papers KANNEMANN (1982 a, b) presents algorithms for the exact distri-bution of the probability and tost statistics in 2 -way contingency tables with <b>fixed</b> <b>margins.</b> In his first paper he states {{that he will not}} review the literature on the subject as he purports it to be of a purely theoretical nature and not relevant for applied statisticians, researchers, and experimenten. This disregard for the literature is, however, unfortunate as an essentially identical algorithm to KANNEMANN (1982 b) has boon proposed by HANCOCK (1 !) 75). HOWELL & GORDON (1976) and CANTOR (1979) made a number of small improvements on Hancock's algorithm. More efficient algorithms were, however, already available at that time, viz. BOULTON (1974), AORESTI & WACKERLY (1977), and BAKER (1977). The latter has also provisions for handling one or two <b>fixed</b> <b>margins.</b> At present {{the state of the art}} has even been further developed by PAGANO & TAYLOR-HALVORSON (1981), MEHTA & PATEL (1983), and VERBEEK, KROONENBERG, and KROONENHERO (1983). A virtually complete survey of the field is given by VERBEEK & KROONENBERG (1985). With respect to closed form formulae for the number of possible tables given the row and column margins, they still do not exist, but good approximation formulae have been developed by BOULTON & WALLACE (1973), GOOD (1976), and especially G A I L & MANTEL (1977). As to the use of the two-parametric gamma distribution as a better approxi-mation than the one parametric chi-square, it should be noted that YARNOLD (1970) presents extensive proof against the superiority of the gamma distribution...|$|R
5000|$|To {{promote the}} {{development}} of the power market & <b>fix</b> the trading <b>margin</b> in the interstate trading of electricity, if considered necessary.|$|R
40|$|In {{this paper}} we use game theoretic {{techniques}} {{to study the}} value of cooperation in distributed spectrum management problems. We show that the celebrated iterative water-filling algorithm {{is subject to the}} prisoner's dilemma and therefore can lead to severe degradation of the achievable rate region in an interference channel environment. We also provide thorough analysis of a simple two bands near-far situation where we are able to provide closed form tight bounds on the rate region of both <b>fixed</b> <b>margin</b> iterative water filling (FM-IWF) and dynamic frequency division multiplexing (DFDM) methods. This is the only case where such analytic expressions are known and all previous studies included only simulated results of the rate region. We then propose an alternative algorithm that alleviates some of the drawbacks of the IWF algorithm in near-far scenarios relevant to DSL access networks. We also provide experimental analysis based on measured DSL channels of both algorithms as well as the centralized optimum spectrum management...|$|E
40|$|Abstract: This paper {{proposes a}} utility-based data rate {{allocation}} algorithm to provide high-quality mobile video streaming over femtocell networks. We first derive a utility function {{to calculate the}} optimal data rates for maximizing the aggregate utilities of all mobile users in the femtocell. The total sum of optimal data rates {{is limited by the}} link capacity of the backhaul connections. Furthermore, electromagnetic cross-talk poses a serious problem for the backhaul connections, and its influence passes on to mobile users, as well as causing data rate degradation in the femtocell networks. We also have studied a <b>fixed</b> <b>margin</b> iterative water-filling algorithm to achieve the target data rate of each backhaul connection as a counter-measure to the cross-talk problem. The results of our simulation show that the algorithm is capable of minimizing the transmission power of backhaul connections while guaranteeing a high overall quality of service for all users of the same binder. In particular, it can provide the target data rate required to maximize user satisfaction with the mobile video streaming service over the femtocell networks...|$|E
40|$|This paper {{deals with}} higher-order vagueness in Williamson's 'logic of clarity'. Its {{aim is to}} prove that for 'fixed margin models' (W,d,α,[]) the notion of higher-order vagueness collapses to second-order vagueness. First, it is shown that <b>fixed</b> <b>margin</b> models can be reformulated in terms of {{similarity}} structures (W,~). The relation ~ {{is assumed to be}} reflexive and symmetric, but not necessarily transitive. Then, it is shown that the structures (W,~) come along with naturally defined maps h and s that define a Galois connection on the power set PW of W. These maps can be used to define two distinct boundary operators bd and BD on W. The main theorem of the paper states that higher-order vagueness with respect to bd collapses to second-order vagueness. This does not hold for BD, the iterations of which behave in quite an erratic way. In contrast, the operator bd defines a variety of tolerance principles that do not fall prey to the sorites paradox and, moreover, do not always satisfy the principles of positive and negative introspection...|$|E
40|$|In a {{weighted}} spatial network, as specified by an exchange matrix, the variances of the spatial values are {{inversely proportional to}} the size of the regions. Spatial values are no more exchangeable under independence, thus weakening the rationale for ordinary permutation and bootstrap tests of spatial autocorrelation. We propose an alternative permutation test for spatial autocorrelation, based upon exchangeable spatial modes, constructed as linear orthogonal combinations of spatial values. The coefficients obtain as eigenvectors of the standardised exchange matrix appearing in spectral clustering, and generalise to the weighted case the concept of spatial filtering for connectivity matrices. Also, two proposals aimed at transforming an acessibility matrix into a exchange matrix with with a priori <b>fixed</b> <b>margins</b> are presented. Two examples (inter-regional migratory flows and binary adjacency networks) illustrate the formalism, rooted in the theory of spectral decomposition for reversible Markov chains...|$|R
40|$|A scissor wing configuration, {{consisting}} of four adjustable wing surfaces, is {{compared with a}} comparable fixed wing baseline configuration. Wave drag, induced drag, viscous drag, thrust required, and gust loading are calculated for both configurations. The scissor wing is shown to have lower zero lift wave drag and higher total lift to drag ratios than the baseline. It is demonstrated that the scissor configurations' sweep can be programmed to keep the static <b>margin</b> <b>fixed.</b> Thrust required for both the <b>fixed</b> static <b>margin</b> case and a constant sweep angle case are presented with the scissor configuration requiring lower thrust levels. The gust loading ratio of the scissor wing to the baseline is also shown to be significantly less than 1. 0 for sweep angles greater than 20 degrees...|$|R
40|$|Earth-space radio systems {{operating}} at frequencies of 10 GHz and above are badly attenuated by rain, cloud, and atmospheric gases. As {{the frequencies of}} operational systems increase, it becomes increasingly uneconomic {{to compensate for the}} effects of fading {{through the use of a}} <b>fixed</b> fade <b>margin,</b> hence the implementation of fade mitigation techniques (FMT). The spatial and temporal variation o...|$|R
40|$|The {{problem of}} {{controlling}} the margin of a classifier is studied. A detailed analytical study is presented on how properties of the classification risk, such as its optimal link and minimum risk functions, {{are related to the}} shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classification margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the <b>fixed</b> <b>margin</b> counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. ...|$|E
40|$|In this article, {{we study}} rates of {{convergence}} of the generalization error of multi-class margin classifiers. In particular, we develop an upper bound theory quantifying the generalization error of various large margin classifiers. The theory permits a treatment of general margin losses, convex or nonconvex, in {{presence or absence}} of a dominating class. Three main results are established. First, for any <b>fixed</b> <b>margin</b> loss, there may be a trade-off between the ideal and actual generalization performances with respect to the choice of the class of candidate decision functions, which is governed by the trade-off between the approximation and estimation errors. In fact, different margin losses lead to different ideal or actual performances in specific cases. Second, we demonstrate, in a problem of linear learning, that the convergence rate can be arbitrarily fast in the sample size n depending on the joint distribution of the input/output pair. This goes beyond the anticipated rate O(n^- 1). Third, we establish rates of convergence of several margin classifiers in feature selection with the number of candidate variables p allowed to greatly exceed the sample size n but no faster than (n). Comment: Published at [URL] in the Electronic Journal of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Summary. A {{fundamental}} problem in many disciplines, including political science, sociology and epidemiology, is {{the examination of}} the association between two binary variables across a series of 2 2 tables, when only the margins are observed, and one of the margins is fixed. Two unobserved fractions are of interest, with only a single response per table, and it is this non-identifiability that is the inherent difficulty lying at the heart of ecological inference. Many methods have been suggested for ecological inference, often without a probabilistic model; we clarify the form of the sampling distribution and critique previous approaches within a for-mal statistical framework, thus allowing clarification and examination of the assumptions that are required under all approaches. A particularly difficult problem is choosing between models with and without contextual effects. Various Bayesian hierarchical modelling approaches are proposed to allow the formal inclusion of supplementary data, and/or prior information, with-out which ecological inference is unreliable. Careful choice of the prior within such models is required, however, since there may be considerable sensitivity to this choice, even when the model assumed is correct and there are no contextual effects. This sensitivity is shown to be a function of the number of areas and the distribution of the proportions in the <b>fixed</b> <b>margin</b> acros...|$|E
50|$|In 1906 C&A, the {{clothing}} retailer {{owned by the}} Brenninkmeijer family, stopped trying to increase gross <b>margin</b> percentages. They <b>fixed</b> the <b>margin</b> at 25% for decades and increased relative contribution per piece in stock per time period. Later when selling different commodities they shifted to relative contribution per square meter. They called this: Rekenen in Centen, in Plaats van Procenten. (Dutch for Calculating cash, not percentages).|$|R
40|$|The {{restoration}} and adaptative re-use of modern interiors require {{awareness of the}} particular role that has the interior space and, in it, the relationship between space and furnishings {{in the idea of}} modern architecture. This has meant the interior design not only as a defining element of detail or furniture, but as a place of primary genesis of the architecture, the design of which starts from the inside to define volumes, prospects and relations with neighboring areas. Programmatically modern architecture has placed the recognition and analysis of the needs, not only primary, but also cultural and existential {{at the base of the}} design action. Compared to the traditional idea of interior space through "areas", understood as spatial units defined by <b>fixed</b> <b>margins,</b> albeit interconnected in different ways, the architecture is divided into modernity by "fields", or chunks of space, which coagulate around actions or movements, or around the desire to build a relationship between the people, and therefore defined by the equipment of furniture, margins even virtual or labile, around which are gathering <b>fixed</b> <b>margins</b> of construction, acquiring and managing the way relationship between inside and outside. The void becomes the raison d'être of the full. The problem of interior space is therefore a key to understanding the meaning of modern architecture. Le Corbusier's work around the concept of space of the house, for example, could not regardless of the research on the role of furniture: the projects of the twenties may not be fully understood unless also investigating his work on furniture, through the projects presented in 1925 and in 1928; it is clearly the desire to entrust to the furnishings and their relationship a generative role in the architectural space and to make them able to live in the middle of the empty space. Awareness of the role of the relationship between furniture and interior spaces in the construction of the sense of modern architecture is essential to define strategies useful for its reuse. Issues relating to language are very important, however, is especially this relationship a crucial point at which you need to compare any conservation work or adaptation to new behaviors to make legible the original sense of the project. Some examples of architectures not transformed into museums but still inhabited, allows to analyze where and in what resides greater the sense of those works, and consequently the impact of their reuse...|$|R
40|$|Recurrent {{networks}} of polynomial threshold elements with random symmetric interactions are studied. Precise asymptotic estimates are derived for the expected number of fixed points {{as a function}} of the margin of stability. In particular, it is shown that there is a critical range of margins of stability (depending on the degree of polynomial interaction) such that the expected number of <b>fixed</b> points with <b>margins</b> below the critical range grows exponentially with the number of nodes in the network, while the expected number of <b>fixed</b> points with <b>margins</b> above the critical range decreases exponentially with the number of nodes in the network. The random energy model is also briefly examined and links with higher order neural networks and higher order spin glass models made explicit...|$|R
40|$|We compare three {{strategies}} {{to model the}} notion of vague knowl-edge in epistemic logic. Williamson’s margin for error semantics typically uses non-transitive Kripke structures, but invalidates the principle of positive introspection. On the contrary, Halpern’s two-dimensional semantics preserves the introspection principle, but using more complex uncertainty relations that are transitive. We present a modification of the standard epistemic semantics, which validates in-trospection over one-dimensional non-transitive structures, and study its correspondence with Halpern’s approach. While the semantics {{can be seen as}} the diagonalization of an explicit two-dimensional seman-tics, it affords a more intuitive representation of the uncertainty char-acteristic of vague knowledge. We examine the implications of the semantics concerning higher-order vagueness and the status of the non-transitivity of perceptual indiscriminability. We respond to a po-tential objection against our approach by giving a dynamic model of the way subjects with inexact knowledge make successive approxima-tions of their margin of error. 1 Intransitivity and introspection One central and debated aspect of the notion of inexact knowledge con-cerns the non-transitivity of the relation of indiscriminability and how it should be represented. On the epistemic account of vagueness put forward by Williamson, the intransitivity of the relation of indiscriminability is pre-sented as the main source for vagueness ([12]: 237). In [11] and in the Appendix to [12], Williamson formulates a <b>fixed</b> <b>margin</b> for error semantics for propositional modal logic in which the relation of epistemic uncertainty...|$|E
40|$|Non‐maturing liabilities, such as savings accounts, lack both {{predetermined}} {{maturity and}} reset dates {{due to the}} fact that the depositor is free to withdraw funds at any time and that the depository institution is free to change the rate. These attributes complicate the risk management of such products and no standardized solution exists. The problem is important however since non‐maturing liabilities typically make up a considerable part of the funding of a bank. In this report different modeling approaches to the risk management are described and a method for managing the interest rate risk is implemented. It is a replicating portfolio approach used to approximate the non‐maturing liabilities with a portfolio of fixed income instruments. The search for a replicating portfolio is formulated as an optimization problem based on regression between the deposit rate and market ratesseparated by a <b>fixed</b> <b>margin.</b> In the report two different optimization criteria are compared for the replicating portfolio, minimizing the standard deviation of the margin versus maximizing the risk‐adjusted margin represented by the Sharpe ratio, of which the latter is found to yield superior results. The choice of historical sample interval over which the portfolio is optimized seems to have a rather big impact on the outcome but recalculating the portfolio weights at regular intervals is found to stabilize the results somewhat. All in all, despite the fact that this type of method cannot fully capture the most advanced dynamics of the non‐maturing liabilities, a replicating portfolio still appears to be a feasible approach for the interest risk management. QC 2011070...|$|E
40|$|Cloud {{computing}} {{holds the}} exciting potential of elastically scaling computation to match time-varying demand, thus {{eliminating the need}} to provision for peak demand. However, the uncertainty of variable loads necessitate the use of 2 ̆ 2 margins 2 ̆ 2 - servers that must be held active to absorb unpredictable potential load surges - {{which can be a}} signicant fraction of overall cost. Further, naively switching to an on-demand cloud model can actually degrade 2 ̆ 2 true costs 2 ̆ 2 (server costs that would be incurred even if margin costs disappeared) because of the fundamental economic rule wherein on-demand services/goods cost more compared to 2 ̆ 2 reserved 2 ̆ 2 goods/services where the user bears some commitment (i. e., on-demand customers must pay a premium in exchange for not undertaking the fixed-cost risk that committed customers undertake). This paper addresses the twin challenges of minimizing margin costs and true costs in an infrastructure-as-a-service (IaaS) cloud. Our paper makes the following two contributions. To address the problem of margin costs, we make two key observations based on real web server traces. First, rather than use a <b>fixed</b> <b>margin,</b> we observe that the margin may be load-dependent. For example, the margin required at low loads may be higher than the margin required at high loads. Second, we observe that the 2 ̆ 2 tolerance 2 ̆ 2 - the fraction of time when the response time target may be violated - need not be uniform across all load levels. For example, compared to a case where we satisfy requests within the target response time 95...|$|E
40|$|A previously-proposed {{method of}} {{constructing}} spatially-extended gauge-invariant three-quark operators {{for use in}} Monte Carlo lattice QCD calculations is tested, and a methodology for using these operators to extract the energies {{of a large number}} of baryon states is developed. This work is part of a long-term project undertaken by the Lattice Hadron Physics Collaboration to carry out a first-principles calculation of the low-lying spectrum of QCD. These techniques are then applied in the construction of nucleon operators. Correlation matrix elements between these operators are estimated using 200 configurations on a 12 ^ 3 × 48 anisotropic lattice in the quenched approximation with unphysically heavy u, d quark masses (the pion mass is approximately 700 MeV). After a change of basis operators using a variational method is applied, the energies of up to eight states are extracted in each symmetry channel. Although comparison with experiment is not justified, the pattern of levels obtained qualitatively agrees with the observed spectrum. A comparison with quark model predictions is also made; the quark model predicts more low-lying even-parity states than this study yields, but both the quark model and this study predict more odd-parity states near 2 GeV than currently observed in experiments. Comment: Ph. D. Dissertation, 180 pages, 32 figures; v 2 : <b>fixed</b> <b>margins</b> and some typo...|$|R
5000|$|Tracker rate - a {{variable}} rate that {{is equal to}} a published interest rate (typically LIBOR, plus a <b>fixed</b> interest rate <b>margin.</b> For instance LIBOR + 1.5%, so if at any time LIBOR is 4% per year, the interest rate charged to the borrower would be 5.5% per year.|$|R
40|$|This report, {{prepared}} by Deloitte Touche Tohmatsu transfer pricing specialists with {{the funding of}} the IDB, compares the transfer pricing regulations in the OECD guidelines, which constitute the international standard that OECD member countries have agreed {{should be used in}} analyzing transfer pricing issues between multinational enterprises and tax administrations, and the situation in Argentina, Brazil, Mexico, the United States, and Venezuela. It concludes that transfer pricing policies are not exclusively about taxation. Transfer pricing regulations should enable tax administrations to obtain a fair tax base {{at the same time they}} minimize the risks of double taxation for multinational enterprises. The OECD Guidelines provide the guidance on transfer pricing issues for both taxpayers and tax authorities by establishing a comparison with what would have happened between independent enterprises. However, there is no universal solution to transfer pricing issues in the OECD Guidelines. Also that the preface to the reform introducing the transfer pricing rules states that the tax administration, for purposes of computing the statutory margins for the import and export RP and cost plus methods, will take into consideration economic analysis by industry sector, branch of activity and based on the current economic situation. Adjustment will be allowed when the economic circumstances necessitate adjustment. This flexibility is different from the Brazilian rules, which provide <b>fixed</b> <b>margins</b> for all economic activities unless the taxpayer establishes a different margin with data from official publications or research carried out by a qualified firm. This report, {{prepared by}} Deloitte Touche Tohmatsu transfer pricing specialists with the funding of the IDB, compares the transfer pricing regulations in the OECD guidelines, which constitute the international standard that OECD member countries have agreed should be used in analyzing transfer pricing issues between multinational enterprises and tax administrations, and the situation in Argentina, Brazil, Mexico, the United States, and Venezuela. It concludes that transfer pricing policies are not exclusively about taxation. Transfer pricing regulations should enable tax administrations to obtain a fair tax base at the same time they minimize the risks of double taxation for multinational enterprises. The OECD Guidelines provide the guidance on transfer pricing issues for both taxpayers and tax authorities by establishing a comparison with what would have happened between independent enterprises. However, there is no universal solution to transfer pricing issues in the OECD Guidelines. Also that the preface to the reform introducing the transfer pricing rules states that the tax administration, for purposes of computing the statutory margins for the import and export RP and cost plus methods, will take into consideration economic analysis by industry sector, branch of activity and based on the current economic situation. Adjustment will be allowed when the economic circumstances necessitate adjustment. This flexibility is different from the Brazilian rules, which provide <b>fixed</b> <b>margins</b> for all economic activities unless the taxpayer establishes a different margin with data from official publications or research carried out by a qualified firm...|$|R
