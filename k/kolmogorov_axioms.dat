26|28|Public
25|$|These {{simple but}} {{counterintuitive}} puzzles {{are used as}} a standard example in teaching probability theory. Their solution illustrates some basic principles, including the <b>Kolmogorov</b> <b>axioms.</b>|$|E
25|$|To {{solve the}} problem, either {{formally}} or informally, one must assign probabilities {{to the events}} of drawing each of the six faces of the three cards. These probabilities could conceivably be very different; perhaps the white card is larger than the black card, or the black side of the mixed card is heavier than the white side. The statement of the question does not explicitly address these concerns. The only constraints implied by the <b>Kolmogorov</b> <b>axioms</b> are that the probabilities are all non-negative, and they sum to 1.|$|E
50|$|From the <b>Kolmogorov</b> <b>axioms,</b> one can deduce other useful {{rules for}} calculating probabilities.|$|E
50|$|The {{mathematical}} {{treatment of}} probabilities, {{especially when there}} are infinitely many possible outcomes, was facilitated by <b>Kolmogorov's</b> <b>axioms</b> (1933).|$|R
50|$|Since the 1990s, {{the theory}} has {{gathered}} strong momentum, initiated by comprehensive foundations {{put forward by}} Walley, who coined the term imprecise probability, by Kuznetsov, and by Weichselberger, who uses the term interval probability. Walley's theory extends the traditional subjective probability theory via buying and selling prices for gambles, whereas Weichselberger's approach generalizes <b>Kolmogorov's</b> <b>axioms</b> without imposing an interpretation.|$|R
40|$|We {{show that}} the known proofs of Bell's inequalities contain {{algebraic}} manipulations that are not appropriate within the syntax of <b>Kolmogorov's</b> <b>axioms</b> for probability theory without detailed justification. Such justification {{can be achieved by}} a variant of the techniques used in Bell-type proofs but only for a subclass of objective local parameter spaces. It cannot be achieved for an extended parameter space that is still objective local and that includes instrument parameters correlated by both time and setting dependencies...|$|R
50|$|These {{simple but}} {{counterintuitive}} puzzles {{are used as}} a standard example in teaching probability theory. Their solution illustrates some basic principles, including the <b>Kolmogorov</b> <b>axioms.</b>|$|E
50|$|In Kolmogorov's {{probability}} theory, {{the probability}} P of some event E, denoted , is usually defined such that P satisfies the <b>Kolmogorov</b> <b>axioms,</b> {{named after the}} Russian mathematician Andrey Kolmogorov, which are described below.|$|E
50|$|Note: This {{approach}} {{results in}} a probability measure {{that is consistent with}} the original probability measure and satisfies all the <b>Kolmogorov</b> <b>axioms.</b> This conditional probability measure also could have resulted by assuming that the relative magnitude of the probability of A with respect to X will be preserved with respect to B (cf. a Formal Derivation below).|$|E
40|$|AbstractThe {{concept of}} interval-probability is {{motivated}} by the goal to generalize classical-probability {{so that it can}} be used for describing uncertainty in general. The foundations of the theory are based on a system of three axioms — in addition to <b>Kolmogorov's</b> <b>axioms</b> — and definitions of independence as well as of conditional-probability. The resulting theory does not depend upon interpretations of the probability concept. As an example of generalising classical results Bayes' theorem is described — other theorems are only mentioned...|$|R
40|$|Many argued (Accardi and Fedullo, Pitowsky) that <b>Kolmogorov’s</b> <b>axioms</b> of {{classical}} probability theory are incompatible with quantum probabilities, {{and this is}} the reason for the violation of Bell’s inequalities. Szabó showed that, in fact, these inequalities are not violated by the experimentally observed frequencies if we consider the real, “effective” frequencies. We prove in this work a theorem which generalizes this result: “effective ” frequencies associated to quantum events always admit a Kolmogorovian representation, when these events are collected through different experimental set ups, the choice of which obeys a classical distribution. ...|$|R
5000|$|A quasiprobability {{distribution}} is a mathematical object {{similar to a}} probability distribution but which relaxes some of <b>Kolmogorov's</b> <b>axioms</b> of probability theory. Although quasiprobabilities share several of general features with ordinary probabilities, such as, crucially, the ability to yield expectation values {{with respect to the}} weights of the distribution, they all violate the σ-additivity axiom, because regions integrated under them do not represent probabilities of mutually exclusive states. To compensate, some quasiprobability distributions also counterintuitively have regions of negative probability density, contradicting the first axiom. Quasiprobability distributions arise naturally in the study of quantum mechanics when treated in phase space formulation, commonly used in quantum optics, time-frequency analysis, [...] and elsewhere.|$|R
50|$|A lecture of his in the 1960s {{concerning}} {{the use of}} Bayesian methods for betting on horses gave John Craven USN, a US Navy scientist {{the idea of using}} Bayesian methods to search for a missing US Air Force hydrogen bomb lost near Palomares, Spain in the 1966 Palomares B-52 crash. Craven used the same methods again in the search for the lost submarine USS Scorpion in 1968. Raiffa has analysed situations involving the use of subjective probability and argues that subjective probabilities should follow the same rules (the <b>Kolmogorov</b> <b>axioms)</b> as objective, frequency-based probabilities.|$|E
50|$|To {{solve the}} problem, either {{formally}} or informally, one must assign probabilities {{to the events}} of drawing each of the six faces of the three cards. These probabilities could conceivably be very different; perhaps the white card is larger than the black card, or the black side of the mixed card is heavier than the white side. The statement of the question does not explicitly address these concerns. The only constraints implied by the <b>Kolmogorov</b> <b>axioms</b> are that the probabilities are all non-negative, and they sum to 1.|$|E
50|$|Quantum {{probability}} {{provides a}} new way to explain human probability judgment errors including the conjunction and disjunction errors. A conjunction error occurs when a person judges the probability of a likely event L and an unlikely event U to be greater than the unlikely event U; a disjunction error occurs when a person judges the probability of a likely event L to be greater than the probability of the likely event L or an unlikely event U. Quantum probability theory is a generalization of Bayesian probability theory because it is based on a set of von Neumann axioms that relax some of the classic <b>Kolmogorov</b> <b>axioms.</b> The quantum model introduces a new fundamental concept to cognition—the compatibility versus incompatibility of questions and the effect this can have on the sequential order of judgments. Quantum probability provides a simple account of conjunction and disjunction errors as well as many other findings such as order effects on probability judgments.|$|E
40|$|Abstract (for The Review of Metaphysics) : This paper {{outlines}} {{a genuinely}} pragmatist conception of propensity, and defends it against common {{objections to the}} propensity interpretation of probability, prominently Humphreys’ paradox. The paper reviews the paradox and identifies one of its key assumptions, the identity thesis, according to which propensities are probabilities (under a suitable interpretation of <b>Kolmogorov’s</b> <b>axioms).</b> The identity thesis is also involved in empiricist propensity interpretations deriving from Popper’s influential original proposal, and makes such interpretations untenable. As an alternative, I urge a return to Charles Peirce’s original insights on probabilistic dispositions, and offer a reconstructed version of his pragmatist conception, which rejects the identity thesis. – Correspondence to: msuarez@filos. ucm. e...|$|R
40|$|Can {{the various}} {{meanings}} of probability be reconciled? 1 Glenn Shafer 2 The stand-off between the frequentist and subjectivist interpretations of probability has hardened into a philosophy. According to this philosophy, probability begins as pure mathematics. The different meanings of probability correspond to different interpretations of <b>Kolmogorov's</b> <b>axioms.</b> This chapter urges {{a slightly different}} philosophy. Probability begins with the description of an unusual {{situation in which the}} different meanings of probability are unified. It is this situation—not merely the mathematics of probability—that we use in applications. And there are many ways of using it. This philosophy reconciles the various meanings of probability at a level deeper than the level of axioms. It allows us to bring together in one framework the unified eighteenth-century understanding of probability, the frequentis...|$|R
40|$|We {{present a}} {{generalization}} {{of the problem}} of pattern recognition to arbitrary probabilistic models. This version deals with the problem of recognizing an individual pattern among a family of different species or classes of objects which obey probabilistic laws which do not comply with <b>Kolmogorov’s</b> <b>axioms.</b> We show that such a scenario accommodates many important examples, and in particular, we provide a rigorous definition of the classical and the quantum pattern recognition problems, respectively. Our framework allows for the introduction of non-trivial correlations (as entanglement or discord) between the different species involved, opening the door to a new way of harnessing these physical resources for solving pattern recognition problems. Finally, we present some examples and discuss the computational complexity of the quantum pattern recognition problem, showing that the most important quantum computation algorithms can be described as non-Kolmogorovian pattern recognition problems...|$|R
5000|$|Albert Einstein, in 1916, {{introduced}} the Einstein notation which summed over {{a set of}} indexed terms in a formula, thus exerting notational brevity. Arnold Sommerfeld would create the contour integral sign in 1917. Also in 1917, Dimitry Mirimanoff proposes axiom of regularity. In 1919, Theodor Kaluza would solve general relativity equations using five dimensions, the results would have electromagnetic equations emerge. This would be published in 1921 in [...] "Zum Unitätsproblem der Physik". In 1922, Abraham Fraenkel and Thoralf Skolem independently proposed replacing the axiom schema of specification with the axiom schema of replacement. Also in 1922, Zermelo-Fraenkel set theory was developed. In 1923, Steinmetz would publish Four Lectures on Relativity and Space. Around 1924, Jan Arnoldus Schouten would develop the modern notation and formalism for the Ricci calculus framework during the absolute differential calculus applications to general relativity and differential geometry {{in the early twentieth}} century. In 1925, Enrico Fermi would describe a system comprising many identical particles that obey the Pauli exclusion principle, afterwards developing a diffusion equation (Fermi age equation). In 1926, Oskar Klein would develop the Kaluza-Klein theory. In 1928, Emil Artin abstracted ring theory with Artinian rings. In 1933, Andrey Kolmogorov introduces the <b>Kolmogorov</b> <b>axioms.</b> In 1937, Bruno de Finetti deduced the [...] "operational subjective" [...] concept.|$|E
40|$|The <b>Kolmogorov</b> <b>axioms</b> for {{probability}} {{functions are}} placed {{in the context of}} signed meadows. A completeness theorem is stated and proven for the resulting equational theory of probability calculus. Elementary definitions of probability theory are restated in this framework. Comment: 20 pages, 6 tables, some minor errors are correcte...|$|E
40|$|Most of the {{standard}} proofs of the Bell theorem {{are based on the}} <b>Kolmogorov</b> <b>axioms</b> of probability theory. We show that these proofs contain mathematical steps that cannot be reconciled with the <b>Kolmogorov</b> <b>axioms.</b> Specifically we demonstrate that these proofs ignore the conclusion of a theorem of Vorob’ev on the consistency of joint distributions. As a consequence Bell’s theorem stated in its full generality remains unproven, in particular, for extended parameter spaces that are still objective local and that include instrument parameters that are correlated by both time and instrument settings. Although the Bell theorem correctly rules out certain small classes of hidden variables, for these extended parameter spaces {{the standard}} proofs come to a halt. The Greenberger-Horne-Zeilinger (GHZ) approach is based on similar fallacious arguments. For this case we are able to present an objective local computer experiment that simulates the experimental test of GHZ performed by Pan, Bouwmeester, Daniell, Weinfurter and Zeilinger and that directly contradicts their claim that Einstein-local elements of reality can neither explain the results of quantum mechanical theory nor their experimental results. ...|$|E
40|$|Query {{independent}} features (also called document priors), such as {{the number}} of incoming links to a document, its PageRank, or the length of its associated URL, have been explored to boost the retrieval effectiveness of Web Information Retrieval (IR) systems. The combination of such query independent features could further enhance the retrieval performance. However, most current combination approaches are based on heuristics, which ignore the possible dependence between the document priors. In this paper, we present a novel and robust method for combining document priors in a principled way. We use a conditional probability rule, which is derived from <b>Kolmogorov’s</b> <b>axioms.</b> In particular, we investigate the retrieval performance attainable by our combination of priors method, in comparison to the use of single priors and a heuristic prior combination method. Furthermore, we examine when and how document priors should be combined...|$|R
40|$|<b>Kolmogorov's</b> first <b>axiom</b> of {{probability}} is probability takes values between 0 and 1; however, in Cox's derivation {{of probability}} having a maximum value of unity is arbitrary since he derives probability {{as a tool}} to rank degrees of plausibility. Probability can then be used to make inferences in instances of incomplete information, which is the foundation of Baysian probability theory. This article formulates a rule, which if obeyed, allows probability to take complex values and still be consistent with the interpretation of probability theory as being a tool to rank plausibility. It is then shown that Kirkwood distributions and the conditional complex probability distributions proposed by Hofmann do not obey this rule and therefore cannot rank plausibility. Not only do these quasiprobability distributions relax <b>Kolmogorov's</b> first <b>axiom</b> of probability, they also are void of the defining property of a probability distribution from a Coxian and Baysian perspective - they lack the ability to rank plausibility...|$|R
50|$|For {{each person}} A, we can define a (partial) {{function}} CA mapping {{the set of}} propositions to the closed interval 1 by stipulating that for a proposition P CA(P)=t {{if and only if}} C has a degree of belief t in the proposition P. Ramsey and de Finetti independently attempted to show that if A is rational, CA is a probability function: that is, CA satisfies the standard (<b>Kolmogorov)</b> probability <b>axioms.</b>|$|R
40|$|AbstractIn {{calculations}} with matrices, block calculations play {{an important}} role. To elucidate the essential structures of block decompositions, we shall in this paper introduce a simple system of axioms which guarantees block calculations of rings. The axioms {{can be interpreted as}} rules of some kind of information filters. We shall also give another system of axioms in terms of idempotents which is equivalent to the above mentioned system and is similar to the <b>Kolmogorov</b> <b>axioms</b> of probability spaces...|$|E
40|$|Probabilistic models {{require the}} notion of event space for {{defining}} a probability measure. An event space has a probability measure which ensues the <b>Kolmogorov</b> <b>axioms.</b> However, the probabilities observed from distinct sources, {{such as that of}} relevance of documents, may not admit a single event space thus causing some issues. In this article, some results are introduced for ensuring whether the observed prob- abilities of relevance of documents admit a single event space. More- over, an alternative framework of probability is introduced, thus chal- lenging the use of classical probability for ranking documents. Some reflections on the convenience of extending the classical probabilis- tic retrieval toward a more general framework which encompasses the issues are made...|$|E
40|$|After recalling proofs of the Bell {{inequality}} {{based on}} the assumptions of separability and of noncontextuality, the most general noncontextual contrapositive conditional probabilities consistent with the Aspect experiment are constructed. In general these probabilities are not all positive. Keywords. Bell, <b>Kolmogorov,</b> <b>axioms</b> PACS Nos 2. 0 1. Introduction At {{the beginning of the}} 21 st century, we are familiar with the idea that Euclid's axioms of geometry do not in general apply to the physical world [...] - when a gravitational `field' is present, Einstein's general theory of relativity has shown us how to use non-Euclidean geometry. Does quantum mechanics similarly imply that classical logic and classical probability theory also do not apply to the physical world? There is no such unanimity {{as in the case of}} geometry. Bas van Fraassen [1] states categorically: The new phenomena do not force violations of classical probability theory or logic. On the other hand, Kummerer and Maassen [ [...] ...|$|E
40|$|The {{question}} {{concerning the}} physical realizability of a probability distribution is of quite importance in Quantum foundations. Specker first {{pointed out that}} this question cannot be answered from <b>Kolmogorov's</b> <b>axioms</b> alone. Lately, this observation of Specker has motivated simple principles (exclusivity principle/ local orthogonality principle) that can explain quantum limit regarding the possible sets of experimental probabilities in various nonlocality and contextuality experiments. We study Specker's observation in the simplest scenario involving three inputs each with two outputs. Then using only linear constraints imposed on joint probabilities by this principle, we reveal unphysical nature of Garg-Mermin (GM) correlation. Interestingly, GM correlation was proposed to falsify the following suggestion by Fine: if the inequalities of Clauser and Horne (CH) holds, then there exists a deterministic local hidden-variable model for a spin- 1 / 2 correlation experiment of the Einstein-Podolsky-Rosen type, even when more than two observables are involved on each side. Our result establishes that, unlike in the CH scenario, the local orthogonality principle at single copy level is not equivalent to the no-signaling condition in the GM scenario. Comment: New references are adde...|$|R
40|$|It {{is natural}} {{to think that}} {{questions}} in the metaphysics of chance are independent of the mathematical representation of chance in probability theory. After all, chance is a feature of events that comes in degrees and the mathematical representation of chance concerns these degrees but leaves the nature of chance open. The mathematical representation of chance could thus, un-controversially, be taken to be what it is commonly taken to be: a probability measure satisfying <b>Kolmogorov’s</b> <b>axioms.</b> The metaphysical questions about chance seem to be left open by all this. I argue {{that this is a}} mistake. The employment of real numbers as measures of chance in standard probability theory brings with it commitments in the metaphysics of (objective) chance that are not only substantial but also mistaken. To measure chance properly we need to employ extensions of the real numbers that contain infinitesimals: positive numbers that are infinitely small. But simply using infinitesimals alone is not enough, as a number of arguments show. Instead we need to put three ideas together: infinitesimals, the non-locality of chance and flexibility in measurement. Only those three together give us a coherent picture of chance and its mathematical representation...|$|R
40|$|It {{is argued}} that realism and true {{randomness}} are fully compatible. Realistic true random events are acts of pure creation that obey strict laws, but do not necessarily satisfy <b>Kolmogorov's</b> <b>axioms</b> of probabilities. Realistic true randomness {{is some sort of}} nondeterministic force, or propensity of physical systems to manifest such and such properties under such and such conditions. Realistic random events reflect preexisting properties, as required by realism, simply the reflection is not deterministic; still, the preexisting properties determine the propensities of the different possible events. It {{is argued that}} deterministic extensions of quantum physics are necessarily incompatible with special relativity. Hence, from today's violations of Bell's inequalities one can conclude that all future physics theories will display true randomness as does quantum physics. It is argued that accepting true randomness and realism leads to new questions with interesting answers, allowing one 1) to study nonlocality in configurations with many independent sources and 2) to bound how much free will is needed for a proper violation of Bell's inequality. Comment: Invited white paper for Quantum Physics and the Nature of Reality, John Polkinghorne 80 th Birthday Conference. St Anne's College, Oxford. 26 - 29 September 201...|$|R
40|$|In this book, I {{attempt to}} reach two goals. The first is purely mathematical: to clarify {{some of the}} basic {{concepts}} of probability theory. The second goal is physical: to clarify the methods to be used when handling the information brought by measurements, in order to understand how accurate are the inferences they allow. Probability theory is solidly based on <b>Kolmogorov</b> <b>axioms,</b> but the basic inference tool provided by Kolmogorov’s theory is the definition of conditional probability. While some simple problems can be solved though this notion of conditional probability, more elaborate problems, in particular, most of the inference problems that use inaccurate observations require a more advanced probability theory. When considering sets, there are some well known notions, for instance, the intersection of two sets, or, when a mapping is considered between two sets, the notion of image of a set, or of reciprocal image of a set. I develop in this book the theory that generalizes these notions when, instead of sets...|$|E
40|$|It {{is argued}} that {{probability}} should be defined implicitly by the distributions of possible measurement values characteristic of a theory. These distributions are tested by, but not defined in terms of, relative frequencies of occurrences of events of a specified kind. The adoption of an a priori probability in an empirical investigation constitutes part of the formulation of a theory. In particular, an assumption of equiprobability in a given situation is merely one hypothesis inter alia, which can be tested, like any other assumption. Probability in relation to some theories - for example quantum mechanics - need not satisfy the <b>Kolmogorov</b> <b>axioms.</b> To illustrate how two theories about the same system can generate quite different probability concepts, and not just different probabilistic predictions, a team game for three players is described. If only classical methods are allowed, a 75 % success rate at best can be achieved. Nevertheless, a quantum strategy exists that gives a 100 % probability of winning...|$|E
40|$|A quantum {{probability}} {{model is}} introduced {{and used to}} explain human probability judgment errors including the conjunction and disjunction fallacies, averaging effects, unpacking effects, and order effects on inference. On the one hand, quantum theory is similar to other categorization and memory models of cognition in that it relies on vector spaces defined by features and similarities between vectors to determine probability judgments. On the other hand, quantum probability theory is a generalization of Bayesian probability theory because {{it is based on}} a set of (von Neumann) axioms that relax some of the classic (<b>Kolmogorov)</b> <b>axioms.</b> The quantum model is compared and contrasted with other competing explanations for these judgment errors, including the anchoring and adjustment model for probability judgments. In the quantum model, a new fundamental concept in cognition is advanced—the compati-bility versus incompatibility of questions and the effect this can have on the sequential order of judgments. We conclude that quantum information-processing principles provide a viable and promising new way to understand human judgment and reasoning...|$|E
40|$|In {{probability}} and statistics, {{uncertainty is}} usually quantified using single-valued probabilities satisfying <b>Kolmogorov’s</b> <b>axioms.</b> Generalisation of classical probability theory leads to various less restrictive representations of uncertainty which are collectively {{referred to as}} imprecise probability. Several approaches to statistical inference using imprecise probability have been suggested, {{one of which is}} nonparametric predictive inference (NPI). The multinomial NPI model was recently proposed [14, 17], which quantifies uncertainty in terms of lower and upper probabilities. It has several advantages, one being the facility to handle multinomial data sets with unknown numbers of possible outcomes. The model gives inferences about a single future observation. This thesis comprises new theoretical developments and applications of the multinomial NPI model. The model is applied to selection problems, for which multiple future observations are also considered. This is the first time inferences about multiple future observations have been presented for the multinomial NPI model. Applications of NPI to classification are also considered and a method is presented for building classification trees using the maximum entropy distribution consistent with the multinomial NPI model. Two algorithms, one approximate and one exact, are proposed for finding this distribution. Finally, a new NPI model is developed for the case of multinomial data with subcategories and several properties of this model are proven. Declaration The work in this thesis is based on research carried out at the Statistics an...|$|R
40|$|One {{implication}} of Bell's theorem {{is that there}} cannot in general be hidden variable models for quantum mechanics that both are noncontextual and retain {{the structure of a}} classical probability space. Thus, some hidden variable programs aim to retain noncontextuality at the cost of using a generalization of the <b>Kolmogorov</b> probability <b>axioms.</b> We generalize a theorem of Feintzeig (2015) to show that such programs are committed to the existence of a finite null cover for some quantum mechanical experiments, i. e., a finite collection of probability zero events whose disjunction exhausts the space of experimental possibilities...|$|R
40|$|We propose an {{alternative}} approach for studying queueing systems by employing robust optimization as opposed to stochastic analysis. While traditional stochastic queueing theory relies on <b>Kolmogorov’s</b> <b>axioms</b> of probability and models arrivals and services as renewal processes, we use the limit laws of probability as the axioms of our methodology and model the queueing systems primitives by uncertainty sets. In this framework, we obtain closed form expressions for the steady-state waiting times in multi-server queues with heavy-tailed arrival and service processes. These expressions are not available under traditional stochastic queueing theory for heavy-tailed processes, while they lead to the same qualitative insights for independent and identically distributed arrival and service times. We also develop an exact calculus for analyzing a network of queues with multiple servers based on the following key principle: a) the departure from a queue, b) the superposition, and c) the thinning of arrival processes have the same uncertainty set representation as the original arrival processes. We show that our approach, which we call the Robust Queueing Network Analyzer (RQNA) a) yields results with error percentages in single digits (for all experiments we performed) relative to simulation, b) performs significantly better than the Queueing Network Analyzer (QNA) proposed in Whitt (1983), and c) is {{to a large extent}} insensitive to the number of servers per queue, the network size, degree of feedback, traffic intensity, and somewhat sensitive to the degree of diversity of external arrival distributions in the network...|$|R
