16|45|Public
5000|$|The prefix eu- is Greek for [...] "good". Triorchis is a Latinization (Pliny the Elder) of Greek triórkhēs (τριόρχης), which Aristotle and Theophrastus {{used for}} a kind of hawk, {{possibly}} the common buzzard. The Greek word means [...] "having three testicles". [...] This <b>erroneous</b> <b>bit</b> of anatomy has been connected with the ease of mistaking a bird's adrenal gland for a testicle.|$|E
50|$|The {{information}} in an ECC memory is stored redundantly enough to correct single bit error per memory word. Hence, an ECC memory can support the scrubbing {{of the memory}} content. Namely, if the memory controller scans systematically through the memory, the single bit errors can be detected, the <b>erroneous</b> <b>bit</b> can be determined using the ECC checksum, and the corrected data can be written back to the memory.|$|E
50|$|Shown {{are only}} 20 encoded bits (5 parity, 15 data) but the pattern {{continues}} indefinitely. The key thing about Hamming Codes {{that can be}} seen from visual inspection is that any given bit is included in a unique set of parity bits. To check for errors, check all of the parity bits. The pattern of errors, called the error syndrome, identifies the bit in error. If all parity bits are correct, there is no error. Otherwise, the sum of the positions of the erroneous parity bits identifies the <b>erroneous</b> <b>bit.</b> For example, if the parity bits in positions 1, 2 and 8 indicate an error, then bit 1+2+8=11 is in error. If only one parity bit indicates an error, the parity bit itself is in error.|$|E
3000|$|..., {{depending}} {{on the number of}} <b>erroneous</b> <b>bits,</b> may still be highly correlated to the source information words b [...]...|$|R
3000|$|... is {{the average}} BER of the destination’s {{detection}} {{on the condition that}} l out of L relays send the destination correct bits, while the other q=L−l relays send the destination <b>erroneous</b> <b>bits.</b>|$|R
3000|$|Considering the {{transmission}} from the L relays to the destination, {{if there are}} l relays sending the destination correct bits and q=L−l relays sending the destination <b>erroneous</b> <b>bits,</b> the decision variable formed by the destination is given by (6), i.e.|$|R
40|$|Abstract-A linear {{feedback}} {{shift register}} {{can be used}} to compress a serial stream of test result data. The compressed <b>erroneous</b> <b>bit</b> stream caused by a fault is said to form the "signature " of the fault. Since the bit stream is compressed, however, it is possible for an <b>erroneous</b> <b>bit</b> stream and the correct one to result in the same signature. In this correspondence, measures of the effectiveness of using linear feedback shift registers for detecting faults in logic networks are ex-amined. After a brief discussion of the underlying theory of fault sig-nature analysis, measures of effectiveness proposed by others are examined and are shown to be of questionable validity since they de-pend on an assumption of independent errors. To provide more accurate measures, two classes of dependent errors that are likely to occur in practice are considered. These are burst errors and errors with in-correct bits spaced at intervals equal to some power of 2 (because most digital systems have dimensions based on powers of 2). Means for determining the effectiveness of fault signature analysis at detecting these classes of errors are given. Index Terms-Data compression, dependent errors, fault detection, fault signature, linear feedback shift registers. I...|$|E
40|$|The ISO/IEC 8802 - 11 : 1999 (E) speci¯cation 1 uses a 32 -bit CRC {{for error}} {{detection}} and whole-packet re-transmissions for recovery. In long-distance or high-interference links where {{the probability of}} a bit error is high, this strategy results in excessive losses, be-cause any <b>erroneous</b> <b>bit</b> causes an entire packet to be discarded. By ignoring the CRC and adding redun-dancy to 802. 11 payloads in software, we achieved substantially reduced loss rates on indoor and out-door long-distance links and extended line-of-sight range outdoors by 70 percent. ...|$|E
40|$|Elevated {{levels of}} {{radiation}} in Low Earth Orbit (LEO) can cause several unexpected behaviors in digital logic. These behaviors, known as Single Event Effects (SEEs), manifest themselves in two ways: unexpected short circuits (Single Event Latch Ups), and <b>erroneous</b> <b>bit</b> flips (Single Event Upsets). Protecting memory from SEEs is usually done via {{some type of}} SECDEC controller, and protecting IO {{can be done in}} a number of ways [...] the simplest of which entails using upper level protocols to verify data integrity. Several techniques are currently employed to deal with SEEs in microprocessors includin...|$|E
3000|$|It {{is quite}} obvious that Case-B outperforms Case-A. In fact, {{if only a}} small {{fraction}} of bits are wrong after physical layer decoding, Case-B is able to discard only the UL packets in which <b>erroneous</b> <b>bits</b> are present, while Case-A discards all [...]...|$|R
30|$|Burst errors {{refer to}} a {{contiguous}} sequence of <b>erroneous</b> <b>bits,</b> which are caused by persistent channel problems, such as prolonged interference, long fading events, handovers and disconnections often due to user mobility, and correlated packet losses are experienced. Consequently, TCP congestion control throttles its sending rate and reduces significantly its throughput.|$|R
40|$|The error {{patterns}} of a wireless digital communication channel {{can be described}} by looking at consecutively correct or <b>erroneous</b> <b>bits</b> (runs and bursts) and at the distribution function of these run and burst lengths. A number of stochastic models exist {{that can be used}} to describe these distributions for wireless channels, e. g., the Gilbert-Elliot model. When attemptin...|$|R
40|$|This paper {{reveals an}} {{analytical}} method of evaluating performance of an {{asynchronous transfer mode}} (ATM) data link based on the chain of two states Markov Modulated Poisson Process (MMPP). Two important traffic parameters, ‘successful retransmission time ’ and ‘average system failure time’ of packet are considered as the key factors for evaluation of save operating region of a network. A group of curves for the above two parameters are drawn against probability of single bit error, taking ‘threshold <b>erroneous</b> <b>bit</b> ’ as a parameter. The trajectory of intersection points of the above set of curves is determined to observe {{the profile of the}} border of safe operating region of the ATM link...|$|E
3000|$|... {{additional}} redundancy packets {{are also}} sent. The term [...] "packet" [...] is loosely applied in this context, {{as in many}} proposals, packet-level FEC is used at the data-link layer. Packet-level FEC aims at recovering some of the lost packets, where the lost packets originate from <b>erroneous</b> <b>bit</b> transmissions and packet discarding at the lower protocol layers, especially for multihop networks, {{as well as from}} congestions in the network and buffer overflows. When packet level FEC is deployed alone to recover from lost packets, the packet loss rate (PLR) is reduced compared to the PLR in the network. However, {{there is no guarantee that}} all packets will be recovered at the destination. This is acceptable in some applications like video and audio streaming or multicasting protocols [2].|$|E
40|$|In {{this paper}} we {{address the problem of}} robust video {{transmission}} in error prone environments. The approach is compatible with the ITU-T video coding standard H. 263. Fading situations in mobile networks are tolerated and the image quality degradation due to spatio-temporal error propagation is minimized utilizing a feedback channel between transmitter and receiver carrying acknowledgment information. In a first step, corrupted Group of Blocks (GOBs) are concealed to avoid annoying artifacts caused by decoding of an <b>erroneous</b> <b>bit</b> stream. The GOB and the corresponding frame number are reported to the transmitter via the back channel. The encoder evaluates the negative acknowledgments and reconstructs the spatial and temporal error propagation. A low complexity algorithm for real-time reconstruction of spatio-temporal error propagation is described in detail. Rapid error recovery is achieved by INTRA refreshing image regions (Macroblocks) bearing visible distortion. The feedback channel m [...] ...|$|E
40|$|A channel {{encoding}} {{apparatus and}} method {{are provided in}} which part of the parity bits are set to <b>erroneous</b> <b>bits,</b> and full parity bits are created by correcting the <b>erroneous</b> <b>bits</b> using a channel decoding apparatus of a receiver in a communication system. In the channel encoding apparatus, in order to generate a coded bit stream by adding a parity bit stream to a message bit stream, a partial parity generator generates a partial parity bit stream {{as a part of}} the parity bit stream using the message bit stream, an erasure generator generates a bit stream having an erroneous value as the remaining part of the parity bit stream, and a decoder calculates the value of the parity bit stream by correcting the bit stream having the erroneous value using a parity-check matrix that determines the parity bit stream, the message bit stream, and the partial parity bit stream. Samsung Electronics Co., Ltd. Georgia Tech Research Corporatio...|$|R
40|$|We {{investigate}} the sink bit error probability (BEP) performance of an intermediate node encoding network (coded network). The network consists of statistically independent binary noisy channels. We observe three {{situations in which}} network coding can affect the sink BEP. An error marking algorithm {{is used to calculate}} the number of <b>erroneous</b> <b>bits</b> (error weight) in the sinks. Then we can calculate the sink BEP from the channel BEPs. Finally, we {{investigate the}} optimal transmitted energy allocation within a BEP constraint network...|$|R
30|$|The {{second case}} {{is when a}} Wi-Fi packet arrives during the correlation. If the {{protector}} detects the collision before channel switching, it can simply abort. In more specific, the protector checks the corrupted bits in the first one byte preamble to detect the collision. In the IEEE 802.15. 4 PHY layer, the one byte preamble is converted into two units of 32 -bit chipping sequences by the spread spectrum technique. When the ZigBee nodes {{are the only ones}} that are occupying the channel, the preamble bits should match well at the receiver side. In contrast, considering that the Wi-Fi interference should be detected as a form of consistent and powerful noise, the number of corrupted bits of the ZigBee preambles significantly increases. After the NBP protector sees the correlation value spike, many <b>erroneous</b> <b>bits</b> in the first preamble means that it is very likely that some other simultaneous transmission exists. For this case, NBP takes a conservative approach; the protector does not send the reservation signal because the source of interference is unknown. This behavior may give more channel access opportunities to Wi-Fi nodes, and thus prevent the channel from being under-utilized. We have measured the trend of <b>erroneous</b> <b>bits</b> in our implementation.|$|R
40|$|Flash {{memories}} {{are one of}} the key microelectronics technologies today. In these devices bits are stored as charge injected into floating gate (FG) MOSFETs, where a polysilicon layer (FG) is interposed between the substrate and the control gate (CG). One or more bits can be written in the FG memory cell by injecting electrons or holes in the FG that is totally surrounded by dielectrics, thus confining the excess carriers in a potential well. Preserving the stored information is achieved if these dielectrics grant almost perfect electrical isolation. To this purpose, a sandwich of Si oxide-nitride-oxide layers (ONO) is deposited between FG and CG, while a highquality thermally grown SiO 2 thin layer (tunnel oxide) separates the FG from the Si substrate. The electrical isolation properties of these dielectric layers is endangered by exposure to ionizing radiation, that may directly drive charge out of the FG, or produce leakage paths across the dielectrics surrounding the FG discharging it. In both cases a degradation of the stored charge takes place and consequently an <b>erroneous</b> <b>bit</b> may be read by the external circuit...|$|E
40|$|Elevated {{levels of}} {{radiation}} in Low Earth Orbit (LEO) can cause several unexpected behaviors in digital logic. These behaviors, known as Single Event Effects (SEEs), manifest themselves in two ways: unexpected short circuits (Single Event Latch Ups), and <b>erroneous</b> <b>bit</b> flips (Single Event Upsets). Protecting memory from SEEs is usually done via {{some type of}} SECDEC controller, and protecting IO {{can be done in}} a number of ways [...] the simplest of which entails using upper level protocols to verify data integrity. Several techniques are currently employed to deal with SEEs in microprocessors including radiation hardening, radiation shielding, software redundancy, and hardware redundancy. TREMOR uses a hardware solution based on an architecture known as Triple Modular Redundancy to achieve SEE tolerance. This paper discusses the TREMOR FPGA system and how it will be used to synchronize the processors and ensure that no erroneous data propagates to the system bus. It will also discuss how the flexibility of this design will allow TREMOR to become a new test bed for various implementations of the TMR architecture...|$|E
40|$|File {{transfer}} is {{the movement of}} digital information from one data terminal to another. This digital information is organized into characters, frames and files. File transfer applications typically demand {{a very high level}} of reliability, unlike voice application, a single <b>erroneous</b> <b>bit</b> can render a multi megabyte file useless to the end user. Communication links in wireless and wired file transfer systems are designed to be highly reliable. The error control systems for these applications use error detection coupled with retransmission requests to maximize reliability at some cost to throughput. At there simplest, such error control systems use parity-check bits or CRC codes to trigger retransmission requests. Since the retransmission requests occur well below the application layer and such protocols are called Automatic Repeat Request (ARQ) protocols. More complicated protocol includes elements of FEC and packet combining to reduce the frequency of retransmission requests. In this article parity, CRC and pure ARQ protocols are discussed. Several popular file transfer protocols are provided as examples. Consideration is then given to hybrid protocols and packet combining...|$|E
40|$|International audienceSoft errors with {{multiple}} <b>erroneous</b> <b>bits</b> {{have become a}} significant threat in embedded systems. New approaches must therefore be proposed to detect errors in a system without assumptions on the error multiplicity. Behavioral checking is in that case appealing. This paper presents a new extended and flexible control flow error detection approach, able to also cover errors in the critical variables of processor-based systems. The approach does not modify the initial system and is compatible with standards such as IEC 61508. Results on a Leon 3 -based system are presented...|$|R
40|$|Abstract — The {{mapping of}} medical texts to {{concepts}} of medical terminology systems {{is a prerequisite}} for many tasks of automatically processing these documents. Due to the complex nature of this task, the results of mapping systems still contain <b>erroneous</b> <b>bits</b> of information. Our editor visualizes the annotation of the text and provides means to easily navigate and modify it. Thus we are able to shorten a cumbersome and time-consuming task and subsequently provide reliable and well-defined information for further processing steps. Even more, the visualization features support {{a better understanding of the}} medical texts. I...|$|R
3000|$|Trapping sets can be {{classified}} into one of three classes according to their behavior ([3], p. 652): (1) a stable trapping set (also called a fixed-point trapping set), (2) a periodically oscillating trapping-set, and (3) an aperiodic oscillating trapping set. The relevance {{of the variation in}} the cardinality of the remaining bit error patterns depends on the class of trapping sets that dominate the error-floor region. In general, there is no known theoretical way to establish the trapping sets for a given code. Thus, finding trapping sets is in general a difficult task because it requires intensive computer simulations at very low error rates which often take months to run ([3], p. 651). As an alternative to the usual computer simulation, we develop an error event analysis, which has a much lower computational cost, and apply it to two LDPC codes of the IEEE 802.11 n standard used as examples in this article. The purpose of the error event analysis is to find out the average behavior of the min-sum BP decoding, with respect to the variation in the number of errors after each iteration, in case of a decoding failure. If the cardinality of the remaining bit error patterns varies significantly, we expect to find out values for the number of iterations for which the number or <b>erroneous</b> <b>bits</b> is minimal and thus choose one of these values for IBP. As a result, the probability that <b>erroneous</b> <b>bits</b> are included among the [...]...|$|R
40|$|Abstract- The goal of {{the paper}} is {{presentation}} of {{a new approach to}} the measurement of effective resolution (effective number of bits- ENOB) of the cyclic A/D converters (CADCs). The core idea of the approach is direct measurement of ENOB using, as a numerical measure, the number of true bits before the first <b>erroneous</b> <b>bit</b> (FEB) position. The position of FEB is determined as the first non-zero bit in the binary presentations of conversion errors. The definition of ENOB based on FEB is introduced and discussed. The particularities of the proposed method are analysed in simulation experiments. There are presented typical evolutions of FEB distributions in sequential cycles of conversion. Values of ENOB obtained using FEB-based method are compared with results obtained using the conventional approach to ENOB assessment. The comparison is performed on example of analysis of influence of DNL and INL errors of internal A/D converter on ENOB of CADC. The proposed method of the ENOB measurement gives more adequate information about the actual ADC resolution and weakens the influence of the form of testing signals on the results of the ENOB measurement...|$|E
40|$|This thesis {{addresses}} {{the problem of}} measuring hardware error sensitivity of computer systems. Hardware error sensitivity is {{the probability that a}} hardware error will result in an erroneous output. Measuring the hardware error sensitivity is important since the rate of transient, intermittent and permanent transistors faults increases as a result of integrated circuit technology scaling. Error sensitivity is influenced by several parameters. This thesis investigates six such parameters, or sources of variation in error sensitivity, in a series of fault injection experiments. In these experiments, bit flip errors were injected into a microprocessors instruction set architecture (ISA) registers and main memory words in order to emulate the errors caused by transient hardware faults. The sources of variation that were addressed include, the ones that deal with systems characteristics, namely, (i) the input processed by a program, (ii) the program’s source code implementation, (iii) the distribution of machine instructions, and (iv) the level of compiler optimization; and the ones that deal with the measurement setup, namely, (v) the number of bits that are targeted in each fault injection experiment and (vi) the significance of the bit, or bits, targeted for fault injection. The experiments identified four factors that had a strong impact on error sensitivity: (1) the location of the <b>erroneous</b> <b>bit,</b> or bits, within a register or memory word, (2) the type of machine instruction targeted for fault injection, (3) the input to program and (4) a programs source code implementation. In contrast, variations in compiler optimization were shown to have a minor impact on error sensitivity. The experiments also show that {{there was no significant difference}} in error sensitivity between single and double bit flips when these occurred within same register or memory word...|$|E
40|$|Mobile Adhoc Networks (MANET) {{refer to}} an {{arrangement}} of autonomous wireless mobile nodes that show the tendency of freely and dynamically self-organizing into arbitrary and temporary network topologies. A variety of protocols have been implemented in MANET at the Network layer which tend to show different performance in various environments. Three {{of the most commonly}} used protocols at the Network Layer in MANET are Destination Sequenced Distance Vector (DSDV) Routing Protocol, Dynamic Source Routing (DSR) Protocol and Adhoc On Demand Distance Vector (AODV) Routing Protocol. A comprehensive study on the performance evaluation of these three routing protocols have been given in this thesis basing upon the TCP window size using Network Simulator (NS- 2. 35) with two different types of network traffics. Tool Command Language (TCL) scripting is used to simulate the environment. Orthogonal Frequency Division Multiplexing (OFDM) is the foremost choice for MANET system designers at the Physical Layer due to its inherent property of high data rate transmission that corresponds to its spectral efficiency. One of the problems inherent in OFDM includes its sensitivity to synchronization errors (frequency offsets and symbol time). Most of the present day techniques employing OFDM for data transmission support mobility as one of the primary feature. This mobility causes small Channel Frequency Offsets (CFO) owing to the production of Doppler frequencies. CFO tends to degrade the signal quality making the system design unsuitable for many error sensitive applications. In this work two efficient pilot-assisted channel estimation strategies have been implemented in the proposed model of OFDM. The implemented solutions for channel estimation include Zero Forcing algorithm and modified Least Square channel estimation algorithm. Both these algorithms have been implemented into the proposed environment of OFDM using two different types of pilot insertion methods i. e. block-type and comb-type pilot insertion techniques. Both these techniques have been compared amongst each other and with the already published work as well. Another serious problem faced by the OFDM based transmission systems is the sensitivity to the noise effects induced by the channel and system. These noise effects tend to increase the BER of the system making it unsuitable for many real-time applications. Turbo Codes have been integrated with the proposed model of OFDM which have the tendency to work in the Forward Error Correction (FEC) manner by not only identifying the <b>erroneous</b> <b>bit</b> locations but also correcting them thus using simplex control information link. The turbo codes have been implemented using parallel concatenation of Recursive Systematic Convolutional (RSC) Codes that tend to introduce redundant information into the user bits in order to mitigate the effects of channel induced noise from the received OFDM symbols. Results have been shown using MATLABÂ® simulation for changing number of iterations of MAP decoder for five different modulation schemes and are compared. The channel, through which the signal has been passed, is simulated using Stanford University Interim Channel Model parameters. These Channel models are six in number and depict three different real outdoor environments including rural, urban and hilly terrains having low, moderate and high tree densities...|$|E
30|$|Concerning mode decision, similar to[28], {{the skip}} mode is {{selected}} on a frequency band basis, in which case, entire frequency bands from the SI are substituted in the reconstructed WZ frames. Such an approach is advantageous {{in the sense}} that high-frequency components are often less important and can be replaced by the corresponding components at a relatively small distortion penalty while no rate is spent. Moreover, skipping entire frequency bands creates more consistent reconstructed coefficients compared to, for instance, a bit-plane-based skip where potentially <b>erroneous</b> <b>bits</b> are introduced. Such <b>erroneous</b> <b>bits</b> undermine the successful decoding of any less significant SW bit-planes since during the creation of the soft-input information every already decoded bit-plane is assumed to be correct. What is more, even when decoding of subsequent bit-planes proves successful, be it intra or SW, any errors in more significant skipped bit-planes would push the reconstructed coefficient value into the wrong quantization bin which increases the incurred distortion. Moreover, the rate spent on any subsequent less significant bit-planes is used sub-optimally. On the other hand, both intra and SW modes are assigned on a bit-plane basis. Intra coding is an attractive alternative when SW coding is expected to be inefficient due to poor SI. Under this condition, SW decoding failure is a potential risk. In this context, intra coding is favoured for bit-planes with higher significance as to further reduce the danger of distortion due to significant SW coded bit-planes that fail to decode.|$|R
25|$|The quantum key {{distribution}} protocols described above provide Alice and Bob with nearly identical shared keys, and also with {{an estimate of the}} discrepancy between the keys. These differences can be caused by eavesdropping, but also by imperfections in the transmission line and detectors. As it is impossible to distinguish between these two types of errors, guaranteed security requires the assumption that all errors are due to eavesdropping. Provided the error rate between the keys is lower than a certain threshold (20% as of April 2007), two steps can be performed to first remove the <b>erroneous</b> <b>bits</b> and then reduce Eve's knowledge of the key to an arbitrary small value. These two steps are known as information reconciliation and privacy amplification respectively, and were first described in 1992.|$|R
40|$|We {{investigate}} the calculation {{approach of the}} sink bit error probability (BEP) for a network with intermediate node encoding. The network consists of statistically independent noisy channels. The main contributions are, for binary network codes, an error marking algorithm is given to collect the error weight (the number of <b>erroneous</b> <b>bits).</b> Thus, we can calculate the exact sink BEP from the channel BEPs. Then we generalize the approach to nonbinary codes. The coding scheme works on the Galois field 2, where m is a positive integer. To reduce computational complexity, a subgraph decomposition approach is proposed. In general, it can significantly reduce computational complexity, and the numerical result is also exact. For approximate results, we discuss the approach of only considering error events in a single channel. The results well approximate the exact results in low BEP regions with much lower complexity...|$|R
40|$|Errors are {{integral}} part of every communication system, whether wired or wireless. There are two broad approaches to deal with errors: (i) detection and discard of data elements in error and (ii) optional recovery from such errors either through proactive or reactive (re) -transmissions. Both these approaches assume that errors are binary in nature, i. e., an error in transmission implies a need to discard or recover the <b>erroneous</b> <b>bits.</b> In this paper, we consider an intriguing alternative, one in which data elements in error are accepted as “approximately correct ” values. We call this approximate communication. More specifically, we introduce the notion that data elements being received are not just correct or incorrect. Instead, there exists a degree of correctness in the received data elements that can be effectively exploited by certain classes of popular applications operating across mobile communication systems. 1...|$|R
40|$|Three new {{techniques}} are proposed for constructing {{a class of}} codes that extends the protection provided by previous single error correcting (SEC) -double error detecting (DED) -single byte error detecting (SBD) codes. The proposed codes are systematic odd-weight-column SEC-DED-SBD codes providing also the correction of any odd number of <b>erroneous</b> <b>bits</b> per byte, where a byte represents a cluster of b bits of the codeword that are fed by the same memory chip or card. These codes are useful for practical applications to enhance the reliability and the data integrity of byte-organized computer memory systems against transient, intermittent, and permanent failures. In particular they represent a good tradeoff between the overhead in terms of additional check bits and the reliability improvement, due to the capability to correct at least 50 % of the multiple errors per byt...|$|R
40|$|When {{investigating}} complex {{communication protocols}} {{and a large}} number of stations using discrete event simulation, it is of great interest to keep the number of necessary simulation events low, since this number directly translates into simulation runtime. One important target for optimizations is the packet or frame exchange over an underlying transmission medium. The number of simulation events involved in transmitting packets should be as low as possible. On the other hand, it is often of great importance to accurately model the channel characteristics and the exact error process. A prominent example are wireless channels. Unfortunately packet errors are very difficult to model, since they depend on channel and source characteristics (e. g. coding). Often the results of <b>erroneous</b> <b>bits</b> differ with the position of the bit hit by an error. Thus {{it may be necessary to}} simulate packet transmission on a bit level to derive the packet error process. Simulation at bit level requires at least [...] ...|$|R
40|$|Hybrid Automatic-Repeat-Request (H-ARQ) Aided Systematic Luby Transform (SLT) Coded Modulation is proposed, where SLT {{codes are}} used both for {{correcting}} <b>erroneous</b> <b>bits</b> and for detecting {{as well as}} retransmitting erroneous Internet Protocol (IP) based packets. Erroneous IP packet detection is implemented using syndrome checking {{with the aid of}} the SLT codes’ Parity Check Matrix (PCM). Optimizing the mapping of SLT-encoded bits to modulated symbols and then using iterative decoding for exchanging extrinsic information between the SLT decoder and the demapper substantially improves the achievable Bit Error Ratio (BER) performance of the scheme. Quantitatively, at Eb/N 0 in excess of 3. 8 dB, this scheme is capable of achieving a BER 10 ? 5 and up to 1. 5 times higher throughput in comparison to less sophisticated benchmarker schemes such as SLT codes, dispensing with ARQ-assistance or joint SLT coded modulation and H-ARQ-SLT codes, when communicating over AWGN channels, using 16 -QAM and a half-rate SLT code...|$|R
40|$|Current {{wireless}} protocols retransmit packets that fail the checksum test, even {{when most of}} the bits are correctly received. Prior work has recognized this inefficiency; however, the proposed solutions (PPR, HARQ, SOFT, etc.) require changes to the hardware and physical layer, and hence are not usable in today’s WLANs and mesh networks. Further, they are tested using fixed modulation and coding schemes, whereas production 802. 11 WLANs adapt their modulation and codes to maximize their ability to correct <b>erroneous</b> <b>bits.</b> This paper makes two key contributions: 1) it introduces ZipTx, a software-only solution that harvests gains from using correct bits in corrupted packets with existing hardware, and 2) it characterizes the gains of partially correct packets for the entire range of operation of 802. 11 networks, and in the presence of adaptive modulation and forward error correction. We implement ZipTx as a driver extension and evaluate our implementation in both outdoor and indoor environments, showing that ZipTx significantly improves throughput...|$|R
40|$|International audienceGenerating secret keys {{in mobile}} {{wireless}} networks {{is considered a}} challenging problem where a key management infrastructure is not always available. Recent security methods have shown that secret keys can be generated using Ultra Wide Band (UWB) channels. These solutions rely on relevant channel properties such as reciprocity and spatial decorrelation. Accordingly, the radio channel responses {{can be used as}} common information to derive secret keys shared by legitimate parties. However, novel studies in the field of UWB channel prediction have demonstrated that channel profiles could be reliably inferred using for instance Ray-Tracing tools. This paper explores this technique to perform attacks and to evaluate the security of UWB secret key generation methods. The main observation here is that it is difficult for a third party to obtain the exact channel responses; thus to retrieve the secret keys. The robustness of UWB key generation methods then depends on the complexity for attackers to describe precisely the physical environment and on the post processing methods to agree on the same key (i. e., quantization, <b>erroneous</b> <b>bits</b> detection, etc.) ...|$|R
