26|9400|Public
50|$|If {{we want to}} {{represent}} language patterns, the most immediate candidate for primitives might be words. However, set phrases, such as “in order to”, immediately indicate the inappropriateness of words as atoms. In searching for other primitives, we might try the rules of grammar. We can represent grammars as finite state automata or context-free grammars. Below is a sample <b>finite</b> <b>state</b> <b>grammar</b> automaton.|$|E
5000|$|In {{the third}} chapter titled [...] "An Elementary Linguistic Theory", Chomsky tries to {{determine}} what sort of device or model gives an adequate account of a given set of [...] "grammatical" [...] sentences. Chomsky hypothesizes that this device has to be finite instead of infinite. He then considers <b>finite</b> <b>state</b> <b>grammar,</b> a communication theoretic model which treats language as a Markov process. Then in the fourth chapter titled [...] "Phrase Structure", he discusses phrase structure grammar, a model based on immediate constituent analysis. In the fifth chapter titled [...] "Limitations of Phrase Structure Description", he claims to show that both these models are inadequate {{for the purpose of}} linguistic description. As a solution, he introduces transformational generative grammar (TGG), [...] "a more powerful model ... that might remedy these inadequacies."Chomsky's transformational grammar has three parts: phrase structure rules, transformational rules and morphophonemic rules. The phrase structure rules are used for expanding grammatical categories and for substitutions. These yield a string of morphemes. A transformational rule [...] "operates on a given string ... with a given constituent structure and converts it into a new string with a new derived constituent structure." [...] It [...] "may rearrange strings or may add or delete morphemes." [...] Transformational rules are of two kinds: obligatory or optional. Obligatory transformations applied on the [...] "terminal strings" [...] of the grammar produce the [...] "kernel of the language". Kernel sentences are simple, active, declarative and affirmative sentences. To produce passive, negative, interrogative or complex sentences, one or more optional transformation rules must be applied in a particular order to the kernel sentences. At the final stage of the grammar, morphophonemic rules convert a string of words into a string of phonemes. Chomsky then applies this idea of transformational rules in the English auxiliary verb system.|$|E
40|$|It {{has been}} pointed out that {{context-free}} grammars of a certain class have the property that a one-one mapping exists between the structural descriptions of its sentences and the sentences generated by a <b>finite</b> <b>state</b> <b>grammar.</b> Given a grammar, it is decidable whether it belongs to this specific class or not. The grammars of this class {{can be divided into}} a countable hierarchy of increasing complexity...|$|E
40|$|In {{his book}} `Mechanisms of Implicit Learning' (1993) Axel Cleeremans {{describes}} how <b>finite</b> <b>state</b> <b>grammars</b> can be modeled successfully with connectionist networks namely with Simple Recurrent Networks (SRNs) developed by Jeffrey Elman. However, SRNs {{cannot be used}} for modeling arbitrary <b>finite</b> <b>state</b> <b>grammars.</b> In this paper I describe the limitations of this approach. 1 Introduction In my thesis research I am comparing the performance of three different machine learning techniques {{on the problem of}} acquiring models for the phonotactic structure of Dutch [TKS 96]. A good phonological model is able to make the distinction between words that are in a language and words that cannot be part of the language because of their structure (for example bda in English). My goal is to find out what machine learning techniques are well suited for linguistic problems and I am also interested in the possible gain the learning processes can have from being supplied with initial knowledge. One of the three [...] ...|$|R
40|$|In this paper, we {{investigate}} both generative and statistical approaches for language modeling in spoken dialogue systems. Semantic class-based <b>finite</b> <b>state</b> ¢ and-gram <b>grammars</b> {{are used for}} improving coverage and modeling accuracy when little training data is available. We have implemented dialogue-state specific language model adaptation to reduce perplexity and improve the efficiency of grammars for spoken dialogue systems. A novel algorithm for state-independent ¢ combining-gram and state-dependent <b>finite</b> <b>state</b> <b>grammars</b> using acoustic confidence scores is proposed. Using this combination strategy, a relative word error reduction of 12 % is achieved for certain dialogue states within a travel reservation task. Finally, semantic class multigrams are proposed and briefly evaluated for language modeling in dialogue systems. 1...|$|R
40|$|We {{describe}} {{techniques for}} enhancing the accuracy, efficiency and {{features of a}} low-resource, medium-vocabulary, grammarbased speech recognition system. Among the issues and techniques we explore are front-end speech/silence detection to reduce computational workload, {{the use of the}} Bayesian information criterion (BIC) to build smaller and better acoustic models, the minimization of <b>finite</b> <b>state</b> <b>grammars,</b> the use of hybrid maximum likelihood and discriminative models, and the automatic generation of baseforms from single new-word utterances. We report WER figures throughout, as appropriate...|$|R
40|$|Four {{experiments}} {{are reported in}} which subjects gained extensive experience with artificial grammars in explicit and implicit processing tasks. Results indicated that (a) implicit processing was sufficient for learning a <b>finite</b> <b>state</b> <b>grammar</b> but was inadequate for learning another type of grammar based on logical rules, (b) Subjects were able to communicate some of their implicit knowledge of the grammars to another person, (c) Consistent with rule induction but not memory array models of learning, verbal protocols indicated there was no tendency to converge on {{the same set of}} cues used to identify valid strings, (d) A synergistic learning effect occurred when both implicit and explicit processing tasks were used in the grammar based on logical rules but not in the <b>finite</b> <b>state</b> <b>grammar.</b> A theoretical framework is proposed in which implicit learning is conceptualized as an automatic, memory-based mechanism for detecting patterns of family resemblance among exemplars. Explicit learning mechanisms for discovering and control-ling task variables are similar to conscious problem solving. These processes include attempts to form a mental represen-tation of the task, searching memory for knowledge of anal-ogous systems, and attempts to build and test mental models of task performance (Gentner & Stevens, 1983; Johnson-Laird, 1983). Implicit learning is thought to be an alternate mode of learning that is automatic, nonconscious, and more powerful than explicit thinking for discovering nonsalient covarianc...|$|E
40|$|An {{algorithm}} that infers a <b>finite</b> <b>state</b> <b>grammar</b> from {{a language}} specified by simple erumeration of legal phrases, is introduced. The algorithm processes every phrase as {{a string of}} phonetic units (words, syllables, demisyllables, etc.), associates one or more states to each phonetic unit and establishes the set of predecessors (and the set of successors) for every state. As a fundamental feature, the algorithm includes an optimization step {{in order to minimize}} the number of states. Additionally, the algorittan candeal with languages specified by a combination of other languages or, even, by a context-free grammar. Peer ReviewedPostprint (published version...|$|E
40|$|The most {{authoritative}} {{description of}} the morphophonemic rules that apply at word boundaries (external sandhi) in Sanskrit is by the great grammarian Pān. ini (fl. 5 th c. B. C. E.). These rules are stated formally in Pān. ini’s grammar, the As. t. ādhyāyī ‘group of eight chapters’. The present paper summarizes Pān. ini’s handling of sandhi, his notational conventions, and formal properties of his theory. An XML vocabulary for expressing Pān. ini’s morphophonemic rules is then introduced, in which his rules for sandhi have been expressed. Although Pān. ini’s notation potentially exceeds a <b>finite</b> <b>state</b> <b>grammar</b> in power, individual rule...|$|E
40|$|This paper {{describes}} a robust, accurate, efficient, low-resource, medium-vocabulary, grammar-based speech recognition system using Hidden Markov Models for mobile applications. Among {{the issues and}} techniques we explore are improving robustness and efficiency of the front-end, using multiple microphones for removing extraneous signals from speech via a new multi-channel CDCN technique, reducing computation via silence detection, applying the Bayesian information criterion (bic) to build smaller and better acoustic models, minimizing <b>finite</b> <b>state</b> <b>grammars,</b> using hybrid maximum likelihood and discriminative models, and automatically generating baseforms from single new-word utterances. I...|$|R
40|$|The paper {{presents}} counter {{evidence against}} Smolensky's theory that human intuitive /nonconscious cognitive processes {{can only be}} accurately {{explained in terms of}} subsymbolic computations carried out in artificial neural networks. We present symbolic learning models of two well studied, complicated cognitive tasks involving nonconscious acquisition of information: learning production rules and artificial <b>finite</b> <b>state</b> <b>grammars.</b> Our results demonstrate that intuitive learning does not imply subsymbolic computation, and that the already well-established, perceived correlation between "conscious" and "symbolic" on the one hand, and between "nonconscious" and "subsymbolic" on the other, does not exist...|$|R
40|$|Automatic speech {{recognition}} (ASR) of non-native utterances with grammatical errors is problematic. A new method {{which makes it}} possible to better recognize such utterances is presented in the current paper. It can be briefly summarized as follows: extract error patterns automatically from a learner corpus, formulate rewrite rules for these syntactic and morphological errors, build <b>finite</b> <b>state</b> <b>grammars</b> (FSGs), and use these FSGs as language models in ASR systems. All rules used in isolation and in different combinations yield lower word error rates (WERs). Index Terms: computer-assisted language learning (CALL), non-native speech, grammatical errors, automatic {{speech recognition}} (ASR), language modeling 1...|$|R
40|$|We {{experimentally}} {{test the}} ability of subjects to identify repeated-game strategies from prisoner's dilemma choice data. In the experiments, subjects use a <b>finite</b> <b>state</b> <b>grammar</b> to build models to fit game histories. The histories are designed to distinguish between strategies with empirical and theoretical validity. We find that subjects successfully identify unconditional, punishment, and counting strategies. When data are observationally equivalent among different strategies, punishment strategies tend to be inferred. When inferred strategies do not fit the data, {{they tend to be}} more complex than necessary and to contain positive reciprocity and forgiveness. Our experiment provides an empirical basis for repeated-game strategies and sheds new light on play in repeated games...|$|E
40|$|International audienceThe most {{authoritative}} {{description of}} the morphophonemic rules that apply at word boundaries (external sandhi) in Sanskrit is by the great grammarian Panini (fl. 5 th c. B. C. E.). These rules are stated formally in Panini's grammar, the Astadhayi group of eight chapters'. The present paper summarizes Panini's handling of sandhi, his notational conventions, and formal properties of his theory. An XML vocabulary for expressing Panini's morphophonemic rules is then introduced, in which his rules for sandhi have been expressed. Although Panini's notation potentially exceeds a <b>finite</b> <b>state</b> <b>grammar</b> in power, individual rules do not rewrite their own output, and thus they may be automatically translated into a rule cascade from which a finite state transducer can be compiled...|$|E
40|$|This paper {{describes}} {{our work}} in developing multilingual (Swedish and English) speech recognition {{systems in the}} ATIS domain. The acoustic component of the multilingual systems is realized through sharing Gaussian codebooks across Swedish and English allophones. The language model (LM) components are constructed by training a statistical bigram model, with a common backoff node, on bilingual texts, and by combining two monolingual LMs into a probabilistic <b>finite</b> <b>state</b> <b>grammar.</b> This system uses a single decoder for Swedish and English sentences, and is capable of recognizing sentences with words from both languages. Preliminary experiments show that sharing acoustic models across the two languages has not resulted in improved performance, while sharing a backoff node at the LM component provides flexibility and ease in recognizing bilingual sentences {{at the expense of}} a slight increase in word error rate in some cases. As a by-product, the bilingual decoder also achieves good performanc [...] ...|$|E
40|$|International audienceIt is {{supposed}} {{that humans are}} genetically predisposed {{to be able to}} recognize sequences of context free grammars with center-embedded recursion while other primates are restricted to the recognition of <b>finite</b> <b>state</b> <b>grammars</b> with tail-recursion. Our aim was to construct a minimalist neural network that is able to parse artificial sentences of both grammars in an efficient way without using the biologically unrealistic backpropagation algorithm. The core of this network is a neural stack-like memory where the push and pop operations are regulated by synaptic gating on the connections between the layers of the stack. The network correctly categorizes novel sentences of both grammars after training. We suggest that the introduction of the neural stack memory {{will turn out to be}} substantial for any biological 'hierarchical processor' and the minimalist design of the model suggests a quest for similar, realistic neural architectures...|$|R
40|$|Turbo {{recognition}} (TR) is {{a communication}} theory {{approach to the}} analysis of rectangular layouts, in the spirit of Document Image Decoding (DID). The TR algorithm, inspired by turbo decoding, is based on a generative model of image production, in which two grammars are used simultaneously to describe structure in orthogonal (horizontal and vertical) directions. This enables TR to strictly embody non-local constraints that cannot be taken into account by local statistical methods. This basis in <b>finite</b> <b>state</b> <b>grammars</b> also allows TR to be quickly retargetable to new domains. We illustrate some of the capabilities of TR with two examples involving realistic images. While TR, like turbo decoding, is not guaranteed to recover the statistically optimal solution, we present an experiment that demonstrates its ability to produce optimal or near-optimal results on a simple yet nontrivial example, the recovery of a filled rectangle in the midst of noise. Unlike methods such as stochastic context [...] ...|$|R
40|$|This paper {{describes}} {{material for}} a course in computational linguistics which concentrates on building (parts of) realistic language technology applications for Dutch. We present {{an overview of the}} reasons for develop- ing new material, rather than using existing text-books. Next we present an overview of the course in the form of six exercises, covering advanced use of <b>finite</b> <b>state</b> methods, <b>grammar</b> development, and natural language interfaces. The exercises emphasise the benefits of special-purpose development tools, the importance of testing on realistic data-sets, and the possibilities for web-applications based on natural language processing...|$|R
40|$|Long and {{complicated}} sentences {{prove to be}} a stumbling block for current systems relying on NL input. These systems stand to gain from methods that syntactically simplify such sentences. To simplify a sentence, we need an idea {{of the structure of the}} sentence, to identify the components to be separated out. Obviously a parser could be used to obtain the complete structure of the sentence. However, full parsing is slow and prone to failure, especially on complex sentences. In this paper, we consider two alternatives to full parsing which could be used for simplification. The first approach uses a <b>Finite</b> <b>State</b> <b>Grammar</b> (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages. We discuss the impact of these two input representations on the simplification process. 1 Reasons for Text Simplification Long {{and complicated}} sentences {{prove to be a}} stumbling block for current systems which rely on natural language input. T [...] ...|$|E
40|$|In a {{guessing}} game, Ss reconstruct {{a sequence}} by guessing each successive {{element of the}} sequence from a finite set of alternatives, receiving feedback after each guess. An upper bound on Ss knowledge of the sequence is given by H, the estimated entropy of the numbers of guesses. The method provides a measure of learning independent of material type and distractors, and the resulting data set is very rich. Here, the method is applied to artificial grammar learning; Ss were exposed to strings from a <b>finite</b> <b>state</b> <b>grammar</b> and subsequently distinguished between strings that followed or violated the grammar reliably better than Ss who {{had not seen the}} learning strings (but who themselves performed at above chance levels). Ss knowledge of the strings, H, reflected both grammaticality and exposure to learning strings, and was correlated with overall judgement performance. For non-grammatical strings, the strings that Ss knew most about were those they found most difficult to classify correc [...] ...|$|E
40|$|This paper reports {{an ongoing}} effort to derive linear {{discourse}} structures from a corpus of telephone conversations. First, {{we would like}} to determine how reliably human annotators can tag discourse segments in dialogues. Second, we begin to investigate how to build machine models for performing this annotation task. To carry out our research, we use a corpus of transcribed and annotated human-human dialogues in a specific information retrieval domain (Movie theater schedules). We conducted an experiment in which 25 different dialogues have each been annotated by at least seven different people. We found that the average precision and recall among annotators in placing segment boundaries is 84. 3 %, and in assigning segment purpose labels is 80. 1 %. A simple discourse segment parser based on finite state machines is able to cover 56 % of the same dialogues. When the <b>finite</b> <b>state</b> <b>grammar</b> is able to analyse a dialogue, it agrees with human annotators in placing segment boundaries with 59. 4 % [...] ...|$|E
40|$|A {{class on}} {{compiler}} design {{is offered by}} many departments because it allows a student to see the interplay between theory (<b>finite</b> <b>state</b> machines, <b>grammars,</b> formal languages) and practice (language translation, problematic features of modern programming languages). Yet this purpose, although important, {{is not the only}} reason to include a compiler class in the curriculum. This paper reports how a compiler class can also provide the base to teach other subjects that students should be exposed to: software reuse, re-engineering, experience with patterns and software architectures, and reverse engineering. These topics are important even if the student never writes a language translator later on in his or her career. ...|$|R
40|$|Abstract. This paper {{presents}} logical reconstructions of four dierent {{methods for}} part of speech tagging: <b>Finite</b> <b>State</b> Intersection <b>Grammar,</b> HMM tagging, Brill tagging, and Constraint Grammar. Each reconstruc-tion consists of a rst-order logical theory and an inference relation {{that can be applied}} to the theory, in conjunction with a description of data, in order to solve the tagging problem. The reconstructed methods are compared along a number of dimensions including ontology, expressive power, mode of reasoning, uncertainty, underspecication, and robust-ness. It is argued that logical reconstruction of NLP methods in general can lead to a deeper understanding of the knowledge and reasoning in-volved, and of the ways in which dierent methods are related. ...|$|R
40|$|A {{probabilistic}} string is {{a sequence}} of probability vectors. Each vector specifies a probability distribution over the possible symbols at its location in the string. In a probabilistic grammar a probability is assigned to every derivation. Given a probabilistic string and a probabilistic grammar {{the concept of a}} maximal derivation is defined. Algorithms for finding the maximal derivation for probabilistic <b>finite</b> <b>state</b> and linear <b>grammars</b> are given. The case where a waveform can be segmented into several possible probabilistic strings is also considered. 1...|$|R
40|$|Abstract: Academic {{effectiveness}} of universities is measured {{with the number}} of publications and citations. However, accessing all the publications of a university reveals a challenge related to the mistakes and standardization problems in citation indexes. The main aim {{of this study is to}} seek a solution for the unstandardized addresses and publication loss of universities with regard to this problem. To achieve this, all Turkey-addressed publications published between 1928 and 2009 were analyzed and evaluated deeply. The results show that the main mistakes are based on character or spelling, indexing and translation errors. Mentioned errors effect international visibility of universities negatively, make bibliometric studies based on affiliations unreliable and reveal incorrect university rankings. To inhibit these negative effects, an algorithm was created with finite state technique by using Nooj Transducer. Frequently used 47 different affiliation variations for Hacettepe University apart from “Hacettepe Univ ” and “Univ Hacettepe ” were determined by the help of <b>finite</b> <b>state</b> <b>grammar</b> graphs. In conclusion, this study presents some reasons of the inconsistencies for university rankings. It is suggested that, mistakes and standardization issues should be considered by librarians, authors, editors, policy makers and managers to be able to solve these problems...|$|E
40|$|This {{dissertation}} {{details the}} implementation of a real-time, speaker-independent telephone auto attendant from first principles on limited quality speech data. An auto attendant is a computerized agent that answers the phone and switches the caller through to the desired person's extension after conducting a limited dialogue to determine the wishes of the caller, through the use of speech recognition technology. The platform is a computer with a telephone interface card. The speech recognition engine uses whole word hidden Markov modelling, with limited vocabulary and constrained (<b>finite</b> <b>state)</b> <b>grammar.</b> The feature set used is based on Mel frequency spaced cepstral coefficients. The Viterbi search is used together with the level building algorithm to recognise speech within the utterances. Word-spotting techniques including a "garbage" model, are used. Various techniques compensating for noise and a varying channel transfer function are employed to improve the recognition rate. An Afrikaans conversational interface prompts the caller for information. Detailed experiments illustrate the dependence and sensitivity of the system on its parameters, and show the influence of several techniques aimed at improving the recognition rate. Dissertation (MEng (Computer Engineering)) [...] University of Pretoria, 2006. Electrical, Electronic and Computer Engineeringunrestricte...|$|E
40|$|Subjects {{exposed to}} strings of letters {{generated}} by a <b>finite</b> <b>state</b> <b>grammar</b> can later classify grammatical and nongrammatical test strings, even though they cannot adequately say what {{the rules of the}} grammar are (e. g., Reber, 1989). The MINERVA 2 (Hintzman, 1986) and Medin and Schaffer (1978) memory-array models and a number of connectionis? autoassociator models are tested against experimental data by derlving mainly parameter-free predictions from the models of the rank order of classification difficulty of test strings. The importance of different assumptions regarding the coding of features (How should the absence of a feature be coded? Should single letters or digrams be coded?), the learning rule used (Hebb rule vs. delta rule), and the connectivity (Should features be predicted only by previous features in the string, or by all features simultaneously?) is investigated by determlning the performance of the models with and without each assumption. Only one class of connectionist model (the simultaneous delta rule) passes all the tests. I? is shown that this class of model can be regarded by abstracting a se? of representative but incomplete rules of the grammar. Recently, there has been considerable interest in how subjects learn artificia...|$|E
40|$|This {{course is}} {{designed}} to introduce students to the current statistical techniques for the automatic analysis of natural (human) language data. It develops an in-depth understanding of both the algorithms available for the processing of linguistic information and the underlying computational properties of natural languages. Potential topics include language modeling, <b>finite</b> <b>state</b> models, stochastic <b>grammars,</b> latent semantic analysis, log-linear models in natural language processing. We will explore how these core techniques {{can be applied to}} user applications such as information extraction, question answering, automatic speech recognition, statistical machine translation...|$|R
40|$|AbstractWe use a <b>finite</b> <b>state</b> (FSA) {{construction}} {{approach to}} address the problem of propositional satisfiability (SAT). We present a very simple translation from formulas in conjunctive normal form (CNF) to regular expressions and use regular expressions to construct an FSA. As a consequence of the FSA construction, we obtain an ALL-SAT solver and model counter. This automata construction can be considered essentially a <b>finite</b> <b>state</b> intersection <b>grammar</b> (FSIG). We also show how an FSIG approach can be encoded. Several variable ordering (state ordering) heuristics are compared in terms of the running time of the FSA and FSIG construction. We also present a strategy for clause ordering (automata composition). Running times of state-of-the-art model counters and BDD based SAT solvers are compared and we show that both the FSA and FSIG approaches obtain an state-of-the-art performance on some hard unsatisfiable benchmarks. It is also shown that clause learning techniques can help improve performance. This work brings up many questions on the possible use of automata and grammar models to address SAT...|$|R
40|$|In this thesis, various {{artificial}} recurrent {{neural network}} models are investigated {{for the problem}} of deriving grammar rules from a finite set of example "sentences. " A general discrete network framework and its corresponding learning algorithm are presented and studied in detail in learning three different types of grammars. The first type of grammars considered is regular grammars. Experiments with conventional analog recurrent networks in learning regular grammars are presented to demonstrate the unstable behavior of such networks in processing very long strings after training. A new network structure to force recurrent networks to learn stable states by discretizing the internal feedback signals is constructed. For training such discrete networks a "pseudo-gradient" learning rule is applied. As an extension to the discrete network model, an external discrete stack is added to accommodate the inference of context-free grammars. A composite error function is devised to deal with various situations during learning. The pseudo-gradient method is also extended to train such a network to learn context-free grammars with the added option of operating on the external stack. Another extension to the discrete network structure is made {{for the purpose of}} learning probabilistic <b>finite</b> <b>state</b> <b>grammars.</b> The network consists of a discrete portion which is intended to represent the structure of the grammar, and an analog portion which is intended to represent the transition probabilities. Two criteria for the network to verify the correctness of its solution during training are proposed. Theoretical analysis of the necessary and sufficient conditions for the correct solution is presented. Experimental results show that the discrete network models have similar capabilities in learning various grammars as their analog counterparts, and have the advantage of being provably stable. </p...|$|R
40|$|A {{total of}} 78 adult {{participants}} were asked to read a sample of strings generated by a <b>finite</b> <b>state</b> <b>grammar</b> and, immediately after reading each string, to mark the natural segmentation positions with a slash bar. They repeated the same task after a phase of familiarization with the material, which consisted, depending on the group involved, of learning items by rote, performing a shortterm matching task, or searching for the rules of the grammar. Participants formed the same number of cognitive units before and after the training phase, thus indicating that they did not tend to form increasingly large units. However, the number of different units reliably decreased, whatever the task that participants had performed during familiarization. This result indicates that segmentation was increasingly consistent with the structure of the grammar. A theoretical account of this phenomenon, based on ubiquitous principles of associative memory and learning, is proposed. This account is supported by the ability of a computer model implementing those principles, PARSER, to reproduce the observed pattern of results. The implications of this study for developmental theories aimed at accounting for how children become able to parse sensory input into physically and linguistically relevant units are discussed. In a typical implicit learning experiment, participants are first exposed to rule-governe...|$|E
40|$|In this ERP study, linear and center-embedded musical {{sequences}} {{are built}} {{according to two}} artificial grammar types in language, named <b>finite</b> <b>state</b> <b>grammar</b> (FSG) and phrase structure grammar (PSG). The aim is to prove if neural sources and processing mechanisms for artificial grammar settings across domains are the same. Isochronous pitch sequences were constructed by two interval categories (3 rd and 6 th) in upward and downward direction. FSG sequences, which have the general form ABAB in artificial grammar, are translated into “small up/small down/large up/large down”. PSG sequences of form A[AB]B are transposed to “small up/large up/large down/small down”. In two ERP recordings testing FSG and PSG separately, non-musicians had to distinguish between correct and false examples after getting familiar with each grammar type. Deviant sequences either include an item of reverse interval or contour. Our main results are: (1) N 1 components indicate a 2 -item-chunking in FSG and a 4 -item-chunking in PSG based on immediate repetition between adjacent tones, thus low-level grouping is different for each grammar type. (2) A late processing negativity at sequence offset indicates syntax-based integration-and-memory processes primarily for PSG. The partially congruent ERP results for artificial grammar learning in language and music confirm that the linguistic perspective on music may be justified...|$|E
40|$|In recent years, gesture {{recognition}} {{has received}} much attention from research communities. Computer vision-based gesture recognition has many potential ap-plications {{in the area}} of human-computer interaction as well as sign language recognition. Sign languages use a combination of hand shapes, motion and loca-tions as well as facial expressions. Finger-spelling is a manual representation of alphabet letters, which is often used where there is no sign word to correspond to a spoken word. In Australia, a sign language called Auslan is used by the deaf community and and the finger-spelling letters use two handed motion, unlike the well known finger-spelling of American Sign Language (ASL) that uses static shapes. This thesis presents the Auslan Finger-spelling Recognizer (AFR) that is a real-time system capable of recognizing signs that consists of Auslan manual alphabet letters from video sequences. The AFR system has two components: the first is the feature extraction process that extracts a combination of spatial and motion features from the images. Which classifies a sequence of features using Hidden Markov Models (HMMs). Tests using a vocabulary of twenty signed words showed the system could achieve 97 % accuracy at the letter level and 88 % at the word level using a <b>finite</b> <b>state</b> <b>grammar</b> network and embedded training...|$|E
40|$|In {{recent years}} therehasbeen an increasedinterest in the {{modelling}} {{and recognition of}} human activities involving highly structured and semantically rich behaviour such as dance, aerobics, and sign language. A novel approachispresented for automatically acquiring stochastic models of the high-level structureof an activity without the assumption of any prior knowledge. The process involves temporal segmentation into plausible atomic behaviour components {{and the use of}} variable length Markov models for the efficient representation of behaviours. Experimental results arepresented which demonstrate the synthesis of realistic sample behaviours and the performanceofmodels for long-term temporal prediction. Keywords: modelling behaviour, behaviour prediction, behaviour synthesis, variable length Markov models, Markov models, N-grams, hidden Markov models, probabilistic <b>finite</b> <b>state</b> automata, statistical <b>grammars,</b> computer animation. 2...|$|R
40|$|LR-regular grammars {{are defined}} {{similarly}} to Knuth's LR(k) grammars, {{with the following}} exception: arbitrarily long look-ahead is allowed before making a parsing decision during the bottom-up syntactical analysis; however, this look-ahead is restricted in that the essential “look-ahead information” can be represented by {{a finite number of}} regular sets, thus can be computed by a <b>finite</b> <b>state</b> machine. LR-regular <b>grammars</b> can be parsed deterministically in linear time by a rather simple two-scan algorithm. Efficient parsers are constructed for given LR-regular grammars. The family of LR-regular languages is studied; it properly includes the family of deterministic CF languages and has similar properties. Necessary and sufficient conditions for a grammar to be LR-regular are derived and then utilized for developing parser generation techniques for arbitrary grammars...|$|R
40|$|The {{theory of}} {{weighted}} <b>finite</b> <b>state</b> transducers (WFST) allows great {{flexibility in the}} early use of multiple sources of information in speech decoders. In this paper, we describe a decoder that relies on the algebra of WFSTs to integrate multiple sources of information in a one-pass search. The system has two modes of operation: time-synchronously for use with <b>finite</b> <b>state</b> problem specific <b>grammars</b> or with a word loop grammar for large vocabulary tasks; or time-asynchronously as a stack decoder also for large vocabulary recognition. Both modes of operation are decoupled from the language model. Experiments done with lattice rescoring tasks showed that the error rate {{is the same as}} with established state of the art decoders. Furthermore, experiments with explicit cross word pronunciation rules showed the feasibility of the inclusion of new knowledge sources early in the decoding process. We also found that the use of a time-synchronous search with a word loop grammar outperforms the sta [...] ...|$|R
