3145|431|Public
5|$|SAS has product {{sets for}} {{specific}} industries, such as government, retail, telecommunications and aerospace and for marketing optimization or <b>high-performance</b> <b>computing.</b>|$|E
5|$|Itanium {{is aimed}} at the {{enterprise}} server and <b>high-performance</b> <b>computing</b> (HPC) markets. Other enterprise- and HPC-focused processor lines include Oracle's and Fujitsu's SPARC processors and IBM's POWER microprocessors. Measured by quantity sold, Itanium's most serious competition comes from x86-64 processors including Intel's own Xeon line and AMD's Opteron line. Since 2009, most servers were being shipped with x86-64 processors.|$|E
5|$|Parallel {{computing}} {{is a type}} of computation {{in which}} many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in <b>high-performance</b> <b>computing,</b> but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.|$|E
50|$|The {{operating}} {{system of the}} Tianhe-1 supercomputer is 64-bit Kylin Linux, which is oriented to <b>high-performance</b> parallel <b>computing</b> optimization, and supports power management and <b>high-performance</b> virtual <b>computing.</b> The newer Tianhe-2 also uses Kylin Linux.|$|R
5000|$|The iDataCool <b>high-performance</b> <b>compute</b> {{cluster is}} a {{research}} project on cooling with hot water and energy reuse in data centers. The waste heat of iDataCool is used to drive an adsorption chiller that generates chilled water. The project pursues the following goals: ...|$|R
50|$|Increasing {{demand for}} <b>high-performance</b> <b>Computed</b> {{tomography}} (CT) scanning and angiography systems has driven development {{of very high}} performance medical X-ray tubes. Contemporary CT tubes have power ratings of up to 100 kW and anode heat capacity of 6 MJ, yet retain an effective focal spot area of less than 1 mm2.|$|R
5|$|UM {{maintains}} one of {{the largest}} centralized academic cyber infrastructures in the country with numerous assets. The Center for Computational Science High Performance Computing group has been in continuous operation since 2007. Over that time the core has grown from a zero HPC cyberinfrastructure to a regional <b>high-performance</b> <b>computing</b> environment that currently supports more than 1,200 users, 220 TFlops of computational power, and more than 3 Petabytes of disk storage. The center's latest system acquisition, an IBM IDataPlex system, was ranked at number 389 on the November 2012 Top 500 Supercomputers.|$|E
25|$|In August 2007, Mercury Computer Systems {{released}} a Software Development Kit for PLAYSTATION(R)3 for <b>High-Performance</b> <b>Computing.</b>|$|E
25|$|Scientific {{computing}} includes {{applied mathematics}} (especially numerical analysis), computing science (especially <b>high-performance</b> <b>computing),</b> and mathematical modelling in a scientific discipline.|$|E
5000|$|Scieneer Common Lisp: {{which is}} {{designed}} for <b>high-performance</b> scientific <b>computing.</b>|$|R
40|$|Leveraging service-oriented {{architectures}} {{and taking}} advantage of the <b>high-performance</b> <b>compute</b> resources provided by XSEDE, we have developed standards-based web services to address the challenges associated with processing large volumes of high resolution topography data. These web services make results from community software packages and other cyberinfrastructure-based applications available to the wider earth sciences community via the OpenTopography Facility and the CyberGIS Gateway...|$|R
50|$|Commercial <b>high-performance</b> {{reconfigurable}} <b>computing</b> {{systems are}} beginning to emerge with the announcement of IBM integrating FPGAs with its POWER processor.|$|R
25|$|The Information, Science, and Technology Pillar leverages {{advances}} in theory, algorithms, and the exponential growth of <b>high-performance</b> <b>computing</b> {{to accelerate the}} integrative and predictive capability of the scientific method.|$|E
25|$|Breakthroughs in <b>high-performance</b> <b>computing,</b> {{including}} {{the development of}} novel concepts for massively parallel computing and the design and application of computers that can carry out hundreds of trillions of operations per second.|$|E
25|$|Instant WebKiosk and Webconverger are {{browser-based}} Linux distributions {{often used}} in web kiosks and digital signage. Thinstation is a minimalist distribution designed for thin clients. Rocks Cluster Distribution is tailored for <b>high-performance</b> <b>computing</b> clusters.|$|E
50|$|The VLSCI Peak Computing Facility (PCF) {{provided}} <b>high-performance</b> <b>compute</b> {{infrastructure and}} computational expertise to Life Sciences researchers across Victoria. The PCF had tightly-coupled clusters with very fast disk subsystems, currently operating at a peak capacity of 855 teraflops. To help researchers maximize {{their use of}} compute time and {{get the most out}} of their allocated resources, the PCF had a team of system administrators, programmers and application specialists accessible through its help request system.|$|R
50|$|<b>High-Performance</b> Reconfigurable <b>Computing</b> (HPRC) is a {{computer}} architecture combining reconfigurable computing-based accelerators like field-programmable gate array with CPUs or multi-core processors.|$|R
50|$|By 2011, Google and IBM were {{completing}} the program since <b>high-performance</b> cloud <b>computing</b> clusters had become widely available to researchers at reasonable costs.|$|R
25|$|David A. Bader (born May 4, 1969) is a Professor, Chair of the School of Computational Science and Engineering, and Executive Director of <b>High-Performance</b> <b>Computing</b> in the Georgia Tech College of Computing. In addition, Bader was {{selected}} {{as the director of the}} first Sony Toshiba IBM Center of Competence for the Cell Processor at the Georgia Institute of Technology. He is an IEEE Fellow,, AAAS Fellow, National Science Foundation CAREER Award recipient and an IEEE Computer Society Distinguished Speaker. Bader is a leading expert in data sciences. His main areas of research are in at the intersection of <b>high-performance</b> <b>computing</b> and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics.|$|E
25|$|The {{success of}} modern {{numerical}} mathematical methods and software {{has led to}} the emergence of computational mathematics, computational science, and computational engineering, which use <b>high-performance</b> <b>computing</b> for the simulation of phenomena and the solution of problems in the sciences and engineering. These are often considered interdisciplinary.|$|E
25|$|The {{difficulty}} of predicting stable crystal structures {{based on the}} knowledge of only the chemical composition {{has long been a}} stumbling block on the way to fully computational materials design. Now, with more powerful algorithms and <b>high-performance</b> <b>computing,</b> structures of medium complexity can be predicted using such approaches as evolutionary algorithms, random sampling, or metadynamics.|$|E
50|$|In 2013 {{researchers}} {{used the}} Kittyhawk project {{to demonstrate a}} novel <b>high-performance</b> cloud <b>computing</b> platform by merging a cloud computing environment with a supercomputer.|$|R
50|$|Marsha J. Berger (born 1953) is an American {{computer}} scientist. Her {{areas of}} research include numerical analysis, computational fluid dynamics, and <b>high-performance</b> parallel <b>computing.</b>|$|R
50|$|The US National Science Foundation has {{a center}} for <b>high-performance</b> {{reconfigurable}} <b>computing</b> (CHREC).In April 2011 the fourth Many-core and Reconfigurable Supercomputing Conference was held in Europe.|$|R
25|$|The Rossmann {{cluster is}} named for Michael Rossmann, Purdue's Hanley Distinguished Professor of Biological Sciences, who is a pioneer in {{employing}} <b>high-performance</b> <b>computing</b> in research to reveal the structure of viruses and their component protein molecules. Rossmann gained worldwide attention in 1985 by determining the structure of human rhinovirus serotype14, HRV-14, one of about 100 known cold virus strains.|$|E
25|$|The {{laboratory}} has a {{long history}} of energy research; nuclear reactor experiments have been conducted since the end of World War II in 1945. Because of the availability of reactors and <b>high-performance</b> <b>computing</b> resources an emphasis on improving the efficiency of nuclear reactors is present. The programs develop more efficient materials, more accurate simulations of aging reactor cores, sensors and controls as well as safety procedures for regulatory authorities.|$|E
25|$|The School {{is one of}} {{a handful}} that offer degrees in Human-Computer Interaction. The School offers master's degrees in Human-Computer Interaction Design, Music Informatics, Bioinformatics, Chemical Informatics, Security Informatics, and Computer Science, and Ph.D. degrees in Computer Science and in Informatics. Specialization areas for the Ph.D. in Computer Science include {{artificial}} intelligence, databases, distributed systems, formal methods, <b>high-performance</b> <b>computing,</b> programming languages, and security. The Informatics Ph.D. program offers tracks in bioinformatics, cheminformatics, complex systems, human-computer interaction design, logic and mathematical foundations of informatics, music informatics, security informatics, and social informatics.|$|E
40|$|The coupled Lagrangian-Eulerian DEM/CFD code {{developed}} {{over several years}} at Aston University and the University of Birmingham has been parallelized using a single program multiple data (SPMD) strategy. Initial implementation on a <b>high-performance</b> <b>compute</b> cluster indicated good scalability up to 32 processors, beyond which speedup gains are swamped by the all-processor communication overhead inherent in the fluid flow algorithm. A Geldart group A powder bed comprising 1 million particles was fluidized in the bubbling regime using the parallelized code. (C) 2010 Elsevier B. V. All rights reserved...|$|R
5000|$|Open Source Cluster Application Resources (OSCAR) is a Linux-based {{software}} installation for <b>high-performance</b> cluster <b>computing.</b> OSCAR {{allows users}} to install a Beowulf type high performance computing cluster.|$|R
50|$|Drost co-authored (with R. Ho) Coupled Data Communication Techniques for <b>High-Performance</b> and Low-Power <b>Computing,</b> Springer, 2010.|$|R
25|$|Bulldozer is AMD's {{microarchitecture}} codename {{for server}} and desktop AMD FX processors first released on October 12, 2011. This family 15h microarchitecture is {{the successor to}} the family 10h (K10) microarchitecture design. Bulldozer is designed from scratch, not a development of earlier processors. The core is specifically aimed at 10–125W TDP computing products. AMD claims dramatic performance-per-watt efficiency improvements in <b>high-performance</b> <b>computing</b> (HPC) applications with Bulldozer cores. While hopes were very high that Bulldozer would bring AMD to be performance competitive with archrival Intel once more, most benchmarks were disappointing. In some cases the new Bulldozer products were slower than the K10 model they were built to replace.|$|E
25|$|Fortran is a general-purpose, procedural, and {{imperative}} {{programming language}} that is especially suited to numeric computation and scientific computing. Fortran came to dominate this area of programming early on {{and has been in}} continual use for over half a century in computationally intensive areas such as numerical weather prediction, finite element analysis, computational fluid dynamics (CFD), computational physics, and computational chemistry. It {{is one of the most}} popular languages in the area of <b>High-performance</b> <b>computing</b> and programs to benchmark and rank the world's fastest supercomputers are written in Fortran. In 1956, John Backus and a team of researchers at IBM invented the Fortran programming language for the IBM 704 mainframe computer.|$|E
25|$|The {{department}} is closely {{affiliated with the}} , which fosters research on the neural underpinnings of psychological function. The CSBMB houses {{state of the art}} facilities for the study of brain function, including a research-dedicated, high-field fMRI scanner, an EEG laboratory, a TMS coil, an eye tracking laboratory, and <b>high-performance</b> <b>computing</b> facilities for data analysis and computational modeling. Seventeen faculty members from the department are affiliated with the CSBMB. Unique among research institutions that own and operate fMRI scanners, the CSBMB is the first facility to own a scanner that is run solely by neuroscientists that conduct basic research. Most scanners in the United States are located in clinical settings and are utilized primarily in applied research.|$|E
40|$|Many factors {{contribute}} to overall application performance in today’s <b>high-performance</b> cluster <b>computing</b> environments. These factors include the memory sub-system, network hardware and software stack, compilers and libraries, and I/O sub-system. The large variability in hardware and software configurations present i...|$|R
40|$|The {{increasingly}} available Graphics Processing Units (GPU) hardware {{resources and}} the emerging General Purpose computing on GPU (GPGPU) technologies provide an alternative and complementary solution to existing cluster based <b>high-performance</b> geospatial <b>computing.</b> However, {{the complexities of the}} unique GPGPU hardware architectures and the steep learning curve of GPGPU programming have imposed signficant technical challenges on the geospatial computing community to develop efficient parallel geospatial data structures and algorithms that can make full use of the hardware capabilities to solve ever growing large and complex real world geospatial problems. In this study, we propose a practical approach to simplifying <b>high-performance</b> geospatial <b>computing</b> on GPGPUs by using parallel primitives. We take a case study of quadtree construction on large-scale geospatial rasters to demonstrate the effectiveness of the proposed approach. Comparing the proposed parallel primitives based implementation with a naïve CUDA implementation, a signficant reduction on coding complexity and a 10 X speedup have been achieved. We believe that GPGPU based software development using generic parallel primitives can be a first step towards developing geospatial-specific and more efficient parallel primitives for <b>high-performance</b> geospatial <b>computing</b> in both personal and cluster computing environments and boost the performance of geospatial cyberinfrastructure. ...|$|R
40|$|The web-based {{interface}} of {{the atomic}} structure code AUTOSTRUCTURE has been integrated to the GENIUS grid portal, which facilitates its use in distributed grid environments. It also establishes a general methodology for adapting <b>high-performance</b> scientific <b>computing</b> codes to the new e-infrastructure. status: publishe...|$|R
