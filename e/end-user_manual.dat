4|6|Public
40|$|The Objective Supply Capability Adaptive Redesign (OSCAR) {{project is}} {{designed}} to identify and develop programs which automate requirements not included in standard army systems. This includes providing automated interfaces between standard army systems at the National Guard Bureau (NGB) level and at the state/territory level. As part of the OSCAR project, custom software has been installed at NGB to streamline management of major end items. This software allows item managers to provide automated disposition on excess equipment to states operating the Standard Army Retail Supply System Objective (SARSS-O). It also accelerates movement of excess assets to improve the readiness of the Army National Guard (ARNG) [...] while reducing excess on hand. The purpose of the <b>End-User</b> <b>Manual</b> is to provide direction and guidance to the customer for implementing the ARNG Excess Management Program...|$|E
40|$|The IPTES toolset and {{methodology}} {{have been developed}} for supporting specifications, design and implementation of real time systems. Such systems are often used in safety critical applications and may thus require intensive testing and analysis before being released for their final use. The IPTES toolset provides analysis mechanisms based on execution and animation of specifications. Such capabilities may be enough for extensive analysis of many hard real-time systems. However, in some cases, part of the system may require additional analysis. New techniques for the analysis of temporal properties based on the IPTES formal kernel model (High-Level Timed Petri Nets) have been studied and developed within the IPTES project. This report describes a first prototype that automatically proofs temporal properties for High-Level Timed Petri Nets. It includes the description of the main design issued, an <b>end-user</b> <b>manual</b> and an experience report that describes the use of the prototype on some in [...] ...|$|E
40|$|What kind of {{software}} documentation a systems needs {{and how much}} documentation that is necessary are questions {{that has a lot}} of different answers depending on a number of variables. Traditional software development methodologies claims that software needs a lot of documentation and more agile approaches claims that it is better to write less documentation since most documentation is never used. According to studies a ratio of 11 % {{of software}} projects costs are spent on documentation alone. The purpose of this study is to create a cost efficient software documentation strategy for an existing web system with a focus on deciding what information are relevant to document in order to keep a high ROI. This study was conducted as a single case study and made in collaboration with a company. The data collection was done by interviewing key people working in the system and doing participants observations. The result shows that information documented in a high level is what is most needed. Artifacts relevant to document are the source code, requirements of updates, functional tests, high-level architecture, reference manual and an <b>end-user</b> <b>manual.</b> The result also shows that new processes need to be implemented for the documentation strategy to be efficient. Recommendation for further research is to create a method of how to calculate the ROI for software documentation based on a number of organizational variables. Validerat; 20160615 (global_studentproject_submitter) </p...|$|E
40|$|In {{the grid}} environment, {{the problem of}} {{application}} software deployment and management is a major practical challenge for the <b>end-users.</b> <b>Manual</b> operation is error-prone and not scalable to large grids. In this work, we propose an automation tool for Application software DEployment and Management on Open Science Grid: ADEM. On {{the basis of the}} grid middleware Globus, it is integrated with pacman. Currently, it can be adaptive to pre-build and dynamic build approaches. NMI B&T system is adopted for the pre-build function. After the application software packaging, ADEM is mainly for automatic deployment, update or removing. The automatic workflow includes automatically getting the available grid sites with their signatures, site signature based automatic deployment or management on a set of grid sites in parallel, automatic dependencies check and integration, automatically preventing some possible errors, automatically getting the results. And the provenance tracking is helpful for the troubleshooting of potential exceptions. Some experiment results on Open Science Grid (OSG) show that ADEM is easy to use, more successful and efficient than manual operation...|$|R
40|$|My {{assignment}} at VVOB Cambodia is {{to improve}} a digital clearinghouse, krou. org. "Krou" is Khmer for teacher. The clearinghouse will be a website where teachers can download materials for their classes, as well as upload their own materials and lesson plans. The main purpose of my work {{is to improve}} the functionality of this Joomla-based site and to provide documentation. Improvements to the site 2 ̆ 7 s functionality involve a new tag based search functionality to replace the old folder based search and administrator tools for more efficient management of the site. These improvements are realized using existing Joomla extensions or editing the PHP files that make up these extensions. The documentation contains both <b>end-user</b> <b>manuals</b> and a complete analysis of the site 2 ̆ 7 s back-end. This will be crucial because the VVOB plans to hand control of the site over to MoEYS (Ministry of Education, Youth and Sport). At the end of my work placement, I will be giving trainings to teachers and to the site administrators on how to use the new functionality as efficiently as possible...|$|R
30|$|Technology {{providers}} and <b>end-users</b> identify that <b>manual</b> process alone is insufficient for searching exhaustively colossal amount of video data and {{for meeting the}} need for screening timely. To alleviate these issues, {{we are trying to}} project outdoor surveillance camera activity to Google Maps that makes it easier to have holistic and summarized view of videos. Vast amounts of video data render ineffective manual video analysis though current automatic video analytics techniques undergo inadequate performance [28].|$|R
40|$|For {{almost a}} decade the {{medicine}} and pharmacy database/search engine [URL] raviminfo. ee has been available on the web. I am the author of that web-site {{and it has been}} nice to see that the popularity of the site is constantly growing. More and more people are using it as the Estonia’s best source for getting information about drugs and other products sold in pharmacies. As almost every person is using mobile phone and {{more and more people are}} using smart phones, the only logical continuation for developing that site further was to create a native mobile application. As pilot platform we chose iOS - Apple’s operating system for all the mobile devices like iPhones, iPod Touches and iPads. The main reason for developing the application for iOS was my interest in smart phone niche and in Apple devices in particular. Further more, as I was also leading and developing similar project (LasaLara learn- ing environment for the iPhone) for the course Software Project, it seemed logical to go on with the chosen path and create another useful application that every person could use. There are two main reasons for using the original raviminfo. ee web-site. First one is to get information about pharmacies, like where they are located and what are their opening hours and other contact information. Second one is searching for the medicines from the pharmacies together with the price information as well as all the detailed information about these medicines. Implementing these two functionalities was also the main scope for the mobile application. However, since mobile devices are more limited, especially in screen sizes compared to regular computers with large screens, the main difference is the data presentation. I had to make compromises for fitting all the needed data into small screen of the mobile phone. In addition, as mobile devices often have GPS built-in this gave an interesting possibility to be able to show users information about the nearest pharmacies, to make the user experience even more comfortable and pleasing. This thesis is divided into four chapters. The first chapter gives an overview why such a mobile application could be useful and describes some of the similar mobile applications available on iOS. The second chapter contains the requirement analysis for the application. In the third chapter the design and implementation of the application is described. In the fourth chapter a short <b>end-user</b> <b>manual</b> is available to give a brief overview on how to use the application...|$|E
40|$|PhD thesis in Information technologyThe {{last decade}} has seen an {{exponential}} increase in mobile computing devices, {{as well as}} an increasing adoption of sensor technology in process industry, homes and public spaces. The increasing amount of information made available by such devices has led to a class of pervasive systems that require little or no user input. Smart home systems is an example of such pervasive systems. A main obstacle for application developers dealing with sensor-based systems is heterogeneity of devices and protocols. A common obstacle for <b>end-users</b> is the <b>manual</b> configuration of networked devices. Our first research contribution is a middleware that overcomes these obstacles: The SENSEWRAP middleware addresses the problem of heterogeneity in a smart home setting through the virtualization of hardware and services. Furthermore, it provides automatic network configuration and service discovery. The usefulness of pervasive systems usually correlates with their ability to perform their functions in the background, without user involvement. Instead, these systems base their actions on available information relevant to their application, e. g., they are information-driven. For information-driven systems, like smart-home systems and other pervasive systems to be able to decide on the correct action at the right time, it is vital that the correct information is made available to them in a timely manner. A primary asset of publish/subscribe interactions is the immediate distribution of new information available to interested parties, and as such, it is a well-suited model for building highly scalable and flexible systems that are able to cope with a dynamic environment. Complex event processing is a fairly new paradigm that refers to the processing and correlation of events as they occur. There exists several specialized programming languages for performing complex event processing. A main goal of such languages is to enable the programmer to express patterns of events in a simpler and more straightforward manner than what is possible with a generalpurpose programming language. A main contribution of this thesis is an exploration of the tradeoffs involved in using a specialized, declarative event processing language versus using a general-purpose, imperative programming language for event processing applications. Our results indicate that going the specialized language route does indeed simplify development of event processing applications, but that this comes at the expense of performance. Furthermore, we present the EVENTCASTER platform for building eventbased systems, on which we have built two novel event processing applications: The viewer statistics and ADSCORER applications are research contributions in their own right. The viewer statistics application demonstrates how event processing techniques can be applied to broadcast television, in order to provide more accurate viewer statistics than what is currently available, in near-real time. With the ADSCORER application, advertisers and broadcasters are provided with a detailed evaluation of each individual advertisement, previously only available to advertisements distributed on the web...|$|R
40|$|Previous {{papers on}} Grey {{literature}} {{by the authors}} have described (1) the need for formal metadata to allow machine understanding and therefore scalable operations; (2) the enhancement of repositories of grey (and other) e-publications by linking with CRIS (Current Research Information Systems); (3) {{the use of the}} research process to collect metadata incrementally reducing the threshold barrier for end-users and improving quality in an ambient GRIDs environment. This paper takes the development one step further and proposes "intelligent" grey objects. The hypothesis is in 2 parts: (1) that the use of passive catalogs of metadata does not scale (a) in a highly distributed environment with millions of nodes and (b) with vastly increased volumes of R and D output grey publications with associated metadata; (2) that a new paradigm is required that (a) integrates grey with white literature and other R and D outputs such as software, data, products and patents (b) in a selfmanaging, self-optimising way and that this paradigm manages automatically curation, provenance digital rights, trust, security and privacy. Concerning (1) existing repositories provide catalogs; harvesting takes increasing time ensuring non-currency. The <b>end-user</b> expends much <b>manual</b> effort / intelligence to utilise the results. The elapsed time of (i) the network (ii) the centralised (or centrally controlled distributed) catalog server searches (iii) end-user intervention becomes unacceptable. Concerning (2) there is no paradigm currently known to the authors that satisfies the requirement. Our proposal is outlined below. Hyperactive combines both hyperlinking and active properties of a (grey) object. Hyperlinking implies multimedia components linked to form the object and also external links to other resources. The term active implies that objects do not lie passively in a repository to be retrieved by end–users. They "get a life" and the object moves through the network knowing where it is going. A hyperactive grey object is wrapped by its (incrementally recorded) formal metadata and an associated (software) agent. It moves through process steps such as initial concept, authoring, reviewing and depositing in a repository. The workflow is based on the rules and information in the corporate data repository with which the agent interacts. Once the object is deposited, the agent associated with it actively pushes the object to the end-users (or systems) whose metadata indicate interest or an obligation in a workflowed process. The agents check the object and user (or system) metadata for rights, privacy, security parameters and for any charges and assure compatibility. Alternatively the object can be found passively by end-user or system agents. The object can also associate itself with other objects forming relationships utilising metadata or content. Declared relationships include references and citations; workflowed relationships include versions and also links to corporate information and research datasets and software; inferenced relationships are discovered relationships such as between documents by different authors developed from an earlier idea of a third author. Components of this paradigm have been implemented to some extent. The challenge is implementing – respecting part two of the hypothesis - the integration architecture. This surely is harnessing the power of grey. Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notesXAInternationa...|$|R

