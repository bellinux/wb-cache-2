22|387|Public
50|$|The Reuters <b>instrument</b> <b>code</b> (RIC) was {{originally}} defined by Herbie Skeete, the Reuters Executive {{who wrote the}} original product specifications for the first products on Reuters' Integrated Data Network (IDN). RICs as originally defined by Skeete {{were meant to be}} logical and intuitive.|$|E
50|$|A Reuters <b>instrument</b> <b>code,</b> or RIC, is a ticker-like code used by Thomson Reuters to {{identify}} financial instruments and indices. The codes {{are used for}} looking up information on various Thomson Reuters financial information networks (like Bridge and RMDS) and appear to have developed from the Quotron service they purchased in the 1980s.|$|E
50|$|The NASDAQ-100 {{is often}} {{abbreviated}} as NDX in the derivatives markets. Its corresponding futures contracts are {{traded on the}} Chicago Mercantile Exchange. The regular futures are denoted by the Reuters <b>Instrument</b> <b>Code</b> ND, and the smaller E-mini version uses the code NQ. Both {{are among the most}} heavily traded futures at the exchange.|$|E
40|$|Abstract- What {{should not}} be surprising, {{especially}} {{to anyone who has}} attempted to build a JAVA model, is that collecting method level data is difficult. The goal {{of this paper is to}} describe how to create Java method models with performance data obtained from ARM <b>instrumented</b> <b>code.</b> The paper begins by discussing how to <b>instrument</b> Java <b>code</b> using ARM and concludes by presenting an overview of analytical Java method modeling...|$|R
5000|$|Having {{regard to}} the need for {{cooperation}} between the International Labour Organization, the World Health Organization, the International Atomic Energy Agency and other relevant institutions and noting the relevant <b>instruments,</b> <b>codes</b> of practice, codes and guidelines issued by these organizations, and ...|$|R
40|$|<b>Instrumenting</b> <b>code</b> {{to collect}} {{profiling}} information can cause substantial execution overhead. This overhead makes instrumentation difficult {{to perform at}} runtime, often preventing many known offline feedback-directed optimizations from being used in online systems. This paper presents a general framework for performing instrumentation sampling to reduce the overhead of previously expensive instrumentation. The framework is simple and effective, using code-duplication and counter-based sampling to allow switching between <b>instrumented</b> and non-instrumented <b>code...</b>|$|R
5000|$|In September 2013, {{the company}} sought an IPO in the United States after a deal {{could not be}} reached with Hong Kong regulators. Planning took over 12 months before the company's market début in September 2014, with Reuters <b>Instrument</b> <b>Code</b> [...] "BABA.N". The pricing of the IPO {{initially}} raised US$21.8 billion, which later increased to US$25 billion, making it the largest IPO in history. Buyers were actually purchasing shares in a Cayman Islands shell corporation, not in the Alibaba group, as China forbids foreign ownership of its companies.|$|E
5000|$|The RIC {{is made up}} {{primarily}} of the security's ticker symbol, optionally followed by a period and exchange code based on {{the name of the}} stock exchange using that ticker. For instance, IBM.N is a valid RIC, referring to IBM being traded on the New York Stock Exchange. IBM.L refers to the same stock trading on the London Stock Exchange. The exchange code used in the RIC is proprietary to Reuters. Exchange codes have an ISO standard, ISO 10383, which is not used by the Reuters <b>Instrument</b> <b>Code,</b> but is in common use elsewhere. Ticker symbols are often reused on different exchanges, so in many cases the same ticker symbol references different securities.|$|E
30|$|AFL is {{the first}} to {{introduce}} the edge measurement method into coverage-based fuzzing. We take AFL as an example and show how coverage-based fuzzers gain coverage information during the fuzzing process. AFL gains the coverage information via lightweight program instrumentation. According to whether the source code is provided, AFL provides two instrumentation mode, the compile-in instrumentation and external instrumentation. In compile-in instrumentation mode, AFL provides both gcc mode and llvm mode, according to the compiler we used, which will <b>instrument</b> <b>code</b> snippet when binary is generated. In external mode, AFL provides qemu mode, which will <b>instrument</b> <b>code</b> snippet when basic block is translated to TCG blocks.|$|E
40|$|<b>Instrumenting</b> <b>code</b> {{to collect}} {{profiling}} information can cause substantial execution overhead. This overhead makes instrumentation difficult {{to perform at}} runtime, often preventing many known offline feedback-directed optimizations from being used in online systems. We present a general framework for <b>instrumenting</b> <b>code</b> that uses fine-grained sampling to allow previously expensive instrumentation to be performed accurately with low overhead. Our framework does not rely on any hardware or operating system support and is fully tunable; the sample rate can be adjusted at any time to match the type of instrumentation being performed. By reducing the overhead of instrumentation, our framework eliminates {{one of the biggest}} obstacles to performing feedback-directed optimizations at runtime. We present experimental results validating the low overhead and high accuracy of our technique...|$|R
40|$|<b>Instrumenting</b> <b>code</b> {{to collect}} proling {{information}} can cause substantial execution overhead. This overhead makes instrumentation difficult {{to perform at}} runtime, often preventing many known offline feedback-directed optimizations from being used in online systems. This paper presents a general framework for performing instrumentation sampling to reduce the overhead of previously expensive instrumentation. The framework is simple and effective, using code-duplication and counter-based sampling to allow switching between <b>instrumented</b> and non-instrumented <b>code.</b> Our framewor...|$|R
50|$|In November 2009, The European Commission opened formal anti-trust {{proceedings}} against Thomson Reuters {{concerning a}} potential infringement of the EC Treaty's rules on abuse of a dominant market position (Article 82). The Commission investigated Thomson Reuters' {{practices in the}} area of real-time market datafeeds, and in particular whether customers or competitors were prevented from translating Reuters <b>Instrument</b> <b>Codes</b> (RICs) to alternative identification codes of other datafeed suppliers (so-called 'mapping') to the detriment of competition. In December 2012, the European Commission adopted a decision that renders legally binding the commitments offered by Thomson Reuters to create a new licence ("ERL") allowing customers, for a monthly fee, to use Reuters <b>Instrument</b> <b>Codes</b> (RICs) in applications for data sourced from Thomson Reuters' real time consolidated datafeed competitors to which they have moved.|$|R
40|$|Abstract We {{address the}} problem of how to <b>instrument</b> <b>code</b> tolog events for {{conformance}} testing purposes, and how to write test oracles that process log files. We specificallyconsider oracles written in languages based on the statemachine formalism. We describe two processes for sys-tematically deriving logging code and oracles from requirements. The first is a process that we have used andtaught, and the second is a more detailed process that we propose to increase the flexibility and traceability of thefirst process. 1...|$|E
40|$|Abstract. An {{extension}} of the λ-calculus is proposed to study historybased access control. It allows for parametrized security policies with a possibly nested, local scope. To govern the rich interplay between local policies, we propose a combination of static analysis and dynamic checking. A type and effect system extracts from programs a correct approximation to the histories obtainable at run-time. A further static analysis over these approximations determines how to <b>instrument</b> <b>code</b> so to enforce the desired security constraints. The execution monitor, based on finite-state automata, runs efficiently the instrumented code. ...|$|E
40|$|Although new Java virtual {{machines}} {{provide an}} API to obtain raw performance data, {{it is still}} the task of a skillful performance analysis tool to take all the strategic decisions for instrumentation and performance analysis of distributed Java programs. In this paper we demonstrate two new tools, Twilight and Aksum, which try to automatically <b>instrument</b> <b>code</b> regions, to determine what performance data to collect, to interpret performance data, and to relate the bottlenecks found back to source code. We present experiments with a widely distributed Java application running on a heterogeneous set of machines with different operating systems to demonstrate the efficacy of our tools. 1...|$|E
5000|$|ISO 10962 - Standard for {{financial}} <b>instrument</b> classification <b>codes</b> ...|$|R
40|$|Governance of {{mobility}} {{is a key}} feature of state-making, which produces and is produced by a dynamic relation between territory (i. e., the spatial limits {{of the state and}} the extent of its coercive power, typically marked as a line on a map) and territoriality (i. e., the cross-cutting legal jurisdictions, <b>instruments,</b> <b>codes,</b> and medi...|$|R
5000|$|... • Issuer {{information}} (Issuer domicile country, Issuer ESA 95 sector, …)• <b>Instrument</b> information (ISIN <b>code,</b> <b>Instrument</b> ESA 95 class, …)• Price data information (Nominal currency, Quotation basis, Price value, …) ...|$|R
40|$|Abstract. The {{problem of}} {{capturing}} provenance for computational tasks has recently received significant attention, {{due to the}} new set of bene-ficial uses (for optimization, debugging, etc.) of the recorded data. We develop a provenance collection system aimed at scientific applications {{that are based on}} the Common Component Architecture (CCA) that alleviates scientists from the responsibility to manually <b>instrument</b> <b>code</b> in order to collect provenance data. Our system collects provenance data at the granularity of component instances, by automatically recording all method invocations between them, including all input and output parameters. By relying on asynchronous communication and using op-timizations to handle large data arrays, the overhead of our system is low-enough to allow continuous provenance collection. ...|$|E
40|$|We {{present a}} technique, called binary wrapping, that allows object code {{routines}} to be instrumented. Binary wrapping allows tracing {{to be placed}} around (and sometimes within) proprietary code, when source code access is difficult or impossible. This technique is based on wrapping user-written code around the object code routine. No modifications are needed to the programs that call the object code routine. Binary wrapping has proven itself invaluable in instrumenting proprietary libraries, and may well be useful in other similar circumstances. 1. MOTIVATION It is often useful to trace the activity of system-provided library routines. This tracing can be made difficult when only the binary code is available. In the case of proprietary libraries, source code is often not available, so the tracing statements can not be inserted in source code form. We were motivated {{by the need to}} <b>instrument</b> <b>code</b> for a performance measurement tool; the additional statements were used to generate traces [...] ...|$|E
40|$|The {{nature of}} variety of {{classical}} biomedical applications covering basic electrophysiology to cardiovascular hemodynamics is nowadays PC-based and being automated using software tools. Muscle Stimulator {{is used by}} physiotherapist to give physiotherapeutic treatment to the patients who have muscle related problems. A patient is subjected to electrical pulses using electrodes and it heals the patient. These signals are produced by implementing a Virtual <b>Instrument</b> <b>code</b> using software tools. This system is more accurate, compact, more flexible and self diagnostic. This is implemented and GUI is developed for the simulation of different current waveforms such as Galvanic, Interrupted Galvanic, Faradic and Surged Faradic which are generated in this work. It has flexibility to adjust its amplitude, frequency and duty cycle values. This will have same functionality as of Electrical Muscle Stimulator like waveform generation, varying parameters of current waveform (voltage, frequency and duty cycle), waveform display, selection of required signal as per patient etc. Finally the system itself can behave/operate as the central controlling unit...|$|E
40|$|Software {{analysis}} tools and techniques often leverage structural code coverage information to reason about the dynamic behavior of software. Existing techniques <b>instrument</b> the <b>code</b> with the required structural obligations and then monitor {{the execution of}} the compiled code to report coverage. Instrumentation based approaches often incur considerable runtime overhead for complex structural coverage metrics such as Modified Condition/Decision (MC/DC). Code instrumentation, in general, has to be approached with great care to ensure it does not modify the behavior of the original <b>code.</b> Furthermore, <b>instrumented</b> <b>code</b> cannot be used in conjunction with other analyses that reason about the structure and semantics of the code under test. In this work, we introduce a non-intrusive preprocessing approach for computing structural coverage information. It uses a static partial evaluation of the decisions in the source code and a source-to-bytecode mapping to generate the information necessary to efficiently track structural coverage metrics during execution. Our technique is flexible; the results of the preprocessing can be used by a variety of coverage-driven software analysis tasks, including automated analyses that are not possible for <b>instrumented</b> <b>code.</b> Experimental results in the context of symbolic execution show the efficiency and flexibility of our nonintrusive approach for computing code coverage informatio...|$|R
50|$|A {{national}} numbering agency (NNA) is {{the organisation}} in each country responsible for issuing International Securities Identification Numbers (ISIN) as described by the ISO 6166 standard and the Classification of Financial <b>Instruments</b> <b>code</b> as described by the ISO 10962 standard. The role of NNA is typically assigned to the national stock exchange, central bank or financial regulator but may be as diverse as a financial data provider or clearing and custodian organisation for that country.|$|R
50|$|One of the {{greatest}} musicians who used the <b>instrument</b> is <b>Codé</b> di Dona. Other musicians with the instrument include Bino Branco from the band Ferro Gaita.|$|R
40|$|Energy {{efficiency}} {{is a key}} concern {{in the design of}} modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output precision for energy efficiency. However, this tradeoff can have unexpected effects on computation quality. This thesis presents dynamic analysis tools to study, debug, and monitor the quality and energy efficiency of approximate computations. We propose three styles of tools: prototyping tools that allow developers to exper-iment with approximation in their applications, offline tools that <b>instrument</b> <b>code</b> to determine the key sources of error, and online tools that monitor the quality of deployed applications in real time. Our prototyping tool is based on an extension to the functional language OCaml. We add approximation constructs to the language, an approximation simulator to the runtime, and profiling and auto-tuning tools for studying and experimenting with energy–quality tradeoffs. We also present two offline debugging tools and three online monitoring tools. The first offline tool identifies correlations between output quality and the total number of executions of, and errors in, individual approximate oper...|$|E
40|$|Supporting atomic blocks (e. g., Transactional Memory (TM)) {{can have}} {{far-reaching}} effects on language design and imple-mentation. While much {{is known about}} the language-level semantics of TM and the performance of algorithms for im-plementing TM, little is known about how platform char-acteristics affect the manner in which a compiler should <b>instrument</b> <b>code</b> to achieve efficient transactional behavior. We explore the interaction between compiler instrumen-tation and the performance of transactions. Through evalu-ation on ARM/Android, SPARC/Solaris, IA 32 /Linux, and IA 32 /MacOS, we show that the compiler must consider the platform when determining which analyses, transforma-tions, and optimizations to perform. Implementation issues include how TM library code is reached, how per-thread TM metadata is stored and accessed, and how a library switches between modes of operation. We also show that different platforms favor different TM algorithms, through the intro-duction of a new TM algorithm for the ARM processor. Our findings will affect compiler and TM library designers: to achieve peak performance for transactions, the compiler must perform platform-dependent analysis, transformation, and optimization, and the interface to the TM library must differ according to platform...|$|E
40|$|SAT-based bounded {{verification}} of annotated code consists of translating the code {{together with the}} annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the error is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this article we present TACO, a prototype tool which implements a novel, general and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We <b>instrument</b> <b>code</b> analysis with a symmetry-breaking predicate that allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading in the experiments we have carried out to an improvement on {{the efficiency of the}} analysis of orders of magnitude, compared to the noninstrumented SAT-based analysis. We show that, in some cases, our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking or SMT-solving...|$|E
40|$|Debugging massively {{parallel}} data analysis programs is cur-rently a difficult process. Traditional debug cycles involve manual code instrumentations, re-execution {{and analysis of}} the resulting data. This is expensive in terms of develop-ment time, execution time, amount of data produced, and cognitive overhead. This work proposes a course of research that is meant to alleviate this situation by automating the code instrumentation and by lowering the re-execution time of <b>instrumented</b> <b>code.</b> By using these techniques, we hope to achieve a higher efficiency compared to manual debugging approaches. 1...|$|R
40|$|Abstract. A {{technique}} for elegantly expressing In-lined Reference Monitor (IRM) certification as model-checking is presented and implemented. In-lined Reference Monitors (IRM’s) enforce software security policies by in-lining dynamic security guards into untrusted binary code. Certifying IRM systems provide strong formal guarantees for such systems by verifying that the <b>instrumented</b> <b>code</b> {{produced by the}} IRM system satisfies the original policy. Expressing this certification step as model-checking allows well-established model-checking technologies {{to be applied to}} this often difficult certification task. The technique is demonstrated through the enforcement and certification of a URL anti-redirection policy for ActionScript web applets. ...|$|R
40|$|Often {{parallel}} scientific {{applications are}} instrumented and traces are collected and analyzed to identify processes with performance problems or operations that cause delays in program execution. The execution of <b>instrumented</b> <b>codes</b> may generate {{large amounts of}} performance data, and the collection, storage, and analysis of such traces are time and space demanding. To address this problem, this paper presents an efficient, systematic, multi-step methodology, based on hierarchical clustering, for analysis of communication traces of parallel scientific applications. The methodology is used to discover potential communication performance problems of three applications: TRACE, REMO, and SWEEP 3 D. 1...|$|R
40|$|Abstract. In model-driven {{verification}} a model checker executes {{a program}} by embedding it within a test harness, thus admitting program verification {{without the need}} to translate the program, which runs as native code. Model checking techniques in which code is actually executed have recently gained popularity due {{to their ability to}} handle the full semantics of actual implementation languages and to support verification of rich properties. In this paper, we show that combination with dynamic analysis can, with relatively low overhead, considerably extend the capabilities of this style of model checking. In particular, we show how to use the CIL framework to <b>instrument</b> <b>code</b> in order to allow the SPIN model checker, when verifying C programs, to check additional properties, simulate system resets, and use local coverage information to guide the model checking search. An additional benefit of our approach is that instrumentations developed for model checking may be used without modification in testing or monitoring code. We are motivated by experience in applying model-driven verification to JPL-developed flight software modules, from which we take our example applications. We believe this is the first investigation in which an independent instrumentation for dynamic analysis has been integrated with model checking. ...|$|E
40|$|Energy {{efficiency}} {{is a key}} concern {{in the design of}} mod-ern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality prob-lems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the qual-ity of approximate computations. We propose both offline debugging tools that <b>instrument</b> <b>code</b> to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications. We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identi-fies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary ap-proaches designed for detecting quality problems in de-ployed applications, while still maintaining the energy sav-ings from approximation. We present implementations of our techniques and de-scribe their usage with seven applications. Our online moni-tors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality...|$|E
40|$|Despite {{the rapid}} growth of the mobile technology, mobile devices are still {{considered}} as resource constrained with limited battery. Same computations are awkward to be undertaken on these devices with limited processing capabilities. Other processes are costly in terms of battery consumption. Ideally, mobile applications will have the possibility to decide either to do a computation locally or remotely depending on the current device capabilities status. Making such decision is very challenging as many interrelated factors are to be considered (e. g. network connection, battery level, and processing capabilities). In this paper, we propose a framework that supports developers in implementing such smartness fitness within their mobile applications. This solution provides approaches in form of algorithms to <b>instrument</b> <b>code</b> of mobile applications to behave in smart way. Incorporating these algorithms will allow for on-the-fly decision of local versus remote computation using a calculated cost function. We conducted some experimental scenarios to evaluate the usability and effectiveness of our decision-based algorithms. The results we have obtained prove that for the same computation, {{depending on the size of}} data, the network status and the device status, the decision of the engine may differ...|$|E
40|$|This study aims {{to develop}} a new <b>coding</b> <b>instrument</b> to {{investigate}} the interactivity of brand websites. The new <b>coding</b> <b>instrument</b> contains 47 interactive functions and is directly linked to three theoretical interactivity dimensions: two-way communication, synchronicity, and active control. Application of the instrument shows that the instrument can be used in different contexts. In addition, the content analysis reveals interesting differences between American and Dutch websites, and between websites of durable goods, non-durable goods, and services. The developed <b>coding</b> <b>instrument</b> can be used in future research, for example to assess which interactive functions contribute to perceived interactivity of a website...|$|R
5000|$|A system trace {{provides}} {{visibility of}} various events/states inside the embedded system. Trace {{data can be}} generated by <b>instrumented</b> application <b>code</b> and/or by hardware modules within the SoC. A SoC may contain several system traces.|$|R
40|$|My {{research}} {{is in the}} area of Systems and High Performance Computing, specifically performance profiling of distributed and parallel programs. My approach combines static analysis with runtime data to map data centric performance information to program variables. This includes direct measurement for data metrics such as cache misses. It also examines an inclusive approach, variable blame, that analyzes all data flow that goes into the computation of a variable, which is utilized for time based metrics. My earlier research included work on the Dyninst project, a dynamic instrumentation tool. I specifically worked on optimizing instrumentation code, adding OpenMP functionality for binary analysis and instrumentation, and register liveness analysis for <b>instrumented</b> <b>code...</b>|$|R
