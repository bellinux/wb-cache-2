6|204|Public
50|$|In {{the same}} year, the Egyptian Government, began issuing smart {{taxation}} cards to taxpayers for security reasons. ACS's ACR100 SIMFlash Smart Card Readers {{were used to}} <b>input,</b> <b>check</b> and back-up transaction information of the taxpayers.|$|E
40|$|The {{contactless}} check procedure {{has been}} substantiated, and the theoretical description of dependences in {{the characteristics of}} heating wires upon the capacitive meter indices has been substantiated. The factors distrurbing this relation have been found. It has been proposed to optimize the geometric meter parameters by a mathematical modelling method. The device for contactless check of the heating wire characteristics on the extrusion plant and for <b>input</b> <b>check</b> of such wires at EHP manufacture has been developed. The contactless check device of heating wire parameters has been introduced in the Production Intra-Economic Association "Saratovoblagropromenergo". The rejection of conductor products was reduced by 15 - 20 %, the labour provuctivity was increased by 10 - 12 %. Application field: research and design institutes, enterprises on manufacture of concrete heating floors, panels, mats, heating and other wires of electrotechnical purposeAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Report on {{the study}} of IDN with respect to south Indian Languages IDNA (Internalizing Domain Names in Applications) allows the non-ASCII {{characters}} to be represented using ASCII characters. This is done by converting the internationalized domain names to punycode (which is a unique and reversible transformation of Unicode string to ASCII). During the conversion of internationalized domain names to punycodes it passes through a process called NAMEPREP. NAMEPREP specifies a framework of processing rules such as Mapping(For each character in the <b>input,</b> <b>check</b> if it has a mapping and, if so, replace it with its mapping), Normalization(Possibly normalize the result of step 1 using Unicode Normalization), Prohibited Output(Check for any characters that are not allowed in the output if any are found, return an error), Bidirectional characters(Possibly check for right-to-left characters, and if any are found, make sure that the whole string satisfies the requirements for bidirectional strings) for Unicode text. A profile of STRINGPREP MUST include:- The intended applicability of the profile- The character repertoire that is the input and output to stringprep- The mapping tables used- Any additional mapping tables specific to the profile- The Unicode normalization used, if any- Th...|$|E
5000|$|If {{the script}} has no <b>input</b> <b>checking,</b> instead trusting that the {{filename}} is always valid, a malicious user could forge a URL to retrieve configuration files from the webserver: ...|$|R
5000|$|... and [...] {{family of}} I/O routines, {{for lack of}} (either any or easy) <b>input</b> length <b>checking.</b>|$|R
40|$|This release has the {{following}} changes: adds (unit) testing with the testthat package adds a tripleCount(store) method some documentation improvements typo in package description now lists all formats in asString() add. data. triple() and load. rdf() should now stop() when exceptions happen (closes # 10) add. data. triple() now checks the data type given sparql. remote() now discusses bif:contains (closes # 32) now uses missing() instead of is. null() for <b>input</b> <b>checking</b> (addresses # 34...|$|R
40|$|Burgeoning coastal development, {{recreational}} use, and {{the future}} affects of climate change are placing increasing strain on regulators to manage risk associated with coastal hazards. Low-lying coastal communities in particular are vulnerable {{to a range of}} natural hazards including coastal erosion, storm surge inundation, tsunami and water safety that come with varying levels of risk to life and property. New Zealand's coastal hazard monitoring network is patchy and resources are limited. As a consequence there is considerable potential for coastal communities are going to need to take {{a more active role in}} monitoring their environment and building data bases and knowledge that can be used to better manage their coast. This paper describes simple methodologies based on the needs of various community groups and sound science principles that can be used to monitor beaches and the coastal environment. By employing these tools councils, technical experts and community groups will be able to make better-informed decisions for managing activities in the coastal environment. One of the keys to the successful uptake of a monitoring programme by a community group is its relevance to the group. The programme and the tools provided must fit the interests, needs, capability and resources of the group. This project develops tools for coastal monitoring and targets coastal community groups such as Coast Care, Coastal Hapu, Secondary Schools, and Surf Life Saving Clubs. The monitoring methodologies have been developed in consultation with Tainui ki Whaingaroa hapu, Raglan Area School, and the Waikato Beach Care and Coast Care Bay of Plenty. Successful methodologies for measuring changes on the coast are also those that are matched to the type of beach, use appropriate equipment, collect structured data, provide data to which analysis can be applied, incorporate local knowledge of the environment, and feed results back to the community and other interested parties such as councils and science organisations. This project provides the target groups with simple monitoring methodologies, field forms/checklists, and appropriate survey and measurement equipment (which have undergone field trials) to carry out coastal monitoring. A web-based facility has been developed to <b>input,</b> <b>check</b> and store data; and provide immediate feedback using graphs and images. It also provides background information on coastal processes relevant to monitoring programmes. In this manner, a scientifically robust data set is collected and stored within a secure and future proofed archive, providing valuable information to coastal groups for years to come. Although the primary objective of this research is to develop a means for coastal communities to monitor changes in their environment, there are additional benefits associated with engaging communities in the study of their environment. These benefits include increasing awareness of coastal hazards, capacity building, providing valuable educational resources, and improving the temporal and spatial data coverage of information for the New Zealand coastline...|$|E
40|$|Concrete slabs {{are today}} usually {{designed}} by traditional hand calculation methods. Powerful numerical calculation methods like the Finite Element Method (FEM) are not recommended in design handbooks for design of slabs, see e. g. Hillerborg et al (1990),(1996). In contrary, its distribution of reinforcement {{is considered to}} be unsuitable for practical use. Most FE-programs are also more adapted for analyses than for design. SKANSKA IT Solutions in Malmö, Sweden, has developed a FE-based design program called FEM-Design. The program handles e. g. FE-analyses and design of frames, trusses, beams, shear walls and plates. In this report the program is checked and improved regarding the crack analysis, the calculated effects of actions and the design of reinforcement. Another importent issue discussed is the extreme-value problem of moments in centre of interior columns/walls in flat slab floors. The models used are checked regarding influence of mesh density, element types, column widths and modelling of column stiffness. The FE-analyses show that the mesh density and the modelling of the column stiffness mainly affects the size of the support moments, whereas the field moments are almost independent of all modelling parameters. Fem-Design's automatically generated mesh gave good results with respect {{to the size of the}} support moments. However, the result of moment distribution or actually the reinforcement distribution could be improved by distributing the column stiffness over one plate element. The multi spring concept is also suggested for interior walls wider than 0. 2 m. The iterative crack analysis is compared with Abaqus/Explicit smeared cracks and an experimental test, McNiece (1978). FEM-Design's crack analysis is found to be adequate for design, despite that the crack propagation differs quite much in comparison with Abaqus. FEM-Design's load-displacement curve shows better agreement with the experimental test. The difference depends on the implemented crack theory i. e. when a crack {{is considered to be}} a crack. The distribution of design moments for a simple flat slab floor are compared with designs by two hand calculation methods, the strip method and the yield line theory. The comparison shows that FEM-Design's design moments or actually the required reinforcements have to be chosen at certain points and redistributed by a design method. A FE-based design method is developed with respect to the capabilities of FEM-Design and FE-analyses performed with the traditionally distributed reinforcement. Comparisons between the three methods show that the FE-based reinforcement design method (FED) distributes less total amount of reinforcement than the two traditional methods with respect of both bending- and final design. The thesis concludes that FE-analyses can be used to get a practical reinforcement design in concrete slabs - if the reinforcement like for other methods are redistributed in appropriate areas/strips. This is in contradiction to statements by Hillerborg et al (1990), (1996). Finally, FEM-Design has proven to give reliable analyses and designs, for all tested cases. Actually, there are very few drawbacks with the use of a FE-based design, especially since FEM-Design's plate module is found to be a very user-friendly design tool. However, three improvements/implementations are suggested to make the program even better: • A distributed stiffness (the multi spring concept) to model interior columns/walls. • A more available and clear <b>input</b> <b>check</b> option. • A distribution method for reinforcement. The distribution method (FED), proposed in the thesis is suggested as one suitable method to implement, because it combines FE-theory with theories behind traditional design methods. Validerat; 20101217 (root...|$|E
40|$|This study {{aimed at}} the re-organization and {{simulation}} for the multi-functional center of rose production in Pu-Li district. Topics of re-organization and simulation included layout of placement, procedure of operation, specification of machines, and labors estimation. The operation procedure was analyzed {{on the base of}} dynamic information of rose production and marketing. Moreover, flowchart of IDEFO was applied to explore the post-harvest processes for rose production. The results suggested the post-harvest processes for rose production should include <b>input</b> <b>check,</b> pre-cooling, pre-treatment, grading, film wrapping, packing, and temporary storage in cooling chamber. However, the facilities considered in simulation contained pre-cooling chamber, AWETA cutting and grading machine, film wrapping machine, packing machine, and cooling chamber. A software named ProModel was used in simulation, and its parameters was decided by layout of placement and the efficiency and functionality of operators, in practice. Two grading level were also tested in simulation and they were 60 %: 40 % and 99. 76 %: 0. 24 % in ratio of grade A and B. Under the condition of eight hours smooth work, the simulation parameters were adjusted based on preliminary study. As a result, the utilization ratio of facilities was upgraded when the packing machine was a semi-automatic type and one packing labor was reduced. The utilization ratio of machines was 100 % for pre-cooling chamber, forklift truck, and grading machine, as the film-wrapping machine was 97 %, semi-automatic packing machine was 45 - 50 %, and cooling chamber was 22 %, respectively. Utilizations of labors, in contrast, were 84 - 94 % for flower hanging, 88 % for pre-cooling, and more than 76 % for packing. In summary, this study simulated the post-harvesting processes for rose production and adjusted the input parameters based on preliminary results. Consequently, the utilization of facilities and labors were upgraded and reached to their optimum conditions. 本研究針對已完工之埔里多功能花卉處理行銷展售中心之玫瑰分級處理區，特就其玫瑰花分級處理相關作業流程、場地佈置、機具設備功能規格等做規劃，並配合設備功能與人力配置預估規劃進行模擬。 經調查以玫瑰花產銷動態來分析作業流程，利用IDEF 0 流程圖清楚表逹玫瑰花採收後處理流程，並據此規劃相關作業流程與設置，其流程規劃為自採收後進入花卉處理中心由進貨點收、真空預冷、預措、分級、包膜、包裝及利用台車暫存至冷藏庫等作業，相關設備規劃有真空預冷庫、荷蘭AWETA玫瑰切花分級機構、自動包膜機、捆包機、冷藏庫等設備，並依此進行模擬。 本研究以ProModel軟體進行整體設備模擬，參考場地佈置及人員操作設備功能及效率列入參數設定，利用動態模擬模技巧方式來模擬分級相關設備及作業人員配置規劃，本模擬另依產銷調查結果設定二組分級作業量模擬，其為A、B等級品項比例為 60 %： 40 %，及 99. 76 %： 0. 24 %二組變數，以符國內市場玫瑰花分級現況及實際情形比較。 在以作業八小時及整體作業順暢之條件下，依第一次模擬結果修正自動捆包機改成半自動設備，減少一封箱人員等作業流程以再次模擬，模擬結果於利用率上大多有提昇，設備利用率分別為：預冷庫、堆高機及分級機接近 100 %，自動包膜機 97 %，半自動捆包機在 45 %- 50 %，另因預留暫存空間較大，故冷藏庫為 22 %。人員利用率分別為：掛花人員為 84 %至 94 %，預冷預措人員為 88 %，裝箱人員亦調整至 76 %以上。利用模擬檢討規劃內容合宜性，並經由結果檢視調整規劃內容，以有效提昇人員之利用率及整場作業效率以達最適規劃。目 錄 目錄……………………………………………………………………………Ⅰ 圖目錄…………………………………………………………………………Ⅲ 表目錄…………………………………………………………………………Ⅴ 一、前言……………………………………………………………………… 1 1. 1 研究動機…………………………………………………………… 1 1. 2 研究目的…………………………………………………………… 3 二、文獻探討………………………………………………………………… 5 2. 1 電腦模擬…………………………………………………………… 5 2. 2 玫瑰花採收後之生理變化與處理………………………………… 9 2. 3 玫瑰花採收後處理分級作業……………………………………… 13 2. 4 IDEF 0 方法論……………………………………………………… 16 三、研究設計………………………………………………………………… 19 3. 1 玫瑰花市場調查…………………………………………………… 19 3. 2 作業現場設置規劃………………………………………………… 24 3. 3 IDEF 0 流程圖製作………………………………………………… 37 3. 4 電腦模擬模式建立………………………………………………… 49 四、結果與討論……………………………………………………………… 53 4. 1 玫瑰切花統計結果討論…………………………………………… 53 4. 2 模擬結果與討論…………………………………………………… 54 五、結論與建議……………………………………………………………… 62 5. 1 結論………………………………………………………………… 62 5. 2 建議………………………………………………………………… 64 六、參考文獻………………………………………………………………… 65 附錄一、模擬結果統計資料模擬一………………………………………… 68 附錄二、模擬結果統計資料模擬二………………………………………… 79 附錄三、第二次模擬結果統計資料模擬一………………………………… 90 附錄二、第二次模擬結果統計資料模擬二………………………………… 101 圖 目 錄 圖 2 - 1 IDEF上下連接圖及定義……………………………………………… 17 圖 2 - 2 IDEF分解圖…………………………………………………………… 18 圖 3 - 1 全台灣省 91 年玫瑰花產區分佈圖…………………………………… 19 圖 3 - 2 全台灣省 91 年花卉產區分佈圖……………………………………… 20 圖 3 - 3 現場設置規劃圖………………………………………………………… 25 圖 3 - 4 AWETA分級機構………………………………………………………… 27 圖 3 - 5 花卉處理中心玫瑰花處理流程………………………………………… 31 圖 3 - 6 花卉預措處理…………………………………………………………… 32 圖 3 - 7 分級機掛花作業………………………………………………………… 34 圖 3 - 8 自動包覆塑膠膜作業…………………………………………………… 34 圖 3 - 9 成品暫存於台車上……………………………………………………… 35 圖 3 - 10 玫瑰放置於立式容器暫存於冷藏庫…………………………………… 36 圖 3 - 11 未分級之玫瑰花預措暫存於冷藏庫…………………………………… 36 圖 3 - 12 玫瑰花採收後處理流程………………………………………………… 38 圖 3 - 13 進貨作業………………………………………………………………… 40 圖 3 - 14 預措預冷作業…………………………………………………………… 42 圖 3 - 15 分級包裝作業…………………………………………………………… 44 圖 3 - 16 冷藏作業………………………………………………………………… 46 圖 3 - 17 出貨作業……………………………………………………………… 48 圖 3 - 18 玫瑰花分級作業模擬配置圖………………………………………… 52 圖 4 - 1 開始模擬時的情況……………………………………………………… 55 圖 4 - 2 模擬過程………………………………………………………………… 55 圖 4 - 3 第二次模擬過程………………………………………………………… 59 表 目 錄 表 2 - 1 模擬的歷史發展……………………………………………………… 7 表 2 - 2 玫瑰花代號等級標準………………………………………………… 13 表 2 - 3 玫瑰花品質標準代號………………………………………………… 14 表 3 - 1 近年來南投縣佔台北花卉產銷(股) 公司玫瑰花進貨百分比……… 21 表 3 - 2 台北花卉產銷(股) 公司 93 年玫瑰花各等級數進貨之百分比……… 21 表 3 - 3 南投縣 92 年供貨至台北花卉產銷(股) 公司玫瑰花各等級數進貨 之百分比………………………………………………………………… 22 表 3 - 4 針對玫瑰A級及B級花之實際佔有率意見調查統計表……………… 23 表 3 - 5 依調查統計另計算南投縣 92 年玫瑰花供貨至台北花卉產銷(股) 公司A、B等級之百分比………………………………………………… 23 表 3 - 6 本研究設備規劃表……………………………………………………… 26 表 4 - 1 二模擬之工作人員利用率……………………………………………… 56 表 4 - 2 二模擬之機具利用率分析……………………………………………… 57 表 4 - 3 修正後得第二次模擬之工作人員之利用率…………………………… 60 表 4 - 3 修正後得第二次模擬之機具之利用率………………………………… 6...|$|E
40|$|Software {{is playing}} {{increasingly}} {{important roles in}} avionics systems. It is widely used in navigation and, in some cases, in control loops that maintain aircraft stability. To guarantee the safety of flight systems, the FAA requires that critical components have a probability of failure no greater than 10 (exp - 9) per hour of flight. Software {{is being used to}} diagnose system components for failure. SIFT (Software Implemented Fault Tolerance) was a computer system developed to study the use of software to check for failure and manage processor reconfiguration. To guarantee that software satisfies its specifications, formal verification can be used. With this a program and its specification are viewed as mathematical objects, and a mathematical proof is used to show that the program and its specification are equivalent. In previous research, a theory of checking was developed to offer assistance in analyzing specifications and designing run-time checks. In the theory, checking is considered abstractly in terms of n-ary relations much like those of relational database theory. Within the theory check are categorized, <b>checks</b> on <b>input</b> and <b>checks</b> on results are considered, and formal attention is given to the minimization and logical combination of checks. The focus is upon <b>input</b> <b>checks</b> and the obstacles in <b>checking</b> <b>input</b> to critical systems. A central concern is with a property referred to as independence. The concern is with circumstances under which it is possible to apply isolated, independent checks to separate sensor inputs and be assure that all illegal input will be properly detected. Presently, independence is being investigated and checked {{in the context of the}} GCS (Guidance and Control System). The GCS simulator is intended for testing software that implements control laws for landing spacecraft. The large number of inputs and their complex interrelationships provide an exciting context in which to investigate independence and the difficulties of supplying <b>input</b> <b>checks...</b>|$|R
50|$|Except {{the extreme}} case with , all the {{security}} vulnerabilities {{can be avoided}} by introducing auxiliary code to perform memory management, bounds <b>checking,</b> <b>input</b> <b>checking,</b> etc. This is often done {{in the form of}} wrappers that make standard library functions safer and easier to use. This dates back to as early as The Practice of Programming book by B. Kernighan and R. Pike where the authors commonly use wrappers that print error messages and quit the program if an error occurs.|$|R
5000|$|Does an {{automaton}} {{accept any}} <b>input</b> word? (Emptiness <b>checking)</b> ...|$|R
5000|$|Data {{comparison}} [...] - [...] cross <b>check</b> <b>inputs</b> from duplicated sensors ...|$|R
50|$|Along with Prof. DeFrancis {{overseeing}} the general planning and {{supervision of the}} project {{as well as its}} detailed operations, a volunteer team of some 50 contributors - including academics, Chinese language teachers, students, lexicographers, and computer consultants - were involved in the myriad tasks of processing dictionary entries, such as defining, <b>inputting,</b> <b>checking,</b> and proofreading. The University of Hawai'i Press published the ABC Chinese-English Dictionary in September 1996. UHP republished the original paperback ABC Chinese-English Dictionary, which had a total 916 pages and was 23 cm. high, into the ABC Chinese-English Dictionary: Pocket Edition (1999, 16 cm.) and hardback ABC Chinese-English Dictionary: Desk Reference Edition (2000, 23 cm.).|$|R
40|$|Developer testing, {{a common}} step in {{software}} development, involves generating desirable test <b>inputs</b> and <b>checking</b> {{the behavior of}} the program unit under test during the execution of the test inputs. Existing industrial developer testing tools include various techniques to address challenges of generating desirable test <b>inputs</b> and <b>checking</b> {{the behavior of the}} program unit under test. This paper presents an overview of techniques implemented in industrial developer testing tools to address challenges in improving automation in developer testing. These techniques are summarized from two main aspects: test efficiency (e. g., with a focus on cost) and test effectiveness (e. g., with a focus on benefit). ...|$|R
30|$|The IsPath {{function}} takes {{sequence of}} blocks and edges as an <b>input</b> and <b>checks</b> if the sequence forms a {{path in the}} edge relation. It is stated that any two consecutive elements in the path sequence constitute an edge in the graph relation.|$|R
50|$|In {{software}} testing, monkey {{testing is}} a technique where the user tests the application or system by providing random <b>inputs</b> and <b>checking</b> the behavior, or seeing whether the application or system will crash. Monkey testing is usually implemented as random, automated unit tests.|$|R
40|$|Abstract. Input {{validation}} is {{the problem}} that need to solve for all Web applications, to calibrate data for its constraints and rules. The Struts 2 framework provides a variety of check method, and validation. xml file {{is the most common}} method in this research. Firstly, study Struts 2 framework architecture composed of three parts such as model, view and controller; Then, show the <b>input</b> <b>checking</b> process of Struts 2 through a graphical mode;Finally, the design of validation. xml validation file example, realizes the checkers such as required string, string length, integer, date and double precision floating point number. Using Validation. xml authentication of Struts 2 has the advantages of convenient and suitable, and can also be combined with other methods in practical application...|$|R
5000|$|Built-in Functions [...] - [...] table search, phonetic conversion, field verify, field edit, bit <b>checking,</b> <b>input</b> formatting, {{weighted}} retrieval.|$|R
50|$|Genealogica Grafica is {{a utility}} for the {{creation}} of genealogical charts and reports for web publishing. Genealogica Grafica relies on genealogy software to produce the GEDCOM file it requires as an <b>input,</b> and <b>checks</b> the GEDCOM for consistency of the data and produces a variety of web reports.|$|R
40|$|Expert {{systems and}} {{artificial}} intelligence can contribute substantially {{to the quality}} of real-estate development analysis. 'A. I. coaches'help correlate <b>input</b> and <b>check</b> the validity of schedules. Expert systems enhance life cycle cost analysis. Property value indicators become viable when used in conjunction with remote sensing and life style mappin...|$|R
40|$|A Green's-function-based solver for the {{modified}} Bessel equation {{has been developed}} with the primary motivation of solving the Poisson and biharmonic equations in cylindrical geometries. The method is implemented using a discrete Hankel transform and a Green's function based on {{the modified}} Bessel functions {{of the first and}} second kind. The computation of these Bessel functions has been implemented to avoid scaling problems due to their exponential and singular behaviour, allowing the method to be used for large-order problems, as would arise in solving the Poisson equation with a dense azimuthal grid. The method has been tested on monotonically decaying and oscillatory <b>inputs,</b> <b>checking</b> for errors due to interpolation and/or aliasing. The error has been found to reach machine precision and to have computational time linearly proportional to the number of nodes...|$|R
40|$|This {{article is}} {{the survey of}} author's work on dialog shells over {{interpreting}} systems. Aspects of the shell for the computational algebra package Bergman are presented: the solved task, homogenization algorithm, <b>input</b> data <b>checking,</b> approaches to implementation. The shell automatizes and strongly simplifies data preparation and monitoring of the Bergman package...|$|R
40|$|A Green's {{function}} based solver for {{the modified}} Bessel equation {{has been developed}} with the primary motivation of solving the Poisson equation in cylindrical geometries. The method is implemented using a Discrete Hankel Transform and a Green's function based on the modified Bessel functions {{of the first and}} second kind. The computation of these Bessel functions has been implemented to avoid scaling problems due to their exponential and singular behavior, allowing the method to be used for large order problems, as would arise in solving the Poisson equation with a dense azimuthal grid. The method has been tested on monotonically decaying and oscillatory <b>inputs,</b> <b>checking</b> for errors due to interpolation and/or aliasing. The error has been found to reach machine precision and to have computational time linearly proportional to the number of nodes. Comment: Submitted to SIAM Journal on Scientific Computin...|$|R
40|$|Current {{software}} {{products such}} as spreadsheets are beginning to include automated graphical design and <b>input</b> <b>checking</b> heuristics to provide added automated functionality. The use of such rules requires higher levels of abstract meaning representation than raw media. They also rely upon representing contextual information describing the domain, task, user and current dialogue. With these technologies interfaces move from being purely multimedia towards multimodality. The MMI 2 multimodal system is described to illustrate the representation and architecture required for this class of system and the cooperative dialogue which it can support. A second, simpler multimedia information retrieval and presentation system (MIPS) is then described to show how these technologies of context and abstract meaning representation can be incorporated in commercial multimedia applications to structure and tailor multimedia information to the user. Keywords: Multimodal User Interface, Multimedia, Intellig [...] ...|$|R
40|$|This {{high school}} level {{computer}} activity requires students to use formulas to calculate heat index from data collected in Lamont, Oklahoma. The activity includes a calculator with air temperature and relative humidity <b>inputs</b> to <b>check</b> answers. It {{is part of the}} Atmospheric Visualization Collection (AVC), which focuses on data from the Atmospheric Radiation Measurement(ARM) program. Educational levels: High school...|$|R
50|$|One of {{the biggest}} {{problems}} found in the second phase was how to <b>input</b> the <b>check</b> information, especially the account numbers, with any sort of speed. Beise demanded a system that would not require the information to be changed from one medium to another, from check to punched card for instance, {{while at the same time}} lowering error rates.|$|R
50|$|A parser is a {{software}} component that takes input data (frequently text) and builds a data structure - often {{some kind of}} parse tree, abstract syntax tree or other hierarchical structure - giving a structural representation of the <b>input,</b> <b>checking</b> for correct syntax in the process. The parsing may be preceded or followed by other steps, or these may be combined into a single step. The parser is often preceded by a separate lexical analyser, which creates tokens from the sequence of input characters; alternatively, these can be combined in scannerless parsing. Parsers may be programmed by hand or may be automatically or semi-automatically generated by a parser generator. Parsing is complementary to templating, which produces formatted output. These may be applied to different domains, but often appear together, such as the scanf/printf pair, or the input (front end parsing) and output (back end code generation) stages of a compiler.|$|R
30|$|After {{the user}} clicks to save in the details-on-demand window, the <b>input</b> is <b>checked</b> for its {{conformity}} with the STIX specification. If {{the object is}} conform it is parsed into a compliant JSON. This happens regardless of whether a new object is added or an existing one is changed. Afterwards the JSON {{is sent to the}} CTI vault where the data is persisted.|$|R
5000|$|The Brake Control Unit (computer) detects the <b>inputs,</b> {{and then}} <b>checks</b> the wheel speed sensors to {{determine}} vehicle speed, {{and to determine}} if a wheel lockup requires the ABS algorithm.|$|R
5000|$|Data Validation {{tests the}} {{validity}} of input into fields by comparing the <b>input</b> to patterns, <b>checking</b> for the correct data type (such as a string or an integer), and in other customizable ways.|$|R
40|$|The {{purpose of}} this study was to {{investigate}} the relationship among the teachers' use of the individual elements of lesson design, students' mastery of mathematics objectives, the hours of inservice training completed by the teachers and the teachers' years of experience. The individual elements of lesson design are defined by Madeline Hunter and are anticipatory set, objective and purpose, <b>input,</b> modeling, <b>checking</b> for understanding, guided practice, and independent practice...|$|R
40|$|Data mappings: # 45 : Changed DataMappings remoteInput/Output from {{records to}} simple Maps Added {{an option to}} the config to switch on/off <b>input</b> objects {{existence}} <b>check</b> # 44 : Renamed DataMapping's id to label # 47 : Added a logger message {{and some kind of}} progress output for the <b>input</b> objects existence <b>check</b> # 50 : Instead of <b>checking</b> <b>input</b> objects object metadata, listing objects with the given prefix # 52 : Added a check for missing/non-needed input/output data keys in the datamappings Data processing: # 51 : Fixed input objects handling by passing the map of (dataKey. label -> fileLocation) from worker to the processing context; this way context. inputFile lookup doesn't rely on conventions and return the actual file location Self-termination: # 48 : Added a time deadline for SQS polling Added some logging and exception handling to the polling Other fixes: # 49 : Removed any worker instance state tagging for now; was causing RequestLimitExceeded on a big autoscaling group with quick tasks # 43 : Changed log messages for downloads/upload...|$|R
40|$|Packet {{filters are}} a {{mechanism}} for efficiently demultiplexing network packets to application endpoints. There is currently no general, formal specification method for packet filters that allows for easy or efficient composition of specifications. In this paper we present an automatic approach that achieves all of these goals. We approach packet filter specification as a language recognition problem: each filter is represented by a context-free grammar, whose language is the set of packets the filter should accept. Thus, packet filters can be formulated through a general, well defined specification; further, the grammar-based approach simplifies filter composition, which is essential where scalability is important. However packet filters based on standard LR parsing techniques suffer from poor performance: they touch every portion of the <b>input,</b> they <b>check</b> <b>input</b> bit by bit, they occupy large amount of space. We present new optimizations to LR parsing that enable our automatic approach to [...] ...|$|R
40|$|The aim of {{the present}} paper is to apply the Laguerre polynomials method for the {{analytical}} solution of the Altarelli- Martinelli equation. We use this method of the low $x$ gluon distribution to the longitudinal structure function using MRST partons as <b>input.</b> Having <b>checked</b> that this model gives a good description of the data to predict of the longitudinal structure function at leading and next to leading order analysis at low $x$Comment: 6 pages, 2 figure...|$|R
40|$|TPOT now detects {{whether there}} are missing values in your dataset and replaces them with the median value of the column. TPOT now allows you to set a group {{parameter}} in the fit function so {{you can use the}} GroupKFold cross-validation strategy. TPOT now allows you to set a subsample ratio of the training instance with the subsample parameter. For example, setting subsample= 0. 5 tells TPOT to create a fixed subsample of half of the training data for the pipeline optimization process. This parameter can be useful for speeding up the pipeline optimization process, but may give less accurate performance estimates from cross-validation. TPOT now has more built-in configurations, including TPOT MDR and TPOT light, for both classification and regression problems. TPOTClassifier and TPOTRegressor now expose three useful internal attributes, fitted_pipeline_, pareto_front_fitted_pipelines_, and evaluated_individuals_. These attributes are described in the API documentation. Oh, TPOT now has thorough API documentation. Check it out! Fixed a reproducibility issue where setting random_seed didn't necessarily result in the same results every time. This bug was present since TPOT v 0. 7. Refined <b>input</b> <b>checking</b> in TPOT. Removed Python 2 uncompliant code...|$|R
