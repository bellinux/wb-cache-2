196|3617|Public
5000|$|Although {{the pursuit}} of cost <b>estimate</b> <b>accuracy</b> should always be encouraged, a study in 2002 found that the {{estimates}} used to determine whether important infrastructure should be built were [...] "highly and systematically misleading." ...|$|E
5000|$|Since a cost {{estimate}} is the approximation {{of the cost}} of a project or operation, then <b>estimate</b> <b>accuracy</b> is a measure of how closely the estimate is able to predict the actual expenditures for the project or operation. This can only be known after the project is completed. If, for example, a project estimate was $1,252,000 for a specific scope and conditions, and at completion the records showed that $1,172,451.26 was expended, the estimate was 6.8% too high. If the project ended up having a different scope or conditions, an unadjusted computation does not fairly assess the <b>estimate</b> <b>accuracy.</b> Predictions of the <b>estimate</b> <b>accuracy</b> may accompany the estimate. Typically this is expressed as a range higher or lower as compared with the point estimate with an expected probability that the actual cost will fall in the range. [...] An example for a definitive estimate might be that the estimate has a -5/+10% range of accuracy with a 90% confidence that the final value will fall in that range. The accuracy of an early estimate relates to the estimate quality. Factors affecting the quality of the estimate include the people who prepared the estimate, how the estimate was prepared, and what was known about the project. For the same project, the range of uncertainty about the total estimate decreases over time, as illustrated in the cone of uncertainty diagram.|$|E
50|$|Sandler O’Neill publishes equity {{research}} on about 300 financial services companies for institutional investors. It hosts three conferences annually to help clients {{learn more about}} the companies it covers. The firm’s analysts have earned industry honors for earnings <b>estimate</b> <b>accuracy</b> and for stock selection. Richard Repetto, a Principal in the research group, won Analyst of the Year from the Financial Times in 2010.|$|E
5000|$|Improves <b>estimating</b> <b>accuracy,</b> {{especially}} for larger software projects; ...|$|R
2500|$|... the a posteriori error {{covariance}} matrix (a {{measure of the}} <b>estimated</b> <b>accuracy</b> of the state estimate).|$|R
30|$|There {{are still}} {{shortcomings}} in our method and {{further research is}} still needed to improve the <b>estimating</b> <b>accuracy.</b>|$|R
5000|$|Geographic {{information}} {{systems such as}} GIS, GPS, and GNSS, have become so widespread that the term [...] "ground truth" [...] has taken on special meaning in that context. If the location coordinates returned by a location method such as GPS are an estimate of a location, then the [...] "ground truth" [...] is the actual location on earth. A smart phone might return a set of estimated location coordinates such as 43.87870,-103.45901. The ground truth being estimated by those coordinates is the tip of George Washington's nose on Mt. Rushmore. The accuracy of the estimate is the maximum distance between the location coordinates and the ground truth. We could say {{in this case that}} the <b>estimate</b> <b>accuracy</b> is 10 meters, meaning that the point on earth represented by the location coordinates is thought to be within 10 meters of George's nose—the ground truth. In slang, the coordinates indicate where we think George Washington's nose is located, and the ground truth is where it's really at. In practice a smart phone or hand-held GPS unit is routinely able to estimate the ground truth within 6-10 meters. Specialized instruments can reduce GPS measurement error to under a centimeter.|$|E
5000|$|Multiple loci VNTR {{analysis}} (MLVA) is {{a method}} employed for the genetic analysis of particular microorganisms, such as pathogenic bacteria, that {{takes advantage of the}} polymorphism of tandemly repeated DNA sequences. A [...] "VNTR" [...] is a [...] "variable-number tandem repeat". This method is well known in forensic science since it is the basis of DNA fingerprinting in humans. When applied to bacteria, it contributes to forensic microbiology through which the source of a particular strain might eventually be traced back, making it a useful technique for outbreak surveillance. In a typical MLVA, a number of well-selected and characterised (in terms of mutation rate and diversity) loci are amplified by polymerase chain reaction (PCR), so that the size of each locus can be measured, usually by electrophoresis of the amplification products together with reference DNA fragments (a so-called DNA size marker). Different electrophoresis equipment can be used depending on the required size <b>estimate</b> <b>accuracy,</b> and the local laboratory set-up, from basic agarose gel electrophoresis up to the more sophisticated and high-throughput capillary electrophoresis devices. From this size estimate, the number of repeat units at each locus can be deduced. The resulting information is a code which can be easily compared to reference databases once the assay has been harmonised and standardised. MLVA has become a major first line typing tool in a number of pathogens where such an harmonisation could be achieved, including Mycobacterium tuberculosis, Bacillus anthracis, Brucella.|$|E
40|$|In the theoretic {{part of my}} thesis, {{a method}} of {{calculation}} <b>estimate</b> <b>accuracy</b> is theoretically presented {{in a way that}} enables easy programming of the processing. All the necessary equations to calculate the gradual <b>estimate</b> <b>accuracy</b> for measuring horizontal angles, vertical angles and lengths, are presented here. In the practical part of my work, the same method is presented in a practical example. We performed measurements with two instruments, namely Leica Wild TC 1600 and Leica TS 30. We measured horizontal and vertical angles, lengths and meteorological parameters and observed 4 directions in 3 or 5 sets of angles. For horizontal angles we carried out <b>estimate</b> <b>accuracy</b> from deviations from the arithmetic mean and the <b>estimate</b> <b>accuracy</b> by taking into account the direction of the initial error. For vertical angles and lengths, we calculated the <b>estimate</b> <b>accuracy</b> from the deviation of the arithmetic mean. ...|$|E
30|$|The new FCI {{approach}} also outperformed the two existing {{methods in}} improving the <b>estimating</b> <b>accuracy,</b> {{especially with regard to}} MAE and MAPE.|$|R
50|$|Logging time, defect, {{and size}} data is an {{essential}} part of planning and tracking PSP projects, as historical data is used to improve <b>estimating</b> <b>accuracy.</b>|$|R
5000|$|<b>Estimated</b> <b>accuracy</b> of the {{predicted}} models (including a confidence score of all models, predicted TM-score and RMSD {{for the first}} model, and per-residue error of all models) ...|$|R
40|$|We {{introduce}} {{the problem of}} fast and fair localization of mobile units in indoor infrastructure wireless sensor networks. We define metrics and derive expressions for delay and fairness of localization and investigate a heuristic algorithm for fast and fair localization. Simulation results show that localization is faster for lower levels of location <b>estimate</b> <b>accuracy,</b> irrespective of anchor density, {{and that it is}} fairer for higher anchor densities, irrespective of location <b>estimate</b> <b>accuracy.</b> Also, localization is faster and fairer for grid deployment of anchors as compared to random deployment. The results also suggest that a guarantee on the desired level of location <b>estimate</b> <b>accuracy</b> can be provided for the entire localization area for specific speeds of movement of the mobile unit, and that these speeds are higher for denser anchor deployments. I...|$|E
3000|$|... in the {{estimation}} problem resulting from noise. Thus, {{it is necessary}} to compromise between fast adaptive capability and the loss of <b>estimate</b> <b>accuracy.</b>|$|E
30|$|By {{exploring}} chosen receiver algorithm, including {{parameter estimation}} for synchronization and channel estimation, {{the effect of}} more complex signal handling are more huge/significant and the channel <b>estimate</b> <b>accuracy</b> is improved.|$|E
40|$|What are {{the common}} methods for <b>estimating</b> <b>accuracy</b> of {{classifiers}} induced by supervised learning algorithms? Based on evidence from literature and your hands-on experience (if any), {{comment on the}} strengths and weaknesses of each approach and its scope of application. Answer <b>Estimating</b> <b>accuracy</b> of learned classifiers is an important problem in machine learning for at least three reasons. First, how accurate a classifier is to a large extent determines how useful it is. An <b>estimate</b> of the <b>accuracy</b> gives an indication of whether to use the classifier for a particular problem and how much confidence to have in its classifications. For example, an accurate classifier can aid doctors in medical diagnosis, while a inaccurate one can be dangerous. Second, <b>accuracy</b> <b>estimates</b> of a learned classifier is part of the learning process of most learning algorithms. Third, a comparing the accuracy of classifiers produced by learning algorithms is important to researchers for understanding and developing learning algorithms. There are several common methods for <b>estimating</b> <b>accuracy.</b> All such methods are necessarily heuristic, in the sense that they require assumptions about the classifier or the classification problem, and can fail if the assumptions are wrong. [Duda et al 2001, p 482]...|$|R
30|$|It can be {{seen from}} Fig.  7 that the {{improved}} algorithm and the PTF MUSIC algorithm have similar <b>estimated</b> <b>accuracy,</b> which are higher than that of the PTF ESPRIT algorithm.|$|R
40|$|We propose an {{efficient}} method to <b>estimate</b> the <b>accuracy</b> of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, {{a set of}} classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method {{is based on the}} intuition that: (i) when classifiers agree, {{they are more likely to}} be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce <b>accuracy</b> <b>estimates</b> within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both <b>estimating</b> <b>accuracies,</b> and combining multiple classifier outputs. The results emphasize the utility of logical constraints in <b>estimating</b> <b>accuracy,</b> thus validating our intuition...|$|R
40|$|An {{application}} of Robust Statistics in a Hough Transform based motion estimation approach is presented. The algorithm is developed and experiments are performed, proving its superior performance {{in terms of}} <b>estimate</b> <b>accuracy,</b> convergence, robustness and better segmentation. Comparative results with standard methods are also included. ...|$|E
30|$|Second, {{we propose}} a novel quality {{monitoring}} model, which has low complexity and fully utilizes the video spatiotemporal complexity estimation at packet layer. In {{contrast to the}} parametric packet-layer models, it takes into consideration the interaction among video content features and EC effect and error propagation effect, thus improves <b>estimate</b> <b>accuracy.</b>|$|E
40|$|A {{methodology}} {{to estimate}} in-bank river discharge exclusively from remotely sensed hydraulic data is developed. Water-surface width and maximum channel width measured from 26 aerial and digital orthophotos of 17 single channel rivers and 41 SAR images of three braided rivers were coupled with channel slope {{data obtained from}} topographic maps to estimate the discharge. The standard error of the discharge estimates were within a factor of 1. 5 – 2 (50 – 100 %) of the observed, with the mean <b>estimate</b> <b>accuracy</b> within 10 %. This level of accuracy was achieved using calibration functions developed from observed discharge. The calibration functions use reach specific geomorphic variables, the maximum channel width and the channel slope, to predict a correction factor. The calibration functions are related to channel type. Surface velocity and width information, obtained from a single C-band image obtained by the Jet Propulsion Laboratory’s (JPL’s) AirSAR was also used to estimate discharge for a reach of the Missouri River. Without using a calibration function, the <b>estimate</b> <b>accuracy</b> was C 72 % of the observed discharge, which is within the expected range of uncertainty for the method. However, using the observed velocity to calibrate the initial estimate improved the <b>estimate</b> <b>accuracy</b> to within C 10 % of the observed. Remotely sensed discharge estimates with accuracies reported in this paper could be useful for regional or continental scale hydrologic studies, or in regions where ground-based data is lacking...|$|E
40|$|The {{problem of}} <b>estimating</b> of <b>accuracy</b> for {{measuring}} of geometrical parameters using television informative-measuring systems. Three approaches for <b>estimating</b> <b>accuracy</b> of measurement were considered: the geometric, spatial frequency and probabilistic. It was {{shown that the}} space-frequency approach {{as a result of}} incorporation of the real values of the input contrast and the terms of the signal can get the most accurate assessment of error. ? ?????? ???????????????? ???????? ?????? ??????????? ????????? ?????????????? ?????????? ??? ?????? ????????????? ?????????????-????????????? ??????. ??????????? ??? ??????? ? ?????? ??????????? ?????????: ??????????????, ???????????????-????????? ? ?????????????. ????????, ??? ???????????????-????????? ?????? ?????????? ????? ???????? ???????? ???????? ????????? ? ??????? ???????????? ??????? ????????? ???????? ???????? ??????????? ?????? ??????????? ?????????...|$|R
30|$|With {{the rapid}} {{development}} of artificial intelligence technology, {{researchers found that}} the neural network filter was suitable for issues of nonlinear. Its main features are smart and well-adapted. And the drawback of this method is its poor universality, and the <b>estimated</b> <b>accuracy</b> is unsatisfactory.|$|R
40|$|One of {{the major}} task of {{wireless}} sensor network is to sense accurate data from the physical environment. Hence in this paper, we develop an <b>estimated</b> data <b>accuracy</b> model for randomly deployed sensor nodes which can sense more accurate data from the physical environment. We compare our results with other information accuracy models and shows that our <b>estimated</b> data <b>accuracy</b> model performs {{better than the other}} models. Moreover we simulate our <b>estimated</b> data <b>accuracy</b> model under such situation when some of the sensor nodes become malicious due to extreme physical environment. Finally using our <b>estimated</b> data <b>accuracy</b> model we construct a probabilistic approach for selecting an optimal set of sensor nodes from the randomly deployed maximal set of sensor nodes in the network. Comment: 6 page...|$|R
3000|$|... [...]. In {{order to}} improve the {{tracking}} <b>estimate</b> <b>accuracy,</b> it will exchange this filtered estimate with its neighbors over noisy communication links and try to reach consensus over the network. Note that, the goal here is to obtain a consensus tracking estimate over the local estimates at each tracking time step k, and thus, the consensus problem is essentially a problem of consensus in estimation.|$|E
40|$|AbstractWith {{reference}} to a (discrete time) linear system filtering problem, we consider the problem of online deciding at which instant we actually measure and process the output values. We propose a convenient measurement policy, able to sensibly reduce the measurement cost, while keeping the <b>estimate</b> <b>accuracy</b> at satisfactory levels. By some probabilistic arguments we analyse the upper bounds for the measurement and non-measurement time intervals...|$|E
30|$|In this article, {{we propose}} a new hybrid {{algorithm}} for 2 D DOA estimation {{in the presence}} of mutual coupling for sparse UCAs. Based on the manifold decomposition technique, we will present two new formulations of the steering vector {{in the presence of}} mutual coupling for sparse UCAs. One formulation, corresponding to Jacobi-Anger expansion [12], allows applying a modified UCA-RARE algorithm to estimate the azimuth angle without the exact knowledge of mutual coupling and elevation angle. The other formulation, corresponding to Bauer's formula [13], allows executing a Root-MUSIC algorithm in the elevation direction to estimate the elevation angle for each estimated azimuth angle. For sparse UCAs, compared with the original UCA-RARE, the modified UCA-RARE is able to avoid obtaining spurious estimates which only arise from the sparseness of the array elements. Note that the steering vector expansion for estimating the elevation angle in this article differs from that in [5] and has a more universal application [10, 14, 15]. In fact, these two kinds of decomposition techniques applied in this article can be considered as manifold decomposition transformations [11, 16 – 18]. It is shown that the DOA <b>estimate</b> <b>accuracy</b> usually depends on the truncation error introduced by the transformation [16 – 19]. Hence, we analyze the truncation errors for sparse UCAs and derive expressions describing the truncation errors in the DOA estimates. We find that the impact of the truncation error on the <b>estimate</b> <b>accuracy</b> of azimuth angle is weaker than it for the elevation angle estimate. Therefore, a method to choose an appropriate truncated degree for the elevation estimates is presented to enhance the <b>estimate</b> <b>accuracy.</b>|$|E
40|$|This paper {{publishes the}} results of a study of surgery time estimations in Swedish healthcare. The study {{examines}} the <b>estimating</b> <b>accuracy</b> of the duration of future surgeries between a surgeon-based estimating system and a computer-based system that examine data from previously made surgeries. The relative <b>estimating</b> <b>accuracy</b> of the two systems is evaluated by using two hypothesis tests. The paper also investigates whether the surgeons’ option to include additional factors in their estimations result in more accurate estimates for procedures requiring longer-than-‘normal’ surgery time. The results show that the surgeon-based estimation system is less accurate in general than the computer-based system. However, in cases where median surgery time is exceeded, the surgeon-based system is more accurate than a calculated average value of the previous surgeries...|$|R
30|$|The {{adaptive}} filter can estimate and correct the model parameters {{and the noise}} characteristics {{at the same time}} to continually reduce the errors of the state estimation. Then the <b>estimated</b> <b>accuracy</b> can be improved availably (Hu et al. 2003; Chang & Liu 2015; Mundla et al. 2012).|$|R
30|$|In {{addition}} to observations from clustering analysis, {{we attempted to}} apply a classifier (decision tree-based) to discover principles or rules for selecting spatiotemporal FSE methods. The <b>estimated</b> <b>accuracy</b> of classification was extremely low, which lead us to the conclusion regarding the absence of straightforward principles available from the literature.|$|R
40|$|A {{criterion}} for {{estimation of the}} quality of a truncated orthogonal subplan of the complete factor experiment plan is introduced, which characterizes the level of mixing effects of the factors and their interactions. A method for calculation the degree of representativeness for truncated orthogonal plans is proposed. Taking into account of the criterion makes it possible to improve the <b>estimate</b> <b>accuracy</b> for multifactor regression under the conditions of small selection. ?????? ???????? ?????? ???????? ?????????? ?????????????? ???????? ????? ??????? ?????????? ????????????, ??????????????? ??????? ?????????? ??????? ???????? ? ?? ??????????????. ?????????? ???????? ??????? ??????? ?????????????????? ????????? ????????????? ??????. ???? ?????????? ???????? ????????? ???????? ???????? ?????????? ?????????? ???????????????? ????????? ? ???????? ????? ???????...|$|E
3000|$|... is {{the error}} {{obtained}} on bootstrap sets themselves, both averaged over all data samples and bootstrap samples. <b>Estimate</b> <b>accuracy</b> is {{directly proportional to}} number of times the process is repeated. More details on bootstrap validation technique {{can be found in}} [177]. Bootstrapping increases the variance that can occur in each fold which makes this strategy more realistic of the real application situation [177]. This validation strategy is rarely used in age estimation.|$|E
40|$|The project {{development}} phases in the Australian coal mining and processing industry {{are similar to}} those used in front end loading for oil and gas development in stages and maturity; however there is no common naming convention for the {{project development}} phases in the Australian coal sector. This is causing pricing and <b>estimate</b> <b>accuracy</b> confusion between government agencies, mine developers, engineering, procurement and construction management (EPCM) providers as well as vendors and industry bodies...|$|E
40|$|Experimentally {{obtained}} ZIP load {{models are}} used for the power system analysis. <b>Estimating</b> <b>accuracy</b> of such models is presented in this paper. The dependence of ZIP load model error on the range in which voltage changes during the experiment, the number of measured values of voltage and power, standard deviation of the load change random process have been obtained using the computing experiment. The results {{could be used to}} <b>estimate</b> the <b>accuracy</b> of ZIP load models or to design the experiment to determine ZIP load models with prescribed accuracy...|$|R
40|$|Detection of intra-abdominal abscesses by {{computed}} tomography (CT) has an <b>estimated</b> <b>accuracy</b> rate of greater than 90 percent. Features helpful in detecting these abscesses include extraluminal gas, the “rind” sign, airfluid interface, and {{low and high}} density areas. The author presents three cases to illustrate the usefulness of this modality...|$|R
40|$|Variational {{approach}} to statistical {{evaluation of a}} maximum likelihood state in a nonlinear dynamic system is proposed. Mathematical justification of the approach and comparison with the direct methods showed its advantages concerning the obtained <b>estimates,</b> <b>accuracy,</b> and computational efficiency. Numerical examples of autonomous orbit determination according to navigational data are considered...|$|R
