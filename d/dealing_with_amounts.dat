1|10000|Public
50|$|Historically, the surrogatum {{principle}} {{has been}} applied by the courts only in the determination of profit from a business or property under general principles. However, {{in the case of}} Tsiaprailis v. The Queen, 2005 DTC 5119, the Supreme Court of Canada applied the principle in its consideration of a more specific statutory provision <b>dealing</b> <b>with</b> <b>amounts</b> received pursuant to a disability insurance plan, namely paragraph 6(1)(f). The case dealt with a lump-sum settlement payment received in respect of a disputed claim under a disability insurance plan. The payment ostensibly represented both past disability benefits accruing to the time of the settlement and the taxpayer's foregone future benefits under the plan. The Court held that the portion of the lump-sum payment reflecting the taxpayer's future benefits was not made pursuant to the insurance plan because there was no obligation to make such a lump-sum payment {{under the terms of the}} plan. Therefore, such amount was not taxable under paragraph 6(1)(f). However, turning to the portion of the payment that represented the past benefits under the plan, the Court applied the surrogatum principle in concluding that the portion was taxable under paragraph 6(1)(f) because it was meant to replace amounts that were payable pursuant to the plan.|$|E
5000|$|National Consumer Disputes Redressal Commission (NCDRC): A {{national}} level court {{works for the}} whole country and <b>deals</b> <b>with</b> <b>amount</b> more than [...] The National Commission is the Apex body of Consumer Courts, {{it is also the}} highest Appellate Court in the hierarchy.|$|R
5000|$|... #Caption: In {{strategy}} games like Freeciv, the game AI must <b>deal</b> <b>with</b> large <b>amounts</b> of information ...|$|R
30|$|In this paper, we utilize MapReduce {{programming}} {{model to}} <b>deal</b> <b>with</b> large <b>amounts</b> of non-serialization distributed vessel data, which overcomes multiple defects of existing methods.|$|R
50|$|Internet vendors {{benefit from}} a {{simplified}} sales model as compared to traditional brick-and-mortar stores. By storing goods remotely at a warehouse location and shipping goods directly to a consumer, significant transportation needs are eliminated both {{on the part of}} the vendor (shipping goods to stores) any by the consumer (traveling to stores). Additionally, near universal access to the Internet means that a relatively few warehouse locations can compete with a market without having to <b>deal</b> <b>with</b> <b>amounts</b> of real-estate.|$|R
5000|$|The Lehman formula was {{originally}} used by investment banks and individual or corporate [...] "finders" [...] for {{the raising of}} capital for a business, either in public offerings or private placements, payable by the vendor(s) of the business once the funds have cleared. It usually <b>deals</b> <b>with</b> <b>amounts</b> greater than one million dollars. Below this mark, brokerage services and investment banks usually offer a set of tiered fees, or set-rate trading prices (such as $9.95 per trade).|$|R
50|$|In addition, {{many of the}} {{techniques}} developed by phenetic taxonomists have been adopted and extended by community ecologists, due to a similar need to <b>deal</b> <b>with</b> large <b>amounts</b> of data.|$|R
40|$|Abstract. Visualisation {{provides}} good {{support for}} software analysis. It copes with the intangible nature of software by providing concrete repre-sentations of it. By reducing {{the complexity of}} software, visualisations are especially useful when <b>dealing</b> <b>with</b> large <b>amounts</b> of code. One domain that usually <b>deals</b> <b>with</b> large <b>amounts</b> of source code data is empirical analysis. Although there are many tools for analysis and visualisation, they do not cope well software corpora. In this paper we present Explora, an infrastructure that is specifically targeted at visualising corpora. We report on early results when conducting a sample analysis on Smalltalk and Java corpora. ...|$|R
6000|$|... "Than any hushed-up scandal could? Quite so. Our {{reputation}} {{among the}} farmers is most unsavory. But I would much sooner <b>deal</b> <b>with</b> any <b>amount</b> of ingenious crime {{of that nature}} than--some other offenses." ...|$|R
40|$|Information Database Abstract: The new {{generation}} leveler control system need to <b>deal</b> <b>with</b> <b>amounts</b> of data. Directed at the characteristics, this article demonstrates the structure design of its database architecture. From the communication interface, {{hardware and software}} environment, functions, technologies, etc., we expounded the implementation of database system, such as real-time database, alarm database, model material databases, historical database, and safety information database and so on. It shows that the database system is a core of {{new generation}} leveler control system. We also present specific examples of design and give the detailed method of approach...|$|R
50|$|This {{alternative}} is the escalation operative, as {{the decision is}} made due to the perception {{that there are no}} other available options. The {{alternative is}} an outcome of the issues <b>dealing</b> <b>with</b> the crisis and <b>dealing</b> <b>with</b> the <b>amount</b> of time the cause of stress has been around.|$|R
50|$|Government organisations, private {{companies}} and many web-based applications have to <b>deal</b> <b>with</b> huge <b>amounts</b> of data. The data will {{often have to}} be accessed multiple times. Keeping the data in a sorted format allows for quick and easy recovery of data.|$|R
50|$|Introduced by Edward Hall in 1966, Proxemics <b>deals</b> <b>with</b> the <b>amount</b> of {{distance}} between {{people as they}} interact with one another. Spatial distance during an interaction can be an indication of what type of relationship exists between the people involved.|$|R
40|$|AbstractIn {{industrial}} {{human-robot collaboration}} (HRC) the question, how can such systems be designed safely is paramount. In general, {{it is difficult}} to assess those systems with all their capabilities prior to commissioning. In this paper we propose specialized simulation tools as one potential solution to this issue. We use real-world geometrical data to investigate different algorithms and safety strategies. One strategy is the use of a genetic algorithm for collision avoidance to <b>deal</b> <b>with</b> <b>amounts</b> of data in short computing times. This is a solution to find a safe distance with adaptive speed in HRC assembly applications...|$|R
5000|$|The {{illicit drug}} trade in Australia still often uses {{imperial}} measurements, particularly when <b>dealing</b> <b>with</b> smaller <b>amounts</b> closer to end user levels e.g. [...] "8-ball" [...] an 8th of an ounce or 3.5 g; cannabis is often traded in ounces ("oz") and pounds ("p") ...|$|R
50|$|Product finder has an {{important}} role in e-commerce, items has to be categorized to better serve consumer in searching the desired product, recommender system for recommending items based on their purchases etc.As people are moving from offline to online commerce (e-commerce), it is getting more difficult and cumbersome to <b>deal</b> <b>with</b> the large <b>amount</b> of data about items, people that need to be kept and analyzed in order to better serve consumer. Large amount of data cannot be handled by just using man power, we need machine to do these things for us, they can <b>deal</b> <b>with</b> large <b>amount</b> of data efficiently and effectively.|$|R
30|$|In particular, {{given the}} need to <b>deal</b> <b>with</b> the large <b>amounts</b> of data (“big” data), {{decision}} analytics methodologies {{play an important role}} in this context.|$|R
40|$|Computer users <b>deal</b> <b>with</b> large <b>amounts</b> of {{personal}} media often face problems in managing and exploring it. This paper presents Semantic Regions, rectangular regions that enable users to specify their semantics or mental models, and the MediaFinder application, which uses Semantic Regions {{as the basis}} of a personal media management tool...|$|R
60|$|The London traffic {{problem is}} just one of those {{questions}} that appeal very strongly to the more prevalent and less charitable types of English mind. It has a practical and constructive air, it <b>deals</b> <b>with</b> impressively enormous <b>amounts</b> of tangible property, it rests with a comforting effect of solidity upon assumptions that are at once doubtful and desirable. It seems free from metaphysical considerations, and it has none of those disconcerting personal applications, those penetrations towards intimate qualities, that makes eugenics, for example, faintly but persistently uncomfortable. It is indeed an ideal problem for a healthy, hopeful, and progressive middle-aged public man. And, as I say, it <b>deals</b> <b>with</b> enormous <b>amounts</b> of tangible property.|$|R
50|$|Many {{homeowners who}} <b>deal</b> <b>with</b> large <b>amounts</b> of snow have {{multiple}} snow shovels for {{different types of}} snow. If lifting is a concern, then they may choose separate shovels for lifting versus pushing. Otherwise, users may wish to have a shovel for fresh light snow and another one to manage icy hard snow.|$|R
40|$|Proteomics {{inherently}} <b>deals</b> <b>with</b> huge <b>amounts</b> of data. Current mass spectrometers acquire {{hundreds of}} thousands of spectra within a single project. Thus, data management and data analysis are a challenge. We have developed a software platform (Proteinscape) that stores all relevant proteomics data efficiently and allows fast access and correlation analysis within proteomics projects...|$|R
40|$|Sigma-point Kalman filter (S-PKF) {{has shown}} {{promising}} performances when parameter identification and state tracking are simultaneously pursued in damaging structures. Unlike the extended Kalman Filter, the S-PKF continuously improves the outcomes (i. e. {{the estimates of}} state and model parameters) by averaging the responses {{of a set of}} independent sigma-points, which evolve in time according to the actual system dynamics. Being N the dimension of the state vector, sigma-points to <b>deal</b> <b>with</b> <b>amount</b> to 2 N+ 1. Even though the S-PKF technique can become computationally demanding, its formulation can be exploited in a parallel implementation. Focusing on a parallelization scheme within a shared-memory (OPEN-MP) architecture, in this work scalability issues are discussed concerning real-time monitoring of damaging, large-scale composite structures...|$|R
40|$|We use an {{iterative}} process of multi-gram alignment between associated words {{in different languages}} {{in an attempt to}} identify cognates. To maximise the amount of data, we use practical orthographies instead of consistently coded phonetic transcriptions. First results indicate that using practical orthographies can be useful, the more so when <b>dealing</b> <b>with</b> large <b>amounts</b> of data. ...|$|R
40|$|Many {{applications}} {{in science and}} business such as signal analysis or costumer segmentation <b>deal</b> <b>with</b> large <b>amounts</b> of data which are usually high dimensional in the feature space. As a part of preprocessing and exploratory data analysis, visualization of the data helps to decide which kind of method probably leads to good results. Since the visual assessment of...|$|R
40|$|The {{recently}} emerging {{fields in}} biology <b>dealing</b> <b>with</b> large <b>amounts</b> of data {{at the molecular}} level, often collectively known as the “omics”, throw up many challenges and opportunities for statisticians. The main fields are genomics, the study of genes at the genome-wide level, i. e. involving {{all or most of}} the organism’s inherited genetic material, transcriptomics, th...|$|R
50|$|Techniques {{to gather}} {{knowledge}} from an overabundance of electronic information (e.g., data fusion may help in data mining) have existed since the 1970s.Another common technique to <b>deal</b> <b>with</b> such <b>amount</b> {{of information is}} qualitative research. Such approach aims at organizing the information, synthesizing, categorizing and systematizing {{in order to be}} more usable and easier to search.|$|R
50|$|<b>Dealing</b> <b>with</b> the <b>amount</b> {{of traffic}} and {{information}} and then applying services requires very high speed look ups {{to be able}} to be effective. Need to compare against full services platforms or else having all traffic is not being utilized effectively. An example is often found in <b>dealing</b> <b>with</b> Viruses and Malicious content where solutions only compare content against a small virus database instead of a full and complete one.|$|R
40|$|This paper <b>deals</b> <b>with</b> {{neural network}} {{protection}} algorithm aimed at classifying transmission line faults. The approach utilizes self-organized adaptive type of neural network specially developed to <b>deal</b> <b>with</b> large <b>amount</b> of data. Various procedures of preprocessing neural network inputs are extensively investigated. The classification performance for different values of data window length and sampling frequency is demonstrated and illustrated. Simulation results show satisfactory algorithm responses {{in each of}} the implemented cases...|$|R
40|$|Large-scale web {{and text}} {{retrieval}} systems <b>deal</b> <b>with</b> <b>amounts</b> {{of data that}} greatly exceed the capacity of any single machine. To handle the necessary data volumes and query throughput rates, parallel systems are used, in which the document and index data are split across tightly-clustered distributed computing systems. The index data can be distributed either by document or by term. In this paper we examine methods for load balancing in term-distributed parallel architectures, and propose a suite of techniques for reducing net querying costs. In combination, the techniques we describe allow a 30 % improvement in query throughput when tested on an eight-node parallel computer system. Categories and Subject Descriptors H. 3. 1 [Information Storage and Retrieval]: Content analysis and indexing – indexing methods; H. 3. 2 [Information Storage and Retrieval]...|$|R
3000|$|... [...]) {{defines the}} share of an AP in each {{transmission}} round {{with respect to the}} workload of the APs in one hop neighborhood. Weighted workload determines {{the share of}} each AP in a transmission round, implementing fair media access. All the APs <b>deal</b> <b>with</b> huge <b>amount</b> of traffic, and one AP is only aware of its one hop neighborhood, so the [...]...|$|R
50|$|The work {{involved}} <b>dealing</b> <b>with</b> large <b>amounts</b> {{of plant}} growth, particularly water pennywort, Hydrocotyle ranunculoides, {{a member of}} the apiaceae family, which were treated with herbicides to remove them. Water pennywort is a non-native invasive species, which quickly forms a dense mat of vegetation on the surface of slow-moving water, preventing the growth of other species and the movement of animals and boats.|$|R
5000|$|The {{decimal point}} is written as [...] "," [...] (comma) and written and {{pronounced}} komma. The digits following the decimal point may be read individually or {{as a pair}} if there are only two. When <b>dealing</b> <b>with</b> monetary <b>amounts</b> (usually <b>with</b> two decimals), the decimal point is read as och, i.e. [...] "and": 3,50 (tre och femtio), 7,88 (sju och åttioåtta).|$|R
40|$|There {{still seems}} to be a misapprehension that neural {{networks}} are capable of <b>dealing</b> <b>with</b> large <b>amounts</b> of noise and useless data. This is true to a certain extent but {{it is also true that}} the cleaner and more descriptive the data is the better the neural networks will perform, especially when <b>dealing</b> <b>with</b> small data sets. A method for determining how useful input features are in giving correct classifcations using neural networks is discussed here...|$|R
40|$|The {{bachelor}} thesis <b>deals</b> <b>with</b> {{problem of}} effectiveness of resources {{spent by the}} various presidential candidates {{in the presidential campaign}} 2012 / 2013. The re-cherché is focused on the presidential elections under the current legislation and the methods of conducting political campaigns. The practical part <b>deals</b> <b>with</b> the <b>amount</b> of media time obtained in the public and private televisions by individual presidential candidates and media time obtained compared with the incurred costs of the campaign of the candidates...|$|R
30|$|The {{use of a}} {{pattern-matching}} (rule based) technique {{instead of}} machine learning stands on Open IE nature that is essentially linked to the Web and <b>deals</b> <b>with</b> large <b>amounts</b> of information, so that computational performance (in terms of speed) is mandatory. On the other hand, machine learning applications, especially the supervised ones, {{require the use of}} training data which are usually unavailable or time consuming and costly to obtain.|$|R
40|$|An {{algorithm}} for approximate skinning through cross-sectionalNURBS curves is presented. The method {{eliminates the}} problem of <b>dealing</b> <b>with</b> huge <b>amounts</b> of control points obtained during the curvecompatability process. It also allows the designer to specify large numbers of cross-sections and approximately fit a smooth surface to these curves to any given tolerance. Depending on the tolerances used, up to 99 % of the control points can be eliminated...|$|R
