15|42|Public
50|$|The file <b>header</b> <b>record</b> {{contains}} metadata about a file. To ensure portability {{across different}} architectures with different byte orderings, {{the information in}} the <b>header</b> <b>record</b> is encoded in ASCII. Thus if all the files in an archive are ASCII text files, and have ASCII names, then the archive is essentially an ASCII text file (containing many NUL characters).|$|E
5000|$|An {{optional}} <b>header</b> <b>record</b> (there is no {{sure way}} to detect whether it is present, so care is required when importing).|$|E
50|$|Since {{this initial}} version, {{there has been}} one major update to the RDOFF format, which added a record-length {{indicator}} on each <b>header</b> <b>record,</b> allowing programs to skip over records whose format they do not recognise, and support for multiple segments; RDOFF1 only supported three segments: text, data and bss (containing uninitialized data).|$|E
50|$|A PDB file {{contains}} a PDB <b>header,</b> PDB <b>record</b> <b>headers</b> and <b>records.</b>|$|R
50|$|This image {{shows the}} byte stream {{structure}} of a SEG-Y file, with rev 1 Extended Textual File <b>Header</b> <b>records.</b>|$|R
5000|$|CSV {{and similar}} files often do this using a <b>header</b> <b>records</b> with field names, and with commas {{to mark the}} field boundaries. Like MIME, CSV has no {{provision}} for structures {{with more than one}} level.|$|R
5000|$|Although some Unix {{documentation}} states [...] "the {{order of}} S-records within a file {{is of no}} significance and no particular order may be assumed", in practice most software has ordered the SREC records. The typical record order starts with a (sometimes optional) S0 <b>header</b> <b>record,</b> continues with a sequence {{of one or more}} S1/S2/S3 data records, may have one optional S5/S6 count record, and ends with one appropriate S7/S8/S9 termination record.|$|E
50|$|Commonly called 'CKiSS', {{this is an}} {{extension}} to the binary data <b>header</b> <b>record,</b> and unlike other extensions makes no changes to the configuration file. It is a specification allowing a cell file to contain raw 24-bit colour data and an 8 bit alpha channel for variable transparency. CKiSS cells tend to {{use a lot of}} disk space compared to palette-based cels, and do not compress well, so they are used sparingly by most artists.|$|E
50|$|The CFBF file {{consists}} of a 512-Byte <b>header</b> <b>record</b> followed {{by a number of}} sectors whose size is defined in the header. The literature defines Sectors to be either 512 or 4096 bytes in length, although the format is potentially capable of supporting sectors ranging in size from 128-Bytes upwards in powers of 2 (128, 256, 512, 1024, etc.). The lower limit of 128 is the minimum required to fit a single directory entry ina Directory Sector.|$|E
50|$|A DNS 'A record' {{may be used}} to {{advertise}} server support for ObsTCP (with a DNS 'CNAME record' providing a 'friendly' name). HTTP <b>header</b> <b>records,</b> or cached/out of band keyset information may also be used instead.|$|R
50|$|A client {{connecting}} to an ObsTCP server parses the DNS entries, uses HTTP <b>header</b> <b>records,</b> or uses cached/out of band data {{to obtain the}} public key and port number, before {{connecting to}} the server and encrypting traffic.|$|R
5000|$|Attendance: 23,764 at North Hobart Oval (Double <b>header)</b> *Ground <b>Record</b> ...|$|R
5000|$|A SOM file {{consists}} of a fixed-size <b>header</b> <b>record</b> followed {{by a number of}} sections, some of which are optional. The header always appears {{at the beginning of the}} file and contains the byte offsets and sizes of where the other sections are located within the file. Except for the header the other sections may appear anywhere in the file, although the typical layout of a SOM file (assuming all sections are present) is as follows: ...|$|E
50|$|Most {{files are}} 'cel' files which are raw, {{uncompressed}} graphics data analogous to animation cels. KiSS/GS2 specification cels also require a KCF (KiSS Colour File) as a palette, but CKiSS specification cels do not. A KCF also can control background colour and contain multiple palettes {{that can be}} swapped for lighting and colour change effects. All KiSS binary files (KCF, standard and CKiSS cels) since KiSS/GS2 share a common 32 byte binary <b>header</b> <b>record</b> identifying the size, type and format of KiSS data they contain.|$|E
50|$|The {{checksum}} {{is calculated}} {{by taking the}} sum of the unsigned byte values of the <b>header</b> <b>record</b> with the eight checksum bytes taken to be ascii spaces (decimal value 32). It is stored as a six digit octal number with leading zeroes followed by a NUL and then a space. Various implementations do not adhere to this format. For better compatibility, ignore leading and trailing whitespace, and take the first six digits. In addition, some historic tar implementations treated bytes as signed. Implementations typically calculate the checksum both ways, and treat it as good if either the signed or unsigned sum matches the included checksum.|$|E
5000|$|As in XML, all files {{begin with}} a header line as the first line. After the XML <b>header</b> a <b>record</b> set should start (for example or [...] ).|$|R
50|$|Following the <b>record</b> <b>header</b> is {{the actual}} data.|$|R
5000|$|Accordingly, every {{fragment}} {{except the}} last must contain {{a multiple of}} 8 bytes of data. It is obvious that Fragment Offset can hold 8192 (2^13) units but the datagram can't have 8192 * 8 = 65536 bytes of data because [...] "Total Length" [...] field of IP <b>header</b> <b>records</b> the total size including the header and data. An IP header is at least 20 bytes long, so the maximum value for [...] "Fragment Offset" [...] is restricted to 8189, which leaves room for 3 bytes in the last fragment.|$|R
5000|$|CALC uses a hashing {{algorithm}} {{to decide where}} to place the record; the hash key then provides efficient retrieval of the record. The entire CALC area is preformatted each with a header consisting of a special CALC [...] "owner" [...] record. The {{hashing algorithm}} determines a page number (from which the physical disk address can be determined), and the record is then stored on this page, or as near as possible to it, and {{is linked to the}} <b>header</b> <b>record</b> on that page using the CALC set. The CALC records are linked to the page's CALC Owner record using a single link-list (pointers). The CALC Owner located in the page header thus owns the set of all records which target to its particular page (whether the records are stored on that page or, {{in the case of an}} overflow, on another page [...] ).|$|E
5000|$|A tar archive {{consists}} {{of a series of}} file objects, hence the popular term tarball, referencing how a tarball collects objects of all kinds that stick to its surface. Each file object includes any file data, and is preceded by a 512-byte <b>header</b> <b>record.</b> The file data is written unaltered except that its length is rounded up to a multiple of 512 bytes. The original tar implementation did not care about the contents of the padding bytes, and left the buffer data unaltered, but most modern tar implementations fill the extra space with zeros. [...] The end of an archive is marked by at least two consecutive zero-filled records. (The origin of tar's record size appears to be the 512-byte disk sectors used in the Version 7 Unix file system.) The final block of an archive is padded out to full length with zeros.|$|E
40|$|The {{purpose of}} this {{document}} is to describe the standard electronic format for data files that will be sent for entry into the Tank Characterization Database (TCD). There are 2 different file types needed for each data load: Analytical Results; and Sample Descriptions. The first record of each file must be a <b>header</b> <b>record.</b> The content of the first 5 fields is ignored. They were used previously to satisfy historic requirements that are no longer applicable. The sixth field of the <b>header</b> <b>record</b> must contain the Standard Electronic Format (SEF) version ID (SEF 3. 0). The remaining records will be formatted as specified below. Fields within a record will be separated using the ''|'' symbol. The ''|''symbol must not appear anywhere in the file except when used as a delimiter...|$|E
50|$|The file {{is stored}} {{using the same}} {{structures}} (<b>header</b> and <b>records)</b> as a shapefile, but since it only stores the visibility values of the triangles many fields of the header are used.|$|R
5000|$|For every record, {{there is}} an eight byte <b>record</b> <b>header,</b> containing: ...|$|R
50|$|A QC record {{consists}} of a fixed 23-byte <b>header</b> (containing <b>record</b> type codes, destination account details, and the transaction amount) followed by zero or more optional fields, {{each of which is}} of variable size.|$|R
40|$|A {{program for}} shuttle post-flight data {{reduction}} is discussed. An extended Best Estimate Trajectory (BET) file was developed. The extended format results in some subtle {{changes to the}} <b>header</b> <b>record.</b> The major change is the addition of twenty-six words to each data record. These words include atmospheric related parameters, body axis rate and acceleration data, computed aerodynamic coefficients, and angular accelerations. These parameters were added to facilitate post-flight aerodynamic coefficient determinations as well as shuttle entry air data sensor analyses. Software (NEWBET) was developed to generate the extended BET file utilizing the previously defined ENTREE BET, a dynamic data file which may be either derived inertial measurement unit data or aerodynamic coefficient instrument package data, and some atmospheric information...|$|E
40|$|Data {{elements}} {{are not necessarily}} in the same sequence for both variable and fixed formats. Transaction Managment will reformat external formats to a common internal variable format. Exceptions to recognizing, and processing variable and fixed formats are listed below. The specific edits and validations done on common fixed and variable format headers are listed in Chapter 2. All transactions that reject from Transaction Management do so under output DIC KRU. See volume 8 for the Input/Output fixed length formats. See volume 9 for the Input/Output variable length formats. Variable format Input/Output transactions always begin with a numeric 4 position segment length. The package sequence number for the first record is always “A 01 ” because variable transactions have {{a minimum of two}} records. The <b>header</b> <b>record</b> will only be repeated (using same dcn) for DIC LMX, the nsn will change on each full header. Mini-Headers will be used under DIC LMD to indicate multiple actions that must be processed together. Character/data element occurs counters are required for some data elements in variable format. Following are examples of variable format transactions: Variable Format LMD with Mini-Header...|$|E
40|$|Sugar-Binding Residue Predictor (SBRP) by Masaki Banno at BILAB Sugar-Binding Residue Predictor (SBRP) is sugar binding {{prediction}} tool using support vector machine. SBRP {{can predict}} sugar-binding residue from {{amino acid sequence}} information. Protein Structure information is not necessary. Installation SBRP requires python 2. 7, libsvm, Legacy Blast and nr database. You can use following links install and download these programs and libraries and database. Legacy Blast (blastpgp) LIBSVM Python 2. 7 x Usage SBRP contains the binding residue predictor for acidic sugar and the binding-residue predictor for nonacidic sugar. Acidic sugar has phosphate group or carboxyl group or sulfate group. NonAcidic Sugar is except for acidic sugar. So, You can get prediction what residue in the protein sequence is belonged to acidic sugar binding residue or nonacidic binding residue. If you download above programs, you can run SBRP like below command.. /SBR_predictor. sh target. fasta The output example is below. 148 L:E|PDBID|CHAIN|SEQUENCE 8 - 1. 67762885786 1. 70627963313 False False 148 L:E|PDBID|CHAIN|SEQUENCE 9 - 0. 49179158867 1. 40920844499 False False 148 L:E|PDBID|CHAIN|SEQUENCE 10 1. 65258659258 0. 973523358096 True False 148 L:E|PDBID|CHAIN|SEQUENCE 11 - 1. 01298805163 1. 14320766578 False False 148 L:E|PDBID|CHAIN|SEQUENCE 12 - 0. 621458142578 1. 37502697583 False False 148 L:E|PDBID|CHAIN|SEQUENCE 13 0. 790677700098 0. 749706572303 True False 148 L:E|PDBID|CHAIN|SEQUENCE 14 - 1. 51129664951 1. 21914159325 False False 148 L:E|PDBID|CHAIN|SEQUENCE 15 - 0. 426752423351 0. 890993747856 False False Column 1 is predicted protein name, this name is equal to <b>header</b> <b>record</b> in FASTA file. Column 2 is predicted residue position. Column 3 and Column 4 are the decision values of binding residue predictor for acidic sugar and binding residue predictor for non acidic sugar. Column 5 and Column 6 are prediction result of acidic sugar binding residue predictor and non sugar binding residue predictor. Method Dataset Training dataset is collected from PDB. Sugar binding residue is defined as residues within 4 Å from sugar. The proteins are collected from PDB if the protein has sugar-binding residue. After that, these datasets are refined under the following condition. The PDB datum which is using the additive for the experiment of sugar. Residue within 1. 5 Å from sugar. (This is not interactions but covalent bond.) Finally, These proteins are removed redundancy using blastclust. Feature Vector SBRP is using the Position Specific Scoring Matrix (PSSM) calculated by PSI-Blast for a feature vector of each residue. Training method Support Vector Machine is needed positive dataset and negative dataset. The positive dataset is sugar-binding residue. The negative dataset is within 25 Å without 5 Å from sugar binding residue...|$|E
50|$|When {{traffic is}} captured, either the entire {{contents}} of packets are <b>recorded,</b> or the <b>headers</b> are <b>recorded</b> without recording the total {{content of the}} packet. This can reduce storage requirements, and avoid legal problems, yet provide sufficient information to diagnose problems.|$|R
40|$|Unkeyed {{data for}} the period 1920 to 1939 were found on 'Metforms' in the UK Met Office Archive. Part of these data with sheet numbers 30001 through 42999 A {{containing}} {{data for the}} period 1935 to 1939 were digitized by Atlantic Data Services Ltd., Blandford, Dorset, between January and April 1996 {{at a cost of}} £ 50, 000 (around 10 p per record). A total of 478, 796 records were keyed - 20, 030 <b>header</b> <b>records</b> and 458, 766 data records. This article describes how the original records were digitized into EBCDIC, quality controlled and converted to Flatfile format in preparation for later conversion to the US LMR format before merging into COADS...|$|R
40|$|The UCLA Space Science Group has {{developed}} a fixed format intermediate data set called a block data set, {{which is designed to}} hold multiple segments of multicomponent sampled data series. The format is sufficiently general so that tensor functions of one or more independent variables can be stored in the form of virtual data. This makes it possible for the unit data records of the block data set to be arrays of a single dependent variable rather than discrete samples. The format is self-documenting with parameter, label and <b>header</b> <b>records</b> completely characterizing the contents of the file. The block data set has been applied to the filing of satellite data (of ATS- 6 among others) ...|$|R
50|$|A {{database}} (.DBF) file {{is composed}} of a <b>header,</b> data <b>records,</b> deletion flags, and an end-of-file marker. The header contains information about the file structure, and the records contain the actual data. One byte of each record is reserved for the deletion flag.|$|R
5000|$|The file then {{contains}} {{any number}} of variable-length records. Each record is prefixed with a <b>record</b> <b>header</b> of 8 bytes: ...|$|R
50|$|A 720p high-definition camera mounted {{within the}} {{windshield}} <b>header</b> trim, which <b>records</b> the driver's point-of-view through the windshield, with audio recorded via a dedicated microphone in the cabin.|$|R
50|$|A GEDCOM file {{consists}} of a <b>header</b> section, <b>records,</b> and a trailer section. Within these sections, records represent people (INDI record), families (FAM records), sources of information (SOUR records), and other miscellaneous records, including notes. Every line of a GEDCOM file begins with a level number where all top-level records (HEAD, TRLR, SUBN, and each INDI, FAM, OBJE, NOTE, REPO, SOUR, and SUBM) begin with a line with level 0, while other level numbers are positive integers.|$|R
40|$|The goal of {{the thesis}} was {{development}} of graphical tool for time-frequency visualization and annotating of records of electrohysterogram (EHG) signals. The developed graphical tool Visual Annotating and Analyzing (VisuAA) visualizes EHG signals, their spectra, spectrograms, Wigner-Ville spectrograms, discrete wavelet transform scalograms (DWT), and continuous wavelet transform scalograms (CWT). Spectrograms, Wigner-Ville spectrograms, DWT scalograms and CWT scalograms can be visualized as 2 D or 3 D graphs. The tool allows saving of visualization results, examination, setting, deleting and editing of clinical and user defined annotations, examining and editing of record information and <b>record</b> <b>header</b> file, <b>record</b> notes, and saving of selected part of the record. The graphic tool VisuAA was developed using NetBeans integrated development environment. During development of graphical user interface we followed Nielsen's principles of graphical user interface design, while the main performance measures were efficiency and usability of the developed graphical tool. Graphical tool is intended for visualization and annotation of records of Term-Preterm EHG Database (TPEHGDB) database, {{as well as of}} any other signal records which are stored in the WFDB format. The VisuAA tool is practical and useful toll for development of advanced algorithms and procedures to analyze EHG records...|$|R
40|$|Abstract:- Triangle routing {{is one of}} {{the serious}} attacks to the Internet infrastructure. It can be caused by {{malicious}} routers which misroute packets to wrong directions. This kind of attacks creates network problems such as network congestion, denial of service and network partition and results in degrade of network performance. This paper gives a comprehensive study on how the path analysis combats the triangle routing attacks. We discuss the method, implementation and limitation of path analysis to detect triangle routing in IPv 4 network. We also discuss the implementation of path analysis in IPv 6 by proposing a new extension <b>header,</b> called <b>Record</b> Path <b>Header...</b>|$|R
