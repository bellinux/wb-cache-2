86|479|Public
2500|$|... svn:mergeinfo : Used {{to track}} merge <b>data</b> (<b>revision</b> numbers) in Subversion 1.5 (or later). This {{property}} is automatically {{maintained by the}} merge command, {{and it is not}} recommended to change its value manually.|$|E
50|$|Schema {{structure}} includes <b>data</b> <b>revision</b> {{and change}} tracking support, and life-cycle status detail, so applications {{can be developed}} to monitor data alterations over time and maintain records throughout the life-cycle of an equipment item. Units of measurement are also supported by the schemas, so AEX cfiXML can be used within the US or internationally.|$|E
50|$|The {{need for}} {{transparency}} {{is essential for}} NSOs to gain {{the trust of the}} public. They have to expose to the public the methods they use to produce official statistics, and be accountable for all the decisions they take and the results they publish. Also, statistical producers should warn users of certain interpretations and false conclusions even if they try to be as precise as possible. Furthermore, the quality of the accurate and timely results must be assessed prior to release. But if errors in the results occur before or after the <b>data</b> <b>revision,</b> they should be directly corrected and information should be disseminated to the users at the earliest possible time. Producers of official statistics have to set analytical systems in order to change or improve their activities and methods.|$|E
40|$|In {{the past}} ten years, {{researchers}} have explored the impact of <b>data</b> <b>revisions</b> in many different contexts. Researchers have examined the properties of <b>data</b> <b>revisions,</b> how structural modeling is affected by <b>data</b> <b>revisions,</b> how <b>data</b> <b>revisions</b> affect forecasting, the impact of <b>data</b> <b>revisions</b> on monetary policy analysis, {{and the use of}} real-time data in current analysis. This paper summarizes many of the questions for which real-time data analysis has provided answers. In addition, researchers and institutions have developed better real-time data sets around the world. Still, additional research is needed in key areas and research to date has uncovered even more fruitful areas worth exploring. (JEL C 52, C 53, C 80, E 01) ...|$|R
40|$|In this paper, using recent {{empirical}} results {{regarding the}} statistical properties of macroeconomic <b>data</b> <b>revisions,</b> we study the e¤ects of <b>data</b> <b>revisions</b> {{in a general}} equilibrium framework. We …nd {{that the presence of}} <b>data</b> <b>revisions,</b> or <b>data</b> uncertainty, creates a precautionary motive and causes signi…cant changes in the decisions of agents. We also …nd that the model with revisions captures some aspects of the business cycle dynamics of the US data better than the benchmark model with no revisions. Using our model we measure the cost of having <b>data</b> <b>revisions</b> to be about $ 33 billion, $ 5 billion of which can be recovered by eliminating the predictability of revisions. Comparing these numbers with the budgets of the major statistical agencies in the US, we conclude that any money spent on the improvement of data collection would be well worth it...|$|R
40|$|This paper tracks <b>data</b> <b>revisions</b> in the Personal Consumption Expenditure {{using the}} exclusions-from-core {{inflation}} persistence model. Keeping {{the number of}} observations the same, the regression parameters of earlier vintages of real-time data, beginning with vintage 1996 :Q 1, are tested for coincidence against the regression parameters of the last vintage of real-time data used in this paper, which is vintage 2008 :Q 2 in a parametric and two nonparametric frameworks. The effects of <b>data</b> <b>revisions</b> are not detectable {{in the vast majority}} of cases in the parametric model, but the flexibility of the two nonparametric models is able to utilize the <b>data</b> <b>revisions.</b> ...|$|R
40|$|Event history {{calendars}} (EHC) {{have proven}} to be a useful tool to collect retrospective autobiographic life course data. One problem is that they are only standardized to some extent. This limits their applicability in large-scale surveys. However, in such surveys a modularized retrospective CATI design can be combined with EHC. This <b>data</b> <b>revision</b> module is directly integrated into the interview and used as a <b>data</b> <b>revision</b> module. Hereby insights from cognitive psychology are applied. The <b>data</b> <b>revision</b> module stimulates the respondent's memory retrieval by detecting both temporal inconsistencies, such as gaps, and overlapping or parallel events. This approach was implemented in the IAB-ALWA study (Work and Learning in a Changing World), a large-scale representative telephone survey with 10, 000 respondents. By comparing the uncorrected data with the final data after revision, we investigate to what extent the application of this <b>data</b> <b>revision</b> module improves data quality or more precisely, time consistency and dating accuracy of individual reports. " (Author's abstract, IAB-Doku) ((en)) Biografieforschung - Methode, empirische Sozialforschung, Lebenslauf...|$|E
40|$|A {{positive}} view of data-mining {{has been}} recently {{presented in a}} Journal of Economic Methodology (JEM) symposium. This is {{in stark contrast to}} the stance normally taken. In this note consideration of the Bayesian philosophy of science literature and the impact of <b>data</b> <b>revision</b> extends the analysis of data-mining. Introduction of these issues is seen to provide support for the arguments presented in the JEM symposium. Data-MINING, <b>Data</b> <b>Revision,</b> Bayeasian Philosophy Of Science, Prediction And Accommodation,...|$|E
40|$|The current {{implementation}} of the Fan Chart displays equal tail probability bands and do {{not take into account}} that the variable of interest may be subject to <b>data</b> <b>revision.</b> In this note I propose the use of Highest Probability Density, HPD, bands and include flexibility to display the risks related to <b>data</b> <b>revision.</b> Click here to obtain a Visual Basic for Excel routine. Please save the the as FanChartGdpGrowth. xls and enable Macros to run the program. ...|$|E
40|$|Recent {{research}} examining U. S. macroeconomic <b>data</b> suggests that <b>revisions</b> {{may be much}} more important than traditionally assumed. This paper extends the analysis to Chinese data, where there has been substantial debate about data quality for some time. The key finding in this paper is that indeed the Chinese macroeconomic <b>data</b> <b>revisions</b> are not well-behaved, but that they are not much different from U. S. macroeconomic <b>data</b> <b>revisions...</b>|$|R
40|$|Traditionally, {{researchers}} have used linear regression models when work-ing with real-time <b>data</b> <b>revisions.</b> In this paper, we forecast real-time <b>data</b> <b>revisions</b> {{with a wide}} set of models including linear, structural break and regime-switching models with and without heteroskedasticity. We address {{the issues raised by}} the presence of model uncertainty through the use of Bayesian model averaging (BMA). Using UK data, we calculate predictive densities for real-time <b>data</b> <b>revisions</b> which average over the model space and compare them with the single best model and the linear model. We also present evidence about whether the revision process is unbiased and calcu-late various functions of the predictive density which might be of interest to a statistical agency. In contrast to the BMA approach, the traditional linear model yields very misleading predictives...|$|R
40|$|Model-based {{estimates}} of future uncertainty are generally {{based on the}} in-sample fit of the model, as when Box-Jenkins prediction intervals are calculated. However, this approach will generate biased uncertainty estimates in real time when there are <b>data</b> <b>revisions.</b> A simple remedy is suggested, and used to generate more accurate prediction intervals for 25 macroeconomic variables, {{in line with the}} theory. A simulation study based on an empirically-estimated model of <b>data</b> <b>revisions</b> for US output growth is used to investigate small-sample properties...|$|R
40|$|A {{modeling}} {{approach to}} real-time forecasting {{that allows for}} data revisions is shown. In this approach, an observed time series is decomposed into stochastic trend, <b>data</b> <b>revision,</b> and observation noise in real time. It is assumed that the stochastic trend is defined such that its first difference is specified as an AR model, and that the <b>data</b> <b>revision,</b> obtained only for the latest {{part of the time}} series, is also specified as an AR model. The proposed method is applicable to the data set with one vintage. Empirical applications to real-time forecasting of quarterly time series of US real GDP and its eight components are shown to illustrate the usefulness of the proposed approach. [*][*]Copyright Â© 2007 John Wiley & Sons, Ltd. ...|$|E
40|$|I {{appreciate}} the useful comments of three anonymous referees and the editor, Roger Gordon, {{as well as}} Richard Anderson, David Papell, Glenn Rudebusch, and participants at the Workshop on Using Euro Area Data and the Workshop on Macroeconomic Forecasting, Analysis and Policy with <b>Data</b> <b>Revision.</b> Thanks to Tom Stark for many discussions about real-time data issues ove...|$|E
40|$|This paper {{places the}} <b>data</b> <b>revision</b> model of Jacobs and van Norden (2011) within {{a class of}} trend-cycle decompositions {{relating}} directly to the Beveridge-Nelson decomposition. In both these approaches identifying restrictions on the covariance matrix under simple and realistic conditions may produce a smoothed estimate of the underlying series which is more volatile than the observed series...|$|E
40|$|Forecasts {{are only}} as good as the data behind them. But {{macroeconomic}} data are revised, often significantly, as time passes and new source data become available and conceptual changes are made. How is forecasting influenced by the fact that data are revised? To answer this question, we begin with the example of the index of leading economic indicators to illustrate the real-time data issues. Then we look at the data that have been developed for U. S. <b>data</b> <b>revisions,</b> called the "Real-Time Data Set for Macroeconomists" and show their basic features, illustrating the magnitude of the revisions and thus motivating their potential influence on forecasts and on forecasting models. The data set consists of a set of data vintages, where a data vintage refers to a date at which someone observes a time series of data; so the data vintage September 1974 refers to all the macroeconomic time series available to someone in September 1974. Next, we examine experiments using that data set by Stark and Croushore (2002), Journal of Macroeconomics 24, 507 - 531, to illustrate how the <b>data</b> <b>revisions</b> could have affected reasonable univariate forecasts. In doing so, we tackle the issues of what variables are used as "actuals" in evaluating forecasts and we examine the techniques of repeated observation forecasting, illustrate the differences in U. S. data of forecasting with real-time data as opposed to latest-available data, and examine the sensitivity to <b>data</b> <b>revisions</b> of model selection governed by various information criteria. Third, we look at the economic literature on the extent to which <b>data</b> <b>revisions</b> affect forecasts, including discussions of how forecasts differ when using first-available compared with latest-available data, whether these effects are bigger or smaller depending on whether a variable is being forecast in levels or growth rates, how much influence <b>data</b> <b>revisions</b> have on model selection and specification, and evidence on the predictive content of variables when subject to <b>revision.</b> Given that <b>data</b> are subject to <b>revision</b> and that <b>data</b> <b>revisions</b> influence forecasts, what should forecasters do? Optimally, forecasters should account for <b>data</b> <b>revisions</b> in developing their forecasting models. We examine various techniques for doing so, including state-space methods. The focus throughout this chapter is on papers mainly concerned with model development - trying to build a better forecasting model, especially by comparing forecasts from a new model to other models or to forecasts made in real time by private-sector or government forecasters. ...|$|R
40|$|This paper {{analyzes}} the relative performance of multi-step forecasting methods {{in the presence}} of breaks and <b>data</b> <b>revisions.</b> Our Monte Carlo simulations indicate that the type and the timing of the break affect the relative accuracy of the methods. The iterated method typically performs the best in unstable environments, especially if the parameters are subject to small breaks. This result holds regardless of whether <b>data</b> <b>revisions</b> add news or reduce noise. Empirical analysis of real-time U. S. output and inflation series shows that the alternative multi-step methods only episodically improve upon the iterated method. ...|$|R
40|$|We {{investigate}} to {{what extent}} estimated relationships of the IMF’s monetary model and their policy implications are sample dependent. This model constitutes {{the core of the}} IMF’s financial programming models for developing and emerging economies. We observe that estimates of the model’s key parameters and model-based measures of macroeconomic disequilibria are highly dependent on data vintage employed. Changes in parameter estimates solely due to <b>data</b> <b>revisions</b> are found to be much smaller than those owing to parameter instability, which may be due to model misspecification. Moreover, instability in parameter estimates contributes to more uncertainty in evaluations of macroeconomic excesses than <b>data</b> <b>revisions.</b> It is shown that analyses based on a version of the model in difference form are more robust across data vintages than those based on the model with variables in levels. Well specified models that take into account known <b>data</b> <b>revisions</b> may also have relatively stable parameter estimates and hence more robust policy implications. Real time data, Data and model uncertainty, IMF, Financial programming...|$|R
40|$|Global {{temperature}} {{increase on}} Earth, due to climate change, indicates {{the need for}} a <b>data</b> <b>revision</b> pertaining to outdoor design temperature and number of degree – days. In this paper, based on the relevant data pertaining to tempereture mesurements in Sarajevo for a period of ten years (since 2001 to 2010), new outdoor design temperature- 13 °C is defined, instead of applicable temperature- 18 °C and number of degree days 2381 °C-ann, instead of applicable 3077 °C-ann. The results of calculation show that requirement for thermal energy is 22 % less for outdoor design temperature of- 13 °C. Therefore, there is the need for <b>data</b> <b>revision</b> pertaining to outdoor design temperature and number of degree – days, wich could lead to a correction in the heating systems. As a consequence, atmospheric CO 2 emmision will be less in the building sector, which can {{play a key role in}} combating climate change...|$|E
40|$|Real-time {{macroeconomic}} data refl ect {{the information}} available to market participants, whereas fi nal data-containing revisions and released with a delay-overstate the information set available to them. We document that the in-sample and out-of-sample Treasury return predictability is signifi cantly diminished when real-time as opposed to revised macroeconomic data are used. In fact, much of the predictive information in macroeconomic time series {{is due to the}} <b>data</b> <b>revision</b> and publication lag components...|$|E
40|$|In this paper, we {{take into}} {{consideration}} some {{issues related to the}} use of a nonlinear structural econometric model {{in the presence of a}} <b>data</b> <b>revision</b> process. We analyse the consequences on the parameter estimation (consistency is still attainable) and on forecast. In the latter case, we show that the asymptotic bias and mean squared prediction error of the deterministic and Monte Carlo predictors have new elements with respect to the traditional analysis in the absence of data uncertainty. ...|$|E
40|$|This paper {{examines}} {{the characteristics of}} the revisions to the inflation rate as measured by the personal consumption expenditures price index both including and excluding food and energy prices. These data series {{play a major role in}} the Federal Reserve’s analysis of inflation.; The author {{examines the}} magnitude and patterns of revisions to both PCE inflation rates. The first question he poses is: What do <b>data</b> <b>revisions</b> look like? The author runs a variety of tests to see if the <b>data</b> <b>revisions</b> have desirable or exploitable properties. The second question he poses is related to the first: Can we forecast <b>data</b> <b>revisions</b> in real time? The answer is that it is possible to forecast revisions from the initial release to August of the following year. Generally, the initial release of inflation is too low and is likely to be revised up. Policymakers should account for this predictability in setting monetary policy. Inflation (Finance); Monetary policy...|$|R
40|$|This paper {{considers}} {{the effects of}} inaccurate real-time output data on fiscal policy, both with respect to budgetary planning and fiscal surveillance. As newer and better information becomes available, output data available in real time get revised {{and are likely to}} conflict with final figures that are only released some years later. By contrast, fiscal policy is inevitably based on real-time figures. The paper develops a simple but comprehensive modeling framework to formalize the linkages between output <b>data</b> <b>revisions</b> and fiscal policy and combines it with a newly compiled dataset from the International Monetary Fund’s World Economic Outlook, comprising final and real-time output data for 175 countries, over a period of 17 years. Based on a simulation exercise, it finds that output <b>data</b> <b>revisions</b> alone may significantly undermine the reliability of real-time estimates of the overall and structural fiscal balances, and that output <b>data</b> <b>revisions</b> may result in unplanned and substantial debt accumulation. The paper also shows that there are significant differences across country income groups...|$|R
5000|$|Frank T. Denton and Ernest H. Oksanen, 1972. “A Multi-Country Analysis of the Effects of <b>Data</b> <b>Revisions</b> on an Econometric Model,” Journal of the American Statistical Association, vol. 67(338), pp. 286-291.|$|R
40|$|A {{well-documented}} {{property of}} the Beveridge-Nelson trend-cycle decomposition is the perfect negative correlation between trend and cycle innovations. This paper gives a novel explanation for this negative correlation originating from the Jacobs-van Norden (2011) <b>data</b> <b>revision</b> model. Trend shocks may enter the equation for the cycle or cyclical shocks may enter the trend equation. We discuss economic interpretations and implications, including ltering and smoothing properties. We illustrate the idea with simulations based on the Morley, Nelson and Zivot (2003) outcome...|$|E
40|$|This paper {{tests the}} {{rationality}} of individual price forecasts in {{a panel of}} professional forecasters. Here, unlike in most previous studies, rationality is not rejected. The results here differ because (1) using individual forecasts avoids aggregation bias, (2) comparison of forecasts to initial data avoids bias due to <b>data</b> <b>revision,</b> (3) the professional forecasters have economic incentives to state their expectations accurately, and (4) a new covariance matrix estimator consistent when forecast errors are correlated across individuals is used. Copyright 1990 by American Economic Association. ...|$|E
40|$|This paper {{analyses}} {{revisions of}} Swiss current account data, {{taking into account}} the actual <b>data</b> <b>revision</b> process and the implied types of revisions. In addition we investigate whether the first release of current account data can be improved upon by the use of survey re- sults as gathered by the KOF Swiss Economic Institute, ETH Zurich. An answer in the affirmative indicates {{that it is possible to}} improve first releases and thereby enhance the current assessment of the Swiss economy. current account statistics, real-time analysis, data revisions...|$|E
40|$|We {{show that}} an {{extension}} of the Markov-switching dynamic factor models that accounts for the specificities of the day to day monitoring of economic developments such as ragged edges, mixed frequencies and <b>data</b> <b>revisions</b> is a good tool to forecast the Euro area recessions in real time. We provide examples that show the nonlinear nature of the relations between <b>data</b> <b>revisions,</b> point forecasts and forecast uncertainty. According to our empirical results, we think that the real time probabilities of recession are an appropriate statistic to capture what the press call green shoots. Business Cycles, Output Growth, Time Series...|$|R
40|$|We {{investigate}} the e¤ects of <b>data</b> <b>revisions</b> on forecasting using autoregressive models, both when {{the data set}} consists of end-of-sample data, and when the data set is constructed such that it comprises only …rst-release values at each point in time. We derive analytical expressions for the e¤ects of noise, news and non-zero mean revisions on the estimates of the models’ parameters. Our calculations indicate that di¤erences {{in the construction of}} the real-time data sets have small e¤ects on forecast accuracy given the nature of <b>data</b> <b>revisions</b> to output growth and in‡ation in the US, which is borne out empirically...|$|R
40|$|Real-time {{macroeconomic}} {{data are}} typically incomplete for {{today and the}} immediate past ('ragged edge') and subject to revision. To enable more timely forecasts the recent missing data have to be imputed. The paper presents a state-space model that can deal with publication lags and <b>data</b> <b>revisions.</b> The framework {{is applied to the}} US leading index. We conclude that including even a simple model of <b>data</b> <b>revisions</b> improves the accuracy of the imputations and that the univariate imputation method in levels adopted by The Conference Board can be improved upon. (C) 2011 Elsevier Inc. All rights reserved...|$|R
40|$|Previous {{versions}} of this paper have been presented at the workshop on Macroeconomic Forecasting, Analysis and Policy with <b>Data</b> <b>Revision</b> (Montreal, 2006), the 22 th Annual Congress of the European Economic Association (Budapest, 2007) and the CSEF seminar series (Napoli, 2008). This paper assesses the role of surveys for the early estimates of GDP in the euro area in a model-based automated procedures which exploits the timeliness of their release. The analysis is conducted using both an historical evaluation and a real time case study on the current conjuncture...|$|E
40|$|This Selected Issues paper {{examines}} {{national accounts}} revisions {{and the economic}} cycle for the United Kingdom. The paper concludes that upward revisions to GDP data are positively correlated with economic activity, in particular, with growth in its domestic component. The paper suggests that although revisions may have become smaller in recent years, the procyclical bias in the <b>data</b> <b>revision</b> has not been eliminated. The regression model employed also suggests that GDP growth in 1996 â€”currently estimated at 2. 4 percentâ€”could {{be as much as}} a 0. 6 percentage point higher than that estimate. ...|$|E
40|$|This paper {{studies the}} {{information}} content of some Ifo indicators. In particular, we investigate whether two Ifo indicators, {{one on the}} current business situation, the other on current production development, provide information on revisions of German industrial production. A new feature of our analysis is the construction and use of a real-time dataset. We conclude that the Ifo indicators {{play a role in}} explaining revisions, but counterintuitively the business situation indicator performs better than the production indicator. Ifo Business Survey indicators, German industrial production, real-time analysis, <b>data</b> <b>revision...</b>|$|E
40|$|We {{propose a}} new VAR {{identification}} strategy {{to study the}} impact of noise shocks on aggregate activity. We do so exploiting the informational advantage the econometrician has, relative to the economic agent. The latter, who is uncertain about the underlying state of the economy, responds to the noisy early data releases. The former, {{with the benefit of}} hindsight, has access to <b>data</b> <b>revisions</b> as well, which can be used to identify noise shocks. By using a VAR we can avoid making very specific assumptions on the process driving <b>data</b> <b>revisions.</b> We rather remain agnostic about it but make our identification strategy robust to whether <b>data</b> <b>revisions</b> are driven by noise or news. Our analysis shows that a surprising report of output growth numbers delivers a persistent and hump-shaped response of real output and unemployment. The responses are qualitatively similar but an order of magnitude smaller than those to a demand shock. Finally, our counterfactual analysis supports the view {{that it would not be}} possible to identify noise shocks unless different vintages of data are used...|$|R
40|$|<b>Data</b> <b>revisions</b> {{routinely}} {{introduced by}} the World Bank can lead to significant revisions in empirical results. We show this by re-estimating our aggregate indicator for predicting the 1997 Asian crisis utilizing the 1999 and 2004 updates of the 1996 World Bank data and comparing these results to those we obtained (this Journal, 2000) for predicting the same event using the original, unrevised, 1996 World Bank data. Since most data-gathering organizations routinely revise their data, this may represent a much greater problem for policy makers than might be recognized. Copyright Springer Science + Business Media, Inc. 2005 <b>data</b> <b>revisions,</b> financial crisis, emerging markets, warning indicators,...|$|R
40|$|In {{the first}} chapter we {{document}} the empirical properties of revisions to major macroeconomic variables in the U. S., over the period 1966 – 2000. We find that these revisions {{do not have a}} zero mean, which indicates that the initial announcements by statistical agencies are biased. We also find that the revisions are quite large compared to the original variables. They are predictable using the information set {{at the time of the}} initial announcement, which means that the initial announcements of statistical agencies are not rational forecasts. We also provide some evidence that professional forecasters ignore this predictability. Our findings suggest that <b>data</b> <b>revisions</b> in the U. S. do not satisfy simple desirable statistical properties. ^ In the second chapter, using the empirical results from the previous chapter as the motivation, we study the effects of <b>data</b> <b>revisions</b> in a general equilibrium framework. We find that the presence of <b>data</b> <b>revisions,</b> or <b>data</b> uncertainty, creates a precautionary motive and causes significant changes in the decisions of agents. We also find that the model with revisions captures some aspects of the business cycle dynamics of the US data better than the benchmark model with no revisions. Using our model we measure the cost of having <b>data</b> <b>revisions</b> to be about $ 33 billion, $ 5 billion of which can be recovered by eliminating the predictability of revisions. Comparing these numbers with the budgets of the major statistical agencies in the US, we conclude that any money spent on the improvement of data collection would be well worth it. ^ In the third chapter we take the first step in analyzing ways of hedging the risk of <b>data</b> <b>revisions.</b> We show that we can construct portfolios of assets, which are maximally correlated with revisions to macroeconomic variables. The average correlation of the returns of these hedging portfolios and the underlying revision risk are in the order of 0. 50, which is very promising in terms of hedging performance. ...|$|R
