140|137|Public
50|$|These are rotors spun at {{constant}} speed, {{mounted on}} gimbals to provide attitude control. Although a CMG provides control {{about the two}} axes orthogonal to the gyro spin axis, triaxial control still requires two units. A CMG {{is a bit more}} expensive in terms of cost and mass, because gimbals and their drive motors must be provided. The maximum torque (but not the maximum angular momentum change) exerted by a CMG is greater than for a momentum wheel, making it better suited to large spacecraft. A major drawback is the additional complexity, which increases the number of failure points. For this reason, the International Space Station uses a set of four CMGs to provide dual <b>failure</b> <b>tolerance.</b>|$|E
50|$|The PCI Express {{link between}} two devices can consist of {{anywhere}} {{from one to}} 32 lanes. In a multi-lane link, the packet data is striped across lanes, and peak data throughput scales with the overall link width. The lane count is automatically negotiated during device initialization, and can be restricted by either endpoint. For example, a single-lane PCI Express (×1) card can be inserted into a multi-lane slot (×4, ×8, etc.), and the initialization cycle auto-negotiates the highest mutually supported lane count. The link can dynamically down-configure itself to use fewer lanes, providing a <b>failure</b> <b>tolerance</b> in case bad or unreliable lanes are present. The PCI Express standard defines slots and connectors for multiple widths: ×1, ×4, ×8, ×12, ×16 and ×32. This allows the PCI Express bus to serve both cost-sensitive applications where high throughput is not needed, as well as performance-critical applications such as 3D graphics, networking (10 Gigabit Ethernet or multiport Gigabit Ethernet), and enterprise storage (SAS or Fibre Channel).|$|E
40|$|We have {{investigated}} an assessment technique {{for studying the}} <b>failure</b> <b>tolerance</b> of large-scale component-based information systems. Our technique assesses the propagation of information through the interfaces between objects in order to predict how software will behave when corrupt information gets passed. Our approach is applicable to both source code and executable commercial off-the-shelf (COTS) components. The key benefit of our approach {{is that it can}} assess the <b>failure</b> <b>tolerance</b> of legacy systems composed entirely of executable software components...|$|E
40|$|This article proposes {{including}} two cooperative robotic manipulators into an automatic disassembly cell apply for recycling. The synergy produced by two units {{working in a}} coordinated way increases the versatility {{and the performance of}} the system. In addition provide the system with all the advantages of carrying out tasks in groups, like are: sharing information and resources, a greater <b>failures</b> <b>tolerance,</b> and assistance between manipulators. Automatic disassembly for recycling aspires to eliminate the amount of residues that a product generates once its useful life finalizes, reducing the consequent damage generated to the environment. This work was funded by the Spanish MCYT project “DESAURO: Desensamblado automático selectivo para reciclado mediante robots cooperativos y sistema multisensorial” (DPI 2002 - 02103) and by the Spanish G. V. project “Desensamblado automático cooperativo para el reciclado de productos” (GV 05 / 003) ...|$|R
40|$|Attacks and <b>failures</b> <b>tolerance</b> {{of complex}} {{networks}} {{is related to}} the concept of vulnerability in different contexts, from biology and sociology to computer science and physics. Studies have revealed that the topologies of these networks are inherently vulnerable to failures of central elements. Complex adaptive systems, emergent in nature, are considered more resilient because, despite facing periodic changes, have the ability to maintain their main topological properties. Inspired by these systems and with the purpose of providing more robust communication networks, this work proposes mechanisms for evaluating fault tolerance of complex networks topologies and procedures for promoting the necessary adjustments to maintain their main topological properties. The performance of the proposed mechanisms showed that failures of central elements can lead to a global state of vulnerability. In such a state, evaluation and adaptation processes are more likely to miscarry. The resulting model also serves as a tool for the computational modelling of problems of vulnerability in several application contexts...|$|R
5|$|Mutant Huntingtin is {{expressed}} {{throughout the body}} and associated with abnormalities in peripheral tissues that are directly caused by such expression outside the brain. These abnormalities include muscle atrophy, cardiac <b>failure,</b> impaired glucose <b>tolerance,</b> weight loss, osteoporosis, and testicular atrophy.|$|R
40|$|At the 1995 Computer Assurance (COMPASS) conference, Voas and Miller {{presented}} atechnique {{for assessing}} the <b>failure</b> <b>tolerance</b> ofaprogram when the program was executing in unlikely modes (with respect to the expected operational pro le). In that paper, several preliminary algorithms werepresented for inverting operational pro les to more easily distinguish the unlikely modes of operation from the likely modes. This paper re nes the original algorithms. This paper then demonstrates the new algorithms being used inconjunction with a <b>failure</b> <b>tolerance</b> assessment technique on two small programs. ...|$|E
40|$|In this paper, {{we examine}} whether a failure-tolerant {{corporate}} culture spurs corporate innovation and increases firm {{value based on}} a sample of venture capital (VC) backed IPO firms. We develop a novel firm-specific measure of a failure-tolerant corporate culture hinging {{on the idea that}} the formation of corporate culture is largely determined by the attitudes and beliefs of its founders and early active investors. VC firms are such investors and their attitudes towards failures can have a profound impact on the formation of a failure-tolerant culture in the entrepreneurial firms in which they invest. We find that firms with a more failure-tolerant culture are significantly more innovative. The <b>failure</b> <b>tolerance</b> effect is persistent and robust to controlling for other VC firm characteristics. Further, the <b>failure</b> <b>tolerance</b> effect on firm innovation is stronger in industries in which innovation is more difficult to achieve. We also find opposite roles of insider equity ownership and <b>failure</b> <b>tolerance</b> in motivating innovation. Finally, we find that a failure-tolerant corporate culture increases firm value in industries in which innovation is important. Overall, our findings are consistent with implications in recent theories on corporate innovation that <b>failure</b> <b>tolerance</b> is critical in motivating innovation. Our work also contributes to the literature on corporate culture by making the first attempt at measuring a specific aspect of corporate culture and providing empirical evidence that corporate culture does matter in significant ways for firms ’ decisions and performances...|$|E
40|$|An {{overview}} of failure-tolerant control is presented, beginning with robust control, progressing through parallel and analytical redundancy, {{and ending with}} rule-based systems and artificial neural networks. By design or implementation, failure-tolerant control systems are 'intelligent' systems. All failure-tolerant systems require some degrees of robustness to protect against catastrophic failure; <b>failure</b> <b>tolerance</b> often can be improved by adaptivity in decision-making and control, {{as well as by}} redundancy in measurement and actuation. Reliability, maintainability, and survivability can be enhanced by <b>failure</b> <b>tolerance,</b> although each objective poses different goals for control system design. Artificial intelligence concepts are helpful for integrating and codifying failure-tolerant control systems, not as alternatives but as adjuncts to conventional design methods...|$|E
40|$|This paper {{presents}} an intelligent fault tolerant {{flight control system}} that blends aerodynamic and propulsion actuation for safe flight operation {{in the presence of}} actuator <b>failures.</b> Fault <b>tolerance</b> is obtained by a nonlinear adaptive control strategy based on on-line learning neural networks and actuator reallocation scheme. The adaptive control block incorporates a recently developed technique for adaptation in the presence of actuator saturation, rate limits and failure. The proposed integrated aerodynamic/propulsion flight control system is evaluated in a nonlinear flight simulation environment...|$|R
40|$|This work studies IPEX (Immunodysregulation with Polyendocrinopathy and Enteropathy X-linked), {{a severe}} {{disorder}} {{of course in}} early childhood due to mutations in the FOXP 3 gene (locus Xp 11. 23 -q 13. 3) leading to <b>failure</b> in immune <b>tolerance</b> and his theraphy with HSCT (Haematopoietic stem cell transplantation) ...|$|R
40|$|International audienceThis paper {{proposes a}} fault {{tolerant}} permission based k-mutual exclusion algorithm {{which does not}} rely on timers, nor on failure detectors, neither does it require extra messages for detecting node <b>failures.</b> Fault <b>tolerance</b> is integrated in the algorithm itself and it is provided if the underlying system guarantees the responsiveness property (RP). Based on Raymondpsilas algorithm, our algorithm exploits the request reply messages exchanged by processes {{to get access to}} one of the k units of the shared resource in order to dynamically detect failures and adapt the algorithm to tolerate them...|$|R
40|$|We {{examine how}} investors ’ {{tolerance}} for failure impacts {{the types of}} projects {{they are willing to}} fund. We show that actions that reduce short term accountability and thus encourage agents to experiment more simultaneously reduce the level of experimentation financial backers are willing to fund. <b>Failure</b> <b>tolerance</b> has an equilibrium price that increases in the level of experimentation. More experimental projects that don’t generate enough to pay the price cannot be started. In fact, an endogenous equilibrium can arise in which all competing financiers choose to be failure tolerant in the attempt to attract entrepreneurs, leaving no capital to fund the most radical, experimental projects in the economy. The tradeoff between <b>failure</b> <b>tolerance</b> and a sharp guillotine help explain when and where radical innovation occurs...|$|E
40|$|It is {{commonly}} asserted that {{high rates of}} entrepreneurship and superior economic performance in the United States is linked to a higher cultural tolerance of business failure. After reviewing cross country patterns of entrepreneurship we develop in this paper a measure of cultural attitudes towards failure which has two components. We term these <b>failure</b> <b>tolerance</b> which captures attitudes towards {{the risk of a}} business failing and second chancing which measures the degree of agreement with the proposition that those who have failed should be given a second chance. Using a unique dataset on attitudes to failure for a sample of 9, 500 individuals drawn from 19 economies for the year 2002 we show that respondents in the USA appear to have relatively high levels of <b>failure</b> <b>tolerance.</b> However, they are less willing to grant a second chance to those who have tried and failed. We find that having relatively high levels of <b>failure</b> <b>tolerance</b> is not positively correlated with GDP growth. Having a relatively positive attitude towards second chancing across countries is positively related to GDP growth. Taken together these results suggest there is a link between attitudes to failure and economic growth, {{but it is not the}} one conventionally assumed i...|$|E
40|$|This {{autonomous}} experiment for foaming {{metals in}} space involved: (1) payload support structure; (2) furnace and foaming apparatus; (3) electronic controls; (4) battery power; and (5) metallurgy. Emphasis was laid on a modular design which was easily modifiable and which offered maximum durability, safety, and <b>failure</b> <b>tolerance...</b>|$|E
50|$|There is a <b>failure</b> of immune <b>tolerance</b> {{against the}} {{mitochondrial}} pyruvate dehydrogenase complex (PDC-E2), {{and this may}} also be the case with other proteins, including the gp210 and p62 nuclear pore proteins. Gp210 has increased expression in the bile duct of anti-gp210 positive patients, and these proteins may be associated with prognosis.|$|R
40|$|Cloud {{computing}} is {{the result}} of evolution of on demand service in computing paradigms of large scale distributed computing. It is the adoptable technology as it provides integration of software and resources which are dynamically scalable. These systems are more or less prone to <b>failure.</b> Fault <b>tolerance</b> assesses the ability of a system to respond gracefully to an unexpected hardware or software failure. In order to achieve robustness and dependability in cloud computing, failure should be assessed and handled effectively. This paper aims to provide a better understanding of fault tolerance techniques used for fault tolerance in cloud environments along with some existing model and further compare them on various parameters...|$|R
40|$|In {{wireless}} sensor {{and actor}} networks (WSANs), fault detection is of prime importance where an unexpected failure should be properly {{identified by the}} network system. Since the sensor nodes are prone to <b>failure,</b> fault <b>tolerance</b> should be seriously considered in many sensor network applications. In this paper, we propose Fault Tolerance using Modified Hausdroff Distance Method (FTMHD). The nodes are randomly deployed in an M*N area. The actor node verifies whether it receives correct data from node or not. The Modified Hausdroff Distance (MHD) is used to determine lower and upper bound of correct data range to be detected by the nodes. The proposed FTMHD has better sampling fraction and compression ratio compared to existing technique...|$|R
40|$|The {{survivability}} of {{the network}} {{is closely related to}} the connectivity {{of the network}}. For many network applications such as a sensor network/ad hoc network for battlefield monitoring or wild fire tracking etc., survivability is one of most important requirements, and therefore a number of related problems are proposed. In this paper we study the survivability with respect to multicast route <b>failure</b> <b>tolerance</b> in K-connected MANET. We analyze the key issues namely connectivity, route <b>failure</b> <b>tolerance</b> and energy of the network. These issues can optimize the design of the routing protocol by means of cross layer interaction across the layers of the network. This paper analyzes the K-connected MANET for various parameters namely connectivity, no. of links, no. of spanning trees, no. of EDMSTs using graph theory metric, connectivity index...|$|E
40|$|Abstract – In {{contrast}} to a typical single source of data updates in Internet applications, data files in a networked information system are often distributed, replicated, accessed and updated by multiple nodes. Due to concurrent updates, replicated data files must be synchronized. For certain applications, stringent concurrency control must be employed to ensure data integrity, while for other applications, periodic data synchronization may enable very efficient data sharing. For the latter applications, this paper devises the ShuffleNet and hypercube schemes for data synchronization in such networked information systems. Their performance in terms of update delay, processing complexity, <b>failure</b> <b>tolerance</b> and growth complexity is examined. Our results reveal that the ShuffleNet and hypercube scheme provide identical maximum update delay and similar processing complexity. However, {{as the number of}} nodes in the system changes (e. g., due to failure or temporary out of service for maintenance), the hypercube scheme maintains all existing synchronization sessions and greatly simplifies system administration overhead such as moving files from node to node for the purpose of data synchronization. The ShuffleNet scheme does provide a higher degree of <b>failure</b> <b>tolerance</b> for global data files, but the hypercube scheme provides more than adequate <b>failure</b> <b>tolerance.</b> Lastly, a generalization of the hypercube scheme, based on the ideas of shift registers, is also proposed for systems where the number of nodes is a perfect power of 2. I...|$|E
40|$|This paper {{describes}} a complex networks approach {{to study the}} <b>failure</b> <b>tolerance</b> of mechatronic software systems under various types of hardware and/or software failures. We produce synthetic system architectures based on evidence of modular and hierarchical modular product architectures and known motifs for the interconnection of physical components to software. The system architectures are then subject to various forms of attack. The attacks simulate failure of critical hardware or software. Four types of attack are investigated: degree centrality, betweenness centrality, closeness centrality and random attack. <b>Failure</b> <b>tolerance</b> {{of the system is}} measured by a 'robustness coefficient', a topological 'size' metric of the connectedness of the attacked network. We find that the betweenness centrality attack results in the most significant reduction in the robustness coefficient, confirming betweenness centrality, rather than the number of connections (i. e. degree), as the most conservative metric of component importance. A counter-intuitive finding is that "designed" system architectures, including a bus, ring, and star architecture, are not significantly more failure-tolerant than interconnections with no prescribed architecture, that is, a random architecture. Our research provides a data-driven approach to engineer the architecture of mechatronic software systems for <b>failure</b> <b>tolerance.</b> Comment: Proceedings of the 2013 ASME International Design Engineering Technical Conferences & Computers and Information in Engineering Conference IDETC/CIE 2013 August 4 - 7, 2013, Portland, Oregon, USA (In Print...|$|E
40|$|Autoimmune {{hemolytic}} anemia (AIHA) occurs when pathogenic autoantibodies against {{red blood cell}} (RBC) antigens are generated. Whilst the basic disease pathology of AIHA is well studied, the underlying mechanism(s) behind the <b>failure</b> in <b>tolerance</b> to RBC autoantigens are poorly understood. Thus, to investigate the tolerance mechanisms required for the establishment and maintenance of tolerance to RBC antigens, we developed a novel murine model. With this model, we evaluated the role of regulatory T cells (Tregs) in tolerance to RBC-specific antigens. Herein, we show that neither sustained depletion of Tregs nor immunization with RBC-specific proteins in conjunction with Treg depletion led to RBC-specific autoantibody generation. Thus, these studies demonstrate that Tregs {{are not required to}} prevent autoantibodies to RBCs and suggest that other tolerance mechanisms are likely involved...|$|R
40|$|Strong Consistency We {{would like}} to have „all-or-nothing ‟ {{semantics}} and ideally have all copies of the same data always cosnsitent as if there is only 1 copy High Availability A data system should always be up. E. g. Werner Vogels keynote: Amazon will always take your order The challenge: the larger the system, the higher the prob of <b>failures</b> Partition <b>Tolerance</b> If there is a network failure that splits the processing nodes into two groups that cannot talk to each other, then the goal would be to allow processing to continue in both subgroups. INFO 5011 "Cloud Computing "- 2011 (U. Röhm and Y. Zhou) 11 adv- 3 How to achieve this? Are distributed transactions with ACID guarantees the solution...|$|R
40|$|Replication is a {{topic of}} {{interest}} in the distributed computing, distributed systems, and database communities. Decision support systems became practical with the development of minicomputer, timeshare operating systems and distributed computing. Replicated data may get insufficient due to system <b>failure,</b> fault <b>tolerance,</b> and reliability. A partial Replication is quantized in the replication system will increase the non replicated system. Fault tolerance is the property that enables a system (often computer-based) to continue operating properly. Transaction Processing Replication (TP-R) and Decision-support replication schema (DDS-R) will clear the non replica and it is used to clear the server problems and system error. This process is well executed in distributed systems and it doesn’t fail to detect the system errors when multiple access are multiplexed. 1...|$|R
40|$|We {{study the}} {{coordination}} and control problem of distributed discrete-event systems with synchronous communication, {{in the presence}} of subsystems whose sensors and/or actuators may be affected by unexpected failures. We model sensor failures as permanent loss of observability of certain sensor events that belong to a subsystem, while characterize actuator failures as loss of controllability of the subsystems' actuator events. The <b>failure</b> <b>tolerance</b> property requires that the distributed discrete-event systems satisfy a global specification prior to as well as after occurrences of potential failures. To prevent the failure-pruned subsystems from jeopardizing the fulfillment of the specification, we propose automaton-theoretic frameworks corresponding to the enforcement of sensor and actuator <b>failure</b> <b>tolerance</b> by incorporating learning-based supervisor synthesis and coordination approaches with appropriate post-failure control reconfiguration schemes. The effectiveness of the proposed frameworks is demonstrated by an illustrative example. Comment: 32 pages; 15 figures; 1 tabl...|$|E
30|$|Tensegrity {{structures}} have {{advantages and}} benefits. A list by Skelton et al. [5] includes: stability, efficiency, deployability, easy tunability, reliability to be modelled, facilitation of high precision control, promoting integration of structure and control, and being inspired by biology. Other features proposed by Komendera [6] include reconfigurability, <b>failure</b> <b>tolerance,</b> simplicity of design, {{and ease of}} modelling.|$|E
40|$|We {{examine whether}} {{tolerance}} for failure spurs corporate innovation {{based on a}} sample of venture capital (VC) backed IPO firms. We develop a novel measure of VC investors ’ <b>failure</b> <b>tolerance</b> by examining their tendency to continue investing in a venture conditional on the venture not meeting milestones. We find that IPO firms backed by more failure-tolerant VC investors are significantly more innovative. A rich set of empirical tests shows that this result is not driven by the endogenous matching between failure-tolerant VCs and startups with high exante innovation potentials. Further, {{we find that the}} marginal impact of VC’s <b>failure</b> <b>tolerance</b> on startup innovation varies significantly in the cross section. Being financed by a failure-tolerant VC is much more important for ventures that are subject to high failure risk, i. e., ventures born in recessions, ventures at early development stages, and ventures in industries in which innovation is difficult to achiev...|$|E
40|$|Cloud {{computing}} is {{the next}} generation computing. The GUI which controls the cloud computing makes is directly controlling the hardware resource and application It {{is the result of}} evolution of on demand service in computing paradigms of large scale distributed computing. It is the adoptable technology as it provides integration of software and resources which are dynamically scalable. These systems are more or less prone to <b>failure.</b> Fault <b>tolerance</b> assesses the ability of a system to respond gracefully to an unexpected hardware or software failure. In order to achieve robustness and dependability in cloud computing, failure should be assessed and handled effectively. This paper aims to provide analysis of fault tolerance techniques used for fault tolerance in Dynamic cloud environments along with some existing model and further compare them on various parameters...|$|R
40|$|Abstract — System- and {{application-level}} failures can {{be characterized}} by mining relevant log files and performing statistical analysis on the provided information. The resulting data may then be used {{in any number of}} future developments and studies on the corresponding computational architecture, including fields such as <b>failure</b> prediction, fault <b>tolerance,</b> performance modelling and power awareness. This paper provides a statistical analysis of the application- and system-level failures encountered and logged by the IBM Blue Gene/L supercomputing system over a six month period. I...|$|R
40|$|Immunological {{tolerance}} to H antigens of Salmonella adelaide may be induced in vitro by {{the exposure of}} mouse spleen cells for 6 hr to an immunogenic dose of polymerized flagellin {{in the presence of}} low concentrations of specific antibody. Such antibody-mediated tolerance requires an optimal antigen: antibody ratio for its induction. A shift in this ratio in favor of the antibody concentration results in <b>failure</b> of <b>tolerance</b> induction and leads to immune suppression commonly known as antibody-mediated feedback inhibition which is not analogous to immunological tolerance. Fragment A of flagellin fails to induce immunological tolerance in vitro. Tolerance to polymerized flagellin may however be induced in vitro, provided the spleen cells are exposed to fragment A in the presence of specific antibody for 6 hr. The results are discussed in the light of current theories of the mechanism of tolerance induction...|$|R
40|$|Recent {{developments}} and current research efforts leading towards realization {{of a large}} scale production wind tunnel Magnetic Suspension and Balance facility are reviewed. Progress {{has been made in}} the areas of model roll control, high angle-of-attack testing, digital system control, high magnetic moment superconducting solenoid model cores, and system <b>failure</b> <b>tolerance.</b> Formal design studies of large scale facilities have commenced and are continuing...|$|E
40|$|A massive {{distributed}} {{storage system}} is the foundation for big data operations. Access latency performance is a key metric in distributed storage systems since it greatly impacts user experience while existing codes mainly focus on improving performance such as storage overhead and repair cost. By generating parity nodes from parity nodes, {{in this paper we}} design new XOR-based erasure codes hierarchical tree structure code (HTSC) and high failure tolerant HTSC (FH_HTSC) to reduce access latency in distributed storage systems. By comparing with other popular and representative codes, we show that, under the same repair cost, HTSC and FH. HTSC codes can reduce access latency while maintaining favorable performance in other metrics. In particular, under the same repair cost, FH. HTSC can achieve lower access latency, higher or equal <b>failure</b> <b>tolerance</b> and lower computation cost compared with the representative codes while enjoying similar storage overhead. Accordingly, FH. HTSC is a superior choice for applications requiring low access latency and outstanding <b>failure</b> <b>tolerance</b> capability at the same time. postprin...|$|E
40|$|Abstract. With {{physical}} attacks {{threatening the}} security of current cryptographic schemes, no security policy can be developed without {{taking into account the}} physical nature of computation. In this article we first introduce the notion of Cryptographic Key <b>Failure</b> <b>Tolerance,</b> then we offer a framework for the determination of upper bounds to the key lifetimes for any cryptographic scheme used in the presence of faults, given a desired (negligible) error-bound to the risk of key exposure. Finally we emphasize the importance of choosing keys and designing schemes with good values of <b>failure</b> <b>tolerance,</b> and recommend minimal values for this metric. In fact, in standard environmental conditions, cryptographic keys that are especially susceptible to erroneous computations (e. g., RSA keys used with CRT-based implementations) are exposed with a probability greater than a standard error-bound (e. g., 2 − 40) after operational times shorter than one year, if the failurerate of the cryptographic infrastructure is greater than 1. 04 × 10 − 16 failures/hours...|$|E
30|$|Fault Prevention and Tolerance. Faults are {{events which}} {{potentially}} may cause a failure. A failure is a malfunction or incorrect output. Thus, fault prevention consists in avoiding events which may cause <b>failures</b> and fault <b>tolerance</b> consists in identifying and managing failures after they occur. To analyze human error in human computation systems, we join together human error concepts from Human Error Theory [23] and concepts {{related to the}} implementation of fault prevention and tolerance in computing system from Human-in-the-Loop [25] and machine-based distributed systems [43] literatures.|$|R
40|$|Abstract—We {{provide a}} {{holistic}} routing and wavelength assignment (RWA) {{solution to a}} 2 -D-torus-based fault-tolerance problem in which an arbitrary bidirectional link (bilink) failure is tolerated. First, we introduce an optimal working lightpath RWA (WRWA) scheme for an N N torus. Then, by observing a uniform pattern by which WRWA arranges working lightpaths through a bilink, we propose a spare lightpath RWA (SRWA) scheme that provisions spare resources to accommodate all traffic originally carried on the bilink subject to failure. The proposed SRWA scheme is at a spare wavelength cost within 1 /N of that for the working lightpaths. We show a factor- 2 performance guarantee for the proposed SRWA scheme. Finally, we derive a polynomial-time resolvable connection reliability expression for the proposed bilink-failure-free scheme and numerical results show a clear evidence of fault-tolerance capacity improvement. Index Terms—Bidirectional link <b>failure,</b> fault <b>tolerance,</b> routing and wavelength assignment, torus, WDM. I...|$|R
40|$|Distributed systems {{includes}} {{a large number}} of processors which increases the risk of <b>failures.</b> Fault <b>tolerance</b> is of a key importance in such systems. Implementing fault tolerant distributed software (FTDS) is a difficult task [2]. Group communication services [8] such as group membership and reliable multicast has been proposed to solve some of the problems in implementing FTDS. In this paper we propose an agreement service build on top of a group communication layer which allows distributed applications to reach agreement. The service, using the underlying group communication layer, ensures that all correct application processes receive the same agreement result or failure notification. The proposed agreement service facilitates the development of FTDS by implementing agreement protocols transparently to the application programmer. 1 Introduction The rapid growth of interconnected networks has led to the increasing use of distributed computing. Distributed systems consists of a nu [...] ...|$|R
