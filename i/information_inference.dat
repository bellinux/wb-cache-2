47|494|Public
50|$|The Provenance Markup Language (abbreviated PML; {{originally}} called Proof Markup Language) is an interlingua for representing {{and sharing}} knowledge about how information {{published on the}} Web was asserted from information sources and/or derived from Web information by intelligent agents. The language was initially developed in support of DARPA Agent Markup Language with a goal of explaining how automated theorem provers (ATP) derive conclusions from a set of axioms. <b>Information,</b> <b>inference</b> steps, inference rules, and agents are the three main building blocks of the language. In {{the context of an}} inference step, information can play the role of antecedent (also called premise) and conclusion. Information can also play the role of axiom that is basically a conclusion with no antecedents. PML uses the broad philosophical definition of agent as opposed to any other more specific definition of agent.|$|E
40|$|We propose an {{intelligent}} document title classification agent {{based on a}} theory of <b>information</b> <b>inference.</b> The information is represented as vectorial spaces computed by a cognitively motivated model, namely Hyperspace Analogue to Language (HAL). A combination heuristic is used to combine a group of concepts into one single combination vector. <b>Information</b> <b>inference</b> can be performed on the HAL spaces via computing information flow between vectors or combination vectors. Based on this theory, a document title is treated as a combination vector by applying the combination heuristic to all the nonstop terms in the title. Two methodologies for learning and assigning categories to document titles are addressed. Experimental results on Reuters- 21578 corpus show that our framework is promising and its performance achieves 71 % of the upper bound (which is approximated by using whole documents) ...|$|E
40|$|How to {{automatically}} capture {{a significant portion}} of relevant background knowledge and keep it up-to-date has been a challenging problem encountered in current research on logic based information retrieval. This paper addresses this problem by investigating various <b>information</b> <b>inference</b> mechanisms based on a high dimensional semantic space constructed from a text corpus using the Hyperspace Analogue to Language (HAL) model. Additionally, the Singular Value Decomposition (SVD) algorithm is considered as an alternative way to enhance the quality of the HAL matrix as well as a mechanism of infering implicit associations. The different characteristics of these inference mechanisms are demonstrated using examples from the Reuters- 21578 collection. Our hope is that the techniques discussed in this paper provide a basis for logic based IR to progress to large scale applications. Keywords: Logic-based Information Retrieval, <b>Information</b> <b>Inference...</b>|$|E
50|$|Many motor tasks exhibit {{adaptation}} to new sensory <b>information.</b> Bayesian <b>inference</b> {{has been most}} commonly studied in reaching.|$|R
40|$|AbstractThe {{literature}} {{in this field}} records many methods and theories of active learning, aimed at fostering critical thinking, strategies for applying the methods of active-participatory methods, methods of stimulating the students ‘creativity, activation methods of the multiple intelligences. All these aspects are studied both {{from the perspective of}} students, teachers, and specialists in researching educational phenomena. This implies knowledge of the problems dealt by specialists in prestigious publications with wide visibility. These concerns follow, in fact, the motivation of students to a bending more interested towards knowledge, for {{a deeper understanding of the}} field of study, an analysis of the critical perspective of the concepts. Learning strategies based on modern methods gives the opportunity to realize the relationships between knowledge, transferring <b>information,</b> <b>inferences,</b> but a cognitive stimulation of independence in learning and an activation of the student's self-confidence...|$|R
40|$|Incorporation {{of expert}} <b>information</b> in <b>inference</b> or {{decision}} settings is often important, especially {{in cases where}} data are unavailable, costly or unreliable. One approach is to elicit prior quantiles from an expert and then to fit these to a statistical distribution and proceed according to Bayes rule. An incentive-compatible elicitation method using an external randomization is available...|$|R
40|$|Humans {{can make}} hasty, but {{generally}} robust judgements {{about what a}} text fragment is, or is not, about. Such judgements are termed <b>information</b> <b>inference.</b> By drawing on theories from non-classical logic and applied cognition, an <b>information</b> <b>inference</b> mechanism is proposed which makes inferences via computations of information flow through a high dimensional conceptual space. Within a conceptual space information is represented geometrically. In this article, an approximation of a conceptual space is employed whereby geometric representations of words are realized as vectors in a high dimensional semantic space, which is automatically constructed from a text corpus. Two approaches were presented for priming vector representations according to context. The first approach uses a concept combination heuristic to adjust the vector representation of a concept {{in the light of}} the representation of another concept. The second approach computes a prototypical concept on the basis of exemplar trace texts and moves it in the dimensional space according to the context. <b>Information</b> <b>inference</b> is evaluated by measuring the effectiveness of query models derived by information flow computations. Results show that information flow contributes significantly to query model effectiveness, particularly with respect to precision. Moreover, retrieval effectiveness compares favourably with two probabilistic query models, and another based on semantic association. More generally, this article can be seen as a contribution towards realizing operational systems which mimic human text-based reasoning. Funded by Cooperative Research Centres Program through the Department of the Prime Minister and Cabinet of Australi...|$|E
30|$|Our {{geocoding}} technique {{falls under}} the category of transductive learning and shares some similarity with “label propagation” [26]. However, unlike label propagation, our labels (latitude/longitude pairs) are continuously valued. Equation 3 exploits this additional structure with geodesic distance and total variation, which has demonstrated superior performance as an optimization heuristic for several <b>information</b> <b>inference</b> problems across a wide variety of fields [27]-[29].|$|E
40|$|Abstract. This article {{investigates the}} {{effectiveness}} of an <b>information</b> <b>inference</b> mechanism on Chinese text. The <b>information</b> <b>inference</b> derives implicit associations via computation of information flow on a high dimensional conceptual space, which is approximated by a cognitively motivated lexical semantic space model, namely Hyperspace Analogue to Language (HAL). A dictionary-based Chinese word segmentation system was used to segment words. To evaluate the Chinese-based information flow model, it is applied to query expansion, in which a set of test queries are expanded automatically via information flow computations and documents are retrieved. Standard recall-precision measures are used to measure performance. Experimental results for TREC- 5 Chinese queries and People Daily’s corpus suggest that the Chinese information flow model significantly increases average precision, though the increase is {{not as high as}} those achieved using English corpus. Nevertheless, there is justification to believe that the HAL-based information flow model, and in turn our psychologistic stance on the next generation of information processing systems, have a promising degree of language independence. ...|$|E
5000|$|Ground {{truth is}} a term used in various fields to refer to {{information}} provided by direct observation (i.e. empirical evidence) as opposed to <b>information</b> provided by <b>inference.</b>|$|R
30|$|The {{intersection}} of GPS-tracked grazing routes with digital land cover data proved very useful to quantify patterns {{of land use}} and herd movement. In view of the utility of such <b>information</b> for <b>inference</b> on rangeland health and livestock nutrition as well as spatio-temporal modelling of livestock distribution, this approach should be a systematic component of quantitative research on pastoral mobility.|$|R
40|$|Phase {{difference}} {{function is}} established {{by means of}} phase transfer function between time domains of source and interference point. The function reveals a necessary interrelation between outcome of two-beam interference, source's frequency and measured subject's kinematic <b>information.</b> As <b>inference</b> unified equations on steady and non-steady interference are derived. Meanwhile relevant property and application are discussed. Comment: 5 pages, 2 figure...|$|R
40|$|Due to {{the rapid}} {{advancement}} of technologies on sensors and processors, engineering systems have become more complex and highly automated to meet ever stringent performance and safety requirements. These systems are usually composed of physical plants (e. g., aircraft, spacecraft, ground vehicles, etc.) and cyber components (e. g., sensing, communication, and computing units), and thus called as Cyber-Physical Systems (CPSs). For safe, efficient, and sustainable operation of a CPS, the states and physical characteristics of the system need to be effectively estimated or inferred from sensing data by proper <b>information</b> <b>inference</b> algorithms. However, due to the complex nature of the interacting multiple-heterogeneous elements of the CPS, the <b>information</b> <b>inference</b> of the CPS is a challenging task, where exiting methods designed for a single-element dynamic system (or for even dynamic systems with multiple-homogenous elements) could not be applicable. Moreover, {{the increasing number of}} sensor resources in CPSs makes the task even more challenging as meaningful information needs to be accurately and effectively inferred from huge amount of data, which is usually noise corrupted. Many aerospace systems such as air traffic control systems, pilot-automation integrated systems, networked unmanned aircraft systems, and space surveillance systems are good examples of CPSs and thus have the aforementioned challenging problems. ^ The goals of this research are to 1) overcome the challenges in complex CPSs by developing new <b>information</b> <b>inference</b> methodologies based on control, estimation, hybrid systems and information theories, and 2) successfully apply them to various complex and safety-critical aerospace systems such as air transportation systems, space surveillance systems, and integrated human-machine systems, to promote their efficiency and safety. ...|$|E
40|$|The {{exponential}} {{explosion of}} web image {{data on the}} Internet has been witnessed {{over the last few}} years. The precise labeling of these images is crucial to effective image retrieval. However, most existing image tagging methods discover the correlations from tag co-occurrence relationship, which leads to the limited scope of extended tags. In this paper, we study how to build a new <b>information</b> <b>inference</b> model over image tag datasets for more effective and complete tag expansion. Specifically, the proposed approach uses modified Hyperspace Analogue to Language (HAL) model instead of association rules or latent dirichlet allocations to mine the correlations between image tags. It takes advantage of context sensitive <b>information</b> <b>inference</b> to overcome the limitation caused by the tag co-occurrence based methods. The strength of this approach lies in its ability to generate additional tags that are relevant to a target image but may have weak co-occurrence relationship with the existing tags in the target image. We demonstrate the effectiveness of this proposal with extensive experiments on a large Flickr image dataset. ...|$|E
40|$|In {{this context}} of {{inference}} proof view processing of XML document in hospital database management aims at treating the inference problem in xml document {{that will provide}} some details to the client or other organization using XML schema for representing its data type. We are proposing an algorithm for weakening of the XML document by eliminating the confidential <b>information,</b> <b>inference</b> capabilities, modifying the schema of XML. The weakened XML document, modified schema conforms to the inference-proof viewed of the generated document to the client...|$|E
25|$|There {{have also}} been other {{attempts}} to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book <b>Information</b> Theory, <b>Inference,</b> and Learning Algorithms, where he emphasizes that a prior bias in favour of simpler models is not required.|$|R
50|$|MacKay's {{contributions}} in machine learning and information theory include {{the development of}} Bayesian methods for neural networks, the rediscovery (with Radford M. Neal) of low-density parity-check codes, and the invention of Dasher, a software application for communication especially popular with those who cannot use a traditional keyboard. He cofounded the knowledge management company Transversal. In 2003, his book <b>Information</b> Theory, <b>Inference,</b> and Learning Algorithms was published.|$|R
50|$|There {{have also}} been other {{attempts}} to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book <b>Information</b> Theory, <b>Inference,</b> and Learning Algorithms, where he emphasizes that a prior bias in favour of simpler models is not required.|$|R
40|$|AbstractThe <b>Information</b> <b>Inference</b> Framework {{presented}} in this paper provides a general-purpose suite of tools enabling the definition and execution of flexible and reliable data processing workflows whose nodes offer application-specific processing capabilities. The IIF is designed for the purpose of processing big data, and it is implemented on top of Apache Hadoop-related technologies to cope with scalability and high-performance execution requirements. As a proof of concept we will describe how the framework is used to support linking and contextualization services {{in the context of the}} OpenAIRE infrastructure for scholarly communication...|$|E
40|$|Semantic-based research: Kyoto Project. In {{the digital}} {{management}} of documentation, {{the use of}} the text itself can be very interesting, in addition to the descriptors. Many descriptors are also text. The use of linguistic engineering techniques opens up new options for accessing information from these databases: multilingual access, semantic grouping, access based on similarity, question-answer systems, <b>information</b> <b>inference,</b> etc. This paper looks in more detail at the possibilities based on semantics, setting out the research areas being developed by the authors as part of the European Kyoto project...|$|E
40|$|Sequence {{data mining}} has many {{interesting}} applications {{in a large}} number of domains including finance, medicine, and business. However, Sequence data often contains sensitive information about individuals and improper release and usage of this data may lead to privacy violation. In this paper, we study the privacy issues in publishing multidimensional sequence data. We propose an anonymization algorithm, using hierarchical clustering and sequence alignment techniques, which is capable of preventing both identity disclosure and sensitive <b>information</b> <b>inference.</b> The empirical results show that our approach can effectively preserve data utility as much as possible, while preserving privacy...|$|E
40|$|Research {{shows that}} people have {{difficulty}} forgetting inferences they make after reading a passage, even when the <b>information</b> that the <b>inferences</b> are based on is later known to be untrue. This dissertation {{examined the effects of}} these inferences on memory for political information and tested if the credibility of the source of the correction influences whether people use the correction, or continue relying on the original <b>information</b> when making <b>inferences.</b> According to source credibility theory, there are two main factors that contribute to credibility, expertise and trustworthiness. Experiment 1 examined credibility as a function of both expertise and trustworthiness. The results from this experiment showed that having a correction from a source who is high on both factors significantly decreased the use of the original information. Experiment 2 examined credibility as a function of expertise. The Experiment 2 results showed no significant decrease in participants' use of the original information, if a correction came from a source that was simply more expert (but not more trustworthy) than another source. This finding suggests that source expertise alone is not sufficient to reduce reliance on the original information. Experiment 3, which examined credibility as a function of trustworthiness, demonstrated that having a highly trustworthy source does significantly decrease the use of the original <b>information</b> when making <b>inferences.</b> This study is the first to provide direct support for the hypothesis that making the source of a correction more believable decreases use of the original discredited <b>information</b> when making <b>inferences...</b>|$|R
40|$|Abstract. The {{growth of}} {{technology}} and sciences has greatly influenced the area of management and decision-making procedures, and has dramatically changed the decision-making processes in different levels, both quantitatively and qualitatively. Knowledge management plays {{a vital role in}} supporting enterprise learning, since it facilitates the effective collective intellect of the enterprise. Different methods for user-friendly knowledge access have been developed previously. The most sophisticated ones provide a simple text box for a query which takes Natural Language (NL) queries as input. Question Answering (QA) system is playing an important role in current search engine optimization. Natural language processing technique is mostly implemented in QA system for asking user‟s question and several steps are also followed for conversion of questions to query form for getting an exact answer. Query languages have complex syntax, requiring a good understanding of the representation schema, including knowledge of details like namespaces, class and property names. In this research we proposed an model to implement Conceptual Question Answering and Automatic <b>Information</b> <b>Inferences</b> for the enterprise's operational knowledge management in ontology-based learning organization...|$|R
40|$|This paper {{seeks to}} reveal the {{semantics}} of geospatial concepts {{as it can be}} elicited from text descriptions. So far, such an approach has dealt with the analysis of existent repositories of geographic information. Taking up with the study of the semantics of geospatial concepts, we turn to the analysis of the linguistic expressions of space and consequently of the “things ” it includes. More specifically, we identify linguistic terms that people use when they locate themselves in space or when they move around it. In addition, we examine how people tend to portray objects positioned in space and which descriptions they use {{to communicate with each other}} about these objects. From this <b>information,</b> <b>inferences</b> can be drawn on what constitutes the semantics of spatial objects and how this is connected to natural language. What is more, the generation of mental maps has employed spatial descriptions due to their expressive power and use of natural language. Consequently, modern cartography has a lot to gain from the semantic analysis of text descriptions of space...|$|R
40|$|Mutual Boosting is {{a method}} aimed at {{incorporating}} contextual information to augment object detection. When multiple detectors of objects and parts are trained in parallel using AdaBoost [1], object detectors might use the remaining intermediate detectors to enrich the weak learner set. This method generalizes the efficient features suggested by Viola and Jones [2] thus enabling <b>information</b> <b>inference</b> between parts and objects in a compositional hierarchy. In our experiments eye-, nose-, mouth- and face detectors are trained using the Mutual Boosting framework. Results show that the method outperforms applications overlooking contextual information. We suggest that achieving contextual integration is a step toward human-like detection capabilities. ...|$|E
40|$|Our {{overarching}} {{issue in}} security {{is the human}} factor – and dealing with it is {{perhaps one of the}} biggest challenges we face today. Human factor is often described as the weakest part of a security system and users are often described as the weakest link in the security chain. In this thesis, we focus on two problems which are caused by human factors in user authentication and propose respective solutions. a) Secrecy <b>information</b> <b>inference</b> attack – publicly available information can be used to infer some secrecy information about the user. b) Coercion attack – where an attacker forces a user to handover his/her secret information such as account details and password. In the secrecy <b>information</b> <b>inference</b> attack, an attacker can use publicly available data to infer secrecy information about a victim. We should be prudent in choosing any information as secrecy information in user authentication. In this work, we exploit public data extracted from Facebook to infer users’ interests. Such interests can also found on their profile pages but such pages are often private. Our experiments conducted on over more than 34, 000 public pages collected from Facebook show that our inference technique can infer interests which are often hidden by users with moderate accuracy. Using the inferred interests, we also demonstrate a secrecy <b>information</b> <b>inference</b> attack to break a preference based backup authentication system BlueMoonTM. To mitigate the effect of secrecy <b>information</b> <b>inference</b> attack, we propose a new authentication mechanism based on user’s cellphone usage data which is often private. The system generates memorable and dynamic fingerprints which can be used to create authentication challenges. In particular, in this work, we explore if the generated behavioral fingerprints are memorable enough to be remembered by end users to be used for authentication credentials. We demonstrate the application of memorable fingerprints by designing an authentication application on top of it. We conducted an extensive user study that involved collecting about one month of continuous usage data from 58 Symbian and Android smartphone users. Results show that the fingerprints generated are remembered by the user to some extent and that they were moderately secure against attacks even by family members and close friends. The second problem which we focus in this thesis is human vulnerability to coercion attacks. In such attacks, the user is forcefully asked by an attacker to reveal the secret/key to gain access to the system. Most authentication mechanisms today are vulnerable to coercion attacks. We present a novel approach in generating cryptographic keys to fight against coercion attacks. Our technique incorporates a measure of user’s emotional status using skin conductance (which changes when the user is under coercion) into the key generation process. A preliminary user study with 39 subjects was conducted which shows that our approach has moderate false acceptance and false rejection rates. Furthermore, to meet the demand of scalability and usability, many real-world authentication systems have adopted the idea of responsibility shifting, where a user’s responsibility of authentication is shifted to another entity, usually in case of failure of the primary authentication method. In a responsibility shifting authentication scenario, a human helper who is involved in regaining access, is vulnerable to coercion attacks. In this work, we report our user study on 29 participants which investigates the helper’s emotional status when being coerced to assist in an attack. Results show that the coercion causes involuntary skin conductance fluctuation on the helper, which indicates that he/she is nervous and stressed. The results from the two studies show that the skin conductance is a viable approach to fight against coercion attacks in user authentication...|$|E
40|$|Abstract—Real-time {{decision-making}} with partial {{information is}} commonplace {{in the daily}} tasks of a construction manager/engineer. However, traditional decision support systems (DSSs) do not support partial <b>information</b> <b>inference.</b> Data pre-processing was adopted to solve the data-incompleteness problem. Unfortunately, such approach may be biased. This paper presents a newly developed neuro-fuzzy system, named Variable-attribute Fuzzy Adaptive Logic Control Network (VaFALCON), for decision-making under partial information environment with the untreated original incomplete data. Two case studies with 91. 7 % and 83. 3 % attribute information were conducted to test the proposed VaFALCON system. It is found that the average accuracy recovery ratio (AARR) is between 90. 57 % and 95. 68 % for testing data with 91. 7 % partial information, and is between 86. 03 % and 93. 67 % for testing data with 83. 3 % partial information. Index Terms—neuro-fuzzy system, real-time decision making, partial information, AI...|$|E
40|$|The {{growth of}} {{technology}} and sciences has greatly influenced the area of management and decision-making procedures, and has dramatically changed the decision-making processes in different levels, both quantitatively and qualitatively. Knowledge management plays {{a vital role in}} supporting enterprise learning, since it facilitates the effective collective intellect of the enterprise. Different methods for user-friendly knowledge access have been developed previously. The most sophisticated ones provide a simple text box for a query which takes Natural Language (NL) queries as input. Question Answering (QA) system is playing an important role in current search engine optimization. Natural language processing technique is mostly implemented in QA system for asking user's question and several steps are also followed for conversion of questions to query form for getting an exact answer. Query languages have complex syntax, requiring a good understanding of the representation schema, including knowledge of details like namespaces, class and property names. In this research we proposed an model to implement Conceptual Question Answering and Automatic <b>Information</b> <b>Inferences</b> for the enterprise's operational knowledge management in ontology-based learning organization. Peer ReviewedPostprint (published version...|$|R
50|$|Horvitz's {{research}} interests span {{theoretical and practical}} challenges with developing systems that perceive, learn, and reason. His contributions include advances in principles and applications of machine learning and <b>inference,</b> <b>information</b> retrieval, human-computer interaction, bioinformatics, and e-commerce.|$|R
40|$|We {{present a}} static {{analysis}} framework for inference of security-related program properties. Within this framework we design and implement ownership, immutability and <b>information</b> flow <b>inference</b> analyses for Java. We perform empirical investigation {{on a set}} of Java components, and {{on a set of}} established security benchmarks. The results indicate that the analyses are practical and precise, and therefore can be integrated in program comprehension tools that support reasoning about software security and software quality. ...|$|R
40|$|Abstract: In {{this paper}} we show how {{to carry out}} robust place {{recognition}} using both near and far information provided by a stereo camera. Visual appearance {{is known to be}} very useful in place recognition tasks. In recent years, {{it has been shown that}} taking geometric information also into account further improves system robustness. Stereo visual systems provide 3 D information and texture of nearby regions, as well as an image of far regions. In order to make use of all this information, our system builds two probabilistic undirected graphs, each considering either near or far <b>information.</b> <b>Inference</b> is carried out in the framework of conditional random fields. We evaluate our algorithm in public indoor and outdoor datasets from the RAWSEEDS project and in an outdoor dataset obtained at the MIT campus. Results show that this combination of information is very useful to solve challenging cases of perceptual aliasing...|$|E
40|$|The {{smart grid}} is a {{promising}} electrical grid paradigm for enhancing flexibility and reliability in power transmission through two-way communications among grid entities. In the smart grid system, {{the privacy of}} usage information measured by individual smart meters has gained significant attention, owing {{to the possibility of}} personal <b>information</b> <b>inference.</b> Moreover, efficient and reliable power provisioning can be seriously impeded through illicit manipulations of aggregated data under the influence of malicious adversaries. Due to such potential risks, it becomes an important requirement for the smart grid to preserve privacy of metering data by secure aggregation and to authenticate the aggregated result in an efficient manner within large scale environments. From this perspective, this paper investigates the current status of security and privacy in smart grid systems and representative state-of-the-art studies in secure aggregation and authentication of metering data for future directions of a smart grid...|$|E
40|$|Detecting hedges {{and their}} scope in natural {{language}} text {{is very important}} for <b>information</b> <b>inference.</b> In this paper, we present a system based on a cascade method for the CoNLL- 2010 shared task. The system composes of two components: one for detecting hedges and another one for detecting their scope. For detecting hedges, we build a cascade subsystem. Firstly, a conditional random field (CRF) model and a large margin-based model are trained respectively. Then, we train another CRF model using the result of the first phase. For detecting the scope of hedges, a CRF model is trained according to the result of the first subtask. The experiments show that our system achieves 86. 36 % F-measure on biological corpus and 55. 05 % F-measure on Wikipedia corpus for hedge detection, and 49. 95 % F-measure on biological corpus for hedge scope detection. Among them, 86. 36 % is the best result on biological corpus for hedge detection. ...|$|E
40|$|Two {{problems}} that dog current microarrays analyses are (i) the relatively arbitrary nature of data preprocessing and (ii) {{the inability to}} incorporate spot quality <b>information</b> in <b>inference</b> except by all-or-nothing spot filtering. In this chapter we propose an approach based on using weights to overcome these two problems. The first approach uses weighted p-values to make inference robust to normalization and the second approach uses weighted spot intensity values to improve inference without any filtering. ...|$|R
40|$|Qualitative {{probabilistic}} networks represent prob-abilistic influences between variables. Due to {{the level}} of representation detail provided, knowledge about influences that hold only in specific contexts cannot be expressed. The results computed from a qualitative network, as a consequence, can be quite weak and uninformative. We extend the ba-sic formalism of qualitative probabilistic networks by providing for the inclusion of context-specific information about influences and show that exploit-ing this <b>information</b> upon <b>inference</b> has the ability to forestall unnecessarily weak results. ...|$|R
40|$|Wyner’s common {{information}} was originally defined {{for a pair}} of dependent discrete random variables. This thesis generalizes its definition in two directions: the num-ber of dependent variables can be arbitrary, so are the alphabets of those random variables. New properties are determined for the generalized Wyner’s common in-formation of multiple dependent variables. More importantly, a lossy source coding interpretation of Wyner’s {{common information}} is developed using the Gray-Wyner network. It is established that the common information equals to the smallest com-mon message rate when the total rate is arbitrarily close to the rate distortion function with joint decoding if the distortions are within some distortion region. The application of Wyner’s common <b>information</b> to <b>inference</b> problems is also explored in the thesis. A central question is under what conditions does Wyner’s common information capture the entire <b>information</b> about the <b>inference</b> object. Under a simple Bayesian model, it is established that for infinitely exchangeable random variables that the common information is asymptotically equal to the information o...|$|R
