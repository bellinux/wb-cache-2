5|16|Public
5000|$|<b>Discover</b> <b>metadata</b> of {{the source}} database, {{including}} value patterns and distributions, key candidates, foreign-key candidates, and functional dependencies ...|$|E
40|$|Abstract Prefetching is an {{effective}} technique for improving file access performance, which can reduce access latency for I/O systems. In distributed storage system, prefetching for metadata files is critical for the overall system performance. In this paper, an Affinitybased Metadata Prefetching (APM) scheme is proposed for metadata servers in large-scale distributed storage systems to provide aggressive metadata prefetching. Through mining useful information about metadata assesses from past history, AMP can <b>discover</b> <b>metadata</b> file affinities accurately and intelligently for prefetching. Compared with LRU {{and some of the}} latest file prefetching algorithms such as NEXUS and C-miner, trace-driven simulations show that AMP can improve the hit rates by up to 12 %, 4. 5 % and 4 %, respectively, whil...|$|E
40|$|Prefetching is an {{effective}} technique for improving file access performance, which can reduce access latency for I/O systems. In distributed storage system, prefetching for metadata files is critical for the overall system performance. In this paper, an Affinity-based Metadata Prefetching (APM) scheme is proposed for metadata servers in large-scale distributed storage systems to provide aggressive metadata prefetching. Through mining useful information about metadata assesses from past history, AMP can <b>discover</b> <b>metadata</b> file affinities accurately and intelligently for prefetching. Compared with LRU {{and some of the}} latest file prefetching algorithms such as NEXUS and C-miner, trace-driven simulations show that AMP can improve the hit rates by up to 12 %, 4. 5 % and 4 %, respectively, while reduce the average response time by up to 60 %, 12 % and 8 %, respectively...|$|E
40|$|We {{propose a}} new {{supervised}} topic model {{that uses a}} nonparametric density estimator to model the distribution of real-valued metadata given a topic. The model is similar to Topics Over Time, but replaces the beta distributions used in that model with a Dirichlet process mixture of normals. The use of a nonparametric density estimator allows for the fitting of a greater class of metadata densities. We compare our model with existing supervised topic models in terms of prediction and show that {{it is capable of}} <b>discovering</b> complex <b>metadata</b> distributions in both synthetic and real data. ...|$|R
40|$|Abstract: Over the past several years the large scale digital library service has undergone enormous popularity. Arco project is a digital library storage project in Portuguese National library. To a digital library storage system like ARCO system,    there are several challenges,  such as the availability of petascale storage,  {{seamless}} spanning of storage cluster,  administration and utilization of distributed storage and computing resources, safety and stability of data transfer,  scalability of the whole system,  automatic discovery and monitoring of metadata,   etc. Grid computing   appears as an effective technology   coupling  geographically  distributed resources for solving large scale problems in the wide area or local area network. The ARCO system has been developed on the Grid computational infrastructure,  and on the basis of various other toolkits,  such as PostgreSQL,  LDAP,  Apache http server. Main developing languages are C,  PHP,  and Perl. In this paper, we discuss the logical structure sketch of the digital library ARCO system,  resources organization,  <b>metadata</b> <b>discovering</b> and utilising,  system operation details and some operations examples,  and the solution of large file transfer problem in Globus grid toolkit...|$|R
5000|$|A {{principal}} {{purpose of}} metadata {{is to help}} users find relevant information and <b>discover</b> resources. <b>Metadata</b> also helps to organize electronic resources, provide digital identification, and support the archiving and preservation of resources. Metadata assists users in resource discovery by [...] "allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information." [...] Metadata of telecommunication activities including Internet traffic is very widely collected by various national governmental organizations. This data {{is used for the}} purposes of traffic analysis and can be used for mass surveillance.|$|R
40|$|A {{wealth of}} data and {{services}} {{are available on the}} Web, and often have geographical context as well. But the vast quan-tity of offered geospatial information is rather difficult to explore, and its quality hard to assess, due to lack of suf-ficient metadata. Hence, the Open Geospatial Consortium has specified application profiles for publishing, accessing, and searching over collections of spatial metadata with stan-dardized Catalogue Services for the Web (CSW). Unfortu-nately, existing spatial metadata remain largely unexploited by Semantic Web technologies. In this paper, we introduce TripleGeo-CSW, a middleware {{that can be used to}} <b>discover</b> <b>metadata</b> from existing CSWs through a virtual SPARQL endpoint. Acting as broker between a request (in SPARQL) and catalogue services (in XML/GML), this platform can provide on-the-fly information (as RDF triples) on available geodata according to multiple, user-specified criteria (e. g., area of interest, date of last update, keywords). As a proof of concept, we have set up an instance of this middleware against CSWs from public authorities across Europe, which involve datasets complying with the EU INSPIRE Directive. Our experience testifies that TripleGeo-CSW can assist stake-holders to repurpose existing CSWs with minimal overhead and readily expose spatial metadata on the Semantic Web...|$|E
40|$|Why: The task of {{describing}} something that already exists was always {{considered to be}} a major challenge because of the important role of documentation and archiving of any historic, archaeological or architectural entity. Their continuous metamorphose, a material or a spatial change, requires to keep into account the fourth dimension of time. The up-to-date surveying techniques and continuous methodology development enable us to almost fully describe any study case considered, through the creation of highly faithful models of reality. The aim of the research was to make an overview of the possibilities, limits and further perspective of photogrammetric disciplines in communicating cultural heritage concentrated on the representation and information communication, often seen as the end product and not as a mean for further dialogue and interpretation. The objective was therefore to produce a valid support to planners involved in restoration and the programming of future destination of the complex. The approach of using different survey techniques has produced a large amount of data in different formats that needed to undergo phases of processing and organization in order to obtain sets of products useful for the professional restaurateurs. Where or Context: The study case is a large complex of Villa Reale in Monza, Italy. Unlike the famous park that hosts Formula 1 Gran Prix circuit, the Villa suffers from a lack of popularity by the general public - this fact is actually an obstacle in achieving the sustainability of an important economic intervention such as restoration, enhancement, preservation and future use, a policy today strongly supported by the Lombardy region, a partial owner of the complex. The memory of Villa Reale as a residence is in fact lost – it was in particular place for summer leisure (1777 - 1814), strongly desired and beloved by Archduke Ferdinand of Habsburg Lorraine, the favorite son of Maria Theresa of Austria, who in 1771 was appointed governor of Milan. It was also a year in which he married Maria Beatrice D'Este and a historical moment of great development for the entire 'Lombardo Veneto' {{on the spur of the}} reforms of Maria Theresa of Austria. The project of the Villa as we see it today was done in 1777 by Italian architect Giuseppe Piermarini (1734 - 1808), at first a countryside villa of the Austrian royal family. During the period of French predominance the complex was expanded and the park was created. In 1859 the Villa becomes the property of Savoy family and it mainly serves as a hunting residence, the interior rooms and the apartments were also used and their interior enriched. In the 1934 Vittorio Emanuele III has donated the complex to the city of Monza. Today the Villa is jointly administered by communes Monza and Milano. The project of the overall survey of the complex has begun in 2003 within the “International Design Competition for the Refurbishment and Enhancement of the Villa Reale in Monza and its Gardens” called by the Lombardy Regional Government and the Monza Municipality, pro-quota owners of the Villa Reale in Monza complex that covers a surface of about 36, 000 square meters, thus being the largest fenced park in Europe. When: Close-range photogrammetry and laser scanning survey technologies were performed to deliver high precision detailed models, used for the heritage digitalization and virtualization. All architectonical elements are represented by geometric drawings, rectified images and orthophotos (up to a 2 mm ground pixel resolution). Orthophoto construction was in some cases aided by surface models obtained from laser scanner data, confronting difficulties in the area of Object Recognition and Reconstruction (ORR), typical of complex surfaces common for heritage scenarios. Until this year (2010), topographic, photogrammetric and laser scanner is done for a total of 71 rooms of the Central building block (I and II Nobile floors) and The North Wing (I Nobile floor). The deliverables of the survey for every room were: connection to the local topographic network; single photogrammetric rectified images (for all facades and interior walls) and orthophotos (of domes and vaults of the rooms atrium, dance hall etc.); photogrammetric products using the layout of geometrical drawings (box-system); point cloud three-dimensional model; a solid CAD model; solid CAD model, texturized using geo-referenced photogrammetric images. As far as representation is regarded, it was decided to follow the methodology (a box-system representation) of the Restoration school of Politecnico di Milano (scientific responsible Prof. Della Torre), used for geometrical survey, material mapping and condition analysis, done for about 355 rooms of the Villa Reale. Main objectives of the study were on one side to improve remote access to 3 D data that in the future could support advanced programs of preventive conservation and guarantee a sustainable intervention and maintenance; on the other hand, most suitable dissemination methods were explored to facilitate knowledge diffusion to public users through a web vehicle. A management model that fully follows the monuments’ life-cycle was conceived as a methodological and operative work environment capable to continuously inherit the information even after the planning phase. The model proposed is based on a concept that sees every element as a face of a cube – every room was “broken down” into six single elements (walls, pavements and vaults). Every face appears as a reality on its own but it is also in a spatial relationship with others, with no restrictions on number of faces or their shape. Being in the same reference system, conservation data can easily be linked, “over imposed” to survey material and examined in a geo-referenced environment for history of damage and other studies. Effectiveness: A Virtual Museum prototype developed using the vast collection of survey data brings closer the monument to general public, but also offers a remote data consultation tool to professional users. A particular attention was given to a visualization tool, based on a concept analogue to geo-portals. The technical architecture of the web service offered, compliant with most recent Open Geospatial Consortium standards, includes metadata, spatial data sets, and network services within a layered architecture: User interface layer (applications and Geoportals), Service layer and Data Sources layer. A catalogue service is still not available but its development is foreseen, as it is an essential element for any dynamic database interaction. The connection between the catalogue and the graphics will be assured with respect to discovery (Get Discovery Service Metadata and <b>Discover</b> <b>Metadata)</b> and view operations (Get View Service Metadata and Get Map), required by Geo-portals for spatial information. The first prototype version did not contain any spatial reference but recent implementation permitted to consult all survey material in geo-referenced environments: the user is enabled to track down the coordinates of any point within the single element and hence make measurements of distances or areas. Object coordinates are in a spatial relationship and coherence, connecting all the elements into a same spatial reference system – a “box-system”. Together with visualization tool, metadata developed for all elements are the central instrument that will allow the interconnection between the spatial geometrical data and the conservation data. 3 D model representation of cultural heritage is in general often limited to simplifying objects into planes or elements with no full respect of their inter-relationship. Future work could meet the needs of an intelligent model with a dynamic (geo) database with a flexible reference system, able to correlate properties and coordinates of all elements, establishing an intelligent dialogue. A permanent and consistent bilateral information flux using web-service could be essential for interdisciplinary collaboration in remote, but also for public awareness rising, guidelines definition and policy making. status: publishe...|$|E
40|$|The CORES {{metadata}} schemas registry {{is designed}} to enable users to <b>discover</b> and navigate <b>metadata</b> element sets. The paper reflects {{on some of the}} experiences of implementing the registry, and examines some of the issues of promoting such services {{in the context of a}} "partially Semantic Web" where metadata applications are evolving and many have not yet adopted the RDF model...|$|R
40|$|An {{increasing}} number of datasets from government, public organizations and institutions are published as open data. Metadata that describes them, are cataloged at central places to enable a better access to these datasets. Quantifying the metadata quality can help to measure the efficiency of a catalog and <b>discover</b> low-quality <b>metadata</b> records which prevent the user from finding what she is looking for. We researched and implemented a range of metrics {{from the field of}} metadata quality assessment as part of an open data platform. This paper describes the platform that automatically assesses the quality of different open government data portals using the CKAN catalog software. The results are aggregated and visualized through a web application in order to establish a continuous and sustainable monitoring service...|$|R
50|$|Singingfish {{employed}} its own web crawler, Asterias, {{designed specifically}} to ferret out audio and video links across the web. In 2003 and 2004, Asterias discovered {{an average of about}} 50,000 new pieces of multimedia content a day. A proprietary system was used to process each of the <b>discovered</b> links, extracting <b>metadata</b> and then enhancing it prior to indexing as much multimedia content on the web has little or poor metadata. Many of the multimedia URLs used as seeds for Singingfish's crawlers and annotation engines came from cache logs from the NSF-funded National Laboratory for Applied Network Research (NLANR) IRCache Web Caching project.|$|R
40|$|Researchers have {{traditionally}} used bibliographic data-bases {{to search out}} information. Today, the full-text of resources is increasingly available for searching, and more researchers are performing full-text searches. This study compares differences {{in the number of}} articles <b>discovered</b> between <b>metadata</b> and full-text searches of the same literature cohort when searching for gene names in two biomedical literature domains. Three re-viewers additionally ranked 100 articles in each domain. Significantly more articles were discovered via full-text searching; however, the precision of full-text searching also is significantly lower than that of metadata search-ing. Certain features of articles correlated with higher relevance ratings. A significant feature measured was the number of matches of the search term in the full-text of the article, with a larger number of matches having a statistically significant higher usefulness (i. e., rele-vance) rating. By using the number of hits of the search term in the full-text to rank the importance of the article, performance of full-text searching was improved so that both recall and precision were as good as or better than that for metadata searching. This suggests that full-text searching alone may be sufficient, and that metadata searching as a surrogate is not necessary...|$|R
40|$|Abstract. This paper {{describes}} the operational characteristics of “CompTorrent”, a general purpose distributed computing platform {{that provides a}} low entry cost to creating new distributed computing projects. An algorithm is embedded into a metadata file along with data set details which are then published on the Internet. Potential nodes <b>discover</b> and download <b>metadata</b> files for projects they wish to participate in, extract the algorithm and data set descriptors, and join other participants in maintaining a swarm. This swarm then cooperatively shares the raw data set in pieces between nodes and applies the algorithm to produce a computed data set. This computed data set is also shared and distributed amongst participating nodes. CompTorrent allows a simple, “home-brewed ” solution for small or individual distributed computing projects. Testing and experimentation have shown CompTorrent {{to be an effective}} system that provides similar benefits for distributed computing to those BitTorrent provides for large file distribution...|$|R
40|$|Abstract. Current dataintegration systems allow avariety of {{heterogeneous}} structured or semi-structured {{data sources}} {{to be combined}} and queried by providing an integrated view over them. The Semantic Web also requires {{us to be able}} to integrate information from a variety of heterogeneous information sources. However, these information sources will also include natural language (e. g. web pages) and ontologies. In this paper we describeanarchitecture which combines the data integration approach with techniques from Information Extraction in order to allow information from ontologies and natural language sources to be integrated with other, semantically related, structured or semi-structured data. Our architecture is based on the AutoMed data integration system, and in this paper we describe several extensions which have been made to AutoMed in order to support this work, including adding RDF to the data sources supported by AutoMed and providing a repository for the data and <b>metadata</b> <b>discovered</b> by the Information Extraction process. ...|$|R
40|$|Flood {{protection}} {{is one of}} several disciplines where geospatial data is very important and is a crucial component. Its management, processing and sharing form the foundation for their efficient use; therefore, special attention is required in the development of effective, precise, standardized, and interoperable models for the discovery and publishing of data on the Web. This paper describes the design of a methodology to discover Open Geospatial Consortium (OGC) services on the Web and collect descriptive information, i. e., metadata in a geocatalogue. A pilot implementation of the proposed methodology - Geocatalogue of geospatial information provided by OGC services discovered on Google (hereinafter “Geocatalogue”) - was used to search for available resources relevant to the area of flood protection. The result is an analysis of the availability of resources <b>discovered</b> through their <b>metadata</b> collected from the OGC services (WMS, WFS, etc.) and the resources they provide (WMS layers, WFS objects, etc.) within the domain of flood protection...|$|R
40|$|Current data {{integration}} systems allow {{a variety of}} heterogeneous structured or semi-structured data sources to be combined and queried by providing an integrated view over them. The Semantic Web also requires {{us to be able}} to integrate information from a variety of heterogeneous information sources. However, these information sources will also include natural language (e. g. web pages) and ontologies. In this paper we describe an architecture which combines the {{data integration}} approach with techniques from Information Extraction in order to allow information from ontologies and natural language sources to be integrated with other, semantically related, structured or semi-structured data. Our architecture is based on the AutoMed data integration system, and in this paper we describe several extensions whichhave been made to AutoMed in order to support this work, including adding RDF to the data sources supported by AutoMed and providing a repository for the data and <b>metadata</b> <b>discovered</b> by the Information Extraction process...|$|R
40|$|Metadata is the {{important}} information describing geographical data resources and their key elements. It is used to guarantee the availability and accessibility of the data. ISO 19115 is a metadata standard for geographical information, making the geographical metadata shareable, retrievable, and understandable at the global level. In order {{to cope with the}} massive, high-dimensional and high-diversity nature of geographical data, data mining is an applicable method to <b>discover</b> the <b>metadata.</b> This thesis develops and evaluates an automated mining method for extracting metadata from the data environment on the Local Area Network at the National Land Survey of Sweden (NLS). These metadata are prepared and provided across Europe according to the metadata implementing rules for the Infrastructure for Spatial Information in Europe (INSPIRE). The metadata elements are defined according to the numerical formats of four different data entities: document data, time-series data, webpage data, and spatial data. For evaluating the method for further improvement, a few attributes and corresponding metadata of geographical data files are extracted automatically as metadata record in testing, and arranged in database. Based on the extracted metadata schema, a retrieving functionality is used to find the file containing the keyword of metadata user input. In general, the average success rate of metadata extraction and retrieval is 90. 0 %. The mining engine is developed in C # programming language on top of the database using SQL Server 2005. Lucene. net is also integrated with Visual Studio 2005 to build an indexing framework for extracting and accessing metadata in database...|$|R
40|$|Improvements and {{portability}} {{of technologies}} and smart devices has enabled {{rapid growth in}} the amount of user generated media such as photographs and videos. Whilst various media generation and management systems exist it still remains a challenge to <b>discover</b> the "right" <b>metadata</b> information for the right purpose. This paper describes an approach to extract relevant geospatial information through reverse geocoding in addition to cross-referencing several public geospatial data sources. The extracted geospatial information can be used to enable enrichment of media with rich semantic geo-metadata and therefore enable improved organisation and searching of the media. Central to the system is the cross-referencing and data fusion of several public geospatial datasets to determine the most relevant Points of Interest and extract their relevant features. These relevant Points of Interest and features are subsequently used to annotate media with human readable information, leading to enriched media repositories. The system has been implemented as a client/server architecture, with a web interface for the client front end and Java for the backend server side processing. Details of the implementation are discussed. Testing in a scenario has been undertaken and a discussion of the testing technique and results is presented. The initial results show the system to be effective at fusing several public geospatial datasets and extracting relevant geo-metadata...|$|R
40|$|We {{sought to}} {{demonstrate}} the effectiveness of techniques to index radiology images using <b>metadata</b> <b>discovered</b> in their free-text figure captions. The ARRS GoldMiner™ image library incorporated 94, 256 figures from 11, 712 articles published in peer-reviewed online radiology journals. Algorithms were developed to discover metadata—age, sex, and imaging modality—from the figures’ free-text captions. Age was recorded in years, and was classified as infant (less than 2  years), child (2 to 17  years), or adult (18 + years). Each figure was {{assigned to one of}} eight imaging modalities. A random sample of 1, 000 images was examined to measure accuracy of the metadata. The patient’s age was identified in 58, 994 cases (63 %), and the patient’s sex was identified in 58, 427 cases (62 %). An imaging modality was assigned to 80, 402 (85 %) of the figures. Based on the 1, 000 sampled cases, recall values for age, sex, and imaging modality were 97. 2 %, 99. 7 %, and 86. 4 %, respectively. Precision values for age, sex, and imaging modality were 100 %, 100 %, and 97. 2 %, respectively. Automated techniques can accurately discover age, sex, and imaging modality metadata from captions of figures published in radiology journals. The metadata can be used to dynamically filter queries for an image search engine...|$|R
40|$|When lawyers {{receive a}} {{document}} — whether hard copy or an electronic document — {{that they know}} the adversary sent them inadvertently (for example, a fax or email mistakenly sent to an adversary lawyer instead of to co-counsel), the black letter rule in Rule 4. 4 requires the lawyer to notify the other side. However, this Rule {{does not require the}} receiving lawyer to return the document unread. Whether the receiving lawyer can use that document depends, in essence, on the law of evidence. If the court decides that the document lost its privileged status (perhaps because the sending lawyer acted unreasonably), the receiving lawyer can use the document. In some cases, the sending lawyer sends a document advertently (for example, in response to a discovery request). In that situation as well, the lawyer should {{be able to use the}} document, unless some other law, such as the law of evidence, says otherwise. For example, the sending lawyer advertently sent over a large group of documents, one of which is a privileged document that the sending lawyer did not intend to disclose. In many cases, courts will hold that the document in question remains privileged if the sending lawyer acted reasonably. Hence, the receiving lawyer should not use that document in that particular circumstance. As stated in Formal Opinion 05 - 437, “Rule 4. 4 (b) thus only obligates the receiving lawyer to notify the sender of the inadvertent transmission promptly. The rule does not require the receiving lawyer either to refrain from examining the materials or to abide by the instructions of the sending lawyer. ”The Restatement (Third) of the Law Governing Lawyers also concludes, it “is not a violation [of legal ethics] to accept the advantage of inadvertent, and even negligent, disclosure of confidential information by the other lawyer, if the effect of the other lawyer’s action is to waive the right of that lawyer’s client to assert confidentiality. ” Moreover, if the receiving lawyer may use the document, the receiving lawyer should be able to examine and use all of the information within the document, including information embedded within the document. That information may be embedded in a hard copy document (such as fingerprints, the age or type of the paper), or it may be embedded in a digital document (metadata). If the sending lawyer does not want the receiving lawyer to look at metadata, he or she should not send it. As ABA Formal Opinion 06 - 442 advised, the Model Rules do not prohibit lawyers from <b>discovering</b> and using <b>metadata</b> found in documents that other lawyers transmit to them, even though the other lawyer may not know that the electronic version of the document contains metadata. However, in 2012, the ABA added a new Comment to Rule 4. 4, which provides that Rule 4. 4 creates an obligation on the receiving lawyer “to promptly notify the sender” only when “the receiving lawyer knows or reasonably should know that the metadata was inadvertently sent to the receiving lawyer. ” The concept of sending metadata inadvertently is peculiar when applied to a lawyer who advertently intends to turn over the particular document that contains the metadata. One wonders how the receiving lawyer should know that the sending lawyer sent the document advertently but sent the metadata “inadvertently,” when the sending lawyer turns over the digital document pursuant to discovery and does not claim that the document itself (as opposed to the metadata within it) was sent by accident. After all, no ethics Rule requires the receiving lawyer “to promptly notify” the sending lawyer that an analogue document has other data embedded within it — such as fingerprints, the age of the paper, and the age of the ink on the paper. In the midst of these Opinions, language in the Comments to ABA Model Rule 4. 4, which the ABA House of Delegates added in 2012, confuses the matter. The Comments suggest that metadata in a digital document has an exalted position, in contrast to analogue data in a hard copy. The Report to the ABA House of Delegates did not make clear that it is exalting metadata or intending to overrule any ABA Formal Opinion. Yet, the changes in the Comments to Rule 4. 4 treat metadata differently. There is no justification for treating metadata differently, as this paper explains. Given past history, many courts will simply refuse to follow the ABA’s lead on this issue, just as they have refused to follow the ABA’s efforts to ban non-misleading lawyer participation in internet chat rooms. Other courts may adopt these Comments, but may interpret them to mean very little by always concluding that the sending lawyers always meant to include the metadata on electronic documents that they voluntarily turned over to the other side...|$|R

