939|724|Public
2500|$|In the table, [...] {{corresponds}} to {{the total number of}} responders across groups, and N to the total number of patients recruited into the trial. The dots denote corresponding <b>frequency</b> <b>counts</b> of no further relevance.|$|E
2500|$|In {{practice}} {{it is necessary}} to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. [...] The reason is that models derived directly from the n-gram <b>frequency</b> <b>counts</b> have severe problems when confronted with any n-grams that have not explicitly been seen before – the zero-frequency problem. [...] Various smoothing methods are used, from simple [...] "add-one" [...] (Laplace) smoothing (assign a count of 1 to unseen n-grams; see Rule of succession) to more sophisticated models, such as Good–Turing discounting or back-off models. [...] Some of these methods are equivalent to assigning a prior distribution to the probabilities of the n-grams and using Bayesian inference to compute the resulting posterior n-gram probabilities. [...] However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.|$|E
5000|$|... <b>frequency</b> <b>counts</b> of {{sentence}} types, words, sentences, {{turns of}} talk, syllables ...|$|E
5000|$|FC (<b>Frequency</b> <b>Count)</b> : For {{each item}} {{maintain}} a <b>frequency</b> <b>count</b> {{of the number}} of accesses to it - when an element is accessed increase its <b>frequency</b> <b>count</b> and reorder the list in the decreasing order of frequencies.|$|R
5000|$|Document lexical {{analysis}} (e.g. word <b>count,</b> word <b>frequency</b> <b>count,</b> phrase count) ...|$|R
40|$|Abstract — Proteins are {{involved}} in many essential processes within cell. Uncovering the diverse function of proteins and their interactions within the cell may improve our understanding of protein functions. Several high-throughput techniques employed to decipher PPI are erroneous and are limited {{by the lack of}} coverage. Computational techniques are therefore sought to predict genome-wide PPI. In this paper, domain structure is used as a feature for computational prediction of PPI and support vector machines (SVM) as a learning system. We have used both, existing method and <b>frequency</b> <b>count</b> (FC) method for feature representation of protein domains and carried our experiment using SVM with different kernels. Both the methods achieved accuracy of about 78 % for RBF kernel. But <b>frequency</b> <b>count</b> method reduced the storage requirement by half. These results indicate that PPI can be predicted from domain structure using <b>frequency</b> <b>count</b> method with reliable accuracy and reduced storage requirement. Keywords- Protein-protein interactions, Support Vector Machines, Domain structure, <b>Frequency</b> <b>Count</b> Method...|$|R
5000|$|Data {{analysis}} services are offered by firms, {{also known as}} tab houses, that specialize in computer analysis of quantitative data such as those obtained in large surveys. Initially most data analysis firms supplied only tabulations (<b>frequency</b> <b>counts)</b> and cross tabulations (<b>frequency</b> <b>counts</b> that describe two or more variables simultaneously). With the proliferation of software, many firms now {{have the capability to}} analyze their own data, but, data analysis firms are still in demand.|$|E
5000|$|For a given list of symbols, {{develop a}} {{corresponding}} list of probabilities or <b>frequency</b> <b>counts</b> {{so that each}} symbol’s relative frequency of occurrence is known.|$|E
5000|$|Divide {{the list}} into two parts, {{with the total}} <b>frequency</b> <b>counts</b> of the left part being {{as close to the}} total of the right as possible.|$|E
3000|$|... [...]). However, for {{categorical}} data {{such as a}} set {{of biological}} species abundance <b>frequencies</b> (<b>counts)</b> (n [...]...|$|R
30|$|In Map Reduce model-mappers computes all {{the suffixes}} {{along with their}} <b>frequency</b> <b>count</b> and reducer {{constructs}} final probabilistic generalized suffix tree (PGST).|$|R
5000|$|CLAN {{provides}} all {{the basic}} tools of corpus analysis such as key-word and line, concordance, <b>frequency</b> <b>counting,</b> partial regular expression search, and so on.|$|R
5000|$|Write Assist: Users {{can enter}} in {{any type of}} text and {{discover}} a variety of writing tips, including word <b>frequency</b> <b>counts,</b> repeated words, alternate word suggestions, and a usage analysis.|$|E
5000|$|Collostructional {{analysis}} requires {{frequencies of}} words and constructions and {{is similar to a}} wide variety of collocation statistics. It differs from raw <b>frequency</b> <b>counts</b> by providing not only observed co-occurrence frequencies {{of words and}} constructions, but also ...|$|E
50|$|Multiple raters made <b>frequency</b> <b>counts</b> of {{sentences}} containing synonyms {{for a number}} of values identified by Rokeach, including freedom and equality, and Rokeach analyzed these results by comparing the relative frequency rankings of all the values {{for each of the four}} texts: In excerpts from...|$|E
30|$|Rule 1 : If {{the number}} of samples in the {{selected}} layer is sufficient, the <b>frequency</b> <b>counting</b> is strictly conducted on the corresponding data collected from the questionnaires.|$|R
2500|$|Since {{the value}} of [...] that defines the left tail or right tail event is a random variable, this makes the p-value a [...] {{function}} of [...] and a random variable in itself defined uniformly over [...] interval, assuming [...] is continuous. Thus, the p-value is not fixed. This implies that p-value cannot be given a <b>frequency</b> <b>counting</b> interpretation since the probability has to be fixed for the <b>frequency</b> <b>counting</b> interpretation to hold. In other words, if the same test is repeated independently bearing upon the same overall null hypothesis, it will yield different p-values at every repetition. Nevertheless, these different p-values can be combined using Fisher's combined probability test. It should further be noted that an instantiation of this random p-value can still be given a <b>frequency</b> <b>counting</b> interpretation {{with respect to the}} number of observations taken during a given test, as per the definition, as the percentage of observations more extreme than the one observed under the assumption that the null hypothesis is true.|$|R
5000|$|Probability update: The {{selected}} context {{model is}} updated {{based on the}} actual coded value (e.g. if the bin value was [...] "1", the <b>frequency</b> <b>count</b> of [...] "1"s is increased).|$|R
5000|$|The trial can be {{summarized}} and analyzed {{in terms of the}} following contingency table.In the table, [...] corresponds to the total number of responders across groups, and N to the total number of patients recruited into the trial. The dots denote corresponding <b>frequency</b> <b>counts</b> of no further relevance.|$|E
50|$|The general {{principle}} behind quantum state tomography {{is that by}} repeatedly performing many different measurements on quantum systems described by identical density matrices, <b>frequency</b> <b>counts</b> {{can be used to}} infer probabilities, and these probabilities are combined with Born's rule to determine a density matrix which fits the best with the observations.|$|E
50|$|UKDA StatServe was {{a service}} {{provided}} by the UK Data Archive (2008-2010) for creating tailor-made figures and tables from {{the data from the}} UK Data Archive’s catalogue.This service produced statistics (including <b>frequency</b> <b>counts</b> and cross tabulations using either single datasets or more complex time series) for customers who are {{not in a position to}} analyze the underlying data themselves.|$|E
50|$|These tasks {{consist of}} simple control-flow and string {{manipulation}} mechanisms that cover {{a lot of}} common usages like searching and replacing string in files, or counting occurrences of strings (<b>frequency</b> <b>counting).</b>|$|R
40|$|<b>Frequency</b> <b>Count</b> (FC) {{algorithm}} is considered as the static optimal algorithm for the list accessing problem. In this paper, {{we have made}} a study of FC algorithm and explore its limitation. Using the concept of weak look ahead, we have proposed a novel Variant of <b>Frequency</b> <b>Count</b> (VFC) list accessing algorithm. We have evaluated the performance of FC and our proposed VFC algorithm experimentally using input data set from Calgary Corpus. Our experiments show that for all request sequences and list generated from the above data set VFC performs better than FC. Comment: 4 pages, Proceedings of International Conference on Recent Advances in Engineering and Technology, Hyderabad, India, April, 201...|$|R
50|$|In 1987, Yeubrey and {{musician}} Robin George of {{the band}} Magnum invented the onboard 'compact tuner', which allowed stringed instruments to include built-in <b>frequency</b> <b>counting</b> tuning device (Patent EP0269652). The Yeubrey/George form of tuning is now a worldwide standard.|$|R
5000|$|... where [...] is the {{probability}} of class [...] generating the term [...] This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model {{is not the same}} as a multinomial NB classifier with <b>frequency</b> <b>counts</b> truncated to one.|$|E
50|$|The {{quality of}} the data should be checked as early as possible. Data quality can be {{assessed}} in several ways, using different types of analysis: <b>frequency</b> <b>counts,</b> descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms, n: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.|$|E
50|$|Educational {{psychologist}} Edward Thorndike of Columbia University noted that, in Russia and Germany, teachers used word <b>frequency</b> <b>counts</b> {{to match}} books to students. Word skill {{was the best}} sign of intellectual development, and the strongest predictor of reading ease. In 1921, Thorndike published Teachers Word Book, which contained the frequencies of 10,000 words. It {{made it easier for}} teachers to choose books that matched class reading skills. It also provided a basis for future research on reading ease.|$|E
40|$|A common {{approach}} to unsupervised relation extraction builds clusters of patterns express-ing the same relation. In {{order to obtain}} clus-ters of relational patterns of good quality, we have two major challenges: the semantic rep-resentation of relational patterns and the scal-ability to large data. In this paper, we ex-plore various methods for modeling the mean-ing of a pattern and for computing the similar-ity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate <b>frequency</b> <b>counting</b> and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate <b>frequency</b> <b>counting</b> and dimen-sion reduction not only speeds up similarity computation but also improves the quality of pattern vectors. ...|$|R
3000|$|... where Or,c is the {{observed}} <b>frequency</b> <b>count</b> in population r for level c of the categorical variable. H 0 should be rejected at the significance level α {{every time the}} value of the test statistic determined in Equation 7 is greater than [...]...|$|R
3000|$|... attack model, but {{confined}} to items. Authors in [34] assume the attacker knows exact frequency of single items, similarly to us. Both [18] and [34] use similar privacy model as ours, which requires that each real item {{must have the}} same <b>frequency</b> <b>count</b> as [...]...|$|R
5000|$|Along {{with its}} {{original}} uses, this {{problem has been}} suggested {{to have a strong}} similarity to problems of improving global context and compressibility following a Burrows-Wheeler Transform. Following this transform, files tend to have large regions with locally high frequencies, and compression efficiency is greatly improved by techniques that tend to move frequently-occurring characters toward zero, or the front of the [...] "list". Due to this, methods and variants of Move-to-Front and <b>frequency</b> <b>counts</b> often follow the BWT algorithm to improve compressibility.|$|E
5000|$|... 4. Update {{the context}} models. For example, if context model 2 was {{selected}} for bin 1 {{and the value of}} bin 1 was “0”, the frequency count of “0”s is incremented. This means that the next time this model is selected, the probability of a “0” will be slightly higher. When the total number of occurrences of a model exceeds a threshold value, the <b>frequency</b> <b>counts</b> for “0” and “1” will be scaled down, which in effect gives higher priority to recent observations.|$|E
5000|$|Typically, {{however, the}} n-gram model probabilities are not derived {{directly}} from the <b>frequency</b> <b>counts,</b> because models derived this way have severe problems when confronted with any n-grams that have not explicitly been seen before. Instead, some form of smoothing is necessary, assigning some of the total probability mass to unseen words or n-grams. Various methods are used, from simple [...] "add-one" [...] smoothing (assign a count of 1 to unseen n-grams) to more sophisticated models, such as Good-Turing discounting or back-off models.|$|E
40|$|Mining {{association}} {{rules in}} large database {{is one of}} most popular data mining techniques for business decision makers. Discovering frequent item set is the core process in association rule mining. Numerous algorithms {{are available in the}} literature to find frequent patterns. Apriori and FP-tree are the most common methods for finding frequent items. Apriori finds significant frequent items using candidate generation with more number of data base scans. FP-tree uses two database scans to find significant frequent items without using candidate generation. This proposed TR-FCTM (Transaction Reduction- <b>Frequency</b> <b>Count</b> Table Method) discovers significant frequent items by generating full candidates once to form <b>frequency</b> <b>count</b> table with one database scan. Experimental results of TR-FCTM shows that this algorithm outperforms than Apriori and FP-tree...|$|R
40|$|Burrows-Wheeler {{compression}} is a three stage {{process in}} which the data is transformed with the Burrows-Wheeler Transform, then transformed with Move-To-Front, and finally encoded with an entropy coder. Move-To-Front, Transpose, and <b>Frequency</b> <b>Count</b> are some of the many algorithms used on the List Update problem. In 1985, Competitive Analysis first showed the superiority of Move-To-Front over Transpose and <b>Frequency</b> <b>Count</b> for the List Update problem with arbitrary data. Earlier studies due to Bitner assumed independent identically distributed data, and showed that while Move-To-Front adapts to a distribution faster, incurring less overwork, the asymptotic costs of <b>Frequency</b> <b>Count</b> and Transpose are less. The improvements to Burrows-Wheeler compression this work covers are increases in the amount, not speed, of compression. Best x of 2 x- 1 is a new family of algorithms created to improve on Move-To-Front's processing of the output of the Burrows-Wheeler Transform which is like piecewise independent identically distributed data. Other algorithms for both the middle stage of Burrows-Wheeler compression and the List Update problem for which overwork, asymptotic cost, and competitive ratios are also analyzed are several variations of Move One From Front and part of the randomized algorithm Timestamp. The Best x of 2 x - 1 family includes Move-To-Front, the part of Timestamp of interest, and <b>Frequency</b> <b>Count.</b> Lastly, a greedy choosing scheme, Snake, switches back and forth as the amount of compression that two List Update algorithms achieves fluctuates, to increase overall compression. The Burrows-Wheeler Transform is based on sorting of contexts. The other improvements are better sorting orders, such as “aeioubcdf [...] . ” instead of standard alphabetical “abcdefghi [...] . ” on English text data, and an algorithm for computing orders for any data, and Gray code sorting instead of standard sorting. Both techniques lessen the overwork incurred by whatever List Update algorithms are used by reducing the difference between adjacent sorted contexts...|$|R
40|$|Thesis (Ed. M.) [...] Boston UniversityThe {{purpose of}} this study was to compile a word <b>frequency</b> <b>count</b> of the words and phrases found in five {{business}} arithmetic textbooks and to give definitions for the terms that are pertinent to business arithmetic which will be helpful to teachers, students, and authors in the field...|$|R
