118|247|Public
5000|$|Sometimes it is {{convenient}} to pad the input with zeros {{on the border}} of the <b>input</b> <b>volume.</b> The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the <b>input</b> <b>volume.</b>|$|E
5000|$|The spatial size of {{the output}} volume can be {{computed}} {{as a function of}} the <b>input</b> <b>volume</b> size , the kernel field {{size of the}} Conv Layer neurons , the stride with which they are applied , and the amount of zero padding [...] used on the border. The formula for calculating how many neurons [...] "fit" [...] in a given volume is given by [...] If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the <b>input</b> <b>volume</b> in a symmetric way. In general, setting zero padding to be [...] when the stride is [...] ensures that the <b>input</b> <b>volume</b> and output volume will have the same size spatially. Though it's generally not completely necessary to use up all of the neurons of the previous layer, for example, you may decide to use just a portion of padding.|$|E
50|$|The {{convolutional}} {{layer is}} the core building block of a CNN. The layer's parameters consist {{of a set of}} learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the <b>input</b> <b>volume.</b> During the forward pass, each filter is convolved across the width and height of the <b>input</b> <b>volume,</b> computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.|$|E
50|$|Typical {{values are}} 2x2. Very large <b>input</b> <b>volumes</b> may warrant 4x4 pooling in the lower-layers. However, {{choosing}} larger shapes will dramatically reduce {{the dimension of}} the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.|$|R
40|$|SUMMARYThis article {{presents}} new estimates of publicly funded healthcare outputs, inputs and productivity {{for the period}} 1995 to 2008. These update the estimates published in Total Public Service Output and Productivity in June 2009 (ONS 2009 a) by incorporating new methods to improve the measurement of output and <b>input</b> <b>volumes</b> and give the first results for 2008. ...|$|R
40|$|The {{research}} {{focus of}} the scientific paper is {{on the problem of}} performance measurement of credit institutions. The author recommends applying frontier analysis techniques such as Data Envelopment Analysis and Stochastic Frontier Approach to the efficiency analysis of Decision Making Units. Using modern computer technologies, the author has calculated dynamics of efficiency score of ten Latvian banks on the basis of DEA CCR approach, identified outliers among investigated banks, provided recommendations concerning optimal <b>input</b> <b>volumes</b> and established hidden development reserves using SFA method...|$|R
5000|$|A CNN {{architecture}} is formed by {{a stack of}} distinct layers that transform the <b>input</b> <b>volume</b> into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. We discuss them further below: ...|$|E
50|$|We {{note that}} the {{productivity}} model reports a 1.4% productivity growth from the same production data. The difference (1.4% versus 1.5%) {{is caused by the}} different production volume used in the models. In the productivity model the <b>input</b> <b>volume</b> is used as a production volume measure giving the growth rate 1.063. In this case productivity is defined as follows: output volume per one unit of <b>input</b> <b>volume.</b> In the growth accounting model the output volume is used as a production volume measure giving the growth rate 1.078. In this case productivity is defined as follows: input consumption per one unit of output volume. The case can be verified easily with the aid of productivity model using output as a production volume.|$|E
50|$|When {{dealing with}} high-dimensional inputs such as images, it is {{impractical}} to connect neurons to all neurons {{in the previous}} volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the <b>input</b> <b>volume.</b> The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the <b>input</b> <b>volume.</b> Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.|$|E
50|$|Model {{verification}} {{is achieved}} by obtaining output data from the model and comparing them to what is expected from the input data. For example, in traffic simulation, traffic volume can be verified to ensure that actual volume throughput in the model is reasonably close to traffic <b>volumes</b> <b>input</b> into the model. Ten percent is a typical threshold used in traffic simulation to determine if output volumes are reasonably close to <b>input</b> <b>volumes.</b> Simulation models handle model inputs in different ways so traffic that enters the network, for example, {{may or may not}} reach its desired destination. Additionally, traffic that wants to enter the network may not be able to, if congestion exists. This is why model verification is {{a very important part of}} the modeling process.|$|R
30|$|A {{technical}} efficiency {{change that is}} slightly greater than 1 might indicate that the European milk sector is capable of satisfying the production limit. On the other hand, a technological change index of less than 1 means that similar <b>input</b> <b>volumes</b> produce smaller output amounts. This implies that milk producers {{have to be considered}} to be efficient and that the reduction in productivity—found especially after 2009 —is not the result of the ability of dairy farmers to use their inputs efficiently but might mainly be related to other factors (e.g., market shocks, milk price volatility).|$|R
40|$|AbstractData {{collected}} into {{an experimental}} laboratory rig {{of three different}} and widely used permeable pavement types has been analyzed. Through a correlation analysis, the key variables were identified: flows (wet conditions), twelve hours cumulated flows (historical conditions) and cumulative <b>input</b> <b>volumes</b> and masses (clogging conditions). As a first attempt, due to its successful application, k-C* model were selected to simulate the porous pavement systems and test its validity in this case. As shown below, results were not so satisfactory than, further investigations were conducted and three new formulations (one for each porous pavement type) has been proposed and validated using the Nash-Sutcliffe coefficient as goodness of fit...|$|R
50|$|Theoretical {{framework}} of the model can be either cost theory or production theory. In a model based on the production theory, the volume of activity is measured by <b>input</b> <b>volume.</b> In a model based on the cost theory, the volume of activity is measured by output volume.|$|E
50|$|The {{technique}} of volume ray casting {{can be derived}} directly from the rendering equation. It provides results of very high quality rendering. Volume ray casting is classified as an image-based volume rendering technique, as the computation emanates from the output image and not the <b>input</b> <b>volume</b> data, {{as is the case}} with object-based techniques.|$|E
5000|$|The {{depth of}} the output volume {{controls}} the number of neurons in a layer that connect to the same region of the <b>input</b> <b>volume.</b> These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate {{in the presence of}} various oriented edges, or blobs of color.|$|E
25|$|Farming is subsidised, with {{subsidies}} to farmers totalling £3.19 billion (after deduction of levies) paid in 2010. These subsidies are mostly channelled through the EU Common Agricultural Policy from member states' contributions. UK farmers receive the fifth largest agricultural subsidy in the EU, with 7% of the subsidy, after France (17%), Spain (13%), Germany (12%), and Italy (10%). There is {{downward pressure on}} the subsidies and on 19 November 2010, the EU announced a reform starting in 2013. Output volume rose by 1.9% in 2010 compared to 2009, productivity increased by 1.6%, and direct subsidies fell by 12%. Between 1979 and 2010, productivity grew by 49%, output volumes by 25% and <b>input</b> <b>volumes</b> fell by 16%.|$|R
40|$|The paper {{deals with}} {{analysis}} of the trade balance in Czech Republic {{in the field of}} agricultural and food products. The main goal is to determine the influence of analytical indicators; in this case these are changes of quantity and average unit price of export and import; over the synthetic indicator – change of trade balance. Next step of this analysis is to calculate the portions of change of <b>inputs</b> <b>volume</b> and change of total productivity of inputs over the change of export caused by change of quantity. To fulfill this aim it is suitable to use methods for pyramidal decomposition of indicators – chain substitution, logarithmic and functional methods. These methods are compared and the results are interpreted...|$|R
40|$|Here {{we propose}} {{a method for}} medial voxel {{extraction}} from large volumetric models based on an out-of-core framework. The method improves upon geodesic-based approaches to enable the handling of large objects. First, distance fields are constructed from <b>input</b> <b>volumes</b> using an out-of-core algorithm. Second, medial voxels are extracted from these distance fields through multi-phase evaluation processes. Trivial medial or non-medial voxels are evaluated by the low-cost pseudo-geodesic distance method first, and the more expensive geodesic distance computation is run last. Using this strategy allows most of the voxels to be extracted in the low-cost process. This paper outlines a number of results regarding the extraction of medial voxels from large volumetric models. Our method also works in parallel, and we demonstrate that computation time becomes even shorter in multi-core environments...|$|R
5000|$|Most {{programs}} show waveforms to {{give the}} user a visual aid {{of what has been}} recorded. If the waveform is of low or high height (with respect to the [...] axis), the recording was most likely conducted under conditions with a low or high <b>input</b> <b>volume,</b> respectively. From this example, it follows that the curve represented by the waveform is affected by both the input signal and conditions under which it is recorded.|$|E
5000|$|As {{this effect}} is more {{pronounced}} with higher input signals, the harder [...] "attack" [...] of a note will be compressed more heavily than the lower-voltage [...] "decay", making the latter seem louder and thereby improving sustain. Additionally, because the level of compression is affected by <b>input</b> <b>volume,</b> the player can control it via their playing intensity: playing harder results in more compression or [...] "sag". In contrast, modern amplifiers often use high-quality, well-regulated power supplies.|$|E
50|$|Since all neurons in {{a single}} depth slice share the same parameters, then the forward pass in each depth slice of the CONV layer can be {{computed}} as a convolution of the neuron's weights with the <b>input</b> <b>volume</b> (hence the name: convolutional layer). Therefore, {{it is common to}} refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.|$|E
40|$|International audienceWe {{introduce}} in {{this paper}} an algorithm that generates from an <b>input</b> tolerance <b>volume</b> a surface triangle mesh guaranteed to be within the tolerance, intersection free and topologically correct. A pliant meshing algorithm is used to capture the topology and discover the anisotropy in the <b>input</b> tolerance <b>volume</b> in order to generate a concise output. We first refine a 3 D Delaunay triangulation over the tolerance volume while maintaining a piecewise-linear function on this triangulation, until an isosurface of this function matches the topology sought after. We then embed the isosurface into the 3 D triangulation via mutual tessellation, and simplify it while preserving the topology. Our approach extends to surfaces with boundaries and to non-manifold surfaces. We demonstrate the versatility and efficacy of our approach {{on a variety of}} data sets and tolerance volumes...|$|R
40|$|Abstract—A major {{challenge}} in the current computer-aided detection (CAD) of polyps in CT colonography (CTC) {{is to reduce the}} number of false-positive (FP) detections while maintaining a high sensitivity level. A pattern-recognition technique based on the use of an artificial neural network (ANN) as a filter, which is called a massive-training ANN (MTANN), has been developed recently for this purpose. The MTANN is trained with a massive number of subvolumes extracted from <b>input</b> <b>volumes</b> together with the teaching volumes containing the distribution for the “likelihood of being a polyp; ” hence the term “massive training. ” Because of the large number of subvolumes and the high dimensionality of voxels in each input subvolume, the training of an MTANN is time-consuming. In order to solve this time issue and make an MTANN work more efficiently, we propose here a dimension reduction method for an MTANN by using Laplacian eigenfunc...|$|R
30|$|Parameters {{are taken}} into account, {{including}} pipeline information, oil <b>input</b> sequence, <b>volume</b> and flow rate of the initial station in {{a certain period of}} time, demand volumes of each offloading station, upper and lower limits of flow rate, and each station’s offloading flow rate limits. The decision variables are actual offloading volumes and the starting and ending time of offloading operation.|$|R
50|$|Even if the {{production}} function variables of profitability and volume {{were in the}} model, in practice the calculation can also be carried out {{in compliance with the}} cost function. This is the case in models C & T as well as Gollop. Calculating methods differ in the use of either output volume or <b>input</b> <b>volume</b> for measuring the volume of activity. The former solution complies with the cost function and the latter with {{the production}} function. It is obvious that the calculation produces different results from the same material. A recommendation is to apply calculation in accordance with the production function. According to the definition of the production function used in the productivity models Saari and Kurosawa, productivity means {{the quantity and quality of}} output per one unit of input.|$|E
5000|$|Worms {{actively}} digest {{the solid}} organic material. An equilibrium is reached whereby volume digested by a stable population of worms matches the <b>input</b> <b>volume</b> of solid waste. Seasonal {{and environmental factors}} (such as temperature) and variable influent volumes can cause buildup of solid waste as a pile. Although oxygen is excluded from the centre of this [...] "wet" [...] compost pile, worms work from the outside in and introduce air as necessary into the pile to meet their nutritional requirements. This food resource buffer ensures primary treatment vermifilters have a level of resilience and reliability, provided space is provided for a pile to build up. There {{is some evidence that}} the wet environment facilitates digestion of solid waste by worms. The volume of vermicast humus increases only slowly and occasionally needs to be removed from the primary treatment reactor.|$|E
5000|$|The {{technique}} of ending a spoken or musical recording by fading out the sound {{goes back to}} the earliest days of recording. In the era of mechanical (pre-electrical) recording, this could only be achieved by either moving the sound source away from the recording horn, or by gradually reducing the volume at which the performer/s were singing, playing or speaking. With the advent of electrical recording, smooth and controllable fadeout effects could be easily achieved by simply reducing the <b>input</b> <b>volume</b> from the microphones using the fader on the mixing desk.The first experimental study on the effect of a fade-out showed that a version of a musical piece with fade-out in comparison to the same piece with a cold end prolonged the perceived duration by 2.4 seconds. This is called the “Pulse Continuity Phenomenon" [...] and was measured by a tapping-along task to measure participants’ perception of pulsation.|$|E
40|$|Topological volume skeletons {{represent}} level-set graphs of 3 D scalar fields, {{and have}} recently become crucial to visualizing the global isosurface transitions in the volume. However, {{it is still}} a time-consuming task to extract them especially when <b>input</b> <b>volumes</b> are large-scale data and/or prone to small-amplitude noise. This paper presents an efficient method for accelerating the computation of such skeletons using adaptive tetrahedralization. The present tetrahedralization is a top-down approach to linear interpolation of the scalar fields in that it selects tetrahedra to be subdivided adaptively using several criteria. As the criteria, the method employs a topological criterion as well as a geometric one in order to pursue all the topological isosurface transitions that may contribute to the global skeleton of the volume. The tetrahedralization also allows us to avoid unnecessary tracking of minor degenerate features that hide the global skeleton. Experimental results are included to demonstrate that the present method smoothes out the original scalar fields effectively without missing any significant topological features. ...|$|R
40|$|This manual {{presents}} and describes {{a package of}} computer models uniquely developed for boiler thermal performance and emissions evaluations by the Energy and Environmental Research Corporation. The model package permits boiler heat transfer, fuels combustion and pollutant emissions predictions related {{to a number of}} practical boiler operations such as fuel-switching, fuels co-firing and reburning NOx reductions. All computer models presented here have been applied to various process-design needs and vigorously tested against data obtained from numerous combustors ranging from bench/pilot to full-utility scales in the past several years. This user's manual presents the computer model package in two volumes. Volume I describes in detail a number of topics which are of general users'interest, including the physical and chemical basis of the models, a complete description of the model applicability, options, input/output, and the default <b>inputs.</b> <b>Volume</b> II contains a detailed record of the workes examples to assist users in applying the models and to illustrate the versatility of the codes. ...|$|R
50|$|This PAC {{allows the}} CLD-A100 {{to use all}} NTSC LaserKaraoke titles. The front panel has two {{microphone}} <b>inputs</b> with separated <b>volume</b> controls, as well as tone control. The retail price was US $350.|$|R
50|$|The {{technique}} of volume ray casting {{can be derived}} directly from the rendering equation. It provides results of very high quality, usually considered {{to provide the best}} image quality. Volume ray casting is classified as image based volume rendering technique, as the computation emanates from the output image, not the <b>input</b> <b>volume</b> data {{as is the case with}} object based techniques. In this technique, a ray is generated for each desired image pixel. Using a simple camera model, the ray starts at the center of projection of the camera (usually the eye point) and passes through the image pixel on the imaginary image plane floating in between the camera and the volume to be rendered. The ray is clipped by the boundaries of the volume in order to save time. Then the ray is sampled at regular or adaptive intervals throughout the volume. The data is interpolated at each sample point, the transfer function applied to form an RGBA sample, the sample is composited onto the accumulated RGBA of the ray, and the process repeated until the ray exits the volume. The RGBA color is converted to an RGB color and deposited in the corresponding image pixel. The process is repeated for every pixel on the screen to form the completed image.|$|E
40|$|The {{relationship}} between specimen <b>input</b> <b>volume</b> {{and the frequency}} of reported human immunodeficiency virus type 1 (HIV- 1) RNA copy numbers by nucleic acid amplification technology (the NASBA HIV- 1 RNA QT system) was investigated. Results obtained with both clinical specimens and dilution panels indicated that both the absolute number of reported results and the reported HIV- 1 RNA copy number were directly proportional to the specimen input volumes evaluated (0. 1, 0. 5, and 1. 0 ml). Conversion of the reported HIV- 1 RNA copy numbers to a constant 1. 0 -ml volume indicated that the numerical relationship among the specimen input volumes and the HIV- 1 RNA copy numbers was multiplicative. The HIV- 1 RNA copy numbers reported for the 0. 5 -ml <b>input</b> <b>volume</b> were approximately 5 -fold increased over those reported for the 0. 1 -ml <b>input</b> <b>volume,</b> and those reported for the 1. 0 -ml <b>input</b> <b>volume</b> were 10 -fold increased over those reported for the 0. 1 -ml <b>input</b> <b>volume.</b> For the specimen input volumes investigated, a common linear range of 264 to 5, 400, 000 HIV- 1 RNA copies was observed. The use of increased specimen input volumes did not result in a loss of assay specificity, as the results reported for specimens from 50 seronegative blood donors were negative at all three specimen input volumes. In conclusion, an increase in the <b>input</b> <b>volume</b> of specimens analyzed by nucleic acid amplification technology can be useful for the enhanced detection of HIV- 1 RNA...|$|E
40|$|Isosurface {{extraction}} {{is one of}} {{the most}} powerful techniques in the investigation of volume datasets in scientific visualization. The contour tree is a fundamental data structure for fast isosurface extraction, and has also been used to build user interfaces to report the complete topological characterization of the isosurfaces embedded in the volume data, as well as to simplify the volume data to build a multiresolution hierarchy while preserving the isosurface topologies. In this paper, we present a new output-sensitive algorithm for computing the contour tree. Our algorithm is simple, and achieves the optimal bound of ¢¤£¦¥¨§�©�������©� � in running time for both structuredand unstructured-grid volume datasets, where ¥ is the number of cells of the <b>input</b> <b>volume,</b> and © is the number of critical points in the <b>input</b> <b>volume,</b> which bounds the size of the contour tree. Our algorithm improves the previous best running time of ��£¦¥���£�����§����������� � given in [5] for unstructured grids (where � is the number of vertices of the <b>input</b> <b>volume),</b> and as the algorithm of [5], works in all dimensions as well. The experiments show that typically © is less than 5 % of the overall number � of the input vertices, and that our algorithm is 2 to 3 times as fast as the previous best algorithm [5]...|$|E
40|$|Abstract. We {{describe}} a new method {{to support the}} segmentation of a volumetric MRI- or CT-dataset such that only the components selected by the user are displayed by a volume renderer for visual inspection. The goal is to combine the advantages of direct volume rendering (high efficiency and semi-transparent display of internal structures) and indirect volume rendering (well defined surface geometry and topology). Our approach {{is based on a}} re-labeling of the <b>input</b> <b>volume’s</b> set of isosurfaces which allows the user to peel off the outer layers and to distinguish unconnected voxel components which happen to have the same voxel values. For memory and time efficiency, isosurfaces are never generated explicitly. Instead a second voxel grid is computed which stores a discretization of the new isosurface labels. Hence the masking of unwanted regions as well as the direct volume rendering of the desired regions of interest (ROI) can be implemented on the GPU which enables interactive frame rates even while the user changes the selection of the ROI. ...|$|R
40|$|An {{analytical}} injector {{model was}} developed specifically to analyze combustion instability coupling between the injector hydraulics and the combustion process. This digital computer dynamic injector model will, for any imposed chamber of inlet pressure profile with a frequency ranging from 100 to 3000 Hz (minimum) accurately predict/calculate the instantaneous injector flowrates. The injector system {{is described in}} terms of which flow segments enter and leave each pressure node. For each flow segment, a resistance, line lengths, and areas are required as inputs (the line lengths and areas are used in determining inertance). For each pressure node, volume and acoustic velocity are required as <b>inputs</b> (<b>volume</b> and acoustic velocity determine capacitance). The geometric criteria for determining inertances of flow segments and capacitance of pressure nodes was set. Also, a technique was developed for analytically determining time averaged steady-state pressure drops and flowrates for every flow segment in an injector when such data is not known. These pressure drops and flowrates are then used in determining the linearized flow resistance for each line segment of flow...|$|R
40|$|Image metamorphosis, or image morphing, is {{a popular}} {{technique}} for creating a smooth transition between two images. For synthetic images, transforming and rendering the underlying three-dimensional (3 D) models {{has a number of}} advantages over morphing between two pre-rendered images. In this paper we consider 3 D metamorphosis applied to volume-based representations of objects. We discuss the issues which arise in volume morphing and present a method for creating morphs. Our morphing method has two components: first a warping of the two <b>input</b> <b>volumes,</b> then a blending of the resulting warped volumes. The warping component, an extension of Beier and Neely's image warping technique to 3 D, is feature-based and allows fine user control, thus ensuring realistic looking intermediate objects. In addition, our warping method is amenable to an efficient approximation which gives a 50 times speedup and is computable to arbitrary accuracy. Also, our technique corrects the ghosting problem present in Beier and Neely's technique. The second component of the morphing process, blending, is also under user control; this guarantees smooth transitions in the renderings...|$|R
