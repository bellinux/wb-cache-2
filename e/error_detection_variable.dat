0|5795|Public
40|$|International audienceIn {{this paper}} is {{described}} a purely software technique allowing to detect SEUs occurring in processor-based digital architectures. <b>Error</b> <b>detection</b> mechanisms (<b>variable</b> replication, inclusion of flags, [...] .) {{are added to the}} targeted program, through its transformation in a new one, having same functionalities but being able to identify bit-flips arising in memory areas as well as those perturbing the processor's internal registers. Experimental results issued from fault injection sessions performed in complex DSP intended to be used in an instrument of a satellite, provide objective figures about the efficiency of the proposed technique...|$|R
40|$|Embedded Block Coding with Optimal Truncation (EBCOT) is {{an entropy}} coding {{algorithm}} adopted by JPEG 2000 still image compression standard. It is a quadtree encoding method representing a fault tolerance design challenge. EBCOT operates on independently coded blocks, {{and is therefore}} more robust to channel errors than many other wavelet based schemes. However, computer-induced errors can still destroy the image. In EBCOT, the condition of each sample is described by binary state variables. Any internal errors introduced in these state variables {{could lead to the}} errors of the neighboring state variables, corrupting the entire block. This work discuss the use of redundant data and arithmetic operations to detect such errors. It demonstrates the detection performance when single and double errors occur in the coding passes. Key words: bit-plane coding, concurrent <b>error</b> <b>detection,</b> state <b>variable,</b> algorithm-based fault tolerance. 1...|$|R
40|$|Operational {{data are}} used for control and {{optimization}} of a process in industrial and energy plants are used. Data provided from measurement are affected by errors arising from uncertainty of measurement (imprecision of measuring instruments). Data reconciliation provides us more precise variables which give {{us the opportunity to}} better optimize and achieve higher economic gains. The objective of this bachelor's thesis was to study the issue of data reconciliation of measurement and gross <b>error</b> <b>detection</b> in measurement <b>variables.</b> This work is divided into two parts. The first part is theoretical, describing measurement errors, solution methods for data reconciliation and statistical tests for gross <b>error</b> <b>detection.</b> The second part is a practical demonstration of data reconciliation on two examples. The results of reconciliation and the solution procedure are included. Complete procedure {{can be found in the}} Annex...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedDesigned {{to circumvent the}} incompatibilities between communicating computer systems, a parameterized protocol convertor permits the use of communication equipment supporting variations of the same communication protocol or completely different framing technique protocols. The analysis of the conversion process includes the engineering trade-offs between speed of conversion and flexibility, {{and the use of}} an alternative flow architecture. Flexibility is enhanced through user selection of input and output protocol types, and the designation of functional specifics, such as code type, header length, and <b>error</b> <b>detection</b> methods, with <b>variable</b> parameters. The speed of conversion is increased through the parallel processing of the framing, transparency, and error control sub-functions and the use of a single byte storage technique. The single byte storage technique imposes some limitations in the use of transparent data. [URL] United States Marine Corp...|$|R
40|$|This {{research}} examines <b>error</b> <b>detection</b> strategies as {{a method}} for ensuring effective World Wide Web accessibility for older adults. It evaluates the underpinnings of web accessibility and their relevance to <b>error</b> <b>detection</b> strategies {{for the support of}} older adults. The research provides a contextual definition of computer systems and an account of how <b>error</b> <b>detection</b> relates to accessibility. The <b>Error</b> <b>Detection</b> System strategies focused on developing profiles of the participants. The profiles (self-assessment, testing, observation, and <b>error</b> <b>detection)</b> were used to modify webpages that the participant accessed. This research compares the performance of each profile, using a task list and error collection from the <b>Error</b> <b>Detection</b> System. Different <b>error</b> <b>detection</b> strategies that may be employed are presented, as well as their potential in the development of <b>error</b> <b>detection</b> strategies...|$|R
40|$|<b>Error</b> <b>detection</b> by fragile watermarking* Abstract Error {{concealment}} {{techniques are}} useful in video transmission over channels that introduce bit errors. The efficiency and result of error concealment technique, however, rely on the <b>error</b> <b>detection</b> capabilities of video decoders. A novel <b>error</b> <b>detection</b> technique employing fragile watermarking is proposed in this paper. By embedding a fragile watermark on the quantized DCT coefficients and examining its integrity on the decoder side, the <b>error</b> <b>detection</b> capability of video decoders is significantly increased compared to widely used syntax-based <b>error</b> <b>detection</b> schemes...|$|R
2500|$|Theorem (Burst <b>error</b> <b>detection</b> ability). The burst <b>error</b> <b>detection</b> {{ability of}} any [...] code is ...|$|R
40|$|In {{the current}} study, we {{investigated}} bilingual <b>error</b> <b>detection</b> {{by measuring the}} repair rate of language intrusions (i. e., involuntary production of nontarget language words) that arose while bilinguals produced sentences in a language switching context. This allowed us to compare two prominent accounts of <b>error</b> <b>detection</b> in a bilingual setting. According to the conflict monitoring account, <b>error</b> <b>detection</b> is initiated by interference. Since language switching increases bilingual language interference, <b>error</b> <b>detection</b> should be better in switch relative to repetition trials. According to the perceptual loop theory, <b>error</b> <b>detection</b> is based on language comprehension. Since language switching is known to impair language comprehension, it follows that <b>error</b> <b>detection</b> should be worse in switch relative to repetition trials. The {{results showed that the}} repair rate of language intrusions was higher in switch than repetition trials, thus providing evidence that bilingual language interference instigates <b>error</b> <b>detection,</b> in line with the conflict monitoring account...|$|R
40|$|This paper {{proposes a}} pure {{software}} technique, <b>Error</b> <b>Detection</b> by Duplicated Instructions (EDDI), for detecting errors during normal system operation. Compared to other <b>error</b> <b>detection</b> techniques that use hardware redundancy, our method {{does not require}} any hardware modifications to add <b>error</b> <b>detection</b> capability to the original syste...|$|R
40|$|Organizational {{databases}} have {{a significant}} rate of data errors and detecting and correcting these errors can be problematic. This paper builds on a stream of research demonstrating that users of these databases can detect data errors under certain circumstances. A theory of <b>error</b> <b>detection</b> and research {{on the effect of}} base rate expectations in probabilistic judgement tasks are applied to the development of two propositions about <b>error</b> <b>detection.</b> It is argued that expectations about the base rate of errors in data affect <b>error</b> <b>detection</b> performance when they are developed through direct experience and that incentives affect <b>error</b> <b>detection</b> performance. The two research propositions are tested in a laboratory experiment. Experience-based expectations about the base rate of errors and incentives are found to affect <b>error</b> <b>detection</b> performance. End-user computing Data quality <b>Error</b> <b>detection...</b>|$|R
50|$|Beside framing, {{data link}} layers also include {{mechanisms}} {{to detect and}} even recover from transmission errors. For a receiver to detect transmission error, the sender must add redundant information (in the form of bits) as an <b>error</b> <b>detection</b> code to the frame sent. When the receiver obtains a frame with an <b>error</b> <b>detection</b> code it recomputes it and verifies whether the received <b>error</b> <b>detection</b> code matches the computed <b>error</b> <b>detection</b> code. If they match the frame {{is considered to be}} valid.|$|R
40|$|Error {{concealment}} {{techniques are}} useful in video transmission over channels that introduce bit errors. The efficiency and result of error concealment technique, however, rely on the <b>error</b> <b>detection</b> capabilities of video decoders. A novel <b>error</b> <b>detection</b> technique employing fragile watermarking is proposed in this paper. By embedding a fragile watermark on the quantized DCT coefficients and examining its integrity on the decoder side, the <b>error</b> <b>detection</b> capability of video decoders is significantly increased compared to widely used syntax-based <b>error</b> <b>detection</b> schemes...|$|R
40|$|This paper {{presents}} a theoretical comparison of different existing data <b>error</b> <b>detection</b> techniques. The techniques are compared by fault coverage, memory overhead and performance overhead. For this comparison, ten different data <b>error</b> <b>detection</b> techniques {{are taken into}} account. In general, the best <b>error</b> <b>detection</b> technique always has the highest fault coverage with low performance and memory overhead. After performing the theoretical comparison, we conclude that GA (genetic algorithm) and SWIFT (software implemented fault tolerance) techniques are the best techniques for data <b>error</b> <b>detection.</b> status: publishe...|$|R
40|$|The article {{describes}} three methods of gross <b>error</b> <b>detection</b> and their localization in geodetic surveying. The prerequisite for any gross <b>error</b> <b>detection</b> procedure is {{the availability of}} a set of redundant observations. The global model test with Data Snooping is the most commonly used method for gross <b>error</b> <b>detection,</b> however, it assumes that the a priori precision of observations is reliably known. As alternatives, the τ test and the Danish method are presented. An example of gross <b>error</b> <b>detection</b> in a plane cross-braced quadrilateral is given for all three methods...|$|R
40|$|<b>Error</b> <b>detection</b> {{incorporated}} with automatic-repeat-request (ARQ) {{is widely}} used for error control in data communication systems. This method of error control is simple and provides high system reliability. If a properly chosen code is used for <b>error</b> <b>detection,</b> virtually error-free data transmission can be attained. Various types of ARQ and hybrid ARQ schemes, and <b>error</b> <b>detection</b> using linear block codes are surveyed...|$|R
40|$|In {{this paper}} we present low-cost, {{concurrent}} checking methods for multiple <b>error</b> <b>detection</b> in S-boxes of symmetric block ciphers. These are redundancy-based fault detection schemes. We describe some studies of parity based concurrent <b>error</b> <b>detection</b> in S-boxes. Probability of multiple <b>error</b> <b>detection</b> is analyzed for random data. In this work 48 -input, 32 -output substitution blocks {{are taken into}} consideration. ...|$|R
40|$|Abstract—Embedded control {{networks}} commonly use checksums {{to detect}} data transmission errors. However, design decisions about which checksum to use are {{difficult because of}} a lack of information about the relative effectiveness of available options. We study the <b>error</b> <b>detection</b> effectiveness of the following commonly used checksum computations: exclusive or (XOR), two’s complement addition, one’s complement addition, Fletcher checksum, Adler checksum, and cyclic redundancy codes (CRCs). A study of <b>error</b> <b>detection</b> capabilities for random independent bit errors and burst errors reveals that the XOR, two’s complement addition, and Adler checksums are suboptimal for typical network use. Instead, one’s complement addition should be used for networks willing to sacrifice <b>error</b> <b>detection</b> effectiveness to reduce computational cost, the Fletcher checksum should be used for networks looking for a balance between <b>error</b> <b>detection</b> and computational cost, and CRCs should be used for networks willing to pay a higher computational cost for significantly improved <b>error</b> <b>detection.</b> Index Terms—Real-time communication, networking, embedded systems, checksums, <b>error</b> <b>detection</b> codes. Ç...|$|R
40|$|From the {{literature}} on <b>error</b> <b>detection,</b> the authors select several concepts relating <b>error</b> <b>detection</b> mechanisms and prospective memory features. They emphasize {{the central role of}} intention in the classification of the errors into slips/lapses/mistakes, in the error handling process and in the usual distinction between action-based and outcome-based detection. Intention is again a core concept in their investigation of prospective memory theory, where they point out the contribution of intention retrievals, intention persistence and output monitoring in the individual's possibilities for detecting their errors. The involvement of the frontal lobes in prospective memory and in <b>error</b> <b>detection</b> is also analysed. From the chronology of a prospective memory task, the authors finally suggest a model for <b>error</b> <b>detection</b> also accounting for neural mechanisms highlighted by studies on error-related brain activity. <b>Error</b> <b>detection</b> has not received much attention from the scientists since human error has been shown as the main cause of accident in complex systems. However, reducing the consequences of error depends largely on <b>error</b> <b>detection.</b> The goal {{of this paper is to}} synthesize the existing scientific knowledge on <b>error</b> <b>detection,</b> mostly based on studies conducted in laboratory or self reporting and to complete it through the analysis of a corpus of cases collected in a complex system: anaesthesia, in order to better describe how this knowledge can be used to improve our understanding of <b>error</b> <b>detection</b> modes. We used an anaesthesia accident reporting system we developed and organized at two Belgium University Hospitals to collect information about the <b>error</b> <b>detection</b> patterns. Results show that <b>detection</b> of <b>errors</b> principally occurred through standard check. We found significant relationships between the type of error, the type of <b>error</b> <b>detection</b> pattern, and the training level of the anaesthetist who committed the error. Peer reviewe...|$|R
30|$|Route <b>error</b> <b>detection</b> in AOMR-LM {{is similar}} to route <b>error</b> <b>detection</b> in AOMDV. It is {{launched}} when a link fails between two nodes along a path from a source to a destination.|$|R
40|$|This paper {{presents}} a new <b>error</b> <b>detection</b> technique called software implemented <b>error</b> <b>detection</b> (SIED). The proposed method {{is based on}} a new control check flow scheme combined with software redundancy. The distinctive advantage of the SIED approach over other fault tolerance techniques is the fault coverage. SIED is able to cope with faults affecting data and the program control flow. By-applying the proposed approach on several benchmark programs, we evaluate the <b>error</b> <b>detection</b> capabilities by means of several fault injection experiments. Experimental results underline very good <b>error</b> <b>detection</b> capabilities for the obtained hardened version of selected benchmark programs...|$|R
40|$|A {{convolutional}} code {{can be used}} to detect or correct infinite sequences of errors or to correct infinite sequences of erasures. First, erasure correction is shown to be related to <b>error</b> <b>detection,</b> as well as <b>error</b> <b>detection</b> to <b>error</b> correction. Next, the active burst distance is exploited, and various bounds on erasure correction, <b>error</b> <b>detection,</b> and <b>error</b> correction are obtained for {{convolutional code}}s. These bounds are illustrated by examples...|$|R
40|$|Bachelor's thesis {{deals with}} <b>error</b> <b>detection</b> and {{recovery}} for syntactic analysis. The main goal of work was {{to design and}} implement a method for <b>error</b> <b>detection</b> and recovery. The proposed method {{is based on the}} Hartmann method for <b>error</b> <b>detection</b> and <b>error</b> recovery. The user interface, using cross-platform framework Qt was also implemented. As a result, the application prints all of the information about errors while parsing the source code...|$|R
40|$|Although error {{has been}} shown as {{the main cause of}} {{accidents}} in complex systems, little {{attention has been paid to}} <b>error</b> <b>detection.</b> However, reducing the consequences of error depends largely on <b>error</b> <b>detection.</b> The goal of this paper is to synthesize the existing scientific knowledge on <b>error</b> <b>detection,</b> mostly based on studies conducted in laboratory or self reporting and to further knowledge through the analysis of a corpus of cases collected in a complex system, anaesthesia. By doing this, this paper is better able to describe how this knowledge can be used to improve understanding of <b>error</b> <b>detection</b> modes. An anaesthesia accident reporting system developed and organized at two Belgian University Hospitals was used in order to collect information about the <b>error</b> <b>detection</b> patterns. Results show that <b>detection</b> of <b>errors</b> principally occurred through the standard check (routine monitoring of the environment). Significant relationships were found between the type of error and the <b>error</b> <b>detection</b> mode, and between the type of error and the training level of the anaesthetist who committed the error. Peer reviewe...|$|R
40|$|Abstract — Network-on-Chip (NoC) {{architectures}} {{are considered}} against variations of interconnections in this paper. A self-calibrated voltage scaling technique is proposed to provide reliable and low energy interconnections in network-on-chip. The self-calibrated voltage scaling technique adjusts the operation voltage by two stages, which are crosstalk-aware test <b>error</b> <b>detection</b> stage and run-time <b>error</b> <b>detection</b> stage. The crosstalk-aware test <b>error</b> <b>detection</b> stage detects the error by maximal aggressor fault (MAF) test {{patterns in the}} testing mode. The run-time <b>error</b> <b>detection</b> stage detects <b>errors</b> by double sampling data checking technique; moreover, it provides the tolerance to timing variations. According to the <b>error</b> <b>detections,</b> the self-calibrated voltage scaling technique can reduce the voltage swing for energy reduction and guarantee the reliability at the same time. The energy of link wires at the lowest voltage can achieve nearly 64. 47 % reduction compared to un-coded link wires. I...|$|R
40|$|Previous <b>error</b> <b>detection</b> {{research}} {{focused on}} the effectiveness of different checking methods. In this paper, we focus on the psychological mechanisms on <b>error</b> <b>detection.</b> We conceptualize working memory (WM) as a critical cognitive component in <b>error</b> <b>detection</b> and two studies were carried out to investigate the effects of WM load and capacity on <b>error</b> <b>detection</b> performance and the <b>detection</b> of different <b>error</b> types. Study I found a significant interaction effect of WM load x capacity: low WM capacity participants performed significantly worse in higher WM load condition, however, high WM capacity participants 2 ̆ 7 performances were unaffected by higher WM load. Study II employed think-aloud technique to gain insights into detectable error types and generated novel predictions about the effect of WM demands on detecting different errors. These predictions allow for a new research direction in <b>error</b> <b>detection...</b>|$|R
40|$|This paper {{proposes a}} {{hardware}} error checking approach(CCRC) by using redundancy core for multiprocessor system-on-chip (MPSoC) and describes several main <b>error</b> <b>detection</b> methods based on Software-Implemented Hardware Fault Tolerance (SHIF) idea proposed in literatures. The CCRC approach insert some <b>error</b> <b>detection</b> code {{in high level}} code, detect the existing of redundancy core in MPSoC, then complete the calculation of detection code in redundancy core. The author compares the CCRC approach with several main <b>error</b> <b>detection</b> methods on <b>error</b> <b>detection</b> capabilities, area, memory and performance overheads in an experiment platform. The result of comparative evaluation shows that the CCRC approach is effective for MPSoC, taking some advantages in versatility and lower cost...|$|R
30|$|Besides {{eliminating}} these inaccessibility times, {{the fault}} containment {{coverage of the}} CAN router is significantly higher compared to bus-based systems. In a bus-based system, local <b>error</b> <b>detection</b> mechanisms are assumed to shutdown a CAN node that is affected by a fault. However, <b>error</b> <b>detection</b> mechanisms {{should be part of}} separate fault-containment regions {{in order to ensure that}} the <b>error</b> <b>detection</b> mechanisms are not impacted by the same fault that caused the message failure [27].|$|R
40|$|Abstract. We {{discuss the}} use of models for {{run-time}} <b>error</b> <b>detection</b> to improve user-perceived reliability of consumer electronics products. The aim is to apply the approach in industrial products and to embed <b>error</b> <b>detection</b> into a general run-time awareness concept. To study this concept, an awareness framework has been developed in which an application and a model of its desired behaviour can be inserted. It allows both time-based and event-based <b>error</b> <b>detection</b> at run-time. ...|$|R
40|$|We {{describe}} {{a new family}} of <b>error</b> <b>detection</b> codes called Weighted Sum Codes. These codes are preferred over four existing codes (CRC, Fletcher checksum, Internet checksum, and XTP CXOR), because they combine powerful <b>error</b> <b>detection</b> properties (as good as the CRC) with attractive implementation properties. One variant, WSC- 1, has efficient software and hardware implementations; while a second variant, WSC- 2, is almost as efficient in software (still significantly better than CRC) and offers commutative processing (that enables efficient out-of-order, parallel, and incremental update processing). 1 Introduction We {{describe a}} new family of <b>error</b> <b>detection</b> codes called Weighted Sum Codes. Although there exist many good <b>error</b> <b>detection</b> codes, none had all the properties we desired for our broadband transport protocol [1]. The CRC [2] has powerful <b>error</b> <b>detection</b> capabilities, but is slow * and does not allow commutative processing (that enables efficient out-of-order, parallel, and incr [...] ...|$|R
40|$|Embedded control {{networks}} commonly use checksums {{to detect}} data transmission errors. However, design decisions about which checksum to use are {{difficult because of}} a lack of information about the relative effectiveness of available options. We study the <b>error</b> <b>detection</b> effectiveness of the following commonly used checksum computations for embedded networks: exclusive or (XOR), two’s complement addition, one’s complement addition, Fletcher checksum, Adler checksum, and cyclic redundancy codes (CRC). A study of <b>error</b> <b>detection</b> capabilities for random independent bit errors and burst errors reveals that XOR, two’s complement addition, and Adler checksums are suboptimal for typical application use. Instead, one’s complement addition should be used for applications willing to sacrifice <b>error</b> <b>detection</b> effectiveness to reduce compute cost, Fletcher checksum for applications looking for a balance of <b>error</b> <b>detection</b> and compute cost, and CRCs for applications willing to pay a higher compute cost for further improved <b>error</b> <b>detection...</b>|$|R
5000|$|Runtime <b>error</b> <b>detection</b> can {{identify}} defects that manifest themselves only at runtime (for example, file overwrites) and {{zeroing in on}} {{the root causes of}} the application crashing, running slowly, or behaving unpredictably. Defects commonly detected by runtime <b>error</b> <b>detection</b> include: ...|$|R
40|$|Abstract:-In {{this paper}} the {{technological}} aspects are presented concerning the <b>error</b> <b>detection</b> during digital data transmission and storage and its effectiveness needed for military applications. Also an analysis in theoretical level is completed for {{the effectiveness of}} <b>error</b> <b>detection</b> during digital data transmission and storage in information systems. Finally, the proposed method {{is based on the}} coding optimization to increase the effectiveness of the <b>error</b> <b>detection</b> during data transmission and data storage using the check sums for Military Applications...|$|R
40|$|The {{paper is}} {{dedicated}} to solving the efficiency increasing problem for data transmission <b>error</b> <b>detection</b> in spectrum modulation channel by properties such errors appearance accounted. For the guaranteed <b>errors</b> <b>detection</b> in one and more channel symbols {{the approach based on}} Chinese Reminder Theorem has been proposed. In the course of the theoretical researches of proposed approach the guaranteed <b>error</b> <b>detection</b> has been proved for errors quantity less or equal to number of the check symbols in modular representation. ?????? ????????? ??????? ???????? ????????? ????????????? ????????? ?????? ???????? ?????? ? ??????? ?? ???????????? ?????????? ?? ???? ????? ??????? ?? ?????????????. ? ????? ???????????????? ??????????? ????????? ?????? ????? ? ????? ????????? ??? ????????? ??????, ???????????? ????????? ????????????? ?? ?????? ????????? ??????? ?? ????????. ??????????? ????????????? ???????????? ????????????? ??????? ????????? ???????? ????????????????? ??????????? ?????????? ?????? ?????????? ????? ??? ?????? ????? ??????????? ???????? ? ??????????????? ????????? ?????????????...|$|R
40|$|Despite the {{potential}} to dominate radiology reporting, current speech recognition technology is thus far a weak and inconsistent alternative to traditional human transcription. This is attributable to poor accuracy rates, in spite of vendor claims, and the wasted resources that go into correcting erroneous reports. A {{solution to this problem}} is post-speech-recognition <b>error</b> <b>detection</b> that will assist the radiologist in proofreading more efficiently. In this paper, we present a statistical method for <b>error</b> <b>detection</b> that can be applied after transcription. The results are encouraging, showing an <b>error</b> <b>detection</b> rate as high as 96 % in some cases. KEY WORDS: Speech recognition, <b>error</b> <b>detection,</b> radiology reporting, co-occurrence relations, statistical natural language processing, computer-assisted proofreadin...|$|R
40|$|Video {{compression}} standards commonly {{employed in}} the delivery of real-time wireless multimedia services regularly adopt variable length codes (VLCs) for efficient transmission. This coding technique achieves the necessary high compression ratios at the expense of an increased system’s vulnerability to transmission errors. The more frequent presence of transmission errors in wireless channels requires video compression standards to accurately detect, localize and conceal any corrupted macroblocks (MBs) present in the video sequence. Unfortunately, standard decoders offer limited <b>error</b> <b>detection</b> and localization capabilities posing a bound on the perceived video quality of the reconstructed video sequence. This paper presents a novel solution which enhances the <b>error</b> <b>detection</b> and localization capabilities of standard decoders through the application of a Probabilistic Neural Network (PNN). The proposed solution generally outperforms other <b>error</b> <b>detection</b> mechanisms present in literature, as it manages to improve the standard decoder’s <b>error</b> <b>detection</b> rate by up to 95. 74 %. Index Terms — <b>Error</b> <b>detection</b> coding, learning systems, multimedia communications, video coding, wireless networks. 1...|$|R
40|$|International Telemetering Conference Proceedings / October 09 - 11, 1973 / Sheraton Inn Northeast, Washington, D. C. In {{this paper}} {{the problem of}} {{achieving}} reliable digital information transfer {{in the presence of}} data errors is addressed. The approach taken is to reject data which is suspected of being in error under the philosophy that it is better to miss data than to receive it incorrectly. To this end, <b>error</b> <b>detection</b> mechanisms are considered and their performance compared for a specific application. The mechanisms are thresholding, <b>error</b> <b>detection</b> coding, waveform <b>error</b> <b>detection</b> and feedback. It is shown that <b>error</b> <b>detection</b> coding is the most effective, followed by feedback, thresholding, then waveform <b>error</b> <b>detection.</b> The results are summarized in Tables I and II. These tables give the undetected word error rate and missed word rates for the techniques considered. The application which originally inspired this work is the use of time-division multiplexing to transfer mission-critical data on the B- 1 aircraft...|$|R
