1|11|Public
40|$|During {{the past}} few decades, the term {{intelligent}} building envelope {{has emerged as a}} building skin designed to meet <b>increasingly</b> <b>varying</b> and complex demands related to user comfort and energy and cost efficiency. The concept is described by a multitude of definitions that range from the use of innovative components and a high-tech visual expression to the rational design, use and maintenance of the building envelope. Within the scope of this Ph. D., intelligent behaviour for a building envelope has been defined as adaptiveness to the environment by means of perception, reasoning and action, allowing the envelope to solve conflicts and deal with new situations that occur in its interaction with the environment, i. e., the local climate and site, and the individual user needs. This definition is used to analyse the functions an intelligent building envelope can be expected to perform in the context of daylighting quality, or an optimisation of the indoor luminous environment to the requirements of the individual building occupant. Of particular importance is the co-operation between artificial intelligence and the material, form and composition of envelope elements, allowing the envelope to learn the occupant’s needs and preferences, to choose the most appropriate response in each situation, to make long-term strategies, to anticipate the development of environmental conditions, and to evaluate its own performance. Simultaneously, it is found that adaptive envelope solutions in no manner reduce the need for envelope design meticulously adjusted to local climate and site and to individual user needs, developed in close co-operation between architects, engineers and manufacturers. All of the sources consulted {{during the course of this}} Ph. D. stress time and time again how difficult it is to control the operation of the envelope components according to the local environment, and, simultaneously, how important it is to do so. dr. ing. dr. ing...|$|E
40|$|Wildlife {{populations in}} urban environments face <b>increasingly</b> diverse pressures. <b>Varying</b> {{environmental}} conditions and human behaviours influence {{the persistence of}} populations. This study investigates the advantages and limitations of using ecological, social research and citizen science concurrently to develop a holistic understanding of declining populations of the freshwater turtle Chelodina colliei in the urban environment of Perth, Western Australia. Ecological methods assessed population size and demographics using mark-recapture over three trap seasons and environmental conditions in five natural and seven anthropogenic wetlands, and their upland environments. Less than 27 C. colliei individuals were captured in 80...|$|R
40|$|This project report {{focuses on}} book {{metadata}} practices at the University of British Columbia Press. Metadata management has become essential for publishers in recent decades, as book buying has moved online. This report details {{the significance of}} metadata, how publishers use it, how customers (both institutional and individual) benefit from it, and how (good) metadata increases sales. Metadata has become <b>increasingly</b> complex, with <b>varying</b> deadlines, standards, levels, and granularity putting immense pressure on publishers to keep current. This project report analyzes the University of British Columbia Press’ metadata operations to identify its challenges and successes. The report also draws on the current literature of metadata “best practices” for publishers. In tandem, these resources clarify optimal future directions and recommendations for the Press...|$|R
40|$|The fuzzy {{analytical}} network process (FANP) {{is introduced}} {{as a potential}} multi-criteria-decision-making (MCDM) method to improve digital marketing management endeavors. Today’s information overload makes digital marketing optimization, which is needed to continuously improve one’s business, increasingly difficult. The proposed FANP framework is a method for enhancing the interaction between customers and marketers (i. e., involved stakeholders) and thus for reducing the challenges of big data. The presented implementation takes realities’ fuzziness into account to manage the constant interaction and continuous development of communication between marketers and customers on the Web. Using this FANP framework, the marketers are able to <b>increasingly</b> meet the <b>varying</b> requirements of their customers. To improve {{the understanding of the}} implementation, advanced visualization methods (e. g., wireframes) are used...|$|R
40|$|The Asia-Pacific {{region is}} now universally {{regarded}} as the primary driver of global economic growth focused on North Asia, with major contributing economies being China, Japan, the Republic of Korea (RoK) and Taiwan. Importantly, they all rely on seaborne trade and significantly for some of them, a very high dependence on imported energy which comes, in the main, from the Middle East. These energy shipments are vulnerable to disruption, although actual threats vary between transit sectors and potential attackers (state and non-state actors). As most disruption scenarios could occur at sea, maritime forces (navies, coastguards and marine police) would have {{a major role in}} deterring such attacks or responding to them. But the maritime battlespace, {{for want of a better}} term, has grown <b>increasingly</b> complex, with <b>varying</b> jurisdictions, maritime boundary disputes, an opaque international shipping industry, and widely disparate capabilities and responsibilities of maritime forces...|$|R
40|$|The {{ability to}} store energy enables organisms {{to deal with}} {{temporarily}} harsh and uncertain conditions. Empirical {{studies have demonstrated that}} organisms adapted to fluctuating energy availability plastically adjust their storage strategies. So far, however, theoretical studies have investigated general storage strategies only in constant or deterministically varying environments. In this study, we analyze how the ability to store energy influences optimal energy allocation to storage, reproduction, and maintenance in environments in which energy availability varies stochastically. We find that allocation to storage is evolutionarily optimal when environmental energy availability is intermediate and energy stores are not yet too full. In environments with low variability and low predictability of energy availability, it is not optimal to store energy. As environments become more variable or more predictable, energy allocation to storage is <b>increasingly</b> favored. By <b>varying</b> environmental variability, environmental predictability, and the cost of survival, we obtain a variety of different optimal life-history strategies, from highly iteroparous to semelparos, which differ significantly in their storage patterns. Our results demonstrate that in a stochastically varying environment simultaneous allocation to reproduction, maintenance, and storage can be optimal, which contrasts with previous findings obtained for deterministic environments...|$|R
40|$|Scyphomedusae play {{important}} roles in marine ecosystems and are of economic significance. However, no reliable techniques for estimating scyphomedusa age have been documented. This study focused on the utility of Cassiopea sp. (Cnidaria: Scyphozoa) statoliths, statocysts, and body size as proxies for age of medusae. Reared medusae of known age and a manipulative experiment {{were used to assess}} the accuracy and reliability of four measures of age: number of statoliths, size (diameter) of statoliths, area of statocyst (housing statoliths), and bell diameter. Bell diameter provided the most accurate measure of age under constant conditions, but was <b>increasingly</b> inaccurate under <b>varying</b> environmental conditions. In contrast, the average number of statoliths per medusa reflected age with relatively low accuracy, but did not vary with changes in food availability and salinity. Only temperature influenced the average number of statoliths. Comparisons of bell diameter to the number of statoliths in medusae under low food availability to those fed well showed that the ratio of medusa size to the number of statoliths can be used to recognise medusae that are relatively poorly conditioned. Statoliths, therefore, provide a tool for studying both population ecology and the influence of environmental variation on medusa growth...|$|R
40|$|After {{a decade}} where HEC (high-end computing) {{capability}} {{was dominated by}} the rapid pace of improvements to CPU clock frequency, the performance of next-generation supercomputers is <b>increasingly</b> differentiated by <b>varying</b> interconnect designs and levels of integration. Understanding the tradeoffs of these system designs, {{in the context of}} high-end numerical simulations, is a key step towards making effective petascale computing a reality. This work represents one of the most comprehensive performance evaluation studies to date on modern HEC systems, including the IBM Power 5, AMD Opteron, IBM BG/L, and Cray X 1 E. A novel aspect of our study is the emphasis on full applications, with real input data at the scale desired by computational scientists in their unique domain. We examine six candidate ultra-scale applications, representing a broad range of algorithms and computational structures. Our work includes the highest concurrency experiments to date on five of our six applications, including 32 K processor scalability for two of our codes and describe several successful optimizations strategies on BG/L, as well as improved X 1 E vectorization. Overall results indicate that our evaluated codes have the potential to effectively utilize petascale resources; however, several applications will require reengineering to incorporate the additional levels of parallelism necessary to achieve the vast concurrency of upcoming ultra-scale systems. ...|$|R
40|$|In recent years, {{automobiles}} {{have become}} <b>increasingly</b> computerized and <b>varying</b> degrees of intelligent control has been integrated into automotive systems. A natural extension {{of this trend}} is full intelligent and autonomous control of vehicle by onboard computer systems. This thesis presents the design, development, and construction of a low-cost, low-power vision system suitable for on-board automated vehicle systems such as intelligent cruise control. The apparatus leverages vision algorithms, simplified by a prescribed camera geometry, to compute depth maps in real-time, given the input from three imagers mounted on the vehicle. The early vision algorithms are implemented using Dr. David Martin's ADAP mixed signal array processor. The back-end algorithms are mplemented in software on PC for simplicity, but could easily be implemented in hardware in a later design. The final apparatus was able to compute depth maps {{at a rate of}} 24 frames per second, limited only by the interrupt latency of the PC executing the algorithms. by Mark Christian Spaeth. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999. Includes bibliographical references (p. 95 - 96). This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections...|$|R
40|$|PURPOSE OF REVIEW: Hepatitis B and C {{infections are}} {{prevalent}} {{around the world}} and a major health burden due to the associated complications of hepatic fibrosis, cirrhosis and hepatocellular carcinoma which occur in the context of chronic infection. Significant advances are being made in assessing and treating infected patients and recent studies are now targeting patients who have failed to respond to previous treatments or who have associated co-morbidities. The purpose of this article to review the recent literature on the subject of hepatitis B and C infections with particular focus on new treatment options, new approaches in patients who have previously failed therapy and in those who have co-morbidity. RECENT FINDINGS: A large number of studies have been carried out investigating the roles of varying doses, targeting treatment in particular groups and new treatment options in patients infected with hepatitis B and C. Several key findings such as the value of prolonging treatment in patients with genotype 1 hepatitis C infection, the use of pegylated interferon in chronic hepatitis B infection and the emergence of new treatments such as adefovir for resistant hepatitis B infection, as well as treatment of patients co-infected with hepatitis C and human immunodeficiency virus, have dominated the recent literature. Patients in particular groups such as those who have had liver transplantation or who are immunosuppressed have also received added attention. SUMMARY: Hepatitis B and C infections are the focus of much current attention with particular regard to new and emerging treatment options which are becoming <b>increasingly</b> focused on <b>varying</b> patient groups...|$|R
40|$|The Gyrokinetic Toroidal Code (GTC) is a global, {{three-dimensional}} particle-in-cell application {{developed to}} study microturbulence in tokamak fusion devices. The global capability of GTC is unique, allowing researchers to systematically analyze important dynamics such as turbulence spreading. In this work we examine a new radial domain decomposition approach to allow scalability onto the latest generation of petascale systems. Extensive performance evaluation is conducted on three high performance computing systems: the IBM BG/P, the Cray XT 4, and an Intel Xeon Cluster. Overall {{results show that}} the radial decomposition approach dramatically increases scalability, while reducing the memory footprint - allowing for fusion device simulations at an unprecedented scale. After a decade where high-end computing (HEC) was dominated by the rapid pace of improvements to processor frequencies, the performance of next-generation supercomputers is <b>increasingly</b> differentiated by <b>varying</b> interconnect designs and levels of integration. Understanding the tradeoffs of these system designs is a key step towards making effective petascale computing a reality. In this work, we examine a new parallelization scheme for the Gyrokinetic Toroidal Code (GTC) [?] micro-turbulence fusion application. Extensive scalability results and analysis are presented on three HEC systems: the IBM BlueGene/P (BG/P) at Argonne National Laboratory, the Cray XT 4 at Lawrence Berkeley National Laboratory, and an Intel Xeon cluster at Lawrence Livermore National Laboratory. Overall results indicate that the new radial decomposition approach successfully attains unprecedented scalability to 131, 072 BG/P cores by overcoming the memory limitations of the previous approach. The new version is well suited to utilize emerging petascale resources to access new regimes of physical phenomena...|$|R
40|$|Title from PDF {{of title}} page (University of Missouri [...] Columbia, viewed on May 13, 2013). The entire thesis text is {{included}} in the research. pdf file; the official abstract appears in the short. pdf file; a non-technical public abstract appears in the public. pdf file. Dissertation advisor: Dr. Chi-Ren ShyuIncludes bibliographical references. Vita. Ph. D. University of Missouri [...] Columbia 2012. Dissertations, Academic [...] University of Missouri [...] Columbia [...] Computer science. "May 2012 "The trend in many scientific disciplines today, especially in biology and genetics, is towards larger scale experiments in which a tremendous amount of data is generated. As imaging of data becomes increasingly more popular in experiments related to phenotypes, the ability to perform high-throughput big data analyses and to efficiently locate specific information within these data based on <b>increasingly</b> complicated and <b>varying</b> search criteria is of great importance to researchers. This research develops several methods for high-throughput phenotype analysis. This notably includes a registration algorithm called variable object pattern matching for mapping multiple indistinct and dynamic objects across images and detecting the presence of missing, extra, and merging objects. Research accomplishments resulted in a number of unique advanced search mechanisms including a retrieval engine that integrates multiple phenotype text sources and domain ontologies and a search method that retrieves objects based on temporal semantics and behavior. These search mechanisms represent the first of their kind in the phenotype community. While this computational framework is developed primarily for the plant community, it has potential applications in other domains including the medical field...|$|R

