1|553|Public
5000|$|TRAK {{requires}} {{the type of}} every architecture description element in a TRAK architecture view to be explicitly shown so that each TRAK view {{can be read as}} a set of declarative statements. TRAK also allows a view to be constructed from textual statements. Since a TRAK view is a set of tuples / triples it is possible to use a graph or a set of RDF triples to present a TRAK view. TRAK also requires every block to have a name. The intent of this is to ensure that a TRAK architecture view is read as the author of the view meant it and <b>improve</b> <b>semantic</b> <b>consistency.</b> Presentation rules that apply to all TRAK architecture views are specified in the overall TRAK specification [...] (as 'Bye Laws').|$|E
40|$|Part 2 : Full PapersInternational audienceThe {{correctness}} {{of model}} transformation is an import research field in model-driven architecture. Syntactic correctness and <b>semantic</b> <b>consistency</b> are hot topics {{in the field}} of model transformation. Syntactic correctness has many mature solutions. However the validation of <b>semantic</b> <b>consistency</b> has some problems. Therefore, how to validate <b>semantic</b> <b>consistency</b> of model transformation is a major problem in model-driven development. In this paper, we propose a validation approach for <b>semantic</b> <b>consistency</b> of model transformation, which is based on pattern. We analyze some patterns in models and make these patterns as transformation pattern. We define transformation rule with transformation pattern and analyze three parts of semantic transformation. We present two theorems to validate <b>semantic</b> <b>consistency</b> of model transformation. Finally, we give a case to illustrate the effectiveness of our approach...|$|R
40|$|One {{fundamental}} problem of distant supervision is the noisy training corpus problem. In this paper, we propose a new distant supervision method, called <b>Semantic</b> <b>Consistency,</b> which can identify reliable instances from noisy instances by inspecting whether an instance {{is located in}} a semantically consistent region. Specifically, we propose a <b>semantic</b> <b>consistency</b> model, which first models the local subspace around an instance as a sparse linear combination of training instances, then estimate the <b>semantic</b> <b>consistency</b> by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method. © 2014 Association for Computational Linguistics. One {{fundamental problem}} of distant supervision is the noisy training corpus problem. In this paper, we propose a new distant supervision method, called <b>Semantic</b> <b>Consistency,</b> which can identify reliable instances from noisy instances by inspecting whether an instance {{is located in a}} semantically consistent region. Specifically, we propose a <b>semantic</b> <b>consistency</b> model, which first models the local subspace around an instance as a sparse linear combination of training instances, then estimate the <b>semantic</b> <b>consistency</b> by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method. © 2014 Association for Computational Linguistics...|$|R
5000|$|The {{development}} and specification of ontologies to ensure <b>semantic</b> <b>consistency</b> within organizations {{affiliated with the}} following priorities: ...|$|R
40|$|Product data {{integration}} {{is an essential}} issue for many e-commerce interoperable business systems. Core to this issue is how to maintain <b>semantic</b> <b>consistency</b> between heterogeneous product data that are semantically different in structure, concept and context. To resolve the issue, this paper proposes a transparent collaborative integration approach, which is built on a layered framework of messaging, structure, concept and collaboration. The key to this framework is three collaboration engines, which resolves <b>semantic</b> <b>consistency</b> issues between millions of contextual vocabularies of SMEs. 1...|$|R
40|$|OF THE THESIS Architecture for <b>Semantic</b> <b>Consistency</b> Optimization in Heterogeneous Virtual Environments By CARLOS D. CORREA Thesis Director: Professor Ivan Marsic Collaborative virtual environments with {{heterogeneous}} {{computing resources}} and user preferences often reduce data fidelity to accommodate such heterogeneity. Given the resource limitations and user preferences, {{the problem is}} to optimize the fidelity degradation so as to achieve maximum <b>semantic</b> <b>consistency</b> across the different data representations. Consistency maximization can be formulated as an integer-programming problem, wherein constraints are resource limitations and user preferences. In this thesis, a mathematical model for semantic interoperation and <b>semantic</b> <b>consistency</b> optimization is presented. Several formulations of the optimization problem are considered, some of which do not enforce topological constraints in degraded representation, while others do. The solutions to this problem result in reduced amounts of distributed data which conserve network bandwidth and other system resources...|$|R
40|$|In this paper, {{we propose}} {{a new idea}} that {{semantic}} frames are taken as the functions, and semantic categories (usually labeled with semantic roles) are taken as arguments. Thus, a semantic frame can apply to semantic categories if semantic categories {{are consistent with the}} semantic frame. Beta-reduction is used to represent the idea of the application of semantic frame to <b>semantic</b> categories. <b>Semantic</b> <b>consistency</b> is tested through β-unification. It is concluded <b>semantic</b> <b>consistency</b> problems are decidable if verbs are typable in the system of frames. ...|$|R
40|$|Abstract: Semantic {{integration}} {{is important in}} constructing electronic marketplaces (e-marketplaces). Although many previous researches have been done, the task of semantic integration and the mechanism of maintaining <b>semantic</b> <b>consistency</b> are still not well known. This paper proposes a Collaboration-Role-Based Semantic Integration Mechanism (CRB-SIM). It promotes collaboration as a key means of resolving semantic conflicts in concepts creation and editing, via a set of service-oriented collaboration roles. This proposal {{has contributed to the}} semantic integration research a novel approach with the features of the role-based and service-oriented <b>semantic</b> <b>consistency</b> maintenance of heterogeneous concepts. The applicability of proposed mechanism is exemplified by two application prototypes...|$|R
40|$|Consistency {{maintenance}} {{is a basic}} issue in computersupported cooperative editing systems. Aimed at providing a clear, appropriate and extended description on this issue, a new three-level consistency model is proposed: (1) Operation Consistency, which promises all operations are executed at the same order; (2) Content (Syntactic or Intention) Consistency, which promises the effects of all operations are the same at all sites; and (3) <b>Semantic</b> <b>Consistency,</b> which promises the user meanings are the same at all sites. Finally, evaluations on existing collaborative editing systems are discussed. Keywords CSCW, collaborative editing, consistency maintenance, operation <b>consistency,</b> content <b>consistency,</b> <b>semantic</b> <b>consistency...</b>|$|R
40|$|In {{the context}} of the Semantic Web, ontologies have to be usable by {{software}} agents as well as by humans. Therefore, they must meet explicit representation and consistency requirements. This article describes a method for managing the <b>semantic</b> <b>consistency</b> of an ontology of brain-cortex anatomy. The methodology relies on the explicit identification of the relationship properties and of the dependencies that might exist among concepts or relationships. These dependencies have to be respected for insuring the <b>semantic</b> <b>consistency</b> of the model. We propose a method for automatically generating all the dependent items. As a consequence, knowledge base updates are easier and safer...|$|R
40|$|Semantic {{collision}} {{is inevitable}} while building a domain ontology from heterogeneous data sources (semi-) automatically. Therefore, the <b>semantic</b> <b>consistency</b> is indispensable precondition {{for building a}} correct ontology. In this paper, a model-checking-based method is proposed to handle the <b>semantic</b> <b>consistency</b> problem {{with a kind of}} middle-model methodology, which could extract a domain ontology from structured and semistructured data sources semiautomatically. The method translates the middle model into the Kripke structure, and consistency assertions into CTL formulae, so a consistency checking problem is promoted to a global model checking. Moreover, the feasibility and correctness of the transformation is proved, and case studies are provided...|$|R
40|$|Abstract—Legal {{transfer}} of electronic title document across heterogeneous contexts is a challenging research problem in e-commerce and implies a wider e-commerce transaction scope. This kind of title transfer requires <b>semantic</b> <b>consistency,</b> confidentiality, integrity and legality between transferor and transferee. During the title transfer, cross-context <b>semantic</b> <b>consistency</b> {{must first be}} maintained between electronic title documents for legal title transfer while other requirements are considered. To solve the problem, this paper proposes a Cross-Context E-Title (CCET) approach, which satisfies the requirements for title transfer. Based on CCET approach, a CCET prototype is further implemented, on which some experiments are made to test the correctness and performance of the proposed approach. I I...|$|R
50|$|Transclusion of {{source code}} into {{software}} design or reference materials lets source code be presented within the document, but not interpreted {{as part of}} the document, preserving the <b>semantic</b> <b>consistency</b> of the inserted code in relation to its source codebase.|$|R
40|$|A very {{important}} issue in constructing global electronic markets is to enable semantic interoperation between fragmented electronic markets by constructing an interoperable electronic product catalogue (EPC) for product data exchange. Nevertheless, the heterogeneous EPCs are highly complex, which causes the severe semantic inconsistency that prevents from semantic interoperation between them. To reduce complexity {{and to provide a}} solution to <b>semantic</b> <b>consistency</b> maintenance, this paper has developed a theory of deconstruction and reconstruction. The theory deconstructs heterogeneous EPCs into ordered concepts, structures and semantic relations by a proposed articulation approach. It reconstructs the deconstructed units into a novel CONEX framework for component EPC integration. The CONEX maintains <b>semantic</b> <b>consistency</b> amongst heterogeneous component EPCs in structure, concept and context. 1...|$|R
40|$|Conference Name: 2011 International Conference on Asian Language Processing, IALP 2011. Conference Address: Penang, Malaysia. Time:November 15, 2011 - November 17, 2011. With {{the growth}} of {{exchange}} activities between four regions of cross strait, the problem to correctly convert between Traditional Chinese (TC) and Simplified Chinese (SC) {{become more and more}} important. Numerous one-to-many mappings and term usage differences {{make it more difficult to}} convert from SC to TC. This paper proposed a novel simplified-traditional Chinese character conversion model based on log-linear models, in which features such as language models and lexical <b>semantic</b> <b>consistency</b> weighs are integrated. When estimating lexical <b>semantic</b> <b>consistency</b> weighs, cross-language word-based semantic spaces were used. Experiments were conducted and the results show that the proposed model achieve better performance. ? 2011 IEEE...|$|R
40|$|We used buffer superposition, Delaunay {{triangulation}} skeleton line, {{and other}} methods {{to achieve the}} aggregation and amalgamation of the vector data, adopted the method of combining mathematical morphology and cellular automata to achieve the patch generalization of the raster data, and selected the two evaluation elements (namely, <b>semantic</b> <b>consistency</b> and <b>semantic</b> completeness) from the semantic perspective to conduct the contrast evaluation study on the generalization results from the two levels, respectively, namely, land type and map. The study results show that: (1) {{before and after the}} generalization, it is easier for the vector data to guarantee the area balance of the patch; the raster data&# 39;s aggregation of the small patch is more obvious. (2) Analyzing from the scale of the land type, most of the land use types of the two kinds of generalization result&# 39;s <b>semantic</b> <b>consistency</b> is above 0. 6; the semantic completeness of all types of land use in raster data is relatively low. (3) Analyzing from the scale of map, the <b>semantic</b> <b>consistency</b> of the generalization results for the two kinds of data is close to 1, while, in the aspect of semantic completeness, the land type deletion situation of the raster data generalization result is more serious...|$|R
40|$|Unified Modelling Language (UML) is {{the most}} popular {{modelling}} language use for software design in software development industries with a class diagram being the most frequently use diagram. Despite the popularity of UML, it is being affected by inconsistency problems of its diagrams at the same or different abstraction levels. Inconsistency in UML is mostly caused by existence of various views on the same system and sometimes leads to potentially conflicting system specifications. In general, syntactic consistency can be automatically checked and therefore is supported by current UML Computer-aided Software Engineering (CASE) tools. <b>Semantic</b> <b>consistency</b> problems, unlike syntactic consistency problems, there exists no specific method for specifying <b>semantic</b> <b>consistency</b> rules and constraints. Therefore, this research has specified twenty-four abstraction rules of class‟s relation semantic among any three related classes of a refined class diagram to semantically equivalent relations of two of the classes using a logical approach. This research has also formalized three vertical <b>semantic</b> <b>consistency</b> rules of a class diagram refinement identified by previous researchers using a logical approach and a set of formalized abstraction rules. The results were successfully evaluated using hotel management system and passenger list system case studies and were found to be reliable and efficient...|$|R
40|$|Abstract—This paper {{presents}} {{a framework for}} reasoning about the semantic impact of aspect weaving {{at the level of}} early design modeling. The framework is based on <b>semantic</b> <b>consistency</b> between a model and its projection in the woven model. If a weaving preserves the <b>semantic</b> <b>consistency</b> between the model and its projection, then it has no impact on the model. The underlying formalisms are Process Algebras. Firstly, notations for aspect weaving are given. Then, semantic preserved weaving is defined, through which the semantic impact of aspect weaving can be reasoned about. Understanding the impact of weaving can aid developers in foreseeing unintended aspect impacts and increase the reliability of the software, which is especially vital for aspect oriented system refinements. Keywords- aspect weaving; impact; reasoning about; process algebra; early design I...|$|R
40|$|The current {{metadata}} modeling {{techniques can}} not {{meet the needs of}} knowledge conception expression, knowledge organization, and metadata <b>semantic</b> <b>consistency</b> in geological domain. This paper introduces ontology and integrates this theory to geological domain metadata modeling. It adopts the first order logic equivalent algorithm and defines the metadata extended model as a quaternion group which is consists of geological term set, geological term definition set, attribute definition set and instance set. It also provides the formal description of each set. Finally the five steps for building geological domain metadata extended model are given. The result presents that this model not only provides the content standards for geological domain knowledge representation and knowledge organization, but also provides the basis for geological domain multi-source data and historical data integration and application in <b>semantic</b> <b>consistency...</b>|$|R
5000|$|Initiatives such as National Data Sharing and Accessibility Policy (NDSAP) ( [...] {{to ensure}} {{systemic}} and <b>semantic</b> <b>consistency</b> {{of data collection}} and data sharing), National e-Governance Plan (to automate administrative processes) and National Knowledge Network (NKN) (for data and resource sharing amongst education and research institutions), if implemented properly, should help {{improve the quality of}} work done by think tanks.|$|R
40|$|Well {{designed}} graphical {{user interfaces}} offer a high potential to increase the productivity of human users. The necessary condition for such a good performance is that the user interface represents the semantics of the underlying application in a clear and comprehensible way. This means, especially, that not only syntactical layout but also <b>semantic</b> <b>consistency</b> conditions between the various interaction objects have to be presented in a graphical user interface appropriately. This is usually termed semantic feedback. Many representation schemes have been proposed to express the properties of graphical man-machine interfaces. However, many of them concentrate on control flow design and do not easily scale up to realistic problems. In this contribution we propose a graph and graph grammar based approach which addresses the problem of <b>semantic</b> <b>consistency</b> of dialogs in {{graphical user interfaces}}. The main emphasis here lies on an appropriate specification of the consistency conditions betw [...] ...|$|R
40|$|Information agents {{integrate}} multiple distributed heterogeneous information sources. The challenging yet unsolved {{problem that}} remains, {{is to ensure}} the <b>semantic</b> <b>consistency</b> of the integrated data. In this paper {{we set out to}} develop a general approach to inconsistency management for information agents. It is implemented as part of the EDITtoTrEMBL system and applied on a large real-world problem in the domain of bioinformatics...|$|R
40|$|Abstract. The FOAF {{project has}} {{prominent}} importance for capturing human relations in Linked Data. We analyze the FOAF data structures and their extensions {{from the point}} of view of formal ontology and discuss problems inherent in its design. We also point out necessary considerations for transforming the FOAF data structures by supplying additional knowledge into them, while achieving/maintaining <b>semantic</b> <b>consistency.</b> ...|$|R
40|$|In {{e-business}} development, semantics-oriented {{document exchange}} is becoming important, {{because it can}} support crossdomain user connection, business transaction and collaboration. To provide this support, this paper proposes a DOC Mechanism to exchange semantically interoperable business documents between heterogeneous enterprise information systems. This mechanism is designed on a layered-sign network, which enables any exchanged e-business document to be independently interpretable without losing <b>semantic</b> <b>consistency...</b>|$|R
40|$|Generating {{alternative}} queries, {{also known}} as query suggestion, has long been proved useful to help a user explore and express his information need. In many scenarios, such suggestions can be generated from a large scale graph of queries and other accessory information, such as the clickthrough. However, how to generate suggestions while ensuring their <b>semantic</b> <b>consistency</b> with the original query remains a challenging problem. In this work, we propose a novel query suggestion algorithm based on ranking queries with the hitting time {{on a large scale}} bipartite graph. Without involvement of twisted heuristics or heavy tuning of parameters, this method clearly captures the <b>semantic</b> <b>consistency</b> between the suggested query and the original query. Empirical experiments on a large scale query log of a commercial search engine and a scientific literature collection show that hitting time is effective to generate semantically consistent query suggestions. The proposed algorithm and its variations can successfully boost long tail queries, accommodating personalized query suggestion, as well as finding related authors in research...|$|R
40|$|The {{maintenance}} of <b>semantic</b> <b>consistency</b> between numerous heterogeneous electronic product catalogues (EPC) that are distributed, autonomous, interdependent and emergent on the Internet is an unsolved {{issue for the}} existing heterogeneous EPC integration approaches. This article attempts {{to solve this issue}} by conceptually designing an interoperable EPC (IEPC) system through a proposed novel collaborative conceptualisation approach. This approach introduces collaboration into the heterogeneous EPC integration. It implies much potential for future e-marketplace research. It theoretically answers why real-world EPCs are so complex, how these complex EPCs can be explained and articulated in a PRODUCT MAP theory for heterogeneous EPC integration, how a <b>semantic</b> <b>consistency</b> maintenance model can be created to satisfy the three heterogeneous EPC integration conditions and implemented by adopting a collaborative integration strategy on a collaborative concept exchange model, and how this collaborative integration strategy can be realised on a collaboration mechanism. This approach has been validated through a theoretical justification and its applicability has been demonstrated in two prototypical e-business applications...|$|R
40|$|OBJECTIVE: In {{the context}} of the Semantic Web, ontologies have to be usable by {{software}} agents as well as by humans. Therefore, they must meet explicit representation and consistency requirements. This article describes a method for managing the <b>semantic</b> <b>consistency</b> of an ontology of brain-cortex anatomy. METHOD: The methodology relies on the explicit identification of the relationship properties and of the dependencies that might exist among concepts or relationships. These dependencies have to be respected for insuring the <b>semantic</b> <b>consistency</b> of the model. We propose a method for automatically generating all the dependent items. As a consequence, knowledge base updates are easier and safer. RESULT: Our approach is composed of three main steps: (1) providing a realistic representation, (2) ensuring the intrinsic consistency of the model and (3) checking its incremental consistency. The corner stone of ontological modeling lies in the expressiveness of the model and in the sound principles that structure it. This part defines the ideal possibilities of the ontology and is called realism of representation. Regardless of how well a model represents reality, the intrinsic consistency of a model corresponds to its lack of contradiction. This step is particularly important as soon as dependencies between relationships or concepts have to be fulfilled. Eventually, the incremental consistency encompasses the respect of the two previous criteria during the successive updates of the ontology. CONCLUSION: The explicit representation of dependencies among concepts and relationships in an ontology can be helpfully used to assist in the management of the knowledge base and to ensure the model's <b>semantic</b> <b>consistency...</b>|$|R
40|$|National audienceIn this paper, we {{account for}} the use of a set of {{reference}} domain (meta) models in order to check the <b>semantic</b> <b>consistency</b> of a set of instances. MDE techniques constitute the core of the validating process and allow a synchronized browsing of the domain metamodel and its related models. We come with a real case study within the field of temporal, periodical information management in web applications...|$|R
40|$|Traditionally coreference is {{resolved}} by satisfying {{a combination of}} salience, syntactic, semantic and discourse constraints. The acquisition of such knowledge is time-consuming, difficult and error-prone. Therefore, we present a knowledgeminimalist methodology of mining coreference rules from annotated text corpora. <b>Semantic</b> <b>consistency</b> evidence, which {{is a form of}} knowledge required by coreference, is easily retrieved from WordNet. Additional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts...|$|R
40|$|Recent {{research}} suggests that new objects appearing in real-world scenes are prioritized for eye fixations and by inference, for attentional processing. We examined whether <b>semantic</b> <b>consistency</b> modulates {{the degree to which}} new objects appearing in a scene are prioritized for viewing. New objects were added to photographs of real-world scenes during a fixation (new object with transient onset) or during a saccade (new object without transient onset). The added object was either consistent or inconsistent with the scene’s meaning. Object consistency did not affect the efficacy with which transient onsets captured attention, suggesting that transient motion signals capture attention in a bottom-up manner. Without a transient motion signal, the <b>semantic</b> <b>consistency</b> of the new object affected its prioritization with new inconsistent objects fixated sooner than new consistent objects, suggesting that attention prioritization without capture is a top-down memory-based phenomenon at least partially controlled by object identity and meaning. Saccades, the eye movements that take the eyes from one locus of fixation to another, are among the most common behaviours humans exhibit, as the eye...|$|R
40|$|Textile {{electronic}} marketplace, business vocabulary, business document, {{product data}} integration, vocabulary integration, document integration, <b>semantic</b> <b>consistency</b> maintenance, <b>semantic</b> interoperability, electronic commerce, electronic business. This paper {{has proposed a}} novel common textile vocabulary and document framework (TexVDF) in a collaborative network to enable cross-domain level business information sharing and business document exchange in a semantically consistent way. The approach to this framework is motivated through presenting some real-world examples of business inquiries with product specifications. By these examples, two problems are detected on how to achieve semantic commonality between cross-domain level business vocabularies for textile e-Marketplace mediators and how to allow specificity of cross-domain level common business document templates for local textile e-Marketplace mediators yet still maintaining <b>semantic</b> <b>consistency.</b> To solve these two problems, this paper has firstly reviewed CONEX technologies relevant to the newly developed TexVDF approach, which includes a TexVDF framework, a P 2 P collaborative textile concept mapping model and a textile business document template model. These two models have been demonstrated by examples {{to see how they}} should work. ...|$|R
5000|$|ROMULUS is a {{foundational}} ontology repository {{aimed at}} <b>improving</b> <b>semantic</b> interoperability. Currently {{there are three}} foundational ontologies in the repository: DOLCE, BFO and GFO.|$|R
40|$|Abstract—For the {{multilingual}} semantic interoperations in cross-organizational enterprise {{systems and}} e-commerce systems, <b>semantic</b> <b>consistency</b> {{is a research}} issue {{that has not been}} well resolved. This paper contributes to <b>improving</b> multilingual <b>semantic</b> interoperation by proposing a concept-connected near synonym (NSG) framework for concept disambiguation. NSG framework provides a vocabulary preprocessing process of collaborative vocabulary editing, which further ensures semantically consistent vocabulary for building semantically consistent business processes and documents between context-different information systems. The vocabulary preprocessing offered by NSG automates the process of finding potential near synonym sets and identifying collaboratively editable near synonym sets. The realization of NSG framework includes a probability model that computes concept values between concepts based on a newly introduced semantic relatedness method—SRCT. In this paper, SRCT-based methods are implemented and compared with some existing semantic relatedness methods. Experiments have shown that SRCT-based methods outperform the existing methods. This paper has made an improvement on the existing methods of semantic relatedness and reduces the collaboration cost of collaborative vocabulary editing. Index Terms—Data engineering, enterprise systems, industrial informatics, interenterprise multilingual interoperation, knowledg...|$|R
40|$|A principled {{and logical}} {{representation}} {{of the structure of}} the human body has led to conflicts with traditional representations of the same knowledge by anatomy textbooks. The examples which illustrate resolution of these conflicts suggest that stricter requirements must be met for <b>semantic</b> <b>consistency,</b> expressivity and specificity by knowledge sources intended to support inference than by textbooks and term lists. These next-generation resources should influence traditional concept representation, rather than be constrained by convention...|$|R
5000|$|ROMULUS is a {{foundational}} ontology repository {{aimed at}} <b>improving</b> <b>semantic</b> interoperability. Currently {{there are three}} foundational ontologies in the repository: DOLCE, BFO and GFO. Features of ROMULUS include: ...|$|R
40|$|Maintaining <b>semantic</b> <b>consistency</b> of data is a {{significant}} problem in distributed information systems, particularly those on which a business may depend. Our current work aims to use Event-B and the Rodin tools to support the specification and design of such systems {{in a way that}} integrates well into existing development processes. This paper presents Event-B patterns that may be used to represent recovery from timebounded inconsistency and illustrates their use in a model derived from industrial applications...|$|R
