0|10000|Public
50|$|Specialisterne {{provides}} {{services such}} as software testing, quality control and <b>data</b> <b>conversion</b> <b>for</b> business companies in Denmark.In addition Specialisterne assesses and trains people with ASD {{to meet the requirements}} of the business sector.|$|R
5000|$|The {{range of}} values allowed for each (except boolean) is {{implementation}} defined. Functions are provided <b>for</b> some <b>data</b> <b>conversions.</b> <b>For</b> <b>conversion</b> of [...] to , the following functions are available: [...] (which rounds to integer using banker's rounding) and [...] (rounds towards zero).|$|R
50|$|Cogniview is an {{international}} computer software company, focusing mainly on <b>data</b> <b>conversion</b> applications <b>for</b> the PC.|$|R
50|$|In January 2007, Chipidea {{acquired}} Nordic Semiconductor ASA's <b>Data</b> <b>Conversion</b> {{business unit}} <b>for</b> USD 5mm.|$|R
40|$|This article {{reports the}} {{implantation}} {{experience in the}} Pergamum System interface as from the headings links of the periodicals available on-line in the Databases, subscribed by the PUCPR. It {{emphasizes the importance of}} computer science in the inherent services of a library, speeding up the technical processes, assisting in the information search and making possible the <b>data</b> <b>conversion</b> <b>for</b> inclusion in its collection. It presents the architecture of the Pergamum System, a software that manages the services of the Library, and tells how the periodicals on-line had been made available by the implementation of the services among the Databases and the Pergamum System...|$|R
5000|$|The term [...] "controller" [...] {{refers to}} the {{influence}} that session border controllers have on the data streams that comprise sessions, as they traverse borders between {{one part of a}} network and another. Additionally, session border controllers often provide measurement, access control, and <b>data</b> <b>conversion</b> facilities <b>for</b> the calls they control.|$|R
40|$|Growing {{uncertainty}} in design parameters (and therefore, in design functionality) renders stochastic computing particularly promising, which represents and processes data as quantized probabilities. However, {{due to the}} difference in data representation, integrating conventional memory (designed and optimized for non-stochastic computing) in stochastic computing systems inevitably incurs a significant <b>data</b> <b>conversion</b> overhead. Barely any stochastic computing proposal to-date covers the memory impact. In this paper, as the first study of its kind {{to the best of}} our knowledge, we rethink the memory system design for stochastic computing. The result is a seamless stochastic system, StochMem, which features analog memory to trade the energy and area overhead of <b>data</b> <b>conversion</b> <b>for</b> computation accuracy. In this manner StochMem can reduce the energy (area) overhead by up-to 52. 8 % (93. 7 %) at the cost of at most 0. 7 % loss in computation accuracy...|$|R
40|$|Part 1 : GIS, GPS, RS and Precision FarmingInternational audienceThis paper {{summarizes}} the current main methods of heterogeneous <b>data</b> <b>conversion,</b> {{focuses on the}} necessity of establishing the <b>data</b> <b>conversion</b> specification <b>for</b> China’s agricultural product quantity safety, as well as specifies the main designs and solutions <b>for</b> <b>data</b> <b>conversion</b> structure, <b>data</b> <b>conversion</b> process and <b>data</b> <b>conversion</b> format in research. Establishing a <b>data</b> <b>conversion</b> specification is a pioneering work of applying modern information technology management methods in the research areas of agricultural macroeconomic policy-making system, which plays a positive role in promoting the study of modern agricultural information system. The establishment of <b>data</b> <b>conversion</b> specification can not only improve the level of agricultural information technology research, but can also create a better research and development environment {{for the study of}} agricultural information system...|$|R
50|$|The second XML and Web Services {{family of}} {{standards}} was initiated in August 2005 with {{the agreement to}} host {{the first year of}} the PRODML, Production XML Markup Language initiative, after which the PRODML SIG was formed. A major new release of the WITSML Standards was released in 2005. Also, an open source <b>data</b> <b>conversion</b> utility <b>for</b> LAS to WITSML well log dataset conversion was developed and released.|$|R
30|$|The {{composition}} of the sensor unit is generally relatively simple and mainly consists of a sensor and an analog-to-digital conversion function module, which is mainly responsible <b>for</b> <b>data</b> <b>conversion</b> <b>for</b> obtaining information in the detected area. The core part of the processor unit is an embedded system, which mainly includes a CPU, a memory, and the like, and is mainly responsible for controlling the nodes of the entire sensor and storing the collected data and processing {{the data obtained by}} other nodes. The main function of the wireless communication unit is to complete the data transmission without using a wired device. The main part of the energy supply unit is the power supply module, whose main function is to provide energy for the sensor nodes. There are also some other modules, such as positioning systems and mobile systems. Through the cooperation of these units, the wireless sensor network can operate normally.|$|R
40|$|Abstract Background Single {{nucleotide}} polymorphism (SNP) genotyping {{is a major}} {{activity in}} biomedical research. Scientists prefer to have a facile access to the results which may require <b>conversions</b> between <b>data</b> formats. First hand SNP data is often entered in or saved in the MS-Excel format, but this software lacks genetic and epidemiological related functions. A general tool to do basic genetic and epidemiological analysis and <b>data</b> <b>conversion</b> <b>for</b> MS-Excel is needed. Findings The SNP_tools package is prepared as an add-in for MS-Excel. The code is written in Visual Basic for Application, embedded in the Microsoft Office package. This add-in is an easy to use tool for users with basic computer knowledge (and requirements for basic statistical analysis). Conclusion Our implementation for Microsoft Excel 2000 - 2007 in Microsoft Windows 2000, XP, Vista and Windows 7 beta can handle files in different formats and converts them into other formats. It is a free software. </p...|$|R
30|$|Primary {{duodenal}} adenocarcinoma (PDC) is a {{rare and}} lethal disease, and cases with nodal or distant metastasis have a poor prognosis. There are several reports of unresectable duodenal adenocarcinoma responding to systemic chemotherapy. However, there is little <b>data</b> on <b>conversion</b> surgery <b>for</b> PDC with distant metastasis.|$|R
50|$|Geoprocessing is a GIS {{operation}} used {{to manipulate}} spatial data. A typical geoprocessing operation takes an input dataset, performs an operation on that dataset, and returns {{the result of}} the operation as an output dataset. Common geoprocessing operations include geographic feature overlay, feature selection and analysis, topology processing, raster processing, and <b>data</b> <b>conversion.</b> Geoprocessing allows <b>for</b> definition, management, and analysis of information used to form decisions.|$|R
50|$|Once the AppsFlyer SDK is {{installed}} in a mobile app, it provides advertisers with <b>conversion</b> <b>data</b> <b>for</b> their user acquisition and retention campaigns. Advertisers login to their dashboard, and can monitor which media source {{was responsible for the}} mobile activity. Based on this information, advertisers then are able to optimize their advertising budget.|$|R
40|$|Biomolecular {{modelling}} {{has provided}} computational simulation based methods for investigating biological processes from quantum chemical to cellular levels. Modelling such microscopic processes requires atomic {{description of a}} biological system and conducts in fine timesteps. Consequently the simulations are extremely computationally demanding. To tackle this limitation, different biomolecular models have to be integrated {{in order to achieve}} high-performance simulations. The integration of diverse biomolecular models needs to convert molecular data between different data representations of different models. This <b>data</b> <b>conversion</b> is often non-trivial, requires extensive human input and is inevitably error prone. In this paper we present an automated <b>data</b> <b>conversion</b> method <b>for</b> biomolecular simulations between molecular dynamics and quantum mechanics/molecular mechanics models. Our approach is developed around an XML data representation called BioSimML (Biomolecular Simulation Markup Language). BioSimML provides a domain specific data representation for biomolecular modelling which can effciently support data interoperability between different biomolecular simulation models and data formats...|$|R
40|$|OnTEAM metadata: GDSID: DOC- 2009 -Dec- 87; Attribute ID: LIBRARY-thesis_ba- 2009 - 002; Title: [GSI Bachelor 2009 - 02] Hardware and Software Implementation of a Radio Frequency High-Speed <b>Data</b> <b>Conversion</b> Unit <b>for</b> Digital Control Systems [02. 10. 2006]; Author(s) : Sanjari, Mohammad Shahab; Corporate author(s) :; Publication date: 20091210; Creator: manton; Creation date: 10. 12. 2009 15 : 10 : 41; Change date: 30. 09. 2010 14 : 57 : 31; Access: Welt; Attribute type: Text. Thesis. BA; Directory path: ['GSI Publications', 'GSI as Publisher']; Attribute path: ['Infrastructure', 'Library and Documentation', 'thesis_ba', 'Added in 2009 ']; File name(s) : ['DOC- 2009 -Dec- 87 - 1. pdf']; File title(s) : ['']; File access: ['nur berechtigte Gruppen'...|$|R
40|$|The use of {{unmanned}} aerial vehicles (UAVs) recently {{is growing}} on military and civil applications. Due to this demand, {{the control and}} navigation subsystems are on continuous improvement, on performance to process data, and decrease of physical dimensions. By using commercial-of-the-shelf solutions, {{it is possible to}} use robust and high performance data acquisition systems. This work presents the development of a data acquisition system for the sensors and actuators installed on the Vector-P UAV. This UAV is part of the studies on identification and control systems on the Aeronautics Systems Laboratory from the Aeronautics Institute of Technology. The unified analysis tool for the sensors and actuators installed in the Vector-P UAV was developed using the graphical programming tool LabVIEW with object-oriented programming concepts, with the goal to provide a reconfigurable environment to the user. This approach allows, in case of installing a new sensor, to reduce the development and testing efforts in the <b>data</b> <b>conversion</b> <b>for</b> this new sensor. Since the beginning of the project up {{to the development of the}} tool in this thesis, it was required to interface with multiple tools in order to have the engineering data from the sensors and actuators installed in the Vector-P UAV. From the use of the software developed in LabVIEW, the data visualization was unified into a single tool. Before this program was designed, the user had to interface with many different tools developed by the sensors manufacturer, as well as to develop a code in MATLAB to process those data...|$|R
30|$|To {{identify}} possible resistance (R) and susceptible (S) haplotypes for {{the regions}} {{significantly associated with}} resistance, haplotype association analysis was performed with Haploview. Phenotype data were converted to case–control datasets. According to the frequency distributions of LT data (Fig.  1 a, c), the accessions showing LT[*]>[*] 3 (S lesions) and[*]≤[*] 3 (R lesions) were assigned as “cases” and “controls”, respectively. There is no standard way to determine a threshold for the quantitative DLA data, so DLA[*]=[*] 15 % was arbitrarily set as the threshold <b>for</b> DLA <b>data</b> <b>conversion.</b> <b>For</b> rice accessions inoculated with D 41 - 2, the ratios of “cases (S): controls (R)” for LT and DLA were 168 : 136 and 210 : 94, respectively, and with 12 YL-DL- 3 - 2, the ratios were 142 : 86 and 142 : 86, respectively. In each LD block, the resistance/susceptibility haplotype was determined by Chi-square analysis and P value for haplotype frequencies in cases versus controls. Putative R and S haplotypes in all candidate regions were identified by using the SNP genotypes of the 44 K SNP data. Haplotype identity was assigned if the SNP genotypes were 100 % identical in length and sequences. Pearson correlation analysis was used for analysis of correlation between the resistance performance (LT and DLA) and total number of non-overlapped R/S haplotypes (in each rice accession) by using SAS 6.1 (SAS Inst. Inc., Cary, NC, USA). The total number of non-overlapped R or S haplotypes {{was defined as the}} sum of the number of R or S haplotypes identified from different phenotype datasets. The R or S haplotype identified from more than one phenotype was counted only once.|$|R
40|$|Alert Packaging Ltd” (Alert) (company I work), I monitor a {{great deal}} of {{activities}} in order to make them more efficient and one of those activities is sales support. In 11 year that I work in Alert there were implemented countless changes and various improvements, some of them worked some of them not that much. One thing never changed, Sales personnel always needed to be in the building or on the phone with sales support in order to have information about their orders. Alert is a small company, about 50 people. There are 7 sales people and 1 person in sales support. Alert has 130 active customers, who are looking for same information as sales executives. Purpose of this project is to provide tools for Sales team so they could carry out the tasks that bring value to the company Project scope is to develop Sales Support Platform (SSP), which will be web base and will provide access to Sales personnel and customers, who will be able access their relevant information. That information will be customized per User requirements and it will be live. Because it will be web base Customers and Sales team will be able access information from their mobile phones as well as their laptops, PC’s or even tables. The core code of the project is PHP and HTML, it supports vast variety other technologies that will be use in this project. Bootstrap – enhance user friendly environment and usability. Ajax – better interactivity, easier and faster navigation and compatibility. MySQL – connecting to DB. JavaScript and Excel VBA script – create various functions like search, print PDF and so on. Google Maps API and Bing Maps API to obtain customers location, display the map and directions. Python – automation of processes. XML – <b>data</b> <b>conversion</b> <b>for</b> the next process...|$|R
40|$|Abstract: Everolimus (ERL) is a {{recently}} developed mTOR inhibitor. This rapamycin analog {{is used to}} prevent acute rejection during kidney and heart transplantation. We review here published clinical trials and experiences concerning potential applications of ERL in liver and lung transplantation. Most of the <b>data</b> concern <b>conversion</b> <b>for</b> rescue situations, but {{a small number of}} studies have been con-ducted in de novo patients. In most cases, everolimus was used to spare renal function and to minimize calcineurine inhibitor use, but also, due to the antiproliferative properties of the drugs of this class, to control malignancy. Safety issues are an important consideration when deciding whether to maintain a patient on treatment. Therapeutic drug monitoring is strongly recommended, to achieve a mean whole-blood trough concentration of 6 ng/mL, with doses of 0. 5 to 1. 5 mg administered twice daily. There is solid evidence that ERL is a feasible and effective treatment, for a selected subset of patients, in the contexts of liver and lung transplantation...|$|R
40|$|The widepsread use of Computer Integrated Manufacturing, {{the quest}} for Total Quality Management, and the {{increasing}} role of technology management have given an added importance to today 2 ̆ 7 s application of DBMS and the accuracy of its data. Indeed, the empirical evidence has documented data inaccuracy as the most (difficult part of implementing information systems softwares. This paper explores {{the process of building}} a data base necessary to ensure the quality of the <b>data</b> <b>for</b> <b>conversion</b> into information. This article also illustrates the application of quality control techniques for maintaining data base quality...|$|R
40|$|Sprinkler {{selection}} {{influences the}} water distribution uniformity of center pivot irrigation systems. The sprinkling uniformity {{of the center}} pivot is crucial for the yield and quality of crops on a large scale. Rotating and fixed spray plate sprinklers (RSPSs and FSPSs) {{are the two most}} popular types. However, sprinkler selection is mainly based on price, not on differences in performance between them. Under low-wind field conditions, the water distributions of individual RSPSs and FSPSs with different nozzles (2. 78, 4. 76, and 6. 75 mm in diameter) were measured using a catch can method. Cubic spline interpolation was used <b>for</b> <b>data</b> <b>conversion</b> <b>for</b> FSPS measurements; the nozzle configuration model was used to simulate the water distribution of the same-nozzle-sprinkler pipe section and full circular irrigated areas in a simulated center pivot under three sprinkler intervals of 1. 5, 3. 0, and 4. 5 m respectively. Results showed that (1) individual RSPSs distributed the most water around the sprinkler, whereas individual FSPSs distributed the most water over a ring-shaped region at the periphery of the sprinkler, and the wetted radii for RSPSs and FSPSs ranged from 4. 88 to 7. 05 m and from 5. 02 to 6. 85 m, respectively; (2) same-nozzle-sprinkler pipe sections of RSPSs distributed the most water around the central axes of the pipe sections, and their sprinkling uniformities were 44. 7 %– 51. 0 %, whereas FSPSs distributed the most water over both sides of the axes symmetrically, and less water around the axes, with sprinkling uniformities of 40. 3 %– 58. 0 %; and (3) the sprinkling uniformities of the full circular irrigated areas were 85. 8 %– 91. 7 % and 85. 8 %– 86. 2 % when using RSPSs and FSPSs, respectively, under different sprinkler intervals, and the uniformities were 3. 1 % and 6. 2 % higher using RSPSs than FSPSs with sprinkler intervals of 3. 0 and 4. 5 m, respectively. RSPS accommodated larger sprinkler intervals (> 3. 0 m) and maintained superior sprinkling performance when compared with FSPS...|$|R
40|$|A {{number of}} Landsat-based {{coniferous}} forest stratum maps {{have been created}} of the Eldorado National Forest in California. These maps were produced in raster image format which is not directly usable by the U. S. Forest Service's vector-based Wildland Resource Information System (WRIS). As a solution, raster-to-vector conversion software has been developed for processing classified images into polygonal <b>data</b> structures. Before <b>conversion,</b> however, the digital classification images must be simplified to remove high spatial variance ('noise', 'speckle') and meet a USFS ten acre minimum requirement. A post-processing (simplification) strategy different from those commonly used in raster image processing may be desired for preparing maps <b>for</b> <b>conversion</b> to vector format, because simplification routines typically permit diagonal connections {{in the process of}} reclassifying pixels and forming new polygons. Diagonal connections are often undesirable when converting to vector format because they permit polygons to effectively cross over each other and occupy a common location. Three alternative methodologies are discussed for simplifying raster <b>data</b> <b>for</b> <b>conversion</b> to vector format...|$|R
40|$|This report {{includes}} {{data on the}} Ohio River, Modified Ohio River, and Panama Canal type linkages. The study contains necessary <b>data</b> <b>for</b> <b>conversion</b> to prototype torque for {{all three}} of the different types of linkages. For direct connected type machines, prototype tests were made at Claiborne Locks and results of the tests are included herein for the determination of gate torque for any proposed direct connected lock machine of similar proportions. A curve of gate torque plotted against percentage of gate closure has been included so that torque at any other submergence or time of operation can be computed by application of Froude's law, adjusting the submergence and time to suit the new condition...|$|R
40|$|A {{mathematical}} {{model for the}} free radical polymerization of styrene is developed to predict the steady-state and dynamic behavior of a continuous process. Special emphasis is given for the kinetic and thermodynamic models, where the most sensitive parameters were estimated using data from an industrial plant. The thermodynamic model {{is based on a}} cubic equation of state and a mixing rule applied to the low-pressure vapor-liquid equilibrium of polymeric solutions, suitable for modeling the auto-refrigerated polymerization reactors, which use the vaporization rate to remove the reaction heat from the exothermic reactions. The simulation results show the high predictive capability of the proposed model when compared with plant <b>data</b> <b>for</b> <b>conversion,</b> average molecular weights, polydispersity, melt flow index, and thermal properties for different polymer grades...|$|R
30|$|All {{experiments}} were performed in duplicate. The yield coefficients were estimated through linear regression of product {{as a function}} of substrate consumption with a regression correlation of above 0.95. Yxo values were similarly determined through linear regression of consumed oxygen {{as a function of}} biomass concentration. Maximum specific growth rate (μ max) was estimated from an exponential fit of biomass concentration as a function of time during exponential growth. The specific uptake rates of oxygen ro were estimated from Yxo by multiplication with μmax. Standard errors on the mean of at least two duplicate {{experiments were}} calculated and have been included in the assessment of <b>data.</b> <b>For</b> <b>conversion</b> of gram dry weight to c-mole basis, a molecular weight of 23.57  g DW cmol - 1 was used.|$|R
40|$|Ecological niche {{modelling}} combines species occurrence {{points with}} environmental raster layers {{in order to}} obtain models for describing the probabilistic distribution of species. The process to generate an ecological niche model is complex. It requires dealing with a large amount of data, use of different software packages <b>for</b> <b>data</b> <b>conversion,</b> <b>for</b> model generation and for different types of processing and analyses, among other functionalities. A software platform that integrates all requirements under a single and seamless interface would be very helpful for users. Furthermore, since biodiversity modelling is constantly evolving, new requirements are constantly being added in terms of functions, algorithms and data formats. This evolution must be accompanied by any software intended to be used in this area. In this scenario, a Service-Oriented Architecture (SOA) is an appropriate choice for designing such systems. According to SOA best practices and methodologies, the design of a reference business process must be performed prior to the architecture definition. The purpose is to understand the complexities of the process (business process in this context refers to the ecological niche modelling problem) and to design an architecture able to offer a comprehensive solution, called a reference architecture, that can be further detailed when implementing specific systems. This paper presents a reference business process for ecological niche modelling, as part of a major work focused on the definition of a reference architecture based on SOA concepts that will be used to evolve the openModeller software package for species modelling. The basic steps that are performed while developing a model are described, highlighting important aspects, based on the knowledge of modelling experts. In order to illustrate the steps defined for the process, an experiment was developed, modelling the distribution of Ouratea spectabilis (Mart.) Engl. (Ochnaceae) using openModeller. As a consequence of the knowledge gained with this work, many desirable improvements on the modelling software packages have been identified and are presented. Also, a discussion on the potential for large-scale experimentation in ecological niche modelling is provided, highlighting opportunities for research. The results obtained are very important for those involved in the development of modelling tools and systems, for requirement analysis and to provide insight on new features and trends for this category of systems. They can also be very helpful for beginners in modelling research, who can use the process and the experiment example as a guide to this complex activity. (c) 2008 Elsevier B. V. All rights reserved...|$|R
40|$|Faster RCNN model Sequence to Sequence {{container}} and char_rae recurrent autoencoder model Reshape Layer that reshapes the input [# 221] Pip requirements in requirements. txt updated to latest versions [# 289] Remove deprecated data loaders and update docs Use NEON_DATA_CACHE_DIR envvar as archive dir to store DataLoader ingested <b>data</b> Eliminate type <b>conversion</b> <b>for</b> FP 16 for CUDA compute capability >= 5. 2 Use GEMV kernels for batch size 1 Alter delta buffers for nesting of merge-broadcast layers Support for ncloud real-time logging Add fast_style Makefile target Fix Python 3 builds on Ubuntu 16. 04 Run setup. py for sysinstall to generate version. py [# 282] Fix broken link in mnist docs Fix conv/deconv tests for CPU execution and fix i 32 data type Fix for average pooling with batch size 1 Change default scale_min to allow random cropping if omitted Fix yaml loading Fix bug with image resize during injest Update {{references to the}} ModelZoo and neon examples to their new location...|$|R
40|$|The SOUP {{instrument}} {{is designed to}} obtain diffraction-limited digital images of the sun with high photometric accuracy. The Video Processor originated from the requirement to provide onboard real-time image processing, both to reduce the telemetry rate and to provide meaningful video displays of scientific data to the payload crew. This original concept {{has evolved into a}} versatile digital processing system with a multitude of other uses in the SOUP program. The central element in the Video Processor design is a 16 -bit central processing unit based on 2900 family bipolar bit-slice devices. All arithmetic, logical and I/O operations are under control of microprograms, stored in programmable read-only memory and initiated by commands from the LSI- 11. Several functions of the Video Processor are described, including interface to the High Rate Multiplexer downlink, cosmetic and scientific <b>data</b> processing, scan <b>conversion</b> <b>for</b> crew displays, focus and exposure testing, and use as ground support equipment...|$|R
40|$|Technology {{advances}} are enabling increasingly data-intensive applications, {{ranging from}} peer-to-peer file sharing, to multimedia, to remote graphics and data visualization. One outcome is the considerable memory pressure {{imposed on the}} machines involved, caused by application-specific data movements and by repeated crossings of user/kernel boundaries. We address this problem with a novel system service, termed KStreams, a general facility for manipulating data without using intermediate buffers when it moves across multiple kernel objects, like files or sockets. KStreams {{may be used to}} implement kernel-level services that range from application-specific implementations of sendfile commands, to data mirroring or proxy functions, to fast path <b>data</b> <b>conversions</b> and transformations <b>for</b> <b>data</b> streaming. The KStreams API permits individual applications to define fast path operations, which will then execute at kernel level and if desired, without further application involvement. By placing application-specific data manipulations into data movement fast paths, user/kernel boundary crossings are avoided. By operating on data streams `in-flight', data buffering is made unnecessary, thereby further reducing the memory pressure imposed on machines. KStreams is implemented [...] ...|$|R
40|$|Abstract. Based on the {{analysis}} of general <b>data</b> <b>conversion</b> techniques of heterogeneous System, an application framework based on ESB is proposed {{in order to achieve}} <b>data</b> <b>conversion</b> in heterogeneous system. The structure of <b>data</b> converter, <b>conversion</b> technique of unstructured data, service description workflow used by WSDL and method of messaging are discussed. Experimental results show that the technique has some of advantages in solving <b>data</b> <b>conversion</b> in lightweight heterogeneous system...|$|R
40|$|<b>Data</b> <b>conversion</b> is {{the crucial}} {{interface}} between {{the real world}} and digital processing systems. Analogue-to-digital converters and digital-to-analogue converters are two key conversion devices and used as the interface. Up to now, the conventional ADCs based on Nyquist sampling theorem are facing a critical challenge: the resolution and the sampling rate must be radically increased when some applications such as radar detection and ultra-wideband communication emerge. The offset of comparators and the setup time of sample-and-hold circuits, however, limit the resulution and clock rate of ADCs. Alternatively, in some applications such as speech, temperature sensor, etc. signals remain possibly unchanged for prolonged periods with brief bursts of significant activity. If trational ADCs are employed in such circumstances a higher bandwidth is required for transmitting the converted samples. On the other hand, sampling signals with an extremely high clock rate are also required for converting the signals with the feature of sparsity in time domain. The level-crossing sampling scheme (LCSS) is one of the <b>data</b> <b>conversions</b> suitable <b>for</b> converting signals with the sparsity feature and brief bursts of signigicant activity. due to the traditional LCSS with a fixed clock rate being limited in applications a novel irregular <b>data</b> <b>conversion</b> scheme called analogue-to-information system (AIS) is proposed in this thesis. The AIS is typically based upon LCSS, but an adjustable clock generator and a real time data compression scheme are applied to it. As the system-level simulations results of AIS show {{it can be seen that}} a data transmission saving rate nearly 30 % is achieved for different signals. PLLs with fast pull-in and locking schemes are very important when they are applied in TDMA systems and fequency hopping wireless systems. So a novel triple path nonlinear phase frequency detector (TPNPFD) is also proposed in this thesis. Compared to otherPFDs, the pll-in and locking time in TPNPFD is much shorter. A proper transmission data format can make the recreation of the skipped samples and the reconstruction of the original signal more efficient, i. e. they can be achieved in a minimum number of the received data without increasing much more hardware complexity. So the preliminary data format used for transmitting the converted data from AIS is also given in the final chapter of this thesis for future works...|$|R
40|$|In this thesis, novel analog-to-digital and digital-to-analog {{generalized}} time-interleaved variable bandpass sigma-delta modulators are designed, analysed, {{evaluated and}} implemented that are suitable <b>for</b> high performance <b>data</b> <b>conversion</b> <b>for</b> a broad-spectrum of applications. These generalized time-interleaved variable bandpass sigma-delta modulators can perform noise-shaping for any centre frequency from DC to Nyquist. The proposed topologies are well-suited for Butterworth, Chebyshev, inverse-Chebyshev and elliptical filters, where designers have {{the flexibility of}} specifying the centre frequency, bandwidth {{as well as the}} passband and stopband attenuation parameters. The application of the time-interleaving approach, in combination with these bandpass loop-filters, not only overcomes the limitations that are associated with conventional and mid-band resonator-based bandpass sigma-delta modulators, but also offers an elegant means to increase the conversion bandwidth, thereby relaxing the need to use faster or higher-order sigma-delta modulators. A step-by-step design technique has been developed for the design of time-interleaved variable bandpass sigma-delta modulators. Using this technique, an assortment of lower- and higher-order single- and multi-path generalized A/D variable bandpass sigma-delta modulators were designed, evaluated and compared in terms of their signal-to-noise ratios, hardware complexity, stability, tonality and sensitivity for ideal and non-ideal topologies. Extensive behavioural-level simulations verified that one of the proposed topologies not only used fewer coefficients but also exhibited greater robustness to non-idealties. Furthermore, second-, fourth- and sixth-order single- and multi-path digital variable bandpass digital sigma-delta modulators are designed using this technique. The mathematical modelling and evaluation of tones caused by the finite wordlengths of these digital multi-path sigmadelta modulators, when excited by sinusoidal input signals, are also derived from first principles and verified using simulation and experimental results. The fourth-order digital variable-band sigma-delta modulator topologies are implemented in VHDL and synthesized on Xilinx® SpartanTM- 3 Development Kit using fixed-point arithmetic. Circuit outputs were taken via RS 232 connection provided on the FPGA board and evaluated using MATLAB routines developed by the author. These routines included the decimation process as well. The experiments undertaken by the author further validated the design methodology presented in the work. In addition, a novel tunable and reconfigurable second-order variable bandpass sigma-delta modulator has been designed and evaluated at the behavioural-level. This topology offers a flexible set of choices for designers and can operate either in single- or dual-mode enabling multi-band implementations on a single digital variable bandpass sigma-delta modulator. This work is also supported by a novel user-friendly design and evaluation tool that has been developed in MATLAB/Simulink that can speed-up the design, evaluation and comparison of analog and digital single-stage and time-interleaved variable bandpass sigma-delta modulators. This tool enables the user to specify the conversion type, topology, loop-filter type, path number and oversampling ratio...|$|R
40|$|Abstract: This paper {{presents}} a methodology {{and a model}} <b>for</b> <b>data</b> <b>conversion</b> or translation. The model assumes that both source and target systems are available and that conversion interfaces {{may be required to}} interact between these systems and the conversion system. To achieve <b>data</b> <b>conversion</b> or translation using this approach, two languages are needed: 1) a language to describe the data structures, and 2) a language to specify the mapping between source and target data. This paper describes these two languages, DE-FINE and CONVERT and gives numerous examples to show the capabilities of these languages and how they can be used in <b>data</b> <b>conversion</b> and restructuring. Both languages are high level and nonprocedural and have the power to deal with most situations en-countered in <b>data</b> <b>conversion</b> processes. In addition, the paper also describes some of the facilities in the languages specifically de-signed for data checking in a <b>data</b> <b>conversion</b> process...|$|R
40|$|This paper {{describes}} {{the issues involved}} in sharing data among processes executing co-operatively in a heterogeneous computing environment. In a heterogeneous environment, the physical representation of a data object {{may need to be}} transformed when the object is moved from one type of processor to another. As a part of a larger project to build a heterogeneous distributed shared memory system we developed an automated tool <b>for</b> generating the <b>conversion</b> routines that are used to implement representation <b>conversion</b> <b>for</b> <b>data</b> objects. We developed a novel method for processing source programs that allowed us to extract detailed information about the physical representation of data objects without access to the source code of the compilers that were generating those representations. A performance comparison with the more general XDR heterogeneous <b>data</b> <b>conversion</b> package shows that the customized conversion routines that we generate are 4 to 8 times faster. key words: Heterogeneity External data representation Automatic <b>data</b> <b>conversion</b> XDR Mermaid HETEROGENEOUS DISTRIBUTED SHARED MEMORY Distributed shared memory (DSM) was developed by Li 1 as a mechanism for providing a logically shared and consistent virtual memory on processors connecte...|$|R
40|$|Universal {{access to}} {{information}} and data is an important goal of current research. Previously different forms of information and data, captured by different devices or held on different systems, were isolated. However, the development of different networks has created the “glue ” by which the overall goal of universal access can be achieved. Another important goal {{is the development of}} personalized information and communication systems in which the user has increasing control over access to and presentation of information and data. The current challenge is to bring these two ideas together into integrated systems. This paper investigates a personalized communication system that is able to intercept, filter, convert and direct communications and data, thereby giving the user control over the delivery and presentation of information. Such a system must provide flexible and intuitive mechanisms whereby users can specify their preferences and control access to and presentation of information and data. It must cater for user mobility and handle routing and <b>data</b> <b>conversion.</b> The need <b>for</b> these components is motivated by using an example scenario to derive the design requirements for the architecture. The implementation technology of components of the architecture is described briefly. ...|$|R
