1|10000|Public
40|$|A {{recent work}} {{obtained}} closed-form solutions to the. problem of optimally grouping a multi-item inventory into subgroups {{with a common}} order cycle per group, when the <b>distribution</b> <b>by</b> <b>value</b> of the inventory could be described by a Pareto function. This paper studies {{the sensitivity of the}} optimal subgroup boundaries so obtained. Closed-form expressions have been developed to find intervals for the subgroup boundaries for any given level of suboptimality. Graphs have been provided to aid the user in selecting a cost-effective level of aggregation and choosing appropriate subgroup boundaries for a whole range of inventory distributions. The results of sensitivity analyses demonstrate the availability of flexibility in the partition boundaries and the cost-effectiveness of any stock control system through three groups, and thus also provide a theoretical support to the intuitive ABC system of classifying the items...|$|E
3000|$|... thus also a {{significantly}} larger bound than (3), but still {{dependent on the}} <b>distribution</b> <b>by</b> the <b>value</b> of γ [...]...|$|R
40|$|Interpolating scaling {{functions}} give {{a faithful}} {{representation of a}} localized charge <b>distribution</b> <b>by</b> its <b>values</b> on a grid. For such charge distributions, using a Fast Fourier method, we obtain highly accurate electrostatic potentials for free boundary conditions {{at the cost of}} O(N log N) operations, where N is the number of grid points. Thus, with our approach, free boundary conditions are treated as efficiently as the periodic conditions via plane wave methods. Comment: 5 pages, 3 figure...|$|R
30|$|A {{quantitative}} way {{to compare}} global <b>distributions</b> is <b>by</b> performing two-sample Kolmogorov-Smirnov tests (K-S tests) (Kolmogorov 1933; Smirnov 1948). The K-S test gives {{the likelihood that}} two samples are drawn from the same <b>distribution,</b> quantified <b>by</b> the <b>value</b> called p. When the p-value is below five percent, the distributions {{are considered to be}} significantly different.|$|R
40|$|This paper {{provides}} some {{properties of the}} linear-exponential <b>distribution</b> <b>by</b> recorded <b>values.</b> Some recurrence relations for single moments of values are derived. In the meantime, all the higher order moments of recorded values may reduce to the first recorded values and simplify the processes of the higher order moments of recorded values. Simultaneously, the expected values of the product moments of recorded values are derived. From these properties, the paper predicts the future unknown recorded values from the given recorded values...|$|R
40|$|We {{explore the}} tail of {{patented}} invention <b>value</b> <b>distributions</b> <b>by</b> using <b>value</b> estimates obtained directly from patent holders. The paper focuses on those full-term German patents of the application year 1977 which were held by West German and U. S. residents. The most valuable patents in our data account for a large fraction of the cumulative value over all observations. Several tests are conducted to pin down more precisely {{the nature of the}} high-value tail distribution. Among the Pareto, Singh-Maddala and log normal distributions, the log normal appears to provide the best fit to our patented invention value data. [...] Patents,Skew Distributions...|$|R
5000|$|Energy {{distance}} and E-statistic were considered as N-distances and N-statistic in Zinger A.A., Kakosyan A.V., Klebanov L.B. Characterization of <b>distributions</b> <b>by</b> means of mean values of some statistics {{in connection with}} some probability metrics, Stability Problems for Stochastic Models. Moscow, VNIISI, 1989,47-55. (in Russian), English Translation: A characterization of <b>distributions</b> <b>by</b> mean <b>values</b> of statistics and certain probabilistic metrics A. A. Zinger, A. V. Kakosyan, L. B. Klebanov in Journal of Soviet Mathematics (1992). In the same paper there was given a definition of strongly negative definite kernel, and provided a generalization on metric spaces, discussed above. The book [...] gives these results and their applications to statistical testing as well. The book contains also some applications to recovering the measure from its potential.|$|R
50|$|Skewness in a data series may {{sometimes}} be observed not only graphically but by simple {{inspection of the}} values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of 50. We can transform this sequence into a negatively skewed <b>distribution</b> <b>by</b> adding a <b>value</b> far below the mean, e.g. (40, 49, 50, 51). Similarly, {{we can make the}} sequence positively skewed <b>by</b> adding a <b>value</b> far above the mean, e.g. (49, 50, 51, 60).|$|R
40|$|This report aims to {{modulate}} the passage {{probability of a}} pulse <b>by</b> the instantaneous <b>value</b> of an analogue input signal. Up to now, some circuits where the status probabilities may be controlled by input from the outside have been found. However, it is considered {{that in order to}} freely vary the probability <b>distribution</b> <b>by</b> the instantaneous <b>values</b> of input signals, a still more complicated circuit construction is necessary. This paper describes that such circuits can be realized simply by using shiftregisters, random pulses, and analogue operation circuits...|$|R
40|$|AbstractLet G be the {{conformal}} {{group of}} a non-Euclidean Jordan algebra and let P be the maximal parabolic subgroup canonically associated to G. Standard intertwining operators between spherical degenerate principal series induced from P determine Zeta distributions. In this article, we obtain a functional equations for Zeta <b>distributions</b> <b>by</b> considering boundary <b>values</b> of Poisson transforms. We relate the constant {{occurring in the}} Zeta functional equation to that occurring in the functional equation of Wallach's Generalized Jacquet functionals...|$|R
30|$|Winsorizing replaces all {{the values}} of a {{particular}} variable at both tails of the <b>distribution</b> <b>by</b> a specified <b>value.</b> In our case, we replace all BMI values below the 1 st percentile of BMI distribution (17.2) with a BMI value of 17.2. We also replace all BMI values above the 99 th percentile (45.5) with a BMI value of 45.5. This allows us to use the extreme observations without outliers having {{a big impact on}} the estimates. For a more detailed discussion, see Barnett and Lewis (1994).|$|R
40|$|Policy {{gradient}} {{methods have}} had great success in solving continuous control tasks, yet the stochastic nature of such problems makes deterministic value estimation difficult. We propose an approach which instead estimates a <b>distribution</b> <b>by</b> fitting the <b>value</b> function with a Bayesian Neural Network. We optimize an α-divergence objective with Bayesian dropout approximation {{to learn and}} estimate this distribution. We show that using the Monte Carlo posterior mean of the Bayesian value function distribution, rather than a deterministic network, improves stability and performance of policy gradient methods in continuous control MuJoCo simulations. Comment: Accepted to Bayesian Deep Learning Workshop at NIPS 201...|$|R
50|$|The ziggurat {{algorithm}} is an algorithm for pseudo-random number sampling. Belonging {{to the class}} of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables. The {{algorithm is}} used to generate values from a monotone decreasing probability distribution. It can also be applied to symmetric unimodal distributions, such as the normal <b>distribution,</b> <b>by</b> choosing a <b>value</b> from {{one half of the}} distribution and then randomly choosing which half the value is considered to have been drawn from. It was developed by George Marsaglia and others in the 1960s.|$|R
40|$|A {{new measure}} of {{reddening}} (E_(B-V) ∼ 0. 00) has been {{obtained from the}} comparison between the observed and the theoretical intensity decrement for 20 emission lines of the Heii Fowler (n→ 3) series. This value has been confirmed by the STIS and IUE continuum <b>distribution,</b> and <b>by</b> the <b>value</b> of n_H from the damped profile of the IS H Ly-α line. We have obtained very accurate measurements for about thirty Bowen lines of Oiii and a precise determination of the efficiency in the O 1 and O 3 excitation channels (18...|$|R
40|$|Society of Kampung Baru village {{living in}} coastal areas utilize ground water to filling {{the need of}} ground water consumption. Over take of Ground water made {{different}} between  ground water level and sea surface level so get seawater intrusion. Therefore, research has done to knowing depth <b>value</b> of  <b>distribution</b> <b>by</b> resistiviting <b>value</b> and well water of NaCl contents in Kampung Baru village. Based on result of geoelectric data processing 2 D Wenner configuration on 3 tracks with each tracks length 200 m which made by Res 2 dinv software, then sea intrusion start from 0. 93 - 19. 6 m with resistivity value 0. 2 - 6. 40 Ωm. Based on the result of laboratory tests on three sample showed levels ranging Na is 12, 055 - 14, 027 mg/l and Cl has a value that ranges between 52, 18 - 94, 58 mg/l. Where this value is below the maximum levels that may be consume...|$|R
40|$|We {{characterize}} the time {{evolution of a}} d-dimensional probability <b>distribution</b> <b>by</b> the <b>value</b> of its final entropy. If it is near the maximally-possible value we call the evolution mixing, if it is near zero we say it is purifying. The evolution {{is determined by the}} simplest non-linear equation and contains a d times d matrix as input. Since we are not interested in a particular evolution but in the general features of evolutions of this type, we take the matrix elements as uniformly-distributed random numbers between zero and some specified upper bound. Computer simulations show how the final entropies are distributed over this field of random numbers. The result is that the distribution crowds at the maximum entropy, if the upper bound is unity. If we restrict the dynamical matrices to certain regions in matrix space, for instance to diagonal or triangular matrices, then the entropy distribution is maximal near zero, and the dynamics typically becomes purifying. Comment: 8 pages, 8 figure...|$|R
40|$|How to {{determine}} the default loss distribution of the whole credit portfolio is the most critical part for pricing CDOs. This paper follows Kalemanova et al (2007) and assesses the pricing efficiency of both one-factor Gaussian Copula model the Normal Inverse Gaussian (NIG) Copula model during the turbulent market condition by using data in 2008 and 2009. In addition, we test the price impact of the skewed NIG <b>distribution</b> <b>by</b> adjusting the <b>value</b> of the two parameters. The results show that NIG Copula performs much better than Gaussian Copula, {{and the introduction of}} the asymmetry factor in NIG distribution can further improve the modeling results...|$|R
50|$|In {{probability}} theory and statistics, the generalized extreme value (GEV) distribution {{is a family}} of continuous probability distributions developed within extreme value theory to combine the Gumbel, Fréchet and Weibull families also known as type I, II and III extreme <b>value</b> <b>distributions.</b> <b>By</b> the extreme <b>value</b> theorem the GEV distribution is the only possible limit distribution of properly normalized maxima of a sequence of independent and identically distributed random variables. Note that a limit distribution need not exist: this requires regularity conditions on {{the tail of the}} distribution. Despite this, the GEV distribution is often used as an approximation to model the maxima of long (finite) sequences of random variables.|$|R
30|$|However, {{the cells}} with the {{different}} GDLs exhibit substantial differences regarding current density distributions. In Fig.  4 a current density distributions recorded at different loads using pristine and patterned MPLs are shown. In the provided test the humidification of gases was 50 % RH. Evidently, {{in the case}} of the pristine MPL the maximum current density is located clearly in the gas inlet region. At the cell outlet the performance is worst. The homogeneity of the <b>distribution,</b> represented <b>by</b> the <b>value</b> a (percentage of values that are within ±  20 % of average), is around a =  60 – 80 %.|$|R
40|$|Abstract. Because of the {{challenging}} data association in similar environments, {{a large number}} of particles are needed to improve the precision in particle filtering SLAM (simultaneous localization and mapping). An improved particle filter SLAM algorithm based on particle swarm optimization in similar environments is proposed. A multimode proposal <b>distribution</b> is acquired <b>by</b> combining the information of the odometry and the laser scanning. Particles are concentrated to the region of each posterior probability <b>distribution</b> maximum <b>value</b> <b>by</b> PSO. The performance of the conventional particle filter SLAM is improved. The simulation experiment results prove its effectiveness and feasibility...|$|R
40|$|A new {{inversion}} {{method of}} identifying stress {{is applied to}} the Japanese Islands to estimate regional stress distribution. This method finds a stress distribution from a strain <b>distribution,</b> <b>by</b> solving boundary <b>value</b> problems. Numerical simulation and model experiment verify the validity of the method. The deformation increment of Japan is measured by a global positioning system (GPS) network. We carry out the numerical computation of estimating regional stress increment distribution for the Japanese Islands, and obtain a self-equilibrating stress increment field. The regional constitutive relations are then estimated from the relation between the computed stress increment and the measured strain increment. Some discussions are made on the usefulness and limitation of the present inversion method...|$|R
40|$|We study an {{allocation}} problem where {{a set of}} heterogeneous objects {{needs to}} be allocated to agents arriving over time. The basic model is of the private, independent values type. The dynamically efficient allocation is implementable if the distribution of agents’ values is known. Whereas {{lack of knowledge about}} the distribution is inconsequential in the static case, endogenous informational externalities arise if the designer gradually learns about the <b>distribution</b> <b>by</b> observing present <b>values.</b> These externalities may prevent the implementation of the dynamically efficient allocation if present observations have a large impact on expectations about the future. We provide necessary and sufficient conditions for the efficient allocation to be implementable, and point out the role of delayed payments...|$|R
40|$|The paper {{describes}} our {{involvement in}} the high court reopened formal investigation into {{the sinking of the}} bulk carrier M. V. Derbyshire. The statistical problem that we addressed concerned the estimation of the probability that the ship had sunk from a particular form of structural failure, resulting from large wave impacts on the ship, for each of a range of possible sea-state and vessel conditions. We considered several statistical models for the wave impacts on the ship with the generalized Pareto <b>distribution,</b> motivated <b>by</b> extreme <b>value</b> theory, providing an excellent description and aiding the investigation to draw clear conclusions about the cause of the sinking. Copyright 2003 Royal Statistical Society. ...|$|R
40|$|Summary: In {{the paper}} we propose an {{approach}} for clustering large datasets of mixed units based on representation of clusters <b>by</b> <b>distributions</b> of <b>values</b> of vari-ables over a cluster – histograms, that are compatible with merging of clusters. The proposed representation can be used also for clustering symbolic data. On {{the basis of this}} representation the adapted versions of leaders method and adding method were implemented. The proposed approach was successfully applied to several large datasets. Key words: large datasets, clustering, mixed units, distribution description com-patible with merging of clusters, leaders method, adding method...|$|R
40|$|This {{document}} concerns {{amendments to}} two contracts placed with the firms ELAY INDUSTRIAL (ES) and MALVESTITI (IT) for {{the supply of}} fine-blanked components {{to be used in}} the special end sections of the LHC dipole yokes. The amendments are required since the components have been redesigned. The Finance Committee is invited to approve two contract amendments, the first with the firm ELAY INDUSTRIAL (ES) for an additional price of 641 156 euros (1 025 767 Swiss francs), bringing the total amount of the contract to 1 122 967 euros (1 796 608 Swiss francs), not subject to revision for deliveries before 31 December 2001, and the second with the firm MALVESTITI (IT) for an additional amount 978 053 Swiss francs, bringing the total amount of the contract to 1 854 898 Swiss francs, not subject to revision for deliveries before 31 December 2001. The firm ELAY INDUSTRIAL (ES) has indicated the following <b>distribution</b> <b>by</b> country of the value of the amendment covered by this proposal: ES - 100 %. The firm MALVESTITI (IT) has indicated the following <b>distribution</b> <b>by</b> of the <b>value</b> of the amendment covered by this proposal: IT - 100 %...|$|R
40|$|We {{report the}} results of a {{preclinical}} evaluation of recently introduced commercial tools for 3 D patient IMRT/VMAT dose reconstruction, the Delta 4 Anatomy calculation algorithm. Based on the same initial measurement, volumetric dose can be reconstructed in two ways. Three-dimensional dose on the Delta 4 phantom can be obtained by renormalizing the planned dose <b>distribution</b> <b>by</b> the measurement <b>values</b> (D 4 Interpolation). Alternatively, incident fluence can be approximated from the phantom measurement and used for volumetric dose calculation on an arbitrary (patient) dataset with a pencil beam algorithm (Delta 4 PB). The primary basis for comparison was 3 D dose obtained by previously validated measurement-guided planned dose perturbation method (ACPDP), based on the ArcCHECK dosimeter with 3 DVH software. For five clinical VMAT plans, D 4 Interpolation agreed well with ACPDP on a homogeneous cylindrical phantom according to gamma analysis with local dose-error normalization. The average agreement rates were 98. 2...|$|R
40|$|Graduation date: 1987 Part of the Mexican {{system of}} forest {{management}} {{is based on}} the "Unidades Industriales de Explotacion Forestal" (UIEF). An economic analys{{is based on the}} profitability of the forest under different alternatives was performed to evaluate the efficiency of the UIEF's system of forest management using Pinus hartwegii growing in the Zoquiapan forest as a model forest. The economic analysis was performed by comparing Present Net Worth (PNW) of the optimal harvest schedules at several rotation ages for different alternatives of forest management and simulating different market situations. A whole-stand growth and yield model for Pinus hartwegii was developed using the model developed by Clutter (1963). The final model computes diameter <b>distributions</b> <b>by</b> estimating <b>values</b> for the three parameter Weibull distribution. Different criteria were tested to fit real data to the Weibull probability density function. A modified approach which uses different parameter estimators to fit Weibull parameters to stand characteristics was performed. The approach yielded excellent fits. A Dynamic programming algorithm was incorporated to the growth and yield model to compute optimal harvest schedules. The algorithm is mostly based on the Valsta (1986) algorithm. The Algorithm optimizes timing, intensity and type of thinning for different rotation ages. Results from economic analysis showed some disadvantages of both the T. JIEF system and some statements of the Mexican forest law...|$|R
40|$|For a {{quantitative}} {{understanding of the}} process of adaptation, we need to understand its "raw material," that is, the frequency and fitness effects of beneficial mutations. At present, most empirical evidence suggests an exponential distribution of fitness effects of beneficial mutations, as predicted for Gumbel-domain <b>distributions</b> <b>by</b> extreme <b>value</b> theory. Here, we study the distribution of mutation effects on cefotaxime (Ctx) resistance and fitness of 48 unique beneficial mutations in the bacterial enzyme TEM- 1 β-lactamase, which were obtained by screening the products of random mutagenesis for increased Ctx resistance. Our contributions are threefold. First, based on the frequency of unique mutations among more than 300 sequenced isolates and correcting for mutation bias, we conservatively estimate that the total number of first-step mutations that increase Ctx resistance in this enzyme is 87 [95 % CI 75 - 189], or 3. 4 % of all 2, 583 possible base-pair substitutions. Of the 48 mutations, 10 are synonymous and the majority of the 38 non-synonymous mutations occur in the pocket surrounding the catalytic site. Second, we estimate the effects of the mutations on Ctx resistance by determining survival at various Ctx concentrations, and we derive their fitness effects by modeling reproduction and survival as a branching process. Third, we find that the distribution of both measures follows a Fréchet-type <b>distribution</b> characterized <b>by</b> a broad tail of a few exceptionally fit mutants. Such distributions have fundamental evolutionary implications, including an increased predictability of evolution, and may provide a partial explanation for recent observations of striking parallel evolution of antibiotic resistance...|$|R
40|$|There are {{two kinds}} of annuity, annuity certain and life annuity. Annuity certain does not depend on life probability, for example, mortgage. Life annuity depends on time until death and life probability, for example, pension payment from {{insurance}} company. The objective {{of this paper is to}} discuss further about life annuity and the relationship with life probability that is influences by time until death and the assumption of interest which is used. Time until death (T) is a random variable, because it is unpredictable. To determine the value of distribution, assumption values on tqx will be used. These <b>values</b> are generated <b>by</b> T simulation which depends on Uniform distribution (0, 1) random values. A few cases of determining life annuity using tpx <b>distribution</b> <b>values</b> <b>by</b> T simulation will be discussed...|$|R
40|$|Interest in the {{quantitative}} effects of neighbourhood characteristics on urban health has recently increased in social epidemiology. Such effects are mostly stydied employing multilevel models {{based on some}} definition of neighbourhood. We investigate the statistical relationship between health and the neighboururhood quality as perceived by individuals, thus avoiding the inconvenient of choosing a specific administrative definition. We use sampling data from the Los Angels Family and Neighb. Survey(L. A. FANS). We choose the number of hospitalizations {{in the last two}} years as health status and we have related this number to several individual characteristics through Generalized Additive Models(GAM),focusing on Zero Inflated Poisson(ZIP), which are unusual choices in this context. We also overcome to some extent the difficulties in interpreting the results from a GAM with a ZIP <b>distribution</b> <b>by</b> simulating predicted <b>values</b> under varying assumptions in order to reveal the relationship of interest. The analyses confirm results already published in the literature, suggesting also that new opportunities, from a statistical methods point of view, are available in this specific field of social epidemiology...|$|R
40|$|Interest in the {{quantitative}} effects of neighbourhood characteristics on urban health has recently increased in social epidemiology. Such effects are mostly studied employing multilevel models {{based on some}} definition of the neighbourhood. We investigate the statistical relationship between health and the neighourhood quality as perceived by individuals, thus avoiding the need of choosing a specific definition of neighbourhood. We use data from the Los Angeles Family and Neighbourhood Survey (L. A. FANS). We measure health status of an individual {{as the number of}} hospitalizations in the last two years. This number is related to individual carachteristics (including neighbourhood perceptions) through generalized additive models (GAM), focusing particularly on the Zero Inflated Poisson (ZIP), which is an unusual choice in this context. We also overcome to some extent the difficulties in interpreting the results from a GAM with a ZIP <b>distribution</b> <b>by</b> simulating predicted <b>values</b> under varying assumptions in order to reveal the relationship of interest. The analysis confirms that the quality of neighbourhood – as measured by perceptions of individuals – significantly relates to the health status of inhabitants – as measured by the number of hospitalizations...|$|R
40|$|Grain size {{refinement}} can {{be achieved}} by recrystallisation during hot deformation, with multiple deformation steps during rolling or forging being used to generate fine grain sizes. Whilst the mode (or average) grain size after recrystallisation can be determined from standard equations the full grain size distribution is required for predicting a range of mechanical properties. In this work an approach has been proposed to allow prediction of the full grain size <b>distribution</b> <b>by</b> varying the <b>value</b> of the D’ parameter value in the Dutta - Sellars equation developed to predict the recrystallised mode/average grain sizes. It has been found that D’ is a function of strain at high strain values (above 0. 3) and the relative position of the grain size in the grain size range (D*). Results for recrystallised grain size distributions for a range of steel grades (including model Fe - Ni steels, commercial Nb-microalloyed plate and high alloy (9 Cr) forging steel) with different initial grain sizes and following deformation to different deformation strains, show that the predictions give good agreement with the experimental data except for samples with larger mode grain sizes deformed to a strain of 0. 15...|$|R
40|$|In this study, we {{investigate}} the radiative forcing of Saharan dust, its {{interactions with the}} Atlantic Intertropical Convergence Zone (ITCZ), through African easterly waves (AEW), African easterly jets (AEJ), and its impacts in short term numerical forecasts of tropical cyclogenesis using the GOCART-GEOS 5 forecast system. Our approach is to develop and use an A-Train satellite simulator (ATSS) to constrain the observed aerosol index of refraction and particle size <b>distribution</b> <b>by</b> finding the <b>values</b> that simultaneously minimize the difference between observed CALIOP, CloudSat, OMI, and MODIS radiances and simulated radiances inverted from atmospheric model output using procedures and physical principles consistent with those used in corresponding retrieval algorithms. We use observations from the A-train and TRMM to determine relationships among the Saharan dust layer, transport by the AEW, and possible responses to dust radiative forcing in developing tropical cyclones in the A-ITCZ. Preliminary model results showing physical processes associated with the generation and transport of the Saharan dust layer, their interactions with the incipient moisture, clouds and rainfall in developing tropical cyclones will be presented. Also presented will be results of {{a case study of}} possible radiative impacts on AEW and AEJ during the NAMMA field campaign...|$|R
40|$|Abstract: Year {{after year}} stock markets {{of the world}} kept on {{breaking}} records. They reached new heights and plunged to new depths. During financial crisis of 2008 many markets shed as many points as they never did in their history. It {{is extremely difficult to}} predict future index value due to their high randomness but is it possible to know if markets are going to achieve a record fall in near future or not. Daily changes in stock market index are not normally distributed, analysis showed they exhibit fatter tails that normal distribution, while extreme fall and rise generally follow generalized extreme <b>value</b> <b>distribution</b> explained <b>by</b> Extreme <b>Value</b> theory. The study models worst losses suffered in a day by National Stock Exchange index CNX-Nifty <b>by</b> fitting GEV <b>distribution</b> on yearly and quarterly maximum losses. GEV distribution function hence obtained is used for predicting probability of obtaining a record maximum loss next year / quarter of 2008. As Indian markets shed maximum point in a day during financial crisis of 2008, study verifies if model gives indication about such extreme event...|$|R
40|$|Year {{after year}} stock markets {{of the world}} kept on {{breaking}} records. They reached new heights and plunged to new depths. During financial crisis of 2008 many markets shed as many points as they never did in their history. It {{is extremely difficult to}} predict future index value due to their high randomness but is it possible to know if markets are going to achieve a record fall in near future or not. Daily changes in stock market index are not normally distributed, analysis showed they exhibit fatter tails that normal distribution, while extreme fall and rise generally follow generalized extreme <b>value</b> <b>distribution</b> explained <b>by</b> Extreme <b>Value</b> theory. The study models worst losses suffered in a day by National Stock Exchange index CNX-Nifty <b>by</b> fitting GEV <b>distribution</b> on yearly and quarterly maximum losses. GEV distribution function hence obtained is used for predicting probability of obtaining a record maximum loss next year / quarter of 2008. As Indian markets shed maximum point in a day during financial crisis of 2008, study verifies if model gives indication about such extreme event. Key words: GEV distribution; Extreme Value Theory; Record Loss; Frechet Density Function; Block Maxim...|$|R
40|$|The {{classical}} Fickian {{model for}} solute transport in porous media cannot correctly predict the spreading (the dispersion) of contaminant plumes in a heterogeneous subsurface unless its structure is completely characterized. Although the required precision {{is outside the}} reach of current field characterization methods, the advection-dispersion model remains {{the most widely used}} model among practitioners. Two approaches can be adopted to solve the effect of physical heterogeneity on transport. First, based on a given characterization of the spatial structure of the subsurface, upscaling methods allow the computation of apparent scale-dependent parameters (especially longitudinal dispersivity) {{to be used in the}} classical Fickian model. In the second approach, upscaled (non-Fickian) transport equations with scale-independent parameters are used. In this paper, efforts are made to classify and review upscaling methods for Fickian transport parameters and non-Fickian upscaled transport equations for solute transport, with an emphasis on their mathematical properties and their (one-dimensional) analytical formulations. In particular, their capacity to mode(scale effects in apparent longitudinal dispersion is investigated. Upscaling methods and upscaled models are illustrated in the case of two three-dimensional synthetic aquifers, with lognormal hydraulic conductivity <b>distributions</b> characterized <b>by</b> variance <b>values</b> of 2 and 8. (C) 2008 Etsevier B. V. All rights reserved...|$|R
