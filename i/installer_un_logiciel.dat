0|29|Public
40|$|CE MEMOIRE EST COMPOSE DE TROIS CHAPITRES. LE PREMIER POSE LE PROBLEME DE LA SPECIFICATION DES SYSTEMES DE COMMANDE COMPLEXES FORMES D'UN ENSEMBLE D'AUTOMATISMES COMMUNICANTS. APRES AVOIR INTRODUIT LES RESEAUX DE PETRI EN TANT QU'OUTIL FORMEL POUR LA SPECIFICATION, IL EST MONTRE QUE CET OUTIL N'EST PAS CONTRADICTOIRE AVEC UNE APPROCHE STRUCTUREE. QUELQUES REGLES DE STRUCTURATION SONT PROPOSEES. CETTE DEMARCHE EST ILLUSTREE PAR UN EXEMPLE CONCRET. LE SECOND CHAPITRE MONTRE COMMENT UNE SPECIFICATION STRUCTUREE PEUT ETRE VALIDEE. LE TROISIEME CHAPITRE PROPOSE UN LANGAGE DE SPECIFICATION ADAPTE A LA DESCRIPTION STRUCTUREE D'AUTOMATISMES INTERCONNECTES. CE LANGAGE EST FONDE SUR L'UTILISATION DES RESEAUX DE PETRI. <b>UN</b> <b>LOGICIEL</b> D'ANALYSE SYNTAXIQUE ET SEMANTIQUE A ETE DEVELOPPE SUR MICROCALCULATEUR EN LANGAGE PASCAL. CE LOGICIEL TRADUIT LA SPECIFICATION EN TABLES ET EST CONCU DE FACON A PERMETTRE LE TELECHARGEMENT D'AUTOMATES PROGRAMMABLES SPECIALISESIndisponibl...|$|R
40|$|We {{present a}} method for obtaining asymptotics for the generic element of a twodimensional {{convolution}} matrix which includes Stirling numbers of both kinds and some other interesting combinatorial quantities. Asy 2 dim is a computer algebra package which implement this method. The current version of Asy 2 dim is written in Maple V. 3 [2]. R'esum'e Nous pr'esentons une m'ethode pour calculer l"evaluation asymptotique d'un g'en'erique 'el'ement d'une matrice de convolution `a deux dimensions. Cette classe d"el'ements comprend, avec plusieurs quantit'es combinatoires, les nombres de Stirling de premi`ere et deuxi`eme esp`ece. Asy 2 dim est <b>un</b> <b>logiciel,</b> developp'e en Maple V. 3 [2], qui r'ealise cette m'ethode. 1 Introduction In many aspects of mathematics and computer science, e. g., combinatorics and algorithm analysis, the problem of finding an expression for the generic element of a sequence ff k g k 2 N come up. It is not always possible to find a closed form for the k th element of the [...] ...|$|R
40|$|An {{analytical}} approach allowing modelling transient phenomena in pipes, valves, surge tanks and Francis turbines based on impedance method is developed. These models are implemented in a software called “SIMSEN”, which simulates the behaviour of complex {{applications in the}} domain of adjustable speed drives and electrical power networks. This program is based on a modular structure, which enables the numerical simulation of transient modes of systems exhibiting arbitrary topologies. The numerical simulation for transient phenomena in hydropower plants with “SIMSEN ” has the benefit of an algorithm that generates and solves an integrated set of differential equations. This algorithm solves simultaneously the electrical, hydraulic and control equations ensuring a proper interaction between the three parts of the system. The case of a Francis turbine power plant is studied. The model of the turbine is based on measured steady state characteristics. The simulation of the dynamic behaviour of the power plant under load variation is investigated. RÉSUMÉ Une approche analytique basée sur la méthode des impédances permettant la modélisation des phénomènes transitoires faisant intervenir des composants tels que conduites, vannes, cheminées d’équilibre et turbines Francis est développée. Ces modèles sont implémentés dans <b>un</b> <b>logiciel</b> nommé “SIMSEN”. Celui-ci permet l...|$|R
40|$|CoCAO : <b>un</b> {{environnement}} <b>logiciel</b> coopératif pour les acteurs de l'architecture et du BTPIn the Architecture Engineering and Construction (AEC) sector, {{cooperation between}} actors {{is essential for}} projects success. During the building construction activity, organization is both hierarchical and adhocratic. Decision assistance tools have to integrate these heterogeneous parameters. The proposition described here consists of {{the design of a}} coordination assistance tool providing synthetic indicators on the statement of the activity and also allowing the user to navigate in the cooperative context through multiple views. This proposition is based on a model architecture allowing us to manage cooperative context information and its visualization...|$|R
40|$|La tavelure du pommier causée par l’agent pathogène Venturia inaequalis est le {{champignon}} qui cause le plus de dommage sur la culture du pommier. Une des variétés les plus cultivées, Malus x domestica McIntosh est très sensible à ce pathogène. Des outils d’aide à la décision existent pour prédire et donc prévenir l’apparition de cette maladie. Parmi eux, RIMpro est <b>un</b> <b>logiciel</b> validé et utilisé en Europe depuis 1993. Il donne de bonsApple scab made by {{the pathogen}} Venturia inaequalis is the fungus which causes the most damages on apple growing. One of the most grown variety, Malus x domectica McIntosh is very sensitive at this pathogen. Some helping decision tools exist to predict and advise of the disease arrival. Among them RIMpro is a software approved since 1993. It gives some good results. Nevertheless, it is adapted for European climatic conditions. The Institute of Research and Development in Agroenvironment realises experiments since 2006 to improve and approve this model in climatic conditions of Québec. For this purpose, ascospores maturation, ascospores ejection and infections severity are compared with the software predictions. Results are compiled and show interesting results. Setup parameters like evaluation and the inclusion of foliage humectation time or whether conditions infection are parameters to improve to allow the softaware to draw closer to reality. The software RIMpro stays a very useful model used by growers and consultants in apple growing. It allows to prevent and to reduce phytosanitary interventions against apple scab. It allows lowering costs joined to V. inaequalis...|$|R
40|$|Dix enfants sourds-aveugles, âgés de 13 à 15 ans, sont filmés lors d'interactions avec un éducateur adulte dans une {{situation}} semi-structurée de résolution de problème. Deux facteurs sont manipulés : la disponibilité du partenaire adulte (disponible / non disponible) et son statut (familier / non familier). <b>Un</b> <b>logiciel</b> détecte automatiquement les combinaisons non aléatoires (patterns) d'au moins deux comportements élémentaires relevés pour chacun des interactants. L'analyse de contenu des patterns montre les compétences sociales des sujets (attirer l'attention; indiquer un réfèrent; requérir la coopération autrui [...] .) et l'adaptation de leurs conduites à la disponibilité et au statut du partenaire, plutôt qu'un fonctionnement pathologique des interactions. Social {{interactions between}} deaf and blind {{children and their}} educators Ten deaf and blind children aged between thirteen and fifteen years old were videotaped while interacting with an adult educator in a semi-structured solving problem situation. Two factors are operated : the adult's status - Familiar vs Unfamiliar - and the familiar adult's availability for interacting - Available vs Unavailable. The elementary behavioral events observed in the dyads were coded and processed by a program which allows the detection of hidden behavior patterns through automatic detection of special relations between the time distribution of behavioral event types. The content analysis of these patterns showed that the deaf-blind children were {{able to participate in}} structured interactions. Those structures demonstrated important social skills as drawing others' attention, referring and requiring for others' cooperation and their capacities to adapt their behaviors {{as a function of the}} partner 's availability and status. Although the interactions were brief and weakly structured, we can't describe them as abnormal. Sèvre Sabine. Les interactions sociales des enfants sourds-aveugles. In: Enfance, tome 52, n° 2, 1999. pp. 111 - 136...|$|R
40|$|International audienceThis {{article focuses}} on best {{protective}} materials. The obtaining the solubility parameters of five of the most used polymer materials with forty chemicals covering {{a broad spectrum of}} HSP values. The approach considers that for chemical substances that are soluble in a polymer protective material, this material will not be resistant. Inversely, for polymer materials will be resistant. Based on the experimental data, the Hansen the solubility parameters (the center of the sphere) and radii, R, of the polymer spheres for Nit Neoprene, values for the glove materials, the differences with the HSP values (dissimilarity A value following the Hansen approach) for more 1 approach states that for chemicals where the solubility parameter distance (A) of a given chemical {{to the center of the}} polymer sphere is higher than the R value, A/R inversely, for A/R the Hansen Sphere with a modified R was developed. An algorithm that compared the predicted values with close to nine hundred experimental data from the literature, allows defining for each material a zone developed that allows the selection of the best glove materials avoiding a dangerous " trial and errors " approach. In any case this software is a substitute of permeation tests to d chemical resistance to a chemical or mixtures of chemicals. Cet article utilise les paramètres tridimensionnels de solubilité (HSP) pour calculer un modèle de sélection des matériaux de protection aux solvants. Des sphères de solubilités ont été définies pour 5 polymères : nitrile, néoprène, latex, viton et butyle. Un facteur de dissimilarité a alors été établi entre ces sphères de polymères et plus de 1200 substances chimiques, par rapport à leur valeur HSP. Finalement, <b>un</b> <b>logiciel</b> web ProtecPo a été développé pour sélectionner les meilleurs matériaux de protection lors de l’utilisation de produits chimiques...|$|R
50|$|He is {{best known}} for his Cast the Sleeping Elephant Project, a life-size bronze statue of an African elephant. It was {{produced}} in 1980 by making an alginate cast of a live, wild Kenyan elephant bull, which survived the 72 minute process unharmed. The bronze statue made from the cast was inaugurated in 1998 by UN Secretary General Kofi Annan and is <b>installed</b> at <b>UN</b> Headquarters in New York.|$|R
40|$|One {{should not}} {{underestimate}} {{the import of}} being able to verify the conformance between a program and its specification. Furthermore, being able to verify it using a dedicated program allows one to check the correctness of this verification. For this purpose, one uses proof assistants, which are programs that allows to describe the problem, build proofs, then check them. There are {{more than one way to}} achieve this goal : one could either generate the program from its specification; one could use an annotated program which carries the specification and hints on how to prove the conformance, then check it afterwards; or one could start from the specification and the program, then prove the conformance of the latter with respect to the first. For our system, we chose the last approach. We implemented a system in which the user describes the speci- fication of a program in a dedicated logical framework, then writes the program in the ML programming language, restricted to the functional subset (with pattern matching, inductive definitions and partial functions), then, after all, interactively builds the proof of conformance of the program with respect to its specification. There are three different aspects in our work : – formalization of a logical framework dedicated to the verification of programs written in the functional part of ML; – precize specification of the proof assistant, its user interface, and the protocol used by both in order to communicate; – the implementation of the proof assistant in Objective Caml, using an original architecture which mixes object oriented programming and functional programming. All these elements may be found in this document, including a precize description of the implementation, the choices we made and the reasons of these choices. The reader will also find here a description on how to use our system, and some examples of problems handled with it. Pouvoir vérifier la conformité d'un programme avec sa spécification représente un enjeu important. On peut utiliser un assistant de preuve : <b>un</b> <b>logiciel</b> permettant la description du problème, la construction des preuves et leur vérification. Nous avons implémenté un système où l'utilisateur décrit la spécification du programme dans un formalisme logique ad hoc, donne le programme dans le sous-ensemble fonctionnel de ML (comprenant filtrage, définitions récursives et fonctions partielles), puis construit interactivement les preuves de correction nécessaires pour prouver la validité du programme...|$|R
40|$|The SIC {{software}} (Simulation of Irrigation Canals) {{is one of}} {{the latest}} hydraulic models developed by Cemagref. The first developments on hydraulic numerical modeling started at Cemagref in the early 1970 's. Lots of improved and updated versions have been made since this period. Right now, several hydraulic models exist at Cemagref, {{depending on the type of}} systems and events to simulate (rivers, irrigation canals, dam break, drainage systems, etc.). One of these models has been particularly dedicated to irrigation canals. This model, called SIC, has been adapted from another hydraulic model, where some features have been removed, new ones have been introduced, and for which special user-friendly interfaces have been developed. It was also adapted to run on currently available IBM PC computers or compatibles. It can be used both by engineers and by canal managers. The very first version of this model has been developed for the I. I. M. I. (International Irrigation Management Institute) on a real canal located in the south coast of Sri Lanka (Kirindi Oya Right Bank Main Canal). One purpose of this model was to be easily usable by canal managers as a decision support tool, in order to help them in the daily operation and maintenance of their system. Since this first application was promising, Cemagref, with other partners, decided to develop a new standard version of this software, that could be used on most of the irrigation canals world-wide. A study Advisory Committee has been set up to decide the required features of the model, and follow its development. This Committee led to the SIC User's Club with representatives of the different partners: BCEOM, CACG, Cemagref, CNABRL, ENGREF, IIMI, LAAS, LHF, MAE, MRE, MCoop., OIE, SCP and SOGREAH (Cf. Annex 1 for acronyms). One main purpose of this Club was also to foster communication among model developers and users. It allows to share experiences, new developments, needs of improvements, etc. / Le Cemagref a développé <b>un</b> <b>logiciel</b> de simulation hydraulique des canaux (SIC) qu'il utilise pour ses besoins de recherche et qu'il commercialise. Ce logiciel permet de modéliser pratiquement n'importe quel canal d'irrigation, d'en étudier son comportement hydraulique, d'en déduire des paramètres hydrauliques importants pour sa commande automatique, de tester diverses modifications de ses structures (conception, réhabilitation) et différents algorithmes de régulation automatique. Une fois que les algorithmes ont été mis au point, et testés avec succès sur logiciel de simulation, ils peuvent êtres implantés sur site...|$|R
40|$|CoCAO : <b>un</b> {{environnement}} <b>logiciel</b> coopératif pour les acteurs de l'architecture et du BTPBuilding {{construction is}} a complex activity, involving numerous and heterogeneous actors during relatively short periods. The methods of project management used are specific because {{of the complexity of}} the architectural “object” and the prototype character of each operation. Methods usually used in industrial sectors (such as workflow definition or inverse engineering) are not transposable in the AEC domain. In fact, coordination modes existing in this domain are adapted to his particularities (decentralised decisions, uncertainties on the building construction methods etc.). We suggest to model this particular cooperative activity using a driven model engineering based on the MOF archi-tecture. This form of engineering allows us to define entities in relation in the cooperation at a high abstraction level. Then this abstraction leads us to design tools adapted to the cooperation in the AEC sector by defining an infrastructure based on a cooperation model. We will suggest a new assistance tool for coor-dination (multi-view cooperative platform) taking into account the analyses of coordination modes and the experiment of IT potentialities. It proposes an in-formation representation adapted to the user context and viewed in an adequate visualization mode...|$|R
40|$|Scheduling in a IEEE 802. 15. 4 e TSCH (6 TiSCH) {{low-power}} {{wireless network}} {{can be done}} in a centralized or distributed way. When using centralized scheduling, a scheduler installs a communication schedule into the network. This {{can be done in}} a standards-based way usingCoAP. In this report, we compute the number of packets and the latency this takes, on real-world examples. The result is that the cost is very high using today's standards, much higher than when using an ad-hoc solution such as OCARI. We conclude by making recommendations to drastically reduce the number of messages and improve the efficiency of the standardized approach. Dans ce rapport de recherche, nous montrons comment <b>installer</b> <b>un</b> ordonnancement d'activités des noeuds dans un réseau contraint radio multi-sauts IEEE 802. 15. 4 e TSCH en utilisant le standard CoAP. A travers un exemple illustratif simple, nous calculons le nombre de messages véhiculés dans l'ensemble du réseau pour différentes méthodes compatibles avec ces standards existants. Nous notons que l'utilisation des standards existants se traduit par un coût très important en terme de nombre de messages et en latence. Ce coût est bien superieur à celuid'une solution ad-hoc comme OCARI. Nous concluons en faisant différentes recommandations pour réduire ce nombre de messages et donc améliorer l'efficacité des protocoles standardisés...|$|R
40|$|International audienceThe {{project of}} {{electronic}} corpora for Manding languages {{was launched in}} St. Petersburg in 2009. By now, it is carried out by an international team with an assistance by specialists in Manding languages from different countries. Tools have been developed {{taking into account the}} specifics of Manding languages (and adaptable to other languages). The Bamana Reference Corpus was put on line in 2012, it was followed by a Maninka corpus (in both Roman and N’ko writing) in February 2014. An orthography corrector for Bamana and a software for the Bamana OCR has been developed {{on the basis of the}} Bamana Reference Corpus tools. An experimental use of the Bamana Corpus in the Bamana teaching in universities and in linguistic studies has proved its effectiveness. The experience accumulated in the framework of this project can be relatively easily extended to other Manding varieties (Jula of Côte d’Ivoire, Jula of Burkina Faso), and, if necessary, to other African languages. Le projet des corpus électroniques de textes en langues mandingues a démarré à St. Petersbourg en 2009. Aujourd'hui, il est effectué par une équipe internationale avec l'implication des spécialistes en langues manding des pays différents. L'outillage tenant compte des caractéristiques spécifiques des langues manding (mais adaptable aux autres langues) a été développé. Le Corpus Bambara de Référence est mis en ligne en 2012, suivi par un corpus maninka (en écriture N'ko et latine) en février 2014. Un correcteur automatique d'orthographe bambara et <b>un</b> <b>logiciel</b> du ROC pour le bambara a été développé sur la base de l'outillage du CBR. L'utilisation expérimentale du CBR dans l'enseignement universitaire du bambara et dans les études linguistiques a montré son efficacité. L'expérience accumulée peut être facilement étendue sur les autres variétés manding (le dioula de RCI, le dioula de Burkina Faso), mais aussi sur d'autres langues africaines. Abstract. The project of electronic corpora for Manding languages was launched in St. Petersburg in 2009. By now, it is carried out by an international team with an assistance by specialists in Manding languages from different countries. Tools have been developed taking into account the specifics of Manding languages (and adaptable to other languages). The Bamana Reference Corpus was put on line in 2012, it was followed by a Maninka corpus (in both Roman and N'ko writing) in February 2014. An orthography corrector for Bamana and a software for the Bamana OCR has been developed on the basis of the Bamana Reference Corpus tools. An experimental use of the Bamana Corpus in the Bamana teaching in universities and in linguistic studies has proved its effectiveness. The experience accumulated in the framework of this project can be relatively easily extended to other Manding varieties (Jula of Côte d'Ivoire, Jula of Burkina Faso), and, if necessary, to other African languages...|$|R
40|$|DIMG 92 est un simulateur de {{formation}} sur le procédé pétrochimique DIMERSOL (procédé IFP de dimérisation du propylène). En simulant le réacteur et ses équipements annexes, les opérateurs s'exercent à conduire l'unité et comprennent les effets des divers paramètres à prendre en compte. Le simulateur est composé de deux <b>logiciels</b> : - <b>Un</b> <b>logiciel</b> de modélisation, basé sur la représentation mathématique de connaissances physico-chimiques et physiques. Aucun empirisme n'est utilisé dans le modèle. Ce logiciel a été développé par l'Institut Français du Pétrole (IFP). - <b>Un</b> <b>logiciel</b> d'environnement de simulation dynamique pour la formation. Ce logiciel à interface graphique et interactive est distribué par la société française Réalisation en Systémique Industrielle (RSI). Industrial {{units are}} tending {{to become more}} and more complex and are characterized by a very high degree of integration. The operating of such units requires highly skilled operators. This is why dynamic simulators have training as their prime goal. The DIMG 92 simulator falls within this category. It simulates the reaction section of the DIMERSOL G process in a steady state or not. The DIMERSOL G process is designed for the dimerization of propylene into hexene. In the modeled unit, the reaction section includes a liquid-phase reactor and a pump-around on which a heat exchanger is installed. The heat exchanger, which uses water as the cooling fluid, evacuates the heat of reaction. The reaction temperature is between 45 and 55 °C. The reactor pressure is in the vicinity of 22 effective bars. The catalyst is in a liquid state. A standard feedstock is made up of 30 weight % inert matter and 70 % propylene. Modeling is broken down into three parts : the kinetics, the reactor, the heat exchanger. Mathematical modeling of the oligomerization kinetics was performed at Institut Français du Pétrole (IFP) in 1987. By taking into consideration the initiation, prolongation and termination reactions starting with propylene and hexene, we can understand the oligomerization kinetics in the DIMERSOL G process. The deactivation of the catalyst is represented by a first-order kinetics. With a reduced number of constants, the model takes the following aspects into consideration :(a) The production of heavy oligomers (no propagation reaction is neglected). (b) The formation of oligomers combined with the catalyst. (c) The deactivation of the catalyst in time and the temperature. The kinetic constants and activation energies were determined so that the model can restore the different kinetic results of pilot tests. The modeling of the reactor considers that this reactor is perfectly stirred and always full of liquid. The principal equations reflect the balances of the molecular species. There are also molar balances for the olefins, for the olefins combined with the catalyst, for the inert matter, for the active catalyst and for the deactivated catalyst. An overall mass balance and a mass balance for the sum of olefinic species are added on. A heat balance is used to predict the temperature of the reactor. Other calculations are performed to obtain the selectivities, the conversion and the outflow of each constituent. The modeling of the heat exchanger is done by a static model. The outlet temperatures of the fluids {{and the power of the}} heat exchanger are calculated by making an estimate of the efficiency of the heat exchanger. The programming of the model is modular. This makes for a standardized and coherent whole. The modules are written in FORTRAN 77. The user has access either to a static solution to find a stable operating point or dynamic calculating during which the console operator can introduce various disturbances in the variables of the process. In the static module the balances for the reactor form a system of nonlinear algebraic equations that is solved by a Newton-Raphson algorithm. The dynamic module mainly includes the integration, in relation to the processing time of the system of differential equations for the balances relative to the reactor. The above two modules make use of various joint subroutines. The modules containing the modeling of the reaction section are integrated in the SORYA-MX application. This application is a graphic and interactive interface designed for training simulators. SORYA-MX is distributed by the French company Réalisation en Systémique Industrielle(RSI). SORYA-MX has a time management system capable of controlling the accelaration of the simulation at will. It is a multitask application. SORYA-MX has two levels of use :(1) A designerlevel for configuring the simulator by creating, for example, mimic diagrams, alarm levels, operating data sheets, controllers, disturbance scenarios, etc. (2) A userlevel for activating the functionalities defined by the designer level, for controlling the controllers, for safeguarding, etc. As the result of this environment, learning on the simulator is much like operating the unit via a centralized control system. This type of simulator has already shown proof of its effectiveness :(a) At the level of research, as a working support for the designing of expert systems or for designing advanced controls. (b) At the level of training, for operators of units operating under an IFP process license. By a fairly fine kinetic model for oligomerization and careful programming, the model thus developed can be used as a basis for other studies. In the future, it seems desirable to : (a) Increase the degree of modeling (reactors in cascade, modeling of impurities and of start-up). (b) Extend modeling to similar processes. Nevertheless, for pursuing this project it is absolutely necessary to arouse greater interest among customers during the selling of processes for this training tool and among process engineers on the spot for obtaining a better understanding of internal phenomena...|$|R
40|$|Abstract- In {{this paper}} we are {{interested}} in developing new and easy software for the sizing of a solar installation of hot water production. Such installation is successful only if its determination, its conception and its realization are made with care. This software that we baptized PREVIS 1. 0 supplies, for fixed operating conditions, the dimensions of each component of the installation without having to use tables of data or diagrams. PREVIS 1. 0 will allow the engineers and the fitters of solar installations of hot water production to make a good sizing in a simple and precise way. Moreover, PREVIS could be coupled to TRNSYS code {{and this is one of}} its highlights. Typical university campus for a 240 students was considered as case study. The hot-water demand temperature (45 °C) is controlled by a conventional fuel auxiliary heater and a tempering valve. The fluids circulate by pumps activated by electricity. Annual energy performance, in terms of solar fraction, was calculated for Tangier. Résumé- Dans cet article, nous nous sommes intéressés à développer <b>un</b> nouveau <b>logiciel</b> de calcul facile pour le dimensionnement d’une installation solaire de production d’eau chaude. Une telle installation peut être réussie seulement si sa détermination, sa conception et sa réalisation sont effectuées avec soin. Ce logiciel, que nous avons baptisé PREVIS 1. 0 fournit, pour des conditions d...|$|R
40|$|This thesis, {{devoted to}} {{simulations}} of sailboat sail, was initiated by K-Epsilon, acompany specialized in numerical computations for naval hydrodynamics, IRENav, the Frenchnaval academy laboratory and LHEEA from Ecole Centrale Nantes. In this context a finiteelement program was developed dedicated to computing sail membranes and sailboat structures. The program was {{coupled with an}} inviscid fluid solver. A more detailed modeling of the flow andinteraction was realized by implementing a coupling with a fluid solver code which solves theReynolds Averaged Navier-Stokes Equations, developed by the DSPM team from LHEEA. Forthe coupling {{it was necessary to}} look at the interface over which a transfer of variables betweenthe fluid and structure occurs. Another key consideration was the deformation of the fluid solversmesh. The part has been revisited and extended to reach the development of a fast, robust, andparallelized method to treat the considered deformations. For good solution convergence andstability properties an iterative, partitioned algorithm that relies on an approximation of theinterface’s Jacobian evaluated by the inviscid code and integrated in the structure’s equationswas used. Finally, applications employing these methodologies are presented. Comparisons weremade with an instrumented sailboat. A second experiment of an oscillating cloth was developedto validate the case of interaction of a fluid with a light and flexible structure. Results were usedto validate the RANSE solver coupling. A more realistic calculation was also conducted on anunsteady sailing spinnaker with an automatic trimming algorithm, showing the potential of thepresent coupling. Cette th`ese CIFRE qui s’int´eresse `a la simulation des voiles de bateaux, a ´et´einiti´ee par la soci´et´e k-Epsilon, entreprise sp´ecialis´ee dans le calcul num´erique en hydrodynamiquenavale, l’IRENav, laboratoire de l’Ecole Navale et le LHEEA de l’Ecole Centrale Nantes. Dans ce contexte, <b>un</b> <b>logiciel</b> d´edi´e au calcul structure a ´et´e d´evelopp´e afin de simuler au moyend’´el´ements finis les membranes des voiles et les structures des voiliers. Ce code a ensuite ´et´ecoupl´e `a un solveur fluide parfait, puis `a un code fluide r´esolvant les ´equations de Navier-Stokesen moyenne de Reynolds d´evelopp´e par l’´equipe DSPM du LHEEA. Pour cela, il a ´et´e n´ecessairede s’int´eresser `a l’interface, o`u s’op`ere le transfert de variables entre le fluide et la structure. Autre point-cl´e, la partie d´eformation de maillage fluide a aussi ´et´e revisit´ee et ´etendue pouraboutir au d´eveloppement d’une m´ethode rapide, robuste et parall`ele permettant de traiter lesd´eformations envisag´ees. Pour obtenir de bonnes propri´et´es de convergence et de stabilit´e, l’algorithmepartitionn´e et it´eratif s’appuie de plus sur une approximation du Jacobien de l’interface´evalu´e par l’approche fluide parfait et int´egr´e au code structure. Enfin, des applications mettanten oeuvre ces m´ethodologies sont pr´esent´ees. Des comparaisons sont effectu´ees avec un voilierinstrument´e. Une seconde exp´erience, la voile oscillante, est mise au point pour valider les casd’interaction fluide-structure souple et l´eg`ere. Les r´esultats ont permis de valider le couplage avecun solveur RANSE. Un calcul plus r´ealiste a aussi ´et´e men´e en instationnaire sur un spinnakerde voilier avec un algorithme de r´eglage...|$|R
40|$|Much of {{the liberal}} {{criticism}} of the Bush administration's prosecution {{of the war in}} Iraq has taken a legalistic form, decrying that law as 'illegal'. This criticism has often implied that US unilateralism has been definitional to the neoconservative project and the geopolitical moment, and that a contrasting and supposedly non-existent 'multilateralism' would be neither illegal nor objectionable. The overthrow of Haiti's President Jean-Bertrande Aristide in 2004 and the subsequent <b>installing</b> of <b>UN</b> MINUSTAH peace-keepers in the country was a model multilateral action, the fact of which should have problematised this model: its almost wholesale ignoring in the scholarly international law literature is therefore investigated. The intervention is understood as a successful imperialist action, and the argument made that multilateralism as much as unilateralism can easily be part of an imperialist strategy...|$|R
2500|$|United Nations Special Commission (UNSCOM) was an {{inspection}} regime created with {{the adoption of}} United Nations Security Council Resolution 687 in April 1991 to oversee Iraq's compliance with the destruction of Iraqi chemical, biological, and missile weapons facilities and {{to cooperate with the}} International Atomic Energy Agency's efforts to eliminate nuclear weapon facilities all {{in the aftermath of the}} Gulf War. [...] The UNSCOM inspection regime was packaged with several other UN Security Council requirements, namely, that Iraq's ruling regime formally recognize Kuwait as an independent state and pay out war reparations for the destruction inflicted in the Gulf War, including the firing of Kuwaiti oil supplies and destruction of public infrastructure. Until the UN Security Council saw that Iraq's weapons programs had been aborted and Iraqi leaders had allowed monitoring systems to be <b>installed,</b> the <b>UN's</b> aforementioned sanctions would continue to be imposed on Iraq.|$|R
40|$|A previsão das condições térmicas de duas casas populares (ocupadas) na cidade de Descalvado-SP é tema deste trabalho. A pesquisa envolve o registro contínuo da variação das temperaturas interna e externa. A fórmula experimental, P̈redicting Indoor Air Temperature Formula,̈ proposta por Givoni (1999), está sendo aplicada e analisada para a previsão das temperaturas internas, máximas e mínimas. Para o registro contínuo das temperaturas, foi instalada {{no local}} uma estação meteorológica automática- CR 10 X- Campbell Scientific Inc., equipada com vários sensores para registro dos elementos do clima, e, também, por meio de multiplexador - AM 416, de 32 canais, para conectar termopares - tipo T (cobre/constantan), que registram as temperaturas no {{interior}} das casas (ocupadas). Dos dados coletados, foram escolhidos 2 (dois) episódios representativos do clima, o primeiro representando a situação de primavera/verão (12 a 15 / 10 / 2000), e segundo, outono/inverno (14 a 20 / 05 / 2001), sendo analisados conforme a abordagem de análise dinâmica do clima, de acordo com a proposta de Vecchia (1997), adaptada de Monteiro (1967). The forecast of the thermal conditions of two popular houses (busy) {{in the city}} of Descalvado - SP is therme of this work. The research involves the continuous registration of the variation of the internal temperatures and it expresses. The experimental formula P̈redicting Indoor Air Temperature Formulates,̈ proposed by Givoni (1999), it is being applied and analysed for the forecast of the internal temperatures, maxims and low. It goes the continuous registration of the temperatures it was <b>installed</b> <b>un</b> the place an automatic meteorological station - CR 10 X - Campbell Scientific Inc, equipped with sensor several, goes registration of the elements of the climate and also, through multiplexer - AM 416, of 32 channels, to connect thermoocouples - type T(copper/constantan) that register the temperatures inside the houses (busy). Of the collected data they were chosen 2 (two) representative episodes of the climate, the first representing the spring/summer situation (12 to 15 / 10 / 2000) and second autumn/winter (14 to 20 / 05 / 2001), being analyzed according to the approach of dynamic analysis of the climate, in agreement with the proposal of Vecchia (1997) adapted of Monteiro (1967) ...|$|R
40|$|Current {{areas of}} research, such as {{ubiquitous}} and cloud computing, consider execution environments {{to be in}} a constant state of change. Dynamic applications [...] where components can be added, removed and substituted during execution [...] allow software to adapt and adjust to changing environments, and to accommodate evolving features. Unfortunately, dynamic applications raise design and development issues that have yet to be fully addressed. In this dissertation we show that dynamism is a crosscutting concern that breaks many of the assumptions that developers are otherwise allowed to make in classic applications. Dynamism deeply impacts software design and development. If not handled correctly, dynamism can silently corrupt the application. Furthermore, writing dynamic applications is complex and error-prone, and given the level of complexity and the impact dynamism has on the development process, software cannot become dynamic without (extensive) modification and dynamism cannot be entirely transparent (although much of it may often be externalized or automated). This work focuses on giving the software architect control over the level, the nature and the granularity of dynamism that is required in dynamic applications. This allows architects and developers to choose where the efforts of programming dynamic components are best spent, avoiding the cost and complexity of making all components dynamic. The idea is to allow architects to determine the balance between the efforts spent and the level of dynamism required for the application's needs. At design-time we perform an impact analysis using the architect's requirements for dynamism. This serves to identify components that can be corrupted by dynamism and to [...] at the architect's disposition [...] render selected components resilient to dynamism. The application becomes a well-defined mix of dynamic areas, where components are expected to change at runtime, and static areas that are protected from dynamism and where programming is simpler and less restrictive. At runtime, our framework ensures the application remains consistent [...] even after unexpected dynamic events [...] by computing and removing potentially corrupt components. The framework attempts to recover quickly from dynamism and to minimize the impact of dynamism on the application. Our work builds on recent Software Engineering and Middleware technologies [...] namely, OSGi, iPOJO and APAM [...] that provide basic mechanisms to handle dynamism, such as dependency injection, late-binding, service availability notifications, deployment, lifecycle and dependency management. Our approach, implemented in the Robusta prototype, extends and complements these technologies by providing design and development-time support, and enforcing application execution consistency in the face of dynamism. Les domaines de recherche actuels, tels que l'informatique ubiquitaire et l'informatique en nuage (cloud computing), considèrent que ces environnements d'exécution sont en changement continue. Les applications dynamiques; où les composants peuvent être ajoutés, supprimés pendant l'exécution, permettent a <b>un</b> <b>logiciel</b> de s'adapter et de s'ajuster à l'évolution des environnements, et de tenir compte de l'évolution du logiciel. Malheureusement, les applications dynamiques soulèvent des questions de conception et de développement qui n'ont pas encore été pleinement explorées. Dans cette thèse, nous montrons que le dynamisme est une préoccupation transversale qui rompt avec un grand nombre d'hypothèses que les développeurs d'applications classiques sont autorisés à prendre. Le dynamisme affecte profondément la conception et développement de logiciels. S'il n'est pas manipulé correctement, le dynamisme peut " silencieusement " corrompre l'application. De plus, l'écriture d'applications dynamiques est complexe et sujette à erreur. Et compte tenu du niveau de complexité et de l'impact du dynamisme sur le processus du développement, le logiciel ne peut pas devenir dynamique sans (de large) modification et le dynamisme ne peut pas être totalement transparent (bien que beaucoup de celui-ci peut souvent être externalisées ou automatisées). Ce travail a pour but d'offrir à l'architecte logiciel le contrôle sur le niveau, la nature et la granularité du dynamisme qui est nécessaire dans les applications dynamiques. Cela permet aux architectes et aux développeurs de choisir les zones de l'application où les efforts de programmation des composants dynamiques seront investis, en évitant le coût et la complexité de rendre tous les composants dynamiques. L'idée est de permettre aux architectes de déterminer l'équilibre entre les efforts à fournir et le niveau de dynamisme requis pour les besoins de l'application...|$|R
40|$|Video codec is {{a process}} which permits to encode or decode a video data stream. We have to be {{attentive}} to the efficiency of the codec that we choose in function of our goals. For instance, if we want to provide the best quality with the best speed, we will need a powerful system with different processors and which has a large memory. In the contrary, if we want to execute our video codec software in an embedded system, we will search to optimize the code and try to find out which are the bottlenecks of our encoder. In our case, we want to implement Kvazaar encoder on a DSP. Kvazaar encoder is a free software developped by a squad of finnish researchers and whose the architecture is really similar to HEVC. The main difference between these two entities is that Kvazaar was programmed in C language which makes it less complex and easier to implement on an embedded system. The main goal of this master thesis was to complete with success, the implantation of Kvazaar on the chosen platform and then to evaluate the performances. Concerning the implementation of Kvazaar on the DSP TI C 66 x, the main executed action was to suppress the use of threads during the compilation. Indeed, even if that DSP is multicore, threading is not allowed in that platform but parallelism can be done thanks to the use of DSP/BIOS which provides interruption use. Then, we had to study the program in order to find the main blocks which compose the video encoding for evaluating the encoding time mandatory to the execution of each block and then find which blocks can be optimized concerning the CPU cycles consumption. Indeed, the main blocks that we wanted to study concern the execution of transform, inverse transform, cabac, inter and intra prediction, quantization and inverse quantization and scaling. We noticed that, after doing many tests in order {{to increase the number of}} frames per second and the speed, the main block which needs the most CPU encoding time is the inter-prediction. The next work can be to find a way to decrease the time needed by that block to be executed because it is the main obstacle in order to obtain a better efficiency and speed of the video encoder Kvazaar. RÉSUMÉ. Le codec video est un precede qui permet de compresser ou decompresser des flux videos. On doit toujours tenir compte des performances permises par le codec en function de l'usage que l'on souhaite en faire. Par example, si on veut obtenir la meilleure qualite possible tout en assurant une certaine rapidite, on va utiliser un systeme multi-processeurs puissant avec une capacite memoire elevee. Si au contraire, on souhaite que notre codec s'execute sur un systeme embarque, on va chercher a optimiser le programme et trouver quelles sont les parties du code qui demandent le plus de temps a s'executer. Dans notre cas, on voudrait implementer l'encodeur video Kvazaar sur un DSP. L'encodeur Kvazaar est <b>un</b> <b>logiciel</b> gratuit developpe par une equipe de chercheurs finlandais et dont l'architecture est similaire au codec HEVC. La difference entre les deux logiciels reside notamment dans le fait que Kvaavar est code en langage C ce qui le rend moins complexe et plus facile implementer sur un systeme embarque. Le but premier de ce stage etait d'arriver a faire fonctionner Kvazaar sur la plateforme choisie et d'evaluer ses performances. En ce qui concerne l'implantation de Kvazaar sur le DSP TI C 6 Gx, la principale action effectuee etait de supprimer l'utilisation des threads dans l'execution du code. En effet, meme si notre DSP est multi coeur, le threading n'est pas autorise sur cette plateforme mais le parallelisme peut se realiser grace a DSP/BIOS qui permet l'usagc d'interruptions. Ensuite, il fallait etudier le code afin de trouver les principaux blocs qui constituent un encodage video pour evaluer le temps d'execution de chacun de ces blocs et trouver quels blocs seraient utiles a optimiser d'un point de vue du nombre de cycles du CPU necessaires. En effet, les principaux blocs que nous souhaitions etudier concernaient l'execution de la transformed et son inverse, Cabac, inter et intra prediction, la quantification et son inverse et le scaling. Il s'avere que parmi tous les tests effectues afin d'augmenter le nombre de frames encodees par seconde et la vitesse, le principal bloc qui monopolise le processeur est l'inter-prediction. Le prochain travail serait done de trouver un moyen pour diminuer le temps necessaire a ce bloc pour s'executer car c'est le principal rempart pour obtenir une plus grande efficacite et rapidite de l'encodeur video Kvazaar...|$|R
40|$|La {{simulation}} numérique est actuellement très utilisée pour étudier les systèmes physiques. Elle nécessite un programme de calcul scientifique constitué d'un modèle mathématique représentatif du problème étudié et des méthodes numériques de résolution associées. Elle fournit des résultats numériques censés représenter le phénomène physique. Pour pouvoir valider la simulation, il est absolument indispensable, d'une part, d'estimer la propagation des erreurs d'arrondi due à l'arithmétique approchée des ordinateurs et, d'autre part, d'évaluer l'influence des erreurs de données sur les résultats fournis. Nous présentons, dans cet article, le logiciel CADNA qui {{permet de}} valider les logiciels numériques. Nous l'appliquons à <b>un</b> <b>logiciel</b> de simulation d'analyse de la combustion dans les moteurs à allumage commandé et en montrons son efficacité. For analyzing physical phenomena, numerical simulation is used more and more frequently. Starting with a mathematical model describing the phenomenon being analyzed, this simulation consists in creating a scientific computing program expressing this model by implementing the numerical methods required for solving it. Simulation {{is considered to be}} valid when the results its provides are in agreement with the results issuing from experimenting with the phenomenon. However, to conclude in the possible validity of the simulation, the numerical results provided by the computer must be previously validated. Yet, these results contain a computing error resulting from the propagation of round-off errors caused by the floating-point arithmetic used by the computer. They also contain an error coming from the uncertainties concerning the data of the problem. Hence it is first indispensable to assess the influence of these errors. This article is made up of two parts. The first part concerns the validation of numerical software results. After making a brief review of the floating-point arithmetic and highlighting the serious consequences it may have on the results obtained, we describe a probabilistic approach to the analysis of round-off errors, the CESTAC (Contrôle et Estimation STochastique des Arrondis de Calculs) method, from the standpoint of both its theoritical bases and its practical implementation. This method has given rise to a new arithmetic, called stochastic arithmetic, the principal properties of which are summed up. Likewise, a probabilistic approach estimating the influence of data errors is described. A software called CADNA (Control of Accuracy and Debugging for Numerical Applications) able to automaticaly implement stochastic arithmetic in any Fortran program, is described in this paper. When used in programs implementing the three classes of numerical computing methods (finite, iterative and approximate methods), it can detect numerical instabilities, control branchings and provide accuracy of the results considering the propagation of round-off errors and data errors. It is an efficient tool for validating the results of numerical software. The second part is devoted {{to the use of the}} CADNA software for qualifying the simulation software, ANALCO (ANALyse de COmbustion) which analyses combustion in spark-ingnition engines. After a description of the normal model of the phenomenon being analyzed and after mathematical model has been deduced, the ANALCO simulation software is described. The results obtained with ANALCO, not using CADNA, reveal the disagreement between the simulation results and the experimental results. The use of the CADNA software eliminates the numerical instabilties, controls the execution of the program and demonstrates that the disagreement between the simulation results and the results observed is due only to numerical problems. Likewise, the CADNA software brings out both the validity range of the model in the light of the data errors and the data that make the mathematical model the most sensitive. From this analysis, we propose to improve the accuracy of the most influential data so as to widen the validity range of the model. This study shows that:(a) The ANALCO software not associated with CADNA;-does not detect the end of the combustion;-leads to results that are contrary to physical reality (oscillations of the mass of burned gases at the end of the combustion);- provides values of the CA 50 and HLC without giving their accuracy. (b) The ANALCO software associated with CADNA can be used :- to analyse the influence of data uncertainty. Hence with current sensors, the CA 50 and HLC are respectively provided with three and two significant decimal figure. It is impossible to provide significant results beyond a burned fraction of 90 %. By improving the accuracy of some sensors, we could reach 96 %;- to determine the three data making the model the most sensitive, i. e. the air flow rate, the angular position of the crankshaft and the heat transfer coefficient. In short, by the use of the CADNA sofware, the simulation software, ANALCO, has been qualified...|$|R
40|$|Le domaine d’étude de la {{cognition}} du primate est caractérisé par deux approches distinctes. L’approche Naturaliste consiste à s’intéresser aux comportements naturels, et à tenter de comprendre les facteurs et pressions sélectives qui ont permis l’émergence de ces fonctions au cours de la phylogenèse. L’approche Généraliste postule qu’il existe des traits cognitifs communs entre l’humain et l’animal, et cherche à identifier ces traits dans une recherche de laboratoire. Dans cet article, je présente une nouvelle méthode de recherche favorable à un interfaçage entre ces deux approches. La démarche consiste à <b>installer</b> <b>un</b> laboratoire en périphérie des enclos d’élevage, et à permettre aux sujets d’interagir librement avec les stations expérimentales, alors qu’ils sont maintenus dans leur groupe social. Cet article présente une synthèse des principaux résultats que nous avons obtenus avec cette approche, et les illustre, en en soulignant les avantages de notre approche {{par rapport}} aux approches plus traditionnelles de laboratoire. The study of primate cognition may adhere to two different paradigms. The {{first one is}} referred to as the Naturalist approach. Investigators in that field attempt to characterize natural behaviors, and to identify the various factors accounting for their evolution in nature.   The second one {{is referred to as}} the Generalist approach. Generalist researchers attempt to identify cognitive processes shared by animals and humans. While Naturalist researchers prefer natural or semi-natural conditions for their science, the context favored by the Generalist researchers is the laboratory allowing the test of isolated primates. In that paper, I present a novel experimental protocol favoring an interface between the Naturalist and Generalist approaches. The general idea of this protocol is to present a laboratory nearby an enclosure where monkeys live in a semi-natural context. The test systems installed within the laboratory are opened and freely accessible from the enclosure. The monkeys (baboons in our research) can voluntarily enter the test booth to participate to the research. This protocol has been fully developed, installed and validated in my laboratory during the last 3 years, and I present here the main results collected during that period. I (1) demonstrate that this approach allows the test of a large number of participants which participate to the research at a high frequency, (2) identify the main factors (age, day time, reinforcement rate, etc [...] ) that affect their behavior in the tasks, (3) illustrate the effect of the test systems on the spontaneous behavior of the monkeys, and (4) briefly present two studies that used this method, in order to illustrate its potential for the Generalist and Naturalist research programs...|$|R
40|$|Network administrators must {{configure}} {{network devices}} to simultaneously provide several interrelated {{services such as}} routing, load balancing, traffic monitoring, and access control. Unfortunately, most interfaces for programming networks are defined at {{the low level of}} abstraction supported by the underlying hardware, leading to complicated programs with subtle bugs. We present Frenetic, a high-level language for OpenFlow networks that enables writing programs in a declarative and compositional style, with a simple "program like you see every packet" abstraction. Building on ideas from functional programming, Frenetic offers a rich pattern algebra for classifying packets into traffic streams and a suite of operators for transforming streams. The run-time system efficiently manages the low-level details of (<b>un)</b> <b>installing</b> packet-processing rules in the switches. We describe the design of Frenetic, an implementation on top of OpenFlow, and experiments and example programs that validate our design choices. Office of Naval Research grant N 00014 - 09 - 1 - 0770 "Networks Opposing Botnets...|$|R
40|$|Cet article a été réalisé dans le cadre d'un projet de trois ans visant à développer une méthodologie d'analyse, de prévision et de contrôle des risques d'inondation au Québec. Il présente une approche concrète pour calculer les impacts potentiels et le risque d'inondation et {{utiliser}} ces résultats afin d'évaluer la situation du risque local, de décider si les impacts doivent être minimisés et de choisir les moyens d'intervention appropriés. Le risque d'inondation est considéré comme étant le produit de la probabilité d'occurrence des crues et des conséquences occasionnées par ces événements. Les pertes de vies potentielles et les dommages directs sont évalués en simulant les niveaux d'eau de différents scénarios d'inondation à partir d'un modèle d'écoulement unidimensionnel non permanent, et en intégrant ces résultats à <b>un</b> <b>logiciel</b> géoréférencé de calcul des dommages d'inondation. L'analyse des impacts et du risque calculé {{permet de}} dresser un portrait du montant des dommages annuels potentiels sur les sites habités le long du cours d'eau et un portrait de l'évolution des impacts en fonction de l'amplitude des crues. Cette analyse mène à l'identification des sites où existe un risque jugé inacceptable selon des critères préétablis. Pour chaque site où des interventions sont justifiées par le niveau de risque, des scénarios de minimisation des impacts tenant compte des mécanismes d'inondation sont élaborés et ensuite simulés afin d'en mesurer l'efficacité. Un exemple d'application à un site de la rivière Châteauguay illustre la méthode et les gains pouvant découler de son utilisation. Each year, several rivers in Quebec {{are responsible for}} severe flooding and these events generate major socio-economic impacts. The frequency and magnitude of these episodes highlight {{the existence of a}} real flood risk. Using global information concerning level and extent of flood risk, authorities {{would be more likely to}} make appropriate decisions in the management of flood risk. This article results from a three year project aimed at developing a methodology for the analysis, forecasting and control of flood risk in Quebec. It suggests a concrete approach for the evaluation of the potential impact of floods in order to obtain a better knowledge of local risk in inhabited areas and exploits there results to evaluate the acceptability of the calculated risk and to plan appropriate risk minimisation interventions. Risk is defined as the product of the mathematical expectation of a specified occurrence with the expected consequences of the event. In floodplain studies, flood risk is the probability of the occurrence at a given flood multiplied by the expected consequences resulting from this event. Different types of consequences may be observed, clearly the easiest to evaluate being direct or material damages and potential loss of life. The risk calculated using the proposed definition is attributable in variable proportions to the frequency of the floods and the amount of damages. A given calculated global risk on a site could be the result of frequent floods, each causing moderate damage or of a single (or more) extreme event, with very low probability of occurrence, but causing severe damage. Risk associated with rare events could be considered as an acceptable risk, a risk we decide to live with, since the resources available to prevent flood damage are often limited and a decision is taken to optimise the allocation of these resources. The flood level corresponding to the limit between acceptable and unacceptable risk must be determined by the population concerned and be based on a good knowledge of the risk situation. The proposed methodology to evaluate and minimise flood risk for a site localised in a river flood plain involves six steps: 1. the realisation of a hydrologic frequency analysis to determine the amplitude of the floods associated with the flood frequency, 2. the hydraulic simulation of floods to predict water level and velocity in the stream for each scenario, 3. the assessment of direct damage and potential loss of life for each flood simulated, 4. the calculation of risk, 5. the risk analysis considering the limit of acceptable risk and 6. the choice and planning of appropriate intervention to eliminate unacceptable risk. This approach has been applied to the study of a site along the Châteauguay River, a tributary of the St Laurence River, a river that experiences flood events every two years or so. Seven flood scenarios (the 2, 3, 10, 20, 100, 1 000, and 10 000 year flood) are used to evaluate the risk for a site localised in the village of Huntingdon. Hydraulic characteristics, water level and velocity, associated with each flood scenario are determined using the DAMBRK model, a one-dimensional unsteady flow model. The results are incorporated in DOMINO, a geo-referenced software calculating flood impacts. This software allows the user to create a three-dimensional numerical model of the site based on topographic information. The superposition of hydraulic results provides the flow depth at any point within the site. Damage is evaluated by integrating the municipal roll number of Huntingdon, which provide the site location and value of each building, and gives an estimate of the population threatened by each flood event at the site. These results of direct damages are used to calculate the risk related to each flood event simulated on the Huntingdon site. For this application, the unacceptable risk has generally been agreed to be the risk resulting from the 20 year flood, or more frequent floods, for the material damages and to the number of potential losses of life associated with the 100 year flood or more frequent flood. The analysis indicates that an unacceptable risk of 23 993 $ per year for material damages and potential loss of life of 50 persons exist. Different site scale interventions to eliminate this risk have been simulated and proven to be efficient only if complemented with a few local modifications to the more exposed buildings. This approach may be extended to the study of any river because it takes into consideration local hydrologic and hydraulic conditions. It has the advantage of being based on existing information and to be automated, which limits the time and resources required to obtain the base data and perform the necessary simulations...|$|R
40|$|International audienceWe {{report on}} the {{suitability}} of statistical model checking forthe analysis of quantitative properties of product line models by an extendedtreatment of earlier work by the authors. The type of analysis thatcan be performed includes the likelihood of specific product behaviour,the expected average cost of products (in terms of the attributes of theproducts’ features) and the probability of features to be (<b>un)</b> <b>installed</b> atruntime. The product lines must be modelled in QFLan, which extendsthe probabilistic feature-oriented language PFLan with novel quantitativeconstraints among features and on behaviour and with advancedfeature installation options. QFLan is a rich process-algebraic specifi-cation language whose operational behaviour interacts with a store ofconstraints, neatly separating product configuration from product behaviour. The resulting probabilistic configurations and probabilistic behaviourconverge in a discrete-time Markov chain semantics, enablingthe analysis of quantitative properties. Technically, a Maude implementationof QFLan, integrated with Microsoft’s SMT constraint solver Z 3,is combined with the distributed statistical model checker MultiVeStA,developed {{by one of the}} authors. We illustrate the feasibility of our frameworkby applying it to a case study of a product line of bikes...|$|R
40|$|We {{report on}} the {{suitability}} of statistical model checking {{for the analysis of}} quantitative properties of product line models by an extended treatment of earlier work by the authors. The type of analysis that can be performed includes the likelihood of specific product behaviour, the expected average cost of products (in terms of the attributes of the products 2 ̆ 7 features) and the probability of features to be (<b>un)</b> <b>installed</b> at runtime. The product lines must be modelled in QFLan, which extends the probabilistic feature-oriented language PFLan with novel quantitative constraints among features and on behaviour and with advanced feature installation options. QFLan is a rich process-algebraic specification language whose operational behaviour interacts with a store of constraints, neatly separating product configuration from product behaviour. The resulting probabilistic configurations and probabilistic behaviour converge in a discrete-time Markov chain semantics, enabling the analysis of quantitative properties. Technically, a Maude implementation of QFLan, integrated with Microsoft 2 ̆ 7 s SMT constraint solver Z 3, is combined with the distributed statistical model checker MultiVeStA, developed by one of the authors. We illustrate the feasibility of our framework by applying it to a case study of a product line of bikes...|$|R
40|$|Posterior al {{terremoto}} del 16 de abril del 2016 en Ecuador, el primer edificio en ser rehabilitado fue la Facultad de Inform 1 tica del campus de la Universidad Laica Eloy Alfaro de Manab (ULEAM), el cual tuvo da 1 o moderado en las paredes {{debido a}} que no eran completamente rectas, y da 1 o severo en las paredes del descanso de la grada. Para la reconstrucci 3 n se reemplazaron dichas paredes que eran de bloque o ladrillo, por paredes con Gypsum livianas y flexibles. En este art se presenta el buen comportamiento que tuvieron las paredes de Gypsum, en otras estructuras en contraste con las paredes de bloque o ladrillo, detall 1 ndose as mismo, algunos aspectos constructivos sobre la forma correcta de instalarlas. Finalmente, se realiz 3 un an 1 lisis espectral de la estructura empleando el espectro del terremoto del 16 de abril de 2016, de magnitud 7. 8, obtenido en Manta. En la estructura estudiada, se encontr 3 los desplazamientos laterales y derivas de piso, para asociar 9 ste valor con la no existencia de da 1 o estructural, y solo da 1 o moderado en la mamposter de las aulas y oficinas y severo en la grada. Posteriormente se realiz 3 un nuevo an 1 lisis s considerando la reducci 3 n del peso por el cambio del material de las paredes, determinando un mejor comportamiento s. Palabras clave: Terremoto del 16 de abril de 2016, Paredes de Gypsum, Facultad de Inform 1 tica de la ULEAM, Ecuador ABSTRACT After the earthquake of April 16, 2016 in Ecuador, the first building to be rehabilitated was the Faculty of Computer Science Campus of the University Laica Eloy Alfaro of Manabi (ULEAM), which had moderate damage in the walls due They were not straight and severe damage to {{the walls of the}} tier. For Reconstruction these walls were replaced that were made of block or brick with plaster walls flexible and lightweight. In this article it is presented the Good behavior plaster walls had in contrast to the brick or block walls, detailing also, some constructive aspects on the correct way of <b>installing.</b> Finally, <b>UN</b> Spectral analysis of the structure was performed using the spectrum of the earthquake of April 16, 2016, magnitude 7. 8, obtained in Manta. In the studied structure, lateral displacements and floors drifts were found. To associate with the latter value with the nonexistence of structural damage and solo moderate damage in the masonry of the classrooms and offices and Severe in the stands. Subsequently a new seismic analysis was made considering weight reduction by changing the material of the walls, determining the best seismic behavior UN. Keywords: Earthquake of April 16, 2016. Gypsum walls, Faculty of Informatics ULEAM, Ecuador <br...|$|R

