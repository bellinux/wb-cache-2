11|844|Public
50|$|Some {{versions}} of Fortran, {{such as those}} on the IBM System/360 and successor mainframes, {{do not support the}} PROGRAM statement. Many compilers from other software manufacturers will allow a fortran program to be compiled without a PROGRAM statement. In these cases, whatever module that has any non-comment statement where no SUBROUTINE, FUNCTION or BLOCK <b>DATA</b> <b>statement</b> occurs, is considered to be the Main program.|$|E
50|$|A similar {{thing as}} when using line number 65535 was typing in 'READY', after this the COMX was not 'READY' anymore. F&M {{discovered}} this one when they designed the F&M screen editor and pressed 'CR' (return) on the 'READY' prompt. As such {{they decided to}} change the prompt into 'OK' to avoid too many accidental hangings when using a screen editor. This fault was actually caused by the basic READ command, when a READ Y (or any other READ) instruction is given {{when there is no}} <b>DATA</b> <b>statement</b> in the loaded basic program the COMX hangs.|$|E
40|$|Abstract. S 7 - 200 Series PLC {{supplied}} by SIEMENS as controller {{has been adopted}} in this system, {{which is based on}} 3 D force-controlled configure software, using GPRS RTU transparent transmission function, to realize the remote monitor for the process, parameter and the operation status of the main equipment of rural domestic sewage treatment. The monitoring & indicating of sewage treatment process, the storage and the query of history memory of various monitoring data, control data and operation data, the generation of <b>data</b> <b>statement</b> and data curve, and the IE explorer remote access monitoring interface have been supported in this system...|$|E
50|$|Notice {{the use of}} {{the colon}} to chain {{together}} consecutive <b>DATA</b> <b>statements.</b>|$|R
5000|$|... {{reads the}} next variable(s) from the BASIC program {{embedded}} with <b>DATA</b> <b>statements</b> ...|$|R
50|$|<b>Data</b> <b>statements,</b> which assign {{numerical}} values to various changing parameters, constants, and initial conditions.|$|R
40|$|In a {{production}} environment, where dozens of programs are run in sequence, often monthly or quarterly, and where logs can span thousands of lines, it‟s easy {{to overlook the}} small stuff. Maybe a <b>data</b> <b>statement</b> fails to execute, but one already exists in the Temp library from a previous program. Maybe a global macro assignment is missed or fails to execute, but a global macro {{of the same name}} already exists from a previous program. This can also happen with macros. The list goes on. This paper offers some suggestions for housekeeping that can be performed {{at the end of each}} SAS ® program to minimize the chance of a hangover...|$|E
40|$|This paper {{features}} Base SAS and the X 12 -ARIMA Seasonal Adjustment program {{developed by}} the U. S. Bureau of the Census to seasonally adjust monthly or quarterly time series. X 12 -ARIMA commonly encounters problems with the time series used as input to the program and generates errors when problems occur. This paper illustrates how SAS {{can be used to}} detect errors generated by X 12 -ARIMA and to notify the user when these errors occur. In a production situation, X 12 -ARIMA is used to process a high volume of time series in a limited amount of time. This paper focuses on using the INDEX function in the <b>DATA</b> <b>statement</b> along with the SAS macro facility {{to reduce the amount of}} time taken to detect errors when processing a large number of time series with X 12 -ARIMA...|$|E
40|$|AbstractOperators {{of control}} rooms may become {{desensitized}} {{when they are}} exposed to a large number of alarms. Minimizing the number of non-actionable alarms can reduce this safety hazard. However, existing practice fails to address how operators anticipate system events, and discards valuable feedback signals. We observed five operators in three satellite control rooms to assess the functionality of their alarm design. We recorded 140 auditory signals, of which 31 were labeled as alarms. Two of these alarms were actionable. Transitional Journey Maps were used to represent qualitative interview data and observation <b>data.</b> <b>Statement</b> analysis revealed that operators anticipated the majority of the alarms. Bayesian inference related anticipation to response behavior. Case-by-case analysis identified various non-actionable anticipated alarms as feedback signals. Existing practice marks these signals as nuisance, which seems inappropriate. Therefore, this study shows that qualitative data analysis is essential for interpreting quantitative data on alarm responses...|$|E
5000|$|... —transfers {{program control}} to a machine {{language}} subroutine, usually entered as an alphanumeric string or in {{a list of}} <b>DATA</b> <b>statements.</b>|$|R
5000|$|The {{programs}} {{most often}} {{took the form}} of a short [...] "stub" [...] loader followed by a (sometimes lengthy) series of <b>DATA</b> <b>statements</b> that contained the core program logic as decimal opcodes. The loader mainly consisted of a For loop that enumerated the <b>DATA</b> <b>statements</b> and POKEd them into memory. The starting location for the POKEs was determined by the memory map of the platform in question; some loaders were able to distinguish between related platforms and alter the starting address based on what it found. Sometimes the <b>DATA</b> <b>statements</b> were in hexadecimal, in which case the loader would contain code to translate from hex to the decimal numbers demanded by the POKE command. After inserting the assembly language program into memory, the loader program could use BASIC's SYS command to initiate execution of the assembly language code from a designated entry point.|$|R
5000|$|Hollerith strings, {{originally}} {{allowed only}} in FORMAT and <b>DATA</b> <b>statements,</b> were prefixed by a character count {{and the letter}} H (e.g., [...] ), allowing blanks to be retained within the character string. Miscounts were a problem.|$|R
40|$|This thesis {{reports on}} an {{investigation}} {{carried out to}} study the effect of flaring and turning movements {{on the performance of}} roundabout entries. A computer simulation program was developed to carry out the investigation. The model simulates an entry with two lanes at the approach section and four at the stop line. It can be modified easily to simulate straight entries by changing the input and one <b>DATA</b> <b>statement.</b> Data were collected at three public road sites at Sheffield to validate the model. A method of analysing the data was developed to obtain values of the gap-acceptance parameters. The values arrived at were used subsequently as input into the model to allow direct comparison of observed and simulated values. The comparison concluded that the model represents adequately the real conditions. The results produced showed that average delay for below-capacity operation is reduced by at least 40...|$|E
40|$|The {{purpose of}} this study was to {{determine}} whether certain assumptions generally given to collect cumulative record data were accurate. The population consisted of 273 students enrolled in grade one, grade two, grade three, and grade four in the Bozeman Public Schools during the years 1968 - 1971. Performance data from three areas of their cumulative records were used in this study. The three areas were grades received, activities participated in, and character traits as measured subjectively by teacher evaluation. Performance data were evaluated according to: a. - whether a child lived with his parent or other than his parent, b. whether a child’s mother worked or remained at home to care for him, and c. whether the child walked to school or rode the bus. When the statistical analysis was applied to the study, significant differences in school performance resulted in each of the above categories. Trends of the results of the study are inconsistent. Further study is needed to clarify the accuracy of the assumptions for collecting cumulative record <b>data.</b> <b>STATEMENT</b> OF PERMISSION TO COPY In presenting this thesis in partial fulfillment of the require ments for an advanced degree at Montana State University, I agree that the Library shall make it. freely available for inspection. I further agree that permission for extensive copying of this thesis for scholarly purposes may be granted by my major professor, or, in his absence, by the Director of Libraries. ' It is understood that any copying or publication of this thesis for financial gain shall not be allowed without my written permission...|$|E
40|$|This {{pilot study}} was {{conducted}} in order to explore factors that facilitate and inhibit the teaching of online courses from an administrative perspective. A random sample of community college and public and private universities was selected, and administrators working closely in online education were invited to participate. A qualitative (interview-based) research design was used. Administrators of two public universities, one private university, and two community colleges participated; 12 interviews were completed and 1 additional participant e-mailed responses for a total of 13 data sets. Facilitating factors included concerns for institutional survival, student demand, fulfilling professional responsibilities to one’s field by expanding access to the profession through online programs, and the flexibility afforded by online courses. Inhibiting factors included preparation time in terms of designing high-quality online courses, fear of and resistance to change, the fit between online education and select curricula, and missing the “energy ” of the classroom. The study considered blended education as well but the data pertained exclusively to online education. Disruptive Innovation Theory (Christensen 2003; Christensen et al. 2011) was used to interpret the <b>data.</b> <b>Statement</b> of the Problem This study explores the factors that facilitate and inhibit faculty members ’ participation in online (including blended) education in colleges and universities in one Midwestern state. Given the growth in online education (IVC Report 2010), a focus on both facilitating and inhibiting factors may seem paradoxical. On the one hand, literature suggests that online education is a “growth industry ” (IVC Report 2010); on the other hand, negative perceptions of and resistance to online education on the part of faculty continue (Ruth 2006). By determining what these facilitating and inhibiting factors are, administrators in similar settings throughout the world may be able to articulate and influence policies and procedures s...|$|E
50|$|Control Data Corporation {{computers}} {{had another}} version of FORTRAN 77, called Minnesota FORTRAN (MNF), designed especially for student use, with variations in output constructs, special uses of COMMONs and <b>DATA</b> <b>statements,</b> optimizations code levels for compiling, and detailed error listings, extensive warning messages, and debugs.|$|R
5000|$|OS JCL: DD {{statements}} {{can be used}} {{to describe}} in-stream data, as well as data sets. A DD statement dealing with in-stream data has an asterisk (*) following the DD identifier, e.g[...] JCL statements can be included as part of in-stream data by using the DD <b>DATA</b> <b>statements.</b>|$|R
40|$|Drawing {{together}} {{research on}} persuasion and text comprehension, two experiments test {{the effects of}} hedge placement (Experiment 1) and hedge type (Experiment 2) on atti-tudes, source evaluations, and perceptions of argument strength. Participants read an editorial in support of implementing comprehensive exams at their university. Experiment 1 shows that hedges placed on <b>data</b> <b>statements</b> (and not interpretation state-ments) lead to negative perceptions of the policy, source, and argument. This is espe-cially pronounced on source evaluations among individuals with more scientific training. Experiment 2 reveals that colloquial, but not professional, hedges placed on interpreta-tion statements lead to more negative evaluations relative to no hedges. Data related to perceptions of the source are moderated by individual differences in scientific reasoning. This research suggests that hedges describing <b>data</b> <b>statements</b> and/or that use colloquial language can, but do not always, undermine persuasive attempts...|$|R
40|$|International audienceBackground, Motivation and ObjectiveIn {{the last}} decade the number of {{beamforming}} methods has exploded. Many innovative ideas have been proposed, but we lack the tools to compare the different techniques efficiently. The PICMUS challenge (IUS 2016, Tours) was a pioneering step that made clear that two things are required to establish a fair comparison: a common data format, and a body of methods to process that <b>data.</b> <b>Statement</b> of Contribution/MethodsThree universities have come together {{to address this problem}} by developing both a dedicated file format and a beamforming toolbox. We present the Ultrasound File Format (UFF) : a HDF 5 data format, open to any programming language, for storage of channel and beamformed data; and the UltraSound ToolBox (USTB) : a MATLAB toolbox made of both native MATLAB and C++ code, with GPU support. Together UFF and USTB provide a unified structure to share and process 2 D and 3 D ultrasound data. Channel data from any origin, e. g. Field II or Verasonics, and using any sequence, e. g. synthetic transmit aperture imaging (STAI) or coherent plane-wave compounding (CPWC), can be stored in UFF and beamformed with USTB. Both UFF and USTB revolve around the concept of the General Beamformer. The wavefronts in most ultrasound sequences can be fully defined using a single point source P: in focus imaging (FI) and retrospective transmit beamforming (RTB) P is placed at the focal point, in diverging wave imaging (DWI) P is at the origin of the diverging wave, in STAIP lies on the active element, in CPWC P is at infinite in a given direction. Based on that definition we developed a data structure (Fig. 1) that makes it possible to beamform all sequences with a single algorithm, or even combine them. To illustrate this, 5 datasets have been simulated (FI, STAI, CPWC, DWI, and RTB) and reconstructed with USTB’s general beamformer. It is known that CPWC is equivalent to STAI under certain circumstances. Here we use USTB to show that the same result can be extrapolated to DWI and RTB. The equivalence is demonstrated in terms of side lobe level(SLL) and full width half maximum (FWHM). Results/DiscussionFig. 2 a-e show the point spread function (PSF) of the tested sequences. The 5 methods have a nearly identical PSF with a FWHM of 369. 00 ± 9. 42 μm (Fig. 2 f) and a SLL of - 26. 00 ± 0. 89 dB of (Fig. 2 g). This validates the general beamformer and supports the equivalence of the methods. The aim of USTB is to facilitate the comparison between beamforming techniques and encourage the publication of datasets and algorithms. It is free, open source, and open to contributors. A fully functional toolbox will be released in June 2017, but an alpha version is available at [URL]...|$|E
40|$|As {{the average}} family income and living {{standard}} increase, both the people and governments {{are becoming more and}} more aware of the environmental issues. Hoping to be part of the solutions for ameliorating the environment issues, thus people and governments started purchasing green products or conducting green procurements. Many corporations saw this whole new green product consumption trend as a brand new opportunity for profit expansion, and started offering all kinds of green product as alternatives to traditional commercial goods. As a result, the number of the sales for green product or the demand for green product is increasing every year. However, the rate for the demand of green products is not climbing as fast as expected. There are many factors may accelerate the rate for demand of green products, for example financial incentives offered by either governments or retailers, and taxation on the non-green products. Whether these factors actually affect the demand is uncertain, which create uncertainties for the retailers to precisely predict the procurement quantity for green product to avoid inventory risk. In order to resolve the procurement problem caused by uncertainties, this thesis proposes using portfolio contracts to mitigate the inventory risk. The portfolio consists of three types of contract: structured (S), option, and short term (ST) contracts. The option contract is the key of this portfolio of contracts, where it uses the concept of real options to grants retailers the flexibility in adjusting order quantity for green products. The Black-Scholes-Merton option pricing model is used to calculate the option price. Simulations are conducted to analyze effectiveness of the portfolio contracts. The simulation creates different green product demand lines bases on the data of historical sales of paper products of Cheng Loong Corporation. The total cost for procuring with portfolio contracts is compared with extreme case of only S contract to see the effectiveness. The simulation results found that (1) switching contract type from long-term only to a portfolio of contracts can lower procurement costs, (2) a portfolio of contracts generates even less procurement costs compare to a long-term contract only as market volatility increases to 80 %, (3) the unit price with option contract affects the overall procurement cost, (4) the simulation result indicates that, the S contract volume inside the portfolio should be around 96 % of the minimum demand of previous year to mitigate procurement cost, (5) increasing option contract volume inside the portfolio generates less procurement cost compare to a long-term contract only. However, the simulation result shows option contract volume exceeding 70 % wanes the cost benefit of a portfolio of contracts, and (6) the minimum procurement cost can be achieved, if a portfolio of contracts has the appropriate contract volume and unit price of green product. Chapter 1 Introduction	 1 1. 1 Background	 1 1. 2 Research Motive and Purpose	 4 1. 3 Methodology	 6 1. 4 Thesis Organization	 6 Chapter 2 Literature Review	 9 2. 1 Green Product	 9 2. 2 Green Procurement	 11 2. 3 Valuation of Options	 13 2. 3. 1 Real Options	 13 2. 3. 2 Black-Scholes-Merton Model	 15 2. 4 Portfolio of Contracts	 18 Chapter 3 Portfolio Model Construction and Results	 21 3. 1 Problem Statement and Assumptions	 21 3. 2 Derivation of the Modeling of Portfolio of Contracts	 23 3. 2. 1 Variables Definition	 24 3. 2. 2 Modeling of Portfolio of Contracts	 24 3. 2. 3 Option Price of the Option Contract	 27 3. 3 Data Analysis	 28 3. 3. 1 <b>Data</b> <b>Statement</b>	 28 3. 3. 2 Geometric Brownian Motion Simulation	 31 3. 3. 4 Simulation Results	 34 Chapter 4 Sensitivity Analysis and Discussion	 39 4. 1 Test of Market Volatility	 39 4. 2 Test of Unit Price in Option Contract	 40 4. 3 Test of S Contract Volume	 42 4. 4 Test of Option Contract Volume	 43 4. 5 Optimal Condition	 45 Chapter 5 Conclusion and Suggestions	 51 5. 1 Conclusion	 51 5. 2 Limitations and Future Suggestions	 52 Reference	 54 Appendix A: Matlab Code	 5...|$|E
40|$|The <b>Data</b> Availability <b>statement</b> {{does not}} appear on the paper. The correct <b>Data</b> Availability <b>statement</b> is: The authors confirm that all data {{underlying}} the findings are freely available without restriction. Data {{are included in the}} paper and structural data {{can be found in the}} Protein Data Bank under PDB IDs: 4 NTA, 4 NTB and 4 NTF. The publisher apologizes for this error...|$|R
40|$|We {{examine the}} {{institutional}} {{details of the}} school milk procurement process, bidding <b>data,</b> <b>statements</b> of dairy executives, and supply characteristics in Ohio during the 1980 s. We compare the bidding behavior {{of a group of}} firms in Cincinnati to a control group. We find that the behavior of these firms is consistent with collusion. The estimated average effect of collusion on market prices is about 6. 5 %, or roughly the cost of shipping school milk about 50 miles. ...|$|R
5000|$|Imperative {{programming}} in D is {{almost identical to}} that in C. Functions, <b>data,</b> <b>statements,</b> declarations and expressions work {{just as they do}} in C, and the C runtime library may be accessed directly. On the other hand, some notable differences between D and C in the area of imperative {{programming in}}clude D's [...] loop construct, which allows looping over a collection, and nested functions, which are functions that are declared inside of another and may access the enclosing function's local variables.|$|R
50|$|Type-ins {{were usually}} written in BASIC or a {{combination}} of a BASIC loader and machine language. In the latter case, the opcodes and operands of the machine language part were often simply given as <b>DATA</b> <b>statements</b> within the BASIC program, and were loaded using a POKE loop, since few users had access to an assembler. In some cases, a special program for entering machine language numerically was provided. Programs with a machine language component sometimes included assembly language listings for users who had assemblers and who were interested in the internal workings of the program.|$|R
5000|$|Understand {{important}} company {{communications and}} <b>data,</b> including financial <b>statements</b> ...|$|R
5000|$|... (<b>Data</b> Definition) <b>statements,</b> which {{identify}} a data file {{to be used}} in a step, and detailed info about that file. [...] statements can be in any order within the step.|$|R
5000|$|... #Subtitle level 2: Terminology, <b>data,</b> and two <b>statements</b> of the theorem ...|$|R
5000|$|Analytical {{procedures}} include {{comparison of}} financial information (<b>data</b> in financial <b>statement)</b> with ...|$|R
30|$|The problem {{seems to}} be very similar to a typical {{periodicity}} mining in time series [8, 12], where analysis is performed on the long sequences of elementary data items discretized into a number of ranges and associated with the timestamps. In our case, input data are a sequence complex <b>data</b> processing <b>statements,</b> like for example SQL statements and due to its internal structure cannot be treated {{in the same way as}} analysis of elementary data elements in time series or genetic sequences. The complex <b>data</b> processing <b>statements</b> form a lattice whose elements are syntax trees of the statements with a partial order determined by an inclusion relationship on syntax trees [14].|$|R
50|$|<b>Data</b> {{declarative}} <b>statements</b> {{provide the}} compiler {{with information about}} data element definitions. They define the format, structure and order of data elements in a compile-time system. The three major kinds of data are switches, variables and aggregates.|$|R
40|$|<b>Data</b> quality <b>statements</b> are now {{entrenched in}} {{metadata}} standards worldwide. I contrast {{the needs of}} the user with the production-control mechanisms of the producer, and argue that metadata standards are producer-centric. To the user, the ability of data sets to interoperate is of major concern, as is the experience of prior users, the accessibility of quality statements, and the ease with which quality information can be handled in local software. Many of the newer geospatial tools that are oriented to the general public provide no <b>data</b> quality <b>statements</b> whatever. I present a series of use cases of metadata, and use them to argue for a reexamination of metadata standards, and the beginnings of a second generation of standards development that addresses these issues. 1...|$|R
40|$|The <b>Data</b> Availability <b>{{statement}}</b> {{for this}} paper is incorrect. The correct statement is: The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are within the paper and its Supporting Information files. Please see the Supporting Information below. Supporting Informatio...|$|R
3000|$|The {{data that}} support the {{findings}} of this study are available from the Tokyo Shoko Research (website: [URL] but restrictions apply to the availability of these data, which were used under license for the current study, and so are not publicly available. TSR offers various services/ data in accordance with customers’ demand, such as TSR Report, TSR Company Profile <b>Data</b> File, financial <b>statement</b> <b>data,</b> Internet Service [...] "tsr-van 2 ", viability scores, etc.|$|R
40|$|As {{part of the}} National Innovation and Science Agenda, the Australian Government has today {{released}} its Public <b>Data</b> Policy <b>Statement.</b> This formalises the Government’s commitment to open data and data-driven innovation. Access to {{and use of the}} data that is collected by Commonwealth Government entities, referred to as ‘public data’, has the potential to stimulate innovation and help grow the economy, improve service delivery and decision making for planning and policy development. Our capacity to remain competitive in the global digital economy depends on how well we can harness the value of public data. The Public <b>Data</b> Policy <b>Statement</b> indicates the Government has prioritised effective data management as one if its core priorities and the Policy Statement provides a clear mandate for Commonwealth Government entities to optimise the use and reuse of public data...|$|R
5000|$|AUTO 11,2 [...] DEFine FN Iso(S,O) LOCal y%,m%,d%,i$,n%,w% [...] REM Step 0 - {{to isolate}} {{components}} of a date Stamp [...] "YEARMoDa" [...] LET y%=S(1TO 4) : m%=S(5TO 6) : d%=S(7TO 8) [...] REM Step 1 - to apply Lachman's Method of Congruence LET i$=m%*2.56+ 193 : S=S(1TO 6)- 3 [...] REM Step 2 - to compute the day-number within the week LET w%=(S(1TO 2)&"32"DIV 16+ S(1TO 4)DIV 4+ y%+ i$(2TO 3)+ d%)MOD 7 [...] REM Step 3 - to return result SELect ON O ON O= 5 : n%=i$(2TO 3) ON O= 4 : n%=y% ON O= 3 : n%=m% ON O= 2 : n%=d% ON O= 1 : n%=w% ON O= REMAINDER : n%=-1 END SELect RETurn n% [...] REMark <b>data</b> <b>statements</b> DIM weekdays$(6,3) RESTORE 190 FOR count=0 TO 6 : READ weekdays$(count) ...|$|R
40|$|Florida Geological Survey {{map series}} number 15 "May 1965. "In upper left margin: United States Department of the Interior, Geological Survey. Includes text, table, and {{ancillary}} map showing <b>data</b> availability. (<b>Statement</b> of Responsibility) by William J. Shampine; prepared by U. S. Geological Survey {{in cooperation with}} the Florida Geological Survey, 1964...|$|R
