26|73|Public
50|$|The back had a then-massive 16MB of RAM {{to act as}} an <b>image</b> <b>buffer,</b> as well as a PCMCIA {{card slot}} for image storage, plus a SCSI socket for {{connection}} to a computer. The imaging element was an APS-C sensor with a 1.5x crop factor, and a resolution of 1268 x 1012 pixels (1.3 mp). The camera back did not have an LCD monitor.|$|E
50|$|TrueBrilliance was {{notable for}} its ability to edit true 15 and 24-bit color images, even on older Amigas which could only display HAM-6 (pseudo-12-bit color) graphics. In such cases, the image was {{rendered}} as a HAM display, but all modifications were performed on the underlying true color <b>image</b> <b>buffer.</b> Even when the final image was intended for HAM display, this had the advantage that successive operations did not accumulate HAM artifacts on top of each other. Loss of quality could be restricted to a single HAM conversion {{at the end of the}} process.|$|E
50|$|The shear warp {{approach}} to volume rendering {{was developed by}} Cameron and Undrill, popularized by Philippe Lacroute and Marc Levoy. In this technique, the viewing transformation is transformed such that the nearest face of the volume becomes axis aligned with an off-screen <b>image</b> <b>buffer</b> with a fixed scale of voxels to pixels. The volume is then rendered into this buffer using the far more favorable memory alignment and fixed scaling and blending factors. Once all slices of the volume have been rendered, the buffer is then warped into the desired orientation and scaled in the displayed image.|$|E
50|$|OGIs use OGAs to wrap <b>image</b> <b>buffers</b> {{from various}} imaging libraries. OGI {{simplifies}} loading/modifying/saving OpenGL textures, FBOs and VBOs.|$|R
50|$|The {{flexibility}} of ARINC 818 allows for receiver implementations using either full <b>image</b> <b>buffers</b> or just display-line buffers. For either, synchronization issues {{must be considered}} at the pixel, line, and frame level.|$|R
50|$|OpenEXR 2.0 was {{released}} in April 2013, extending the format with support for deep <b>image</b> <b>buffers</b> and multiple <b>images</b> embedded in a single file. Version 2.2, released August 2014, added the lossy DWA compression format.|$|R
5000|$|Additionally, {{post-processing}} {{is commonly}} used in 3D rendering, especially for video games. Instead of rendering 3D objects directly to the display, the scene is first rendered to a buffer {{in the memory of}} the video card. Pixel shaders and optionally Vertex shaders are then used to apply post-processing filters to the <b>image</b> <b>buffer</b> before displaying it to the screen. Some post-processing effects also require multiple-passes, gamma inputs, vertex manipulation and depth buffer access. Post-processing allows effects to be used that require awareness of the entire image (since normally each 3D object is rendered in isolation). Such effects include: ...|$|E
5000|$|Major updates to the {{graphics}} code in May 2002 resulted in Chasys Draw DTFx (Direct Tool eFfects). The new graphics code being referred to here {{was actually a}} miniature bitmap abstraction engine that allowed for fast per-pixel operations and direct <b>image</b> <b>buffer</b> access (much as the DIB engine does for GDI). The engine was named JpDRAW. This version was also done in VB, but was much faster than all the previous versions. The new graphics code allowed for more tools to be implemented than was ever possible before. Later on in 2002, the developer decided to completely abandon VB as a programming platform and moved all the code to C/C++. The move to C/C++ allowed {{the development of a}} full-fledged graphics engine which was named JpDRAW2. Chasys was renamed to Chasys Draw Artist, and the CD5 image format was also updated to reflect the new features. By coincidence, the module that implemented the file format was the fifth module to be added, so the format was called Chasys Draw module 5, retaining the [...]cd5 file extension.|$|E
40|$|This paper {{introduces}} {{a framework for}} shot boundary detection used by the Language and Media Processing Laboratory (LAMP) at University of Maryland. It handles detection of cuts, dissolves and fades. Generally, it computes pixel-to-neightbor image differences in the videos. The cut detection applies the so-called secondmax ratio criterion in a sequential <b>image</b> <b>buffer.</b> The dissolve detection {{is based on a}} skipping image difference and linearity error in a sequential <b>image</b> <b>buffer.</b> We also filter out the effects of camera flash. We submitted five runs to TRECVID 2004. Their run-ids are D 0, D 1, [...] ., D 4. They use the same set of algorithms with different parameter settings. 1...|$|E
40|$|Xr {{provides}} a vector-based rendering API with output {{support for the}} X Window System and local <b>image</b> <b>buffers.</b> PostScript and PDF file output is planned. Xr is designed to produce identical output on all output media while taking advantage of display hardware acceleration through the X Render Extension...|$|R
40|$|Previously, some of {{the authors}} {{demonstrated}} a technique for parallel vector lock-in thermal wave infrared (IR) video imaging, using a patented technique. [1 – 3] The initial version [2] of the technique was restricted to the frame rate frequency of 15 Hz for the single-detector IR camera used for the experimental verification. In an application for imaging cracks in Cu microbridges, an alternative version of the technique [3] succeeded in achieving IR video lock-in imaging at frequencies up to 2 kHz. However, the latter implementation was seriously complicated by the complex timing pattern through which the pixels were acquired by the IR imaging system. The complication resulted {{from the fact that}} one needs to know the timing of each pixel, because the method assumes that the IR signal corresponding to each pixel of the image can be multiplied by the sine and cosine functions of the thermal modulation frequency, computed at the time at which the pixel datum was obtained. After multiplication, the results are accumulated, pixel-by-pixel, in two <b>image</b> <b>buffers,</b> which represent the two vector components of the lock-in image. If desired, these <b>image</b> <b>buffers</b> can then be transformed into magnitude (the square root of the sum of the squares of the components) and phase (the arctangent of the ratio of the two components) <b>image</b> <b>buffers...</b>|$|R
5000|$|Cairo {{supports}} {{output to}} a number of different back-ends, known as [...] "surfaces" [...] in its code. Back-ends support includes output to the X Window System, via both Xlib and XCB, Win32 GDI, OS X Quartz Compositor, the BeOS API, OS/2, OpenGL contexts (directly and via glitz), local <b>image</b> <b>buffers,</b> PNG files, PDF, PostScript, DirectFB and SVG files.|$|R
40|$|This paper {{introduced}} {{a framework of}} shot boundary detection used by the LAMP LAB, University of Maryland, College Park. It deals with cuts and dissolves detection. Generally, {{it is based on}} image correlation features in the videos. The cut detection is based on the so-called 2 max ratio criterion in a sequential <b>image</b> <b>buffer.</b> The dissolve detection is based on the skipping image difference and linearity error in a sequential <b>image</b> <b>buffer.</b> We also eliminated the effect of flashlights in our algorithms. We submitted 5 runs to TRECVID 2004. Their run-ids are “D 0, D 1, …, D 4 ”. They use the same algorithms with different parameter settings. These runs show a tradeoff curve between precision and recall on the testing data of TRECVID 2004. 1...|$|E
40|$|Abstract. In this paper, {{we propose}} {{a method for}} {{directly}} rendering point sets which only have positional information by using recent graph-ics processors (GPUs). Almost all the algorithms in our method are pro-cessed on GPU. Our point-based rendering algorithms apply an <b>image</b> <b>buffer</b> which has lower-resolution image than a frame buffer. Normal vectors are computed and various types of noises are reduced on such an <b>image</b> <b>buffer.</b> Our approach then produces high-quality images even for noisy point clouds especially acquired by 3 D scanning devices. Our approach also uses splats in the actual rendering process. However, the number of points to be rendered in our method is in general less {{than the number of}} input points due to the use of selected points on an im-age buffer, which allows our approach to be processed faster than the previous approaches of GPU-based point rendering. ...|$|E
30|$|Listing Listing 3 CMjpegDec::Transform method partial {{assembly}} code shows the {{assembly code}} of CMjpegDec::Transform method that receives a media sample (i.e., IMediaSample *pIn) and outputs another media sample (i.e., IMediaSample *pOut), {{with the image}} decompression carried out at line 7 by the function DecompressBegin that receives as arguments an output YCbCr buffer, an input JPEG <b>image</b> <b>buffer</b> and an image header info.|$|E
5000|$|Improved OpenGL {{interoperability}} through efficient {{sharing of}} <b>images</b> and <b>buffers</b> by linking OpenCL and OpenGL events.|$|R
5000|$|In Talisman, <b>image</b> <b>buffers</b> {{were broken}} down into 32 x 32 pixel [...] "chunks" [...] that were {{individually}} rendered using the 3D objects and textures provided by the CPU. Pointers to the chunks were then stored in a z-ordered (front to back) list for every 32 scan-lines on the display. One {{concern is that the}} chunks cannot be cleanly [...] "stitched together", a problem that has sometimes been visible in various videogames using software rendering. To avoid this, Talisman also stored a separate [...] "edge buffer" [...] for every chunk that stored an [...] "overflow" [...] area that would cover gaps in the mapping.|$|R
40|$|Xr {{provides}} a vector-based rendering API with output {{support for the}} X Window System and local <b>image</b> <b>buffers.</b> PostScript and PDF file output is planned. Xr is designed to produce identical output on all output media while taking advantage of display hardware acceleration through the X Render Extension. Xr {{provides a}} stateful user-level API with support for the PDF 1. 4 imaging model. Xr provides operations including stroking and filling Bézier cubic splines, transforming and compositing translucent images, and antialiased text rendering. The PostScript drawing model has been adapted for use within C applications. Extensions needed to support much of the PDF 1. 4 imaging operations have been included. This integration of the familiar PostScript operational model within the native application language environment provides a simple and powerful new tool for graphics application development. The design of the Xr library is motivated {{by the desire to}} provide a high-quality rendering interface for all areas of application presentation, from labels and shading on buttons to the central image manipulation in a drawing or painting program. Xr targets displays, printers and local <b>image</b> <b>buffers</b> with a uniform rendering model so that applications can use the same API to present information regardless of the media. The Xr library provides a device-independent API, and can currently drive X Window System[10] applications as well as manipulate images in the application address space. It {{can take advantage of the}} X Render Extension[7] where available but does not require it. The intent is to add support for Xr to produce PostScript[1] and PDF 1. 4 [5] output. Moving from the primitive original graphics system available in the X Window System to a complete device-independent rendering environment should serve to drive future application development in exciting directions. 1. 1 Vector Graphics...|$|R
40|$|Photorealistic {{rendering}} {{presents a}} main approach for depicting virtual models and worlds. In photorealistic rendering, illumination {{is a key}} functionality to achieve appealing and plausible visual results. Due to the diversity of effects and global nature of light, interactive applications require a simplification of illumination (commonly to local illumination only considering direct illumination). But even with this limitation the number of light sources remains restricted for arbitrary scenes. Forward rendering is the standard way of applying lighting, the operation of calculating illumination, which is performed for each object. As such, the complexity of lighting depends on scene complexity. However, virtual scenes constantly increase in complexity by pursuing more realism. This development also increases lighting costs and thus limits a similar {{growth in the number}} of light sources. One solution is separating lighting from scene complexity. This thesis describes two approaches to decouple lighting from geometry processing. Both techniques, deferred shading and deferred lighting, render a view dependent scene description into an <b>image</b> <b>buffer</b> (so called G-Buffer). Lighting is performed in a deferred stage in image space with geometry data read from the <b>image</b> <b>buffer.</b> We limit the light distribution of a light source to a volume and thus perfor...|$|E
3000|$|... {{domain of}} the {{projection}} data. A chunk of the data {{is sent to the}} FPGA and processed. The host code waits until the FPGA signals processing is complete, and then transmits the next chunk of data. When all projection data have been processed, the host code requests that the final target image be sent from the FPGA. The pixels of the target image are scaled, rearranged into an <b>image</b> <b>buffer,</b> and an image file is optionally produced using a library call to the GTK+ library [29].|$|E
30|$|However, {{the process}} of {{converting}} the backprojection output into an <b>image</b> <b>buffer</b> that {{can be converted to}} a PNG image (Form Image) runs faster when executed as part of the software program. This step is performed in software regardless of where the backprojection algorithm was executed. The difference in run time can be attributed to memory caching. When backprojection occurs in software, the result data lie in the processor cache. When backprojection occurs in hardware, the result data are copied via DMA into the processor main memory, and must be loaded into the cache before the Form Image step can begin.|$|E
40|$|Xr {{provides}} a vector-based rendering API with output {{support for the}} X window system, lo-cal <b>image</b> <b>buffers</b> and plans for PostScript and PDF files. Xr is designed to produce identical output on all output media while taking advan-tage of display hardware acceleration through the X Render Extension. Xr {{provides a}} stateful user-level API with sup-port for the PDF 1. 4 imaging model. Xr pro-vides operations including stroking and filling Bézier cubic splines, transforming and com-positing translucent images, and antialiased text rendering. The PostScript drawing model has been adapted for use within C applications. Extensions needed to support much of the PDF 1. 4 imaging operations have been included. This integration of the familiar PostScript op-erational model within the native application language environment provides a simple and powerful new tool for graphics application de-velopment. ...|$|R
50|$|Initially, Color QuickDraw {{was only}} capable of {{operating}} with 1, 2, 4 and 8-bit video cards, which {{were all that}} was available at the time. Soon after however, 24-bit video cards appeared (so-called true color), and QuickDraw was updated again to support up to 32 bits per pixel (in reality, 24 bits, with 8 unused) of color data ("32-Bit QuickDraw"). The architecture always allowed for this, however, so no new APIs were necessary. The color data structures themselves allowed a color depth of 1, 2, 4, 8, 15 and 24 bits, yielding 1, 4, 16, 256, 32,768 and 16,777,216 colors respectively, or 4, 16 and 256 scales of grey. QuickDraw took care of managing the resampling of colors to the available color depths of the actual video hardware, or transfer between offscreen <b>image</b> <b>buffers,</b> including optionally dithering images down to a lower depth to improve image quality. A set of color sampling utilities were also added so that programmers could generate optimal color palettes for use with indexed video devices.|$|R
40|$|Polytene {{chromosomes}} from Drosophila melanogaster, observed from squash preparations, and chromosomes from Chironomus thummi thummi, investigated under physiological conditions, are imaged {{using an}} Atomic Force Microscope. Various chromatin fiber structures {{can be observed}} with high detail in fixed chromosomes and correspond to structures which are also observed in chromosomes of diploid cells. Unfixed chromosomes can be <b>imaged</b> in <b>buffer</b> and show less fiber-like details because of the inherent soft nature of the chromatin material...|$|R
40|$|We {{demonstrate}} a compact real-time optical image correlator using diode lasers and a semi-insulating GaAs/AlGaAs multiple-quantum-well (SI-MQW) device as the holographic element. With only 3 mW of power incident upon the SI-MQW device, the correlation is obtained in 1 microsec {{and can be}} erased in as fast as 2 microsec, which presents {{the possibility of a}} system capable of 3 x 10 exp 5 correlations/s. We also show that images can be stored for a controllable length of time (2 - 25 microsec), which presents possibilities {{for the use of the}} SI-MQW device as a data or <b>image</b> <b>buffer</b> memory...|$|E
40|$|We compare {{two methods}} of {{acquiring}} ultrasound tongue images. A new system capable of recording {{directly from the}} cineloop <b>image</b> <b>buffer</b> at a high frame rate and which is more accurately synchronized with audio is compared with an optimised method of recording images via the NTSC video output of an ultrasound machine. As a focus for this comparison we gathered representative data on English /l from a single speaker, using a headset restraint system. Both systems performed well, but while the video system is at its limits, the cineloop system is inherently more accurate and offers greater opportunity for development. ...|$|E
40|$|Abstract—A {{space sensor}} often {{consists}} {{of a number of}} dies integrated side-by-side on a single substrate of package due to its high-resolution requirement. The sensing areas of these dies often cannot align both vertically and horizontally. This problem could cause serious image distortion if not handled properly. Trying to fix this problem at the base station on the earth is less effective since the expanded image data would have been too overwhelming. To overcome this problem, a low-cost built-in self-repair scheme is proposed in this paper. The misalignment amount at each die-to-die boundary is first decided continually, and then used to repair the distorted image on-the-fly right before they are stored into the <b>image</b> <b>buffer.</b> Experiments on a large number of test images indicate that a repair rate of almost 100 % is achievable at only 2. 43 % area overhead. Index Terms—Misalignment, multidie, repair, space sensor. I...|$|E
40|$|Purple {{membranes}} adsorbed to mica were <b>imaged</b> in <b>buffer</b> solution {{using the}} atomic force microscope. The hexagonal diffraction patterns of topographs from the cytoplasmic and the extracellular surface showed {{a resolution of}} 0. 7 and 1. 2 nm, respectively. On the cytoplasmic surface, individual bacteriorhodopsin molecules consistently exhibited a distinct substructure. Depending on the pH value of the buffer solution, {{the height of the}} purple membranes decreased from 5. 6 nm (pH 10. 5) to 5. 1 nm (pH 4). The results are discussed with respect to the structure determined by cryo-electron microscopy...|$|R
40|$|In {{your final}} {{hardback}} copy, replace this page with the signed exercise sheet. In this Project a lightweight miniature camera system is created. This systems mainly {{consist of a}} Toshiba TCM 8240 MD CMOS camera, an STM 32 F 407 micro-controller and an 32 MBit SRAM. This system is able to record <b>images</b> and <b>buffer</b> them in the SRAM. Because of the powerful Microcontroller the system also have image processing capabilities. Therefore some image processing algorithms are de-veloped and implemented on the microcontroller. This report presents this system and the image processing algorithms in detail...|$|R
40|$|This thesis {{deals with}} {{distance}} transforms {{which are a}} fundamental issue in image processing and computer vision. In this thesis, two new distance transforms for gray level images are presented. As a new application for distance transforms, they are applied to gray level image compression. The new distance transforms are both new extensions of the well known distance transform algorithm developed by Rosenfeld, Pfaltz and Lay. With some modification their algorithm which calculates a distance transform on binary images with a chosen kernel {{has been made to}} calculate a chessboard like distance transform with integer numbers (DTOCS) and a real value distance transform (EDTOCS) on gray level images. Both distance transforms, the DTOCS and EDTOCS, require only two passes over the graylevel image and are extremely simple to implement. Only two <b>image</b> <b>buffers</b> are needed: The original gray level image and the binary image which defines the region(s) of calculation. No other <b>image</b> <b>buffers</b> are needed even if more than one iteration round is performed. For large neighborhoods and complicated images the two pass distance algorithm has to be applied to the image more than once, typically 3 10 times. Different types of kernels can be adopted. It is important to notice that no other existing transform calculates the same kind of distance map as the DTOCS. All the other gray weighted distance function, GRAYMAT etc. algorithms find the minimum path joining two points by the smallest sum of gray levels or weighting the distance values directly by the gray levels in some manner. The DTOCS does not weight them that way. The DTOCS gives a weighted version of the chessboard distance map. The weights are not constant, but gray value differences of the original image. The difference between the DTOCS map and other distance transforms for gray level images is shown. The difference between the DTOCS and EDTOCS is that the EDTOCS calculates these gray level differences in a different way. It propagates local Euclidean distances inside a kernel. Analytical derivations of some results concerning the DTOCS and the EDTOCS are presented. Commonly distance transforms are used for feature extraction in pattern recognition and learning. Their use in image compression is very rare. This thesis introduces a new application area for distance transforms. Three new image compression algorithms based on the DTOCS and one based on the EDTOCS are presented. Control points, i. e. points that are considered fundamental for the reconstruction of the image, are selected from the gray level image using the DTOCS and the EDTOCS. The first group of methods select the maximas of the distance image to new control points and the second group of methods compare the DTOCS distance to binary image chessboard distance. The effect of applying threshold masks of different sizes along the threshold boundaries is studied. The time complexity of the compression algorithms is analyzed both analytically and experimentally. It is shown that the time complexity of the algorithms is independent of the number of control points, i. e. the compression ratio. Also a new morphological image decompression scheme is presented, the 8 kernels' method. Several decompressed images are presented. The best results are obtained using the Delaunay triangulation. The obtained image quality equals that of the DCT images with a 4 x...|$|R
40|$|This {{dissertation}} {{explores the}} use of an analytic visibility algorithm for the high-speed generation of anti-aliased images with textures. Wben visibility is known analytically before any sampling takes place, low-pass filtering for anti-aliasing can be done to arbitrary accuracy at a cost proportionate to the output resolution. Furthermore, since the filtering and sampling processes can be expressed as a set of integrations over the image plane, the filtering process can be decomposed into a set of sums of integrations over each visible surface in the image plane, allowing the rendering of each visible surface to be done in parallel using an <b>image</b> <b>buffer</b> to accumulate the results. Thus, analytic visibility can {{serve as the basis for}} high-speed, high-quality image synthesis. In this dissertation, algorithms for computing analytic visibility and for producing filtered renderings from the resulting visible surfaces are presented. In order to provide real-time performance, these algorithms have bee...|$|E
40|$|This paper {{describes}} a system architecture that supports realtime generation of complex images, efficient generation of extremely high-quality images, and a smooth trade-off between the two. Based on {{the paradigm of}} integration, the architecture extends a state-of-the-art rendering system with an additional high-precision <b>image</b> <b>buffer.</b> This additional buffer, called the Accumulation Buffer, is used to integrate images that are rendered into the framebuffer. While originally conceived {{as a solution to}} the problem of aliasing, the Accumulation Buffer provides a general solution to the problems of motion blur and depth-of-field as well. Because the architecture is a direct extension of current workstation rendering technology, we begin by discussing the performance and quality characteristics of that technology. The problem of spatial aliasing is then discussed, and the Accumulation Buffer is shown to be a desirable solution. Finally the generality of the Accumulation Buffer is explored, concentrating on its application to the problems of motion blur, depth-of-field, and soft shadows...|$|E
40|$|Direct cell-to-cell volume {{visualization}} algorithms, {{also called}} projective rendering methods, present {{the advantage of}} allowing semi-transparencies and, additionally, as they project all the samples, they avoid the voxel-space aliasing. However, they are generally computationally expensive and artifacts may appear in the projection. In this paper, different projective strategies are reviewed and compared. A new algorithm, based on a back-to-front (BTF) voxel-to-voxel traversal, is proposed. It consists of the computation, in a preprocess, of 16 Extended Generalized Weight (EGW) matrices, based on the voxel's geometry and the viewpoint direction. Taking into advantage the voxel-to-voxel coherence, the EGW are applied incrementally to each voxel as footprints to be composited with the <b>image</b> <b>buffer,</b> according to the voxel shade and opacity values. It is shown that the method avoids artifacts, {{and that it has}} a low computational cost because it does not require re-voxellization and it does not perform any geometric computation in the projection process. It is specially suitable for voxel representations and image space resolutions such that the projection of a single voxel covers more than one pixel. Postprint (published version...|$|E
40|$|Presented is a {{progress}} report for the Regional Planetary Image Facilities (RPIF) prototype image data management and reduction system being jointly implemented by Washington University and the USGS, Flagstaff. The system will consist of a MicroVAX with a high capacity (approx 300 megabyte) disk drive, a compact disk player, an <b>image</b> display <b>buffer,</b> a videodisk player, USGS image processing software, and SYSTEM 1032 - a commercial relational database management package. The USGS, Flagstaff, will transfer their image processing software including radiometric and geometric calibration routines, to the MicroVAX environment. Washington University will have primary responsibility for developing the database management aspects {{of the system and}} for integrating the various aspects into a working system...|$|R
40|$|Quartz tuning forks are {{extremely}} good resonators {{and their use}} is growing in scanning probe microscopy. Nevertheless, only a few studies on soft biological samples have been reported using these probes. In this work, we present the methodology to develop and use these nanosensors to properly work with biological samples. The working principles, fabrication and experimental setup are presented. The results in the nanocharacterization of different samples in different ambients are presented by using different working modes: amplitude modulation with and {{without the use of}} a Phase-Locked Loop (PLL) and frequency modulation. Pseudomonas aeruginosa bacteria are imaged in nitrogen using amplitude modulation. Microcontact printed antibodies are <b>imaged</b> in <b>buffer</b> using amplitude modulation with a PLL. Finally, metastatic cells are imaged in air using frequency modulation...|$|R
40|$|RNA {{polymerase}} (RNAP) molecules to be <b>imaged</b> under aqueous <b>buffer</b> using tapping-mode {{atomic force microscopy}} (AFM). Recombinant RNAP {{molecules containing}} histidine tags (hisRNAP) on the C-terminus were specifically immobilized on ultraflat gold via a mixed monolayer of two different Ω-functionalized alkanethiols. One alkanethiol was terminated in an ethylene-glycol (EG) group, which resists protein adsorption, {{and the other was}} terminated in an N-nitrilotriacetic acid (NTA) group, which binds the histidine tag through two coordination sites with a nickel ion. AFM images showed that these two alkanethiols phase-segregate. Specific binding of the hisRNAP molecules was followed in situ by injecting proteins directly into the AFM fluid cell. The activity of the hisRNAP bound to the NTA groups was confirmed with a 42 -base circular single-stranded DNA template (rolling circle), which the RNAP uses to produce huge RNA transcripts. These transcripts were imaged in air after the samples were rinsed and dried, since RNA also has low affinity for the EG-thiol and cannot be <b>imaged</b> under the <b>buffers</b> we used. Combining a system for binding proteins to surfaces (Sigal, G. B., C. Bamdad, A. Barberis, J. Strominger, and G. M. Whitesides. 1996. Anal. Chem. 68 : 490 - 497) with a method for making ultraflat gold surfaces (Hegner, M., P. Wagner, and G. Semenza. 1993. Surface Sci. 291 : 39 - 46 1993) has enabled single, oriented, active Escherichia coli RNA polymerase (RNAP) molecules to be <b>imaged</b> under aqueous <b>buffer</b> using tapping-mode atomic force microscopy (AFM). Recombinant RNAP molecules containing histidine tags (hisRNAP) on the C-terminus were specifically immobilized on ultraflat gold via a mixed monolayer of two different Ω-functionalized alkanethiols. One alkanethiol was terminated in an ethylene-glycol (EG) group, which resists protein adsorption, and the other was terminated in an N-nitrilotriacetic acid (NTA) group, which binds the histidine tag through two coordination sites with a nickel ion. AFM images showed that these two alkanethiols phase-segregate. Specific binding of the hisRNAP molecules was followed in situ by injecting proteins directly into the AFM fluid cell. The activity of the hisRNAP bound to the NTA groups was confirmed with a 42 -base circular single-stranded DNA template (rolling circle), which the RNAP uses to produce huge RNA transcripts. These transcripts were imaged in air after the samples were rinsed and dried, since RNA also has low affinity for the EG-thiol and cannot be <b>imaged</b> under the <b>buffers</b> we used. Godkänd; 1999; 20070410 (ysko...|$|R
