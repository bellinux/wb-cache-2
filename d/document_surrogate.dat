12|45|Public
50|$|Little Man (stylized {{as little}} man) is a 2005 American {{documentary}} film by Nicole Conn. The film {{was intended to}} <b>document</b> <b>surrogate</b> pregnancy but the baby was delivered 100 days early so the film documents experiences of a family dealing with an extremely premature birth involving 158 days in a NICU (neonatal intensive care unit).|$|E
40|$|Abstract. Documents are {{unstructured}} data consisting of natural language. <b>Document</b> <b>surrogate</b> means the structured data converted from original documents to process them in computer systems. <b>Document</b> <b>surrogate</b> is usually represented into {{a list of}} words. Because not all words in a document reflect its content, {{it is necessary to}} select imp ortant words related with its content among them. Such important words are called keywords and they are selected with a particular equation based on TF (Term Frequency) and IDF (inverted Document Frequency). Actually, not only TF and IDF but also the position of each word in the document and the inclusion of the word in the title should be considered to select keywords among words contained in the text. The equation based on these factors gets too complicate to be applied to the selection of keywords. This paper proposes the neural network model, back propagation, in which these factors are used as the features and feature vectors are generated, and with which keywords are selected. This paper will show that backpropagation outperforms the equation in distinguishing keywords. ...|$|E
40|$|We {{present the}} design of a {{visualization}} tool that graphically displays the strength of query concepts in the retrieved documents. Graphically displaying <b>document</b> <b>surrogate</b> information enables set-at-a-time perusal of documents, rather than document-at-a-time perusal of textual displays. By prvialing additional relevance information about the retrieved documents, the tool aids the user in accurately identifying relevant documents. Results of an experiment evaluating the tool shows that when users have the tool they are able to identify relevant documents in a shorter period of time than without the tool, and with increased accuracy. We have evidence to believe that appropriately designed graphical displays can enable users to better interact with the system. ...|$|E
40|$|We {{introduce}} {{the notion of}} time-centered snippets, called TSnippet, as <b>document</b> <b>surrogates</b> for <b>document</b> retrieval and exploration. We propose an alternative document snippet based on temporal {{information that can be}} useful for supporting exploratory search. The idea of using sentences that contain the most frequent chronons (units of time) can be used for constructing <b>document</b> <b>surrogates.</b> We conducted a series of experiments to evaluate this new approach using a crowdsourcing approach. The evaluation against two Web search engines shows that our technique produces good snippets and users like to see time-sensitive information in search results...|$|R
50|$|Depending on the {{application}} the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by <b>document</b> <b>surrogates</b> or metadata.|$|R
5000|$|There {{are several}} {{scientific}} papers {{that use the}} term to describe the browsing of [...] "graphical representations" [...] of documents. In this context [...] "metabrowsing" [...] refers to a high-level way of browsing through information: instead of browsing through document contents or <b>document</b> <b>surrogates,</b> the user browses through a graphical representation of the documents and their relations to the domain.|$|R
40|$|Abstract. Users of web search systems {{often have}} {{difficulty}} determining {{the relevance of}} search results to their information needs. Clustering has been suggested as a method for making this task easier. However, this introduces new challenges such as naming the clusters, selecting multiple clusters, and re-sorting the search results based on the cluster information. To address these challenges, we have developed Concept Highlighter, a tool for visually exploring concept-based fuzzy clusters in web search results. This tool automatically generates a set of concepts related to the users ’ queries, and performs single-pass fuzzy c-means clustering on the search results using these concepts as the cluster centroids. A visual interface is provided for interactively exploring the search results. In this paper, we describe the features of Concept Highlighter and its use in finding relevant documents within the search results through concept selection and <b>document</b> <b>surrogate</b> highlighting. ...|$|E
40|$|With {{the rapid}} {{expansion}} of scientific research, the ability to effectively find or integrate new domain knowledge in the sciences is proving increasingly difficult. The development of methods and tools for assisting researchers to effectively extract problem-oriented knowledge from heterogeneous and massive information sources, and for using this knowledge in problem-solving {{is one of the most}} fundamental research directions for the information and computer sciences today. There is a need for new tools to support more precise identification of relevant research articles and provide visual clues regarding relationships among the document sets. We present the Telemakus system in which aggregated citation information and extracted research findings are displayed in a schema-based <b>document</b> <b>surrogate</b> and an interactive mapping tool provides graphical displays of research interrelationships from documents across a domain. This system is an innovative approach to creating useful and precise document surrogates and may re-conceptualize the way we currently represent, retrieve, and assimilate research findings from the published literature. Keywords...|$|E
40|$|Introduction We report {{results for}} {{experiments}} conducted in Melbourne [...] -at CSIRO, RMIT, and The University of Melbourne [...] -for TREC- 9. We present {{results for the}} interactive track, cross-lingual track, main web track, and the query track. 2 Interactive Track 2. 1 Introduction We have been continuously investigating technologies for delivering retrieved documents to support interactive question answering. In this year's interactive track, we focused {{on the role of}} a <b>document</b> <b>surrogate</b> in the interactive fact finding task. In this experiment, we compared two types of document surrogates Submitted to the 9 th Text Retrieval Conference (TREC- 9), Gaithersburg, MD, USA, November 13 [...] 16, 2000. in the two experimental systems. One system uses the document title and the first 20 words of a document as the document's surrogate, while the other system uses the document title and the best three Answer Indicative Sentences extracted from the document as the document's surrogate. The results sho...|$|E
40|$|It {{is common}} for web searchers to have {{difficulties}} crafting queries to fulfill their information needs. Even when they provide a good query, users often find it challenging to evaluate {{the results of their}} web searches. Sources of these problems include the lack of support for query refinement, and the static nature of the list-based representations of web search results. To address these issues, we have developed WordBars, an interactive tool for web information retrieval. WordBars visually represents the frequencies of the terms found in the first 100 <b>document</b> <b>surrogates</b> returned from the initial query. This system allows the users to interactively re-sort the search results based on the frequencies of the selected terms within the <b>document</b> <b>surrogates,</b> as well as to add and remove terms from the query, generating a new set of search results. Examples illustrate how WordBars can provide valuable support for query refinement and search results exploration, both when specific and vague initial queries are provided...|$|R
25|$|An {{object is}} an entity that is {{represented}} by information in a database. User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by <b>document</b> <b>surrogates</b> or metadata.|$|R
40|$|This is {{the report}} of a project {{supported}} by the Council on Library Resources. The four project objectives are: 1. To identify and retrieve published literature on the library of the future. 2. To formulate <b>document</b> <b>surrogates</b> for this literature and add them to a computerized database. 3. To generate an analytical bibliography of published library of the future literature. And 4. To synthesize literature in the bibliography with a thinkpiece on the library of the future...|$|R
40|$|This paper {{reports on}} a four-part {{qualitative}} research project aimed at designing an online <b>document</b> <b>surrogate</b> tailored {{to the needs of}} physicians seeking biomedical literature for use in clinical problem solving. The clinical extract, designed in collaboration with three practicing physicians, combines traditional elements of the MEDLINE record (e. g., title, author, source, abstract) with new elements (e. g., table captions, text headings, case profiles) suggested by the physicians. Specifications for the prototype clinical extract were developed through a series of relevance-scoring exercises and semi-structured interviews. For six clinical questions, three physicians assessed the applicability of selected articles and their document surrogates, articulating relevance criteria and reasons for their judgments. A prototype clinical extract based on their suggestions was developed, tested, evaluated, and revised. The final version includes content and format aids to make the extract easy to use. The goals, methods, and outcomes of the research study are summarized, and a template of the final design is provided...|$|E
40|$|Automatic {{document}} summarization is {{a problem}} of creating a <b>document</b> <b>surrogate</b> that adequately represents the full document content. We aim at a summarization system that can replicate the quality of summaries created by humans. In this paper we investigate the machine learning method for extracting full sentences from documents based on the document semantic graph structure. In particular, we explore how the Support Vector Machines (SVM) learning method is affected by the quality of linguistic analyses and the corresponding semantic graph representations. We apply two types of linguistic analysis: (1) a simple part-of-speech tagging of noun phrases and verbs and (2) full logical form analysis which identifies Subject-Predicate-Object triples, and then build the semantic graphs. We train the SVM classifier to identify summary nodes and use these nodes to extract sentences. Experiments with the DUC 2002 and CAST datasets show that the SVM based extraction of sentences does not differ significantly for the simple and the sophisticated syntactic analysis. In both cases the graph attributes used in learning are essential for the classifier performance and the quality of extracted summaries...|$|E
40|$|Presentation {{of search}} results in Web-based {{information}} retrieval (IR) systems {{has been dominated}} by a textual form of information such as the title, snippet, URL, and/or file type of retrieved documents. On the other hand, document’s visual aspects such as the layout, colour scheme, or presence of images have been studied in a limited context with regard to their effectiveness of search result presentation. This paper presents a comparative evaluation of textual and visual forms of document summaries as the additional <b>document</b> <b>surrogate</b> in the search result presentation. In our study, a sentence-based summarisation technique was used to create a textual document summary, and the thumbnail image of web pages was used to represent a visual summary. The experimental results suggest that both have the cases where the additional elements contributed to a positive effect not only in users’ relevance assessment but also in query re/formulation. The results also suggest that the two forms of document summary are likely to have different contexts to facilitate user’s search experience. Therefore, our study calls for further research on adaptive models of IR systems to make use of their advantages in appropriate contexts...|$|E
40|$|Perusal {{of textual}} {{displays}} of <b>document</b> <b>surrogates</b> produced by Web-based ranked-output retrieval services may require much user time, effort, and money. In this paper we present VIEWER, a graphical interface that allows visualization and manipulation of views of retrieval results, where a view is the subset of retrieved surrogates that contain a specified subset of query terms. We argue that VIEWER helps the user focus on relevant {{parts of the}} results and, in addition, it may facilitate query reformulation. We present an experimental evaluation in which VIEWER, used as an interactive ranking systems, outperforms both best match ranking and coordination level-based ranking...|$|R
40|$|Abstract: Perusal {{of textual}} {{displays}} of <b>document</b> <b>surrogates</b> produced by Web-based rankedoutput retrieval services may require much user time, effort, and money. In this paper we present VIEWER, a graphical interface that allows visualization and manipulation of views of retrieval results, where a view is the subset of retrieved surrogates that contain a specified subset of query terms. We argue that VIEWER helps the user focus on relevant {{parts of the}} results and, in addition, it may facilitate query reformulation. We present {{the results of an}} experiment performed by six subjects on two medium size bibliographical test collections in which VIEWER, used as an interactive ranking systems, outperformed both best match ranking and coordination level-based ranking...|$|R
40|$|This paper {{examines}} the problems involved in subject retrieval from full-text databases of secondary {{materials in the}} humanities. Ten such databases were studied and their search functionality evaluated, focusing on factors such as Boolean operators, <b>document</b> <b>surrogates,</b> limiting by subject area, proximity operators, phrase searching, wildcards, weighting of search terms, limiting by type of document, controlled vocabulary indexing and ranking, and display of search results. The author suggests ways in which full-text searching might be improved, whether by enhancement of database records, by introduction of enhanced search functionality, or by the education of searchers in more effective search techniques. The conclusion is that current digitisation projects are not producing databases that {{meet the needs of}} scholars...|$|R
40|$|A {{surrogate}} is {{an object}} that stands for a document and enables navigation to that document. Hypermedia is often represented with textual surrogates, even though {{studies have shown that}} image and text surrogates facilitate the formation of mental models and overall understanding. Surrogates may be formed by breaking a document down into a set of smaller elements, each of which is a surrogate candidate. While processing these surrogate candidates from an HTML document, relevant information may appear together with less useful junk material, such as navigation bars and advertisements. This paper develops a pattern recognition based approach for eliminating junk while building the set of surrogate candidates. The approach defines features on candidate elements, and uses classification algorithms to make selection decisions based on these features. For the purpose of defining features in surrogate candidates, we introduce the <b>Document</b> <b>Surrogate</b> Model (DSM), a streamlined Document Object Model (DOM) -like representation of semantic structure. Using a quadratic classifier, we were able to eliminate junk surrogate candidates with an average classification rate of 80 %. By using this technique, semiautonomous agents can be developed to more effectively generate surrogate collections for users. We end by describing a new approach for hypermedia and the semantic web, which uses the DSM to define value-added surrogates for a document...|$|E
40|$|Background: With {{the rapid}} {{expansion}} of scientific research, the ability to effectively find or integrate new domain knowledge in the sciences is proving increasingly difficult. Efforts to improve and speed up scientific discovery are being explored on a number of fronts. However, much of this work is based on traditional search and retrieval approaches and the bibliographic citation presentation format remains unchanged. Methods: Case study. Results: The Telemakus KnowledgeBase System provides flexible new tools for creating knowledgebases to facilitate retrieval and review of scientific research reports. In formalizing the representation of the research methods and results of scientific reports, Telemakus offers a potential strategy to enhance the scientific discovery process. While other research has demonstrated that aggregating and analyzing research findings across domains augments knowledge discovery, the Telemakus system is unique in combining document surrogates with interactive concept maps of linked relationships across groups of research reports. Conclusion: Based on how scientists conduct research and read the literature, the Telemakus KnowledgeBase System brings together three innovations in analyzing, displaying and summarizing research reports across a domain: (1) research report schema, a <b>document</b> <b>surrogate</b> of extracted research methods and findings presented in a consistent and structured schema format which mimics the research process itself and provides a high-level surrogate to facilitate searching and rapid review of retrieved documents; (2) research findings, used to index the documents, allowing searchers to request, for example, research studies which have studied the relationship between neoplasms and vitamin E; and (3) visual exploration interface of linked relationships for interactive querying of research findings across the knowledgebase and graphical displays of what is known as well as, through gaps in the map, what is yet to be tested. The rationale and system architecture are described and plans for the future are discussed. We gratefully acknowledge funding provided by the Ellison Medical Foundation and the assistance provided by its Executive Director, Dr. Richard Sprott...|$|E
40|$|The organisation, {{content and}} {{presentation}} of <b>document</b> <b>surrogates</b> has a substantial {{impact on the}} effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks...|$|R
40|$|Suggestions {{and ideas}} for {{acquiring}} <b>documents</b> or their <b>surrogates</b> for a planned or fledgling information system are offered. The problems of selectivity of <b>documents</b> or their <b>surrogates,</b> both in superabundant quantities, and duplicate checking are highlighted. Acquisitioning flow, a semiautomated duplicate search technique, and alerting methods for prospective documentation are described. Appendices include two category systems, selected definitions and acronyms, and a selected address list for document procurement...|$|R
40|$|Collection sizes, query rates, and {{the number}} of users of Web search engines are increasing. Therefore, there is {{continued}} demand for innovation in providing search services that meet user information needs. In this article, we propose new techniques to add additional terms to documents with the goal of providing more accurate searches. Our techniques are based on query association, where queries are stored with documents that are highly similar statistically. We show that adding query associations to documents improves the accuracy of Web topic finding searches by up to 7 %, and provides an excellent complement to existing supplement techniques for site finding. We conclude that using <b>document</b> <b>surrogates</b> derived from query association is a valuable new technique for accurate Web searching...|$|R
40|$|As {{outlined}} in the Association of College and Research Libraries (ACRL) report Value of Academic Libraries: A Comprehensive Research Review and Report([URL] Helmke Library is <b>documenting</b> <b>surrogate</b> measures to demonstrate its value to students, faculty and staff on an academic campus and to the community in general. We are documenting and leveraging opportunities and partnerships to enhance awareness and use of library resources and services and most importantly, enhance and publicize the varied skills and contributions of the library’s staff and librarians. Using a simple spreadsheet, we are documenting these outreach and collaborative activities as well as librarians’ involvement in campus committees and projects in a Library Collaboration Index. This document, which lists the Activity, Audience, Contact Hours and other information, describes and quantifies the library’s impact, value and integration with campus and community activities...|$|R
40|$|The {{presentation}} of search {{results on the}} web {{has been dominated by}} the textual form of document representation. On the other hand, the document’s visual aspects such as the layout, colour scheme, or presence of images have been studied in a limited context with regard to their effectiveness of search result presentation. This article presents a comparative evaluation of textual and visual forms of document representation as additional components of <b>document</b> <b>surrogates.</b> A total of 24 people were recruited for our task-based user study. The experimental results suggest that an increased level of document representation available in the search results can facilitate users’ interaction with a search interface. The results also suggest that the two forms of additional representations are likely beneficial to users’ information searching process in different contexts...|$|R
40|$|A {{large amount}} of {{computing}} literature has become available over the Internet, as university departments and research institutions have made their technical reports, preprints, and theses available electronically. Access to these items has been limited, however, by the difficulties involved in locating documents of interest. We describe {{a proposal for a}} New Zealand-based index of computer science technical reports, where the reports themselves are located in repositories that are distributed world-wide. Our scheme is unique in that it is based on indexing the full text of the technical reports, rather than on <b>document</b> <b>surrogates.</b> The index is constructed so as to minimize network traffic and local storage costs (of particular importance for geographically isolated countries like New Zealand, which incur high Internet costs). We also will provide support for bibliometric/scientometric studies of the computing literature and our users. 1. Introduction The migration of information fro [...] ...|$|R
40|$|In most {{information}} retrieval systems, {{especially those in}} the Web, users suffer from too many search results force them to go through a long list of <b>document</b> <b>surrogates</b> that may not reveal their contents sufficiently. This paper describes our attempt to provide a solution to this problem: a new visual interface for search results and a system architecture that supports the visualization technique. The visual interface is evaluated for its efficacy with human subjects. Our interface, DART, displays document clustering information and the similarity values between individual documents and different sub-queries. With the distinguishing colors and brightness of the icons representing documents scattered on a series of concentric circles resembling a dart target, users can make intuitive judgment as to which one they want to read next. Based on our experiments, users acquire relevant documents more quickly with the DART display than with the conventional document list. 1. Introduction I [...] ...|$|R
40|$|Context {{influences}} the search process, but to date research has not definitively identified which aspects of context {{are the most}} influential for information retrieval, and thus are worthy of integration in today’s retrieval systems. In this research, we isolated for examination two aspects of context: task and document genre and {{examined the relationship between}} them within a software engineering work domain. In this domain, the nature of the task has an impact on decisions of relevance and usefulness, and the document collection contains a distinctive set of genre. Our data set was a document repository created and used by our target population. The <b>document</b> <b>surrogates</b> were meta-tagged by purpose and document type. Correspondence analysis of this categorical data identified some specific relationships between genres and tasks, as well as four broad dimensions of variability underlying these relationships. These results have the potential to inform the design of a contextual retrieval system by refining search results for this domain...|$|R
40|$|Keywords and keyphrases {{have many}} useful roles as <b>document</b> <b>surrogates</b> and descriptors, but the manual {{production}} of keyphrase metadata for large digital library collections {{is at best}} expensive and time-consuming, and at worst logistically impossible. Algorithms for keyphrase extraction like Kea and Extractor produce a set of phrases {{that are associated with}} a document. Though these sets are often utilized as a group, keyphrase extraction is usually evaluated by measuring the quality of individual keyphrases. This paper reports an assessment that asks human assessors to rate entire sets of keyphrases produced by Kea, Extractor and document authors. The results provide further evidence that human assessors rate all three sources highly (with some caveats), but show that the relationship between the quality of the phrases in a set and the set as a whole is not always simple. Choosing the best individual phrases will not necessarily produce the best set; combinations of lesser phrases may result in better overall quality...|$|R
40|$|This paper {{presents}} {{new approaches}} to headline generation for English newspaper texts, {{with an eye toward}} the production of <b>document</b> <b>surrogates</b> for <b>document</b> selection in cross-language information retrieval. This task is difficult because the user must make decisions about relevance based on (often poor) translations of retrieved documents. To facilitate the decision-making process we need translations that can be assessed rapidly and accurately; our approach is to provide an English headline for the non-English document. We describe two approaches to headline generation and their application to the recent DARPA TIDES- 2003 Surprise Language Exercise for Hindi. For comparison, we also implemented an alternative method for surrogate generation: a system that produces topic lists for (Hindi) articles. We present the results of a series of experiments comparing each of these approaches. We demonstrate in both automatic and human evaluations that our linguistically motivated approach outperforms two other surrogate-generation methods: a statistical system and a topic discovery system...|$|R
40|$|This paper investigates user {{interpretation}} of search result displays on small screen devices. Such devices present interesting design challenges given their limited display capabilities, particularly {{in relation to}} screen size. Our aim is to provide users with succinct yet useful representations of search results that allow rapid and accurate decisions to be made about the utility of result documents, yet minimize user actions (such as scrolling), the use of device resources, and the volume of data to be downloaded. Our hypothesis is that keyphrases that are automatically extracted from documents can support this aim. We report on a user study that compared how accurately users categorized result documents on small screens when the <b>document</b> <b>surrogates</b> consisted of either keyphrases only, or document titles. We found no significant performance {{differences between the two}} conditions. In addition to these encouraging results, keyphrases have the benefit that they can be extracted and presented when no other document metadata can be identified...|$|R
40|$|This article investigates a new, {{effective}} browsing approach called metabrowsing. It is {{an alternative}} for current information retrieval systems, which still face six prominent difficulties. We identify and classify the difficulties and show that the metabrowsing approach alleviates the difficulties associated with query formulation and missing domain knowledge. Metabrowsing is a high-level way of browsing through information: instead of browsing through document contents or <b>document</b> <b>surrogates,</b> the user browses through a graphical representation of the documents and their relations to the domain. The approach requires other cognitive skills from the user than what is currently required. Yet, a user evaluation in which the metabrowsing system was compared with an ordinary query-oriented system showed only some small indicatory differences in effectiveness, efficiency, and user satisfaction. We expect that more experience with metabrowsing {{will result in a}} significantly better performance difference. Hence, our conclusion is that the development of new cognitive skills requires some time before the technologies are ready to be used...|$|R
40|$|Searching {{within the}} context of {{information}} retrieval may be viewed as a communication process between the users and the indexers (or the authors). It is known that in expressing the same concept or idea, different people tend to use different words or phrases, and also that the meaning of words attached to <b>document</b> <b>surrogates</b> tends to change over time. To overcome these phenomena, various learning schemes have been designed so as to automatically infer knowledge about document content from the relevance assessments of past queries. Thus, in contrast to most retrieval models that represent the semantic content of documents as static entities, these adaptive search models might change the descriptions of documents through an inductive learning scheme. The evaluation of such dynamic document space strategies may be based on retrospective tests within which the same set of queries is applied to train and test the system. Based on cross-validation principles, this paper suggests a more "ho [...] ...|$|R
40|$|Web searchers {{typically}} fail to view {{search results}} beyond {{the first page}} nor fully examine those results presented to them. In this article we describe an approach that encourages a deeper examination {{of the contents of}} the document set retrieved in response to a searcher’s query. The approach shifts the focus of perusal and interaction away from potentially uninformative <b>document</b> <b>surrogates</b> (such as titles, sentence fragments and URLs) to actual document content, and uses this content to drive the information seeking process. Current search interfaces assume searchers examine results document-by-document. In contrast our approach extracts, ranks and presents the contents of the top-ranked document set. We use query-relevant top-ranking sentences extracted from the top documents at retrieval time as fine-grained representations of top-ranked document content and, when combined in a ranked list, an overview of these documents. The interaction of the searcher provides implicit evidence that is used to reorder the sentences where appropriate. We evaluate our approach in three separate user studies, each applying these sentences in a different way. The findings of these studies show that top-ranking sentences can facilitate effective information access...|$|R
40|$|People {{engaged in}} {{knowledge}} work must often rapidly identify valuable material from within large sets of potentially relevant documents. Document triage {{is a type}} of sensemaking task that involves skimming documents {{to get a sense of}} their content, evaluating documents to assess their worth in the context of the current activity, and organizing documents to prepare for their subsequent use and more in-depth reading. We have performed a study of document triage by collecting multiple forms of qualitative and quantitative data to characterize how 24 subjects read about a new topic and assessed and organized a set of 40 relevant Web documents. Our results indicate that there are multiple strategies for document triage, each involving different styles of reading, interacting, and organizing. Common strategies include: 1) focused reading early in the task, relegating the organizing until later in the process; 2) skimming performed in tandem with organizing, which relies on gaining an incremental understanding of the topic; and 3) metadata-based organizing, a strategy that stresses working with <b>document</b> <b>surrogates</b> to minimize the time spent reading. The findings suggest ways applications may better support the intertwined nature of the browsing, reading, and organizing activities in document triage...|$|R
40|$|Statistical pattern {{recognition}} techniques, supervised and unsupervised classification techniques being two good examples here, {{rely on the}} computations of similarity and distance metrics. The distances are computed in a multi-dimensional space. The axes of this space in principle relate to the features inherent in the input data. Usually such features are chosen by neural network developers, thereby introducing a possible bias. A method of automatically generating feature sets is discussed, with specific reference to the categorisation of streams of free-text news items. The feature sets were generated by a procedure that automatically selects a group of keywords based on a lexico-semantic analysis. Three different types of text streams [...] headlines only, news summaries and full news items including {{the body of the}} text [...] have been categorised using self-organising feature maps (SOFM) [2]. A method for assessing the discrimination ability of a SOFM, based on Fisher's Linear Discriminant Rule suggests that an SOFM trained on vectors related to summaries only provides a fairly accurate cluster when compared with vectors related to full text. The use of summaries as <b>document</b> <b>surrogates</b> for <b>document</b> categorisation is suggested...|$|R
