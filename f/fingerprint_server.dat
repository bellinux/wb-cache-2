1|16|Public
40|$|Positioning and {{tracking}} technologies can detect {{the location and}} the movement of mobile nodes (MNs), such as cellular phone, vehicular and mobile sensor, to predict potential handoffs. However, most motion detection mechanisms require additional hardware (e. g., GPS and directed antenna), costs (e. g., power consumption and monetary cost) and supply systems (e. g., network <b>fingerprint</b> <b>server).</b> This paper proposes a Momentum of Received Signal Strength (MRSS) based motion detection method and its application on handoff. MRSS uses the exponentially weighted moving average filter with multiple moving average window size to analyze the received radio signal. With MRSS, an MN can predict its motion state and make a handoff trigger at the right time without any assistance from positioning systems. Moreover, a novel motion state dependent MRSS scheme called Dynamic MRSS (DMRSS) algorithm is proposed to adjust the motion detection sensitivity. In our simulation, the MRSS- and DMRSS-based handoff algorithms can reduce the number of unnecessary handoffs up to 44 % and save battery power up to 75 %...|$|E
40|$|Abstract In this paper, {{we present}} YuruBackup, a space-efficient and highly scal-able {{incremental}} backup {{system in the}} cloud. YuruBackup enables fine-grained data de-duplication with hierarchical partitioning to improve space efficiency to reduce bandwidth of both backup and restore processes, and storage costs. On the other hand, YuruBackup explores a highly scalable architecture for <b>fingerprint</b> <b>servers</b> that allows to add one or more <b>fingerprint</b> <b>servers</b> dynamically to cope with increasing clients. In this architecture, the <b>fingerprint</b> <b>servers</b> in a DB cluster are used for scaling writes of fingerprint catalog, while the slaves are used for scaling reads of finger-print catalog. We present the system architecture of YuruBackup and its components, and we have implemented a proof-of-concept prototype of YuruBackup. By conduct-ing performance evaluation in a public cloud, experimental results demonstrate {{the efficiency of the}} system...|$|R
40|$|Doctor of PhilosophyDepartment of Computing ScienceDavid GustafsonThe web is {{no longer}} {{composed}} of static resources. Technology and demand have driven the web towards a complex, dynamic model that tailors content toward specific client <b>fingerprints.</b> <b>Servers</b> now commonly modify responses based on the browser, operating system, or location of the connecting client. While this information {{may be used for}} legitimate purposes, malicious adversaries can also use this information to deliver misinformation or tailored exploits. Currently, there are no tools that allow a user to detect when a response contains tailored content. Developing an easily configurable multiplexing system solved the problem of detecting tailored web content. In this solution, a custom proxy receives the initial request from a client, duplicating and modifying it in many ways to change the browser, operating system, and location-based client fingerprint. All of the requests with various client fingerprints are simultaneously sent to the server. As the responses are received back at the proxy, they are aggregated and analyzed against the original response. The results of the analysis are then sent to the user along with the original response. This process allowed the proxy to detect tailored content that was previously undetectable through casual browsing. Theoretical and empirical analysis was performed to ensure the multiplexing proxy detected tailored content at an acceptable false alarm rate. Additionally, the tool was analyzed for its ability to provide utility to open source analysts, cyber analysts, and reverse engineers. The results showed that the proxy is an essential, scalable tool that provides capabilities that were not previously available...|$|R
40|$|Google hacking {{is a term}} to {{describe}} the search queries that find out security and privacy flaws. Finding vulnerable servers and web applications, <b>server</b> <b>fingerprinting,</b> accessing to admin and user login pages and revealing username-passwords are all possible in Google with a single click. Google can also reveal secrets of cryptography applications, i. e., clear text and hashed passwords, secret and private keys, encrypted messages, signed messages etc. In this paper, advanced search techniques in Google and the search queries that reveal cryptographic secrets are explained with examples in details. ...|$|R
40|$|Fingerprinting of {{services}} and operating systems {{is an essential part}} of penetration tests. In order to successfully penetrate the computing system's security measurements, preexisting fingerprinting methods are described and the paradigm of fingerprinting with mutation-based fuzzing is established. A case study about operating system and FTP <b>server</b> <b>fingerprinting</b> is presented whereby the feasibility of the approach is demonstrated. The research results show that the developed tools can be used for even more precise fingerprinting than the preexisting tools. Comment: 75 pages, Diplom thesis / Diplomarbeit. September 2008, German language conten...|$|R
40|$|Abstract—Hidden {{services}} are anonymously hosted services {{that can be}} accessed over Tor, an anonymity network. In this paper we present an attack that allows an entity to prove, once a machine suspect to host a hidden server has been confiscated, that such machine has in fact hosted a particular content. Our solution is based on leaving a timing channel fingerprint in the confiscated machine’s log file. In {{order to be able}} to <b>fingerprint</b> the log <b>server</b> through Tor we first study the noise sources: the delay introduced by Tor and the log entries due to other users. We then describe our fingerprint method, and analytically determine the detection probability and the rate of false positives. Finally, we empirically validate our results. I...|$|R
40|$|The Virtual Hybridization {{approach}} {{predicts the}} most probable hybridization sites across a target nucleic acid of known sequence, including both perfect and mismatched pairings. Potential hybridization sites, having a user-defined {{minimum number of}} bases that are paired with the oligonucleotide probe, are first identified. Then free energy values are evaluated for each potential hybridization site, and {{if it has a}} calculated free energy of equal or higher negative value than a user-defined free energy cut-off value, it is considered as a site of high probability of hybridization. The Universal <b>Fingerprinting</b> Chip Applications <b>Server</b> contains the software for visualizing predicted hybridization patterns, which yields a simulated hybridization fingerprint that can be compared with experimentally derived fingerprints or with a virtual fingerprint arising from a different sample...|$|R
5000|$|The primary {{motivation}} {{behind the}} file {{system is to}} address the shortcomings of , administratively configured distributed file systems in larger organizations, and various remote file transfer protocols. It is designed to operate securely between separate administrative realms. For example, with SFS, one could store all their files on a single remote server, and access the same files securely and transparently from any location {{as if they were}} stored locally, without any special privileges or administrative cooperation (other than running the SFS client daemon). Available file systems will be found at the same path regardless of physical location, and are implicitly authenticated by their path names [...] - [...] as they include the public-key <b>fingerprint</b> of the <b>server</b> (hence why it is called [...] "self-certifying").|$|R
40|$|In this work, {{we present}} a {{multiple}} <b>server</b> <b>fingerprint</b> verification scheme that provides enhanced template secu-rity by eliminating several known vulnerabilities of the fuzzy vault scheme. We secure templates from adversarial attacks in honest-but-curious server scenarios by utilizing commu-tative encryption in which the raw fingerprint template is never used in matching or storage. In this system, there is a matching server that performs the enrollment and matching functions on fingerprint data that has been encrypted by a separate encryption server. Since the encrypted template is stored at one server and the encryption key is on another server, an attacker would have to compromise both servers to decrypt the data. Even in this case, the templates are protected by the fuzzy vault scheme. Thus, this scheme limits an attacker’s ability to attack active users even after compromising both servers providing multiple layers of template security. 1...|$|R
40|$|With {{the rapid}} growth in the {{development}} of smart devices equipped with biometric sensors, client identification system using biometric traits are widely adopted across various applications. Among many biometric traits, fingerprint-based identification systems have been extensively studied and deployed. However, to adopt biometric identification systems in practical applications, two main obstacles in terms of efficiency and client privacy must be resolved simultaneously. That is, identification should be performed at an acceptable time, and only a client should have access to his/her biometric traits, which are not revocable if leaked. Until now, multiple studies have demonstrated successful protection of client biometric data; however, such systems lack efficiency that leads to excessive time utilization for identification. The most recently researched scheme shows efficiency improvements but reveals client biometric traits to other entities such as biometric database server. This violates client privacy. In this paper, we propose an efficient and privacy-preserving fingerprint identification scheme by using cloud systems. The proposed scheme extensively exploits the computation power of a cloud so that most of the laborious computations are performed by the cloud service provider. According to our experimental results on an Amazon EC 2 cloud, the proposed scheme is faster than the existing schemes and guarantees client privacy by exploiting symmetric homomorphic encryption. Our security analysis shows that during identification, the client fingerprint data is not disclosed to the cloud service provider or <b>fingerprint</b> database <b>server...</b>|$|R
40|$|AbstractThis paper {{proposes a}} novel feedback-based control {{technique}} that tackles distributed {{denial of service}} (DDoS) attacks in four consecutive phases. While protection routers close to the server control inbound traffc rate andkeeps the server alive (phase 1), the server negotiate with upstream routers close to traffc sources to install leaky-buckets for its IP address. The negotiation continues until a defense router on each traffc link accepts the request (phase 2). Next, the server through a feedback-control process adjusts size of leaky-buckets until inbound traffc locates in a desired range (phase 3). Then through a <b>ﬁngerprint</b> test, the <b>server</b> detects which port interfaces of defense routers purely carry good traffc and subsequently asks corresponding defense routers to remove the leaky-bucket limitations for those port interfaces. Additionally, the server amends size of leaky-buckets for the defense routers proportional to amount of good traffc that each one carries (phase 4). Simulation-based results shows that our technique effectively, defenses a victim server against various DDoS attacks such that in most cases more than 90 % of good inbound traffc reaches the server while the DDoS attack has been controlled as well...|$|R
40|$|In {{this paper}} we propose a system that automates the whole process of taking {{attendance}} and maintaining its records in an academic institute. Managing people is a difficult task for most of the organizations, and maintaining the attendance record {{is an important factor in}} people management. When considering academic institutes, taking the attendance of students on daily basis and maintaining the records is a major task. Manually taking the attendance and maintaining it for a long time adds to the difficulty of this task as well as wastes a lot of time. For this reason an efficient system is designed. This system takes attendance electronically with the help of a fingerprint sensor and all the records are saved on a computer <b>server.</b> <b>Fingerprint</b> sensors and LCD screens are placed at the entrance of each room. In order to mark the attendance, student has to place his/her finger on the fingerprint sensor. On identification student’s attendance record is updated in the database and he/she is notified through LCD screen. No need of all the stationary material and special personal for keeping the records. Furthermore an automated system replaces the manual system...|$|R
40|$|Clients, administrators, and law {{enforcement}} personnel have many privacy concerns {{when it comes to}} network forensics. Clients would like to use network services in a freedom-friendly environment that protects their privacy and personal data. Administrators would like to monitor their network, and audit its behavior and functionality for debugging and statistical purposes (which could involve invading the privacy of its network users). Finally, members of {{law enforcement}} would like to track and identify any type of digital crimes that occur on the network, and charge the suspects with the appropriate crimes. Members of law enforcement could use some security back doors made available by network administrators, or other forensic tools, that could potentially invade the privacy of network users. In my dissertation, I will be identifying and implementing techniques that each of these entities could use to achieve their goals while preserving the privacy of users on the network. I will show a privacy-preserving implementation of network flow recording that can allow administrators to monitor and audit their network behavior and functionality for debugging and statistical purposes without having this data contain any private information about its users. This implementation is based on identity-based encryption and differential privacy. I will also be showing how law enforcement could use timing channel techniques to <b>fingerprint</b> anonymous <b>servers</b> that are running websites with illegal content and services. Finally I will show the results from a thought experiment about how network administrators can identify pattern-like software that is running on clients' machines remotely without any administrative privileges. The goal of my work is to understand what privileges administrators or law enforcement need to achieve their goals, and the privacy issues inherent in this, and to develop technologies that help administrators {{and law enforcement}} achieve their goals while preserving the privacy of network users. Computer ScienceDoctoralUniversity of New Mexico. Dept. of Computer ScienceCrandall, JedidiahArnold, DorianPerez-Gonzalez, FernandoComesana-Alfaro, Pedr...|$|R
40|$|Malware {{continues}} {{to be one of}} the major threats to Internet security. In the battle against cybercriminals, accurately identifying the underlying malicious server infrastructure (e. g., C&C servers for botnet command and control) is of vital importance. Most existing passive monitoring approaches cannot keep up with the highly dynamic, ever-evolving malware server infrastructure. As an effective complementary technique, active probing has recently attracted attention due to its high accuracy, efficiency, and scalabil-ity (even to the Internet level). In this paper, we propose AUTOPROBE, a novel system to automatically generate effective and efficient fingerprints of remote malicious servers. AUTOPROBE addresses two fundamental limita-tions of existing active probing approaches: it supports pull-based C&C protocols, used by the majority of malware, and it generates fingerprints even in the common case when C&C servers are not alive during fingerprint generation. Using real-world malware samples we show that AUTOPROBE can successfully generate accurate C&C <b>server</b> <b>fingerprints</b> through novel applications of dynamic binary analysis techniques. By con-ducting Internet-scale active probing, we show that AUTOPROBE can successfully uncover hundreds of malicious servers on the Internet, many of them unknown to existing blacklists. We believe AUTOPROBE is a great complement to existing defenses, and can play a unique role in the battle against cybercriminals...|$|R
40|$|Ever {{since we}} were born, there are few parts of our body have been {{developed}} uniquely to represent our identities. Parts like retina patterns, veins, DNA, fingerprints are all unique for every human being. In order to recognize these unique patterns, scientists have spent {{billions of dollars and}} years of research and development to produce tools and techniques to identify humans based on their unique body parts. Today, the most highly utilized parts of our body for the identification purposes are fingerprints. Although there are so many readers and applications have been made for the job, there are still weaknesses {{that need to be addressed}} especially in the area of fingerprint matching. Therefore, the aim of this research is to find ways to increase the performance of fingerprint matching in response to the increased number of fingerprint records available in the system. This research focuses on developing a parallel matching process in a computer cluster to increase the performance of fingerprint matching, without taking into account additional features such as high throughput and others. The main idea is to minimize processing time with multiple processes executing parallels in several computers. As a proof of concept, a prototype consists of client and server processes has been developed. An interface which been integrated with a fingerprint device on client side enables a request (a fingerprint image) been send to a server process. Upon receiving a <b>fingerprint</b> image, the <b>server</b> process will place the request into a queue before distributing to a cluster of matching server throughout the network. The processing time for fingerprint matching has been improved especially when the amount of records increased. The detailed results of identification, duration of matching and similarity are discussed...|$|R
40|$|Tez (Yüksek Lisans) [...] İstanbul Teknik Üniversitesi, Fen Bilimleri Enstitüsü, 2015 Thesis (M. Sc.) [...] İstanbul Technical University, Instıtute of Science and Technology, 2015 Akıllı telefonların kullanımı her geçen gün daha da fazla artmaktadır. Artık insanlar günlük hayatlarında birçok faaliyetle eş zamanlı olarak akıllı telefon kullanmaktadırlar. Akıllı telefonun eş zamanlı olarak en çok kullanıldığı faaliyetlerden biri de televizyon izlemektir. Akıllı telefon kullanıcılarının % 84 'ü televizyon izlerken eşzamanlı olarak akıllı telefonlarını kullanmaktadırlar.   Televizyon izlerken akıllı telefon kullanımına yönlendiren sebeplerden biri de televizyonda izlenen konu ile ilgili daha detaylı bilgiye ulaşmak için telefondan arama yapmaktır. Metinsel olarak arama yapmak zahmetli ve uzun bir işlemdir. Özellikle reklam sektörünün, tanıttıkları ürünle ilgili kullanıcıların daha kolay ve hızlı şekilde bilgiye ulaşması için başka çözümlere ihtiyacı vardır.   Çalışmamızda, televizyonda oynayan reklamın, akıllı telefondaki bir uygulamaya dinletilip tanınmasını sağlayacak bir algoritmanın geliştirilmesi amaçlanmıştır. Reklam tanıması için ses parmakizi (audio fingerprinting) yöntemlerinin kullanılmasına karar verilmiştir. Geliştirilecek yöntemin, ses tanıma sistemlerinin sahip olması gereken gürbüzlük, güvenilirlik, veri boyutu küçüklüğü, parçalılık, arama süresi kısalığı ve hesaplama maliyeti küçüklüğü özelliklerine sahip olması hedeflenmiştir. Farklı reklamlarda, şarkılardan farklı olarak, aynı müzik veya konuşma bölümleri geçebilmektedir, bu nedenle güvenilirlik kontrolü aşamasında, müzik tanıma sistemlerinden farklı bir yaklaşım geliştirilmesi amaçlanmıştır.   Ses parmakizi olarak, spektrogramdaki zirve noktalarının aralarındaki zaman ve frekans farklarından yararlanılarak üretilen karmalar kullanılmıştır. Gürültü ve sinyal bozulmaları durumlarında bile spektrogramdaki zirve noktaların en azından bir kısmının korunması beklenmektedir.   İlgili yöntemin prototip geliştirmesi yapıldıktan sonra deneylerde bazı zayıf yönleri tespit edilmiştir. Genelde, spektrogramdaki zirve noktaların zaman veya frekans yönünde küçük kaymaları nedeniyle ortaya çıkan bu problemlere çözümler geliştirilmiştir. Yöntemin aynı başarım oranlarına daha az veri kullanarak ulaşabilmesi için de katkılar sunulmuştur. Güvenilirlik kontrolü aşamasında da iki eşik değeri parametreli bir çözüm üretilmiştir.   Yapısal katkılar tamamlandıktan sonra, yapılan birçok deney ile yöntemin en iyi başarım oranlarını verdiği sistem parametreleri belirlenmiştir.   Deneylerde şarkılar ve reklamlardan oluşan 2 deney kümesinin, çeşitli seviyelerde beyaz gürültülü, pembe gürültülü, kahverengi gürültülü, kırpılma uygulanmış, bar ortamı etkisi uygulanmış, canlı kayıt etkisi uygulanmış, akıllı telefon kayıt etkisi uygulanmış, akıllı telefon çalma etkisi uygulanmış alt-deney kümeleriyle, ayrıca bir de İstanbul'daki bir alış veriş merkezinde akıllı telefon ile kaydedilmiş versiyonları kullanılmıştır.   Deney sonuçları, anma, kesinlik, kullanılan veri boyutu ve tanıma süresi açılarından değerlendirilmiş, baz alınan yöntemle karşılaştırılmıştır. Bahsedilen bütün kriterlerde baz alınan yönteme göre daha iyi sonuçlar elde edildiği tespit edilmiştir. Smart phone usage {{is rapidly}} {{increasing}} day by day. Nowadays, {{in their daily}} lives, people often use their smart phones simultaneously with another activity. One of the most often conducted activities that goes with smart phone usage is watching television. 84 % of smart phone owners, use their smart phones while watching television.   One of the main factors that leads people to use their smart phones while watching television is to search for more detailed information about the topics they are watching on TV. Text-based searching is inconvenient and also time consuming. Especially advertising industry needs some other resolution to let their customers find information about their products more quickly and easily.   In this thesis, it is aimed to develop an algorithm {{that can be used}} in a smart  phone application to identify a TV commercial by listening through microphone. It is decided to use audio fingerprinting techniques for commercial identification. The algorithm is aimed to have the properties that most of the audio fingerprinting systems should have, such as robustness, reliability, granularity, fingerprint size, search speed and scalability. Some commercials contain same part of music or speech, so another type of reliability check method is needed to be developed, apart from the ones used in song identification systems. Audio fingerprinting literature is reviewed and one algorithm is chosen to be used as base, to satisfy the said requirements.   According to the algorithm, temporal and frequencial distances between the peaks in audio spectrogram were used to generate the fingerprint. A time-frequency point is a candidate peak if it has a higher energy content than all its neighbors in a region centered around the point. At least some of the peaks are supposed to survive in presence of noise or signal distortions.   Base algorithm was developed as a prototype and some weaknesses were identified in the initial tests. It was seen that the problems are often caused by small shifts of peaks in time or frequency directions. Especially in situations with high level of noise, it was observed that, those shifts could be much more and cause success rates to decrease excessively. Some solutions were presented for the mentioned problems. Firstly, in the database search step, hashes, that were generated using the distances between the peaks, were searched in the fingerprint database with some alternative values to increase the success rates. Although increase in success rates were observed in the tests, database search duration was also increased as expected. As search duration was still under real time, it was evaluated as applicable. In the initial test, it was observed that using all the peaks in search step was decreasing the success rates and also increasing the search duration and computation cost, because most of the peaks couldn't survive in high levels of noise. Therefore another contribution was made to use only the strongest n peaks in search step. Value of n was decided to be calculated for every second of the query using the equation . This contribution helped achieve the same success rates using smaller data sizes. It was observed that shifts of peaks caused some problems also in scoring step. According to this, right answer had smaller score than it should have, so it took longer time to exceed the threshold. to overcome this problem, another contribution, that is called histogram normalization, was presented. This contribution led higher success rates. Another weakness of the method was identified in the tests with commercial test sets. Since more than one commercial could have same music or speech parts, querying with this parts resulted in false positives with the original hypothesis testing method. Hypothesis testing step was also improved with a method that uses two threshold parameters, which are called matching rate and power rate. Test results showed that false positives rate decreased excessively after this contribution.   After the structural contributions were completed, a number of tests were employed to find the optimal values for the 3 system parameters, α (alpha), β (beta) and σ (sigma), to achieve the best success rates. Two sets, consisting of songs and commercials, were used as test sets. A number of degradations including white noise, pink noise, brown noise, clipping, bar environment effect, smart phone recording effect, smart phone playback effect and live recording effect were applied to test sets. Furthermore, another test set was generated by recording with a smart phone in a real shopping mall in İstanbul.   Test results were examined in terms of recall, precision, required data size and search time, and then compared with the base algorithm. These result values also refer to the required performance parameters of any audio fingerprinting system, which are robustness, reliability, fingerprint size, granularity and search time.  It has been observed that the developed system is more advantageous than the base algorithm. In another words, search time and storage need were decreased and recall and precision were increased with the contributions. For now, developed system is running as a standalone windows application, written in C++ programming language. When a song or commercial, that were introduced to the system beforehand, is played, testing is started by clicking the button on the user interface and  then application starts listening from the microphone and tries to find a match in the database. For now, the fingerprints of songs and commercials, that are introduced to the system beforehand, are stored in a text file. Before testing step, another button is used to start reading this fingerprints from text file into system memory. Since memory is limited, the number of song and commercial fingerprints is limited too. In tests, a maximum of 500 songs and commercials were used. As the system to be usable in real life applications, it is needed to make some developments to use a real database as fingerprint storage. The database structure needs to be indexable to prevent search times from increasing as the fingerprint count goes up, otherwise the system will be unusable. As the system to be usable in real life applications, a client-server architecture must be developed, with a smart phone application as client, and <b>fingerprint</b> database as <b>server.</b> Smart phone application should listen from the microphone, generate the fingerprint, and send the <b>fingerprint</b> to <b>server</b> instead of all the song data. Server should search the fingerprint in database, make scoring and hypothesis testing and response the client appropriately.   Audio fingerprint generation speed with the smart phone application,  should be observed and if it is above the real time, some optimizations should be done to speed up the process.   Searching and scoring processes in server side, are convenient for parallel processing. If the server is developed in parallel processing architecture, searching and scoring times can be reduced excessively. Yüksek LisansM. Sc...|$|R

