21|27|Public
40|$|A {{search engine}} is an {{essential}} tool in our daily life. With the development of society and net-work technology, the users’ requirement of Internet information is increasing. For most search methods, keyword searching is in a crucial position. However, what {{about the quality of}} <b>key-word</b> <b>search</b> in different search engines? This paper evaluates the quality of keyword searching among different search engines project...|$|E
40|$|Abstract. Keyword {{search on}} {{encrypted}} data enables one to search key-word ciphertexts without compromising keyword security. We further in-vestigate {{this problem and}} propose a novel variant, dubbed certificateless keyword search on encrypted data (CLKS). CLKS not only supports <b>key-word</b> <b>search</b> on encrypted data, but also brings promising features due to the certificateless cryptography. In contrast to the certificated-based keyword search, CLKS requires no validation on the trustworthy of th...|$|E
40|$|The {{improved}} performance of computer-based text analysis represents {{a major step}} forward for knowledge management. Reliable text interpretation allows focus to be placed upon the content of documents, rather than just the document wrapping, and this helps to emphasise the fundamental difference between knowledge management and document management. It is not uncommon for companies who wish to join the KM band-wagon to re-package existing document management programs with a KM label, even if such programs offer little more than a hierarchical file system and simple <b>key-word</b> <b>search</b> to support content management...|$|E
40|$|This {{research}} {{aimed at}} creating a software about Diabetes mellitus {{in order to}} stimulate the continuing education of health professionals as well as educative practices. The system was developed using Delphi application, through a data bank utilizing key-words and the Pascal language. The software includes <b>key-words</b> <b>searching</b> tools, improving the process of accessing the data bank...|$|R
2500|$|The Netherlands has {{the second}} highest {{incidence}} of antisemitic incidents in the European Union. [...] However, {{it is difficult to}} obtain exact figures because the specific groups against whom attacks are made are not specifically identified in police reports, and analyses of police data for antisemitism therefore relies on <b>key-word</b> <b>searches,</b> e.g. [...] "Jew" [...] or [...] "Israel". [...] According to Centre for Information and Documentation on Israel (CIDI), a pro-Israel lobby group in the Netherlands, the number of antisemitic incidents reported in the whole of the Netherlands was 108 in 2008, 93 in 2009, and 124 in 2010. Some two thirds of this are acts of aggression. [...] There are approximately 52 000 Dutch Jews.|$|R
40|$|Health {{outcomes}} for rare diseases can be greatly affected by timely diagnosis. This paper presents a narrative review of current literature on rare diseases, with a focuson Pulmonary Arterial Hypertension (PAH), to identify needs for early diagnosisinitiatives. The review assessed: {{what needs to}} be done, what is currently being done,and what are the approaches or change theories that underlie these initiatives. Literature from online <b>key-word</b> <b>searches</b> included academic articles pertaining todiagnostic methods and physician surveys, and reports from advocacy groups andhealth authorities. Findings centred on the needs for: physician awareness/education,public awareness/education, research needs, consolidation of disease information, andthe need for system-wide early diagnosis strategies. Recommendations highlightedsteps to promote awareness and education among physicians and the public, investigatetheories of behaviour change, and develop and diffuse evaluation criteria of earlydiagnosis initiatives...|$|R
40|$|This {{position}} paper discusses {{the need for}} considering <b>key-word</b> <b>search</b> over relational databases {{in the light of}} broader systems, where keyword search {{is just one of the}} compo-nents and which are aimed at better supporting users in their search tasks. These more complex systems call for ap-propriate evaluation methodologies which go beyond what is typically done today, i. e. measuring performances of com-ponents mostly in isolation or not related to the actual user needs, and, instead, able to consider the system as a whole, its constituent components, and their inter-relations with the ultimate goal of supporting actual user search tasks. 1...|$|E
40|$|Keyword search enables web {{users to}} easily access XML data with-out {{the need to}} learn a {{structured}} query language and to study pos-sibly complex data schemas. Existing work has addressed the prob-lem of selecting qualied data nodes that match keywords and con-necting them in a meaningful way, {{in the spirit of}} inferring a where clause in XQuery. However, how to infer the return clause for <b>key-word</b> <b>search</b> is an open problem. To address this challenge, we present an XML keyword search en-gine, XSeek, to infer the semantics of the search and identify return nodes effectively. XSeek recognizes possible entities and attributes inherently represented in the data. It also distinguishes betwee...|$|E
40|$|For {{magazine}} {{editors and}} others, finding suitable photographs {{for a particular}} pur-pose is increasingly problematic. Advances in storage media along with the Web enable us to store and distribute photographic images worldwide. While large databases containing photographic images exist, the tools and methods for searching and selecting an image are limited. Typically, the databases have a semistructured indexing scheme that allows a <b>key-word</b> <b>search</b> but not much more to help the user find the desired photograph. Currently, researchers promote the use of explicit background knowledge as {{a way out of}} the search prob-lems encountered on the Internet and in multimedia databases. The semantic Web 1 and emerging standards (such as the resource description framework (RDF) 2) make creating a syntactic format specifying background knowledge for information resources possible. In this article, we explore the use of backgroun...|$|E
40|$|Coronary {{heart disease}} (CHD) {{is a leading}} cause of death globally. Despite proven health {{benefits}} and international recommendations, attendance at cardiac rehabilitation programs is poor. Telehealth (phone, Internet, and videoconference communication between patient and health-care provider) has emerged as an innovative way of delivering health interventions. This review aimed to determine telehealth effectiveness in CHD management. Study design includes systematic review with meta-analysis. Randomized controlled trials evaluating telehealth interventions in patients with CHD were identified by searching multiple electronic databases, reference lists, relevant conference lists, gray literature, and <b>key-word</b> <b>searching</b> of the Internet. Studies were selected if they evaluated a telephone, videoconference, or web-based intervention, provided objective measurements of mortality, changes in multiple risk factor levels or quality of life. In total, 11 trials were identified (3145 patients). Telehealth interventions were associated with nonsignificant lower all-cause mortality than controls [relative risk = 0. 70, 95...|$|R
40|$|The Pharmacology Program, a {{computer}} aided instructional {{program has been}} designed for educating second year medical students about pharmacology. The Pharmacology Program is a PC based program which consists of an extensive pharmacology data base in both text and graphic formats. The program contains information {{on most of the}} drugs that are taught as part of the standard pharmacology course in medical school. This data base can be searched through the use of indexes, <b>key-word</b> <b>searches,</b> and hyper-media links between graphics and text. The program incorporates an intuitive, mouse-driven user interface. The program originated as a HyperCard Stack which ran only on Macintosh computers. We report the development of tools needed for the transport of a HyperCard application so that it is able to run in the DIALOG authoring system in the PC environment. In addition, we report on several aspects of user-interface design that are important in the design of multi-media software for medical education...|$|R
2500|$|The Netherlands has {{the second}} highest {{incidence}} of antisemitic incidents in the European Union. However, {{it is difficult to}} obtain exact figures because the specific groups against whom attacks are made are not specifically identified in police reports, and analyses of police data for antisemitism therefore relies on <b>key-word</b> <b>searches,</b> e.g. [...] "Jew" [...] or [...] "Israel". According to Centre for Information and Documentation on Israel (CIDI), a pro-Israel lobby group in the Netherlands, the number of antisemitic incidents reported in the whole of the Netherlands was 108 in 2008, 93 in 2009, and 124 in 2010. Some two thirds of this are acts of aggression. There are approximately 52 000 Dutch Jews. According to the NRC Handelsblad newspaper, the number of antisemitic incidents in Amsterdam was 14 in 2008 and 30 in 2009. In 2010, Raphaël Evers, an orthodox rabbi in Amsterdam, told the Norwegian newspaper Aftenposten that Jews can no longer be safe in the city anymore due to the risk of violent assaults. [...] "We Jews no longer feel at home here in the Netherlands. Many people talk about moving to Israel," [...] he said.|$|R
40|$|Since 1995 the {{techniques}} and capacities to store new electronic data {{and to make}} it available to many persons have become a common good. As of then, different organizations, such as research institutes, universities, libraries, and private companies (Google) started to scan older documents and make them electronically available as well. This has generated a lot of new research opportunities for all kinds of academic disciplines. The use of software to analyze large datasets has become an important part of doing research in the social sciences. Most academics rely on human coded datasets, both in qualitative and quantitative research. However, with the increasing amount of datasets and the complexity of the questions scholars pose to the datasets, the quest for more efficient and effective methods is now on the agenda. One of the most common techniques of content analysis is the Boolean <b>key-word</b> <b>search</b> method. To find certain topics in a dataset, the researcher creates first a list of keywords, added with certain parameters (AND, OR etc.). All keys are usually grouped in families and the entire list of keys and groups is called the ontology. Then the keywords are searched in the dataset, retrieving all documents containing the specified keywords. The online newspaper dataset, LexisNexis, provides the user with such a Boolean search method. However, the Boolean <b>key-word</b> <b>search</b> is not always satisfying in terms of reliability and validity. For that reason social scientists rely on hand-coding. Two projects that do so are the congressional bills project (www. congressionalbills. org) and the policy agenda-setting project (see www. policyagendas. org). They developed a topic code book and coded various different sources, such as, {{the state of the union}} speeches, bills, newspaper articles etcetera. The continuous improving automated coding techniques, and the increasing number of agenda setting projects (in especially European countries), however, has made the use of automated coding software a feasible option and also a necessity...|$|E
40|$|This paper {{presents}} a weighted finite state transducer (WFST) based syllable decoding and transduction framework for <b>key-word</b> <b>search</b> (KWS). Acoustic context dependent phone models are trained from word forced alignments. Then syllable decoding {{is done with}} lattices generated using a syllable lexicon and language model (LM). To process out-of-vocabulary (OOV) keywords, pronunciations are produced using a grapheme-to-syllable (G 2 S) system. A syllable to word lexical transducer containing both in-vocabulary (IV) and OOV keywords is then constructed and composed with a keyword-boosted LM transducer. The composed transducer is then used to transduce syllable lattices to word lattices for final KWS. We show that our method can effectively perform KWS on both IV and OOV keywords, and yields up to 0. 03 Actual Term-Weighted Value (ATWV) improvement over searching keywords directly in subword lattices. Word Error Rates (WER) and KWS results are reported for three different languages...|$|E
40|$|International audienceWe {{study the}} problem of searching on {{encrypted}} data, where the search is performed using a plaintext message or a keyword, rather than a message-specific trapdoor as done by state-of-the-art schemes. The use cases include delegation of <b>key-word</b> <b>search</b> e. g. to a cloud data storage provider or to an email server, using a plaintext message. We define a new cryptographic primitive called plaintext-checkable encryption (PCE), which extends public-key encryption by the following functionality: given a plaintext, a ciphertext and a public key, it is universally possible to check whether the ciphertext encrypts the plaintext under the key. We provide efficient generic random-oracle constructions for PCE based on any probabilistic or deterministic encryption scheme; we also give a practical construction in the standard model. As another application we show how PCE {{can be used to}} improve the efficiency in group signatures with verifier-local revocation (VLR) and backward unlinkability. These group signatures provide efficient revocation of group members, which is a key issue in practical applications...|$|E
40|$|This paper {{presents}} an exploratory investigation of {{situations in which}} people with aphasia may be vulnerable to legal and access to justice issues. The study used a qualitative descriptive approach to analyse 167 de-identified transcriptions of previously collected interviews, with 50 participants with mild-to-severe aphasia following stroke, 48 family members, and their treating speech-language pathologists. Situations experienced by people with aphasia and their family members were coded using <b>key-word</b> <b>searches</b> based on the previously published framework developed by Ellison and colleagues to describe situations of vulnerability to legal and access to justice needs for older people. Health and financial and consumer situations were most frequently identified in the data. Additionally, {{there were a number}} of situations found specifically relating to people with aphasia involving their signatures and credit card use. Instances of discrimination and abuse were also identified, and, although infrequent, these issues point to the profound impact of aphasia on the ability to complain and, hence, to ensure rights to care are upheld. The findings of this study are consistent with previous research in suggesting that legal and access to justice needs are an important issue for people with aphasia and their families...|$|R
5000|$|The Netherlands has {{the second}} highest {{incidence}} of antisemitic incidents in the European Union. However, {{it is difficult to}} obtain exact figures because the specific groups against whom attacks are made are not specifically identified in police reports, and analyses of police data for antisemitism therefore relies on <b>key-word</b> <b>searches,</b> e.g. [...] "Jew" [...] or [...] "Israel". According to Centre for Information and Documentation on Israel (CIDI), a pro-Israel lobby group in the Netherlands, the number of antisemitic incidents reported in the whole of the Netherlands was 108 in 2008, 93 in 2009, and 124 in 2010. Some two thirds of this are acts of aggression. There are approximately 52 000 Dutch Jews. According to the NRC Handelsblad newspaper, the number of antisemitic incidents in Amsterdam was 14 in 2008 and 30 in 2009. In 2010, Raphaël Evers, an orthodox rabbi in Amsterdam, told the Norwegian newspaper Aftenposten that Jews can no longer be safe in the city anymore due to the risk of violent assaults. [...] "We Jews no longer feel at home here in the Netherlands. Many people talk about moving to Israel," [...] he said.|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "Although the National Highway Traffic Safety Administration (NHTSA) {{has the authority to}} regulate aftermarket crash parts, the agency has not developed safety standards for them because it has not determined that any aftermarket crash parts contain safety-related defects. NHTSA has more limited authority to regulate the use of recycled air bags. NHTSA could elect to develop safety standards for occupant restraint systems under the used vehicle provision of the Motor Vehicle Safety Act. NHTSA has not developed such standards because it has not identified significant problems with occupant restraint systems that could be addressed by state motor vehicle inspection programs. The limitations of NHTSA's complaint system may hamper NHTSA's ability to detect safety-related trends through broad <b>key-word</b> <b>searches</b> of its complaint database and make it unlikely that NHTSA can identify all unsafe parts. In addition, the ability to recall unsafe aftermarket crash parts is limited because some parts are not stamped with the manufacturer's name and there is no trail leading from the manufacturer to the ultimate user of the part. Two studies on the safety of recycled airbags concluded that they can be a potentially safe, economical alternative to new airbags {{as long as they are}} undamaged and properly handled and installed. However, the failure of some flood-damaged airbags to deploy correctly also demonstrates the potential for serious safety consequences. ...|$|R
40|$|Abstract. We {{study the}} problem of searching on {{encrypted}} data, where the search is performed using a plaintext message or a keyword, rather than a message-specific trapdoor as done by state-of-the-art schemes. The use cases include delegation of <b>key-word</b> <b>search</b> e. g. to a cloud data storage provider or to an email server, using a plaintext message. We define a new cryptographic primitive called plaintext-checkable encryption (PCE), which extends public-key encryption by the following functionality: given a plaintext, a ciphertext and a public key, it is universally possible to check whether the ciphertext encrypts the plaintext under the key. We provide efficient generic random-oracle constructions for PCE based on any probabilistic or deterministic encryption scheme; we also give a practical construction in the standard model. As another application we show how PCE {{can be used to}} improve the efficiency in group signatures with verifier-local revocation (VLR) and backward unlinkability. These group signatures provide efficient revocation of group members, which is a key issue in practical applications. Keywords. Deterministic/probabilistic encryption, unlinkability, group signature with VLR and backward unlinkability. ...|$|E
40|$|Abstract—Distributed Hash Tables (DHTs) {{establish}} a struc-tured Peer-to-Peer (P 2 P) overlay network by applying proactive routing algorithms. Due to their well defined structure {{this class of}} P 2 P protocols is able to locate any content in the system within {{a limited number of}} hops. However, basic DHT algorithms are limited to queries for content that exactly matches the search term. In this paper, we propose a Prefix-based Multi-Attribute <b>Key-word</b> <b>Search</b> (PriMA KeyS) that is specially designed for searching persons in a distributed phone book and similar applications. Our architecture is fully distributed and pays special attention to a balanced storage load distribution as well as low network traffic. Hierarchical identifiers generated from multiple keywords help to reduce the load on nodes that host common keywords. Additionally, a locality preserving hash function enables prefix-based queries. An extensive linguistic analysis of search keywords is carried out to select optimum design parameters. Using sample queries we show that our system can efficiently handle both detailed and unspecific queries. I...|$|E
40|$|In {{this paper}} we give an {{overview}} of Semantic Web technologies {{and the impact of}} these ones for multilingual Web. We present a possible solution for improving the quality of on-line translation systems, using mechanisms and standards from Semantic Web. We focus on Example based machine translation and the automatization of the translation examples extraction by means of RDF-repositories. 1 Basic Principles of Semantic Web In WWW are used each year more than 5 Billion Documents from more than 800 million active users. However, up to now is WWW self-organised System without any predefined or standardised structure or administration. The huge the quantity of information in WWW becomes the more difficult its administration is. Especially the update and targeted retrieval of information are more and more difficult. Usually the information is retrieved following a <b>key-word</b> <b>search.</b> This lexical search creates several problems:- too strong specialisation (better information about „violin “ can be found maybe under „string instruments “ or 2 instruments“), because of- too many different specialised meanings, here for e. g. „instruments “ are only „music instruments “ but no surgery instruments. - No possibility of searching also synonym...|$|E
40|$|Surveillance data on {{occupational}} {{injuries and}} deaths are frequently analyzed {{in order to}} identify high-risk worker populations, characterize injury circumstances, and determine potential risk factors. Results drive injury prevention efforts at the national, state and local levels, and impact legislation and regulatory policy. The inclusion of narrative fields in injury surveillance data allow for identification of specific hazards and injury incidents. NIOSH maintains several surveillance systems that include narrative lields. Analyses of these narrative ntries, through computerized <b>key-word</b> <b>searches</b> and manual review, has allowed us to go beyond the limits of coded data to better understand specific circumstances and risks. NIOSH's pnmary system lot surveillance of fatal occupational injuries is the National Traumatic Occupational Fatalities System, or NTOF. NTOF is comprised of information from death certificates for people who die from injuries at work. Death certificates are provided by all 50 states for cases that meet hese criteria: age 16 years or older, external cause of death (ES 00 -E 999), and a positive response to the "injury at work? " item. The NTOF database contains data on fatal occupational injuries since 1980. We recently released a publication: "Fatal Injuries to Workers in the United States, 1980 - 1989 : A Decade of Surveillance", that Dr. Satcher described, which provides an overview of the NTOF surveillance system and contains both national and state-specific analyses of worker deaths for the decade of the 1980 s...|$|R
40|$|Organizations in today’s {{software}} industry are increasingly {{faced with the}} challenge of managing information about their past, present, and future projects. The effective and efficient reuse of past knowledge, experience, and assets {{is one of the key}} success factors in the software business. To organize the huge number of documents arising during software projects, e. g. use case documents, a digital library offering content-based organization may be used. It allows the user to explore and analyze a potentially unknown library in an intuitive way. In software reuse, finding suitable reuse candidates for a more or less accurately specified problem is one of the critical questions. In knowledge managment, an important issue is finding correct sources of tacit knowledge. Mapping the use cases of a new project to the existing collection may reveal valuable similarities that might not be uncovered by traditional information retrieval methods like <b>key-word</b> based <b>search.</b> This thesis investigates a digital library system based on self-organizing map...|$|R
40|$|Abstract. Existing <b>key-word</b> based image <b>search</b> engines return images whose title or {{immediate}} surrounding text {{contains the}} search term as a keyword. When the search term is ambiguous and means different things, the results often {{come in a}} mixed bag of different entities. This paper proposes a novel framework that understands the context and thus infers the most likely entity in the given im-age by disambiguating the terms in the context into the corresponding concepts from external knowledge in a process called conceptualization. The images can subsequently be clustered by the most likely associated entities. This approach outperforms the best competing image clustering techniques by 29. 2 % in NMI score. In addition, the framework automatically annotates each cluster of images by its key entities which allows users to quickly identify the images they want. ...|$|R
40|$|Heart failure (HF) is a chronic, {{progressive}} {{illness that}} is highly prevalent in the United States and worldwide. This morbid illness carries a very poor prognosis, and leads to frequent hospitalizations. Repeat hospitalization in HF is both largely burdensome to {{the patient and the}} healthcare system, as {{it is one of the}} most costly medical diagnoses among Medicare recipients. For years, investigators have strived to determine methods to reduce hospitalization rates of HF patients. Despite such efforts, recent reports indicate that re-hospitalization rates remain persistently high, without any improvement over the past several years and thus, this topic clearly needs aggressive attention. We performed a <b>key-word</b> <b>search</b> of the literature for relevant citations. Published articles, limited to English abstracts indexed primarily in the PubMed database through the year 2011, were reviewed. This article discusses various clinical parameters, serum biomarkers, hemodynamic parameters, and psychosocial factors that have been reviewed in the literature as predictors of re-hospitalization of HF patients. With this information, our hope is that the future holds better risk-stratification models that will allow providers to identify high-risk patients, and better customize effective interventions according to the needs of each individual HF patient...|$|E
40|$|This {{research}} {{proposes a}} framework for ef-ficient information extraction and filtering in situations where 1) extreme reliability is important, 2) {{the amount of information}} to be combed through is massive, and 3) we can expect a relatively large number of human workers to be available. In particu-lar, we are motivated by needs in times of crisis, and assume that in order to ensure the high level of reliability required, {{it will be necessary to}} have at least one human worker confirm all extracted information. Given this setting, we propose a method to improve the efficiency of manual veri-fication by deciding which information to present to workers using machine learn-ing techniques. Even given this efficient search framework, the amount of informa-tion on the internet is still too much for one user to handle, so we additionally create a web-based framework that allows for col-laborative work, and an algorithm that al-lows for this framework to work on large data in real-time. We perform an eval-uation using data from Twitter after the Great East Japan Earthquake, and com-pare efficiency using both traditional <b>key-word</b> <b>search</b> and the proposed learning-based method. ...|$|E
40|$|Copyright: © 2015 OuYang YC. This is an open-access article {{distributed}} {{under the}} terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Purpose: The purpose of this paper is to review and present that separate capabilities of knowledge management (KM) are identified and classified as sequential capability. These KM capabilities are utilized to affect organizational performance with a KM-cyclic view. Design/methodology/approach: This paper is a literature review research, through which KM capability and performance are identified, synthesized, from related books, literature and other research studies. Using <b>key-word</b> <b>search</b> in electronic databases, we generated a fairly exhaustive list of the articles dealing with the topic of KM capability in the period of 1979 to 2010 as well as various sources were extended from searching results. A total of 60 articles were identified for KM capability and 30 articles for performance measurement. Findings: The results of the research suggest two main aspects: 1) The KM capabilities were organized as four categories with cyclic view: acquisition/creation, conversion, sharing, and application. 2) The KM performance measurements were classified as two categories: non-financial and financial measurements. Research limitations/implications: There is a need to undertake empirical research and in-depth case studie...|$|E
40|$|Background: Intimate partner {{violence}} (IPV) is {{very high}} in Africa. However, information obtained from {{the increasing number of}} African studies on IPV among pregnant women has not been scientifically analyzed. This paper presents a systematic review summing up the evidence from African studies on IPV prevalence and risk factors among pregnant women. Methods: A <b>key-word</b> defined <b>search</b> of various electronic databases, specific journals and reference lists on IPV prevalence and risk factors during pregnancy resulted in 19 peer-reviewed journal articles which matched our inclusion criteria. Quantitative articles about pregnant women from Africa published in English between 2000 and 2010 were reviewed. At least two reviewers assessed each paper for quality and content. We conducted meta-analysis of prevalence data and reported odds ratios of risk factors. Results: The prevalence of IPV during pregnancy ranges from 2 % to 57 % (n = 13 studies) with meta-analysis yielding an overall prevalence of 15. 23 % (95 % CI: 14. 38 to 16. 08 %). After adjustment for known confounders, five studies retained significant associations between HIV and IPV during pregnancy (OR 1. 48 – 3. 10). Five studies demonstrated strong evidence that a history of violence is significantly associated with IPV in pregnancy and alcohol abuse by a partner also increases a woman’s chances of being abused during pregnancy (OR 2. 89 – 11. 60). Other risk factors include risky sexual behaviours, lo...|$|R
40|$|AbstractThe {{present data}} article {{describes}} high-school drop-out related web activities in Canada, from 2004 to 2012, obtained mining Google Trends (GT), using high-school drop-out as <b>key-word.</b> The <b>searches</b> volumes were processed, correlated and cross-correlated with statistical data obtained at national and province level and broken down for gender. Further, an autoregressive moving-average (ARMA) model {{was used to}} model the GT-generated data. From a qualitative point of view, GT-generated relative search volumes (RSVs) reflect the decrease in drop-out rate. The peak in the Internet-related activities occurs in 2004 (56. 35 %, normalized value), and gradually declines to 40. 59 % (normalized value) in 2007. After, it remains substantially stable until 2012 (40. 32 %, normalized value). From a quantitative standpoint, the correlations between Canadian high-school drop-out rate and GT-generated RSVs in the study period (2004 – 2012) were statistically significant both using the drop-out rate for academic year and the 3 -years moving average. Examining the data broken down by gender, the correlations were higher and statistically significant in males than in females. GT-based data for drop-out resulted best modeled by an ARMA(1, 0) model. Considering the cross correlation of Canadian regions, all of them resulted statistically significant at lag 0, apart from for New Brunswick, Newfoundland and Labrador and the Prince Edward island. A number or cross-correlations resulted statistically significant also at lag − 1 (namely, Alberta, Manitoba, New Brunswick and Saskatchewan) ...|$|R
40|$|We advance {{first mover}} {{advantage}} (FMA) theory by examining how the pace of market evolution and technology evolution potentially enables or disables FMA. Integrating several streams of literature, we elaborate on the interplay among these two envi-ronmental (macro) conditions and the “isolating mechanisms ” that underpin FMA. We model these dynamics to help researchers negotiate the current debate, arising from conflicting empirical evidence, on the conditions necessary for FMA to exist. In the management literature the conceptual appeal of {{first mover advantage}}s (FMA) is evi-dent. Using “first mover advantage ” as <b>key-words,</b> our <b>search</b> for peer-refereed journal arti-cles in the Business Source Premier database yielded a total of 839 articles. The concept has also enjoyed ample diffusion in the practitioner-oriented literature and has fueled aggressive claims and an ongoing debate about whether the advantage actually exists. Consider, for in-stance, Arthur’s claim that “two maxims are widely accepted in knowledge based markets: it pays to hit the market first and it pays to have superb technology ” (1998 : 100). This assertion contrasts sharply with Sandberg’s claim that “in most cases [...] . being the first mover is no guar-antee of success ” (2001 : 3). The academic literature {{has been unable to}} provide conclusive empirical evidence to sup-port or refute the existence of FMA. Some empir-ical studies point to a negative relationship be-tween order of entry and such measures of a firm’s performance as market share (Bond &...|$|R
40|$|Systematic reviews are an {{instrument}} of Evidence-Based Policy designed to produce comprehensive, unbiased, transparent and clear assessments of interventions’ effectiveness. From their origins in medical fields, systematic reviews have recently been promoted as offering important advances {{in a range of}} applied social science fields, including international development. Drawing on a case study of a systematic review of the effectiveness of community mobilisation as an intervention to tackle HIV/AIDS, this article problematizes the use of systematic reviews to summarise complex and context-specific bodies of evidence. Social development interventions, such as ‘community mobilisation’ often take different forms in different interventions; are made successful by their situation in particular contexts, rather than being successful or unsuccessful universally; and have a rhetorical value that leads to the over-application of positively valued terms (e. g. ‘community mobilisation’), invalidating the <b>key-word</b> <b>search</b> process of a systematic review. The article suggests that the policy interest in definitive summary statements of ‘the evidence’ is at odds with academic assessments that evidence takes multiple, contradictory and complex forms, and with practitioner experience of the variability of practice in context. A pragmatist philosophy of evidence is explored as an alternative. Taking this approach implies expanding the definition of forms of research considered to be ‘useful evidence’ for evidence-based policy-making; decentralising decisions about ‘what works’ to allow for the use of local practical wisdom; and prioritising the establishment of good processes for the critical use of evidence, rather than producing context-insensitive summaries of ‘the evidence’...|$|E
40|$|IntroductionImprovements in diet {{can prevent}} obesity and type 2 diabetes. Although policy changes provide a {{foundation}} for improvement at the population level, evidence for the effectiveness of such changes is slim. This study summarizes the literature on recent efforts in the United States to change food-related policies to prevent obesity and diabetes among adults. MethodsWe conducted a systematic review of evidence {{of the impact of}} food policies. Websites of government, academic, and nonprofit organizations were scanned to generate a typology of food-related policies, which we classified into 18 categories. A <b>key-word</b> <b>search</b> and a search of policy reports identified empirical evaluation studies of these categories. Analyses were limited to strategies with 10 or more reports. Of 422 articles identified, 94 met these criteria. Using publication date, study design, study quality, and dietary outcomes assessed, we evaluated the strength of evidence for each strategy in 3 assessment categories: time period, quality, and study design. ResultsFive strategies yielded 10 or more reports. Only 2 of the 5 strategies, menu labeling and taxes on unhealthy foods, had 50 % or more studies with positive findings in at least 2 of 3 assessment categories. Most studies used methods that were rated medium quality. Although the number of published studies increased over 11 years, study quality did not show any clear trend nor did it vary by strategy. ConclusionResearchers and policy makers can improve the quality and rigor of policy evaluations to synthesize existing evidence and develop better methods for gleaning policy guidance from the ample but imperfect data available. 26513438 PMC 465111...|$|E
40|$|Taxonomies, {{especially}} {{the ones in}} specific domains, are becoming indispensable to {{a growing number of}} applications. State-of-the-art approaches assume there exists a text corpus to accurately charac-terize the domain of interest, and that a taxonomy can be derived from the text corpus using information extraction techniques. In reality, neither assumption is valid, especially for highly focused or fast-changing domains. In this paper, we study a challenging problem: Deriving a taxonomy from a set of keyword phrases. A solution can benefit many real life applications because i) keywords give users the flexibility and ease to characterize a specific domain; and ii) in many applications, such as online advertisements, the do-main of interest is already represented by a set of keywords. How-ever, it is impossible to create a taxonomy out of a keyword set it-self. We argue that additional knowledge and contexts are needed. To this end, we first use a general purpose knowledgebase and <b>key-word</b> <b>search</b> to supply the required knowledge and context. Then we develop a Bayesian approach to build a hierarchical taxonomy for a given set of keywords. We reduce the complexity of previous hierarchical clustering approaches from O(n 2 log n) to O(n log n), so that we can derive a domain specific taxonomy from one mil-lion keyword phrases in less than an hour. Finally, we conduct comprehensive large scale experiments to show the effectiveness and efficiency of our approach. A real life example of building an insurance-related query taxonomy illustrates the usefulness of our approach for specific domains...|$|E
40|$|The {{present data}} article {{describes}} high-school drop-out related web activities in Canada, from 2004 to 2012, obtained mining Google Trends (GT), using high-school drop-out as <b>key-word.</b> The <b>searches</b> volumes were processed, correlated and cross-correlated with statistical data obtained at national and province level and broken down for gender. Further, an autoregressive moving-average (ARMA) model {{was used to}} model the GT-generated data. From a qualitative point of view, GT-generated relative search volumes (RSVs) reflect the decrease in drop-out rate. The peak in the Internet-related activities occurs in 2004 (56. 35 %, normalized value), and gradually declines to 40. 59 % (normalized value) in 2007. After, it remains substantially stable until 2012 (40. 32 %, normalized value). From a quantitative standpoint, the correlations between Canadian high-school drop-out rate and GT-generated RSVs in the study period (2004 - 2012) were statistically significant both using the drop-out rate for academic year and the 3 -years moving average. Examining the data broken down by gender, the correlations were higher and statistically significant in males than in females. GT-based data for drop-out resulted best modeled by an ARMA(1, 0) model. Considering the cross correlation of Canadian regions, all of them resulted statistically significant at lag 0, apart from for New Brunswick, Newfoundland and Labrador and the Prince Edward island. A number or cross-correlations resulted statistically significant also at lag - 1 (namely, Alberta, Manitoba, New Brunswick and Saskatchewan) ...|$|R
40|$|Abstract. Advertising {{mechanisms}} for search engines (i. e., sponsored search auctions) have recently {{received a lot}} of attention in the scientific community. Advertisers bid on keywords and, when a user enters <b>key-words</b> for her <b>search,</b> the search engines uses an auction mechanism to select the list of sponsored links to display alongside the search results. In this paper, we make a first attempt to extend the currently available {{mechanisms for}} sponsored search auctions to the new paradigms of search computing. According to them, multiple federated domain-specific search engines are integrated by a special search engine (called integrator). The user can enter a multi-domain query that is decomposed by the integra-tor in single-domain queries and these are singularly addressed to the most appropriate domain-specific search engine. The integrator merges the search results. We propose a business model for this scenario and we develop an economic mechanism for it resorting to the automated mechanism design approach. ...|$|R
40|$|Abstract — In this paper, we {{introduce}} a new method of ontology based text classification for Telugu documents and retrieval system. Many of the text categorization techniques are based on word and/or phrase analysis of the text. Term frequency analysis signifies {{the importance of a}} term within a document. Two terms within a document can have the same frequency, but one term may contribute more to the meaning of the sentence compared to the other term. Our aim is to capture the semantics of a text. The model we worked enables to capture the terms that presents the concepts in the text and thus identifies the topic of the document. We have introduced the new concept based model which analyzes the terms on the sentences and documents level. This concept-based model effectively discriminates between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The limitations of <b>key-word</b> based <b>search</b> are overcome by usage of Ontology which is a motivation of semantic IR. The retrieval model is based on an adaptation of the classic vector-space model. The concept of ontology is associated with the related words and their weights from the pre-classified documents as a learning stage. In the main process, the words and their mutual relations are extracted from the target documents. The concept of Ontology is used to map the target document. A detailed description of the test results is illustrated in the paper and we explained thoroughly how the concept based classification is far more superior when compared to the word based classification for telugu documents...|$|R
