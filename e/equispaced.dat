372|5|Public
25|$|Thus, antitriptic isobars {{need to be}} <b>equispaced</b> circles or {{straight}} lines.|$|E
2500|$|Various {{groups have}} also {{published}} [...] "FFT" [...] algorithms for non-equispaced data, as reviewed in Potts et al. (2001). Such algorithms do not strictly compute the DFT (which is only defined for <b>equispaced</b> data), but rather some approximation thereof (a non-uniform discrete Fourier transform, or NDFT, which itself is often computed only approximately). More generally {{there are various}} other methods of spectral estimation.|$|E
5000|$|RectifiedGridCoverage: a regular, <b>equispaced</b> grid {{which is}} {{spatially}} referenced (like a satellite image which does have geo coordinates associated) ...|$|E
40|$|Taking {{consideration}} of the integrity modeling with correlative multiple deformation points, this paper extends the single point deformation analysis into spatial multi-point integrity analysis and presents a non-equidistant multi-point modeling by improving <b>equispacing</b> processing for the non-equidistant series. The real practical analysis and comparative {{results indicate that the}} non-equidistant multi-point prediction model is feasible and effective, which is a new nonlinear approach to the integrated deformation analysis and prediction in time and space domain...|$|R
40|$|Abstract. We {{consider}} constrained biobjective optimization problems. One of {{the extant}} {{issues in this}} area is that of uniform sampling of the Pareto front. We utilize <b>equispacing</b> constraints on the vector of objective values, as discussed in a previous paper dealing with the unconstrained problem. We present a direct and a dual formulation based on arc-length homotopy continuation and illustrate the direct method (using standard nonlinear programming tools) on some problems from the literature. We contrast the performance of our method with the results of three other algorithms, showing several orders of magnitude speed-up with respect to evolutionary algorithms, while simultaneously providing perfectly sampled fronts by construction. We then consider a large-scale application: the variational approach to mesh generation for partial differential equations in complex domains. Balancing multiple criteria leads to significantly improved mesh design...|$|R
40|$|We propose an {{adiabatic}} mean-field {{model for}} dynamical collective state transitions {{of a nuclear}} system. The transition process is {{described in terms of}} the nuclear mean-field wave functions which are adiabatically determined {{in the course of the}} transition. A principal steering meson field approximation simplifies the model. In the simplified model, the Hamiltonian is expressed by a tridiagonal matrix on the basis of the adiabatic mean-field states, because the mean-field states are coupled by the residual interaction. The model has two degenerate lowest mean-field states. These states are separated by a potential barrier made of intermediate mean-field states and are coupled to each other by the interaction through the intermediate states. We solve the eigenvalue equation for the Hamiltonian both in an exact diagonalization and in a perturbation method. The perturbation expression for the splitting of the energies of the two almost degenerate ground states exhibits analytically a coherent structure in favor of the dynamical transition between the two isolated lowest mean-field states. The net current for the collective tunneling from an initial lowest mean-field state to the degenerate counterpart through the potential barrier is much smaller than the quantum mechanically fluctuating local currents. The energy eigenvalue equation for a tridiagonal Hamiltonian matrix leads to a Schrödinger difference equation on a finite range of integral discrete coordinates. Higher energy states on a repulsive parabolic potential on the finite range of discrete coordinate are shown to have some features resembling the energy states of a harmonic oscillator: <b>equispacing</b> energy levels and Gaussian distribution of the wave functions...|$|R
5000|$|GridCoverage: a regular, <b>equispaced</b> grid {{which is}} not {{spatially}} referenced (like a raster image which has no geo coordinates associated) ...|$|E
5000|$|ReferenceableGridCoverage: a grid {{which is}} not {{necessarily}} <b>equispaced</b> (like satellite image time series where images do not arrive at regular time intervals, or curvilinear grids following river estuaries) ...|$|E
5000|$|Various {{groups have}} also {{published}} [...] "FFT" [...] algorithms for non-equispaced data, as reviewed in Potts et al. (2001). Such algorithms do not strictly compute the DFT (which is only defined for <b>equispaced</b> data), but rather some approximation thereof (a non-uniform discrete Fourier transform, or NDFT, which itself is often computed only approximately). More generally {{there are various}} other methods of spectral estimation.|$|E
40|$|This {{thesis is}} {{primarily}} concerned with self-matching and interleaving within the following integer sequences: Paperfolding, the Stern-Brocot Sequence (and the related Stern-Brocot Tree), the Hyperbinary Sequence (and the related Hyperbinary Tree), the Gray Code and the Stickbreaking Sequence. It also investigates symmetry properties of the Gauss Map and the connection the Gauss Map has with the Stern-Brocot Tree. Symmetry within the Gauss Map is shown to exist between neighbouring iterates. Repetition and Symmetry are shown to exist both within the same iterate, and amongst neighbouring iterates of the Generalised Gauss Map, which {{is an extension of}} the Gauss Map such that its domain covers all reals. The fixed points, periodic points and conditions for symmetry of the Gauss Map are each defined and explored, as is the location of each symmetric and repetitive element of the Generalised Gauss Map. Self-matching and interleaving are shown to be fundamental properties of the Stern-Brocot Tree. The Stern-Brocot sequence is shown to be simply an interleave of mediants formed from an ancestral Stern-Brocot sequence. Mediants of the Stern-Brocot sequence are found to represent new terms forming a new level of the tree. The connections between the partial convergents of an irrational number and the Stern-Brocot Tree are also established. We show that these partial convergents when mapped into R 2 reveal an interesting collinearity and <b>equispacing.</b> T w o of the most important techniques found in this thesis are diagonalisation and additive factorisation. These techniques allow us to answer the following questions: 2 ̆ 2 What is the jth term in the nth level of the tree? 2 ̆ 2, and the related question 2 ̆ 2 Where does the term a-b appear in the tree? 2 ̆ 2 We demonstrate that additive factorisation is also useful in identifying terms in the Hyperbinary Tree. This tree similar to the Stern-Brocot Tree. Another key result of this thesis is the demonstration that, for all their apparent dissimilarity, the Gauss Map and the Stern-Brocot Tree are intimately linked. We show that Branches in the Stern-Brocot Tree are analogues of Clusters in the Gauss Map. These linkages are preserved in the Generalised Stern-Brocot Tree and the Generalised Gauss Map. The Paperfolding Sequence is shown to be a sequence that can be represented in both a self-matching manner as well as by an interleave operator. We show that these dual formulations are derivable from each other indicating the interconnectedness that can exist between self-matching sequences and interleave sequences. We also reveal that the Paperfolding sequence is subtly embedded in both the Stem-Brocot Tree and the Gray Code. Another interleaving sequence, the Stickbreaking sequence, is also defined. It is also examined in terms of its mixedness, a concept that measures the disorder that arises in a sequence. Stickbreaking and Perfect Shuffling are compared terms of their relative mixedness...|$|R
40|$|Filtered backprojection (FBP) {{reconstruction}} algorithms {{are very}} popular {{in the field of}} X-ray computed tomography (CT) because they give advantages in terms of the numerical accuracy and computational complexity. Ramp filter based fan-beam FBP reconstruction algorithms have the position dependent weight in the backprojection which is responsible for spatially non-uniform distribution of noise and resolution, and artifacts. Many algorithms based on shift variant filtering or spatially-invariant interpolation in the backprojection step have been developed to deal with this issue. However, these algorithms are computationally demanding. Recently, fan-beam algorithms based on Hilbert filtering with inverse distance weight and no weight in the backprojection have been derived using the Hamaker’s relation. These fan-beam reconstruction algorithms have been shown to improve noise uniformity and uniformity in resolution. In this thesis, fan-beam FBP reconstruction algorithms with inverse distance back-projection weight and no backprojection weight for 2 D image reconstruction are presented and discussed for the two fan-beam scan geometries -equi-angular and <b>equispace</b> detector array. Based on the proposed and discussed fan-beam reconstruction algorithms with inverse distance backprojection and no backprojection weight, new 3 D cone-beam FDK reconstruction algorithms with circular and helical scan trajectories for curved and planar detector geometries are proposed. To start with three rebinning formulae from literature are presented and it is shown that one can derive all fan-beam FBP reconstruction algorithms from these rebinning formulae. Specifically, two fan-beam algorithms with no backprojection weight based on Hilbert filtering for equi-space linear array detector and one new fan-beam algorithm with inverse distance backprojection weight based on hybrid filtering for both equi-angular and equi-space linear array detector are derived. Simulation results for these algorithms in terms of uniformity of noise and resolution in comparison to standard fan-beam FBP reconstruction algorithm (ramp filter based fan-beam reconstruction algorithm) are presented. It is shown through simulation that the fan-beam reconstruction algorithm with inverse distance in the backprojection gives better noise performance while retaining the resolution properities. A comparison between above mentioned reconstruction algorithms is given in terms of computational complexity. The state of the art 3 D X-ray imaging systems in medicine with cone-beam (CB) circular and helical computed tomography scanners use non-exact (approximate) FBP based reconstruction algorithm. They are attractive because of their simplicity and low computational cost. However, they produce sub-optimal reconstructed images with respect to cone-beam artifacts, noise and axial intensity drop in case of circular trajectory scan imaging. Axial intensity drop in the reconstructed image is due to the insufficient data acquired by the circular-scan trajectory CB CT. This thesis deals with investigations to improve the image quality by means of the Hilbert and hybrid filtering based algorithms using redundancy data for Feldkamp, Davis and Kress (FDK) type reconstruction algorithms. In this thesis, new FDK type reconstruction algorithms for cylindrical detector and planar detector for CB circular CT are developed, which are obtained by extending to three dimensions (3 D) an exact Hilbert filtering based FBP algorithm for 2 D fan-beam beam algorithms with no position dependent backprojection weight and fan-beam algorithm with inverse distance backprojection weight. The proposed FDK reconstruction algorithm with inverse distance weight in the backprojection requires full-scan projection data while the FDK reconstruction algorithm with no backprojection weight can handle partial-scan data including very short-scan. The FDK reconstruction algorithms with no backprojection weight for circular CB CT are compared with Hu’s, FDK and T-FDK reconstruction algorithms in-terms of axial intensity drop and computational complexity. The simulation results of noise, CB artifacts performance and execution timing as well as the partial-scan reconstruction abilities are presented. We show that FDK reconstruction algorithms with no backprojection weight have better noise performance characteristics than the conventional FDK reconstruction algorithm where the backprojection weight is known to result in spatial non-uniformity in the noise characteristics. In this thesis, we present an efficient method to reduce the axial intensity drop in circular CB CT. The efficient method consists of two steps: the first one is reconstruction of the object using FDK reconstruction algorithm with no backprojection weight and the second is estimating the missing term. The efficient method is comparable to Zhu et al. ’s method in terms of reduction in axial intensity drop, noise and computational complexity. The helical scanning trajectory satisfies the Tuy-smith condition, hence an exact and stable reconstruction is possible. However, the helical FDK reconstruction algorithm is responsible for the cone-beam artifacts since the helical FDK reconstruction algorithm is approximate in its derivation. In this thesis, helical FDK reconstruction algorithms based on Hilbert filtering with no backprojection weight and FDK reconstruction algorithm based on hybrid filtering with inverse distance backprojection weight are presented to reduce the CB artifacts. These algorithms are compared with standard helical FDK in-terms of noise, CB artifacts and computational complexity...|$|R
5000|$|Sometimes this {{is known}} as {{quadriphase}} PSK, 4-PSK, or 4-QAM. (Although the root concepts of QPSK and 4-QAM are different, the resulting modulated radio waves are exactly the same.) QPSK uses four points on the constellation diagram, <b>equispaced</b> around a circle. With four phases, QPSK can encode two bits per symbol, shown in the diagram with Gray coding to minimize the bit error rate (BER) [...] - [...] sometimes misperceived as twice the BER of BPSK.|$|E
50|$|Consider {{the case}} where one desires to {{interpolate}} through n+1 <b>equispaced</b> points of a function f(x) using the n-degree polynomial Pn(x) that passes through those points. Naturally, one might expect from Weierstrass' theorem that using more points {{would lead to a}} more accurate reconstruction of f(x). However, this particular set of polynomial functions Pn(x) is not guaranteed to have the property of uniform convergence; the theorem only states that a set of polynomial functions exists, without providing a general method of finding one.|$|E
50|$|The Clavius method {{looks at}} {{a quarter of the}} dial. It views the {{horizontal}} and the perpendicular plane to the polar axis as two rectangles hinged around the top edge of both dials. the polar axis will be at φ degrees to the polar axis, and the hour lines will be <b>equispaced</b> on the polar plane an equatorial dial. (15°). Hour points on the polar plane will connect to the matching point on the horizontal plane. The horizontal hour lines are plotted to the origin.|$|E
5000|$|In the cross-stream {{momentum}} equation, the Coriolis {{force and}} normal pressure gradient are both negligible, leading to no net bending action.As the centrifugal term [...] vanishes while the speed is non-zero, the {{radius of curvature}} goes to infinity, and the trajectory must be a straight line.In addition, the trajectory is perpendicular to the isobars since [...] Since this condition occurs when the n direction is that of an isobar, s is perpendicular to the isobars.Thus, antitriptic isobars need to be <b>equispaced</b> circles or straight lines.|$|E
50|$|In the {{mathematical}} field of numerical analysis, Runge's phenomenon {{is a problem}} of oscillation at the edges of an interval that occurs when using polynomial interpolation with polynomials of high degree over a set of <b>equispaced</b> interpolation points. It was discovered by Carl David Tolmé Runge (1901) when exploring the behavior of errors when using polynomial interpolation to approximate certain functions.The discovery was important because it shows that going to higher degrees does not always improve accuracy. The phenomenon is similar to the Gibbs phenomenon in Fourier series approximations.|$|E
5000|$|A MATLAB {{implementation}} of the above can be found here and is given by:function P = triginterp(xi,x,y)% TRIGINTERP Trigonometric interpolation.% Input:% xi evaluation points for the interpolant (vector)% x <b>equispaced</b> interpolation nodes (vector, length N)% y interpolation values (vector, length N)% Output:% P values of the trigonometric interpolant (vector)N = length(x);% Adjust the spacing of the given independent variable.h = 2/N;scale = (x(2)-x(1)) / h;x = x/scale; xi = xi/scale;% Evaluate interpolant.P = zeros(size(xi));for k = 1:N P = P + y(k)*trigcardinal(xi-x(k),N);endfunction tau = trigcardinal(x,N)ws = warning('off','MATLAB:divideByZero');% Form is different for even and odd N.if rem(N,2)==1 % odd tau = sin(N*pi*x/2) [...]/ (N*sin(pi*x/2));else % even tau = sin(N*pi*x/2) [...]/ (N*tan(pi*x/2));endwarning(ws)tau(x==0) = 1; % fix value at x=0 ...|$|E
40|$|We {{construct}} a fast algorithm for the computation of discrete Gauss transforms with complex parameters, capable {{of dealing with}} non <b>equispaced</b> points. Our algorithm {{is based on the}} fast Fourier transform at non <b>equispaced</b> knots and requires only O(N) arithmetic operations. Key words and phrases. Gauss transform; Unequally spaced Fourier transforms; Fast algorithms; Chirped Gaussian; NFF...|$|E
40|$|This paper {{addresses}} {{the design of}} MSE-optimal preambles for multicarrier channel estimation under a maximum likelihood or minimum mean squared error criterion. The derived optimality condition gives insight on how to allocate the pilots that compose the preamble. While many papers show that <b>equispaced</b> and equipowered allocation is optimal, the generalized condition demonstrates that there exist many different configurations that offer the same optimal performance. Furthermore, the condition applies not only to maximum likelihood but also to minimum mean squared error channel estimation. An application of the generalized condition {{in the presence of}} inactive subcarriers (virtual subcarriers problem) is shown such that a non <b>equispaced</b> allocation can achieve the same optimal performance as if an <b>equispaced</b> one could be used. info:eu-repo/semantics/publishe...|$|E
40|$|This paper {{addresses}} {{the problem of}} set membership system identification with quantized measurements. Following the work developed for binary measurements, the problem of optimal input design with multiple sensor thresholds is tackled. For a FIR model of order n, the problem is decomposed into n static gain problems. The one-step optimal input problem is solved both for <b>equispaced</b> and generic sensor threshold distribution. Moreover, the N-step optimal input problem for the case of <b>equispaced</b> thresholds is addressed, and a solution is provided under a suitable assumption on the sensor range and resolution. The obtained results allow us to construct an upper bound on the time complexity of the FIR identification problem for the case of <b>equispaced</b> thresholds. Numerical application examples are reported to show {{the effectiveness of the}} proposed algorithms...|$|E
40|$|Abstract—Digital {{architectures}} for Chebyshev interpolation are explored and {{a variation}} which is word-serial in nature is proposed. These architectures are contrasted with <b>equispaced</b> system structures. Further, Chebyshev interpolation scheme {{is compared to}} the conventional <b>equispaced</b> interpolation vis-á-vis reconstruction error and relative number of samples. It is also shown {{that the use of}} a hybrid (or dual) Analog to Digital converter unit can reduce system power consumption by as much as 1 / 3 rd of the original. I...|$|E
40|$|Digital {{architectures}} for Chebyshev interpolation are explored and {{a variation}} which is word-serial in nature is proposed. These architectures are contrasted with <b>equispaced</b> system structures. Further, Chebyshev interpolation scheme {{is compared to}} the conventional <b>equispaced</b> interpolation vis-a-vis reconstruction error and relative number of samples. It is also shown {{that the use of}} a hybrid (or dual) Analog to Digital converter unit can reduce system power consumption by as much as 1 / 3 rd of the original. Comment: 4 pages, 3 figures, draf...|$|E
3000|$|... meets {{conditions}} (C 6), (C 7), and (C 8) that mandate {{the usage}} of equipowered, <b>equispaced</b> pilots at each transmit antenna. In (D. 11), {ϕ [...]...|$|E
40|$|The {{sampling}} theorem for bandlimited functions {{allows one}} to reconstruct exactly a function containing no frequencies higher than W cps, given {{the values of the}} function at <b>equispaced</b> sampling points (R + 1) /W sec apart. This theorem is generalized to allow reconstruction, given the values of the function and its first R derivatives at <b>equispaced</b> sampling points, (R + 1) / 2 W sec apart. For large R, the R-derivative expansion approaches a Taylor's series weighted by a Gaussian density about each sample point...|$|E
40|$|We {{show that}} for nonparametric {{regression}} if the samples have random uniform design, the wavelet method with universal thresholding {{can be applied}} directly to the samples {{as if they were}} <b>equispaced.</b> The resulting estimator achieves within a logarithmic factor from the minimax rate of convergence over a family of Holder classes. Simulation result is also discussed. Keywords: wavelets, nonparametric regression, minimax, adaptivity, Holder class. AMS 1991 Subject Classification: Primary 62 G 07, Secondary 62 G 20. 1 Introduction Wavelet shrinkage methods have been very successful in nonparametric regression. But so far most of the wavelet regression methods have been focused on <b>equispaced</b> samples. There, data are transformed into empirical wavelet coefficients and threshold rules are applied to the coefficients. The estimators are obtained via the inverse transform of the denoised wavelet coefficients. The most widely used wavelet shrinkage method for <b>equispaced</b> samples is the Donoho-Joh [...] ...|$|E
40|$|The current {{research}} on wavelet regression has been mostly focused on <b>equispaced</b> samples. In general nonequispaced samples require different treatment. And the currently available wavelet methods for nonequispaced samples are relatively difficult to implement. In the present paper, we consider samples with random uniform design. We show {{that if the}} samples have random uniform design, the universal thresholding method can be applied directly to the samples {{as if they were}} <b>equispaced.</b> The resulting estimator achieves within a logarithmic factor from the minimax rate of convergence over a family of Holder classes. Simulation results also show that the mean squared error for samples with random uniform design is comparable to that for samples with <b>equispaced</b> design. Keywords: wavelets, nonparametric regression, minimax, adaptivity, Holder class. AMS 1991 Subject Classification: Primary 62 G 07, Secondary 62 G 20. 1 Introduction Wavelet shrinkage methods have been very successful in nonpar [...] ...|$|E
40|$|Rotating {{machines}} {{can generate}} signals composed of sinusoids <b>equispaced</b> in frequency. The spectrum of such signals {{is useful in}} detecting faults. Traditional Fourier [...] based spectrum estimation methods are often not sufficient due to the nonstationarity of measured signals. We propose maximum likelihood method for estimating the constant spacing of frequencies, {{when the number of}} samples is to be minimised. The spacing can be used to estimate the spectrum of the <b>equispaced</b> signal. Minimizing the measurement time reduces the undesirable effects of nonstationarity because short measurements are often almost stationary. The application of the maximum likelihood method is feasible because the <b>equispaced</b> property of the signal allows easy maximization of the likelihood function in one dimensional parameter space. Using this approach, the number of samples required for reliable estimation of the power spectrum can be greatly reduced compared to the Fourier [...] based methods. 1 INTRODUCTION Fo [...] ...|$|E
40|$|R topics documented: anova. grouped [...] 2 <b>equispaced</b> [...] 3 grouped [...] . 4 plot. grouped [...] . 6 power. grouped [...] 7 residuals. grouped [...] 10 {{rounding}} [...] . 1...|$|E
40|$|This paper derives {{the exact}} {{discrete}} model (EDM) of a kth-order system of stochastic differential equations {{driven by a}} vector fractional noise under fixed initial conditions. The EDM {{can be used for}} the Gaussian estimation and forecasting with long-memory discrete-time <b>equispaced</b> data. Detailed formulae which are necessary for the construction and numerical evaluation of the Gaussian likelihood under two observation schemes are established. State variables can be observed either at <b>equispaced</b> points in time or as integrals over the observational interval. Copyright 2008 The Author. Journal compilation 2008 Blackwell Publishing Ltd...|$|E
40|$|<b>Equispaced</b> beam {{arrangements}} are typically used for IMRT plans. This beam arrangement provides adequate dose coverage {{to the target}} while sparing dose to other structures. However, an <b>equispaced</b> beam arrangement may not provide the best dose coverage to the target while sparing dose to the other structures. Beam angle optimization attempts to optimize the beam directions to produce a better IMRT plan; this is achieved by increasing dose to the target while minimizing dose to the remaining structures. Most methods of beam angle optimization attempt to optimize the beam angles and the beam intensity profiles to find an optimal set of beam angles. This thesis attempts to optimize the beam angles without determining the beam intensity profiles. An MCNP simulation is run to score the beam directions; the simulation is run as an adjoint problem to reduce simulation time, with the target as the source and the detectors scoring the dose for the gantry angles of the beam. Then, an optimization algorithm is run to select a set of beam angles for an optimized IMRT plan. The optimized IMRT plan is compared to an <b>equispaced</b> IMRT plan on a commercial treatment planning system to determine if this method of beam angle optimization is better than using an <b>equispaced</b> beam arrangement. The results of this thesis indicate that the coupling of an MCNP simulation for scoring with an optimization algorithm to select beam angles will produce a better IMRT plan than an <b>equispaced</b> IMRT plan. Three different geometries were used and for all geometries, the optimized IMRT plan had a higher average dose to the target while maintaining or increasing dose sparing to the critical structure and normal tissue. M. S. Committee Chair: de Oliveira, Cassiano; Committee Member: Elder, Eric; Committee Member: Fox, Tim; Committee Member: Hertel, Nola...|$|E
40|$|A signal {{composed}} of sinusoids <b>equispaced</b> in frequency appears in certain applications. One such applicationis fault detection of rotating machines. We propose maximum likelihood method for estimating the spacing of frequencies, {{when the number}} of samples is to be minimised. The application of maximum likelihood method is feasible because the <b>equispaced</b> property of the signal allows easy maximization of the likelihood function in one dimensional parameter space. Using this approach, the number of samples required for reliable estimation of the power spectrum can be reduced dramatically compared to the Fourier-based methods used previously, such as the Welch method...|$|E
40|$|Stable {{high-order}} {{linear interpolation}} schemes are {{well suited for}} the accurate approximation of antiderivatives {{and the construction of}} efficient quadrature rules. In this paper we utilize for this purpose the family of linear barycentric rational interpolants by Floater and Hormann, which are particularly useful for interpolation with <b>equispaced</b> nodes. We analyze the convergence of integrals of these interpolants to those of analytic functions as well as functions with a finite number of continuous derivatives. As a by-product, our convergence analysis leads to an extrapolation scheme for rational quadrature at <b>equispaced</b> nodes. Furthermore, as a main application of our analysis, we present and investigate a new iterated deferred correction method for the solution of initial value problems, which allows to work efficiently even with large numbers of <b>equispaced</b> data. This so-called rational deferred correction (RDC) method turns out to be highly competitive with other methods relying on more involved implementations or non-equispaced node distributions. Extensive numerical experiments are carried out, comparing the RDC method to the well established spectral deferred correction (SDC) method by Dutt, Greengard and Rokhlin...|$|E
3000|$|... th OFDM symbol. The comb-type pilot pattern [4] {{is adopted}} in this paper. The pilot subcarriers are <b>equispaced</b> {{inserted}} into each OFDM symbol. It {{is assumed that}} the number of the total pilot subcarriers is [...]...|$|E
40|$|International audienceThis paper {{regards the}} problem of optimally placing {{unreliable}} sensors in a one-dimensional environment. We assume that sensors can fail with a certain probability and we minimize the expected maximum distance between any point in {{the environment and the}} closest active sensor. We provide a computational method to find the optimal placement and we estimate the costs of the <b>equispaced</b> placement and of the uniform random placement. When the number of sensors goes to infinity, the <b>equispaced</b> placement is asymptotically equivalent to the optimal placement (that is, the ratio between their costs converges to one), whereas the cost of the random placement remains strictly larger...|$|E
40|$|International audienceThe {{synthesis}} of optimal narrow main beam (toward broadside) low sidelobe level linear array is addressed. Only {{the number of}} radiating elements of the linear array is constrained. Their positions and the weightings of the elements are left free. To our knowledge this problem has never been addressed from a theoretical point of view. If the desired main lobe width is reasonably narrow {{with respect to the}} number of elements, the optimal array has <b>equispaced</b> elements with no holes. It is only when the main lobe width is quite small that a larger aperture is beneficial and that some holes appear in an otherwise <b>equispaced</b> array...|$|E
40|$|This paper⇤ {{addresses}} {{the design of}} MSE-optimal preambles for multicarrier channelestimation under a maximum likelihood or minimum mean squared error criterion. The derived optimality condition gives insight on how to allocate the powerof the pilots that compose the preamble. While many papers show that equispacedand equipowered allocation is optimal, the generalized condition demonstratesthat there exist many different configurations that offer the same optimalperformance. Furthermore, the condition applies not only to maximum likelihoodbut also to minimum mean squared error channel estimation. An application ofthe generalized condition {{in the presence of}} inactive subcarriers (virtual subcarriersproblem) is shown such that a non <b>equispaced</b> allocation can achieve thesame optimal performance as if an <b>equispaced</b> one could be used. info:eu-repo/semantics/publishe...|$|E
