37|0|Public
50|$|Les gouvernements des deux républiques précédentes (tout au moins après la chute de Mac-Mahon) devaient toute leur légitimité au parlement, le président de la République ne faisant guère que les {{proposer}} aux assemblées, auxquelles il devait d'ailleurs lui aussi sa fonction. Un soutien {{trop faible}} du parlement, même sans que la censure soit votée, les conduisait souvent à démissionner. Le président de la Ve République a sa légitimité propre et considère, hors période de cohabitation, que le gouvernement est le sien et est responsible devant lui, ce qui mène souvent à faire un parallèle entre la Ve République et les régimes parlementaires dualistes de la première moitié du XIXe siècle. La responsabilité devant le parlement subsiste, mais le gouvernement peut s'estimer légitime tant qu'il n'est pas renversé dans les formes. Si la censure est votée, elle peut s'interpréter comme un conflit entre les légitimités, toutes deux fondées sur l'élection, du président de la République et de l'Assemblée, conflit qui, explique de Gaulle, L'assemblée ne doit plus alors renverser le gouvernement qu'en raison d'un désaccord majeur, et le président de la République pour faire trancher le conflit de légitimité « peut recourir à la nation », en prononçant la dissolution de l'Assemblée nationale (article 12). Il s'agit d'un de ses pouvoirs propres, il n'y a aucune condition que de pure forme à sa mise en œuvre, et il a été <b>effectivement</b> utilisé la seule fois où la censure a été votée. La constitution veille d'ailleurs à interdire la censure lorsque la dissolution n'est pas possible, pendant la vacance ou l'empêchement de la présidence (article 7). La possibilité, voire la probabilité de cette dissolution peut avoir une forte valeur dissuasive sur l'Assemblée, beaucoup de députés risquant d'y perdre leur siège.|$|E
40|$|International audienceWe provide {{essentially}} optimal, effective {{conditions to}} ensure that, when available, the Halberstam–Richert upper {{bound for the}} mean value of a non-negative multiplicative function actually furnishes the true order of magnitude. This is applied, in particular, to short sums of multiplicative functions over arithmetic progressions, to exponential sums with multiplicative coefficients, and to strong law of large numbers with multiplicative weights. Nous donnons des conditions suffisantes effectives quasi-optimales pour que la majoration de Halberstam–Richert fournisse <b>effectivement</b> l'ordre de grandeur de la valeur moyenne d'une fonction multiplicative positive ou nulle...|$|E
40|$|International audienceThe {{territorial}} engineering aims {{at increasing}} {{the quality of}} projects on which it has been used. The Content Analysis of 150 application Files to a national grant and labeling program and the carrying out of 75 interviews show however that, although the files which are the most filled with territorial engineering are actually the best and are acknowledged as such, {{they are also the}} ones which experiment mostly difficulties of implementation. L'ingénierie territoriale vise à accroître la qualité des projets sur lesquels elle est déployée. L'analyse de contenu de 150 dossiers de candidature à un programme national de subvention et de labellisation et la conduite de 75 entretiens semi-directifs montre toutefois que, si les dossiers les plus riches en ingénierie territoriale sont <b>effectivement</b> les " meilleurs " et sont identifiés comme tels, ce sont aussi ceux qui connaissent le plus de difficultés de réalisation...|$|E
40|$|National audienceIn {{this paper}} we present the {{mechanisms}} we implement within the OpenMASK framework {{with the aim}} to make more easy the 3 D interactions within virtual worlds. We are interested in technical objects {{with a lot of}} constraints during the interactions. We talk about the generic mechanisms used to interact with virtual objects and to make the users aware of the interaction services offered by the interactive objects, before and during the interactions. Finally we explain how we take into account the need to make the users aware of other users interactions within collaborative virtual universes. Nous présentons ici les mécanismes que nous implémentons au-dessus de la plate-forme Open- MASK dans le but de faciliter l'interaction dans les mondes virtuels 3 D, en particulier lorsqu'ils sont peuplés d'objets à forte composante technique, c'est-à-dire imposant de fortes contraintes lors des interactions 3 D. Nous présentons les mécanismes génériques utilisés pour permettre d'interagir avec des objets ainsi que pour obtenir des informations permettant de visualiser le plus explicitement possible les possibilités d'interactions offertes par les objets ainsi que les interactions <b>effectivement</b> en cours. Nous terminons par la façon de rendre explicite les interactions des autres utilisateurs dans le cas des univers 3 D coopératifs...|$|E
40|$|Résumé: On donne une généralisation à la {{dimension}} supérieure des résultats obtenus par Birkhoff et Mather sur l’existence d’orbites errant dans les zones d’instabilité des applications de l’anneau déviant la verticale. Notre généralisation s’inspire fortement de celle proposée par Mather dans [7]. Elle présente cependant l’avantage de contenir <b>effectivement</b> l’essentiel des resultats de Birkhoff et Mather sur les difféomorphismes de l’anneau. Abstract: We generalize {{to higher}} dimension results of Birkhoff and Mather {{on the existence}} of orbits wandering in regions of instability of twist maps. This generalization is strongly inspired by the one proposed by Mather in [7]. However, its advantage is that it contains most of the results of Birkhoff and Mather on twist maps. A very natural class of problems in dynamical systems is the existence of orbits connecting prescribed regions of phase space. There are several important open questions in this line, like the one posed by Arnold: Is a generic Hamiltonian system transitive on its energy shells? Birkhoff’s theory of regions of instability of twists maps, recently extended by Mather using variational methods and by Le Calvez, provide very relevant results in that direction. In short, these works establish the existence, for a certain class of mappings of the annulus...|$|E
40|$|International audienceRelations between China and African {{countries}} {{are getting better}} known even if sub-Saharan Africa is more often documented than Maghreb. The organisation of strikes by Algerians workers employed by Chinese firms or the repatriation of Chinese workers from building sites in Libya has been widely discussed. On the other hand, little is said about the part China might effectively play in the economy of Maghreb countries and about {{the diversity of the}} situations of these countries in their economic relations with China. This is the issue I shall deal with on the basis of available statistics. Successively, I shall attend {{to the question of the}} Chinese direct investment, the exports of products from the Maghreb and the imports of Chinese products. Les relations que la Chine entretient avec l’Afrique commencent être mieux documentées même s’il est plus souvent question de l’Afrique sub-saharienne que du Maghreb. Certes, les grèves en Algérie de salariés algériens des entreprises chinoises et le rapatriement de travailleurs chinois enLibye sont connus. En revanche, peu est dit du rôle que la Chine pourrait <b>effectivement</b> jouer dans l’économie de ces pays et de la diversité des situations de ces pays dans leurs relations économiques avec la Chine. C’est la question que nous tenterons de traiter sur la base des statistiques disponibles. Successivement, nous aborderons l’investissement direct chinois, les exportations de produits maghrébins et les importations de produits chinois...|$|E
40|$|Cette thèse est une {{contribution}} à l'analyse de l'action médico-sociale comme forme particulière d'intervention. À partir du terrain des consultations gratuites de protection infantile, il s'agit d'interroger la prévention médico-sociale en acte et d'observer les décalages et tensions qui existent entre législation, organisation des consultations, positionnement professionnel des intervenants, et problématique spécifique du public auquel ils s'adressent en priorité. L'analyse des enjeux que recouvre l'articulation médical/ social et celle des éléments médicaux et sociaux <b>effectivement</b> mobilises en situation révèle que la prévention se réalise, s'invente et se redéfinit nécessairement au quotidien. This {{thesis is}} {{a contribution to}} the analysis of the medico-social action as a particular type of intervention. It is based on a field work in the free consultations of "child protection" (protection infantile in French) in France, and questions the medico-social prevention while being "performed". 1 bis research also observes the gaps and tensions which exist between legislation, organization of the consultations, professional positions taken by the protagonists, and the specific issue of the public addressed in priority by these consultations. There are numerous stakes within the articulation of medical and social, as well as various medical and social elements raised in the real-life situations. The analysis of these stakes and elements reveals that prevention is crafted, invented, and necessarily redefined in daily interactions. PARIS 3 -BU (751052102) / SudocSudocFranceF...|$|E
40|$|An art. ̇ rk {{does not}} {{obliterate}} {{the traces of}} its making. In the contemporary technological context such &quot;untidiness &quot; is cffectively erased through instrumental methods of production. A great (livide exists between instrumental and poetic modalities of disclosure which is no more apparent than in the arts of cinema and architecture. The motion picture camera is the emblem par excellence of the voyeuristic sensibility of the disembodied 20 &quot; ' centiiry spectator who inevitably is the very same &quot;body &quot; which inhabits the built world. The British artist-filminaker Peter Greenaway calls into question numerous aspects of traditional and technological ways of looking and making relevant to architectural creation as similarly constitutivc of both functional and poetic realms. Rather tlian suppress or deny the traces left upon a work, he valorizes and amplifies them IO create a self-reflexivity between work and working as an intertwining {{of the process and}} work horizons. His ultimate concern is for &quot;conteni &quot; or poetic meaninfi as the aestlietic experience of depth which he sees as a matter of the iorm and style of mnking. Une oeuvre d'art n'oblitère pas les traces de sa fabrication. Dans Ic contexte technologique contemporain, un tel &quot;désordre &quot; est <b>effectivement</b> efbcé A travers les méthodes instrumentales de production. Une importante division existe entre les modalités instrumentale et poétique d...|$|E
40|$|This {{thesis is}} a {{contribution}} to the analysis of the medico-social action as a particular type of intervention. It is based on a field work in the free consultations of “child protection” (protection infantile in French) in France, and questions the medico-social prevention while being "performed". This research also observes the gaps and tensions which exist between legislation, organization of the consultations, professional positions taken by the protagonists, and the specific issue of the public addressed in priority by these consultations. There are numerous stakes within the articulation of medical and social, as well as various medical and social elements raised in the real-life situations. The analysis of these stakes and elements reveals that prevention is crafted, invented, and necessarily redefined in daily interactions. Cette thèse est une contribution à l’analyse de l’action médico-sociale comme forme particulière d’intervention. À partir du terrain des consultations gratuites de protection infantile, il s’agit d’interroger la prévention médico-sociale « en acte » et d’observer les décalages et tensions qui existent entre législation, organisation des consultations, positionnement professionnel des intervenants, et problématique spécifique du public auquel ils s’adressent en priorité. L’analyse des enjeux que recouvre l’articulation médical/social et celle des éléments médicaux et sociaux <b>effectivement</b> mobilisés en situation révèle que la prévention se réalise, s’invente et se redéfinit nécessairement au quotidien...|$|E
40|$|The {{purpose of}} this thesis is to analyze the {{function}} of the French adverbial collocation en effet by applying contrastive bidrectional analysis on corpus material. The goal is to find out which factors influence its functions and meanings. En effet has two main roles: it either expresses the cause and works as an argumentative connector between sentences or it is an adverb of confirmation. This double role is reflected in the Czech language as well, where the equivalents of en effet can be totiž as well as skutečně. This paper aims to find out which function of en effet prevails in which conditions and whether the equivalents offered in Czech-French dictionaries correspond to the real practical use of en effet. The analysis is done on four types of texts - novels, newspaper articles, law texts acquis communautaire of the EU and the speeches given by the EU MPs in the European parliament. Four factors are analysed: frequency, position and argumentative/confirmative type of en effet in the texts as well as its Czech translations. This will enable us to contrast en effet with its potential synonyms <b>effectivement</b> and en fait. The French corpus Frantext as well as the Czech-French and French-Czech part of the parallel corpus InterCorp are used for the analysis. The first part of the thesis deals with [...] ...|$|E
40|$|The {{need for}} {{developing}} particular skills {{for working with}} clients who are racially or culturally dissimilar from the counsellor is becoming increasingly apparent; especially in the Canadian society. Problems for both client and counsellor do occur in the cross-cultural situation. The more frequent difficulties are discussed and recommendations for ways in which counsellors and counsellor educators can meet this challenge are discussed. Resume Il devient de plus en plus impérieux, spécialement au sein de la société cana-dienne, d'acquérir des habiletés particulières en vue de travailler auprès de clients qui, d'un point de vue racial ou culturel, sont différents de leur conseiller. La relation de nature interculturelle est <b>effectivement</b> problématique à Ia fois pour le client et le conseiller. On trouve dans le présent article une discussion des difficultés les plus fréquentes ainsi que des recommandations quant à diverses façons auxquelles peuvent recourir les conseillers et les formateurs en vue de relever un tel défi. The {{purpose of this paper}} is to examine some of the problems which occur when the culturally or racially dissimilar individual comes in contact with a counsellor, especially a counsellor who is a member of the majority group. The contact may occur between the client and a counsellor in a variety of settings e. g. school, manpower offices, social service agencies etc. Some recommendations for ways in which counsellors can better meet the needs of these clients are also presented. Methods and means of counselling minori-ty group clients have been less than satisfactory Reprint requests should be sent to Marvin J. West...|$|E
40|$|International audienceThe {{occurrence}} {{of human and}} veterinary pharmaceuticals in the rivers flowing through watershed draining agricultural soils is of growing concern. Veterinary drugs are particularly widespread in intensive livestock watershed. Nevertheless, in these contexts, their occurrence {{has not yet been}} extensively studied. To develop analytical methods dedicated to the analysis of those veterinary pharmaceuticals, a prioritization of the molecules of interest is necessary. Surveys have been conducted with cattle, pigs and poultry veterinarians working on two watershed. The identification of the prescribed molecules and their hierarchy has been performed thanks to an indicator of the level of global prescription and has led to a list of 60 prioritized molecules to analyze in the mixed-use watershed. Among those molecules, 27 have been effectively analyzed in the superficial water from two watershed in Brittany. The quantification of 5 veterinary drugs (sulfamethazine, flunixin, enrofloxacin, tylosine and eprinomectin) in the rivers evidenced the {{occurrence of}} pharmaceuticals from livestock origin in the mixed-use watersheds. La contamination des eaux par les résidus médicamenteux d’origine vétérinaire a été peu étudiée en contexte d’élevage intensif. Des enquêtes ont été réalisées auprès de vétérinaires spécialisés bovins lait, porcins et volaille de chair sur deux bassins versants. L’identification des molécules prescrites et leur hiérarchisation à partir d’enquêtes auprès des vétérinaires ont permis de définir une liste d’environ 60 molécules à rechercher dans les eaux superficielles, appartenant à différentes classes thérapeutiques. Seules 27 d’entre elles ont pu être <b>effectivement</b> recherchées, pour des raisons de coûts et de faisabilité dans les laboratoires. La quantification de cinq molécules à usage exclusivement vétérinaire (la sulfaméthazine, la flunixine, l’enrofloxacine, la tylosine et l’éprinomectine) confirme la présence dans les eaux superficielles de résidus médicamenteux d’origine agricole...|$|E
40|$|DANS UNE USINE DE PRODUCTION AUTOMOBILE, ON CREE UNE LISTE EN ORDONNANCANT LES COMMANDES ARRIVANT A L'USINE DE FACON A OPTIMISER GLOBALEMENT LES COUTS DE PRODUCTION. CETTE LISTE DOIT PRENDRE EN COMPTE UNE GRANDE PARTIE DES CONTRAINTES DES ATELIERS ET NOTAMMENT CELLES DU MONTAGE. ELLE SERT AUSSI A DONNER DES INFORMATIONS AUX FOURNISSEURS POUR QU'ILS FABRIQUENT LES COMPOSANTS NECESSAIRES A LIVRER A L'ATELIER MONTAGE. PSA A POUR OBJECTIF TRES AMBITIEUX QUE CETTE LISTE SOIT <b>EFFECTIVEMENT</b> EN ENTREE DE L'ATELIER DE MONTAGE, C'EST A DIRE DE REALISER UNE GESTION GLOBALEMENT FIFO DES DEUX ATELIERS AMONT DU MONTAGE (FERRAGE, PEINTURE). CECI PERMETTRAIT EN PARTICULIER DE DIMINUER SENSIBLEMENT LES STOCKS DE COMPOSANTS. MAIS REALISER UNE GESTION FIFO POSE BEAUCOUP DE PROBLEMES. TOUT D'ABORD, DANS L'USINE, ON DOIT CONSTRUIRE DES ORDONNANCEMENTS POUR SATISFAIRE LES CONTRAINTES PLUS SPECIFIQUES A CHACUN DES ATELIERS (FERRAGE, PEINTURE, MONTAGE), APPELES CADENCEMENT. ENSUITE DES PERTURBATIONS MODIFIENT EGALEMENT L'ORDRE INITIAL. LA LISTE EST ALORS SENSIBLEMENT PERTURBEE ET IL FAUT DONC LA RECONSTRUIRE, C'EST A DIRE LA RESEQUENCER. L'OBJECTIF DE CETTE THESE EST DE DEFINIR UNE NOUVELLE POLITIQUE DE GESTION DES FLUX BAPTISEE CADENCEMENT RESEQUENCABLE. PSA AVAIT TRAVAILLE SUR LE MOYEN DE RECONSTRUIRE CETTE LISTE EN ENTREE MONTAGE (LE RESEQUENCEMENT). NOTRE TRAVAIL EST LA SUITE DIRECTE : IL CONSISTE A DEFINIR, EN FONCTION DES PERTURBATIONS, LE CADENCEMENT MAXIMAL QUE L'ON EST AUTORISE A FAIRE EN ENTREE DE CHAQUE ATELIER AFIN DE GARANTIR LA REMISE EN ORDRE EN ENTREE MONTAGE. NOUS AVONS ETUDIE LES PERTURBATIONS DANS LES USINES A L'AIDE DE NOUVEAUX INDICATEURS. EN S'APPUYANT SUR CES RESULTATS, NOUS AVONS ETE AMENES A PROPOSER UNE NOUVELLE DEFINITION DE LA REMISE EN ORDRE DE LA LISTE. NOUS AVONS ENSUITE ETUDIE DES PROBLEMES THEORIQUES. NOUS AVONS AINSI LES OUTILS DE BASE. NOUS AVONS ALORS ANALYSE LEUR MISE EN UVRE DANS LA STRUCTURE COMPLEXE D'UNE USINE AUTOMOBILE. GRENOBLE 1 -BU Sciences (384212103) / SudocSudocFranceF...|$|E
40|$|International audienceThis {{introductory}} {{article has}} a twofold purpose: it first outlines {{the current state}} of the historiography on the early Irish republicanism; it then underlines the new approaches of the papers presented at the conference organised at the IHRF in May 2016, and available now. To achieve this purpose, this introduction starts with an examination of the memory and historiography of the Irish republicanism that emerged in the 1790 s. Associated with revolutionary France, it has then been claimed by nationalist movements throughout the 19 th and 20 th centuries. Hence, it was associated with armed struggle and Catholicism. Going back to the period of the Atlantic Revolutions, the different contributions presented here analyse the emergence of Irish republicanism in its synergies with America and France, shedding new light on how the different strands of discontentment and multiple aspirations at the end of the 18 th century crystallised into a horizon of expectation called the Republic, which was indeed synonymous with nationalism, though it then meant independence and popular sovereignty, i. e. liberty. Cette introduction cherche à dresser un bilan historiographique sur le premier républicanisme irlandais afin de souligner dans quelle optique se situaient les contributions à la journée d’étude organisée sur ce thème à l’IHRF en mai 2016, ici présentées. Pour cela, elle pose d’emblée la question des écrans mémoriels et historiographiques qui, s’ils associent les origines du républicanisme irlandais à la France révolutionnaire, ont, depuis les XIXe et XXe siècles, chargé ce mot d’une connotation nationaliste, elle-même associée à la lutte armée et au catholicisme. Remonter à la période de l’Atlantique des révolutions a permis d’analyser l’émergence du républicanisme irlandais dans ses synergies avec l’Amérique et la France, et ainsi de mieux comprendre comment les multiples mécontentements et aspirations de la fin du XVIIIe siècle se sont cristallisés pour dessiner un horizon d’attente appelé République et qui, s’il était <b>effectivement</b> synonyme de nationalisme, signifiait alors indépendance et souveraineté populaire, c’est-à-dire liberté...|$|E
40|$|Article dans revue scientifique avec comité de lecture. internationale. International audienceThis paper {{addresses}} {{the issue of}} how to design online help that will really prove effective, accessible, and usable for all categories of users in the coming Information Society and, most of all, that will actually be used by novice users. The paper demonstrates the intrinsic necessity of online help and the actual failure of approaches claiming that "transparent" user interfaces eliminate the need for online support chiefly {{on the grounds that they}} encourage exploration. Empirical results in the literature or stemming from analyses of data we collected are put forward in the discussion. Based on a brief survey of the relevant literature, the major specific design issues that designers of online help systems are confronted with are presented, existing design approaches that might contribute to solving these issues are discussed, and a realistic short-term approach for improving the accessibility, effectiveness, and usability of online systems is recommended. Our recommendation is mainly based on a recently performed experimental study. These results led us to advise, at least for the near future, the design of noncontextual help systems rather than the implementation of dynamic adaptation to the current user's cognitive profile or the development of contextual help systems that generate the information content of help messages dynamically according to the user's current intention and goal. We assume that it is possible, within the framework of universal design principles, to significantly enhance the effectiveness and usability of standard noncontextual help systems, mainly by making the most of the recent advances in research on multimodal interaction, especially on the integration of speech into input modalities. || Cet article porte sur la conception de systèmes d'aide en ligne qui soient efficaces, accessibles et utilisables, et surtout, qui soient <b>effectivement</b> utilisés. Nous démontrons d'abord la nécessité des aides en ligne à partir des échecs des approches cen...|$|E
40|$|International audiencemiRNAs {{regulate}} {{gene expression}} by binding with mRNAs of many genes. Studying {{their effects on}} genes involved in oncogenesis is important in cancer diagnostics and therapeutics. The RNAHybrid 2. 1 program was used to predict the strong miRNA binding sites (p < 0. 0005) in target mRNAs. The program Finder 2. 2 was created to verify 784 intergenic miRNAs (ig-miRNA) origin. Among 54 considered oncogenes and tumor suppressor genes, 47 genes are the best targets for ig-miRNAs. Accordingly, these genes are strongly regulated by 111 ig-miRNAs. Some miRNAs bind several mRNAs, and some mRNAs have several binding sites for miRNAs. Of the 54 mRNAs, 21. 8 %, 43. 0 %, and 35. 2 % of the miRNA binding sites {{are present in the}} 5 'UTRs, CDSes, and 3 'UTRs, respectively. The average density of the binding sites for miRNAs in the 5 'UTR was 4. 4 times and 4. 1 times greater than in the CDS and the 3 'UTR, respectively. Three types of interactions between miRNAs and mRNAs were identified, which differ according to the region of the miRNA bound to the mRNA: 1) binding occurs predominantly via the 3 '-region of the miRNA; 2) binding occurs predominantly through the central region of the miRNA; and 3) binding occurs predominantly via the 5 '-region of the miRNA. Several miRNAs effectively regulate only one gene, and this information could be useful in molecular medicine to modulate translation of the target mRNA. We recommend described new sites for validation by experimental investigation. miRNAs regulent l'expression des genes par fixation avec le mRNAs de nombreux gènes. L'étude de leur effet sur les gènes impliqués dans l'oncogénèse est importante pour le diagnostic du cancer et sa thérapeutique. Le programme RNAHybrid 2. 1 a été utilisé pour prédire des sites de fixation forts de miRNA (p < 0. 0005) dans des mRNAs cibles. Le program Finder 2. 2 a été écrit pour vérifier l'origine 784 miRNAs intergeniques (ig-miRNA). Plusieurs miRNAs ne reegulent <b>effectivement</b> qu'un seul gène et cette information pourrait être utile en médecine moléculaire pour modérer la traduction du mRNA cible...|$|E
40|$|La forme neutre de l’adjectif khalepós, l’adverbe khalepôs, l’adjectif rheídios et les adverbes rheîa / rhéa / rheidíos caractérisent l’action divine et l’action héroïque, dans la mesure où celle-ci peut {{souffrir}} l’intervention négative ou positive de celle-là. Nous soutenons que ces termes traduisent le rapport téléologique entre la parole et l’action. Au contraire des traductions habituelles par ‘facile’, ‘difficile’, ‘facilement’, ‘difficilement’ etc., qui renvoient à l’effort et à l’absence d’effort, nous interprétons ces termes selon l’engagement créé entre la parole et l’action: l'action est souvent précédé par un engagement préalable, par lequel le dieu ou le héros annonce ce qu’il va accomplir. Le héros homérique n'est pas concerné prioritairement par ce qui demande plus ou moins d'effort, mais, d'une part, par ce qui peut ou ne peut pas arriver <b>effectivement</b> et, d'autre part, par ce ce qui trahit son expectative. neuter form of {{the adjective}} khalepos, the adverb khalepôs, the adjective rheídios and the adverbs rheîa / rhéa / rheidíos characterize the divine action and the heroic one, insofar the last one can suffer the negative influence of the former. We propose {{in this study that}} those terms indicate the teleological connection between word and action. Instead of the usual translations 'easy', 'difficult', 'easily', 'with difficulty', etc., which express effort or lack of effort, we interpret those terms according to the connection between word and action: the god and the hero undertake to do something when they announce what they will accomplish. The homeric hero is not mainly concerned by what demands more or less effort, but, on the one hand, by what can or cannot effectively happen and, on the other hand, by what betrays their expectations. According to this perspective, we translate the adverb rheîa / rhéa / rheidíos by 'effectively'; the adjective rheidíos, by 'favourable' / 'effective'; the adjective khalepos, by 'treacherous'; the neuter khalepos and the adverb khalepôs, which express an 'impossible according to the circumstances', by 'improbable' and by the periphrasis 'it is improbable that'. After the main arguments presented in the first two chapters, chapiter 3 examines two particularly controversial passages: the two occurrences of khalepos qualifiyeng the proofs of Heracles (Odyssée, XI, 622 et 624) and the one qualifying divine manifestation (Iliad, XX, 131) ...|$|E
40|$|International audienceIt is now {{generally}} accepted that Pentecost {{is in fact}} the theme of the tympanum of the central portal of the nave of the church at Vézelay - Pentecost, an event which was then regarded, as it still is, as the founding act of the Church. Representations of Apostles are seen at various points on the portal, indicating that the Church is already at work, teaching and preaching. In order to evoke the nature of the Church, the creator of the concept for the portal appears to have drawn on the two final chapters of the Apocalypse (21 and 22) and the relevant exegesis. For a characteristic feature of these texts is that pratically ail the component elements of the portal are therein alluded to, either directly or more or less implicitly. And it is from this source that certain of those elements derive their meaning, namely the waters at the feet of Christ; the throne on which he sits; the figures carved on the embrasure and the representation of John the Baptist; and especially the zodiac with its four insets as well as the two capitals on the right-hand side. It is perfectly possible therefore that the conception of the iconographie message of the portai should have taken shape with those chapters of the Apocalypse in mind. II paraît désormais établi que le tympan du portail central de la nef de Vézelay est <b>effectivement</b> consacré au thème de la Pentecôte, un événement qui, au moyen âge, était déjà conçu, notamment, comme l'acte fondateur de l'Église. En plusieurs endroits du portail, des figures d'apôtres indiquent que l'Église œuvre déjà par la prédication et l'enseignement. Pour évoquer cette institution, le concepteur semble avoir utilisé les deux derniers chapitres de l'Apocalypse (XXI et XXII), ainsi que l'exégèse qui s'y rapporte. Ces textes présentent en effet la particularité de mentionner, de manière plus ou moins explicite, la quasi totalité des composantes du portail. Ils donnent ainsi un sens à la présence de certaines d'entre elles : les ondulations situées aux pieds du Christ, le trône sur lequel ce dernier est assis, les figures des ébrasements et celle de Jean-Baptiste, et surtout le zodiaque, les quatre médaillons qui l'interrompent et les deux chapiteaux de droite. Ces textes ont donc parfaitement pu servir de fil conducteur dans la conception du programme iconographique du portail...|$|E
40|$|Is it true {{that the}} Beijing dialect was Mandarin within the period {{from the end of}} the Yuan dynasty to the {{beginning}} of the Ming dynasty (the second half of the 14 th century) ? Were the Interpreter Piao (《樸通事》, Pi&aacute;o T ōngsh&igrave;) and the Lǎo Qǐd&agrave;(《老乞大》，Lǎo Qǐd&agrave;) written in the Beijing colloquial language? Whether had the College of the Standard Pronunciation (正音書院, Zh&egrave;ngyīn Shūyu&agrave;n) within the period of King Yongzheng of the Qing dynasty (1723 - 1735) given an impulse to shape the common language of the Chinese? All of these questions mentioned before not only are the key tasks, which have deals with the history of the Chinese language of the modern times; but also are the unavoidable questions in researching the contemporary Chinese language. However, the popular, or the so-called &ldquo;mainstream&rdquo; of current opinion looks logical and well argued, but it is specious. Pronunciation is the main basis for estimating the linguistic character of the ancient literature. The Beijing pronunciation (Northern Mandarin) was not the representative of the standard pronunciation of Mandarin of the modern times; the representative was Nanjing pronunciation (Southern Mandarin). The Interpreter Piao and Lǎo Qǐd &agrave; were not written in the Beijing colloquial language. And the College of the Standard Pronunciation did not give an impulse to shape the common language of the Chinese; on the opposite, it delayed the process of shaping. Keywords: Mandarin, Beijing dialect, Nanjing pronunciation, the standard and the colloquial pronunciations, the Old China R&eacute;sum&eacute; Le mandarin est-il repr&eacute;sent&eacute; par le dialecte de P&eacute;kin au quatorzi&egrave;me si&egrave;cle ? Putongshi et Laoqida sont-ils &eacute;crits en dialecte de P&eacute;kin ? &ldquo;L&rsquo;&eacute;cole de ton orthodoxe &rdquo; dans la dynastie de Tsing a-t-elle promu la formation du language commun de nationalit&eacute; Han d&rsquo;aujourd&rsquo;hui ? Voil&agrave; les questions cl&eacute;s concernant l&rsquo;histoire du mandarin contemporain et moderne que les manuels de la langue chinoise moderne ne peuvent pas &eacute;luder. Pourtant, le point de vue majeur ou plut&ocirc;t populaire, est faux malgr&eacute; son apparence bien raisonnable. Les sons du language est la crit&egrave;re principale pour d&eacute;finir la nature languagi&egrave;re des documents anciens. Le symbole de ton orthodoxe du mandarin contemporain n&rsquo;est <b>effectivement</b> pas le ton de P&eacute;kin, mais le ton de Nankin. En Cor&eacute;e, les manuels de chinois Putongshi et Laoqida ne sont pas r&eacute;dig&eacute;s en dialecte de P&eacute;kin. &ldquo;L&rsquo;&eacute;cole de ton orthodoxe &rdquo; dans la dynastie de Tsing n&rsquo;a pratiquement pas promu le processus de formation du language commun de nationalit&eacute; Han d&rsquo;aujourd&rsquo;hui, mais bien au contraire, elle a retard&eacute; ce processus. Mots-cl&eacute;s: mandarin, language de P&eacute;kin, ton de Nankin, ton orthodoxe et ton populaire, Laoqida 摘 要 元末明初漢語官話 &ldquo;以北京話為代表&rdquo;？《樸通事》和《老乞大》 &ldquo;是用北京口語寫的&rdquo;？清雍正朝的&ldquo;正音書院&rdquo;推動了現代漢民族共同語的形成？這是涉及近現代漢語史的關鍵問題，也是現代漢語教材不能回避的問題。然而流行的或曰 &ldquo;主流&rdquo;觀點，看似頭頭是道，實則似是而非。判斷古文獻語言性質的依據主要是語音。近代漢語官話正音的代表，並非北方官話的北京音，而是南方官話的南京音；朝鮮漢語課本《樸通事》和《老乞大》不 &ldquo;是用北京口語寫的 &rdquo;；清雍正朝的 &ldquo;正音書院&rdquo;並未推動、反而延緩了現代漢民族共同語形成的進程。關鍵詞：官話；北京話；南京音；正俗音；《老乞大...|$|E
40|$|The aim of {{this study}} was to {{discover}} whether the working of a micro-organization such as a teaching team in physical education (P. E.) can explain teaching content in the work of each of its members. In order to make the observed contents comparable, we have chosen those whose objectives were to socialize or to train for citizenship in teams of P. E. teachers who had included these themes for P. E. pedagogical project. The theoretical model which we use combines the action logic model of Amblard et al. (1996) and that of Dubet (1994). The former makes it possible to pinpoint the various factors which intervene in the constitution of an action logic, and the latter enable us to distinguish between distinctive, integrating and strategic logics. Our hypothesis is that the dominant action logic in a P. E. team is an integrating one, and that this therefore produces what we have called "physical education team effect", that is an intentional similarity in the majority of the teaching contents chosen by the members of the team. The results show that, in the two junior secondary schools which were studied, there is a dominant logic of the integrating type and which effectively results in a "team effects" in terms of teaching content. However, this team effect is greater in the first school where the integrating orientation has been reinforced, whereas it il less marked in the other school where the teaching team has added a strong strategic orientation (taking into account personal interest) to the initial integrating logic. L'objectif de cette étude est de voir si le fonctionnement de la micro-organisation constituée par une équipe pédagogique d'EPS peut expliquer les contenus d'enseignement de chacun de ses membres. Pour rendre comparables les contenus observés, on étudie ceux qui ont pour objectifs de socialiser et former à la citoyenneté, dans les équipes pédagogiques qui ont inscrit ces thèmes dans leur projet d'EPS. Le modèle théorique retenu associe le modèle des logiques d'action d'Amblard et al. (1996) et celui de Dubet (1994). Le premier permet de cerner les différents facteurs intervenant dans la constitution des logiques d'action, le second permet leur caractérisation en logiques distinctive, intégratrice et stratégique. L'hypothèse est que la logique d'action dominante dans une équipe d'EPS est intégratrice et produit, de ce fait, ce qu'on a appelé un "effet équipe d'EPS", c'est-à-dire une parenté majoritaire et intentionnelle des contenus d'enseignement choisis par les membres de l'équipe. Les résultats font apparaître que, dans les deux collèges étudiés, il existe une logique dominante de nature intégratrice qui aboutit <b>effectivement</b> à un "effet équipe" au niveau des contenus. Mais cet effet équipe est maximum dans le premier collège où l'orientation intégratrice est renforcée, alors qu'il est plus restreint dans le deuxième collège où se superpose à la logique intégratrice primitive une orientation stratégique (c'est-à-dire d'intérêt personnel) importante de la part de l'équipe...|$|E
40|$|CE TRAVAL DE THESE ABORDE DES PROBLEMATIQUES LIEES AU CONTROLE NON DESTRUCTIF (CND) PAR COURANTS DE FOUCAULT, IL EST DECOMPOSE EN TROIS GRANDES PARTIES. LA PREMIERE A ETE CONSACREE A LAMISE EN ŒUVRE DE METHODES D'INVERSION POUR L'ESTIMATION DES PARAMETRES PHYSIQUES ET GEOMETRIQUES DE CIBLES. POUR CE FAIT, NOUS NOUS SOMMES INTERESSES AUX MODELES INVERSES A BASE DE RESEAUX DE NEURONES. DANS CE CONTEXTE DEUX APPLICATIONS ONT ETE ETUDIEES. LES MICRO-BOBINES SONT DEDIEES A PLUSIEURS APPUCATIONS TELLES QUE RADIO FREQUENCE (RF), RESONANCE MAGNETIQUE NUCLEAIRE (RMN), CONTROLE NON DESTRUCTIF (CND). SELON L'APPLICATION, CES MICRO-BOBINES PEUVENT ËTRE UTILISEES EN HAUTE FREQUENCE (HF). <b>EFFECTIVEMENT,</b> LA REPONSE EN HF EST DIFFERENTE DE CELLE EN BF A CAUSE DES EFFETS DE PEAU ET DE PROXIMITE QUI INFLUENT SUR LES PARAMETRES ELECTRIQUES DE LA MICRO-BOBINE. LA RESISTANCE ET L'INDUCTANCE DEPENDENT DE LA FREQUENCE ET LA CAPACITE DE PARASITE NE PEUT ETRE NEGLIGEE. LA SECONDE PARTIE EST CONSACREE A LA MISE EN ŒUVRE D'UNE METHODE ORIGINAL EN COMBINANT ENTRE UNE ANALYSE MAGNETODYNAMIQUE ET UNE ANALYSE ELECTROSTATIQUE POUR LA DETERMINATION DES PARAMETRES ELECTRIQUES DU SCHEMA EQUIVALENT. LE DERNIER ASPECT ABORDE A ETE CONSACRE A L'ELABORATION D'UNE APPROCHE HYBRIDE ASSOSIANT LA METHODE DES ELEMENTS FINIS ET LA METHODE DES INTEGRALES DE FRONTIERES (MIF) POUR CALCULER LA REPONSE DU CAPTEUR EN PRESENCE DE FISSURES. DEUX TYPES DE VALIDATIONS ONT ETE REALISES DANS CE TRAVAIL, UNE COMPARAISON DES RESULTATS OBTENUS PAR LA METHODE DES ELMENTS FINIS AVEC DES RESULTATS FOURNIS PAR LE CEA (LOGICIEL CIVA) ET UNE VALIDATION DE LA METHODE HYBRIDE EN UTILISANT UN DISPOSITIF DE MESURE AU LABORATOIRE. THIS THESIS WORK APPROACHES THE PROBLEMATIC RELATED ONES TO THE NON DESTRUCTIF TESTING (NDT) BY EDDY CURRENT, IT'S DIVIDED INTO THREE GREAT PARTS. THE FIRST PARTS IS CONSECRATED TO IMPLEMENT OF THE INVERSE MODEL FOR THE ESTIMATE OF THE PHYSICAL AND GEOMETRICAL PARAMTERS OF THE TESTED SPICEMEN. FOR THIS FACT, WE WERE INTERESTED IN THE INVERSE MODELS BASED BY NEURAL NETWORKS. IN THIS CONTEXT TWO APPLICATION WERE STUDIED. THE MICRO-COIL ARE DEDICATED FOR DIFFERENT APPLICATIONS : RADIO FREQUENCY (RF), NUCLEAR MAGNETIC RESONANCE (NMR), NON DESTRUCTIVE TESTING (NDT) [...] . DEPENDING ON THE APPLICATIONS, THESE MICRO-COILS CAN BE USED IN HIGH FREQUENCY. ACCTUALLY, THE RESPONSE OF THE MICRO-COIL AT HIGH FREQUENCY IS SIGNIFICANLY DIFFERENT FROM THEIR LOW FREQUENCY RESPONSE BECAUSE OF THE SKIN AND PROXIMITY EFFECTS HAVE AN INFLUENCE ON THE ELECTRICAL PARAMETERS OF THE MICRO-COILS. THE RESISTANCE AND THE INDUCTANCE OF THE WINDING DEPEND ON THE FREQUENCY. THE PARASITIC CAPACITANCE OF THE WINDING CANNOT NEGLECTED. IN THE SECOND PART OF THIS WORK, AN ORIGINAL METHOD COMBINING BY 3 D MAGNETODYNAMIC ALALYSIS AND ELECTROSTATIC ANALYSIS IS PRESENTED TO DETERMINE THE ELEMENTS OF AN ELECTRIC EQUIVALENT CIRCUIT. THE LAST ASPECT STUDIED WAS DEVOTED TO DEVELOPPEMENT OF AN HYBRID APPROCH ASSOCIATING THE FINITE ELEMENT METHOD (FEM) AND THE BOUNDARY INTEGRAL METHOD (BIM) TO CALCULATE THE RESPONSE OF THE COIL IN THE PRESENCE OF THE CRACKS. TWO TYPES OF VALIDATIONS WERE CARRIED OUT IN THIS WORK, A COMPARISON OF THE RESULTS OBTAINED BY FEM WITH RESULTS PROVIDED BY THE CEA (SOFTWARE CIVA) AND A VALIDATION OF THE HYBRID METHOD BY MEASUREMENT RESULTS. ORSAY-PARIS 11 -BU Sciences (914712101) / SudocSudocFranceF...|$|E
40|$|LA DEGRADATION DE QUALITE VIDEO TRANSMISE SUR LIENS MOBILES EST LIEE A L'UTILISATION DES CODES A LONGUEUR VARIABLE (CLV) ET LA METHODE DE DECODAGE UTILISANT LA PROPRIETE DU PREFIXE. PEU DE TRAVAUX EXISTENT SUR LA SEMANTIQUE COMPLETE PROPRE AUX DONNEES VIDEO ET SUR L'ANALYSE & LA QUANTIFICATION DE CES REDONDANCES RESIDUELLES. UNE METHODE ORIGINALE EST PROPOSEE ICI POUR ANALYSER ET QUANTIFIER CES REDONDANCES RESIDUELLES SOURCES: CELLES DUE A LA STRUCTURE DES CLV ET A LA SEMANTIQUE DES DONNEES VIDEO. NOUS MONTRONS QUE LA REDONDANCE RESIDUELLE DANS LES DONNEES VIDEO COMPRESSEES RESTE SIGNIFICATIVE; ET QU'EN UTILISANT LA SEMANTIQUE DES DONNEES ET LA STRUCTURE DES CLV, LA REDONDANCE EXPLOITABLE PAR LE DECODAGE EST BIEN PLUS IMPORTANTE QUE LA REDONDANCE DUE A L'EXPLOITATION SEULE DE LA STRUCTURE DES CLV. DE MEME, MOINS DE REDONDANCE RESIDUELLE PEUT ETRE EXPLOITEE SI LA SEMANTIQUE SOURCE N'EST UTILISEE QUE PARTIELLEMENT. CE RESULTAT MENE A LA PROPOSITION D'UN NOUVEAU DECODEUR DE CLV CAPABLE D'EXPLOITER TOUTES LES PROPRIETES DE LA SOURCE. CE DECODEUR DELIVRE UNE SOLUTION A DECISION DURE ET OPTIMALE AU SENS DU MAXIMUM DE VRAISEMBLANCE (MV), AINSI QU'UNE SOLUTION A DECISION SOUPLE ET OPTIMALE AU SENS DU MAXIMUM A POSTERIORI (MAP). UN SYSTEME DE DECODAGE ITERATIF SOURCE-CANAL CONJOINT A ETE CONÇU AMELIORANT SIGNIFICATIVEMENT LES PERFORMANCES. COMME POUR LES TURBO-CODES, LE DECODAGE ITERATIF SOURCE-CANAL CONJOINT PERMET D'AMELIORER CONSIDERABLEMENT LES PERFORMANCES DE CORRECTION D'ERREURS DU DECODAGE. AINSI, NOUS AVONS MONTRE QUE LA REDONDANCE RESIDUELLE SOURCE CONTRIBUE <b>EFFECTIVEMENT</b> A CORRIGER DES ERREURS, EXACTEMENT COMME LA REDONDANCE AJOUTEE PAR UN CODAGE CANAL. THE LOSS OF VIDEO QUALITY DUE TO TRANSMISSION OVER WIRELESS CHANNELS IS RELATED TO THE USE OF VARIABLE LENGTH CODES (VLC) AND THE PREFIX-BASED DECODING METHODS. THE REDUNDANCY REMAINING IN COMPRESSED VIDEO DATA HAS BEEN ANALYZED AND EVALUATED. TWO TYPES OF RESIDUAL REDUNDANCY HAVE BEEN EVIDENCED: THE REDUNDANCY DUE TO THE STRUCTURE OF THE CODE (VLC SYNTAX) AND THE REDUNDANCY DUE TO THE SOURCE CONSTRAINTS ON THE SEMANTICS OF THE SEQUENCE (VLC SEMANTICS). A TOOL HAS BEEN PROVIDED FOR COMPUTING THESE TWO TYPES OF RESIDUAL REDUNDANCY. NUMERICAL APPLICATION HAS DEMONSTRATED THAT SIGNIFICANT RESIDUAL SOURCE REDUNDANCY STILL EXISTS IN THE COMPRESSED VIDEO DATA. IT IS SHOWN THAT THE QUANTITY OF REDUNDANCY DUE TO BOTH VLC SYNTAX AND SOURCE CONSTRAINTS IS MUCH HIGHER THAN THAT DUE ONLY TO VLC SYNTAX. REDUNDANCY INDUCED BY BOTH RUN & LAST CONSTRAINTS IS MORE IMPORTANT THAN REDUNDANCY INDUCED BY ONLY THE LAST CONSTRAINT. THUS, FURTHER SOURCE CONSTRAINTS INDUCE FURTHER RESIDUAL SOURCE REDUNDANCY WHICH CAN BE EXPLOITED TO IMPROVE THE DECODING PERFORMANCE. WE PROPOSED A NEW VLC DECODER WHICH CAN DELIVER BOTH ML OPTIMUM HARD-OUTPUT AND MAP OPTIMUM SOFT-OUTPUT SOLUTIONS. THIS DECODER CAN EXPLOIT BOTH THE VLC SYNTAX AND THE VLC SEMANTICS DUE TO SOURCE CONSTRAINTS. SIGNIFICANT DECODING PERFORMANCES HAVE BEEN OBTAINED. AN ITERATIVE JOINT SOURCE-CHANNEL DECODING SCHEME IS PROPOSED FOR IMPROVING THE DECODING PERFORMANCE. HENCE, THE PROPOSED VLC SOURCE DECODING HAS THE SAME BEHAVIOUR AS A CHANNEL DECODING: THE RELATIONSHIP BETWEEN THE RESIDUAL REDUNDANCY AND THE ERROR CORRECTION PERFORMANCE, THE IMPROVEMENT BY ITERATIONS BETWEEN SOFT-DECISION QUANTITIES SIMILARLY TO THE CASE OF TURBO-CODES. ORSAY-PARIS 11 -BU Sciences (914712101) / SudocSudocFranceF...|$|E
40|$|Un problème relationnel évident entre l Eglise Catholique et les Média constituait le point de départ de cette thèse de doctorat. Des membres de l Eglise se plaignent à répétition de la machinerie des Média et leur préférence pour les scandales et des accroches négatives. La {{critique}} est que l image de l Eglise créée par les Média ne correspond pas à la réalité, les aspects positifs sont souvent ignorés. Mais de quelle façon les Média représentent <b>effectivement</b> l Eglise ? La thèse répond à cette question en démontrant les conditions de communication médiatiques ainsi qu en décrivant l image de l Eglise dans les discours journalistiques. The {{relationship between}} the Catholic Church and the media is apparently difficult. Members of the church frequently complain about the media s partiality for scandals and negative headlines. The Catholic Church, they say, is portrayed in an excessively bad light, while positive aspects are overlooked. But how does the press actually write about the Church? By illustrating media-related terms of communication and characterizing the Church s image in press-discourse, the current thesis answers this question. On the assumption that impartial reality is not available and therefore not illustratable, news coverage never reflects reality, but reconstructs it. Reality as transported by the media is affected by ideas about life and moral concepts not only by journalists, but also by editorial boards, cultures and other media-related terms of communication. Thereby, and through factors influencing the selection of news, e. g. focussing on hot potatoes and bad news, coverage of the Church turns out one-sided and negative. This thesis is, on the one hand, a media-linguistic investigation, but also uses an interdisciplinary approach by drawing upon knowledge of linguistic sub-disciplines {{as well as of}} media- and communication science. In 2009, a sample of 212 articles of Austrian and French daily newspapers was investigated by means of Inhaltsanalyse (content analysis) following Werner Früh and Kritischer Diskursanalyse (critical discourse analysis) following Siegfried Jäger. The articles (Austria: Die Presse, Der Standard, Kronen Zeitung; France: Aujourd hui en France, Le Figaro, Le Monde) were reviewed with regard to content, topic and language. Only such articles were chosen, which were information- and opinion-centred and in line with the editorial board. Additionally, journalists of the mentioned newspapers were interviewed to gain information on the coverage of religious issues and the attitude of the editorial staff towards the Catholic Church. The content analysis was, on the one hand, aimed at the classification of reported topics. On the other hand, news factors should be identified, which were relevant for the selected events. Finally, the content analysis should gain information on to what extent topic structure and distribution of news factors differed between the newspapers. The discourse analysis particularly aimed at classifying content and language of explicit and implicit comments in the articles. In addition, linguistic characteristics of coverage of the Catholic Church should be identified, as far as they allowed conclusions to be drawn with respect to underlying ideologies. Finally, press photos should be analysed in terms of ideological and evaluative contents. Comparison of the different newspapers should permit to trace the individual editorial staff s principles and identify country-and culture-specific differences, as the relationship between state and church is regulated differently. While in France there is a very strict disestablishment of state and church (Laizism), in Austria there is a system of cooperation, which is founded in a concordat. This thesis shows, that all of the reviewed daily newspapers consider the topic Catholic Church as socio-politically relevant. At the same time, no consistent disapproval of the Church by means of the media was visible. This becomes evident by the quite extensive and frequent coverage that expands for example into the rubric politics. Furthermore, all of the reviewed newspapers had one journalist being in charge of topics concerning catholic matters, and who was socialised in - and in many cases - is close to the Church [...] PARIS 5 -Bibliotheque electronique (751069902) / SudocSudocFranceF...|$|E
40|$|Prenant appui sur une méthodologie d'enquête, de {{conceptualisation}} et de développement itérative - la Théorie Ancrée - notre recherche sur l'instrumentation informatique des annotations et des pratiques d'écriture professionnelles nous a permis de développer des concepts, un modèle et un prototype informatique originaux pour le support de l'organisation des soins en cancérologie. Pour mener à bien ce travail itératif de modélisation, nous nous sommes inspirés des pratiques d'écriture des soignants que nous avons observées durant plusieurs années dans un hôpital en cancérologie. Cette étude qualitative a été complétée par un état de l'art pluridisciplinaire. Ce travail d'articulation entre différents domaines scientifiques, enquête et confrontation aux acteurs du terrain nous a permis de développer les concepts de pratique annotative et d'écriture heuristique considérés comme constitutifs du travail d'organisation des soins. Ces pratiques permettent aux soignants d'appréhender leurs environnements de travail complexes, et de gérer efficacement les dynamiques et les variétés de situations et de prise {{en charge}} des patients. La caractérisation de ces pratiques nous a permis de développer un modèle informatique simple, robuste et ouvert, mais surtout ancré dans le terrain de recherche et dans les pratiques soignantes. Ce prototype grâce notamment aux contextes d'affichage diversifiés des réseaux d'annotation qu'il autorise, grâce à sa flexibilité et son évolutivité, permet <b>effectivement</b> d'adresser la question du support du travail collectif d'organisation des soins en cancérologie, autant qu'il est prometteur pour d'autres contextes d'exploitation. Our research about computerized equipment of annotations and professional writing practices has articulated Computer Sciences and Communication and Information Sciences {{in order for}} us to develop a software prototype for the support of organisation of care in oncology thanks to an empirical and longitudinal study. This articulation has relied on 4 years long of multimodal ethnography, a conceptualization based upon Grounded Theory rules, a model and a software development that we iteratively conduct simultaneously with this qualitative inquiry. This original combination of methodologies from different domains has been remarkably rich to help us build a specific point of view about oncology hospital organizations, and about the accomplishment of care work and about patients' management in these complex professional organizations. The modern medical and hospital rationalizations, including ICT and e-health tools, are in the confluence of various movements that impact upon several elements of hospital organizations, and upon medical and care practices. We focused our study on the situated writing practices of caregivers and specifically on the richness of the materiality of writings that led us to question notions such as information systems, collective and distributed production of knowledge, and the documentary production cycles for the organization of activity. We are making the hypothesis that annotations can be opportunely considered as constitutive elements in the production of "organizational texts" that are in the core of the support of the organization and the realization of collective and individual work. Caregivers rely on these "organizational texts" that they build, actualize and stabilize thanks to what we call the "annotative practice" that enables them to apprehend their complex environments of work, and to handle the dynamics and the variety of situations and to manage patients. We will show how the characterization of this "annotative practice" helped us to develop a simple, robust and open software model. We will detail the iterative process between inquiry, analysis, conceptualizations and developments that led us to stabilize our model and our web prototype that implement this annotative practice. We will conclude our work by showing that this prototype, thanks to the numerous context displays of annotations networks, thanks to its flexibility and evolutivity achieves the support of the collective organization of patients' care in oncology, as much as it seems to be relevant in other exploitation context of collective organization of work...|$|E
40|$|On the Development of Novel Multi-Layer Passive Components and the Implementation of Compact Wideband Two-Layer 4  4 Butler Matrix in SIW Technology Multibeam antennas {{have become}} {{a key element in}} {{nowadays}} wireless communication systems where increased channel capacity, improved transmission quality with minimum interference and multipath phenomena are severe design constraints. These antennas are classified in two main categories namely adaptive smart antennas and switched-beam antennas. Switched-beam antennas consist of an elementary antenna array connected to a Multiple Beam Forming Network (M-BFN). Among the different M-BFNs, the Butler matrix has received particular attention as it is theoretically lossless and employs the minimum number of components to generate a given set of orthogonal beams (provided that the number of beams is a power of 2). However, the Butler matrix has a main design problem which is the presence of path crossings that has been previously addressed in different research works. Substrate Integrated Waveguide (SIW) features interesting characteristics for the design of microwave and millimetre-wave integrated circuits. SIW based components combine the advantages of the rectangular waveguide, such as the high Q factor, low insertion loss and high power capability while being compatible with low-cost PCB and LTCC technologies. Owing to its attractive features, the use of SIW technology appears as a good candidate for the implementation of BFNs. The resulting structure is therefore suitable for both waveguide-like and planar structures. In this thesis, different novel passive components (couplers and phase shifters) have been developed exploring the multi-layer SIW technology towards the implementation of a two-layer compact 4  4 Butler matrix offering wideband performances for both transmission magnitudes and phases with good isolation and input reflection characteristics. Different techniques for the implementation of wideband fixed phase shifters in SIW technology are presented. First, a novel waveguide-based CRLH structure is proposed. The structure is based on a single-layer waveguide with shunt inductive windows (irises) and series transverse capacitive slots, suitable for SIW implementations for compact phase shifters. The structure suffers relatively large insertion loss which remains however within the typical range of non-lumped elements based CRLH implementations. Second, the well-known equal length, unequal width SIW phase shifters is discussed. These phase shifters are very adapted for SIW implementations as they fully exploit the flexibility of the SIW technology in different path shapes while offering wideband phase characteristics. To satisfy good return loss characteristics with this type of phase shifters, the length has to be compromised with respect to the progressive width variations associated with the required phase shift values. A two-layer, wideband low-loss SIW transition is then proposed. The transition is analyzed using its equivalent circuit model bringing a deeper understanding of its transmission characteristics for both amplitude and phase providing therefore the basic guidelines for electromagnetic optimization. Based on its equivalent circuit model, the transition can be optimized within the well equal-length SIW phase shifters in order to compensate its additional phase shift within the frequency band of interest. This two-layer wideband phase shifter scheme has been adopted in the final developed matrix architecture. This transition is then exploited to develop a three-layer, multiply-folded waveguide structure as a good candidate for compensated-length, variable width, low-loss, compact wideband phase shifters in SIW technology. Novel two-layer SIW couplers are also addressed. For BFNs applications, an original structure for a two-layer 90 ° broadband coupler is developed. The proposed coupler consists of two parallel waveguides coupled together by means of two parallel inclined-offset resonant slots in their common broad wall. A complete parametric study of the coupler is carried out including the effect of the slot length, inclination angle and offset on both the coupling level and the transmission phase. The first advantage of the proposed coupler is providing a wide coupling dynamic range by varying the slot parameters allowing the design of wideband SIW Butler matrix in two-layer topology. In addition, previously published SIW couplers suffer from direct correlation between the transmission phase and the coupling level, while the coupler, hereby proposed, allows controlling the transmission phase without significantly affecting the coupling level, making it a good candidate for BFNs employing different couplers, such as, the Nolen matrix. A novel dual-band hybrid ring coupler is also developed in multi-layer Ridged SIW (RSIW) technology. The coupler has an original structure based on two concentric rings in RSIW topology with the outer ring periodically loaded with radial, stub-loaded transverse slots. A design procedure is presented based on the Transverse Resonance Method (TRM) of the ridged waveguide together with the simple design rules of the hybrid ring coupler. A C/K dual band coupler with bandwidths of 8. 5 % and 14. 6 % centered at 7. 2 GHz and 20. 5 GHz, respectively, is presented. The coupler provides independent dual band operation with low-dispersive wideband operation. Finally, for the Butler matrix design, the two-layer SIW implementation is explored through a two-fold enhancement approach for both the matrix electrical and physical characteristics. On the one hand, the two-layer topology allows an inherent solution for the crossing problem allowing therefore more flexibility for phase compensation over a wide frequency band. This is achieved by proper geometrical optimization of the surface on each layer and exploiting the SIW technology in the realization of variable width waveguides sections with the corresponding SIW bends. On the other hand, the two-layer SIW technology is exploited for an optimized space saving design by implementing common SIW lateral walls for the matrix adjacent components seeking maximum size reduction. The two corresponding 4  4 Butler matrix prototypes are optimized, fabricated and measured. Measured results are in good agreement with the simulated ones. Isolation characteristics better than - 15 dB with input reflection levels lower than - 12 dB are experimentally validated over 24 % frequency bandwidth centered at 12. 5 GHz. Measured transmission magnitudes and phases exhibit good dispersive characteristics of 1 dB, around an average value of - 6. 8 dB, and 10 ° with respect to the theoretical phase values, respectively, over the entire frequency band. Développement de Nouveaux Composants Passifs Multicouches et l'Implémentation d'une Matrice de Butler Large-Bande et Compacte en Technologie GIS Les systèmes de communications sans fils actuels imposent des contraintes très sévères en termes de la capacité du canal, la qualité de transmission tout en gardant les niveaux d'interférences et multi-trajets assez faibles. De telles contraintes ont rendu les antennes multifaisceaux un élément essentiel dans ces systèmes. Parmi les techniques permettant de réaliser une antenne multifaisceaux (sans avoir recours aux systèmes à balayages électroniques), un réseau d'antennes élémentaires est associé à un réseau d'alimentation (une matrice) à formation de faisceau (Beam Forming Network-BFN). Parmi les différents types de ces matrices, la matrice de Butler a reçu une attention particulière. Ceci est dû au fait qu'elle est théoriquement sans pertes et qu'elle emploie un nombre minimum de composants (coupleurs et déphaseurs) afin de générer l'ensemble de faisceaux orthogonaux demandé (avec l'hypothèse que le nombre de faisceau est une puissance de 2). Néanmoins, la matrice de Butler a un problème de conception majeur. Ce problème réside dans la structure de la matrice qui renferme des croisements ce qui a été adressé par différents travaux de recherches dans la littérature. Les Guide Intégré au Substrat (GIS) offrent des caractéristiques intéressants pour la conception des composants microondes et millimétriques faciles à intégrer sur un même support avec d'autres composants planaires. Les composants à base de GIS combinent les avantages des guides d'ondes rectangulaires, comme leur grand facteur de qualité Q, leur faibles pertes tout en étant compatible avec les technologies à faibles coûts comme le PCB et le LTCC. Vus ses caractéristiques attrayants, la technologie GIS devient un bon candidat pour la réalisation des matrices multifaisceaux faciles à intégrer avec d'autres systèmes en technologies planaires ou à base de guide GIS. Dans cette thèse, de nouveaux composants passifs sont développés en exploitant la technologie GIS en multicouches en vue de la réalisation d'une matrice de Butler 4 x 4 compacte et large bande. Les composants recherchés sont donc des coupleurs et des déphaseurs ayant des performances large bande en termes des amplitudes des coefficients de transmissions et les phases associés tout en gardant de faibles niveaux de pertes et de bonnes isolations. Différents techniques pour l'implémentation de déphaseurs large bande en technologie GIS sont présentés. Une nouvelle structure à base d'une propagation composite : main gauche main droite (Composite Right/Left- Handed, CRLH) dans un guide d'onde est proposée. La structure consiste d'un guide d'onde monocouche ayant des fenêtres inductives et des fentes transversales à réactances capacitives pour synthétiser l'inductance parallèle et la capacité série main gauche, respectivement. La structure est adaptée pour les réalisations de déphaseurs compacts en technologie GIS. Bien que les pertes d'insertions restent dans le même ordre de grandeur de celles des structures CRLH à base d'éléments non-localisés, ces niveaux de pertes restent relativement grands par rapport aux applications nécessitant plusieurs déphaseurs. Les déphaseurs à bases de GIS ayant des longueurs égales et des largeurs variables sont ensuite abordés. Ce type de déphaseur est <b>effectivement</b> très adapté à la technologie GIS qui permet des réalisations de parcours avec différentes formes (parcours droits, courbés, coudés, [...] ) tout en assurant des différences de phase large bande. Afin de satisfaire de faibles pertes d'insertions pour une large dynamique de phase, la longueur de ces déphaseurs est en compromis avec les variations progressives des différentes largeurs associées aux valeurs de déphasages requises. Une transition large bande, double couche et à faible perte est ainsi proposée. La transition est analysée à partir de son circuit électrique équivalent afin d'étudier les performances en termes de l'amplitude et la phase du coefficient de transmission par rapport aux différents paramètres structurels de la transition. Cette transition est ensuite exploitée pour développer un déphaseur à trois couches, large bande, en GIS. La structure consiste <b>effectivement</b> d'un guide d'onde replié à plusieurs reprises sur lui-même selon la longueur dans une topologie trois couches à faibles pertes. De nouveaux coupleurs double couche en GIS sont également proposés. Pour les applications BFNs, une structure originale d'un coupleur large bande est développée. La structure consiste de deux guides d'onde parallèles qui partagent leur grand mur ayant une paire de fentes inclinées et décalées par rapport au centre de la structure. Une étude paramétrique détaillée est faite pour étudier l'impact des différents paramètres des fentes sur l'amplitude et la phase du coefficient de transmission. Le coupleur proposé a l'avantage d'assurer une large dynamique de couplage ayant des performances larges bandes en termes des amplitudes et les phases des coefficients de transmission avec de faibles pertes et de bonnes isolations entre le port d'entré et celui isolé. D'autre part, contrairement à d'autres travaux antérieurs et récents qui souffraient d'une corrélation directe entre la phase en transmission et le niveau de couplage, la structure proposée permet de contrôler le niveau de couplage en maintenant presque les mêmes valeurs de phase en transmission pour différents niveaux de couplage. Ceci le rend un bon candidat pour les BFNs déployant différents coupleurs telle la matrice de Nolen. Finalement, pour l'implémentation de la matrice de Butler, la topologie double couche est explorée à deux niveaux. Le premier consiste à optimiser les caractéristiques électriques de la matrice, tandis que le second concerne l'optimisation de la surface occupée afin de rendre la matrice la plus compacte possible sans dégrader ses performances électriques. D'une part, la structure double couche présente une solution intrinsèque au problème de croisement permettant ainsi une plus grande flexibilité pour la compensation de phase sur une large bande de fréquence. Ceci est réalisé par une conception adéquate de la surface géométrique sur chaque couche de substrat et optimiser les différentes sections de GIS avec les différents parcours adoptés. La deuxième étape consiste <b>effectivement</b> à optimiser la surface sur chaque couche en profitant de la technologie GIS. Ceci consiste à réaliser des murs latéraux communs entre différents chemin électrique de la matrice en vue d'une compacité optimale. Les deux prototypes de matrices de Butler 4 x 4 sont optimisés, fabriqués et mesurés. Les résultats de mesures sont en bon accord avec ceux de la simulation. Des niveaux d'isolations mieux que - 15 dB avec des niveaux de réflexions inférieurs à - 12 dB sont validés expérimentalement sur plus de 24 % de bande autour de 12. 5 GHz. Les coefficients de transmission montrent de faibles dispersions d'environ 1 dB avec une moyenne de - 6. 8 dB, et 10 ° par rapport aux valeurs théoriques, respectivement, sur toute la bande de fréquence...|$|E
40|$|Les lois Auroux de 1982 ont impulsé, en France, un {{mouvement}} de décentralisation des relations professionnelles. On peut <b>effectivement</b> constater, au cours de la dernière décennie, un développement notable {{des accords}} d'entreprise. Aux yeux de l'observateur, ce mouvement ne doit pas occulter, malgré tout, ni la pérennité des accords de branche, ni l'importante variable des accords d'entreprise selon les secteurs et les tailles d'entreprise ni, enfin, le fait que la dynamique n'est pas nécessairement la même d'un thème de négociation à l'autre. Une enquête menée dans deux secteurs (électronique et santé) permet de spécifier les limites de la décentralisation en matière de négociation sur l'emploi. The 1982 Auroux laws in France served to bring unions and enterprises closer together. Whereas historically labour relations had {{been based on}} confrontation between a labour movement in which the "revolutionary" spirit had prevalled for a long time, and on a management which had carefully avoided the introduction of collective bargaining in firms, the 1980 s were characterized by a significant move towards decentralization of negotiations to enterprises and establishments. In {{the first part of}} this article, it is shown that, in fact, the Auroux laws contributed to this change: in 1981, barely 1500 enterprise — level agreements were registered in France; in 1992 there were 6370 such agreements. Nevertheless, it is worth pointing out three complementary phenomena. First, the law related to employees' freedom of expression has had limited success: according to a recent report, in reality groups for self— expression seldom function and do so poorly. Weak union involvement and management participation policies (quality circles) have undeniably contributed to weakening the interest in and importance of these groups. Secondly, the growth of enterprise — level collective bargaining has not resulted in a decline in industry — wide agreements (their number has remained stable throughout the decade); nor is their growth homogeneous (such agreements are increasingly the reality in large industrial sector firms). Finally, the dynamic varies according to the objects of negotiation. Hence, the following observations: a certain importance given to salaries in negotiations at both the industry and enterprise levels, industry — level negotiation giving way to enterprise — level negotiation regarding work time; a renewal of energy though the volume of agreements remains limited; and interprofessional industry — and enterprise — level negotiations on the issue of employment. The aim of the second part of the article is, precisely, to evaluate the impact of negotiations on the management of employment relations in two sectors that are at first glance opposites: this interest is based on a study conducted by the two authors in which they examine the management of redundant personnel in a sector in crisis (electronics) and the management of labour shortages in a protected public sector (health care). Firstly, during recent years programs have multiplied in the electronics sector to deal with layoffs. In order to institute preventive action, in 1990 and 1991, the social actors established joint committees on employment whose mission was to make forecasts of employment in the industry in order to formulate ways to deal with the effects of fluctuating employment (forecasts of movement of the workforce, establishment of special training programs, and so on). In reality, for reasons related as much to the desire of employers to bring back the management of employment relations to the level of the enterprise as to the difficulty in adjusting trade union structures to the reality of a specific industry (electronics and not the metallurgical industry, which remains the usual framework for negotiations), these committees have scarcely been effective. Furthermore, it was observed that there is a real asymmetry between the enterprise (strategic level where important decisions about employment are made) and the establishments. Within the latter, in spite of rights given to union organizations, they, as well as the local administrations which are sometimes just as powerless as themselves, can only negotiate the consequences of strategies elaborated at the group and enterprise levels. Thus one of the major limitations of the policy of decentralization of industrial relations can be clearly seen here. As regards the health care sector, its System of industrial relations is structured around a statutory logic regulating recruitment, remuneration and employment of permanent personnel. Since the hospital reforms of July 31, 1991, with a concern for democratization similar to the one which was behind the Auroux laws, dialogue between all actors can take place within a series of councils who are responsible for both investment programs and internal regulations, etc. The fact remains that in the case of electronics, decisions about the volume of employment are made at the central level (the Ministry) : the volume of employment is thus a fact that the actors cannot negotiate because it is the result of political arbitration and certain financial constraints. This weak union influence on determining factors of employment policy was one of the reasons, among many others, for the emergence of new forms of collective action during the second half of the 1980 s. During the conflicts that characterized this period, nurses and auxiliary nurses went beyond union and institutional channels of communication in aid of sectorial organizations. This was significant each time, the conflicts having a direct impact on topics that normally cannot be debated within the framework of joint committees: such is the case of the level of employment, re-establishment of indexation and recruitment terms. This is amply demonstrated in the two sectorial studies that were conducted. These two case studies tend to show that in France the movement towards decentralization not only increases the tendency to negotiate salary changes, management of work time, and so on in a compartmentalized way, but also makes it more difficult for union organizations to negotiate employment policy determinants at the enterprise level. Furthermore, beyond differences between the private and public sectors that many researchers have rightly emphasized, it appears to us that the two sectors examined continue to have certain common characteristics which reflect quite well the difficulty experienced by the current French trade union movement in adapting itself structurally to the new social and economic context...|$|E
40|$|L'une des difficultés rencontrées couramment dans la {{conception}} des réseaux de mesure - au moins {{en ce qui concerne}} les micropolluants - porte sur la sélection des paramètres à mesurer. C'est notamment le cas pour les pesticides, dont plusieurs centaines sont utilisées en agriculture, mais qu'il est impossible de surveiller dans les eaux en totalité pour des raisons à la fois techniques et économiques. C'est la raison pour laquelle les autorités françaises ont fait procéder à la mise au point d'une méthode de sélection des matières actives utilisées en agriculture basée sur l'évaluation du risque. Dans cette méthode, l'exposition est figurée par un rang combinant les données relatives aux usages des matières actives (superficie, dose par ha) et leurs caractéristiques physico-chimiques. Le danger est représenté par la toxicité, soit pour l'homme, soit pour les espèces aquatiques. Cette approche a été appliquée à l'échelle nationale et dans un certain nombre de régions françaises, dont l'Alsace et la Lorraine. Les résultats des mesures de pesticides réalisées ensuite pendant un an ont été confrontés aux indices d'exposition obtenus. Les substances détectées le plus fréquemment correspondent <b>effectivement</b> à celles dotées des rangs d'exposition les plus élevés (ajustement exponentiel, r 2 ≈ 0. 82); cependant, le diuron apparaît à une fréquence plus élevée que celle attendue, {{en raison de}} ses usages non agricoles. La corrélation est moins bonne pour les substances dont les rangs d'exposition sont proches de la valeur considérée comme significative pour les eaux superficielles, ce qui peut provenir soit de l'utilisation de données erronées lors de la sélection, soit d'un poids insuffisant attribué à certains facteurs dans la méthode de sélection, soit enfin d'aléas météorologiques. Monitoring of micropollutants is {{a rather}} recent activity (10 - 15 years), at least in surface waters; because of the need for sophisticated analytical methods and of the potential number of analytes, this type of activity is confronted with important economic constraints, which require that one make a selection among the range of substances to monitor. Among organic micropollutants, pesticides constitute a well-identified category, since they are used mainly in agriculture; this use on broad surfaces may have important impacts on the quality of surface water. Various methods have been used to select those pesticides likely to have the greatest impacts on water quality; some of these methods might be considered to be "hazard assessment", whereas others correspond to simplified "risk assessment" methods (this appears particularly true for pesticides, of which several hundreds are used in agriculture). Recently, a French panel of experts mandated by different Ministries designed a selection method called SIRIS, which allows one to define three different lists of pesticides according to the media to be monitored (surface or ground-water) and to the monitoring objectives (ecosystem protection, drinking water production). This paper deals with the application of the SIRIS method at a regional level, {{in the context of a}} permanent survey of river quality. As a simplified risk assessment method, SIRIS combines data on hazard and exposure; hazard is estimated by a single parameter, either toxicity for aquatic species or acceptable daily intake (ADI). Exposure represents the probability that a transfer to water bodies may occur; for surface water, this probability is influenced by the crop acreage, the applied dose (kg/ha), the solubility, the pesticide half-life, the hydrolysis and the distribution coefficient between water and organic matter (Koc). These factors are considered in this hierarchical order, and for each substance a score is assigned to each of these factors among three possible values ("o"=slight, "m"=medium, "d"=high, according to the relative influence on transfer); finally exposure is estimated by a relative rank obtained by a combination of these values following a "penalisation" principle. Two tables are available for applying this approach at a regional level: the first contains the values (o,m,d) assigned to more than 300 substances by the expert panel for solubility, half-life, etc., and should be completed with crop acreage and dose. The second table provides the ranks corresponding to the different combinations of o,m,d values. A final rank of 35 was considered by consensus to be a pragmatic threshold for the transfer to surface water. This method was applied in 1996 in two regions in France (Alsace and Lorraine) separately; most of the selected chemicals (but unfortunately not all, due to technical constraints) were then analysed monthly in surface waters (24 sampling points, yielding 144 samples in Alsace and 169 in Lorraine). Occurrences fell between 0 % and 60 % in Alsace, and between 0 % and 90 % in Lorraine; in both regions, the most frequently detected chemicals were atrazine and diuron. The relevance of the selection method may be discussed under several aspects: the choice of the factors, their order, the position of thresholds corresponding to o,m,d values, the value of the overall threshold, and the availability of the data. Some pesticides are not ranked only because no data were available concerning their solubility, hydrolysis rate or Koc, but the relative importance of such gaps cannot be appreciated with the current set of data. Other items may be assessed through the comparison of the exposure rank versus the occurrence. This relationship takes an exponential shape, with some anomalies: for example, the occurrence of diuron in Alsace is higher than expected, based on its exposure rank. This situation can be explained by the fact that there are non-agricultural uses of this substance, such that the exposure rank appears to be underestimated. For other substances, like aldicarb and chlorpyrifos-ethyl, discrepancies are observed between the exposure rank and occurrences, when comparing with substances with higher exposure ranks. This anomaly may be due to poor data quality. For carbendazime, the occurrence in Lorraine appears underestimated, probably because of a dry period deficit after the application. Finally, chlortoluron received the same rank in the 2 regions, but is more frequently detected in Lorraine; crop acreage may have been overestimated in Alsace. However, the dataset is still limited to one year of sampling; some discrepancies may appear less important when more data are available. For chemicals with ranks > 50, there is a good exponential fit between ranks and occurrences (y= 0. 0235 *e 0. 0739 x; r 2 = 0. 82). This observation means that pesticides with ranks > 50 are systematically encountered in surface waters; however, the current threshold (35) should be maintained, because some substances with ranks < 50 are also detected. Thus, the SIRIS method appears to be a good tool for selecting agricultural pesticides for monitoring purposes at a regional level...|$|E
40|$|Nous présentons une revue bibliographique à propos des effets des {{herbicides}} inhibiteurs du Photosystème II (PS II) sur les communautés algales. Ces herbicides sont abondamment utilisés dans les pratiques phytosanitaires. Ils sont susceptibles de contaminer les milieux aquatiques et, étant donné leur mode d'action inhibitrice de la photosynthèse, ils peuvent agir directement sur les algues. De nombreuses études ont été réalisées afin d'évaluer l'impact des contaminations par ces herbicides sur les microphytes, en particulier leur effet sur {{la croissance}} et la physiologie de certaines algues (monocultures en laboratoire). D'autres études expérimentales et quelques rares in situ, ont porté sur l'impact des herbicides inhibiteurs de la photosynthèse sur la structure des peuplements algaux. Certaines tendances ont pu être ainsi dégagées quant à la sensibilité et la résistance aux herbicides des différentes espèces étudiées soit isolément, soit au sein des peuplements. Les herbicides inhibiteurs du PS II perturbent <b>effectivement</b> la structure des peuplements phytoplanctoniques de façon plus ou moins marquée. L'impact des herbicides sur les algues est variable selon la structure des peuplements (liée aux successions) et les paramètres environnementaux, notamment liés à la saison. Nous devons donc développer nos connaissances à propos des interactions entre toxiques et facteurs environnementaux sur des pas de temps correspondant non seulement aux rythmes des contaminations mais aussi aux rythmes des succesions alagles, car ces interactions sont susceptibles de réduire ou d'amplifier les conséquences d'une pollution par ces toxiques dans les milieux aquatiques. The {{aim of this}} paper is to present a review about the impact of Photo system II (PS II) inhibitors on algae communities. A brief discussion of the use in agriculture, the different chemical families, the photosynthetic inhibition effect and the occurrence of these compounds in aquatics systems is followed by the presentation of the impacts these herbicides have on algae. Many studies investigate the effects of PS II inhibitors on algae growth and physiology. The response to pollutants were studied by monitoring changes in terms of different parameters : chlorophyll fluorescence induction usually increases with PS II inhibitors. Concentration of pigments decreases with PS II inhibitors, but increases sometimes with low contaminations of these toxicants (it is probably an homeostasis effect). Pigment ratio can change with herbicide exposure. Primary production (measured by 14 C incorporation or dissolved O 2) usually decreases with PS II inhibitors. But, the " excretion " of dissolved organic compounds may increase with PS II inhibitors. These herbicides may alter or change cell morphology of algae. In consequence algae growth is inhibited by PS II inhibitors exposure. But growth inhibition varies, depending on each species' (and strain) sensibility or resistance to each herbicide. By this way, PS II inhibitors can affect the algae community structure. In consequence, herbicides exert a selection pressure when the exposure reaches a certain level, and this for a sufficient period of time. Since organisms vary in their resistance to toxicants, the selection pressure will exclude the sensitive ones which will be replaced by resistant ones. Sometimes, responses to pollutants which are measured by global changes in biomass, pigments, dissolved O 2 [...] . can recover after a lagtime. This apparent ability to recover from effects of herbicides can be explained by the following selection effect : resistant species are indirectly stimulated and develop in the contaminated environment. The result is an algae community which has an increased resistance to these toxicants as compared to a community which has not been affected by the toxicants. This difference in resistance, between the unselected and the selected communities, may be detected by comparison of results from short term physiological tests performed with the respective community and by comparison of each community structure (taxonomy). This methodological approach called Pollution-Induced Community Tolerance (PICT) is of a great interest as a biological marker of specific pollution in aquatic systems. Indirect effects of algae response to PS II inhibitors occur in the polluted ecosystem according to changes of the physicochemical conditions (decrease of dissolved O 2 concentrations and pH, increase of dissolved organic and inorganic matters [...] .). Furthermore, the impact of herbicides on algae communities varies, depending on the community's species composition (depending on successions) and on environmental factors such as interspecific interactions and physicochemical parameters (depending on seasonal changes). Interspecific interactions implies competition for the limiting nutrients among algae and allelopathic interactions among algae, as well as interactions between algae and other trophic levels (microbial loop, grazing pressure [...] .). These interactions between herbicides and environmental factors may reduce or emphasize the consequences of such a pollution in aquatic systems. Seasonal change of algae communities species composition (algae successions) occurs as a response to changing environmental factors by the way of interspecific interactions and physicochemical parameters. Therefore, algae succession is affected by the herbicide destructuration of the algae communities. At the opposite, interactions and successions may affect the response of algae communities to the toxic. In this sense, herbicides act as a supplementary factor of disturbance in algae successions. Structural changes, induced by these herbicides, are usually accompanied by the attributes which are typical of an early successional stage. But, according to the " Intermediate Disturbance Hypothesis " the species richness should be maximal at intermediate intensities of herbicide contamination and at intermeadite frequencies of contaminations. The question is to compare the algae successions rythms and the frequencies of herbicides contaminations. Then, the time factor (or the persisting quality, which is difficult to assess in experimental studies) has to be taken in account in monitoring aquatic polluted systems, because of the seasonal variability in the response of algae to PS II inhibitors as well as the seasonal variability of water bodies' contamination by these herbicides through watersheds. Moreover, usually, low values of herbicides' concentrations occur in aquatic environments but are persistant. It results that aquatic organisms are exposed during long periods of time, meaning that indirect effects, via interactions between herbicides and environmental factors, may be emphazised. To further investigate these interactions and the herbicide persistance in aquatic systems, we have to develop experimental studies. This approach, however, must be complemented with in situ studies monitored with a timing of investigation relative to the natural population fluctuations and " pulses " of herbicides in these systems. Investigation must take place on various aquatic ecosystems. If a greater effort is given to monitor natural systems, for both herbicides and herbicides-induced effects, this will provide greater confidence in future predictions regarding the safety of PS II inhibitors in aquatic environments...|$|E
40|$|The {{object of}} the work was to study Quantum Paraelectric {{anomalies}} in KTaO_ 3 pure and Na-doped crystals, with E. Courtens and A. K. Tagantsev. What are quantum paraelectrics (QPE, also called incipient ferroelectrics) ? They are crystals (such as KTaO_ 3 and SrTiO_ 3) that should be ferroelectric under a certain Curie-Weiss temperature T_c, but they aren't ! At low temperature, the ferroelectric fluctuations are controlled by a zone-center transverse optic mode (TO) called ferroelectric mode which softens (tends to zero frequency) as the temperature decreases. This phonon {{is associated with the}} polarization fluctuations induced by the vibration of the cell-center ion (Ta or Ti) inside the octaedral oxygen cage. Around T_q (37 K for SrTiO 3 and about 10 K for KTaO 3), the related variation of the dielectric constant epsilon deviates from the normal Curie-Weiss divergence for ferroelectrics, and stabilizes at a large but finite value below T_q. Thus, ferroelectricity is not achieved. It is usually considered to be prevented by zero-point quantum fluctuations which are about the same amplitude as the would-be ferroelectric displacements. This corresponds to a quantum paraelectric state which remains stable down to the lowest temperatures. Moreover, the softening of the TO branch depresses the transverse acoustic phonon (TA), due to a strong TO-TA coupling. KTaO_ 3 was chosen for this study because it remains cubic down to the lowest temperature (and SrTiO_ 3 becomes tetragonal at about 105 K). It is then considered to be 'simpler' than other QPE's. When decreasing temperature in KTaO_ 3 crystals, some unexpected features appear on Brillouin scattering spectra : (i) a broad quasi-elastic central peak, first reported by Lyons and Fleury (1976), which is usually interpreted as second order scattering, and (ii) a new doublet recently observed over the quasi-elastic central peak of QPE's, which was associated to second sound phenomena (propagation of heat) with acoustic phonons (Hehlen 1995). I first measured precisely inelastic low energy phonons along high symetry axis (C_ 2, C_ 3 and C_ 4) in pure KTaO_ 3 (IN 14, with B. Hehlen and R. Currat), in order to look for those up-mentionned anomalies near Brillouin zone center by neutron scattering experiments. But, except for the strong TA-TO coupling, no particular 'strange' behaviour showed up. Then an extensive Brillouin characterisation of low frequency excitations (acoustic phonons, central peak, doublets) in the vicinity of Brillouin zone center was performed in pure and Na-doped KTaO_ 3 crystals. Many anomalous features were reported, and the effect of doping, eventually leading to ferroelectric transition, was also studied. Using both neutron and Brillouin data, a phenomenological parametrisation of low frequency phonon sheets was successfully applied to the center part of Brillouin zone (|q| velocities, some kind of 'valleys'), whereas C_ 3 axis are rather up-hill shaped (higher energies and group velocities). This model was first used in order to compute a lower value of three phonon electrostrictive normal processes dampings for the five lower energy phonons over the center part of Brillouin zone. The phonon energy anisotropy is also visible in dampings. Comparison with experimental data shows reasonable agreement. Then, the second sound hypothesis for the origin of doublet was tested. We here notice than second sound is the collective propagation mode of thermalised phonons : phonons whos normal dampings are greater than the secound sound frequency constitute thermal waves in which temperature is well defined and quasi-momentum is conserved (no resistive processes : defects, Umklapps, [...] .). Those waves can interact with light, and show up as doublets on Brillouin scattering spectra (Wehner and Klein, 1972). We computed second sound Brillouin zone center velocity with temperature, which is found to be in excellent agreement with experimental measurements of doublet frequency obtained through Brillouin spectroscopy. But secound sound intensity estimations could not fit with observations (either in value or in anisotropy). Consecutively, we looked for an alternative interpretation of doublets in terms of phonon density fluctuations. When phonon dampings are smaller than the secound sound frequency, the collective mode cannot propagate. But light can still couple to over-damped second sound (usual central entropy fluctuation Rayleigh peak, diffusion of heat) and pairs of phonons (two-phonon difference scattering). The latter process was computed for low energy phonons, using energy parametrisation and damping evaluations. The resulting Brillouin spectra are in excellent agreement with measured ones, in all directions and for all temperatures below 100 K. The doublet and the broad central peak can then be associated to two-phonon difference scattering processes from transverse acoustic phonons according to their dampings and group velocities. Optical phonons contribution are very broad, and rather appear as a quasi-constant background, while longitudinal acoustic phonons produce very small contributions when compared to others. Les cristaux para'electriques quantiques sont des di'electriques qui devraient ?etre ferro'electriques en dessous de la temp'erature de Curie­Weiss Tc. Que se passe­t­il ? A basse temp'erature, les fluctuations ferro'electriques sont contr?ol'ees par un mode de vibration transverse optique (TO), dit ferro'electrique qui devient mou (sa fr'equence diminue) au centre de la zone de Brillouin lorsque la temp'erature d'ecro?it. Mais pour T ! T q ¸ Tc, le fort mouvement quantique de point z'ero de l'ion central (Ta dans KTaO 3) emp?eche la condensation du mode ferro'electrique, dont la fr'equence se sta­ bilise `a basse 'energie. La constante di'electrique ffl sature `a une tr`es grande valeur, et un fort cou­ plage avec le mode transverse acoustique devient visible. En 1991, K. A. M¨uller, d'apr`es une 'etude RPE, envisage une transition de phase d'un nouveau type dans ces mat'eriaux. B. Hehlen et E. Courtens s'int'eressent alors aux propri'et'es basses fr'equences du SrTiO 3 et du KTaO 3. Les 'etudes de diffusion neutronique, mais surtout Brillouin, montrent alors, parmi les nombreuses anomalies, une nouvelle excitation tr`es basse fr'equence, qu'ils attribuent au second son, la propagation ondula­ toire de la chaleur. Ce travail pr'esente des r'esultats originaux obtenus en diffusion neutronique et Brillouin `a basse temp'erature (5 ­ 300 K). Une param'etrisation des nappes de phonons dans KTaO 3 a servi de base au calcul de la largeur normale des phonons, de la vitesse th'eorique du second son, dans le cas o`u il serait <b>effectivement</b> observable, et du spectre complet de diffusion Brillouin par des processus de diff'erence de deux phonons. La comparaison des simulations avec l'exp'erience montre un bon ac­ cord pour le calcul des largeurs et des nappes, et le processus physique observ'e en spectroscopie Bril­ louin est probablement celui de diff'erence de deux phonons. Les diff'erents aspects de la dynamique des phonons basse fr'equence peuvent ?etre unifi'es au sein d'une th'eorie de diffusion Brillouin au sec­ ond ordre. Mots clefs : phonons, para'electriques quan­ tiques, second son, anharmonicit'e, diffusion neu­ tronique, spectroscopie Brillouin, mod'elisation et simulation num'erique, KTaO 3, SrTiO 3...|$|E
40|$|Like {{the three}} other {{experiments}} at RHIC (Relativistic Heavy Ion Collider) in the Brookhaven National Laboratory near New York, STAR (Solenoidal Tracker At RHIC) {{is dedicated to the}} search of a particular state of nuclear matter, predicted by lattice QCD (Quantum ChromoDynamics) calculations : the Quark Gluon Plasma (QGP). This state, supposed to be that of the Universe a few fraction of second after the Big Bang, should consist according to its first definition (1975) in a matter where quarks and gluons are deconfined and without any interaction. It could be created in laboratory with ultra-relativistic heavy ion collisions allowing to reach extreme temperatures and pressure. After 20 years of research in Europe and United States, CERN annnnounced (on february 20 th, 2000) that a particular state of matter as been created, compatible with a QGP state, but {{it was not possible to}} characterize it completely. RHIC experiments then take over. Today, all the numerous new results that have been collected let us believe that indeed an atypical state of nuclear matter has been reated at RHIC and our understanding of the QGP as a perfect partonic state without any interaction has been reviewed. A new acronym has been defined : sQGP for Strongly Interacting QGP. This has been obtained by the characterization of the heavy ion collision evolution, from a chemical and dynamical point of view, by comparing effect in heavy ion collisions (for which conditions should be satisfied to form a QGP to collisions at lower energies or involving lighter ions that can not produce the requiered conditions. The QGP is indeed to furtive to be probed directly. My "Habilitation à Diriger des Recherches" presents results of the analyses that I have directed and that contributed to highlight the formation of a new state of matter at RHIC and this new conception of the QGP. Signs of the QGP have been searched with strange particles : resonances of particles containing one strange quark and baryons with two strange quarks. The production of strange resonances provides indeed information on the hadronization phase (when partons recombined in hadrons) : according to their measurement or not, it is possible to characterize the chemical freeze-out (at which inelastic interactions stop) and cinetic freeze-out (at which elastic interactions stop), if these two freeze-outs coincide or not, if there is some delay between them. The idea is the following : Lambdas(1520) rescatters in a proton and a kaon. Hence, if the time between both freeze-outs is long, products of the desintegration can be rescattered in the dense medium. However, if both freeze-outs coincide or are very close, products of the desintegration are not influenced and the particle mother (resonance) can be identified. Thus, by measuring yields of production of resonances in proton-proton collisions (for which freeze-out coincide) and by comparing to the yields obtained in Au–Au collisions, it appears that indeed at least both freeze-outs are separated by 4 fm/c in Au–Au collisions. This conclusion is an important step in the understanding of ultra_relativistic heavy ion collisions. This analyse was very original within the STAR Collaboration because it was the first study done on strange resonances. Dedicated algorithms have been developed and are used in the collaboration which studies various other resonances or exotic particles. The production of strange baryons was extensively investigated in the passed experiments since an anormal enhancement of the yields of production is expected if a QGP is formed. CERN experiments have indeed observed an overproduction of strangeness in Pb-Pb collisions but were not able to conlude if a QGP has been created or not since it was possible to explain their results by hadronic gas models. We have realised a similar analysis with the STAR data by comparing the yields of multi-strange baryons (Xi containing two strange quarks), in proton–proton and Au–Au collisiobs at sqrt(s_NN) = 200 GeV. Again, results remain ambigous. These results lead some physicists to the conclusion that strange particle yields can not be viewed as a potential signature of the QGP. However, strangeness comes on stage in a more indirect manner, providing very various information and on the various phases of the collision. Xi particles indicated firstly that the system created at the full energy at RHIC is thermally and chemically equilibrated (at least in strangeness production. Chemical freeze-out temperatures are close to the temperature predicted by QCD for the phase transition. We have also investigated dynamical collective effects (flow phenomenon) coming from interactions between constituants and lead to an emision of matter in particular directions in phase space. In agreement with their small interaction cross sections, Xi baryons seem to decouple earlier than the lighter particles. However, the fact that baryons suffer a substantial flow, may indicate that they have develop a flow hence that they encountered interactions, before the hadronic phase, differently speaking during the partonic phase. Partons seem to interact ontrarily to the first expectations and predictions of the theoricians 20 years ago. Moreover, during 2003, the fourth RHIC experiments revealed conjointly the discover of the jet-quenching effect in heavy ion collisions : it corresponds to a suppression of charged particles at high transverse momentum which is nowadays explained by the energy loss of partons in a dense medium. This effect has been studied in the case of Xi particles and showed that jet-quenching affects also baryons and also that they have a different behaviour than the mesons. A particle type dependance has been shown in agreement to coalescence models claiming that hadrons come from quark recombination. This point highlights the fact that quarks may be the relevant degree of freedom. From these results among others, some theoricians claim the QGP discover at RHIC but experimentalists are more careful and prefer to confirm their results with the study of other signatures. These last five years were particularly exciting with the progress of our knowledges thanks to the wonderful results produced by the fourth RHIC experiments. The QGP has evolved : it is not anymore a perfect plasma without any interactions but a sQGP. A l'instar des trois autres expériences auprès du collisionneur RHIC (Relativistic Heavy Ion Collider) du Brookhaven National Laboratory près de New York, STAR (Solenoidal Tracker At RHIC) est entièrement consacrée à la mise en évidence de cet état particulier de la matière nucléaire prédit par les calculs de QCD (Quantum ChromoDynamics) sur réseau : le plasma de quarks et de gluons (QGP pour Quark Gluon Plasma). Cet état, supposé être celui de l'Univers quelques fractions de secondes après le Big Bang, consisterait d'après sa définition originelle de 1975, en une matière dans laquelle quarks et gluons seraient déconfinés, sans interaction. Il pourrait être créé en laboratoire lors de collisions d'ions lourds réalisées à des énergies ultra-relativistes afin d'atteindre des températures et densités d'énergie extrêmes. Après quasiment 20 ans de recherche auprès des différents accélérateurs de particules américains et européens, le CERN annonce le 10 février 2000 au cours d'une conférence de presse, la mise en évidence expérimentale d'un état particulier de la matière nucléaire, compatible avec la formation d'un QGP, sans pouvoir toutefois le caractériser pleinement. Les expériences du RHIC ont alors pris le relais. Aujourd'hui, au travers une pléthore de résultats nouveaux et parfois bien surprenants, il apparaît de façon de plus en plus certaine, qu'effectivement un état atypique de matière nucléaire a été créé à RHIC et notre vision du QGP comme un gaz parfait de partons n'interagissant que très faiblement, a depuis changé. Un nouvel acronyme a été défini : sQGP pour Strongly Interacting QGP. Pour parvenir à cette observation, il a fallu passer par la caractérisation même de l'évolution des collisions d'ions lourds, du point de vue chimique et dynamique, en comparant les phénomènes des collisions d'ions lourds pour lesquelles les conditions devraient être réunies pour former un QGP à des collisions d'énergies moindres ou de systèmes plus légers qui ne peuvent permettre cette formation. Le QGP est en effet produit de manière beaucoup trop furtive pour pouvoir le sonder directement. Mon mémoire d'Habilitation à Diriger des Recherches présente les résultats des analyses que j'ai menées et qui ont contribué à la mise en évidence de la formation d'un état nouveau au RHIC et à cette nouvelle vision du plasma. Les stigmates du QGP ont été recherchés avec les particules contenant des quarks étranges : les résonances de particules simplement étranges et les baryons doublement étranges. La production des résonances étranges Lambda(1520) apporte en effet des informations sur la phase d'hadronisation du plasma (lorsque les partons se recomposent en hadrons) : selon leur observation ou non, il pourrait être possible de caractériser le freeze-out chimique (instant où les interactions inélastiques cessent et la composition chimique du système est figée), le freeze-out cinétique (instant où les interactions élastiques cessent et les particules n'interagissent plus), si ces deux freeze-out coïncident ou si, au contraire ils sont séparés dans le temps et de combien. L'idée est la suivante : les Lambdas(1520) se désintègrent quasiment instantanément en un proton et un kaon. Par conséquent, si le temps entre les freeze-out chimique et cinétique est long, les produits de désintégration de ces particules peuvent être absorbés dans le milieu dense qui a été créé. En revanche, si les deux freeze-out coïncident ou sont très proches, les produits de désintégration ne sont pas affectés et la particule mère, c'est-à-dire la résonance, peut être identifiée. Ainsi, en mesurant les taux de production de ces particules dans les collisions proton–proton pour lesquelles les deux freeze-out coïncident, et en comparant les taux obtenus dans les collisions Au–Au, à l'énergie nominale du RHIC, il est apparu qu'effectivement, au moins 4 fm/c séparent les deux freeze-out dans les collisions Au–Au. Cette conclusion constitue une étape importante dans la compréhension des collisions d'ions lourds ultra-relativistes et du comportement de la matière dans des conditions extrêmes. Cette analyse est apparue comme originale au sein de la collaboration STAR, étant la première étude sur les résonances étranges. Des algorithmes spécifiques ont dû être mis au point et sont largement utilisés au sein de la collaboration qui depuis étudie de nombreuses autres résonances ou recherche des objets plus exotiques. La production des baryons étranges a été largement investiguée les années passées car une augmentation « anormale » des taux de production est attendue si un QGP est formé. Les expériences du CERN ont observé <b>effectivement</b> une surproduction de l'étrangeté dans les collisions Pb–Pb mais n'ont pu conclure de manière décisive quant à une formation éventuelle d'un plasma car ces résultats pouvaient être également reproduits par des modèles de gaz de hadrons. Nous avons mené une analyse similaire avec les données de STAR en comparant les taux de production des Xi, baryons doublement étranges, dans les collisions proton–proton et Au–Au à sqrt(s_NN) = 200 GeV. Là aussi, les résultats sont demeurés ambigus. Ainsi, ces résultats ont conduit un certain nombre de physiciens à ne plus considérer les taux de production de particules étranges comme une signature robuste de la formation d'un QGP. En revanche, l'étrangeté est revenue sur le devant de la scène, de façon plus indirecte donnant des informations très diverses et sur les différentes étapes de la collision. Les Xi ont révélé tout d'abord que le système créé à l'énergie nominale du RHIC serait en équilibre thermique et chimique et que les températures de freeze-out chimique sont proches de la température de déconfinement prédite par QCD. Nous avons également étudié les phénomènes dynamiques collectifs, appelés flot, qui naissent des interactions entre constituants et se traduisent par une émission de matière dans des directions privilégiées de l'espace de phase. En accord avec leurs faibles sections efficaces d'interaction, les Xi semblent émis bien plus tôt que les particules plus légères. Toutefois, le fait que ces baryons subissent un flot important, laisse supposer qu'elles auraient développé un flot, donc qu'elles auraient été soumises à des interactions, avant la phase d'hadronisation, autrement dit, dans une phase partonique. Les partons subiraient donc des interactions résiduelles, contrairement à ce que préconisaient les théoriciens du milieu des années soixante-dix. Par ailleurs, en 2003, les quatre expériences du RHIC ont révélé conjointement la mise en évidence du phénomène de jet-quenching dans les collisions d'ions lourds : il traduit une diminution de la production de particules chargées de très haute impulsion transverse s'expliquant par la perte d'énergie des partons dans un milieu très dense. Nous avons réalisé cette analyse en considérant les X et montré que non seulement ces baryons subissent un jet-quenching mais aussi qu'ils ont un comportement différent de celui des mésons. Une dépendance des phénomènes dynamiques au type de particules a ainsi été mise en évidence en accord avec les modèles de coalescence préconisant que les hadrons se forment à partir de la recombinaison des quarks. Là aussi, émergence des partons comme degrés de liberté pertinents. A partir de ces résultats entre autres, certains théoriciens affirment la découverte du QGP à RHIC mais les expérimentateurs sont plus prudents et désirent auparavant confirmer et enrichir leurs résultats par l'étude d'autres observables qui viendraient corroborer ces observations. Ces années ont été particulièrement stimulantes par l'évolution de nos connaissances grâce aux formidables résultats produits par les quatre expériences du RHIC. Les « vielles » signatures ont fait peau neuve se transformant en sondes nouvelles et riches en informations originales. La conception du QGP a évolué : il ne s'agit plus d'un gaz parfait constitué de partons évoluant librement mais d'un sQGP...|$|E
40|$|This thesis {{presents}} various quantal {{approaches for}} {{the exploration of}} dynamical processes in multielectronic systems, especially after an intense excitation which can possibly lead to dissipative effects. Mean field theories constitute useful tools in that respect. Indeed in such an approach, individual couplings are replaced by an effective coupling to a common field (the so-called ``mean-field"). This allows one to compute the dynamical evolution of a quantum system for a low computational effort. These theories have been refined over the last decades {{with the development of}} density functional theory (DFT). Since we are mainly interested in time-dependent nonlinear phenomena, the background of this thesis is its dynamical version, that is, time-dependent density functional theory (TDDFT). One can formally show that there exists an effective mean field that reproduces the exact evolution of the electronic density of the system. Therefore, any electronic observable can be expressed as a functional of the density. However, the exact functionals are still unknown and one has to deal with approximations in practice. A lot of work on these approximations has been dedicated {{during the past two decades}} to make them more and more accurate. Nevertheless, they have strong difficulties to capture full 2 -body dynamics and correlation. And most of the improvements lie in the linear response theory that in principle performs well only in slightly perturbed systems. On the other hand, for nonlinear problems, approximated (quantal) functionals give sometimes results which can be even worse than those obtained in a semiclassical calculation (i. e., when the two-electron dynamics is treated classically). Thermalization is one of these effects that stems from electron-electron collisions. Thermalization effects have been observed for instance in experiments implying clusters or molecules excited by an intense femtosecond laser. Indeed, the energy absorbed by the cluster can be, after some time, distributed amongst all the electrons of the system, leading to the heating of the cluster. The signature of thermal emission of electrons have been observed experimentally in metal clusters and in the fullerene C 60. During the past years, our group led a systematic study of photoelectrons from C 60, with the observation that dissipative features are indeed underestimated in mean field approaches, especially in the multiphoton regime. After an introductory chapter, we present in Chapter 2 the formalism of the various schemes studied in this thesis, toward the description of such an effect by including collisional terms on top of a mean field theory. Connections between electronic and nuclear systems are strong in that respect. The starting point is Stochastic Time-Dependent Hartree Fock (STDHF). The derivation of Extended TDHF (ETDHF) is also provided. From STDHF, we derive a new scheme, namely Collisional TDHF (CTDHF). The latter scheme constitutes in some sense the main achievement of this thesis. The numerical realizations of each scheme are also discussed in detail. In Chapters 3, 4 and 5, we apply the approaches discussed in Chapter 2 but in various systems. In Chapter 3, we first explore a rare reaction channel, that is the probability of an electron to attach on a molecule. Small water clusters are here studied. These calculations are of great interest in the understanding of irradiation mechanisms in biological systems as this kind of process is one of those expected to cause DNA strand breaks. To that end, a collision potential is introduced in a TDDFT calculation in a pertubative manner. We show that the probability of attachment exhibits peaks that are consistent with experimental measurements, even with a crude approximation made on the potential of interaction. In Chapter 4, a schematic model is derived from the Lipkin-Meshkov-Glick model, which has been widely studied in nuclear physics to describe phase transitions in nuclei. This model is viewed here as an exactly solvable model and therefore serves as the benchmark of STDHF. A thorough comparison between STDHF and the exact solution with a small number of physical particles is performed, by playing with the physical and the numerical parameters entering the model. The time evolution of 1 -body observables agrees well in both schemes, especially what concerns thermal behavior. However coherent oscillations, present in the exact solution, cannot be by nature reproduced by STDHF which treat 2 -particle- 2 -hole jumps in an incoherent way. The overall agreement is nevertheless very satisfactory and thus provides a sound basis for STDHF applied in larger (and consequently less coherent) systems. However, to allow a good description of the dynamics, one is bound to use a large statistics, which can constitute a hindrance of the use of STDHF in larger systems. To overcome this problem, in Chapter 5, we go for a testing of CTDHF developed in Chapter 2 in a one-dimensional system (and without electronic emission). This system consists in electrons in a jellium potential with a simplified self-consistent interaction expressed as a functional of the density. The advantage of this 1 D model is that STDHF calculations are numerically manageable and therefore allows a direct comparison with CTDHF calculations. The collisional interaction potential used here is similar to that introduced in Chapter 3. An initial excitation is produced by a 1 -particle- 1 -hole transition. Playing with this transition allowed us to explore a large range of excitation energy. In all cases, the system relaxes to a thermalized state described by the famous Fermi-Dirac distribution from which we can extract a temperature. In this proof of concept study, CTDHF compares remarkably well with STDHF. This thus paves the road toward an efficient description of dissipation in realistic 3 D systems by CTDHF. CTDHF should also allow us to revisit the fact that a mean-field theory can only provide, in a process strong enough to ionize the system, non-integer numbers of emitted electrons. We have in mind here what we call ``open" systems, at variance to ``close" systems as those studied in Chapter 5. This is however not an easy task at all, and deep formal issues are still to be solved. This is discussed in the last Chapter as a perspective. Once a well founded solution is found, if any, we can tackle 3 D systems for instance irradiated by a strong laser field and compute electronic observables as photoelectron spectra or photoangular distributions, where numerous experimental data already exist, as already mentioned in Chapter 1. From a theoretical perspective, one would be able to quantitatively compare CTDHF to semiclassical calculations. Last but not least, we mention some possible solutions for the inclusion of memory effects (which are at that stage neglected) as a formal improvement of ETDHF, STDHF or CTDHF. Cette thèse présente différentes approches quantiques pour l'exploration de processus dynamiques dans des systèmes multiélectroniques, en particulier après une forte excitation qui peut aboutir à des effets dissipatifs. Les théories de champ moyen sont un outil utile à cet égard. En effet, dans une telle approche, les cloupages individuels sont remplacés par un couplage effectif avec un champ commun (ledit "champ moyen"). Cela permet de calculer l'évolution dynamique d'un système quantique avec un faible coût numérique. Ces théories ont été améliorées dans les dernières décennies avec le développement de la Density Functional Theory (DFT). Comme nous nous intéressons principalement au phénomène non linéaire dépendant du temps, le cadre de cette thèse est sa version synamique, à savoir Time-Dependent DFT (TDDFT). On peut formellement montrer qu'il existe un champ moyen effectif qui reproduit l'évolution exacte de la densité électronique du système. Aussi, tout observable électronique peut être exprimé comme une fonctionnelle de la densité. Cependant, les fonctionnelles exactes restent inconnues et, dans la pratique, il faut procéder à des approximations. Une bonne partie des travaux sur ces approximations a été consacrée, dans les deux dernières décennies, à les rendre de plus en plus précises. Néanmoins, ces théories peinent à reproduire complètement la dynamique et la corrélation à deux corps. Et la plupart des améliorations concerne la théorie de la réponse linéaire qui, en principe, n'est fiable que dans les systèmes soumis à une faible perturbation. D'autre part, pour les problèmes non linéaires, les fonctionnelles (quantiques) approximées donnent parfois des résultats qui peuvent s'avérer pire que ceux obtenus avec un calcul semi-classique (i. e., lorsque la dynamique à deux électrons est traitée classiquement). La thermalisation est un des effets des collisions électron-électron. Les effets de thermalisation ont par exemple été observés dans des expériences sur des agrégats ou des molécules excités par un intense laser femtoseconde. En effet, l'énergie absorbée par l'agrégat peut être, après un moment, répartie entre tous les électrons du système, aboutissant au réchauffement de l'agrégat. La signature de l'émission thermique de l'électron a été observé de façon expérimentale dans lles agrégats métalliques et dans le fullerène C 60. Ces dernières années, notre groupe a conduit une étude systématique des photoélectrons émis par le C 60 et observé que les effets dissipatifs sont <b>effectivement</b> sous-estimés dans l'approche champ moyen, en particulier dans le régime multiphoton. Après un chapitre d'introduction, nous présentons au chapitre 2 le formalisme des différentes méthodes étudiées dans cette thèse, dans la perspective de décrire un tel effet en incluant des termes collisionnels à la théorie du champ moyen. A cet égard, il existe une forte connexion entre les sytèmes électroniques et nucléaires. Le point de départ est : Stochastic Time-Dependent Hartree Fock (STDHF). La dérivation de Extended TDHF (ETDHF) est également fournie. A partir de STDHF, nous déduisons une nouvelle méthode, appelée Collisional TDHF (CTDHF). Cette dernière représente d'une certaine façon le résultat principal de cette thèse. L'implémentation numérique de chacune de ces méthodes est aussi examinée en détail. Dans les chapitres 3, 4 et 5, nous appliquons à différents systèmes les approches présentées au chapitre 2. Dans le chapitre 3, nous étudions d'abord un canal de réaction rare, ici la probabilité d'un électron de s'attacher à une molécule. L'étude concerne de petits agrégats d'eau. Ces calculs sont d'un grand intérêt dans la compréhension des mécanismes d'irradiation dans les systèmes biologiques, étant donné que ce type de processus est susceptible de causer des ruptures de brins d'ADN. A cette fin, un potentiel de collision est introduit dans un calcul TDDFT de manière perturbative. Nous montrons que la probabilité d'attachement dessine des pics cohérents avec les mesures expérimentales, même avec une approximation grossière du potentiel d'interaction. Dans le chapitre 4, un modèle schématique est dérivé du modèle Lipkin-Meshkov-Glick, qui a été largement étudié en physique nucléaire pour décrire des transitions de phase dans les noyaux. Ce modèle, exactement soluble, est ici utilisé pour fournir un point de référence à STDHF. Une comparaison approfondie est réalisée entre STDHF et la solution exacte, avec un petit nombre de particules physiques, en jouant avec les paramètres physiques et numériques du modèle. L'évolution temporelle des observables à un corps s'accorde entre les deux méthodes, plus particulièrement en ce qui concerne le comportement thermique. Cependant, des oscillations dues à la cohérence quantique, présentes dans la solution exacte, ne peuvent pas, par nature, être reproduites par STDHF, qui traite de transitions 2 -particules- 2 -trous de façon incohérente. Dans l'ensemble, l'accord entre les résultats est néanmoins très satisfaisant et fournit ainsi une base solide à STDHF appliquée dans des systèmes plus grands (et par conséquent moins cohérents). Pour autant, pour permettre une bonne description de la dynamique, il est nécessaire d'avoir une grande statistique, ce qui peut être un frein à l'utilisation de STDHF sur de larges systèmes. "Pour surpasser cette difficulté, dans le chapitre 5, nous testons CTDHF, qui a été introduit dans le chapitre 2, sur un modèle à une dimension (et sans émission électronique). Le modèle se compose d'électrons dans un potentiel de type jellium avec une interaction auto-cohérente sous la forme d'une fonctionnelle de la densité. L'avantage de ce modèle à une dimension est que les calculs STDHF sont possibles numériquement, ce qui permet une comparaison directe aux calculs CTDHF. Le potentiel d'interaction collisionnelle ici utilisé est similaire à celui introduit dans le chapitre 3. Une excitation initiale est produite par une transition 1 -particule- 1 -trou. Enjouant avec cette transition nous pouvons explorer une large gamme d'énergies d'excitation. Dans chacun des cas, le système relaxe vers un état thermalisé descrit par la bien connue distribution de Fermi-Dirac, à partir de laquelle il est possible d'extraire une température. Dans cette étude de validité du concept, CTDHF s'accorde remarquablement bien avec STDHF. Cela pose les jalons pour une description efficace de la dissipation dans des systèmes réalistes en trois dimensions par CTDHF. CTDHF devrait également nous permettre de réévaluer le fait qu'une théorie du champ moyen puisse seulement fournir, dans un processus assez robuste pour ioniser le système, des nombres non entiers d'électrons émis. Nous avons ici à l'esprit ce que nous appelons des systèmes "ouverts", par opposition aux systèmes "fermés" comme ceux étudiés dans le chapitre 5. Il ne s'agit pas pour autant d'une tâche aisée : de profondes questions formelles restent à résoudre. Ce point est abordé dans le dernier chapitre en tant que perspective. Lorsque qu'un solution bienfondée, s'il en est, sera trouvée, nous pourrons aborder les systèmes à trois dimensions par exemple irradiés par un fort champ laser et calculer les observables électroniques tel que le spectre de photoélectrons simple ou résolu en angle, pour lesquels de nombreuses données expérimentales existent déjà, comme mentionné dans le chapitre 1. D'un point de vue théorique, il s'avèrerait possible de comparer quantitativement CTDHF aux calculs semi-classiques. Enfin et surtout, nous évoquons de possibles solutions à l'inclusion d'effets de mémoire (qui sont négligés à ce stade) comme une amélioration formelle de ETDHF, STDHF ou CTDHF...|$|E
40|$|This {{thesis is}} about {{immigrants}} in self-employment in Germany. More specifically, {{it is about}} immigrants who make a culturally-endowed practice or knowledge economically available in their new home. These immigrant business owners are generally referred to as ethnic entrepreneurs. Yet, according to research and widely-held public opinions in Germany, a problem that hampers {{the success of these}} businesses is the owner's lack of proficiency in the German language, which apparently impedes them from accessing institutional support and from offering their services to the majority population. However, the ways that language competence actually affects the entry of immigrants into self-employment and the execution of their daily work as business owners has received only insufficient attention, including from a sociolinguistic perspective. 	Hence, this thesis aspires to examine the pathways of immigrants into self-employment and how these pathways are shaped by the owners' language knowledge. Secondly, by analysing the workplace practices in close empirical detail, it aims to understand how the owners' language knowledge impacts the execution of their work and whether these practices constitute sites of language learning for the owners. Thirdly, it intends to document the challenges that the business owners face at their workplaces, in order to understand to what extent they are due to insufficient levels of language knowledge. 	In order to answer these research questions, this study focuses on three businesses owned and operated by first-generation female immigrants from Thailand in the federal state of Saarland in southwest Germany. Businesses by Thai immigrants are particularly interesting {{for the purposes of this}} investigation, because they have flourished in recent years and because they offer their services in markets that target the majority, primarily German-speaking population. 	The ventures are typical examples of ethnic small businesses created by Thai immigrants in Germany: Thai massage salons and food retails stores. The first is a large Thai massage salon run by Kanita, the second a small Thai massage salon managed by Patcharin, and the third business is a food retail store owned and operated by Wipa. The three owners (and their staff) differ in terms of their competence in German. Kanita has only minimal competence in German, Patcharin has partial competence, while Wipa has maximum competence in the language. 	The analysis of the pathways into self-employment of these three owners exhibits a number of similarities. In all three cases, their migration to Germany was triggered by marriage to a German national. Previously, they all had professional careers which were directly connected to their appropriation of English, a language that later enabled communication with their German husbands. While their move to Germany was generally motivated by prospects of a better future, they were unable to find work due to a lack of German proficiency. German was primarily learned informally. Informal learning was fostered by their prior experience of language learning, their individual engagement in the form of self-study and reflexivity, a deliberate exposure to German through media, and their engagement with native speakers in practices towards which that they had developed an affinity. Interestingly, these practices were either their previous professional practices or activities that they took up after migration and later on developed into their own businesses. 	Realising the demand for their services in the open market and among the majority population kindled Kanita's, Patcharin's and Wipa's flame to enter self-employment. They deliberately adapted their services to the requirements of their customers, but also in reaction to prejudices about their professions. The support of their husbands and other associates was also significant, as they handled tasks that were difficult to perform linguistically, as, for example, the registration of the businesses and the preparation of administrative paperwork. In sum, these findings suggest that an advanced standard level of German was no prerequisite for these owners to enter self-employment, but that the prospects of engaging in self-employed work acted as an incentive to improve their German language skills. 	A detailed analysis of the workplace practices at Kanita's Massage Salon shows that working with minimal competence in German is possible, but that its success depends on several factors. To complete their workplace actions, Kanita and her staff draw on both resources in their linguistic repertoires, Thai and German. In addition, all workplace actions are designed as routine actions with discursive routines that all staff members are able to master quickly. Their successful accomplishment also depends on the customers' familiarity with these actions. Therefore, formal inconsistencies in German do generally not impede the performance of work, but problems are primarily due to the staff's or the customer's inexperience with the routines. If problems occur, they tend to be solved in cooperation with colleagues or by drawing on the material resources available to the staff members. 	In comparison to Kanita, Patcharin and her staff have partial competence in German, which is instrumental for the performance of the key practices at their workplace. The analysis shows that their restricted competence in German is important for their work, as it provides the tools for Patcharin and her staff to perform discursive practices during the massage treatment, such as finding out about their clients' health problems, building rapport with clients, giving instructions or clarifications about the treatment, or providing assessments of their customers' health issues. Talk is an important part of the massage treatment at Patcharin's salon and it aids to construct the professional identity that Patcharin claims for herself, namely to provide a high-quality and personalised service to her customers. Thai is less relevant, but in interactions between Patcharin and her staff it serves to exchange information and coordinate work. 	Wipa has maximum competence in German and Thai, which allows her to manage her store and serve her customers independently and competently in line with her professional aspirations. The key practices at her store of explaining and ordering stock illustrate how Wipa relies on both the use of Thai and German to effectively perform these actions. Her maximum competence in German and in Thai permits her to make 'rational' choices about the suitability of her suppliers and to provide her customers with advice that is tuned to their linguistic and cultural background. 	The conclusions drawn from these findings are that an advanced competence in German in not a prerequisite for immigrant entrepreneurs to start their businesses. The owners attune their workplace actions to the level of competence in their linguistic repertoires and operate effectively. Moreover, self-employed work provides the owners with the necessary motivation and the need to appropriate German. On the other hand, the data suggest that a greater proficiency in German becomes important, if the immigrant entrepreneur wants to differentiate her business from direct competitors, as it allows them to move beyond the concrete performance of routine actions. Cette thèse a pour sujet les immigrés établis professionnellement à leur compte en Allemagne. Elle concerne plus précisément les immigrés qui rendent une pratique ou un savoir chargé culturellement abordable dans leur nouveau foyer. On qualifie généralement ces immigrés chefs d’entreprises d’entrepreneurs ethniques. Cependant, selon les recherches et selon l’opinion publique largement répandue en Allemagne, le manque de maîtrise de l’allemand de ces entrepreneurs serait une entrave sérieuse à leur succès. Cela les empêcherait visiblement d’avoir accès aux appuis institutionnels et de proposer leurs services à une large part de la population. Toutefois, la façon dont les compétences langagières affectent <b>effectivement</b> l’accès des immigrés à l’entrepreneuriat ainsi que l’exécution de leur travail quotidien en tant que chefs d’entreprise n’a reçu que peu d’attention, y compris du point de vue de la sociolinguistique. 	Par conséquent, nous souhaitons tout d’abord étudier l’accès à l’entrepreneuriat des immigrés ainsi que la façon dont cet entrepreneuriat est façonné par leurs compétences linguistiques. Dans un deuxième temps, nous analyserons empiriquement et dans le détail les pratiques professionnelles afin de comprendre non seulement dans quelle mesure les compétences linguistiques des entrepreneurs impactent la réalisation de leurs tâches professionnelles mais aussi si ces tâches professionnelles sont une manière d’acquérir des compétences linguistiques. Dans un troisième temps, il s’agira d’analyser les défis auxquels sont confrontés les entrepreneurs sur leurs lieux de travail, afin de comprendre dans quelle mesure ces obstacles sont dus à des niveaux de langue insuffisants. 	Pour répondre à ces questions de recherche, nous allons orienter essentiellement notre étude sur l’analyse de trois entreprises tenues et exploitées par trois femmes issues de la première génération d’ immigrés thaïlandais dans l’état fédéral de la Sarre. Ces entreprises tenues par des immigrés thaïlandais sont particulièrement intéressants pour notre étude non seulement car elles sont devenues plus nombreuses ces dernières années mais aussi parce qu’elles proposent leurs services sur un marché qui cible prioritairement et majoritairement la population germanophone. 	Ces entreprises sont des exemples typiques de petits commerces créés par des immigrés thaïlandais en Allemagne : les salons de massages thaïlandais et les magasins d’alimentation. Le premier commerce est un grand salon de massage thaïlandais tenu par Kanita, le deuxième est un petit salon de massage thaïlandais dirigé par Patcharin et le troisième est un magasin d’alimentation détenu et exploité par Wipa. Les trois chefs d’entreprise (et leur personnel) ont des compétences linguistiques différentes en allemand. Kanita ne possède que des compétences linguistiques minimum en allemand, Patcharin maîtrise l’allemand de façon partielle alors que Wipa possède de solides compétences linguistiques. 	L’analyse de l’accès à l’entrepreneuriat de ces trois chefs d’entreprise montre un certain nombre de similitudes. C’est en épousant un allemand que les trois femmes ont immigré en Allemagne. Auparavant, elles avaient toutes les trois une carrière professionnelle où une bonne connaissance de l’anglais était indispensable. C’est d’ailleurs en anglais que les trois femmes communiqueront par la suite avec leurs maris. Alors que leur emménagement en Allemagne était motivé par l’espoir d’une vie meilleure, elles ont été incapables de trouver un emploi du fait de leur manque de compétences en allemand. L’allemand a d’abord été appris de façon informelle. L’apprentissage informel a été favorisé par leur expérience préalable de l’apprentissage d’une langue étrangère, leur engagement individuel par le biais de l’auto-apprentissage et d’une approche introspective, une exposition délibérée à l’allemand à travers les médias et leur engagement auprès de natifs (de langue maternelle allemande) dans des activités pour lesquelles elles ont développé des affinités. Il est intéressant de noter qu’il s’agissait soit d’une activité professionnelle déjà exercée par le passé, soit d’activités vers lesquelles elles se sont tournées après leur immigration, et dont elles ont ensuite fait leur propre profession. 	C’est en réalisant qu’il y avait une demande pour leurs services dans un marché ouvert et parmi une majorité de la population que la flamme de l’entrepreneuriat s’est allumée dans le cœur de Kanita, de Patcharin et de Wipa. Elles ont délibérément adapté leurs services aux attentes de leur clientèle mais aussi en réaction aux préjugés liés à leurs professions. Le soutien de leurs époux et d’autres associés a été déterminant puisqu’ils ont pris en charge les tâches difficiles à accomplir sans maîtrise de l’allemand, comme par exemple l’inscription au registre des commerces et la gestion des documents administratifs. En résumé, les découvertes suggèrent qu’un niveau avancé en allemand n’est pas un pré requis indispensable à l’accès à l’entrepreneuriat mais que la perspective de s’engager dans le domaine de l’entrepreneuriat agit comme une motivation pour améliorer ses compétences en allemand. 	Une analyse détaillée des pratiques professionnelles de Kanita dans son salon de massage montre qu’il est possible de travailler en possédant un niveau minimal en allemand. Mais le succès dépend alors de plusieurs facteurs. Pour réaliser au mieux leurs activités professionnelles, Kanita et son personnel puisent dans les ressources linguistiques à leur disposition, en thaï et en allemand. De plus, chaque action réalisée au salon de massage est conçue comme une série d’actions discursives routinières que chaque membre du personnel est capable de maîtriser rapidement. Le plein succès de l’entreprise repose aussi sur le fait que les clients connaissent bien son fonctionnement. Par conséquent, des incohérences formelles en allemand ne gênent pas l’efficacité du travail. Les problèmes viennent alors principalement d’une méconnaissance des routines de travail, de la part des clients ou du personnel. Lorsqu’un problème survient, on tend à le résoudre en faisant appel à la coopération entre collègues ou en utilisant les ressources matérielles accessibles aux membres du personnel. 	En comparaison, Patcharin et son personnel possèdent des compétences linguistiques partielles en allemand, ce qui joue un rôle important pour la réalisation des principales actions dans le salon de massage. L’analyse montre que leurs compétences en allemand, même restreintes, sont importantes pour leur travail car il fournit à Patcharin et à ses employés les outils nécessaires à l’élaboration de pratiques discursives pendant le massage. Ils peuvent ainsi déterminer les problèmes de santé de leurs clients, construire des relations avec eux, donner des instructions ou des précisions quant à un traitement ou fournir une évaluation liée à leurs questions de santé. Parler est une part importante du traitement par le massage dans le salon de Patcharin et la parole permet de construire l’identité professionnelle qu’elle affirme être la sienne, notamment pour fournir un service haut de gamme et personnalisé à ses clients. Le thaï est moins pertinent, mais les interactions entre Patcharin et ses employés sont utiles à l’organisation du travail et à l’échange d’informations. 	Wipa possède des compétences importantes en allemand et en thaï, ce qui lui permet de diriger son commerce et de servir ses clients en toute indépendance et avec compétence, conformément à ses aspirations professionnelles. Les principales actions permettant à Wipa d’expliquer et d’organiser son stock montrent bien comment elle utilise aussi bien le thaï que l’allemand afin de les réaliser efficacement. Ses compétences en allemand et en thaï lui permettent de faire des choix rationnels quant aux choix des fournisseurs appropriés, elle peut également fournir à ses clients une écoute attentive et des conseils selon leur langue et de leur culture. 	Les conclusions qui se dessinent à la lumière de ces résultats tendent à indiquer qu’un niveau avancé en allemand n’est pas un pré requis à l’accès pour les immigrés à l’entrepreneuriat. Les chefs d’entreprise adaptent alors leurs tâches professionnelles à leur propre niveau de compétences linguistiques et ils sont alors efficaces. D’ailleurs, un entrepreneur trouve dans son travail la motivation nécessaire et le besoin de s’approprier l’allemand. D’un autre côté, les données suggèrent qu’une meilleure connaissance de l’allemand est fondamentale si l’entrepreneur tient à se démarquer de ses concurrents puisque cela lui permet d’aller au-delà de la réalisation des tâches routinières...|$|E
40|$|This {{work is a}} {{contribution}} {{to the development of a}} specific method to assess the presence of residues in agricultural commodities. The following objectives are formulated: to identify and describe main processes in environment — plant exchanges, to build of a model to assess the residue concentration at harvest in agricultural commodities, to understand the functioning of the modelled system, to characterise pesticides used in field crops and identify optimisation potentials in phytosanitary measures. The frame for the methodological developments corresponds to the procedure for the evaluation of the toxicity provided for the Life Cycle Impact Assessment methodology and for the method Impact 2002 +. In chapter 2, the methodological procedure for the assessment of human toxicity potential is introduced. First the factors of fate and exposure are described, including the notion of harvest fraction, the amount of substance found in harvest per unit of substance emitted initially in the system, the main result of the present study. Then the effect factors and the framework for impact evaluation are introduced. Chapter 3 describes the principles accounted for the building of the fate model. Wheat crop and a restricted list of substances are chosen for these methodological developments. The model is composed by compartments describing the environment and the plant. Its functioning is based on initial amounts of substance in the source compartments, on transfer rates linking the compartments and on a dynamic evolution as a function of time between the treatment and the harvest. Air, soil and formulation deposit on plant are the primer compartments receiving the treated substance. Each transport is described by a transfer rate accounting for the process and for the equilibrium partitioning between the two exchanging compartments. Degradation of substance and plant growth are additional processes considered. Each compartment is described by a linear differential equation for the variation of mass accumulating and dissipating. Their assembly builds the model solved as a function of time. This exact resolution is complemented by additional tools to better understand the system functioning and to provide further approximations of the results: the system is simplified into subsystems describing the source and the receiving plant compartment and analytically solved using interpretable equations. Chapter 4 describes and discusses all transport and dissipation processes determining the fate of the substance in the limits of the system. The recent publications concerning the understanding and the modelling of pesticide transfer from formulation deposit on plant through the cuticular membranes give new possibilities to model pesticide fate and to better account for the direct applications on the plant. In chapter 5, the core model is first applied and its functioning analysed. The low availability and partly unsatisfying quality of data for pesticides description is a main complication for the methodology application: the lack of data for the half-life of the substance in the plant especially leads to a strong extrapolation for this determinant factor. A large difference is observed between early and late applied pesticides with respectively a major release to soil or a release to formulation deposit on plant surface. The initial transport processes quickly distribute the substance in the system. Once each plant compartment has accumulated residues up to a maximum amount, a dissipation phase occurs. The duration of these periods is determinant for the level of residue in harvest. The soil is a determinant source for long term evolutions of the system, for soon applied substances with low degradation rate. The half-life of substance deposited on plant is equal to a few days, but the transfer is fast from formulation deposit to the inner plant, where degradation is generally much slower. The accumulation of substance from the air is mostly negligible. The sum of the subsystems gives an approximation of the total system, useful for interpretation. The possibility to simplify the subsystem by ignoring the transfer back from receiving to the source compartment underlines the low contribution of these transfers in the functioning of the model. An approximated resolution is based on the determination of the maximum accumulated substance and on the subsequent dissipation process. However, an important loss of precision is observed. This approximation is useful for interpretation and for extrapolations. In chapter 6, an evaluation of the model is conducted through a sensitivity and uncertainty analysis. The sensitivity analysis consists of evaluating the effect on the output of a change in an input, on the basis of three complementary approaches: the effect of a fixed change in the input of e. g. 0. 1 %, the effect of a change specific to the uncertainty of the input and the effect of a change in input value from a minimum to a maximum. The uncertainty of an output is evaluated according to the relative contribution of the confidence factors of the inputs. Results show that the half-lives and the time are the most important factors determining the sensitivity of the system and the propagation of uncertainty. The contribution of the half-life to the confidence factor of the harvest fraction reaches between 30 % to 98 % of the total uncertainty. The confidence factors of results increase exponentially with the time interval between application and harvest. The role of partition coefficients to the behaviour of substances is highly variable, may be determinant or negligible, with increasing or limiting effect on mobility. Sensitivity and uncertainty for parameters describing the agricultural or environmental system are very variable, but sometimes determinant and so confirmed as essential for the system functioning. Consequently, differences in harvest fractions between substances are only significant if they are high. A first comparison of the computed results with measures of residues obtained by an experiment and with references such as tolerance values lead to a pertinent verification of the overall methodology. Finally, the qualitative comparison with other models underlines the specificities and the originality of the present methodology in particular by comparison with environmental multi-media models running in steady state. In chapter 7, the model is finally applied for an ultimate interpretation. The harvest fractions for more than 100 substances are evaluated. Among all types of substances, low and high levels of residues per treatment are found, representative for the high variability of harvest fractions from 5 E- 16 for bromoxynil to 7 E- 03 for tebuconazole sprayed on wheat. The fate process represents the highest source of variation for the toxicity. If the application rate does not explain the high differences in residue level at harvest, the time of application may represent an optimisation potential particularly for late treatments. However, the toxicity needs to account for both fate and effect factors, as only their combination effectively allows to evaluate the toxicity. According to the available list of Human Damage Factors per treatment, problematic substances may be effectively identified and substituted. In chapter 8 answers to questions brought with the objectives bring a conclusion to the study. The appendices include notably the results of harvest fractions and toxicity per unit substance applied, per treatment and per unit cultivated crop area, for the main substances and field crops. A LCA is also presented on the intensity level of wheat production. Ce travail contribue au développement d'une méthode pour l'évaluation de la présence de résidus dans les produits agricoles. Les objectifs suivants sont formulés: identifier et décrire les principaux processus d’échanges entre l'environnement et la plante, créer un modèle pour évaluer la concentration en résidus au moment de la récolte, comprendre le fonctionnement du système modélisé, caractériser les pesticides utilisés dans les grandes cultures et identifier les potentiels d'optimisation dans la lutte phytosanitaire. Le cadre de ces développements correspond à la procédure d'évaluation de la toxicité de la méthode de l'Analyse du Cycle de Vie et de la méthode Impact 2002 +. Dans le chapitre 2, la procédure méthodologique pour l'évaluation du potentiel de toxicité humaine est introduite. D'abord, les facteurs de devenir et d'exposition sont décrits, incluant la notion de fraction récoltée, la quantité de substance trouvée dans les récoltes par unité de substance émise initialement dans le système, le principal résultat de cette étude. Ensuite, les facteurs d'effet et le cadre de l'évaluation de l'impact sont introduits. Le chapitre 3 décrit les principes considérés pour la création du modèle. La culture du blé et une liste réduite de substances sont choisis pour les développements méthodologiques. Le modèle est composé de compartiments décrivant l’environnement et la plante. Son fonctionnement se base sur les quantités initiales de substance dans les compartiments sources, sur les taux de transfert reliant des compartiments et sur une évolution dynamique en fonction du temps entre le traitement et la récolte. L'air, le sol et le dépôt de substance sur la plante sont les compartiments primaires recevant la substance traitée. Chaque transport est décrit par un taux de transfert comprenant le processus et l'équilibre de partition entre les deux compartiments d'échange. La dégradation de la substance et la croissance de la plante sont des processus supplémentaires considérés. Chaque compartiment est décrit par une équation différentielle linéaire pour la variation de masse accumulée et dissipée. Leur assemblage compose le modèle, résolu en fonction du temps. Cette résolution exacte est complétée d'outils additionnels pour mieux comprendre le fonctionnement du système et fournir des approximations supplémentaires des résultats: le système est simplifié en sous-systèmes décrivant la source et le compartiment plante, et est résolu par des équations interprétables. Le chapitre 4 décrit et discute tous les processus de transport et de dissipation déterminant le devenir de la substance dans les limites du système. Les publications récentes concernant la compréhension et la modélisation du transfert de pesticides depuis les produits déposés sur la plante à travers les membranes cuticulaires donnent de nouvelles possibilités de modéliser le devenir des pesticides et de mieux considérer les applications directes sur la plante. Dans le chapitre 5, le modèle est d'abord appliqué et son fonctionnement est analysé. La faible disponibilité et la qualité partiellement insatisfaisante des données pour la description des pesticides constitue la principale complication dans l'application du modèle : l'absence de données pour la demi-vie des substances dans la plante conduit en particulier à une extrapolation forte pour ce facteur déterminant. Une différence importante est observée entre les pesticides appliqués précocement ou tardivement, respectivement entre un apport majeur vers le sol ou un apport majeur vers la surface de la plante. Les processus initiaux de transports distribuent rapidement la substance dans le système. Après que chaque compartiment eut accumulé une quantité maximale de résidus, une phase de dissipation survient. La durée de ces périodes est déterminante pour le niveau de résidus. Le sol est une source déterminante pour des évolutions de longue durée et pour des substances avec une faible dégradation. La demi-vie d'une substance déposée sur la plante est égale à quelques jours, mais le taux de transfert est rapide vers l'intérieur de la plante, où la dégradation est plus lente. Les contributions depuis l'air sont la plupart du temps négligeables. La somme des sous-systèmes donne une approximation du système utile pour l'interprétation. La possibilité de simplifier le système en ignorant le transfert de retour vers la source souligne la faible contribution de ces transferts dans le fonctionnement du modèle. Une résolution approximative est basée sur la détermination de la quantité maximale de substance accumulée et sur sa dissipation subséquente. Toutefois une perte importante de précision peut être observée. Cette approximation est utile pour l'interprétation ou pour certaines extrapolations. Le chapitre 6 comprend une évaluation du modèle. L’analyse de sensibilité consiste à évaluer l’effet du changement d’un paramètre sur le résultat, selon trois approches: l’effet d’un changement fixe par exemple de 0, 1 %, l’effet d’un changement spécifique à l’incertitude du paramètre, et l’effet d’un changement considérant les valeurs minimales et maximales du paramètre. L’incertitude du résultat est évaluée sur la base de l{{a contribution}} relative des facteurs de confiance des paramètres. Les résultats montrent que les demi-vies et le temps sont les facteurs les plus importants déterminant la sensibilité du système et la propagation de l'incertitude. La contribution de la demi-vie au facteur de confiance de la fraction récoltée atteint entre 305 et 98 % du total de l’incertitude. Les facteurs de confiance des résultats augmentent de façon exponentielle avec l’intervalle entre le traitement et la récolte. Le rôle des facteurs de partition dans le comportement des substances est très variable, peut être déterminant ou négligeable, avec un effet croissant ou limitant sur la mobilité. La sensibilité et l'incertitude des paramètres décrivant le système environnemental ou agricole sont très variables, parfois déterminants, et ainsi confirmés comme essentiels au fonctionnement du système. Par conséquent, seules de larges différences de fractions récoltées entre substances sont significatives. Une première comparaison des résultats modélisés avec des mesures de résidus obtenues par une expérimentation et avec des références comme les valeurs de tolérance conduisent à une vérification pertinente de la méthodologie. Finalement, la comparaison qualitative avec d'autres modèles souligne la spécificité et l'originalité de la présente méthodologie, en particulier par la comparaison avec des modèles environnementaux multi-media évoluant en état stationnaire. Dans le chapitre 7, le modèle est finalement appliqué pour une ultime interprétation. L'évaluation porte sur une plus large série de substances. Les fractions récoltées pour plus de 100 substances sont évaluées. Parmi tous les types de substances, des niveaux bas et élevés de résidus par traitement sont trouvés, représentatifs de la variabilité des fractions récoltées, de 5 E- 16 pour le bromoxynil à 7 E- 03 pour le tébuconazole utilisés sur le blé. Le processus de devenir représente la source la plus élevée de variation pour l'évaluation de la toxicité. Si la dose de traitement n'explique pas les larges différences de résidus à la récolte, le moment du traitement peut représenter un potentiel d'optimisation, en particulier pour les traitements tardifs. Toutefois, l'évaluation de la toxicité doit prendre en compte les deux facteurs, puisque seule leur combinaison permet <b>effectivement</b> d'évaluer la toxicité. Sur la base de la liste actuellement disponible des facteurs de dommages sur l'humain, les substances problématiques peuvent être identifiées et substituées. Dans le chapitre 8, les réponses aux défis et questions soulevées avec les objectifs apportent une conclusion à l'étude. Les annexes de l'étude comprennent notamment les résultats des fractions récoltées, des résultats de toxicité par kg appliqué, par traitement, par unité de surface cultivée, pour les principales substances et grandes cultures. Une analyse de cycle de vie est également présentée pour le niveau d'intensité de production du blé...|$|E
40|$|Aiming {{to define}} {{irrigation}} strategies improving the water productivity by folder crops under water scarcity in the irrigated perimeter of Tadla (Morocco), this work combines field experimentation and modeling. The field study of crop response to water stress {{is important to}} maximize yield and improve agricultural water use efficiency (WUE) in areas where water resources are limited. On the silage maize, {{the results showed that}} water deficit affected plant height growth, accelerated the senescence of the leaves and reduced the leaf area index. Dry matter yields varied from 3. 9 t. ha- 1 under T 5 (20 % ETc) to 16. 4 t. ha- 1 under T 1 (100 % ETc). The establishment of the water budget by growth phase showed that the water use efficiency was higher during the linear phase of growth. WUE calculated at harvest varied between 2. 99 kg. m- 3 under T 1 and 1. 84 kg. m- 3 under T 5. The actual evapotranspiration under T 1 (100 % ETc) was 478 mm and 463 mm in 2009 and 2010, respectively. The yield response factor (Ky) for the silage maize for both growth seasons was 1. 12. The ETc of silage maize was determined using lysimeter drainage at 415 mm. the mean values of crop coefficients Kc were 0. 56, 1. 22 and 1. 05 for beginning phase, mid-season and at harvest (grain milky pasty stage) respectively. Drip irrigation allows obtaining dry matter yields similar to flood irrigation but with less water and saves about 30 % of irrigation water applied. Over five cycles, berseem dry biomass yields achieved under T 1 are 14. 3 and 13. 9 t/ha in 2009 / 10 and 2010 / 11 respectively. The yield reductions by applying 60 % of water requirements are 40 and 42 % in 2009 / 10 and 2010 / 11 respectively. Berseem daily productivity increases with more water applied with the highest value of 102 kg DM/ ha/ day. The dry matter content increases with water stress. The mean values range between 12. 3 and 23. 7 % under T 1 (100 % ETc) and T 4 (40 % ETc) respectively. The contribution of without irrigation cycles (rainy period) on the total annual yield may vary from 35 % to 52 % under treatments T 1 and T 4 respectively. Water balance achieved by water regime shows that drainage losses increase with more water applied especially in the first cycle. WUE is low during the first cycle, optimal in 2 nd, 3 rd and 4 th cycle and decreases in the last one with water stress. Global WUE of berseem determined over the entire crop period (slope of the regression line) is 3. 37 kg. m- 3. The yield response factor (Ky) for the berseem for both growth season was 1. 11. Berseem ETc determined by drainage lysimeter was 520 mm. The Kc values were estimated for each cycle for all three phases: initial, development (median) and mid-season. Maximum yield average under drip irrigation was 15. 7 t/ha and obtained with 411 mm of water supply allowing to save 57 % of water compared to traditional irrigation technique. Comparing the behavior of six alfalfa varieties most commonly practiced in the irrigated perimeter of Tadla shows that the "Super Siriver" cultivars followed by «Trifecta» have higher yield potential and higher tolerance to water deficit. Alfalfa maximum annual yield obtained was 24. 2 t. ha- 1. The contribution of the spring cycles to the annual yields range from 55 % under T 1 (100 % ETc) to 65 % under T 4 (40 % ETc). In addition to water quantities, alfalfa yields depend on time application during a growth cycle. WUE varies from cycle to another and from one season to another. The maximum value was 2. 57 kg. m- 3 and obtained in spring 2011, while the low value was 0. 64 kg. m- 3 and obtained in winter 2010. WUE decreases with water stress, with mean values of 1. 83, 1. 67, 1. 54 and 1. 23 kg. m- 3 under T 1 (100 % ETc), T 2 (80 % ETc), T 3 (60 % ETc) and T 4 (40 % ETc) respectively. The yield response factor (Ky) of alfalfa was 0. 92. The determination of the alfalfa water requirements was performed on the basis of cycle’s calendar during two years in 2010 and 2011. The values founded for flood irrigation are 1388 and 1364 mm respectively for the two years. Drip irrigation allows achieving similar dry mater yield to flood irrigation with less water and agronomic efficiency. Water applied under T 1 in drip irrigation with 50 cm of spacing between ramps was less than water requirements (of alfalfa) by about 7 % and 18 % in 2010 and 2011 respectively. Under the same treatment in flood irrigation, the water requirements are exceeded by 16 % and 21 % in 2010 and 2011 respectively. Two crop models, PILOTE and CropSyst, had been selected to be tested on their ability to simulate the growth and yield of the studied crops under the edaphic-climatic conditions of Tadla. Tested on silage maize, both models correctly simulated the growth and development of the crop under different water regimes. The parameters of both models are validated and shown effective for simulation of biomass, leaf area index and soil water storage. Although PILOTE requires less parameters and data than CropSyst, it often proves to be more successful in simulating the biomass of silage maize and water balance. As to berseem, predictions of biomass by CropSyst seem to be more accurate than PILOTE model. The latter was best at predicting the soil water reserve on the soil depth exploited by the roots (0 - 80 cm). Given its ease of integrating daily climatic data for several years, CropSyst model was chosen to test its ability to simulate the crop rotation of berseem and maize silage. The results show that this model correctly simulates the evolution of biomass and yields of the two crops considered in rotation during three years. Modeling the growth and production of alfalfa is made by both models outside the crop installation period (seeding year). After calibration and validation achieved, the model CropSyst simulates adequately biomass and soil water reserve under all water regimes considered while PILOTE best simulations were limited to non-stressed treatment T 1 (100 % ETc). Although CropSyst model takes into account several parameters in the simulation of alfalfa growth, their simplifications (unique values) reduce its performance in more water stress situation. Less parameters considered in the PILOTE model makes it validation difficult for perennial crops such as alfalfa. CropSyst model was used to evaluate irrigation practices of farmers and develop irrigation virtual scenarios for the three crops studied. The assessment shows that virtual scenario developed for alfalfa that applying 1600 mm of irrigation water amount through 14 applications divided into six irrigations during the spring, six in summer, one in the fall and another in early winter maximizes irrigation water efficiency (1. 21 kg/m 3) and achieve a yield of 23. 1 t/ha (95 % of the yield potential). In the case of maize, if water is available, application of 648 mm according to the combination [2 irrigations in initial phase (after sowing), 2 irrigations in linear and two in final phases] allows to achieve high biomass yield and better water use. On berseem, the simulation results confirm that the adoption of the scenario that provides 625 mm through 7 irrigations (3 in autumn, 2 in winter and 2 in spring) allows obtaining 14. 1 t/ha of dry matter which represents 94 % of the yield potential of the 6454 cultivar. This scenario allows greater water efficiency (1. 24 kg/m 3) and results in low water drainage losses estimated at about 17 % of applied water. The comparison of the two cropping systems represented by alfalfa and silage maize-berseem rotation shows that the rotation allows the better water use and mobilizes less water than alfalfa, which is distinguished by its profitability. Finally, the coupling of the results of three years (2008 to 2011) "in situ" experimentations with the simulations of scenarios by CropSyst and PILOTE models has shown to be highly effective in order to improve the folder crops irrigation in the Tadla irrigated area in Morocco. Dans l’objectif d’améliorer la productivité des principales cultures fourragères au périmètre irrigué de Tadla dans des situations hydriques de plus en plus sévères, le présent travail associe l’expérimentation sur terrain et la modélisation. L’étude de la réponse des cultures au déficit hydrique est importante pour maximiser les rendements et améliorer l’efficience d’utilisation de l’eau. Sur le maïs ensilage, les résultats ont montré que le déficit hydrique affecte la croissance en hauteur des plants, accélère la sénescence des feuilles et réduit l’indice de la surface foliaire. Les rendements en matière sèche ont varié de 3, 9 t. ha- 1 sous T 5 (20 % ETc) à 16, 4 t. ha- 1 sous T 1 (100 % ETc). L’efficience d’utilisation de l’eau (EUE) est plus élevée durant la phase linéaire de croissance. L’EUE calculée à la récolte varie entre 2, 99 kg. m- 3 sous T 1 à 1, 84 kg. m- 3 sous T 5. L’évapotranspiration réelle sous T 1 (100 % ETc) est de 478 mm et 463 mm en 2009 et 2010, respectivement. La valeur du coefficient de réponse à l’eau (Ky) du maïs ensilage est de 1, 12. L’ETc du maïs a été déterminé par lysimètre à drainage à 415 mm. Les valeurs moyennes obtenues des coefficients culturaux sont de 0, 56, 1, 22 et 1, 05 pour le stade initial, mi-saison et avant la récolte (stade grain laiteux pâteux) respectivement. L’irrigation localisée permet d’obtenir des rendements similaires au système gravitaire mais avec des apports hydriques très réduits et permet ainsi d’économiser environ 30 % de l’eau d’irrigation. Sur des périodes de culture du bersim de 5 cycles, les rendements en biomasse atteints sous le régime T 1 sont de 14, 3 et 13, 9 t/ha en 2009 / 10 et 2010 / 11 respectivement. Les réductions de rendements en appliquant 60 % des apports en eau sont de 40 et 42 % en 2009 / 10 et 2010 / 11 respectivement. La productivité journalière du bersim augmente avec plus d’apport en eau avec comme valeur maximale 102 kg MS/ha/jour. Le taux de matière sèche augmente avec le stress hydrique avec des valeurs moyennes qui varient entre 12, 3 et 23, 7 % sous T 1 et T 4 respectivement. La contribution des cycles sans irrigation (période pluvieuse) au rendement total annuel peut varier de 35 % à 52 % sous les régimes T 1 et T 4 respectivement. La réalisation des bilans hydriques par régime hydrique montre que les pertes par drainage augmentent avec plus d’apport en eau surtout en 1 er cycle. L’efficience d’utilisation de l’eau est faible pendant le cycle d’installation, optimale pendant les trois cycles 2, 3 et 4 et diminue au dernier cycle avec le stress hydrique. L’EUE globale du bersim déterminée sur toute la période de culture (pente de la droite de régression) est de 3, 37 kg. m- 3. Le coefficient de réponse du rendement du bersim à l’eau est de 1, 11. L’ETc déterminée moyennant un lysimètre à drainage est 520 mm. Les valeurs de Kc ont été estimées pour chaque cycle pour les trois phases : initiale, développement (valeur médiane) et mi-saison. Le rendement moyen maximal obtenu sous le goutte à goutte est de 15, 7 t/ha et a été obtenu avec un apport en eau de 411 mm ce qui a permis une économie d’eau de 57 % par rapport au gravitaire. La comparaison du comportement de six variétés de luzerne les plus pratiquées dans le périmètre irrigué de Tadla montre que la variété « Super Siriver » suivie de « Trifecta » ont des potentialités de production très élevées avec des meilleures capacités de tolérance du stress hydrique. Le rendement annuel maximal obtenu pour la luzerne est de 24, 2 t. ha- 1. La contribution des cycles de printemps au rendement total annuel varie de 55 % sous T 1 (100 % ETc) à 65 % sous T 4 (40 % ETc). Les rendements de la luzerne dépendent en plus des quantités d’eau apportées de l’emplacement des apports à l’intérieur d’un cycle. L’efficience d’utilisation de l’eau varie d’un cycle à l’autre et d’une saison à l’autre. La valeur maximale est de 2, 57 kg. m- 3 et obtenue au printemps 2011 alors que la faible valeur est de 0, 64 kg. m- 3 qui est obtenue en hiver 2010. L’EUE diminue avec le stress hydrique avec des valeurs moyennes de 1, 83, 1, 67, 1, 54 et 1, 23 kg. m- 3 sous T 1, T 2, T 3 et T 4 respectivement. Le coefficient de réponse de rendement à l’eau (Ky) de la luzerne est de 0, 92. La détermination des besoins en eau de la luzerne a été réalisée sur la base des calendriers des cycles de la culture durant les deux campagnes 2010 et 2011. Les valeurs trouvées sous irrigation gravitaire sont de 1388 et 1364 mm pour les deux campagnes respectivement. L’irrigation localisée permet de réaliser des rendements similaires à l’irrigation gravitaire avec moins d’eau et plus d’efficience agronomique. Les apports en eau <b>effectivement</b> réalisés sous le régime T 1 avec l’écartement entre rampes de 50 cm sont inférieurs au besoin net de la culture d’environ 7 % et 18 % en 2010 et 2011 respectivement. Sous le même régime en irrigation gravitaire, les besoins sont dépassés de 16 % et 21 % en 2010 et 2011 respectivement. Deux modèles de cultures, PILOTE et CropSyst, avaient été retenus pour être testés sur leurs aptitudes à simuler la croissance et les rendements des trois cultures étudiés sous les conditions édapho-climatiques de Tadla. Sur le maïs, les deux modèles ont simulé correctement la croissance et le développement de cette culture sous des régimes hydriques variés. Les paramètres des deux modèles se sont montrés validés et efficaces pour la simulation de la biomasse, le LAI et le stock hydrique. Bien que PILOTE nécessite moins de paramètres et de données que CropSyst, il s'avère être souvent plus performant dans la simulation de la biomasse du maïs et du bilan hydrique. Concernant le bersim, les prédictions de la biomasse par CropSyst semblent être plus précises que celles du modèle PILOTE qui est à son tour meilleur au niveau de la prédiction du stock hydrique sur la tranche du sol exploitée par les racines (0 - 80 cm). Le modèle CropSyst a été retenu pour tester la possibilité de simuler la rotation bersim-maïs ensilage vu sa facilité d’intégrer des données climatiques journalières de plusieurs années. Les résultats montrent que ce modèle simule correctement l’évolution de la biomasse et les rendements des deux cultures considérées en rotation durant trois ans. La modélisation de la croissance et la production de la luzerne est réalisée pour les deux modèles en dehors de l’année d’installation de la culture. A l’issue de la calibration et de la validation réalisées, le modèle CropSyst simule convenablement la biomasse et le stock hydrique sur tous les régimes hydriques étudiés alors que les meilleures simulations de ces sorties par PILOTE se sont limitées au traitement non stressant T 1. Bien que le modèle CropSyst prenne en considération plusieurs paramètres dans la simulation de la croissance de la luzerne, leurs simplifications (valeurs uniques) réduisent ses performances surtout en situation de stress hydrique accentuée. Le peu de paramètre pris en considération dans le modèle PILOTE rend difficile sa validation pour une culture pérenne telle que la luzerne dans des situations variables. Le modèle CropSyst a été retenu pour évaluer les pratiques d’irrigation des agriculteurs et élaborer des scénarios virtuels d’irrigation pour les trois cultures étudiées. L’évaluation des scénarios virtuels élaborés montre que pour la luzerne, l’application d’une dose d’irrigation de 1600 mm selon un scénario qui prévoit l’application de 14 irrigations réparties en six arrosages durant le printemps, six en été, un en automne et un autre en début d’hiver permet de maximiser l’efficience de l’eau d’irrigation (EEI) (1, 21 kg/m 3) et de réaliser un rendement de 23, 1 t/ha (soit 95 % du potentiel). Sur le maïs, l’application d’une dose de 648 mm selon la combinaison [2 en phase initiale (au semis), 2 en phase linéaire et 2 en étape finale] permet d’atteindre le double objectif de réaliser un rendement élevé et garantir une meilleure valorisation de l’eau. Concernant le bersim, les résultats de simulation confirment que l’adoption du scénario qui prévoit l’application de 625 mm en 7 irrigations (3 en automne, 2 en hiver et 2 au printemps) permet de réaliser un rendement de 14, 1 t/ha qui représente 94 % du potentiel de la variété 6454. Ce scénario permet la meilleure efficience de l’eau d’irrigation (1, 24 kg/m 3) et occasionne de faibles pertes par drainage estimées à environ 17 % des apports en eau. La comparaison des deux systèmes de cultures représentés par la luzerne et la rotation maïs ensilage-bersim montre que la rotation valorise mieux l’eau et mobilise moins d’eau que la luzerne qui se distingue par sa rentabilité économique. Ainsi, le couplage des résultats de trois années d’expérimentations "in situ" (2008 à 2011) à ceux du calage, de la validation et des simulations de scénarios par les deux modèles CropSyst et PILOTE s'est révélé d’un grand intérêt pour l’amélioration de la conduite de l’irrigation des fourrages dans le périmètre irrigué de Tadla au Maroc...|$|E
40|$|It {{has been}} known for more than 150 years that action effects in bridges due to traffic action are higher than it has to be {{expected}} for purely static loads. In the design of road bridges, this difference is considered by multiplying static traffic loads with a "dynamic amplification factor". The amplification factors defined in codes are based on dynamic load tests on existing bridges. Despite of hundreds of tests in several countries, experimental investigation has not given satisfactory explanation of the observed phenomena, which has resulted in marked differences between amplification factors defined in different codes. This {{is due to the fact}} that the core of the matter – the dynamic interaction between vehicles and bridges– is a complex mechanical problem. Based on a detailed analysis it is shown in the introduction, that it can also be attributed to the fact, that the experimental investigation is more part of the problem than its solution. This thesis aims at getting a solid and systematic grounding in the problem using theoretical analysis. The centre of attention is the question, which importance dynamic phenomena have in those scenarios which are effectively relevant for the structural safety of a bridge. All scenarios are considered that justify an amplification factor, and not only dynamic vehicle – bridge interaction. The structural safety evaluation of a bridge includes the verification of the ultimate and the fatigue limit state. Accordingly, this thesis distinguishes between the interaction at ultimate limit state, for which inelastic bridge behaviour is assumed, and the interaction at service limit state with linear elastic bridge behaviour. The structural analysis of a bridge shows in addition, that the elements of the bridge deck differ considerably from the main girders: For the elements of the deck – i. e. primarily for the deck slab – dynamic interaction is of little importance, and amplification of action effects is essentially due to amplification of traffic action. In the case of the main girders, action effects are additionally amplified due to the oscillations of the structure. In order to analyse interaction at service limit state in detail, very sophisticated models are required, which do not only cover all relevant eigenmodes of the bridge but also the non-linear, dynamic behaviour of heavy vehicles and the precise road surface profile. Design and analysis of such models are mostly conferred to specialists in numeric analysis and structural dynamics. In the contrary, this thesis aims at capturing the fundamental connections by simple models, which facilitates the identification of the key parameters and the interpretation of their influence. The most important result of the analysis of vehicle – bridge interaction at service limit state is that the amplification factor is most influenced by the weight and the number of vehicles on a bridge. Whereas the amplification is negligible for high vehicle loads, tests with relatively lightweight vehicles on long bridges lead to a significant over-estimation of amplification factors. Furthermore it is shown that neither the span nor the natural frequency of a bridge is appropriate for fixing the amplification factor for a particular bridge and safety verification, respectively. It has been observed in dynamic load tests that deflection measurements consistently result in higher amplification factors than strain measurements. This phenomenon {{has been known}} for more than fifty years, but no explanation has been given so far. In this thesis an explanation is proposed and it is shown that deflection measurements result in an over-estimation of amplification factors. Similar considerations lead to a proposal for a more suitable application of amplification factors in the verification of shear force. A completely new approach is chosen for the analysis of vehicle – bridge interaction at ultimate limit state. The effective behaviour at rupture is taken into account, which necessitates first to deal with the influence of loading velocity on material strength. It is shown that only for impact loading of deck slabs due to dynamic tyre forces a minor increase in concrete strength can be expected. An important prerequisite for the understanding of dynamic behaviour at ultimate limit state is the "gravity effect", which is shown to cause massive reduction in the dissipation capacity of a structure. The determinant criterion with inelastic behaviour is deformability and not stiffness. Simple models are used to study the influence of deformability and gravity effect in the most important cases of dynamically amplified traffic action. The results show, under which conditions the dynamic amplification of action effects can be compensated by plastic deformation of the structure without causing its failure. If the steel yield stress is already attained due to the static part of traffic action, compensation of the dynamic part is only assured if the rupture behaviour is characterised by strain hardening. A simple condition of equilibrium shows that dynamic amplification due to centrifugal forces cannot be absorbed by deformations of the structure. However, rupture behaviour characterised by significant deformation causes a delay in the failure of the structure, which can be sufficient to prevent the definitive rupture anyway, depending on the scenario. In addition to these reflections, it is attempted to determine the importance of shear failures with respect to flexural failures, in order to estimate the probability of this comparatively brittle failure mechanism. In view of the application of the findings, the relevant results are synthesized and a concept for the safety verification accounting for dynamic traffic action is developed. The concept is based on the distinction between verifications at ultimate and service limit state on the one hand, and the separate treatment of elements of the deck and main girders on the other hand. This differentiation allows integrating risk based considerations using explicit hazard scenarios. An important point in the application of the findings is the recommendation to emphasize the benefit of good road surface evenness in the maintenance of structures. A necessary complement in establishing the recommended amplification factors is the detailed analysis of the reaction of vehicles to road surface irregularities. The dynamic tyre forces for different vehicle and axle types, respectively, are analysed, since the findings indicate that the amplification of tyre forces is much more important in fixing amplification factors than the dynamic behaviour of bridges. The investigations clearly show that higher axle loads imply lower amplification factors, and that the maximum amplification of axle forces in axle groups never occurs simultaneously for all axles. The thesis is finished by an annexe including introductions to the dynamic behaviour of vehicles and bridges as well as to the modelling of traffic loads and road surface irregularities. In addition to an extensive review of the state of the art, these introductions constitute an important basis of the work and facilitate understanding of the calculations in the main part. Seit mehr als 150 Jahren ist bekannt, dass die Beanspruchung einer Brücke bei der Belastung durch bewegte Fahrzeuge höher ist als bei Stillstand derselben Fahrzeuge. Diesem Unterschied wird in der Bemessung von Strassenbrücken dadurch Rechnung getragen, dass die statischen Verkehrslasten mit einem "dynamischen Vergrösserungsfaktor" multipliziert werden. Die in den Tragsicherheitsnachweisen verwendeten Vergrösserungsfaktoren stützen sich auf Lastversuche an bestehenden Brücken. Trotz hunderter Versuche in diversen Ländern haben diese Versuche jedoch keine befriedigende Erklärung der Phänomene geliefert, sodass sich die Vergrösserungsfaktoren von Land zu Land teilweise beträchtlich unterscheiden. Dies hängt damit zusammen, dass es sich beim Kernproblem – der dynamischen Wechselwirkung zwischen Fahrzeug und Brücke – um eine komplizierte mechanische Fragestellung handelt. Aufgrund einer eingehenden Analyse in der Einleitung wird dies jedoch auch darauf zurückgeführt, dass die experimentelle Untersuchung mehr Teil des Problems als dessen Lösung ist. Diese Arbeit zielt darauf ab, die Fragestellung aufgrund einer theoretischen Analyse systematisch aufzubereiten. Im Zentrum steht dabei die Fragestellung, welche Bedeutung dynamische Phänomene in jenen Szenarien haben, welche effektiv für die Tragsicherheit einer Brücke massgeblich sind. Zudem werden alle Szenarien betrachtet, welche einen Vergrösserungsfaktor rechtfertigen, und nicht nur die Szenarien mit dynamischer Wechselwirkung. Der Tragsicherheitsnachweis einer Brücke umfasst die Nachweise des Bruch- und des Ermüdungswiderstands. Dementsprechend wird unterschieden zwischen der Wechselwirkung auf Bruchniveau, bei der von inelastischem Materialverhalten ausgegangen wird, sowie der Wechselwirkung auf Gebrauchsniveau unter Beschränkung auf rein elastisches Verhalten. Die Analyse des Tragverhaltens einer Brücke zeigt zudem, dass sich die Elemente der Fahrbahn von den Brückenlängsträgern ganz wesentlich unterschieden: Bei den Elementen der Fahrbahn – das heisst vor allem bei der Fahrbahnplatte – spielt die dynamische Wechselwirkung praktisch keine Rolle, und die Vergrösserung der Beanspruchung besteht im Wesentlichen aus der Lastvergrösserung. Bei den Brückenlängsträgern erhöht sich die Beanspruchung, zusätzlich zur Lastvergrösserung, durch deren eigene Schwingungen. Zur genauen Erfassung der dynamischen Wechselwirkung auf Gebrauchniveau sind sehr komplizierte Modelle erforderlich, welche nicht nur alle Eigenschwingungsformen der Brücke sondern auch das nicht-lineare, dynamische Verhalten von Fahrzeugen sowie die Fahrbahnunebenheiten präzise abbilden. Erstellung und Analyse dieser Modelle werden daher meistens von Spezialisten der Numerik und Baudynamik übernommen. In der vorliegenden Arbeit wird im Gegensatz dazu versucht, die wesentlichen Zusammenhänge durch möglichst einfache Modelle zu erfassen. Dies vereinfacht es, den Einfluss der wichtigsten Parameter zu erfassen und auszuwerten. Das wichtigste Resultat der Analyse der Wechselwirkung auf Gebrauchsniveau ist, dass das Fahrzeuggewicht sowie die Anzahl an Fahrzeugen auf einer Brücke einen enormen Einfluss auf den Vergrösserungsfaktor haben. Während die dynamische Vergrösserung bei hohen Verkehrslasten praktisch vernachlässigbar ist, führen Versuche mit relativ leichten Fahrzeugen auf langen Brücken zu einer markanten Überschätzung des Vergrösserungsfaktors für den Tragsicherheitsnachweis. Weiters konnte gezeigt werden, dass sich weder die Spannweite einer Brücke noch deren Grundfrequenz zur Festlegung eines Vergrösserungsfaktors eignen. Bei dynamischen Lastversuchen ergeben Durchbiegungsmessungen durchwegs höhere Vergrösserungsfaktoren als Dehnungsmessungen. Dieses Phänomen ist seit fünfzig Jahren bekannt, ohne dass bisher eine Erklärung gegeben werden konnte. In dieser wird eine Erklärung vorgeschlagen und gezeigt, dass der Vergrösserungsfaktor aufgrund von Durchbiegungsmessungen deutlich überschätzt wird. Analoge Überlegungen erlauben auch zu zeigen, wie die dynamische Vergrösserung der Querkraft besser erfasst werden kann als dies bis anhin der Fall ist. Für die Analyse der Wechselwirkung auf Bruchniveau wird ein gänzlich neuer Ansatz gewählt, der das effektive Bruchverhalten der Brücke berücksichtigt. Daher wird zuerst grundsätzlich auf den Einfluss der Belastungsgeschwindigkeit bei der Beanspruchung durch Verkehrslasten eingegangen. Es zeigt sich, dass nur bei der stossförmigen Belastung der Fahrbahnplatte durch dynamische Radkräfte eine gewisse Erhöhung der Festigkeiten erwartet werden kann. Eine wichtige Voraussetzung für das Verständnis des dynamischen Verhaltens auf Bruchniveau ist die Schwerkraftwirkung. Diese bewirkt eine drastische Verringerung der Dissipationskapazität des Tragwerks. Entscheidend ist bei inelastischem Verhalten die Verformbarkeit und nicht die Steifigkeit des Tragwerks. Anhand einfacher Modelle wird versucht, die wichtigsten Szenarien mit dynamischer Vergrösserung zu erfassen, wobei in erster Linie das Versagen auf Biegung untersucht wird. Anhand des Vergleichs der Resultate für verschiedene Kraft-Verschiebungs-Diagramme wird abgeschätzt, unter welchen Voraussetzungen der dynamische Anteil der Beanspruchung durch plastische Verformungen aufgenommen werden kann, ohne dass es zum Bruch kommt. Wenn die statische Beanspruchung bereits das Fliessmoment erreicht, dann gelingt dies aufgrund der Schwerkraftwirkung nur noch bei einem Bruchverhalten mit Dehnungsverfestigung. Anhand einer einfachen Gleichgewichtsbetrachtung wird gezeigt, dass bei der dynamischen Lastvergrösserung infolge von Kurvenfahrt oder Bremsung der dynamische Anteil der Einwirkung nicht dissipiert werden kann. Ein verformungsreiches Bruchverhalten führt in diesem Fall jedoch zu einer Verzögerung des Bruchs, die je nach Szenario ausreichen kann, damit ein Auto den Gefahrenbereich verlässt. Darüber hinaus wird auch versucht, die Bedeutung des Schubbruchs im Vergleich zum Biegebruch zu bestimmen, um die Wahrscheinlichkeit dieses verformungsarmen Bruchs abzuschätzen. Die gewonnenen Erkenntnisse fliessen schliesslich in ein Nachweiskonzept ein, in welchem geeignete Vergrösserungsfaktoren angegeben werden, wobei einerseits zwischen Bruch- und Gebrauchsniveau und andererseits zwischen Elementen der Fahrbahn und Brückenlängsträgern unterschieden wird. Die empfohlenen Werte beruhen dabei auf expliziten Gefährdungsbildern, was die Einbeziehung von Risikoüberlegungen ermöglicht. Die Erkenntnisse zeigen bei der Erhaltung bestehender Brücken künftig mehr auf die Gewährleistung einer möglichst ebenen Fahrbahn zu achten. Abgerundet wird die Arbeit durch ausführliche Einführungen zum dynamischen Verhalten von Schwerfahrzeugen und Strassenbrücken, sowie zu Verkehrslasten und Fahrbahnunebenheiten. Dies erleichtert Fachleuten aus dem Brückenbau den Zugang, welche nicht Spezialisten der Baudynamik sind. Den dynamischen Radkräften verschiedener Fahrzeug- bzw. Achstypen infolge von Fahrbahnunebenheiten wird sehr grosse Aufmerksamkeit gewidmet, da dieser Aspekt oft zu Gunsten der Fokussierung auf das Verhalten der Brücke vernachlässigt wurde. Il est connu depuis plus de 150 ans que les efforts internes d'un pont sollicité par le trafic sont plus grands quand les charges sont en mouvement que quand elles sont à l'arrêt. Cette différence est prise en compte lors du dimensionnement d'un pont routier en multipliant les charges de trafic statiques par un « facteur d'amplification dynamique ». Les facteurs d'amplification utilisés dans la vérification structurale se basent sur des essais de charge sur des ponts routiers existants. Malgré des centaines d'essais dans différents pays, l'expérimentation n'a pas fourni une explication satisfaisante des phénomènes observés. En conséquence, les facteurs d'amplification prescrits dans les normes varient parfois considérablement. D'une coté ceci peut être ramené au fait que le cœur du problème – l'interaction dynamique entre un véhicule et un pont – est un problème mécanique complexe. D'un autre coté, une analyse approfondie dans l'introduction de cette thèse montre que l'étude expérimentale fait plutôt partie du problème que de sa solution. Cette thèse a pour but de traiter le problème de manière systématique à l'aide d'une approche théorique. Le cœur est formé par la question : quelle est l'importance des phénomènes dynamiques dans les scénarios, qui sont <b>effectivement</b> déterminants pour la sécurité structurale d'un pont. Tous les scénarios, qui justifient un facteur d'amplification, sont considérés, et non pas seulement l'interaction dynamique véhicule – pont. L'évaluation de la sécurité structurale d'un pont comprend la vérification de la résistance à l'état ultime ainsi qu'à la fatigue. En conséquence, cette thèse distingue entre l'interaction véhicule – pont à l'état ultime, caractérisée par un comportement inélastique du pont, et l'interaction à l'état de service, où un pont se comporte essentiellement de manière linéaire élastique. L'analyse du comportement structural d'un pont montre de plus que, les éléments du tablier se distinguent fondamentalement des poutres longitudinales : pour les éléments du tablier – principalement la dalle de roulement – l'interaction dynamique n'a pratiquement pas d'importance, et l'amplification de la sollicitation est due à l'oscillation des véhicules. Pour les poutres longitudinales s'ajoutent les oscillations de la structure aux oscillations des véhicules. Une étude théorique précise de l'interaction dynamique au niveau de service nécessite des modèles très complexes, qui reproduisent non seulement les modes d'oscillation d'un pont mais aussi le comportement non-linéaire et dynamique des véhicules ainsi que le profil précis de la chaussée. Par conséquence, le développement et l'analyse de tels modèles sont normalement pris en charge par des spécialistes du calcul numérique et dynamique. Cette thèse tente, au contraire, de reproduire les effets principaux à l'aide de modèles les plus simples possibles. Ceci facilite l'identification des paramètres clés et la mise en évidence de leurs influences. Le résultat principal de l'analyse de l'interaction véhicules - pont à l'état de service est l'identification du poids et du nombre des véhicules comme paramètre principal pour le facteur d'amplification. Alors que l'amplification dynamique est pratiquement négligeable pour des charges très élevées, les essais avec des véhicules relativement légers, sur un grand pont, résultent dans une surestimation significative du facteur d'amplification pour une vérification structurale. En outre on montre que ni une portée, ni la fréquence fondamentale d'un pont, ne sont des critères adéquats pour déterminer un facteur d'amplification dans le cas concret d'une vérification structurale. Lors d'un essai dynamique, les mesures de flèches résultent dans des facteurs d'amplification plus élevés que des mesures de déformations. Ce phénomène est connu depuis cinquante ans, sans qu'une explication n'ait été fournie. Cette thèse propose une explication et montre la surestimation de facteurs d'amplification dérivés de mesures de flèches. Des réflexions analogues amènent à une proposition pour une meilleure application du facteur d'amplification dans la vérification de l'effort tranchant. Une approche nouvelle est utilisée pour analyser l'interaction dynamique à l'état ultime. Elle tient compte du comportement effectif à la rupture, ce qui nécessite d'étudier d'abord l'influence de la vitesse de chargement sur la résistance apparente des matériaux. Dans le cas de charges de trafic, seul une légère augmentation de la résistance du béton de la dalle de roulement est à attendre, si une roue heurte un obstacle. Une condition essentielle pour comprendre le comportement à l'état ultime est « l'effet de la gravité », qui cause une réduction radicale de la capacité de dissipation de la structure. Le critère déterminant dans un comportement inélastique est la déformabilité et non la rigidité. Des modèles simples sont utilisés pour étudier l'influence de la déformabilité ainsi que de l'effet de la gravité dans les scénarios avec une amplification dynamique importante. Les résultats montrent sous quelles conditions l'amplification dynamique peut être compensée par la déformation plastique de la structure sans qu'une rupture n'ait lieu. Si l'acier atteint sa limite d'élasticité sous l'effet des charges statiques, une sollicitation dynamique additionnelle ne peut être compensée si l'acier présente un comportement durcissant. Une simple considération d'équilibre montre que l'amplification dynamique due aux forces centrifuges ou aux forces de freinage ne peut pas être compensé par une déformation de la structure. Dans ce cas, un comportement ductile aide à ralentir la rupture, ce qui peut être suffisante pour permettre un véhicule de passer la zone critique. En outre, des réflexions sont présentées sur l'importance d'une rupture à l'effort tranchant, qui consiste généralement en un mode de rupture avec peu de déformation. On montre que la probabilité que ce mode ait lieu avant une rupture à la flexion est faible. En vue de l'application des résultats, les points les plus importants sont résumés et un concept pour la vérification structurale est développé, qui contient de valeurs explicites pour le facteur d'amplification. Le concept se base sur la distinction entre l'état ultime et l'état de service. On distingue également le cas des éléments du tablier et celui des poutres longitudinales. Cette différentiation permet l'intégration de critères de risque sur la base de scénarios détaillés. En plus du concept de vérification, il est recommandé de mettre l'accent plus explicitement sur le bénéfice d'une bonne planéité de la chaussée lors de la maintenance d'une structure. Un complément nécessaire pour l'établissement des facteurs d'amplification recommandés est l'analyse détaillée de l'excitation de véhicules par les irrégularités dans la chaussée. Les forces de roues dynamiques pour différents types de véhicules et d'essieux, respectivement, sont analysées, car les résultats des autres chapitres indiquent que l'amplification des forces d'essieux est nettement plus importante que le comportement dynamique du pont. Les résultats de cette analyse montrent clairement que des essieux plus chargés ont des facteurs d'amplification plus bas, et que la force maximale n'est jamais atteinte pour tous les essieux d'un groupe simultanément...|$|E
40|$|The {{study of}} the {{interaction}} of lightning electromagnetic fields with electrical systems and the design of appropriate protection strategies are generally based on statistical distributions of the lightning current measured at the channel base using either instrumented towers or artificial initiation of lightning using rockets. Recent studies based both on numerical modeling and experimental observations {{have shown that the}} presence of the structure struck by (or used to initiate) lightning does affect the current measurement in a way depending upon the geometry of the structure itself, compromising therefore the reliability of the statistics adopted so far for lightning data. The aim of this thesis is to provide new elements (from both theoretical and experimental investigations) to improve the understanding of the electromagnetic consequences of the impact of lightning return strokes to tall structures. Chapter 2 introduces to the phenomenology of cloud-to-ground lightning and the importance of lightning return-stroke modeling. Among the different classes of return-stroke models existing in the literature, the attention is focused in this thesis on the so-called engineering models, which allow describing the current distribution along the channel {{as a function of the}} current at the channel base and the return-stroke speed, two quantities for which data can be obtained experimentally. After presenting a review of five engineering return-stroke models describing lightning strikes to ground, the extension of the engineering models to take into account the presence of an elevated strike object is presented and discussed. The original contributions of this thesis, consisting of both theoretical and experimental works, are presented in Chapters 3 through 6. Chapter 3 is devoted to the computation of the electromagnetic field produced by lightning return strokes to elevated strike objects, using the extension of the engineering models to include an elevated strike object presented in the previous chapter. It is shown, for the first time, that the current distribution associated with these extended models exhibits a discontinuity at the return-stroke wavefront which (although not physically conceivable) needs to be taken into account by an additional term in the equations for the electromagnetic field, the so-called "turn-on" term. A general analytical formula describing the "turn-on" term associated with this discontinuity for various engineering models is derived and simulation results illustrating the effect of the "turn-on" term on the radiated electric and magnetic fields are also presented. In the second part of the chapter, dedicated to the investigation of the propagation effects on lightning electromagnetic field traveling along a finitely-conducting ground, the commonly-used assumption of an idealized perfectly-conducting ground is relaxed in order to analyze, for the first time, how the electromagnetic field radiated by a tower-initiated strike is affected while propagating along a soil characterized by a finite conductivity. The results showed that the attenuation of the initial peak of the field radiated by a tower-initiated strike, resulting from the propagation over finitely conducting ground, depends strongly on the risetime of the current, the tower height and the ground conductivity and is, in general, much more important than the attenuation experienced, while propagating along the same finite ground, by the field produced by ground-initiated strikes. Chapter 4 presents a comparison among the predictions obtained using the five extended engineering return-stroke models for lightning strikes to tall structures described in Chapter 2. The spatial-temporal current profiles along the tower-channel axis predicted by the engineering models, as well as the respective predictions for the radiated electric and magnetic fields, calculated at different distances, are compared and discussed. It is shown that the computed electromagnetic fields associated with a strike to a tall tower are generally less model-dependent than those corresponding to a strike to ground, especially as far as the first-peak value is concerned, which is nearly model-insensitive in case of tall-tower strikes. A theoretical analysis is performed in the last part of the chapter with the aim to provide, for the same five engineering models extended to take into account the presence of the tower, expressions relating the return-stroke current and the associated distant radiated electric and magnetic fields. It is demonstrated, in addition, that only one model among the five presented is characterized by simple analytical formulas relating current-peak and far-field peak values, which (being the electromagnetic field peak value nearly independent of the adopted model) become general expressions applicable for any engineering return-stroke model in case of tower-initiated lightning. It was also shown that the peak amplitude of the electromagnetic field radiated by a lightning strike to a tall structure is relatively insensitive to both the values of the top reflection coefficient and the return-stroke speed. This latter result is important, in particular, because, unlike ground-initiated strikes, for which the far-field peak is strongly dependent on the return-stroke speed, far field peaks associated with strikes to tall structures are little sensitive to the return stroke speed. Since in most practical cases the value of the return-stroke speed is unknown, this interesting result suggests a possible calibration procedure for lightning detection systems by means of direct measurement of lightning currents on instrumented towers. Chapter 5 reports on the simultaneous measurements of the return-stroke current and of the electric and magnetic fields at three distances associated with lightning strikes to the Toronto CN Tower (553 m) that have been carried out during the summer of 2005. This is the first time ever that simultaneous records of lightning current and associated electric and magnetic fields at three distances have been obtained. Two propagation paths for the electromagnetic field to the first and to the second field measurement stations (located, respectively, 2. 0 km and 16. 8 km away from the CN Tower) were along the soil and through the Toronto city, whereas for the third location (50. 9 km away) the propagation path was nearly entirely across the fresh water of Lake Ontario. It is shown that the waveforms of the electric and magnetic fields at 16. 8 km and 50. 9 km exhibit a first zero-crossing about 5 microseconds after the onset of the return-stroke, which is part of a narrow undershoot and which may be attributed to the transient processes along the tower. Effects of propagation (decrease of field amplitude and increase of its risetime) could also be observed in experimental records. It is shown that the fields at 50. 9 km are less affected by such attenuation, compared to those at 16. 8 km, presumably because the path of propagation was mostly across Lake Ontario. The measured waveforms are compared with the theoretical predictions obtained using five engineering return-stroke models, extended to include the presence of the strike object, finding a reasonable agreement for the magnetic field waveforms at the three considered distances. The overall agreement between the theoretically predicted and the experimentally observed field-peak-to-current-peak ratio is reasonable, although the theoretical expression appears to underestimate the experimentally measured ratio (by about 25 %). This may be due, at least in part, to the enhancement effect of the buildings on which the field measurement antennas were installed. Finally, the directly-measured lightning currents at the tower were correlated and compared with the current-peak estimations provided by the US National Lightning Detection Network (NLDN). It is shown that the NLDN-inferred values overestimate the actual current peaks because the presence of the tall struck object produces an enhanced radiated field at far distances (with respect to strikes to flat ground), which is not included in the algorithm used to infer lightning current peaks from remote field measurements. It is shown in this thesis that correcting the NLDN estimates using the correction factor introduced by the tower results in an excellent estimation of lightning current peaks. This is an important conclusion of this study showing that the estimation of lightning peak currents for tall towers can be greatly improved by considering the tower correction factor. Chapter 6 is devoted to the measurement of electromagnetic fields radiated by lightning. In its first part, the need for guidelines for reporting lightning data obtained experimentally is emphasized. The second part of the chapter presents the design, the construction and preliminary tests of a low-cost, multi-channel lightning field measuring system for the simultaneous measurement of three components of the electromagnetic field radiated by lightning. The proposed system uses one single optical link for the transmission of the three signals, appropriately digitized and multiplexed, lowering considerably the overall cost of the system itself. L'étude de l'interaction entre le champ électromagnétique rayonné par la foudre et les systèmes électriques, ainsi que la coordination des stratégies de protection sont généralement basées sur des distributions statistiques du courant mesuré à la base du canal de foudre obtenues en utilisant des tours instrumentées ou par la technique de déclenchement artificiel de la foudre. Des études récentes basées sur des modélisations numériques et des observations expérimentales ont montré que la présence de la structure foudroyée, ou celle utilisée pour la déclencher, "contamine" la mesure du courant de foudre. Cette "contamination", qui dépend de la géométrie de la structure elle-même, compromet la fiabilité des statistiques adoptées jusqu'alors pour les paramètres du courant de foudre. L'objectif de cette thèse est d'apporter de nouveaux éléments (issus d'études à la fois théoriques et expérimentales) à la compréhension des conséquences électromagnétiques de l'impact d'arcs en retour sur des tours élevées. Le Chapitre 2 présente brièvement la phénoménologie des coups de foudre nuage-sol et souligne l'importance des modèles d'arc en retour. Parmi les différentes classes de modèles d'arc en retour existantes dans la littérature, l'attention est focalisée dans cette thèse sur ce qu'on appelle les "modèles d'ingénieur". Ces modèles permettent une description de la distribution du courant le long du canal en fonction du courant à la base du canal et de la vitesse de l'arc en retour, deux grandeurs pour lesquelles il est possible d'obtenir des données expérimentales. Après une description de cinq modèles d'ingénieur de l'arc en retour pour des coups de foudre tombant au sol, l'attention est portée sur l'extension des modèles d'ingénieurs en tenant compte de la présence d'un objet élevé foudroyé. Les contributions originales de cette thèse, comprenant des travaux théoriques et expérimentales, sont présentées dans les Chapitres 3 à 6. Le Chapitre 3 est consacré au calcul du champ électromagnétique produit par l'impact d'arcs en retour sur des tours élevées, en se basant sur l'extension des modèles d'ingénieur pour inclure la présence d'un objet élevé qui a été présentée dans le chapitre précédent. Il est montré, pour la première fois, que selon ces modèles d'ingénieurs, la distribution du courant le long du canal présente une discontinuité au front d'onde de l'arc en retour qui, bien que physiquement inconcevable, nécessite d'être prise en compte par un terme additionnel dans les équations du champ électromagnétique, le terme communément appelé "turn-on term". Une formule analytique générale décrivant ce "turn-on term" associé à cette discontinuité est développée pour les différents modèles d'ingénieurs et des résultats de simulation illustrant l'effet de cette discontinuité sur les champs électrique et magnétique rayonnés sont présentés. Dans la deuxième partie du chapitre, dédiée à l'étude des effets de propagation du champ électromagnétique de foudre se propageant le long d'un sol de conductivité finie, l'hypothèse habituelle d'un sol idéal parfaitement conducteur est abandonnée afin d'analyser, pour la première fois, comment le champ électromagnétique rayonné par un coup de foudre tombant sur un objet élevé est affecté suite à la propagation le long d'un sol de conductivité finie. Les résultats de simulation montrent que l'atténuation du pic initial du champ rayonné par un coup de foudre tombant sur une tour, suite à la propagation le long d'un sol de conductivité finie, dépend fortement du temps de montée du courant, de la hauteur de la tour et de la conductivité du sol; en outre, cette atténuation est, en général, bien plus importante que l'atténuation subie par le champ produit par un coup de foudre tombant au sol, suite à la propagation le long du même parcours. Le Chapitre 4 présente une comparaison entre les prédictions des cinq modèles d'ingénieur généralisés pour tenir compte de la présence d'une structure élevée, décrits dans le Chapitre 2. Les distributions spatio-temporelles du courant le long de l'axe tour-canal prévues par les modèles d'ingénieur, ainsi que les champs électriques et magnétiques associés, calculés à différentes distances, sont comparés et discutés. Les résultats montrent que les champs électromagnétiques associés à un coup de foudre tombant sur une tour élevée sont généralement moins dépendants du modèle d'arc en retour adopté que ceux correspondants à un coup de foudre tombant au sol, spécialement en ce qui concerne le premier pic du champ, lequel est quasi insensible au choix du modèle dans le cas d'une tour foudroyée. Une analyse théorique est présentée dans la dernière partie du chapitre avec l'objectif d'obtenir, pour les mêmes cinq modèles d'ingénieur généralisés, des expressions liant le courant de l'arc en retour et les champs électrique et magnétique rayonnés correspondants. Les résultats montrent qu'un modèle seulement, parmi les cinq considérés, est caractérisé par des formules analytiques simples reliant les valeurs du pic du courant et du pic du champ rayonné à longue distance. Sachant que la valeur du premier pic du champ est pratiquement indépendante du modèle choisi, ces expressions deviennent des expressions générales applicables à n'importe quel modèle d'ingénieur dans le cas d'arcs en retour tombant sur des tours élevées. Il a également été montré que le pic du champ électromagnétique rayonné par un coup de foudre tombant sur une structure élevée est relativement insensible aux valeurs du coefficient de réflexion au sommet de la tour et de la vitesse de l'arc en retour. Ce dernier résultat est particulièrement important, car contrairement au cas des coups de foudre tombant au sol, pour lesquels le pic du champ lointain est fortement dépendant de la vitesse de l'arc en retour, le pic du champ lointain associé aux coups de foudre tombant sur des structures élevées est peu sensible à la vitesse de l'arc en retour. Étant donné que dans la plupart des cas pratiques, la vitesse de l'arc en retour est inconnue, ce résultat intéressant suggère une procédure de calibration pour les systèmes de détection de la foudre à travers la mesure directe du courant de foudre sur une tour instrumentée. Le Chapitre 5 présente les mesures simultanées du courant de l'arc en retour et des champs électrique et magnétique à trois distances associé à coups de foudre tombants sur la CN Tower (553 m) de Toronto, obtenues durant l'été 2005. Durant cette campagne de mesure, nous avons obtenu pour la première fois au monde, des mesures simultanées de courant de foudre et de champs électriques et magnétiques à trois distances du point d'impact. Les chemins de propagation pour le champ électromagnétique reliant la tour et les deux stations de mesure les plus proches (situées respectivement à 2 km et à 16. 8 km de la CN Tower) étaient à travers l'environnement urbain de la ville de Toronto, alors que pour la troisième station (située à 50. 9 km de la tour) le chemin de propagation était presque exclusivement le long de l'eau douce du Lac Ontario. Les résultats de mesure montrent que les formes d'onde des champs électrique et magnétique à 16. 8 km et 50. 9 km présentent, à environ 5 microsecondes après l'établissement de l'arc en retour, un premier passage par zéro qui fait parti d'un creux étroit (undershoot) et qui peut être attribué au processus transitoires le long de la tour. Les effets de propagation (décroissance du pic du champ et augmentation de son temps de montée) ont pu être observés dans les courbes expérimentales. Il a été montré que les champs à 50. 9 km sont moins affectés par cette atténuation par rapport à ceux mesurés à 16. 8 km, vraisemblablement dû au fait que le chemin de propagation, pour la station de mesure à 50. 9 km, était presque entièrement sur la surface du Lac Ontario. Les formes d'onde mesurées sont comparées avec les prédictions théoriques obtenues utilisant cinq modèles d'ingénieur, généralisés pour inclure la présence de l'objet foudroyé, et un accord raisonnable a été trouvé pour les courbes de champ magnétique aux trois distances considérées. Une bonne concordance a été également constatée entre les prévisions théoriques et les observations expérimentales du rapport entre les pics de champ magnétique et de courant, bien que l'expression théorique semble sous-estimer les valeurs observées expérimentalement d'environ 25 %. Cela peut être dû, au moins partiellement, à l'effet d'amplification du champ introduit par la présence du bâtiment sur lequel les antennes de mesure étaient placées. Enfin, les courants de foudre mesurés directement sur la tour ont été corrélés et comparés avec les estimations de leur valeur de crête fournis par le système de détection de foudre américain (NLDN). Il est montré que les valeurs obtenues par le NLDN surestiment les valeurs de courant <b>effectivement</b> mesurées en raison du fait que la présence d'une tour élevée foudroyée produit une amplification du champ rayonné à longue distance (par rapport aux coups de foudre tombants au sol) et cet effet n'est pas considéré dans les algorithmes utilisés pour inférer le pic du courant à partir des mesures de champ lointain. Il est montré dans cette thèse qu'en corrigeant les estimations du NLDN par un facteur de correction qui tient compte de la présence de la structure foudroyée, il est possible d'obtenir une excellente estimation du courant de foudre. Ceci est une importante conclusion de cette étude montrant que l'estimation du courant pour les coups de foudre tombants sur des structures élevées peut être fortement améliorée en considérant le facteur introduit par la tour. Le Chapitre 6 est dédié à la mesure du champ électromagnétique rayonné par la foudre. Dans la première partie de ce chapitre, la nécessité d'établir des recommandations pour la présentation des données expérimentales liées à la foudre est soulignée. La deuxième partie du chapitre présente la conception, la construction et les tests préliminaires d'un système de mesure à bas coût, multicanaux, pour la mesure simultanée de trois composantes du champ électromagnétique rayonné par la foudre. Le système proposé utilise une seule liaison optique pour la transmission de trois signaux, numérisés et multiplexés, réduisant considérablement le prix global du système lui-même...|$|E
