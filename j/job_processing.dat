304|547|Public
50|$|Distributed <b>job</b> <b>processing</b> in JobServer {{is enabled}} using an agent model where remote nodes {{communicate}} {{with a central}} pair (primary/secondary) of master nodes. The master nodes {{are responsible for the}} job scheduling and distribute the <b>job</b> <b>processing</b> across a cluster of agent nodes.|$|E
5000|$|<b>Job</b> <b>Processing</b> Cycle - for {{detailed}} description of batch processing in the mainframe field ...|$|E
5000|$|It {{shifts the}} time of <b>job</b> <b>processing</b> to when the {{computing}} resources are less busy ...|$|E
40|$|In {{this paper}} we study the NP-hard problem of {{scheduling}} n deteriorating jobs on m identical parallel machines {{to minimize the}} makespan. Each <b>job's</b> <b>processing</b> time is a linear nondecreasing function of its start time. We present a fully polynomial-time approximation scheme for the problem, thus establishing {{that the problem is}} NP-hard in the ordinary sense only. Department of Logistics and Maritime Studie...|$|R
40|$|We {{consider}} the online competitiveness for scheduling a single resource non-preemptively {{in order to}} maximize its utilization. Our work examines this model when parameterizing an instance by a new value which we term the patience. This parameter measures each job's willingness to endure a delay before starting, relative to this same <b>job's</b> <b>processing</b> time. Specifically, the slack of a job is defined as the gap between its release time and the last possible time at which it may be started while still meeting its deadline. We say that a problem instance has patience, if each job with length kJk has a slack of at least Δ kJk. Without any restrictions placed on the job characteristics, previous lower bounds show that no algorithm, deterministic or randomized, can guarantee a constant bound on the competitiveness of a resulting schedule. Previous researchers have analyzed a problem instance by parameterizing based on the ratio between the longest <b>job's</b> <b>processing</b> time and the shortest <b>job's</b> <b>processing</b> time. Our main contribution is to provide a fine-grained analysis of the problem when simultaneously parameterized by patience and the range of job lengths. We are able to give tight or almost tight bounds on the deterministic competitiveness for all parameter combinations. If viewing the analysis of each parameter individually, our evidence suggests that parameterizing solely on patience provides a richer analysis then parameterizing solely on the ratio of the job lengths. For example, in the special case where all jobs have the same length, we generalize a previous bound of 2 for the deterministic competitiveness with arbitrary slacks, showing that the competitiveness for any 0 is exactly 1 + 1 bc+ 1. Without any bound on the job lengths, a simpl [...] ...|$|R
40|$|Abstract. In this paper, {{we study}} the NP-hard problem of {{scheduling}} n jobs on m uniform machines {{to minimize the}} total completion time of the accepted jobs and the total rejection penalty of the rejected <b>jobs.</b> Each <b>job's</b> <b>processing</b> time is a simple linear increasing function of its starting time. A job can be rejected by paying a penalty cost. We propose a fully polynomial-time approximation scheme (FPTAS) to show the problem is NP-hard in the ordinary sense...|$|R
50|$|It {{can shift}} {{the time of}} <b>job</b> <b>processing</b> to when the {{computing}} resources are less busy.|$|E
50|$|The terms Remote Batch, Remote Job System and Remote <b>Job</b> <b>Processing</b> {{are also}} used for RJE facilities.|$|E
5000|$|BatchPipes is a batch <b>job</b> <b>processing</b> utility {{designed}} for the MVS/ESA operating system, and all later incarnations - OS/390 and z/OS.|$|E
40|$|We {{consider}} {{the problem of}} scheduling n deteriorating jobs with release dates on a single batching machine. Each <b>job's</b> <b>processing</b> time is an increasing simple linear function of its starting time. The machine can process up to b jobs simultaneously as a batch. The objective is to minimize the maximum completion time, i. e., makespan. For the unbounded model, i. e., b = [infinity], we obtain an O(n log n) dynamic programming algorithm. For the bounded model, i. e., b Scheduling Batching Deterioration Release dates Dynamic programming...|$|R
50|$|Total {{completion}} time scheduling:We wish to schedule <b>jobs</b> with fixed <b>processing</b> times on m identical machines. The <b>processing</b> time of <b>job</b> j is pj.Each job becomes {{known to the}} scheduler on its release time rj. The goal is to minimize thesum of {{completion time}}s over all jobs. A simplified problem is one single machine with the following input: at time 0, a <b>job</b> with <b>processing</b> time 1 arrives; k <b>jobs</b> with <b>processing</b> time 0 arrive at some unknown time. We need to choose a start time for the first job. Waiting incurs a cost of 1 per time unit, yet starting the first job before the later k jobs may incur an extra cost of k in the worst case. This simplified problem {{may be viewed as}} a continuous version of the ski rental problem.|$|R
40|$|We {{consider}} a two-machine flow shop problem {{in which each}} job is processed through an in-house system or outsourced to a subcontractor. A schedule is established for the in-house jobs, and performance {{is measured by the}} makespan. Jobs processed by subcontractors require paying an outsourcing cost. The objective is to minimize the sum of the makespan and total outsourcing costs. We show that the problem is NP-hard in the ordinary sense. We {{consider a}} special case in which each <b>job</b> has a <b>processing</b> requirement, and each machine a characteristic value. In this case, the time a job occupies a machine is equal to the <b>job's</b> <b>processing</b> requirement plus a setup time equal to the characteristic value of that machine. We introduce some optimality conditions and present a polynomial-time algorithm to solve the special case. Scheduling Outsourcing Ordered flow shop Computational complexity...|$|R
50|$|On IBM mainframes, BatchPipes is a batch <b>job</b> <b>processing</b> utility {{which runs}} under the MVS/ESA {{operating}} system and later versions - OS/390 and z/OS.|$|E
50|$|OS/360 {{provided}} limited interactive {{facilities in}} Conversational Remote Job Entry (CRJE), Graphic <b>Job</b> <b>Processing</b> (GJP), Interactive Terminal Facility (ITF) and Satellite Graphic <b>Job</b> <b>Processing</b> (SGJP) {{prior to the}} Time Sharing Option (TSO), but IBM did not carry those forward to SVS. TSO continues to provide equivalent facilities, except {{that it does not}} support use of a 2250 as a terminal. Use of a 2250 from a batch job using Graphics Access Method (GAM) and Graphics Subroutine Package (GSP) remains supported. OS/360 included a batch debugging facility named TESTRAN; it was clumsier than the equivalent facility in IBSYS/IBJOB, and was not used much. With the advent of TSO TESTRAN became even less relevant, and SVS did not include it.|$|E
50|$|JobServer {{supports}} some connectivity with Hadoop {{and can be}} used as a way {{of launching}} and monitoring Hadoop <b>job</b> <b>processing</b> activity. JobServer also includes support for the open source community distribution of Mule which can allow jobs and tasks to work with ESB and SOA platforms such as Mule.|$|E
40|$|AbstractLongest Expected {{processing}} time (LEPT) {{policy is a}} machine loading rule where {{out of all the}} jobs waiting to be processed by a machine, with their {{processing time}}s following given probability distributions, the one with the largest expected processing time is chosen first. Using a method based on Markov process and dynamic programming, we show that a LEPT policy will minimize the expected makespan for a two-machine stochastic open shop with Poisson arrival for <b>jobs.</b> <b>Processing</b> time of any job at any machine is exponential. We assume that all jobs are identical but the two machines are not...|$|R
40|$|In {{this paper}} we study {{the problem of}} {{scheduling}} n deteriorating jobs on m identical parallel machines. Each <b>job’s</b> <b>processing</b> time is a nondecreasing function of its start time. The problem is to determine an optimal combination of the due-date and schedule so as to minimize {{the sum of the}} due-date, earliness and tardiness penalties. We show that this problem is NP-hard, and we present a heuristic algorithm to find near-optimal solutions for the problem. When the due-date penalty is 0, we present a polynomial time algorithm to solve it. Key words: deteriorating jobs; parallel-machine scheduling; due-dat...|$|R
40|$|In {{this paper}} we {{consider}} a two-machine flow shop scheduling problem with deteriorating jobs. By a deteriorating job we {{mean that the}} <b>job's</b> <b>processing</b> time is an increasing function of its starting time. We model job deterioration as a function that is proportional to a linear function of time. The objective {{is to find a}} sequence that minimizes the total completion time of the jobs. For the general case, we derive several dominance properties, some lower bounds, and an initial upper bound by using a heuristic algorithm, and apply them to speed up the elimination process of a branch-and-bound algorithm developed to solve the problem. Department of Logistics and Maritime Studie...|$|R
50|$|Though {{his father}} was a {{well-known}} figure in the field, Kurant was initially reluctant to pursue a career as a cinematographer, instead studying still photography. While working at a <b>job</b> <b>processing</b> film at a research lab in France, Kurant took an evening class at a small film school; it was then that he decided to pursue cinematography as a career.|$|E
50|$|The Broker is OurGrid's user frontend. It {{provides}} {{support to}} describe, execute, and monitor jobs. <b>Job</b> <b>processing</b> {{is done by}} machines running OurGrid Workers. During the execution of a job, the Broker gets Workers on-demand from its associated Peer. It is the Broker's role to schedule the tasks to run on the Workers and to deploy and retrieve all data to/from Workers {{before and after the}} execution of tasks.|$|E
50|$|Spring Batch {{provides}} reusable {{functions that}} are essential in processing large volumes of records, including logging/tracing, transaction management, <b>job</b> <b>processing</b> statistics, job restart, skip, and resource management. It also provides more advanced technical services and features that will enable extremely high-volume and high performance batch jobs though optimization and partitioning techniques. Simple as well as complex, high-volume batch jobs can leverage the framework in a highly scalable manner to process significant volumes of information.|$|E
40|$|This paper {{considers}} a bi-criteria scheduling on parallel machines in fuzzy environment which optimizes the weighted flow time and maximum tardiness. The fuzziness, vagueness or uncertainty in <b>processing</b> time of <b>jobs</b> {{is represented by}} triangular fuzzy membership function. The bi-criteria problem with weighted flow time and maximum tardiness as primary and secondary criteria, respectively, {{for any number of}} parallel machines is NP-hard. So, a heuristic algorithm is proposed to find the optimal sequence of <b>jobs</b> <b>processing</b> on parallel machines so as to minimize the secondary criterion of maximum tardiness without violating the primary criterion of weighted flow time. A numerical illustration is also given in support of the algorithm proposed...|$|R
50|$|For {{three or}} more workstations, or {{three or more}} <b>jobs,</b> with varying <b>processing</b> times, open-shop {{scheduling}} is NP-hard.|$|R
40|$|In {{this paper}} we study the job shop {{scheduling}} problem {{under the assumption}} that the <b>jobs</b> have controllable <b>processing</b> times. The fact that the <b>jobs</b> have controllable <b>processing</b> times means {{that it is possible to}} reduce the processing time of the jobs by paying a certain cost. We consider two models of controllable processing times: continuous and discrete. For both models we present polynomial time approximation schemes when the number of machines and the number of operations per job are fixed...|$|R
50|$|QTAM was {{announced}} by IBM in 1965 {{as part of}} OS/360 and DOS/360 aimed at inquiry and data collection. As announced it also supported remote job entry (RJE) applications, called <b>job</b> <b>processing,</b> which was dropped by 1968. Originally QTAM supported the IBM 1030 Data Collection System, IBM 1050 Data Communications System, the IBM 1060 Data Communications System, AT&T 83B2 Selective Calling Stations, Western Union Plan 115A Outstations, and AT&T Teletype Model 33 or 35 Teletypewriters. By 1968 terminal support had expanded to include the IBM 2260 display complex, and the IBM 2740 communications terminal.|$|E
5000|$|He took a <b>job</b> <b>processing</b> {{emigration}} {{applications for}} Soviet Jews and in 1973, he immigrated to Israel {{with his wife}} {{around the time of}} the Yom Kippur War. He changed his last name to Sapir while in Israel and moved to the United States first to Louisville, Kentucky where he learned English and worked as a bus driver, janitor and a loader; and then to New York City where he worked as a taxicab driver. He then opened an electronics store with fellow immigrant Sam Kislin catering primarily to Russian clientele.|$|E
50|$|After Groden {{returned}} from his Army tour in 1967, he became a photo technician working in a New York City motion picture processing lab; he had special expertise blowing up 8mm film for theatrical distribution. In 1969 the company did a large <b>job</b> <b>processing</b> film for the documentary Woodstock; {{and because of that}} work, it was awarded a contract from Life to work on the Zapruder film, the 27-second home movie captured by Abraham Zapruder of the Kennedy assassination. Groden worked on that project and made an additional unauthorized copy of the film, which he then kept hidden for several years, fearing not only the legal ramifications but also for his own life.|$|E
40|$|This paper {{considers}} single machine {{scheduling and}} due date assignment with setup time. The setup time {{is proportional to}} the length of the already processed jobs; that is, the setup time is past-sequence-dependent (p-s-d). It is assumed that a <b>job's</b> <b>processing</b> time depends on its position in a sequence. The objective functions include total earliness, the weighted number of tardy jobs, and the cost of due date assignment. We analyze these problems with two different due date assignment methods. We first consider the model with job-dependent position effects. For each case, by converting the problem to a series of assignment problems, we proved that the problems can be solved in On 4 time. For the model with job-independent position effects, we proved that the problems can be solved in On 3 time by providing a dynamic programming algorithm...|$|R
3000|$|... • <b>Job</b> {{undertaken}} for <b>processing</b> is to {{be completed}} for all its operation before considering a new job. This is called non-splitting of the job.|$|R
5000|$|In data processing, {{a scratch}} tape is a {{magnetic}} tape {{that is used}} for temporary storage and can be reused or erased {{after the completion of}} a <b>job</b> or <b>processing</b> run. During the early years of computing, when magnetic tape was the primary form of mass storage, many programs, notably sorting routines, required such temporary storage.|$|R
50|$|The Model 20 {{offered a}} {{simplified}} and rarely used tape-based system called TPS (Tape Processing System), and DPS (Disk Processing System) that provided {{support for the}} 2311 disk drive. TPS could run on a machine with 8 KB of memory; DPS required 12 KB, which was pretty hefty for a Model 20. Many customers ran quite happily with 4 KB and CPS (Card Processing System). With TPS and DPS, the card reader was used to read the Job Control Language cards that defined the stack of jobs to run and to read in transaction data such as customer payments. The operating system was held on tape or disk, and results could also be stored on the tapes or hard drives. Stacked <b>job</b> <b>processing</b> became an exciting possibility for the small but adventurous computer user.|$|E
5000|$|In MVS, JES is a {{task that}} runs under MVS and {{provides}} the necessary functions to get jobs {{into and out of}} the MVS operating system, and to control the scheduling of, and, indeed, their execution. It is designed to provide efficient spooling, scheduling, and management facilities for the MVS operating system. By separating <b>job</b> <b>processing</b> into a number of tasks, MVS operates more efficiently. At any point in time, the computer system resources are busy processing the tasks for individual jobs, while other tasks are waiting for those resources to become available. In its most simple view, MVS divides the management of jobs and resources between the JES and the base control program of MVS. In this manner, the JES manages jobs before (i.e., during the reading and scheduling phases) and after the completion of running the program (i.e., the printing, punching and purging phases); the base control program manages them during processing, usually without any specific knowledge of JES.|$|E
40|$|We study {{a problem}} of {{scheduling}} n jobs on a single machine in batches. A batch {{is a set of}} jobs processed contiguously and completed together when the processing of all jobs in the batch is finished. Processing of a batch requires a machine setup time dependent on the position of this batch in the sequence. Setup times and <b>job</b> <b>processing</b> times are continuously controllable, that is, they are real-valued variables within their lower and upper bounds. A deviation of a setup time or <b>job</b> <b>processing</b> time from its upper bound is called a compression. The problem is to find a job sequence, its partition into batches, and the values for setup times and <b>job</b> <b>processing</b> times such that (a) total job completion time is minimized, subject to an upper bound on total weighted setup time and <b>job</b> <b>processing</b> time compression, or (b) a linear combination of total job completion time, total setup time compression, and total <b>job</b> <b>processing</b> time compression is minimized. Properties of optimal solutions are established. If the lower and upper bounds on <b>job</b> <b>processing</b> times can be similarly ordered or the job sequence is fixed, then O(n 3 log n) and O(n 5) time algorithms are developed to solve cases (a) and (b), respectively. If all <b>job</b> <b>processing</b> times are fixed or all setup times are fixed, then more efficient algorithms can be devised to solve the problems. Department of Logistics and Maritime Studie...|$|E
40|$|Copyright © 2014 Chuan-Li Zhao et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This paper considers single machine scheduling and due date assignment with setup time. The setup time {{is proportional to the}} length of the already processed jobs; that is, the setup time is past-sequence-dependent (p-s-d). It is assumed that a <b>job’s</b> <b>processing</b> time depends on its position in a sequence. The objective functions include total earliness, the weighted number of tardy jobs, and the cost of due date assignment. We analyze these problems with two different due date assignment methods. We first consider the model with job-dependent position effects. For each case, by converting the problem to a series of assignment problems, we proved that the problems can be solved i...|$|R
40|$|Abstract Motivated by a {{high-throughput}} logging system, {{we investigate}} the sin-gle machine scheduling problem with batching, where jobs have release times and processing times, and batches require a setup time. Our {{objective is to}} minimize the total flow time, in the online setting. For the online problem where all <b>jobs</b> have iden-tical <b>processing</b> times, we propose a 2 -competitive algorithm and we prove a cor-responding lower bound. Moreover, we show that if <b>jobs</b> with arbitrary <b>processing</b> times can be processed in any order, any online algorithm has a linear competitive ratio in the worst case...|$|R
30|$|From the figures, it {{is first}} {{possible}} {{to see that the}} maintainers do a great <b>job</b> in <b>processing</b> pull requests, given the small number of pull requests kept open. Projects electron, git-lfs, and hubot present low rates of open pull requests, 0.88, 0.13, and 0.08 %(!), respectively. For the latter, at the time of data collection, only 3 pull requests were left open.|$|R
