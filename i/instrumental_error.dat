104|265|Public
2500|$|Recent {{observations}} {{with the}} Planck telescope, {{which is very}} much more sensitive than WMAP and has a larger angular resolution, record the same anomaly, and so <b>instrumental</b> <b>error</b> (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, [...] "I do {{think there is a}} bit of a psychological effect; people want to find unusual things." ...|$|E
5000|$|In June 2009, Hiera was {{observed}} by Italian astronomer Stefano Mottola at the Spanish Calar Alto Observatory during 5 consecutive nights. Although a lightcurve {{could not be}} obtained and a systematic <b>instrumental</b> <b>error</b> could not be ruled out, the body displayed a slowly, ever decreasing brightness of 0.1 in magnitude, which would translate into a rotation period of at least 400 hours (...) [...] This would make Hiera a slow rotator.|$|E
50|$|Gravimetric analysis, if {{methods are}} {{followed}} carefully, provides for exceedingly precise analysis. In fact, gravimetric {{analysis was used}} to determine the atomic masses of many elements to six figure accuracy. Gravimetry provides very little room for <b>instrumental</b> <b>error</b> and does not require a series of standards for calculation of an unknown. Also, methods often do not require expensive equipment. Gravimetric analysis, due to its high degree of accuracy, when performed correctly, {{can also be used to}} calibrate other instruments in lieu of reference standards.|$|E
40|$|TRIAD. ???????? ??????? ??? ?????????? ??????? ???????????????? ??????? ??????????. ???????? ?????? ????????? QUEST. The {{deterministic}} algorithms {{of orientation}} determination are analyzed {{from the point}} of analysis <b>instrumental</b> <b>errors</b> view. Algorithms, more simple, are offered, than the known algorithm of TRIAD. Formulas are got for the further analysis of <b>instrumental</b> <b>errors</b> of algorithms. The analysis of algorithm of QUEST is executed. ????????????? ????????????????? ????????? ??????????? ?????????? ? ????? ?????? ??????? ???????????????? ??????. ?????????? ?????????, ????? ???????, ??? ????????? ???????? TRIAD. ???????? ??????? ??? ??????????? ??????? ???????????????? ?????? ??????????. ???????? ?????? ????????? QUEST...|$|R
40|$|A {{model that}} {{approximately}} {{takes into account}} <b>instrumental</b> <b>errors</b> in problems of precision reconstruction of quantum states is considered. The model {{is based on the}} notion of coherence volume, which characterizes the quality of the experimental and technological realization of the measurement protocol of a quantum state. Various sources of <b>instrumental</b> <b>errors</b> that affect the reconstruction accuracy of quantum states are mathematically modeled. It is shown that, for precision tomography of quantum states, one should take into account random coincidences that occur because the operating time of the photon detection system is finite. Among other possible sources of <b>instrumental</b> <b>errors</b> of the measurement protocol, one should take into account errors of the rotation angles and optical thicknesses of polarization plates, a possible drift in the parameters of the pump laser, and so on...|$|R
40|$|This paper proposes an {{analytical}} {{formulation of the}} <b>instrumental</b> <b>errors</b> in multilevel hierarchic SAR architecture equipped with active phased array antennas. The basis {{of the study is}} the derivation of the so called post-calibration errors, which remain as residual error contributions after the application of a dedicated internal calibration procedure. The limitations of the current internal calibration approaches for the-state-of-the-art space-borne SAR missions are analyzed and alternative internal global calibration strategies are proposed to reduce the impact of the post-calibration error. Numerical simulations of <b>instrumental</b> <b>errors</b> are presented to evaluate the different proposed calibration procedures. Peer ReviewedPostprint (published version...|$|R
5000|$|With {{the current}} methods, the <b>instrumental</b> <b>error</b> in the {{measured}} hairpin length is 1-1.5 nm. [...] The {{length of a}} basepair, or 2 extended single-stranded nucleotides, is approximately 0.85 nm. [...] Therefore, {{the resolution of the}} system is at a few nucleotides. The sources of noise arise from length-dependent Brownian motion of the bead anchored by the extended hairpin, statistical error in bead position determination, and slow mechanical drifts. [...] However, as mentioned earlier, such resolution is sufficient for the current sequencing method because changes in >4 nm are being measured.|$|E
50|$|In general, {{algorithms}} {{are based}} on the optimal estimation method. This essentially involves comparing the measured spectra with an a priori spectrum. Subsequently, the a priori model is contaminated {{with a certain amount of}} the item one wants to measure (e.g. SO2) and the resulting spectra are once again compared to the measured ones. The process is repeated again and again, the aim being to adjust the amount of contaminants such that simulated spectrum resembles the measured one as closely as possible. It must be noted that a variety of errors must be taken into consideration while perturbing the a priori, such as the error on the a priori, the <b>instrumental</b> <b>error</b> or the expected error.|$|E
5000|$|With the {{increasingly}} precise {{data provided by}} WMAP, {{there have been a}} number of claims that the CMB exhibits anomalies, such as very large scale anisotropies, anomalous alignments, and non-Gaussian distributions. The most longstanding of these is the low-l multipole controversy. Even in the COBE map, it was observed that the quadrupole (l = 2, spherical harmonic) has a low amplitude compared to the predictions of the Big Bang. In particular, the quadrupole and octupole (l = 3) modes appear to have an unexplained alignment with each other and with both the ecliptic plane and equinoxes, A number of groups have suggested that this could be the signature of new physics at the greatest observable scales; other groups suspect systematic errors in the data. Ultimately, due to the foregrounds and the cosmic variance problem, the greatest modes will never be as well measured as the small angular scale modes. The analyses were performed on two maps that have had the foregrounds removed as far as possible: the [...] "internal linear combination" [...] map of the WMAP collaboration and a similar map prepared by Max Tegmark and others. Later analyses have pointed out that these are the modes most susceptible to foreground contamination from synchrotron, dust, and Bremsstrahlung emission, and from experimental uncertainty in the monopole and dipole. A full Bayesian analysis of the WMAP power spectrum demonstrates that the quadrupole prediction of Lambda-CDM cosmology is consistent with the data at the 10% level and that the observed octupole is not remarkable. Carefully accounting for the procedure used to remove the foregrounds from the full sky map further reduces the significance of the alignment by ~5%.Recent observations with the Planck telescope, which is very much more sensitive than WMAP and has a larger angular resolution, record the same anomaly, and so <b>instrumental</b> <b>error</b> (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, [...] "I do think there is a bit of a psychological effect; people want to find unusual things." ...|$|E
40|$|At present, {{not enough}} {{attention}} is paid to the impact of uncertainty Earth's magnetic field, taking into account the errors of the sensor of the sun, setting errors inertia tensor microsatellite (MS) and noise measurements. This paper considers the problem of evaluating the accuracy of the angular orientation of the MS Kalman filter (FK) under uncertainty and measurement noise. Included <b>instrumental</b> <b>errors</b> and inaccurate meters mathematical model of MS. To solve this problem based simulation model of MS, where the mathematical tools used quaternion algebra. As of position sensors used magnetometer and sun sensor. The research of precision angular orientation of MS using positional sensors and FC in the presence of <b>instrumental</b> <b>errors</b> and uncertainties meters mathematical model of MS. A study evaluating the accuracy of the sensor orientation at the hit of the Sun in Earth's shadow. This study showed a significant effect of <b>instrumental</b> <b>errors</b> and uncertainties meters mathematical model of MS on the error estimation of the angular orientation of the metabolic syndrome, in particular, a significant impact inaccurate model of the geomagnetic field. FC is particularly sensitive to the absence of information from the sensor due to the sun getting MS in Earth's shadow. ??????????? ???????? ??????? ?????????? ????????????? ??????? ??????? ? ???????? ???????????????? ? ????? ?????????, ??? ? ???????? ??????????????? ???????? ???????????? ??????? ????????????. ???????????? ????????? ??????? ??????????? ? ?????? ??????...|$|R
40|$|Nonlinear matrix {{inversion}} operators {{have been developed}} which, applied to observed radiances, infer maximal information regarding atmospheric scattering parameters and vertical distribution of radiant sources and sinks. The algorithm has the attractive feature of noise discrimination, attributing <b>instrumental</b> <b>errors</b> to extra-atmospheric sources...|$|R
40|$|There are {{considered}} <b>instrumental</b> <b>errors</b> of electronic, precision bridge switcher with current stabilization, {{which is used}} in width-pulse modulators. There is made an expression for specified errors estimation, which is giving also a possibility to determine requirements parameter errors of electronic switcher?s components. ??????????? ???????????????? ??????????? ????????????, ?????????????, ????????? ??????????? ?? ????????????? ????, ??????? ???????????? ? ???????-?????????? ???????????. ????????? ????????? ??? ?????? ????????? ????????????, ?????? ????? ??????????? ?????????? ?????????? ? ???????????? ?????????? ??????????? ??????????? ???????????...|$|R
3000|$|... hlk---angle of {{diffraction}} {{from the}} (hlk) plains system. Urotropine {{was used as}} an etalon for higher confidence. <b>Instrumental</b> <b>error</b> was estimated using the etalon, and corrections for the reflex half-width were calculated.|$|E
40|$|Based on {{a hybrid}} {{simulation}} concept of HVDC, the developed hybrid AC filter models, providing the sufficiently full and adequate modeling of all single continuous spectrum of quasi-steady-state and transient {{processes in the}} filter, are presented. The obtained results suggest that usage of the hybrid simulation approach is carried out a methodically accurate with guaranteed <b>instrumental</b> <b>error</b> solution of differential equation systems of mathematical models of HVDC...|$|E
40|$|Bioanalysis {{frequently}} {{involves the}} measurement of very low analyte concentrations in complex and potentially variable matrices. An initial attempt {{has been made to}} apply a risk management tool to the bioanalytical method development like selection of spiked plasma volume, selection of internal standard to minimize processing error, selection of medium and extraction procedure, setting of mobile phase and pH, determination of chromatographic conditions etc. and to minimize <b>instrumental</b> <b>error</b> like; ion suppression and matrix effect...|$|E
40|$|Radio interferometers {{suffer from}} the problem of missing {{information}} in their data, due to the gaps between the antennae. This results in artifacts, such as bright rings around sources, in the images obtained. Multiple deconvolution algorithms have been proposed {{to solve this problem}} and produce cleaner radio images. However, these algorithms are unable to correctly estimate uncertainties in derived scientific parameters or to always include the effects of <b>instrumental</b> <b>errors.</b> We propose an alternative technique called Bayesian Inference for Radio Observations (BIRO) which uses a Bayesian statistical framework to determine the scientific parameters and <b>instrumental</b> <b>errors</b> simultaneously directly from the raw data, without making an image. We use a simple simulation of Westerbork Synthesis Radio Telescope data including pointing errors and beam parameters as instrumental effects, to demonstrate the use of BIR...|$|R
50|$|Certain <b>instrumental</b> <b>errors</b> {{could be}} {{averaged}} out by reversing the telescope on its mounting. A carriage was provided, which ran on rails between the piers, and {{on which the}} axis, circles and telescope could be raised by a screw-jack, wheeled out from between the piers, turned 180°, wheeled back, and lowered again.|$|R
30|$|It {{should be}} noted that the above {{discussion}} is the simplest and the most optimistic estimation of errors. The estimate given here means that the error caused by the random noise in the 0.90 -µm image is negligible. If possible, we should take into account all uncertainties, such as <b>instrumental</b> <b>errors</b> (calibration errors, nonlinearity, temperature dependences, among others) and model errors (cloud parameters, gas parameters, among others). Also, we should validate the whole procudure with real data and simulataneous measurements from the ground or other missions.|$|R
40|$|A new {{methodology}} {{is developed}} for integrating complementary ground-based data sources to provide consistent ozone vertical distribution time series {{as well as}} tropospheric and stratospheric ozone partial columns. Primary results are presented for the Alpine station of the Network for the Detection of Atmospheric Composition Changes (NDACC). Ozone measurements from the lidar at Haute-Provence Observatory, the microwave spectrometer at Bern and the FTIR spectrometer at the Jungfrauch station are used for this purpose. First step is to evaluate the validity domain of ozone profile data considered here by assessing <b>instrumental</b> <b>error</b> and vertical resolution. Each instrument has its own vertical resolution; therefore adjustments {{need to be done}} for the creation of an homogeneous data set. Indeed, because of the higher resolution of lidar measurements, smoothing of the data is necessary for the comparison with FTIR and microwave measurements. However, smoothing the data induces a loss of scientific information. Therefore a compromise has to be established and discussed. The various intercomparisons provide an evaluation of the differences due to <b>instrumental</b> <b>error</b> and atmospheric variability. The statistical method used for combining the different measurements in order to obtain ozone vertical profile time series consistent with total ozone measurements is then discussed...|$|E
40|$|Field {{portable}} {{energy dispersive}} x -ray fluorescence (FP-EDXRF) spectrometers {{are becoming a}} standard tool for non-destructive analysis of metals. This artìcle discusses the major {{issues related to the}} infield use of such equipments. The quantitative analysis of alloys is in principle possible with some restrietions due to the measuring conditions. The difficulties for a quantitative analysis and some evaluation of <b>instrumental</b> <b>error</b> are presenled. Finally, the possibihty to use such systems for the assay of ancient gold artefacts is shortly discussed...|$|E
40|$|Weak {{magnetic}} materials whose susceptibility values {{are close to}} the instrument's accuracy show very large errors in the direct evaluation of their ellipsoid parameters. This may lead to misinterpretation of the magnetic fabric, which is often used as a geological indicator. In order to estimate the measurement uncertainties, several statistical methods have been proposed. Within the available statistical methods, the Linear Perturbation Analysis (Hext, 1963) and the non-parametric bootstrap (Constable and Tauxe, 1990) technique have been widely used. In this paper, we make a complete study about these methods to estimate their limitations when applied to n measurements of a single sample. We will analyze which method is better in terms of uncertainties, we will determine when the methods do not provide reliable results and we will establish a measuring protocol. For that, we run simulations for the Linear Perturbation Analysis and the non-parametric bootstrap varying i) the number of measurements, ii) the <b>instrumental</b> <b>error</b> and iii) the shape parameter and the anisotropy degree of the AMS ellipsoid. The results show that both methods are not reliable when the difference between eigenvalues is too close in relation to the <b>instrumental</b> <b>error,</b> but increasing the number of measurements can improve the results. Peer reviewe...|$|E
40|$|Review of {{the first}} results {{obtained}} from the {{development and use of}} a new type of differential receiver designed to determine the relative locations on the lunar surface of two Apollo-deposited transmitters. This receiver has made it possible to reduce random and systematic <b>instrumental</b> <b>errors</b> to nearly negligible levels (displacement uncertainties equivalent to a few centimeters) ...|$|R
5000|$|Theodolite (Describing a {{theodolite}} as {{a transit}} may {{refer to the}} ability to turn the telescope a full rotation on the horizontal axis, which provides a convenient way to reverse the direction of view, or to sight the same object with the yoke in opposite directions, which causes some <b>instrumental</b> <b>errors</b> to cancel.) (Wolf and Brinker 183 - 6) ...|$|R
40|$|This paper {{reviews the}} main aspects {{involved}} with the management of <b>instrumental</b> <b>errors</b> associated with video-based optoelectronic stereophotogrammetry. Insights on how such errors propagate to kinematic quantities are of great interest {{in the field of}} human movement analysis to improve the precision and reliability of measurements. The review focuses on the technical assessment and analytical compensation procedures to cope with <b>instrumental</b> <b>errors.</b> Relevant contributions dealing with intrinsic sources of systematic and random errors, such as the issues concerning camera calibration and filtering and smoothing of marker position data, are presented. Procedures for marker imaged processing, and missing marker recovery are also surveyed. Methods for checking the accuracy and precision of stereophotogrammetric systems are then reviewed. Finally, since the desired outcome of the movement measurements is a reliable estimate of body segment kinematics, state-of-the-art techniques proposed for minimization of error propagation arising from a cluster of external markers are described...|$|R
40|$|Terrestrial laser {{scanning}} (TLS) {{is one of}} the most promising surveying techniques for rockslope characterization and monitoring. Landslide and rockfall movements can be detected by means of comparison of sequential scans. One of the most pressing challenges of natural hazards is combined temporal and spatial prediction of rockfall. An outdoor experiment was performed to ascertain whether the TLS <b>instrumental</b> <b>error</b> is small enough to enable detection of precursory displacements of millimetric magnitude. This consists of a known displacement of three objects relative to a stable surface. Results show that millimetric changes cannot be detected by the analysis of the unprocessed datasets. Displacement measurement are improved considerably by applying Nearest Neighbour (NN) averaging, which reduces the error (1 &sigma;) up to a factor of 6. This technique was applied to displacements prior to the April 2007 rockfall event at Castellfollit de la Roca, Spain. The maximum precursory displacement measured was 45 mm, approximately 2. 5 times the standard deviation of the model comparison, hampering the distinction between actual displacement and <b>instrumental</b> <b>error</b> using conventional methodologies. Encouragingly, the precursory displacement was clearly detected by applying the NN averaging method. These results show that millimetric displacements prior to failure can be detected using TLS...|$|E
3000|$|... where {{subscripts}} {{corresponding to}} the axes {{have been added to}} the measurement, model, and error values. In some fields where a modeling approach is applied, errors may be rigorously determined, but it is usually the case in geomagnetism that <b>instrumental</b> <b>error</b> is small, so that most of the error arises from the model not being able to completely represent the data. Setting a large error value for a certain station’s data is equivalent to applying a small weighting to it in the sum. The geometry of the χ [...]...|$|E
40|$|Discussion of the {{guidance}} system of a balloon-borne coronagraph designed for IR solar corona observations {{at an altitude}} of 36 km. The coronagraph has two telescopes, measures 6 m in length, weighs 73 kg, is stabilized by a three-axis system, and has a pointing precision of plus or minus 10 sec of arc. Three torque motors are used to activate the tracking servo system of the coronagraph. An integral control system is used to prevent a permanent <b>instrumental</b> <b>error.</b> The coronagraph has been tested in two balloon missions...|$|E
40|$|A {{method for}} {{estimation}} {{of the quality of}} a thin-film board has been developed, {{based on the results of}} resistance measuring and <b>instrumental</b> <b>errors</b> calculation. Recommendations are given for the exclusion of gross errors in the application of the method. The practical estimation carried out shows the high efficiency of the developed algorithms. This method allows to increase the boards yield by 1, 5 — 2 times...|$|R
40|$|Dual {{frequency}} (S/X band) {{very long}} baseline interferometry (VLBI) observations {{were used to}} test troposphere calibration by water vapor radiometers (WVRs). Comparison of the VLBI and WVR measurements show a statistical agreement (specifically, their structure functions agree) on time scales less than 700 seconds. On longer time scales, VLBI <b>instrumental</b> <b>errors</b> become important. The improvement in VLBI residual delays from WVR calibration {{was consistent with the}} measured level of tropospheric fluctuations...|$|R
40|$|Most VLBI {{images have}} low dynamic range {{because they are}} limited by {{instrumental}} effects such as calibration errors and poor u, v-coverage. We outline the method {{used to make a}} new image of the bright quasar 3 C 345 which has very high dynamic range and which is limited by the thermal noise, not <b>instrumental</b> <b>errors.</b> Both the Caltech VLBI package and the NRAO AIPS package were required to manipulate the data...|$|R
40|$|We {{discuss a}} model of {{repeated}} measurements of position in a quantum system which is monitored for a finite amount of time with a finite <b>instrumental</b> <b>error.</b> In this framework we recover the optimum monitoring of a harmonic oscillator proposed {{in the case of}} an instantaneous collapse of the wave function into an infinite-accuracy measurement result. We also establish numerically the existence of an optimal measurement strategy in the case of a nonlinear system. This optimal strategy is completely defined by the spectral properties of the nonlinear system...|$|E
30|$|The authors {{acknowledged}} that further {{investigation of the}} steady state tube furnace was warranted as {{in some of the}} testing they suspected an <b>instrumental</b> <b>error,</b> since they were unable to account for roughly two-thirds of the total carbon from the sample and detected unusually low levels of CO 2 during the under-ventilated tests. While there were some problems, the data does show that the yields of toxicants from the polyurethane foam were generally most representative of post-flashover conditions in the test methods that were designed for ventilation controlled conditions, such as the steady state tube furnace and the controlled atmosphere cone calorimeter.|$|E
40|$|A precise {{characterization}} of optical components generally occurs via interferometric measurements. It is show {{in this paper}} that Fourier based deflectometry method {{can be used for}} very sensitive and precise wavefront reconstruction. The wavefront is expressed from the raw measurements of the wavefront derivatives as a Zernike polynomial expansion. The form of the polynomials permits absolute <b>instrumental</b> <b>error</b> characterization by repeated measurement of the element under test oriented at several azimuthal angles. It is shown in this paper that nanometric precision of Zernike based reconstructions can be performed and that the air turbulences are the experimental limiting factor to the instrumental precision. Anglai...|$|E
40|$|The thermal {{diffusivity}} and effective infrared emissivity of water–methanol mix-turesweremeasuredat atmospheric pressure andambient temperature using apyro-electric thermal-wave resonator cavity. The applied frequency-scan method allows keeping the cavity length fixed, which eliminates <b>instrumental</b> <b>errors</b> and substan-tially improves the precision andaccuracyof themeasurements. A theoreticalmodel describing conduction and radiation heat transfer in the cavity was developed. The model predictions and the frequency-scan experimental datawere compared, show-ing excellent agreement. The measurements were performed for methanol volum...|$|R
40|$|The {{design of}} a {{direction-finding}} system for the LF and VLF bands is discussed {{from the point of}} view of sensitivity. That portion of atmospheric noise which excludes high-level impulses and which is usually about 10 db below the rms level, is regarded as setting the lower limit of sensitivity. <b>Instrumental</b> <b>errors</b> are evaluated and some component design is given for a system which provides up to four independent outputs. Peer reviewed: NoNRC publication: Ye...|$|R
40|$|The article {{described}} a technique for calibrating accelerometers unit on special test stands. The developed method allows to the determining the bias accelerometers unit, errors of scale factors accelerometers unit and angles of non-orthogonal accelerometers unit without presenting stringent requirements for high-precision testing equipment. Obtained {{by measuring the}} provisions of a test bed to which you want to set a block of accelerometers to obtain estimates of the <b>instrumental</b> <b>errors</b> of the block accelerometers. However, that requires precise measurement outputs of the accelerometers...|$|R
