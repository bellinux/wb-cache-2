10|34|Public
5000|$|The web color named [...] "lime" [...] {{actually}} {{corresponds to}} the green primary of an RGB display: it has a <b>different</b> <b>HTML</b> color code (#00FF00). A sample {{can be seen to}} the right.|$|E
40|$|The paper {{describes}} {{the development of}} program for scraping data from partially structured web pages. Web is a document based system. Every documents have their metadata that {{describes the}}ir structure. Documents on the web are written in HTML. Problem with HTML is that it's primary purpose is to describe visual properties of the document and not its content. Besides that, web documents are made with <b>different</b> <b>HTML</b> standards and almost all web pages are not 100...|$|E
40|$|Abstract — Internet {{contains}} {{large amount}} of data which user want to retrieve {{with the help of}} search input query. But the result return from the web has multiple dynamic output records. Hence, there is need of flexible information extraction system to convert web pages into machine process able structure which is essential for much application. This, essential information need to be extracted & annotated automatically which is challenge in data mining. In this paper, we survey on <b>different</b> <b>HTML</b> structure based technique to scrap data from web pages...|$|E
50|$|See {{the chart}} Color names that clash between X11 and HTML/CSS in the X11 color names article {{to see those}} colors which are <b>different</b> in <b>HTML</b> and X11.|$|R
5000|$|The author {{can write}} the content only once, then {{republish}} in different file formats or with <b>different</b> presentation (<b>HTML,</b> SCORM export, OpenOffice.org / pdf, flash slideshow... {{as long as}} they have been planned in the document model).|$|R
30|$|Defined two novel {{characteristics}} for extraction, namely visual text {{information and}} content semantics, especially, content semantics is very <b>different</b> from <b>HTML</b> tag semantics. The {{two kinds of}} characteristics are integrated with the DOM model of web pages to support accurate extraction decisions in different web sites. We also permit to extend different characteristics for extraction.|$|R
40|$|Traditionally, {{information}} extraction from web tables {{has focused on}} small, more or less homogeneous corpora, often based on assumptions {{about the use of}} tags. A multitude of <b>different</b> <b>HTML</b> implementations of web tables make these approaches difficult to scale. In this paper, we approach the problem of domain-independent {{information extraction}} from web tables by shifting our attention from the tree-based representation of web pages to a variation of the two-dimensional visual box model used by web browsers to display the information on the screen. The thereby obtained topological and style information allows us to fill the gap created by missing domain-specific knowledge about content and table templates. We believe that, in a future step, this approach can become the basis for a new way of large-scale knowledge acquisition from the current “Visual Web. ...|$|E
40|$|This {{document}} describes !startex?, {{a special}} !tex? format for students writing their first project report. !/abstract? !h 1 ? The basic philosophy of !Startex? !/h 1 ? !Startex? {{was designed for}} novice !tex? users. It employs a different notation and {{a different set of}} commands from !latex?, and the idea is that this makes it more user-friendly for these users than plain !tex? or !latex?. !p? The notation used in !startex? resembles HTML and some of the commands are the same, but the philosophy of the two is <b>different.</b> <b>HTML</b> was designed to display hypertext information on a computer screen, while !startex? is used to produce a student report on paper. !/body? Figure 1 : An example StarT E X document This solution does not solve the problem of obtaining good paragraph breaks, but experience so far has shown that it goes a long way...|$|E
40|$|Numerous genomic {{annotations}} {{are currently}} stored in different Web-accessible databanks that scientists need to mine with user-defined queries {{and in a}} batch mode to orderly integrate the diverse extracted data in suitable user-customizable working environments. Unfortunately, to date, most accessible databanks can be interrogated only for a single gene or protein {{at a time and}} generally the data retrieved are available in HTML page format only. We developed GeneWebEx to effectively mine data of interest in <b>different</b> <b>HTML</b> pages of Web-interfaced databanks, and organize extracted data for further analyses. GeneWebEx utilizes user-defined templates to identify data to extract, and aggregates and structures them in a database designed to allocate the various extractions from distinct biomolecular databanks. Moreover, a template-based module enables automatic updating of extracted data. Validations performed on GeneWebEx allowed us to efficiently gather relevant annotations from various sources, and comprehensively query them to highlight significant biological characteristics...|$|E
50|$|The Delivery Manager is {{responsible}} to deliver the output to different destinations, such as fax and email, with the flexibility of delivering the same output to <b>different</b> destinations; e.g. <b>HTML</b> format {{can be sent to}} email while a PDF format sent to the printer.|$|R
50|$|Some of {{the main}} {{features}} are: forms customization (e.g. labels, <b>different</b> types of <b>HTML</b> input field, content check), lookup fields, master/details view, files upload, calculated fields, hooks, graph report generation, export to CSV, checking for possible duplication during insertions, authentication, authorization restrictions on view/update/delete, and e-mail notices.|$|R
50|$|A {{serializer}} turns an XML event stream into {{a sequence}} of bytes (such as HTML) that can {{be returned to the}} client. There are serializers that allow you to send the data in many <b>different</b> formats including <b>HTML,</b> XHTML, PDF, RTF, SVG, WML and plain text, for example.|$|R
40|$|Abstract. This chapter {{presents}} an automatic method for creating hypermedia documents from conventional transcriptions of television programs. Using parallel text alignment techniques, the temporal information {{derived from the}} closed caption signal is exploited to convert the transcription into a synchronized text stream. Given this text stream, we can create links between the transcription and the image and audio media streams. We first describe an appropriate method for aligning texts based on dynamic programming techniques, then present results of text alignment on a database of 610 broadcasts (including three television news programs over a one-year period) for which we have caption, transcript, audio and image streams. We have found correspondences for from 77 to 92 % of the transcript sentences depending on the program set. Approximately 12 % of the correspondences involve sentence splitting or merging. We describe a system that generates several <b>different</b> <b>HTML</b> representations of television programs given the closed captioned video and a transcription. The techniques presented here can produce high quality hypermedia documents of video programs {{with little or no}} additional manual effort. 1...|$|E
40|$|Reprinted {{from the}} Proceedings of WebNet World Conference on the WWW and Internet 1999 with {{permission}} of AACE ([URL] is an electronic {{version of the}} {{paper presented at the}} World Conference on the WWW and Internet (WebNet ' 99) held in Honolulu (United States) on 1999 In this paper we describe a new approach for developing adaptive Web based courses. These courses are defined by means of teaching tasks which correspond to basic knowledge units, and rules which describe how teaching tasks are divided into subtasks. Both tasks and rules are used at execution time to guide the students during their learning process by determining the set of achievable tasks to be presented to the student at every step. Adaptivity is implemented by presenting students with <b>different</b> <b>HTML</b> pages depending on their profile, their previous actions, and the active learning strategy. The HTML pages presented to the students are generated dynamically from general information about the type of media elements associated to each task and their layout. The whole approach is exemplified by means of a course on traffic signs...|$|E
40|$|HTML 5 is a {{language}} for structuring and presenting {{content for the}} World Wide Web. It is expected to provide better flexibility and interoperability for HTML implementations and offering better user experience than the current HTML 4. Previous studies have shown some HTML 5 elements are able to provide better accessibility and interaction. In this study, we compare the page load time and server response time of equivalent page, implemented in HTML 5 or HTML 4, 15 HTML 5 elements such as , and CSS 3. 0 are compared with the Flash plug-in, CSS 2. 0 and JavaScript in IIS/Apache server. Page load time {{is defined as the}} total latency users experience when they open a page, and server response time is the time taken by the first HTTP SNK request that is initiated by device browser to the last FIN packet received from the server while accessing web content. Our results extend previous studies by evaluating new HTML 5 elements in Chrome and IE 9 browser environment, showing that HTML 5 can provide better performance in both client and server side, and the combination of Apache + Chrome is able to maximize the potential of HTML 5, which describes a better future for HTML 5 developmentIn this study, we have evaluated and compared the page load time and server response time of HTML 4 and HTML 5 for <b>different</b> <b>HTML</b> elements, i. e.,CSS, plug-in and form elements. The measurements have been performed in the IE 9 /Chrome browser and Apache/IIS web server. we conclude that although HTML 5 is in the development process, it has fully demonstrated better performance than legacy HTML 4 page in both client and server side. Using HTML 5 elements for web page building will significantly improve page performance and provide better user experience. The combination of Apache + Chrome is able to maximize the potential of HTML 5, which describes a better future for HTML 5 development. + 861527794318...|$|E
50|$|The {{goal was}} to examine the way Google read <b>different</b> types of <b>HTML</b> construction. Eventually, it became used in an {{unofficial}} contest to see which sites could receive the higher rankings. It has also become a synonym for expressions that cannot yet be found in search engine indexes.|$|R
40|$|The {{following}} are possible topics for projects, {{along with the}} area for each topic, {{and a list of}} suggested references. Most references are on- line papers, while some are web pages that link to other relevant papers. Please note that the on- line references are in <b>different</b> formats including:. <b>html,.</b> pdf,. ps,. ps. gz, and. doc...|$|R
5000|$|The user {{interface}} offers <b>different</b> Ajax and <b>Html</b> modes {{and can be}} customized by skins or an integrated color manager.The image gallery can be configured by an administration area {{and it is possible}} to customize each individual album by text files. Files can be uploaded with the integrated freeware TWG Flash Uploader. [...] TinyWebGallery includes the follow features ...|$|R
40|$|This article {{describes}} version 2. 7 of html 2 tex, a program {{which can be}} used to converts a single HTML file or a collection of related HTML files into a single LaTeX file. Such a LaTeX file can be processed into a PostScript file. To generate a single LaTeX file from a collection of HTML files, the user needs to give a skeleton LaTeX file, and indicate where translated versions of the HTML files should be included. The user also has to specify for each HTML file at which level (chapter, section, subsection, [...] ) it should be included. Links between the <b>different</b> <b>HTML</b> files are mapped to references in the LaTeX file. External links can be included as footnotes or as a bibliography. The generation of LaTeX is configurable. The mapping of each HTML tag to LaTeX commands can be specified. (This mapping can even be changed dynamically during the processing of the HTML file.) It is also possible to exclude certain parts from the HTML files from the generated LaTeX file, or to include LaTeX parts in HTML comment lines, which are ignored by HTML viewers. This makes it possible to maintain sources for both HTML and LaTeX in the same HTML files. The program performs certain checking of the HTML files, {{in order to be able}} to generate correct LaTeX output, but this checking is not guaranteed to conform any HTML standard. At some places the checking might be more relaxed, while at other places more restrictive then HTML 2. 0. So far, there is not much support for extensions beyond HTML 2. 0. The program does extensive checking of links between the different files. Because of this reason it can also be used as a link checking program, by giving it a single HTML file, and the option-c, or to change its name into chkhtml. In order to also check all referenced pages in the local directory (and its sub-directories), the option-s should be used as well. Links to excluded HTML files (and other URL’s) can either be reported as footnotes, or as a sorted bibliography in the LaTeX file. Error messages are reported on the standard output file. The program can also generate an extensive cross-references file mentioning all the anchor tags...|$|E
50|$|The HTML {{files that}} Sandcastle {{produces}} are either conceptual (user) documentation, being {{the result of}} a transformation from Microsoft Assistance Markup Language (MAML) topics, or they are reference documentation, which is automatically generated from reflection data and XML documentation comments. These two <b>different</b> types of <b>HTML</b> output share the same presentation style and may be compiled together to produce mixed user/reference documentation.|$|R
50|$|Paper Killer {{works with}} a {{document}} that encapsulates a set of pages and images, instead of working with single pages/files, so its design is <b>different</b> from common <b>HTML</b> editor programs: the user {{does not have to}} manage / memorize single filenames; with the editor it is possible to revise, browse, and search within the whole hypertext without the need to continuously save and load the information.|$|R
40|$|The Twenty-One project brings {{together}} environmental organisations, technology providers and research institutes from several European countries. The main {{objective of the}} project is to make documents on environmental issues—in particular, {{on the subject of}} sustainable development—available on CD-ROM and on the Internet. At present, these documents exist on different media (paper, electronic documents, audio-visual material), in <b>different</b> formats (<b>HTML,</b> word processor) and in different languages. This diversity impedes the distribution of documents through normal channels, and makes it hard to search for and retrieve targeted material on specified subjects. The project is developing search engines that can locate required information, and uses automatic translation tools to make foreign-language texts available. Authors and editors gain economic advantages by the increased distribution of their documents, and users find documents more readily available and easily accessible...|$|R
50|$|In {{the early}} years of the World Wide Web, the {{standards}} which today are used in most web browsers and web pages were mostly non-existent. <b>Different</b> browsers and <b>HTML</b> editors were being offered by competing vendors such as Netscape, Microsoft and WebTV, all of whom had their own ideas for how web sites should be constructed. These varying features resulted in web developers coding for specific web browsers, using elements which could often be recognized by only one browser.|$|R
50|$|HyperPublish {{works with}} a {{document}} that encapsulates a set of pages and images, instead of working with single HTML pages/files, so its design is <b>different</b> from traditional <b>HTML</b> editor programs: the user {{does not have to}} manage / memorize single filenames; with the editor it is possible to revise, browse, and search within the whole hypertext without the need to continuously save and load the information.The software includes an integrated all-in-one environment that permits both automated Web publishing and automated CD publishing.|$|R
5000|$|December 1998 saw the {{publication}} of a W3C Working Draft entitled Reformulating HTML in XML. This introduced Voyager, the codename for a new markup language based on HTML 4, but adhering to the stricter syntax rules of XML. By February 1999 the name of the specification had changed to XHTML 1.0: The Extensible HyperText Markup Language, and in January 2000 it was officially adopted as a W3C Recommendation. There are three formal DTDs for XHTML 1.0, corresponding to the three <b>different</b> versions of <b>HTML</b> 4.01: ...|$|R
40|$|International audienceSearch {{engines are}} now tools {{of every day}} life, Google being the most famous example. While they allow to access easily Internet documents, they {{are more and more}} used, under the {{denomination}} of "enterprise search engines", to manage the considerable amount of documents that constitute the memory of entreprises. We present the ground of a search engine that aims to propose a unified vision of the <b>different</b> documents formats (<b>HTML,</b> XML, Word, Mail, etc. but also data from databases) and to make search into these documents transparent to the user...|$|R
50|$|The WCAG is {{separated}} into 3 levels of compliance, A, AA and AAA. Each level requires a stricter set of conformance guidelines, such as <b>different</b> versions of <b>HTML</b> (Transitional vs Strict) and other techniques {{that need to}} be incorporated into your code before accomplishing validation. Online tools allow users to submit their website and automatically run it through the WCAG guidelines and produce a report, stating whether or not they conform to each level of compliance. Adobe Dreamweaver also offers plugins which allow web developers to test these guidelines on their work from within the program.|$|R
40|$|A full {{translation}} from L a T E X to HTML (or SGML) is not possible. This {{is because}} a L a T E X document contains {{a lot of}} formatting and page layout information that has no direct equivalent in HTML. It is not due to inadequacies in HTML but rather {{to the fact that}} <b>HTML</b> addresses <b>different</b> issues. <b>HTML</b> provides constructs for describing the structure of a document and not its precise layout. Having said {{that it is possible to}} translate L a T E X commands which have direct equivalents in HTML as well as mirror the structure of a document using hypertext links. The remaining heavily formatted items such as equations, pictures or tables can be converted to images which can be included in the final document or made available through hypertext links. Apart from dealing with the above issues, the L a T E X to HTML translator described in this document extends L a T E X by supporting arbitrary hypertext links and symbolic cross-references between evolving remote docume [...] ...|$|R
40|$|A {{great deal}} of the Web is {{replicate}} or near- replicate content. Documents may be served in <b>different</b> formats: <b>HTML,</b> PDF, and Text for different audiences. Documents may get mirrored to avoid delays or to provide fault tolerance. Algorithms for detecting replicate documents are critical in applications where data is obtained from multiple sources. The removal of replicate documents is necessary, not only to reduce runtime, but also to improve search accuracy. Today, search engine crawlers are retrieving billions of unique URL’s, of which hundreds of millions are replicates of some form. Thus, quickly identifying replicate detection expedites indexing and searching. One vendor’s analysis of 1. 2 billion URL’s resulted in 400 million exact replicates found with a MD 5 hash. Reducing the collection sizes by tens of percentage point’s results in great savings in indexing time and a reduction in the amount of hardware required to support the system. Last and probably more significant, users benefit by eliminating replicate results. By efficiently presenting only unique documents, user satisfaction is likely to increase...|$|R
40|$|This site {{offers a}} {{collection}} of United States Geological Survey (USGS) factsheets that introduce and explain {{a vast array of}} topics related to mapping. Each subject covers the history and development of that particular topic and includes any software or instruments that may be requried to utilize the information sources described. The topics covered on this site include: GIS, UTM, aerial photographs, map projections, map accuracy measurements, digital elevation models (DEM), satellite imagery, landcover mapping, hydrography, and numerous other components of maps. Information is available in several <b>different</b> formats (paper, <b>html</b> and pdf) and includes links back to other USGS services. Educational levels: General public, Graduate or professional, Undergraduate lower division, Undergraduate upper division...|$|R
40|$|Abstract: Dynamic HTML {{documents}} on the Internet contain useful {{information that can}} be reused by other applications. Unlike XML documents, the problem with HTML documents is {{that they do not have}} any semantics for the data in the page. Although a programmer can write a program that retrieves a peace of information from a specific HTML document available on the Internet, it will be very difficult to write several different programs to retrieve information from <b>different</b> dynamic <b>HTML</b> pages with varying formats. This paper develops a simple and generic approach to retrieve dynamic HTML Internet-based information. In this approach, several techniques that can be used to retrieve data from dynamic HTML documents are developed. These techniques were developed as a Java class which programmers can use to integrate and reuse HTML Internet Information for other real-time applications that need this information for their operations. The integrated Internet information can be weather information, stock prices, and top news. A number of experiments were conducted to measure the performance of these techniques. In addition, the paper discusses a number of applications that may benefit from this approach...|$|R
5000|$|XML and XHTML {{introduce}} {{the concept of}} namespaces. With namespaces, authors or communities of authors can define new elements and attributes with new semantics, and intermix those within their XHTML documents. Namespaces ensure that element names from the various namespaces will not be conflated. For example, a [...] "table" [...] element could be defined in a new namespace with new semantics <b>different</b> from the <b>HTML</b> [...] "table" [...] element and the browser {{will be able to}} differentiate between the two. In providing namespaces, XHTML combined with CSS allow authoring communities to easily extend the semantic vocabulary of documents. This accommodates the use of proprietary elements so long as those elements can be presented to the intended audience through complete style sheet definitions (including aural/speech and tactile styles).|$|R
40|$|Applications in {{real time}} {{environment}} demands interactions from different devices both fixed and mobile devices in a seamless manner. The users interacting are in different geographic environments. The datasets are of different nature and allows for interaction across users in different levels with more levels of constraints. This in turn leads to different directions for addressing the issues in mobile computing. The queries from users are also complex and involve joins. The data residing also leads to issues of data management and prioritization of requests in communication. The technology enabled devices are also improving with processors and <b>different</b> platforms like <b>HTML</b> 5 to address the existing issues. This paper presents a detailed survey of different issues in query processing in mobile environment. The solutions are also presented for the existing issues {{at the end of}} the paper...|$|R
40|$|Functional {{programming}} fits {{well with}} the use of descriptive markup in HTML and XML. There is also a good fit between S-expressions in Lisp and the means of expression in HTML and XML. These similarities are exploited in LAML (Lisp Abstracted Markup Language) which is a software package for Scheme. LAML supports exact mirrors of <b>different</b> versions of <b>HTML.</b> In the mirrors each HTML element is represented by a named Scheme function. The mirror functions guarantee that the generated HTML code is valid. LAML has been used for both server side CGI programming and programmatic authoring of non-trivial static web materials. The programmatic LAML author can use the power of functional programming for the production of everyday web documents. Equally important, it is straightforward to define domain-specific web languages in Scheme syntax which parallel the advantages of XML. ...|$|R
40|$|The {{increasing}} {{relevance of}} the Web as a mean for sharing information around the world has posed several new interesting issues to the computer science research community. The traditional approaches to information handling are ineffective in the new context: they are mainly devoted to the management of highly structured information, like relational databases, whereas Web data are semistructured and encoded using <b>different</b> formats (<b>HTML,</b> XML, and so on). In such context, we {{address the problem of}} clustering structurally similar Web documents, and in particular XML documents. This problem has several interesting applications, related, e. g., to the management of Web data. For example, the detection of structural similarities among documents can help in solving the problem of recognizing different sources providing the same kind of information [2], or in the structural analysis of a Web site. In this paper we propose a novel methodology for clustering XML documents, focusing on the notion of XML cluster representative, i. e., a prototype XML document subsuming the most relevant features of the set of XML documents within the cluster. In particular, we devise a technique to compute a representative of a set of XML documents, which is capable of capturing all the structural specificities within the represented documents. To this purpose, the notion of structural matching between the trees associated to two XML documents is exploited. Structural matchings allow to both identify the structural similarities between two XML documents and to build a representative around these similarities. We also investigate the exploitation of merging and pruning strategies for refining XML document trees into effective cluster representatives. ...|$|R
40|$|Since {{the advent}} of the World Wide Web, {{efficient}} searching of information on the web has become a challenge to the Internet technology community. However in earlier days when the net was not such a huge pool of information as it is today, most of the information used to be stored in static HTML pages. Thus searching techniques were primarily centered around web crawling, indexing and ranking of those static pages by the major search engines. With the rapid growth of information content of the web sites, this technique gave way to the more efficient emerging technology of integrating databases to the web. ASP (Active Server Pages) technology by Microsoft and other similar developments in different platforms like PHP for Unix came into being. The scenario now is quite <b>different</b> where <b>HTML</b> pages are dynamically generated and populated with data that is retrieved in response to direct and specific queries on databases of the web sites. This is called dynamic serving. Unfortunately this has led to a major drawback in using static page ranking, the method used by common search engines, since the HTML pages as mentioned above doesn't exist statically, but are generated only on querying the server databases. Thus they cannot be detected by web crawlers and ranked by any ranking algorithm. These pages invisible to the common search engines constitute the deep web, which is today an immense collection of the most specific, valuable quality searchable data. The paper will discuss on the strategic issues of searching the deep web and finally develop a probabilistic model of a search engine, detecting and ranking the deep web contents optimally...|$|R
