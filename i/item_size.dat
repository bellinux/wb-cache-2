50|459|Public
500|$|As in its {{predecessor}} Resident Evil 4, players can upgrade weapons {{with money and}} treasure collected in-game and heal themselves with herbs, but cannot run and shoot at the same time. New features include infected enemies with guns and grenades, the ability to upgrade weapons at any time from the inventory screen without having to find a merchant, and the equipping of weapons and items in real time during gameplay. Each player can store nine items. Unlike the previous games, the <b>item</b> <b>size</b> is irrelevant; a herb or a grenade launcher each occupy one space, and four items may be assigned to the D-pad. The game also features less puzzles than previous titles.|$|E
50|$|As in its {{predecessor}} Resident Evil 4, players can upgrade weapons {{with money and}} treasure collected in-game and heal themselves with herbs, but cannot run and shoot at the same time. New features include infected enemies with guns and grenades, the ability to upgrade weapons at any time from the inventory screen without having to find a merchant, and the equipping of weapons and items in real time during gameplay. Each player can store nine items. Unlike the previous games, the <b>item</b> <b>size</b> is irrelevant; a herb or a grenade launcher each occupy one space, and four items may be assigned to the D-pad.|$|E
50|$|Stomach {{sampling}} {{studies have}} discovered that small Atlantic cod feed primarily on crustaceans, while large Atlantic cod feed primarily on fish. In certain regions, the main food source is decapods with fish as a complementary food item in the diet. Wild Atlantic cod throughout the North Sea depend, to a large extent, on commercial fish species also used in fisheries, such as Atlantic mackerel, haddock, whiting, Atlantic herring, European plaice, and common sole, making fishery manipulation of cod significantly easier. Ultimately, food selection by cod {{is affected by the}} food <b>item</b> <b>size</b> relative to their own size. However, providing for size, cod do exhibit food preference and are not simply driven by availability.|$|E
40|$|AbstractWe {{follow the}} work of [G. Gutin, T. Jensen, A. Yeo, On-line bin packing with two <b>item</b> <b>sizes,</b> Algorithmic Operations Research 1 (2) (2006) ] and study the online bin packing problem, where every item has one of two {{possible}} sizes which are known in advance. We focus on the parametric case, where both <b>item</b> <b>sizes</b> are bounded from above by 1 k for some natural number k≥ 1. We show that for every possible pair of <b>item</b> <b>sizes,</b> there is an algorithm with competitive ratio of at most (k+ 1) 2 k 2 +k+ 1. We prove that this bound is tight for every k and, moreover, that it cannot be achieved if the two <b>item</b> <b>sizes</b> are not known in advance...|$|R
40|$|A {{polynomial}} algorithm for {{the multiple}} bounded knapsack problem with divisible <b>item</b> <b>sizes</b> is presented. The {{complexity of the}} algorithm is O(n 2 + nm), where n and m are the number of different <b>item</b> <b>sizes</b> and knapsacks, respectively. It is also shown that the algorithm complexity reduces to O(n logn +nm) when a single copy exists of each item...|$|R
50|$|The Jacob Lingerie {{boutique}} features {{items like}} bras, panties, lingerie, sleepwear and loungewear. Bra selection is sized 32A-B-C, 34A-B-C-D, 36A-B-C-D, 38A-B-C; panties XS-S-M-L; most other <b>items</b> <b>sized</b> XS-L.|$|R
30|$|Note {{that the}} <b>item</b> <b>size</b> used in Facebook is 64 B.|$|E
40|$|The {{functional}} response {{describes the}} relationship between feeding rate and prey density, and is important ecologically as it describes how foraging behaviour may change in response to food availability. The effects of habitat complexity and food <b>item</b> <b>size</b> were experimentally tested on the foraging parameters and functional responses of the freshwater fish roach Rutilus rutilus. Habitat complexity was varied through the manipulation of substrate and turbidity, and food <b>item</b> <b>size</b> was varied by using fishmeal pellets in two sizes. As water turbidity and substrate complexity increased, the reaction distance and consumption rate (per number) significantly decreased. Increased food <b>item</b> <b>size</b> significantly decreased consumption rates (per number) but had no influence on any other foraging parameter. Analysis of the interactions between substrate complexity, turbidity and food <b>item</b> <b>size</b> revealed food <b>item</b> <b>size</b> had the greatest influence on consumption rate (per number). Turbidity had the least effect on all the foraging parameters tested. Across all experiments, the functional responses were best described by the Type II response, a relatively consistent finding for R. rutilus. These outputs reveal that fish foraging behaviours and functional responses are highly context dependent, varying with environmental parameters {{and the availability of}} food resources of different sizes...|$|E
40|$|Objective: Examine the {{influence}} on short-term energy intake of <b>item</b> <b>size</b> variations of snack foods (i. e. small vs. large cookies) of equal sized portions. Methods: 77 children from 1 st and 6 th grade classes of {{an elementary school}} participated in a between subjects experimental design. All participants consumed in tables of four a same amount of cookies during an afternoon tea at their school. For half of the participants, foods were cut in two to make the small <b>item</b> <b>size.</b> Results: Decreasing the <b>item</b> <b>size</b> of foods without altering total portion size lead to a decrease of 25 % in gram weight intake. Appetitive ratings, subject and food characteristics had no moderating effect. Conclusions and Implications: Reducing the <b>item</b> <b>size</b> of foods could prove a useful dietary strategy based on decreased consumption, especially given the concern about the increased energy density of snack foods and children’s increased intake from these foods. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
30|$|The average case {{performance}} ratio measures the expected {{performance ratio}} of approximation algorithms by considering a uniform distribution of <b>item</b> <b>sizes.</b> The average case performance ratio of both BF and FF converge to 1 asymptotically [27].|$|R
40|$|This paper reports {{experiments}} {{with a new}} and surprisingly robust on-line heuristic for one-dimensional bin packing. This new Sum of Squares algorithm (SS) is restricted to the class of " distributions, i. e., ones in which the bin capacity and all <b>item</b> <b>sizes</b> are rational, as is common in practice. One begins by scaling up the <b>item</b> <b>sizes</b> (and the original unit bin capacity) so as to obtain an equivalent " distribution. One then repeatedly applies the following packing rule: Suppose B is the bin capacity and n(g) {{is the number of}} bins in the current packing whose contents total B g, 1 g < B, i. e., which have a " of size g. Place the next item so as to minimize...|$|R
40|$|The {{stochastic}} {{knapsack problem}} is the stochastic variant of the classical knapsack problem in which the algorithm designer is given a a knapsack with a given capacity {{and a collection of}} items where each item is associated with a profit and a probability distribution on its size. The goal is to select a subset of items with maximum profit and violate the capacity constraint with probability at most $p$ (referred to as the overflow probability). While several approximation algorithms have been developed for this problem, most of these algorithms relax the capacity constraint of the knapsack. In this paper, we design efficient approximation schemes for this problem without relaxing the capacity constraint. (i) Our first result is in the case when <b>item</b> <b>sizes</b> are Bernoulli random variables. In this case, we design a (nearly) fully polynomial time approximation scheme (FPTAS) which only relaxes the overflow probability. (ii) Our second result generalizes the first result to the case when all the <b>item</b> <b>sizes</b> are supported on a (common) set of constant size. (iii) Our third result is in the case when <b>item</b> <b>sizes</b> are so-called "hypercontractive" random variables i. e., random variables whose second and fourth moments are within constant factors of each other. In other words, the kurtosis of the random variable is upper bounded by a constant. Crucially, all of our algorithms meet the capacity constraint exactly, a result which was previously known only when the <b>item</b> <b>sizes</b> were Poisson or Gaussian random variables. Our results rely on new connections between Boolean function analysis and stochastic optimization. We believe that these ideas and techniques may prove to be useful in other stochastic optimization problems as well. Comment: To appear in SODA 201...|$|R
40|$|Objective: Examine the {{influence}} of altering the size of snack food (ie, small vs large cookies) on short-term energy intake. Methods: First- and sixth-graders (n = 77) participated in a between-subjects experimental design. All participants were offered the same gram weight of cookies during an afternoon tea at their school. For half of the participants, food was cut in 2 to make the small <b>item</b> <b>size.</b> Food intake (number of cookies, gram weight, and energy intake) was examined using ANOVA. Results: Decreasing the <b>item</b> <b>size</b> of food led to a decrease of 25 % in gram weight intake, corresponding to 68 kcal. Appetitive ratings and subject and food characteristics had no moderating effect. Conclusions and Implications: Reducing the <b>item</b> <b>size</b> of food could prove a useful dietary prevention strategy based on decreased consumption, aimed at countering obesity-promoting eating behaviors favored by the easy availability of large food portions...|$|E
40|$|Studies {{considering}} {{the impact of}} food size variations on consumption have predominantly focused on portion size, while very little literature investigated variations in food <b>item</b> <b>size,</b> especially at snacking occasions which yielded contradictory results. This study evaluated the effect of how altering the size of food items (i. e. small vs. large candies) of equal sized food portions would affect short-term energy intake while snacking. The study used a between subjects design (n= 33; spring 2008) in a randomized experiment. In a psychology laboratory (separate cubicles), participants (undergraduate psychology students, 29 of 33 female, age 20. 3 ± 2 years, body mass index 21. 7 ± 3. 7) were offered candies to free consumption while participating in an unrelated computerized experiment. For half of them, items were cut in two to make the small food <b>item</b> <b>size.</b> Food intake (grams, kcal and number of food items) was examined using analysis of variance. Results showed that decreasing the <b>item</b> <b>size</b> of candies led participants to decrease by half their gram weight intake, resulting in an energy intake decrease of 60 kcals. Appetitive ratings, subject and food characteristics had no moderating effect. A cognitive bias could explain why people tend to consider that one unit of food (e. g. ten candies) is the appropriate amount to consume, regardless {{of the size of}} the food items in the unit. This study suggests a simple dietary strategy: decreasing food <b>item</b> <b>size</b> without having to alter portion size may reduce energy intake at snacking occasions. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|We {{describe}} a probabilistic reference disambiguation mechanism developed for a spoken dialogue system mounted on an autonomous robotic agent. Our mechanism receives as input referring expressions containing intrinsic features of individual concepts (lexical <b>item,</b> <b>size</b> and colour) and features involving {{more than one}} concept (ownership and location). It then performs probabilistic comparisons between the given features and features of objects in the domain, yielding a ranked list of candidate referents. Our evaluation shows high reference resolution accuracy {{across a range of}} spoken referring expressions...|$|E
40|$|International audienceA {{state-of-the-art}} of {{a particular}} planning problem, the Single <b>Item</b> Lot <b>Sizing</b> Problem (SILSP), is given for its uncapacitated and capacitated versions. First classes of lot sizing problems are briefly surveyed. Various solution methods for the Uncapacitated Single <b>Item</b> Lot <b>Sizing</b> Problem (USILSP) are reviewed. Four different mathematical programming formulations of the classical problem are presented. Different extensions for real-world applications of this problem are discussed. Complexity results of the Capacitated Single <b>Item</b> Lot <b>Sizing</b> Problem (CSILSP) are given together with its different formulations and solution techniques...|$|R
40|$|In this paper, {{we present}} an {{efficient}} algorithm to compute a tighter lower {{bound for the}} one-dimensional bin packing problem. The time complexity of the algorithm is O(n log n). We have simulated the algorithm on randomly generated bin packing problems with <b>item</b> <b>sizes</b> drawn uniformly from (a; b], 0 a ! b B. If our lower bound is used, on average, the error of BFD is less than 2...|$|R
40|$|In this paper, {{we present}} an O(n 1 ogn) {{algorithm}} to compute a tighter lower {{bound for the}} one-dimensional bin packing problem. We have simulated the algorithm on randomly generated bin packing problems with <b>item</b> <b>sizes</b> drawn uniformly from (a, b], where 0 5 a 3 ̆c b 5 B and B is bin size. Using our lower bound, the average error of BFD is less than 2...|$|R
40|$|AbstractWe study {{a variety}} of NP-hard bin packing {{problems}} under a divisibility constraint that generalizes the often encountered situation in which all item sizes are powers of 2. For ordinary one-dimensional bin packing, we show that First Fit Decreasing produces optimal packings under this restriction, and that if in addition the largest <b>item</b> <b>size</b> divides the bin capacity, then even the less powerful First Fit algorithm is optimal. Similar results are obtained for two-dimensional bin packing and multiprocessor scheduling, along with several other simple variants. For more complicated problems, like vector packing and dynamic bin packing, the improvement is less substantial, and we indicate why...|$|E
40|$|Abstract Replication is {{a widely}} used {{technique}} in unstructured overlays to improve content availability or system performance. A fundamental question often addressed by previous work focused on: how many replicas ought to be allocated for each data item given the fixed query rates and limited storage capability? In this paper, we have put forth two optimal replica distributions to achieve the highest success rate and the lowest message consumption. Especially, we have investigated the influence of <b>item</b> <b>size</b> on replica distribution. Our results show that Square-Root Replication, which is traditionally considered to be optimal, {{is not always the}} best choice. Our study offers a new deep understanding of resource managment in self-organized unstructured overlays...|$|E
40|$|The {{results of}} an optimal {{foraging}} model using linear programming with constraints for feeding time, digestive capacity, sodium requirements, and energy requirements indicate that snowshoe hare (Lepus americanus) may forage as energy maximizers. The solution provides the quantities of major food classes (leaves, herbs, fungus, twigs) included in the diet. The species composition of each diet class also is determined using a simultaneous search model based upon the probability of encounter, the probability of sufficient <b>item</b> <b>size,</b> and the probability of sufficient quality. The results also indicate that hare life history parameters (weaning size, size at first reproduction, average adult size) and potential demographic changes in hare populations may be controlled by foraging considerations...|$|E
40|$|Observers can {{accurately}} perceive {{and evaluate}} the statistical properties {{of a set of}} objects, forming what is now known as an ensemble representation. The accuracy and speed with which people can judge the mean size of a set of objects have led to the proposal that ensemble representations of average size can be computed in parallel when attention is distributed across the display. Consistent with this idea, judgments of mean size show little or no decrement in accuracy when the number of objects in the set increases. However, the lack of a set size effect might result from the regularity of the <b>item</b> <b>sizes</b> used in previous studies. Here, we replicate these previous findings, but show that judgments of mean set size become less accurate when set size increases and the heterogeneity of the <b>item</b> <b>sizes</b> increases. This pattern can be explained by assuming that average size judgments are computed using a limited capacity sampling strategy, and it does not necessitate an ensemble representation computed in parallel across all items in a display...|$|R
40|$|We {{study the}} Best Fit {{algorithm}} for on-line bin packing under the distribution {{in which the}} <b>item</b> <b>sizes</b> are uniformly distributed in the discrete range f 1 =k � 2 =k�:::�j=kg. Our main result is that, in the case j = k; 2, the asymptotic expected waste remains bounded. This settles an open problem of Co man et al [3], and involves {{a detailed analysis of}} the in nite multi-dimensional Markov chain underlying the algorithm...|$|R
40|$|AbstractGiven a {{knapsack}} of size K, non-negative values d and Δ, {{and a set}} S of items, each <b>item</b> e∈S with <b>size</b> se {{and value}} ve, we define a shelf as a subset of items packed inside a bin with total <b>items</b> <b>size</b> at most Δ. Two subsequent shelves must be separated by a shelf divisor of size d. The size of a shelf is the total <b>size</b> of its <b>items</b> plus the <b>size</b> of the shelf divisor. The SHELF-KNAPSACK Problem (SK) {{is to find a}} subset S′⊆S partitioned into shelves with total shelves size at most K and maximum value. The CLASS CONSTRAINED SHELF KNAPSACK (CCSK) is a generalization of the problem SK, where each item in S has a class and each shelf in the solution must have only items of the same class. We present approximation schemes for the SK and the CCSK problems. To our knowledge, these are the first approximation results where shelves of non-null size are used in knapsack problems...|$|R
40|$|Several {{predictions}} of central place foraging theory were tested. As predicted, beavers foraged more selectively at increasing {{distance from the}} central place. With increasing distance from the river 2 ̆ 7 s edge, beavers cut fewer branches and deleted small branches from their diet. Large branches were favored at all distances, which differs from patterns observed in previous studies of beaver foraging. This difference, however, is expected and supports Schoener 2 ̆ 7 s (1979) predictions {{which are based on}} differences between provisioning costs and <b>item</b> <b>size.</b> The selective harvesting of branches predicted by foraging theory affects plant growth form and may influence plant reproductive patterns. High rates of branch removal caused cottonwoods to develop a shrubby architecture. The importance of selective branch choice by beavers on patterns of cottonwood reproduction (i. e., delayed sexual maturity and induced cloning) is discussed...|$|E
40|$|We {{analyze the}} average case {{performance}} of bounded space bin packing algorithms. The analysis {{is based on}} a novel technique of average case analysis which is suitable for analyzing a wide variety of algorithms. Our analysis covers algorithms such as Next-K Fit, K-Bounded Best Fit and Next Fit Decreasing, as well as of other algorithms. We consider the one-dimensional bin packing problem with discrete item sizes. Discrete item sizes appear in most realworld applications of bin packing. However, standard average case analysis assume that items are chosen from a continuous interval. We show that many important results are lost in the transition from the discrete to the continuous distribution. Our technique is general enough to calculate results for any discrete <b>item</b> <b>size</b> distribution. This is significant for real-world applications where the uniform distribution does not always hold...|$|E
40|$|Yehuda Afek Eli Gafni y Adi Ros'en z Abstract This paper {{presents}} {{a simple and}} efficient building block, called slide, for constructing communication protocols in dynamic networks whose topology frequently changes. We employ slide to derive (1) an end-to-end communication protocol with optimal amortized message complexity, and (2) a general method to efficiently and systematically combine dynamic and static algorithms. (Dynamic algorithms are designed for dynamic networks, and static algorithms work in networks with stable topology.) The new end-to-end communication protocol has amortized message communication complexity O(n) (assuming that the sender is allowed to gather enough data items before transmitting them to the receiver), where n is {{the total number of}} nodes in the network (the previous best bound was O(m), where m is the total number of links in the network). This protocol also has bit communication complexity O(nD), where D is the data <b>item</b> <b>size</b> in bits (assuming [...] ...|$|E
40|$|AbstractGiven bins of size B, non-negative values d and Δ, {{and a list}} L of items, each <b>item</b> e∈L with <b>size</b> se {{and class}} ce, we define a shelf as a subset of items packed inside a bin with total <b>item</b> <b>sizes</b> at most Δ such that all items in this shelf have the same class. Two {{subsequent}} shelves must be separated by a shelf division of size d. The size of a shelf is the total <b>size</b> of its <b>items</b> plus the <b>size</b> of the shelf division. The class constrained shelf bin packing problem (CCSBP) is to pack the items of L into the minimum number of bins, such that the items are divided into shelves and the total size of the shelves in a bin is at most B. We present hybrid algorithms based on the First Fit (Decreasing) and Best Fit (Decreasing) algorithms, and an APTAS for the problem CCSBP {{when the number of}} different classes is bounded by a constant C...|$|R
40|$|We {{propose a}} branch-and-price {{algorithm}} for solving {{a class of}} stochastic transportation problems with single-sourcing constraints. Our approach allows for general demand distributions, nonlinear cost structures, and capacity expansion opportunities. The pricing problem is a knapsack problem with variable <b>item</b> <b>sizes</b> and concave costs that is interesting in its own right. We perform an extensive set of computational experiments illustrating the efficacy of our approach. In addition, we study {{the cost of the}} single-sourcing constraints. Transportation problem Random demands Nonlinear costs...|$|R
40|$|K={ 1, [...] ,k} {{is a set}} of {{dimensions}} • I={ 1, [...] ,n} {{is a set of}} <b>items</b> with <b>size</b> wil (for each dimension) • B={ 1, [...] ,m} is a set of bins with capacity cjl (for each dimension) Multidimensional Binpacking • K={ 1, [...] ,k} is a set of dimensions • I={ 1, [...] ,n} is a set of <b>items</b> with <b>size</b> wil (for each dimension) • B={ 1, [...] ,m} is a set of bins with capacity cjl (for each dimension...|$|R
40|$|Abstract. The {{increase}} {{number of}} mobile users in wireless environment affects query access time substantially. To minimise the query access time, one possible {{way is to}} employ data broadcasting strategy. In this paper, we propose cost models for both query access time over broadcast channel and on-demand channel. We examine the cost models to find optimum number of broadcast items in a channel while utilising query access time over on-demand channel as a threshold point. The optimum number indicates a point to split the broadcast cycle and allocate the data items in the new channel or else the on-demand channel outperforms the broadcast channel. The cost model involves several factors that dynamically change the optimum number of broadcast items like request arrival rate, service rate, size of data <b>item,</b> <b>size</b> of request, and bandwidth. Simulation model is developed to verify {{the performance of the}} cost model. This paper focuses on request that returns a single data item. ...|$|E
40|$|TheData Diffusion Machine(DDM) is {{a virtual}} shared memory {{architecture}} in which the data has no home location. In this paper we present a preliminary evaluation of a link-based DDM, show {{the influence of the}} major design parameters, and show the performance of a set of eight benchmark programs. Most programs are sensitive to <b>item</b> <b>size</b> but to varying degrees. The associativity is unimportant for system performance, apparently because {{of the size of the}} associative memories. The hardware is shown to be quite well balanced. Although the network latency is a critical factor for some programs, others gain more from a faster processor or a faster memory. Preliminary scalability results are quite encouraging. Except for a few cases, scalability is limited more by the software overheads in the application than by the DDM. The results will improve further when realistic topologies can be evaluated and when programs are run where locality has been a design issue. 1 Introduction The DDM [1] is [...] ...|$|E
40|$|TheDataDiffusion Machine (DDM) is {{a virtual}} shared memory {{architecture}} in which the data has no home location. In this paper we present a preliminary evaluation of a link-based DDM, show {{the influence of the}} major design parameters, and show the performance of a set of eight benchmark programs. Most programs are sensitive to <b>item</b> <b>size</b> but to varying degrees. The associativity is unimportant for system performance, apparently because {{of the size of the}} associative memories. The hardware is shown to be quite well balanced. Although the network latency is a critical factor for some programs, others gain more from a faster processor or a faster memory. Preliminary scalability results are quite encouraging. Except for a few cases, scalability is limited more by the software overheads in the application than by the DDM. The results will improve further when realistic topologies can be evaluated and when programs are run where locality has been a design issue...|$|E
40|$|In {{this paper}} we present {{approximation}} {{results for the}} on-line class constrained bin packingproblem. In this problem we are given bins of capacity 1 with C compartments, and n items of Q different classes, each item i 2 { 1, [...] ., n} with class c(i) and size s(i). The problem consiststo pack the items into bins in an on-line way, where each bin contains at most C different classesand has total <b>items</b> <b>size</b> at most 1. We show that the bounded space version of this problem {{does not have an}} algorithm with constant competitive ratio. If each <b>item</b> have <b>size</b> at least &quot; < 1, we show that the problem does not admit an algorithm with competitive ratio better than O(1 /C&quot;). In the unbounded case we show that the First-Fit algorithm has competitive ratio in [2. 7, 3] andwe present an algorithm with competitive ratio in [2. 66, 2. 75]...|$|R
40|$|AbstractIn {{this paper}} we present {{approximation}} {{results for the}} class constrained bin packing problem that has applications to Video-on-Demand Systems. In this problem we are given bins of size B with C compartments, and n items of Q different classes, each item i∈{ 1,…,n} with class ci and size si. The problem is to pack the items into bins, where each bin contains at most C different classes and has total <b>items</b> <b>size</b> at most B. We present several approximation algorithms for offline and online versions of the problem...|$|R
40|$|In {{this paper}} {{we present a}} {{theoretical}} analysis of the deterministic on-line Sum of Squares algorithm (SS) for bin packing introduced and studied experimentally in [CJK 99], along with several new variants. SS is applicable to any instance of bin packing in which the bin capacity B and <b>item</b> <b>sizes</b> s(a) are integral (or can be scaled to be so), and runs in time O(nB). It performs remarkably well from an average case point of view: For any discrete distribution in which the optimal expected waste is sublinear, SS also has sublinear expected waste...|$|R
