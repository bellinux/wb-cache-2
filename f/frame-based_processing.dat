10|2|Public
40|$|The task of zero {{resource}} query-by-example keyword search {{has received}} much attention {{in recent years}} as the speech technology needs of the developing world grow. These systems traditionally rely upon dynamic time warping (DTW) based retrieval algorithms with runtimes that are linear {{in the size of the}} search collection. As a result, their scalability substantially lags that of their super-vised counterparts, which take advantage of efficient word-based in-dices. In this paper, we present a novel audio indexing approach called Segmental Randomized Acoustic Indexing and Logarithmic-time Search (S-RAILS). S-RAILS generalizes the original frame-based RAILS methodology to word-scale segments by exploiting a recently proposed acoustic segment embedding technique. By in-dexing word-scale segments directly, we avoid higher cost <b>frame-based</b> <b>processing</b> of RAILS while taking advantage of the improved lexical discrimination of the embeddings. Using the same conversa-tional telephone speech benchmark, we demonstrate major improve-ments in both speed and accuracy over the original RAILS system. Index Terms â€” Zero resource, query-by-example search, speech indexing, fixed-dimensional embeddin...|$|E
30|$|Recently, {{certain studies}} have {{investigated}} the use of cepstral <b>frame-based</b> <b>processing</b> {{to compensate for the}} noise effect to achieve better recognition accuracy. For example, the work in [11] revealed that in the CHN method, even though each cepstral channel is processed by histogram equalization (HEQ), a significant histogram mismatch still exists among the training and testing cepstral features for the low-pass filtered (LPF) and high-pass filtered (HPF) portions of the intra-frame cepstra. Thus, the method of spatial HEQ in [11] further performs HEQ on the LPF and HPF portions to eliminate the aforementioned mismatch for the CHN-preprocessed cepstra. Compared with conventional CHN that processes each individual cepstral channel, spatial HEQ (S-HEQ) additionally takes the neighboring cepstral channels into consideration collectively and produces superior noise robustness. Furthermore, for a frame signal, the LPF and HPF portions of the cepstral vector just correspond to the logarithmic filter-bank (LFB) components at lower and higher frequencies, respectively. However, compensation performed directly on LPF and HPF is more helpful than that applied to the LFB components, most likely because the LFB components are significantly correlated [11].|$|E
40|$|The {{following}} {{defines a}} set of coding properties that are used {{as part of the}} media personalization solution. As indicated below, {{one of the advantages of}} this solution is that it is standard-based, as are the tools. The properties defined here are a combination of MPEG- 4 (H. 264 mostly) and MPEG- 2. The combination provides a solution for both coding schemes. MPEG- 4 is composed of a collection of &quot;tools &quot; built to support and enhance scalable composition applications Among the tools discussed here are shape coding, motion estimation and compensation, texture coding, error resilience, sprite coding and scalability. Unlike MPEG- 4, MPEG- 2 provides a very limited set of functionality for scalable personalization. The tools defined in this document are nevertheless sufficient to provide personalization in the MPEG- 2 domain. Object-Based Structure and Syntax Content-based interactivity, The MPEG 4 standard extends the traditional <b>frame-based</b> <b>processing</b> towards the composition of several video objects superimposed on a background image. For the proper rendering of the scene without disturbing artifacts on the border of video objects (VO), the compressed stream contains the encoded shape of the VO representing video as objects rather than i...|$|E
40|$|In this thesis, the {{problems}} associated with the encoding and transcoding of multiple video objects are considered. In contrast to <b>frame-based</b> video <b>processing,</b> the handling of multiple video objects introduces new degrees of freedom that can be exploited in terms of compression capabilities and transmission through the network. Additionally, new sources of information must be considered, such as the boundary definition of an object. One of th...|$|R
40|$|Graduation date: 1997 There is a {{software}} gap in parallel processing. The short lifespan and small installation base of parallel architectures {{have made it}} economically infeasible to develop platform-specific parallel programming environments that deliver performance and programmability. One obvious solution is to build architecture-independent programming environments. But the architecture independence usually comes {{at the expense of}} performance, since the most efficient parallel algorithm for solving a problem often depends on the target platform. Thus, unless a parallel programming system has the ability to adapt the algorithm to the architecture, it will not be effectively machine-independent. This research develops a new methodology for architecture-adaptable parallel programming. The methodology is built on three key ideas: (1) the use of a database of parameterized algorithmic templates to represent computable functions; (2) <b>frame-based</b> representation of <b>processing</b> environments; and (3) the use of an analytical performance prediction tool for automatic algorithm design. This methodology pursues a problem-oriented approach to parallel processing as opposed to the traditional algorithm-oriented approach. This enables the development of software environments {{with a high level of}} abstraction. The users state the problem to be solved using a high-level notation; they are freed from the esoteric tasks of parallel algorithm design and implementation. This methodology has been validated in the format of a prototype of a system capable of automatically generating an efficient parallel program when presented with a well-defined problem and the description of a target platform. The use of object technology has made the system easily extensible. The templates are designed using a parallel adaptation of the well-known divide-and-conquer paradigm. The prototype system has been used to solve several numerical problems efficiently on a wide spectrum of architectures. The target platforms include multicomputers (Thinking Machines CM- 5 and Meiko CS- 2), networks of workstations (IBM RS/ 6000 s connected by FDDI), multiprocessors (Sequent Symmetry, SGI Power Challenge, and Sun SPARCServer), and a hierarchical system consisting of a cluster of multiprocessors on Myrinet...|$|R
40|$|Radio {{broadcasting}} {{technology has evolved}} rapidly {{over the last few}} years due to ever increasing demands for as high quality sound services with ancillary data transmission in mobile environment. In order to accomplish this, Members of European Broadcasting Union (EBU), the European Telecommunications Standards Institute (ETSI) and International Telecommunications Union (ITU-R) developed a completely new digital radio {{broadcasting technology}} called the Eureka- 147 Digital Audio Broadcasting (DAB) system which improves the overall broadcasting performance by delivering near CD quality audio and data services in mobile receivers along with efficient use of the available radio frequency spectrum. Digital Audio Broadcasting (DAB) system developed within the Eureka 147 Project is a new digital radio technology for broadcasting radio stations that provides high-quality audio and data services to both fixed and mobile receivers. The system uses COFDM technology that combats the effect of multipath fading & ISI and makes it spectrally more efficient compared with existing AM/FM systems. This project presents the performance analysis of Eureka- 147 DAB system. DAB transmission mode-II is implemented first and then extended successfully to other modes. A <b>frame-based</b> <b>processing</b> is used in this study. Performance studies for AWGN, Rayleigh and Rician channels have been conducted. For all studies BER has been used as performance criteria. This project also discusses issues related to system performance using concatenated coding technique, including the outer Block code, the inner convolutional code, outer BCH code and the inner convolutional code. ...|$|E
40|$|The MPEG- 4 video {{standard}} {{extends the}} traditional <b>frame-based</b> <b>processing</b> with {{the option to}} compose several video objects (VO) superimposed on a background sprite image. In our previous work, we presented a distributed, multiprocessor based, scalable implementation of an MPEG- 4 arbitrary-shaped decoder, which forms together with the background sprite decoder an essential part for further scene rendering. For control of the multiprocessor architecture, we have constructed a Quality-of-Service (QoS) management that monitors the availability of required data and distributes the processing of individual tasks with guaranteed or best-effort services of the platform. However, the proposed architecture with the combined guaranteed and best-effort services poses problems for real-time scene rendering. In this paper, we present a technique for proper run-time rendering of the final scene after decoding one VO Layer. The individual video-object monitors check the data availability and select the highest quality for the final scene rendering. The algorithm operates hierarchically both at the scene level and at the task level of the video object processing. Whereas the earlier work on scalable implementation concentrated only on guaranteed services, we now introduce a new element in the system architecture for the real-time control and fall back mechanism of the best-effort services. This element is based on first, controlling data availability at task level, and second, introducing the propagation service to QoS management. We present our simulation results in the comparison with the standard 2 Ì† 2 frame-skipping 2 Ì† 2 technique {{that is the only}} currently available solution to this type of rendering a scalable processing...|$|E
40|$|The aim of {{this thesis}} is to {{implement}} a Digital Audio Broadcast (DAB) system in base-band. In this work I designed a DAB transmitter for mode II, following the ETSI standards for DAB transmitter was designed. This mode has been chosen for simulation because of its suitability in the local area terrestrial broadcasting {{and to be a}} model that presents other transmission mode implementation. The physical modulation part of the DAB for transmission modes II as well as its receiver is implemented. The prime focus of this thesis is on the reception side. In the receiver side, receiver synchronization has been implemented and a new classification method for the QPSK mapping proposed. Support vector Machine (SVM) is used as a QPSK Classifier and as a channel estimator for DAB system. It uses the continuous learning algorithm, for training and testing. This learning algorithm needs one-time training for the classification of first DAB symbol, and for the remaining symbols the system learns the pattern from the previous symbol estimated by classifier. A <b>frame-based</b> <b>processing</b> was used in this study. Performance studies for AWGN and Rayleigh channels have been conducted. Bit error rate (BER) has been considered as the performance index. The result obtained showed that the implemented system work successfully in AWGN channel. For Rayleigh fading channel, the system performance is desirable in urban area, below the speeds of 50 - 70 Kmph of the receiver. All the simulation work is implemented using the Microsoft Windows Operating system and MATLAB...|$|E
40|$|In {{this paper}} we show how {{established}} object modelling techniques {{can be used}} in the creation of spoken dialogue management systems. One of the motivations behind the particular approach adopted here is the observation that, in spoken human-to-human dialogues, certain skillsets and patterns of dialogue evolution are common to many different contexts; other dialogue skills and accompanying real-world knowledge are required only for more specialised transactions within particular business domains. As a starting point for modelling an automated spoken dialogue management system we recommend a use case analysis of the required functionality. The use case analysis encourages the developer to identify generic-specific relationships and interactions between different dialogue management skills. We consider some of the broad philosophies underlying current dialogue management systems and outline practical high-level dialogue behaviour based on mixed-initiative, <b>frame-based</b> <b>processing,</b> combined with a rigorously applied confirmation strategy. On the basis of the use case requirements analysis, we explore a possible design for an object-oriented dialogue management system, indicating the roles and relationships of the various classes that embody the required dialogue functionality, and showing how implemented objects within the system will interact. The manner of this interaction is such as to allow one overall system to process transactions in several business domains. We also indicate some of the advantages of a rule-based implementation: the proposed design is tailored towards such an implementation in Prolog++. An object-oriented development process places high-level, generic dialogue management functionality at the disposal of more specialised â€˜expert â€™ components. Maintainability and extensibility are therefore enhanced: if the developer chooses to refine generic behaviour, it is immediately available to the more specialised components; if new domain-specific expertise is required, it can be added with minimal impact on generic behaviour. 1...|$|E
30|$|Typically, a {{huge number}} of {{rate-distortion}} (RD) cost computations are required to find the best mode from 64 [*]Ã—[*] 64 to 8 [*]Ã—[*] 8 block sizes in the encoder side for HEVC. With respect to applications, HEVC would be employed for ultrahigh-resolution video services. For such cases, fast video coders are required to process more data with a given processing power. Thus, parallelization techniques would be crucial, with multiple low-power processors or platforms. The single instruction multiple data (SIMD) implementation of the most time-consuming modules on HM 6.2 encoders was proposed [6]. This work implemented the cost functions, transformation, and interpolation filter with SIMD, and it reported that the average time saving obtained is approximately 50 % to 80 %, depending on the modules. Wavefront parallel processing (WPP) for HEVC encoders and decoders was introduced [7]. For the decoder case, they achieved parallel speed-up by a factor of 3. The acceleration factor of the wavefront parallelism is in general saturated into 2 or 3 due to data communication overhead, epilog, and prolog parts. There are no works that incorporate all the parallel algorithms, with maximum harmonization for fast HEVC encoders. In this paper, we focus on load-balanced slice parallelization, with optimization implementation of HEVC. This paper presents several optimization techniques using SIMD operations for the RD cost computations and transforms for variable block sizes. In addition, motion estimation is also efficiently implemented with a <b>frame-based</b> <b>processing</b> {{to reduce the number of}} redundant filtering. For data-level parallelization, this paper demonstrates how to allocate encoding jobs to all the available cores through the use of complexity estimation. As a result, it is possible to achieve load-balanced slice parallelism in HEVC encoders to significantly reduce the average encoding time. With all the proposed techniques, the optimized HEVC encoder achieves a 90.1 % average time saving within 3.0 % Bjontegaard distortion (BD) rate increases compared to HM 9.0 reference software.|$|E
30|$|Speech coding is {{a process}} to {{transform}} a digitized speech signal into a bit-efficient representation that keeps reasonable speech quality so as to facilitate speech transmission over a band-limited channel or speech storage in a memory-limited media. In general, speech coding techniques can be classified into three categories, including waveform coding, parametric coding, and hybrid coding. The waveform coding technique attempts to maintain the waveform shape of the original speech signal in sample level without any knowledge about the speech generation process. Famous standard speech coders of this category are G. 711 A-law and Î¼-law Pulse Code Modulation (PCM) coders [1], and G. 726 and G. 727 Adaptive Differential PCM coders [2]. Generally, a waveform coder works well at a high bit rate of 32 Â kbps or above. The parametric coding technique represents a speech signal by parameters of a speech-generating model. Among various speech-generating models, the most successful one is the linear predictive coding (LPC) model that assumes speech signal is the output of an all-pole model (autoregressive model) fed with an excitation input signal. The parameters of the all-pole filter conceptually represent the vocal tract shape that is highly correlated with the spectral envelope of the speech, while the excitation signal uses a quasiperiodic impulse train to represent information of fundamental frequency (or F 0) for voiced speech, pseudorandom noise for unvoiced speech, {{or a combination of}} the two (i.e., mixed excitation). Coders of this type encode speech signal in a <b>frame-based</b> <b>processing</b> manner and could operate at low bit rates ranging from 2 to 5 Â kbps. Differing from waveform coders, parametric coders make no attempt to preserve the original waveform shape but to keep the perceptual quality of the reconstructed speech. Famous standard LPC-based speech coders are FS 1015 LPC of LPC- 10 e algorithm [3, 4] and MELP (mixed excitation linear prediction) [5]. The hybrid coding technique tries to combine the advantages of both waveform coding and parametric coding. Coders of this type are similar to parametric coders in utilizing speech-generating models, but also similar to waveform coders in keeping encoded speech waveforms close to the original ones by more detailed modeling of the excitation signal. They generally adopt the code-excited linear prediction (CELP) algorithm [6] to minimize perceptually weighted error. Representative standard hybrid coders are FS 1016 CELP [7, 8], ITU-T G. 728 LD-CELP [9], ETSI AMR-ACELP [10], etc. A hybrid coder generally operates at a medium bit rate of 5 to 15 Â kbps.|$|E
40|$|Tez (Doktora) [...] Ä°stanbul Teknik Ãœniversitesi, Fen Bilimleri EnstitÃ¼sÃ¼, 2002 Thesis (PhD) [...] Ä°stanbul Technical University, Institute of Science and Technology, 2002 GÃ¼nÃ¼mÃ¼zde, mikrofotogrametri elektronik sanayiinden saÄŸlik hizmetlerine ve Ã¶zellikle uzay endÃ¼strisine kadar pek Ã§ok bilimle ortak olarak calÄ±ÅŸmaktadir. Bu Ã§alÄ±ÅŸmalara destek saÄŸlayan ve Ã§alÄ±ÅŸmalarda kullanÄ±lan uzman sistemler laboratuvar Ã§alÄ±ÅŸmalarÄ±nda Ã¶nemli bir yer tutmaktadÄ±r. BÃ¼tÃ¼n bunlarÄ±n Ä±ÅŸÄ±ÄŸÄ±nda ve Ã¼niversitemizin teknolojik olarak diÄŸer dÃ¼nya Ã¼niversiteleri ile aynÄ± Ã§aÄŸdaÅŸ ve bilimsel dÃ¼zeye cÄ±kmasÄ± iÃ§in bir uzman sistemin tasarÄ±mÄ± ve kurulumunun Fotogrametri Anabilim DalÄ± LaboratuvarÄ±na kurulmasÄ± Ã§alÄ±ÅŸmasÄ± ve sistemin kalibre edilmesi aynÄ± zamanda tek baÅŸÄ±na bir doktora Ã§alÄ±ÅŸmasÄ±nÄ± iÃ§ermektedir. Dijital olarak 0. 5 cm ve daha kÃ¼Ã§Ã¼k objelerin modelenmesi, Ã¶lÃ§Ã¼lmesi ve kalite kontrolÃ¼nÃ¼n saÄŸlanmasÄ± amacÄ±na yÃ¶nelik olan bu sistem aynÄ± zamanda uydu ve uzay Ã§alÄ±ÅŸmalarÄ±nda kullanÄ±lan dijital sensÃ¶rlerin testi, kalibrasyonu ve modern gÃ¶rÃ¼ntÃ¼leme ve Ã¶lÃ§me Ã§alÄ±ÅŸmalarÄ±nda da kullanÄ±labilmekte ve bu sayede Ã¼niversitemizde yapÄ±lacak birÃ§ok lisans, yÃ¼ksek lisans ve doktora Ã§alÄ±ÅŸmasÄ±nda temel sistem olacaktÄ±r. AyrÄ±ca ekonomik yÃ¶nden, yÃ¼ksek hassasiyette yapÄ±lmasÄ± gereken Ã§alÄ±ÅŸmalar iÃ§in Ã¼niversitemize destek saÄŸlayacaktÄ±r. AyrÄ±ca yapÄ±lacak yeni yÃ¼ksek lisans ve doktora Ã§alÄ±ÅŸmalarÄ± iÃ§inde kaynak saÄŸlayan bir konudur. Dijital fotogrametrik uzman sistemler gÃ¼nÃ¼mÃ¼zde endÃ¼stride pek Ã§ok Ã§eÅŸit Ã¶lÃ§me problemlerinde kullanÄ±lmaktadÄ±r. YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸe sahip CCD (Charge Couple Device) kameralarÄ±n geliÅŸtirilmesi ile endÃ¼striyel Ã§izgisel alan tarama yÃ¶nrtemi ile Ã§alÄ±ÅŸan kameralar yakÄ±n resim fotogrametrisinde Ã¶nemli oranda kullanÄ±lmaya baÅŸlanmÄ±ÅŸtÄ±r. Bu sistemler iÃ§in elektronik endÃ¼strisi desteÄŸinde tasarlanan veri toplama kartlarÄ± teknoloji ile birlikte geliÅŸerek yeterli kapasiteye ulaÅŸmÄ±ÅŸtÄ±r. Burada en Ã¶nemli konu, bu Ã§eÅŸit digital kameralarÄ±n Ã¶lÃ§me amaÃ§larÄ± ile kullanÄ±labilmesi iÃ§in kalibre edilerek iÃ§ yÃ¶neltme parametrelerinin ve lensin distorsiyon deÄŸerlerinin belirlenebilmesidir. Kalibrasyon Ã§alÄ±ÅŸmasÄ± laboratuvar ortamÄ±nda bir test alanÄ± kullanÄ±larak gerÃ§ekleÅŸtirildi. Uzman sistemlerde donanÄ±m ve yazÄ±lÄ±m birlikte Ã§alÄ±ÅŸÄ±r. Digital EndÃ¼striyel sistemler gerÃ§ekten maliyet aÃ§Ä±sÄ±ndan Ã§ok pahalÄ± olmalarÄ±na karÅŸÄ±n uzun sÃ¼re kullanÄ±labilmeleri ve Ã¼retimin kontrol ve hÄ±zÄ±nÄ± artÄ±rmalarÄ± aÃ§Ä±sÄ±ndan Ã¶zellikle yÃ¼ksek maliyet gerektiren sanayilerde (uÃ§ak motoru ve otomotiv sanayii gibi) en Ã§ok tercih edilen sistemlerdir. EndÃ¼striyel fotogrametrinin Ã¼retim sektÃ¶rÃ¼nde kullanÄ±lan engeniÅŸ uygulama alanÄ± iÃ§erisinde en Ã¶nemli Ã¶zellik gerÃ§ek zaman ve ya gerÃ§ek zamana yakÄ±nlÄ±k denilen Ã¼Ã§ boyutlu konum belirlemedeki kesinliktir. ÃœÃ§ boyutlu hassas konum belirleme Mikro-Fotogrametrinin bir amacÄ±dÄ±r. Deneysel Ã§alÄ±ÅŸma olarak, sistemde kullanÄ±lan kameralar ile yapÄ±lan diÄŸer bir yakÄ±n resim fotogrametrisi uygulamasÄ± ise Ä°stanbul Teknik Ãœniversitesi Ä°nÅŸaat FakÃ¼ltesinin YapÄ± Deprem LaboratuvarÄ±nda yapÄ±lmakta olan zemin deneylerinden iki adet zemin numunesi Ã¼zerinde yapÄ±lan yÃ¼ke baÄŸlÄ± deformasyon analizinin fotogrametrik olarak yapÄ±lmasÄ± Ã§alÄ±ÅŸmasÄ±dÄ±r. SonuÃ§ta Fotogrametrik olarak elde edilen deformasyonlar klasik yÃ¶ntem olarak kullanÄ±lan CDP 50 (Linear Variable Differential Transducer) ile elde edilen deformasyonlar ile karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±. Nowadays, microphotogrammetric {{works have}} been {{deal with the}} other science like electronic, {{medicine}} espeacially space researchers. Expert Systems which {{have been used for}} this works have been improved and become improtant day by day. Therefore an expert system would have been built and used for researching the new tecnologies and measuring the very small objects for quality control and 3 D modeling with aided computer vision technologies also using new calibration researching for the sensor technologies, etc. Firstly, built an expert system and calibrate this system is the aim of this Ph. D. Thesis. After that this expert system will have been used for diploma thesis, Master Degree Thesis and also Ph. D. Thesis in the future. The modern universities which deal with this tecnologies of speace reasearching have an expert system like this for imaging, 0. 5 mm high precise measuring and researching the sensor tecnologies. Multisensory remote sensing requires the simultaneous registration and real-time processing of the time-varying multi-sensor image data. The frame-based programming technique was developed to provide the appropriate multi-sensor data management Any <b>frame-based</b> <b>processing</b> system supports the automatic data updating since the output of any sensor has been changed. The visual programming of data flows is naturally available through the usage of this approach. The appropriate set of the frame types is formed to design the most generic multisensory framework. Finally, the problem of real-time, multi-processor implementation of the frame-based software architecture is used. This expert system works with digital video cameras which included digital sensors and also PC. Digital photogrammetric systems have been used to solve various measurement problems in industrial applications for many years, ever since high-resolution CCD cameras and powerful computer technologies have been available. In close range photogrammetric applications, working conditions of the industrial platforms are usually difficult. In these conditions, a surveying engineer has to find the best solution for the problem especially in the experimental area. Industrial line-scan video cameras have been widely adapted for close range photogrammetry and machine vision applications. Although the advantages of onboard storage of digital images, such as quick data storage and portability, industrial cameras are replacing small format sensor on cameras with possibility of the huge image scales and PCI cards requirements. An important task in 3 D computer vision is camera calibration, especially when metric information is required for applications involving accurate dimensional measurements. The proposed technique only requires that the camera observes a pattern shown at different orientations. The motion of the pattern need not be known and the pattern itself can be imprecise. This technique has been recently extended to the calibration of a stereovision sensor. Using a photogrammetry approach, the intrinsic parameters of each camera, the 3 D points of the pattern and the relative position and orientation of the two cameras are computed all together using bundle adjustment. For the expert systems, hardware and the software have been worked together. This is the fundamental algorithm of the expert systems. In this study, the equipments of expert system have been tested with CDP 50 (Linear Variable Differential Transducer) in an experiment of reinforced concrete slab deformation. Experimental process in deformation analysis has been designed for civil engineering close range applications in this study. For an application in industrial photogrammetry, twin industrial cameras with IEEE 1394 standard, which are the essential part of an expert system has been calibrated with 16 mm fix focused lenses from Pentax on a test field. In order to determine the deformations of a reinforced concrete slab loaded stepwise by an actuator in the Earthquake Laboratory, these cameras are located in the object diagonal with stereoscopic view of the slab surface. The main orientation of the photogrammetric extraction has been obtained from the first image set of the experiment steps. The configuration of the signal points have been designed which coordinate differences give us the deformation directly. Afterwards for orientation process, coordinate differences between suitable signal points, which is decided during the test, have been measured with a compass. These differences have been used for condition equations in the Bundle Block Adjustment. At the end of the bundle block adjustment, the derived exterior orientation parameters have been obtained with enough accuracy. Afterwards the system has been used for determining geometric properties of the slab. The 3 D surface model of the slab has been obtained precisely and the deformations of the slab have been documented. The deformations on the slab have been obtained in the photogrammetric restitution for signal points in image scale. At last these test shows that industrial expert systems can be used in such industrial applications with success. At the end, the results of photogrammetric and TML CDP 50 have been compared. DoktoraPh...|$|E

