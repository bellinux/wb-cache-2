0|96|Public
40|$|International audienceThis paper revisits {{the failure}} 1 {{temporal}} independence hypothesis which is omnipresent {{in the analysis}} of resilience methods for HPC. We explain why a previous approach is incorrect, and we propose a new method to detect failure cascades, i. e., series of non-independent consecutive failures. We use this new method to assess whether public archive <b>failure</b> <b>logs</b> contain <b>failure</b> cascades. Then we design and compare several cascade-aware checkpointing algorithms to quantify the maximum gain that could be obtained, and we report extensive simulation results with archive and synthetic <b>failure</b> <b>logs.</b> Altogether, there are a few logs that contain cascades, but we show that the gain that can be achieved from this knowledge is not significant. The conclusion is that we can wrongly, but safely, assume failure independence...|$|R
40|$|Abstract — Scalability {{models are}} {{powerful}} analytical tools for evaluating and predicting {{the performance of}} parallel applica-tions. Unfortunately, existing scalability models do not quantify failure impact and therefore cannot accurately account for application performance {{in the presence of}} failures. In this study, we extend two well-known models, namely Amdahl’s law and Gustafson’s law, by considering the impact of failures and the effect of fault tolerance techniques on applications. The derived reliability-aware models can be used to predict application scalability in failure-present environments and evaluate fault tolerance techniques. Trace-based simulations via real <b>failure</b> <b>logs</b> demonstrate that the newly developed models provide a better understanding of application performance and scalability in the presence of failures. I...|$|R
40|$|Abstract—In this paper, {{we present}} an ongoing {{research}} effort on developing an automatic fault diagnosis and prognosis service for large-scale computing systems, such as TeraGrid clusters. By leveraging {{the research on}} system health monitoring, the proposed service aims at automatically revealing fault patterns from historical data by applying data mining and machine learning techniques. To address key challenges posted by fault diagnosis and prognosis, two integrated techniques are developed: a knowledge base to accumulate empirical and inferred fault patterns from historical data and a meta-learning mechanism to optimally combine separately learned classifiers for improved detection and prediction accuracy. We also present preliminary studies on <b>failure</b> <b>logs</b> from the BlueGene/L systems at SDSC and ANL...|$|R
40|$|Dependencies between {{failures}} in operational networks {{may have a}} huge impact on their reliability and availability. In this paper we analyze <b>failure</b> <b>logs</b> to identify simultaneous and potentially correlated {{failures in}} routers and links of an IP backbone network. We show that the actual behavior of failure processes does not support the independence assumption commonly used in theoretical studies. Scatter plots are presented to visualize the failure processes, and it is seen that geographical adjacency has a pronounced effect. The existence of high correlation coefficients and high autocorrelation in some failure processes was observed. A formal analysis confirms this. The consequences of these dependencies on the provisioning of guaranteed availability are briefly discussed...|$|R
40|$|The current overall ICT {{infrastructure}} {{mainly the}} Internet and Telecom networks can be {{looked upon as}} an ecosystem, {{which is the result}} of the cooperation between a huge number of Autonomous systems (ASes). The interconnection and interdependence between ASes become large and complex as technology advances. This interdependence of ASes or subsystems create vulnerabilities in such a way that problems in one of the interconnected networks affect the normal operation of other networks and even might result in a failure of services across the whole system. The aim of this study is twofold. The first is to discuss about the basic features and trends in the <b>logs</b> of <b>failure</b> data to get some insight about the network s behaviour. In addition to this, the study looks into failure prediction by using the primary failure data to model normal behaviours and predict the system level(critical) <b>failures.</b> <b>Failure</b> <b>log</b> data will be used to model the normal(expected) behaviours of the failures and hence for prediction when there happens a change in the normal behaviour. The report first discusses the conceptual model mainly about some related works as well as a background knowledge on wavelet technique. Then, a simple failure data analysis and brief discussion on the main trend observed during the preliminary study is presented. Lastly, a simple approach for failure prediction using wavelet technique is presented followed by evaluation and discussion of results. The report focuses in using a frequency domain approach which is called wavelet technique. A wavelet based failure prediction approach is proposed which uses some frequencies in the <b>failure</b> <b>log</b> data to characterize the normal operation and hence identify deviations(abnormal behaviours) from the variation in those frequencies when something bad occurs in the network. Once the deviations are identified, a root cause analysis can be conducted for a detail investigation of the problem areas...|$|R
40|$|A new {{distributed}} {{mutual exclusion}} algorithm, using a token and based upon an original rooted tree structure is presented. The rooted tree introduced, named "open-cube", has noteworthy stability and locality properties, allowing the proposed algorithm to achieve good performances and high tolerance to node failures : {{the worst case}} message complexity per request is, {{in the absence of}} node <b>failures,</b> <b>log</b> 2 N+ 1 where N is the number of nodes, whereas 0 (log 2 N) extra messages in the average are necessary to tolerate each node failure. This algorithm is a particular instance of a general scheme for token and tree-based distributed mutual exclusion algorithms, previously presented in part by the authors, consequently, its safety and liveness properties are inherited from the general one, however, the present paper is self-contained...|$|R
40|$|Cooperative checkpointing, {{in which}} the system {{dynamically}} skips checkpoints requested by applications at runtime, can exploit system-level information to improve performance and reliability {{in the face of}} failures. We evaluate the applicability of cooperative checkpointing to large-scale systems through simulation studies considering real workloads, <b>failure</b> <b>logs,</b> and different network topologies. We consider two cooperative checkpointing algorithms: work-based cooperative checkpointing uses a heuristic based on the amount of unsaved work and risk-based cooperative checkpointing leverages failure event prediction. Our results demonstrate that, compared to periodic checkpointing, riskbased checkpointing with event prediction accuracy as low as 10 % is able to significantly improve system utilization and reduce average bounded slowdown by a factor of 9, without losing any additional work to failures. Similarly, work-based checkpointing conferred tremendous performance benefits in the face of large checkpoint overheads. ...|$|R
40|$|The {{software}} {{developed for}} the Fault-Tolerant Multiprocessor (FTMP) is described. The FTMP executive is a timer-interrupt driven dispatcher that schedules iterative tasks which run at 3. 125, 12. 5, and 25 Hz. Major tasks which run under the executive include system configuration control, flight control, and display. The flight control task includes autopilot and autoland functions for a jet transport aircraft. System Displays include status displays of all hardware elements (processors, memories, I/O ports, buses), <b>failure</b> <b>log</b> displays showing transient and hard faults, and an autopilot display. All software is in a higher order language (AED, an ALGOL derivative). The executive is a fully distributed general purpose executive which automatically balances the load among available processor triads. Provisions for graceful performance degradation under processing overload {{are an integral part}} of the scheduling algorithms...|$|R
50|$|Medellín-based West Caribbean Airways {{started as}} a charter service in 1998. It {{specialized}} in flights to San Andrés in the Caribbean, parts of the Colombian mainland and Central America. A {{few months before the}} accident, the airline had been fined $46,000 for lack of pilot training and <b>failure</b> to <b>log</b> required flight data.|$|R
40|$|Introduction The {{experimental}} {{analysis of}} <b>failure</b> <b>log</b> data {{is an important}} part of fault tolerance validation measures and/or evaluation of dependability characteristics is. These experiments can be carried out either during the normal operation of a system or in a combination with fault injections in order to increase the frequency of fault occurrences. In the evaluation of such experiments performed on complex systems crucial difficulties result from the large number of patterns to be recorded in experiments and/or in ob servations,. partially in order to have statistically meaningful estimates from a sufficiently large and representative failure pattern set,. moreover due the typically highly sequential nature of the objects under test (like composite hw/sw systems) a sufficiently long period of system activity must be recorded. Similarly the proper understanding of the error propagation mechanisms necessitates the recording of as much observab...|$|R
40|$|As {{the scale}} of cluster {{computing}} grows, it is becoming hard for long-running applications to complete without facing failures on large-scale clusters. To address this issue, checkpointing/restart is widely used to provide the basic fault-tolerant functionality, yet it suffers from high overhead and its reactive characteriristic. In this work, we propose FT-Pro, an adaptive fault management mechanism that optimally chooses migration, checkpointing or no action to reduce the application execution time {{in the presence of}} failures based on the failure prediction. A cost-based evaluation model is presented for dynamic decision at run-time. Using the actual <b>failure</b> <b>log</b> from a production cluster at NCSA, we demonstrate that even with modest failure prediction accuracy, FT-Pro outperforms the traditional checkpointing/restart strategy by 13 %- 30 % in terms of reducing the application execution time despite failures, which is a significant performance improvement for long-running applications...|$|R
40|$|Mean Time To failure, MTTF, is a {{commonly}} accepted metric for reliability. In this paper {{we present a}} novel approach to achieve the desired MTTF with minimum redundancy. We analyze the failure behavior of large scale systems using <b>failure</b> <b>logs</b> collected by Los Alamos National Laboratory. We analyze {{the root cause of}} failures and present a choice of specific hardware and software components to be made fault-tolerant, through duplication, to achieve target MTTF at minimum expense. Not all components show similar failure behavior in the systems. Our objective, therefore, was to arrive at an ordering of components to be incrementally selected for protection to achieve a target MTTF. We propose a model for MTTF for tolerating failures in a specific component, system-wide, and order components according to the coverage provided. Systems grouped based on hardware configuration showed similar improvements in MTTF when different components in them were targeted for fault-tolerance. ...|$|R
40|$|Research in {{the field}} of <b>failure</b> <b>log</b> {{analysis}} shows that spatial and temporal patterns exist among events contained within the system logs of nodes that comprise large-scale systems. Existing works in this field use clustering mechanisms on log events to represent these patterns and recommend proactive methods to prevent failures in the immediate future. Recent works use discrete-time Semi Markov Models to closely model such events and calculate node reliability. In this research, we use a Hidden Semi Markov Model to predict subsystem failure events leading to a degraded or failure state of a node. As a proactive measure, this method can allow a job scheduler to intelligently assign time and resource consuming jobs to appropriate nodes based on this assessment of reliability of nodes. It will also enable system administrators to be informed of specific subsystem events that are likely to occur in the future. ...|$|R
40|$|Log preprocessing, {{a process}} applied on the raw log be-fore {{applying}} a predictive method, is of paramount impor-tance to failure prediction and diagnosis. While existing fil-tering methods have demonstrated good compression rate, {{they fail to}} preserve important failure patterns that are cru-cial for failure analysis. To address the problem, {{in this paper we}} present a log preprocessing method. It consists of three integrated steps: (1) event categorization to uni-formly classify system events and identify fatal events; (2) event filtering to remove temporal and spatial redundant records, while also preserving necessary failure patterns for failure analysis; (3) causality-related filtering to com-bine correlated events for filtering through apriori associ-ation rule mining. We demonstrate the effectiveness of our preprocessing method by using real <b>failure</b> <b>logs</b> collected from the Cray XT 4 at ORNL and the Blue Gene/L system at SDSC. Experiments show that our method can preserve more failure patterns for failure analysis, thereby improv-ing failure prediction by up to 174 %...|$|R
40|$|Objective: This study {{sought to}} {{determine}} whether infusion device event logs could support accident investigation. Methods: An incident reporting database was searched for infor-mation about log file use in investigations. Log file data from devices in clinical use were downloaded and electronically searched for characteristics (signatures) matching specific function queries. Dif-ferent programming sequences were simulated, and device logs were downloaded for analysis. Results: Database reports mentioned difficulties resolving log file data to the incident report and used log file data to confirm pro-gramming <b>failures.</b> <b>Log</b> file search revealed that, aside from alarm types and times, the devices were unable to adequately satisfy func-tional queries. Different simulated programming scenarios could not be easily differentiated by log file analysis. Conclusions: The device logs we studied collect data that are poorly suited to accident investigation. We conclude that infusion device logs cannot function as black boxes do in aviation accidents. Logs would be better applied to assist routine operations. Key Words: patient safety, infusion devices, incident reporting, data recording, accident investigatio...|$|R
40|$|Abstract. When {{monitoring}} system behavior to check compliance against a given policy, one is sometimes confronted with incomplete knowledge about system events. In IT systems, such incompleteness {{may arise from}} <b>logging</b> infrastructure <b>failures</b> and corrupted <b>log</b> files, or when the logs produced by different system components disagree on whether actions took place. In this paper, we present a policy language with a three-valued semantics that allows one to explicitly reason about incom-plete knowledge and handle disagreements. Furthermore, we present a monitoring algorithm for an expressive fragment of our policy language. We illustrate through examples how our approach extends compliance monitoring to systems with <b>logging</b> <b>failures</b> and disagreements. ...|$|R
40|$|Logging and replay is {{important}} to reproducing software failures and recovering from failures. Replaying a long execution is time consuming, especially when replay is further integrated with runtime techniques that require expensive instrumentation, such as dependence detection. In this paper, we propose a technique to reduce a replay log while retaining its ability to reproduce a <b>failure.</b> While traditional <b>logging</b> records only system calls and signals, our technique leverages the compiler to selectively collect additional information on the fly. Upon a <b>failure,</b> the <b>log</b> can be reduced by analyzing itself. The collection is highly optimized. The additional runtime overhead of our technique, compared to a plain logging tool, is trivial (2. 61 % average) {{and the size of}} additional log is comparable to the original log. Substantial reduction can be costeffectively achieved through a search based algorithm. The reduced log is guaranteed to reproduce the failure...|$|R
40|$|In this paper, {{we present}} an {{efficient}} failure recovery scheme for mobile applications based on movement-based checkpointing and logging. Current approaches take checkpoints periodically {{without regard to}} the mobility rate of the user. Our movementbased checkpointing scheme takes a checkpoint only after a threshold of mobility handoffs has been exceeded. The optimal threshold is governed by the <b>failure</b> rate, <b>log</b> arrival rate, and the mobility rate of the application and the mobile host. This allows the tuning of the checkpointing rate on a per application and per mobile host basis. We identify the optimal movement threshold which will minimize the recovery cost per failure {{as a function of the}} mobile node’s mobility rate, <b>failure</b> rate and <b>log</b> arrival rate. We also calculate the recoverability, i. e., the probability that the recovery can be done by a specified recovery time, and discuss the applicability of the approach...|$|R
40|$|International audienceThis work {{provides}} {{an analysis of}} checkpointing strategies for minimizing expected job execution times in an environ- ment that is subject to processor failures. In the case of both sequential and parallel jobs, we give the optimal solu- tion for exponentially distributed failure inter-arrival times, which, {{to the best of}} our knowledge, is the first rigorous proof that periodic checkpointing is optimal. For non-ex- ponentially distributed failures, we develop a dynamic pro- gramming algorithm to maximize the amount of work com- pleted before the next failure, which provides a good heuris- tic for minimizing the expected execution time. Our work considers various models of job parallelism and of parallel checkpointing overhead. We first perform extensive simula- tion experiments assuming that failures follow Exponential or Weibull distributions, the latter being more representa- tive of real-world systems. The obtained results not only corroborate our theoretical findings, but also show that our dynamic programming algorithm significantly outperforms previously proposed solutions in the case of Weibull fail- ures. We then discuss results from simulation experiments that use <b>failure</b> <b>logs</b> from production clusters. These results confirm that our dynamic programming algorithm signifi- cantly outperforms existing solutions for real-world clusters...|$|R
40|$|Analyzing, {{understanding}} and predicting failure is {{of paramount importance}} to achieve effective fault manage-ment. While various fault prediction methods have been studied in the past, many of them are not practical for use in real systems. In particular, they fail to address two crucial issues: one is to provide location information (i. e., the com-ponents where the failure is expected to occur on) and the other is to provide sufficient lead time (i. e., the time interval preceding the time of failure occurrence). In this paper, we first refine the widely-used metrics for evaluating prediction accuracy by including location as well as lead time. We, then, present a practical failure prediction mechanism for IBM Blue Gene systems. A Genetic Algorithm based method is exploited, which takes into consideration the location and the lead time for failure prediction. We demonstrate the ef-fectiveness of this mechanism by means of real <b>failure</b> <b>logs</b> and job logs collected from the IBM Blue Gene/P system at Argonne National Laboratory. Our experiments show that the presented method can significantly improve fault man-agement (e. g., to reduce service unit loss by up to 52. 4 %) by incorporating location and lead time information in the prediction. ...|$|R
40|$|This work {{provides}} {{an analysis of}} checkpointing strategies for minimizing expected job execution times {{in an environment that}} is subject to processor failures. In the case of both sequential and parallel jobs, we give the optimal solution for exponentially distributed failure inter-arrival times, which, {{to the best of our}} knowledge, is the first rigorous proof that periodic checkpointing is optimal. For non-exponentially distributed failures, we develop a dynamic programming algorithm to maximize the amount of work completed before the next failure, which provides a good heuristic for minimizing the expected execution time. Our work considers various models of job parallelism and of parallel checkpointing overhead. We first perform extensive simulation experiments assuming that failures follow Exponential or Weibull distributions, the latter being more representative of real-world systems. The obtained results not only corroborate our theoretical findings, but also show that our dynamic programming algorithm significantly outperforms previously proposed solutions in the case of Weibull failures. We then discuss results from simulation experiments that use <b>failure</b> <b>logs</b> from production clusters. These results confirm that our dynamic programming algorithm significantly outperforms existing solutions for real-world clusters...|$|R
5000|$|Redo log files, {{recording}} all {{changes to}} the database - used to recover from an instance failure. Often, a database stores these files multiple times for extra security in case of disk <b>failure.</b> Identical redo <b>log</b> files are associated in a [...] "group".|$|R
30|$|Recently, Hashmi et al. [181] use {{different}} unsupervised algorithms, such as k-Means, FCM, Kohonens SOM, Local Outlier Factor, and Local Outlier Probabilities, to detect faults in a broadband service provider network that serves about 1.3 million customers. For this purpose, they analyze a real network <b>failure</b> <b>log</b> (NFL) dataset that contains status of customer complaints, along with network generated alarms affecting a particular region during a certain time. The selected data spans a duration of 12 months and contains about 1 million NFL data points from 5 service {{regions of the}} provider. The collected NFL dataset has 9 attributes, out of which 5 are selected for the analysis: (i) fault occurrence date, (ii) time of the day, (iii) geographical region, (iv) fault cause, and (v) resolution time. At first, k-Means, FCM and Kohonens SOM clustering techniques are applied to cluster the NFL dataset that is completely unlabeled. Afterwards, density-based outlier determination algorithms, such as Local Outlier Factor, and Local Outlier Probabilities, are used on the clustered data to determine the degree of anomalous behavior for every SOM node. The evaluation results show that SOM outperforms k-Means and FCM in terms of error metric. Furthermore, Local Outlier Probabilities algorithm applied on SOM is more reliable in identifying the spatio-temporal patterns linked with high fault resolution times.|$|R
40|$|Abstract. Designing {{algorithms}} {{taking into}} account the hierarchical structure of systems is known to improve dramatically the complexity. We show here, through the analysis of compensation of Web services compositions, that it can also be really efficient at preserving privacy. Systems we consider are hierarchical finite state machines, each com-ponent corresponding to a (composite) service invocation. In the event of a <b>failure,</b> <b>logs</b> are used to recover the sequence of actions that was executed: each action is compensated, in reverse order of occurrence, leading to a stable state. However, for heterogeneous services, logging all the actions is often impracticable due to security issues and expensive in terms of both time and space. We thus design two algorithms. The first one computes a set of actions to log ensuring that the sequence of executed actions can be uniquely determined. This set should be as small as possible. The second algorithm shows how to perform compensation based on this log, {{in the event of a}} failure. Furthermore, the algorithms preserve privacy and handle heterogeneity by exchanging only a very limited amount of information between different components: logs are recorded and kept local to each component, and a black-box view of the implementation details of component services is sufficient. ...|$|R
40|$|Abstract—Core {{backbone}} networks must {{be designed}} to guarantee high levels of availability. Any interruption in the services that they provide may have massive consequences. For this reason {{there is a huge}} interest in developing methods able to keep the network robustness in the desired level. For the design of these methods are used models that need input information such as the operational state of network components which are stochastic variables. The aim {{of this paper is to}} provide an insight into the core networks behavior based on real operational data in order to help future related works to take more realistic assumptions. Based on <b>failure</b> <b>logs</b> provided by UNINETT we analyze availability levels and failure intensities in routers and links. We show that links may be classified in three groups with different properties. Additionally we observe that some links have similar dependability features than routers, making the perfect node assumption used on many related studies not correct. Finally, there were used parametrization techniques in order to fit the empirical processes with well-known distributions. We observe that the Weibull assumption that is traditionally used to model link failures processes fits properly the behavior of routers and short distance links but for the case of long distance fibers the gamma distribution seems to fit better. I...|$|R
40|$|The continual {{accumulation}} of power grid <b>failure</b> <b>logs</b> provides a valuable but rarely used source for data mining. Sequential analysis, aiming at exploiting the temporal evolution and exploring the future trend in power grid failures, is an increasingly promising alternative for predictive scheduling and decision-making. In this paper, a temporal Latent Dirichlet Allocation (TLDA) framework is proposed to proactively reduce the cardinality {{of the event}} categories and estimate the future failure distributions by automatically uncovering the hidden patterns. The aim was to model the failure sequence as a mixture of several failure patterns, each of which was characterized by an infinite mixture of failures with certain probabilities. This state space dependency was captured by a hierarchical Bayesian framework. The model was temporally extended by establishing the long-term dependency with new co-occurrence patterns. Evaluation of the high voltage circuit breakers (HVCBs) demonstrated that the TLDA model had higher fidelities of 51. 13 %, 73. 86 %, and 92. 93 % in the Top- 1, Top- 5, and Top- 10 failure prediction tasks over the baselines, respectively. In addition to the quantitative results, we showed that the TLDA can be successfully used for extracting the time-varying failure patterns and capture the failure association with a cluster coalition method...|$|R
40|$|Recent {{work have}} used both <b>failure</b> <b>logs</b> and {{resource}} use data separately (and together) to detect system failure-inducing errors and to diagnose system failures. System failure occurs {{as a result}} of error propagation and the (unsuccessful) execution of error recovery mechanisms. Knowledge of error propagation patterns and unsuccessful error recovery is important for more accurate and detailed failure diagnosis, and knowledge of recovery protocols deployment is important for improving system reliability. This paper presents the CORRMEXT framework which carries failure diagnosis another significant step forward by analyzing and reporting error propagation patterns and degrees of success and failure of error recovery protocols. CORRMEXT uses both error messages and resource use data in its analyses. Application of CORRMEXT to data from the Ranger supercomputer have produced new insights. CORRMEXT has: (i) identified correlations between resource use counters that capture recovery attempts after an error, (ii) identified correlations between error events to capture error propagation patterns within the system, (iii) identified error propagation and recovery paths during system execution to explain system behaviour, (iv) showed that the earliest times of change in system behaviour can only be identified by analyzing both the correlated resource use counters and correlated errors. CORRMEXT will be installed on the HPC clusters at the Texas Advanced Computing Center in Autumn 2017...|$|R
50|$|IBM Tivoli Storage Productivity Center is {{designed}} to provide the foundation for storage service level management by offering storage system and SAN performance and availability management. This includes connectivity reporting between file systems and physical disk as well as SAN and disk subsystem <b>failure</b> and audit <b>logging.</b>|$|R
50|$|Modern {{actuators}} {{have extensive}} diagnostic functions {{which can help}} identify {{the cause of a}} <b>failure.</b> They also <b>log</b> the operating data. Study of the logged data allows the operation to be optimised by changing the parameters and the wear of both actuator and valve to be reduced.|$|R
5000|$|The other {{element of}} a queue manager is the log, the {{mechanism}} used to create the robustness. As a message is placed on a queue or a configuration change is made, the data is also logged. In {{the event of a}} <b>failure,</b> the <b>log</b> is used to recreate damaged objects and recreate messages. Only [...] "persistent" [...] messages will be recreated when a failure occurs—"non-persistent" [...] messages are lost. Non-persistent messages can be sent across a channel set to a fast mode, in which delivery is not assured {{in the event of a}} channel failure.|$|R
40|$|Movement-based {{checkpointing}} and <b>logging</b> for <b>failure</b> {{recovery of}} database applications in mobile environments Sapna E. George · Ing-Ray Chen © Springer Science+Business Media, LLC 2008 Abstract In this paper, we present an efficient failure recovery scheme for mobile database applications based on movement-based checkpointing and logging. Current approaches take checkpoints periodically {{without regard to}} the mobility behavior of mobile users. Our movement-based checkpointing scheme takes a checkpoint only after a threshold of mobility handoffs has been exceeded. The optimal threshold is governed by the <b>failure</b> rate, <b>log</b> arrival rate, and the mobility rate of the mobile host. This allows the tuning of the checkpointing rate on a per-user basis. We identify the optimal movement threshold which will minimize the recovery cost per failure {{as a function of the}} mobile node’s mobility rate, <b>failure</b> rate and <b>log</b> arrival rate. We derive the mobile database application recoverability, i. e., the probability that the recovery can be done by a specified recovery time deadline. Numeric data are presented to demonstrate the feasibility of our approach with its applicability given...|$|R
40|$|Abstract — Various {{mechanisms}} for fault-tolerance (FT) are used today {{in order to}} reduce the impact of failures on application execution. In the case of system failure, standard FT mechanisms are checkpoint/restart (for reactive FT) and migration (for pro-active FT). However, each of these mechanisms create an overhead on application execution, overhead that for instance becomes critical on large-scale systems where previous studies have shown that applications may spend more time checkpointing state than performing useful work. In order to decrease this overhead, researchers try to both optimize existing FT mechanisms and implement new FT policies. For instance, combining reactive and pro-active approaches in order to decrease the number of checkpoints that must be performed during the application’s execution. However, currently no solutions exist which enable the evaluation of these FT approaches through simulation, instead experimentations must be done using real platforms. This increases complexity and limits experimentation into alternate solutions. This paper presents a simulation framework that evaluates different FT mechanisms and policies. The framework uses system <b>failure</b> <b>logs</b> for the simulation with a default behavior based on logs taken from the ASCI White at Lawrence Livermore National Laboratory. We evaluate the accuracy of our simulator comparing simulated results with those taken from experiments done on a 32 -node compute cluster. Therefore such a simulator can be used to develop new FT policies and/or to tune existing policies. I...|$|R
40|$|Researchers, {{students}} and practitioners often encounter a situation when the build process of a third-party software system fails. In this paper, {{we aim to}} confirm this observation present mainly as anecdotal evidence so far. Using a virtual environment simulating a programmer's one, we try to fully automatically build target archives from the source code of over 7, 200 open source Java projects. We {{found that more than}} 38 % of builds ended in <b>failure.</b> Build <b>log</b> analysis reveals the largest portion of errors are dependency-related. We also conduct an association study of factors affecting build success...|$|R
50|$|In {{the field}} of {{databases}} in computer science, a transaction log (also transaction journal, database log, binary log or audit trail) {{is a history of}} actions executed by a database management system used to guarantee ACID properties over crashes or hardware <b>failures.</b> Physically, a <b>log</b> is a file listing changes to the database, stored in a stable storage format.|$|R
40|$|Abstract — Frequent failure occurrences are {{becoming}} a serious concern to the community of high-end computing, especially when the applications and the underlying systems rapidly grow in size and complexity. In order {{to better understand the}} failure behavior of such systems and further develop effective faulttolerant strategies, we have collected detailed event logs from IBM Blue Gene/L, which has as many as 128 K processors, and is currently the fastest supercomputer in the world. Due to the scale of such machines and the granularity of the logging mechanisms, the logs can get voluminous and usually contain records which may not all be distinct. Consequently, it is crucial to filter these logs towards isolating the specific failures, which can then be useful for subsequent analysis. However, existing filtering methods either require too much domain expertise, or produce erroneous results. This paper thus fills this crucial void by designing and developing an Adaptive Semantic Filtering (ASF) method, which is accurate, light-weight, and more importantly, easy to automate. Specifically, ASF exploits the semantic correlation between two events, and dynamically adapts the correlation threshold based on the temporal gap between the events. We have validated the ASF method using the <b>failure</b> <b>logs</b> collected from Blue Gene/L over a period of 98 days. Our experimental results show that ASF can effectively remove redundant entries in the logs, and the filtering results can serve as a good base for future failure analysis studies. I...|$|R
40|$|High {{performance}} computing applications must be resilient to faults, which are common occurrences especially in post-petascale settings. The traditional fault-tolerance solution is checkpoint-recovery, {{by which the}} application saves its state to secondary storage throughout execution and recovers from the latest saved state {{in case of a}} failure. An oft studied research question is that of the optimal checkpointing strategy: when should state be saved? Unfortunately, even using an optimal checkpointing strategy, the checkpointing frequency must increase as platform scale increases, leading to higher checkpointing overhead. This overhead precludes high parallel efficiency for large-scale platforms, thus mandating other more scalable fault-tolerance mechanisms. One such mechanism is replication, which can be used in addition to checkpoint-recovery. Using replication, multiple processors perform the same computation so that a processor failure does not necessarily imply application failure. While at first glance replication may seem wasteful, it may be significantly more efficient than using solely checkpointrecovery at large scale. In this work we investigate a simple approach where entire application instances are replicated. We provide a theoretical study of checkpoint-recovery with replication in terms of expected application execution time, under an exponential distribution of failures. We design dynamic-programming based algorithms to define checkpointing dates that work under any failure distribution. We also conduct simulation experiments assuming that failures follow Exponential or Weibull distributions, the latter being more representative of real-world systems, and using <b>failure</b> <b>logs</b> from production clusters. Our results show that replication is useful in a variety of realistic application and checkpointing cost scenarios for future exascale platforms. ...|$|R
