26|8|Public
40|$|First-lactation milk {{records of}} artificially sired Holstein cows in New York were {{analyzed}} by a sire-by-herd varianee com-ponent analysis separately for each year from 1954 to 1962. The two variables analyzed were 305 -day, 2 ×, mature quiva-lent records and the mature equivalent records expressed as deviations from the average of their herd-mates. The total variance steadily increased with change in time. Most of the increase (r =. 97) was accounted for by increase in nlean produc-tion. The relative inerease in the sire com-ponent of variance was slightly <b>faster</b> <b>tha...</b>|$|E
40|$|Computational ghost imaging {{needs to}} acquire {{a large number of}} {{correlated}} measurements between reference patterns and the scene for reconstruction, so extremely high acquisition speed is crucial for fast ghost imaging. With the development of technologies, high frequency illumination and detectors are both available, but their synchronization needs technique demanding customization and lacks flexibility for different setup configura-tions. This letter proposes a self-synchronization scheme that can eliminate this difficulty by introducing a high precision synchronization technique and corresponding algorithm. We physically implement the proposed scheme using a 20 kHz spatial light modulator to generate random binary patterns together with a 100 times faster photodiode for high speed ghost imaging, and the acquisition frequency is around 14 times <b>faster</b> <b>tha...</b>|$|E
40|$|Collective {{classification}} {{has been}} widely studied to predict class labels simultaneously for relational data, such as hyperlinked webpages, social networks, and data in a relational database. Recently there have been studies on relational models for collective inference, such as relational dependency networks [1], relational Markov networks [2], and Markov logic networks [3]. The existing collective classification methods are usually expensive due to the iterative inference in graphical models and their learning procedures based on iterative optimization. When the dataset is large, the cost of maintaining large graphs or related instances in memory becomes a problem as well. Stacked graphical learning (SGL) has been proposed for collective classification with efficient inference- as shown in [4], stacked graphical learning is 40 to 80 times <b>faster</b> <b>tha...</b>|$|E
40|$|Many seabirds {{including}} penguins are {{adapted to}} {{long periods of}} fasting, particularly during parts of the reproductive cycle and during moult. However, the influence of fasting on the gastrointestinal (GI) microbiota has not been investigated in seabirds. Therefore, the present study aimed to examine the microbial composition and diversity of the GI microbiota of fasting little (Eudyptula minor) and king penguins (Aptenodytes patagonicus) penguins during early and late moult. The {{results from this study}} indicated that there was little change in the abundance of the major phyla during moult, except for {{a significant increase in the}} level of Proteobacteria in king penguins. In king penguins the abundance of Fusobacteria increases from 1. 73 % during early moult to 33. 6 % by late moult, whilst the abundance of Proteobacteria (35. 7 % to 17. 2 %) and Bacteroidetes (19. 5 % to 11 %) decrease from early to late moult. In little penguins, a decrease in the abundances of Firmicutes (44 % to 29 %) and an increase in the abundance of Bacteroidetes (11 % to 20 %) were observed from early to late moult respectively. The results from this study indicate that the microbial composition of both king and little penguins alters during fasting. However, it appears that the microbial composition of king penguins is more affected by <b>fasting</b> <b>tha...</b>|$|R
40|$|We {{present a}} novel method that {{constructs}} and navigates {{a network of}} local minima of potential fields defined over multi-dimensional spaces. Though motivated by problems of motion planning for robotic manipulators, similar techniques have been proposed for use in other domains such as molecular chemistry and drug design. The method is based on building a roadmap of paths connecting local minima of a potential function. The novel approach consists of an up-hill search strategy used {{to climb out of}} local minima and find new nearby local minima, without doubling back on previous local minima. With this up-hill search strategy, one can find local minima otherwise difficult to encounter, and one can focus the search to specific local minima and specific directions from those local minima. The construction of the roadmap can be done in parallel with very little communication. We present extensive simulation results. 1 Introduction and Background We present a novel <b>fast</b> method <b>tha</b> [...] ...|$|R
40|$|Background:	 The	aim	of	the	study	was	to	evaluate	the	comparative	effects	of	propofol	infusion	versus	sevoflurane	for {{maintenance}} of	anesthesia	with	respect	to	hemodynamics,	 recovery	characteristics,	 nausea	and	vomiting	in	patients undergoing percutaneous	nephrolithotomy. Methods:	 Forty	American	Society	of	Anesthesiologists	physical	status	I-II	patients,	 aged	between	 22 	and	 65 	years were randomly	divided	to	receive	either	intravenous	anesthesia	with	propofol	(group	P) 	 or	sevoflurane	(group	S). Cardiovascular variables,	 peripheral	oxygen	saturation	(SpO 2),	 end-tidal	carbon	dioxide	(ETCO 2),	 bispectral	index (BIS) 	 and	train-of-four	(TOF) 	 values	were	recorded	at	intervals	throughout	the	procedure. Time	to	spontaneous respiration,	 eye	opening,	 extubation,	 obey	commands,	 hand	squeezing,	 Aldrete	Score>	 9 	and	the	incidence	of postoperative nausea	and	vomiting	were	recorded. Results:	 Early	recovery	times	[spontaneous	respiration	(P	 =	 0. 002),	 eye	opening	(P	 =	 0. 006),	 extubation	(P	 =	 0. 013), obey commands	(P		 0. 05). Conclusions:	 The	present	study	which	adjusted	sevoflurane	concentration	and	propofol	infusion	rate	according to BIS	values	revealed	that	maintenance	of	anesthesia	with	sevoflurane	is	associated	with	<b>faster</b>	recovery	<b>tha...</b>|$|R
40|$|The Sparse Fast Fourier Transform is {{a recent}} {{algorithm}} developed by Hassanieh et al. at MIT for Discrete Fourier Transforms on signals with a sparse frequency domain. A reference implementation of the algorithm exists and proves that the Sparse Fast Fourier Transform can be faster than modern FFT libraries. However, the reference implementation does {{not take advantage of}} modern hardware features like vector instruction sets or multithreading. In this Master Thesis the reference implementation’s performance will be analyzed and evaluated. Several optimizations are proposed and im-plemented in a high-performance Sparse Fast Fourier Transform library. The optimized code is evaluated for performance and compared to the reference implementation as well as the FFTW library. The main result is that, depending on the input parameters, the opti-mized Sparse Fast Fourier Transform library is two to five times <b>faster</b> <b>tha...</b>|$|E
40|$|Streamlining {{communication}} {{is key to}} achieving good performance in shared-memory parallel programs. While full hardware support for cache coherence generally offers the best performance, not all parallel machines provide it. Instead, software layers using Shared Virtual Memory (SVM) can be built to enforce coherence at a higher level. In prior work, researchers have studied application-specific cache coherence protocols implemented either in SVM systems or as handlers run by programmable protocol processors. Since the protocols are specialized {{to the needs of}} a single application, they can be particularly helpful in reducing the long latencies and processing overhead that sometimes degrade performance in SVM systems. This paper studies implementing application-specific protocols in hardware, but not via an instruction-based protocol processor as is typical. Instead, we consider configurable implementations based on Field-Programmable Gate Arrays (FPGAs). This approach can be <b>faster</b> <b>tha</b> [...] ...|$|E
40|$|Carbamazepine and phenytoin, two of {{the most}} {{commonly}} prescribed antiepileptic drugs, have been proposed to share a similar mechanism of action by use-dependent inhibition of Na 1 channels. The proposed similar mechanism of action, however, cannot explain the common clinical experiences that the two drugs are different; in some patients, one drug may be more effective than the other. This may occur even when op-timal therapeutic concentrations are reached with both medi-cations in plasma or the cerebrospinal fluid. In this study, we show that the action of the two drugs on Na 1 channels are quantitatively very different. The affinity between inactivated Na 1 channels and carbamazepine (apparent dissociation con-stant; 25 mM) is; 3 times lower than that of phenytoin, yet the binding rate constant of carbamazepine onto the inactivated Na 1 channels is; 38, 000 M 21 /sec 21, or; 5 times <b>faster</b> <b>tha...</b>|$|E
40|$|Millions of slide presentations {{are being}} {{authored}} and delivered with computer software every day. Yet {{much of the}} computer’s power for these tasks remains untapped. Existing interaction techniques leave presenters wrestling with limited size computer displays to get meaningful overviews of their work. Without these overviews, they have trouble finding patterns in their data and experimenting with alternate organizations. They also have difficulty communicating the structure of large or complex talks to the audience and keeping the audience oriented during unexpected transitions between ideas. A natural solution is Zoomable User Interfaces (ZUIs) since they offer the capability to view information at multiple levels of detail and smoothly transition between ideas. This work presents two ZUIs, Niagara and CounterPoint, for authoring and delivering slide presentations. Niagara is a ZUI workspace for authoring presentation content with techniques to improve authoring in the zoomable environment. Empirical evaluations of ZUI-based authoring tools revealed performance improvements and subjective preferences over folder-based interfaces for organization tasks. Users were 30 % <b>faster</b> with ZUIs <b>tha...</b>|$|R
40|$|Everything in a {{paper mill}} {{operates}} on a gargantuan scale, including the hazards, which can include everything from falls to in-running nip points to heat stress. While heat stress would not be {{considered one of the}} major hazards, it can still put some employees at a high risk of developing heat stress-related injuries. To combat the threat posed by heat stress, a variety of different engineering and administrative controls such as work restrictions, ventilation changes and vapor permeable clothing have been developed. Two separate tests were conducted to study the effects of clothing and ventilation controls on worker heat strain. In the first test, five subjects wore a shirt containing vapor permeable material followed by a cotton shirt while conducting normal work duties. The vapor permeable material was manufactured by Nike, Inc. and marketed under the trade name Dri-FIT. A Questemp II personal heat stress monitor was used to determine body core temperatures and a subjective evaluation of the two shirts was conducted. Results indicate the vapor permeable shirt reduced core temperatures by about 0. 3 - 0. 5 ºC compared to the cotton shirt. In addition, the vapor 3 permeable shirt was rated to be more comfortable and to have a <b>faster</b> drying time <b>tha...</b>|$|R
40|$|The present {{microdialysis}} {{study has}} examined whether exercise-elicited increases in brain tryptophan availability {{and in turn}} 5 -HT. synthesis alter 5 -HT release in the hippocampus of food-deprived rats. To this end, we compared the respective effects of acute exercise,. administration of tryptophan, and the combination of both treatments, upon extracellular 5 -HT and 5 -hydroxyindoleacetic acid 5 -HIAA levels. All rats were trained to run on a treadmill before implantation of the microdialysis probe and 24 h of food deprivation. Acute.. exercise 12 mrmin for 1 h increased in a time-dependent manner extracellular 5 -HT levels maximal increase: 47 %, these levels returning to their baseline levels within the first hour of the recovery period. Besides, exercise-induced increases in extracellular 5 -HIAA. levels did not reach significance. Acute administration of a tryptophan dose 50 mgrkg i. p. that increased extracellular 5 -HIAA but not.. 5 -HT levels in fed rats, increased within 60 min extracellular 5 -HT levels maximal increase: 55 % in food-deprived rats. Whereas 5 -HT levels returned toward their baseline levels within the 160 min that followed tryptophan administration, extracellular 5 -HIAA levels rose.. throughout the experiment maximal increase: 75 %. Lastly, treatment with tryptophan 60 min beforehand before acute exercise led to. marked increases in extracellular 5 -HT and 5 -HIAA levels maximal increases: 100 % and 83 %, respectively throughout the 240 min that followed tryptophan administration. This study indicates that exercise stimulates 5 -HT release in the hippocampus of <b>fasted</b> rats, and <b>tha...</b>|$|R
40|$|TON. Kinetics of CO uptake and {{diffusing}} {{capacity in}} transition from rest to steady-state exercise. J. Appl. Physiol. 72 (5) : 1764 - 1772, 1992. -In {{the transition from}} rest to steady-state exer-cise, 0, uptake from the lungs (VO,) depends on the product of pulmonary blood flow and pulmonary arteriovenous 0, content difference. The kinetics of pulmonary blood flow {{are believed to be}} somewhat faster than changes in pulmonary arteriove-nous 0, content difference. We hypothesized that during CO breathing, the kinetics of CO uptake (VCO) and diffusing capac-ity for CO (DL & should be faster than VO, because changes in pulmonary arteriovenous CO content difference should be rela-tively small. Six subjects went abruptly from rest to constant exercise (inspired CO fraction = 0. 0005) at 40, 60, and 80 % of their peak VO,,. measured with an incremental test wo 2 peak). At all exercise levels, DL~ ~ and VCO rose <b>faster</b> <b>tha...</b>|$|E
40|$|Innovation in the {{knowledge}} economy differs sharply from the industrial age. Inventions now primarily yield intellectual property. IP requires major investments and often, {{the result can be}} reproduced quickly at negligible cost. To make invention economy viable, patents, trademarks, and copyright have become the protectors of innovators and inventors. Typically, IP entrepreneurs are well educated, accustomed to working in small teams, and were introduced to information and computer technology (ICT) at an early age; sometimes even before going to school. They are accustomed to using technology and many have the skills to improve it. They are motivated to improve technology and its use. Two well-known examples are Bill Gates and colleagues who developed the software that later led to Microsoft Corporation and Mark Zuckerberg and colleagues who developed the Facebook social-networking software and founded Facebook Inc. All were Harvard College undergraduates when their ideas were first developed as improvements to Harvard’s processes. Software entrepreneurs such as these are growing the global economy <b>faster</b> <b>tha...</b>|$|E
40|$|Abstract. The FSB (fast syndrome-based) hash {{function}} was {{submitted to the}} SHA- 3 competition by Augot, Finiasz, Gaborit, Manuel, and Sendrier in 2008, after preliminary designs proposed in 2003, 2005, and 2007. Many FSB parameter choices were broken by Coron and Joux in 2004, Saarinen in 2007, and Fouque and Leurent in 2008, but the basic FSB idea appears to be secure, and the FSB submission remains unbroken. On the other hand, the FSB submission is also quite slow, and was not selected for {{the second round of}} the competition. This paper introduces RFSB, an enhancement to FSB. In particular, this paper introduces the RFSB- 509 compression function, RFSB with a particular set of parameters. RFSB- 509, like the FSB- 256 compression function, is designed to be used inside a 256 -bit collision-resistant {{hash function}}: all known attack strategies cost more than 2 128 to find collisions in RFSB- 509. However, RFSB- 509 is an order of magnitude <b>faster</b> <b>tha...</b>|$|E
40|$|The {{fact that}} the motion of solvent {{molecules}} defines the reaction coordinate for electron-transfer and other chemical reactions has generated great interest in solvation dynamics, the study of how the solvent responds to changes in a solute’s electronic state. In the limit of linear response (LR), when the perturbation caused by the solute is “small”, the relaxation of the excited solute’s energy gap should behave identically to the relaxation dynamics of the unperturbed solute following a natural fluctuation of the gap away from equilibrium. Despite the {{fact that the}} addition of a fundamental unit of charge to a small solute results in a solvation energy that is tens or hundreds of kT, computer simulations of solvation dynamics have found, with only a few exceptions, that LR is obeyed for changes in solute charge. Essentially none of this work, however, accounts for {{the fact that the}} solutes in real chemical reactions undergo changes in size and shape as well as in charge distribution. In this paper, we compare the results of molecular simulations of polar and nonpolar solvation dynamics for a simple Lennard-Jones solute in a flexible-water solution to explore the validity of LR. We find that, when short-range forces are involved, LR breaks down dramatically: both the inertial and diffusive components of the relaxation differ from those predicted by LR. For increases in solute size, expansion of the solute drives the first-shell solvent molecules into the second shell. The resulting nonequilibrium relaxation takes advantage of translation-rotation coupling that does not occur at equilibrium, resulting in <b>faster</b> solvation than <b>tha...</b>|$|R
40|$|As the {{complexity}} {{and the size of}} systems on a chip (SOCs) increase, new methodologies are rapidly evolving to support design of larger networks on a chip (NOCs). Power consumption is one of the primary issues in design of such systems. In this work we present a new methodology for managing power consumption of NOCs. Power management problem is formulated for the first time using closed-loop control concepts. We introduce an estimator and a controller that implement our power management methodology. The estimator is capable of very fast and accurate tracking of changes in the system parameters. Parameters estimated are used to form the system model. Our system model combines node and network centric power management decisions. Node centric power management assumes no a priori knowledge of requests coming in from outside the core. Thus it implements a more traditional dynamic voltage scaling and power management control algorithms. Dynamic voltage scaling (DVS) algorithms reduce energy consumption by changing processor speed and voltage at run-time depending on the needs of the running tasks. Dynamic power management (DPM) algorithms trade off the performance for the power consumption by selectively placing components into low-power states. In addition to node centric power management, our methodology also supports network-centric power management by utilizing interaction with the other system cores regarding the power and the QoS needs. A core can be informed ahead of time when the other cores in the system might need its services (thus we can mask the performance penalty incurred due to wakeup time needed once a request arrives). In addition, a core can be told that its service is no longer needed (we can transition to a low-power state without waiting for a timeout in the idle state). Node and network centric power management are combined in one system model that is used to formulate the power management optimization problem. The model is based on Renewal theory, and thus guarantees globally optimal results. We introduce a <b>fast</b> optimization method <b>tha...</b>|$|R
40|$|In {{molecular}} biology, {{the central}} dogma {{has been the}} most important framework to explain major processes to maintain life. The first step is transcription in which the genetic information of a very tiny section of DNA is transferred to a piece of mRNA. During the process, several proteins are involved, like RNA polymerase and transcription factors (TFs). Especially, the binding of a TF to the right segment of DNA is critical, and has been studied a lot since early seventies. 2 For instance, a TF, called a lac repressor, makes gene expression regulate by site-specific binding to DNA, called a lac operon, in a living E. coli cell. The process is to search a rare event because a FT should find a right specific binding site among millions of sites on DNA. Earlier, it was reported that searching time in an experi-ment in vitro was around two orders of magnitude <b>faster</b> <b>tha...</b>|$|E
40|$|The {{programs}} and activities described herein were {{supported in part}} by funds provided by the United States Department of the Interior as authorized under the Water Resources Act of 1964, Public Law 88 - 379. Monthly rainfall data of several stations in Kalihi Basin 3 Manoa Basin 3 and Kaneohe Area 3 all on Oahu 3 and the central sloping area of Molokai were correlated to watershed parameters of the areas. Distance measured from the station to a common station located seaward trom all stations has proven {{to be the most important}} of the three parameters studied 3 the other two being the exposure and the elevation of the area. Both linear and nonlinear regression functions were developed. The central tendency of the monthly rainfall for the high rainfall part of the southeastern part of the Island of Oahu was found to require approximately forty years of record to stabilize. The analysis also shows that mean converges to a specified level generally <b>faster</b> <b>tha...</b>|$|E
40|$|The Delay/Disruption-Tolerant Networking Architecture {{calls for}} new design prin-ciples that will govern data {{transmission}} and retransmission scheduling over chal-lenged environments. In that context, novel routing, transport and application layer algorithms {{have to be}} established {{in order to achieve}} efficient and reliable commu-nication between DTN-nodes. In this study, we focus on the evolution of the terrestrial Internet into the In-terplanetary or Space Internet and propose adoption of the Deep-Space Transport Protocol (DS-TP) as the transport layer scheme of choice for the space networking protocol stack. We present DS-TP’s basic design principles and we evaluate its per-formance both theoretically and experimentally. We verify that practice conforms with theory and observe great performance boost, in terms of file delivery time between DTN-nodes, in case of DS-TP. In particular, the gain of DS-TP against conventional proposals for deep-space communications increases with the link er-ror rate; under conditions DS-TP can improve the performance of the transport layer protocol by a factor of two (i. e., DS-TP can become two times <b>faster</b> <b>tha...</b>|$|E
40|$|High {{impulsive}} {{individuals have}} problems with self - monitoring and learning from their mistakes. The {{aim of this study}} was to investigate whether error processing is impaired in high trait impulsivity, and how it is modulated by the task difficulty. Adults were classified as high (n = 10) and low (n = 10) impulsive participants based on the Barratt Impulsiveness Scale, and they participated in a modified flanker task. The flanker trials had three levels of task difficulty manipulated by visual degradation of the stimuli. We measured RTs and ERP components (Ne, Pe) related to erroneous responses. L ow impulsive participants responded significantly <b>faster</b> <b>tha</b> n high impulsive participants. The two groups did not differ in accuracy. The Ne amplitude was smaller in high than in low impulsivity in case of medium and high difficulty levels, but not at low difficulty level. However, the groups did not differ either in the amplitude or in the latency of Pe. We suggest that trait impulsivity is characterized by impaired error detection...|$|E
40|$|The {{evolutionary}} {{origins of}} speech remain obscure. Recently, it was proposed that speech derived from monkey facial signals which exhibit a speech-like rhythm of, 5 open-close lip cycles per second. In monkeys, these signals {{may also be}} vocalized, offering a plausible evolutionary stepping stone towards speech. Three essential predictions remain, however, to be tested to assess this hypothesis’ validity; (i) Great apes, our closest relatives, should likewise produce 5 Hz-rhythm signals, (ii) speech-like rhythm should involve calls articulatorily similar to consonants and vowels given that speech rhythm is the direct product of stringing together these two basic elements, and (iii) speech-like rhythm should be experience-based. Via cinematic analyses we demonstrate that an ex-entertainment orangutan produces two calls at a speech-like rhythm, coined ‘‘clicks’ ’ and ‘‘faux-speech. ’ ’ Like voiceless consonants, clicks required no vocal fold action, but did involve independent manoeuvring over lips and tongue. In parallel to vowels, faux-speech showed harmonic and formant modulations, implying vocal fold and supralaryngeal action. This rhythm was several times <b>faster</b> <b>tha...</b>|$|E
40|$|This paper {{contains}} a few {{comments on the}} physical analysis of the factors controlling the rate of cooling, and {{the relation of the}} rate of cooling to ice crystal formation which has been carried out in connection with the electron microscopic work described by Dr. C-ersh at this conference. Of the many factors which influence the rate of cooling, by far the most important is the size of the sample. The necessity for cutting small pieces to minimize ice crystal artefact has been realized since the introduction of freezing as a method of histological fixation (1, 2), but until we examined the problem mathematically before starting to cut these extremely small pieces, we did not appreciate just how small was small. The essential point brought olat by computation was that if anything approaching ideal cooling (that is, cooling in which the surface of the sample is maintained at the temperature of the body of the coolant) could be achieved, the cooling rate would be inversely proportional to the square of the least linear dimension. This is illustrated in Fig. 1. Thus a cube 0. 1 ram. on edge might be expected to cool 100 times <b>faster</b> <b>tha...</b>|$|E
40|$|The study {{presents}} {{an analysis of}} the relationship between living in female-headed households and educational attainment among White and Black respondents. A statistically signiJicant relationship is found for the White respondents. For the Black respondents, the relationship is not statistically signijicant. The empirical findings cast doubt on the ‘>pathology of matriarchy I’ hypothesis that the breakdown of the Black family has placed Black children at a distinct disadvantage in obtaining education and social well-being. The results point instead to the importance of the social and economic resources within the family. The proposition that growing up in a female-headed household is more devastating for males than for females in terms of educational attainment is not supported. The data used for this stu @ came fiom the General Social Survey (GSS) of the National Opinion Research Center (NORC). The analysis was conductedfiom the I 970 s, I 980 s, I 990 s, and the year 2000 data. American families are changing in profound ways. The most dramatic are the changes in family structure (Ellwood, 1993). Over the past two decades, the number of female-headed households has grown by 45 percent, much <b>faster</b> <b>tha...</b>|$|E
40|$|Wireless sensor {{networks}} {{have the potential}} to become significant subsystems of engineering applications. Before relegating important and safety-critical tasks to such subsystems, it is necessary to understand the dynamic behavior of these subsystems in simulation environments. There is an urgent need to develop simulation platforms that are useful to explore both the networking issues and the distributed computing aspects of wireless sensor networks. Current efforts to simulating wireless sensor networks largely focus on the networking issues. These approaches use well-known network simulation tools that are difficult to extend to explore distributed computing issues. Discrete-event simulation is a trusted platform for modeling and simulating a variety of systems. We report results obtained from a new simulator for wireless sensor networks networks that is based on the discrete event simulation framework called OMNeT++. Work is underway to develop a simulation platform that allows developers and researchers to investigate topological, phenomenological, networking, robustness and scaling issues related to wireless sensor networks. As a first step, we have developed simulations for the 802. 11 MAC and the well-known sensor network protocol called Directed Diffusion. We demonstrate the performance of our simulator by comparing its performance to that of the well-known simulator ns 2. Our results indicate that our simulator executes at least an order of magnitude <b>faster</b> <b>tha...</b>|$|E
40|$|We {{consider}} coherent tunneling of one-dimensional model {{systems in}} non-cyclic or cyclic symmetric double well potentials. Generic potentials are constructed which allow for analytical {{estimates of the}} quantum dynamics in the non-relativistic deep tunneling regime, {{in terms of the}} tunneling distance, barrier height and mass (or moment of inertia). For cyclic systems, the results may be scaled to agree well with periodic potentials for which semi-analytical results in terms of Mathieu functions exist. Starting from a wavepacket which is initially localized in one of the potential wells, the subsequent periodic tunneling is as-sociated with tunneling velocities. These velocities (or angular velocities) are evaluated as the ratio of the flux densities versus the probability densities. The maximum velocities are found under the top of the barrier where they scale as the square root of the ratio of barrier height and mass (or moment of inertia), independent of the tunneling distance. They are applied exemplarily to several prototypical molecular models of non-cyclic and cyclic tunneling, including am-monia inversion, Cope rearrangement of semibullvalene, torsions of molecular fragments, and rotational tunneling in strong laser fields. Typical maximum velocities and angular velocities are in the order of a few km/s and from 10 to 100 THz for our non-cyclic and cyclic systems, respectively, much <b>faster</b> <b>tha...</b>|$|E
40|$|Abstract approved: Parameters of herd {{dynamics}} and production performances were analyzed from data collected using a long-term survey of village cattle herds in the agro-pastoral production system of Kolda (Southern Senegal) conducted from 1987 to 1995. Monthly distribution of births averaged over {{the study period}} shows that peak conceptions occurred during the months of October and February (73 % of total conceptions). This period corresponded also to that of maximum body weight of cows. Cow reproductive performance was poor, as heifers produced their first calves at a relatively late age (1703 days; C. V. = 13. 1 %) and tended to calve on alternate years. Average estimate of intervals between successive calving was 690 days (C. V. = 32 %) and was significantly reduced by the dam's age and experience (i. e. parity number) and by a reduction {{of the length of}} the suckling/milk extraction period which resulted from calf loss. Average weights (± s. e.) of calves at birth, 6 and 12 month old were respectively 16. 8 kg (± 0. 4), 49. 3 kg (± 0. 9) and 78. 9 kg (± 2. 2). Cumulative growth rate of calves (± s. e.) was 0. 18 kg (14 (± 0. 05) from birth to 12 months old, but males grew 40 -g d 4 <b>faster</b> <b>tha...</b>|$|E
40|$|The main {{objective}} of case retrieval is to scan and {{to map the}} most similar old cases in case base with a new problem. Beside accurateness, the time taken to retrieve case is also important. With {{the increasing number of}} cases in case base, the retrieval task is becoming more challenging where faster retrieval time and good accuracy are the main aim. Traditionally, sequential indexing method has been applied to search for possible cases in case base. This technique worked fast when the number of cases is small but requires more time to retrieve when the number of data in case base grows. As an alternative, this paper presents the integration of hashing indexing technique in case retrieval to mine large cases and speed up the retrieval time. Hashing indexing searches a record by determining the index using only an entry’s search key without traversing all records. To test the proposed method, real data namely Timah Tasoh Dam operational dataset, which is temporal in nature that represents the historical hydrological data of daily Timah Tasoh dam operation in Perlis, Malaysia ranging from year 1997 - 2005, was chosen as experiment. Then, the hashing indexing performance is compared with sequential method in term of retrieval time and accuracy. The finding indicates that hashing indexing is more accurate and <b>faster</b> <b>tha...</b>|$|E
40|$|Abstract: In {{the second}} half of the 1990 s, U. S. {{productivity}} growth moved up to rates not seen in several decades. In this paper, I use time-varying parameter techniques to isolate trend from cyclical movements in productivity and to obtain an estimate of the trend rate of productivity growth. I examine models both with and without an explicit role for capital accumulation. I find that in the models without an explicit role for capital accumulation, trend productivity growth is estimated to have moved up from around 1 - 1 / 2 percent in the period from the early 1970 s to the mid 1990 s, to about 2 - 1 / 2 percent by the final observation used in this paper, the second quarter of 2000. I find that if I allow for an explicit role for capital accumulation, the recent pace of trend productivity growth is even higher, at around 3 percent. The views expressed in this paper are those of the author and do not necessarily reflect the views of the Board of Governors of the Federal Reserve System or any other member of its staff. I am grateful to Dan Sichel, Bill Wascher, Spencer Krane, Bruce Fallick, Charles Fleischman, and Jeremy Rudd for helpful discussions, and to Ken Kuttner for providing his computer programs. In the latter half of the 1990 s, hourly productivity grew considerably <b>faster</b> <b>tha...</b>|$|E
40|$|Fast {{execution}} of physical system models has various uses, such as simulating physical phenomena or real-time testing of medical equipment. Physical system models commonly consist {{of thousands of}} differential equations. Solving such equations using software on microprocessor devices may be slow. Several past efforts implement such models as parallel circuits on special computing devices called Field-Programmable Gate Arrays (FPGAs), demonstrating large speedups due to the excellent match between the massive fine-grained local communication parallelism common in physical models and the fine-grained parallel compute elements and local connectivity of FPGAs. However, past implementation efforts were mostly manual or ad hoc. We present the first method for automatically converting a set of ordinary differential equations into circuits on FPGAs. The method uses a general Processing Element (PE) that we developed, designed to quickly solve a set of ordinary differential equations while using few FPGA resources. The method instantiates a network of general PEs, partitions equations among the PEs to minimize communication, generates each PE’s custom program, creates custom connections among PEs, and maintains synchronization of all PEs in the network. Our experiments show that the method generates a 400 -PE network on a commercial FPGA that executes four different models on average 15 x faster than a 3 GHz Intel processor, 30 x faster than a commercial 4 -core ARM, 14 x faster than a commercial 6 -core Texas Instruments digital signal processor, and 4. 4 x <b>faster</b> <b>tha...</b>|$|E
40|$|Motivation: Protein {{homology}} {{detection and}} sequence alignment {{are at the}} basis of protein structure prediction, function prediction, and evolution. Results: We have generalized the alignment of protein se-quences with a profile hidden Markov model (HMM) {{to the case of}} pairwise alignment of profile HMMs. We present a method for detecting distant homologous relationships between proteins based on this approach. The method (HHsearch) is benchmarked together with BLAST, PSI-BLAST, HMMER, and the profile-profile comparison tools PROF_SIM and COMPASS, in an all-against-all compari-son of a database of 3691 protein domains from SCOP 1. 63 with pairwise sequence identities below 20 %. Sensitivity: When predicted secondary structure is included in the HMMs, HHsearch is able to detect between 2. 7 and 4. 2 times more homologs than PSI-BLAST or HMMER and between 1. 44 and 1. 9 times more than COMPASS or PROF_SIM for a rate of false positives of 10 %. Approxi-mately half of the improvement over the profile–profile com-parison methods is attributable to the use of profile HMMs in place of simple profiles. Alignment quality: Higher sensitivity is mirrored by an in-creased alignment quality. HHsearch produced 1. 2, 1. 7, and 3. 3 times more good alignments (“balanced ” score> 0. 3) than the next best method (COMPASS), and 1. 6, 2. 9, and 9. 4 times more than PSI-BLAST, at the family, super-family, and fold level. Speed: HHsearch scans a query of 200 residues against 3691 domains in 33 s on an AMD 64 3 GHz PC. This is 10 times faster than PROF_SIM and 17 times <b>faster</b> <b>tha...</b>|$|E
40|$|Objective. To {{examine the}} {{relationship}} between community-level uninsurance rates and the self-reported unmet medical needs of insured and uninsured adults in the U. S. Data Sources. 2000 – 2001 Community Tracking Study, which includes data from 60 randomly selected U. S. communities. The sample is representative of the contiguous U. S. states. Study Design. Multilevel logistic regressions were employed to investigate whether the local uninsurance rate was related to having reported unmet medical needs within the last year. The models also included individual and community variables that could be potentially related to both community uninsurance rates and having reported unmet medical needs. Principal Findings. The community uninsurance rate was positively associated with having reported unmet medical needs, but only for insured adults. On average, a five percentage point increment in the local uninsured population is associated with a 10. 5 percent increase in the likelihood that an insured adult will report having unmet medical needs during the 12 -month period studied. Conclusion. Local health care delivery systems seem to be negatively affected by high uninsurance rates. These effects could have negative consequences for health care access, even for individuals who are themselves insured. Key Words. Uninsured, communities, access, spillovers The availability and affordability of health insurance is a major policy concern in the U. S. About one-fifth of the U. S. population between the ages of 18 and 64 (36. 3 million) lacks health insurance and the size of this population is expected to rise as health insurance becomes more costly relative to income for many people (DeNavas-Walt, Proctor, and Mills 2004; Rowland 2004). For most of the last decade, yearly job-based health insurance premium in-creases were higher than increases in both earnings and inflation (Gabel et al. 2001). If the premiums for health insurance continue to increase <b>faster</b> <b>tha...</b>|$|E
40|$|Motivation: Markov {{models are}} very popular for {{analyzing}} complex sequences such as protein sequences, whose sources are unknown, or whose underlying statistical characteristics {{are not well}} understood. A major problem is the computational complexity involved with using Markov models, especially the exponential growth of their size with {{the order of the}} model. The probabilistic suffix tree (PST) and its improved variant sparse probabilistic suffix tree (SPST) have been proposed to address some of the key problems with Markov models. The use of the suffix tree, however, implies that the space requirement for the PST/SPST could still be high. Results: We present the probabilistic suffix array (PSA), a data structure for representing information in variable length Markov chains. The PSA essentially encodes information in a Markov model by providing a time and space-efficient alternative to the PST/SPST. Given a sequence of length N, construction and learning in the PSA is done in O(N) time and space, independent of the Markov order. Prediction using the PSA is performed in O(m log N |Σ|) time, where m is the pattern length, and Σ is the symbol alphabet. In terms of modeling and prediction accuracy, using protein families from Pfam 25. 0, SPST and PSA produced similar results (SPST 89. 82 %, PSA 89. 56 %), but slightly lower than HMMER 3 (92. 55 %). A modified algorithm for PSA prediction improved the performance to 91. 7 %, or just 0. 79 % from HMMER 3 results. The average (maximum) practical construction space for the protein families tested was 21. 58 ± 6. 32 N (41. 11 N) bytes using the PSA, 27. 55 ± 13. 16 N (63. 01 N) bytes using SPST, and 47 ± 24. 95 N (140. 3 N) bytes for HMMER 3. The PSA was 255 times faster to construct than the SPST, and 11 times <b>faster</b> <b>tha...</b>|$|E
40|$|We study gossip {{algorithms}} for {{the rumor}} spreading problem which asks each node {{to deliver a}} rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple and robust solutions to the rumor spreading problem. A long series of papers analyzed the performance of uniform random gossip in which nodes repeatedly call a random neighbor to exchange all rumors with. A main result {{of this investigation was}} that uniform gossip com-log n pletes in O(Φ) rounds where Φ is the conductance of the network. More recently, non-uniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC’ 12] gave an O(log 3 n) algorithm to solve the 1 -local broadcast problem in which each node wants to exchange rumors locally with its 1 -neighborhood. By repeatedly applying this protocol one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. All these algorithms are inherently randomized in their design and analysis. A parallel research direction has been to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness, e. g., by using pseudorandom generators with short random seeds. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors and that at least a polylogarithmic number of random bit are crucially needed. In this paper we improves over this state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2 (k + log n) log n rounds 1. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust and <b>faster</b> <b>tha...</b>|$|E

