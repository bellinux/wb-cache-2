2|5589|Public
30|$|The cry {{recordings}} {{utilized in}} this study were captured under a wide variation in the recording conditions (i.e., context of recording, type of cry trigger, and types of <b>extraneous</b> <b>sound</b> <b>sources</b> present while recording). Moreover, infant-related attributes known to affect acoustic characteristics of cry (e.g., weight of the infant, prematurity of birth) varied as well.|$|E
30|$|Two datasets, Tampere cohort with 57 cry {{recordings}} and Cape Town cohort with 52 recordings, were analyzed. The recordings were captured under realistic clinical environments which often consisted of <b>extraneous</b> <b>sound</b> <b>sources.</b> The output of this segmentation system {{can then be}} utilized for performing further analysis involving extraction of required acoustic parameters from the identified acoustic parts. This segmentation system thus offers to be an essential pre-processing step for an infant cry analysis system especially {{when the number of}} cry recording to be analyzed is large enough to render manual segmentation unfeasible.|$|E
40|$|LES of the trailing-edge {{flow and}} noise of a NACA 0012 airfoil near stall By S. Moreau†, J. Christophe ‡ AND M. Roger¶ Reynolds-averaged Navier-Stokes (RANS) {{simulations}} and large-eddy simulations (LES) of flow over a low-speed airfoil are performed {{at very high}} angle of attack in near-stall or stall conditions. In the parallel experimental effort at Ecole Centrale de Lyon (ECL), such flow conditions were obtained at 16 ◦ and 18 ◦ of angle of attack, respectively, on the selected NACA 0012 airfoil. In the ECL small anechoic wind tunnel, the wall pressure noise sources have been measured simultaneously with the far field acoustic pressure. The stall condition is found to have an <b>extraneous</b> <b>sound</b> <b>source</b> at low frequencies {{on top of the}} trailing-edge noise. It is characterized by two specific tones at Strouhal number of 0. 31 and 0. 56. Both noise sources are found to be of dipolar nature. In the RANS k − ω SST simulations, the airfoil goes into stall at a much higher angle of attack of 25 ◦. Yet, during convergence at 18 ◦, the flow solution temporarily goes through a fully separated flow on the suction side, hinting at the observed experimental stall hysteresis. Even when this initial condition is selected, the corresponding LES returns to attached flow, yielding unrealistic unsteady lift on the airfoil. For the stall noise acoustic radiation, a simple model for a compact dipole is proposed based on the experimental evidences. From the experimental data collected on the airfoil, an analytical model is given for the spanwise coherence length...|$|R
5000|$|... remove <b>extraneous</b> <b>sounds</b> such as {{production}} equipment noise, traffic, wind, or other undesirable {{sounds from the}} environment.|$|R
5000|$|The phonemic {{restoration}} {{effect was}} first documented in a 1970 paper by Richard M. Warren entitled [...] "Perceptual Restoration of Missing Speech Sounds". The {{purpose of the}} experiment was to give a reason to why in background of <b>extraneous</b> <b>sounds,</b> masked individual phonemes were still comprehensible.|$|R
500|$|Szigeti's {{performing}} {{technique was}} not always flawless and his tone lacked sensuous beauty, although it acquired a spiritual quality in moments of inspiration ... Szigeti held the bow in an old-fashioned way, with the elbow close to the body, and produced much emphatic power, but not without <b>extraneous</b> <b>sounds.</b> Minor reservations, however, were swept aside {{by the force of}} his musical personality.|$|R
5000|$|The Rivingtons had {{originally}} been known as the Sharps and had had success in the charts with Thurston Harris's [...] "Little Bitty Pretty One" [...] in 1957. They then appeared on several Duane Eddy recordings whenever <b>extraneous</b> <b>sounds</b> of rebel yells were required, including Eddy's 1958 hit [...] "Rebel Rouser". They also recorded on Warner Brothers Records as The Crenshaws in 1961.|$|R
40|$|An audio {{localization}} test comparing {{accuracy of}} static and moving <b>sound</b> <b>sources</b> {{was carried out}} in a spatially immersive virtual environment, using loudspeaker array with vector based amplitude panning for virtual <b>sound</b> <b>sources.</b> Azimuth and elevation error in localization were measured with different sound signals. As was expected errors in azimuth localization accuracy were smaller than errors in elevation accuracy. There were more localization blur with virtual <b>sound</b> <b>sources</b> than <b>sound</b> <b>sources</b> reproduced directly from a single loudspeaker. Localization blur with moving <b>sound</b> <b>sources</b> were in the same level as with static panorated <b>sound</b> <b>sources.</b> Although the <b>sound</b> <b>sources</b> moved steadily, the measurements indicated that subjects perceived the changes in <b>sound</b> <b>source</b> location stepwise due to applied amplitude panning...|$|R
40|$|Earlier {{experiments}} {{have shown that}} when one or more speech sounds in a sentence are replaced by a noise meeting certain criteria, the listener mislocalizes the <b>extraneous</b> <b>sound</b> and believes he hears the missing phoneme(s) clearly. The present study confirms and extends these earlier reports of phonemic restorat. ions {{under a variety of}} novel conditions. All stimuli had some of the context necessary for the appropriate phonemic restoration following the missing sound, and all sentences had the missing phoneme deliberately mispronounced before electronic deletion (so that the neighboring phonemes could not provide acoustic cues to aid phonemic restorations). The results are interpreted in terms of mechanisms normally aiding veridical perception of speech and nonspeech sounds. This study is concerned with processes which normally aid auditory perception under noisy conditions. If an item in an auditory sequence is replaced hy a louder sound having an appropriate spectrum, hoth the missing <b>sound</b> and the <b>extraneous</b> <b>sound</b> may be heard clearly. This illusory perception of missing sounds has been called "auditory induction"...|$|R
50|$|The {{binaural}} {{aspect of}} the cocktail party effect {{is related to the}} localization of <b>sound</b> <b>sources.</b> The auditory system is able to localize at least two <b>sound</b> <b>sources</b> and assign the correct characteristics to these sources simultaneously. As soon as the auditory system has localized a <b>sound</b> <b>source,</b> it can extract the signals of this <b>sound</b> <b>source</b> out of a mixture of interfering <b>sound</b> <b>sources.</b>|$|R
40|$|Sound power {{describes}} a <b>sound</b> <b>source</b> {{regardless of its}} environment and is useful in noise control applications, but can be cumbersome and time consuming to measure. Sound power levels can rank different <b>sound</b> <b>sources</b> and is often restricted in noise control legislation. An acoustic camera records a sound field with a microphone array. Due to properties of the array, and by using beamforming algorithms, an acoustic camera can separate sound from different directions. The acoustic camera measures sound pressure from a <b>sound</b> <b>source.</b> By assuming directivity properties the sound power of a <b>sound</b> <b>source</b> {{can be derived from}} the sound pressure. In this thesis an acoustic camera has been evaluated in order to determine sound power estimation performance and <b>sound</b> <b>source</b> separation ability. This is tested by six different measurement set-ups in an anechoic chamber. Two different <b>sound</b> <b>sources</b> are used in the trials: one reference <b>sound</b> <b>source</b> and one disturbing <b>sound</b> <b>source.</b> The reference <b>sound</b> <b>source</b> has a calibrated and documented sound power level to which the measurement results are compared. Measurements were performed at 1 to 5 m distance from the acoustic camera with both <b>sound</b> <b>sources.</b> The influence of a disturbing <b>sound</b> <b>source</b> on the reference <b>sound</b> <b>source</b> <b>sound</b> power level was measured with the <b>sound</b> <b>sources</b> separated 0. 65 m to 2. 6 m. The measurements show that the sound power level could at best be determined within 1 dB. The acoustic camera can separate different <b>sound</b> <b>sources</b> well. Influence from a disturbing <b>sound</b> <b>source,</b> 10 dB SPL stronger and distanced 1 m from a reference <b>sound</b> <b>source</b> was 2 to 3 dB for mid-frequency one-third octave bands at 5 m measurement distance. Measuring sound power with an acoustic camera is fast and mobile compared to room interaction methods and sound intensity measurements. The results of this thesis are useful when measuring sound power levels, especially for <b>sound</b> <b>sources</b> such as chimney outlets, wind power stations and big objects that can not be moved or do not fit in a room. Validerat; 20101217 (root...|$|R
5000|$|... #Caption: A Numark DM2002X Pro Master DJ mixer. This three channel mixer {{can have}} up to three input <b>sound</b> <b>sources.</b> The gain control knobs and {{equalization}} control knobs allow the volume and tone of each <b>sound</b> <b>source</b> to be adjusted. The vertical faders allow for further adjustment {{of the volume of}} each <b>sound</b> <b>source.</b> The horizontally-mounted crossfader enables the DJ to smoothly transition from a song on one <b>sound</b> <b>source</b> to a song from a different <b>sound</b> <b>source.</b>|$|R
40|$|Monopole <b>sound</b> <b>sources</b> (i. e. omni {{directional}} <b>sound</b> <b>sources</b> with a known volume velocity) {{are essential}} for reciprocal measurements used in vehicle interior panel noise contribution analysis. Until recently, these monopole <b>sound</b> <b>sources</b> use a <b>sound</b> pressure transducer sensor as a reference sensor. A novel monopole <b>sound</b> <b>source</b> principle is demonstrated that uses a Microflown (acoustic particle velocity) sensor as a reference sensor. As compared to a <b>sound</b> <b>source</b> that uses a sound pressure transducer as a reference sensor, the new <b>sound</b> <b>sources</b> demonstrated are relatively easy to calibrate, not sensitive to changes in ambient temperatures, and suitable to use {{in all sorts of}} acoustic environments. © 2008 SAE International...|$|R
40|$|Measuring the {{contribution}} of a particular <b>sound</b> <b>source</b> to the ambient sound level at an arbitrary location is impossible without some form of <b>sound</b> <b>source</b> separation. This made it difficult, if not impossible, to design automated systems that measure {{the contribution}} of a target sound to the ambient sound level. This paper introduces <b>sound</b> <b>source</b> separation technology {{that can be used}} to measure {{the contribution of}} a <b>sound</b> <b>source,</b> a passing plane, in environments where planes are not the dominant <b>sound</b> <b>source.</b> This <b>sound</b> <b>source</b> separation and classification technology, developed by Sound Intelligence makes it, in principle, possible to monitor the temporal development of any soundscape...|$|R
50|$|Applications of <b>sound</b> <b>source</b> {{localization}} include <b>sound</b> <b>source</b> separation, <b>sound</b> <b>source</b> tracking, {{and speech}} enhancement. Sonar uses <b>sound</b> <b>source</b> localization techniques {{to identify the}} location of a target. 3D sound localization is also used for effective human-robot interaction. With the increasing demand for robotic hearing, some applications of 3D sound localization such as human-machine interface, handicapped aid, and military applications, are being explored.|$|R
40|$|It is {{necessary}} {{that the number}} of the observation signals equals to the number of source signals, if independent component analysis is used to perform the <b>sound</b> <b>source</b> separation. It is difficult to perform <b>sound</b> <b>source</b> separation from a stereo music sound signal when the number of <b>sound</b> <b>sources</b> are more than two. We propose the technique to perform <b>sound</b> <b>source</b> separation from a stereo music sound signal {{that the number of}} <b>sound</b> <b>sources</b> is more than two using the frequency analysis and independent component analysis. 1...|$|R
50|$|As a {{consequence}} the auditory system seems {{only to be}} able to localize <b>sound</b> <b>sources</b> in reverberant environment at sound onsets or at bigger spectral changes. Then the direct sound of the <b>sound</b> <b>source</b> prevails at least in some frequency ranges and the direction of the <b>sound</b> <b>source</b> can be determined. Some milliseconds later, when the sound of the wall reflections arrives, a <b>sound</b> <b>source</b> localization seems no more to be possible. As long as no new localization is possible, the auditory systems seems to keep the last localized direction as perceived <b>sound</b> <b>source</b> direction.|$|R
50|$|<b>Sound</b> <b>sources</b> {{refer to}} the {{conversion}} of aerodynamic energy into acoustic energy. There are two main types of <b>sound</b> <b>sources</b> in the articulatory system: periodic (or more precisely semi-periodic) and aperiodic. A periodic <b>sound</b> <b>source</b> is vocal fold vibration produced at the glottis found in vowels and voiced consonants. A less common periodic <b>sound</b> <b>source</b> is the vibration of an oral articulator like the tongue found in alveolar trills. Aperiodic <b>sound</b> <b>sources</b> are the turbulent noise of fricative consonants and the short-noise burst of plosive releases produced in the oral cavity.|$|R
3000|$|The {{objective}} of this work is to estimate the multiple fixed <b>sound</b> <b>source</b> directions without a priori information of the <b>sound</b> <b>source</b> number and the environment. This work utilizes the time delay information and microphone array geometry to estimate the <b>sound</b> <b>source</b> directions [23]. A novel eigenstructure-based GCC (ES-GCC) method to estimate the time delay under a multi-source environment between two microphones is proposed. The theoretical proof of the ES-GCC method is given, and the experimental results show that it is robust in a noisy environment. As a result, the <b>sound</b> <b>source</b> direction and velocity {{can be obtained by}} solving the proposed linear equation model using the time delay information. Fundamentally, the <b>sound</b> <b>source</b> number should be known while estimating the <b>sound</b> <b>source</b> directions. Hence, the method which can estimate <b>sound</b> <b>source</b> number and directions simultaneously using the proposed adaptive [...]...|$|R
40|$|<b>Sound</b> <b>source</b> {{tracking}} is {{an important}} function for a robot operating in a daily environment, because the robot should recognize where a sound event such as speech, music and other environmental sounds originates from. This paper addresses <b>sound</b> <b>source</b> tracking by integrating a room and a robot microphone array. The room microphone array consists of 64 microphones attached to the walls. It provides 2 D (x-y) <b>sound</b> <b>source</b> localization based on a weighted delay-and-sum beamforming method. The robot microphone array consists of eight microphones installed on a robot head, and localizes multiple <b>sound</b> <b>sources</b> in azimuth. The localization results are integrated to track <b>sound</b> <b>sources</b> by using a particle filter for multiple <b>sound</b> <b>sources.</b> The experimental results show that particle filter based integration reduces localization errors and provides accurate and robust 2 D <b>sound</b> <b>source</b> tracking. 1...|$|R
40|$|This work {{explores the}} design and {{effectiveness}} of a robot that uses a combination of active sonar and a pseudo-random acoustic signal to navigate and reconstruct an unknown environment. The robot sends the pseudo-random signal into the environment and records the resulting response. This response is processed to gain information on the robot’s immediate surroundings. Previous work done in this area focused on the recording and processing aspect to determine {{the location of a}} <b>sound</b> <b>source</b> or multiple <b>sound</b> <b>sources.</b> We apply similar algorithms to localize what are known as virtual <b>sound</b> <b>sources.</b> Virtual <b>sound</b> <b>sources</b> are created when sound from a <b>sound</b> <b>source</b> reflects off of a surface, such as a wall or object, and are recorded by a receiver. The recorded reflected sound is commonly known as an echo. A virtual <b>sound</b> <b>source</b> is placed at the location where the sound incident to the receiver would have originated from had no reflection taken place. By placing the real <b>sound</b> <b>source</b> and receivers on the same platform, if we can accurately localize the generated virtual <b>sound</b> <b>sources,</b> we can compute the pat...|$|R
30|$|Two {{evaluation}} metrics were calculated: Number {{and location}} of sources For each session, a <b>sound</b> <b>source</b> was considered as ‘estimated’ if it was detected for at least 25 % of {{the duration of the}} session (30 s). If a source is estimated within a ± 15 ∘ range of the expected direction of an actual <b>sound</b> <b>source,</b> it is considered a true positive. If a <b>sound</b> <b>source</b> is estimated outside that range from an actual <b>sound</b> <b>source,</b> it is considered a false positive. If an actual <b>sound</b> <b>source</b> is not estimated during the experiment, it is considered a false negative. Using these metrics, the precision, recall, and F 1 scores [49] (Chapter 8) of the proposed system’s ability to detect <b>sound</b> <b>sources</b> are calculated. Average error Once estimated, an average absolute error is calculated for every <b>sound</b> <b>source</b> that is deemed true positive, from the direction it is actually located.|$|R
30|$|The methods above {{assume that}} the <b>sound</b> <b>source</b> number is known. But {{this may not be}} a {{realistic}} assumption because the environment usually contains various kinds of <b>sound</b> <b>sources.</b> Several eigenvalue-based methods have been proposed [20, 21] to estimate the <b>sound</b> <b>source</b> number. However, the eigenvalue distribution is sensitive to noise and reverberation. The work in [22] used the support vector machine (SVM) to classify the distribution with respect to the <b>sound</b> <b>source</b> number. However, it still requires a training stage for a robust result and the binary classification is inadequate when the <b>sound</b> <b>source</b> number is larger than two.|$|R
5000|$|Loudness: Distant <b>sound</b> <b>sources</b> {{have a lower}} {{loudness}} than close ones. This aspect can {{be evaluated}} especially for well-known <b>sound</b> <b>sources.</b>|$|R
40|$|<b>Sound</b> <b>source</b> {{occlusion}} {{occurs when}} the direct path from a <b>sound</b> <b>source</b> to a listener is blocked by an intervening object. Currently, {{a variety of methods}} exist for modeling <b>sound</b> <b>source</b> occlusion. These include finite element and boundary element methods, as well as methods based on time-domain models of edge diffraction. At present, the high computational requirements of these methods precludes their use in real-time environments. In the case of real-time geometric room acoustic methods (e. g. the image method, ray tracing), the model of sound propagation employed makes it difficult to incorporate wave-related effects such as occlusion. As a result, these methods generally do not incorporate <b>sound</b> <b>source</b> occlusion. The lack of a suitable <b>sound</b> <b>source</b> occlusion method means that developers of real-time virtual environments (such as computer games) have generally either ignored this phenomenon or used rudimentary and perceptually implausible approximations. A potential solution to this problem is the use of shadow algorithms from computer graphics. These algorithms can provide a way to efficiently simulate <b>sound</b> <b>source</b> occlusion in real-time and in a physically plausible manner. Two simulation prototypes are presented, one for fixed-position <b>sound</b> <b>sources</b> and another for moving <b>sound</b> <b>sources...</b>|$|R
40|$|AbstractA three {{dimensional}} {{imaging method}} using electromagnetic induction type <b>sound</b> <b>source</b> and amplitude correlation synthesis processing method was proposed {{in our previous}} work. Considering the directivity of the <b>sound</b> <b>source</b> and the nonlinearity of the processing method, this paper discusses the lateral detecting ability of the method by both numerical simulation and experimental testing. Three objects at identical depth are measured with varying lateral positions relating to the <b>sound</b> <b>source</b> and receiver array. The {{results show that the}} lateral detecting ability is determined mainly by the directivity of the <b>sound</b> <b>source</b> and the efficient detecting area is about 35 ∘ inside the spread angle of the <b>sound</b> <b>source...</b>|$|R
40|$|Abstract—Sound source {{localization}} is {{an important}} feature in robot audition. This work proposes a <b>sound</b> <b>source</b> number and directions estimation method by using the delay information of microphone array. An eigenstructure-based generalized cross correlation method is proposed to estimate time delay between microphones. Upon obtaining the time delay information, the <b>sound</b> <b>source</b> direction and velocity can be estimated by least square method. In multiple <b>sound</b> <b>source</b> case, the time delay combination among microphones is arranged such that the estimated sound speed value falls within an acceptable range. By accumulating the estimation results of <b>sound</b> <b>source</b> direction and using adaptive K-means++ algorithm, the <b>sound</b> <b>source</b> number and directions can be estimated. I...|$|R
3000|$|This paper {{assumes that}} the {{distance}} from source to the array is {{much larger than the}} array aperture, and (29) is used to solve the <b>sound</b> <b>source</b> direction estimation problem. If the number of <b>sound</b> <b>sources</b> is known, the <b>sound</b> <b>source</b> directions can be estimated by putting time delay vector [...]...|$|R
3000|$|This work {{explains}} a <b>sound</b> <b>source</b> number and directions estimation algorithm. The multiple source time delay vector combination {{problem can be}} solved by the proposed reasonable sound velocity-based method. By accumulating the estimated <b>sound</b> <b>source</b> angle, the <b>sound</b> <b>source</b> number and directions {{can be obtained by}} the proposed adaptive [...]...|$|R
40|$|<b>Sound</b> <b>source</b> {{characteristics}} {{may be one}} of {{the main}} causes of objective speech intelligibility metric inaccuracy. In this study, the influences of the <b>sound</b> <b>source</b> directivity and frequency response were investigated using three typical sound sources: an artificial mouth, a monitor speaker, and a dodecahedral <b>sound</b> <b>source.</b> The results show that, the simultaneous influences of directivity and frequency response on the objective speech intelligibility metric are significant, typically with a variation of 0. 147 in speech transmission index (STI); <b>sound</b> <b>source</b> directivity may also result in a noticeable difference in the objective speech intelligibility metric, typically with a variation of 0. 123 in STI. In comparison with <b>sound</b> <b>sources</b> with a high directivity index (DI), the measurement results for <b>sound</b> <b>sources</b> with a relatively low DI may be higher when background noise is high, and may be lower when background noise is low. The influence of <b>sound</b> <b>source</b> directivity may also depend on the room acoustic conditions, and at receiver position where reflections are abundant, the influence of <b>sound</b> <b>source</b> directivity may be more significant. Not applying frequency response equalisation resulted in large errors in the values being measured, which deviate from the real values of STI by up to 0. 172, depending on the original frequency response characteristics of the <b>sound</b> <b>sources</b> that are used...|$|R
40|$|A longstanding {{philosophical}} tradition {{holds that}} the primary objects of hearing are sounds rather than <b>sound</b> <b>sources.</b> In this case, we hear <b>sound</b> <b>sources</b> by—or in virtue of—hearing their sounds. This paper argues that, on the contrary, we {{have good reason to}} believe that the primary objects of hearing are <b>sound</b> <b>sources,</b> and that the relationship between a <b>sound</b> and its <b>source</b> is much like the relationship between a color and its bearer. Just as we see objects in seeing their colors, so we hear <b>sound</b> <b>sources</b> in hearing their sounds...|$|R
40|$|In this paper, {{we present}} an active {{audition}} system for humanoid robot “SIG the humanoid”. The audition {{system of the}} highly intelligent humanoid requires localization of <b>sound</b> <b>sources</b> and identification of meanings of the sound in the auditory scene. The active audition reported in this paper focuses on improved <b>sound</b> <b>source</b> tracking by integrating audition, vision, and motor movements. Given the multiple <b>sound</b> <b>sources</b> in the auditory scene, SIG actively moves its head to improve localization by aligning microphones orthogonal to the <b>sound</b> <b>source</b> and by capturing the possible <b>sound</b> <b>sources</b> by vision. However, such an active head movement inevitably creates motor noise. The system must adaptively cancel motor noise using motor control signals. The experimental result demonstrates that the active audition by integration of audition, vision, and motor control enables <b>sound</b> <b>source</b> tracking in variety of conditions...|$|R
30|$|The {{direction}} of the <b>sound</b> <b>source</b> can be estimated using the sound localization method described above. With one stationary microphone array, {{it is hard to}} estimate the <b>sound</b> <b>source</b> position. However, the home service robot can move around, which makes it possible to use triangulation to localize the <b>sound</b> <b>source.</b> Figure  4 shows an example of using triangulation to estimate the positions of two <b>sound</b> <b>sources.</b> If the robot can measure the sound direction at two different positions on the 2 D map, the sound position can be estimated by calculating the intersection of two lines pointing to the <b>sound</b> <b>sources</b> from the robot positions. This method may create a undesired intersection point like point P as shown in Fig.  4. However, this point moves when the robot measures at another position. Therefore, it can be eliminated given the assumption that the <b>sound</b> <b>sources</b> are stationary. With multiple steps, the robot can improve the accuracy of position estimation using the RANdom SAmple Consensus (RANSAC) algorithm [29]. The <b>sound</b> <b>source</b> position estimation algorithm is shown in Algorithm 1.|$|R
40|$|A {{flexible}} <b>sound</b> <b>source</b> {{is essential}} in a whole flexible system. It’s hard to integrate a conventional <b>sound</b> <b>source</b> based on a piezoelectric part into a whole flexible system. Moreover, the sound pressure from {{the back side of}} a <b>sound</b> <b>source</b> is usually weaker than that from the front side. With the help of direct laser writing (DLW) technology, the fabrication of a flexible 360 -degree thermal <b>sound</b> <b>source</b> becomes possible. A 650 -nm low-power laser was used to reduce the graphene oxide (GO). The stripped laser induced graphene thermal <b>sound</b> <b>source</b> was then attached to the surface of a cylindrical bottle so that it could emit sound in a 360 -degree direction. The sound pressure level and directivity of the <b>sound</b> <b>source</b> were tested, and the results were in good agreement with the theoretical results. Because of its 360 -degree sound field, high flexibility, high efficiency, low cost, and good reliability, the 360 -degree thermal acoustic <b>sound</b> <b>source</b> will be widely applied in consumer electronics, multi-media systems, and ultrasonic detection and imaging...|$|R
30|$|Conditions of {{experiment}} 2 : occlusion of the <b>sound</b> <b>source</b> This experiment {{was conducted to}} evaluate {{the effect of the}} occlusion of <b>sound</b> <b>source</b> on the localization accuracy. Cardboard box (approximate dimensions height: 1 m, width: 0.5 m for each) was placed as shown in Fig. 5 and it completely occlude <b>sound</b> <b>source</b> 1.|$|R
2500|$|If a <b>sound</b> <b>source</b> and two {{microphones}} {{are arranged}} in a straight line, with the <b>sound</b> <b>source</b> at one end, then the following can be measured: ...|$|R
