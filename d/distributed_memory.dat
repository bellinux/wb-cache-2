4098|1881|Public
5|$|A {{distributed}} computer (also {{known as}} a <b>distributed</b> <b>memory</b> multiprocessor) is a <b>distributed</b> <b>memory</b> computer {{system in which the}} processing elements are connected by a network. Distributed computers are highly scalable.|$|E
5|$|Concurrent {{programming}} languages, libraries, APIs, {{and parallel}} programming models (such as algorithmic skeletons) {{have been created}} for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, <b>distributed</b> <b>memory,</b> or shared <b>distributed</b> <b>memory.</b> Shared memory programming languages communicate by manipulating shared memory variables. <b>Distributed</b> <b>memory</b> uses message passing. POSIX Threads and OpenMP {{are two of the}} most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.|$|E
5|$|Main {{memory in}} a {{parallel}} computer is either shared memory (shared between all processing elements in a single address space), or <b>distributed</b> <b>memory</b> (in which each processing element has its own local address space). <b>Distributed</b> <b>memory</b> refers {{to the fact that}} the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory.|$|E
40|$|<b>Distributed</b> shared <b>memory</b> is an {{architectural}} approach that allows multiprocessors {{to support a}} single shared address space that is implemented with physically <b>distributed</b> <b>memories.</b> Hardware-supported <b>distributed</b> shared <b>memory</b> is becoming the dominant approach for building multiprocessors with moderate to large numbers of processors. Cache coherence allows such architectures to use caching {{to take advantage of}} locality in applications without changing the programmer's model of memory. We review the key developments that {{led to the creation of}} cache-coherent <b>distributed</b> shared <b>memory</b> and describe the Stanford DASH Multiprocessor, the first working implementation of hardware-supported scalable cache coherence. We then provide a perspective on such architectures and discuss important remaining technical challenges...|$|R
40|$|Recently, <b>distributed</b> shared <b>memory</b> {{systems have}} {{received}} much attention because such an abstraction simplifies programming. In this paper, {{we present a}} simple protocol which implements the linearizability consistency criterion in a <b>distributed</b> shared <b>memory</b> system. Unlike previously implemented protocols, our protocol {{is based on an}} optimistic approach. The protocol eliminates the necessity of potentially expensive synchronization among processors for each write operation, but may require processes to rollback. Keywords : data consistency, <b>distributed</b> shared <b>memory,</b> linearizability, optimistic. 1 Introduction In a distributed computing environment, <b>distributed</b> shared <b>memory</b> systems (DSMS) have received much attention. Solutions for many classical synchronization problems have been developed using the shared memory abstraction, and the same solutions can be used in <b>distributed</b> shared <b>memory</b> systems. As a result, it simplifies programming. This work {{was supported in part by}} the N [...] ...|$|R
50|$|Processor Consistency {{is one of}} the {{consistency}} models used in the domain of concurrent computing (e.g. in <b>distributed</b> shared <b>memory,</b> <b>distributed</b> transactions, etc.).|$|R
5|$|Computer {{architectures}} {{in which}} each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, {{that can be achieved}} only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. <b>Distributed</b> <b>memory</b> systems have non-uniform memory access.|$|E
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as <b>distributed</b> <b>memory</b> systems do.|$|E
25|$|Memcached: a high-performance, <b>distributed</b> <b>memory</b> object caching system.|$|E
50|$|Strong {{consistency}} {{is one of}} {{the consistency}} models used in the domain of the concurrent programming (e.g., in <b>distributed</b> shared <b>memory,</b> <b>distributed</b> transactions).|$|R
40|$|Current <b>distributed</b> shared <b>memory</b> systems {{suffer from}} {{portability}} problems which hinder popularity. We present a <b>distributed</b> shared <b>memory</b> {{system as a}} distributed implementation of the Java Virtual Machine. The proposed system is unique in that it provides a user-friendly, flexible programming model based on pure Java. It is an object-based memory system which maintains the synchronization scope as the whole address space, like pagebased systems. MultiJav demonstrates {{that it is possible}} to design an efficient, portable, <b>distributed</b> shared <b>memory</b> system for running parallel and distributed applications written in a standard language. Keywords: <b>distributed</b> shared <b>memory,</b> objectbased sharing, Java, memory consistency 1 Introduction Numerous <b>distributed</b> shared <b>memory</b> systems have been designed to promote parallel computing on a cluster of workstations. Through continuous effort of performance optimization, systems of this kind have become a reasonable choice for applications of massive [...] ...|$|R
50|$|Release {{consistency}} {{is one of}} the synchronization-based consistency models used in {{the domain}} of the concurrent programming (e.g. in <b>distributed</b> shared <b>memory,</b> <b>distributed</b> transactions etc.).|$|R
25|$|In {{distributed}} computing, each processor has its {{own private}} memory (<b>distributed</b> <b>memory).</b> Information is exchanged by passing messages between the processors.|$|E
25|$|During the 1980s, as {{the demand}} for {{computing}} power increased, the trend to a much larger number of processors began, ushering {{in the age of}} massively parallel systems, with <b>distributed</b> <b>memory</b> and distributed file systems, given that shared memory architectures could not scale to a large number of processors. Hybrid approaches such as distributed shared memory also appeared after the early systems.|$|E
25|$|Computer {{clustering}} {{relies on}} a centralized management approach which makes the nodes available as orchestrated shared servers. It is distinct from other approaches such as peer to peer or grid computing which also use many nodes, but with a far more distributed nature. By the 21st century, the TOP500 organization's semiannual list of the 500 fastest supercomputers often includes many clusters, e.g. the world's fastest in 2011, the K computer with a <b>distributed</b> <b>memory,</b> cluster architecture.|$|E
5000|$|Delta {{consistency}} {{is one of}} {{the consistency}} models used in the domain of parallel programming, for example in <b>distributed</b> shared <b>memory,</b> <b>distributed</b> transactions, and Optimistic replication ...|$|R
40|$|This paper {{describes}} {{results obtained}} {{in a study}} of algorithms to implement a <b>Distributed</b> Shared <b>Memory</b> in a <b>distributed</b> (loosely coupled) environment. <b>Distributed</b> Shared <b>Memory</b> is the implementation of shared memory across multiple nodes in a distributed system. This is accomplished using only the private memories of the nodes by controlling access to the pages of the shared memory and transferring data to and from the private memories when necessary. We analyze alternative algorithms to implement <b>Distributed</b> Shared <b>Memory,</b> all of them based on the ideas presented in [Li 86 b]. 	The <b>Distributed</b> Shared <b>Memory</b> algorithms are analyzed and compared over a wide range of conditions. Application characteristics are identified which can be exploited by the <b>Distributed</b> Shared <b>Memory</b> algorithms. We will show the conditions under which the algorithms analyzed in this paper perform better or worse than the other alternatives. Results are obtained via simulation using a synthetic reference generator...|$|R
40|$|We present recent {{results in}} the {{application}} of <b>distributed</b> shared <b>memory</b> to image parallel ray tracing on clusters. Image parallel rendering is traditionally limited to scenes that are small enough to be replicated in the memory of each node, because any processor may require access to any piece of the scene. We solve this problem by making all of a cluster’s memory available through software <b>distributed</b> shared <b>memory</b> layers. With gigabit ethernet connections, this mechanism is sufficiently fast for interactive rendering of multi-gigabyte datasets. Object- and page-based <b>distributed</b> shared <b>memories</b> are compared, and optimizations for efficient memory use are discussed. Key words: scientific visualization, out-of-core rendering, <b>distributed</b> shared <b>memory,</b> ray tracing, cache miss reduction...|$|R
25|$|Since these {{formulas}} are matrix {{operations with}} dominant Level 3 operations, they {{are suitable for}} efficient implementation using software packages such as LAPACK (on serial and shared memory computers) and ScaLAPACK (on <b>distributed</b> <b>memory</b> computers). Instead of computing the inverse of a matrix and multiplying by it, it is much better (several times cheaper and also more accurate) to compute the Cholesky decomposition of the matrix and treat the multiplication by the inverse as solution of a linear system with many simultaneous right-hand sides.|$|E
25|$|Approaches that {{represent}} previous experiences directly {{and use a}} similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses {{in such a way}} that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse <b>distributed</b> <b>memory</b> that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.|$|E
25|$|The K {{computer}} is a water-cooled, homogeneous processor, <b>distributed</b> <b>memory</b> {{system with a}} cluster architecture. It uses more than 80,000 SPARC64 VIIIfx processors, each with eight cores, {{for a total of}} over 700,000 coresalmost twice as many as any other system. It comprises more than 800 cabinets, each with 96 computing nodes (each with 16GB of memory), and 6 I/O nodes. Although it is more powerful than the next five systems on the TOP500 list combined, at 824.56MFLOPS/W it has the lowest power to performance ratio of any current major supercomputer system. The follow up system for the K computer, called the PRIMEHPC FX10 uses the same six-dimensional torus interconnect, but still only one processor per node.|$|E
40|$|The {{abstraction}} {{for supporting}} the notion of shared memory in a non-shared <b>memory</b> (<b>distributed)</b> architecture {{is referred to as}} <b>distributed</b> shared <b>memory.</b> We have implemented a set of mechanisms for maintaining the coherence of <b>distributed</b> shared <b>memory.</b> In this paper we show how distributed fault-tolerant transactions can be implemented elegantly using these mechanisms. Since our mechanisms combine the tasks of synchronization and data transfer into one, transaction-level locking of segments comes for free. Fault-tolerance is achieved by using replication on top of <b>distributed</b> shared <b>memory.</b> We show that the resulting implementation incurs much less overhead compared to message-passing schemes. Index Terms: <b>Distributed</b> Shared <b>Memory,</b> Transactions, Concurrency Control, Fault Tolerance, Replication 1 Introduction There are two paradigms for interprocess communication and synchronization: message-passing and shared <b>memory.</b> In a <b>distributed</b> system, the former takes the more structured [...] ...|$|R
40|$|This paper {{presents}} an {{implementation of a}} parallel logic programming system on a distributed shared memory(DSM) system. Firstly, we give a brief introduction of Andorra-I parallel logic programming system implemented on multiprocessors. Secondly, we outline the concurrent programming environment provided by a <b>distributed</b> shared <b>memory</b> system [...] TreadMarks. Thirdly, we discuss the implementation issues of Andorra-I system based on TreadMarks, such as, shared memory consistency, synchronization, mutual exclusion, shared memory allocation. Finally, we discuss implementation results and further research issues for building a practically useful logic-based reasoning system on <b>distributed</b> shared <b>memory</b> systems. Key Words: <b>Distributed</b> Shared <b>Memory,</b> Parallel Logic Programming System, Lazy Release Consistency, AND-parallelism, OR-parallelism 1. INTRODUCTION <b>Distributed</b> Shared <b>Memory</b> (DSM) systems have been rapidly developed these years, with various kinds of memory consistency models proposed [...] ...|$|R
40|$|<b>Distributed</b> shared <b>memory</b> {{systems are}} an {{important}} tool for developingand executing parallel computations on networks of workstations. However, their performance strongly depends on the efficient implementation of concurrent access totheshared data. We propose several techniques withthepotential of improving the performance of important basic protocols utilized in the existing <b>distributed</b> shared <b>memory</b> systems...|$|R
25|$|Supercomputing: DRDO's ANURAG {{developed}} the PACE+ Supercomputer for strategic purposes for supporting its various programmes. The initial version, as detailed in 1995, had the following specifications: The system delivered a sustained performance {{of more than}} 960 Mflops (million floating operations per second) for computational fluid dynamics programmes. Pace-Plus included 32 advanced computing nodes, each with 64 megabytes(MB) of memory that can be expanded up to 256MB and a powerful front-end processor which is a hyperSPARC with a speed of 66/90/100 megahertz (MHz). Besides fluid dynamics, these high-speed computer systems were used {{in areas such as}} vision, medical imaging, signal processing, molecular modeling, neural networks and finite element analysis. The latest variant of the PACE series is the PACE ++, a 128 node parallel processing system. With a front-end processor, it has a <b>distributed</b> <b>memory</b> and message passing system. Under Project Chitra, the DRDO is implementing a system with a computational speed of 2-3 Teraflops utilising commercial off the shelf components and the Open Source Linux Operating System.|$|E
2500|$|Sparse <b>Distributed</b> <b>Memory</b> by Pentti Kanerva (Bradford Books/MIT Press, 1988). (...) ...|$|E
2500|$|Integrating {{external}} memory components {{with artificial}} neural networks dates to early research in distributed representations and self-organizing maps. For example, in sparse <b>distributed</b> <b>memory,</b> the patterns encoded by neural networks {{are used as}} memory addresses for content-addressable memory, with [...] "neurons" [...] essentially serving as address encoders and decoders.|$|E
5000|$|Transcend Information, Inc. (...) is a Taiwanese company {{headquartered}} in Taipei, Taiwan that manufactures and <b>distributes</b> <b>memory</b> products. Transcend's product portfolio consists of over 2,000 products including memory modules, flash memory cards, USB flash drives, portable hard drives, multimedia products, solid state drives, dashcams, body cameras, personal cloud storage, card readers and accessories.|$|R
40|$|This paper {{shows how}} to define {{consistency}} conditions for <b>distributed</b> shared <b>memories</b> in virtually synchronous environments. Such definitions allow to develop fault tolerant implementations of <b>distributed</b> shared <b>memories,</b> in which during normal execution, operations {{can be performed}} very efficiently, and only those operations which take place during a configuration change must be delayed. Three well known consistency conditions, namely, linearizability, sequential consistency, and causal memory, are redefined for virtually synchronous environments. It is then shown how to provide efficient fault tolerant implementations for these definitions. This work was supported by ARPA/ONR grant N 00014 - 92 -J- 1866 1 Introduction <b>Distributed</b> shared <b>memory</b> is an important communication paradigm for making massively parallel computers usable, and for the successful treatment of networks of workstations as a parallel machine. <b>Distributed</b> shared <b>memory</b> is a convenient model to work with, it is a nat [...] ...|$|R
40|$|We {{study the}} {{performance}} benefits of speculation in a release consistent software <b>distributed</b> shared <b>memory</b> system. We propose a new protocol, Speculative Home-based Release Consistency, that speculatively updates data at remote nodes {{to reduce the}} latency of remote memory accesses. Our protocol employs a predictor that uses patterns in past accesses to shared memory to predict future accesses. We have implemented our protocol in a software <b>distributed</b> shared <b>memory</b> system that runs on commodity hardware. We evaluate our protocol implementation {{on a number of}} software <b>distributed</b> shared <b>memory</b> benchmarks and show that it can result in significant performance improvements...|$|R
2500|$|Two {{issues that}} need to be {{addressed}} as the number of processors increases are the distribution of memory and processing. In the <b>distributed</b> <b>memory</b> approach, each processor is physically packaged close with some local memory. The memory associated with other processors is then [...] "further away" [...] based on bandwidth and latency parameters in non-uniform memory access.|$|E
2500|$|Integrating {{external}} memory with ANNs {{dates to}} early research in distributed representations and Kohonen's self-organizing maps. For example, in sparse <b>distributed</b> <b>memory</b> or hierarchical temporal memory, the patterns encoded by neural networks {{are used as}} addresses for content-addressable memory, with [...] "neurons" [...] essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.|$|E
2500|$|Wikipedia {{receives}} between 25,000 and 60,000 page requests per second, {{depending on}} time of day. [...] page requests are first passed to a front-end layer of Squid caching servers. Further statistics, {{based on a}} publicly available 3-month Wikipedia access trace, are available. Requests that cannot be served from the Squid cache are sent to load-balancing servers running the Linux Virtual Server software, which in turn pass them {{to one of the}} Apache web servers for page rendering from the database. The web servers deliver pages as requested, performing page rendering for all the language editions of Wikipedia. To increase speed further, rendered pages are cached in a <b>distributed</b> <b>memory</b> cache until invalidated, allowing page rendering to be skipped entirely for most common page accesses.|$|E
5000|$|JavaSpaces is a Sun {{specification}} for a <b>distributed,</b> shared <b>memory</b> (spaces based) ...|$|R
40|$|This paper {{presents}} a new <b>Distributed</b> Shared <b>Memory</b> (DSM) management concept that is {{integrated into a}} scalable <b>distributed</b> virtual <b>memory</b> management technique and circumvents false sharing while still preserving simplicity to the application level. Objects defined as usual by variables in the declaration part of functions are made sharable among threads executing in the distributed environment. These object...|$|R
5000|$|The name weak {{consistency}} {{may be used}} in two senses. In {{the first}} sense, strict and more popular, the weak consistency is one of the consistency models used in the domain of the concurrent programming (e.g. in <b>distributed</b> shared <b>memory,</b> <b>distributed</b> transactions etc.).|$|R
