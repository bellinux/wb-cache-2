1973|8904|Public
25|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning vector quantization, decision tree analysis, random forests, naive Bayes, logistic regression, kernel regression, artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, covariance matrix self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or <b>information</b> <b>gain</b> (entropy).|$|E
25|$|In {{mathematical}} statistics, the Kullback–Leibler divergence (also called relative entropy) is {{a measure}} of how one probability distribution diverges from a second, expected probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and <b>information</b> <b>gain</b> when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread. In the simple case, a Kullback–Leibler divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a Kullback–Leibler divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. In simplified terms, it {{is a measure}} of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.|$|E
2500|$|As {{stated above}} Busch's theorem {{prevents}} a free lunch: {{there can be}} no <b>information</b> <b>gain</b> without disturbance. However the tradeoff between <b>information</b> <b>gain</b> and disturbance has been characterized by many authors including Fuchs and Peres; [...] Fuchs; [...] Fuchs and Jacobs; and Banaszek.|$|E
3000|$|The entropy then is {{used with}} the score {{equation}} Score(v) = InfoGain(v)/AnonyLoss([...] v [...]) + 1, which defines the relation between <b>information</b> <b>gained</b> and anonymization loss. As shown in Eq.  7, the <b>information</b> <b>gained</b> adopts the entropy equation [24].|$|R
5000|$|... might misuse {{confidential}} <b>information</b> <b>gained</b> in {{the course}} of the solicitor-client relationship.|$|R
5000|$|... #Subtitle level 2: Cover Test & <b>Information</b> <b>Gained</b> {{from the}} Cover Test ...|$|R
2500|$|An {{information}} theoretic {{analysis using}} a simplified but useful model {{shows that in}} asexual reproduction, the <b>information</b> <b>gain</b> per generation of a species is limited to 1 bit per generation, while in sexual reproduction, the <b>information</b> <b>gain</b> is bounded by , where [...] {{is the size of}} the genome in bits.|$|E
2500|$|... {{and so the}} {{combined}} <b>information</b> <b>gain</b> does not obey the triangle inequality: ...|$|E
2500|$|Recently the <b>information</b> <b>gain</b> {{disturbance}} tradeoff relation {{has been}} {{examined in the}} context of what is called the [...] "Gentle measurement lemma".|$|E
50|$|The <b>information</b> <b>gained</b> from decryptions was {{eventually}} code-named Magic within the US government.|$|R
5000|$|Competitive {{selection}} {{is the process}} that determines which <b>information</b> <b>gains</b> access to working memory.|$|R
50|$|Connect: <b>Information</b> <b>gained</b> so {{can be used}} to {{communicate}} with doctors, care teams or family members.|$|R
2500|$|... to {{the message}} length. This {{therefore}} represents the amount of useful information, or <b>information</b> <b>gain,</b> about , that we can estimate has been learned by discovering [...]|$|E
2500|$|The {{expected}} {{weight of}} evidence for H1 over H0 {{is not the same}} as the <b>information</b> <b>gain</b> expected per sample about the probability distribution p(H) of the hypotheses, ...|$|E
2500|$|Mutual {{information}} can be expressed as the average Kullback–Leibler divergence (<b>information</b> <b>gain)</b> between the posterior probability distribution of X given the value of Y and the prior distribution on X: ...|$|E
5000|$|As an {{alternative}} to parenthetical references; it is a simpler way to acknowledge <b>information</b> <b>gained</b> from another source.|$|R
5000|$|... #Caption: British Government Statement of Information {{regarding}} recent attacks, {{including a}} summary of <b>information</b> <b>gained</b> from Operation Agatha.|$|R
40|$|During {{election}} campaigns {{political parties}} compete to inform voters about their leaders, the issues, {{and where they}} stand on these issues. In that sense, election campaigns {{can be viewed as}} a particular kind of information campaign. Democratic theory supposes that participatory democracies are better served by an informed electorate rather than an uninformed one. But do all voters make equal <b>information</b> <b>gains</b> during campaigns? Why do some people make more <b>information</b> <b>gains</b> than others? And does the acquisition of campaign information have any impact on vote intentions? Drawing on the combined insights from political science research, communications theory and social psychology, we develop specific hypotheses about these campaign information dynamics. These hypotheses are tested with data from the 1997 Canadian Election Study, which includes a rolling cross-national campaign component, a post-election component, and a media content analysis. The results show that some people do make more <b>information</b> <b>gains</b> than others; campaigns produce a knowledge gap. Further, the intensity of media signals on different issues has an important impact on who receives what <b>information</b> and <b>information</b> <b>gains</b> have a significant impact on vote intentions...|$|R
2500|$|... {{which can}} be {{interpreted}} as the expected <b>information</b> <b>gain</b> about X from discovering which probability distribution X is drawn from, P or Q, if they currently have probabilities λ and (1−λ) respectively.|$|E
2500|$|If {{a further}} piece of data, , {{subsequently}} comes in, the probability distribution for [...] can be updated further, {{to give a}} new best guess [...] [...] If one reinvestigates the <b>information</b> <b>gain</b> for using [...] rather than , {{it turns out that}} it may be either greater or less than previously estimated: ...|$|E
2500|$|In Bayesian {{statistics}} the Kullback–Leibler divergence {{can be used}} as {{a measure}} of the <b>information</b> <b>gain</b> in moving from a prior distribution to a posterior distribution: [...] [...] If some new fact [...] is discovered, it can be used to update the posterior distribution for [...] from [...] to a new posterior distribution [...] using Bayes' theorem: ...|$|E
30|$|H 1 - 5 : Searching <b>information</b> <b>gained</b> {{through the}} prior {{experience}} {{has a positive}} effect on care quality.|$|R
30|$|H 2 - 8 : Supporting system and/or <b>information</b> <b>gained</b> {{through the}} current {{experience}} {{has a positive}} effect on recommendation.|$|R
50|$|It {{will feed}} <b>information</b> <b>gained</b> by, among other methods, {{automatic}} number plate recognition to the Office of the Traffic Commissioner.|$|R
2500|$|The idea of Kullback–Leibler {{divergence}} as discrimination information led Kullback {{to propose}} the Principle of Minimum Discrimination Information (MDI): given new facts, a new distribution f should be chosen {{which is as}} hard to discriminate from the original distribution f0 as possible; so that the new data produces as small an <b>information</b> <b>gain</b> DKL( [...] f ‖ f0 [...] ) as possible.|$|E
2500|$|The Kullback–Leibler {{divergence}} (or information divergence, <b>information</b> <b>gain,</b> {{or relative}} entropy) {{is a way}} of comparing two distributions: a [...] "true" [...] probability distribution p(X), and an arbitrary probability distribution q(X). If we compress data in a manner that assumes q(X) is the distribution underlying some data, when, in reality, p(X) is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression. [...] It is thus defined ...|$|E
2500|$|In {{the context}} of machine learning, DKL(P‖Q) is often called the <b>information</b> <b>gain</b> {{achieved}} if P is used instead of Q. [...] By analogy with information theory, it is also called the relative entropy of P with respect to Q. [...] In {{the context of}} coding theory, DKL(P‖Q) can be construed as measuring the expected number of extra bits required to code samples from P using a code optimized for Q rather than the code optimized for P.|$|E
5000|$|The <b>information</b> <b>gained</b> can be {{utilized}} to ensure business growth. Information for {{voice of the}} business can be extracted from: ...|$|R
30|$|H 1 - 6 : Searching <b>information</b> <b>gained</b> {{through the}} prior {{experience}} {{has a positive}} effect on supporting system and/or information.|$|R
30|$|H 2 - 7 : Supporting system and/or <b>information</b> <b>gained</b> {{through the}} current {{experience}} {{has a positive}} effect on relationship building.|$|R
2500|$|On the entropy {{scale of}} <b>information</b> <b>gain</b> {{there is very}} little {{difference}} between near certainty and absolute certainty—coding according to a near certainty requires hardly any more bits than coding according to an absolute certainty. On the other hand, on the logit scale implied by weight of evidence, {{the difference between the two}} is enormous – infinite perhaps; this might reflect the difference between being almost sure (on a probabilistic level) that, say, the Riemann hypothesis is correct, compared to being certain that it is correct because one has a mathematical proof. [...] These two different scales of loss function for uncertainty are both useful, according to how well each reflects the particular circumstances of the problem in question.|$|E
5000|$|The <b>information</b> <b>gain</b> {{ratio is}} just the ratio between the <b>information</b> <b>gain</b> and the {{intrinsic}} value: ...|$|E
5000|$|In ID3, <b>information</b> <b>gain</b> can be {{calculated}} (instead of entropy) for each remaining attribute. The attribute with the largest <b>information</b> <b>gain</b> is used to split the set [...] on this iteration.|$|E
5000|$|Pathologic stage adds {{additional}} <b>information</b> <b>gained</b> by {{examination of}} the tumor microscopically by a pathologist {{after it has been}} surgically removed.|$|R
30|$|H 1 - 4 : Searching <b>information</b> <b>gained</b> {{through the}} prior {{experience}} {{has a positive}} effect on costs related with medical tourism.|$|R
50|$|The New Zealand Police {{raided the}} Daktory on Saturday, January 9, 2010 {{apparently}} acting on <b>information</b> <b>gained</b> from a Sunday News journalist.|$|R
