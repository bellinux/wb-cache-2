7|240|Public
40|$|Anintention of MapReduce Sets forChain Foldingexpressions {{analysis}} has to suggest criteria how Chain Foldingexpressions in Chain Folding {{data can be}} defined {{in a meaningful way}} and how they should be compared. Similitude based MapReduce Sets for Chain FoldingExpression Analysis and MapReduce Sets for Assignment is expected to adhere to fundamental principles of the scientific Chain Foldingprocess that are expressiveness of Chain Folding models and reproducibility of their Chain Foldinginference. Chain Foldingexpressions are assumed to be elements of a Chain Foldingexpression space or Conjecture class and Chain Folding data provide “information ” which of these Chain Foldingexpressions should be used to interpret the Chain Folding data. An <b>inference</b> <b>Chain</b> Folding algorithm constructs the mapping between Chain Folding data and Chain Folding expressions, in particular by a Chain Foldingcost minimization process. Fluctuations in the Chain Folding data often limit the Chain Folding precision, which we can achieve to uniquely identify a single Chain Foldingexpression as interpretation of the Chain Folding data. We advocate an information theoretic perspective on Chain Foldingexpression analysis to resolve this dilemma where the tradeoff between Chain Foldinginformativeness of statistical <b>inference</b> <b>Chain</b> Foldingand their Chain Foldingstability is mirrored in the information-theoretic Chain Foldingoptimum of high Chain Foldinginformation rate and zero communicationexpression error. The <b>inference</b> <b>Chain</b> Foldingalgorithm is considered as anoutlier objectChain Foldingpath, which naturally limits the resolution of the Chain Foldingexpression space given the uncertainty of the Chain Folding data...|$|E
40|$|As belief {{networks}} {{are used to}} model increasingly complex situations, the need to automatically construct them from large databases will become paramount. This paper concentrates on solving {{a part of the}} belief network induction problem: that of learning the quantitative structure (the conditional probabilities), given the qualitative structure. In particular, a theory is presented that shows how to propagate inference distributions in a belief network, with the only assumption being that the given qualitative structure is correct. Most inference algorithms must make at least this assumption. The theory is based on four network transformations that are sufficient for any inference in a belief network. Furthermore, the claim is made that contrary to popular belief, error will not necessarily grow as the <b>inference</b> <b>chain</b> grows. Instead, for QBN belief nets induced from large enough samples, the error is more likely to decrease as the size of the <b>inference</b> <b>chain</b> increases. Comment: Appears in Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (UAI 1993...|$|E
40|$|Three {{dimensional}} integration {{schemes for}} VLSI {{have the potential}} for enabling the development of new high-performance architectures for applications such as focal plane sensors. Due to the high costs involved in 3 -D VLSI fabrication and the fabrication complexity of 3 -D integration, analysis of the design and process tradeoffs for a particular application is essential. An architectural and topological design tool is presented that enables the high-level analysis and optimization of sensor architectures targeted to a variety of 3 -D VLSI process options. This design tool is based on an <b>inference</b> <b>chain</b> evaluation framework, and allows for a high-level structural representation of a circuit architecture to be considered in conjunction with low-level process models. Approximation strategies for projecting circuit area and performance are incorporated into the <b>inference</b> <b>chain</b> relations. by Brian Tyrrell. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2004. Includes bibliographical references (p. 153 - 159) ...|$|E
40|$|Deterministic {{dynamics}} {{can produce}} stable distributions of behavior Discretizing with partitions gives symbol sequences These need a statistical description <b>Inference</b> for Markov <b>chains</b> <b>Inference</b> for higher-order Markov <b>chains</b> <b>Inference</b> for stochastic machines FURTHER READING: Everyone steals {{the theory of}} likelihood <b>inference</b> for Markov <b>chains</b> from Billingsley (1961). But Guttorp (1995) is easier reading. For hidden Markov models, see Fraser (2008) ...|$|R
40|$|Distantly {{supervised}} relation extraction {{has been}} widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing {{only one of the}} target entities, which provide rich and useful information for relation extraction. To address this issue, we build <b>inference</b> <b>chains</b> between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and <b>inference</b> <b>chains.</b> Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from [URL] //github. com/thunlp/PathNRE. Comment: Proceedings of EMNLP 2017. Code and dataset availabl...|$|R
50|$|Because {{the data}} determines which rules are {{selected}} and used, {{this method is}} called data-driven, in contrast to goal-driven backward <b>chaining</b> <b>inference.</b> The forward <b>chaining</b> approach is often employed by expert systems, such as CLIPS.|$|R
30|$|In contrast, in comprehending implicatures {{conveyed}} through non-conventionalized utterances, examinees typically explained their <b>inference</b> <b>chain</b> {{based on}} the content of the most relevant adjacency pair, and, in doing so, they often drew on background knowledge and personal experience to justify their interpretations. For example, in the following excerpt, the examinee first repeated the last adjacency pair containing the implicature; she then explained her (incorrect) interpretation by referring to a courteous behavior that routinely occurs, that is, closing the door before playing loud music so as to not disturb other people.|$|E
30|$|Automatic model {{generation}} by compositional modeling is {{not sufficient}} for automatic explanation generation in education. That is, in automatic model composition, an <b>inference</b> <b>chain</b> is generated which starts with the preconditions (given facts) and ends with the derived consequences (required amounts). However, sequencing the steps in the chain simply in one direction (either forward or backward) results in the explanation which is different from human experts’ explanation. As discussed in the Explanation Generation section, experts first identify the dominant principle/law in solving a problem, then apply other principles/laws to derive its conditions. That is, the explanation goes back and forth. Our explanation generator enables such an explanation by using the SOC-based description of solution. In fact, {{the description of the}} solution in our framework is “redundant” as an <b>inference</b> <b>chain</b> of automatic model composition. For example, in Fig.  1 (a), though the fact “acceleration(b 1)[*]=[*] 0 ” is linked to the model fragment “balance of forces,” this fact is not necessary for the automatic application of this model fragment (only the fact “net-force(b 1)[*]=[*] 0 ” is necessary). However, this link {{plays an important role in}} explanation generation because it represents experts’ insight “balance of forces is the dominant principle of the solution.” Since such a link must be written manually, it is our important future work to assist the authors in describing such information based on the SOC framework.|$|E
40|$|Abstract. This project {{description}} {{focuses on a}} specific occurrence of norma-tive conflicts. It addresses the need of deciding the applicable law when con-flicting pieces of legislation coming from different legal systems have to be merged. I will check whether and how the argumentation method could help {{to deal with these}} cases of normative interactions. Actually, from the logical point of view, the situation described poses some challenging issues, first of all that of taking into consideration contextual reasoning. Furthermore, the task of merging of normative provisions from different legal systems is itself far from being just an automatic activity: merging is not obvious and when it is required in a concrete case, the point is that in principle the systematic character of law cannot allow the legal operator to mechanically combine X from system x and Y from system y. The argumentation method can show its efficacy when rea-soning in such situations is often characterized by interpretive uncertainty. The methodology I will follow consists of two main steps: firstly, I outline the legal case study drawn from international taxation law, i. e. juridical double imposi-tion; then, I propose some argument schemes that can exemplify the legal rea-soning and the <b>inference</b> <b>chain</b> that lead the legal operator when facing such sit-uations...|$|E
40|$|Commonsense {{reasoning}} {{at scale}} is a core problem for cognitive systems. In this paper, we discuss two {{ways in which}} heuristic graph traversal methods {{can be used to}} generate plausible <b>inference</b> <b>chains.</b> First, we discuss how Cyc’s predicate-type hierarchy can be used to get reasonable answers to queries. Second, we explain how connection graph-based techniques can be used to identify script-like structures. Finally, we demonstrate through experiments that these methods lead to significant improvement in accuracy for both Q/A and script construction...|$|R
40|$|Production {{systems have}} a special value since they are used in {{state-space}} searching algorithms and expert systems {{in addition to their}} use as a model for problem solving in artificial intelligence. Therefore, it is of high importance to consider different techniques to improve their performance. In this research, rule base is the component of the production system that we aim to focus on. This work therefore seeks to investigate this component and its relationship with other components and demonstrate how the improvement of its quality has a great impact on the performance of the production system as a whole. In this paper, the improvement of rule base quality is accomplished in two steps. The first step involves re-writing the rules having conjunctions of literals and producing a new set of equivalent rules in which long <b>inference</b> <b>chains</b> can be obtained easily. The second step involves augmenting the rule base with inference short-cut rules devised from the long <b>inference</b> <b>chains.</b> These <b>inference</b> short-cut rules have a great impact on the performance of the production system. Finally, simulations are performed on randomly generated rule bases with different sizes and goals to be proved. The simulations demonstrate that the suggested enhancements are very beneficial in improving the performance of production systems...|$|R
40|$|This paper {{reports a}} {{research}} effort in scenario recognition task in information extraction. The presented approach uses partial semantic analysis based on logical form {{representation of the}} templates and the processed text. It is implemented in the system FRET 1 (Football Reports Extraction Templates), which processes specific temporally structured texts. A logical inference mechanism is used for filling template forms. Only scenariorelevant relations between events are linked in the <b>inference</b> <b>chains.</b> The knowledge base {{plays an important role}} in this process. Some aspects of negation and modalities that occur in the texts are also taken into account. 1...|$|R
30|$|In {{most current}} {{research}} on intelligent network simulation model, a risk strategy is typically adopted {{to handle the}} risk of network uncertainty [23, 24, 25, 26, 27, 28]. In the similar manner, the most important characteristics of the IES are its high-quality performance and adaptive learning. The high-quality performance assures system reliability and timeliness. The adaptive learning allows the IES to update dynamically and learn on its own through its experience. The reliability of the IES depends upon its decision-making process, which can be observed by two specific features, i.e. the capability to explain about the decision taken, {{and to deal with}} uncertainty. The IES is capable to explain how and why a decision was reached. The execution of rules tending towards certain decisions {{can be explained by the}} <b>inference</b> <b>chain</b> or the rule flow. Furthermore, the rule of inference has also been designed to deal with knowledge that is incomplete or not completely certain. A heuristic algorithm is developed to solve the uncertainty problem and generate a suitable decision for the provided facts. This ability of the IES allows it to provide a solution even when the provided information is not 100  % accurate or is incomplete. In this regard, it is also necessary to analyse the level of accuracy of the provided solution. To check the accuracy of the designed algorithm, an experiment was conducted and its level of mode prediction accuracy was analysed and has been reported in an earlier study by the Authors [22].|$|E
40|$|Logistic smooth {{transition}} autoregressive (LSTAR) models•First •Prev •Next •Last •Go Back •Full Screen •Close •Quit Outline {{of the talk}} • Logistic {{smooth transition}} autoregressive (LSTAR) models • Bayesian <b>inference</b> through Markov <b>Chain</b> Monte Carlo (MCMC) •First •Prev •Next •Last •Go Back •Full Screen •Close •Quit Outline of the talk • Logistic smooth transition autoregressive (LSTAR) models • Bayesian <b>inference</b> through Markov <b>Chain</b> Monte Carlo (MCMC...|$|R
50|$|In {{patients}} {{suffering from}} schizophrenia, grandiose and religious delusions {{are found to}} be the least susceptible to cognitive behavioral interventions. Cognitive behavioral intervention is a form of psychological therapy, initially used for depression, but currently used for a variety of different mental disorders, in hope of providing relief from distress and disability. During therapy, grandiose delusions were linked to patients' underlying beliefs by using <b>inference</b> <b>chaining.</b> Some examples of interventions performed to improve the patient's state were focus on specific themes, clarification of neologisms, and thought linkage. During thought linkage, the patient is asked repeatedly by the therapist to explain his/her jumps in thought from one subject to a completely different one.|$|R
5000|$|In {{situations}} in which user needs are known and distributed information resources are well described, this approach can be highly effective; in situations that are not foreseen and that bring together an unanticipated array of information resources, the Google approach is more robust. Furthermore, the Semantic Web relies on <b>inference</b> <b>chains</b> that are more brittle; a missing element of the chain results in a failure to perform the desired action, while the human can supply missing pieces in a more Google-like approach. … cost-benefit tradeoffs can work in favor of specially-created Semantic Web metadata directed at weaving together sensible well-structured domain-specific information resources; close attention to user/customer needs will drive these federations {{if they are to}} be successful.|$|R
5000|$|Cyc {{inference}} engine, {{a forward}} and backward <b>chaining</b> <b>inference</b> engine with numerous specialized modules for high-order logic. (http://research.cyc.com/ ResearchCyc) (http://opencyc.org/ OpenCyc) ...|$|R
50|$|Bayesian {{inference}} using Gibbs sampling (BUGs) is {{a software}} package for performing Bayesian <b>inference</b> using Markov <b>chain</b> Monte Carlo (based on Gibbs sampling).|$|R
50|$|KEE also {{includes}} a frame-based rule system. Rules themselves are frames in the KEE knowledge base. Both forward and backward <b>chaining</b> <b>inference</b> is available.|$|R
40|$|User-supplied content—in {{the form}} of photos, videos, and text—is a crucial {{ingredient}} to many web sites and services today. However, many users who provide content do not realize that their uploads may be leaking personal information in forms hard to intuitively grasp. Correlation of seemingly innocuous information can create <b>inference</b> <b>chains</b> that tell much more about individuals than {{they are aware of}} revealing. We contend that adversaries can system-atically exploit such relationships by correlating information from different sources in what we term global inference attacks: assem-bling a comprehensive understanding from individual pieces found at a variety of locations, Sherlock-style. Not only are such attacks already technically viable given the capabilities that today’s mul-timedia content analysis and correlation technologies readily pro-vide, but we also find business models that provide adversaries with powerful incentives for pursuing them. 1...|$|R
40|$|Why do some {{individuals}} pick up arms {{as opposed to}} others who live under the same conditions? Environmental and group theories fail to differentiate between these individuals. In response, we apply the cognitive mapping approach and model violence as decisions based on chains of beliefs about various types of factors, including state aggression, access to violent groups, religion, and personal charac-teristics. Based on a double-paired comparison, data are constructed from ethno-graphic interviews with Muslim and non-Muslim individuals engaging in violent and nonviolent activity in authoritarian and democratic states—Egypt and Germany. The analysis develops a computational model formalizing the cognitive maps into Bayesian networks. In 477, 604 runs, the model (1) identifies the beliefs connected to decisions, (2) traces <b>inference</b> <b>chains</b> antecedent to decisions, and (3) explores counterfactuals. This suggests that both violent and nonviolent activities are responses to state aggression, and not to Islam, group access, or personal characteristics...|$|R
40|$|Abstract. ConceptNet is a {{very large}} {{semantic}} network of commonsense knowledge suitable for making various kinds of practical inferences over text. ConceptNet captures a wide range of commonsense concepts and relations like those in Cyc, while its simple semantic network structure lends it an ease-of-use comparable to WordNet. To meet the dual challenge of having to encode complex higher-order concepts, and maintaining ease-of-use, we introduce a novel use of semi-structured natural language fragments as the knowledge representation of commonsense concepts. In this paper, we present a methodology for reasoning flexibly about these semi-structured natural language fragments. We also examine the tradeoffs associated with representing commonsense knowledge in formal logic versus in natural language. We conclude that the flexibility of natural language makes it a highly suitable representation for achieving practical inferences over text, such as context finding, <b>inference</b> <b>chaining,</b> and conceptual analogy. 1 What is ConceptNet? ConceptNet (www. conceptnet. org) is the largest freely available, machine-useabl...|$|R
50|$|GNU MCSim is a {{suite of}} {{simulation}} software. It allows one to design one's own statistical or simulation models,perform Monte Carlo simulations, and Bayesian <b>inference</b> through Markov <b>chain</b> Monte Carlo simulations.|$|R
50|$|Because {{the list}} of goals determines which rules are {{selected}} and used, this method is called goal-driven, in contrast to data-driven forward-chaining <b>inference.</b> The backward <b>chaining</b> approach is often employed by expert systems.|$|R
50|$|Drools is a {{business}} rule management system (BRMS) with a forward and backward <b>chaining</b> <b>inference</b> based rules engine, more correctly known as a production rule system, using an enhanced implementation of the Rete algorithm.|$|R
40|$|This study {{investigated}} how domain knowledge, about diabetes, influences {{the process and}} outcome of answering complex questions using the internet. The internet has become {{an important source of}} knowledge for people seeking health information about diseases. People with chronic diseases often need {{a great deal of information}} for selfmanagement and have emerging needs for new information. Participants in our exploratory study were 8 people with diabetes and 2 without. An initial interview identified individuals with high versus low knowledge about diabetes. We then traced the activity of individuals as they used the internet to answer questions about diabetes. Questions were designed to be difficult, require reasoning, and lack a single, integrated source with a packaged answer. Here we report on case analyses of one individual with high and one with low domain knowledge. Domain knowledge influenced activity in multiple respects, including initial orienting to the task and supplying facts needed in <b>inference</b> <b>chains.</b> How do people use their existing knowledge to find new knowledge? People often pose and seek t...|$|R
40|$|In this paper, a {{new form}} of {{generalized}} additive models is proposed. The proposed models consist of a distribution function of the mean response and a weighted linear combination of distribution functions of covariates. This form does not make any structural problems on linking the mean response and covariates. Markov chain Monte Carlo methods are used to estimate the parameters within a Bayesian framework. Bayesian <b>inference</b> Markov <b>chain</b> Monte Carlo Beta mixtures Parametric transformation...|$|R
40|$|ABSTRACT. We {{prove that}} for any finite {{deduction}} structure {{there exists a}} unique concise-widest chain-preserved split. Based on this result, we propose a logical splitting strategy which enables an agent to split its belief structure such that all the original <b>inference</b> <b>chains</b> can be preserved. The significance of such logical splitting at least is four-fold: (1) It {{can be used by}} an agent to separate its concerns appropriately, or even create smaller and smarter clones which could save time and efforts in their deliberation computing; (2) It will enable an agent to adjust its concerns dynamically based on its recognition of the current situation, which may further enable the agent to make rapid and rational decisions when situation shifting occurs; (3) In periodic team synchronization (PTS) domains where agents can periodically synchronize their behaviors with no restriction on communication, it can be employed as a critical process for maintaining the shared mental models among a team of agents; and (4) It is helpful in alleviating the complexity of reasoning about relevant information, and in better anticipating both explicit and implicit information needs based on the recognition of the current situation...|$|R
50|$|Julian Ernst Besag FRS (26 March 1945 - 6 August 2010) was a British {{statistician}} known chiefly for {{his work}} in spatial statistics (including its applications to epidemiology, image analysis and agricultural science), and Bayesian <b>inference</b> (including Markov <b>chain</b> Monte Carlo algorithms).|$|R
40|$|Flood {{water in}} Lake Manapouri is {{released}} according to strictly formulated flood rules. B e Real-time Flood Assistant {{is an expert}} system which incorporates Lake Manapouri flood rules and the txperience of the control room operators at Transpower NZ Ltd. {{to assist them in}} the release of flood water. The expert system is being developed in Level 5 Object. The program is mainly an event-driven system based on forward <b>chaining</b> <b>inferencing</b> in conjunction with object oriented methods. Backward <b>chaining</b> <b>inferencing</b> is used only {{at the beginning of the}} program to establish initial data. W t h the use of built-in clocks, the expert system runs in real-time, assisting the operators wit?i the release decisions which are made about every 1 1 /...|$|R
40|$|The {{ability to}} reason with natural {{language}} is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i. e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82 % of generated sentences are correct, an improvement of 10. 3 % over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language <b>inference</b> <b>chains</b> {{that can be used}} to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence...|$|R
40|$|The paper {{introduces}} {{the idea of}} Bayesian quantile regression employing a likelihood function {{that is based on}} the asymmetric Laplace distribution. It is shown that irrespective of the original distribution of the data, the use of the asymmetric Laplace distribution is a very natural and effective way for modelling Bayesian quantile regression. The paper also demonstrates that improper uniform priors for the unknown model parameters yield a proper joint posterior. The approach is illustrated via a simulated and two real data sets. Asymmetric Laplace distribution Bayesian <b>inference</b> Markov <b>chain</b> Monte Carlo methods Quantile regression...|$|R
40|$|An {{essential}} step {{in understanding}} connected discourse {{is the ability}} to link the meanings of successive sentences together. Given a growing database to which new sentence meanings must be linked, which out of many possible <b>inference</b> <b>chains</b> will succeed? To which items already in a data base is a new item relevent? To assure easy understandability of text the amount of processing time spent on unsuccessful linkage attempts must be reduced. This paper develops a preliminary theory of discourse structure. Several newspaper articles were examined in the light of this theory. Two examples were worked out in detail to explore how a hypothetical discourse understander might use the model of discourse structure to represent knowledge gained from processing text. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. It {{was supported in part by}} the National Science Foundation under grant C 40708 X and in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N 00014 - 75 -C- 0643. The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the National Science Foundation or the United States Government. MIT Artificial Intelligence Laboratory Department of Defense Advanced Research Projects Agency National Science Foundatio...|$|R
40|$|Hierarchical Bayesian {{models are}} a {{mainstay}} of the machine learning and statistics communities. Exact posterior inference in such models is rarely tractable, so researchers and practitioners must usually resort to approximate inference methods. Perhaps the most popular class of approximate posterior <b>inference</b> algorithms, Markov <b>Chain</b> Monte Carlo (MCMC) methods offer schemes fo...|$|R
40|$|In this chapter, {{we review}} basic {{concepts}} from probability theory and computational statistics that are fundamental to evolutionary genomics. We provide {{a very basic}} introduction to statistical modeling and discuss general principles, including maximum likelihood and Bayesian <b>inference.</b> Markov <b>chains,</b> hidden Markov models, and Bayesian network models are introduced in more detail as they occur frequently and in many variations in genomics applications. In particular, we discuss efficient inference algorithms and methods for learning these models from partially observed data. Several simple examples are given throughout the text, some of which point to models that are discussed in more detail in subsequent chapters...|$|R
40|$|Cataract {{is the one}} of eyes diseases. The {{disease is}} number one cause of human {{blindness}} in the world. It is marked by the opacity in the lens of human’s eyes that is previously clear. There are several types of cataract such as congenital cataracts, juvenile, senile, traumatic, and complication. The method to diagnose the cataract is inference method, namely forward chaining. In applying the <b>inference</b> forward <b>chaining</b> method, it is started by analyzing the indication of cataract and finally the conclusion as a solution. Research result shows that the cataract is caused by traumatic and complication can occur at all ages of humans. So that, {{it can be concluded}} that the types of cataract which is being studied are congenital cataracts, traumatic congenital, complication congenital, juvenile cataracts, traumatic juvenile, complication juvenile and senile cataract, traumatic senile, complication senile. The use of forward <b>chaining</b> <b>inference</b> method in diagnosing the cataract is relatively similar with the expert’s diagnosing (doctor). Furthermore, in order to achieve the testing result of diagnose which will be more accurate and exactly, it needs to be examined by using many data...|$|R
