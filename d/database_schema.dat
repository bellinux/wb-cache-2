1383|678|Public
25|$|This {{software}} {{can be used}} to access the public MySQL database, avoiding the need to download enormous datasets. The users could even choose to retrieve data from the MySQL with direct SQL queries, but this requires an extensive knowledge of the current <b>database</b> <b>schema.</b>|$|E
25|$|Structural quality {{analysis}} and measurement is performed through {{the analysis of}} the source code, the architecture, software framework, <b>database</b> <b>schema</b> in relationship to principles and standards that together define the conceptual and logical architecture of a system. This is distinct from the basic, local, component-level code analysis typically performed by development tools which are mostly concerned with implementation considerations and are crucial during debugging and testing activities.|$|E
25|$|A central {{feature of}} SQL Server Management Studio is the Object Explorer, {{which allows the}} user to browse, select, and act upon any of the objects within the server. It {{can be used to}} {{visually}} observe and analyze query plans and optimize the database performance, among others. SQL Server Management Studio {{can also be used to}} create a new database, alter any existing <b>database</b> <b>schema</b> by adding or modifying tables and indexes, or analyze performance. It includes the query windows which provide a GUI based interface to write and execute queries.|$|E
50|$|In {{software}} engineering, <b>schema</b> migration (also <b>database</b> migration, database change management) {{refers to}} the management of incremental, reversible changes to relational <b>database</b> <b>schemas.</b> A schema migration is performed on a database whenever {{it is necessary to}} update or revert that <b>database's</b> <b>schema</b> to some newer or older version.|$|R
5000|$|Concrete {{things such}} as {{programming}} language statements, <b>database</b> <b>schemas,</b> and ...|$|R
40|$|Two {{approaches}} for integrating heterogeneous <b>database</b> <b>schemas</b> {{based on the}} concept of a global schema are surveyed. The first approach relies heavily on manual resolution of various incompatibilities, particularly semantic, that exist among different local <b>database</b> <b>schemas.</b> The second handles this resolution automatically. In this work, usage of an on-line general taxonomy as a tool for resolving various incompatibilities among component schemas is introduced. This tool is a part of an on-going research for developing a new methodology for integrating heterogeneous <b>database</b> <b>schemas</b> using an on-line general taxonomy. Many advantages offered by the tool are discussed...|$|R
25|$|Visual Studio {{includes}} a code editor supporting IntelliSense (the code completion component) {{as well as}} code refactoring. The integrated debugger works both as a source-level debugger and a machine-level debugger. Other built-in tools include a code profiler, forms designer for building GUI applications, web designer, class designer, and <b>database</b> <b>schema</b> designer. It accepts plug-ins that enhance the functionality at almost every levelâ€”including adding support for source control systems (like Subversion) and adding new toolsets like editors and visual designers for domain-specific languages or toolsets for {{other aspects of the}} software development lifecycle (like the Team Foundation Server client: Team Explorer).|$|E
2500|$|Content Construction Kit (CCK): allows site {{administrators to}} {{dynamically}} create content types by extending the <b>database</b> <b>schema.</b> [...] "Content type" [...] describes {{the kind of}} information. Content types include, {{but are not limited}} to, events, invitations, reviews, articles, and products. The CCK Fields API is in Drupal core in Drupal 7.|$|E
2500|$|SQLCMD is {{a command}} line {{application}} {{that comes with}} Microsoft SQL Server, and exposes the management features of SQL Server. It allows SQL queries to be written and executed from the command prompt. It can also act as a scripting language to create and run a set of SQL statements as a script. Such scripts are stored as a [...]sql file, and are used either for management of databases or to create the <b>database</b> <b>schema</b> during the deployment of a database.|$|E
40|$|DLR is an {{expressive}} Description Logic (DL) with n-ary relations, particularly {{suited for}} modeling <b>database</b> <b>schemas</b> and queries. Although DLR has constituted {{one of the}} crucial steps for applying DL technology to data management, there is one important aspect of <b>database</b> <b>schemas</b> that DLs, including DLR, do not capture yet, namely the notion of key...|$|R
40|$|Abstract. In this paper, we briefly {{describe}} {{currently used}} methods for generating relational <b>database</b> <b>schemas,</b> their limitations and drawbacks, and propose a method which advances them by generating full-fledged relational <b>database</b> <b>schemas</b> from a conceptual model. The proposed method consists of metamodel-based ant pattern-based transformations. Principles of creating pattern-based transformations are defined for transformation of OCL expressions to corresponding SQL code...|$|R
40|$|<b>Database</b> <b>schemas,</b> in many organizations, are {{considered}} one of the critical assets to be protected. From <b>database</b> <b>schemas,</b> it is not only possible to infer the information being collected but also the way organizations manage their businesses and/or activities. One of the ways to disclose <b>database</b> <b>schemas</b> is through the Create, Read, Update and Delete (CRUD) expressions. In fact, their use can follow strict security rules or be unregulated by malicious users. In the first case, users are required to master <b>database</b> <b>schemas.</b> This can be critical when applications that access the database directly, which we call database interface applications (DIA), are developed by third party organizations via outsourcing. In the second case, users can disclose partially or totally <b>database</b> <b>schemas</b> following malicious algorithms based on CRUD expressions. To overcome this vulnerability, we propose a new technique where CRUD expressions cannot be directly manipulated by DIAs any more. Whenever a DIA starts-up, the associated database server generates a random codified token for each CRUD expression and sends it to the DIA that the database servers can use to execute the correspondent CRUD expression. In order to validate our proposal, we present a conceptual architectural model and a proof of concept...|$|R
2500|$|Performance/scalability: In 2008, {{performance}} tests between Drupal 6.1 and Joomla 1.5 demonstrated that Drupal's pages were delivered [...] "significantly faster" [...] {{than those of}} Joomla. Despite this, arguments over speed persist. Drupal {{is likely to be}} slower than a special-purpose application for a given task. For example, WordPress typically outperforms Drupal as a single-user blogging tool. Drupal positions itself for broader applications requirements that are outside the scope of more narrowly focused applications. Drupal offers caching to store various page elements, the use of which resulted in a 508% improvement in one benchmark. When using Drupal's default Page Cache mechanism, the cached pages are delivered only to anonymous users, so contributed modules must be installed to allow caching content for logged in users. Like performance, scalability (the ability to add servers to handle growing numbers of visitors with consistent response) can become a concern on large, interactive sites. MySQL's query caching can help reduce the load on the database server caused by Drupal's high query rate. Drupal caches <b>database</b> <b>schema</b> metadata as well as elements such as blocks, forms and menus. Drupal 7 increases performance in database queries and reduces PHP code usage.|$|E
2500|$|SQL Server {{includes}} a cost-based query optimizer which tries to optimize on the cost, {{in terms of}} the resources it will take to execute the query. Given a query, then the query optimizer looks at the <b>database</b> <b>schema,</b> the database statistics and the system load at that time. It then decides which sequence to access the tables referred in the query, which sequence to execute the operations and what access method to be used to access the tables. For example, if the table has an associated index, whether the index should be used or not: if the index is on a column which is not unique for most of the columns (low [...] "selectivity"), it might not be worthwhile to use the index to access the data. Finally, it decides whether to execute the query concurrently or not. While a concurrent execution is more costly in terms of total processor time, because the execution is actually split to different processors might mean it will execute faster. Once a query plan is generated for a query, it is temporarily cached. For further invocations of the same query, the cached plan is used. Unused plans are discarded after some time.|$|E
5000|$|If any {{elements}} of the <b>database</b> <b>schema</b> are dropped then the <b>database</b> <b>schema</b> is not ideal.|$|E
5000|$|Federated Archaeological Information Management System - {{generation}} of <b>database</b> <b>schemas</b> and interoperability with Android field data collection system ...|$|R
40|$|We are {{developing}} a software tool to support schema matching for data transformations. Schema match-ing {{is the process of}} finding relationships between com-ponents of two given <b>database</b> <b>schemas.</b> The tool is unique in that it first extracts conceptual schemas from the two <b>database</b> <b>schemas</b> and allows the user to use the extracted conceptual schemas as clues to perform schema matching. This paper overviews the tool and explains its implementation. ...|$|R
40|$|In this paper, {{we propose}} an {{approach}} and {{a framework for}} an a priori change impact analysis of <b>database</b> <b>schemas,</b> in distributed environment. The approach {{is based on a}} model, that describes program source codes and <b>database</b> <b>schemas</b> as software components linked by meaningful relationships. This model takes into account the software components for both centralized and distributed database applications. We deal with the distributed issue, in accordance with the Object Database Management Group and the Object Management Group specifications. The change impact analysis is done using, a Knowledge Based System, that includes impact propagation rules, in a distributed way. This is achieved by proposing a framework, that implements our model, in order to simulate the evolution of CORBA-based distributed and heterogeneous <b>database</b> <b>schemas...</b>|$|R
5000|$|High {{quality of}} {{database}} design: In evolutionary database design, the developer makes small {{changes to the}} <b>database</b> <b>schema</b> in an incremental manner and this achieves a highly optimized <b>database</b> <b>schema.</b>|$|E
50|$|When {{developing}} {{software applications}} {{backed by a}} database, developers typically develop the application source code in tandem with an evolving <b>database</b> <b>schema.</b> The code typically has rigid expectations of what columns, tables and constraints {{are present in the}} <b>database</b> <b>schema</b> whenever it needs to interact with one, so only the version of <b>database</b> <b>schema</b> against which the code was developed is considered fully compatible with that version of source code.|$|E
5000|$|The {{process of}} {{database}} refactoring is {{the act of}} applying database refactorings to evolve an existing <b>database</b> <b>schema</b> (database refactoring is a core practice of evolutionary database design). You refactor a <b>database</b> <b>schema</b> for one of two reasons: to develop the schema in an evolutionary manner in parallel with the evolutionary design {{of the rest of}} your system or to fix design problems with an existing legacy <b>database</b> <b>schema</b> ...|$|E
5000|$|Oracle {{data models}}: <b>database</b> <b>schemas,</b> {{offering}} pre-built data models with database analytics and business intelligence capabilities for specific industries.|$|R
5000|$|Compare Databases {{enables you}} to compare two <b>database</b> <b>schemas</b> to {{identify}} change and easily move it to your change log.|$|R
5000|$|Similar to a {{relational}} database model, but objects, classes and inheritance are directly supported in <b>database</b> <b>schemas</b> {{and in the}} query language.|$|R
50|$|The {{terminology}} {{model is}} a means for subject matter experts to express their knowledge about the subject in subject specific terms. Since the terminology model is structured rather similar to an object-oriented <b>database</b> <b>schema,</b> is can be transformed without loss of information into an object-oriented <b>database</b> <b>schema.</b> Thus, the terminology {{model is a}} method for problem analysis {{on the one side}} and a mean of defining <b>database</b> <b>schema</b> on the other side.|$|E
50|$|MantisBT {{maintains}} a database upgrade path between versions by modifying the <b>database</b> <b>schema</b> incrementally. During {{the installation of}} MantisBT, <b>database</b> <b>schema</b> modifications are replayed {{all the way back}} from early versions of MantisBT to reach the latest state.|$|E
50|$|Some {{frameworks}} minimize {{web application}} configuration {{through the use}} of introspection and/or following well-known conventions. For example, many Java frameworks use Hibernate as a persistence layer, which can generate a <b>database</b> <b>schema</b> at runtime capable of persisting the necessary information. This allows the application designer to design business objects without needing to explicitly define a <b>database</b> <b>schema.</b> Frameworks such as Ruby on Rails can also work in reverse, that is, define properties of model objects at runtime based on a <b>database</b> <b>schema.</b>|$|E
40|$|Project "Computer aided {{design of}} {{database}} relation schemes" shows {{the importance and}} problems with designing nowadays <b>Database</b> <b>schemas</b> and gives ideas on possible solutions to these problems. Its main purpose is to describe a solution on optimizing Database designing that is transforming <b>database</b> <b>schemas</b> into tables. These tables should also fulfill the requirements based {{on the need for}} data consistency and database maintenance. Main part of this project is an application, which implements algorithms for solving these problems in an efficient way...|$|R
40|$|Hereby I {{certify that}} the thesis {{has been written}} by me and I have not used any {{auxiliary}} sources for my thesis work other than those have been cited. i Signature of Author Description Logics (DLs) are a family of logic-based knowledge representation formalisms for representing and reasoning about conceptual knowledge. To represent and reason about concrete qualities of real-world entities such as size, duration, or amounts, DLs are equipped with concrete domains. Interestingly, DLs and DLs with concrete domains are useful in many applications, such as modelling <b>database</b> <b>schemas</b> and the semantic web. Recently, {{it has been suggested}} that the expressive power of DLs with concrete domains can be further extended by adding database-like key constraints. In <b>database</b> <b>schemas,</b> key constraints can be a source of additional inconsistencies, which DLs used in reasoning about <b>database</b> <b>schemas</b> should be able t...|$|R
50|$|A data {{definition}} language or data description language (DDL) is a syntax {{similar to a}} computer programming language for defining data structures, especially <b>database</b> <b>schemas.</b>|$|R
5000|$|It validates {{database}} tables, data models, <b>database</b> <b>schema</b> etc.|$|E
50|$|The <b>database</b> <b>schema</b> is {{published}} and publicly viewable at http://opendental.com/manual/programmingresources.html.|$|E
5000|$|... <b>{{database}}</b> <b>schema</b> migration, {{also from}} one database system to another ...|$|E
40|$|Abstract. This paper {{presents}} {{two basic}} operations, Separation and Data feder-ation, for <b>database</b> <b>schemas</b> {{that are used}} in database mapping systems and ex-plains why functorial semantics for database mappings need a new base category instead of usual Set category. A definition of graph G for a <b>schema</b> <b>database</b> map...|$|R
40|$|Abstract. Data {{can contain}} {{information}} {{at different levels}} of granularities but this metadata is generally left implicit in the data model. If we {{want to take advantage of}} different levels of granularity when integrating data, we need to first extend <b>database</b> <b>schemas</b> to include granularity information. In this article we (i) provide a multi-granular domain schema that is used in the formalization of <b>database</b> <b>schemas</b> so that each attribute is assigned a certain granularity; and (ii) explore the issues when integrating data at different granularities and suggest possible global schemas and instances. ...|$|R
40|$|The {{characterization}} of an algorithmâ€™s worst-case time complexity is useful because it succinctly captures how its runtime will grow as the input size becomes arbitrarily large. However, for certain algorithmsâ€”such as those per-forming search-based test data generationâ€”a theoretical analysis to determine worst-case time complexity is diffi-cult to generalize and thus not often {{reported in the}} liter-ature. This paper introduces a framework that empirically determines an algorithmâ€™s worst-case time complexity by doubling {{the size of the}} input and observing the change in runtime. Since the relational database is a centerpiece of modern software and the <b>databaseâ€™s</b> <b>schema</b> is frequently untested, we apply the doubling technique to the domain of data generation for relational <b>database</b> <b>schemas,</b> a field where worst-case time complexities are often unknown. In addition to demonstrating the feasibility of suggesting the worst-case runtimes of the chosen algorithms and configu-rations, the results of our study reveal performance trade-offs in testing strategies for relational <b>database</b> <b>schemas.</b> ...|$|R
