25|10000|Public
5000|$|The {{algorithm}} takes a workflow log [...] as <b>input</b> <b>and</b> <b>results</b> in a workflow net being constructed.|$|E
5000|$|Green Kenue (formerly EnSim Hydrologic) is an {{advanced}} data preparation, analysis, and visualization tool for hydrologic modellers. It is a Windows/OpenGL-based graphical user interface, integrating environmental databases and geo-spatial data with model <b>input</b> <b>and</b> <b>results</b> data. Green Kenue provides complete pre- and post-processing for the WATFLOOD and HBV-EC hydrologic models.Also included is a 1D [...] "reach scale" [...] unsteady hydrodynamic flow solver, Gen1D.|$|E
30|$|Learning {{applications}} need data <b>input</b> <b>and</b> <b>results.</b> In this application, {{the input}} data are the projective integrals {{of the image}} {{and the results are}} whether there is a crack or not and the type of the crack, if exists.|$|E
5000|$|Similar, {{we could}} obtain all the <b>inputs</b> <b>and</b> <b>results</b> of the 5 PEs: ...|$|R
5000|$|A {{description}} of how local thermal discomfort will be addressed, including calculation methods, <b>inputs</b> <b>and</b> <b>results</b> ...|$|R
5000|$|This {{recursive}} definition {{consists of}} a function signature giving the types of the <b>input</b> <b>and</b> <b>result</b> <b>and</b> a function body. An implicit definition of the same function might take the following form: ...|$|R
40|$|Hazard {{resistant}} {{building materials}} are not adequately represented by current LCA approaches, {{which do not}} account for environmental benefits of the avoided losses (e. g. avoided waste, avoided materials for repairs) promised by hazard resistant materials. The goal {{of this study was}} to encourage a more complete understanding of materials’ environmental impacts by developing a framework to include hazard related damages in LCA. Catastrophe modeling is a well-developed tool used by the insurance industry to assess the probability of hazard and quantify related impacts. These models were studied to identify required input data as well as the format of output results. The <b>input</b> <b>and</b> <b>results</b> from the catastrophe models were then compared with the required <b>input</b> <b>and</b> <b>results</b> for prominent software tools used in ISO 14040 compliant LCAs. Through this comparison, an approach was identified for incorporating the catastrophe modeling results into LCAs for building materials. A hazard related damage inclusive LCA, H-LCA, was developed to combine data set...|$|E
30|$|For the {{proposed}} OLVFOFR method, results are depicted in Figs.  2 h to 5 h. There are no viewing artifacts found. The background brightness is amplified, {{making it easier}} to detect wear particles in the foreground. Furthermore, backgrounds remain more even. This method performs satisfactorily against other methods being compared. In particular, for complex image contents as shown in Figs.  2 h to 5 h, the contrast of wear particles is higher than the <b>input</b> <b>and</b> <b>results</b> from all other approaches.|$|E
40|$|The {{details of}} the project {{planning}} and project management work done on the Baseball II-T experiment are reviewed. The LLL Baseball program is a plasma confinement experiment accomplished with a superconducting magnet {{in the shape of}} a baseball seam. Both project planning and project management made use of the Critical Path Management (CPM) computer code. The computer code, <b>input,</b> <b>and</b> <b>results</b> from the project planning and project management runs, and the cost and effectiveness of this method of systems planning are discussed. (auth...|$|E
50|$|Calculators {{have been}} used since ancient times and until the advent of {{software}} calculators they were physical, hardware machines. The most recent hardware calculators are electronic hand-held devices with buttons for digits and operations, and a small window for <b>inputs</b> <b>and</b> <b>results.</b>|$|R
50|$|Data-driven {{testing is}} the {{creation}} of test scripts to run together with their related data sets in a framework. The framework provides re-usable test logic to reduce maintenance and improve test coverage. <b>Input</b> <b>and</b> <b>result</b> (test criteria) data values can be stored in one or more central data sources or databases, the actual format and organisation can be implementation specific.|$|R
40|$|This {{document}} {{describes the}} actual {{state of the}} estimation of the TerraSAR-X performance. The assumptions and parameters are here reported. There are previous optimizations introduced in the first chapters <b>and</b> follow the <b>inputs</b> <b>and</b> <b>results.</b> In the first stage, the performance estimator is made for the ideal case and in a second stage the errors concerning the instrument and ground segment are included...|$|R
40|$|Manual game content {{creation}} is an increasingly laborious task; with each advance in graphics hardware, {{a higher level}} of fidelity and detail is achievable and, therefore, expected. Although numerous automatic (e. g. procedural) content generation algorithms and techniques have been developed over the years, their application in both games and simulations is not widespread. What lacks is a unifying modeling framework that combines these techniques in a usable manner. We propose to develop a new, high-level framework for automatic generation of virtual worlds (e. g. game levels, simulation terrain) that requires intuitive user <b>input</b> <b>and</b> <b>results</b> in a rich 3 D terrain mode...|$|E
40|$|Abstract:- In {{this paper}} we present our initial {{experiences}} {{with the development of}} an environment where educational simulations could be executed in real-time over the world wide web using the power and versatility provided by the software package MATLAB and the web toolbox. The developed tools run on a Sun Ultra- 2 server. The developed tools focused on simulations for linear dynamic systems. The user interface is graphical in nature and through the ubiquitous web browser both for data <b>input</b> <b>and</b> <b>results</b> presentation. The initial experiments demonstrated the ease of implementation and use. Examples are presented that demonstrate the developed tools...|$|E
40|$|In {{the first}} part of this work, a model of {{optimization}} was presented that minimizes the consumption of the hydrogen of a refinery. In this second part, the model will be augmented to take into account the length of the pipelines, the addition of purification units and the installation of new compressors, all features of industrial real networks. The model developed was implemented in the LINGO software environment. For data <b>input</b> <b>and</b> <b>results</b> output, an Excel spreadsheet was developed that interfaces with LINGO. The model is currently being used in YPFLuján de Cuyo refinery (Mendoza, Argentina...|$|E
5000|$|... #Caption: The {{appropriate}} weights {{are applied}} to the <b>inputs,</b> <b>and</b> the <b>resulting</b> weighted sum passed to a function that produces the output o.|$|R
40|$|State-of-the art {{techniques}} for automated test generation focus on generating executions that cover program behavior. As {{they do not}} generate oracles, {{it is up to}} the developer to figure out what a test does and how to check the correctness of the observed behavior. In this paper, we present an approach to generate parameterized unit tests—unit tests containing symbolic pre- and postconditions char-acterizing test <b>input</b> <b>and</b> test <b>result.</b> Starting from concrete <b>inputs</b> <b>and</b> <b>results,</b> we use test generation and mutation to systematically generalize pre- and postconditions while simplifying the compu-tation steps. Evaluated on five open source libraries, the generat-ed parameterized unit tests are (a) more expressive, characterizing general rather than concrete behavior; (b) need fewer computation steps, making them easier to understand; and (c) achieve a higher coverage than regular unit tests...|$|R
30|$|Recognition of <b>input</b> {{parameters}} <b>and</b> output <b>results.</b>|$|R
40|$|A Self-Tunable Dynamic Vibration Absorber (DVA) is {{proposed}} here {{as an alternative}} device for Parkinson's Disease's (PD) Tremor suppression. A hydraulic system that enables the DVA's self-tuning using a control law was designed and implemented. Two different configurations for the DVA, un-dampen and dampen, were simulated using PD's tremors collected by sensors in patients as the system's <b>input</b> <b>and</b> <b>results</b> show oscillations reductions {{in the order of}} 50 % and 80 %, for the un-dampen and the dampen configuration respectively. The results are promising and demonstrate the self-tunable DVA's potential as a wearable biomedical device for PD's tremor suppression and for other type of neurological tremors. This work has been partially financed by national funds by COMPETE and FCT in the framework of the project FCOMP- 01 - 0124 -FEDER- 022674, and FP 7 Marie Curie ITN - NETT (project no 289146) and by FEDER funds...|$|E
40|$|This study {{deals with}} {{numerical}} stability analysis for inde-pendent modal vibration suppression of a fluid-conveying pipe using a piezoelectric inertia actuator (PIA). The stability {{issue of the}} approach as proposed by the pioneer developers is ad-dressed. The approach utilizes an infinite control weight for one component of the modal control <b>input</b> <b>and</b> <b>results</b> in a severe control spillover problem for the complex mode con-trolled, easily leading to closed loop instability even for open loop stable systems. The stability of the system depends on how the left eigenvector is normalized for transforming the original coupled equations to the decoupled ones in the modal space. A novel approach by rotating the left eigenvector on the complex plane is systematically examined to define the region of stabil-ity in this work. A feasible modal control design for systems possessing complex modes can thus be accomplished using the proposed approach. I...|$|E
40|$|This paper {{presents}} the applications of Geographic Information System (GIS) {{in three different}} Land Use Planning (LUP) approaches: The participatory LUP (PLUP) which strongly considers the local people perceptions for land utilizations, the guidelines for LUP by FAO enhanced with multi-criteria evaluation (FAO-MCE), and the LUP and analysis system (LUPAS) using interactive multiple-goal linear programming. GIS {{plays an important role}} in the application of these approaches. In PLUP, GIS help to integrate the acquired spatial and attribute data from farmer discussions, and to analyzes the changes not only in biophysical, land cover but also in farmers' perception on land utilizations. In FAO-MCE, GIS was used to combine biophysical and socio-economic characteristics and to perform multi-criteria evaluation. In LUPAS, an optimization model was developed. The model is linked with a GIS for data <b>input</b> <b>and</b> <b>results</b> presentation. The land use planners can use the model to explore different land use scenarios with different objectives and constraints, both biophysically and socio-economicall...|$|E
30|$|We can {{conclude}} that contracts offer better opportunities to honey producers than cooperatives. Contracts facilitate the acquisition of technology <b>and</b> <b>inputs</b> <b>and</b> <b>result</b> in better market conditions. They facilitate closer communication between producers and contractors (processors or retailers), and reduce transaction costs in multiple ways. While cooperatives also entail the potential to reduce transaction costs <b>and</b> <b>result</b> in better market conditions, in the honey sector in Tigray they are not successful in doing so, likely because of problems with management and incentives of producers.|$|R
40|$|The {{prices of}} {{agricultural}} products mirror {{not only the}} agriculture situation, its <b>input</b> <b>and</b> <b>result</b> but also the importance that the state gives to this basic economy component. In the Romanian agriculture, {{the role of the}} state is to make flexible investments, this being a direct consequence of the Romanian’s status as member of the European Union. The prices are an important factor through which agriculture can win the position of a competitor on the European and international market...|$|R
40|$|Scientific {{indicators}} {{arise from}} the measurement of <b>inputs</b> <b>and</b> <b>results</b> of the scientific institution. Scientometrics develops methodologies for set up those indicators based on interdisplinary technics from the economics, statistics, management and documentation. The methodologies that are internationally accepted (Frascati Manual, Oslo Manual and Canberra Manual) constitute the classical references to measure the econocmic <b>inputs</b> <b>and</b> <b>results,</b> {{as well as the}} technological results of the research and development. However, there is no international consensus about how to measure and evaluate intellectual and academic production in the ways in which it is expressed in the editorial system, either in the interpretation of its impacts e influences. Defining bibliometrics, scientometrics and infometrics, as well as their scope and application, this paper presents alternative views to interpret current scientometric indicators, which greatly outcome from compilation of the Citation Index, published by the Institute for Scientific Information, and other similar databases. Particularly, an hypothesis is presented aiming at explaining the bias of the Citation Index in favour of the publications that belong to the mainstream of the developed countries, against those publications of similar quality published by Third World countries...|$|R
40|$|Managers {{are often}} {{required}} to quickly and accurately estimate resource needs. At times, additional {{work can be}} absorbed without additional resources. At other times, threshold resource boundaries are exceeded requiring an additional quantum of a specific resource. Cost savings` estimates, resulting from a reduction in efforts, are also increasingly becoming a requirement of today`s managers. The modeling effort described in this paper was designed to estimate instrumentation and manpower resource needs for an analytical laboratory. It was written using only simple spreadsheet software. Analysis can be readily performed {{with a minimum of}} <b>input</b> <b>and</b> <b>results</b> obtained in a matter of minutes. This model has been tuned with many years of empirical data yielding a high degree of capability. The model was expanded to meet other needs. It can be used to justify capital expenditure when the ultimate result is cost savings; to examine procedures and operations for efficiency increases; and for reporting and regulatory compliance. This paper demonstrates that accurate and credible estimates of resource needs can be readily obtained with a minimum of effort or specialized knowledge employing only tools that are readily available in today`s business environment...|$|E
40|$|Building {{regulatory}} {{systems have been}} evolving in recent decades, first with a transition to a functional or performance basis, and more recently {{with the introduction of}} new societal objectives, including those related to sustainability and climate change resiliency. Various policy and technical challenges have been identified with this evolution, including the lack of a common basis for establishing performance expectations, quantified performance metrics, and robust mechanisms to incorporate new objectives in a manner that effectively integrates a diversity of stakeholder <b>input</b> <b>and</b> <b>results</b> in regulatory requirements that do not compete with long-standing objectives. Among the mechanisms being explored to facilitate a managed evolution is the use of risk as a basis for performance, and modifications within the building regulatory environment to enable this. It is posited that framing the building regulatory system as a socio-technical system (STS) will highlight the complex interactions that exist between regulators and the market, the roles stakeholders play in characterizing risk for use in building regulation, and what steps are required to shift to a risk-informed performance-based building regulatory system, taking into account different legal structures and regulatory approaches that exist between jurisdiction...|$|E
40|$|Abstract—The {{retrieval}} of similar {{documents from the}} Web using documents as input instead of key-term queries is not currently supported by traditional Web search engines. One approach for solving the problem consists of fingerprint the document’s content into a set of queries that are submitted {{to a list of}} Web search engines. Afterward, results are merged, their URLs are fetched and their content is compared with the given document using text comparison algorithms. However, the action of requesting results to multiple web servers could take a significant amount of time and effort. In this work, a similarity function between the given document and retrieved results is estimated. The function uses as variables features that come from information provided by search engine results records, like rankings, titles and snippets. Avoiding therefore, the bottleneck of requesting external Web Servers. We created a collection of around 10, 000 search engine results by generating queries from 2, 000 crawled Web documents. Then we fitted the similarity function using the cosine similarity between the <b>input</b> <b>and</b> <b>results</b> content as the target variable. The execution time between the exact and approximated solution was compared. Results obtained for our approximated solution showed a reduction of computational time of 86 % at an acceptable level of precision with respect to the exact solution of the web document retrieval problem...|$|E
30|$|Modeling {{and finding}} the {{relationship}} among <b>input</b> parameters <b>and</b> output <b>results.</b>|$|R
40|$|In this thesis, I {{designed}} and implemented an object-oriented simulation framework which provides {{information that can}} be used to optimize end-system monitoring design. The simu-lation was based on the DEVS formalism which provides a formal representation of discrete event systems capable of mathematical manipulation [8] [9]. The simulation engine is imple-mented in Java. It has a hierarchical structure and is easy to manage and expand. A sample simulation setup, <b>inputs</b> <b>and</b> <b>results</b> were provided show the effectiveness of the simulation framework...|$|R
40|$|This report {{contains}} the technical basis {{in support of}} the DOE?s derivation of Authorized Limits (ALs) for the DOE Paducah C- 746 -U Landfill. A complete description of the methodology, including an assessment of the input parameters, model <b>inputs,</b> <b>and</b> <b>results</b> is provided in this report. This report also provides initial recommendations on applying the derived soil guidelines. The ORISE-derived soil guidelines are specifically applicable to the Landfill {{at the end of its}} operational life. A suggested &#x 27;upper bound&#x 27; multiple of the derived soil guidelines for individual shipments is provided...|$|R
40|$|The {{privacy of}} source code makes, in many situations, {{commercial}} application software unable to academic purposes. Such applicative programs are like “black-boxes”, allowing only data <b>input</b> <b>and</b> <b>results</b> output, {{as a function}} of the graphical user interface manipulation. It strongly hinders their application to education, since users remains alienated of what exactly is performed in the applicative background. A common proposal for overcoming such limitations is the development and usage of free software. The academic community of distinct knowledge areas is, indeed, being benefited by free software development for educational and research purposes. However, Digital Photogrammetry is a field that remains practically unexplored. One exception is the E-FOTO project, which develops the only GNU/GPL environment for research, teaching and learning of Digital Photogrammetry that encompasses an Educational Digital Photogrammetric Workstation (EDPW). E-FOTO project aims at producing and sharing without limitations with the academic community software and documents. This paper is devoted to the pedagogical design of E-FOTO Educational Digital Photogrammetric Workstation, besides the project main achievements and repercussions. Along this paper, it is shown what tasks of the photogrammetric process should be covered by the EDPW, the free electronic version of the only digital photogrammetry book edited in Brazil, the software tutorials and the self-learning conception of E-FOTO. Finally, the future expectations for this project are enumerated. 1...|$|E
40|$|Over {{the last}} decade the amount of {{population}} injured by accident furniture overturning is increased and this trend is expected to continue. Non anchored body motion can be categorized in 6 basic types: rest, slide, rock, slide-rock, free flight and impact. The aim {{of this research is}} to analyze through numerical analysis the behavior of a free-standing rigid body during the phase of rocking motion under dynamic load. The model of free body rocking motion was proposed by G. W. Housner in 1963 first. Later a number of alternative models have been developed opportunely change the original assumption in order to and obtain more realistic results. In this research different models are compared with numerical simulations, using different excitation <b>input</b> <b>and</b> <b>results</b> are shown in the acceleration-frequency plane. These results confirmed the importance of the geometrical shape of the rigid block and the necessity to individuate the risk factors that can cause overturning phenomena. Nonlinear time history analysis using real earthquake floor motions recorded in a real time monitored building in California are used to simulate the behavior of a furniture located inside a multi-story building. Sensitivity analysis is performed in order to find the optimal location of the furniture inside the building. The research goal is to establish how {{to reduce the risk of}} overturning during an earthquake in real structures and to propose a simple and practical procedure to locate the risk area in the building where slender furniture can overturn. Prevention actions are proposed by changing the shape of the furniture, the friction coefficient or by using some appropriate device...|$|E
40|$|This is a {{guide for}} the use of the {{pressure}} disk rotor model that has been placed in the incom-pressible Navier-Stokes code INS 3 D-UP. The pressure disk rotor model approximates a helicop-ter rotor or propeller in a time averaged manner and is intended to simulate the effect of a rotor in forward flight on the fuselage or the effect of a propeller on other aerodynamic components. The model uses a modified actuator disk that allows the pressure jump across the disk to vary with radius and azimuth. The cyclic and collective blade pitch angles needed to achieve a specified thrust coefficient and zero moment about the hub are predicted. The method has been validated with experimentally measured mean induced inflow velocities as well as surface pressures on a generic fuselage. Overset grids, sometimes referred to as Chimera grids, are used to simplify the grid generation process. The pressure disk model is applied to a cylindrical grid which is embed-ded in the grid or grids used for the rest of the configuration. This document will outline the development of the method, and present <b>input</b> <b>and</b> <b>results</b> for a sample case. Nomenclature Note: Unless specified, the following variables are made nondimensional as follows: lengths are made nondimensional by radius, areas are made nondimensional by radius 2, velocities are made nondimensional by U ∞, densities are made nondimensional by ρ ∞, pressures are made nondimensional by ρ∞U, forces are made nondimensional by, and torques are 2 ∞ ρ ∞ (U∞radius) 2 2 radius 3 made nondimensional by ρ∞U ∞. A 0 Collective pitch angle, radian...|$|E
5000|$|In VDM-SL, {{functions}} are defined over the data types defined in a model. Support for abstraction requires {{that it should}} be possible to characterize the result that a function should compute without having to say how it should be computed. The main mechanism for doing this is the implicit function definition in which, instead of a formula computing a result, a logical predicate over the <b>input</b> <b>and</b> <b>result</b> variables, termed a postcondition, gives the result's properties. For example, a function [...] for calculating a square root of a natural number might be defined as follows: ...|$|R
5000|$|Variables are {{set for the}} <b>input</b> {{expressions}} <b>and</b> <b>results.</b> These {{variables are}} also available in the REPL. For example in Common Lisp * refers to the last <b>result,</b> ** <b>and</b> *** to the results before that.|$|R
40|$|The US Department of Energy is {{currently}} considering {{a range of}} technologies for disposition of excess weapon plutonium. Use of plutonium fuel in fission reactors to generate spent fuel is one class of technology options. This report describes the <b>inputs</b> <b>and</b> <b>results</b> of decision analyses conducted to evaluate four evolutionary/advanced and three existing fission reactor designs for plutonium disposition. The evaluation incorporates multiple objectives or decision criteria, and accounts for uncertainty. The {{purpose of the study}} is to identify important and discriminating decision criteria, and to identify combinations of value judgments and assumptions that tend to favor one reactor design over another...|$|R
