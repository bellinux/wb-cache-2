197|1876|Public
5|$|On July10, 2008, {{a tanker}} truck {{full of water}} crashed into a tree along a portion of NY149 between Bay Road (CR7) and Ridge Road (NY9L) that {{features}} a sharp curve. After the first accident occurred at 3:10 a.m., a second tractor-trailer, who had stopped because of the tanker, rolled in reverse, <b>jackknifing</b> across the road and hitting a car. The road was shut down for the accident, where {{the driver of the}} tanker was taken to Glens Falls Hospital. The drivers of the car and the tractor-trailer that jackknifed were not hurt. The route was reopened by 11:40 a.m. This accident was one of many tractor-trailer accidents that has occurred on this stretch of NY149.|$|E
500|$|August 23, 1999– Hurricane Bret makes {{landfall}} as a Category3 hurricane on Padre Island, {{becoming the}} first major hurricane to hit Texas since Hurricane Alicia in 1983. As it approaches landfall, large swells cause minor beach erosion along the coast. Along with rainfall from Bret, beaches in Matagorda County are closed due to the high water level. Further inland, heavy rainfall occurs across South Texas, reaching [...] in Sarita. At {{the height of the}} storm, power outages cut electricity to an estimated 64,000 customers. Due to the small size of Bret and its landfall in a relatively unpopulated region of the Texas coast, damages from Bret total just $15million, but it causes four deaths, all of which are attributed to a semi-trailer truck <b>jackknifing</b> in Laredo.|$|E
500|$|The episode {{received}} generally favorable {{reviews from}} television sources and critics. Ahsan Haque of IGN gave the episode 7.5/10 and said: [...] "While it featured more than [...] {{a couple of}} genuinely unfunny ideas, this episode succeeds for the most part. [...] While this episode definitely won't make anyone's top ten list of great Family Guy, there was a much better balance between random humor and storytelling in this outing". Genevieve Koski of The A.V. Club gave the episode an A-, and said that Family Guy [...] "started things out {{on the wrong foot}} with an oh-so-relevant Jackass storyline. [...] Most of the Jackass stuff was way too stale–Peter <b>jackknifing</b> Quagmire into the crate of bees notwithstanding–but thankfully it was just setup for a far superior storyline, the introduction of a younger 'New Brian'".|$|E
40|$|Metode <b>Jackknife</b> dapat digunakan untuk mereduksi bias suatu {{estimator}}. Untuk mengetahui keefektifan dari estimator <b>Jackknife</b> order satu dan estimator <b>Jackknife</b> order dua sebagai pereduksi bias dapat dilihat dari sifat-sifatnya. Ciri-ciri bias estimator terfetak pada suku-suku bias dari estimator yang asli. Kemudian bias dari <b>Jackknife</b> order dua dibandingkan dengan bias estimator yang asli dan juga dibandingkan dengan bias <b>Jackknife</b> order satu. Dari perbandingan ini dapat dilihat untuk keadaan tertentu <b>Jackknife</b> order dua lebih efektif daripada <b>Jackknife</b> order satu maupun terhadap estimator aslinya. <b>Jackknife</b> method {{can be used}} to bias reduction. To {{investigates the}} effectiveness of the first and second order <b>Jackknife</b> estimators,as tools for bias reduction, we must look from the characterized. The biases of estimators are characterized in terms of the bias original estimator. Then biases of the two estimators are compared, biases second order <b>Jackknife</b> are compared with biases first order <b>Jackknife</b> and biases second order <b>Jackknife</b> compared with bias of the original estimator. From this comparison we can see that second order <b>Jackknife</b> more effectif than first order <b>Jackknife</b> or the original estimator. L. This document is Undip Institutional Repository Collection. The author(s) or copyright owner(s) agree that UNDIP-IR may, without changing the content, translate the submission to any medium or kormat for the purpose of preservation. The author(s) or copyright owner(s) also agree that UNDIP-IR may keep more than one copy of this submission for purpose of security, back-up and preservation: ([URL]) ...|$|R
40|$|Abstract. The <b>jackknife</b> {{variance}} stimator and the infinitesimal <b>jackknife</b> variance estimator {{are shown}} to be asymptotically equivalent if the functional of interest is a smooth function of the mean or a trimmed L-statistic with HSlder continuous weight function. Key words and phrases: <b>Jackknife</b> variance estimator, infinitesimal <b>jackknife,</b> tr immed L-statistics, asymptotic normality. 1...|$|R
5000|$|A <b>jackknife</b> is an {{abdominal}} exercise. <b>Jackknife</b> {{exercises are}} designed to work the abdominal muscles, particularly the lower abs. There {{are a number of}} variations of <b>jackknife</b> exercises that allow people of different ages and ability to work their abdominal muscles. The <b>jackknife</b> can be done on an exercise ball, exercise bench, the floor or on an exercise mat.|$|R
50|$|Common {{resampling}} techniques include bootstrapping, <b>jackknifing</b> and permutation tests.|$|E
5000|$|Having the {{passenger}} lean over {{to some degree}} to avoid <b>jackknifing</b> or submarining.|$|E
50|$|January 10, 2004: host Jennifer Aniston as Taytay Phillips, a {{pregnant}} woman brought in after she fell on a toilet seat after <b>jackknifing</b> off a Big Wheel.|$|E
40|$|We study <b>jackknife</b> estimators in a first-order {{autoregression}} with a unit root. Non-overlapping sub-sample estimators {{have different}} limit distributions, so the <b>jackknife</b> {{does not fully}} eliminate first-order bias. We therefore derive explicit limit distributions of the numerator and denominator to calculate the expectations that determine optimal <b>jackknife</b> weights. Simulations show that the resulting <b>jackknife</b> estimator produces substantial reductions in bias and RMS...|$|R
5000|$|A <b>jackknife</b> {{estimate}} of the ratio is less biased than the naive form. A <b>jackknife</b> estimator of the ratio is ...|$|R
50|$|In statistics, the <b>jackknife</b> is a {{resampling}} technique {{especially useful}} for variance and bias estimation. The <b>jackknife</b> predates other common resampling {{methods such as}} the bootstrap. The <b>jackknife</b> estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size , the <b>jackknife</b> estimate is found by aggregating the estimates of each -sized sub-sample.|$|R
5000|$|Estimating the {{precision}} of sample statistics (medians, variances, percentiles) by using subsets of available data (<b>jackknifing)</b> or drawing randomly with replacement from a set of data points (bootstrapping) ...|$|E
50|$|Tractors used to {{be fitted}} with a lever in the cab to operate the trailer brakes. The vehicle could be slowed down or stopped using the trailer brakes only. Theoretically this was a sure way to prevent <b>jackknifing,</b> but this lever was often the cause of <b>jackknifing</b> in a {{roundabout}} way. Frequent use of the trailer brakes alone caused them to overheat and fade while the tractor brakes remained fresh. In {{the event of an}} emergency stop, the driver would go straight for the foot brake and the truck would surely jackknife because the tractor brakes would lock while the trailer brakes would be ineffective due to previous overheating.|$|E
50|$|Leave-one-out {{cross-validation}} (LOOCV) is {{a particular}} case of leave-p-out cross-validation with p = 1. The process looks similar to jackknife, however with cross-validation you compute a statistic on the left-out sample(s), while with <b>jackknifing</b> you compute a statistic from the kept samples only.|$|E
40|$|This paper proposes an {{algorithm}} for {{the estimation}} of the parameters of logistic regression analysis using <b>Jackknife.</b> <b>Jackknife</b> delete-one and delete-d algorithm was used to provide estimates of logistic regression coefficient. The <b>Jackknife</b> standard deviation provides an estimate of variability of the standard deviation of sample {{and it is a}} good measure of precision. The method was illustrated with real life data; and the results obtained from the <b>Jackknife</b> samples was compared with the result from ordinary logistic regression using the maximum likelihood method and results obtained reveals that the values from the <b>jackknife</b> algorithm for the parameter estimation, standard deviation and confidence interval were so close to the result from ordinary logistic regression analysis, this provides a good approximation to the result which shows that there is no bias in the <b>jackknife</b> coefficients...|$|R
40|$|We {{consider}} {{the estimation of}} quantiles in finite population susing the <b>jackknife</b> technique. We use a <b>jackknife</b> variance estimator for unequal probability sampling that works better than the classical <b>jackknife</b> estimator. The quality of the confidence interval found is demonstrated via simulation. The proposed confidence interval showed coverage probabilities that were close to the nominal confidence level, and mean lengths and variances {{much smaller than the}} mean lengths and variances of the intervals using the traditional <b>jackknife</b> methodology...|$|R
40|$|The <b>jackknife</b> has a {{long history}} as a tool for {{reducing}} bias and estimating variances. Here we highlight the use of the <b>jackknife</b> to produce standard errors in Monte Carlo studies and illustrate it with F-statistics also based on the <b>jackknife.</b> Key words and phrases: Monte Carlo studies; standard errors; k-sample comparisons; Oneway F-statistic. ...|$|R
50|$|The {{error of}} E(D) is the {{standard}} error of experimentally calculated E(D) values and the error of Var(D) {{is the standard}} error of experimentally calculated Var(D) values. These standard error values can be estimated using theoretical models or resampling methods (bootstrapping, <b>jackknifing).</b>|$|E
5000|$|<b>Jackknifing</b> in phylogenetics is {{a similar}} procedure, except the columns of the matrix are sampled without replacement. Pseudoreplicates are {{generated}} by randomly subsampling the data—for example, a [...] "10% jackknife" [...] would involve randomly sampling 10% of the matrix many times to evaluate nodal support.|$|E
50|$|If the {{requirement}} {{is simply to}} reduce the bias of an estimated standard deviation, rather than to eliminate it entirely, then two practical approaches are available, both {{within the context of}} resampling. These are <b>jackknifing</b> and bootstrapping. Both can be applied either to parametrically based estimates of the standard deviation or to the sample standard deviation.|$|E
40|$|The <b>jackknife</b> {{procedure}} is introduced {{as a means}} of making comparisons among Michaelis-Menten parameter estimates for six different experimental conditions. In addition to providing a solution to the general inter-experimental comparison problem, the <b>jackknife</b> procedure will provide valid parameter estimates even when some of the assumptions usually required for statistical analysis are violated, e. g., the random errors are not normally distributed and the variances are not homogeneous. Other recent variations of the <b>jackknife</b> have also been introduced and briefly investigated: (i) the linear <b>jackknife,</b> which is more efficient computationally, and (ii) the weighted <b>jackknife,</b> which reduces the influence of design points (substrate concentrations) that have an excessive influence on the precision of parameter estimates...|$|R
40|$|In this study, <b>Jackknifed</b> estimators are {{obtained}} for the parameters @ and ß of the Uniform(ß,@) distribution based on progressively type-II right censored sample. Expected values and variances of <b>jackknifed</b> and non-jackknifed estimators are derived and compared. A numerical example is given for using of estimators. Uniform distribution, progressive type-II right censoring, <b>jackknifed</b> estimator, order statistics. ...|$|R
50|$|The <b>jackknife,</b> {{like the}} {{original}} bootstrap, {{is dependent on}} the independence of the data. Extensions of the <b>jackknife</b> to allow for dependence in the data have been proposed.|$|R
50|$|Professor Künsch's main {{research}} {{areas are}} spatial statistics and random fields (geostatistics, parameter estimation for Gibbs fields, image analysis, space-time models); {{time series analysis}} (long range dependence, bootstrap methods for dependent data, general state-space models); and robust statistics and statistical model selection. Among his most frequently cited contributions is the Annals of Statistics 1989 article on bootstrapping and <b>jackknifing</b> in stationary time series.|$|E
5000|$|In typical punk fashion, Romanoff's live {{performances}} are often wild and exuberant. John Leland of The New York Times described {{him at a}} 2013 concert, [...] "side curls flailing, knees <b>jackknifing</b> up around his torso, leaping, crouching, shouting..." [...] Matthue Roth, in a Tablet Magazine piece covering Blanket Statementstein, referred to Romanoff as [...] "the drummer for whom Animal the Muppet is probably not only a musical guide but a fashion icon." ...|$|E
5000|$|In December 1983, his car {{collided}} with a <b>jackknifing</b> tractor-trailer on an icy highway (State Route 9 in Hyde Park, New York) and he died on January 2. Unification Church leader Chung Hwan Kwak stated: [...] "A truck lost control as it approached the car Heung Jin Nim was driving. Heung Jin Nim swerved the car to prevent the two friends who were with him from taking {{the brunt of the}} impact, and instead took it on himself." ...|$|E
40|$|Estimation of {{the value}} of a {{regression}} function at a point of continuity using a kernel-type estimator is discussed and improvements of the technique by a generalized <b>jackknife</b> estimator are presented. It is shown that the generalized <b>jackknife</b> technique produces estimators with faster bias rates. In a small example it is investigated, if the generalized <b>jackknife</b> method works for all choices of bandwidths. It turns out that an improper choice of this parameter may inflate the mean square error of the generalized <b>jackknife</b> estimator. AMS Subject Classifications: Primary 62 G 05, Secondary 62 J 02...|$|R
40|$|Singh et al. (1986) {{proposed}} an almost unbiased ridge estimator using <b>Jackknife</b> method that required {{transformation of the}} regression parameters. This article shows that the same method {{can be used to}} derive the <b>Jackknifed</b> ridge estimator of the original (untransformed) parameter without transformation. This method also leads in deriving easily the second order <b>Jackknifed</b> ridge that may reduce the bias further. We further investigate the performance of these estimators along with a recent method by Batah et al. (2008) called modified <b>Jackknifed</b> ridge theoretically as well as numerically...|$|R
40|$|In this paper, the {{hierarchical}} ways {{for building a}} regression model by using bootstrap and <b>jackknife</b> resampling methods were presented. Bootstrap approaches based on the observations and errors resampling, and <b>jackknife</b> approaches based on the delete-one and delete-d observations were considered. And also we consider estimating bootstrap and <b>jackknife</b> bias, standard errors and confidence intervals of the regression coefficients, and comparing with the concerning estimates of ordinary least squares. Obtaining of the estimates was presented with an illustrative real numerical example. The <b>jackknife</b> bias, the standard errors and confidence intervals of regression coefficients are substantially larger than the bootstrap and estimated asymptotic OLS standard errors. The <b>jackknife</b> percentile intervals also are larger than to the bootstrap percentile intervals of the regression coefficients...|$|R
50|$|There is {{a special}} {{consideration}} with the jackknife, particularly with the delete-1 observation jackknife. It should only be used with smooth, differentiable statistics (e.g., totals, means, proportions, ratios, odd ratios, regression coefficients, etc.; not with medians or quantiles). This could become a practical disadvantage. This disadvantage is usually the argument favoring bootstrapping over <b>jackknifing.</b> More general jackknifes than the delete-1, such as the delete-m jackknife, overcome this problem for the medians and quantiles by relaxing the smoothness requirements for consistent variance estimation.|$|E
50|$|<b>Jackknifing,</b> {{which is}} similar to bootstrapping, is used in {{statistical}} inference to estimate the bias and standard error (variance) of a statistic, when a random sample of observations is used to calculate it. Historically this method preceded the invention of the bootstrap with Quenouille inventing this method in 1949 and Tukey extending it in 1958. This method was foreshadowed by Mahalanobis who in 1946 suggested repeated estimates of the statistic of interest with half the sample chosen at random. He coined the name 'interpenetrating samples' for this method.|$|E
5000|$|The {{free surface}} effect can affect {{any kind of}} craft, {{including}} watercraft (where it is most common), bulk cargo or liquid tanker semi-trailers and trucks (causing either <b>jackknifing</b> or roll-overs), and aircraft (especially fire-fighting water-droppers and refueling tankers where baffles mitigate but do not eliminate the effects). The term [...] "free surface effect" [...] implies a liquid {{under the influence of}} gravity. Slosh dynamics is the overarching field which covers both free surface effects and situations such as space vehicles, where gravity is inconsequential but inertia and momentum interact with complex fluid mechanics to cause vehicle instability.|$|E
40|$|In {{statistics}} it is {{of interest}} {{to find a better}} interval estimator of the absolute mean deviation. In this thesis, we focus on using the <b>jackknife,</b> the adjusted and the extended <b>jackknife</b> empirical likelihood methods to construct confidence intervals for the mean absolute deviation of a random variable. The empirical log-likelihood ratio statistics is derived whose asymptotic distribution is a standard chi-square distribution. The results of simulation study show the comparison of the average length and coverage probability by using <b>jackknife</b> empirical likelihood methods and normal approximation method. The proposed adjusted and extended <b>jackknife</b> empirical likelihood methods perform better than other methods for symmetric and skewed distributions. We use real data sets to illustrate the proposed <b>jackknife</b> empirical likelihood methods...|$|R
50|$|For many {{statistical}} parameters the <b>jackknife</b> {{estimate of}} variance tends asymptotically {{to the true}} value almost surely. In technical terms one says that the <b>jackknife</b> estimate is consistent. The <b>jackknife</b> is consistent for the sample means, sample variances, central and non-central t-statistics (with possibly non-normal populations), sample coefficient of variation, maximum likelihood estimators, least squares estimators, correlation coefficients and regression coefficients.|$|R
40|$|The paper reviews {{empirical}} best linear prediction (EBLUP) and {{the associated}} <b>jackknife</b> MSE estimator of EBLUP. The bias of <b>jackknife</b> MSE estimator is of order o(m− 1), where m {{is the number of}} small areas. The <b>jackknife</b> works well both for normal and nonnormal Fay-Herriot models. The proposed methodology is illustrated using a real life example from the National Health and Interview Survey. ...|$|R
