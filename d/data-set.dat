2238|1436|Public
5|$|According to analyst firm Ovum, the {{software}} is made possible through advances in predictive analytics, machine learning and the NoSQL data caching methodology. The software uses semantic algorithms to {{understand the meaning of}} a data table's columns and pattern recognition algorithms to find potential duplicates in a <b>data-set.</b> It also uses indexing, text pattern recognition and other technologies traditionally found in social media and search software.|$|E
5|$|Recently, free {{electron}} lasers {{have been developed}} for use in X-ray crystallography. These are the brightest X-ray sources currently available; with the X-rays coming in femtosecond bursts. The intensity of the source is such that atomic resolution diffraction patterns can be resolved for crystals otherwise too small for collection. However, the intense light source also destroys the sample, requiring multiple crystals to be shot. As each crystal is randomly oriented in the beam, hundreds of thousands of individual diffraction images must be collected {{in order to get a}} complete <b>data-set.</b> This method, serial femtosecond crystallography, has been used in solving the structure of a number of protein crystal structures, sometimes noting differences with equivalent structures collected from synchrotron sources.|$|E
25|$|Computations where {{a number}} of similar, and often nested, models are {{considered}} for the same <b>data-set.</b> That is, where models with the same dependent variable but different sets of independent variables are to be considered, for essentially {{the same set of}} data-points.|$|E
40|$|Recently, several <b>data-sets</b> associating data to text {{have been}} created to train data-to-text surface realisers. It is unclear however {{to what extent the}} surface {{realisation}} task exercised by these <b>data-sets</b> is linguistically challenging. Do these <b>data-sets</b> provide enough variety to encourage the development of generic, high-quality data-to-text surface realisers ? In this paper, we argue that these <b>data-sets</b> have important drawbacks. We back up our claim using statistics, metrics and manual evaluation. We conclude by eliciting a set of criteria {{for the creation of a}} data-to-text benchmark which could help better support the development, evaluation and comparison of linguistically sophisticated data-to-text surface realisers...|$|R
5000|$|In statistics, the pseudomedian is {{a measure}} of {{centrality}} for <b>data-sets</b> and populations. It agrees with the median for symmetric <b>data-sets</b> or populations. In mathematical statistics, the pseudomedian is also a location parameter for probability distributions.|$|R
40|$|Abstract. In this {{contribution}} {{we carry}} out {{an analysis of the}} Fuzzy Reasoning Methods for Fuzzy Rule Based Classification Systems in the framework of balanced and imbalanced <b>data-sets</b> with different degrees of imbalance. We analyze the behaviour of the Fuzzy Rule Based Classification Systems searching for the best type of Fuzzy Reasoning Method in each case, also studying the cooperation of some pre-processing methods of instances for imbalanced <b>data-sets.</b> To do so we use a fuzzy rule learning method that extends the well-known Wang and Mendel algorithm to classification problems. The results obtained show the necessity to apply an instance preprocessing step and the differences for the most appropriate Fuzzy Reasoning Method in balanced and imbalanced <b>data-sets,</b> concluding that the choice of the Fuzzy Reasoning Method depends on the degree of imbalance, being the most adequate the use of the Winning Rule for high imbalanced <b>data-sets</b> and the Additive Combination method for the remaining <b>data-sets...</b>|$|R
500|$|... where F is the {{structure}} factor. A similar quality criterion is Rfree, which is calculated from a subset (~10%) of reflections {{that were not}} included in {{the structure}} refinement. Both R factors depend on the resolution of the data. As a rule of thumb, Rfree should be approximately the resolution in angstroms divided by 10; thus, a <b>data-set</b> with 2Å resolution should yield a final Rfree ~ 0.2. Chemical bonding features such as stereochemistry, hydrogen bonding and distribution of bond lengths and angles are complementary measures of the model quality. Phase bias is a serious problem in such iterative model building. Omit maps are a common technique used to check for this.|$|E
2500|$|An {{alternative}} {{formulation of}} the condition that a statistic be sufficient, set in a Bayesian context, involves the posterior distributions obtained by using the full <b>data-set</b> and by using only a statistic. Thus the requirement is that, for almost every x, ...|$|E
2500|$|Andy Gaunt's 2010 {{resistivity}} survey widened the area selected by Masters {{to incorporate the}} entirety of Castlefield and proved many of Masters' findings as well as adding new anomalies to the <b>data-set.</b> Gaunt's work discovered the following anomalies and is summarised from: ...|$|E
25|$|Special {{considerations}} for very extensive <b>data-sets.</b>|$|R
40|$|Much {{scientific}} research {{is based on}} the gathering and analysis of measurement data. Scientific <b>data-sets</b> are, at least, intermediate results in many {{scientific research}} projects. For some time <b>data-sets</b> weren’t even published and even if they were published it was mostly as a (not re-usable) by-product of the publication. But an interesting phenomenon might be observed here: <b>data-sets</b> (often in combination with models and parameters) are becoming more important themselves and can sometimes be seen as the primary intellectual output of the research. Publishing and preserving <b>data-sets</b> should therefore seriously be considered. This will especially be the case if the data cannot be reproduced (as they result from unique events) and will be necessary in the future for longitudinal research or to test or check future insights. Delft University of Technolog...|$|R
5000|$|Collected <b>data-sets</b> about people, {{places and}} {{institutions}} (3,000+ entries) ...|$|R
2500|$|Democide — R. J. Rummel {{coined the}} term [...] "democide", which {{includes}} genocide, politicide, and mass murder. [...] Helen Fein has termed the mass state killings in the Soviet Union and Cambodia as [...] "genocide and democide." [...] Frank Wayman and Atsushi Tago have shown the significance of terminology in that, depending {{on the use of}} democide (generalised state-sponsored killing) or politicide (eliminating groups who are politically opposed) as the criterion for inclusion in a <b>data-set,</b> statistical analyses seeking to establish a connection between mass killings can produce very different results, including the significance or otherwise of regime type.|$|E
5000|$|Some {{important}} {{methods of}} statistical inference use resampling from the observed data. Multiple alternative {{versions of the}} <b>data-set</b> that [...] "might have been observed" [...] are created by randomization of the original <b>data-set,</b> the only one observed. The variation of statistics calculated for these alternative data-sets is {{a guide to the}} uncertainty of statistics estimated from the original data.|$|E
50|$|In reality {{autonomous}} agents {{possess an}} initial <b>data-set</b> or knowledge-base, but this cannot be immutable {{or it would}} hamper autonomy and heuristic ability. Even if the <b>data-set</b> is empty, it usually may be argued {{that there is a}} built-in bias in the reasoning and planning mechanisms. Either intentionally or unintentionally placed there by the human designer, it thus negates the true spirit of tabula rasa.|$|E
40|$|This paper {{addresses}} {{the problem of}} reproducing the effect of different visibility conditions on people’s walking speed when using evacuation models. In particular, different strategies {{regarding the use of}} default settings and embedded <b>data-sets</b> are investigated. Currently, the correlation between smoke and walking speed is typically based on two different sets of experimental data produced by (1) Jin and (2) Frantzich and Nilsson. The two <b>data-sets</b> present different experimental conditions, but are often applied as if equivalent. In addition, models may implement the same <b>data-sets</b> in different ways. To test the impact of this representation within evacuation tools, the authors have employed six evacuation models, making different assumptions and employing different <b>data-sets</b> (FDS+EVAC, Gridflow, buildingEXODUS, STEPS, Pathfinder and Simulex). A simple case-study is simulated in order to investigate the sensitivity of the representation of two key variables: (1) initial occupant speeds in clear conditions, (2) extinction coefficients. Results show that (1) evacuation times appear to be consistent if models use the same <b>data-sets</b> and interpret the smoke vs speed correlation in the same manner (2) the same model may provide different results if applying different <b>data-sets</b> or interpretations for configuring the inputs; i. e. default settings are crucial for the calculation of the model results (3) models using embedded data-sets/assumptions require user expertise, experience and understanding to be employed appropriately and the results evaluated in a credible manner...|$|R
40|$|Multicasts are a {{powerful}} means to implement coordinated operations on distributed <b>data-sets</b> {{as well as}} synchronized reductions of multiple computed results. In this paper we present a topology based approach to implement parallel operations on distributed <b>data-sets</b> as multicasts. Multicast groups are described as reusable application-specific topology classes that coordinate both the spreading of multicast messages and the collection (reduction) of the computed results. Thus global operations are controllable through applications and existing communication topologies as well as synchronization patterns can effectively be reused. 1 Introduction Multicasts are {{a powerful}} means to implement coordinated operations on distributed <b>data-sets</b> as well as synchronized reductions of multiple computed results. In this paper we present a topology based approach to implement parallel operations on distributed <b>data-sets</b> as multicasts based on point-to-point communication. We {{want to be able}} to co [...] ...|$|R
40|$|This paper {{examines}} {{the role of}} secondary <b>data-sets</b> in empirical economic research, taking the field of income distribution as a case study. We illustrate problems faced by users of "secondary" statistics, showing how both cross-country comparisons and time-series analysis can depend sensitively on the choice of data. After describing the genealogy of secondary <b>data-sets</b> on income inequality, we consider the main methodological issues and discuss their implications for comparisons of income inequality across OECD countries and over time. The lessons to be drawn for the construction and use of secondary <b>data-sets</b> are summarized {{at the end of}} the paper. ...|$|R
50|$|Section II: Free-Response (one <b>data-set</b> question, one {{document-based}} question, and two synthesis {{and evaluation}} questions, 90 minutes).|$|E
50|$|The CT {{scan of the}} {{crocodile}} mummy has resolution 3000×512×512 (16bit), the skull <b>data-set</b> has resolution 512×512×750 (16bit).|$|E
50|$|An ongoing wiki-like on-line {{large scale}} {{collaboration}} for maintaining and diversifying the Barrington Atlas <b>data-set</b> {{is carried on}} by the Pleiades Project.|$|E
50|$|A list of {{commonly}} used multi-label <b>data-sets</b> {{is available at}} the Mulan website.|$|R
5000|$|On the fly {{generation}} of N-grams with optional skips (useful for word/language <b>data-sets)</b> ...|$|R
40|$|This paper {{proposes a}} Genetic Algorithm for jointly {{performing}} a feature selection and granularity learning for Fuzzy Rule-Based Classification Systems in the scenario of highly imbalanced <b>data-sets.</b> We refer to imbalanced <b>data-sets</b> when the class distribution is not uniform, {{a situation that}} it is present in many real application areas. The aim of this work is to get more compact models by selecting the adequate variables and adapting the number of fuzzy labels for each problem, improving the interpretability of the model. The experimental analysis is carried out {{over a wide range}} of highly imbalanced <b>data-sets</b> and uses the statistical tests suggested in the specialized literature...|$|R
5000|$|In {{assessing}} {{whether a}} given distribution is {{suited to a}} <b>data-set,</b> the following tests and their underlying measures of fit can be used: ...|$|E
5000|$|Vowpal wabbit {{has been}} used to learn a tera-feature (1012) <b>data-set</b> on 1000 nodes in one hour. Its {{scalability}} is aided by several factors: ...|$|E
5000|$|The {{complete}} saved <b>data-set</b> can be re-examined at {{any time}} if a second expert opinion is required, even after burial or cremation of the body.|$|E
40|$|This book is {{the result}} of a working group {{sponsored}} by ISSI in Bern, which was initially created to study possible ways to calibrate a Far Ultraviolet (FUV) instrument after launch. In most cases, ultraviolet instruments are well calibrated on the ground, but unfortunately, optics and detectors in the FUV are very sensitive to contaminants and it is very challenging to prevent contamination before and during the test and launch sequences of a space mission. Therefore, ground calibrations need to be confirmed after launch and it is necessary to keep track of the temporal evolution of the sensitivity of the instrument during the mission. The studies presented here cover various fields of FUV spectroscopy with the exclusion of direct solar UV spectroscopy, including a catalog of stellar spectra, <b>data-sets</b> of lunar Irradiance, observations of comets and measurements of the interplanetary background. Detailed modeling of the interplanetary background is presented as well. This work also includes comparisons of older <b>data-sets</b> with current ones. This raises the question of the consistency of the existing <b>data-sets.</b> Previous experiments have been calibrated independently and comparison of the <b>data-sets</b> may lead to inconsistencies. The authors have tried to check that possibility in the <b>data-sets</b> and when relevant, suggest a correction factor for the corresponding data...|$|R
5000|$|The {{texts are}} created {{according}} to library-determined standards, uniform across multiple <b>data-sets</b> and potentially cross-searchable.|$|R
40|$|Frequency {{distributions}} from 49 published wildlife host-macroparasite {{systems were}} analysed by maximum likelihood for {{goodness of fit}} to the negative binomial distribution. In 45 of the 49 (90 %) <b>data-sets,</b> the negative binomial distribution provided a statistically satisfactory fit. In the other 4 <b>data-sets</b> the negative binomial distribution still provided a better fit than the Poisson distribution, and only 1 of the <b>data-sets</b> fitted the Poisson distribution. The degree of aggregation was large, with 43 of the 49 <b>data-sets</b> having an estimated k of less than 1 From these 19 <b>data-sets,</b> 22 subsets of host data were available (i. e. host data could be divided by either host sex, age, where or when hosts were sampled). In 11 of these 22 subsets there was significant variation {{in the degree of}} aggregation between host subsets of the same host-parasite system. A common k estimate was always larger than that obtained with all the host data considered together. These results indicate that lumping host data can hide important variations in aggregation between hosts and can exaggerate the true degree of aggregation. Wherever possible common k estimates should be used to estimate the degree of aggregation. In addition, significant differences in the degree of aggregation between subgroups of host data, were generally associated with significant differences in both mean parasite burdens and the prevalence of infection...|$|R
50|$|This {{provides}} {{a much more}} rounded <b>data-set</b> for marketing purposes and can be integrated with industry standard software such as CRM Systems, AdWords, Google Analytics etc.|$|E
5000|$|In {{descriptive}} statistics, the pseudomedian of a <b>data-set</b> is {{measure of}} centrality, {{similar to a}} sample median. Other centrality statistics include the sample mean and a mode.|$|E
50|$|Cardiocorax is a {{relatively}} distinctive elasmosaurid, possessing four unique traits, i.e. autapomorphies. Notably, it shows a reduced dorsal blade of the scapula, a feature unique among elasmosaurids, but convergent with the related cryptoclidid plesiosaurs. This trait indicates a longitudinal protraction-retraction limb cycle rowing style with simple pitch rotation at the glenohumeral articulation - a style unique to Cardiocorax. Araújo et al. (2015) tested the phylogenetic position of the genus using {{a modified version of}} Ketchum & Benson (2010) <b>data-set,</b> as well as a second <b>data-set</b> from Vincent et al. (2011) which was found to be consistent with the former. The cladogram below follows their results.|$|E
40|$|Background: High-throughput {{proteomics}} techniques, such as {{mass spectrometry}} (MS) -based approaches, produce very high-dimensional <b>data-sets.</b> In a clinical setting one is often {{interested in how}} mass spectra differ between patients of different classes, for example spectra from healthy patients vs. spectra from patients having a particular disease. Machine learning algorithms are needed to (a) identify these discriminating features and (b) classify unknown spectra based on this feature set. Since the acquired data is usually noisy, the algorithms should be robust against noise and outliers, while the identified feature set should be as small as possible. Results: We present a new algorithm, Sparse Proteomics Analysis (SPA), based on the theory of compressed sensing {{that allows us to}} identify a minimal discriminating set of features from mass spectrometry <b>data-sets.</b> We show (1) how our method performs on artificial and real-world <b>data-sets,</b> (2) that its performance is competitive with standard (and widely used) algorithms for analyzing proteomics data, and (3) that it is robust against random and systematic noise. We further demonstrate the applicability of our algorithm to two previously published clinical <b>data-sets...</b>|$|R
40|$|Crowdsourcing {{has become}} a popular means to acquire data about the Earth and its {{environment}} inexpensively, but the <b>data-sets</b> obtained are typically imperfect and of unknown quality. Two common imperfections with crowdsourced data are the contributions from cheats or spammers and missing cases. The effect of the latter two imperfections on a method to evaluate the accuracy of crowdsourced data via a latent class model was explored. Using simulated and real <b>data-sets,</b> it was shown that the method is able to derive useful information on the accuracy of crowdsourced data even when the degree of imperfection was very high. The practical potential of this ability to obtain accuracy information within the geospatial sciences and the realm of Digital Earth applications was indicated with reference to an evaluation of building damage maps produced by multiple bodies after the 2010 earthquake in Haiti. Critically, the method allowed <b>data-sets</b> to be ranked in approximately the correct order of accuracy and this could help ensure that the most appropriate <b>data-sets</b> are used...|$|R
40|$|In recent years, the {{computation}} of {{load and}} energy requirement in buildings, energy conservation studies in buildings, {{and the design}} of solar systems are not accomplished using long-term averages of weather data as inputs, but preferably with <b>data-sets</b> representative of the climatological features of the site that are generated for this purpose. Such <b>data-sets</b> are the test reference year, typical meteorological year and weather year for energy calculations. In this paper, the basis, the selection procedure {{and the use of}} each of these <b>data-sets</b> are discussed. Also, an attempt is made to generate typical weather years for the cities of Dhahran, Riyadh, Jeddah, Khamis-Mushayt and Hail, using the standard meteorological data obtained by measurements by the Meteorology and Environmental Protection Administration. ...|$|R
