164|393|Public
25|$|It {{may be that}} {{a player}} has an {{infinite}} number of possible actions to choose from at a particular decision node. The device used to represent this is an arc joining two edges protruding from the decision node in question. If the action space is a continuum between two numbers, the lower and upper delimiting numbers are placed at the bottom and top of the arc respectively, usually with a variable that is used to express the payoffs. The infinite number of <b>decision</b> <b>nodes</b> that could result are represented by a single node placed in the centre of the arc. A similar device is used to represent action spaces that, whilst not infinite, are large enough to prove impractical to represent with an edge for each action.|$|E
2500|$|An ADTree {{consists}} of an alternation of <b>decision</b> <b>nodes,</b> which specify a predicate condition, and prediction nodes, which contain a single number. [...] An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true, and summing any prediction nodes that are traversed.|$|E
2500|$|An {{alternating}} {{decision tree}} consists of <b>decision</b> <b>nodes</b> and prediction nodes. [...] <b>Decision</b> <b>nodes</b> specify a predicate condition. [...] Prediction nodes contain a single number. [...] ADTrees always have prediction nodes as both root and leaves. [...] An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true and summing any prediction nodes that are traversed. [...] This {{is different from}} binary classification trees such as CART (Classification and regression tree) or C4.5 in which an instance follows only one path through the tree.|$|E
50|$|Create a <b>decision</b> <b>node</b> that splits on a_best.|$|R
50|$|<b>Decision</b> <b>node</b> (corresponding to each {{decision}} to be made) is drawn as a rectangle.|$|R
40|$|Abstract. Univariate {{decision}} {{trees at}} each <b>decision</b> <b>node</b> consider {{the value of}} only one feature leading to axis-aligned splits. In a linear multivariate decision tree, each <b>decision</b> <b>node</b> divides the input space into two with an arbitrary hyperplane leading to oblique splits. In a nonlinear one, a multilayer perceptron at each node divides the input space arbitrarily, {{at the expense of}} increased complexity. In this paper, we detail and compare using a set of simulations linear and non-linear neural based decision tree methods with the univariate decision tree induction method ID 3 and the linear multivariate decision tree induction method CART. We also propose hybrid trees where the <b>decision</b> <b>node</b> may be linear or nonlinear depending on the outcome of a statistical test on accuracy. ...|$|R
2500|$|... is {{a finite}} tree {{with a set}} of nodes , a unique initial node , a set of {{terminal}} nodes [...] (let [...] be a set of <b>decision</b> <b>nodes)</b> and an immediate predecessor function [...] on which {{the rules of the game}} are represented, ...|$|E
2500|$|An {{advantage}} of representing {{the game in}} this way is that it is clear what the order of play is. The tree shows clearly that player 1 moves first and player 2 observes this move. However, in some games play does not occur like this. One player does not always observe the choice of another (for example, moves may be simultaneous or a move may be hidden). An information set is a set of <b>decision</b> <b>nodes</b> such that: ...|$|E
50|$|An {{alternating}} {{decision tree}} consists of <b>decision</b> <b>nodes</b> and prediction nodes. <b>Decision</b> <b>nodes</b> specify a predicate condition. Prediction nodes contain a single number. ADTrees always have prediction nodes as both root and leaves. An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true and summing any prediction nodes that are traversed. This {{is different from}} binary classification trees such as CART (Classification and regression tree) or C4.5 in which an instance follows only one path through the tree.|$|E
50|$|Instance of previously-unseen class encountered. Again, C4.5 {{creates a}} <b>decision</b> <b>node</b> higher {{up the tree}} using the {{expected}} value.|$|R
50|$|There is 1 <b>decision</b> <b>node</b> (Vacation Activity), 2 {{uncertainty}} nodes (Weather Condition, Weather Forecast), and 1 value node (Satisfaction).|$|R
40|$|Univariate {{decision}} {{trees at}} each <b>decision</b> <b>node</b> consider {{the value of}} only one feature leading to axis-aligned splits. In a linear multivariate decision tree, each <b>decision</b> <b>node</b> divides the input space into two with a hyperplane. In a nonlinear multivariate tree, a multilayer perceptron at each node divides the input space arbitrarily, {{at the expense of}} increased complexity and higher risk of overfitting. We propose omnivariate trees where the <b>decision</b> <b>node</b> may be univariate, linear, or nonlinear depending on the outcome of comparative statistical tests on accuracy thus matching automatically the complexity of the node with the subproblem defined by the data reaching that node. Such an architecture frees the designer from choosing the appropriate node type, doing model selection automatically at each node. Our simulation results indicate that such a decision tree induction method generalizes better than trees with the same types of nodes everywhere and induces small trees...|$|R
50|$|An ADTree {{consists}} of an alternation of <b>decision</b> <b>nodes,</b> which specify a predicate condition, and prediction nodes, which contain a single number. An instance is classified by an ADTree by following all paths for which all <b>decision</b> <b>nodes</b> are true, and summing any prediction nodes that are traversed.|$|E
5000|$|More specifically, in the {{extensive}} form, an information set {{is a set}} of <b>decision</b> <b>nodes</b> such that: ...|$|E
5000|$|<b>Decision</b> <b>nodes</b> and {{incoming}} information arcs collectively {{state the}} alternatives (what {{can be done}} when the outcome of certain decisions and/or uncertainties are known beforehand) ...|$|E
50|$|Informational arcs (ending in <b>decision</b> <b>node)</b> {{indicate}} that the decision at their heads is made with the outcome of all the nodes at their tails known beforehand.|$|R
50|$|None of the {{features}} provide any information gain. In this case, C4.5 creates a <b>decision</b> <b>node</b> higher up the tree using the expected value of the class.|$|R
30|$|The Bayesian Network model {{structure}} is also {{dependent on the}} type of nodes involved in it. Three commonly used BN nodes are the Nature node, Utility <b>node</b> and <b>Decision</b> <b>node</b> (Ticehurst et al. 2007). The nature node describes possible states of a variable and the probability of each state. This type of node could be qualitative or quantitative (discrete or continuous). A utility node is a continuous variable describing the desirability of the consequences of a set of outcomes. The <b>decision</b> <b>node</b> represents a controllable variable providing choice to the decision maker (Robertson 2004).|$|R
5000|$|... is {{a finite}} tree {{with a set}} of nodes , a unique initial node , a set of {{terminal}} nodes [...] (let [...] be a set of <b>decision</b> <b>nodes)</b> and an immediate predecessor function [...] on which {{the rules of the game}} are represented, ...|$|E
50|$|To {{refine the}} equilibria {{generated}} by the Bayesian Nash solution concept or subgame perfection, one can apply the Perfect Bayesian equilibrium solution concept. PBE is {{in the spirit of}} subgame perfection in that it demands that subsequent play be optimal. However, it places player beliefs on <b>decision</b> <b>nodes</b> that enables moves in non-singleton information sets to be dealt more satisfactorily.|$|E
5000|$|The Northrop report {{argued that}} [...] "the U.S. Department of Defense (DoD) has {{a goal of}} {{information}} dominance … this goal depends on increasingly complex systems characterized by thousands of platforms, sensors, <b>decision</b> <b>nodes,</b> weapons, and warfighters connected through heterogeneous wired and wireless networks. … These systems will push far beyond the size of today's systems by every measure … They will be ultra-large-scale systems." ...|$|E
40|$|Univariate {{decision}} {{trees at}} each <b>decision</b> <b>node</b> consider {{the value of}} only one feature leading to axisaligned splits. In a multivariate decision tree, each <b>decision</b> <b>node</b> divides the input space into two with an arbitrary hyperplane leading to oblique splits. In this paper, we detail and compare using a set of simulations the univariate decision tree induction method ID 3 and the multivariate decision tree induction method CART. We see that the multivariate trees when preceded by feature selection are more accurate and are smaller but require longer training time. ...|$|R
5000|$|<b>Decision</b> Support <b>node.</b> This node {{allows the}} Program to invoke {{business}} rules {{that run on}} a component of IBM Decision Server that is provided with the Program. Use of this component is supported only via <b>Decision</b> Service <b>nodes.</b> The Program license provides entitlement for the Licensee {{to make use of}} <b>Decision</b> Service <b>nodes</b> for development and functional test uses. Refer to the IBM Integration Bus License Information text for details about the program-unique terms.|$|R
40|$|Univariate {{decision}} {{trees at}} each <b>decision</b> <b>node</b> consider {{the value of}} only one feature leading to axis-aligned splits. In a multivariate decision tree, each <b>decision</b> <b>node</b> divides the input space into two with an arbitrary hyperplane leading to oblique splits. In this paper, we propose linear discriminant trees where each node implements Fisher's linear discriminant analysis (LDA) to make a binary split. On twenty data sets from the UCI repository, we compare the linear discriminant trees with the univariate decision tree method ID 3, multivariate decision tree methods CART, neural trees and linear machine decision trees (LMDT). Our results indicate that linear discriminant trees learn faster than other multivariate methods with binary splits, generalize well and construct small trees...|$|R
5000|$|An {{advantage}} of representing {{the game in}} this way is that it is clear what the order of play is. The tree shows clearly that player 1 moves first and player 2 observes this move. However, in some games play does not occur like this. One player does not always observe the choice of another (for example, moves may be simultaneous or a move may be hidden). An information set is a set of <b>decision</b> <b>nodes</b> such that: ...|$|E
5000|$|A Boolean {{function}} can {{be represented}} as a rooted, directed, acyclic graph, which consists of several <b>decision</b> <b>nodes</b> and terminal nodes. There {{are two types of}} terminal nodes called 0-terminal and 1-terminal. Each decision node [...] is labeled by Boolean variable [...] and has two child nodes called low child and high child. The edge from node [...] to a low (or high) child represents an assignment of [...] to 0 (resp. 1).Such a BDD is called 'ordered' if different variables appear in the same order on all paths from the root. A BDD is said to be 'reduced' if the following two rules have been applied to its graph: ...|$|E
50|$|It {{may be that}} {{a player}} has an {{infinite}} number of possible actions to choose from at a particular decision node. The device used to represent this is an arc joining two edges protruding from the decision node in question. If the action space is a continuum between two numbers, the lower and upper delimiting numbers are placed at the bottom and top of the arc respectively, usually with a variable that is used to express the payoffs. The infinite number of <b>decision</b> <b>nodes</b> that could result are represented by a single node placed in the centre of the arc. A similar device is used to represent action spaces that, whilst not infinite, are large enough to prove impractical to represent with an edge for each action.|$|E
40|$|This {{paper is}} aimed at {{crowding}} phenomenon in the subway. As passengers are inclined to choose the route with minimum disutility, we put forward a route choice model which is constructed to achieve minimum objective function of feasibility for the optimal solution. Meanwhile we set passenger volume threshold values according to capacity of facilities. In the case of actual capacity exceeding the threshold, the <b>decision</b> <b>node</b> of constrained route will be selected; computing procedure about searching decision points will be presented. Then we should set rational restrictions at the <b>decision</b> <b>node</b> of the minimum utility function route to prevent too many passengers’ access to platform. Through certification, this series of methods can effectively {{ensure the safety of}} the station efficient operation...|$|R
40|$|Data Mining is the {{extraction}} of hidden predictive information from large database. Classification {{is the process of}} finding a model that describes and distinguishes data classes or concept. This paper performs the study of prediction of class label using C 4. 5 and Naïve Bayesian algorithm. C 4. 5 generates classifiers expressed as decision trees from a fixed set of examples. The resulting tree is used to classify future samples. The leaf <b>nodes</b> of the <b>decision</b> tree contain the class name whereas a non-leaf <b>node</b> is a <b>decision</b> <b>node.</b> The <b>decision</b> <b>node</b> is an attribute test with each branch (to another decision tree) being a possible value of the attribute. C 4. 5 uses information gain to help it decide which attribute goes into a <b>decision</b> <b>node.</b> A Naïve Bayesian classifier is a simple probabilistic classifier based on applying Baye’s theorem with strong (naive) independence assumptions. Naive Bayesian classifier assumes that the effect of an attribute value on a given class is independent of the values of the other attribute. This assumption is called class conditional independence. The results indicate that Predicting of class label using Naïve Bayesian classifier is very effective and simple compared to C 4. 5 classifier...|$|R
30|$|Due to the {{existence}} of expressways and parallel streets in the study network, the routing algorithm adopted in the PARAMICS simulation was important. The network was calibrated using the dynamic feedback assignment model provided by PARAMICS. Dynamic feedback assignment in PARAMICS assumes that different drivers perceive different costs from a <b>decision</b> <b>node</b> to the destination. The perceived cost was calculated, and the perceived shortest route was chosen at the <b>decision</b> <b>node.</b> At this stage the parameter to be calibrated for the route choice model is the number of drivers who are familiar with the road network. Since there was no data to calibrate it, it was assumed that most drivers in the morning peak period were familiar drivers who had the knowledge of road network and traffic conditions. Therefore, it was assumed that 95  % of drivers were familiar drivers, who could choose their route from the available options.|$|R
40|$|An {{information}} {{completion of}} an extensive game is obtained by extending the information partition of every player {{from the set}} of her <b>decision</b> <b>nodes</b> to the set of all nodes. The extended partition satisfies Memory of Past Knowledge (MPK) if at any node a player remembers what she knew at earlier no des. It is shown that MPK c an b e satisfi ed in a game i f an d only if the game is von Neumann (vN) and satisfies memory at <b>decision</b> <b>nodes</b> (the restriction of MPK to a player’s own <b>decision</b> <b>nodes).</b> A game is vN if any two <b>decision</b> <b>nodes</b> that {{belong to the same}} information set of a player have the same number of predecessors. By providing an axiom for MPK we also obtain a syntactic characterization of the said class of vN games. ...|$|E
40|$|This paper {{presents}} a new decision tree learning algorithm called CSNL that induces Cost-Sensitive Non-Linear decision trees. The algorithm {{is based on}} the hypothesis that non-linear <b>decision</b> <b>nodes</b> provide a better basis than axis-parallel <b>decision</b> <b>nodes</b> and utilizes discriminant analysis to construct non-linear decision trees that take account of costs of misclassification. The performance of the algorithm is evaluated by applying it to seventeen data sets and the results are compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost, which generate multiple trees to obtain some of the best results to date. The results show that CSNL performs at least as well, if not better than these algorithms, in more than twelve of the data sets and is considerably faster. The use of bagging with CSNL further enhances its performance showing the significant benefits of using non-linear <b>decision</b> <b>nodes...</b>|$|E
40|$|This paper {{presents}} a new decision tree learning algorithm that takes account of costs of misclassification. The algorithm {{is based on}} the hypothesis that non-linear <b>decision</b> <b>nodes</b> provide a better basis for cost-sensitive induction than axis-parallel <b>decision</b> <b>nodes</b> and utilizes discriminant analysis to construct non-linear cost-sensitive decision trees. The performance of the algorithm is evaluated by applying it to seven data sets and the results compared with those obtained by two well known cost-sensitive algorithms, ICET and MetaCost. MetaCost is applied both with a base learner that uses axis-parallel spits and a base learner that uses non-linear splits, thereby enabling an evaluation of the value of non-linear <b>decision</b> <b>nodes.</b> The results show that the new algorithm displays a better profile than ICET as the ratio of costs of misclassification departs from unity and that it provides a better base learner for MetaCost than an axis-parallel learner. ...|$|E
50|$|For dynamic (extensive form) games, McKelvey and Palfrey defined agent quantal {{response}} equilibrium (AQRE). AQRE is somewhat analogous to subgame perfection. In an AQRE, each player plays with some error as in QRE. At a given <b>decision</b> <b>node,</b> the player determines the expected payoff of each action by treating their future self {{as an independent}} player with a known probability distribution over actions.|$|R
30|$|The {{decision}} tree {{is generated by}} a complete semantic relation framework given in Fig.  1 and the features mentioned in Section 3.2 as discriminative <b>decision</b> <b>node.</b> In the tree structure, relation types also play roles as discriminative <b>decision</b> <b>node.</b> The most salient relation should be selected at first, and the closer to the bottom, the more inclusive and vague sense the relation linked. The {{decision tree}} demonstrates their priorities while determining the major relation. For example, in the sentence of  他在台灣出生(V 1)成長(V 2) tā zài táiwān chūshēng chéngzhǎng ‘he {{was born and raised}} in Taiwan’, we first use realis to determine it is a fact description; since there is no intention within V 1 and V 2, we look for causality in-between. Lacking causality, we then examine if V 1 modifies V 2 or if V 2 is a result state of V 1; disproving them both, we continue to look for a temporal relation between V 1 and V 2. With a positive answer, we then judge whether they are synchronous; since they are not, the TimeBefore-TimeAfter relation is determined.|$|R
25|$|The game on {{the right}} has two players: 1 and 2. The numbers by every non-terminal node {{indicate}} to which player that <b>decision</b> <b>node</b> belongs. The numbers by every terminal node represent the payoffs to the players (e.g. 2,1 represents a payoff of 2 to player 1 and a payoff of 1 to player 2). The labels by every edge of the graph are {{the name of the}} action that edge represents.|$|R
