2|30|Public
40|$|The {{essence of}} this thesis is the {{cognition}} of theoretical basis for measuring on optical fibres, elements and optical wireway as well. This work describes {{the methods of}} measuring and detection of parameters of these networks. The gained knowledge is followed by its application on two laboratory tasks. The first task is focused on usage of method of <b>embedded</b> <b>loss</b> for measuring of attenuation of fibre and followed by usage of the method OTDR for localization of defect. The second task is focused on measuring of attenuation on passive optical element using only the method of embedded losses. The laboratory tasks were described and respective conclusion and findings were drawed from each other...|$|E
40|$|Since the fifties, several {{measures}} have been developed {{in order to measure}} the performance of investments or choices involving uncertain outcomes. Much of these measures are based on Expected Utility Theory, but since the nineties a number of {{measures have}} been proposed based on Non-Expected Utility Theory. Among the Theories of Non-Expected Utility highlights Prospect Theory, which is the foundation of Behavioral Finance. Based on this theory this study proposes a new performance measure in which are <b>embedded</b> <b>loss</b> aversion along with the likelihood of distortions in the choice of alternatives. A hypothetical example is presented in which various performance measures, including the new measure are compared. The results showed that the ordering of the assets varied depending on the performance measure adopted. According to what was expected, the new performance measure clearly has captured the distortion of probabilities and loss aversion of the decision maker, ie, those assets with the greatest negative deviations from the target were those who had the worst performance...|$|E
40|$|Deep {{learning}} {{can be an}} effective and efficient means to automatically detect and classify targets in synthetic aperture radar (SAR) images, but it is critical for trained neural networks to be robust to variations that exist between training and test environments. The layers in a neural network can be understood as successive transformations of an input image into embedded feature representations and ultimately into a semantic class label. To address the overfitting problem in SAR target classification, we train neural networks to optimize the spatial clustering of points in the embedded space in addition to optimizing the final classification score. We demonstrate that networks trained with this dual <b>embedding</b> and classification <b>loss</b> outperform networks with classification loss only. We study placing the <b>embedding</b> <b>loss</b> after different network layers and find that applying the <b>embedding</b> <b>loss</b> on the classification space results in the best SAR classification performance. Finally, our visualization of the network's ten-dimensional classification space supports our claim that the <b>embedding</b> <b>loss</b> encourages greater separation between target class clusters for both training and testing partitions of the MSTAR dataset...|$|R
5000|$|No need to {{dehydrate}} tissues {{prior to}} <b>embedding,</b> thus decreased <b>loss</b> of cell constituents ...|$|R
40|$|In {{this paper}} we propose a novel {{framework}} for learning local image descriptors in a discriminative manner. For this purpose we explore a siamese architecture of Deep Convolutional Neural Networks (CNN), with a Hinge <b>embedding</b> <b>loss</b> on the L 2 distance between descriptors. Since a siamese architecture uses pairs rather than single image patches to train, there exist a large number of positive samples and an exponential number of negative samples. We propose to explore this space with a stochastic sampling approach of the training set, in combination with an aggressive mining strategy over both the positive and negative samples which we denote as “fracking”. We perform a thorough evaluation of the architecture hyper-parameters, and demonstrate very large performance gains compared to both stan-dard CNN learning strategies and hand-crafted image descriptors like SIFT, up to 2. 5 x in terms of the area under the Precision-Recall curve. ...|$|R
30|$|Chen et al. [65] {{proposed}} a neural network based framework called Transfer Neural Trees (TNT) for semi-supervised HDA tasks. This framework {{is divided into}} two layers: mapping and prediction. The first layer is that of mapping the source and target into a domain-invariant representation while the second layer performs adaptation and classification. This process of mapping, adaptation and classification are all solved in a joint manner. Mapping of the source domain data is performed separately to that of the target domain data while each using a single-layer neural network. For the case of this study, these neural networks apply the hyperbolic tangent as the activation function with an output dimension of 100. To account for the unlabeled target instances in semi-supervised tasks, an <b>embedding</b> <b>loss</b> term is incorporated into the target domain feature mapping. When performing adaptation and classification, minimizing this loss term can increase predictive consistency for the outputs of the individual trees and the forest as a whole. This process also preserves the structural consistency between the labeled and unlabeled target instances.|$|R
5000|$|The CVO {{is focused}} on {{security}} systems as they contribute to higher profits and elevated store performance levels. The importance of the CVO position has risen as criminals and organized retail crime grows more sophisticated in thwarting security solutions. According to the Centre for Retail Research Global Retail Theft Barometer 2011 study, Total global shrink in 2011 cost retailers $119.092 billion, an average of 1.45% of global retail sales. The shrinkage rate was an average of 6.6% higher than 2010. The losses are said to cost each family $199.89 per that year. Working with security integrators to achieve maximum visibility into what {{takes place in a}} retail store on a daily basis allows the CVO to impact not only shrinkage numbers, but also to improve store operations. This approach was cited by the University of Leicester study on Effective Retail Loss Prevention, 10 Ways to Keep Shrinkage Low [...] where it was noted that among the keys to reducing shrinkage in an organization included establishing a senior management commitment, ensuring organisational ownership, <b>embedding</b> <b>loss</b> prevention into the business and providing strong leadership.|$|R
40|$|Abstract We {{introduce}} a reduction-based model for an-alyzing supervised learning tasks. We use this model {{to devise a}} new reduction frommulti-class cost-sensitive classification to binary classification with the following guaran-tee: If the learned binary classifier has error rate at most ffl then the cost-sensitive classi-fier has cost at most 2 ffl times the expectedsum of costs of all possible lables. Since cost-sensitive classification can <b>embed</b> anybounded <b>loss</b> finite choice supervised learning task, this result shows that any such taskcan be solved using a binary classification oracle. Finally, we present experimental re-sults showing that our new reduction outperforms existing algorithms for multi-classcost-sensitive learning...|$|R
40|$|Abstract. The {{problem of}} {{embedding}} arises in many machine learning applications {{with the assumption}} that there may exist a small number of variabilities which can guarantee the “semantics ” of the original high-dimensional data. Most of the existing embedding algorithms perform to maintain the locality-preserving property. In this study, inspired by the remarkable success of representation learning and deep learning, we pro-pose a framework of embedding with autoencoder regularization (EAER for short), which incorporates embedding and autoencoding techniques naturally. In this framework, the original data are embedded into the lower dimension, represented by the output of the hidden layer of the autoencoder, thus the resulting data can not only maintain the locality-preserving property but also easily revert to their original forms. This is guaranteed by the joint minimization of the <b>embedding</b> <b>loss</b> and the autoencoder reconstruction error. It is worth mentioning that instead of operating in a batch mode as most of the previous embedding algo-rithms conduct, the proposed framework actually generates an induc-tive embedding model and thus supports incremental embedding effi-ciently. To show the effectiveness of EAER, we adapt this joint learning framework to three canonical embedding algorithms, and apply them to both synthetic and real-world data sets. The experimental results show that the adaption of EAER outperforms its original counterpart. Be-sides, compared with the existing incremental embedding algorithms, the results demonstrate that EAER performs incremental embedding with more competitive efficiency and effectiveness...|$|R
40|$|Hashing {{methods are}} {{effective}} in generating compact binary signatures for images and videos. This paper addresses an important open issue in the literature, i. e., how to learn compact hash codes by enhancing the complementarity among different hash functions. Most of prior studies solve this problem either by adopting time-consuming sequential learning algorithms or by generating the hash functions which are subject to some deliberately-designed constraints (e. g., enforcing hash functions orthogonal to one another). We analyze the drawbacks of past works and propose a new solution to this problem. Our idea is to decompose the feature space into a subspace shared by all hash functions and its complementary subspace. On one hand, the shared subspace, corresponding to the common structure across different hash functions, conveys most relevant information for the hashing task. Similar to data de-noising, irrelevant information is explicitly suppressed during hash function generation. On the other hand, in case that the complementary subspace also contains useful information for specific hash functions, the final form of our proposed hashing scheme is a compromise between these two kinds of subspaces. To make hash functions not only preserve the local neighborhood structure but also capture the global cluster distribution of the whole data, an objective function incorporating spectral <b>embedding</b> <b>loss,</b> binary quantization loss, and shared subspace contribution is introduced to guide the hash function learning. We propose an efficient alternating optimization method to simultaneously learn both the shared structure and the hash functions. Experimental results on three well-known benchmarks CIFAR- 10, NUS-WIDE, and a-TRECVID demonstrate that our approach significantly outperforms state-of-the-art hashing methods...|$|R
40|$|Empirical {{studies using}} survey data on {{expectations}} have frequently observed that forecasts are biased and {{have concluded that}} agents are not rational. We establish that existing rationality tests are not robust to even small deviations from symmetric loss and hence have little ability to tell whether the forecaster is irrational or the loss function is asymmetric. We quantify the exact trade-off between forecast inefficiency and asymmetric loss leading to identical outcomes of standard rationality tests and explore new and more general methods for testing forecast rationality jointly with flexible families of <b>loss</b> functions that <b>embed</b> quadratic <b>loss</b> as a special case. An empirical application to survey data on forecasts of nominal output growth demonstrates the empirical significance of our results and finds that rejections of rationality may largely have been driven by the assumption of symmetric loss...|$|R
40|$|Survey data on {{expectations}} frequently {{find evidence}} that forecasts are biased, rejecting the joint hypothesis of rational expectations and symmetric loss. While the literature {{has attempted to}} explain this bias through forecasters' strategic behavior, we propose a simpler explanation based on asymmetric loss. We establish that existing rationality tests are not robust to even small deviations from symmetry and hence have little ability to tell whether the forecaster is irrational or the loss function is asymmetric. We propose new and more general methods for testing forecast rationality jointly with flexible families of <b>loss</b> functions that <b>embed</b> quadratic <b>loss</b> as a special case. An empirical application to survey data on forecasts of nominal output growth shows strong evidence against rationality and symmetric loss. There is considerably weaker evidence against rationality once asymmetric loss is permittedrationality testing, forecasting, asymmetric loss...|$|R
40|$|The c-myc {{protein is}} thought to be a DNA-associated nuclear protein. However, {{immunohistochemical}} studies on normal or tumor tissues have shown conflicting findings on its subcellular distribution. By using various fixation procedures on cytospin preparations of HL 60 cells, the authors found the subcellular distribution of the c-myc protein to be dependent on the method of fixation. When studying mouse tissues in frozen sections using a biotinylated monoclonal antibody against the c-myc protein, they found the protein to be widely distributed in various normal adult mouse tissues, in most cases localized to the nucleus. However, when these tissues were studied after formalin fixation and paraffin <b>embedding,</b> a <b>loss</b> of nuclear staining was observed concurrent with the appearance of c-myc protein immunoreactivity in the cytoplasm. It is concluded that immunohistochemical studies on the expression of this oncogene should take into consideration the effects of fixation when its subcellular distribution is being examined...|$|R
40|$|We {{propose a}} blind {{watermarking}} technique to embed multiple watermarks simultaneously. It allows {{the use of}} correlated key to embed multiple watermarks. A dual-key system is used to reduce the chance of the removal of watermark. Each embedded watermark can be decoded/detected by its own key. One direct approach and two iterative approaches are proposed to reduce the correlation effects among the noise-like keys. Experimental results show that multiple watermarks can be <b>embedded</b> without significant <b>loss</b> in PSNR and the watermark can be detected when the watermarked image is JPEG compressed. 1...|$|R
40|$|The highly skewed {{and heavy}} tailed {{distributions}} used to model insurance losses (claims) raise {{a concern about}} the validity of the applications of the capital asset pricing model (CAPM) to insurance pricing when market risks are essential. This paper provides an alternative pricing model, called the Rubinstein-Leland model, which can be used to price insurance contracts. The Rubinstein-Leland model has a distribution-free feature that can fully capture the asymmetry <b>embedded</b> in insurance <b>losses.</b> Thus, this model is better able to derive fair prices for insurance policies than is the CAPM...|$|R
40|$|We {{introduce}} a reduction-based model for analyzing supervised learning tasks. We use this model {{to devise a}} new reduction from cost-sensitive classification to binary classification with the following guarantee: If the learned binary classifier has error rate at most ɛ then the cost-sensitive classifier has cost at most 2 ɛ times the expected sum of costs of all choices. Since cost-sensitve classification can <b>embed</b> any bounded <b>loss</b> finite choice supervised learning task, this result shows that any such task can be solved using a binary classification oracle. Finally, we present experimental results showing that our new reduction outperforms existing algorithms for multi-class cost-sensitive learning. ...|$|R
30|$|We {{implement}} the multi-task deblurring network on the Pytorch platform and train the network on NVIDIA Titan X GPU. We set the batch size of 16 and learning rate to 1 e− 4. To guarantee {{the convergence of}} the multi-task framework, we firstly train the multi-task deblurring network with a content loss for about 5 days. we then add the perceptual loss and adversarial loss individually for joint training. Specifically, first, we train this multi-task deblurring network using the loss (2) for 100, 000 iterations. Second, we <b>embed</b> the perceptual <b>loss</b> (3) for 50, 000 iterations. Finally, we add the adversarial loss (4) and jointly train this network for 50, 000 iterations.|$|R
40|$|This paper {{estimates}} the <b>losses</b> <b>embedded</b> {{in the capital}} positions of the 996 FSLIC-insured savings and loan institutions that did not meet capital standards {{at the end of}} the 1970 s. We compare the estimated cost of resolving the insolvencies of these institutions {{at the end of the}} 1970 s with the actual failure-resolution costs for those that were closed by July 3 1, 1992, and the projected resolution costs for the remaining thrifts that are likely to be closed. Our results show that even when one considers only the direct costs associated with delayed closure of economically failed thrifts, these costs significantly exceed reasonable estimates of the cost of prompt failure resolution. Savings and loan associations...|$|R
40|$|Procedures {{developed}} from Hagfors' (1964) scattering model {{have been used}} with success to extract various properties (reflectivity, emissivity, slope, etc.) from both Pioneer Venus and Magellan data. However, under certain circumstances, such as physically rough areas or areas of nonhomogeneous composition, Hagfors' model may lead to serious errors {{in the interpretation of}} radar and radiometry data. The Magellan-derived surface properties are examined for some areas of below average emissivity in Alpha Regio and in an area to the east of Alpha, Crater Stuart, in order to determine if Hagfors' model is applicable. Methods of classical multiple scattering theory are also applied to show that enhanced radar backscattering and depressed thermal emission can be expected with a matrix of silicic or mafic rocks <b>embedded</b> in low <b>loss</b> soils...|$|R
40|$|Cermet {{coatings}} {{are popular}} solar selective absorbers as they allow capturing {{most of the}} solar energy while minimising radiative <b>losses.</b> <b>Embedded</b> metallic nanoparticles in dielectric matrices promote multiple internal reflection of light and provide an overall low emissivity. VO 2 in the metamaterial state is regarded in this study as a responsive mixed phase comprising metallic rutile VO 2 inclusions in semiconducting monoclinic VO 2 phase mimicking cermet. The smart cermet responds to thermal stimuli by modulating {{the size of the}} metallic inclusions and thereby enabling the manipulation of their interaction with light. The highly reliable and reproducible response of the smart cermet corroborates with the observed ramp reversal memory effect in VO 2. We demonstrate a thermally controlled 85...|$|R
40|$|We propose an {{approach}} for capturing the signal variability in hyperspectral imagery using {{the framework of}} the Grassmann manifold. Labeled points from each class are sampled and used to form abstract points on the Grassmannian. The resulting points on the Grassmannian have representations as orthonormal matrices and as such do not reside in Euclidean space in the usual sense. There are a variety of metrics which allow us to determine a distance matrices {{that can be used to}} realize the Grassmannian as an embedding in Euclidean space. We illustrate that we can achieve an approximately isometric embedding of the Grassmann manifold using the chordal metric while this is not the case with geodesic distances. However, non-isometric embeddings generated by using a pseudometric on the Grassmannian lead to the best classification results. We observe that as the dimension of the Grassmannian grows, the accuracy of the classification grows to 100 % on two illustrative examples. We also observe a decrease in classification rates if the dimension of the points on the Grassmannian is too large for the dimension of the Euclidean space. We use sparse support vector machines to perform additional model reduction. The resulting classifier selects a subset of dimensions of the <b>embedding</b> without <b>loss</b> in classification performance...|$|R
40|$|International audienceCermet {{coatings}} {{are popular}} solar selective absorbers as they allow capturing {{most of the}} solar energywhile minimising radiative <b>losses.</b> <b>Embedded</b> metallic nanoparticles in dielectric matrices promotemultiple internal reflection of light and provide an overall low emissivity. VO 2 in the metamaterial stateis regarded in this study as a responsive mixed phase comprising metallic rutile VO 2 inclusions insemiconducting monoclinic VO 2 phase mimicking cermet. The smart cermet responds to thermal stimuliby modulating {{the size of the}} metallic inclusions and thereby enabling the manipulation of theirinteraction with light. The highly reliable and reproducible response of the smart cermet corroborateswith the observed ramp reversal memory effect in VO 2. We demonstrate a thermally controlled 85 %emissivity switch taking advantage of the narrow hysteresis and tuning abilities of the disorderedmetamaterial...|$|R
40|$|In {{applications}} involving matching {{of image}} sets, {{the information from}} multiple images must be effectively exploited to represent each set. State-of-the-art methods use probabilistic distribution or subspace to model a set and use specific distance measure to compare two sets. These methods are slow to compute and not compact to use {{in a large scale}} scenario. Learning-based hashing is often used in large scale image retrieval as they provide a compact representation of each sample and the Hamming distance can be used to efficiently compare two samples. However, most hashing methods encode each image separately and discard knowledge that multiple images in the same set represent the same object or person. We investigate the set hashing problem by combining both set representation and hashing in a single deep neural network. An image set is first passed to a CNN module to extract image features, then these features are aggregated using two types of set feature to capture both set specific and database-wide distribution information. The computed set feature is then fed into a multilayer perceptron to learn a compact binary <b>embedding.</b> Triplet <b>loss</b> is used to train the network by forming set similarity relations using class labels. We extensively evaluate our approach on datasets used for image matching and show highly competitive performance compared to state-of-the-art methods...|$|R
40|$|In the {{framework}} of the problem of combining different gene trees into a unique species phylogeny, a model for duplication/speciation/loss events along the evolutionary tree is introduced. The model is employed for embedding a phylogeny tree into another one via so called Duplication/Speciation principle requiring that the gene duplicated evolves {{in such a way that}} any of the contemporary species involved bears only one of the gene copies diverged. The number of biologically meaningful elements in the <b>embedding</b> result (duplications, <b>losses,</b> information gaps) is considered a (asymmetric) dissimilarity measure between the trees. The model duplication concept is compared with that one defined previously in terms of a mapping procedure for the trees. A graph-theoretic reformulation of the measure is derived. Contents 1 Introduction 2 2 Model for Duplications in Phylogenies 5 3 Method for Comparing a Gene Tree with a Species Tree 9 3. 1 Root Inconsistency and Duplication : : : : : : : : [...] ...|$|R
40|$|This paper {{addresses}} {{the allocation of}} electrical losses in distribution networks with embedded generation, in a liberalized environment. The nonlinear nature of the issue, the loss changes due to voltage variation and, specially, the contribution of <b>embedded</b> generation to <b>loss</b> variation are considered. The proposed method is based on tracing the real and imaginary parts of the currents and has two steps. First, the losses in the distribution network, {{in the absence of}} embedded generation, are allocated to the consumers (or their providers). Second, the variations in the losses that result from the influence of embedded generation are allocated to the generators. These variations are a measure of the avoided or added costs related to losses. In the allocation process, made in a branch basis, both real and reactive powers are considered. The methodology presented in this paper can be used to evaluate embedded generation incentives or to design tariffs {{for the use of the}} distribution network...|$|R
40|$|We {{introduce}} the simplest one-dimensional nonlinear model with the parity-time (PT) symmetry, {{which makes it}} possible to find exact analytical solutions for localized modes ("solitons"). The PT-symmetric element is represented by a point-like (delta-functional) gain-loss dipole δ^'(x), combined with the usual attractive potential δ(x). The nonlinearity is represented by self-focusing (SF) or self-defocusing (SDF) Kerr terms, both spatially uniform and localized ones. The system can be implemented in planar optical waveguides. For the sake of comparison, also introduced is a model with separated δ-functional gain and <b>loss,</b> <b>embedded</b> into the linear medium and combined with the δ-localized Kerr nonlinearity and attractive potential. Full analytical solutions for pinned modes are found in both models. The exact solutions are compared with numerical counterparts, which are obtained in the gain-loss-dipole model with the δ^'- and δ- functions replaced by their Lorentzian regularization. With the increase of the dipole's strength, γ, the single-peak shape of the numerically found mode, supported by the uniform SF nonlinearity, transforms into a double-peak one. This transition coincides with the onset of the escape instability of the pinned soliton. In the case of the SDF uniform nonlinearity, the pinned modes are stable, keeping the single-peak shape. Comment: 21 pages, 11 figures, Physical Review E, in pres...|$|R
40|$|Let {{me begin}} with a {{somewhat}} provocative slide. I’ve been updating and extending cross-country data on the fiscal costs of previous banking crises and this (Fig 1) shows the estimated fiscal costs of some 78 systemic crises {{over the past four}} decades. Now if we take the IMF’s estimate of the total potential credit losses (banks and nonbanks) in the latest events and round it up to USD 1 trillion, where would it fit into this chart? If we take EU+US GDP as the denominator, we get to the yellow bar (3 % of GDP). And those are mostly costs absorbed by private shareholders: making a reasonable provision for the losses related to the main official bailouts that have already occurred would move us over to the red bar (% of GDP). Of course the denominator is large – but many of the countries in the chart are also big – 47 % for the 1. 3 billion people in China….) Now, I do not want to minimize the importance of the recent events. Far from it. For one thing, the dynamics of the current crisis have not yet fully worked themselves out. Even the total credit <b>losses</b> <b>embedded</b> in existing financial intermediary portfolios remain quite unclear. Furthermore, the reductions in bank capital (even though partly made good with new equity issues), the liquidity premia and generalized uncertainty abou...|$|R
40|$|The {{concept of}} pointwise Fisher {{consistency}} (or classification calibration) states necessary and sufficient conditions to have Bayes consistency when a classifier minimizes a surrogate loss function {{instead of the}} 0 - 1 loss. We present a family of multiclass hinge loss functions defined by a continuous control parameter. representing {{the margin of the}} positive points of a given class. The parameter. allows shifting from classification uncalibrated to classification calibrated loss functions. Though previous results suggest that increasing the margin of positive points has positive effects on the classification model, other approaches have failed to give increasing weight to the positive examples without losing the classification calibration property. Our lambda-based loss function can give unlimited weight to the positive examples without breaking the classification calibration property. Moreover, when <b>embedding</b> these <b>loss</b> functions into the Support Vector Machine's framework (lambda-SVM), the parameter. defines different regions for the Karush-Kuhn-Tucker conditions. A large margin on positive points also facilitates faster convergence of the Sequential Minimal Optimization algorithm, leading to lower training times than other classification calibrated methods. lambda-SVM allows easy implementation, and its practical use in different datasets not only supports our theoretical analysis, but also provides good classification performance and fast training times. The authors acknowledge the referees' comments and suggestions that helped to improve the manuscript. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the Federal Bureau of Investigations, Finance Division. The views and conclusions contained herein {{are those of the authors}} and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U. S. Government. The U. S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. I. R-L acknowledges partial support by Spain's grants TIN 2013 - 42351 -P (MINECO) and S 2013 /ICE- 2845 CASI-CAM-CM (Comunidad de Madrid). The authors gratefully acknowledge the use of the facilities of Centro de Computacion Cientifica (CCC) at Universidad Autonoma de Madrid...|$|R
40|$|This paper {{presents}} a conceptual {{view of the}} digital steganography & exploits {{the use of a}} host data to hide a piece of information that is hidden directly in media content, {{in such a way that}} it is imperceptible to a human observer, but easily detected by a computer. The principal advantage of this is that the content is inseparable from the hidden image. Steganography has long been a means of secure communication. Security is achieved by camouflaging the secret message. The main application of steganography includes that military application where military organizations use unobtrusive communications. The steganography means in military applications include Spread Spectrum and Meteor Scatter radio which gives various combinations of resistances to detection. Other applications include medical safety, indexing of voice mails, etc. The wavelet domain is growing up very quickly. Wavelets have been effectively utilized as a powerful tool in many diversified fields, including approximation theory, signal processing, physics, astronomy and image processing. Integer wavelet transforms that map integers to integers allow perfect reconstruction of the original image. Hence, an algorithm that embeds the message bit stream into the LSB’s of the integer wavelet coefficients of true cover image is presented. The algorithm presented here also applies a pre-processing step on the cover image to adjust saturated pixel components in order to recover the <b>embedded</b> message without <b>loss.</b> Simulation results showed the high invisibility of the model even with large message size. The simulation results presented here shows the effectiveness of the method presented...|$|R
40|$|The soil in {{conventional}} Mediterranean vineyards {{is an active}} and non-sustainable source of sediment and water. Lack of vegetation cover, small soil organic matter content and intense ploughing result in large rates of erosion in a millennia-old tillage system. There {{is a need for}} soil conservation strategies that enable sustainability of wine and grape production; therefore, it is essential to measure the rates and to investigate the processes and factors of soil erosion. This study evaluated factors that can reduce soil losses in traditional Mediterranean vineyards. The investigation was carried out with 96 rainfall simulation experiments at the pedon scale (0. 24 m 2) to measure soil detachment and runoff yield under low frequency-high magnitude rainfall events of 1 hour at 55 mm hour- 1. On average, runoff was 40. 6 % of the rainfall, and the rate of soil erosion (i. e. amount of soil lost) was 71. 5 g m- 2. The key factor controlling erosion was the rock fragment cover. There was a clear decrease in soil losses with increased rock fragment cover on the soil surface, but an increase in surface runoff. The results of our study showed that rock fragments at the pedon scale reduced soil erosion in Mediterranean vineyards, but when a layer of embedded rock fragments developed, large rates of runoff were triggered. Highlights: We investigated soil erosion factors in Mediterranean vineyards. Rainfall simulation at the pedon scale achieved accurate measurements. Rock fragment cover reduces soil <b>losses.</b> <b>Embedded</b> rock fragment cover will trigger large runoff rates...|$|R
40|$|What {{happens to}} a body when {{circumstance}} demands it enact its own forgetting? What reaction in turn does a body {{in the process of}} violent self-erasure prompt in its spectators? These and related questions propel my investigation of Katie Mitchell 2 ̆ 7 s 2004 National Theatre production of Euripedes 2 ̆ 7 Iphigenia at Aulis. Mitchell 2 ̆ 7 s chilling representation of Iphigenia 2 ̆ 7 s final moments, during which the young girl speaks with apparently patriotic fervour her willingness to be murdered for her nation 2 ̆ 7 s sake, <b>embeds</b> the very <b>loss</b> that such a performance of sacrifice typically elides. The result: two bodies collide on stage before our eyes - the compliant, self-effacing body sacrifice demands, and the physically and emotionally overwhelmed body sacrifice denies. Mitchell 2 ̆ 7 s practice of 2 ̆ 7 radical naturalism 2 ̆ 7, an acting technique that straddles Stanislavsky and Brecht and infects her stage with an uncanny critique of realism even as it inhabits the genre with clockwork precision, works together with innovative staging techniques in this production in order to force an uncomfortable proximity between audiences, actors and characters. The overwhelming affect performed by Hattie Morahan as Iphigenia, combined with the uncanny echo of its amplification via an onstage microphone that appears to give Morahan 2 ̆ 7 s voice a body of its own, stage Mitchell 2 ̆ 7 s challenge to contemporary spectators: can we reassess the codes and attitudes by which we recognize and receive realist performance? More importantly, can we re-imagine what it might mean, at the theatre, to bear witness to a body captured in the moment of its most profound loss...|$|R
40|$|MCom (Risk Management), North-West University, Potchefstroom Campus, 2015 The Basel Committee on Banking Supervision (BCBS) {{designed}} the Internal Ratings Based (IRB) approach, {{which is based}} on a single risk factor model. This IRB approach was de-signed to determine banks’ regulatory capital for credit risk. The asymptotic single risk factor (ASRF) model they used makes use of prescribed asset correlations, which banks must use for their credit risk regulatory capital, in order to abide by the BCBS’s rules. Banks need to abide by these rules to reach an international standard of banking that promotes the health of the specific bank. To evaluate whether these correlations are as conservative as the BCBS intended, i. e. not too onerous or too lenient, empirical asset correlations <b>embedded</b> in gross <b>loss</b> data, spanning different economic milieus, were backed out of the regulatory credit risk model. A technique to extract these asset correlations from a Vasicek distribution of empirical loan losses was proposed and tested in international markets. This technique was used to extract the empirical asset correlation, and then compare the prescribed correlations for developed (US) and developing (South Africa) economies over the total time period, as well as a rolling time period. For the first analysis, the BCBS’s asset correlation was conservative when com-pared to South Africa and the US for all loan types. Comparing the empirical asset correlation over a seven-year rolling time period for South Africa and the BCBS, the specified asset cor-relation was found to be as conservative as the BCBS intended. Comparing the US empirical asset correlation for the same rolling period to that of the BCBS, it was found that for all loans, the BCBS was conservative, up until 2012. In 2012 the empirical asset correlation sur-passed that of the BCBS, and thus the BCBS was not as conservative as they had originally intended. Master...|$|R

