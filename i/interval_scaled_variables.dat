2|3346|Public
40|$|Grouping or {{unsupervised}} classification has variety of demands {{in which the}} major one is the capability of the chosen clustering approach to deal with scalability and to handle the mixed variety of data set. There are variety of data sets like categorical/nominal, ordinal, binary (symmetric or asymmetric), ratio and <b>interval</b> <b>scaled</b> <b>variables.</b> In the present scenario, latest approaches of {{unsupervised classification}} are Swarm Optimization based, Customer Segmentation based, Soft Computing methods like Fuzzy Based and GA based, Entropy Based methods and hierarchical approaches. These approaches have two serious bottlenecks…Either they are hybrid mathematical techniques or large computation demanding which increases their complexity and hence compromises with accuracy. It {{is very easy to}} compare and analyze that unsupervised classification by Genetic Algorithm is feasible, suitable and efficient for high-dimensional data sets with mixed data values that are obtained from real life results, events and happenings...|$|E
30|$|In {{addition}} {{to finding the}} feasibility of parcelling questionable, the necessity of using parcelling to conduct CFAs deserves scrutinisation. The rationale for parcelling {{lies in the fact}} that ordinal data (such as Likert scale data) often does not satisfy the assumptions of continuity and multivariate normality that various estimation techniques in structural equation modelling, such as maximum likelihood estimation, rely on, while total scores of ordinal items tend to be continuous and normally distributed (Kline 2011). However, Blunch (2008) stated that ordinal variables can be treated as if they were normally distributed <b>interval</b> <b>scaled</b> <b>variables</b> if they can take on at least five possible values, if their skewness and kurtosis values are close to zero, and if a possible limited skewness goes to the same side for all variables. These pointers need to be evaluated and if they are satisfied, commonly used estimation methods in CFA, like maximum likelihood, can be applied and parcelling would not be needed.|$|E
40|$|This papaer {{considers}} {{the problem of}} <b>interval</b> <b>scale</b> data in {{the most widely used}} models of Data Envelopment Analysis (DEA), the CCR, and the BCC models. Radial models require inputs and outputs measured on the ratio scale. Our focus is {{on how to deal with}} <b>interval</b> <b>scale</b> <b>variables</b> especially when the <b>interval</b> <b>scale</b> <b>variable</b> is a difference of two ratio <b>scale</b> <b>variables</b> like profit or the decrease/increase in bank accounts. Using these ratio <b>scale</b> <b>variables</b> as variables in the DEA model we suggest radial models. An approach to how to deal with <b>interval</b> <b>scale</b> <b>variables</b> when we relax the radiality assumption is also discussed. ...|$|R
40|$|This paper {{considers}} {{the problem of}} <b>interval</b> <b>scale</b> data in {{the most widely used}} models of Data Envelopment Analysis (DEA), the CCR and BCC models. Radial models require inputs and outputs measured on the ratio scale. Our focus is {{on how to deal with}} <b>interval</b> <b>scale</b> <b>variables</b> especially when the <b>interval</b> <b>scale</b> <b>variable</b> is a difference of two ratio <b>scale</b> <b>variables</b> like profit or the decrease/increase in bank accounts. Using these ratio <b>scale</b> <b>variables</b> as variables in a DEA model we suggest radial models. An approach to how to deal with <b>interval</b> <b>scale</b> <b>variables</b> when we relax the radiality assumption is also discussed Keywords: Efficiency Analysis, Data Envelopment Analysis, <b>Interval</b> <b>Scale</b> <b>Variables,</b> Negative Variables Acknowledgments The research was supported, in part, by grants from the Foundation of the Helsinki School of Economics and Business Administration and Academy of Finland. The authors wish to thank Professor Pekka Korhonen, IIASA, for valuable comments. About the Authors [...] ...|$|R
40|$|Data Envelopment Analysis (DEA) {{models with}} {{interval}} {{inputs and outputs}} have been rarely discussed in DEA literature. This paper, using the enhanced Russell measurement proposes an extended model which permits the presence of <b>interval</b> <b>scale</b> <b>variables</b> which can take both negative and positive values. The model is compared with most well-known DEA models of which include the CCR model, the BCC model and the additive model. An empirical data set is used to illustrate the model...|$|R
40|$|Abstract: This paper {{considers}} {{the problem of}} <b>interval</b> <b>scale</b> data in Data Envelopment Analysis (DEA) models. Our focus is on how to measure value efficiency in DEA models with negative data especially when these data derived from <b>interval</b> <b>scale</b> <b>variables.</b> At first, we search for the most preferred combination of inputs/outputs of Decision Making Units (DMUs). Then, by approximation of indifference contour of the unknown value function at this Most Preferred Solution (MPS), value efficiency scores are calculated related to the units having the same value as the MPS. Key words: Data envelopment analysis • negative data • <b>interval</b> <b>scale</b> data • most preference solution • value functio...|$|R
40|$|This paper {{describes}} an empirical study to reveal rules associated with defect correction effort. We defined defect correction effort as a quantitative (ratio <b>scale)</b> <b>variable,</b> and extended conventional (nominal scale based) association rule mining to directly handle such quantitative variables. An extended rule describes the statistical {{characteristic of a}} ratio or <b>interval</b> <b>scale</b> <b>variable</b> in the consequent part of the rule by its mean value and standard deviation so that conditions producing distinctive statistics can be discovered. As an analysis target, we collected various attributes of about 1, 200 defects found in a typical medium-scale, multi-vendor (distance development) information system development project in Japan. Our findings based on extracted rules include: (1) Defects detected in coding/unit testing were easily corrected (less than 7 % of mean effort) when they are related to data output or validation of input data. (2) Nevertheless, they sometimes required much more effort (lift of standard deviation was 5. 845) in case of low reproducibility, (3) Defects introduced in coding/unit testing often required large correction effort (mean was 12. 596 staff-hours and standard deviation was 25. 716) when they were related to data handing. From these findings, we confirmed {{that we need to}} pay attention to types of defects having large mean effort as well as those having large standard deviation of effort since such defects sometimes cause excess effort. 1...|$|R
40|$|Abstract OBJECTIVES/HYPOTHESES: To {{investigate}} the interrater reliability of stroboscopy evaluations assessed using Poburka's Stroboscopy Evaluation Rating Form (SERF). STUDY DESIGN: Single-factor experiment with repeated measures {{on the same}} element. METHODS: Evaluations of nine experts pertaining to 68 stroboscopy recordings and 16 SERF variables were analyzed. For the 14 SERF <b>variables</b> possessing <b>interval</b> <b>scale</b> level, interrater reliability was investigated using the intraclass correlations for absolute agreement (ICC-a) and consistency (ICC-c). ICCs-c were computed for both original values and values standardized with respect to raters' {{means and standard deviations}} (ipsative values). For the two nominally <b>scaled</b> SERF <b>variables,</b> "vertical level" and "glottal closure" interrater reliability was investigated using kappa coefficients. RESULTS: For evaluations of single raters, ICCs-a ranged from 0. 32 to 0. 71, ICCs-c for original values from 0. 41 to 0. 72, and ICCs-c for ipsative values from 0. 43 to 0. 72. For mean evaluations of two raters, the corresponding values were 0. 48 to 0. 83 for ICCs-a, 0. 58 to 0. 84 for ICCs-c for original values, and 0. 60 to 0. 84 for ICCs-c for ipsative values. The <b>interval</b> <b>scale</b> <b>variables</b> with the lowest interrater reliabilities were phase closure, phase symmetry, and regularity. The kappa coefficients for vertical level and glottal closure were 0. 15 and 0. 38, respectively. CONCLUSIONS: The interrater reliabilities for vertical level, glottal closure, phase closure, phase symmetry, and regularity are so low that these variables should not be assessed via stroboscopy. For the remaining variables, adequate reliability can be obtained by aggregating evaluations from at least two raters...|$|R
40|$|AbstractThe Munsell {{color system}} is almost universally used for {{measuring}} colors of archeological artifacts. In addition to recording soil colors, many use the Munsell system {{to record the}} colors of ceramic attributes, such as pastes, slips, glazes, and paints. These data are often used in both modal and typological analyses of ceramic style. Most often, however, Munsell color data are used in a purely descriptive fashion, and the quantitative potential of the information is not fully exploited. We propose here a new protocol for manipulating, analyzing, and interpreting Munsell color data that permits the statistical investigation of sets of Munsell observations as well as hypothesis testing. A Munsell color reading is composed of three continuous <b>interval</b> <b>scale</b> <b>variables,</b> and these data can be transformed into x, y, z coordinates that define a location in color space (D'Andrade and Romney, 2003). Once transformed into spatial coordinates, Munsell data can be analyzed using spatial analysis techniques, such as k-means cluster analysis, as well as non-spatial statistical methods. In our case, we chose to use logistic regression to study {{the degree to which}} color could differentiate ceramic types and varieties. We argue that logistic regression is an appropriate approach for testing common types of hypotheses that archeologists may pose with ceramic color data. We illustrate the approach on sets of Munsell data representing ceramic slip colors from Mayapán, Yucatán, México. We were able to show a clear separation between the Mama Red and Polbox Buff types. Using the techniques we suggest here, archeologists can significantly expand their modal analyses of ceramics because color is a common and easily measured attribute in most ceramic assemblages. The same techniques can easily be extended to other kinds of artifacts, including lithics and textiles, as well as soils, sediments, plants, and other objects studied by field scientists...|$|R
40|$|The {{purpose of}} this {{research}} is: 1. Analyze performance benefits and working discipline simultaneously {{on the performance of}} civil servants on Agriculture Quarantine Class I Banjarmasin. 2. Analyze the performance benefits on the performance of civil servants at the Institute for Agriculture Quarantine Class I Banjarmasin. 3. Analyze labor discipline on the performance of civil servants on Agriculture Quarantine Class I Banjarmasin. Explanatory research, the sampling technique using saturated sampling technique with the number of respondents is 66 civil servants. The research instrument used questionnaires and measurement <b>interval</b> <b>scale</b> <b>variable</b> with the characteristics of the respondents were civil servants at the Institute for Agriculture Quarantine Class I Banjarmasin who receive performance benefits since 2012. Allowance performance (X 1) and discipline (X 2) as independent variables to be tested simultaneously (Simultaneous) influence on the performance of civil servants (Y) as the dependent variable. Examine the effect of direct (partial) performance benefits (X 1) on the performance of civil servants (Y) and examine the effect of direct (partial) labor discipline (X 2) on the performance of civil servants (Y). The data obtained were analyzed qualitatively and quantitatively by using analytical tools such as validity, reliability, and multiple regression analysis with the classical assumption using multicollinearity test, test heterokedastisitas, test for normality and linearity test, to test the hypothesis by using the F test and t test with IBM SPSS version 19 program. The study states that the benefits of performance (X 1) and discipline (X 2) positive and significant impact on the performance of civil servants (Y). Performance benefits (X 1) significant positive effect on the performance of civil servants (Y) and discipline (X 2) significant positive effect on the performance of civil servants (Y). Keywords:  performance benefits, work discipline and performance...|$|R
40|$|Credit scoring {{models are}} usually {{formulated}} by fitting {{the probability of}} loan default {{as a function of}} individual evaluation attributes. Typically, these attributes are measured using a Likert-type scale, but are treated as <b>interval</b> <b>scale</b> explanatory <b>variables</b> to predict loan defaults. Existing models also do not distinguish between types of default, although they vary: default by an insolvent company and default by an insolvent debtor. This practice can bias the results. In this paper, we applied Quantification Method II, a categorical version of canonical correlation analysis, to determine the relationship between two sets of categorical variables: a set of default types and a set of evaluation attributes. We distinguished between two types of loan default patterns based on quantification scores. In the first set of quantification scores, we found knowledge management, new technology development, and venture registration as important predictors of default from non-default status. Based on the second quantification score, we found that the technology and profitability factors influence loan defaults due to an insolvent company. Finally, we proposed a credit-risk rating model based on the quantification score...|$|R
40|$|This thesis {{examined}} the psychological characteristics of clerical and laymen who had sexually abused children. A three group design was used which permitted comparisons {{to be made}} between a group of 30 clerical men who had sexually abused children, a group of 73 laymen who had sexually abused children, {{and a group of}} 30 laymen who had not sexually abused children. The following instruments (all but two of which are from the Sex Offender Assessment Pack) were included in the assessment protocol: the Personal Reactivity Index, the Interpersonal Reaction Inventory, the Assertiveness Inventory, the Locus of Control Inventory, the UCLA Emotional Loneliness Scale, the Self-Esteem Inventory, the Victim Empathy Scale, the Children and Sex Scale, the SHAPS Lie Scale, the Multiphasic Sex Inventory, and the NEO Personality Inventory-Revised. Groups were compared on dependent variables using analysis of variance with post hoc comparisons for <b>interval</b> <b>scale</b> <b>variables</b> and found to differ significantly on 11 of 18 variables. Clerical offenders were more conscientious than lay offenders, and were more agreeable, more empathically concerned, and reported greater social sexual desirability than normal controls. But they also had lower self-esteem than the normal control group. The lay offenders had greater neuroticism, less extraversion, less openness, more agreeableness, greater emotional loneliness, more empathic concern, more personal distress, lower self-esteem, less assertiveness, and social sexual desirability than normal controls. In addition to the ANOVAs a multivariate discriminant analysis (MDA) was completed to identify which set of dependent variables best predicted group membership. The MDA identified 2 discriminant functions that predicted group membership of 72 % of participants. Function 1 which accounted for 76 % of the variance, distinguished clerical and lay offenders from normal controls and included these variables: agreeableness, self-esteem, openness, social sexual desirability, extraversion, personal distress and neuroticism. Function 2, which accounted for 23 % of the variance, distinguished clerical offenders from the other two groups and included these variables: under assertiveness, conscientiousness, empathic concern and emotional loneliness. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
50|$|For example, {{applications}} of measurement models in educational contexts often indicate that total scores {{have a fairly}} linear relationship with measurements across the range of an assessment. Thus, some argue that {{so long as the}} unknown interval difference between ordinal scale ranks is not too <b>variable,</b> <b>interval</b> <b>scale</b> statistics such as means can meaningfully be used on ordinal <b>scale</b> <b>variables.</b> Statistical analysis software such as SPSS requires the user to select the appropriate measurement class for each variable. This ensures that subsequent user errors cannot inadvertently perform meaningless analyses (for example correlation analysis with a variable on a nominal level).|$|R
40|$|A {{simplified}} {{version of}} successive <b>intervals</b> <b>scaling</b> is described. Scale values for various datasets obtained with simplified successive <b>intervals</b> <b>scaling</b> are approximately linearly related to those obtained with traditional successive <b>intervals</b> <b>scaling</b> {{and with the}} method of pair comparisons. Simplified successive <b>intervals</b> <b>scaling</b> {{can be used with}} any number of stimuli and is as easy to apply as the method of equal-appearing intervals. However, simplified successive <b>intervals</b> <b>scaling</b> does not assume, as does the method of equal-appearing intervals, that rating categories or intervals are of equal width. Index terms: attitude measurement, pair comparisons <b>scaling,</b> successive <b>intervals</b> <b>scaling,</b> Thurstonian scaling...|$|R
40|$|This paper {{explores the}} factors that {{influence}} the consumers’ intentions towards purchasing behaviour. Specifically, the objective {{of this study is to}} determine the relationships between retail satisfaction, loyalty to the store and competitive resistance toward purchase intention among the staff of a university in the North of Malaysia. All variables were measured from developed instrument using 7 -point <b>interval</b> <b>scale.</b> The exogenous <b>variable</b> are retail satisfactions (8 items), loyalty to the store (9 items) while endogenous variables are competitive resistance (2 items) and purchase intention (2 items). A total of 103 staffs, based on random sampling method selected from various age, gender, education and income, participated in this study. The instrument is validated using exploratory factor analysis (EFA) resulting in three similar components as the exogenous. The data was then analysed using Structural Equation Modeling (SEM). The result shows that the goodness of fit indices of structural equation model are achieved at GFI= 0. 915, P-value= 0. 087, RMSEA= 0. 057 and ratio (cmin/df) = 1. 316. The finding supports one significant direct effect in the revised model, thus supporting the hypothesis regarding store loyalty is positively related to purchase intention (β= 0. 961, CR= 3. 578, p< 0. 001). The structural model explains 80. 7 per cent variance in purchase intention. The result is discussed in the perspective of consumers’ loyalty towards purchase intention...|$|R
40|$|Several {{questions}} are raised concerning differences between traditional metric multiple regression, which assumes all variables {{to be measured}} on <b>interval</b> <b>scales,</b> and nonmetric multiple regression, which treats variables measured on any scale. Both models are applied to 30 derivation and cross-validation samples drawn from two sets of empirical data composed of ordinally <b>scaled</b> <b>variables.</b> Results indicate that the nonmetric model is, on the average, far superior in fitting derivation samples but that it exhibits much more shrinkage than the metric model. The metric technique fits better than the nonmetric in cross-validation samples. In addition, results produced by the nonmetric model are more unstable across repeated samples. A probable cause of these results is presented, {{and the need for}} further research is discussed. A common problem in data analysis involve...|$|R
40|$|It {{is often}} {{assumed that the}} {{measurement}} of utility attains the status of an ordinal but not of an <b>interval</b> <b>scale.</b> If utility arises from integrating information from different dimensions or attributes and trade-offs are permitted, such utility satisfies either <b>interval</b> <b>scale</b> status or only weak (often very weak) ordering can be attained. If, on the other hand, utility is regarded as determined behavioural from preference orders, {{it is very difficult}} to rank the goods without resorting to ratings based on <b>interval</b> <b>scales</b> unless the number of items is small. The combination of these two considerations should lead us to question seriously whether in practice ordinal utility is attainable unless <b>interval</b> <b>scale</b> status is also attainable...|$|R
5000|$|For {{interval}} data , where v and v' [...] are <b>interval</b> <b>scale</b> values.|$|R
5000|$|<b>Interval</b> <b>scales</b> are used. Relative {{assessments}} {{are more important}} than absolute assessments.|$|R
5000|$|The use of <b>interval</b> <b>scale</b> or {{ratio scale}} {{measurements}} of decision-makers' preferences; ...|$|R
5000|$|... a {{subjective}} assessment regarding of the article’s overall quality on an <b>interval</b> <b>scale</b> from 1 (low) to 7 (high).|$|R
50|$|JFugue 5.0 {{contains}} {{a set of}} classes that represent ideas in music theory, including <b>intervals,</b> <b>scales,</b> chords, and chord progressions.|$|R
5000|$|An <b>interval</b> <b>scale</b> is {{constructed}} by determining {{the equality of}} differences between the things measured. That is, numbers form an <b>interval</b> <b>scale</b> when {{the differences between the}} numbers correspond to differences between the properties measured. For instance, one can say that the difference between 5 and 10 degrees on a Fahrenheit thermometer equals the difference between 25 and 30, but it is meaningless to say that something with a temperature of 20 degrees Fahrenheit is [...] "twice as hot" [...] as something with a temperature of 10 degrees. (Such ratios are meaningful on an absolute temperature scale such as the Kelvin scale. See next section.) [...] "Standard scores" [...] on an achievement test are said to be measurements on an <b>interval</b> <b>scale,</b> but this is difficult to prove.|$|R
5000|$|Time-trade-off {{results are}} often used to {{calculate}} quality-adjusted life years (QALYs), allowing healthcare decisionmakers to combine mortality and morbidity into a single <b>interval</b> <b>scale.</b>|$|R
30|$|Measuring {{the right}} thing on a {{communicable}} scale lets us stockpile information about amounts. Such information can be useful, {{whether or not the}} chosen <b>scale</b> is an <b>interval</b> <b>scale.</b> Before the second law of thermodynamics—and there were many decades of progress in physics and chemistry before it appeared—the scale of temperature was not, in any nontrivial sense, an <b>interval</b> <b>scale.</b> Yet these decades of progress would have been impossible had physicists and chemists refused either to record temperatures or to calculate with them.|$|R
40|$|Traditionally, real GNP or {{permanent}} income or wealth {{have been the}} <b>scale</b> <b>variable</b> of choice in empirical money demand equations. Recently, Mankiw and Summers (1986) argue that consumer expenditures are an ideal proxy f{{or permanent}} income in money demand, and they provide evidence that total consumption expenditures or consumption expenditures on non-durables and services are better <b>scale</b> <b>variables</b> in money demand than current GNP. This result is odd because consumer expenditures reflect only the desires of the households, and {{a significant proportion of}} money balances is held by firms. This paper shows the difficulties in using consumer expenditures as a proxy for permanent income, shows that, properly estimated and compared, consumer expenditures are no better as a <b>scale</b> <b>variable</b> than real GNP and provides evidence that permanent income is a better <b>scale</b> <b>variable</b> than either consumer expenditures or GNP. Consumption (Economics); Money theory...|$|R
40|$|Objective: To {{exemplify the}} {{construction}} of <b>interval</b> <b>scales</b> for specified categories of the International Classification of Functioning, Disability and Health (ICF) by integrating items {{from a variety of}} patient-oriented instruments. Study Design and Setting: Psychometric study using data from a convenience sample of 122 patients with rheumatoid arthritis. Patients completed six different patient-oriented instruments. The contents of the instrument items were linked to the ICF. Rasch analyses for ordered-response options were used to examine whether the instrument items addressing the ICF category b 130 : Energy and drive functions constitute a psychometrically sound <b>interval</b> <b>scale.</b> Results: Nineteen items were linked to b 130 : Energy and drive functions. Sixteen of the 19 items fit the Rasch model according to the chi-square (chi(2)) statistic (chi(2) (df= 32) = 38. 25, P= 0. 21) and the Z-fit statistic (Z(Mean) = 0. 451, Z(SD) = 1. 085 and Z(Mean) =- 0. 223, Z(SD) = 1. 132 for items and persons, respectively). The Person Separation Index r(beta) was 0. 93. Conclusion: The ICF category <b>interval</b> <b>scales</b> to operationalize single ICF categories can be constructed. The original format of the items included in the <b>interval</b> <b>scales</b> remains unchanged. This study represents a step forward in the operationalization and future implementation of the ICF...|$|R
40|$|It is {{well known}} that data envelopment {{analysis}} (DEA) models are sensitive to selection of input and output variables. As the number of variables increases, the ability to discriminate between the decision making units (DMUs) decreases. Thus, to preserve the discriminatory power of a DEA model, the number of inputs and outputs should be kept at a reasonable level. There are many cases in which an <b>interval</b> <b>scale</b> output in the sample is derived from the subtraction of nonnegative linear combination of ratio scale outputs and nonnegative linear combination of ratio scale inputs. There are also cases in which an <b>interval</b> <b>scale</b> input is derived from the subtraction of nonnegative linear combination of ratio scale inputs and nonnegative linear combination of ratio scale outputs. Lee and Choi (2010) called such <b>interval</b> <b>scale</b> output and input a cross redundancy. They proved that the addition or deletion of a cross-redundant output variable does not affect the efficiency estimates yielded by the CCR or BCC models. In this paper, we present an extension of cross redundancy of <b>interval</b> <b>scale</b> outputs and inputs in DEA models. We prove that the addition or deletion of a cross-redundant output and input variable does not affect the efficiency estimates yielded by the CCR or BCC models...|$|R
40|$|This paper reexamines {{the choice}} of the <b>scale</b> <b>variable</b> in the money-demand function. A variety of {{evidence}} suggests that consumer spending is a better <b>scale</b> <b>variable</b> than GNP. Changing the money- demand specification along these lines can profoundly affect the standard Keynesian analysis of a tax cut. Copyright 1986 by Ohio State University Press. ...|$|R
3000|$|... {{stand for}} the finite time <b>scale</b> <b>interval,</b> {{infinite}} time <b>scale</b> <b>interval,</b> forward jump operator, backward jump operator, graininess and Δ-derivative of f. Further, we use the symbols [...]...|$|R
50|$|The {{valuation}} function V(S) is an empirically derived {{mapping of}} stimuli to an <b>interval</b> <b>scale.</b> It is unique {{up to an}} interval transformation (y = ax + b).|$|R
40|$|Reliability of the {{listening}} test design {{has a great}} influence {{on the performance of}} the quality estimation model. In this paper we compare four different listening test designs by Monte Carlo simulation. Three common problems of <b>interval</b> <b>scale</b> ratings are included in the simulation, and their influences on the performance of estimating the underlying true quality are investigated. It turns out that in these methods, randomly choosing partial trials for Scaled Comparison could be the most reliable way to perform listening test under the influences of <b>interval</b> <b>scale</b> ratings problems. 1...|$|R
40|$|This paper {{investigates the}} {{implications}} of alternative <b>scale</b> <b>variables</b> of money demand for the comparison of a flexible exchange rate regime with a monetary union in a New Open Economy Macroeconomics setup. The welfare evaluation of exchange rate regimes essentially depends on the exchange rate response under the flexible regime. When the <b>scale</b> <b>variable</b> is private consumption, a domestic fiscal expansion yields a depreciation of the domestic currency. The combined expenditure switching and terms-of-trade effects are beneficial to domestic households, who thus prefer a flexible exchange rate regime. However, when the <b>scale</b> <b>variable</b> is total absorption, the domestic currency appreciates and the welfare results are reversed. ...|$|R
40|$|When {{international}} relations theorists use {{the concept of}} risk aversion, they usually cite the economics conception involving concave utility functions. However, concavity is meaningful only when the goal is measurable on an <b>interval</b> <b>scale.</b> International decisions are usually not of this type, so that many statements appearing in the literature are formally meaningless. Applications of prospect theory face this difficulty especially, as risk aversion and acceptance are at their center. This paper gives two definitions of risk attitude {{that do not require}} an <b>interval</b> <b>scale.</b> The second and more distinctive one uses the property of submodularity in place of concavity. R. D. Luce has devised a theory of choice with features of prospect theory but not requiring on an <b>interval</b> <b>scale,</b> and the second definition in combination with this theory yields the traditional claim that decision makers are risk-averse for gains and risk-seeking for losses. (orig.) SIGLEAvailable from TIB Hannover: RO 3009 (445) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|Researchers often {{assume the}} {{numerical}} ratings approach {{used to measure}} values, such as Schwartz’s Value Survey (SVS), conforms to an <b>interval</b> <b>scale.</b> Correspondence {{analysis was used to}} examine this assumption by analyzing SVS data obtained from four Anglo (Australia, New Zealand, United Kingdom, and United States) and two Asian (South Korea and China) countries. The analysis suggested the SVS did not exhibit the characteristics of an <b>interval</b> <b>scale,</b> with responses across all countries producing larger intervals {{at the low end of}} the <b>scale</b> and smaller <b>intervals</b> from the mid to high end of the scale. Further analysis suggested there were significant differences in the traditional SVS means and the means suggested by the correspondence analysis. However, when correlations and Euclidian distances between SVS and correspondence analysis scores were examined, they were very high, suggesting the lack of <b>interval</b> <b>scaling</b> was unlikely to affect the relationships between the SVS value types and other constructs...|$|R
5000|$|This {{scaling law}} {{contains}} a purely imaginary <b>scaling</b> <b>variable</b> and {{a critical time}} scale ...|$|R
