91|10000|Public
5000|$|Error correction: Incorrect <b>input</b> <b>of</b> <b>data.</b> The data {{should be}} {{corrected}} {{and all the}} secondary effects of the incorrect data be removed.|$|E
5000|$|The {{dispersion}} models {{vary depending}} on the mathematics used to develop the model, but all require the <b>input</b> <b>of</b> <b>data</b> that may include: ...|$|E
50|$|There {{are several}} {{graphical}} interfaces to MODFLOW, which often include the compiled MODFLOW code with modifications. These programs aid the <b>input</b> <b>of</b> <b>data</b> for creating MODFLOW models.|$|E
5000|$|... (d) The term [...] "analysed {{information}}" [...] {{means the}} information {{resulting from the}} interpretation <b>of</b> processed <b>data,</b> <b>inputs</b> <b>of</b> <b>data</b> and knowledge from other sources; ...|$|R
5000|$|Handling {{of error}} conditions—both planned errors and {{exceptions}} (<b>input</b> <b>of</b> bad <b>data,</b> for example) ...|$|R
2500|$|... … indeed, {{there are}} no {{provisions}} in this specification for <b>input</b> <b>of</b> external <b>data</b> or output <b>of</b> computed results.|$|R
50|$|Steel - {{tools for}} {{standard}} BS steel sections, bracing, angles and beams. Automatic creation of handrails, catladders, steel stairs, fences and steel trusses including <b>input</b> <b>of</b> <b>data</b> from Prokon.|$|E
50|$|The Z22 {{operated}} at 3kHz operating frequency, {{which was}} synchronous {{with the speed}} of the drum storage. The <b>input</b> <b>of</b> <b>data</b> was possible via punch-card reader and by directly programming drum storage or core memory using pushbuttons.The Z22 also had glow-lamps which showed the memory state and machine state as output.|$|E
50|$|The general {{coordination}} {{of the system}} is based in the Department of Latin American Bibliography, part of the Assistant Office for Information Services of the General Directorate for Libraries (DGB) of the National Autonomous University of Mexico (UNAM). In each participant country there is an institution responsible for the <b>input</b> <b>of</b> <b>data</b> to the system’s databases.|$|E
25|$|<b>Input</b> <b>of</b> the {{fabrication}} <b>data.</b>|$|R
40|$|Disregard Restriction of Header and Footer: The {{purpose of}} the Analyst Templates is to provide the COBRA analyst {{with a set of}} paper forms that {{replicate}} all COBRA data entry screens that require the <b>input</b> <b>of</b> scenario specific <b>data.</b> These forms can then be provided to the COBRA user for the <b>input</b> <b>of</b> this <b>data</b> into COBRA. The forms can also be used for documentation of each COBRA scenario analyzed...|$|R
25|$|The {{ruling was}} {{announced}} on October 5, 2010, at 11 am. Jérôme Kerviel {{was found guilty}} of the three charges filed against him: breach <b>of</b> trust, fraudulent <b>inputting</b> <b>of</b> <b>data</b> into an IT system and forgery and use of forged documents. He was found to be solely responsible for the record loss suffered by Société Générale in early 2008, and was sentenced to five years in prison, with two of those years suspended, and ordered to pay damages of 4.9 billion euros to the Bank.|$|R
5000|$|The {{second step}} is the <b>input</b> <b>of</b> <b>data</b> which {{triggers}} the computation. In this case Range is a class holding an array and two indexes which allow {{the representation of}} a subarray. For every data entered into the framework a new Future object is created. More than one Future can be entered into a skeleton simultaneously.|$|E
5000|$|Third, the {{collective}} <b>input</b> <b>of</b> <b>data</b> {{from a variety}} of sensors fed into a central server. This included ground and infrared sensors designed to detect explosions around early-warning radar stations, command posts, and silos; missile signatures from radar stations, and data from the Oko early warning satellite system. Perimetr-PTS was deliberately designed not to launch {{in the event of a}} smaller strike from US allies or an Asian nuclear power, given their inability to wage 'total war'. It was also designed to discount the possibility of an earthquake or natural disaster, by referencing data from seismograph stations.|$|E
50|$|CURA {{partnered with}} Center for Aviation Studies at OSU to create maps {{displaying}} {{the accessibility of}} domestic air service in the United States. The maps show data for over 400 airports with an index describing the level of air service of a given region. The project requires the continued <b>input</b> <b>of</b> <b>data</b> {{to show how the}} accessibility of given airports changes over time. Graduate Affiliate Kejing Peng created a detailed web application displaying the results of the air traffic analysis. The T100 Airport Traffic Analysis is both interactive and informative to users, providing {{a great deal of information}} on individual airports across the country.|$|E
25|$|Despite the {{development}} <b>of</b> alternative <b>input</b> devices, {{such as the}} mouse, touchscreen, pen devices, character recognition and voice recognition, the keyboard remains {{the most commonly used}} device for direct (human) <b>input</b> <b>of</b> alphanumeric <b>data</b> into computers.|$|R
30|$|From {{the variety}} perspective, because the {{incoming}} data may use different types or have incomplete data, {{how to handle}} them also bring up another issue for the <b>input</b> operators <b>of</b> <b>data</b> analytics.|$|R
5000|$|The {{creation}} <b>of</b> <b>input</b> <b>data</b> {{based on}} the function's specifications ...|$|R
50|$|Agros2D is an {{open-source}} {{code for}} numerical solutions of 2D coupled problems in technical disciplines. Its principal part is a user interface serving for complete preprocessing and postprocessing {{of the tasks}} (it contains sophisticated tools for building geometrical models and <b>input</b> <b>of</b> <b>data,</b> generators of meshes, tables of weak forms for the partial differential equations and tools for evaluating results and drawing graphs and maps). The processor {{is based on the}} library Hermes containing the most advanced numerical algorithms for monolithic and fully adaptive solution of systems of generally nonlinear and nonstationary partial differential equations (PDEs) based on hp-FEM (adaptive finite element method of higher order of accuracy). Both parts of the code are written in C++.|$|E
50|$|Interpolation is {{the process}} by which a surface is created, usually a raster dataset, through the <b>input</b> <b>of</b> <b>data</b> {{collected}} at a number of sample points. There are several forms of interpolation, each which treats the data differently, depending on the properties of the data set. In comparing interpolation methods, the first consideration should be whether or not the source data will change (exact or approximate). Next is whether the method is subjective, a human interpretation, or objective. Then there is the nature of transitions between points: are they abrupt or gradual. Finally, there is whether a method is global (it uses the entire data set to form the model), or local where an algorithm is repeated for a small section of terrain.|$|E
5000|$|To {{achieve its}} thin chassis, the Macbook only {{contains}} a single, USB Type-C port, and a 3.5 mm headphone jack. The vertically symmetrical port supports transmission speeds {{of up to}} 5 Gbit/s, {{and can be used}} for charging, output and <b>input</b> <b>of</b> <b>data,</b> and output of video and audio. Apple markets an adapter that can provide a full-size USB connector, and a [...] "Digital AV Multiport Adapter" [...] with a charging pass-through, full-size USB port, and HDMI output. Although Thunderbolt 3 technology uses USB Type-C connectors, the MacBook's USB Type-C port does not support Thunderbolt. Thunderbolt devices, such as storage media and the Apple Thunderbolt Display, are not compatible with the MacBook. Shortly after the MacBook’s introduction, various companies began announcing cables and adapters for the USB Type-C port.|$|E
40|$|We {{present a}} study of {{performance}} in algorithms of inference in bayesian networks. Our results describe the influence of several properties <b>of</b> <b>input</b> <b>data</b> <b>of</b> inference algorithms on performance time and percentages of error on computed probabilities. We use bayesian networks of nxm bits multiplier an we study the algorithms available in the SMILE library [11]...|$|R
40|$|An {{apparatus}} for mixing {{a plurality}} <b>of</b> <b>input</b> <b>data</b> streams is described, {{which has a}} processing unit adapted to compare the frames of the plurality <b>of</b> <b>input</b> <b>data</b> streams, and determine, based on the comparison, for a spectral component of an output frame <b>of</b> an output <b>data</b> stream, exactly one <b>input</b> <b>data</b> stream <b>of</b> the plurality <b>of</b> <b>input</b> <b>data</b> streams. The output data stream is generated by copying at least a part of an information of a corresponding spectral component of the frame <b>of</b> the determined <b>data</b> stream. Further or alternatively, the control values of the frames {{of the first and}} second input data streams are compared, and, if so, the control value is adopted...|$|R
5000|$|Do {{not accept}} large amounts <b>of</b> <b>input</b> <b>data</b> or produce many results.|$|R
5000|$|Johnstone {{advanced}} and discussed {{the notion of}} rhetoric as [...] "a wedge," [...] {{in the sense that}} it serves as a tool to open a gap between the <b>input</b> <b>of</b> <b>data</b> and its acceptance. He wrote: [...] "By 'a wedge', I simply mean whatever introduces such a gap." [...] Rhetoric serves as a wedge, because it helps individuals better understand, rationalize, and make judgments. Therefore, if individuals already knew everything, there would be no need for rhetoric as a means of interpreting data. This process proceeds by a bilateral exchange of responses. Although one may not respond to the orator, one does in one's own mind and perhaps writes a critique. Johnstone makes a distinction between rhetoric as a wedge and rhetoric in a degenerate sense in the form of a command or threat. Once rhetoric degenerates to the level of commands or threats, it is no longer bilateral but unilateral, no longer making it a wedge by which one can interpret data.|$|E
30|$|Excel is used purely as {{a vehicle}} for <b>input</b> <b>of</b> <b>data</b> and output of {{numerical}} results. It plays no part in computations.|$|E
40|$|A general {{approach}} to the synthesis of an optimal order of executing jobs in engineering systems with indeterminate (interval) times of job processing is presented. As a mathematical model of the system, a two-stage pipeline is taken whose first and second stages are, respectively, the <b>input</b> <b>of</b> <b>data</b> and its processing, and the corresponding mathematical apparatus is continuous logic and logic determinants...|$|E
50|$|Planners {{may also}} {{consider}} whether the <b>input</b> <b>of</b> new <b>data</b> and updating <b>of</b> existing <b>data</b> {{is to be}} centrally controlled or devolve. These decisions sit alongside to the hardware and software considerations (like content management systems), participation issues (like good taste, harassment, confidentiality), and features to be supported.|$|R
30|$|Unlike {{statistical}} techniques, ANNs do {{not need}} assumptions on the distribution <b>of</b> <b>input</b> <b>data.</b>|$|R
5000|$|... : similarly, size <b>of</b> <b>input</b> <b>data</b> (decimal, in octets) if {{provided}} via HTTP header.|$|R
40|$|The {{language}} for forms, FOSPRA, {{is used for}} checking the <b>input</b> <b>of</b> <b>data</b> by means of forms using alphanumeric displays as the input devices. This checking can be done either on the bases of formal or of semantical criteria, with a pre-defined syntax. It is also possible to have results computed and to entry in output domains of the forms. REDAS offers, together with its driver, a comfortable real time data recording system...|$|E
40|$|Yield {{forecast}} in Brazil needs {{quantitative methods}} using low <b>input</b> <b>of</b> <b>data.</b> The {{aim of this}} study was to estimate regional yield decrease of soybean crop in Paraná state, Brazil, during 2003 / 2004 season. A soil moisture model, couple with a crop production function was used. The simulations predicted yield decrease ranging from 0 to 40 %, according to rainfall supply during crop development in the region. These results agreed well with field survey, indicating a great potential of the method for applications in yield forecast in Brazil. Pages: 103 - 11...|$|E
30|$|We {{obtained}} {{data collected}} by the sensors that are under supervision of a Brazilian entity responsible for monitoring natural disasters (Brazilian Center for Monitoring and Warnings of Natural Disasters - CEMADEN) [59]. These data were parsed and stored in a text file. Stimuli generator are fed with them, emitting them to the simulation, stimulating it {{until the end of}} the execution. The <b>input</b> <b>of</b> <b>data</b> triggers the constituents operation, cause their interoperability, reach the gateway, are processed creating new data that correspond to positive or negative flood alerts. We also collect these data to analyze results.|$|E
40|$|A {{computerized}} system is described {{which is used}} to store, manipulate and retrieve antimicrobial susceptibility data in the clinical microbiology lab. Features include facilitated <b>input</b> <b>of</b> susceptibility <b>data,</b> rapid generation <b>of</b> reports, realtime access to data, and enhanced retrieval of information for Infection Control...|$|R
40|$|UnrestrictedSpeaker {{clustering}} {{refers to}} a process of classifying a set <b>of</b> <b>input</b> speech <b>data</b> (or speech segments) by a speaker identity in an unsupervised way, based on the similarity of speaker-specific characteristics between the data. The process identifies the speech segments of the same speaker source without any prior speaker-specific information <b>of</b> the given <b>input</b> <b>data.</b> This speaker-perspective, unsupervised classification <b>of</b> speech <b>data</b> can be applied as a pre-processing step to speech/speaker recognition or multimedia data segmentation/classification in various ways. Thus, speaker clustering has been recently attracting much attention in the research area of speech recognition and multimedia data processing.; One big, yet unsolved, issue in the research field of speaker clustering is unreliable clustering performance under the variation <b>of</b> <b>input</b> speech <b>data.</b> In this dissertation, we {{deal with this problem}} in the framework of agglomerative hierarchical speaker clustering (AHSC) in two perspectives: stopping point estimation and inter-cluster distance measurement. In order to improve the robustness of stopping point estimation for AHSC under the variation <b>of</b> <b>input</b> speech <b>data,</b> we propose a new statistical measure called information change rate (ICR), which can improve estimation of the optimal stopping point. The ICR-based stopping point estimation method is not only empirically but also theoretically verified to be more robust to the variation <b>of</b> <b>input</b> speech <b>data</b> than the conventional BIC-based method. In order to improve the robustness of inter-cluster distance measurement for AHSC under the variation <b>of</b> <b>input</b> speech <b>data,</b> we also propose selective AHSC and incremental Gaussian mixture cluster modeling. These two approaches are proven to provide much more reliability for speaker clustering performance under the variation <b>of</b> <b>input</b> speech data.; Based on these results on robust speaker clustering under the variation <b>of</b> <b>input</b> speech <b>data,</b> we extend our interest to implementing a speaker diarization system, which is more robust to the variation <b>of</b> <b>input</b> audio <b>data.</b> (Speaker diarization {{refers to a}}n automated process that can annotate a given audio source in terms of "who spoke when".) Focusing on speaker diarization of meeting conversations speech, we propose two refinement schemes to further improve the reliability of speaker clustering performance in the framework of speaker diarization under the variation <b>of</b> <b>input</b> audio <b>data.</b> One is selection of representative speech segments and the other is interaction pattern modeling between meeting participants, and both of them are experimentally verified to enhance the reliability of speaker clustering performance and hence improve the overall diarization accuracy under the variation <b>of</b> <b>input</b> audio <b>data...</b>|$|R
5000|$|A {{computer}} program, prog, {{is seen as}} {{a mapping}} <b>of</b> <b>input</b> <b>data</b> into output data: ...|$|R
