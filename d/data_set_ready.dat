4|10000|Public
5000|$|DTR (Data Terminal Ready) and DSR (<b>Data</b> <b>Set</b> <b>Ready),</b> DTR {{flow control}} ...|$|E
5000|$|Pin # GeoPort RS-422 RS-232 Name 1 SCLK HSKo DTR Serial Clock (out), Handshake Out, Data Terminal Ready 2 SCLK HSKi DSR Serial Clock (in), Handshake In, <b>Data</b> <b>Set</b> <b>Ready</b> 3 TxD- TxD- TD Transmit data (-ve signal) 4 GND GND GND Cable ground 5 RxD- RxD- RD Receive data (-ve signal) 6 TxD+ TxD+ Transmit data (+ve signal) 7 TxHS GPi CD Wakeup/DMA Request, General Purpose input, Carrier Detect 8 RxD+ RxD+ (ground) Receive data (+ve signal) 9 +5 V Power, 350 mA maximum ...|$|E
5000|$|The RS-232 {{standard}} {{defines the}} voltage levels {{that correspond to}} logical one and logical zero levels for the data transmission and the control signal lines. Valid signals are either {{in the range of}} +3 to +15 volts or the range −3 to −15 volts with respect to the [...] "Common Ground" [...] (GND) pin; consequently, the range between −3 to +3 volts is not a valid RS-232 level. For data transmission lines (TxD, RxD, and their secondary channel equivalents), logic one is defined as a negative voltage, the signal condition is called [...] "mark". Logic zero is positive and the signal condition is termed [...] "space". Control signals have the opposite polarity: the asserted or active state is positive voltage and the deasserted or inactive state is negative voltage. Examples of control lines include request to send (RTS), clear to send (CTS), data terminal ready (DTR), and <b>data</b> <b>set</b> <b>ready</b> (DSR).|$|E
40|$|Organizations may {{periodically}} perform benchmarks {{as a way}} {{to measure}} their performance. This study attemptsto find a method to provide a way of working to effectively go from raw <b>data</b> to <b>data</b> <b>sets</b> <b>ready</b> to be analyzed in the context of contract benchmarking. Furthermore, scripting tools are developed to automate some of the required steps and guidelines are provided to efficiently and effectively use the method provided by this research...|$|R
40|$|In {{the year}} 2004 several {{milestones}} {{in the measurement}} of the top quark mass were reached. The DØ collaboration published a significant improvement of their Run I measurement of the top quark mass, and both Tevatron experiments released preliminary measurements based on Run II <b>data</b> <b>sets</b> collected in the period 2002 - 2004. The preliminary Run II results presented here do not yet surpass the current world average in precision, but this is expected to change soon. With larger <b>data</b> <b>sets</b> <b>ready</b> to be analyzed, {{a better understanding of}} the Run II detectors and improved analysis methods, 2005 promises to be a remarkable year for Top physics. ...|$|R
40|$|In {{the early}} 2000 s, the International Livestock Research Institute (ILRI) started {{developing}} {{a tool to}} capture key factors of farming systems in standardized, quantitative <b>data</b> <b>sets,</b> <b>ready</b> to feed into models. Their product, the Integrated Modelling Platform for Mixed Animal-Crop Systems (IMPACT), included a comprehensive survey and a data collection software package. The CGIAR Research Program on Climate Change, Agriculture and Food Security (CCAFS) recently backed an effort to streamline and update this tool, making it much quicker to implement and more useful for modelling and agricultural development studies. The new version, IMPACTlite, has now been rolled out at 13 CCAFS research sites in 12 countries, and other research programs are also starting to use IMPACTLite {{as a foundation for}} their own studies...|$|R
40|$|Inertial {{characteristics}} and {{dimensions of the}} body and body segments form an integral part of a biomechanical analysis of motion. In primate studies, however, segment inertial parameters of non-human hominoids are scarce and often obtained using varying techniques. Therefore, the principal aim {{of this study was to}} expand the existing chimpanzee inertial property data set using a non-invasive measuring technique. We also considered age- and sex-related differences within our sample. By means of a geometric model based on Crompton et al. (1996); Am J Phys Anthropol 99, 547 – 570) we generated inertial properties using external segment length and diameter measurements of 53 anaesthetized chimpanzees (Pan troglodytes). We report absolute inertial parameters for immature and mature subjects and for males and females separately. Proportional data were computed to allow the comparison between age classes and sex classes. In addition, we calculated whole limb inertial properties and we discuss their potential biomechanical consequences. We found no significant differences between the age classes in the proportional data except for hand and foot measures where juveniles exhibit relatively longer and heavier distal segments than adults. Furthermore, most sex-related differences can be directly attributed to the higher absolute segment masses in male chimpanzees resulting in higher moments of inertia. Additionally, males tend to have longer upper limbs than females. However, regarding proportional data we discuss the general inertial properties of the chimpanzee. The described segment inertial parameters of males and females, and of the two age classes, represent a valuable <b>data</b> <b>set</b> <b>ready</b> for use in a range of biomechanical locomotor models. These models offer great potential for improving our understanding of early hominin locomotor patterns...|$|E
40|$|Existing {{software}} was modified to handle 3 -D density and magnetization {{models of the}} Kentucky body and is being tested. Gravity and magnetic anomaly <b>data</b> <b>sets</b> are <b>ready</b> for use. A preliminary block model is under construction using the 1 : 1, 000, 000 maps. An x-y grid to overlay the 1 : 2, 500, 000 Albers maps and keyed to the 1 : 1, 000, 000 scale block models was created. Software was developed to generate a smoothed MAGSAT <b>data</b> <b>set</b> over this grid; {{this is to be}} input to an inversion program for generating the regional magnetization map. The regional scale 1 : 2, 500, 000 map mosaic is being digitized using previous magnetization models, the U. S. magnetic anomaly map, and regional tectonic maps as a guide...|$|R
50|$|DECserver 300The DECserver 300 Terminal Server was an Ethernet Communications Server for Ethernet Local Area Networks, {{providing}} a convenient method to logically connect up to sixteen digital asynchronous terminals {{to one or}} more service nodes (hosts) on an Ethernet. The DECserver 300 used MMJs (Modified Modular Jacks) for the attachment of asynchronous devices. The MMJ segregated a Data from a Voice connection. The DECserver 300 utilized the EIA 423-A electrical interface standard for local connections. EIA 423-A is compatible with the EIA 232-D interface and supports DTR/DSR (<b>Data</b> Terminal Ready/Data <b>Set</b> <b>Ready)</b> signals. EIA 423-A supports longer cable runs and higher signaling speeds. The DECserver 300 implemented the LAT protocol for communication with service nodes that implemented this protocol on the same Ethernet. The DECserver 300 also implemented the TCP/IP protocol suite for communication with host systems that implemented TCP/IP.|$|R
40|$|In many review {{articles}} or studies, {{the researchers}} have encouraged further exploration on the causal links between Information Technology (IT) investments and a fi rm’s sustainable competitive advantage. The outcomes of empirical studies have been inconclusive, which is {{to a certain extent}} due to the omission of IT-business strategic alignment. Indeed, strategic alignment has {{emerged as one of the}} most important issues facing business and IT executives all over the world. This paper reports on the empirical investigation of the success factors, which consist of leadership, structure and process, service quality, and values and beliefs, which are representative of the culture gap between IT strategy and business strategy. A questionnaire survey among 200 IT managers was carried out and 172 <b>data</b> <b>sets</b> were collected. This represented a 86 % response rate. After a rigorous data screening process including outliers, normality, reliability and validity, 172 <b>data</b> <b>sets</b> were <b>ready</b> for structural equation modelling (SEM) analysis. Confi rmatory Factor Analysis (CFA) was performed to examine the composite reliability, convergent validity and goodness of fi t of the individual constructs and measurement models. The revised structural model demonstrates the relationships between all the four exogenous variables and IT-business strategic alignment, and all the four exogenous variables and sustainable competitive advantage. In addition, regarding the revised model there are two mediating eff ects of strategic alignment in the relationship between leadership, structure and process, service quality, values and beliefs, and sustainable competitive advantage...|$|R
5000|$|... #Caption: 'Gabriel' in the 'up' position, <b>set</b> <b>ready</b> for ringing ...|$|R
40|$|The {{results from}} most machine {{learning}} experiments {{are used for}} a specific purpose and then discarded. This results in a significant loss of information and requires rerunning experiments to compare learning algorithms. This also requires implementation of another algorithm for comparison, that {{may not always be}} correctly implemented. By storing the results from previous experiments, machine learning algorithms can be compared easily and the knowledge gained from them can be used to improve their performance. The purpose of this work is to provide easy access to previous experimental results for learning and comparison. These stored results are comprehensive [...] storing the prediction for each test instance as well as the learning algorithm, hyperparameters, and training set that were used. Previous results are particularly important for meta-learning, which, in a broad sense, is the process of learning from previous machine learning results such that the learning process is improved. While other experiment databases do exist, one of our focuses is on easy access to the data. We provide meta-learning <b>data</b> <b>sets</b> that are <b>ready</b> to be downloaded for meta-learning experiments. In addition, queries to the underlying database can be made if specific information is desired. We also differ from previous experiment databases in that our databases is designed at the instance level, where an instance is an example in a <b>data</b> <b>set.</b> We store the predictions of a learning algorithm trained on a specific training set for each instance in the test <b>set.</b> <b>Data</b> <b>set</b> level information can then be obtained by aggregating the results from the instances. The instance level information can be used for many tasks such as determining the diversity of a classifier or algorithmically determining the optimal subset of training instances for a learning algorithm. Comment: 7 pages, 1 figure, 6 table...|$|R
40|$|Abstract. The {{results from}} most machine {{learning}} experiments {{are used for}} a specific purpose and then discarded. This causes sig-nificant loss of information and requires rerunning experiments to compare learning algorithms. Often, this also requires a researcher or practitioner to implement another algorithm for comparison, that {{may not always be}} correctly implemented. By storing the results from previous experiments, machine learning algorithms can be compared easily and the knowledge gained from them can be used to improve the performance of future machine learning experiments. The purpose of this work is to provide easy access to previous ex-perimental results for learning and comparison. These stored results are comprehensive – storing the prediction for each test instance as well as the learning algorithm, hyperparameters, and training set that were used in the experiment. Previous experimental results are par-ticularly important for meta-learning, which, in a broad sense, is the process of learning from previous machine learning results such that the learning process is improved. While other experiment databases do exist, one of our focuses is on easy access to the data, eliminat-ing any learning curve required to acquire the desired information. We provide meta-learning <b>data</b> <b>sets</b> that are <b>ready</b> to be downloaded for meta-learning experiments. Easy access to previous experimental results aids other researchers looking to do meta-learning and helps in comparing meta-learning algorithms. In addition, simple queries to the underlying database can be made if specific information is de-sired. We also differ from previous experiment databases in that our database is designed at the instance level, where an instance is an ex-ample in a <b>data</b> <b>set.</b> We store the predictions of a learning algorithm trained on a specific training set for each instance in the test <b>set.</b> <b>Data</b> <b>set</b> level information can then be obtained by aggregating the results from the instances. The instance level information can be used for many tasks such as determining the diversity of a classifier or algo-rithmically determining the optimal subset of training instances for a learning algorithm. ...|$|R
6000|$|... "Do come {{up while}} I'm {{changing}} my dress"; she had followed up the stairs. The girl {{led the way}} into Imogen's old bedroom, <b>set</b> <b>ready</b> for her toilet.|$|R
6000|$|When Fleur {{came forward}} and said to her, [...] "Do come up while I'm {{changing}} my dress," [...] she had followed up the stairs. The girl led the way into Imogen's old bedroom, <b>set</b> <b>ready</b> for her toilet.|$|R
50|$|On April 12, 2011, Marcus Whitman High School won the 98PXY High School Challenge again, {{this time}} earning a concert by the band The <b>Ready</b> <b>Set.</b> The <b>Ready</b> <b>Set</b> played for {{approximately}} 30 minutes. The {{date of the}} concert coincided {{with the release of}} a new single by the band, {{which was one of the}} songs played during the concert.|$|R
40|$|Results {{from using}} {{different}} breast cancer <b>data</b> <b>set</b> as the training <b>data</b> <b>set</b> in the MCL+superpc approach In order {{to check the}} robustness of our MCL+superpc approach, we used each of four validation <b>data</b> <b>sets</b> as the training <b>data</b> <b>set,</b> and the remaining four <b>data</b> <b>sets</b> as validation <b>data</b> <b>sets.</b> The following tables show these results. Table S 1. Superpc continuous prediction results from breast cancer data analysis. The results were generated by using the GSE 4922 <b>data</b> <b>set</b> as the training <b>data</b> <b>set</b> and four independent <b>data</b> <b>sets</b> as validation <b>data</b> with a threshold value of 1. 10 and 9 selected MCL modules. The training <b>data</b> <b>set</b> is highlighted in red. P-values less than 0. 05 are highlighted in yellow...|$|R
30|$|<b>Data</b> <b>set</b> feature {{processing}} scheme includes feature collection, feature {{conversion and}} reservation. The filter feature of <b>data</b> <b>set</b> would be pre-fetched. The classification characteristics of <b>data</b> <b>set</b> {{could be obtained}} based on the classification accuracy of <b>data</b> <b>set.</b> According to {{the characteristics of the}} <b>data</b> <b>set,</b> the crowd filter would select the appropriate crowd incentive strategy. Based on the complexity characteristics of the <b>data</b> <b>set</b> classification, the transformation of the subset of the <b>data</b> <b>set</b> would be completed.|$|R
40|$|At last, the {{overhead}} was completed and the boiler, steam engine and the dynamos were all <b>set,</b> <b>ready</b> for the generating {{of the electric}} current. On November 9, electric motor number 1, named J. R. Thomas, made a trial trip with only Dr. Gochenauer, the engineer and the conductor aboard. Continued on page...|$|R
30|$|<b>Data</b> <b>set</b> 4 : Reduction, transformation, and {{clustering}} <b>data</b> <b>sets</b> (14 <b>data</b> <b>sets,</b> 2 variables).|$|R
3000|$|... (a) to {{represent}} target <b>data</b> <b>set</b> and auxiliary <b>data</b> <b>set,</b> respectively. Denote the feature matrix of target <b>data</b> <b>set</b> as X^(t)∈R^k× n^(t), the feature matrix of spectral information of auxiliary <b>data</b> <b>set</b> as X^(a)∈R^k× n^(a), and the texture feature information matrix in auxiliary <b>data</b> <b>set</b> as T^(a)∈R^m× n^(a). For target <b>data</b> <b>set,</b> {{we assume that}} each sample corresponds to particular auxiliary information. We use S [...]...|$|R
5000|$|... #Caption: Harriott on the <b>set</b> of <b>Ready</b> Steady Cook, August 2004 ...|$|R
40|$|A pre-coding {{method and}} device for {{improving}} data compression performance by removing correlation between a first original <b>data</b> <b>set</b> {{and a second}} original <b>data</b> <b>set,</b> each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference <b>data</b> <b>set.</b> The method and device produce a double-difference <b>data</b> <b>set,</b> i. e., an adjacent-delta calculation performed on a cross-delta <b>data</b> <b>set</b> or a cross-delta calculation performed on two adjacent-delta <b>data</b> <b>sets,</b> from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted <b>data</b> <b>sets</b> coming from a single source. The resulting double-difference <b>data</b> <b>set</b> is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original <b>data</b> <b>set</b> having been represented by such a double-difference <b>data</b> <b>set...</b>|$|R
40|$|Several <b>data</b> <b>sets</b> {{have been}} {{proposed}} for benchmarking in time series prediction. A popular one is <b>Data</b> <b>Set</b> A from the Santa Fe Competition. This <b>data</b> <b>set</b> {{was the subject of}} analysis in many papers. In this note, it is shown that predicting the continuation of <b>Data</b> <b>Set</b> A is nothing else than a pattern matching problem. Looking at studies of this <b>data</b> <b>set,</b> it is remarkable that most of the very good forecasts of <b>Data</b> <b>Set</b> A used upsampled training data. We explain why upsampling is crucial for this <b>data</b> <b>set.</b> Finally, it is demonstrated that simple pattern matching performs as good as sophisticated prediction methods on <b>Data</b> <b>Set</b> A...|$|R
40|$|This paper {{describes}} a prototype system for registering geologic <b>data</b> <b>sets</b> through ontologies {{to assist in}} integrating and querying heterogeneous geologic <b>data</b> <b>sets.</b> The system consists of three components: an ontology repository, the <b>data</b> <b>set</b> registration, and ontology-aware applications. User-defined ontologies in OWL are saved and used by the system. Each <b>data</b> <b>set</b> must be registered before it becomes available, and the registration semi-automatically generates a mapping from <b>data</b> <b>sets</b> to ontologies. The mapping between <b>data</b> <b>sets</b> and ontologies are used by applications to explore and extract information from the <b>data</b> <b>set...</b>|$|R
30|$|The {{experiments}} adopts {{the classic}} <b>data</b> <b>set</b> DataSet 1 provided by KDD Cup’ 99 {{to test the}} correctness of the proposed parallel spectral clustering algorithm; we use respectively 10000 (<b>Data</b> <b>Set</b> DS 1), 50000 (<b>Data</b> <b>Set</b> DS 2), 100000 (<b>Data</b> <b>Set</b> DS 3), 1000000 (<b>Data</b> <b>Set</b> DS 4), 5000000 (<b>Data</b> <b>Set</b> DS 5) to verify {{the superiority of the}} proposed parallel algorithm, and data samples is the multidimensional data listed in literature [20, 21].|$|R
50|$|Wylbur {{provides}} a line editor {{that works with}} temporary <b>data</b> <b>sets,</b> similar to buffers in other editors. At any point in time one of the temporary <b>data</b> <b>sets</b> is designated as default. Wylbur maintains a current line pointer for each temporary <b>data</b> <b>set.</b> The user may specify an explicit working <b>data</b> <b>set</b> on a command; if he omits it, then the default temporary <b>data</b> <b>set</b> is used as the working <b>data</b> <b>set.</b>|$|R
30|$|To {{evaluate}} our approach, we use two <b>data</b> <b>sets,</b> the TEE 2014 <b>data</b> <b>set</b> and the CLEF Replab 2014 <b>data</b> <b>set.</b>|$|R
5000|$|... #Caption: Ainsley Harriott on the <b>set</b> of <b>Ready</b> Steady Cook, August 2004 ...|$|R
40|$|The <b>data</b> <b>set</b> {{specifications}} for the NASA Aerospace Safety Information System (NASIS) are presented. The <b>data</b> <b>set</b> specifications describe the content, format, and medium of communication of every <b>data</b> <b>set</b> {{required by the}} system. All relevant information pertinent to a particular <b>data</b> <b>set</b> is prepared in a standard form and centralized in a single document. The format for the <b>data</b> <b>set</b> is provided...|$|R
40|$|Abstract Background The {{information}} from different <b>data</b> <b>sets</b> experimented under different conditions may be inconsistent {{even though they}} are performed with the same research objectives. More than that, even when the <b>data</b> <b>sets</b> were generated from the same platform, the data agreement may be affected by the technical variation among the laboratories. In this case, it is necessary to use the combined <b>data</b> <b>set</b> after adjusting the differences between such <b>data</b> <b>sets,</b> for detecting the more reliable information. Results The proposed method combines <b>data</b> <b>sets</b> posterior to the discretization of <b>data</b> <b>sets</b> based on the ranks of the gene expression ratios, and the statistical method is applied to the combined <b>data</b> <b>set</b> for predictive gene selection. The efficiency of the proposed method was evaluated using five colon cancer related <b>data</b> <b>sets,</b> which were experimented using cDNA microarrays with different RNA sources, and one experiment utilized oligonucleotide arrays. NCI- 60 cell lines <b>data</b> <b>sets</b> were used, which were performed with two different platforms of cDNA microarrays and Affymetrix HU 6800 oligonucleotide arrays. The combined <b>data</b> <b>set</b> by the proposed method predicted the test <b>data</b> <b>sets</b> more accurately than the separated <b>data</b> <b>sets</b> did. The biological significant genes were detected from the combined <b>data</b> <b>set,</b> which were missed on the separated <b>data</b> <b>sets.</b> Conclusion By transforming gene expressions using ranks, the proposed method is not influenced by systematic bias among chips and normalization method. The method may be especially more useful to find predictive genes from <b>data</b> <b>sets</b> which have different scale in gene expressions. </p...|$|R
50|$|Documents and <b>data</b> <b>sets</b> sectionAnother {{section of}} the Facility Information Model {{consists}} of documents and <b>data</b> <b>sets</b> in various formats. Each of those documents and <b>data</b> <b>sets</b> {{is related to the}} element in the facility model about which the document or <b>data</b> <b>set</b> contains information.|$|R
30|$|Hypotheses {{related to}} {{operating}} core (innovation process, cross-functional organisation, {{and implementation of}} tools/technology) and competition-informed pricing The result of H 4 is supported in the full <b>data</b> <b>set</b> (β =  0.170, p <  0.05) and the Malaysian <b>data</b> <b>set</b> (β =  0.255, p <  0.05), while in the Bangladeshi data it was not supported. The result of H 5 was supported in the full <b>data</b> <b>set</b> (β =  0.266, p <  0.05) and the Bangladeshi <b>data</b> <b>set</b> (β =  0.275, p <  0.05), while in the Malaysian <b>data</b> <b>set</b> it was not supported. The result of H 6 was supported in the full <b>data</b> <b>set</b> (β =  0.295, p <  0.01) and the Bangladeshi <b>data</b> <b>set</b> (β =  0.536, p <  0.01), while in the Malaysian <b>data</b> <b>set,</b> H 6 was not supported.|$|R
50|$|In the {{geospatial}} (GIS) domain, {{data fusion}} is often synonymous with data integration. In these applications, {{there is often}} a need to combine diverse <b>data</b> <b>sets</b> into a unified (fused) <b>data</b> <b>set</b> which includes all of the data points and time steps from the input <b>data</b> <b>sets.</b> The fused <b>data</b> <b>set</b> is different from a simple combined superset in that the points in the fused <b>data</b> <b>set</b> contain attributes and metadata which might not have been included for these points in the original <b>data</b> <b>set.</b>|$|R
40|$|This paper {{describes}} {{how to build}} an audit and tracking system using SAS/AF and SCL. There will be a master <b>data</b> <b>set</b> which contains the original information and several transaction <b>data</b> <b>sets</b> which contain the new information. The audit system will take information from the transaction <b>data</b> <b>sets</b> and apply changes to the master <b>data</b> <b>set.</b> An audit <b>data</b> <b>set</b> {{will be used to}} keep track of all changes. The tracking system will display the master <b>data</b> <b>set</b> in the first data table on {{the top half of the}} screen and the audit <b>data</b> <b>set</b> in the second data table on the bottom half...|$|R
40|$|Abstract. This paper {{reviews the}} {{appropriateness}} for application to large <b>data</b> <b>sets</b> of standard machine learning algorithms, which were mainly {{developed in the}} context of small <b>data</b> <b>sets.</b> Sampling and parallelisation have proved useful means for reducing computation time when learning from large <b>data</b> <b>sets.</b> However, such methods assume that algorithms that were designed for use with what are now considered small <b>data</b> <b>sets</b> are also fundamentally suitable for large <b>data</b> <b>sets.</b> It is plausible that optimal learning from large <b>data</b> <b>sets</b> requires a different type of algorithm to optimal learning from small <b>data</b> <b>sets.</b> This paper investigates one respect in which <b>data</b> <b>set</b> size may affect the requirements of a learning algorithm – the bias plus variance decomposition of classification error. Experiments show that learning from large <b>data</b> <b>sets</b> may be more effective when using an algorithm that places greater emphasis on bias management, rather than variance management. ...|$|R
