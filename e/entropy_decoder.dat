20|2|Public
50|$|The first {{approach}} {{is still the}} preferred one. Because of FMO, decoding macroblocks in raster scan order may require to switch between different slices and/or slice groups. To speed up the DRAM access, one buffer for each Slice Group must be used (Figure 4). This additional intelligence of the DRAM access unit further increase the decoder complexity. Moreover, switching between different slices and/or slice groups requires swapping the <b>Entropy</b> <b>Decoder</b> (ED) status information. In the worst case, swapping occurs after decoding each macroblock. If the entire <b>Entropy</b> <b>Decoder</b> status information is too large to be stored in the processor local memory, each ED status need to be loaded from and stored into DRAM, thus further increasing the DRAM to processor’s memory bandwidth (Figure 4).|$|E
40|$|International audienceThis paper {{describes}} {{the implementation of}} the MPEG AVC CABAC <b>entropy</b> <b>decoder</b> using the RVC-CAL dataflow programming language. CABAC is the Context based Adaptive Binary Arithmetic Coding <b>entropy</b> <b>decoder</b> that is used by the MPEG AVC/H. 264 main and high profile video standard. CABAC algorithm provides increased compression efficiency, however presents a higher complexity compared to other entropy coding algorithms. This implementation of the CABAC <b>entropy</b> <b>decoder</b> using RVC-CAL proofs that complex algorithms can be implemented using a high level design language. This paper analyzes in detail two possible methods of implementing the CABAC <b>entropy</b> <b>decoder</b> in the dataflow paradigm...|$|E
40|$|An {{audio decoder}} for {{providing}} a decoded audio {{information on the}} basis of an entropy encoded audio information comprises a context-based <b>entropy</b> <b>decoder</b> configured to decode the entropy-encoded audio information in dependence on a context, which context is based on a previously-decoded audio information in a non-reset state-of-operation. The context-based <b>entropy</b> <b>decoder</b> is configured to select a mapping information, for deriving the decoded audio information from the encoded audio information, in dependence on the context. The context-based <b>entropy</b> <b>decoder</b> comprises a context resetter configured to reset the context for selecting the mapping information to a default context, which default context is independent from the previously-decoded audio information, in response to a side information of the encoded audio information...|$|E
40|$|The {{standard}} {{theoretical model}} for transform coding has strict modularity, {{meaning that the}} transform, quantization, and entropy coding blocks operate independently. mapping from I to strings of bits. The former is called a lossy encoder and the latter a lossless code or an <b>entropy</b> code. The <b>decoder</b> inverts γ and then approximates x from the index α (x) ∈I. This {{is shown in the}} top half of Fig. 1. It is assumed that communication between the encoder and decoder is perfect. (The last article of this issue [13] describes techniques that work when some transmitted bits are lost.) To assess the quality of a lossy source code, we need numerical measures of approximation accuracy and description length. The measure for description length is simply the expected number of bits output by the encoder divided by N; this is called the rate in bits per scalar sample and denoted by R. Here we will measure approximation accuracy by squared Euclidean norm divided by the vector lengt...|$|R
40|$|Yau Pui Yuk. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2002. Includes bibliographical {{references}} (leaves 147 - 152). Abstracts in English and Chinese. Chapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Overview [...] - p. 1 Chapter 1. 2 [...] - Speech Recognition [...] - p. 2 Chapter 1. 2. 1 [...] - How Speech Recognition Works [...] - p. 3 Chapter 1. 2. 2 [...] - Types of Speech Recognition Tasks [...] - p. 4 Chapter 1. 2. 3 [...] - Variabilities in Speech 一 a Challenge for Speech Recog- nition [...] - p. 6 Chapter 1. 3 [...] - Performance Prediction of Speech Recognition Task [...] - p. 7 Chapter 1. 4 [...] - Thesis Goals [...] - p. 9 Chapter 1. 5 [...] - Thesis Organization [...] - p. 10 Chapter 2 [...] - Background [...] - p. 11 Chapter 2. 1 [...] - The Acoustic-phonetic Approach [...] - p. 12 Chapter 2. 1. 1 [...] - Prediction {{based on}} the Degree of Mismatch [...] - p. 12 Chapter 2. 1. 2 [...] - Prediction based on Acoustic Similarity [...] - p. 13 Chapter 2. 1. 3 [...] - Prediction based on Between-Word Distance [...] - p. 14 Chapter 2. 2 [...] - The Lexical Approach [...] - p. 16 Chapter 2. 2. 1 [...] - Perplexity [...] - p. 16 Chapter 2. 2. 2 [...] - SMR-perplexity [...] - p. 17 Chapter 2. 3 [...] - The Combined Acoustic-phonetic and Lexical Approach [...] - p. 18 Chapter 2. 3. 1 [...] - Speech <b>Decoder</b> <b>Entropy</b> (SDE) [...] - p. 19 Chapter 2. 3. 2 [...] - Ideal Speech Decoding Difficulty (ISDD) [...] - p. 20 Chapter 2. 4 [...] - Chapter Summary [...] - p. 23 Chapter 3 [...] - Components for Predicting the Performance of Speech Recog- nition Task [...] - p. 24 Chapter 3. 1 [...] - Components of Speech Recognizer [...] - p. 25 Chapter 3. 2 [...] - Word Similarity Measure [...] - p. 27 Chapter 3. 2. 1 [...] - Universal Phoneme Symbol (UPS) [...] - p. 30 Chapter 3. 2. 2 [...] - Definition of Phonetic Distance [...] - p. 31 Chapter 3. 2. 3 [...] - Definition of Word Pair Phonetic Distance [...] - p. 45 Chapter 3. 2. 4 [...] - Definition of Word Similarity Measure [...] - p. 47 Chapter 3. 3 [...] - Word Occurrence Measure [...] - p. 62 Chapter 3. 4 [...] - Chapter Summary [...] - p. 64 Chapter 4 [...] - Formulation of Recognition Error Predictive Index (REPI) [...] - p. 65 Chapter 4. 1 [...] - Formulation of Recognition Error Predictive Index (REPI) [...] - p. 66 Chapter 4. 2 [...] - Characteristics of Recognition Error Predictive Index (REPI) [...] - p. 74 Chapter 4. 2. 1 [...] - Weakness of Ideal Speech Decoding Difficulty (ISDD) [...] - p. 75 Chapter 4. 2. 2 [...] - Advantages of Recognition Error Predictive Index (REPI) [...] - p. 79 Chapter 4. 3 [...] - Chapter Summary [...] - p. 82 Chapter 5 [...] - Experimental Design and Setup [...] - p. 83 Chapter 5. 1 [...] - Objectives [...] - p. 83 Chapter 5. 2 [...] - Experiments Preparation [...] - p. 84 Chapter 5. 2. 1 [...] - Speech Corpus and Speech Recognizers [...] - p. 85 Chapter 5. 2. 2 [...] - Speech Recognition Tasks [...] - p. 93 Chapter 5. 2. 3 [...] - Evaluation Criterion [...] - p. 98 Chapter 5. 3 [...] - Experiment Categories and their Setup [...] - p. 99 Chapter 5. 3. 1 [...] - Experiment Category 1 一 Investigating and comparing the overall prediction performance of the two predictive indices [...] - p. 102 Chapter 5. 3. 2 [...] - Experiment Category 2 一 Comparing {{the applicability of the}} word similarity measures of the two predictive indices on predicting the recognition performance [...] - p. 104 Chapter 5. 3. 3 [...] - Experiment Category 3 - Comparing the applicability of the formulation method of the two predictive indices on predicting the recognition performance [...] - p. 107 Chapter 5. 3. 4 [...] - Experiment Category 4 一 Comparing the performance of different phonetic distance definitions [...] - p. 109 Chapter 5. 4 [...] - Chapter Summary [...] - p. 111 Chapter 6 [...] - Experimental Results and Analysis [...] - p. 112 Chapter 6. 1 [...] - Experimental Results and Analysis [...] - p. 113 Chapter 6. 1. 1 [...] - Experiment Category 1 - Investigating and comparing the overall prediction performance of the two predictive indices [...] - p. 113 Chapter 6. 1. 2 [...] - Experiment Category 2 - Comparing the applicability of the word similarity measures of the two predictive indices on predicting the recognition performance [...] - p. 117 Chapter 6. 1. 3 [...] - Experiment Category 3 一 Comparing the applicability of the formulation method of the two predictive indices on predicting the recognition performance [...] - p. 124 Chapter 6. 1. 4 [...] - Experiment Category 4 - Comparing the performance of different phonetic distance definitions [...] - p. 131 Chapter 6. 2 [...] - Experimental Summary [...] - p. 137 Chapter 6. 3 [...] - Chapter Summary [...] - p. 141 Chapter 7 [...] - Conclusions [...] - p. 142 Chapter 7. 1 [...] - Contributions [...] - p. 144 Chapter 7. 2 [...] - Future Directions [...] - p. 145 Bibliography [...] - p. 147 Chapter A [...] - Table of Universal Phoneme Symbol [...] - p. 153 Chapter B [...] - Vocabulary Lists [...] - p. 157 Chapter C [...] - Experimental Results of Two-words Speech Recognition Tasks [...] - p. 171 Chapter D [...] - Experimental Results of Three-words Speech Recognition Tasks [...] - p. 180 Chapter E [...] - Significance Testing [...] - p. 190 Chapter E. 1 [...] - Procedures of Significance Testing [...] - p. 190 Chapter E. 2 [...] - Results of the Significance Testing [...] - p. 191 Chapter E. 2. 1 [...] - Experiment Category 1 [...] - p. 191 Chapter E. 2. 2 [...] - Experiment Category 2 [...] - p. 192 Chapter E. 2. 3 [...] - Experiment Category 3 [...] - p. 194 Chapter E. 2. 4 [...] - Experiment Category 4 [...] - p. 196 Chapter F [...] - Linear Regression Models [...] - p. 19...|$|R
40|$|As {{advances}} in neurotechnology {{allow us to}} access the ensemble activity of multiple neurons simultaneously, many neurophysiologic studies have investigated how to decode neuronal ensemble activity. Neuronal ensemble activity from different brain regions exhibits a variety of characteristics, requiring substantially different decoding approaches. Among various models, a maximum <b>entropy</b> <b>decoder</b> is known to exploit not only individual firing activity but also interactions between neurons, extracting information more accurately for the cases with persistent neuronal activity and/or low-frequency firing activity. However, it does not consider temporal changes in neuronal states and therefore would be susceptible to poor performance for nonstationary neuronal information processing. To address this issue, we develop a novel decoder that extends a maximum <b>entropy</b> <b>decoder</b> to take time-varying neural information into account. This decoder blends a dynamical system model of neural networks into the maximum entropy model to better suit for nonstationary circumstances. From two simulation studies, we demonstrate that the proposed dynamic maximum <b>entropy</b> <b>decoder</b> could cope well with time-varying information, which the conventional maximum <b>entropy</b> <b>decoder</b> could not achieve. The {{results suggest that the}} proposed decoder may be able to infer neural information more effectively as it exploits dynamical properties of underlying neural networks. open 0...|$|E
40|$|The paper {{describes}} a software implementation of an MPEG [...] compliant <b>Entropy</b> <b>Decoder</b> on a TriMedia/CPU 64 processor. We first outline entropy decoding basics and TriMedia/CPU 64 architecture. Then, {{we describe the}} reference implementation of the <b>entropy</b> <b>decoder,</b> which consists mainly of a software pipelined loop. On each iteration, a set of look-up tables partitioning the VariableLength Codes (VLC) table defined by the MPEG standard are accessed in order to retrieve the run-level pair, or detect an end-of-block or error condition. An average of 21. 0 cycles are needed to decode a DCT coefficient according to this reference implementation. Then, we focus on software techniques to optimize the entropy decoding software pipelined loop. In particular, we propose {{a new way to}} partition the VLC table such that by exposing the loop prologue to the compiler, testing each of the end-of-block and error conditions within the prologue becomes superfluous. This is based on the observation that either an end-of-block or error condition will never occur within the first table look-up. For the proposed implementation, the simulation results indicate that an average of 16. 9 cycles are needed to decode a DCT coefficient. That is, our <b>entropy</b> <b>decoder</b> is more than 20 % faster than its reference counterpart...|$|E
3000|$|From the {{analysis}} of the MJLS in Section 3, {{it is not easy to}} assess the computational burden required, when using the proposed system in practice. In this section, we provide a brief overview of the complexity of the encoder and decoder. The encoder includes the controller, quantizer, entropy coder, and channel (FEC) coder. The decoder includes channel decoder, <b>entropy</b> <b>decoder,</b> buffering, and selection of the control values: [...]...|$|E
30|$|Model-based {{steganography}} (MB) [5] {{introduces a}} different methodology, where {{the message is}} embedded in the cover according to a model representing cover message statistics. In [5], two image steganographic techniques (MB 1 and MB 2) are illustrated: MB 1 models DCT AC histograms by the generalized Cauchy distribution and embeds the message in the cover image through an <b>entropy</b> <b>decoder</b> driven by the model. MB 2 also preserves blockiness [6]. In [7], an ad hoc steganalytical test is developed to detect MB 1.|$|E
30|$|Among {{different}} reversible watermarking schemes, {{the expansion}} based methods received more attention {{because they have}} the highest embedding capacity along with the lowest quality degradations. However, a location map is needed to determine the positions of the expanded values. This location map should be compressed in order to decrease its influence on embedding capacity. As a result, lots of efforts have been done to decrease the size of location map. Using location map, however, has some other shortcomings, for example, a single bit modification may break the <b>entropy</b> <b>decoder</b> synchronization. Furthermore, {{it is not possible to}} employ such embedding scheme in block-based manner.|$|E
40|$|Image {{and video}} {{analytics}} are being increasingly {{used on a}} massive scale. Not only {{is the amount of}} data growing, but the complexity of the data processing pipelines is also increasing, thereby exacerbating the problem. It is becoming increasingly important to save computational resources wherever possible. We focus on one of the poster problems of visual analytics [...] face detection [...] and approach the issue of reducing the computation by asking: Is it possible to detect a face without full image reconstruction from the High Efficiency Video Coding (HEVC) bitstream? We demonstrate that this is indeed possible, with accuracy comparable to conventional face detection, by training a Convolutional Neural Network on the output of the HEVC <b>entropy</b> <b>decoder...</b>|$|E
40|$|We {{present a}} method for {{utilizing}} soft information in decoding of variable length codes (VLCs). When compared with traditional VLC decoding, which is performed using "hard" input bits and a state machine, the soft-input VLC decoding offers improved performance in terms of packet and symbol error rates. Soft-input VLC decoding is free from the risk, encountered in hard decision VLC decoders in noisy environments, of terminating the decoding in an unsynchronized state, and it offers the possibility to exploit a priori knowledge, if available, {{of the number of}} symbols contained in the packet. 1 Introduction In most applications of variable length codes (VLCs), decoding is performed bit by bit, with the input to the <b>entropy</b> <b>decoder</b> assumed to be a sequence of "hard" bits about which no soft information is available. However, in noisy environments, soft information can be associated with each information bit, either by direct use of channel observations in the case of uncoded transmission [...] ...|$|E
40|$|This paper proposes {{an error}} {{resilient}} transcoding scheme to perform {{unequal error protection}} for H. 264 /AVC coded video sequences over error prone channels. In order to protect those macroblocks which impact mostly on the distortion introduced at the decoder, a slice partitioning algorithm is designed {{on the basis of}} information that is available as the output of the <b>entropy</b> <b>decoder.</b> Hence, the proposed error resilient transcoding algorithm does not require full decoding, which includes inverse transform and motion compensation, and it is indicated to work in those devices where the computational resources are scarce (routers and switching transmitter stations). The proposed method has been evaluated against a classic forward error correction scheme with equal error protection of the transmitted content. Experimental results on real video test sequences show gains of up to 3 dB with respect to equal error protection. Index Terms — Flexible macroblock ordering, unequal error protection, slicing, H. 264 /AVC standard 1...|$|E
40|$|International audienceIt was {{demonstrated}} in recent contributions that Joint Source/Channel (JSC) decoding {{could be a}} good issue to make error correction in the case of transmission of entropy-encoded data. This paper addresses a new scheme for JSC decoding of Variable-Length Codes (VLC) and Arithmetic Codes (AC) based on Maximum a posteriori (MAP) sequence estimation. Previous contributions used a trellis description of the entropy encoding machine to perform soft input decoding. Referred to hard input classical decoding, significant improvements are achieved. Nevertheless, for realistic contexts, the complexity of the trellis-based technique becomes intractable. The decoding algorithm we propose performs Chase-like decoding using a priori knowledge of the source symbol sequence and the compressed bit-stream lengths. Performance in the case of transmission on an Additive White Gaussian Noise (AWGN) channel is evaluated in terms of Packet Error Rate (PER). Simulation results show that the proposed decoding algorithm leads to significant performance gain in comparison to classical VLC and AC decoding while exhibiting very low complexity. The practical relevance of the proposed technique is validated in the case of image transmission across the AWGN channel. Lossless and lossy image compression schemes are considered, and the Chase-like <b>entropy</b> <b>decoder</b> shows excellent results in terms of PER and reconstructed image quality...|$|E
40|$|High Efficiency Video Coding (HEVC) is {{the newest}} video coding {{standard}} approved by the ISO/IEC and ITU-T in January 2013. By providing a video coding efficiency gain of up to 50 % compared to its predecessor H. 264 /MPEG- 4 AVC, {{the complexity of the}} used algorithms has raised significantly. Targeting video formats with higher spatial and temporal resolutions - e. g. 4 Kp 60 in broadcast applications - make implementing encoders and decoders a challenging task. A well known bottleneck in the decoder architecture is the Context-Based Adaptive Binary Arithmetic Coding (CABAC), specified as entropy coding method in the H. 265 /MPEG-HEVC standard. Reaching high throughput for real time applications is a demanding task in terms of context modeling and serial bin-to-bin dependencies in the binary arithmetic coding engine. Especially the context selection and the update of the context model in binary decision mode requires a complex underlying control flow for the decoding process of each syntax element. This paper presents a pure hardware implementation of a Main Profile H. 265 /MPEG-HEVC compliant <b>entropy</b> <b>decoder.</b> The design has been optimized to achieve real time performance up to 4 k video resolutions. By using state-of-the-art FPGA technology, data rates of up to 104 MBit/s can be achieved...|$|E
40|$|Abstract- Variable length coding (VLC) is very {{suitable}} for regular data and efficient to compress data without any loss. VLC uses shorter bits of codewords instead of data occurring frequently, but uses the longer bits of codewords instead of data occurring infrequently. It {{is used in}} MPEG – 1 / 2 / 4 and H. 26 X (video and image compression standards). The <b>entropy</b> <b>decoder</b> in MPEG- 4 AVC/H. 264 baseline standard adopts Content Adaptive Variable Length Decoder (CAVLD). Because of symbol-to-symbol dependency, a traditional CAVLC decoder consumes lots of clock cycles in decoding and brings down the performance. We discover the decoding of two parameters spending almost eighty percent of computing time through profiling the computation of sub-modules and analyzing the encoding rules, which are non-zero coefficient (Level) and run_before. Thus this paper proposes a fast algorithm adapted for run_before decoder and the parallel architecture for level decoder, to improve the decoding performance. According to the features of these two methods, we name these two new methods as MLD (Multiple Level Decoding) and NZS (Non-Zero Skipping for run_before decoding). By performing parallel operation on level decoder, MLD can decode two levels in one cycle at most situations, and NZS can produce several values of run_before in the same cycle. These two methods have the advantages of low complexity and regularity design. Keywords [...] CAVLC Encoder, Decoder, H. 264. I...|$|E
40|$|In {{this paper}} a {{framework}} is proposed for efficient entropy coding of data {{which can be}} represented by a parametric distribution model. Based on the proposed framework, an entropy coder achieves coding efficiency by estimating {{the parameters of the}} statistical model (for the coded data), either via Maximum A Posteriori (MAP) or Maximum Likelihood (ML) parameter estimation techniques. The problem of optimal entropy coding for transmission of a block of data Nxxx, [...] ., 21, can be formulated by assuming that the data comes from a source with a parametric Probability Mass Function (pmf));, [...] ., (21 ΘNXXXP with parameter Θ (in general Θ is a vector). The parametric model assumption makes it possible to assign a probability to the event of observing Nxxx, [...] ., 21, and use this probability for entropy coding of this data, only by conveying the parameter Θ. In general, the parameter Θ is not known and the entropy coder should choose this parameter to minimize the entropy rate. At the same time, the <b>entropy</b> <b>decoder</b> with no access to the current data but only access to the past coded data (pNpp xxx,, 21), needs to be informed of the optimal parameter Θ which is used for entropy coding of the current block. If the model parameters are sent to the decoder, then the encoder should minimize th...|$|E
40|$|The paper {{presents}} a Design Space Exploration (DSE) experiment {{which has been}} carried out {{in order to determine the}} optimum FPGA [...] based Variable-Length Decoder (VLD) computing resource and its associated instructions, with respect to an entropy decoding task which is to be executed on the FPGA-augmentedTriMedia/CPU 64 processor. We first outline the extension of the TriMedia/CPU 64 architecture, which consists of an FPGA [...] based Reconfigurable Functional Unit (RFU) and the associated generic instructions. Then we address entropy decoding and propose a strategy to partially break the data dependency related to variable-length decoding. Three VLDs (VLD- 1, VLD- 2, VLD- 3) instructions which can return 1, 2, or 3 symbols, respectively, are subsequently analyzed. After completing the DSE, we determined that VLD- 2 instruction leads to the most efficient entropy decoding in terms of instruction cycles and FPGA area. The FPGA [...] based implementation of the computing resource associated to VLD- 2 instruction is subsequently presented. When mapped on an ACEX EP 1 K 100 FPGA from Altera, VLD- 2 exhibits a latency of 8 TriMedia cycles, and uses all the Electronic Array Blocks and 51 % of the logic cells of the device. The simulation results indicate that the VLD- 2 [...] based <b>entropy</b> <b>decoder</b> is 43 % faster than its pure software counterpart...|$|E
30|$|On {{the other}} hand, {{software-based}} multi-format video decoders {{have been developed}} on multi-core platforms. Various parallel implementations are utilized to alleviate the implementation costs of multi-format decoders with hardwired circuits. On the multi-core platform, {{it is possible to}} develop multi-format decoders that allow easy performance evaluation with minimum cost and effort compared to other hardwire designs. In addition, the multi-core platform requires less power for fast decoding of multi-format multimedia content due to the low clock speed. Parallel video decoders with multi-core platform can be implemented based on both data-level and functional-level parallelism. However, considering resolution, scalability, and performance in parallelism, the macroblock-level parallelism approach, which is a form of data-level parallelization, is widely used for video decoders [2]–[4]. For macroblock-level parallelism, the 2 D wave-front approach is widely known and used. This method can perform parallel decoding of multiple macroblocks without any decoding dependencies. However, this 2 D wave-front approach cannot be used for entropy decoding because of bit-by-bit dependency in a slice, even though back-end decoding can be parallelized with the multi-core platform [5]. Because the performance of parallel video decoders is highly influenced by sequential parts such as entropy decoding, the high-performance <b>entropy</b> <b>decoder</b> is an essential prerequisite for parallel video decoders. In addition, entropy decoding is one form of bottleneck that can decrease decoding throughput not only in parallel decoders but also in sequential decoders, especially for high-bitrate streams.|$|E
40|$|High Efficiency Video Coding (HEVC) is {{the latest}} video coding {{standard}} that specifies video resolutions up to 8 K Ultra-HD (UHD) at 120 fps to support the next decade of video applications. This results in high-throughput requirements for the context adaptive binary arithmetic coding (CABAC) <b>entropy</b> <b>decoder,</b> which was already a well-known bottleneck in H. 264 /AVC. To address the throughput challenges, several modifications were made to CABAC during the standardization of HEVC. This work leverages these improvements {{in the design of}} a high-throughput HEVC CABAC decoder. It also supports the high-level parallel processing tools introduced by HEVC, including tile and wavefront parallel processing. The proposed design uses a deeply pipelined architecture to achieve a high clock rate. Additional techniques such as the state prefetch logic, latched-based context memory, and separate finite state machines are applied to minimize stall cycles, while multibypass- bin decoding is used to further increase the throughput. The design is implemented in an IBM 45 nm SOI process. After place-and-route, its operating frequency reaches 1. 6 GHz. The corresponding throughputs achieve up to 1696 and 2314 Mbin/s under common and theoretical worst-case test conditions, respectively. The results show that the design is sufficient to decode in real-time high-tier video bitstreams at level 6. 2 (8 K UHD at 120 fps), or main-tier bitstreams at level 5. 1 (4 K UHD at 60 fps) for applications requiring sub-frame latency, such as video conferencing...|$|E
40|$|Motivated by {{streaming}} multi-view video coding {{and wireless}} sensor networks, {{we consider the}} problem of blockwise streaming compression {{of a pair of}} correlated sources, which we term streaming Slepian-Wolf coding. We study the moderate deviations regime in which the rate pairs of a sequence of codes converge, along a straight line, to various points on the boundary of the Slepian-Wolf region at a speed slower than the inverse square root of the blocklength $n$, while the error probability decays subexponentially fast in $n$. Our main result focuses on directions of approaches to corner points of the Slepian-Wolf region. It states that for each correlated source and all corner points, there exists a non-empty subset of directions of approaches such that the moderate deviations constant (the constant of proportionality for the subexponential decay of the error probability) is enhanced (over the non-streaming case) by at least a factor of $T$, the block delay of decoding source block pairs. We specialize our main result to the setting of streaming lossless source coding and generalize this result to the setting where we have different delay requirements for each of the two source blocks. The proof of our main result involves the use of various analytical tools and amalgamates several ideas from the recent information-theoretic streaming literature. We adapt the so-called truncated memory encoding idea from Draper and Khisti (2011) and Lee, Tan, and Khisti (2016) to ensure that the effect of error accumulation is nullified in the limit of large blocklengths. We also adapt the use of the so-called minimum weighted empirical suffix <b>entropy</b> <b>decoder</b> which was used by Draper, Chang, and Sahai (2014) to derive achievable error exponents for symbolwise streaming Slepian-Wolf coding. Comment: 31 pages, 6 figures, under revision with IEEE Transactions on Information Theory; short version presented ISIT 201...|$|E

