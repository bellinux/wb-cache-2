1|10000|Public
40|$|Reacting {{against the}} {{limitation}} of statistics to decision procedures, R. A. Fisher proposed for inductive reasoning {{the use of the}} fiducial distribution, a parameter-space distribution of epistemological probability transferred directly from limiting relative frequencies rather than computed according to the Bayes update rule. The proposal is developed as follows using the confidence measure of a scalar parameter of interest. (With the restriction to one-dimensional parameter space, a confidence measure is essentially a <b>fiducial</b> <b>probability</b> <b>distribution</b> free of complications involving ancillary statistics.) A betting game establishes a sense in which confidence measures are the only reliable inferential probability distributions. The equality between the probabilities encoded in a confidence measure and the coverage rates of the corresponding confidence intervals ensures that the measure's rule for assigning confidence levels to hypotheses is uniquely minimax in the game. Although a confidence measure can be computed without any prior distribution, previous knowledge can be incorporated into confidence-based reasoning. To adjust a p-value or confidence interval for prior information, the confidence measure from the observed data can be combined with one or more independent confidence measures representing previous agent opinion. (The former confidence measure may correspond to a posterior distribution with frequentist matching of coverage probabilities.) The representation of subjective knowledge in terms of confidence measures rather than prior probability distributions preserves approximate frequentist validity. Comment: major revisio...|$|E
5000|$|Fiducial {{inference}} was {{an approach}} to statistical inference based on <b>fiducial</b> <b>probability,</b> {{also known as a}} [...] "fiducial distribution". In subsequent work, this approach has been called ill-defined, extremely limited in applicability, and even fallacious. However this argument is the same as that which shows [...] that a so-called confidence distribution is not a valid <b>probability</b> <b>distribution</b> and, since this has not invalidated the application of confidence intervals, it does not necessarily invalidate conclusions drawn from fiducial arguments. An attempt was made to reinterpret the early work of Fisher's fiducial argument as a special case of an inference theory using Upper and lower probabilities.|$|R
5000|$|The {{interpretation}} of probability {{has not been}} resolved (but <b>fiducial</b> <b>probability</b> is an orphan).|$|R
40|$|By {{discussing}} several examples, {{the theory}} of generalized functional models is shown to be very natural for modeling some situations of reasoning under uncertainty. A generalized functional model is a pair (f, P) where f is a function describing the interactions between a parameter variable, an observation variable and a random source, and P is a <b>probability</b> <b>distribution</b> for the random source. Unlike traditional functional models, generalized functional models do not require {{that there is only}} one value of the parameter variable that is compatible with an observation and a realization of the random source. As a consequence, the results of the analysis of a generalized functional model are not expressed in terms of <b>probability</b> <b>distributions</b> but rather by support and plausibility functions. The analysis of a generalized functional model is very logical and is inspired from ideas already put forward by R. A. Fisher in his theory of <b>fiducial</b> <b>probability.</b> Comment: Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI 1997...|$|R
40|$|The {{geometric}} {{formulation of}} <b>fiducial</b> <b>probability</b> employed {{in this paper}} is an improvement over the usual pivotal quantity formulation. For a single parameter and single observation, the new formulation {{is based on the}} geometric properties of an ordinary two variable function and its surface representation. The following theorem is proved: A fiducial distribution for the continuous parameter θ exists if and only if (i) the continuous random <b>probability</b> <b>distributions</b> of x for different θ's are non-intersecting, and (ii) the random distributions are complete, i. e. at the extreme values of θ the limiting <b>probability</b> <b>distributions</b> are zero and one for all x. The proof yields also a complete characterization of random distributions that lead to fiducial distributions. The paper also treats intersecting distributions and non-intersecting incomplete distributions. The latter, which are frequently encountered in a null hypothesis, are shown to be associated with intersecting "composite" distributions. An appendix compares the pivotal and geometric formulations...|$|R
5000|$|Lindley {{showed that}} <b>fiducial</b> <b>{{probability}}</b> lacked additivity, {{and so was}} not a probability measure. Cox {{points out that the}} same argument applies to the so-called [...] "confidence distribution" [...] associated with confidence intervals, so the conclusion to be drawn from this is moot. Fisher sketched [...] "proofs" [...] of results using <b>fiducial</b> <b>probability.</b> When the conclusions of Fisher's fiducial arguments are not false, many have been shown to also follow from Bayesian inference.|$|R
5000|$|... where p(x, y) is {{the joint}} <b>probability</b> <b>distribution</b> {{of the two}} variables, p1(x) is the <b>probability</b> <b>distribution</b> of X, and p2(y) is the <b>probability</b> <b>distribution</b> of Y.|$|R
5000|$|For any <b>probability</b> <b>distribution</b> f, {{the range}} <b>probability</b> <b>distribution</b> is: ...|$|R
5000|$|... is a <b>probability</b> <b>distribution.</b> Then for {{any other}} <b>probability</b> <b>distribution</b> ...|$|R
50|$|The {{fractional}} Poisson <b>probability</b> <b>distribution</b> {{captures the}} long-memory effect {{which results in}} the non-exponential waiting time <b>probability</b> <b>distribution</b> function empirically observed in complex classical and quantum systems. Thus, fractional Poisson process and fractional Poisson <b>probability</b> <b>distribution</b> functioncan be considered as natural generalization of the famous Poisson process and the Poisson <b>probability</b> <b>distribution.</b>|$|R
30|$|The joint <b>probability</b> <b>distribution</b> {{values are}} {{computed}} using initialized <b>probability</b> <b>distribution.</b>|$|R
40|$|Abstract. In IDEAs, the <b>probability</b> <b>distribution</b> of a {{selection}} of solutions is estimated each generation. From this <b>probability</b> <b>distribution,</b> new solutions are drawn. Through the <b>probability</b> <b>distribution,</b> various relations between problem variables can be exploited to achieve efficient optimization. For permutation optimization, only real valued <b>probability</b> <b>distributions</b> have been applied to a real valued encoding of permutations. In this paper, we present two approaches to estimating marginal product factorized <b>probability</b> <b>distributions</b> {{in the space of}} permutations directly. The estimated <b>probability</b> <b>distribution</b> is used to identify crossover positions in a real valued encoding of permutations. The resulting evolutionary algorithm (EA) is capable of more efficient scalable optimization of deceptive permutation problems of a bounded order of difficulty than when real valued <b>probability</b> <b>distributions</b> are used. ...|$|R
40|$|In IDEAs, the <b>probability</b> <b>distribution</b> of a {{selection}} of solutions is estimated each generation. From this <b>probability</b> <b>distribution,</b> new solutions are drawn. Through the <b>probability</b> <b>distribution,</b> various relations between problem variables can be exploited to achieve e#cient optimization. For permutation optimization, only real valued <b>probability</b> <b>distributions</b> have been applied to a real valued encoding of permutations...|$|R
5000|$|If f is a {{probability}} density function, then {{the value of}} the integral above is called the -th moment of the <b>probability</b> <b>distribution.</b> More generally, if F is a cumulative <b>probability</b> <b>distribution</b> function of any <b>probability</b> <b>distribution,</b> which may not have a density function, then the -th moment of the <b>probability</b> <b>distribution</b> is given by the Riemann-Stieltjes integral ...|$|R
50|$|For a set {{empirical}} measurements sampled {{from some}} <b>probability</b> <b>distribution,</b> the Freedman-Diaconis rule {{is designed to}} minimize {{the difference between the}} area under the empirical <b>probability</b> <b>distribution</b> and the area under the theoretical <b>probability</b> <b>distribution.</b>|$|R
3000|$|... is, its <b>probability</b> <b>distribution</b> {{function}} {{will never}} converge to some <b>probability</b> <b>distribution</b> function.|$|R
30|$|The sub-domain, which {{derives from}} the {{conditioning}} relation, identifies a conditional <b>probability</b> <b>distribution.</b> The <b>probability</b> <b>distribution</b> function (PDF) of the service parameter p is the <b>probability</b> <b>distribution</b> of the bivariate PDF conditional to the domain L.|$|R
40|$|We present estimators for entropy {{and other}} {{functions}} of a discrete <b>probability</b> <b>distribution</b> when {{the data is}} a finite sample drawn from that <b>probability</b> <b>distribution.</b> In particular, for the case when the <b>probability</b> <b>distribution</b> is a joint distribution, we present finite sample estimators for the mutual information, covariance, and chi-squared functions of that <b>probability</b> <b>distribution.</b> Comment: uuencoded compressed postscript, submitte...|$|R
40|$|For a given metric g_μν, {{which is}} {{identified}} as Fisher information metric, we generate new constraints for the <b>probability</b> <b>distributions</b> for physical systems. We postulate the existence of intrinsic <b>probability</b> <b>distributions</b> for physical systems, and calculate the <b>probability</b> <b>distribution</b> by optimizing the Fisher information metric under specified constraints. Accordingly, we get differential equations for the <b>probability</b> <b>distributions.</b> Comment: 6 page...|$|R
5000|$|The use of {{available}} knowledge {{to establish a}} <b>probability</b> <b>distribution</b> to characterize each quantity of interest applies to the [...] and also to [...] In the latter case, the characterizing <b>probability</b> <b>distribution</b> for [...] {{is determined by the}} measurement model together with the <b>probability</b> <b>distributions</b> for the [...] The determination of the <b>probability</b> <b>distribution</b> for [...] from this information is known as the propagation of distributions.|$|R
40|$|Conditionalization, i. e., {{computation}} of a conditional <b>probability</b> <b>distribution</b> given a joint <b>probability</b> <b>distribution</b> {{of two or}} more {{random variables}} is an important operation in some probabilistic database models. While the computation of the conditional <b>probability</b> <b>distribution</b> is straightforward when the exact point probabilities are involved, it is often the case that such exact point <b>probability</b> <b>distributions</b> of random variables are not known, but are known to lie in a particular interval. This paper investigates the conditionalization operation for interval <b>probability</b> <b>distribution</b> functions under a possible world semantics. In particular, given a joint <b>probability</b> <b>distribution</b> {{of two or more}} random variables, where the probability of each outcome is represented as an interval, we (i) provide formal model-theoretic semantics; (ii) define the operation of conditionalization and (iii) provide a closed form solution/efficient algorithm to compute the conditional <b>probability</b> <b>distribution...</b>|$|R
5000|$|The product ∇ maps a <b>probability</b> <b>distribution</b> on two {{variables}} to a <b>probability</b> <b>distribution</b> on one variable; ...|$|R
5000|$|Models and <b>probability</b> <b>distributions</b> {{of various}} {{business}} activities either {{in terms of}} various parameters or <b>probability</b> <b>distributions.</b>|$|R
3000|$|Step I: Fitting the <b>probability</b> <b>distribution</b> The <b>probability</b> <b>distributions,</b> viz. chi squared, chi squared (2 P), {{exponential}}, exponential (2 P), gamma, gamma (3 P), gen. {{extreme value}} (GEV), log-Pearson 3, Weibull, Weibull (3 P), {{were applied to}} find out the best-fit <b>probability</b> <b>distribution.</b>|$|R
5000|$|... "Among <b>probability</b> <b>distributions</b> on the {{interval}} from 0 to ∞ {{on the real}} line, memorylessness characterizes the exponential distributions." [...] This statement means that the exponential distributions are the only such <b>probability</b> <b>distributions</b> that are memoryless. (See also Characterization of <b>probability</b> <b>distributions.)</b> ...|$|R
40|$|The Behrens [...] Fisher problem {{concerns}} {{finding an}} interval estimate {{for the difference}} between the means of two normal populations, without making any assumption about the variances. At present, statisticians cannot agree on its solution. In this paper, a credible and sharp solution is described. It is compared with the solutions of Behrens (see Fisher, 1956), Welch (1947) and Wilkinson, Venables and James (1979). <b>Fiducial</b> <b>probability</b> reference set confidence statement relevant subset two-means problem Behrens problem parameter of interest...|$|R
50|$|The {{computer}} program allows {{determination of the}} best fitting <b>probability</b> <b>distribution.</b> Alternatively it provides the user with the option to select the <b>probability</b> <b>distribution</b> to be fitted. The following <b>probability</b> <b>distributions</b> are included: normal, lognormal, logistic, loglogistic, exponential, Cauchy, Fréchet, Gumbel, Pareto, Weibull and others.|$|R
40|$|In {{this paper}} we discuss {{knowledge}} integration {{as a process of}} building a joint <b>probability</b> <b>distribution</b> from an input set of lowdimensional <b>probability</b> <b>distributions</b> starting with an initial joint <b>probability</b> <b>distribution.</b> Since the solution of the problem for a consistent input set of <b>probability</b> <b>distributions</b> is known we concentrate on a setup where the input <b>probability</b> <b>distributions</b> are inconsistent. In this case the iterative proportional tting procedure (IPFP), which converges in the consistent case, tends to come to cycles. We propose to use an algorithm, which we call GEMA (an abbreviation of Generalized Expectation Maximization Algorithm), that converges even in inconsistent case to a reasonable joint <b>probability</b> <b>distribution.</b> The important property of GEMA is that it can be eciently implemented exploiting decomposability of considered distributions...|$|R
3000|$|... [...]) with an {{associated}} <b>probability</b> <b>distribution.</b> At each iteration, drivers choose their routes {{according to the}} <b>probability</b> <b>distribution</b> over [...]...|$|R
50|$|Consider {{a random}} {{variable}} X whose <b>probability</b> <b>distribution</b> {{belongs to a}} parametric family of <b>probability</b> <b>distributions</b> Pθ parametrized by θ.|$|R
50|$|Any {{equation}} {{that gives}} the value 1 when integrated from a lower limit to an upper limit agreeing well with the data range, {{can be used as}} a <b>probability</b> <b>distribution</b> for fitting. A sample of <b>probability</b> <b>distributions</b> that may be used can be found in <b>probability</b> <b>distributions.</b>|$|R
5000|$|There is {{not enough}} {{information}} to build <b>probability</b> <b>distributions</b> for the inputs. <b>Probability</b> <b>distributions</b> can be constructed from expert elicitation, although even then {{it may be hard}} to build distributions with great confidence. The subjectivity of the <b>probability</b> <b>distributions</b> or ranges will strongly affect the sensitivity analysis.|$|R
40|$|Conditionalization, i. e., {{computation}} of a conditional <b>probability</b> <b>distribution</b> given a joint <b>probability</b> <b>distribution</b> {{of two or}} more {{random variables}} is an important operation in some probabilistic database models. While the computation of the conditional <b>probability</b> <b>distribution</b> is straightforward when the exact point probabilities are involved, it is often the case that such exact point <b>probability</b> <b>distributions</b> of random variables are not known, but are known to lie in a particular interval...|$|R
40|$|Abstract Conditionalization, i. e., {{computation}} of a conditional <b>probability</b> <b>distribution</b> given a joint probabilitydistribution {{of two or}} more {{random variables}} is an important operation in some probabilistic database models. While the computation of the conditional <b>probability</b> <b>distribution</b> is straightforward when theexact point probabilities are involved, it is often the case that such exact point <b>probability</b> <b>distributions</b> of random variables are not known, but are known to lie in a particular interval. This paper investigates the conditionalization operation for interval <b>probability</b> <b>distribution</b> functions under a possible world semantics. In particular, given a joint <b>probability</b> <b>distribution</b> {{of two or more}}random variables, where the probability of each outcome is represented as an interval, we (i) provide formal model-theoretic semantics; (ii) define the operation of conditionalization and (iii) provide a closedform solution/efficient algorithm to compute the conditional <b>probability</b> <b>distribution...</b>|$|R
3000|$|RAC; (2) {{given the}} <b>probability</b> <b>distribution</b> {{of the content}} and {{chloride}} diffusion coefficient of old mortar, the <b>probability</b> <b>distribution</b> of D [...]...|$|R
40|$|In this work, {{we present}} a method to {{generate}} <b>probability</b> <b>distributions</b> and classes of <b>probability</b> <b>distributions,</b> which broadens a process of <b>probability</b> <b>distribution</b> construction. In this method, distribution classes are built from pre-defined monotonic functions and from known distributions. With {{the use of this}} method, we can obtain different classes of <b>probability</b> <b>distributions</b> described in literature. Beside these results, we could obtain results on the support and nature of the generated distributions. Comment: 50 pages, submitted for journal publicatio...|$|R
