164|274|Public
50|$|Precise {{control of}} the system is essential. If the line speed of web is reduced, the amount of lateral <b>displacement</b> <b>error</b> that can be {{controlled}} by the steering guide system also decreases. If the input error decreases, the lateral <b>displacement</b> <b>error</b> also becomes smaller. Lateral displacement occurs on the transport web by the air blow from the dryer and the increase of blowing frequency can reduce the lateral displacement.|$|E
50|$|These {{indicators}} actually measure {{angular displacement}} and not linear displacement; linear distance is correlated to the angular displacement {{based on the}} correlating variables. If the cause of movement is perpendicular to the finger, the linear <b>displacement</b> <b>error</b> is acceptably small within the display range of the dial. However, this error starts to become noticeable when this cause is as much as 10° off the ideal 90°. This is called cosine error, because the indicator is only registering the cosine of the movement, whereas the user likely {{is interested in the}} net movement vector. Cosine error is discussed in more detail below.|$|E
30|$|The primary {{outcome is}} {{measuring}} the angular <b>displacement</b> <b>error</b> during active and passive repositioning of shoulder {{external and internal}} rotation. Absolute angular <b>displacement</b> <b>error</b> {{is defined as the}} difference in degrees between indicated and reference position during joint position sense assessment [12].|$|E
40|$|Abstract. For {{finding the}} {{influence}} of the original errors on the follower of the globoidal cam, vector analysis method is used. Seven original errors are defined by analyzing the machine tool and the mathematics models between the follower angular <b>displacement</b> <b>errors</b> and the original errors are established. When the original errors are designed as 0. 01, the relationship curves between the largest follower angular <b>displacement</b> <b>errors</b> and the cam angles are got. The error influence coefficients between the largest follower angular <b>displacement</b> <b>errors</b> and the original errors are established on that basis. The results of this study lay the theoretical foundation of the design of the machine tool precision index system and the adjustment of the cam processing...|$|R
40|$|The {{triangulation}} method developed {{specifically for the}} Barium Ion Cloud Project is discussed. Expression for the four <b>displacement</b> <b>errors,</b> the three slope errors, and the curvature error in the triangulation solution due to a probable error in the lines-of-sight from the observation stations to points on the cloud are derived. The {{triangulation method}} is then {{used to determine the}} effect of the following on these different errors in the solution: the number and location of the stations, the observation duration, east-west cloud drift, the number of input data points, and the addition of extra cameras to one of the stations. The pointing <b>displacement</b> <b>errors,</b> and the pointing slope errors are compared. The <b>displacement</b> <b>errors</b> in the solution due to a probable error in the position of a moving station plus the weighting factors for the data from the moving station are also determined...|$|R
40|$|High-resolution image {{reconstruction}} {{refers to}} reconstructing a higher resolution image from multiple low-resolution samples {{of a true}} image. In [2], we considered the case {{where there are no}} <b>displacement</b> <b>errors</b> in the low-resolution samples, i. e. the samples are aligned properly, and hence the blurring operator is spatially invariant. In this paper, we consider the case where there are <b>displacement</b> <b>errors</b> in the low-resolution samples. The resulting blurring operator is spatially varying and is formed by sampling and summing different spatially invariant blurring operators...|$|R
30|$|In (53), {{the first}} term {{is due to the}} azimuth <b>displacement</b> <b>error.</b> The second term and the third term {{represent}} the residual RCM error and may result in a range <b>displacement</b> <b>error.</b> The last term is the residual coupling phase which may result in a defocus.|$|E
30|$|The {{processed}} {{satellite images}} were edge matched in AutoSync tool of ERDAS Imagine {{in order to}} overcome the <b>displacement</b> <b>error</b> of generated thematic layers so that accurate change analysis could be done.|$|E
40|$|Multihypothesis motion-compensating {{predictors}} combine several motioncompensated {{signals to}} predict the current frame of a video signal. This paper applies the wide-sense stationary theory of multihypothesis motion compensation for hybrid video codecs to multihypothesis motion estimation. This allows us to study {{the influence of the}} <b>displacement</b> <b>error</b> correlation on the e#ciency of multihypothesis motion compensation. Reducing the <b>displacement</b> <b>error</b> correlation between the hypotheses decreases the variance of the multihypothesis prediction error. We derive a property for the <b>displacement</b> <b>error</b> correlation coe #cient for an optimal multihypothesis motion estimator in the mean squared error sense. We observe for the wide-sense stationary model that jointly optimal motion estimation improves the prediction performance and reduces the prediction error variance up to 12 dB per accuracy refinement step compared to 6 dB per accuracy refinement step for uncorrelated displacement errors. Consequently, the gain of multihypothesis motion-compensated prediction with jointly optimal motion estimation over motion-compensated prediction increases by improving the accuracy of each hypothesis. We also discuss the combination of hypotheses with additive noise and extend the predictor by the optimum Wiener filter...|$|E
40|$|This paper {{discusses}} {{the effect of}} spatial quantization <b>errors</b> and <b>displacement</b> <b>errors</b> on the precision dimensional measurements for an edge segment. Probabilistic analysis {{in terms of the}} resolution of the image is developed for 2 D quantization errors. Expressions for the mean and variance of these errors are developed. The probability density function of the quantization error is derived. The position and orientation errors of the active head are assumed to be normally distributed. A probabilistic analysis in terms of these errors is developed for the <b>displacement</b> <b>errors.</b> Through integrating the spatial quantization <b>errors</b> and the <b>displacement</b> <b>errors,</b> we can compute the total error in the active vision inspection system. Based on the developed analysis, we investigate whether a given set of sensor setting parameters in an active system is suitable to obtain a desired accuracy for specific dimensional measurements, and one can determine sensor positions and view directions which meet the necessary tolerance and accuracy of inspection. published_or_final_versio...|$|R
40|$|The {{effects of}} various {{experimental}} parameters on the <b>displacement</b> <b>errors</b> in the triangulation solution of an elongated object in space due to pointing uncertainties in {{the lines of}} sight have been determined. These parameters were the number and location of observation stations, the object's location in latitude and longitude, and the spacing of the input data points on the azimuth-elevation image traces. The <b>displacement</b> <b>errors</b> due to uncertainties in the coordinates of a moving station have been determined as functions of the number and location of the stations. The effects of incorporating the input data from additional cameras {{at one of the}} stations were also investigated...|$|R
40|$|This article {{presents}} an optimized sensor planning system for active visual inspection Ž. of three-dimensional manufacturing computer-aided design CAD models. Quantiza-tion <b>errors</b> and <b>displacement</b> <b>errors</b> are inevitable in active visual inspection. To obtain high accuracy for dimensioning the entities of three-dimensional CAD models, mini-mization of these errors is essential. Spatial quantization errors result in digitization. The errors are serious when {{the size of}} the pixel is significant compared to the allowable tolerance in the object dimension on the image. In placing the active sensor to perform inspection, displacement of the sensors in orientation and location is common. The difference between observed dimensions obtained by the displaced sensor and the actual dimensions is defined as <b>displacement</b> <b>errors.</b> The density functions of quantization <b>errors</b> and <b>displacement</b> <b>errors</b> depend on camera resolution and camera locations and orientations. The sensor constraints, such as resolution, focus, field-of-view, and visibility constraints, restrict sensor placement. To obtain a satisfactory view of the targeted entities of the CAD models, these constraints have to be satisfied. In this article, we focus on the edge line segments as the inspected * To whom all correspondence should be addressed...|$|R
30|$|In each {{position}} {{reached by}} the patient, absolute angle <b>displacement</b> <b>error</b> was obtained by measuring {{the difference between the}} reference angle and indicated actual angle. Test was reproduced three times, and the mean of the three readings was considered final angular error.|$|E
3000|$|When {{a set of}} real {{images are}} segmented, the {{criteria}} {{used to compare the}} segmentation performance of various algorithms were the Probabilistic Rand Index (PRI) for evaluation of the results obtained from the tested algorithm to a set of manually segmented images [27], the Variation of Information (VOI) for quantification of the loss of information and the gain between two clusters belonging to the lattice of possible partitions [28], the Global Consistency Error (GCE) for quantification to what extent a segmentation can be viewed as the refinement of the other [29], and the Boundary <b>Displacement</b> <b>Error</b> (BDE) for evaluation of the average <b>displacement</b> <b>error</b> of boundary pixels between two segmented images by computing the distance between the pixel and the closest pixel in the other segmentation [30]. The Kappa coefficient [...]...|$|E
30|$|Thirty adult {{patients}} (G 1) with ischemic strokes {{ranging from}} 6  months to 1  year and 30 healthy control (G 2) were assessed for shoulder proprioception. Angular <b>displacement</b> <b>error</b> was measured during active and passive repositioning of shoulder {{external and internal}} rotation in both patients’ shoulders and in control’s dominant upper limb.|$|E
40|$|Quantization <b>errors</b> and <b>displacement</b> <b>errors</b> are {{inevitable}} in active vision inspection [21, 24, 2 S]. In {{order to obtain}} high accuracy for dimensioning the entities of three-dimensional CAD models, minimization of these errors is essential. Spatial quantization errors are resulted in digitization. The errors are serious when {{the size of the}} pixel is sigtllficant compared to the allowable tolerance in the object dimension on the image. In placing the active sensor to perform inspection, dtsplacement of the sensors in orientation and location is common. The Merence between observed dimensions obtained by the dtsplaced sensor and the actual dimensions is defined as <b>displacement</b> <b>errors.</b> The density functions of quantization <b>errors</b> and <b>displacement</b> <b>errors</b> depend on camera resolution and camera locations and orientations. We us genetic algorithm to minimize the probabilistic magnitude of the errors subject to the sensor constraints, such as the resolution, field-of-view, focus, and visibility constraints. Since the objective functions and the constmint functions are both complicated and nonlinear, traditional nonlinear programming may not be efficient and trapping at a local minimum may occur. Using crossover operations, mutation operations, and the stochastic selection in genetic algorithm, trapping can be avoided. 1...|$|R
40|$|In {{this paper}} we derive {{nonlinear}} evolution equations {{associated with a}} class of non-convex energy functionals {{which can be used}} for correcting <b>displacement</b> <b>errors</b> in imaging data. We study properties of these filtering flows and provide experiments for correcting angular perturbations in tomographical data...|$|R
40|$|AbstractThis paper {{studies the}} {{application}} of preconditioned conjugate gradient methods in high resolution image reconstruction problems. We consider reconstructing high resolution images from multiple undersampled, shifted, degraded frames with subpixel <b>displacement</b> <b>errors.</b> The resulting blurring matrices are spatially variant. The classical Tikhonov regularization and the Neumann boundary condition {{are used in the}} reconstruction process. The preconditioners are derived by taking the cosine transform approximation of the blurring matrices. We prove that when the L 2 or H 1 norm regularization functional is used, the spectra of the preconditioned normal systems are clustered around 1 for sufficiently small subpixel <b>displacement</b> <b>errors.</b> Conjugate gradient methods will hence converge very quickly when applied to solving these preconditioned normal equations. Numerical examples are given to illustrate the fast convergence...|$|R
40|$|Trihedral corner reflectors {{are being}} {{increasingly}} used as point targets in deformation monitoring studies using interferometric {{synthetic aperture radar}} (InSAR) techniques. The frequency and size dependence of the corner reflector Radar Cross Section (RCS) means that no single design can perform equally in all the possible imaging modes and radar frequencies available on the currently orbiting Synthetic Aperture Radar (SAR) satellites. Therefore, either a corner reflector design tailored to a specific data type or a compromise design for multiple data types is required. In this paper, I outline the practical and theoretical considerations {{that need to be}} made when designing appropriate radar targets, with a focus on supporting multi-frequency SAR data. These considerations are tested by performing field experiments on targets of different size using SAR images from TerraSAR-X, COSMO-SkyMed and RADARSAT- 2. Phase noise behaviour in SAR images can be estimated by measuring the Signal-to-Clutter ratio (SCR) in individual SAR images. The measured SCR of a point target is dependent on its RCS performance and the influence of clutter near to the deployed target. The SCR is used as a metric to estimate the expected InSAR <b>displacement</b> <b>error</b> incurred by the design of each target and to validate these observations against theoretical expectations. I find that triangular trihedral corner reflectors as small as 1 m in dimension can achieve a <b>displacement</b> <b>error</b> magnitude of a tenth of a millimetre or less in medium-resolution X-band data. Much larger corner reflectors (2. 5 m or greater) are required to achieve the same <b>displacement</b> <b>error</b> magnitude in medium-resolution C-band data. Compromise designs should aim to satisfy the requirements of the lowest SAR frequency to be used, providing that these targets will not saturate the sensor of the highest frequency to be used. Finally, accurate boresight alignment of the corner reflector can be critical to the overall target performance. Alignment accuracies better than 4 ° in azimuth and elevation will incur a minimal impact on the <b>displacement</b> <b>error</b> in X and C-band data...|$|E
40|$|The {{observation}} of a flowfield in PIV via a mirror causes aberrations {{in the image}} plane which can reach several percent of the displacement to be measured. These imaging errors are investigated theoreticaly {{and the results are}} confirmed by comparison with experimental data. Several experimental parameters are discussed in respect to their influence on the <b>displacement</b> <b>error...</b>|$|E
30|$|Statistically {{significant}} increase in angular <b>displacement</b> <b>error</b> was found in all tests in the affected shoulder compared to the unaffected contralateral shoulder and dominant arm of control subjects. The contralateral unaffected shoulder of patients showed within normal values and no differences with control values. Passive external and internal rotations showed statistically higher errors in patients with cortical lesions compared to those with subcortical lesions.|$|E
40|$|This paper {{studies the}} {{application}} of preconditioned conjugate gradient methods in high resolution image reconstruction problems. We consider reconstructing high resolution images from multiple undersampled, shifted, degraded frames with subpixel <b>displacement</b> <b>errors.</b> The resulting blurring matrices are spatially variant. The classical Tikhonov regularization and the Neumann boundary condition {{are used in the}} reconstruction process. The preconditioners are derived by taking the cosine transform approximation of the blurring matrices. We prove that when the L 2 or H 1 norm regularization functional is used, the spectra of the preconditioned normal systems are clustered around 1 for sufficiently small subpixel <b>displacement</b> <b>errors.</b> Conjugate gradient methods will hence converge very quickly when applied to solving these preconditioned normal equations. Numerical examples are given to illustrate the fast convergence. 1 Introduction Due to hardware limitations, imaging systems often provide [...] ...|$|R
40|$|Spatial {{quantization}} {{errors are}} resulted in digitization. The errors are serious when {{the size of}} the pixel is significant compared to the allowable tolerance in the object dimension on the image. In placing the active sensor to perform inspection, displacement of the sensors in orientation and location is common. The difference between observed dimensions obtained by the displaced sensor and the actual dimensions is defined as <b>displacement</b> <b>errors.</b> The density functions of quantization <b>errors</b> and <b>displacement</b> <b>errors</b> depend on the camera resolution and camera locations and orientations. We use genetic algorithm to minimize the probabilistic magnitude of the errors subject to the sensor constraints, such as the resolution, field-of-view, focus, and visibility constraints. Since the objective functions and the constraint functions are both complicated and nonlinear, traditional nonlinear programming may not be efficient and trapping at a local minimum may occur. Using crossover operations, mutation operations, and the stochastic selection in genetic algorithm, trapping can be avoided. published_or_final_versio...|$|R
40|$|In {{this paper}} we {{consider}} the estimation of the unknown hyperpa-rameters for the problem of reconsmcting a hi&-resolution image from multiple undersampled, shifted, blurred and degraded frames with subpixel <b>displacement</b> <b>errors.</b> We derive mathematical ex-pressions for the iterative calculation of the maximum likelihwd estimate (mle) of the unknown hyperparameters given the low res-olution observed images. Finally, the proposed method is tested on a synthetic image. 1...|$|R
30|$|It can be {{seen that}} the maximum {{distances}} were greater than the noise value (173  mm) added to the OPC. The average distance for all tested objects was smaller than 43.3  mm, that is ¼ of the maximum <b>displacement</b> <b>error.</b> The standard deviation in all cases was lower than 86.6  mm, that is ½ of the maximum error. Thus, the precision was on the level of 2 P[*]=[*] 95 %.|$|E
40|$|Electromagnetic {{scattering}} {{caused by}} the site environment of airports will cause multipath effect. In the presence of multipath, the measured angle position, which is derived through the measuring of the time interval between &ldquo;From&rdquo; and &ldquo;To&rdquo; scanning pulse pairs, will deviate from the actual position, leading lower system guidance accuracy and higher risk. An error research method based on the equivalent replacement of the scanning beaming main lobe is presented. Through the equivalent Gaussian replacement of the beam main-lobe of &ldquo;From&rdquo; and &ldquo;To&rdquo; pulse, the error formula {{in the presence of}} several multipath components is derived. The method is used to calculate the angle-measuring performance error of the receiver for the special multipath environment and a practical angle-measurement error model is established. The results derived from the replacement model are compared with that of the classical model. The new model can overcome the limitation and deficiency of classical model in exact error calculation. Moreover, the accuracy is more improved than the classical model. The <b>displacement</b> <b>error</b> of envelope peak position and <b>displacement</b> <b>error</b> of envelope peak level caused by multipath effect is further analyzed based on the replacement method...|$|E
40|$|Intraoperative laparoscopic {{calibration}} {{remains a}} challenging task. In this work {{we present a}} new method and instrumentation for intraoperative camera calibration. Contrary to conventional calibration methods, the proposed technique allows intraoperative laparoscope calibration from single perspective observations, resulting in a standardized scheme for calibrating in a clinical scenario. Results show an average <b>displacement</b> <b>error</b> of 0. 52 ± 0. 19 mm, indicating sufficient accuracy for clinical use. Additionally, the proposed method is validated clinically by performing a calibration during the surgery...|$|E
40|$|We {{study the}} problem of reconstructing a super-resolution image f from {{multiple}} undersampled, sifted, degraded frames with subpixel <b>displacement</b> <b>errors.</b> The corresponding operator H is a spatially- variant operator. In this paper, we apply the preconditioned conjugate gradient method with cosine transform preconditioners to solve the discrete problems. Preliminary results show that our method converges very fast and gives sound recovery of the super- resolution images. published_or_final_versio...|$|R
40|$|An optical {{heterodyne}} interferometer {{that can}} be used to measure linear <b>displacements</b> with an <b>error</b> <= 20 pm has been developed. The remarkable accuracy of this interferometer is achieved through a design that includes (1) a wavefront split that reduces (relative to amplitude splits used in other interferometers) self interference and (2) a common-optical-path configuration that affords common-mode cancellation of the interference effects of thermal-expansion changes in optical-path lengths. The most popular method of displacement- measuring interferometry involves two beams, the polarizations of which are meant to be kept orthogonal upstream of the final interference location, where the difference between the phases of the two beams is measured. Polarization leakages (deviations from the desired perfect orthogonality) contaminate the phase measurement with periodic nonlinear errors. In commercial interferometers, these phase-measurement <b>errors</b> result in <b>displacement</b> <b>errors</b> in the approximate range of 1 to 10 nm. Moreover, because prior interferometers lack compensation for thermal-expansion changes in optical-path lengths, they are subject to additional <b>displacement</b> <b>errors</b> characterized by a temperature sensitivity of about 100 nm/K. Because the present interferometer does not utilize polarization in the separation and combination of the two interfering beams and because of the common-mode cancellation of thermal-expansion effects, the periodic nonlinear errors and the sensitivity to temperature changes are much smaller than in other interferometer...|$|R
40|$|An image-acquisition system {{composed}} of {{an array of}} sensors, where each sensor has a subarray of sensing elements of suitable size, has recently been popular for increasing the spatial resolution with high signal-to-noise ratio beyond the performance bound of technologies that constrain the manufacture of imaging devices. Small perturbations around the ideal subpixel locations of the sensing elements (responsible for capturing the sequence of undersampled degraded frames), because of imperfections in fabrication, limit {{the performance of the}} signal-processing algorithms for processing and integrating the acquired images for the desired enhanced resolution and quality. The contributions of this paper include an analysis of the <b>displacement</b> <b>errors</b> on the convergence rate of the iterative approach for solving the transform based preconditioned system of equations. Subsequently, it is established that the use of the MAP, L 2 norm or H 1 norm regularization functional leads to a proof of linear convergence of the conjugate gradient method in terms of the <b>displacement</b> <b>errors</b> caused by the imperfect subpixel locations. Results of simulation support the analytical results. published_or_final_versio...|$|R
40|$|Biacetyl {{phosphorescence}} {{has been}} the commonly used molecular tagging velocimetry (MTV) technique to investigate in-cylinder flow evolution and cycle-to-cycle variations in an optical engine. As the phosphorescence of biacetyl tracer deteriorates {{in the presence of}} oxygen, nitrogen was adopted as the working medium in the past. Recently, nitrous oxide MTV technique was employed to measure the velocity profile of an air jet. The authors here plan to investigate the potential application of this technique for engine flow studies. A possible experimental setup for this task indicated different permutations of image signal-to-noise ratio (SNR) and laser line width. In the current work, a numerical analysis is performed to study the effect of these two factors on <b>displacement</b> <b>error</b> in MTV image processing. Also, several image filtering techniques were evaluated and the performance of selected filters was analyzed in terms of enhancing the image quality and minimizing displacement errors. The flow <b>displacement</b> <b>error</b> without image preprocessing was observed to be inversely proportional to SNR and directly proportional to laser line width. The mean filter resulted in the smallest errors for line widths smaller than 9 pixels. The effect of filter size on subpixel accuracy showed that error levels increased as the filter size increased...|$|E
40|$|This thesis {{described}} a morphing-based precipitation verification strategy inspired by Keil and Craig. This strategy {{is based on}} an optical flow algorithm to morph the image (field) of the forecast precipitation into an image that resembles the image (field) of the observed (analyzed) precipitation. This method treats the precipitation as a passive scalar and carries out the morphing by computing a vector field, called the optical flow, which is then used to advect the original forecast precipitation field. The information provided by the optical flow and the morphed image of the forecast precipitation field is used to define the measures of the <b>displacement</b> <b>error</b> and residual error. There are two novel aspects of our strategy. First, it imposes a constrain on the morphing process in order to prevent the over-convergence of pixels during morphing to a few locations of large errors. Second, it uses a new definition of the <b>displacement</b> <b>error</b> and provides a new interpretation of the other error terms. By applying the new morphing-based precipitation strategy to a schematic idealized example and a real hurricane example, we demonstrate that the constrain imposed largely reduces the risk of over-convergence and the error measures we derive from the morphing process accurately measure the corresponding error components...|$|E
40|$|Jet Propulsion Laboratory (JPL) is {{transmitting}} {{real time}} stream of GPS measurement over Internet via a UDP socket. It {{is a high}} rate stream that sends 1 Hz data from several stations, today the total number is 70 – 80 stations. This thesis is focusing on finding problems {{not only with the}} stream itself, also current problems with the actual measurement. There will be some improvement described however no focus is in network setup or geodetic detection of an actual earthquake. The delay of the stream is gradually increasing from 5 to 15 seconds and drops down to 5 seconds again. This delay is end to end time and have a rate of about one to one and a half hour. Over this time period JPL receives data, process it and resend it to end user. At the receiver end not all data arrives. There are 10 missing measurement in a row at the time the delay drops. The statement is that anywhere in the processing, either at JPL or at receiving computer there is a overflow in a buffer which explains the drop. There are other drops of data, which can be from seconds to hours in length. Whenever the dropped station starts to transmit again, sigma tends to be very large, which is a good thing. However it returns very quickly to normal, much faster than the actual GPS position. A sigma can take in the order of minutes to return while positions takes hours. This problem can be misleading and can create further problem for filters or other processing jobs where sigma is used. Different stations seem to be affected by the missing measure differently. This can explain the variation in result of <b>displacement</b> <b>error.</b> The measurement is subject to a centimeter to decimeter <b>displacement</b> <b>error</b> for a ten- minute gap of data. The <b>displacement</b> <b>error</b> is larger when ever a loss of data occurs. During the next coming hours a large variation of the measurement occurs and a <b>displacement</b> <b>error</b> will be much greater. The solution for this is either to minimize any drop of data which can be a matter of investing in better connection for the receiver stations around the world to JPL or look into the models JPL have and give a better sigma estimate. There are a significant autocorrelation in the signal, the peak can be up to 80 % of top value and is close to a sidereal day. Because of this continues correlation a filter can be used to improve the time series. A five-day median filter gives an improvement up to 60 %. This number will increase whenever the stated problems above are resolved...|$|E
30|$|The errors on the {{identified}} parameters can have different origins. They {{can be related}} to errors on the measured <b>displacement</b> (i.e. DIC <b>errors),</b> on the geometry description (domain and boundaries), on the behavior law (description of the secant tensor B_n^s), on the phase description (material mesh), or on the stress description (stress mesh and boundary conditions used for the stress computation). In this paper we address the influence of DIC errors, of phase description errors and, in a lesser extent of stress description errors. Since conform meshes are used, phase description errors are related both to stress description errors and to measured <b>displacement</b> <b>errors.</b>|$|R
40|$|Abstract—In {{this paper}} a new {{approach}} based on Shuffled Frog Leaping Algorithm (SFL) for measurement-type current transformer(CT) design has been presented. This algorithm can present designed parameters of sample current transformer so that minimizes ratio and phase <b>displacement</b> <b>errors</b> to 1. 2 times of rated current and transformer made cost also. Finally, several current transformers with different rated values are designed and {{results show that the}} proposed approach can be used for optimal design of current transformer perfect...|$|R
40|$|This paper {{discusses}} {{the efficiency of}} a compression scheme for video sequences that jointly encodes groups of pictures. Our approach, motion-compensated transform coding, applies a KLT to decorrelate a set of motion-compensated pictures for efficient encoding. The theoretical investigation utilizes a signal model for inaccurate motion compensation and provides a performance comparison to motion-compensated prediction. We discuss the influence of motion accuracy, residual noise, and the correlation of <b>displacement</b> <b>errors</b> dependent {{on the number of}} coded pictures...|$|R
