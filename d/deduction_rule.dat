37|341|Public
5000|$|Deduction rules {{can then}} be {{represented}} by binary relations on Gödel numbers of lists of formulas. In other words, suppose {{that there is a}} <b>deduction</b> <b>rule</b> , by which one can move from the formulas [...] to a new formula [...] Then the relation [...] corresponding to this <b>deduction</b> <b>rule</b> says that [...] is related to [...] (in other words, [...] holds) if [...] is the Gödel number of a list of formulas containing [...] and [...] and [...] is the Gödel number of the list of formulas consisting of those in the list coded by [...] together with [...] Because each <b>deduction</b> <b>rule</b> is concrete, it is possible to effectively determine for any natural numbers [...] and [...] whether they are related by the relation.|$|E
50|$|Axioms P1, P2 and P3, {{with the}} <b>deduction</b> <b>rule</b> modus ponens (formalising intuitionistic propositional logic), {{correspond}} to combinatory logic base combinators I, K and S with the application operator. Proofs in the Hilbert system then correspond to combinator terms in combinatory logic. See also Curry-Howard correspondence.|$|E
50|$|A propositional {{formula is}} a {{tautology}} {{if it is}} true under every valuation (or interpretation) of its predicate symbols. If Φ is a tautology, and Θ is a substitution instance of Φ, then Θ is again a tautology. This fact implies the soundness of the <b>deduction</b> <b>rule</b> described in the previous section.|$|E
50|$|Because Hilbert-style {{systems have}} very few <b>deduction</b> <b>rules,</b> {{it is common to}} prove metatheorems that show that {{additional}} <b>deduction</b> <b>rules</b> add no deductive power, in the sense that a deduction using the new <b>deduction</b> <b>rules</b> can be converted into a deduction using only the original <b>deduction</b> <b>rules.</b>|$|R
40|$|I {{show that}} the model-theoretic meaning that can be read off the natural <b>deduction</b> <b>rules</b> for {{disjunction}} fails to have certain desirable properties. I use this result to argue against a modest form of inferentialism which uses natural <b>deduction</b> <b>rules</b> to fix model-theoretic truth-conditions for logical connectives...|$|R
5000|$|Outlines {{a common}} {{approach}} to end base erosion by interest <b>deduction</b> <b>rules</b> for eligible MNEs.|$|R
5000|$|The <b>deduction</b> <b>rule</b> is an {{important}} property of Hilbert-style systems because {{the use of this}} metatheorem leads to much shorter proofs than would be possible without it. Although the deduction theorem could be taken as a primitive rule of inference in such systems, this approach is not generally followed; instead, the deduction theorem is obtained as an admissible rule using the other logical axioms and modus ponens. In other formal proof systems, the deduction theorem is sometimes taken as a primitive rule of inference. For example, in natural deduction, the deduction theorem is recast as an introduction rule for [...] "→".|$|E
40|$|Abstract. Hex is {{a classic}} board game invented {{in the middle of}} the twentieth century by Piet Hein and rediscovered later by John Nash. The best Hex {{artificial}} players analyse the board positions by deducing complex virtual connections from elementary connections using the H-Search algorithm. In this paper, we extend the H-search with a new <b>deduction</b> <b>rule.</b> This new <b>deduction</b> <b>rule</b> is capable of discovering virtual connections that the H-search cannot prove. Thanks to this new <b>deduction</b> <b>rule,</b> the horizon of the artificial Hex players should move further away. 1...|$|E
40|$|Frege {{systems with}} the <b>deduction</b> <b>rule</b> produce at most {{quadratic}} speedup over Frege systems using {{as a measure}} of length the number of symbols in the proof. We study whether that speedup is in reality smaller. We show that the speedup is linear when the Frege proofs are tree-like. Also, two groups of formulas, permutation formulas and transitive closure formulas, that seemed most likely to produce an almost quadratic speedup when using the <b>deduction</b> <b>rule,</b> are shown to produce only log n and log 2 n factors respectively. ...|$|E
50|$|Mandarax is an {{open source}} Java class library for <b>deduction</b> <b>rules.</b> It {{provides}} an infrastructure for defining, managing and querying rule bases.|$|R
2500|$|Logic, {{especially}} {{in the field of}} proof theory, considers theorems as statements (called formulas or well formed formulas) of a formal language. The statements of the language are strings of symbols and [...] may be broadly divided into nonsense and well-formed formulas. A set of <b>deduction</b> <b>rules,</b> also called transformation rules or rules of inference, must be provided. These <b>deduction</b> <b>rules</b> tell exactly when a formula can be derived from a set of premises. The set of well-formed formulas may be broadly divided into theorems and non-theorems. However, according to Hofstadter, a formal system often simply defines all its well-formed formula as theorems.|$|R
40|$|Abstract. This paper {{describes}} {{an approach to}} transform a Structural Operational Semantics given {{as a set of}} <b>deduction</b> <b>rules</b> to a Linear Process Specification. The transformation is provided for <b>deduction</b> <b>rules</b> in De Simone format and extended to incorporate predicates. The Linear Process Specifications are specified in syntax of the language mCRL 2, that, with help of the underlying (higher-order) re-writer/toolset, can be used for simulation, labelled transition system generation and verification of behavioural properties. We illustrate the technique by showing the effect of the transformation from the Structural Operational Semantics specification of a simple process algebra to aLinear Process Specification. ...|$|R
40|$|We {{introduce}} new proof systems for propositional logic, simple deduction Frege systems, general deduction Frege systems and nested deduction Frege systems, which augment Frege systems with variants of the <b>deduction</b> <b>rule.</b> We give upper bounds on the lengths of proofs in Frege proof systems compared to lengths in these new systems...|$|E
40|$|This paper first {{refers to}} the key concept of {{recognition}} of asset losses under the corporate tax law. The tax law basically restricts the loss deduction and imposes requirements of "settlements" with a fact of physical or monetary damage for the special loss deduction unless potential nonrecognized losses may be deducted under the accounting standards {{from the viewpoint of}} disclosure for asset fair values. This loss <b>deduction</b> <b>rule</b> is derived from the foreseeability and legal stability in calculation of taxable income. This paper secondly explains the content and legislative context of the recent amendments in the depreciation system and allowance expenses. Some allowance systems has been repealed in order to enlarge the tax base and increase the tax revenue, however, the accelerated depreciation, newly introduced system in the recent corporate tax reform, brought a broad accrual expense, where it caused an opposite result to the tax base. I would rather mention the background of the past tax reforms and suggest the range of estimated accrual expenses should be more broadened. Finally, this paper would clarify the contemporary signification in the loss <b>deduction</b> <b>rule.</b> ASBJ has issued the cumulative accounting standards for the global convergence, and is now required the final decision for the IFRS adoption. It would likely be said that harmonization between the accounting and tax enforcement would be continuously pursed through this convergence process. The loss <b>deduction</b> <b>rule</b> would have a vital role in the fair value measurement in tax accounting, where tax income and each tax item on a balance sheet are measured by an index with high accuracy and legal settlement...|$|E
40|$|Abstract. The Medial {{rule was}} first devised as a <b>deduction</b> <b>rule</b> in the Calculus of Structures. In this {{paper we explore}} it {{from the point of}} view of {{category}} theory, as additional structure on a *-autonomous category. This gives us some insights on the denotational semantics of classical propositional logic, and allows us to construct new models for it, based on suitable generalizations of the theory of coherence spaces. ...|$|E
50|$|Logic, {{especially}} {{in the field of}} proof theory, considers theorems as statements (called formulas or well formed formulas) of a formal language. The statements of the language are strings of symbols and may be broadly divided into nonsense and well-formed formulas. A set of <b>deduction</b> <b>rules,</b> also called transformation rules or rules of inference, must be provided. These <b>deduction</b> <b>rules</b> tell exactly when a formula can be derived from a set of premises. The set of well-formed formulas may be broadly divided into theorems and non-theorems. However, according to Hofstadter, a formal system often simply defines all its well-formed formula as theorems.|$|R
40|$|Abstract. Structured Operational Semantics (SOS) is {{a popular}} method for {{defining}} semantics by means of <b>deduction</b> <b>rules.</b> An important feature of <b>deduction</b> <b>rules,</b> or simply SOS rules, are negative premises, which are crucial in the definitions of such phenomena as priority mechanisms and time-outs. Orderings on SOS rules were proposed by Phillips and Ulidowski {{as an alternative to}} negative premises. The meaning of general types of SOS rules with orderings has not been studied hitherto. This paper presents satisfactory ways of giving a meaning to general SOS rules with orderings. We also give semantics-preserving transformations between the two paradigms, namely, SOS with negative premises and SOS with orderings. ...|$|R
40|$|The mCRL 2 {{language}} is a formal specification language {{that is used to}} specify and model the behavior of distributed systems and protocols. With the accompanying toolset, it is possible to simulate, visualize, analyze and verify behavioral properties of mCRL 2 models automatically. The semantics of the mCRL 2 {{language is}} defined formally using Structural Operational Semantics (SOS) but implemented manually in the underlying toolset using C++. Like with most formal languages, the underlying toolset was created with the formal semantics in mind but {{there is no way to}} actually guarantee that the implementation matches the intended semantics. To validate that the implemented behavior for the mCRL 2 language corresponds to its formal semantics, we describe the SOS <b>deduction</b> <b>rules</b> of the mCRL 2 language, and perform the transformation from the mCRL 2 ’s SOS <b>deduction</b> <b>rules</b> to a Linear Process Specification. As our transformation directly takes the SOS <b>deduction</b> <b>rules</b> and transforms them into mCRL 2 data equations, we are basically feeding the mCRL 2 toolset its own formal language definition. This report describes (i) the semantics for the untimed fragment of the mCRL 2 language, (ii) the transformation of the <b>deduction</b> <b>rules</b> into data equations including the underlying design decisions and (iii) the experiments that have been conducted with our semantic transformation. Despite its formal characterization, thorough study and broad use in many areas, our semantic dogfooding approach revealed a number of (subtle) differences between the mCRL 2 ’s intended semantics, the defined semantics and its actual implementation...|$|R
40|$|Pandora {{is a tool}} {{to support}} the {{learning}} of first order natural deduction. It includes a help window, an interactive context sensitive tutorial known as the “e-tutor ” and facilities to save, reload and export to L AT E X. Every attempt to apply a natural <b>deduction</b> <b>rule</b> is met with either success or a helpful error message, providing the student with instant feedback. This paper describes the e-tutor and our experiences of using the tool in teaching...|$|E
40|$|We {{introduce}} new proof systems for propositional logic, simple deduction Frege systems, general deduction Frege systems and nested deduction Frege systems, which augment Frege systems with variants of the <b>deduction</b> <b>rule.</b> We give upper bounds on the lengths of proofs in these systems compared to lengths in Frege proof systems. As an application we give a near-linear simulation of the propositional Gentzen sequent calculus by Frege proofs. The {{length of a}} proof {{is the number of}} steps or lines in the proof. A genera...|$|E
40|$|Abstract. For proving {{response}} {{properties for}} systems with compassion requirements, a <b>deduction</b> <b>rule</b> is introduced in [13]. In order {{to use the}} rule, auxiliary constructs are needed. They include helpful assertions and ranking functions defined on a well-founded domain. Along {{the line of the}} work [2] that computes ranking functions for response properties for systems with justice requirements, we develop an approach which extends that of [2] for computing ranking functions for systems with compassion requirements. We illustrate the use of the approach on three examples. ...|$|E
5000|$|The Lambek {{calculus}} {{consists of}} several <b>deduction</b> <b>rules,</b> which specifyhow type inclusion assertions can be derived. In the followingrules, upper case roman letters stand for types, upper case Greekletters stand for sequences of types. A sequent {{of the form}} ...|$|R
40|$|Algebraic {{properties}} specify {{some natural}} properties of programming and specification constructs. This paper {{provides an overview}} of techniques to guarantee or generate algebraic properties of language constructs by investigating the syntactic shape of the <b>deduction</b> <b>rules</b> defining their operational semantics. ...|$|R
40|$|Inference {{rules for}} {{resolution}} based {{systems can be}} classified into <b>deduction</b> <b>rules,</b> which add new objects, and reduction rules, which remove objects. Traditional reduction rules like subsumption do not actively contribute to a solution, but they help to avoid redundancies in the search space. We present a number of advanced reduction rules, which can cope with high degrees of redundancy and play a distinctly active part because they find trivial solutions {{on their own and}} thus relieve the control component for the <b>deduction</b> <b>rules</b> from low level tasks. We describe how these reduction rules can be implemented with reasonable efficiency in a clause graph resolution system, but they are not restricted to this particular representation...|$|R
40|$|Abstract. The {{tautology}} {{problem is}} the problem to prove the validity of statements. In this paper, we present a calculus for this undecidable problem on graphical conditions, prove its soundness, investigate the necessity of each <b>deduction</b> <b>rule,</b> and discuss practical aspects concerning an implementation. As we use the framework of weak adhesive HLR categories, the calculus is applicable {{to a number of}} replacement capable structures, such as Petri-Nets, graphs or hypergraphs. Key words: first-order tautology problem, high-level conditions, theo-rem proving, resolution, weak adhesive HLR categories...|$|E
40|$|Published on 2007 - 10 - 09. A {{paper by}} my {{collaborator}} Lutz Strassburger on a closely related subject {{was published in}} the same journal on the same day. International audienceThe Medial rule was first devised as a <b>deduction</b> <b>rule</b> in the Calculus of Structures. In this paper we explore it {{from the point of view}} of category theory, as additional structure on a *-autonomous category. This gives us some insights on the denotational semantics of classical propositional logic, and allows us to construct new models for it, based on suitable generalizations of the theory of coherence spaces...|$|E
40|$|The {{so called}} frame axioms problem or frame problem is {{well-known}} {{to anybody who}} is interested in automatic problem solving. Some information concerning this problem {{can be found in}} (1) or (2). In (2) the author proposes to eliminate the frame axioms {{in such a way that}} they are replaced by a new <b>deduction</b> <b>rule</b> denoted as UNLESS-operator. In fact,this operator is a rule enabling to derive that something concerning the environment is valid at the present situation supposing it was valid in a past situation and it is not provable that a change concerning the validity of this statement has occured. Fo...|$|E
40|$|Abstract. Recent {{studies on}} {{frequent}} itemset mining algorithms resulted in significant performance improvements. However, if the minimal support threshold is set too low, or {{the data is}} highly correlated, the number of frequent itemsets itself can be prohibitively large. To overcome this problem, recently several proposals {{have been made to}} construct a concise representation of the frequent itemsets, instead of mining all frequent itemsets. The main goal {{of this paper is to}} identify redundancies in the set of all frequent itemsets and to exploit these redundancies in order to reduce the result of a mining operation. We present <b>deduction</b> <b>rules</b> to derive tight bounds on the support of candidate itemsets. We show how the <b>deduction</b> <b>rules</b> allow for constructing a minimal representation for all frequent itemsets. We also present connections between our proposal and recent proposals for concise representations and we give the results of experiments on real-life datasets that show the effectiveness of the <b>deduction</b> <b>rules.</b> In fact, the experiments even show that in many cases, first mining the concise representation, and then creating the frequent itemsets from this representation outperforms existing frequent set mining algorithms. ...|$|R
50|$|Still by, for instance, proposing {{alternative}} <b>deduction</b> <b>rules</b> involving Leibniz's law {{or other}} rules for validity some philosophers {{are willing to}} defend vagueness {{as some kind of}} metaphysical phenomenon. One has, for example, Peter van Inwagen (1990), Trenton Merricks and Terence Parsons (2000).|$|R
2500|$|The side box {{shows the}} <b>deduction</b> <b>rules</b> of the HM type system. Remember that [...] and [...] denote poly- and monotypes respectively, so the {{premises}} [...] of [...] and [...] of , for example, are distinct. One can roughly divide the rules into two groups: ...|$|R
40|$|The {{paper is}} {{dedicated}} to the problem of adding a modality to the many-valued logics in the purpose of obtaining completeness results for Kripke semantics. We define a class of modal many-valued logics and their corresponding Kripke models and modal many-valued algebras. Completeness results are considered through the construction of a canonical model. Completeness is obtained for modal finitely-valued logics but also for a modal many-valued system with an infinitary <b>deduction</b> <b>rule.</b> We introduce two classes of frames for the finitely-valued logics and show that they define two distinct classes of Kripke-complete logics. Comment: 12 page...|$|E
40|$|Interactive theorem provers require {{too much}} effort from their users. We have been {{developing}} {{a system in}} which Isabelle users obtain automatic support from automatic theorem provers (ATPs) such as Vampire and SPASS. An ATP is invoked at suitable points in the interactive session, and any proof found is given to the user in a window displaying an Isar proof script. There are numerous differences between Isabelle (polymorphic higher-order logic with type classes, natural <b>deduction</b> <b>rule</b> format) and classical ATPs (first-order, untyped, clause form). Many of these differences have been bridged, and a working prototype that uses background processes already provides much of the desired functionality. ...|$|E
40|$|A useful {{enhancement}} of an NLG system for verbalising ontologies {{would be a}} module capable of explaining undesired entailments of the axioms encoded by the developer. This task raises interesting issues of content planning. One approach, useful as a baseline, is simply to list the subset of axioms relevant to inferring the entailment; however, in many cases it will still not be obvious, even to OWL experts, why the entailment follows. We suggest an approach in which further statements are added in order to construct a proof tree, with every step based on a relatively simple <b>deduction</b> <b>rule</b> of known difficulty; we also describe an empirical study through which the difficulty of these simple deduction patterns has been measured. ...|$|E
40|$|Abstract: All {{frequent}} itemset mining algorithms {{rely heavily}} on the monotonicity principle for pruning. This principle allows for excluding candidate itemsets from the expensive counting phase. In this paper, we present sound and complete <b>deduction</b> <b>rules</b> to derive bounds on the support of an itemset. Based on these <b>deduction</b> <b>rules,</b> we construct a condensed representation of all frequent itemsets, by removing those itemsets for which the support can be derived, resulting in the so called Non-Derivable Itemsets (NDI) representation. We also present connections between our proposal and recent other proposals for condensed representations of frequent itemsets. Experiments on real-life datasets show {{the effectiveness of the}} NDI representation, making the search for frequent non-derivable itemsets a useful and tractable alternative to mining all frequent itemsets...|$|R
40|$|A logical {{framework}} {{consisting of a}} polymorphic call-by-value functional language and a first-order logic on the values is presented, which is a reconstruction of {{the logic of the}} verification system VeriFun. The reconstruction uses contextual semantics to define the logical value of equations. It equates undefinedness and non-termination, which is a standard semantical approach. The main results of this paper are: Meta-theorems about the globality of several classes of theorems in the logic, and proofs of global correctness of transformations and <b>deduction</b> <b>rules.</b> The <b>deduction</b> <b>rules</b> of VeriFun are globally correct if rules depending on termination are appropriately formulated. The reconstruction also gives hints on generalizations of the VeriFun framework: reasoning on nonterminating expressions and functions, mutual recursive functions and abstractions in the data values, and formulas with arbitrary quantifier prefix could be allowed...|$|R
40|$|Abstract. First {{experiences}} with utilization of formalized items of do-main knowledge {{in a process}} of association rules mining are described. We use association rules- atomic consequences of items of domain knowl-edge and suitable <b>deduction</b> <b>rules</b> to filter out uninteresting association rules. The approach is experimentally implemented in the LISp–Miner system...|$|R
