13|33|Public
25|$|Most {{people do}} not keep record of minute details of their {{healthcare}} experiences and therefore {{find it difficult to}} make use of web-based PHRs. Overall, the sites selected for evaluation offered limited functionality to the general public. Low adoption of web-based PHRs can be a direct result of limitations in these applications’ data <b>entry,</b> <b>validation</b> and information display methods. PHR development should be guided by ample patient-oriented research in future.|$|E
5000|$|Layered caching scheme, {{supporting}} separate caching of data queries {{and output}} fragments, via database, shared memory, memcached) for storing cached data, and dynamic cache <b>entry</b> <b>validation</b> upon retrieval ...|$|E
50|$|Most {{people do}} not keep record of minute details of their {{healthcare}} experiences and therefore {{find it difficult to}} make use of web-based PHRs. Overall, the sites selected for evaluation offered limited functionality to the general public. Low adoption of web-based PHRs can be a direct result of limitations in these applications’ data <b>entry,</b> <b>validation</b> and information display methods. PHR development should be guided by ample patient-oriented research in future.|$|E
5000|$|... 1.1.Data <b>Entry</b> <b>Validations</b> [...] 1.2.Logical and Mathematical Operations [...] 1.3.Data Formatting [...] 1.4.Internal Data Movements [...] 1.5.Delivering Added Value to Users by Data Configuration ...|$|R
5000|$|Data {{validation}} is {{the application}} of validation rules to the data. For electronic CRFs the validation rules may be applied in real time {{at the point of}} <b>entry.</b> Offline <b>validation</b> may still be required (e.g. for cross checks between data types) ...|$|R
50|$|Oracle Clinical or OC is a {{database}} management system designed by Oracle to provide data management, data <b>entry</b> and data <b>validation</b> functionalities to support Clinical Trial operations.|$|R
5000|$|Many {{organizations}} {{develop their}} own [...] "baseline" [...] security standards and designs detailing basic security control measures for their database systems. These may reflect general information security requirements or obligations imposed by corporate information security policies and applicable laws and regulations (e.g. concerning privacy, financial management and reporting systems), along with generally accepted good database security practices (such as appropriate hardening of the underlying systems) and perhaps security recommendations from the relevant database system and software vendors. The security designs for specific database systems typically specify further security administration and management functions (such as administration and reporting of user access rights, log management and analysis, database replication/synchronization and backups) along with various business-driven information security controls within the database programs and functions (e.g. data <b>entry</b> <b>validation</b> and audit trails). Furthermore, various security-related activities (manual controls) are normally incorporated into the procedures, guidelines etc. relating to the design, development, configuration, use, management and maintenance of databases.|$|E
40|$|Abstract- This work {{describes}} {{a novel approach}} to teach computer organization concepts with extensive hands-on design experience very early in Computer Science curricula. While describing the proposed teaching method, it addresses relevant questions about teaching VLSI design to students in computer science and related fields. The approach involves the analysis, simulation, design and effective construction of processors. It is enabled {{by the use of}} both, VLSI hardware prototyping platforms constructed with reconfigurable hardware and powerful computer aided design tools for design <b>entry,</b> <b>validation</b> and implementation. The approach comprises a 4 -hour a week lecture course on computer organization and a 2 -hour a week laboratory, both taught in the 3 rd semester. In the first two editions of the course, most students have obtained successful processor implementations. In some cases, considerably complex applications such as bubble sort and quick sort procedures were programmed and run in the designed processors...|$|E
40|$|An {{indicator}} {{analysis was}} performed on the fish production chain by assessing the indicators which may be of importance for the detection of emerging risks. The indicators were embedded in a “risk pathway”, in which the relations between different indicators could be illustrated. The risk pathways illustrate the main characteristics of the salmon production chain. However further research is required in order to develop further interaction with other sectors, and production chains to bring emerging risk detection to the next level. In relation to the assigned indicators, a selection of (electronic) data sources were identified {{in order to be able}} to combine data flows with indicators and risk pathways. The indicators were analyzed for availability, sources of data <b>entry,</b> <b>validation</b> of data sources, update frequency and delay in input. The combination of risk pathways, indicators and data sources, will be one of the key information sources for the further development of an Emerging Risk Detection Support System (ERDSS) ...|$|E
30|$|The 2012 ELMPS was {{implemented}} by 39 {{teams in the}} field, each consisting of one supervisor, one reviewer, and four enumerators. Additionally, there were two teams undertaking quality control. All interviewers were trained for 10  days by the technical director and CAPMAS prior to fielding the survey. The fielding of the full 2012 survey took place from March 1, 2012 to June 10, 2012, with more than 90 % of households and individuals surveyed during March and April. Desk review, coding, and data <b>entry</b> and <b>validation</b> at CAPMAS occurred after fielding.|$|R
40|$|Many {{end users}} still take {{advantage}} of the immediacy and simplicity of spreadsheets as a medium for storing simple data. It is not always clear which {{is the best way to}} arrange the data in a spreadsheet. We have identified a number of arrangements that can be used for storing multi-valued data. In particular we look at the ease of data <b>entry</b> and <b>validation,</b> querying the data, and producing summaries and reports for each of the arrangements. Understanding these issues will allow users to decide on the most appropriate arrangement for their multi-valued data...|$|R
50|$|Online, offline {{and mobile}} {{collection}} of aid information - DAD’s customized data <b>entry</b> forms with <b>validation</b> control enable to submit and update aid information though web-based interface, in offline mode which is specifically {{for use in}} areas with limited or no Internet access {{as well as through}} smartphones and tablets.|$|R
40|$|Comprehensive {{geriatric}} assessment (CGA), {{supported by}} e-health, {{was provided to}} 89 frail veterans aged 70 years or older who were admitted to medical, surgical or orthopaedic units at two private hospitals in rural Queensland. Patients were screened and assessed by trained nurses using the interRAI-Acute Care tools. Data <b>entry,</b> <b>validation,</b> reporting and geriatrician comment were facilitated by an online, secure database and reporting system. The process was evaluated based on its utilization and staff satisfaction. The response to the project was generally very positive, {{but there were also}} some problems: (1) lack of staff, staff time and resources to successfully complete the assessments; (2) limited referrals from general practitioners (GPs) and difficulty with developing a sustainable communication and referral system between GPs and the hospital; (3) significant variations in caseloads between the study sites; and (4) the unfamiliarity of sufficient staff at the trial sites with the CGA process. Despite these challenges, the use of e-health-supported strategies in geriatric medicine certainly appears achievable and deserve further study. No Full Tex...|$|E
40|$|Abstract: In the {{analysis}} phase {{of information systems}} development, {{it is important to}} have the conceptual schema validated by the business domain expert, to ensure that the schema accurately models the relevant aspects of the business domain. An effective way to facilitate this validation is to verbalize the schema in language that is both unambiguous and easily understood by the domain expert, who may be non-technical. Such verbalization has long been a major aspect of the Object-Role Modeling (ORM) approach, and basic support for verbalization exists in some ORM tools. Second generation ORM (ORM 2) significantly extends the expressibility of ORM models (e. g. deontic modalities, role value constraints, etc.). This paper discusses the automated support for verbalization of ORM 2 models provided by NORMA (Neumont ORM Architect), an open-source software tool that facilitates <b>entry,</b> <b>validation,</b> and mapping of ORM 2 models. NORMA supports verbalization patterns that go well beyond previous verbalization work. The verbalization for individual elements in the core ORM model is generated using an XSLT transform applied to an XML file that succinctly identifies different verbalization patterns and describes how phrases are combined to produce a readable verbalization. This paper discusses the XML patterns used to describe ORM constraints and the tightly coupled facilities that enable end-users to easily adapt the verbalization phrases to cater for different domain experts and native languages. ...|$|E
40|$|Background and purpose: EpiData and Epi Info {{are often}} used {{together}} by public health agencies around the world, particularly in developing countries, {{to meet their needs}} of low-cost public health data management; however, the current open source data management technology lacks a mobile component {{to meet the needs of}} mobile public health data collectors. The goal of this project is to explore the opportunity of filling this gap through developing and trial of a personal digital assistant (PDA) based data collection/entry system. It evaluated whether such a system could increase efficiency and reduce data transcription errors for public surveillance data collection in developing countries represented by Fiji. Methods: A generic PDA-based data collection software eSTEPS was developed. The software and the data collected using it directly interfaces with EpiData. A field trial was conducted to test the viability of public health surveillance data collection using eSTEPS. The design was a randomised, controlled trial with cross-over design. 120 participants recruited from the Fiji School of Medicine were randomly assigned to be interviewed by one of six interviewers in one of the two ways: (1) paper-based survey followed by PDA survey and (2) PDA survey followed by paper-based survey. Data quality was measured by error rates (logical range errors/inconsistencies, skip errors, missing values, date or time field errors and incorrect data type). Work flow and cost were evaluated in three stages of the survey process: (1) preparation of data collection instrument, (2) data collection and (3) data <b>entry,</b> <b>validation</b> and cleaning. User acceptance was also evaluated in the two groups of participants: (1) data collectors and (2) survey participants. Results: None of the errors presented in 20. 8 % of the paper questionnaires was found in the data set collected using PDA. Sixty two percent of the participants perceived that the PDA-based questionnaire took less time to complete. Data <b>entry,</b> <b>validation</b> and cleaning for the PDA-based data collection from 120 participants took a total of 1. 5 hours, a 93. 26 % reduction of time from 20. 5 hours required using paper and pen. The cost is also significantly reduced with PDA-based protocol. Both data collectors and participants prefer to use PDA instead of paper for data collection. The trial results prove that eSTEPS is a feasible solution for public health surveillance data collection in the field. Several deficiencies of the software were also identified and would be addressed in the next version. Conclusion: eSTEPS offers the potential to meet the need for an effective mobile public health data collection tool for use in the field. The eSTEPS field trial proves that PDA was more efficient than paper for public health survey data collection. It also significantly reduced errors in data entry. The later benefit was derived from the software providing its users with the flexibility of building their own constraints to control the data type, range and logic of data entry...|$|E
30|$|The {{development}} of an online application gives the form designer control over the sequence in which the form must be filled in, {{which means that the}} user can be constrained to provide information according to a required pattern. An online application also provides an opportunity to validate the entered data at the point of <b>entry.</b> Using <b>validation</b> in the sleep tool application means that the physician does not need to question the patient about the data contained in the application, but instead has time to talk about what the information means to the patient, for instance the impact of smoking on sleep disorders.|$|R
40|$|AbstractIn {{order to}} solve {{problems}} existing in household travel survey, such as tedious designing work of survey forms, single survey method, single function, poor pertinence of existing data statistical analysis software, and the difficulty to examine data validation, a survey data statistical analysis software named OD Star had been developed for engineering. The OD Star sets a series functions of independent designing and generating of the questionnaire, data <b>entry,</b> data <b>validation,</b> statistical analysis, graphic charts generation and engineering management. The main improved points of the software lie in highly integrated function, key technologies of statistical analysis, examination of data validation and so on...|$|R
40|$|Abstract. Appropriate {{reasonable}} cutting parameters {{is one of}} {{the important}} factors to improve the product quality and the efficiency. Based on dynamic simulation optimization function of cutting parameter database through the definitions on the acquisition data standardization, the coefficient of cutting force using Dynacut dynamic test and identification experiment, and through the simulation and optimization and final of Opticut Simucut actual cutting parameters of process <b>validation</b> <b>entry</b> in the database...|$|R
40|$|In Germany, the {{national}} routine childhood immunization schedule comprises 12 vaccinations. Primary immunizations {{should be completed}} by 24 mo of age. However, nationwide monitoring of vaccination coverage (VC) is performed only at school entry. We utilized health insurance claims data covering ~ 85 % {{of the total population}} with the objectives to (1) assess VC of all recommended childhood vaccinations in birth-cohorts 2004 – 2009, (2) analyze cross-sectional (at 24 and 36 mo) and longitudinal trends, and (3) validate the method internally and externally. Counting vaccine doses in a retrospective cohort fashion, we assembled individual vaccination histories and summarized VC to nationwide figures. For most long-established vaccinations, VC at 24 mo was at moderate levels (~ 73 – 80 %) and increased slightly across birth-cohorts. One dose measles VC was high (94 %), but low (69 %) for the second dose. VC with a full course of recently introduced varicella, pneumococcal, and meningococcal C vaccines increased across birth-cohorts from below 10 % above 60 %, 70 %, and 80 %, respectively. At 36 mo, VC had increased further by up to 15 percentage points depending on vaccination. Longitudinal analysis suggested a continued VC increase until school <b>entry.</b> <b>Validation</b> of VC figures with primary data showed an overall good agreement. In conclusion, analysis of health insurance claims data allows for the estimation of VC among children in Germany considering completeness and timeliness of vaccination series. This approach provides valid nationwide VC figures for all currently recommended pediatric vaccinations and fills the information gap between early infancy and late assessment at school entry...|$|E
40|$|Background {{and purpose}} EpiData and Epi Info {{are often used}} {{together}} by public health agencies around the world, particularly in developing countries, {{to meet their needs}} of low-cost public health data management; however, the current open source data management technology lacks a mobile component {{to meet the needs of}} mobile public health data collectors. The goal of this project is to explore the opportunity of filling this gap through developing and trial of a personal digital assistant (PDA) based data collection/entry system. It evaluated whether such a system could increase efficiency and reduce data transcription errors for public surveillance data collection in developing countries represented by Fiji. Methods A generic PDA-based data collection software eSTEPS was developed. The software and the data collected using it directly interfaces with EpiData. A field trial was conducted to test the viability of public health surveillance data collection using eSTEPS. The design was a randomised, controlled trial with cross-over design. 120 participants recruited from the Fiji School of Medicine were randomly assigned to be interviewed by one of six interviewers in one of the two ways: (1) paper-based survey followed by PDA survey and (2) PDA survey followed by paper-based survey. Data quality was measured by error rates (logical range errors/inconsistencies, skip errors, missing values, date or time field errors and incorrect data type). Work flow and cost were evaluated in three stages of the survey process: (1) preparation of data collection instrument, (2) data collection and (3) data <b>entry,</b> <b>validation</b> and cleaning. User acceptance was also evaluated in the two groups of participants: (1) data collectors and (2) survey participants. Results None of the errors presented in 20. 8...|$|E
40|$|Objectives: Previously, web-based {{tools for}} {{cross-sectional}} antimicrobial point prevalence surveys (PPSs) {{have been used}} in adults to develop indicators of quality improvement. We aimed to determine the feasibility of developing similar quality indicators of improved antimicrobial prescribing focusing specifically on hospitalized neonates and children worldwide. Methods: A standardized antimicrobial PPS method was employed. Included were all inpatient children and neonates receiving an antimicrobial at 8 : 00 am {{on the day of the}} PPS. Denominators included the total number of inpatients. A web-based application was used for data <b>entry,</b> <b>validation</b> and reporting. We analysed 2012 data from 226 hospitals (H) in 41 countries (C) from Europe (174 H; 24 C), Africa (6 H; 4 C), Asia (25 H; 8 C), Australia (6 H), Latin America (11 H; 3 C) and North America (4 H). Results: Of 17 693 admissions, 6499 (36. 7 %) inpatients received at least one antimicrobial, but this varied considerably between wards and regions. Potential indicators included very high broad-spectrum antibiotic prescribing in children of mainly ceftriaxone (ranked first in Eastern Europe, 31. 3 %; Asia, 13. 0 %; Southern Europe, 9. 8 %), cefepime (ranked third in North America, 7. 8 %) and meropenem (ranked first in Latin America, 13. 1 %). The survey identified worryingly high use of critically important antibiotics for hospital-acquired infections in neonates (34. 9 %; range from 14. 2 % in Africa to 68. 0 % in Latin America) compared with children (28. 3 %; range from 14. 5 % in Africa to 48. 9 % in Latin America). Parenteral administration was very common among children in Asia (88 %), Latin America (81 %) and Europe (67 %). Documentation of the reasons for antibiotic prescribing was lowest in Latin America (52 %). Prolonged surgical prophylaxis rates ranged from 78 % (Europe) to 84 % (Latin America). Conclusions: Simple web-based PPS tools provide a feasible method to identify areas for improvement of antibiotic use, to set benchmarks and to monitor future interventions in hospitalized neonates and children. To our knowledge, this study has derived the first global quality indicators for antibiotic use in hospitalized neonates and children. 0 SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|This is {{a report}} on {{national}} data on Perinatal events in 2013. Information on every birth in the Republic of Ireland is submitted to the National Perinatal Reporting System (NPRS). All births are notified and registered on a standard four part birth notification form (BNF 01) which is completed where the birth takes place. Part 3 of this form {{is sent to the}} HPO for data <b>entry</b> and <b>validation.</b> The information collected includes data on pregnancy outcomes (with particular reference to perinatal mortality and important aspects of perinatal care), as well as descriptive social and biological characteristics of mothers giving birth. The time frame to which the information relates is from 22 weeks gestation to the first week of life...|$|R
40|$|The Protein Circular Dichroism Data Bank (PCDDB) is {{a public}} {{repository}} that archives and freely distributes circular dichroism (CD) and synchrotron radiation CD (SRCD) spectral data and their associated experimental metadata. All <b>entries</b> undergo <b>validation</b> and curation procedures to ensure completeness, consistency {{and quality of the}} data included. A web-based interface enables users to browse and query sample types, sample conditions, experimental parameters and provides spectra in both graphical display format and as downloadable text files. The entries are linked, when appropriate, to primary sequence (UniProt) and structural (PDB) databases, as well as to secondary databases such as the Enzyme Commission functional classification database and the CATH fold classification database, as well as to literature citations. The PCDDB is available at: [URL]...|$|R
40|$|Increasing {{dependability}} {{in collaboration}} work among health professionals will directly improve patient outcomes, and reduce healthcare costs. Our research examines {{the development of}} a shared visual display to facilitate data <b>entry</b> and <b>validation</b> of an electronic record during multidisciplinary team meeting discussion, where specialists discuss patient symptoms, test results, and image findings. The problem of gen- erating an electronic record for patient files that will serve as a record of collaboration, communication and a guide for later tasks is addressed through use of the shared visual display. Shortcomings in user-informed designed, structured data-entry screens became evident when in actual use. Time constraints prompt the synopsis of discussion in acronyms, free text, abbreviations, and the use of inferences. We demonstrate how common ground, team cohesiveness and the use of a shared visual display can improve dependability, but these factors can also provide {{a false sense of security}} and increase vulnerability in the patient management system...|$|R
40|$|Abstract Background In view of {{the fact}} that a {{significant}} proportion of neonates with malaria may be missed on our wards on the assumption that the disease condition is rare, this study aims at documenting the prevalence of malaria in neonates admitted into our neonatal ward. Specifically, we hope to describe its clinical features and outcome of this illness. Knowledge of these may ensure early diagnosis and institution of prompt management. Methods Methods Hospital records of all patients (two hundred and thirty) admitted into the Neonatal ward of Olabisi Onabanjo University Teaching Hospital, Sagamu between 1 st January 1998 and 31 st December 1999 were reviewed. All neonates (fifty-seven) who had a positive blood smear for the malaria parasite were included in the study. Socio-demographic data as well as clinical correlates of each of the patients were reviewed. The Epi-Info 6 statistical software was used for data <b>entry,</b> <b>validation</b> and analysis. A frequency distribution was generated for categorical variables. To test for an association between categorical variables, the chi-square test was used. The level of significance was put at values less than 5 %. Results Prevalence of neonatal malaria in this study was 24. 8 % and 17. 4 % for congenital malaria. While the mean duration of illness was 3. 60 days, it varied from 5. 14 days in those that died and and 3. 55 in those that survived respectively. The duration of illness significantly affected the outcome (p value = 0. 03). Fever alone was the clinical presentation in 44 (77. 4 %) of the patients. Maturity of the baby, sex and age did not significantly affect infestation. However, history of malaria/febrile illness within the 2 weeks preceding the delivery was present in 61. 2 % of the mothers. Maternal age, concurrent infection and duration of illness all significantly affected the outcome of illness. Forty-two (73. 7 %) of the babies were discharged home in satisfactory condition. Conclusion It was concluded that taking a blood smear to check for the presence of the malaria parasite should be included as part of routine workup for all neonates with fever or those whose mothers have history of fever two weeks prior to delivery. In addition, health education of pregnant mothers in the antenatal clinic should include early care-seeking for newborns. </p...|$|E
40|$|Presented {{here is an}} {{architecture}} for implementing real-time telemetry based diagnostic systems using model-based reasoning. First, we describe Paragon, a knowledge acquisition tool for offline <b>entry</b> and <b>validation</b> of physical system models. Paragon provides domain experts with a structured editing capability to capture the physical component's structure, behavior, and causal relationships. We next describe {{the architecture of the}} run time diagnostic system. The diagnostic system, written entirely in Ada, uses the behavioral model developed offline by Paragon to simulate expected component states as reflected in the telemetry stream. The diagnostic algorithm traces causal relationships contained within the model to isolate system faults. Since the diagnostic process relies exclusively on the behavioral model and is implemented without the use of heuristic rules, {{it can be used to}} isolate unpredicted faults in a wide variety of systems. Finally, we discuss the implementation of a prototype system constructed using this technique for diagnosing faults in a science instrument. The prototype demonstrates the use of model-based reasoning to develop maintainable systems with greater diagnostic capabilities at a lower cost...|$|R
40|$|Error {{messages}} may {{be displayed}} during any activity that may update the NWIS database. There are two sources of error messages: GWSI software may issue errors {{based on data}} validation using reference list checks and logical checks of entered data during Screen Entry, Query Entry, Edit, and Update. Status errors may be returned by the RETR routines that, in most cases, indicate database problems. These generally occur during the Update procedure. The following {{is a list of}} the error messages that may occur during data entry using Screen Entry, Query <b>Entry,</b> Data <b>Validation,</b> and the Edit and Update procedures. Negative numbered error messages represent non-fatal warning messages. Error messages are displayed in specific formats, depending on the activity being performed, and convey information about the error beyond the textual description: (1) Screen and Query Entry, and Sitefile/GWSI Data validation (gwcheck) programs display error message surrounded by either dashes or asterisks to indicate whether the reported error is a warning (dashes) or fatal (asterisks), for example: Warning error message...|$|R
40|$|Visual Basic is {{a popular}} {{programming}} package because of its user-friendly interface and data access features. Two popular data access mechanisms are data bound controls with data access objects, and remote data controls with remote data objects. The main difference between these methods are that data bound controls are usually used to connect to desktop databases, and remote data objects are used to connect to client/server data (for client database manipulation of a remote database). Using Microsoft's FlexGrid control (MSFLXGRD. OCX) you can create utilities to display, filter, edit, validate and update your data. For example, such utilities could include: 1. data <b>entry</b> & <b>validation</b> 2. high level reports 3. ported spreadsheet macro applications retaining cell layout & format Also, users do not require any addition software or tools for it to connect it accurately {{in order to provide}} authentication. This technique is helpful for connect with database programming system and it save the extra time which is wasted for connect with different Sheets i. e. Excel Sheet and other Style Sheets and we connect the database with this programming tool and access the data...|$|R
40|$|Background & objectives: Unmet {{need for}} {{contraception}} remains a national problem. The {{study was conducted}} in an urban area of Puducherry, India, among the eligible couples to assess the unmet need for contraception and to determine the awareness and pattern of use of contraceptives along with the socio-demographic factors associated with the unmet needs for contraception. Methods: This cross-sectional study included eligible couples with married women in age group of 15 - 45 yr as the study population (n= 267). Probability proportional to size sampling followed by systematic random sampling was used. A pre-tested questionnaire was administered to collect data from the respondents. Double data <b>entry</b> and <b>validation</b> of data was done. Results: Unmet need for contraception was 27. 3 per cent (95 % CI: 22. 3 - 33); unmet need for spacing and limiting was 4. 9 and 22. 5 per cent, respectively. Among those with unmet need (n= 73), 50 per cent reported client related factors (lack of knowledge, shyness, etc.); and 37 per cent reported contraception related factors (availability, accessibility, affordability, side effects) as a cause for unmet need. Interpretation & conclusions: Our study showed a high unmet need for contraception in the study area indicating towards a necessity to address user perspective to meet the contraception needs...|$|R
40|$|This paper {{describes}} OWL ontology re-engineering {{from the}} wiki-based social science codebook (thesaurus) {{developed by the}} Seshat: Global History Databank. The ontology describes human history {{as a set of}} over 1500 time series variables and supports variable uncertainty, temporal scoping, annotations and bibliographic references. The ontology was developed to transition from traditional social science data collection and storage techniques to an RDF-based approach. RDF supports automated generation of high usability data <b>entry</b> and <b>validation</b> tools, data quality management, incorporation of facts from the web of data and management of the data curation lifecycle. This ontology re-engineering exercise identified several pitfalls in modelling social science codebooks with semantic web technologies; provided insights into the practical application of OWL to complex, real-world modelling challenges; and has enabled the construction of new, RDF-based tools to support the large-scale Seshat data curation effort. The Seshat ontology is an exemplar of a set of ontology design patterns for modelling uncertainty or temporal bounds in standard RDF. Thus the paper provides guidance for deploying RDF in the social sciences. Within Seshat, OWL-based data quality management will assure the data is suitable for statistical analysis. Publication of Seshat as high-quality, linked open data will enable other researchers to build on it...|$|R
40|$|The INTERGROWTH- 21 (st) Project data {{management}} was structured incorporating both a centralised and decentralised {{system for the}} eight study centres, which all used the same database and standardised data collection instruments, manuals and processes. Each centre {{was responsible for the}} <b>entry</b> and <b>validation</b> of their country-specific data, which were entered onto a centralised system maintained by the Data Coordinating Unit in Oxford. A comprehensive {{data management}} system was designed to handle the very large volumes of data. It contained internal validations to prevent incorrect and inconsistent values being captured, and allowed online data entry by local Data Management Units, as well as real-time management of recruitment and data collection by the Data Coordinating Unit in Oxford. To maintain data integrity, only the Data Coordinating Unit in Oxford had access to all the eight centres' data, which were continually monitored. All queries identified were raised with the relevant local data manager for verification and correction, if necessary. The system automatically logged an audit trail of all updates to the database with the date and name of the person who made the changes. These rigorous processes ensured that the data collected in the INTERGROWTH- 21 (st) Project were of exceptionally high quality...|$|R
40|$|The INTERGROWTH- 21 st Project data {{management}} was structured incorporating both a centralised and decentralised {{system for the}} eight study centres, which all used the same database and standardised data collection instruments, manuals and processes. Each centre {{was responsible for the}} <b>entry</b> and <b>validation</b> of their country-specific data, which were entered onto a centralised system maintained by the Data Coordinating Unit in Oxford. A comprehensive {{data management}} system was designed to handle the very large volumes of data. It contained internal validations to prevent incorrect and inconsistent values being captured, and allowed online data entry by local Data Management Units, as well as real-time management of recruitment and data collection by the Data Coordinating Unit in Oxford. To maintain data integrity, only the Data Coordinating Unit in Oxford had access to all the eight centres' data, which were continually monitored. All queries identified were raised with the relevant local data manager for verification and correction, if necessary. The system automatically logged an audit trail of all updates to the database with the date and name of the person who made the changes. These rigorous processes ensured that the data collected in the INTERGROWTH- 21 st Project were of exceptionally high quality. © 2013 Royal College of Obstetricians and Gynaecologists...|$|R
40|$|Clinical {{research}} often generates vast {{arrays of}} complex, interrelated demographic, patient exam, and laboratory measurement data. Researchers have long contended with the costly, fatiguing and inefficient processes of entering this {{data on a}} database and verifying its accuracy prior to report generation and statistical analysis. We have examined several new systems in which data entry forms are scanned, interpreted through Optical Character Recognition technology, and the research data automatically stored on a database. The Berkeley Contact Lens Extended Wear Study (CLEWS), a large, randomized controlled clinical trial conducted at the University of California at Berkeley School of Optometry, {{was used as a}} testing ground for an automated data <b>entry</b> and <b>validation</b> system based on the Teleform software and an Epson scanner. The system allowed design of data entry forms that satisfied the needs of our clinicians, biostatisticians, and administrative staff. The system drastically reduced the time required to enter patient exam, demographic, and laboratory measurement data onto the study database, and provided tools for verifying that the data were scanned accurately. The system improved both the quality of patient care and the integrity of clinical patient data, allowing clinicians to quickly and easily retrieve patient records, and permitted our biostatisticians to generate periodic recruitment monitoring, patient safety, protocol adherence, and data quality assurance reports in a timely fashion...|$|R
40|$|Data {{management}} has {{significant impact on}} the quality control of clinical studies. Every clinical study should have a data management plan to provide overall work instructions and ensure that all of these tasks are completed according to the Good Clinical Data Management Practice (GCDMP). Meanwhile, the data management plan (DMP) is an auditable document requested by regulatory inspectors and must be written {{in a manner that is}} realistic and of high quality. The significance of DMP, the minimum standards and the best practices provided by GCDMP, the main contents of DMP based on electronic data capture (EDC) and some key factors of DMP influencing the quality of clinical study were elaborated in this paper. Specifically, DMP generally consists of 15 parts, namely, the approval page, the protocol summary, role and training, timelines, database design, creation, maintenance and security, data <b>entry,</b> data <b>validation,</b> quality control and quality assurance, the management of external data, serious adverse event data reconciliation, coding, database lock, data management reports, the communication plan and the abbreviated terms. Among them, the following three parts are regarded as the key factors: designing a standardized database of the clinical study, entering data in time and cleansing data efficiently. In the last part of this article, the authors also analyzed the problems in clinical research of traditional Chinese medicine using the EDC system and put forward some suggestions for improvement...|$|R
40|$|The Mars Pathfinder {{heatshield}} contained several thermocouples {{and resistance}} thermometers. A {{description of the}} experiment, the entry data, {{and analysis of the}} entry environment and material response is presented. In particular, the analysis addresses uncertainties of the data and the fluid dynamics and material response models. The calculations use the latest trajectory and atmosphere reconstructions for the Pathfinder entry. A modified version of the GIANTS code is used for CFD (computational fluid dynamics) analyses, and FIAT is used for material response. The material response and flowfield are coupled appropriately. Three different material response models are considered. The analysis of Pathfinder <b>entry</b> data for <b>validation</b> of aerothermal heating and material response models is complicated by model uncertainties and unanticipated data-acquisition and processing problems. We will discuss these issues as well as ramifications of the data and analysis for future Mars missions...|$|R
40|$|A check {{calibration}} of the 10 - by 10 -Foot Supersonic Wind Tunnel (SWT) {{was conducted}} in May/June 2014 using an array of five supersonic wedge probes to verify the 1999 Calibration. This check calibration was necessary following a control systems upgrade and an integrated systems test (IST). This check calibration was required to verify the tunnel flow quality was unchanged by the control systems upgrade prior to the next test customer beginning their test entry. The previous check calibration of the tunnel occurred in 2007, prior to the Mars Science Laboratory test program. Secondary objectives of this test <b>entry</b> included the <b>validation</b> of the new Cobra data acquisition system (DAS) against the current Escort DAS {{and the creation of}} statistical process control (SPC) charts through the collection of series of repeated test points at certain predetermined tunnel parameters. The SPC charts secondary objective was not completed due to schedule constraints. It is hoped that this effort will be readdressed and completed in the near future...|$|R
