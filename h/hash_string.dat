22|54|Public
5000|$|The recipient's {{computer}} inserts the <b>hash</b> <b>string</b> into a database. If {{the string}} {{is already in}} the database (indicating that an attempt is being made to re-use the <b>hash</b> <b>string),</b> it is invalid.|$|E
50|$|If the <b>hash</b> <b>string</b> passes all {{of these}} tests, it is {{considered}} a valid <b>hash</b> <b>string.</b> All of these tests take far less time and disk space than receiving the body content of the e-mail.|$|E
5000|$|The recipient's {{computer}} checks {{whether the}} e-mail address in the <b>hash</b> <b>string</b> matches any of the valid e-mail addresses registered by the recipient, or matches any of the mailing lists to which the recipient is subscribed. If a match is not found, the <b>hash</b> <b>string</b> is invalid.|$|E
5000|$|<b>Hash</b> input <b>strings</b> str1 and str2 {{separately}} using MostFreqKHashing {{and output}} hstr1 and hstr2 respectively ...|$|R
40|$|Copyright © 2015 ISSR Journals. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT: This paper presents the systematic way of <b>hashing</b> <b>string</b> values using NFO and NOF collision resolution strategies. NFO and NOF are techniques used for hashing numeric keys. The same principles and techniques for hashing numeric keys are deployed in the <b>hashing</b> of <b>string</b> values but with slight modifications in the hashing process and implementations. These variants followed the standard ways of evaluating and implementing algorithms to resolve collisions in hash tables. They are very effective in resolving the problem of collisions of string keys or values in the same slot of a hash table...|$|R
50|$|Other {{universal}} {{families of}} hash functions used to <b>hash</b> unknown-length <b>strings</b> to fixed-length <b>hash</b> values include the Rabin fingerprint and the Buzhash.|$|R
5000|$|The bcrypt {{function}} is the default password hash algorithm for OpenBSD and other systems including some Linux distributions such as SUSE Linux.The prefix [...] "$2a$" [...] or [...] "$2b$" [...] (or [...] "$2y$") in a <b>hash</b> <b>string</b> in a shadow password file indicates that <b>hash</b> <b>string</b> is a bcrypt hash in modular crypt format.The {{rest of the}} <b>hash</b> <b>string</b> includes the cost parameter, a 128-bit salt (base-64 encoded as 22 characters), and 184 bits of the resulting hash value (base-64 encoded as 31 characters).The cost parameter specifies a key expansion iteration count as a power of two, which is an input to the crypt algorithm.|$|E
50|$|Another (lesser) {{benefit of}} a salt is as follows: two users might choose the same string as their {{password}}, or the same user might choose {{to use the same}} password on two machines. Without a salt, this password would be stored as the same <b>hash</b> <b>string</b> in the password file. This would disclose the fact that the two accounts have the same password, allowing anyone who knows one of the account's passwords to access the other account. By salting the passwords with two random characters, even if two accounts use the same password, no one can discover this just by reading hashes.|$|E
30|$|A {{possible}} attacker aims at maliciously tampering {{the modified}} image {{in a way}} that the <b>hash</b> <b>string</b> becomes similar or even identical to the <b>hash</b> <b>string</b> of the original image while preserving the visual content (this is the attacked image). In this way, the attacked image would be rated as being authentic by the hashing algorithm.|$|E
40|$|<b>String</b> <b>hashing</b> is a {{fundamental}} operation, used in countless applications where fast access to distinct strings is required. In this paper we describe a class of <b>string</b> <b>hashing</b> functions and explore its performance. In particular, using experiments with both small sets of keys and a large key set from a text database, we show {{that it is possible}} to achieve performance close to that theoretically predicted for hashing functions. We also consider criteria for choosing a hashing function and use them to compare our class of functions to other methods for <b>string</b> <b>hashing.</b> These results show that our class of hashing functions is reliable and efficient, and is therefore an appropriate choice for general-purpose <b>hashing.</b> 1 Introduction <b>String</b> <b>hashing</b> is the process of reducing a string to a pseudo-random number in a specified range. It is {{a fundamental}} operation, used widely in applications where speed is critical. On a small scale, a hash table is often the basic data structure in applicat [...] ...|$|R
5000|$|One {{can apply}} vector hashing to blocks. For instance, one applies vector hashing to each 16-word block of the string, and applies <b>string</b> <b>hashing</b> to the [...] results. Since the slower <b>string</b> <b>hashing</b> is applied on a {{substantially}} smaller vector, this will essentially be {{as fast as}} vector hashing.|$|R
30|$|The {{modification}} {{performed on}} the Plane image is rich in contrast and affects a considerable area in the image. This modification is clearly detected for all keys assuming a detection threshold of 0.15 or lower as displayed by the middle histogram. The modification of the Goldhill image also affects {{a considerable number of}} pixels, but the contrast in this area is not changed that much. Therefore, the detection threshold had been set to 0.04 to detect the modification for all filters (which in turn negatively influences robustness of course). Finally, the modification done to Lena image affects only few pixels and hardly changes the contrast in the areas modified. Consequently, for some filter parameters, the modification is not detected at all (i.e., the Hamming distance between the <b>hash</b> <b>strings</b> is 0). Similar to the key-independent JPEG 2000 PBHash, sensitivity can be controlled by setting the hash length accordingly. In the key-dependent scheme, the variations among different filters need to be considered additionally which means that longer <b>hash</b> <b>strings</b> as compared to the key-independent scheme should be used to guarantee sufficient sensitivity for all filters. Overall, employing the key-dependent hashing scheme with different filters on the same image (see Figures 10 – 12) results in larger Hamming distances as compared to using it with the same filters on an original and a slightly modified image (Figure 16).|$|R
40|$|We {{introduce}} a novel way to authenticate an image using Low Density Parity Check (LDPC) and Secure Hash Algorithm (SHA) based iris recognition method with reversible watermarking scheme, {{which is based}} on Integer Wavelet Transform (IWT) and threshold embedding technique. The parity checks and parity matrix of LDPC encoding and cancellable biometrics i. e., <b>hash</b> <b>string</b> of unique iris code from SHA- 512 are embedded into an image for authentication purpose using reversible watermarking scheme based on IWT and threshold embedding technique. Simply by reversing the embedding process, the original image, parity checks, parity matrix and SHA- 512 hash are extracted back from watermarked-image. For authentication, the new <b>hash</b> <b>string</b> produced by employing SHA- 512 on error corrected iris code from live person is compared with <b>hash</b> <b>string</b> extracted from watermarked-image. The LDPC code reduces the hamming distance for genuine comparisons by a larger amount than for the impostor comparisons. This results in better separation between genuine and impostor users which improves the authentication performance. Security of this scheme is very high due to the security complexity of SHA- 512, which is 2256 under birthday attack. Experimental results show that this approach can assure more accurate authentication with a low false rejection or false acceptance rate and outperforms the prior arts in terms of PSNR...|$|E
40|$|We use Low Density Parity Check (LDPC) error {{correction}} code to solve fuzziness i. e., the variability and noise in iris code generated from Iris Recognition System (IRS) and Secure Hash Algorithm (SHA- 512) to transform unique iris code into <b>hash</b> <b>string</b> to make them as a Cancellable Biometric. SHA- 512 <b>hash</b> <b>string</b> {{is used as a}} key for 512 bits Advanced Encryption Standard (AES) encryption process. In the decryption process, Hash comparison is made between new hash generated by SHA- 512 from error corrected iris code and hash stored in smart card. If authentic, 512 bits AES decryption is accomplished. Experiments indicate that, use of LDPC code results in better separation between genuine and impostor users which improves the performance of this novel cryptosystem. Security of this system is very high, which is 2 256 under birthday attack. AES algorithm is enhanced by using 512 bits key and increasing the number of rounds...|$|E
30|$|Using a key-dependent hashing scheme, the {{advantage}} of the JPEG 2000 PBHash to generate hash strings from already JPEG 2000 -encoded visual data by simple parsing and concatenation is lost. An image present as JPEG 2000 file needs to be JPEG 2000 -decoded (with the standard filters) into raw pixel data and reencoded into the key-dependent JPEG 2000 domain (with the key-dependent filters) for generating the corresponding <b>hash</b> <b>string.</b>|$|E
40|$|Motivation: At present, {{mapping of}} {{sequence}} identifiers across data-bases is a daunting, time-consuming and computationally expensive process, usually achieved by sequence similarity searches with strict threshold values. Summary: We present a rapid and efficient method to map sequence identifiers across databases. The method uses the MD 5 checksum algorithm for message integrity to generate sequence fingerprints and uses these fingerprints as <b>hash</b> <b>strings</b> to map sequences across databases. The program, called MagicMatch, {{is able to}} cross-link any of the major sequence databases within a few seconds on a modest desktop computer. Availability: MagicMatch {{is available at the}} followin...|$|R
50|$|Bernstein is {{also known}} for his popular <b>string</b> <b>hashing</b> {{function}} djb2.|$|R
40|$|We {{present a}} new {{technique}} for generating biometric fingerprint hashes, or summaries of information contained in human fingerprints. Our method calculates and aggregates various key-determined metrics over fingerprint images, producing short <b>hash</b> <b>strings</b> that cannot be used to reconstruct the source fingerprints without knowledge of the key. This {{can be considered a}} randomized form of the Radon transform, where a custom metric replaces the standard linebased metric. Resistant to minor distortions and noise, the resulting fingerprint hashes are useful for secure biometric authentication, either augmenting or replacing traditional password hashes. This approach can help increase the security and usability of Web services and other client-server systems...|$|R
30|$|Unless noted otherwise, we use JPEG 2000 with layer {{progression}} order, output bitrate set to 1 [*]bit per pixel, and wavelet {{decomposition level}} 5 {{to generate the}} <b>hash</b> <b>string.</b> The length of the hash and the wavelet decomposition depth employed {{can be used as}} parameters to control the tradeoff between robustness and sensitivity of the hashing scheme [14]—obviously a shorter hash leads to increased robustness and decreased sensitivity (see [17, 18] for detailed results). A shallow decomposition depth is not at all suited for the JPEG 2000 PBHash application since settings of this type lead to a large LL subband. For a large LL band, the hash only consists of coefficient data of the LL band corresponding to {{the upper part of the}} image (due to the size of the subband and the raster-scan order used in the bitstream assembly stage). Therefore, a certain minimal decomposition depth (e.g., down to decomposition level 3) is a must and a short <b>hash</b> <b>string</b> requires a higher decomposition depth for sensible employment of the JPEG 2000 PBHash in order to avoid the phenomenon described before.|$|E
30|$|Hash {{function}} {{serves as}} an effective tool for message authentication in cryptographic applications. The hash function [such as a message digest- 5 (MD- 5) and a secure hash algorithm- 1 (SHA- 1)] defines a mapping from an arbitrary-length message to a short digest (i.e. <b>hash</b> <b>string)</b> (Stamp 2006). The prerequisite condition to accomplish the cryptographic authentication using a hashing is the avalanche effect, in which a single bit modification on the message {{may lead to the}} significant changes in the <b>hash</b> <b>string.</b> However, the over-sensitivity of the cryptographic hash function limits its applications in the multimedia domain. In the multimedia applications, the hash function should be robust against the content-preserving operations, such as the geometric transformation, format conversion, and an image compression, etc. In other words, the hash of original image and its perceptually similar version should be approximately same. Hash should be considerably different only when the visual content is distorted by the malevolent process such as object deletion/insertion operations etc. One more important property is the discriminative capability, i.e. images with dissimilar content should provide different hashes.|$|E
30|$|The histograms {{shown in}} Figures 26 and 27 show that again the attack can be {{prevented}} reliably. Most Hamming distances between the attacked image and the original image are > 0.2 and actually all are > 0.1 The <b>hash</b> <b>string</b> of the attacked image does no longer exhibit {{a high degree of}} similarity to the original image in the authentication. The same is of course true with respect to the original version of the attacked image (histograms look similar but are not shown).|$|E
2500|$|Alice {{immediately}} computes a cryptographic <b>hash</b> of the <b>string</b> [...] "tail ljngjkrjgnfdudiudd gfdgdfjkherfsfsd", {{which is}} 59dea408d43183a3937957e71a4bcacc616d9cbc and sends it to Bob ...|$|R
30|$|The Location Manager also stores {{a policy}} digest that states, in {{summarized}} form, the entities (and entity namespaces) that are mentioned in all active policies (an entity or set appears {{only once in}} the digest regardless {{of the number of}} occurrences in policies); for that purpose, Jano uses a bloom filter [6] storing <b>hashes</b> of <b>strings</b> (entity names or namespaces).|$|R
40|$|This paper {{presents}} a new Arabic sign language recognition using K-nearest Neighbor algorithm. The algorithm {{is designed to}} work as a first level detection upon a series of steps to bring the captured character images into actual spelling. The algorithm acts in a high performance execution which is exactly needed for such type of systems. K-Nearest Neighbor Algorithm and feature extraction are the guidelines of the recognition system, because hand gestures is treated as a block of curves needed to be extracted in the best fit with a predefined character set in the knowledge base. The specific image preprocessing to form a new idea of histogram and a histogram transition table is formed as a <b>hashed</b> <b>string</b> of transformation of block histogram sequence using K-Nearest Neighbor Algorithm. Preparing the knowledge base as a sequence of characters for one time and will and fast easily compared to detecting the character input...|$|R
30|$|We {{discuss a}} robust image {{authentication}} {{scheme based on}} a <b>hash</b> <b>string</b> constructed from leading JPEG 2000 packet data. Motivated by attacks against the approach, key-dependency is added by means of employing a parameterized lifting scheme in the wavelet decomposition stage. Attacks can be prevented effectively in this manner and {{the security of the}} scheme in terms of unicity distance is assumed to be high. Key-dependency however can lead to reduced sensitivity of the scheme. This effect has to be compensated by an increase of the hash length which in turn decreases robustness.|$|E
30|$|To summarize, we may {{conclude}} that the key-dependency introduced into the JPEG 2000 PBHash has undesired effects on sensitivity and robustness. Caused by the varying sensitivity for different filters used in the hashing scheme, {{the length of the}} <b>hash</b> <b>string</b> has to be increased as compared to the key-independent scheme to detect even small modifications reliably. For this setting, compression robustness is already hard to achieve for all filters. So, in a way, adding key-dependency to the scheme has to be paid with an aggravation of the tradeoff between sensitivity and robustness of the scheme caused by the varying respective properties of the filters used.|$|E
30|$|It can be {{observed}} that the distributions of the Hamming distances are centered around 0.5 as desired. The variance of the distribution is larger for the more robust settings, which is also to be expected. The influence of the wavelet decomposition level may not be immediately derived from these results but it is known from earlier experiments [18] {{that there is a}} trend to result in higher robustness for a lower decomposition level value (please refer also to the results in Section 3.2 on this issue). The reason is obvious—low-decomposition depth causes the <b>hash</b> <b>string</b> to be mainly consisting of low frequency coefficient data while differences caused by subtle image modifications are found in higher frequency coefficient data.|$|E
50|$|The Rabin-Karp {{algorithm}} {{is a relatively}} fast string searching algorithm that works in O(n) time on average. It {{is based on the}} use of <b>hashing</b> to compare <b>strings.</b>|$|R
40|$|We {{present a}} text-independent privacy-preserving speaker {{verification}} system that functions similar to conventional password-based authentication. Our privacy constraints {{require that the}} system does not observe the speech input provided by the user, as this {{can be used by}} an adversary to impersonate the user in the same system or elsewhere. We represent the speech input using supervectors and apply locality sensitive hashing (LSH) to transform these into bit strings, where two supervectors, and therefore inputs, are likely to be similar if they map to the same string. This transformation, therefore, reduces the problem of identifying nearest neighbors to string comparison. The users then apply a cryptographic hash function to the strings obtained from their enrollment and verification data, thereby obfuscating it from the server, who can only check if two <b>hashed</b> <b>strings</b> match without being able to reconstruct their content. We present execution time and accuracy experiments with the system on the YOHO dataset, and observe that the system achieves acceptable accuracy with minimal computational overhead needed to satisfy the privacy constraints. Index Terms — Privacy, Speaker Verification, LSH 1...|$|R
5000|$|Processing phaseIn this phase, {{the data}} is hashed to a 64-bit string. A core {{function}} [...] : [...] is used in this processing phase, that <b>hashes</b> a 128-bit <b>string</b> [...] to a 64-bit string [...] as follows: ...|$|R
40|$|Audio Fingerprints (AFP’s) are compact, {{content-based}} {{representations of}} audio signals {{used to measure}} distances among them. An AFP has to be small, fast computed and robust to signal degradations. In this paper an entropy based AFP is presented that performed very well when the signal was corrupted with lossy compression, scaling and even 1 KHz Low-pass filtering in the experiments. The AFP is determined by computing the instantaneous amount of information of the audio signal in two-second frames with fifty percent overlapping, the resulting entropy signal is binary coded in order to compare different interpretations (e. g. live vs. studio recording) of the same song with good results. The AFP’s robustness is {{compared with that of}} Haitsma-Kalker’s <b>Hash</b> <b>string</b> based AFP with encouraging results. 1...|$|E
30|$|Sensitivity is the {{property}} of a hashing scheme to detect image alterations—for the JPEG 2000 PBHash, high sensitivity means that a low number of packet body bytes are required to detect image manipulations. Robustness {{on the other hand}} is {{the property}} of a hashing scheme to maintain an identical <b>hash</b> <b>string</b> even under common image processing manipulations like compression—for the JPEG 2000 PBHash, high robustness means that a high number of packet body bytes are required to detect such types of manipulations. While sensitivity against intentional image modifications and robustness with respect to image compression has been discussed in detail for the key-independent JPEG 2000 PBHash in previous work [17, 18], the impact of the different filters used in the key-dependency scheme on these properties of the hashing scheme is not clear yet. Therefore, we conduct several experiments on these issues.|$|E
40|$|In recent years, {{the attack}} which leverages {{register}} information (e. g. accounts and passwords) leaked from 3 rd party applications to try other applications is popular and serious. We call this attack "database collision". Traditionally, {{people have to}} keep dozens of accounts and passwords for different applications to prevent this attack. In this paper, we propose a novel encryption scheme for hiding users' register information and preventing this attack. Specifically, we first hash the register information using existing safe hash function. Then the <b>hash</b> <b>string</b> is hidden, instead a coefficient vector is stored for verification. Coefficient vectors of the same register information are generated randomly for different applications. Hence, the original information is hardly cracked by dictionary based attack or database collision in practice. Using our encryption scheme, each user only needs to keep one password for dozens of applications...|$|E
30|$|<b>Hashing</b> takes a <b>string</b> of {{text and}} turns it into a shorter value with a fixed length and mixes these keys up into irreversibly random strings of unique {{characters}} or “hash”. This obscures data so the device isn’t storing any of it in its original form.|$|R
30|$|Find {{and remove}} some {{disadvantages}} {{in previous research}} in design and implementation key-value store such as SILT [20] and FAWN-DS [8]. disadvantages from FAWN-DS and SILT: hash keys using SHA and use <b>hash</b> values as <b>string</b> keys in key-value store. It is difficult to iterate the store.|$|R
5000|$|Every class may in {{addition}} have one function aliased to [...] "", the [...] "bracket" [...] operator, allowing the notation [...] as {{a synonym for}} [...] where [...] is the chosen function. This is particularly useful for container structures such as arrays, hash tables, lists etc. For example access to an element of a <b>hash</b> table with <b>string</b> keys can be written ...|$|R
