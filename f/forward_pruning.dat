23|6|Public
5000|$|Searching: Alpha-Beta Nega-Max Principal Variation search, Iterative Deepening, Null-move <b>Forward</b> <b>Pruning,</b> Static Exchange Evaluation (SEE).|$|E
50|$|This {{would enable}} them to look further ahead ('deeper') at the most {{significant}} lines in a reasonable time. The test of time has borne out the first approach; all modern programs employ a terminal quiescence search before evaluating positions. The second approach (now called <b>forward</b> <b>pruning)</b> has been dropped in favor of search extensions.|$|E
50|$|Hydra ran on a 32-node Intel Xeon with a Xilinx FPGA {{accelerator}} card cluster, {{with a total}} of 64 gigabytes of RAM. It evaluates about 150,000,000 chess positions per second, roughly the same as the 1997 Deep Blue which defeated Garry Kasparov, but with several times more overall computing power. Whilst FPGAs generally have a lower performance level than ASIC chips, modern-day FPGAs run about as fast as the older ASICs used for Deep Blue. The engine is on average able to evaluate up to a depth of about 18 ply (nine moves by each player), whereas Deep Blue only evaluated to about 12 ply on average. Hydra's search used alpha-beta pruning as well as null-move heuristics http://tournament.hydrachess.com/faq.php. The extra search depth over Deep Blue is due to its use of more modern type B <b>forward</b> <b>pruning</b> techniques that can miss some possibilities but generally play better due to the greater search depth permitted.|$|E
40|$|Abstract — This article {{describes}} a new, game-independent forward-pruning technique for EXPECTIMAX, called CHAN-CEPROBCUT. It {{is the first}} technique to <b>forward</b> <b>prune</b> in chance nodes. Based on the strong correlation between evaluations obtained from searches at different depths, the technique prunes chance events if {{the result of the}} chance node is likely to fall outside the search window. In this article, CHANCEPROBCUT is tested in two games, i. e., Stratego and Dice. Experiments reveal that the technique is able to reduce the search tree significantly without a loss of move quality. Moreover, in both games there is also an increase of playing performance. I...|$|R
40|$|GoTools is {{the name}} of a {{computer}} program to analyse life & death fights in the game of go. First, a motivation for programing go is given. General aspects of the heuristic search performed in GoTools are discussed. The absolute strength and the effect of <b>forward</b> tree <b>pruning</b> are tested in a competition between GoTools and dan level human players. It is found that with a speed up of a factor of 30 the program still finds for 70 % of the problems of a test set the correct first move. A suggestion is made to compare intelligence between contestants (in solving fully enclosed life/death problems) by comparing their relative increase in the time to solve problems with the increase of the difficulty of the problems. The rules of go are explained in the shortest possible form in the appendix. Introduction The game of go is a 4500 yr. old Asian board game in which two persons put stones on a board to gain territory and take prisoners. Despite having very few, simple rules and the substantial [...] ...|$|R
40|$|In this paper, {{we address}} the problem of signal pruning in static timing {{analysis}} (STA). Traditionally, signals are propagated through the circuit and are pruned, such that only the signal with the latest arrival time at each node is propagated <b>forward.</b> This signal <b>pruning</b> is a key to the linear run time of STA. However, it was previously observed that a signal with the latest arrival time {{may not be the most}} critical signal, as an earlier signal with a larger transition time can result in a longer delay in the down-stream logic. Hence, arrival time based pruning can result in an optimistic delay, incorrect critical paths, and discontinuities of the delay during circuit optimization. Although algorithms were proposed to remedy this issue, they rely on propagation of multiple signals and have an exponential worst-case complexity. In this paper, we propose a new timing analysis algorithm, which uses a two pass traversa...|$|R
40|$|Several early game-playing {{computer}} programs used <b>forward</b> <b>pruning</b> (i. e., {{the practice of}} deliberately ignoring nodes that are believed unlikely to a ect a game tree's minimax value), but this technique {{did not seem to}} result in good decision-making. The poor performance of <b>forward</b> <b>pruning</b> presents a major puzzle for AI research ongameplaying, because some version of <b>forward</b> <b>pruning</b> seems to be people do," and the best chess-playing programs still do not play as well as the best humans. As a step toward deeper understanding of <b>forward</b> <b>pruning,</b> we have set up models of <b>forward</b> <b>pruning</b> on two di erent kinds of game trees, and used these models to investigate how <b>forward</b> <b>pruning</b> a ects the probability ofchoosing the correct move. In our studies, <b>forward</b> <b>pruning</b> did better than minimaxing when there was a high correlation among the minimax values of sibling nodes in a game tree. This result suggests that <b>forward</b> <b>pruning</b> may possibly be a useful decision-making technique in certain kinds of games. In particular, we believe that bridge may be such a game...|$|E
40|$|<b>Forward</b> <b>pruning,</b> {{also known}} as {{selective}} search, is now employed in many strong game-playing programs. In this paper, we introduce RankCut – a domain independent <b>forward</b> <b>pruning</b> technique which makes use of move ordering, and prunes once no better move {{is likely to be}} available. Since gameplaying programs already perform move ordering to improve the performance of αβ search, this information is available at no extra cost. As RankCut uses additional information untapped by current <b>forward</b> <b>pruning</b> techniques, RankCut is a complementary <b>forward</b> <b>pruning</b> method that can be used with existing methods, and is able to achieve improvements even when conventional pruning techniques are simultaneously employed. We implemented RankCut in a modern open-source chess program, CRAFTY. RankCut reduces the game-tree size by approximately 10 %- 40 % for search depths 8 - 12 while retaining tactical reliability, when implemented alongside CRAFTY’s existing <b>forward</b> <b>pruning</b> techniques...|$|E
40|$|Sever 0, 1 early game-playing {{computer}} programs used <b>forward</b> <b>pruning</b> (i. e., {{the practice of}} delib-erately ignoring nodes that are believed unlikely to affect a game tree’s minimax value), but this technique {{did not seem to}} result in good decision-making. The poor performance of forward prun-ing presents a major puzzle for AI research on game playing, because some version of <b>forward</b> <b>pruning</b> seems to be "what people do, " and the best chess-playing programs still do not play as well as the best humans. As a step toward deeper understanding of how for-ward pruning affects quality of play, in this paper we set up a model of <b>forward</b> <b>pruning</b> on two ab-stract classes of binary game trees, and we use this model to investigate how <b>forward</b> <b>pruning</b> affects the accuracy of the minimax values returned. The primary result of our study is that forward prun-ing does better when there is a high correlation among the minimax values of sibling nodes in a game tree. This result suggests that <b>forward</b> <b>pruning</b> may possibly be a useful decision-making technique in certain kinds of games. In particular, we believe that bridge may be such a game...|$|E
40|$|Two {{algorithms}} {{are proposed}} for the adaptive model selection of polynomial non-linear autoregressive with exogenous variable (NARX) models. The recursive <b>forward</b> regression with <b>pruning</b> (RFRP) algorithm {{is based on a}} recursive orthogonal least-squares (ROLS) procedure and efficiently integrates model augmentation and pruning to reduce processing time whenever new data are available. The algorithm provides excellent model structure tracking compared to different OLS-based model selection policies. A less accurate but much faster algorithm {{that can be used for}} time-critical applications is the ROLS-LASSO. This algorithm uses a recursive version of the least absolute shrinkage and selection operator (LASSO) regularisation approach for structure selection. It features a recursive standardisation of the regressors and performs parameter estimation with ROLS. A sliding window data updating is here adopted for both algorithms, although the methods seamlessly generalise to exponential windowing with forgetting factor. Some simulation examples are provided to demonstrate the model tracking capabilities of the algorithms...|$|R
40|$|Abstract Nowadays “live ” content, such as weblog, wikipedia, and news, is {{ubiquitous}} in the Internet. Providing users with relevant content {{in a timely}} manner becomes a challenging problem. Differing from Web search technologies and RSS feeds/reader applications, this paper envisions a personalized full-text content filtering and dissemination system in a highly distributed environment such as a Distributed Hash Table (DHT) based Peer-to-Peer (P 2 P) Network. Users subscribe to their interested content by specifying input keywords and thresholds as filters. Then, content is disseminated to those users having interest in it. In the literature, full-text document publishing in DHTs has suffered for a long time from the high cost of forwarding a document to home nodes of all distinct terms. It is aggravated by the fact that a document contains a large number of distinct terms (typically tens or thousands of terms per document). In this paper, we propose a set of novel techniques to overcome such a high forwarding cost by carefully selecting {{a very small number of}} meaningful terms (or key features) among candidate terms inside each document. Next, to reduce the average hop count per <b>forwarding,</b> we further <b>prune</b> irrelevant documents during the forwarding path. Experiments based on two real query logs and two real data sets demonstrate the effectiveness of our solution...|$|R
40|$|Ozone and PM 10 {{constitute}} the major concern for air quality of Milan. This paper addresses {{the problem of}} the prediction of such two pollutants, using to this end several statistical approaches. In particular, feed-forward neural networks (FFNNs), currently recognized as state-of-the-art approach for statistical prediction of air quality, are compared with two alternative approaches derived from machine learning: pruned neural networks (PNNs) and lazy learning (LL). PNNs constitute a parameter-parsimonious approach, based on the removal of redundant parameters from fully connected neural networks; LL, on the other hand, is a local linear prediction algorithm, which performs a local learning procedure each time a prediction is required. All the three approaches are tested in the prediction of ozone and PM 10; predictors are trained to return at 9 a. m. the concentration estimated for the current day. No strong differences are found between the forecast accuracies of the different models; nevertheless, LL provides the best performances on indicators related to average goodness of the prediction (correlation, mean absolute error, etc.), while PNNs are superior to the other approaches in detecting of the exceedances of alarm and attention thresholds. In some cases, data-deseasonalization is found to improve the prediction accuracy of the models. Finally, some striking features of lazy learning deserve consideration: the LL predictor can be quickly designed, and, thanks to the simplicity of the local linear regressors, it both gets rid of overfitting problems and can be readily interpreted; moreover, it can be also easily kept up-to-date. Key words: feed <b>forward</b> neural networks, <b>pruned</b> neural networks, lazy learning, time series prediction, atmospheric pollution...|$|R
30|$|This rule {{will serve}} as a <b>forward</b> <b>pruning</b> {{criterion}} where all parameters which are not associated with another parameter with non-zero lag value are excluded from the combination of future search. The minimum required support makes the search space manageable.|$|E
40|$|<b>Forward</b> <b>pruning,</b> or {{selectively}} searching {{a subset}} of moves, is now commonly used in game-playing programs {{to reduce the number}} of nodes searched with manageable risk. <b>Forward</b> <b>pruning</b> techniques should consider how pruning errors in a game-tree search propagate to the root to minimize the risk of making errors. In this paper, we explore <b>forward</b> <b>pruning</b> using theoretical analyses and Monte Carlo simulations and report on two findings. Firstly, we find that pruning errors propagate differently depending on the player to move, and show that pruning errors on the opponent’s moves are potentially more serious than pruning errors on the player’s own moves. This suggests that pruning on the player’s own move can be performed more aggressively compared to pruning on the opponent’s move. Secondly, we examine the ability of the minimax search to filter away pruning errors and give bounds on the rate of error propagation to the root. We find that if the rate of pruning error is kept constant, the growth of errors with the depth of the tree dominates the filtering effect, therefore suggesting that pruning should be done more aggressively near the root and less aggressively near the leaves...|$|E
40|$|This paper {{presents}} an efficient method for constructing high quality word graphs for large vocabulary continuous speech recognition. The word graphs are constructed in a two-pass strategy. In the first pass, a huge word graph is produced using the timesynchronous lexical tree search method. Then, {{in the second}} pass, this huge word graph is pruned by applying a modified forwardbackward algorithm. To analyze the characteristic properties of this word graph pruning method, we present a detailed comparison with the conventional time-synchronous <b>forward</b> <b>pruning.</b> The recognition experiments, carried out on the North American Business (NAB) 20 000 -word task, demonstrate that, {{in comparison to the}} <b>forward</b> <b>pruning,</b> the new method leads to a significant reduction {{in the size of the}} word graph without an increase in the graph word error rate...|$|E
40|$|In {{the half}} century since minimax was first {{suggested}} {{as a strategy}} for adversary game search, various search algorithms have been developed. The standard approach has been to use improvements to the Alpha-Beta (-) algorithm. Some of the more powerful im-provements examine continuations beyond the nominal search depth if they are of special interest, while others terminate the search early. The latter case {{is referred to as}} <b>forward</b> <b>pruning.</b> In this paper we discuss some important aspects of <b>forward</b> <b>pruning,</b> especially regarding risk-management, and propose ways of making risk-assessment. Finally, we introduce two new pruning methods based on some of the principles discussed here, and present experi-mental results from application of the methods in an established chess program. 1 Introduction to Decision Quality and Search The standard approach to game-tree search is to use improvements to the Alpha-Beta (-) algorithm to explore all combination of moves to some fixed depth (continuation length or search horizon). In practice, however, the algorithms are no...|$|E
40|$|In {{this paper}} {{we take a}} general look at <b>forward</b> <b>pruning</b> in tree search. By {{identifying}} what we think are desirable characteristics of pruning heuristics, and what attributes are {{important for them to}} consider, we hope to understand better the shortcomings of existing techniques, and to provide some additional insight into how they can be improved. We view this work as a first step towards the goal of improving existing forward-pruning methods...|$|E
40|$|In {{the half}} century since minimax was first {{suggested}} {{as a strategy}} for adversary game search, various search algorithms have been developed. The standard approach has been to use improvements to the Alpha-Beta (#-#) algorithm. Some of the more powerful improvements examine continuations beyond the nominal search depth if they are of special interest, while others terminate the search early. The latter case {{is referred to as}} <b>forward</b> <b>pruning.</b> In this pap...|$|E
40|$|This paper {{introduces}} Competitive Neural Trees (CNeT) for pattern classification. The CNeT performs {{hierarchical classification}} and employ competitive unsupervised learning at the node level. The generalization {{ability of the}} CNeT is guaranteed by <b>forward</b> <b>pruning,</b> which is an inherent part of the learning process. Different search methods for the CNeT are introduced and used for training and recall. The influence of different search methods {{on the performance of}} the CNeT is experimentally evaluated...|$|E
40|$|Abstract. This paper {{presents}} the first performance results for Ballard’s *-Minimax algorithms {{applied to a}} real–world domain: backgammon. It is shown that with effective move ordering and probing the Star 2 algorithm considerably outperforms Expectimax. Star 2 allows strong backgammon programs to conduct depth 5 full-width searches (up from 3) under tournament conditions on regular hardware without using risky <b>forward</b> <b>pruning</b> techniques. We also present empirical evidence that with today’s sophisticated evaluation functions good checker play in backgammon does not require deep searches. ...|$|E
40|$|Transposition {{tables are}} a {{powerful}} tool in search domains for avoiding duplicate effort and for guiding node expansions. Traditionally, however, they have only been applicable when the current state {{is exactly the same}} as a previously explored state. We consider a generalized transposition table, whereby a similarity metric that exploits local structure is used to compare the current state with a neighbourhood of previously seen states. We illustrate this concept and <b>forward</b> <b>pruning</b> based on function approximation in the domain of Skat, and show that we can achieve speedups of 16 + over standard methods. ...|$|E
40|$|In {{this paper}} forward-pruning methods, such as multi-cut and null move, are tested at {{so-called}} ALL nodes. We improved the Principal Variation Search by four small but essential additions. The new PVS algorithm guarantees that <b>forward</b> <b>pruning</b> is safe at ALL nodes. Experiments show that multi-cut at ALL nodes (MC-A) {{when combined with}} other forward-pruning mechanisms give a significant reduction {{of the number of}} nodes searched. In comparison, a (more) aggressive version of the null move (variable null-move bound) gives less reduction at expected ALL nodes. Finally, it is demonstrated that the playing strength of the Lines of Action program MIA is significantly (scoring 21 per cent more winning points than the opponent) increased by MC-A...|$|E
40|$|This chapter {{provides}} a brief historical overview of how variabledepth-search methods have {{evolved in the}} last half a century of computer chess work. We focus mainly on techniques that have not only withstood the test of time but also embody ideas that are still relevant in contemporary game-playing programs. Pseudo code is provided for a special formulation of the PVS/ZWS alpha-beta search algorithm, as well for an implementation of the method of singular extensions. We provide some data from recent experiments with Abyss’ 99, an updated Chinese Chess program. We also pinpoint the current research in <b>forward</b> <b>pruning,</b> since this is where the greatest performance improvements are possible. The work closes with a short summary of the impact of computer chess work on Chinese Chess, Shogi and Go...|$|E
40|$|Abstract—This paper {{presents}} competitive neural trees (CNeT’s) for pattern classification. The CNeT contains ���-ary {{nodes and}} grows during {{learning by using}} inheritance to initialize new nodes. At the node level, the CNeT employs unsupervised competitive learning. The CNeT performs hierarchical clustering of the feature vectors presented to it as examples, while its growth is controlled by <b>forward</b> <b>pruning.</b> Because of the tree structure, the prototype in the CNeT close to any example can be determined by searching {{only a fraction of}} the tree. This paper introduces different search methods for the CNeT, which are utilized for training as well as for recall. The CNeT is evaluated and compared with existing classifiers on a variety of pattern classification problems. Index Terms—Classification, competitive learning, competitive neural tree, decision tree, neural tree, search method, splitting criterion, stopping criterion, tree pruning. I...|$|E
40|$|Advances in {{technology}} allow for increasingly deeper searches in competitive chess programs. Several experiments with chess indicate a constant improvement {{in a program}} 's performance for deeper searches; a program searching to depth d + 1 scores roughly 80 % of the possible points in a match with a program searching to depth d. In other board games, such as Othello and checkers, additional plies of search translated into decreasing benefits, giving rise to diminishing returns for deeper searching. This paper demonstrates that there are diminishing returns in chess. However, the high percentage of errors made by chess programs for search depths through 9 ply hides the effect. 1 Introduction It is common knowledge that chess program performance is strongly correlated to {{the depth of the}} search tree explored by the program. Deeper searching can be achieved in several ways, including more efficient search algorithms (smaller search trees), <b>forward</b> <b>pruning</b> (with the possibility of introduc [...] ...|$|E
40|$|In recent years, {{there is}} a growing {{awareness}} of the importance of reachability and relevance-based pruning techniques for planning, but little work specically targets these techniques. In this paper, we compare the ability of two classes of algorithms to propagate and discover reachability and relevance constraints in classical planning problems. The rst class of algorithms operates on SAT encoded planning problems obtained using the linear and Graphplan encoding schemes. It applies unit-propagation and more general resolu-tion steps (involving larger clauses) to these plan encodings. The second class operates at the plan level and contains two families of pruning algorithms: Reachable-k and Relevant-k. Reachable-k provides a coherent description of a number of existing <b>forward</b> <b>pruning</b> techniques used in numerous algorithms, while Relevant-k captures dierent grades of back-ward pruning. Our results shed light on the ability of dierent plan-encoding schemes to propagate information forward and backward and on the relative merit of plan-level and SAT-level pruning methods. 1...|$|E
40|$|Searching {{and mining}} large graphs today is {{critical}} {{to a variety of}} application domains, ranging from personalized recommendation in social networks, to searches for functional associations in biological pathways. In these domains, {{there is a need to}} perform aggregation operations on large-scale networks. Unfortunately the existing implementation of aggregation operations on relational databases does not guarantee superior performance in network space, especially when it involves edge traversals and joins of gigantic tables. In this paper, we investigate the neighborhood aggregation queries: Find nodes that have top-k highest aggregate values over their h-hop neighbors. While these basic queries are common in a wide range of search and recommendation tasks, surprisingly they have not been studied systematically. We developed a Local Neighborhood Aggregation framework, called LONA, to answer them efficiently. LONA exploits two properties unique in network space: First, the aggregate value for the neighboring nodes should be similar in most cases; Second, given the distribution of attribute values, it is possible to estimate the upper-bound value of aggregates. These two properties inspire the development of novel pruning techniques, <b>forward</b> <b>pruning</b> using differential index and backward pruning using partial distribution. Empirical results show that LONA could outperform the baseline algorithm up to 10 times in real-life large networks...|$|E
40|$|Amazons, as a {{relatively}} young game, has {{caught the attention of}} AI game programmers. Though Amazons is a simple game for humans, it is difficult for computers. It has 2176 possible moves at the initial position and about five hundred possible moves on average during the game. This makes brute-force search highly impractical. Hence, it is an excellent subject for AI techniques such as selective search and evaluation functions. In this project, we create a computer Amazons program [...] -Mulan. Using this program, we implemented and tested some popular Alpha-Beta enhancements and <b>forward</b> <b>pruning</b> algorithms. For Chess, a variety of studies have been performed investigating the relative performance of these techniques. But for Amazons, no one seems to know how effective they are. We experimentally answered this question. For studying evaluation functions in Amazons, we implemented and compared three existing vii features used for Amazons and four new features we developed. Furthermore, starting with the three features which gained the best results in our experiments, we combined them to form an evaluation function using two methods: 1) linear combinations using tournaments; 2) a pattern classification approach based on Bayesian learning. Though Mulan is created as a test bed for Artificial Intelligence, it has evolved to a strong Amazons playing program. Our test results show that Mulan can substantially beat some strong Amazons playing programs such as Yamazon and Arrow [...] ...|$|E
40|$|Tong Kwong-Bun. Thesis {{submitted}} in: December 2002. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2003. Includes bibliographical references (leaves 77 -[80]). Abstracts in English and Chinese. Chapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - An Overview [...] - p. 1 Chapter 1. 2 [...] - Tree Search [...] - p. 2 Chapter 1. 2. 1 [...] - Minimax Algorithm [...] - p. 2 Chapter 1. 2. 2 [...] - The Alpha-Beta Algorithm [...] - p. 4 Chapter 1. 2. 3 [...] - Alpha-Beta Enhancements [...] - p. 5 Chapter 1. 2. 4 [...] - Selective Search [...] - p. 9 Chapter 1. 3 [...] - Construction of Evaluation Function [...] - p. 16 Chapter 1. 4 [...] - Contribution of the Thesis [...] - p. 17 Chapter 1. 5 [...] - Structure of the Thesis [...] - p. 19 Chapter 2 [...] - The Probabilistic <b>Forward</b> <b>Pruning</b> Framework [...] - p. 20 Chapter 2. 1 [...] - Introduction [...] - p. 20 Chapter 2. 2 [...] - The Generalized Probabilistic Forward Cuts Heuristic [...] - p. 21 Chapter 2. 3 [...] - The GPC Framework [...] - p. 24 Chapter 2. 3. 1 [...] - The Alpha-Beta Algorithm [...] - p. 24 Chapter 2. 3. 2 [...] - The NegaScout Algorithm [...] - p. 25 Chapter 2. 3. 3 [...] - The Memory-enhanced Test Algorithm [...] - p. 27 Chapter 2. 4 [...] - Summary [...] - p. 27 Chapter 3 [...] - The Fast Probabilistic <b>Forward</b> <b>Pruning</b> Framework [...] - p. 30 Chapter 3. 1 [...] - Introduction [...] - p. 30 Chapter 3. 2 [...] - The Fast GPC Heuristic [...] - p. 30 Chapter 3. 2. 1 [...] - The Alpha-Beta algorithm [...] - p. 32 Chapter 3. 2. 2 [...] - The NegaScout algorithm [...] - p. 32 Chapter 3. 2. 3 [...] - The Memory-enhanced Test algorithm [...] - p. 35 Chapter 3. 3 [...] - Performance Evaluation [...] - p. 35 Chapter 3. 3. 1 [...] - Determination of the Parameters [...] - p. 35 Chapter 3. 3. 2 [...] - Result of Experiments [...] - p. 38 Chapter 3. 4 [...] - Summary [...] - p. 42 Chapter 4 [...] - The Node-Cutting Heuristic [...] - p. 43 Chapter 4. 1 [...] - Introduction [...] - p. 43 Chapter 4. 2 [...] - Move Ordering [...] - p. 43 Chapter 4. 2. 1 [...] - Quality of Move Ordering [...] - p. 44 Chapter 4. 3 [...] - Node-Cutting Heuristic [...] - p. 46 Chapter 4. 4 [...] - Performance Evaluation [...] - p. 48 Chapter 4. 4. 1 [...] - Determination of the Parameters [...] - p. 48 Chapter 4. 4. 2 [...] - Result of Experiments [...] - p. 50 Chapter 4. 5 [...] - Summary [...] - p. 55 Chapter 5 [...] - The Integrated Strategy [...] - p. 56 Chapter 5. 1 [...] - Introduction [...] - p. 56 Chapter 5. 2 [...] - "Combination of GPC, FGPC and Node-Cutting Heuristic" [...] - p. 56 Chapter 5. 3 [...] - Performance Evaluation [...] - p. 58 Chapter 5. 4 [...] - Summary [...] - p. 63 Chapter 6 [...] - Conclusions and Future Works [...] - p. 64 Chapter 6. 1 [...] - Conclusions [...] - p. 64 Chapter 6. 2 [...] - Future Works [...] - p. 65 Chapter A [...] - Examples [...] - p. 67 Chapter B [...] - The Rules of Chinese Checkers [...] - p. 73 Chapter C [...] - Application to Chinese Checkers [...] - p. 75 Bibliography [...] - p. 7...|$|E

