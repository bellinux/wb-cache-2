237|94|Public
40|$|Abstract—In {{the present}} communication, the {{existing}} measures of <b>fuzzy</b> <b>entropy</b> are reviewed. A generalized parametric exponential <b>fuzzy</b> <b>entropy</b> is defined. Our {{study of the}} four essential and some other properties of the proposed measure, clearly establishes {{the validity of the}} measure as an entropy. Keywords—fuzzy sets, <b>fuzzy</b> <b>entropy,</b> exponential entropy, exponential <b>fuzzy</b> <b>entropy.</b> I...|$|E
40|$|Abstract—In the {{literature}} of information theory, there is necessity for comparing the different measures of <b>fuzzy</b> <b>entropy</b> and this consequently, {{gives rise to the}} need for normalizing measures of <b>fuzzy</b> <b>entropy.</b> In this paper, we have discussed this need and hence developed some normalized measures of <b>fuzzy</b> <b>entropy.</b> It is also desirable to maximize entropy and to minimize directed divergence or distance. Keeping in mind this idea, we have explained the method of optimizing different measures of <b>fuzzy</b> <b>entropy.</b> Keywords—Fuzzy set, Uncertainty, <b>Fuzzy</b> <b>entropy...</b>|$|E
40|$|In {{the present}} paper, {{based on the}} concept of <b>fuzzy</b> <b>entropy,</b> an {{exponential}} intuitionistic <b>fuzzy</b> <b>entropy</b> measure is proposed in the setting of Atanassov’s intuitionistic fuzzy set theory. This measure is a generalized version of exponential <b>fuzzy</b> <b>entropy</b> proposed by Pal and Pal. A connection between exponential <b>fuzzy</b> <b>entropy</b> and exponential intuitionistic <b>fuzzy</b> <b>entropy</b> is also established. Some interesting properties of this measure are analyzed. Finally, a numerical example is given to show that the proposed entropy measure for Atanassov’s intuitionistic fuzzy set is consistent by comparing it with other existing entropies...|$|E
40|$|AbstractIn this paper, we {{introduce}} {{a new type of}} <b>fuzzy</b> topological <b>entropy</b> of a <b>fuzzy</b> continuous function ψ:X→X, where X is an arbitrary fuzzy topological space, and investigate several of its properties. We also establish that, for fuzzy compact spaces, this new concept of <b>fuzzy</b> topological <b>entropy</b> coincides with the <b>fuzzy</b> topological <b>entropy</b> of İsmail Tok (2005) [11]...|$|R
40|$|The {{first phase}} of this {{research}} project involves the integration of <b>fuzzy</b> <b>entropies</b> (used {{in the context of}} measuring uncertainty and information) with computational neural networks. An algorithm for the creation and manipulation of <b>fuzzy</b> <b>entropies,</b> extracted by a neural network from a data set, is being designed and implemented. Advantage is being taken of the learning capability of the neural network to manipulate <b>fuzzy</b> <b>entropies</b> for classification and recognition processes. The neural network is being used to find patterns in terms of structural features and properties that correspond to a desired level of activity in various classes of molecules, such as azo dyes. Each molecule is described by a set of structural features, a set of physical properties and the strength of some activity under consideration. After developing an appropriate set of input parameters, the neural network is trained with selected molecules, then a search is carried out for compounds that exhibit the desired level of activity. High level molecular orbital and density functional techniques are being employed to establish databases of various molecular properties required by the neural network approach. We are making extensive use of th...|$|R
30|$|<b>Fuzzy</b> {{partitioned}} <b>entropy</b> segmentation algorithm {{takes into}} account the inherent fuzzy characteristics of the particle and uses the membership function to describe the fuzzy information that traditional classical logic can hardly represent. The recursive <b>fuzzy</b> partition <b>entropy</b> threshold segmentation method was proposed to segment and optimize the settlement particles.|$|R
40|$|Using {{the idea}} of Rènyi’s entropy, intuitionistic <b>fuzzy</b> <b>entropy</b> of order-α is {{proposed}} {{in the setting of}} intuitionistic fuzzy sets theory. This measure is a generalized version of <b>fuzzy</b> <b>entropy</b> of order-α proposed by Bhandari and Pal and intuitionistic <b>fuzzy</b> <b>entropy</b> defined by Vlachos and Sergiadis. Our study of the four essential and some other properties of the proposed measure clearly establishes the validity of the measure as intuitionistic <b>fuzzy</b> <b>entropy.</b> Finally, a numerical example is given to show that the proposed entropy measure for intuitionistic fuzzy set is reasonable by comparing it with other existing entropies...|$|E
40|$|Image {{segmentation}} using <b>fuzzy</b> <b>entropy</b> is {{an important}} and common segmentation method. The threshold of <b>fuzzy</b> <b>entropy</b> is mostly selected at the gray value with fuzzy membership degree 0. 5. It is a limitation in some cases. In order to solve this problem, we present a new definition of generalized <b>fuzzy</b> <b>entropy</b> {{and apply it to}} image segmentation. Compared with the traditional fuzzy entropy-based image segmentation method, the proposed method segments an image using the threshold with membership degree m (0 <m< 1), and increases the opportunity of choosing appropriate thresholds. Experiments show that our method can obtain better segmentation result than the traditional <b>fuzzy</b> <b>entropy</b> method...|$|E
40|$|The <b>fuzzy</b> <b>entropy</b> {{designed}} for multiple facts selection {{has been carried}} out in this work. The entropy for the fuzzy data with respect to a specified fact is designed through a distance measure method. The obtained <b>fuzzy</b> <b>entropy</b> is then applied for the selection from multiple facts. From the relevant <b>fuzzy</b> <b>entropy,</b> it is concluded that data uncertainty information is limited by the total fact of n- 1. The bounded calculation of data uncertainty to each fact is proven for multiple facts, and the decision of fuzzy data to the certain fact among multiple facts has been considered with the assistance of <b>fuzzy</b> <b>entropy</b> calculation...|$|E
40|$|AMS Mathematics Subject Classification(2000) : 94 A 17, 94 A 24 Abstract. Noiseless coding theorems {{connected}} with <b>fuzzy</b> <b>entropies</b> corresponding to Shannon, Renyi and Havrada and Charvat have been established. The upper bounds of these entropies {{in terms of}} mean code word lengths have been provided and some interesting properties of these codeword lengths have been studied...|$|R
40|$|Abstract This {{research}} {{involves the}} integration of <b>fuzzy</b> <b>entropies</b> (used {{in the context of}} measuring uncertainty and information) with computational neural networks. An algorithm for the creation and manipulation of <b>fuzzy</b> <b>entropies,</b> extracted by a neural network from a data set, is designed and implemented. The neural network is used to find patterns in terms of structural features and properties that correspond to a desired level of activity in various azo dyes. Each molecule is described by a set of structural features, a set of physical properties and the strength of some activity under consideration. After developing an appropriate set of input parameters, the neural network is trained with selected molecules, then a search is carried out for compounds that exhibit the desired level of activity. High level molecular orbital and density functional techniques are employed to establish databases of various molecular properties required by the neural network approach. Key Words: Soft computing (hybrid fuzzy-neural algorithm), molecular modeling. ...|$|R
40|$|Abstract- The {{importance}} of service quality ranking is apparent with increasingly demand to meet customer needs in highly competitive service related industry. However, service quality ranking {{is not always}} straightforward as the criteria in ranking processes and customer perceptions toward services are intangible measures. This paper presents a ranking for service quality of four vehicle insurance companies using intuitionistic <b>fuzzy</b> weighted <b>entropy.</b> The intuitionistic <b>fuzzy</b> weighted <b>entropy</b> is useful to represent the decision information {{in the process of}} decision making since it was characterized by degrees of membership, degrees of non-membership and hesitation degrees. The crisp survey results were collected via questionnaires from customers of the selected vehicle insurance companies and analysed using the intuitionistic <b>fuzzy</b> weight <b>entropy.</b> It is found that BS Insurance is the first in ranking thereby the best company in service quality out of the four companies. These ranking results would be useful for insurance companies in upgrading their service quality and eventually able to fulfil customers’needs...|$|R
40|$|Abstract [...] In {{this paper}} {{incomplete}} quantitative data has been dealt {{by using the}} concept of <b>fuzzy</b> <b>entropy.</b> <b>Fuzzy</b> <b>entropy</b> {{has been used to}} extrapolate the data pertaining to the compressor current. Certain attributes related to the compressor current have been considered. Test data of compressor current used in this knowledge discovery algorithm knows the entire attribute clearly. The developed algorithm is very effective and can be used in the various application related to knowledge discovery and machine learning. The developed knowledge discovery algorithm using <b>fuzzy</b> <b>entropy</b> has been tested on a multi-compressor system for incomplete compressor current data and it is found that the error level is merely ± 4. 40 %, which is far better than other available knowledge discovery algorithms. Key words: <b>fuzzy</b> <b>entropy,</b> genetic programming, incomplete data, classification, and knowledge discovery, multi-compressor system I...|$|E
40|$|Abstract—This paper {{presents}} an efficient fuzzy classifier {{with the ability}} of feature selection based on a <b>fuzzy</b> <b>entropy</b> measure. <b>Fuzzy</b> <b>entropy</b> is employed to evaluate the information of pattern distribution in the pattern space. With this information, we can partition the pattern space into nonoverlapping decision regions for pattern classification. Since the decision regions do not overlap, both the complexity and computational load of the classifier are reduced and thus the training time and classification time are extremely short. Although the decision regions are partitioned into nonoverlapping subspaces, we can achieve good classification performance since the decision regions can be correctly determined via our proposed <b>fuzzy</b> <b>entropy</b> measure. In addition, we also investigate the use of <b>fuzzy</b> <b>entropy</b> to select relevant features. The feature selection procedure not only reduces the dimensionality of a problem but also discards noise-corrupted, redundant and unimportant features. Finally, we apply the proposed classifier to the Iris database and Wisconsin breast cancer database to evaluate the classification performance. Both of {{the results show that}} the proposed classifier can work well for the pattern classification application. Index Terms—Feature selection, fuzzy classifier, <b>fuzzy</b> <b>entropy.</b> I...|$|E
30|$|<b>Fuzzy</b> <b>entropy</b> {{is used to}} {{quantify}} the uncertainty associated to fuzzy variables.|$|E
40|$|Purpose: The aim of {{this study}} is {{applying}} a new method for Industrial robotic system selection. Design/methodology/approach: In this paper, the weights of each criterion are calculated using <b>fuzzy</b> Shannon’s <b>Entropy.</b> After that, <b>fuzzy</b> TOPSIS is utilized to rank the alternatives. After that we compare the result of Fuzzy TOPSIS with Fuzzy VIKOR method. Then we select the best Industrial Robotic System based on these results. Findings: The outcome of this research is ranking and selecting industrial robotic systems with the help of <b>Fuzzy</b> Shannon’s <b>Entropy</b> and <b>Fuzzy</b> TOPSIS techniques. Originality/value: This paper offers a new integrated method for industria...|$|R
40|$|Infrared {{images are}} fuzzy and noisy by nature; thus the {{segmentation}} of human targets in infrared images is a challenging task. In this paper, a fast thresholding method of infrared human images based on two-dimensional <b>fuzzy</b> Tsallis <b>entropy</b> is introduced. First, {{to address the}} fuzziness of infrared image, the <b>fuzzy</b> Tsallis <b>entropy</b> of objects and that of background are defined, respectively, according to probability partition principle. Next, this newly defined entropy is extended to two dimensions to {{make good use of}} spatial information to deal with the noise in infrared images, and correspondingly a fast computation method of two-dimensional <b>fuzzy</b> Tsallis <b>entropy</b> is put forward to reduce its computation complexity from O(L 2) to O(L). Finally, the optimal parameters of fuzzy membership function are searched by shuffled frog-leaping algorithm following maximum entropy principle, and then the best threshold of an infrared human image is computed from the optimal parameters. Compared with typical entropy-based thresholding methods by experiments, the method presented in this paper is proved to be more efficient and robust...|$|R
30|$|In 2007, Li and Liu [9] {{proposed}} a <b>fuzzy</b> maximum <b>entropy</b> principle, which {{tells us that}} {{out of all the}} membership functions satisfying the given constraints, we should select the one that maximizes the entropy.|$|R
40|$|Abstract: Some new {{trigonometric}} {{measures of}} <b>fuzzy</b> <b>entropy</b> involving trigonometric functions {{have been provided}} and their validity is checked by studying their essential properties and certain inequalities involving trigonometric angles of a convex polygon of n sides have been proved by making use of some concepts from fuzzy information theory. Key words: Trigonometry, fuzzy information theory, entropy, <b>fuzzy</b> <b>entropy</b> trigonometric inequalities 1...|$|E
40|$|Abstract:- The aim of {{this paper}} is the {{proposition}} of a soft computing approach for solving patter recognition problems. In particular, starting from Shannon’s <b>Fuzzy</b> <b>Entropy,</b> we propose a mathematical model which extracts fuzzy inference with minimal entropy. The proposal approach has been applied to evaluate Synthetic Aperture Radar imagery. In addition, a comparison with the classical Shannon’s <b>Fuzzy</b> <b>Entropy</b> has been taken into account...|$|E
30|$|Entropy {{is one of}} the key {{measures}} of information first used by Shannon [1]. Entropy as a measure of fuzziness was introduced by Zadeh [2]. <b>Fuzzy</b> <b>entropy</b> is an important concept for measuring fuzzy information. A measure of the <b>fuzzy</b> <b>entropy</b> of a fuzzy set is a measure of the fuzziness of the set. Kapur [3] argues that <b>fuzzy</b> <b>entropy</b> measures uncertainty due to fuzziness of information, while probabilistic entropy measures uncertainty due to the information being available in terms of a probability distribution only. The concept of fuzzy sets proposed by Zadeh [2] has proven useful in the context of pattern recognition, image processing, speech recognition, bioinformatics, fuzzy aircraft control, feature selection, decision-making, etc.|$|E
40|$|As {{a measure}} of {{information}} shared between two fuzzy pattern vectors, the fuzzy information proximity measure (FIPM) plays {{an important part in}} fuzzy pattern recognition, fuzzy clustering analysis and fuzzy approximate reasoning. In this paper, two novel FIPMs are set up. Firstly, an axiom theory about the FIPM is given, and different expressions of the FIPM are discussed. A new FIPM is then proposed based on the axiom theory of the FIPM and the concept of fuzzy subsethood function. Two concepts based on the idea of Shannon information <b>entropy,</b> <b>fuzzy</b> joint <b>entropy</b> (FJE) and <b>fuzzy</b> conditional <b>entropy</b> (FCE), are proposed and the basic properties of FJE and FCE are given and proved. Finally, classical similarity measures such as dissimilarity measure (DM) and similarity measure (SM) are studied, and two new measures, fuzzy absolute information measure (FAIM) and fuzzy relative information measure (FRIM), are set up, which can be used as measures of the proximity between fuzzy sets A and B...|$|R
30|$|After {{the detail}} {{research}} on the settlement particle images and settlement particle sequence images, a multi-threshold segmentation method with the <b>fuzzy</b> 3 -partitioning <b>entropy</b> is proposed, where the improved artificial bee colony algorithm is used to optimize the threshold, the recursive idea {{is also used to}} reduce the large number of repeated calculations in the <b>fuzzy</b> partitioning <b>entropy,</b> and the region-based label assignment method is applied to improve the accuracy of image segmentation. The experiment results show that the proposed algorithm has better segmentation performance.|$|R
30|$|An {{extensive}} {{search in}} the literature revealed only three algorithms {{that have been used}} for feature selection: functional networks, decision trees, and <b>fuzzy</b> information <b>entropy</b> (FIE) (also known as fuzzy ranking). Each of them is discussed in the following sections.|$|R
40|$|Abstract- The {{well-known}} generalisation of hard C-means (HCM) clustering is fuzzy C-means (FCM) cluster-ing where {{a weight}} exponent on each fuzzy membership is introduced as {{the degree of}} fuzziness. An alternative generalisation of HCM clustering is proposed in this pa-per. This is called <b>fuzzy</b> <b>entropy</b> (FE) clustering where a weight factor of the <b>fuzzy</b> <b>entropy</b> function is introduced as the degree of <b>fuzzy</b> <b>entropy.</b> The weight factor is simi-lar to the weight exponent and has a physical interpreta-tion. The noise clustering approach, the fuzzy covariance matrix and the fuzzy mixture weight are also proposed. Moreover, we can show Gaussian mixture clustering is re-garded as a special case of FE clustering. Some illustrative examples are performed on the Butterfly and Iris data. I...|$|E
40|$|The Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) {{has been}} used to propose a new method for {{filtering}} time series originating from nonlinear systems. The filtering method is based on <b>fuzzy</b> <b>entropy</b> and a new waveform. A new waveform is defined wherein Intrinsic Mode Functions (IMFs) —which are obtained by CEEMDAN algorithm—are firstly sorted in ascending order (the sorted IMFs is symmetric about center point, because at any point, the mean value of the envelope line defined by the local maxima and the local minima is zero), and the energy of the sorted IMFs are calculated, respectively. Finally, the new waveform with axial symmetry can be obtained. The complexity of the new waveform can be quantified by <b>fuzzy</b> <b>entropy.</b> The relevant modes (noisy signal modes and useful signal modes) can be identified by the difference between the <b>fuzzy</b> <b>entropy</b> of the new waveform and the next adjacent new waveform. To evaluate the filter performance, CEEMDAN and sample entropy, Ensemble Empirical Mode Decomposition (EEMD) and <b>fuzzy</b> <b>entropy,</b> and EEMD and sample entropy were used to filter the synthesizing signals with various levels of input signal-to-noise ratio (SNRin). In particular, this approach is successful in filtering impact signal. The results of the filtering are evaluated by a de-trended fluctuation analysis (DFA) algorithm, revised mean square error (RMSE), and revised signal-to-noise ratio (RSNR), respectively. The filtering results of simulated and impact signal show that the filtering method based on CEEMDAN and <b>fuzzy</b> <b>entropy</b> outperforms other signal filtering methods...|$|E
40|$|A fuzzy {{aspect is}} {{introduced}} into L systems {{and some of}} its properties are investigated. The relationship between fuzzy L languages and the fuzzy languages generated by fuzzy grammar defined by Lee and Zadeh is elucidated. The concept of <b>fuzzy</b> <b>entropy</b> of a string is proposed as a measure of fuzziness of the latter. Some relationship between fuzzy L systems and the ordinary L systems is discussed. Key words: Fuzzy set, fuzzy L system/language, <b>fuzzy</b> <b>entropy</b> of a string. ...|$|E
30|$|In our research, a multi-threshold {{segmentation}} method with recursive <b>fuzzy</b> 3 -partition <b>entropy</b> is proposed, and the S {{membership function}} containing three parameters is selected {{to replace the}} trapezoid membership function, and the fuzzy N division of the image is implemented to improve the segmentation precision. Then, the segmentation accuracy is improved. The <b>fuzzy</b> partition <b>entropy</b> is calculated by recursive algorithm, and the instantaneous recursive value is preserved for subsequent population optimization algorithm, so as to reduce repeated computation and improve the efficiency of optimization. Finally, the fuzzy comprehensive evaluation method is used to identify the settlement particles.|$|R
40|$|This paper {{presents}} a reliable, easy and more objective approach for ranking and determining preference in a multi-criteria decision-making problem within the shipping industry. Through {{the integration of}} the improved score function, <b>fuzzy</b> Shannon’s <b>entropy</b> method and the interval-valued intuitionistic fuzzy M-TOPSIS method, for ranking and for representing the aggregated effect of positive and negative evaluations in the performance ratings of the alternatives based on interval-valued intuitionistic fuzzy set (IVIFS) data. The integration of the improved score function, <b>fuzzy</b> Shannon’s <b>entropy</b> method and the interval-valued intuitionistic fuzzy M-TOPSIS method in this paper has provided a whole new approach for solving multi-criteria decision-making problems. The improved score function which is applied to the calculation of the separation measures of each alternative from the positive and negative ideal solutions. Reflect and model the fuzziness and hesitation of the decision-maker subjective assessment, while the <b>fuzzy</b> Shannon’s <b>entropy</b> method is been used for calculating the criteria weight. The proposed method has successfully been applied to rank and determined the most appropriate shipping partner for a shipping company located in Malaysia, and for a modified hypothetical example which is based on the selection of a preferred Ship as a reference for a new design. The model has been compared with existing model and we can conclude, it provides a better alternative method for ranking and for the determination of preference in a multi-criteria decision-making proble...|$|R
40|$|International audienceThis paper {{presents}} two {{new concepts}} for discrimination of signals of different complexity. The first focused initially on {{solving the problem}} of setting entropy descriptors by varying the pattern size instead of the tolerance. This led {{to the search for}} the optimal pattern size that maximized the similarity entropy. The second paradigm was based on the n-order similarity entropy that encompasses the 1 -order similarity entropy. To improve the statistical stability, n-order <b>fuzzy</b> similarity <b>entropy</b> was proposed. Fractional Brownian motion was simulated to validate the different methods proposed, and fetal heart rate signals were used to discriminate normal from abnormal fetuses. In all cases, it was found {{that it was possible to}} discriminate time series of different complexity such as fractional Brownian motion and fetal heart rate signals. The best levels of performance in terms of sensitivity (90 %) and specificity (90 %) were obtained with the n-order <b>fuzzy</b> similarity <b>entropy.</b> However, it was shown that the optimal pattern size and the maximum similarity measurement were related to intrinsic features of the time series...|$|R
30|$|<b>Fuzzy</b> <b>entropy</b> {{provides}} a quantitative {{measure of the}} uncertainty associated with each fuzzy variable. Since Zadeh [1] introduced the <b>fuzzy</b> <b>entropy</b> as a weighted shannon entropy, researchers gave several definitions from different angles, such as De Luca and Termini [2], Yager [3], Kaufmann [4], Kosko [5], Pal and Pal [6]. The above definitions characterize the uncertainty resulting primarily from the linguistic vagueness rather than resulting from information deficiency and vanish when the fuzzy variable is an equipossible one. However, Liu [7] suggested that a <b>fuzzy</b> <b>entropy</b> should meet at least the following three basic requirements: the entropy of a crisp number is zero; the entropy of an equipossible fuzzy variable is maximum; and the entropy is applicable not only to finite and infinite cases but also to discrete and continuous cases. In order to meet these requirements, {{within the framework of}} credibility theory, Li and Liu [8] provided a new definition of <b>fuzzy</b> <b>entropy</b> to characterize the uncertainty resulting from information deficiency which is caused by the impossibility to predict the specified value that a fuzzy variable takes. Based on this definition, Li and Liu [9] proposed the fuzzy maximum entropy principle and proved some maximum entropy theorems.|$|E
40|$|Feature {{selection}} using <b>fuzzy</b> <b>entropy</b> measures with Yu's similarity measure. Master Thesis 2012 50 pages, 6 figures, 8 tables In this study, {{feature selection}} in classification based problems is highlighted. The role of feature selection methods is to select impor-tant features by discarding redundant and irrelevant {{features in the}} data set, we investigated this case by using <b>fuzzy</b> <b>entropy</b> measures. We developed <b>fuzzy</b> <b>entropy</b> based feature selection method using Yu's similarity and test this using similarity classifier. As the similarity classifier we used Yu's similarity, we tested our similarity on the real world data set which is dermatological data set. By performing feature selection based on <b>fuzzy</b> <b>entropy</b> measures before classification on our data set the empirical results were very promising, the highest classi-fication accuracy of 98. 83 % was achieved when testing our similarity measure to the data set. The achieved results were then compared with some other results previously obtained using different similarity classi-fiers, the obtained results show better accuracy than the one achieved before. The used methods helped to reduce the dimensionality of the used data set, {{to speed up the}} computation time of a learning algorithm and therefore have simplified the classification task...|$|E
3000|$|Compared to the {{traditional}} methods, the proposed image definition measure based on <b>fuzzy</b> <b>entropy</b> has two important advantages. First, the membership function μ [...]...|$|E
40|$|AbstractThis paper {{develops}} a constructing algorithm for an appropriate membership function as objectively as possible. It {{is important to}} set an appropriate membership function for real-world decision making. The main academic contribution of our proposed algorithm is to integrate a general continuous and nonlinear function with <b>fuzzy</b> Shannon <b>entropy</b> into subjective interval estimation by a heuristic method under a given probability density function based on real-world data. Two main steps of our proposed approach are to set membership values a decision maker confidently judges whether an element {{is included in the}} given set or not and to obtain other values objectively by solving a mathematical programming problem with <b>fuzzy</b> Shannon <b>entropy.</b> It is difficult to solve the problem efficiently using previous constructing approaches due to nonlinear function. In this paper, the given nonlinear membership function is approximately transformed into a piecewise linear membership function, and the appropriate values are determined. Furthermore, by introducing natural assumptions in the real-world and interactively adjusting the membership values, an algorithm to obtain the optimal condition of each appropriate membership value is developed...|$|R
40|$|In medicine, {{artificial}} {{neural networks}} (ANN) have been extensively applied in many fields {{to model the}} nonlinear relationship of multivariate data. Due {{to the difficulty of}} selecting input variables, attribute reduction techniques were widely used to reduce data to get a smaller set of attributes. However, to compute reductions from heterogeneous data, a discretizing algorithm was often introduced in dimensionality reduction methods, which may cause information loss. In this study, we developed an integrated method for estimating the medical care costs, obtained from 798 cases, associated with myocardial infarction disease. The subset of attributes was selected as the input variables of ANN by using an entropy-based information measure, <b>fuzzy</b> information <b>entropy,</b> which can deal with both categorical attributes and numerical attributes without discretization. Then, we applied a correction for the Akaike information criterion (ΑICc) to compare the networks. The results revealed that <b>fuzzy</b> information <b>entropy</b> was capable of selecting input variables from heterogeneous data for ANN, and the proposed procedure of this study provided a reasonable estimation of medical care costs, which can be adopted in other fields of medical science...|$|R
30|$|In the {{previous}} section, the recursive <b>fuzzy</b> partition <b>entropy</b> threshold segmentation algorithm {{was used to}} segment and optimize the settlement particles, which is {{in order to better}} identify the settlement particles. The settlement particles studied in this paper have many uncertain fuzzy information. The traditional identification methods (such as the Otsu threshold segmentation, Gaussian mixture model) cannot be effectively identified. This paper uses the method of settlement particle identification based on fuzzy comprehensive evaluation.|$|R
