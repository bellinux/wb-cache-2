102|5285|Public
2500|$|... {{where the}} {{distance}} measure [...] determines {{the level of}} discrepancy between [...] and [...] based on a given metric (e.g., the Euclidean distance). A strictly positive tolerance is usually necessary, since {{the probability that the}} simulation outcome coincides exactly with the <b>data</b> (<b>event</b> [...] ) is negligible for all but trivial applications of ABC, which would in practice lead to rejection of nearly all sampled parameter points. The outcome of the ABC rejection algorithm is a sample of parameter values approximately distributed according to the desired posterior distribution, and, crucially, obtained without the need of explicitly evaluating the likelihood function (Figure 1).|$|E
50|$|A <b>data</b> <b>event</b> is a {{relevant}} state transition defined in an event schema. Typically, event schemata are described by {{pre- and post}} condition for a single or a set of data items. In contrast to ECA (Event condition action), which considers an event to be a signal, the <b>data</b> <b>event</b> not only refers to the change (signal), but describes specific state transitions, which are referred to in ECA as conditions.|$|E
50|$|Every summer since 2012, DataMeet has {{organized}} a two-day open <b>data</b> <b>event</b> called Open Data Camp in Bangalore. Open Data Camp {{has also been}} held in Delhi since November 2014.|$|E
40|$|This {{report is}} {{preliminary}} {{and has not}} been edited or reviewed for conformity with U. S. Geological Survey editorial standards and stratigraphic nomenclature. Table 179. Station <b>data</b> for <b>event</b> 19 [...] 540 Table 180. Station <b>data</b> for <b>event</b> 31 [...] 543 Table 181. Station <b>data</b> for <b>event</b> 32 [...] 545 Table 182. Station <b>data</b> for <b>event</b> 48 [...] 548 Table 183. Station <b>data</b> for <b>event</b> 52 [...] 550 Table 184. Station <b>data</b> for <b>event</b> 72 [...] 552 Table 185. Station <b>data</b> for <b>event</b> 73 [...] 554 Table 186. Station <b>data</b> for <b>event</b> 76 [...] 556 Table 187. Station <b>data</b> for <b>event</b> 77 [...] 559 Table 188. Station <b>data</b> for <b>event</b> 91 [...] 561 Table 189. Station <b>data</b> for <b>event</b> 92 [...] 563 Table 190. Station <b>data</b> for <b>event</b> 96 [...] 564 Table 191. Station <b>data</b> for <b>event</b> 104 [...] 56...|$|R
30|$|Throughput {{is defined}} as the average number of the {{published}} <b>data</b> <b>events</b> per second.|$|R
5000|$|Analysis: The system {{analyzes}} and correlates the <b>data,</b> <b>events,</b> and alarms, {{to identify}} the real situations and their priority.|$|R
50|$|Considering data {{events as}} {{relevant}} data item state transitions allows defining complex event-reaction schemata for a database. Defining <b>data</b> <b>event</b> schemata for relational databases {{is limited to}} attribute and instance events. Object-oriented databases also support collection properties, which allows defining changes in collections as data events, too.|$|E
5000|$|The {{specification}} {{also recommends}} {{two types of}} notifications, which one, the Extension Notification, can contain control or command data to handle devices or material services. For this reason this notification is regarded like an exception, a standard control event that is not described as a pair Request > Response (in a previous version {{it was called the}} <b>Data</b> <b>Event).</b> The two notifications are: ...|$|E
50|$|There are {{two kinds}} of EPCIS data: event data and master <b>data.</b> <b>Event</b> data is created in the process of {{carrying}} out business processes, and is captured through the EPCIS Capture Interface and made available for query through the EPCIS Query Interfaces. Master data is additional data that provides the necessary context for interpreting the event data. It is available for query through the EPCIS Query Control Interface, but the means by which master data enters the system is not specified in the 1.0 specification.|$|E
50|$|Events {{consist of}} a {{standard}} header and zero or more bytes of published <b>event</b> <b>data.</b> The <b>Event</b> Service API does not impose a specific layout for the published <b>event</b> <b>data.</b>|$|R
40|$|The CDF <b>Event</b> <b>Data</b> Model {{describes}} how CDF C++ and Fortran- 77 software may access <b>event</b> <b>data</b> in CDF Off-line software. The CDF <b>Event</b> <b>Data</b> Model Working Group has reviewed the Run I <b>Event</b> <b>Data</b> Model {{which was based}} on YBOS data banks stored in a global array. While this model was successful for Run I analyses, it did not take advantage of the Object-Oriented design of the CDF Run II Off-line software. In this paper, we describe the CDF Run II <b>Event</b> <b>Data</b> Model. <b>Event</b> <b>data</b> is passed via an event record amongst user-written software modules whose execution is coordinated by the AC++/Framework package. C++ classes meeting a few criteria can have instances stored in the event record. Objects in the event record are assigned a unique identifier and become write-locked, key changes in behavior from the old model. Support for storable links, collections, YBOS banks, and data access by Fortran- 77 software are accommodated. The ROOT Object I/O system is used to read/write the event record to disk files. This model has simplified the effort to make <b>event</b> <b>data</b> persistent, without requiring an overwhelming effort to retrofit our large, existing code base written to the old <b>event</b> <b>data</b> model. Keywords: CDF, <b>event,</b> <b>data,</b> model, ROO...|$|R
50|$|A highly {{effective}} backup system would have duplicate copies of every file and program that were immediately accessible whenever a <b>Data</b> Loss <b>Event</b> was noticed. However, in most situations, {{there is an}} inverse correlation between {{the value of a}} unit of data and the length of time it takes to notice the loss of that data. Taking this into consideration, many backup strategies decrease the granularity of restorability as the time increases since the potential <b>Data</b> Loss <b>Event.</b> By this logic, recovery from recent <b>Data</b> Loss <b>Events</b> is easier and more complete than recovery from <b>Data</b> Loss <b>Events</b> that happened further in the past.|$|R
5000|$|... {{where the}} {{distance}} measure [...] determines {{the level of}} discrepancy between [...] and [...] based on a given metric (e.g., the Euclidean distance). A strictly positive tolerance is usually necessary, since {{the probability that the}} simulation outcome coincides exactly with the <b>data</b> (<b>event</b> [...] ) is negligible for all but trivial applications of ABC, which would in practice lead to rejection of nearly all sampled parameter points. The outcome of the ABC rejection algorithm is a sample of parameter values approximately distributed according to the desired posterior distribution, and, crucially, obtained without the need of explicitly evaluating the likelihood function (Figure 1).|$|E
5000|$|Light levels {{need to be}} {{measured}} when the exhibition is prepared. UV light meters will check radiation levels in an exhibit space, and <b>data</b> <b>event</b> loggers help determine visible light levels {{over an extended period}} of time. Blue wool standards cards can also be utilized to predict the extent to which materials will be damaged during exhibits. UV radiation must be eliminated to the extent it is physically possible; it is recommended that light with a wavelength below 400 nm (ultraviolet radiation) be limited to no more than 75 microwatts per lumen at 10 to 100 lux. Furthermore, exposure to natural light is undesirable because of its intensity and high UV content. When such exposure is unavoidable, preventative measures must be taken to control UV radiation, including the use of blinds, shades, curtains, UV filtering films, and UV-filtering panels in windows or cases. Artificial light sources are safer options for exhibition. Among these sources, incandescent lamps are most suitable because they emit little or no UV radiation. [...] Fluorescent lamps, common in most institutions, may be used only when they produce a low UV output and when covered with plastic sleeves before exhibition. [...] Though tungsten-halogen lamps are currently a favorite artificial lighting source, they still give off significant amounts of UV radiation; use these only with special UV filters and dimmers. [...] Lights should be lowered or turned off completely when visitors are not in the exhibition space.|$|E
30|$|Test Results (2) Test case (2) {{was carried}} out by {{increasing}} <b>data</b> <b>event</b> size and by adding one access control policy to the broker; results are shown in Fig.  9 a, b, and the horizontal axis is logarithmic (base 10). We make a performance comparison between the pub-to-sub latency with plain and with access control, as well as the broker latency with plain and with access control. For small <b>data</b> <b>event</b> sizes, the pub-to-sub latency and broker latency are low, such as for the 1 KB <b>data</b> <b>event</b> size, and the whole latency event messaging latency takes less than 20  ms (Fig.  9 a); the policy matching latency taken on the broker takes 5  ms (Fig.  9 b). As the <b>data</b> <b>event</b> size becomes larger, the latency is continuous curve. PS-ACF shows the same behaviour as the baseline. As with the pub-to-sub latency and the broker latency, the <b>data</b> <b>event</b> size is one of factors in the overhead.|$|E
25|$|Pervasive AuditMaster {{provides}} real-time auditing of all database interactions, whether Btrieve or SQL. Logs of <b>data</b> <b>events</b> can be queried {{to track}} changes to sensitive data. Alerts {{can also be}} created to notify the appropriate personnel or launch the associated process.|$|R
40|$|Abstract: Wireless sensor {{networks}} have been {{emerged as a}} challenging and unexplored field for researchers worldwide {{in the past few}} years. Wireless sensor {{networks have}} a vast range of practical applications. Wireless technology can able to reach virtually every location {{on the surface of the}} earth. A wireless sensor network is a network of a large number of independently working small sensing units which communicate wirelessly. It may be used for sensing scalar <b>data</b> <b>events</b> like as pressure, acceleration, temperature, proximity, strain etc. It may also be used to sense multimedia <b>data</b> <b>events</b> and transfer of multimedia data like as audio, video, image etc. In this paper, we have discussed the current situation, hardships in implementation & possible solutions...|$|R
40|$|The {{purpose of}} a network {{management}} (NM) system is to monitor and control a network. Monitoring and control functions entail dealing with large volumes of <b>data,</b> <b>events,</b> and the presentation of relevant information on a management station. In this thesis we focus on <b>data</b> and <b>events</b> management and information presentation issues of an NM system. Existing NM systems either use traditional database systems which are not well suited for an NM system or they lack intelligent event and information presentation management frameworks. None of the systems provides a unified framework for managing <b>data,</b> <b>events</b> and information presentation tasks on an NM station. We believe that the complexities of network management can be reduced substantially by exploiting, enhancing and combining the features of new generation database systems such as active temporal and database visualization systems. In this thesis we show that an active database system where active behaviors are specified as Event-Conditio [...] ...|$|R
30|$|Pub-to-sub latency {{refers to}} the total time spent by a <b>data</b> <b>event</b> from its {{publisher}} to its subscriber including the time taken for broker matching.|$|E
3000|$|Global {{standards}} Develop {{and coordinate}} globally standardized open source information and <b>data,</b> <b>event</b> documentation and analysis procedures, guidelines, and frameworks for integrated and effective disaster risk management and sustainable development.|$|E
3000|$|According to the “Little Law”, we can derive the {{throughput}} in Events Per Second (EPS) as [...] "Throughput= 1 /Latency". The pub-to-sub throughput {{results are}} presented based on the average pub-to-sub latencies with or without access control. Figure  12 shows the average sustainable throughput in processing events per second using different event sizes; the horizontal axis is given in base- 10 logarithms. As with pub-to-sub latencies, the <b>data</b> <b>event</b> size is the main factor in the baseline. With <b>data</b> <b>event</b> sizes increasing, pub-to-sub throughput decreases, that is to say, fewer data events per second can be sent from the publisher to the subscriber.|$|E
5000|$|Event-centric: <b>Data</b> <b>events</b> (which {{may have}} {{initially}} originated from a device, application, user, data store or clock) and event detection logic which may conditionally discard the event, initiate an event-related process, alert a user or device manager, or update a data store.|$|R
50|$|Basically, {{there are}} two {{categories}} of data: who is the subject (describing subject characteristics related to the organization, such as socio-demographic-geographic <b>data,</b> <b>events,</b> etc.), and what does the subject do (describing characteristics of subject behavior, product purchase, product usage, payment behavior, relationship instances, etc.).|$|R
5000|$|A [...] "historian", is a {{software}} service within the HMI which accumulates time-stamped <b>data,</b> <b>events,</b> and alarms in a database {{which can be}} queried or used to populate graphic trends in the HMI. The historian is a client that requests data from a data acquisition server.|$|R
40|$|The current raw <b>data</b> <b>event</b> {{model for}} ATLAS is benchmarked for writing and reading per-formances {{as well as}} an {{extension}} of it based on the the use of segmented VArray (SegVAr-ray). SegVArray is a multilevel variable-size array with the same interface as the Objectivi-ty/DB VArray class, but containing ooVArray of SVArraySegments, each of them containing an ooVArray of objects that are the elements of the SegVArray. The advantages compared to the ooVArray class might be manifold as illustrated through the performance benchmarks performed on a toy model as well as the ATLAS raw <b>data</b> <b>event</b> model...|$|E
30|$|Broker latency {{is defined}} as the time spent by a broker in {{receiving}} the published event, performing matching operations against all the requested subscribers and outgoing the <b>data</b> <b>event</b> to the matching subscribers.|$|E
3000|$|... “Analyzing both {{physical}} real world data (heterogeneous data with implicit semantics such as science <b>data,</b> <b>event</b> data, and transportation data) and social data (social media data with explicit semantics) by relating them to each other, is called Social Big Data science or Social Big Data for short”  [22].|$|E
5000|$|The design {{should be}} {{structured}} to degrade gently, even when aberrant <b>data,</b> <b>events,</b> or operating conditions are encountered. Well- designed software should never [...] "bomb"; {{it should be}} designed to accommodate unusual circumstances, and if it must terminate processing, it should {{do so in a}} graceful manner.|$|R
40|$|Decision-making {{laboratory}} {{computer systems}} as essential tools for achievement of total quality Areas {{other than the}} analytical process should {{be the focus of}} concern about quality issues in the laboratory because nearly 95 % of errors occur at the nonanalytical front and back ends of the testing process. Until now, computer systems have been designed to handle the more predictable aspects of laboratory testing, necessitating that the infrequent and unpredictable <b>data</b> <b>events</b> be handled by manual systems. The manual systems are termed “workarounds ” and indeed, because they occur sporadically, they are frequently not handled predictably. Here, I describe and give examples of an expert laboratory computer system that can be designed to handle both predictable and unpredictable <b>data</b> <b>events</b> without the use of manual workarounds. This expert system works in concert with a dynamic database allowing such <b>data</b> <b>events</b> to be detected in real time and handled predictably, thus providing a tool to address quality assurance issues throughout the testing process. The system performs up to 31 separate actions or tasks based on <b>data</b> <b>events</b> that in the past were handled by human workarounds. INDEXING TERMS: expert system • quality control While many would argue that the time, effort, and expense involved in laboratory computerization is justified solely on the ability to do more with less, I suggest that the major motivation for laboratory computerization and automation should be to improve the total quality and predictability of the laboratory service. Quality can be defined as doing the right thing in a timely manner and doing it right the first time. Of course, what is “right” must be defined from the customer’s perspective and expectations. When comparing the efficiency of computers to humans, it is clear that computers can perform repetitiv...|$|R
40|$|<b>Event</b> <b>data</b> from {{different}} parts of a system might be found recorded in event logs. Often the individual logs only show {{a small part of the}} system, but by correlating different sources into a consistent context it will be possible to gain further information and a wider view. This would facilitate in finding source of errors or certain behaviors within the system. This thesis will present the correlation possibilities between <b>event</b> <b>data</b> {{from different}} layers of the Ericsson Connectivity Packet Platform (CPP). This was done first by developing and using a test base application for the OSE operating system through which the <b>event</b> <b>data</b> can be recorded for the same test cases. The log files containing the <b>event</b> <b>data</b> have been studied and results will be presented regarding format, structure and content. For reading and storing the <b>event</b> <b>data,</b> suggestions of interpreters and data models are also provided. Finally a prototype application will be presented, which will provide the defined interpreters, data models and a graphical user interface to represent the <b>event</b> <b>data</b> and <b>event</b> <b>data</b> correlations. The programming was conducted using Java and the application is implemented as an Eclipse Plug-in. With the help of the application the user will get a better overview and a more intuitive way of working with the <b>event</b> <b>data...</b>|$|R
30|$|The dataset {{contains}} data-type events {{rather than}} voice CDRs [6]. Unlike typical Call Detail Records for voice, each <b>data</b> <b>event</b> {{has only one}} assigned tower, {{as there is no}} need for a destination tower. Each event has a size attribute that indicates the number of KiB downloaded since the last registered event.|$|E
40|$|The HiRes {{experiment}} has now completed data taking. We {{describe the}} experiment, the detector and atmo-spheric calibration and studies to improve precision {{of knowledge of}} air-fluorescence efficiency. The current results on the monocular and stereo spectrum and stereo-based composition measurements are summarized. Possible correlation of stereo <b>data</b> <b>event</b> directions with a subset of Bl-Lac objects is discussed...|$|E
3000|$|During the {{sequential}} simulation {{employed by}} SIMPAT, each unknown node u is randomly visited {{and the local}} conditioning <b>data</b> <b>event</b> dev T (u) is recorded simultaneously. Afterward, dev T (u) is compared to all available patterns in the database according to a predefined similarity criterion; {{the most common of}} which are distance functions. The purpose is to find out the most similar training pattern pat* T [...] to the local <b>data</b> <b>event</b> dev T (u). In other words, the algorithm evaluates the distance d for all existing patterns and picks up the pattern with the smallest distance as pat* T [...]. Having selected the most similar pattern, dev T (u) is replaced by pat* T [...] such that part of the values inside pat [...] T [...] * [...] is pasted onto the simulation grid centered at the current location u (Caers and Arpat 2005).|$|E
50|$|Operational {{intelligence}} (OI) is {{a category}} of real-time dynamic, business analytics that delivers visibility and insight into <b>data,</b> streaming <b>events</b> and business operations. OI solutions run queries against streaming <b>data</b> feeds and <b>event</b> <b>data</b> to deliver analytic results as operational instructions. OI provides organizations {{the ability to make}} decisions and immediately act on these analytic insights, through manual or automated actions.|$|R
5000|$|Powerful <b>data</b> triggers, <b>event</b> handlers, {{and push}} notification.|$|R
50|$|The U.S. congressional review {{concluded}} {{medical devices}} would require actual device {{experience in a}} clinical setting and sufficient reporting of adverse <b>data</b> <b>events.</b> The legislation would encompass medical devices demonstrating the potential for life threatening events and accurate adverse data collection would be required for informed regulatory decisions.|$|R
