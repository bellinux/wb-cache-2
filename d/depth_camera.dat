411|446|Public
5000|$|Honorable Mention Demo: Learning to be a <b>Depth</b> <b>Camera,</b> Sean Ryan Fanello, Cem Keskin, Shahram Izadi, Pushmeet Kohli, David Kim, David Sweeney, Antonio Criminisi, Jamie Shotton, Sing Bing Kang, Tim Paek ...|$|E
50|$|Basic patents for TOF-chips {{with new}} Photomischelemente (i.e. Photonic Mixing Elements resp. Devices) were filed in 1996 onwards by Prof. Rudolf Schwarte. A 3D <b>Depth</b> <b>Camera</b> with Time of Flight PMD sensors was {{available}} in the year 2000.|$|E
5000|$|The HoloLens {{features}} an {{inertial measurement unit}} (IMU) (which includes an accelerometer, gyroscope, and a magnetometer) four [...] "environment understanding" [...] sensors (two on each side), an energy-efficient <b>depth</b> <b>camera</b> with a 120°×120° angle of view, a 2.4-megapixel photographic video camera, a four-microphone array, and an ambient light sensor.|$|E
40|$|Abstract: How {{good are}} cheap <b>depth</b> <b>cameras,</b> namely the Microsoft Kinect, {{compared}} to {{state of the}} art Time-of-Flight <b>depth</b> <b>cameras?</b> In this paper several <b>depth</b> <b>cameras</b> of different types were put to the test on a variety of tasks in order to judge their respective performance and to find out their weaknesses. We will concentrate on the common area of applications for which both types are specified, i. e. near field indoor scenes. The characteristics and limitations of the different technologies as well as the concrete hardware implementations are discussed and evaluated with a set of experimental setups. Especially, the noise level and the axial and angular resolutions are compared. Additionally, refined formulas to generate depth values based on the raw measurements of the Kinect are presented. ...|$|R
25|$|Recently some {{researchers}} have used RGBD cameras like Microsoft Kinect to detect human activities. <b>Depth</b> <b>cameras</b> add extra dimension i.e. depth which normal 2d camera fails to provide. Sensory information from these <b>depth</b> <b>cameras</b> {{have been used to}} generate real-time skeleton model of humans with different body positions. These skeleton information provides meaningful information that researchers have used to model human activities which are trained and later used to recognize unknown activities.|$|R
40|$|Kinect-style (or <b>Depth)</b> <b>cameras</b> {{use both}} an RGB and a depth sensor that acquire color and per-pixel depth data (depth-map), respectively. Due to their {{affordable}} price and rich data they provide, <b>depth</b> <b>cameras</b> are being extensively used on research in assistive environments. Most of the robotic and computer-vision systems that use these Kinectstyle cameras require an accurate {{knowledge of the}} cameracalibration parameters. Traditional calibration methods, e. g., thosethatuseachecker-boardpattern, cannotbestraightforwardly used to calibrate the Kinect-style <b>cameras</b> since the <b>depth</b> sensor can not distinguish patterns. Several calibration methods have emerged that try to calibrate <b>depth</b> <b>cameras.</b> In this paper, we present a comparative study {{of some of the}} most important Kinect-sytle calibration algorithms. Our work includes an implementation of these methods along with a comparison of their performance in both simulation and real-world experiments...|$|R
50|$|Motion capture, {{known as}} Mocap, is another iClone {{application}} allowing users to connect any infrared <b>depth</b> <b>camera,</b> or Microsoft Kinect sensor via a USB port {{in order to}} live capture body motions in real-time. These motions can then be exported by using iClone's 3DXchange Pipeline software using FBX and BVH.|$|E
50|$|Yuneec {{released}} the Typhoon H in July 2016. The drone used the Intel RealSense 3D <b>depth</b> <b>camera</b> technology which tracks depth and human motion. The SkyView FPV heaset {{was released in}} August 2016. The headset connects to a drone's onboard camera allowing the user to control the drone from the first person view.|$|E
50|$|Commercially, pose {{estimation}} {{has been}} used in the context of video games, popularized with the Microsoft Kinect sensor (a <b>depth</b> <b>camera).</b> These systems track the user to render their avatar in-game, in addition to performing tasks like gesture recognition to enable the user to interact with the game. As such, this application has a strict real-time requirement.|$|E
40|$|International audienceAn {{increasing}} number of systems use indoor positioning for many scenarios such as asset tracking, health care, games, manufacturing, logistics, shopping, and security. Many technologies are available {{and the use of}} <b>depth</b> <b>cameras</b> {{is becoming more and more}} attractive as this kind of device becomes affordable and easy to handle. This paper contributes to the effort of creating an indoor positioning system based on low cost <b>depth</b> <b>cameras</b> (Kinect). A method is proposed to optimize the calibration of the <b>depth</b> <b>cameras,</b> to describe the multi-camera data fusion and to specify a global positioning projection to maintain the compatibility with outdoor positioning systems. The monitoring of the people trajectories at home is intended for the early detection of a shift in daily activities which highlights disabilities and loss of autonomy. This system is meant to improve homecare health management at home for a better end of life at a sustainable cost for the community...|$|R
40|$|We {{address the}} {{limitation}} of low resolution <b>depth</b> <b>cameras</b> {{in the context of}} face recognition. Considering a face as a surface in 3 -D, we reformulate the recently proposed Upsampling for Precise Super–Resolution algorithm as a new approach on three dimensional points. This reformulation allows an efﬁcient implementation, and leads to a largely enhanced 3 -D face reconstruction. Moreover, combined with a dedicated face detection and representation pipeline, the proposed method provides an improved face recognition system using low resolution <b>depth</b> <b>cameras.</b> We show experimentally that this system increases the face recognition rate as compared to directly using the low resolution raw data...|$|R
3000|$|... [5] won {{the best}} paper award at CVPR 2011 {{for their work}} on human pose {{recognition}} using Kinect, while ICCV 2011 included a workshop on <b>depth</b> <b>cameras,</b> especially focused {{on the use of}} Kinect for computer vision applications.|$|R
5000|$|At MWC 2013, the Japanese company Brilliant Service {{introduced}} the Viking OS, an operating system for HMD's {{which was written}} in Objective-C and relies on gesture control as a primary form of input. It includes a facial recognition system and was demonstrated on a revamp version of Vuzix STAR 1200XL glasses ($4,999) which combined a generic RGB camera and a PMD CamBoard nano <b>depth</b> <b>camera.</b>|$|E
50|$|Proximity {{sensors can be}} used to {{recognise}} air {{gestures and}} hover-manipulations. An array of proximity sensing elements can replace vision-camera or <b>depth</b> <b>camera</b> based solutions for the hand gesture detection. In particular, a car infotainment system (7 inch - 14 inch) in vehicle can employ the proximity sensors to cover the sensing area over the screen. For example, LG Electronics has recently filed several patents addressing this advanced technology.|$|E
5000|$|RoomAlive, {{a related}} Microsoft Research project, also uses a <b>depth</b> <b>camera</b> and video {{projector}} in a projector-camera, or [...] "procam" [...] setup. It is a scalable system for dynamic, real-time interactive projection mapping in which multiple such procams {{can be used}} together in a room to generate an immersive unified projection mapping that is automatically adapted to the room environment, and which users can interact with physically. Unlike IllumiRoom, which implements focus-plus-context visual presentation centered on a television screen, RoomAlive focuses on spatial augmented reality applications.|$|E
40|$|Abstract — Consumer <b>depth</b> <b>cameras,</b> {{such as the}} Microsoft Kinect, {{are capable}} of {{providing}} frames of dense depth values at real time. One fundamental question in utilizing <b>depth</b> <b>cameras</b> is how to best extract features from depth frames. Motivated by local descriptors on images, in particular kernel descriptors, we develop a set of kernel features on depth images that model size, 3 D shape, and depth edges in a single framework. Through extensive experiments on object recognition, we show that (1) our local features capture different aspects of cues from a depth frame/view that complement one another; (2) our kernel features significantly outperform traditional 3 D features (e. g. Spin images); and (3) we significantly improve the capabilities of depth and RGB-D (color+depth) recognition, achieving 10 − 15 % improvement in accuracy over {{the state of the}} art. I...|$|R
40|$|International audienceThis paper {{presents}} a novel method {{to estimate the}} relative poses between RGB and <b>depth</b> <b>cameras</b> without the requirement of an overlapping field of view, thus providing flexibility to calibrate a variety of sensor configurations. This calibration problem is relevant to robotic applications which can benefit of using several cameras to increase the field of view. In our approach, we extract and match lines of {{the scene in the}} RGB and <b>depth</b> <b>cameras,</b> and impose geometric constraints to find the relative poses between the sensors. An analysis of the observability properties of the problem is presented. We have validated our method in both synthetic and real scenarios with different camera configurations, demonstrating that our approach achieves good accuracy and is very simple to apply, in contrast with previous methods based on trajectory matching using visual odometry or SLAM...|$|R
40|$|Depth sensing {{devices have}} created various new {{applications}} in scientific and commercial {{research with the}} advent of Microsoft Kinect and PMD (Photon Mixing Device) cameras. Most of these applications require the <b>depth</b> <b>cameras</b> to be pre-calibrated. However, traditional calibration methods using a checkerboard do not work very well for <b>depth</b> <b>cameras</b> due to the low image resolution. In this paper, we propose a depth calibration scheme which excels in estimating camera calibration parameters when only a handful of corners and calibration images are available. We exploit the noise properties of PMD devices to denoise depth measurements and perform camera calibration using the denoised depth as an additional set of measurements. Our synthetic and real experiments show that our depth denoising and depth based calibration scheme provides significantly better results than traditional calibration methods. Comment: 5 pages, 3 figures, conferenc...|$|R
5000|$|Optical methods {{represent}} {{a set of}} computer vision algorithms and tracking devices such as a camera of visible or infrared range, a stereo camera and a <b>depth</b> <b>camera.</b> Optical tracking {{is based on the}} same principle as stereoscopic human vision. When a person looks at an object using binocular vision, he is able to define approximately at what distance the object is placed.Not enough just to install a pair of cameras to simulate stereoscopic vision of a person. Cameras have to determine the distance to the object and its position in space, so it’s necessary to calibrate. Infants learn to calibrate their vision when they try to take something, correlating the location of the object with outstretched hand.Optical systems are reliable and relatively non-expensive but it’s difficult to calibrate. Furthermore, the system requires a direct line of light without occlusions, otherwise we receive wrong data.There are two approaches: ...|$|E
40|$|Abstract—In this paper, {{we present}} an {{integrated}} approach for robot localization, obstacle mapping, and path planning in 3 D environments {{based on data}} of an onboard consumerlevel <b>depth</b> <b>camera.</b> We rely on state-of-the-art techniques for environment modeling and localization, which we extend for <b>depth</b> <b>camera</b> data. We thoroughly evaluated our system with a Nao humanoid equipped with an Asus Xtion Pro Live <b>depth</b> <b>camera</b> {{on top of the}} humanoid’s head and present navigation experiments in a multi-level environment containing static and non-static obstacles. Our approach performs in real-time, maintains a 3 D environment representation, and estimates the robot’s pose in 6 D. As our results demonstrate, the <b>depth</b> <b>camera</b> is well-suited for robust localization and reliable obstacle avoidance in complex indoor environments. I...|$|E
40|$|Virtual view {{synthesis}} from {{an array}} of cameras has been {{an essential element of}} three-dimensional video broadcasting/conferencing. In this paper, we propose a scheme based on a hybrid camera array consisting of four regular video cameras and one time-of-flight <b>depth</b> <b>camera.</b> During rendering, we use the depth image from the <b>depth</b> <b>camera</b> as initialization, and compute a view-dependent scene geometry using constrained plane sweeping from the regular cameras. View-dependent texture mapping is then deployed to render the scene at the desired virtual viewpoint. Experimental results show that the addition of the time-of-flight <b>depth</b> <b>camera</b> greatly improves the rendering quality compared with {{an array of}} regular cameras with similar sparsity. In the application of 3 D video boardcasting/conferencing, our hybrid camera system demonstrates great potential in reducing the amount of data for compression/streaming while maintaining high rendering quality. ...|$|E
40|$|We {{address the}} issue of {{improving}} depth coverage in consumer <b>depth</b> <b>cameras</b> based on the combined use of cross-spectral stereo and near infra-red structured light sensing. Specifically we show that fusion of disparity over these modalities, within the disparity space image, prior to disparity optimization facilitates the recovery of scene depth information in regions where structured light sensing fails. We show that this joint approach, leveraging disparity information from both structured light and cross-spectral sensing, facilitates the joint recovery of global scene depth comprising both texture-less object depth, where conventional stereo otherwise fails, and highly reflective object depth, where structured light (and similar) active sensing commonly fails. The proposed solution is illustrated using dense gradient feature matching and shown to outperform prior approaches that use late-stage fused cross-spectral stereo depth as a facet of improved sensing for consumer <b>depth</b> <b>cameras...</b>|$|R
40|$|<b>Depth</b> sensing <b>camera</b> Tracks 20 body joints in {{real time}} Recognises your face and voicedepth image (camera view) What the Kinect SeesWhat the Kinect Sees top view side view <b>depth</b> image (<b>camera</b> view) Structured light object at depth d 2 object at depth d 1 y z x imaging plane optic centre of IR laser optic centre of camer...|$|R
40|$|Sign {{language}} {{recognition is}} a difficult task, yet required for many applications in real-time speed. Using RGB cameras for recognition of sign languages is not very successful in practical situations and accurate 3 D imaging requires expensive and complex instruments. With introduction of Time-of-Flight (ToF) <b>depth</b> <b>cameras</b> in recent years, it has become easier to scan the environment for accurate, yet fast depth images of the objects without the need of any extra calibrating object. In this paper, a robust system for sign language recognition using ToF <b>depth</b> <b>cameras</b> is presented for converting the recorded signs to a standard and portable XML sign language named SiGML for easy transferring and converting to real-time 3 D virtual characters animations. Feature extraction using moments and classification using nearest neighbor classifier are used to track hand gestures and significant result of 100 % is achieved for the proposed approach. Comment: 6 Page...|$|R
40|$|Recently, depth cameras {{have emerged}} which capture dense depth images in real-time. To benefit from their 3 D imaging {{capabilities}} in interactive applications which support an arbitrary camera movement, {{the position and}} orientation of the <b>depth</b> <b>camera</b> needs to be robustly estimated in realtime for each captured depth image. Therefore, this paper describes how to combine a <b>depth</b> <b>camera</b> with a mechanical measurement arm to fuse real-time depth imaging with realtime, high precision pose estimation. Estimating the pose of a <b>depth</b> <b>camera</b> with a measurement arm has three major advantages over 2 D/ 3 D image based optical pose estimation: The measurement arm has a very precise guaranteed accuracy better than 0. 1 mm, the pose estimation accuracy is not influenced by the captured scene and the computational load is much lower than for optical pose estimation, leaving more processing power for the applications themselves...|$|E
40|$|A {{new method}} is {{proposed}} {{for using a}} combination of measurements from a laser range finder and a <b>depth</b> <b>camera</b> in a data fusion process that benefits from each modality's strong side. The combination leads to a significantly improved performance of the human detection and tracking in comparison with what is achievable from the singular modalities. The useful information from both laser and <b>depth</b> <b>camera</b> is automatically extracted and combined in a Bayesian formulation that is estimated using a Markov Chain Monte Carlo (MCMC) sampling framework. The experiments show that this algorithm can track robustly multiple people in real world assistive robotics applications...|$|E
40|$|This paper {{presents}} {{a novel approach}} to reconstruct complete 3 D deformable models over time by a single <b>depth</b> <b>camera.</b> These are the steps employed for deforming objects from single <b>depth</b> <b>camera.</b> The partial surfaces reconstructed from various times of capture are assembled {{together to form a}} complete 3 D surface. A mesh warping algorithm is used to align different partial surfaces based on linear mesh deformation. A volumetric method is then applied to combine partial surfaces, fix missing holes and smooth alignment errors. Comment: arXiv admin note: submission has been withdrawn by arXiv administrators due to inappropriate text reuse from external source...|$|E
40|$|Since the {{introduction}} of the Microsoft Kinect in November 2010, low cost consumer <b>depth</b> <b>cameras</b> have rapidly increased in popularity. Their integral technology provides a means of low cost 3 D scanning, extending its accessibility to a far wider audience. Previous work has shown the 3 D data from consumer <b>depth</b> <b>cameras</b> to exhibit fundamental measurement errors: likely due to their low cost and original intended application. A number of techniques to correct the errors are presented in the literature, but are typically device specific, or rely on specific open source drivers. Presented here is a simple method of calibrating consumer <b>depth</b> <b>cameras,</b> relying only on 3 D scans of a plane filling the field of view: thereby compatible with any device capable of providing 3 D point cloud data. Validation of the technique using a Microsoft Kinect sensor has shown non planarity errors to reduce to around ± 3 mm: nearing the device’s resolution. Further validation based on circumference measures of a cylindrical object has shown a variable error of up to 45 mm to reduce to a systematic overestimation of 10 mm, based on a 113 mm diameter cylinder. Further work is required to test the proposed method on objects of greater complexity and over greater distances. However, this initial work suggests great potential for a simple method of reducing the error apparent in the 3 D data from consumer depth cameras: possibly increasing their suitability for a number of applications...|$|R
40|$|Abstract. This paper proposes an {{adaptive}} color-guided auto-regressive (AR) model for high quality depth recovery from low quality measure-ments captured by <b>depth</b> <b>cameras.</b> We formulate the depth recovery task into a minimization of AR prediction errors subject to measurement con-sistency. The AR predictor for each pixel is constructed according {{to both the}} local correlation in the initial depth map and the nonlocal similarity in the accompanied high quality color image. Experimental results show that our method outperforms existing state-of-the-art schemes, and is versatile for both mainstream <b>depth</b> sensors: ToF <b>camera</b> and Kinect...|$|R
40|$|Human action {{recognition}} {{is an important}} yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the <b>depth</b> <b>cameras</b> are very noisy and the 3 D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity <b>depth</b> <b>cameras,</b> and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance {{to the state of}} the art algorithms. 1...|$|R
40|$|In {{this paper}} {{we present a}} depth-guided {{photometric}} 3 D reconstruction method that works solely with a depth cam-era like the Kinect. Existing methods that fuse depth with normal estimates use an external RGB camera to obtain photometric information and treat the <b>depth</b> <b>camera</b> as a black box that provides a low quality depth estimate. Our contribution to such methods are two fold. Firstly, instead of using an extra RGB camera, we use the infra-red (IR) cam-era of the <b>depth</b> <b>camera</b> system itself to directly obtain high resolution photometric information. We believe that ours is the first method to use an IR <b>depth</b> <b>camera</b> system in this manner. Secondly, photometric methods applied to complex objects result in numerous holes in the reconstructed sur-face due to shadows and self-occlusions. To mitigate this problem, we develop a simple and effective multiview recon-struction approach that fuses depth and normal information from multiple viewpoints to build a complete, consistent and accurate 3 D surface representation. We demonstrate the ef-ficacy of our method to generate high quality 3 D surface reconstructions for some complex 3 D figurines. 1...|$|E
40|$|In {{order to}} {{optimize}} the three-dimensional (3 D) reconstruction and obtain more precise actual distances of the object, a 3 D reconstruction system combining binocular and depth cameras is proposed in this paper. The whole system consists of two identical color cameras, a TOF <b>depth</b> <b>camera,</b> an image processing host, a mobile robot control host, and a mobile robot. Because of structural constraints, the resolution of TOF <b>depth</b> <b>camera</b> is very low, which difficultly meets the requirement of trajectory planning. The resolution of binocular stereo cameras can be very high, but the effect of stereo matching is not ideal for low-texture scenes. Hence binocular stereo cameras also difficultly {{meet the requirements of}} high accuracy. In this paper, the proposed system integrates <b>depth</b> <b>camera</b> and stereo matching to improve the precision of the 3 D reconstruction. Moreover, a double threads processing method is applied to improve the efficiency of the system. The experimental results show that the system can effectively improve the accuracy of 3 D reconstruction, identify the distance from the camera accurately, and achieve the strategy of trajectory planning...|$|E
30|$|The {{use of a}} <b>depth</b> <b>camera</b> {{to provide}} visual {{biofeedback}} increased the reproducibility of breath-hold expiration level in healthy volunteers, with a potential to eliminate targeting errors caused by respiratory movement during lung image-guided procedures.|$|E
40|$|This paper {{proposes a}} depth {{measurement}} error model of consumer <b>depth</b> <b>cameras</b> such as Microsoft KINECT, and its calibration method. These devices are originally designed for video game interface, thus, the obtained depth map {{are not enough}} accurate for 3 D measurement. To decrease these depth errors, several models have been proposed, however, these models con-sider only camera-related parameters. Since the depth sensors are based on projector-camera systems, we should consider projector-related parameters. There-fore, we propose the error model of the consumer <b>depth</b> <b>cameras</b> especially the KINECT, considering both in-trinsic parameters of the camera and the projector. To calibrate the error model, we also propose the parame-ter estimation method by only showing a planar board to the depth sensors. Our error model and its calibra-tion are necessary step for using the KINECT as a 3 D measuring device. Experimental results show the valid-ity and effectiveness of the error model and its calibra-tion. ...|$|R
40|$|Figure 1 : Our system {{automatically}} {{and accurately}} reconstructs full-body kinematics and dynamics data using input data captured by three <b>depth</b> <b>cameras</b> {{and a pair}} of pressure-sensing shoes. (top) reference image data; (bottom) the reconstructed full-body poses and contact forces (red arrows) and torsional torques (yellow arrows) applied at the center of pressure. We present a new method for full-body motion capture that uses input data captured by three <b>depth</b> <b>cameras</b> {{and a pair of}} pressure-sensing shoes. Our system is appealing because it is low-cost, non-intrusive and fully automatic, and can accurately reconstruct both full-body kinematics and dynamics data. We first introduce a novel tracking process that automatically reconstructs 3 D skeletal poses using input data captured by three Kinect cameras and wear-able pressure sensors. We formulate the problem in an optimiza-tion framework and incrementally update 3 D skeletal poses with observed depth data and pressure data via iterative linear solvers. The system is highly accurate because we integrate depth data from multiple <b>depth</b> <b>cameras,</b> foot pressure data, detailed full-body ge-ometry, and environmental contact constraints into a unified frame-work. In addition, we develop an efficient physics-based motion reconstruction algorithm for solving internal joint torques and con-tact forces in the quadratic programming framework. During re-construction, we leverage Newtonian physics, friction cone con-straints, contact pressure information, and 3 D kinematic poses ob-tained from the kinematic tracking process to reconstruct full-body dynamics data. We demonstrate the power of our approach by cap-turing a wide range of human movements and achieve state-of-the-art accuracy in our comparison against alternative systems...|$|R
30|$|It {{should be}} noted that most of {{consumer}} <b>depth</b> <b>cameras</b> are developed for indoor usages. They project IR patterns into an environment to measure the depth information. It is difficult to detect emitted IR patterns in outdoor environments. In addition, there is a limitation of a range of depth measurement such that the RGB-D sensors can capture the environment from 1 to 4 m.|$|R
