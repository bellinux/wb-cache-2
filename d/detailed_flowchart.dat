13|27|Public
5000|$|However, {{there are}} several of these classifications. For example, Andrew Veronis (1978) named three basic types of flowcharts: the system flowchart, the general flowchart, and the <b>detailed</b> <b>flowchart.</b> That same year Marilyn Bohl (1978) stated [...] "in practice, two kinds of flowcharts are used in {{solution}} planning: system flowcharts and program flowcharts...". More recently Mark A. Fryman (2001) stated {{that there are more}} differences: [...] "Decision flowcharts, logic flowcharts, systems flowcharts, product flowcharts, and process flowcharts {{are just a few of}} the different types of flowcharts that are used in business and government".|$|E
5000|$|The album's {{front cover}} {{made use of}} a David Gahr {{photograph}} featuring the last line-up of The Byrds to be represented on the album: Roger McGuinn, Skip Battin, Gene Parsons, and Clarence White. [...] The same photograph had already been used for {{the cover of the}} U.S. compilation album The Best of The Byrds: Greatest Hits, Volume II just six months earlier. [...] The back cover included liner notes by Kim Fowley and the inside gatefold sleeve of the double vinyl LP featured Pete Frame's [...] "Byrds Family Tree". [...] This intricately <b>detailed</b> <b>flowchart</b> traced the group's roots and complicated membership history over the years. The very first pressing of this double LP mentioned The Byrds English fan club's (run by Chrissie Oakes) name and address {{in the middle of the}} family tree.|$|E
40|$|This paper {{studies the}} dynamic {{modeling}} of a nonholonomic mobile robotic manipulator {{that consists of}} a serial manipulator and an autonomous wheeled mobile platform. Variational-Vector Calculus Method and Forward Recursive Formulation for dynamics of multibody systems were employed to establish the governing equation of the mobile manipulator system. The resulting dynamic equation is in state-space presentation {{and can be used}} for the purpose of simulation and control design. The <b>detailed</b> <b>flowchart</b> of the simulation implementation is illustrated. 1...|$|E
40|$|This paper {{describes}} {{previous research}} on flowcharts {{and a series of}} controlled experiments to test the utility of <b>detailed</b> <b>flowcharts</b> as an aid to program composition, comprehension, debugging, and modification. No statistically significant difference between flowchart and nonflowchart groups has been shown, thereby calling into question the utility of <b>detailed</b> <b>flowcharting.</b> A program of further research is suggested...|$|R
50|$|Although our {{original}} {{intention was to}} ascertain under which conditions <b>detailed</b> <b>flowcharts</b> were most helpful, our repeated negative results have led us to a more skeptical opinion of the utility of <b>detailed</b> <b>flowcharts</b> under modern programming conditions. We repeatedly selected problems and tried to create test conditions which would favor the flowchart groups, but found {{no statistically significant differences}} between the flowchart and non-flowchart groups. In some cases the mean scores for the non-flowchart groups even surpassed the means for the flowchart groups. We conjecture that <b>detailed</b> <b>flowcharts</b> are merely a redundant presentation of the information contained in the programming language statements. The flowcharts may even be at a disadvantage because they are not as complete (omitting declarations, statement labels, and input/output formats) and require many more pages than do the concise programming language statements.|$|R
5000|$|In the 1970s Shneiderman {{continued}} to study programmers, {{and the use}} of flow charts. In the 1977 article [...] "Experimental investigations of the utility of <b>detailed</b> <b>flowcharts</b> in programming" [...] Shneiderman et al. summarized the origin and status quo of flowcharts in computer programming: ...|$|R
40|$|A {{multiple}} user {{version of}} AMTRAn was implemented on the Datacraft DC 6024 computer is reported. The {{major portion of}} the multiple user logic is incorporated in the main program which remains in core during all AMTRAN processes. A <b>detailed</b> <b>flowchart</b> of the main program is provided as documentation of the multiple user capability. Activities are directed toward perfecting its capability, providing new features in response to user needs and requests, providing a two-dimensional array AMTRAN containing multiple user logic, and providing documentation as the tasks progress...|$|E
30|$|In the {{conventional}} unidirectional Fano algorithm, the decoder starts decoding {{from the initial}} state zero (or origin node). During each iteration of the algorithm, the decoder may move forward (increase depth within the tree), move backward (reduce depth), or stay at the current tree depth. The decision is made based on the comparison between the threshold value and the path metric. If a forward movement is made, the threshold value needs to be tightened. If the decoder cannot move forward or backward, the threshold value needs to be loosened. A <b>detailed</b> <b>flowchart</b> of the UFA {{can be found in}} [11].|$|E
40|$|Adaptive Hypermedia Systems (AHS) {{have long}} been mainly {{represented}} by domain- or application-specific systems. Few reference models exist and they provide only {{a brief overview of}} how to describe and organize the ‘adaptation process’ in a generic way. In this paper we consider the process aspects of AHS from the very first classical ‘user modelling-adaptation’ loop to a generic <b>detailed</b> <b>flowchart</b> of the adaptation in AHS. We introduce a Generic Adaptation Process and by aligning it with a layered (data-oriented) AHS architecture we show that it can serve as the process part of a new reference model for AHS...|$|E
40|$|In this study, a {{computer}} software for two-dimensional wave propagation and runup over arbitrary bottom topography, was documented, and {{made available for}} use in the public domain. This software was developed {{over the past five years}} by Grilli et al. 31, 37, based on fully nonlinear potential flow equations, and although well validated as a research tool, it was not available in a form allowing easy use by others (see, e. g., 35, 36, 38). After an introduction covering theoretical aspects of the problem, numerical algorithms used in the solution are presented (Boundary Element Method, time updating), as well as <b>detailed</b> <b>flowcharts</b> for the software. A user's manual is finally provided, giving step by step instructions on how to use the software, along with a few typical applications of the program, that can be used for training and verification. The source code for the software (FORTRAN 77) has also been documented as part of this project. Each of the 64 subroutines and functions i [...] ...|$|R
50|$|Storyboards {{also exist}} in {{accounting}} in the ABC System Activity - Based Costing (ABC) {{to develop a}} <b>detailed</b> process <b>flowchart</b> which visually shows all activities and the relationships among activities. They are used in this way to measure the cost of resources consumed, identify and eliminate non-value-added costs, determine the efficiency and effectiveness of all major activities, and identity and evaluate new activities that can improve future performance.|$|R
40|$|The Hybrid Air Revitalization System (HARS) closed {{ecosystem}} concept presented encompasses electrochemical CO 2 and O 2 separators, {{in conjunction}} with a mechanical condenser/separator for maintaining CO 2, O 2, and humidity levels in crew and plant habitats at optimal conditions. HARS requires no expendables, and allows flexible process control on the bases of electrochemical cell current, temperature, and airflow rate variations. HARS capacity can be easily increased through the incorporation of additional chemical cells. <b>Detailed</b> system <b>flowcharts</b> are provided...|$|R
40|$|Background and objectives: Considering the {{importance}} of process improvement and support system,we tried {{to take a step}} to clarify logistics processes and initiate quality improvement in health and nutrition faculty of Tabriz Medical Science University using process mapping and decision making matrix. Material and Methods: This study is qualitative and was done in 2012. The data were gathered by interview with those involving in the process and direct observation. In this study we clarified logistics processes of financial and administration deputy using block diagram and <b>detailed</b> <b>flowchart.</b> After clarifying the process, research team analyzed the data and proposed an improved way for process as a suggestive <b>detailed</b> <b>flowchart.</b> At the next stage we compared the processes in decision making matrix and finally the best option for intervention was chose between 10 worst functioned processes in decision making matrix of prioritization. Results: In this study we documented 35 processes using process mapping in general affairs, personnel affairs, secretariat, archive deputy, storage department, accounting, properties, services, supply and financial department. The accounting documenting process had the worst function according to comparison matrix and the purchase and supply process was selected as the best option of intervention. Conclusion: The research's results showed that most of the processes in this deputy have problems in theory and practice, and system improvement is in need of reforming which will improve quality and prevent organization sources to be wasted. Due to the multi department function of processes, the unity of departments and a qualified management is required for better reform...|$|E
40|$|Background and {{objectives}} : Considering {{the importance of}} process improvement and support system, we tried {{to take a step}} to clarify logistics’ processes and initiate quality improvement in health and nutrition faculty of Tabriz University of Medical Sciences using process mapping and decision making matrix. Material and Methods : This study is qualitative and was conducted in 2012. The data were gathered by interview with the participation of the individuals involving in the process and researchers’ direct observation. In this study, we clarified logistics’ processes of financial and administration deputy using block diagram and <b>detailed</b> <b>flowchart.</b> After clarifying the process, research team analyzed the data and proposed an improved way for process as a suggestive <b>detailed</b> <b>flowchart.</b> At the next stage, we compared the processes in decision making matrix and finally, the best option for intervention was chose among 10 worst functioning processes in decision making matrix of prioritization. Results : In this study, 35 processes were documented using process mapping in general affairs, personnel affairs, secretariat, archive deputy, storage department, accounting, properties, services, supply and financial department. The accounting documenting process had the worst function according to comparison matrix and the purchase and supply process was selected as the best option of intervention. Conclusion : The results of this study showed that most of the processes in this deputy have problems in theory and practice and system improvement is in need of reforming which will improve quality and prevent organization sources to be wasted. Due to the multi-department function of the processes, the unity of departments and a qualified management is required for better reform...|$|E
40|$|This study {{addresses}} the in-detail steps {{to create a}} chaotic oscillator having continuous-time equations using a microcontroller hardware which has a lower clock-frequency and narrower data bus, as well as much lower hardware, software and algorithm development costs compared to chaotic oscillators developed using analog circuit components or a hardware-based software platform such as FPGA. For this purpose, a Lorenz chaotic oscillator with continuous-time nonlinear equations was selected. Lorenz t-domain equations were transformed into S-domain and Z-domain respectively. After these transformations, a <b>detailed</b> <b>flowchart</b> was given to illustrate the steps required to implement the chaotic oscillator in the microcontroller. All the details derived were simulated by running simultaneous MATLAB-SIMULINK simulations. And, {{the performance of the}} discrete-time chaotic oscillator executed in the PIC 18 F 452 microcontroller produced by the Microchip Technology Inc. was visualized by 1 D and 2 D graphs on an oscilloscope screen...|$|E
50|$|Six Sigma {{practitioners}} use {{the term}} Business Process Architecture to describe the mapping of business processes as series of cross-functional flowcharts. Under this school of thought, each flowchart is of a certain level (between 0 and 4) based {{on the amount of}} <b>detail</b> the <b>flowchart</b> contains. A level 0 flowchart represents the least amount of detail, and usually contains one or two steps. A level 4 flowchart represents the most amount of detail, and can include hundreds of steps. At this level every task, however minor, is represented.|$|R
40|$|This {{textbook}} {{provides a}} comprehensive introduction to nature-inspired metaheuristic methods for search and optimization, including the latest trends in evolutionary algorithms {{and other forms}} of natural computing. Over 100 different types of these methods are discussed in detail. The authors emphasize non-standard optimization problems and utilize a natural approach to the topic, moving from basic notions to more complex ones. An introductory chapter covers the necessary biological and mathematical backgrounds for understanding the main material. Subsequent chapters then explore almost all of the major metaheuristics for search and optimization created based on natural phenomena, including simulated annealing, recurrent neural networks, genetic algorithms and genetic programming, differential evolution, memetic algorithms, particle swarm optimization, artificial immune systems, ant colony optimization, tabu search and scatter search, bee and bacteria foraging algorithms, harmony search, biomolecular computing, quantum computing, and many others. General topics on dynamic, multimodal, constrained, and multiobjective optimizations are also described. Each chapter includes <b>detailed</b> <b>flowcharts</b> that illustrate specific algorithms and exercises that reinforce important topics. Introduced in the appendix are some benchmarks for the evaluation of metaheuristics. Search and Optimization by Metaheuristics is intended primarily as a textbook for graduate and advanced undergraduate students specializing in engineering and computer science. It will also serve as a valuable resource for scientists and researchers working in these areas, {{as well as those who}} are interested in search and optimization methods...|$|R
50|$|Starting {{at the end}} of {{the nineteenth}} century, well before the advent of {{electronic}} computers, data processing was performed using electromechanical machines called unit record equipment, electric accounting machines (EAM) or tabulating machines.Unit record machines came to be as ubiquitous in industry and government in the first two-thirds of the twentieth century as computers became in the last third. They allowed large volume, sophisticated data-processing tasks to be accomplished before electronic computers were invented and while they were still in their infancy. This data processing was accomplished by processing punched cards through various unit record machines in a carefully choreographed progression. This progression, or flow, from machine to machine was often planned and documented with <b>detailed</b> <b>flowcharts</b> that used standardized symbols for documents and the various machine functions. All but the earliest machines had high-speed mechanical feeders to process cards at rates from around 100 to 2,000 per minute, sensing punched holes with mechanical, electrical, or, later, optical sensors. The operation of many machines was directed by the use of a removable plugboard, control panel, or connection box. Initially all machines were manual or electromechanical. The first use of an electronic component was in 1940 when a gas triode vacuum tube replaced a relay in an IBM card sorter. Electronic components were used on other machines beginning in the late 1940s.|$|R
40|$|In the {{following}} paper, geovisualisation {{will be applied}} to one spatial phenomenon and understood {{as a process of}} creating complementary visualisations: static two-dimensional, surface three-dimensional, and interactive. The central challenge that the researchers faced was to find a method of presenting the phenomenon in a multi-faceted way. The main objective of the four-stage study was to show the capacity of the contemporary software for presenting geographical space from various perspectives while maintaining the standards of cartographic presentation and making sure that the form remains attractive for the user. The correctness, effectiveness, and usefulness of the proposed approach was analysed {{on the basis of a}} geovisualisation of natural aggregate extraction in the Gniezno district in the years 2005 – 2015. For each of the three visualisations, the researchers planned a different range of information, different forms of graphic and cartographic presentation, different use and function, but as far as possible the same accessible databases and the same free technologies. On the basis of the final publication, the researchers pointed out the advantages of the proposed workflow and the correctness of the <b>detailed</b> <b>flowchart...</b>|$|E
40|$|The finite-difference time-domain (FDTD) {{method has}} been {{commonly}} utilized in the numerical solution of electromagnetic (EM) waves propagation through the plasma media. However, the FDTD method may {{bring about a}} significant increment in additional run-times consuming for computationally large and complicated EM problems. Graphics Processing Unit (GPU) computing based on Compute Unified Device Architecture (CUDA) has grown in response to increased concern for reduction of run-times. We represent the CUDA-based FDTD method with the Runge-Kutta exponential time differencing scheme (RKETD) for the unmagnetized plasma implemented on GPU. In the paper, we derive the RKETD-FDTD formulation for the unmagnetized plasma comprehensively, and describe the <b>detailed</b> <b>flowchart</b> of CUDA-implemented RKETD-FDTD method on GPU. The accuracy and acceleration performance of the posed CUDA-based RKETD-FDTD method implemented on GPU are substantiated by the numerical experiment that simulates the EM waves traveling through the unmagnetized plasma slab, compared with merely CPU-based RKETD-FDTD method. The accuracy is validated by calculating the reflection and transmission coefficients for one-dimensional unmagnetized plasma slab. Comparison between the elapsed times of two methods proves that the GPU-based RKETD-FDTD method can acquire better application acceleration performance with sufficient accuracy. Comment: 8 pages, 9 figure...|$|E
40|$|With {{advances}} in microelectronic and wireless communication technologies, smartphones have computer-like capabilities {{in terms of}} computing power and communication bandwidth. They allow users to use advanced applications {{that used to be}} run on computers only. Web browsing, email fetching, gaming, social networking, and multimedia streaming are examples of wide-spread smartphone applications. Unsurprisingly, network-related applications are dominant in the realm of smartphones. Users love to be connected while they are mobile. Streaming applications, as a part of network-related applications, are getting increasingly popular. Mobile TV, video on demand, and video sharing are some popular streaming services in the mobile world. Thus, the expected operational time of smartphones is rising rapidly. On the other hand, the enormous growth of smartphone applications and services adds up to a significant increase in complexity in the context of computation and communication needs, and thus there is a growing demand for energy in smartphones. Unlike the exponential growth in computing and communication technologies, the growth in battery technologies is not keeping up with the rapidly growing energy demand of these devices. Therefore, the smartphone's utility has been severely constrained by its limited battery lifetime. It is very important to conserve the smartphone's battery power. Even though hardware components are the actual energy consumers, software applications utilize the hardware components through the operating system. Thus, by making smartphone applications energy-efficient, the battery lifetime can be extended. With this view, this work focuses on two main problems: i) developing an energy testing methodology for smartphone applications, and ii) evaluating the energy cost and designing an energy-friendly downloader for smartphone streaming applications. The detailed contributions of this thesis are as follows: (i) it gives a generalized framework for energy performance testing and shows a <b>detailed</b> <b>flowchart</b> that application developers can easily follow to test their applications; (ii) it evaluates the energy cost of some popular streaming applications showing how the download strategy that an application developer adopts may adversely affect the energy savings; (iii) it develops a model of an energy-friendly downloader for streaming applications and studies the effects of the downloader's parameters regarding energy consumption; and finally, (iv) it gives a mathematical model for the proposed downloader and validates it by means of experiments...|$|E
40|$|In {{the past}} decade, {{proteomics}} and mass spectrometry have taken tremendous strides forward, {{particularly in the}} life sciences, spurred on by rapid advances in technology resulting in generation and conglomeration of vast amounts of data. Though {{this has led to}} tremendous advancements in biology, the interpretation of the data poses serious challenges for many practitioners due to the immense size and complexity of the data. Furthermore, the lack of annotation means that a potential gold mine of relevant biological information may be hiding within this data. We present here a simple and intuitive workflow for the research community to investigate and mine this data, not only to extract relevant data but also to segregate usable, quality data to develop hypotheses for investigation and validation. We apply an MS evidence workflow for verifying peptides of proteins from one’s own data as well as publicly available databases. We then integrate a suite of freely available bioinformatics analysis and annotation software tools to identify homologues and map putative functional signatures, gene ontology and biochemical pathways. We also provide an example of the functional annotation of missing proteins in human chromosome 7 data from the NeXtProt database, where no evidence is available at the proteomic, antibody, or structural levels. We give examples of protocols, tools and <b>detailed</b> <b>flowcharts</b> that can be extended or tailored to interpret and annotate the proteome of any novel organism. 14 page(s...|$|R
40|$|The image {{reconstruction}} {{process in}} super-resolution structured illumination microscopy (SIM) is investigated. The structured pattern {{is generated by}} the interference of two Gaussian beams to encode undetectable spectra into detectable region of microscope. After parameters estimation of the structured pattern, the encoded spectra are computationally decoded and recombined in Fourier domain to equivalently increase the cut-off frequency of microscope, resulting in the extension of detectable spectra and a reconstructed image with about two-fold enhanced resolution. Three different methods to estimate the initial phase of structured pattern are compared, verifying the auto-correlation algorithm affords the fast, most precise and robust measurement. The artifacts sources and <b>detailed</b> reconstruction <b>flowchart</b> for both linear and nonlinear SIM are also presented...|$|R
40|$|This paper {{deals with}} the {{application}} and development of a systematic methodology called Task Analysis {{which is based on}} the analytical investigation of the task allocation processes and bottlenecks in terms of work system goals, in order to evaluate synergy between worker's essential motions and mental activities of different functional levels which contributes to conduct worker's adaptive behavioral performances during the execution of production operation. A comprehensive consideration of adopting this approach to analyze some key behavioral factors in work system design is expanded to acquire consecutive work performance feedback, determine the instructional work goals, describe the <b>detailed</b> work <b>flowchart,</b> structure the clear interaction assessment, improve the standard procedures, and supply the useful criteria. © 2011 Springer-Verlag Berlin Heidelberg. link_to_subscribed_fulltex...|$|R
40|$|Sustainable {{resource}} management and development {{have been at}} the forefront of important issues concerning the construction industry for the past several years. Specifically, the use of sustainable building materials and the reuse and recycling of previously used building materials is gaining acceptance and becoming common place in many areas. As one of the most commonly used building materials in the world, concrete, composed of aggregate, sand, cement and water, can be recycled and reused in a variety of applications. Using crushed concrete as fill and subgrade material under roads, sidewalks and foundations has been the most common of these applications. However, research has been ongoing over the past 50 years in many countries including Germany, Canada, Japan, the United States, China, and Australia investigating the use of crushed concrete from demolished old concrete structures to fully or partially replace the virgin aggregate used to produce new concrete for use in building and pavement applications. Producing concrete using recycled concrete aggregates (RCAs) has several advantages, namely, the burden placed on non-renewable aggregate resources may be significantly decreased, the service life and capacity of landfill and waste management facilities can be extended, and the carbon dioxide emissions and traffic congestion associated with the transport of virgin aggregates from remote sites can be reduced. This research is directed at benchmarking typical RCA sources for usage in structural concrete and investigating the inter-relationships between aggregate properties, concrete properties and the bond properties between reinforcing steel and RCA concrete. The experimental program focused on four main areas: aggregate properties testing, development of concrete mixture proportions, concrete fresh and hardened properties testing, and beam-end bond testing. Four coarse aggregate sources were investigated including one virgin or natural aggregate (NA) source, and three RCA sources. Two RCA sources were derived from the crushing of decommissioned building and pavement structures (RCA- 1 and RCA- 2) while the third source was derived from the crushing of returned ready-mix concrete (RCA- 3). A variety of typical and non-typical aggregate tests were performed to provide a basis for correlation with fresh and hardened concrete properties results. A total of 24 concrete mixtures were developed and divided into three separate categories, 1) control, 2) direct replacement, and 3) strength-based mixtures. The control mixtures were proportioned to achieve compressive strengths of 30, 40, 50 and 60 MPa with slump values between 75 and 125 mm and served as a basis for comparison with the RCA concrete mixtures. The direct replacement mixtures were developed to investigate the effect that fully replacing (i. e., 100 % replacement by volume) virgin coarse aggregate with RCA has on the fresh and hardened properties of the resulting concrete. The strength-based mixtures were developed to investigate the influence of aggregate properties on reinforcement bond in concrete having the same compressive strength. In addition, two separate experimental phases were carried out which had varying compressive strength ranges, different RCA sources, and different suppliers of the same type GU cement. Concrete properties such as slump, compressive strength, splitting tensile strength, modulus of elasticity, Poisson’s ratio, linear coefficient of thermal expansion (LCTE), modulus of rupture and fracture energy were all measured. In total, 48 beam-end specimens were tested that incorporated three bonded lengths (125, 375, and 450 mm) and four concrete compressive strengths (30, 40, 50 and 60 MPa). Based on the results of the aggregate testing it was found that concrete incorporating pre-soaked (i. e., fully saturated) RCA as a 100 % replacement for natural aggregate had slump values between 22 % and 75 %, compressive strengths between 81 % and 137 %, splitting tensile strengths between 78 % and 109 %, modulus of elasticity values between 81 % and 98 %, LCTE values in the same range, flexural strengths between 85 % and 136 %, and fracture energies between 68 % and 118 %, of the equivalent control (natural aggregate) concrete mixture. Overall, reductions in bond strength between natural aggregate and RCA concrete ranged between 3 and 21 %. The strength of coarse aggregate as quantified by the aggregate crushing value (ACV) was found to be the most significant aggregate property for influencing bond strength. A regression model (based on the beam-end specimens test results) was developed to extrapolate the experimental development lengths as a function of f’c 1 / 4 and ACV. The model, while not intended for use as a design equation, predicted that the required development lengths for the RCA concrete tested as part of this research study were up to 9 % longer as compared to the natural aggregate concrete. A <b>detailed</b> <b>flowchart</b> of the various inter-relationships between aggregate properties, concrete properties and reinforced concrete bond properties was compiled based on the results of this research. A comprehensive guideline for use of RCA in concrete was developed based on the findings of this research. It includes a systematic decision tree approach for assessing whether a particular RCA source can be categorized into one of three performance classes. The range of allowable applications of a concrete which incorporates the RCA source as replacement of natural coarse aggregate will depend on the RCA performance class...|$|E
40|$|The {{computation}} of {{parameters for}} symmetric laminates involves series of tedious calculations for both in-plane and flexural properties. In order {{to alleviate the}} magnitude and conplexi J-y of calculations for complicated symmetric laminates, the program of this report was designed for a commercially available microcomputer, the Hewlett-Packard HP- 87, and is based {{for the most part}} or. program logic developed by the Mr Force Wright Aeronautical Laboratories for the TI- 59 handheld prcgramma bis calculator. Program logic is explained in <b>detail</b> with <b>flowcharts</b> and a full listing cf the program is included. A description of the program logic provides the user with a comprehensive explanation of program operation. Prepared for : Prepared for: Dr. Stephen Tsai, AFWAL/MLBM Wright Patterson AFB, auspices of AE 4103, Advanced Aircraft Construction, at the Naval Postgraduate School. [URL] Postgraduate School Monterey, CAN...|$|R
40|$|Proyecto de Graduación (Maestría en Gerencia de Proyectos). Instituto Tecnológico de Costa Rica. Área Académica de Gerencia de Proyectos, 2013. This project {{provides}} a road conservation project management methodology for the Consejo Nacional de Vialidad (CONAVI). It provides tools, formats and templates {{to ease the}} management of road maintenance projects, and offers an implementation strategy. The Project Management Methodology was developed from a diagnosis based on criteria of successful professionals and experts involved in such projects, this in order to filter out those practices and tools that could be useful or marked difference in the processes management to achieve success of the project. This project gives a process-based methodology comprised of the basic phases of a project, <b>detailed</b> through <b>flowcharts</b> and uses tools that can facilitate such process. It also proposes a strategy {{the implementation of the}} methodology, to be executed in defined terms...|$|R
50|$|In December 2012 Merseytravel {{commissioned}} Network Rail {{to study}} route options {{and costs of}} connecting to Skelmersdale with Merseytravel contributing £50,000 and West Lancashire Council contributing £100,000. The range of options considered including a simple park and ride on the existing Northern Line Kirkby branch, {{an extension of the}} Northern Line Kirkby branch to a new terminus in Skelmersdale and finally a connection from the Northern Line Ormskirk branch, possibly extended to create a loop via Skelmersdale between Kirkby and Ormskirk. Merseytravel are represented on a board led by Lancashire County Council which has developed a <b>flowchart</b> <b>detailing</b> how the scheme may be delivered.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimited. The {{backbone of the}} internetworking technology widely used by the military, {{as well as many}} civilian installations, is commonly referred to as TCP/IP. Transmission Control Protocol (TCP) and Internet Protocol (IP) are the two standard communication protocols from which TCP/IP receives its name. By utilizing TCP/IP, the majority of technical issues of interconnecting various computer technologies have become transparent to the user. This thesis conducts an in depth study of many aspects of the TCP/IP technology. Based upon descriptions provided, <b>flowcharts</b> <b>detailing</b> the series of procedures of numerous functions of both TCP and IP are created. Additionally, inefficient TCP/IP functions are discussed and possible solutions to the inefficiencies are provided. Captain, United States Marine Corps...|$|R
40|$|Carbon {{emissions}} {{derived from}} a variety of sources is a significant contribution to global warming. Emission reduction policies and schemes are currently under development to reduce and discourage carbon emissions from the airlines and at the airport premises and building. Aviation is one of the major contributors to carbon emissions in the environment with significant implications for environmental sustainability, mostly arising from airline management facilities such as airports and other associated precinct tenants. Thus, this paper provides insights into new approaches for “greener” airports. In this paper, we analyse the airport ecosystem with the objective of identifying points of emissions arising in the daily operation of an airport. In addition, we assess the emissions that result from day to day operations of the airport with recommendations for the design of the airports decrease emissions. We provide a <b>detailed</b> operational <b>flowchart</b> of an airport and then determine the potential points where emissions are likely to occur. This paper will also discuss the means of accounting harmful airport emissions. The main objective of this study is to lay foundation for suggesting emission reduction strategies for airports. This study uses the publicly accessible departure and arrival area in the Perth international airport as an exemplar...|$|R
40|$|A {{major problem}} in {{clinical}} lung transplantation is the shortage of donor lungs. Only about 20 % of donor lungs are accepted for transplantation. We have recently reported {{the results of the}} first six double lung transplantations performed with donor lungs reconditioned ex vivo that had been deemed unsuitable for transplantation by the Scandiatransplant, Eurotransplant, and UK Transplant organizations because the arterial oxygen pressure was less than 40 [*]kPa. The three-month survival of patients undergoing transplant with these lungs was 100 %. One patient died due to sepsis after 95 days, and one due to rejection after 9 months. Four recipients are still alive and well 24 months after transplantation, with no signs of bronchiolitis obliterans syndrome. The donor lungs were reconditioned ex vivo in an extracorporeal membrane oxygenation circuit using STEEN solution mixed with erythrocytes, to dehydrate edematous lung tissue. Functional evaluation was performed with deoxygenated perfusate at different inspired fractions of oxygen. The arterial oxygen pressure was significantly improved in this model. This ex vivo evaluation model is thus a valuable addition to the armamentarium in increasing the number of acceptable lungs in a donor population with inferior arterial oxygen pressure values, thereby, increasing the lung donor pool for transplantation. In the following paper we present our clinical experience from the first six patients in the world. We also present the technique we used in <b>detail</b> with <b>flowchart...</b>|$|R
40|$|Objective and Rationale: The {{objective}} {{of this paper is}} to present a comprehensive, yet brief, flowchart type of overview of the salient literature describing the key chronological steps involved in developing “pencil and paper,” self-report, health-related survey instruments – particularly survey instruments which endeavor to measure abstract construct such as “quality of life,” “disability,” or “productivity. ” This overview was designed to serve as a convenient reference guide for individuals who need to understand the basics of the whole process. Because it does not describe any of the steps in <b>detail,</b> the <b>flowchart</b> will likely be most useful to individuals who have at least some prior familiarity with the concepts, procedures and analyses mentioned, yet are not fully “expert ” in this topic area. In short, this overview is not actually meant to be a “checklist ” of key steps; brief explanations are included in order to remind the – at least somewhat – initiated user of the concepts mentioned, without the reader necessarily having to look them up elsewhere. Design: This is a distillation of the salient surveydevelopment literature into a procedural overview flowchart. Method: This overview was a distillation of several authoritative sources in the literature covering the key areas of questionnaire development and psychometric theory. The overview flowchart was constructed in the form of 5 chronological, developmental phases, which formed the overall framework...|$|R
50|$|Atrium Carceri is {{typically}} described as dark ambient, black ambient and ambient industrial music. Similar to projects like Lull and Lustmord, Atrium Carceri uses synthesizers, sound effects, samples from films and anime, piano and other instrumentation to create slow rhythms, bitter melodies and complex textures generally based on themes of desolation, loneliness (solitary confinement) and environmental decay. Atrium Carceri has been praised by music critics and embraced by a cult audience for its depth of atmosphere. According to Heath himself, each of Atrium Carceri's solo releases are centered around specific 'story arcs' within a 'grand story,' {{and following the}} releases of Reliquiae and Void in 2012, has made a <b>flowchart</b> <b>detailing</b> the story progression up to that point. The exact nature of the concept behind this 'grand story' has been intentionally left ambiguous by Heath, stating it's 'up to the listener and his/her interpretation' to piece together this story.|$|R
40|$|Isolated hypertrophic {{cardiomyopathy}} {{may represent the}} sole clinical feature of a mitochondrial disorder in adult patients. The clinical outcome {{is characterized by a}} rapid progression to dilation and failure. A mitochondrial etiology in these cases is not obvious at clinical investigation and may represent an unexpected finding at autopsy or after cardiac transplant. We describe the morphologic, biochemical, and molecular features of hearts from 3 transplanted patients with isolated mitochondrial cardiomyopathy caused by homoplasmic mutations in the MTTI gene, coding for mitochondrial isoleucine tRNA (mt-tRNA(Ile)). On gross examination, the 3 hearts showed a symmetric pattern of hypertrophy. At histology, cardiomyocytes were hypertrophic and showed sarcoplasmic vacuoles filled with granules that stain with antimitochondrial antibodies. On frozen sections, the combined cytochrome c oxidase (COX) /succinate dehydrogenase stain showed a large prevalence of COX-deficient cardiomyocytes. Mitochondrially encoded COX subunit I was almost absent on immunohistochemistry, whereas the nuclear-encoded COX subunit IV was normally expressed. Ultrastructural analysis confirmed the marked mitochondrial proliferation. Biochemical studies of cardiac homogenates revealed a combined respiratory chain defect. Quantitative restriction fragment length polymorphism analysis of DNA from cardiac homogenate confirmed that the mt-tRNA mutations were also detected in the patient's blood. High-resolution Northern blot analysis showed a marked decrease in the steady-state level of mt-tRNA(Ile), confirming pathogenicity. In conclusion, pathologists {{play a major role in}} unraveling the mitochondrial etiology of isolated hypertrophic cardiomyopathies, provided that a <b>detailed</b> diagnostic <b>flowchart</b> is followed. Once the mitochondrial etiology is clearly defined, molecular analyses on the heart are an invaluable tool to assign mutation pathogenicity. (C) 2013 Elsevier Inc. All rights reserved...|$|R
40|$|A {{methodology}} for predicting the equivalent properties and constituent microstresses for particulate matrix composites, {{based on the}} micromechanics approach, is developed. These equations are integrated into a computer code developed to predict the equivalent properties and microstresses of fiber reinforced polymer matrix composites {{to form a new}} computer code, ICAN/PART. <b>Details</b> of the <b>flowchart,</b> input and output for ICAN/PART are described, along with examples of the input and output. Only the differences between ICAN/PART and the original ICAN code are described in detail, and the user is assumed to be familiar with the structure and usage of the original ICAN code. Detailed verification studies, utilizing dim dimensional finite element and boundary element analyses, are conducted in order to verify that the micromechanics methodology accurately models the mechanics of particulate matrix composites. ne equivalent properties computed by ICAN/PART fall within bounds established by the finite element and boundary element results. Furthermore, constituent microstresses computed by ICAN/PART agree in average sense with results computed using the finite element method. The verification studies indicate that the micromechanics programmed into ICAN/PART do indeed accurately model the mechanics of particulate matrix composites...|$|R
40|$|Abstract—A novel {{explicit}} knowledge embedded space map-ping (SM) {{optimization technique}} {{is presented in}} this paper. It generalizes the implementation procedure of the efficient SM technique by introducing a buffer space called embedded knowl-edge space between the original coarse model space and the fine model space, where the ingredients of the coarse model space can be completely {{different from those of}} the fine model space. Therefore, this generic scheme can be used to map the coarse model space of arbitrary physical content to the fine model space of different physical content through the embedded knowledge space that is built up with the available radio frequency (RF) circuit CAD formula. The emphasis of the application of the pro-posed scheme is put on the design of low temperature cofired ceramic (LTCC) RF passive circuits in this paper, along with the required CAD formulas (knowledge) for typical embedded mul-tilayer passives. The effectiveness of the proposed new scheme is demonstrated through two design examples of LTCC lumped element band pass filters for wireless applications. The <b>detailed</b> procedure and <b>flowchart</b> of the proposed implementation scheme are also given in the paper. Index Terms—CAD, embedded RF components, LTCC, opti-mization, space mapping. I...|$|R
