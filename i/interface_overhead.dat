8|29|Public
40|$|Management of {{communication}} by on-line routing in new FPGAs {{with a large}} amount of logic resources and partial reconfigurability is a new challenging problem. A Network-on-Chip (NoC) typically uses packet routing mechanism, which has often unsafe data transfers, and network <b>interface</b> <b>overhead.</b> In this paper, circuit routing for such dynamic NoCs is investigated, and a practical 1 -dimensional network with an efficient routing algorithm is proposed and implemented. Also, this concept has been extended to the 2 -dimensional case. The implementation results show the low area overhead and high performance of this network. 1...|$|E
40|$|Multicomputers {{built around}} a general network are now a viable {{alternative}} to multicomputersbased ona system-specific interconnect because of architectural improvements in two areas. First, the host-network <b>interface</b> <b>overhead</b> can be minimized by reducing copy operations and host interrupts. Second, the network can provide high bandwidth and low latency by using high-speed crossbar switches and efficient protocol implementations. While still enjoying the flexibility of general networks, the resulting network-based multicomputers achieve high performance for typical multicomputer applications that use system-specific interconnects. We have developed a network-based multicomputer called Nectar that supports these claims. 1 Introduction Current commercial parallel machines cover a wide spectrum of architectures: shared-memory parallel computers such as the Alliant, Encore, Sequent, and CRAY Y-MP; and distributed-memory computers including MIMD machines such as the Transputer [15], iWarp [...] ...|$|E
40|$|Experience using {{constraint}} {{programming to}} solve real-life problems {{has shown that}} nding an ecient {{solution to a problem}} often requires experimentation with dierent constraint solvers or even building a problem-specic solver. HAL is a new constraint logic programming language expressly designed to facilitate this process. In this paper we examine dierent ways of building solvers in HAL. We explain how type classes can be used to specify solver interfaces, allowing the constraint programmer to support modelling of a constraint problem independently of a particular solver, leading to easy and play" experimentation. We compare a number of dierent ways of writing a simple solver in HAL: using dynamic scheduling, constraint handling rules and building on an existing solver. We also examine how external solvers may be interfaced with HAL, and approaches for removing <b>interface</b> <b>overhead.</b> ...|$|E
25|$|The raw maximum {{transfer}} rate of 3½-inch HD floppy drives and <b>interfaces,</b> disregarding <b>overheads,</b> {{is as much}} as 1,000kilobits/s, or approximately 83% that of single-speed CD‑ROM (71% of audio CD). This represents the speed of raw data bits moving under the read head; however, the effective speed is somewhat less due to space used for headers, gaps and other format fields.|$|R
40|$|Machine {{vision is}} a {{powerful}} tool for process automation and control. However, the memory requirements and <b>interface</b> <b>overheads</b> of video buffers found in typical video grabbers are not easy to interface to low cost embedded systems. This paper describes an active vision interface that enables a low end microcontroller to use a standard video camera as an input for process control. The active vision interface consists of a frame grabber with a small (4 kb) memory mapped buffer that can be zoomed out to sample the whole image at low resolution, or zoomed in and panned around the image at high resolution. This scheme takes advantage of the fact that most real time vision systems operate on low resolution images or small parts of a high resolution system {{in order to reduce the}} number of operations performed on each frame. The system integrates tightly with the embedded microcontroller by providing current video in the microcontroller's memory space. This minimises the overheads in interfaci [...] ...|$|R
50|$|In the batch era, {{computing}} power was extremely scarce and expensive. User interfaces were rudimentary. Users had to accommodate computers {{rather than the}} other way around; user <b>interfaces</b> were considered <b>overhead,</b> and software was designed to keep the processor at maximum utilization with as little overhead as possible.|$|R
30|$|However, {{in case of}} multiplexing, {{the number}} of devices served by the RN nearly doubles. This {{is due to the}} fact that in the case of “no multiplexing”, each data packet {{contains}} an additional Un air <b>interface</b> <b>overhead</b> of GTP, UDP, IP and layer 2. The additional overhead causes an extra PRB usage and overall reduces the PRB utilization efficiency. Moreover, in low-load scenarios, the average number of PRBs used is slightly higher in the case of “multiplexing with timer”. This {{is due to the fact}} that the RN serves the traffic at the latest after 9 ms and thus the PRB is not necessarily used with its maximum capacity due to the low arrival rate. However, in high-load scenarios, the timer has almost no impact and nearly equal numbers of PRBs are used with and without timer.|$|E
40|$|This paper {{presents}} a simple method of mutual translation between Japanese sentences and Horn clauses, {{which has been}} implemented in our analogical reasoning system as a man-machine interface. Natural language interface systems generally need complicated semantic processing using a large dictionary. However, such processing is not suitable for our interface system because the <b>interface</b> <b>overhead</b> becomes too large {{in comparison with the}} main reasoning processing and the vocabulary is limited by the dictionary. From this viewpoint, we adopt a textual processing. Our system does not have a large dictionary, but instead we put some restrictions on sentences inputted to the system; for example, each sentence should be separated into words by spaces and be expressed uniformly. We also present inethods which keep input Japanese sentences natural and which remove ambiguities by the interaction between users and the system...|$|E
40|$|This paper {{describes}} an FPGA-based accelerator for maze routing {{applications such as}} integrated circuit detailed routing. The accelerator efficiently supports multiple layers, multi-terminal nets, and rip up and reroute. By time-multiplexing multiple layers over a two-dimensional array of processing elements, this approach can support multi-layer grids large enough for detailed routing while providing at 1 - 2 orders of magnitude speedup over software running on a modern desktop computer. The current implementation supports a 32 X 32 routing grid with up to 16 layers in a single Xilinx XC 2 V 6000 FPGA. Up to 64 X 64 routing grids are feasible in larger commercially available FPGAs. Performance measurements (including <b>interface</b> <b>overhead)</b> show a speedup of 29 X- 93 X over the classic Lee Algorithm and 5 X- 19 X over the A * Algorithm. An improved interface design could yield significantly larger speedups. 1...|$|E
40|$|In chip-multiprocessor (CMP) designs, {{when the}} number of core increases, the size of on-chip {{communication}} fabric and data storage grows accordingly and therefore the chip power challenge is exacerbated. This thesis work considers the power management for networks-on-chip (NoC) and the last level cache, which constitute the uncore in CMP designs. NoC is regarded as a scalable approach to cope with the increasing demand for on-chip communication bandwidth. The last level cache is shared among all cores. The focus of this work is on the control techniques for uncore dynamic voltage and frequency scaling. A realistic but not well-studied scenario is investigated. That is, the entire uncore shares a single voltage/frequency domain, as opposed to separated domains in most of previous works. One appealing advantage here is that data packets no longer experience the <b>interfacing</b> <b>overhead</b> across different voltage/frequency domains. The classic PI (Proportional and Integral) control method is adopted due to its simplicity, flexibility and low implementation overhead. This thesis research outcome includes three parts. First, stability of the PI control is analyzed. Second, a model-assisted PI control scheme is proposed and studied. The model assist is to address the problem that no universally good reference point exists for the control. Third, the windup issue for the PI control is investigated. Full architecture simulations are performed on public benchmark suites to validate the proposed techniques. The result show 76 % energy reduction with less than 6 % performance degradation compared to constantly high voltage/frequency for uncore...|$|R
50|$|Too-fine-grained microservices {{have been}} criticized as an anti-pattern, dubbed a {{nanoservice}} by Arnon Rotem-Gal-Oz: A nanoservice is an anti-pattern where a service is too fine grained. A nanoservice is a service whose overhead (communications, maintenance etc.) outweighs its utility. Problems include the code <b>overhead</b> (<b>interface</b> definition, retries), runtime overhead (serialization/deserialization, network traffic), and fragmented logic (useful functionality not implemented in one place, instead requiring combining many services).|$|R
40|$|Abstract—As future {{vehicular}} communications {{will most}} likely involve multiple wireless networks, intelligent interface switching is of essen-tial importance to support various user preferences across different performance metrics. In this paper, we present a general optimization framework for solving the interface switching problem, and develop a flexible and efficient solution based on stochastic dynamic programming (SDP). Our framework is designed to accommodate different performance metrics such as data transfer efficiency, monetary cost, and <b>interface</b> switching <b>overhead.</b> Accordingly, the proposed SDP-based policy can easily adapt its decision based on user-specified relative importance of the various metrics. Simulation study confirms the optimality of the SDP-based policy over a range of user preference choices, and shows that it consistent outperforms several heuristic schemes. Index Terms—vehicular communications, interface switching, hetero-geneous networks, stochastic dynamic programming (SDP) I...|$|R
40|$|This work {{presents}} an interface to the widely used PVM and MPI message passing systems, {{by means of}} which MATLAB users can prototype applications in the usual highlevel programming environment, while retaining {{the ability to make}} PVM and MPI calls. The rapid spread of clusters of computers in academic and industrial facilities, making parallel computing available to any user at a fraction of its former price, explains the interest and timeliness of such an interface. Two PVM/MPI functions are studied, showing to some extent the implementation details of the interface. Performance measures on code written in C as well as in MATLAB show that the overhead introduced by the interface is very small, and certainly acceptable for the coarse-grained simulations typically run under MATLAB. Two parallelization approaches are then applied to a case study on wavelet analysis, in order to show typically achievable speedups. The user's knowledge of the application can be used to choose the coarsest grained approach, which in turn leads to performance gains that make the <b>interface</b> <b>overhead</b> and the choice between PVM and MPI irrelevant. ...|$|E
40|$|The {{standard}} way {{to notify}} the processor of a network event, such as the arrival or transmission of a packet, is through interrupts. Interrupts are more effective than polling, {{in terms of the}} per packet send/receive latency. Interrupts, however, incur a high overhead both during and after the interrupt handling, because modern superscalar processors use long pipelines, out-of-order and speculative execution, and multi-level memory systems, all of which tend to increase the interrupt overhead in terms of clock cycles. In this paper, we attempt to reduce the network <b>interface</b> <b>overhead</b> by introducing a hybrid scheme (HIP) that uses interrupts under low network load conditions and polling otherwise. Even though such hybrid schemes have been proposed in the past, the polling period in HIP is adjusted dynamically based on the rate of the arriving packet stream. In this way, the increase in the per packet latency, which occurs with polling, is quite low. This is quantified with trace-driven simulations, which also show that the per packet overhead with HIP is significantly reduced compared to the conventional interrupt-based mechanism. HIP would be beneficial for high bandwidth network interfacing in servers with a heavy WWW or streaming media workload. ...|$|E
40|$|Traditional 3 d {{modeling}} software utilizes {{a complex}} interface that restricts free conceptual thinking. We {{have developed an}} alternative approach {{that is based on}} making marks with the body in space. This simplified <b>interface</b> reduces cognitive <b>overhead,</b> enhancing the conceptual development of geometric shapes. This system uses the Responsive Workbench to display geometry in the space of the user, and senses the hand and a number of tangible tools to translate motion and action into form...|$|R
30|$|To {{determine}} whether or not the network behaves as expected, link quality information (received signal strength indicator—RSSI, average noise floor, etc.) is collected by the sinks. In case of multi-hop networks, intermediate receiver nodes will transfer the link quality information to the sink. For experiments, the same wired debugging interface is then used. However, even when this data are collected over the wireless <b>interface,</b> the <b>overhead</b> will be very low since (i) the configuration data are typically minimal (e.g., one message every hour) compared to the data traffic from the sensor network and (ii) network monitoring and configuration is only needed {{at the beginning of the}} network deployment or when the wireless environment changes significantly. In fact, once the cognitive planning tool finalizes optimizing the network, only changes to the network environment that last a long time need to be communicated. As such, after the initial configuration, the monitoring overhead becomes insignificant compared to the operational network energy consumption.|$|R
30|$|Inter-RAT (IRAT) {{load control}} policy with {{information}} exchange (RAN-assisted approach): This approach {{relies on a}} tighter coupling between the LTE and WLAN systems and requires that {{a small amount of}} information related to the WLAN AP, referred to as information exchange, e.g., WLAN AP load levels, is collected by an LTE cell, which is strategically paired with the WLAN AP, as will be discussed later. The SON function runs on the LTE side. The exchanged information is processed by the LTE cell, in addition to the local (LTE radio related) information, to determine a possible adjustment of the ANS control parameters. The information exchange between the LTE and WLAN systems may rely on UE terminals as relays, on proprietary or a standardized signaling interface between the paired WLAN AP and LTE cell. The latter is the most promising approach when considering air <b>interface</b> signaling <b>overhead,</b> UE battery consumption, and multi-vendor interworking. The RAN assistance information is sent from the serving LTE cell to the UEs.|$|R
30|$|The next {{research}} project {{that has been}} considered is a Flexible Architecture for Ambient Intelligence Systems [37] that interacts with a subject through a virtual character, which mimics a relative or friend {{so that they can}} interact with a friendly face. A virtual character may have several complicating issues, the virtual character needs to be programmed and this may add to development time and the virtual character may require more processing power during interactions due to the rendering process. As highlighted by the authors, the virtual character is non-persistent; AALFI has persistence as key interventions and actions are remembered so that feedback may be presented to the subject. AALFI is touch screen based and provides a simple GUI that has large buttons and text that is of a large font and is clear. By implementing a simple <b>interface,</b> processing <b>overheads</b> may be reduced and the device that the interface runs on may not need to be that powerful.|$|R
40|$|Abstract. The network {{interface}} {{is the key}} position in an intelligent sensor system to realize plug and play. Based on UPnP (Universal Plug and Play) and IEEE 1451 serie standards, an IEEE 1451 /UPnP bridge is designed to solve the differences of message formats, self-identification description, and device discovery between these two standards. Using UPnP device selection method based on information axiom, the calculation of parameters information is distributed to each NCAP (Network Capable Application Processor) that meeting the service request. The test results indicate that this method simplifies the service discovery process, reduces network <b>interface</b> data traffic <b>overhead,</b> and increases UPnP device service efficiency...|$|R
40|$|For the {{microprocessor}} technology of {{today and the}} foreseeable future, multi-core is a key engine that drives performance growth under very tight power dissipation constraints. While previous research has been mostly focused on individual processor cores, there is a compelling need for studying how to efficiently manage shared resources among cores, including physical space, on-chip communication and on-chip storage. In managing physical space, floorplanning is {{the first and most}} critical step that largely affects communication efficiency and cost-effectiveness of chip designs. We consider floorplanning with regularity constraints that requires identical processing/memory cores to form an array. Such regularity can greatly facilitate design modularity and therefore shorten design turn-around time. Very little {{attention has been paid to}} automatic floorplanning considering regularity constraints because manual floorplanning has difficulty handling the complexity as chip core count increases. In this dissertation work, we investigate the regularity constraints in a simulated-annealing based floorplanner for multi/many core processor designs. A simple and effective technique is proposed to encode the regularity constraints in sequence-pair, which is a classic format of data representation in automatic floorplanning. To the best of our knowledge, this is the first work on regularity-constrained floorplanning in the context of multi/many core processor designs. On-chip communication and shared last level cache (LLC) play a role that is at least as equally important as processor cores in terms of chip performance and power. This dissertation research studies dynamic voltage and frequency scaling for on-chip network and LLC, which forms a single uncore domain of voltage and frequency. This is in contrast to most previous works where the network and LLC are partitioned and associated with processor cores based on physical proximity. The single shared domain can largely avoid the <b>interfacing</b> <b>overhead</b> across domain boundaries and is practical and very useful for industrial products. Our goal is to minimize uncore energy dissipation with little, e. g., 5 % or less, performance degradation. The first part of this study is to identify a metric that can reflect the chip performance determined by uncore voltage/frequency. The second part is about how to monitor this metric with low overhead and high fidelity. The last part is the control policy that decides uncore voltage/frequency based on monitoring results. Our approach is validated through full system simulations on public architecture benchmarks...|$|R
40|$|Current Software-Defined Radio {{applications}} (waveforms) are {{tailored to}} specific hardware. Processor vendors frequently adapt internal OS mechanisms for its specific architecture (e. g. scheduling and synchronization). The Abstraction Layer and Operating Environment (ALOE) {{is an open}} source SDR operating environment that isolates platform architecture from the application design. An integrated resource manager is capable of automatically mapping waveform components to a network of heterogeneous processors while meeting the waveform’s real-time requirements. This paper analyzes the ALOE performance for x 86 and ARM processors. It presents computing histograms of UTRAN transceiver components, the maximum achievable throughput of a simple BPSK modem, <b>interface</b> latencies, and <b>overhead</b> measurements of the ALOE background processes. Peer ReviewedPostprint (published version...|$|R
40|$|Myth is a {{programming}} language that {{is an extension of}} C. Myth adds modules, interfaces, tuple returns, and bit sets. These features are very helpful in nearly any programming environment. The Myth language as a whole is a good choice for embedded systems where resources are limited. Modules help organize code into logical groupings. Interfaces provide an automated mechanism to write reusable code. Tuple returns provide a simple mechanism for returning multiple values from a function. Bitsets make it easy to define logical groups of bit flags. Bisets are particularly useful in code that interfaces with hardware. The advantage of Myth is modules, tuple returns, and <b>interfaces</b> without the <b>overhead</b> of full-blown object orientation. Myth has been implemented as a preprocessor that produces C source code...|$|R
40|$|High-performance network {{hardware}} is advancing, with multi-gigabit link bandwidths and sub-microsecond switch latencies. Network-interface hardware also {{continues to}} evolve, although the design space remains large and diverse. One critical abstraction, a simple, portable, and general-purpose communications interface, {{is required to}} make effective use of these increasingly high-performance networks and their capable interfaces. Without a new <b>interface,</b> the software <b>overheads</b> of existing ones will dominate communication costs, and many applications may not benefit from the advancements in network hardware. This document specifies a new active message communications interface for these networks. Its primitives, in essence an instruction set for communications, map efficiently onto underlying network hardware and compose effectively into higher-level protocols and applications. For high-performance implementations, the interface enables direct application-network interface interactions. In [...] ...|$|R
40|$|Dynomitc is a {{portable}} electronic notebook for the capture and retrieval of handwritten and audio notes. The goal of Dynomitc is to merge the organization, search, and data acquisition capabilities {{of a computer}} with {{the benefits of a}} paper-based notebook. Dynomite provides novel solutions in tour key problem areas. First, Dynomite uses a casual, low cognitive <b>overhead</b> <b>interface.</b> Second, for content indexing of notes, Dynomite uses ink properties and keywords. Third, to assist organization, Dynomite’s properties and keywords define views, presenting a subset of the notebook content that dynamically changes as users add new information. Finally, to augment handwritten notes with audio on devices with limited storage, Dynomite continuously records audio, but only permanently stores those parts highlighted by the user...|$|R
40|$|Portable {{electronic}} devices and crafts such as unmanned aerial vehicles (UAV 2 ̆ 7 s) may benefit greatly from {{the ability to}} extract power from overhead distribution power lines on a temporary basis to power electronics or charge on-board batteries. However, most of the current literature {{on the subject of}} energy scavenging is focused on micropower and other small-scale applications. Several high-power energy scavenging methods are investigated here with an emphasis on relating physical sensor dimensions with output power. A novel power scavenging mechanism is introduced that shows excellent correlation between theoretical and experimental performance. In addition, a universal power supply is proposed which may be <b>interfaced</b> with an <b>overhead</b> distribution line of 4. 16 - 34. 5 kVAC to create a temporary source of high-quality regulated power for portable device electronics and battery charging...|$|R
40|$|Abstract—The {{design and}} {{implementation}} of the Thomas algorithm optimised for hardware acceleration on an FPGA is presented. The hardware based algorithm combined with custom data flow and low level parallelism available in an FPGA reduces the overall complexity from 8 N down to 5 N arithmetic operations, and combined with a data streaming <b>interface</b> reduces memory <b>overheads</b> to only 2 N-length vectors per N-tridiagonal system to be solved. The Thomas Core developed allows for multiple tridiagonal systems to be solved in parallel, giving potential use for solving multiple implicit finite difference schemes or acceler-ating higher dimensional alternating-direction-implicit schemes used in financial derivatives pricing. This paper also discusses the limitations arising from the fixed-point arithmetic used in the design and how the resultant rounding errors can be controlled to meet a specified tolerance level. I...|$|R
40|$|View-Oriented Parallel Programming {{is based}} on Distributed Shared Memory which is {{friendly}} and easy for programmers to use. It requires the programmer to divide shared data into views according to the memory access pattern of the parallel algorithm. One {{of the advantages of}} this programming style is that it offers the performance potential for the underlying Distributed Shared Memory system to optimize consistency maintenance. Also it allows the programmer to participate in performance optimization of a program through wise partitioning of the shared data into views. In this paper, we compare the performance of View-Oriented Parallel Programming against Message Passing Interface. Our experimental results demonstrate a performance gap between View-Oriented Parallel Programming and Message Passing <b>Interface.</b> The contributing <b>overheads</b> behind the performance gap are discussed and analyzed, which sheds much light on further performance improvement of View-Oriented Parallel Programming...|$|R
40|$|Providing Java {{applications}} {{with access}} to low-level system resources, including fast network and I/O interfaces, requires functionality not provided by the Java Virtual Machine instruction set. Currently, Java applications obtain this functionality by executing code written in a lower-level language, such as C, through a native method <b>interface.</b> However, the <b>overhead</b> of this <b>interface</b> can be very high, and executing arbitrary native code raises serious protection and portability concerns. Jaguar [37] provides Java applications with efficient access to hardware resources through a bytecode specialization technique which transforms Java bytecode sequences {{to make use of}} inlined Jaguar bytecode which implements low-level functionality. Jaguar bytecode is portable and type-exact, making it both safer and more efficient than native methods. Jaguar requires that the target JVM or compiler recognizes Jaguar bytecode, which is a superset of the Java bytecode instruction set. We describe two impl [...] ...|$|R
40|$|NICAM is a {{communication}} layer for SMP PC clusters connected via Myrinet, {{designed to reduce}} overhead and latency by directly utilizing a micro-processor equipped on the network interface. It adopts remote memory operations to reduce much of the overhead found in message passing. NICAM employs an Active Messages framework for flexibility in programming on the network interface, and this flexibility will compensate for the large latency resulting from the relatively slow micro-processor. Running message handlers directly on the network <b>interface</b> reduces the <b>overhead</b> by freeing the main processors {{from the work of}} polling incoming messages. The handlers also make synchronizations faster by avoiding the costly interactions between the main processors and the network interface. In addition, this implementation can completely hide latency of barriers in data-parallel programs, because handlers running in the background of the main processors allow reposition of barriers to any place where [...] ...|$|R
40|$|The Xen Virtual Machine Monitor {{has proven}} to achieve higher {{efficiency}} in virtualizing the x 86 architecture than competing x 86 virtualization technologies. This makes virtualization on the x 86 platform more feasible in High-Performance and mainframe computing, where virtualization can offer attractive solutions for managing resources between users. Virtualization is also attractive on the Itanium architecture. Future x 86 and Itanium computer architectures include extensions which make virtualization more efficient. Moving to virtualizing resources through Xen may ready computer centers for the possibilities offered by these extensions. The Itanium architecture is ``uncooperative'' in terms of virtualization. Privilege-sensitive instructions make full virtualization inefficient and impose the need for para-virtualization. Para-virtualizing Linux involves changing certain native operations in the guest kernel in order to adapt it to the Xen virtual architecture. Minimum para-virtualizing impact on Linux is achieved by, instead of replacing illegal instructions, trapping them by the hypervisor, which then emulates them. Transparent para-virtualization allows the same Linux kernel binary to run on top of Xen and on physical hardware. Itanium region registers allow more graceful distribution of memory between guest operating systems, while not disturbing the Translation Lookaside Buffer. The Extensible Firmware Interface provides a standardized interface to hardware functions, and is easier to virtualize than legacy hardware <b>interfaces.</b> The <b>overhead</b> of running para-virtualized Linux on Itanium is reasonably small and measured to be around 4. 9 %. Also, the overhead of running transparently para-virtualized Linux on physical hardware is reasonably small compared to non-virtualized Linux. </p...|$|R
40|$|In this work, {{a hybrid}} CPU/accelerator platform, which runs a {{standard}} operating system, is proto-typed using an FPGA. The {{goal is to}} provide consistent, protected, low <b>overhead</b> <b>interfaces</b> for accelerator developers to target. These interfaces enable software application developers to exploit the parallel pro-cessing power of hardware accelerators and reconfigurable computing. Furthermore, we aim to decouple the process of software development from accelerator design in a heterogeneous model of computation. A switchable interconnect accelerator framework is developed to explore what resources should be embedded in future CPU/accelerator systems. We mate our accelerator and application-level inter-faces to provide a seamless platform for developing hardware accelerated applications. Complexity of debugging and development using our framework is lowered by reducing the signal count by over 80 %, compared to a stand-alone accelerator design. A hardware accelerated JPEG encoding application serves as a motivating example throughout this work. A computation model is developed to allow standard applications running on our platform to ac-cess reconfigurable accelerators. The applications access accelerators, synthesized in the FPGA fabric, through an operating system layer. Applications running on the CPU interface with the accelerator...|$|R
40|$|Dynamic Spectrum Access (DSA) allows {{unlicensed}} wireless {{devices to}} opportunistically access unoccupied licensed spectrum bands. DSA yields {{efficient spectrum utilization}} which can greatly improve wireless networking performance. In this paper, we advocate application-awareness to effectively manage the side-effects of DSA that can offset its benefits by adversely impacting application QoS. Simple application hints {{are found to be}} able to serve as key inputs in evaluating current spectrum conditions relative to application needs, leading to an informed DSA mechanism that minimizes the impact of undesirable DSA side-effects. Towards this goal, we propose a wireless service architecture called Context-Aware Spectrum Agility (CASA). The key elements of CASA are: (a) semantic dependency equations that provide the relationship between application-layer QoS state and lower-layer DSA parameters, (b) CASA Algorithm that adapts DSA parameters and activities to better suit application needs, and, (c) a low <b>overhead</b> <b>interface</b> to provide application context to DSA. CASA has been explicitly designed with the goals of practical deployment, low overhead operation, and is compatible with any DSA protocol. Compared to state-of-art DSA, the deployment of CASA along with DSA protocols is shown to improve QoS metrics, such as delay and jitter, by an average of 30 an...|$|R
40|$|As {{part of the}} Formerly Utilized Sites Remedial Action Program, a {{team from}} Oak Ridge National Laboratory (ORNL) {{conducted}} a radiological verification survey of Building 14 at the former Linde Uranium Refinery, Tonawanda, New York. The purpose of the survey was to verify that remedial action completed by the project management contractor had reduced contamination levels to within authorized limits. Prior to remediation, fixed and removable beta-gamma emitting material was Prevalent throughout Building 14 and {{in some of the}} process piping. Decontamination consisted of removal of surface contamination from floors, floor-wall interfaces, walls, wall-ceiling <b>interfaces,</b> and <b>overhead</b> areas; decontamination or removal of process piping; excavation and removal of subsurface soil; and vacuuming of dust. This independent radiological assessment was performed to verify that the remedial action had reduced contamination levels to within authorized limits. Building 14 at the former Linde site in Tonawanda, New York, was thoroughly investigated inside for radionuclide residues. Surface residual activity levels were generally well below applicable guidelines for protection against radiation. Similarly, removable alpha and beta-gamma activity levels were below guidelines. Gamma exposure rates within the building were at typical background levels, and no elevated indoor radon concentrations were measured. However, numerous areas exceeding U. S. Department of Energy (DOE) applicable guidelines still remain inside and underneath the building. These areas were either (1) inaccessible or (2) removal was not cost-effective or (3) removal would affect the structural integrity of the building. These above-guideline areas have been listed, described, and characterized by the remediation subcontractor (Appendix A), and dose to an exposed worker during typical exposure scenarios has been calculated. Based on the remediation subcontractor's characterization data and dose assessment calculations, these areas pose insignificant risk to building inhabitants under current use scenarios. However, future renovations, repairs, or demolition of the building must require prior evaluation and consideration of the areas. Analysis of the project management contractor's post-remedial action data and results of this independent radiological verification survey by ORNL confirm that residual contamination inside the building is either below the limits prescribed by DOE applicable guidelines for protection against radiation or areas exceeding applicable guidelines have been characterized and a risk assessment completed. Building 14 can be released for unrestricted use under current use scenarios; however, arrangements must be made to inform current and future building owners of the locations of areas exceeding DOE guidelines and any associated restrictions concerning renovations, repairs, or demolition of the building...|$|R
40|$|This paper {{presents}} an architecture that combines VLIW (very long instruction word) processing with {{the capability to}} introduce application-specific customized instructions and highly parallel combinational hardware functions for the acceleration of signal processing applications. To support this architecture, a compilation and design automation flow is described for algorithms written in C. The key contributions of this paper are as follows: (1) a 4 -way VLIW processor implemented in an FPGA, (2) large speedups through hardware functions, (3) a hardware/software <b>interface</b> with zero <b>overhead,</b> (4) a design methodology for implementing signal processing applications on this architecture, (5) tractable design automation techniques for extracting and synthesizing hardware functions. Several design tradeoffs for the architecture were examined including the number of VLIW functional units and register file size. The architecture was implemented on an Altera Stratix II FPGA. The Stratix II device was selected because it offers {{a large number of}} high-speed DSP (digital signal processing) blocks that execute multiply-accumulate operations. Using the MediaBench benchmark suite, we tested our methodology and architecture to accelerate software. Our combined VLIW processor with hardware functions was compared to that of software executing on a RISC processor, specifically the soft core embedded NIOS II processor. For software kernels converted into hardware functions, we show a hardware performance multiplier of up to times that of software with an average times faster. For the entire application in which only a portion of the software is converted to hardware, the performance improvement is as much as 30 X times faster than the nonaccelerated application, with a 12 X improvement on average. </p...|$|R
40|$|There is {{a growing}} demand to {{validate}} and test designs that integrate a multitude of discrete, standard and commercial off the shelf (COTS) components and software. Often, the proprietary nature of COTS further complicates the validation process by prohibiting designers from implementing a robust test and validation strategy. In order to preserve a vendor's intellectual property, a customer is often provided a high level model, with limited access to its internals. This limitation may preclude a customer from fully exercising their embedded designs, which may create safety issues for some embedded systems. The increasing use of high level models is creating a need for tools {{that are capable of}} testing and validating COTS components in conjunction with an engineer's more detailed design. Unfortunately, simulation tools currently available are incapable of simulating multilevel designs, which include gate, RTL and behavioral levels. Those tools that do attempt to solve these problems resort to merging simulators together, each targeted at handling a specific level of%% [Page: 1]%% With the advent of commercial off the shelf (COTS) components and software, engineers are faced with intellectual property issues and limited abilities to validate and test their designs. Often the COTS components provided to them are very high level models, which may render the simulation tools inadequate. To simulate design environments that include both gate level and behavioral models, designers have resorted to using a variety of tools and simulators. Often these simulators are &quot;glued &quot; together to accommodate different portions of a design, resulting in complex <b>interfaces,</b> with high <b>overhead,</b> impaired observation and accuracy[1][2]...|$|R
30|$|The {{presented}} co-simulation setup {{has been}} shown to contain heterogeneous components that require different forms of interfacing and data exchange, run in geographically distributed computation infrastructures, and partly display composite structures with interconnected subsystems. Such a setup presents some challenges to the idea of generic co-simulation since this approach typically follows the idea of somewhat similar simulator types. Differences between simulation components are mostly discussed in terms of different modeling paradigms, i.e. simulators displaying continuous time, discrete time or discrete event dynamics (e.g. (Ptolemaeus, 2014; Camus et al., 2016)). However, especially the interfacing of simulation components is subjected to considerable standardization effort with the FMI standard gaining increasing popularity (e.g. (Awais et al., 2013; Müller & Widl, 2015)). Since this standard expects the wrapping of a simulation component within a so-called Functional Mock-up Unit, it can quickly lead to <b>interfacing</b> hurdles or <b>overhead</b> when dealing with remote connections (e.g. for the VPP simulation) or composite, modular component structures (e.g. for the KDP). Overall, the CPES domain is quite diverse and contains few standardized simulation tools. Complex simulation components, on the other hand, representing in-house solutions with their own computation environments are likely to come up – as is presented in this study. Therefore, FMI is rather unlikely to cover all co-simulation interfacing needs in this domain in the near future. The MOSAIK Component-API used in this project possesses the advantage of directly supporting several programming languages and being interoperable with MOSAIK without requiring any additional wrapping of the simulation components. This allows for greater versatility when simulators require interfacing via web services or shared folders.|$|R
