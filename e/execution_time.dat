10000|3185|Public
5|$|In {{addition}} to reporting active processors, Folding@home determines its computing performance as measured in floating point {{operations per second}} (FLOPS) based on the actual <b>execution</b> <b>time</b> of its calculations. Originally this was reported as native FLOPS: the raw performance from each given type of processing hardware. In March 2009 Folding@home began reporting the performance in native and x86 FLOPS, the latter being an estimation of how many FLOPS the calculation would take on a standard x86 CPU architecture, which is commonly used as a performance reference. Specialized hardware such as GPUs can efficiently perform some complex functions in one floating point operation which otherwise needs multiple operations on the x86 architecture. The x86 measurement attempts to even out these hardware differences. Despite conservative conversions, the GPU clients' x86 FLOPS are consistently greater than their native FLOPS and comprise {{a large majority of}} Folding@home's measured computing performance.|$|E
25|$|Tabling is a space-time tradeoff; <b>execution</b> <b>time</b> can {{be reduced}} by using more memory to store {{intermediate}} results.|$|E
25|$|Each {{higher layer}} of the tree {{operates}} with a longer interval of planning and <b>execution</b> <b>time</b> than its immediately lower layer.|$|E
40|$|This paper {{contrasts}} {{two methods}} to verify timing constraints of real-time applications. The method of static analysis predicts the worst-case and best-case <b>execution</b> <b>times</b> of a task's code by analyzing execution paths and simulating processor characteristics without ever executing the program or requiring the program's input. Evolutionary testing is an iterative testing procedure, which approximates the extreme <b>execution</b> <b>times</b> within several generations. By executing the test object dynamically and measuring the <b>execution</b> <b>times</b> the inputs are guided yielding gradually tighter {{predictions of the}} extreme <b>execution</b> <b>times.</b> We examined both approaches {{on a number of}} real world examples. The results show that static analysis and evolutionary testing are complementary methods, which together provide upper and lower bounds for both worst-case and best-case <b>execution</b> <b>times.</b> 1. Introduction For real-time systems the correct system functionality depends on their logical correctness as well as o [...] ...|$|R
30|$|The 27 [*]factorial design {{plus two}} {{treatments}} for the sequential executions were replicated 40 times. It constituted a matrix with 40 columns and 130 lines filled with <b>execution</b> <b>times.</b> The <b>execution</b> <b>times</b> {{added up to}} approximately 120.4 h.|$|R
30|$|The <b>execution</b> <b>times</b> of PGA-I on {{solving the}} Rastrigin {{function}} presented some extreme <b>execution</b> <b>times.</b> We investigated these outlying values, {{and we could}} reproduce them using the same PRNG seed. They were not due to erroneous measures, failures of registry, or external perturbations. By trimming them, we were effectively discarding information about {{the best and worst}} performances of the algorithm. The distribution of the <b>execution</b> <b>times</b> were also changed. This is a limitation of our method.|$|R
25|$|A faster, fully {{hardware-based}} multiplier makes instructions such as MUL and IMUL {{several times}} as fast (and more predictable) than in the 80486; the <b>execution</b> <b>time</b> is reduced from 13~42 clock cycles down to 10~11 for 32-bit operands.|$|E
25|$|Optimization will {{generally}} focus on improving {{just one or}} two aspects of performance: <b>execution</b> <b>time,</b> memory usage, disk space, bandwidth, power consumption or some other resource. This will usually require a trade-offnbsp&— where one factor is optimized at the expense of others. For example, increasing the size of cache improves runtime performance, but also increases the memory consumption. Other common trade-offs include code clarity and conciseness.|$|E
25|$|A {{compiler}} is {{a computer}} program that translates one computer language into another. To improve the <b>execution</b> <b>time</b> of the resulting code, one of the techniques of compiler optimization is register allocation, where {{the most frequently used}} values of the compiled program are kept in the fast processor registers. Ideally, values are assigned to registers so that they can all reside in the registers when they are used.|$|E
40|$|We {{introduce}} a server-based approach {{to schedule a}} general class of multiprocessor soft real-time systems with stochastic <b>execution</b> <b>times,</b> when bounded average-case tardiness is sufficient for schedulability. A key feature {{of this approach is}} that the stochastic execution-time demands can have arbitrary amounts of dependence within prespecified time intervals of bounded length. This is an important practical step forward from requiring complete independence of <b>execution</b> <b>times</b> between successive jobs of the same task. Our main result requires only averagecase utilization to be bounded by the number of processors. This constraint is mild compared to constraints on worstcase utilization because in multiprocessor systems, worstcase <b>execution</b> <b>times</b> may be orders of magnitude higher than average-case <b>execution</b> <b>times.</b> ...|$|R
40|$|This paper {{presents}} a general framework for determining average program <b>execution</b> <b>times</b> and their variance, {{based on the}} program's interval structure and control dependence graph. Average <b>execution</b> <b>times</b> and variance values are computed using frequency information from an optimized counter-based execution profile of the program. 1 Introduction It is important for a compiler to obtain estimates of <b>execution</b> <b>times</b> for subcomputations of an input program, {{if it is to}} attempt optimizations related to overhead values in the target architecture. In earlier work [SH 86 a, SH 86 b, Sar 87, Sar 89], we used estimates of <b>execution</b> <b>times</b> to facilitate the automatic partitioning and scheduling of programs written in the singleassignment language, Sisal, for parallel execution on multiprocessors. In this paper, we present a general framework for estimating average <b>execution</b> <b>times</b> in a program. This approach is based on the interval structure [ASU 86] and the control dependence relation [FOW 87], both of w [...] ...|$|R
40|$|We {{introduce}} a scheduling method where stochasticallyexecuting soft real-time tasks {{are assigned to}} simple sporadic servers with predetermined execution budgets. We show that using this method, any task system whose average-case total utilization {{is less than the}} number of processors can be scheduled so that tardiness is bounded in the average case. The constraint on average-case utilization is extremely mild compared to constraints on worst-case utilization because in multiprocessor systems, worst-case <b>execution</b> <b>times</b> may be orders of magnitude higher than average-case <b>execution</b> <b>times.</b> Unlike in previous work, the derived tardiness bound depends only on the mean and variance of <b>execution</b> <b>times.</b> For soft realtime systems where bounded tardiness is acceptable, this result eliminates the need for timing analysis to determine worst-case <b>execution</b> <b>times.</b> ...|$|R
25|$|In {{computer}} science, resource consumption often {{follows a}} form of power law distribution, and the Pareto principle {{can be applied to}} resource optimization by observing that 80% of the resources are typically used by 20% of the operations. In software engineering, it is often a better approximation that 90% of the <b>execution</b> <b>time</b> of a computer program is spent executing 10% of the code (known as the 90/10 law in this context).|$|E
25|$|The {{languages}} of this class have great practical importance {{in computer science}} {{as they can be}} parsed much more efficiently than nondeterministic context-free languages. The complexity of the program and <b>execution</b> <b>time</b> of a deterministic pushdown automaton is vastly less than that of a nondeterministic one. In the naive implementation, the latter must make copies of the stack every time a nondeterministic step occurs. The best known algorithm to test membership in any context-free language is Valiant's algorithm, taking O(n2.378) time, where n is the length of the string. On the other hand, deterministic context-free languages can be accepted in O(n) time by an LR(k) parser. This is very important for computer language translation because many computer languages belong to this class of languages.|$|E
25|$|On {{the morning}} of September 21, the Butts County Superior Court denied Davis's request to halt his execution. The Georgia Supreme Court also denied his appeal. Davis was due to be {{executed}} at 7pm EDT. The same night, Jay Carney, the White House Press Secretary, announced that President Obama would not intervene in the case (while the president could not have pardoned Davis, he did {{have the authority to}} order a federal investigation that might have led to a delay in the execution). Davis filed a request with the U.S. Supreme Court to stay his execution. Almost an hour after Davis's scheduled <b>execution</b> <b>time,</b> the Supreme Court announced they would review his petition, thereby postponing the execution. The Supreme Court, however, denied Davis's petition, after deliberating for several hours.|$|E
40|$|Impact of job {{dropping}} on the schedulability of uniprocessor probabilistic real-time systems with variable <b>execution</b> <b>times</b> Olivier Buffet, Liliana Cucu-Grosjean To cite this version: Olivier Buffet, Liliana Cucu-Grosjean. Impact of job {{dropping on}} the schedulability of unipro-cessor probabilistic real-time systems with variable <b>execution</b> <b>times.</b> 1 st International Real...|$|R
40|$|As Hadoop {{has gained}} {{popularity}} in big data era, {{it is widely}} used in various fields. The self-design and self-developed large-scale network traffic analysis cluster works well based on Hadoop, with off-line applications running on it to analyze the massive network traffic data. On purpose of scientifically and reasonably evaluating the performance of analysis cluster, we propose a performance evaluation system. Firstly, we set the <b>execution</b> <b>times</b> of three benchmark applications as the benchmark of the performance, and pick 40 metrics of customized statistical resource data. Then we identify {{the relationship between the}} resource data and the <b>execution</b> <b>times</b> by a statistic modeling analysis approach, which is composed of principal component analysis and multiple linear regression. After training models by historical data, we can predict the <b>execution</b> <b>times</b> by current resource data. Finally, we evaluate the performance of analysis cluster by the validated predicting of <b>execution</b> <b>times.</b> Experimental results show that the predicted <b>execution</b> <b>times</b> by trained models are within acceptable error range, and the evaluation results of performance are accurate and reliable...|$|R
40|$|Developing e cient {{programs}} for distributed systems is di cult because computations must be e ciently distributed and managed on multiple processors. In particular, the programmer must partition functions and data {{in an attempt}} to nd a reasonable balance between parallelism and overhead. Furthermore, it is very expensive to code an algorithm only to nd out that the implementation is not e cient. As a result, it is often necessary to determine and examine those characteristics of an algorithm {{that can be used to}} predict its suitability for a distributed computing system. In earlier work [7, 8], we presented a framework for the study of synchronization and communication e ects on the theoretical performance of common homogeneous algorithmic structures. In particular, we examined the synchronous, asynchronous, nearest-neighbor, and asynchronous master-slave structures in terms of expected <b>execution</b> <b>times.</b> In this paper, we examine the e ects of synchronization and communication on the expected <b>execution</b> <b>times</b> of heterogeneous algorithmic structures. Speci cally, we consider structures containing two di erent types of tasks, where the <b>execution</b> <b>times</b> of the tasks follow one of two di erent uniform distributions or one of two di erent normal distributions. Furthermore, we compare the expected <b>execution</b> <b>times</b> of the heterogeneous algorithmic structures with times for corresponding homogeneous structures. Finally, wedevelop bounds for the expected <b>execution</b> <b>times</b> of the heterogeneous structures and compare those bounds to simulated <b>execution</b> <b>times...</b>|$|R
25|$|The AGC {{also had}} a {{sophisticated}} software interpreter, developed by the MIT Instrumentation Laboratory, that implemented a virtual machine with more complex and capable pseudo-instructions than the native AGC. These instructions simplified the navigational programs. Interpreted code, which featured double precision trigonometric, scalar and vector arithmetic (16 and 24-bit), even an MXV (matrix × vector) instruction, could be mixed with native AGC code. While the <b>execution</b> <b>time</b> of the pseudo-instructions was increased (due {{to the need to}} interpret these instructions at runtime) the interpreter provided many more instructions than AGC natively supported and the memory requirements were much lower than in the case of adding these instructions to the AGC native language which would require additional memory built into the computer (at that time the memory capacity was very expensive). The average pseudo-instruction required about 24 ms to execute. The assembler and version control system, named YUL for an early prototype Christmas Computer, enforced proper transitions between native and interpreted code.|$|E
500|$|... p is the {{percentage}} of the <b>execution</b> <b>time</b> of the whole task concerning the parallelizable part of the task before parallelization.|$|E
500|$|PHP's single-request-per-script-execution model, and {{the fact}} that the Zend Engine is an interpreter, leads to inefficiency; as a result, various {{products}} have been developed to help improve PHP performance. In order to speed up <b>execution</b> <b>time</b> and not have to compile the PHP source code every time the web page is accessed, PHP scripts can also be deployed in the PHP engine's internal format by using an opcode cache, which works by caching the compiled form of a PHP script (opcodes) in shared memory to avoid the overhead of parsing and compiling the code every time the script runs. [...] An opcode cache, Zend Opcache, is built into PHP since version 5.5. [...] Another example of a widely used opcode cache is the Alternative PHP Cache (APC), which is available as a PECL extension.|$|E
5000|$|Benchmark—a {{method for}} {{measuring}} comparative <b>execution</b> <b>times</b> in defined cases ...|$|R
30|$|In {{multiprocessor}} SoC performance evaluation, simulating the profiled or estimated <b>execution</b> <b>times</b> (or {{number of}} operations) of tasks on abstract HW resource models {{is an effective}} way of observing combined effects of task <b>execution</b> <b>times,</b> mapping, scheduling, and HW platform parameters on resulting task response times, response time jitters, and processing element utilizations.|$|R
30|$|The 27 [*]factorial design {{plus two}} {{treatments}} for the sequential executions were replicated 40 times. It constituted a matrix with 40 columns and 130 lines filled with <b>execution</b> <b>times.</b> The <b>execution</b> <b>times</b> {{added up to}} approximately 21.3 h. The conditions to the ratio distribution to approximate to a normal distribution were checked.|$|R
2500|$|... (Note: 	Parallel {{processing}} such as two simultaneous {{single precision}} operations is permitted without additional <b>execution</b> <b>time.)</b> ...|$|E
2500|$|It {{measures}} {{only one}} aspect of performance: time, which means <b>execution</b> <b>time</b> and not the time to acquire or learn a task ...|$|E
2500|$|In 2008, Philippe Hanrigou (then at ThoughtWorks) made [...] "Selenium Grid", which {{provides}} a hub allowing the running of multiple Selenium tests concurrently on any number of local or remote systems, thus minimizing test <b>execution</b> <b>time.</b> Grid offered, as open source, a similar capability to the internal/private Google cloud for Selenium RC. Pat Lightbody had already made a private cloud for [...] "HostedQA" [...] which {{he went on to}} sell to Gomez, Inc.|$|E
40|$|Wide {{fluctuations}} in the availability of idle processor cycles and communication latencies over multiple resource administrative domains present a challenge to provide quality of service in executing grid applications. In this paper, we propose an adaptive scheduling framework, compensation-based scheduling, to provide predictable <b>execution</b> <b>times</b> by using feedback control. The compensation-based scheduling compensates resource loss during application execution by dynamically allocating additional resources. The framework is evaluated by experiments conducted on the ALiCE scheduler. Scalability simulation {{studies show that the}} gaps between actual and estimated <b>execution</b> <b>times</b> are reduced to less than 15 % of the estimated <b>execution</b> <b>times...</b>|$|R
40|$|As {{real-time}} {{embedded systems}} get more diverse and more complicated, systems with {{different types of}} tasks (e. g., periodic tasks and aperiodic tasks) are prevailing. In such a system, {{it is important that}} schedulability of periodic tasks is guaranteed {{and at the same time}} response times to aperiodic requests are short enough. Total Bandwidth Server is one of convincing task scheduling algorithms for mixed task sets of periodic and aperiodic tasks. Considering a fact that in most cases tasks' <b>execution</b> <b>times</b> are much shorter than their worst-case <b>execution</b> <b>times,</b> this paper proposes a method of reducing response <b>times</b> of aperiodic <b>execution</b> by using predictive <b>execution</b> <b>times</b> instead of worst-case <b>execution</b> <b>times</b> for deadline calculations in the total bandwidth server to obtain shorter deadlines, while ensuring the integrity of periodic tasks. In the evaluation by simulation, the proposed method combined with a resource reclaiming technique improved average response times for aperiodic tasks, by up to 22 % compared with the original total bandwidth server technique, and by up to 48 % compared with Constant Bandwidth Server, which is another algorithm appropriate for tasks with varying <b>execution</b> <b>times...</b>|$|R
5000|$|Best, {{worst and}} average case—considerations for {{estimating}} <b>execution</b> <b>times</b> in three scenarios ...|$|R
2500|$|Owning a {{high and}} {{powerful}} soprano C voice, in the intermittent low notes Thu Minh presented without recognizable speciality. After {{her efforts to}} learn from many international singers and especially inspired by Whitney Houston, she gradually overcame this drawback. She always believed: [...] "fame is not what makes me happiest, but artistic differences and uniqueness", thus, she [...] "always tries to cultivate and train herself for her own 'Thu Minh style', vocal technique and performance style". [...] Thu Minh also genuinely said that the [...] "frugal","straight-forwarded, critical and perfectionism" [...] in her music career, inherited from her mother, {{is such a good}} element for her job because it helps bring the audience her best music performances, regardless of how long it costs in <b>execution</b> <b>time.</b> For example, the album Body Language took two years to complete. Thus, in 2007, Thu Minh had shifted to Boston, Massachusetts, in the US to pursue international education on music and graduated in Berklee College of Music.|$|E
5000|$|The <b>execution</b> <b>time</b> of {{the whole}} task before the {{improvement}} of the resources of the system is denoted as [...] It includes the <b>execution</b> <b>time</b> of the part that would not benefit from {{the improvement of the}} resources and the <b>execution</b> <b>time</b> of the one that would benefit from it. The fraction of the <b>execution</b> <b>time</b> of the task that would benefit from the improvement of the resources is denoted by [...] The one concerning the part that would not benefit from it is therefore [...] Then: ...|$|E
5000|$|If {{the process}} reaches its maximum <b>execution</b> <b>time</b> or is {{otherwise}} stopped (voluntarily or via interrupt) it is reinserted into the scheduling tree {{based on its}} new spent <b>execution</b> <b>time.</b>|$|E
40|$|An {{experimental}} {{analysis of}} quantized Cell-DEVS models is presented. The experiments show that <b>execution</b> <b>times</b> {{can be reduced}} according with bx -a when quantized cell spaces are used. The error introduced has linear growth for small quanta. The experimental studies suggest how to define dynamic strategies to improve the <b>execution</b> <b>times</b> in timed Cell-DEVS. Quantization ideas are easy to apply, and the concept is generic {{to be used in}} any simulation environment...|$|R
40|$|The single-path software/hardware {{architecture}} {{has been}} conceived {{with the goal}} to support real-time task execution with highly predictable timing. By using WCET-oriented programming (WCET [...] . Worst-Case Execu-tion Time) and then converting WCET-oriented code to single-path code we do not only aim at producing code with good temporal predictability but also target at a high worst-case performance. First experiments {{presented in this paper}} show that the combination of the two concepts yields indeed code with constant, hence fully predictable <b>execution</b> <b>times.</b> Further, these <b>execution</b> <b>times</b> are very competitive com-pared to the worst-case <b>execution</b> <b>times</b> of the alternative, non WCET-oriented solutions of the programming problems we investigated. ...|$|R
40|$|Abstract — Programmable {{multiprocessor}} systems-on-chip {{are becoming}} the preferred implementation platform for embedded streaming applications. This enables using more software components, {{which leads to}} large and frequent dynamic variations of data-dependent <b>execution</b> <b>times.</b> In this context, accurate and conservative prediction of <b>execution</b> <b>times</b> helps in maintaining good audio/video quality and reducing energy consumption by dynamic evaluation {{of the amount of}} on-chip resources needed by applications. To be effective, multiprocessor systems have to employ the available parallelism. The combination of task-level parallelism and task delay variations makes predicting <b>execution</b> <b>times</b> a very hard problem. So far, under these conditions, no appropriate techniques exist for the conservative prediction of <b>execution</b> <b>times</b> with the required accuracy. In this paper, we present a novel technique for this problem, exploiting the concept of scenario-based prediction, and taking into account the transient and periodic behavior of scenarios and the effect of scenario transitions. In our MPEG- 4 shape-decoder case study, we observe no more than 11 % average overestimation. I...|$|R
