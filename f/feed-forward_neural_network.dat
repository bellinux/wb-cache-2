778|10000|Public
25|$|As {{an attempt}} to correct some of the error due to a non-zero , the usage of local linear {{weighted}} regression with ABC to reduce the variance of the posterior estimates has been suggested. The method assigns weights to the parameters according to how well simulated summaries adhere to the observed ones and performs linear regression between the summaries and the weighted parameters {{in the vicinity of}} observed summaries. The obtained regression coefficients are used to correct sampled parameters in the direction of observed summaries. An improvement was suggested in the form of nonlinear regression using a <b>feed-forward</b> <b>neural</b> <b>network</b> model. However, {{it has been shown that}} the posterior distributions obtained with these approaches are not always consistent with the prior distribution, which did lead to a reformulation of the regression adjustment that respects the prior distribution.|$|E
50|$|The tree parity {{machine is}} a special type of {{multi-layer}} <b>feed-forward</b> <b>neural</b> <b>network.</b>|$|E
5000|$|Training then {{proceeds}} in {{a manner}} similar to training a <b>feed-forward</b> <b>neural</b> <b>network</b> with backpropagation, except that the training patterns are visited in sequential order. Each training pattern consists of [...] (All of the actions for k time-steps are needed because the unfolded network contains inputs at each unfolded level.) After a pattern is presented for training, the weight updates in each instance of f (...) are summed together, then applied to all instances of f. The zero vector is typically used to initialize [...]|$|E
40|$|Back-propagation with {{gradient}} {{method is}} the most popular learning algorithm for <b>feed-forward</b> <b>neural</b> <b>networks.</b> However, it is critical to determine a proper fixed learning rate for the algorithm. In this paper, an optimized recursive algorithm is presented for online learning based on matrix operation and optimization methods analytically, which can avoid the trouble to select a proper learning rate for the gradient method. The proof of weak convergence of the proposed algorithm also is given. Although this approach is proposed for three-layer, <b>feed-forward</b> <b>neural</b> <b>networks,</b> it could be extended to multiple layer <b>feed-forward</b> <b>neural</b> <b>networks.</b> The effectiveness of the proposed algorithms applied to the identification of behavior of a two-input and two-output non-linear dynamic system is demonstrated by simulation experiments. Comment: 15 pages, 5 figure...|$|R
40|$|Human {{brain is}} a complex and {{powerful}} system that is able to solve {{a wide variety of}} tasks. The aim of many scientists is to develop a computer simulation that mimics the brain functions and solves problems the way our brains do. Very simplified models of biological <b>neural</b> <b>networks</b> are artificial <b>neural</b> <b>networks.</b> There are two different types of artificial <b>neural</b> <b>networks</b> – feed forward <b>neural</b> <b>networks</b> and recurrent <b>neural</b> <b>networks.</b> This thesis gives an overview of <b>feed-forward</b> <b>neural</b> <b>networks</b> and their working principles. The thesis is divided into two main parts. The first part is the theory of <b>feed-forward</b> <b>neural</b> <b>networks</b> and the second part is a practical example of <b>neural</b> <b>network</b> with software R. The first part gives an overview of the artificial neuron and its history. Also different types of artificial neurons are introduced. The first part includes instructions of how <b>feed-forward</b> <b>neural</b> <b>networks</b> are composed and explains how they calculate the results. Separate chapter is devoted to training artificial <b>neural</b> <b>networks.</b> The chapter gives an overview of two main training algorithms – perceptron training algorithm and back-propagation algorithm. The first is designed to train perceptrons and the second is often used in training multi-layer <b>feed-forward</b> <b>neural</b> <b>networks.</b> The last topic explains how to construct <b>feed-forward</b> <b>neural</b> <b>networks</b> with software R. It includes a tutorial of how to build a <b>neural</b> <b>network</b> that calculates the square root. The tutorial will produce a <b>neural</b> <b>network</b> which takes a single input and produces a single output. Input is the number that we want square rooting and the output is the square root of the input...|$|R
40|$|The paper {{presents}} {{the design of}} the parameters for long-term municipal rating. Modelling of the rating is realized by feed- forward <b>neural</b> <b>networks.</b> The paper deals with a layout of model for classification of municipalities by <b>feed-forward</b> <b>neural</b> <b>networks</b> and analysis his results. The paper {{presents the}} design of the parameters for long-term municipal rating. Modelling of the rating is realized by feed- forward <b>neural</b> <b>networks.</b> The paper deals with a layout of model for classification of municipalities by <b>feed-forward</b> <b>neural</b> <b>networks</b> and analysis his results...|$|R
50|$|As {{an attempt}} to correct some of the error due to a non-zero , the usage of local linear {{weighted}} regression with ABC to reduce the variance of the posterior estimates has been suggested. The method assigns weights to the parameters according to how well simulated summaries adhere to the observed ones and performs linear regression between the summaries and the weighted parameters {{in the vicinity of}} observed summaries. The obtained regression coefficients are used to correct sampled parameters in the direction of observed summaries. An improvement was suggested in the form of nonlinear regression using a <b>feed-forward</b> <b>neural</b> <b>network</b> model. However, {{it has been shown that}} the posterior distributions obtained with these approaches are not always consistent with the prior distribution, which did lead to a reformulation of the regression adjustment that respects the prior distribution.|$|E
50|$|An autoencoder is a <b>feed-forward</b> <b>neural</b> <b>network</b> {{which is}} trained to {{approximate}} the identity function. That is, it is trained to map from a vector of values {{to the same}} vector. When used for dimensionality reduction purposes, one of the hidden layers in the network is limited to contain {{only a small number}} of network units. Thus, the network must learn to encode the vector into a small number of dimensions and then decode it back into the original space. Thus, the first half of the network is a model which maps from high to low-dimensional space, and the second half maps from low to high-dimensional space. Although the idea of autoencoders is quite old, training of deep autoencoders has only recently become possible through the use of restricted Boltzmann machines and stacked denoising autoencoders. Related to autoencoders is the NeuroScale algorithm, which uses stress functions inspired by multidimensional scaling and Sammon mappings (see below) to learn a non-linear mapping from the high-dimensional to the embedded space. The mappings in NeuroScale are based on radial basis function networks.|$|E
5000|$|Similar {{ideas have}} been used in <b>feed-forward</b> <b>neural</b> <b>network</b> for {{unsupervised}} pre-training to structure a neural network, making it first learn generally useful feature detectors. Then the network is trained further by supervised back-propagation to classify labeled data. The Deep belief network model by Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued latent variables. It uses a restricted Boltzmann machine to model each new layer of higher level features. Each new layer guarantees an increase on the lower-bound of the log likelihood of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an [...] "ancestral pass") from the top level feature activations.Hinton reports that his models are effective feature extractors over high-dimensional, structured data. This work plays a keyrole in reintroducing the interests in deep neural network research and consequently leads to the developments of Deep learning, although deep belief network is no longer the main deep learning technique.|$|E
40|$|A genetic {{algorithm}} {{is used for}} the topology design of <b>feed-forward</b> <b>neural</b> <b>networks.</b> The generated <b>neural</b> <b>network</b> topology populations are then trained to approximate non-linear relationships of multiple variables. A specifically designed fitness function using the epistemological principle of dimensional homogeneity {{is used for the}} evaluation of the individual <b>neural</b> <b>network</b> generations and for the selection of the best generalizing <b>neural</b> <b>networks</b> topologies. A general theory for the topology design and the explanation of the correct generalization capability of non-linear <b>feed-forward</b> <b>neural</b> <b>networks</b> is developed, mathematically proved and explained using simulation results...|$|R
40|$|Abstract:- The paper {{presents}} {{the possibility of}} the design of frontal <b>neural</b> <b>networks</b> and <b>feed-forward</b> <b>neural</b> <b>networks</b> (without pre-processing of inputs time series) with learning algorithms on the basis genetic and eugenic algorithms and Takagi-Sugeno fuzzy inference system (with pre-processing of inputs time series) in predicting of gross domestic product development by designing a prediction models whose accuracy is superior to the models used in praxis [1, 2]. Key-Words:- Gross domestic product, frontal <b>neural</b> <b>networks,</b> <b>feed-forward</b> <b>neural</b> <b>networks,</b> Takagi-Sugeno fuzzy inference systems, genetic and eugenic algorithms, EuSANE algorithm. ...|$|R
40|$|This paper investigates a {{technique}} for creating sparsely connected <b>feed-forward</b> <b>neural</b> <b>networks</b> {{which may be}} capable of producing networks that have very large input and output layers. The architecture appears to be particularly suited to tasks that involve sparse training data as {{it is able to}} take advantage of the sparseness to further reduce training time. Some initial results are presented based on tests on the 16 bit compression problem. 1. Introduction <b>Feed-forward</b> <b>neural</b> <b>networks</b> are generally restricted to relatively few neurons. Very large networks take a prohibitively long time to train and contain so many connections that a huge training set is often required in order to obtain good generalisation. This restriction on network size prevents the use of <b>feed-forward</b> <b>neural</b> <b>networks</b> in applications which require large numbers of inputs and outputs. The training time of a feed-forward network may be reduced by restricting the number of its interconnections. One way of ensuring [...] ...|$|R
40|$|Abstract:- The paper {{presents}} (on {{the basis}} of passive investment strategies analysis) {{the design of the}} Takagi-Sugeno fuzzy inference system and the <b>feed-forward</b> <b>neural</b> <b>network</b> (with pre-processing of inputs time series) for prediction of the index fund. By means of the Takagi-Sugeno fuzzy inference system and the <b>feed-forward</b> <b>neural</b> <b>network</b> the investor is able to predict the closing price of the index fund. Key-Words:- Takagi-Sugeno fuzzy inference systems, <b>feed-forward</b> <b>neural</b> <b>network,</b> prediction, index fund, indicator...|$|E
40|$|The {{approach}} for discrete dynamical system modeling based on <b>feed-forward</b> <b>neural</b> <b>network</b> is considered. The proposed method is realized in two stages. The {{first one is}} training of the <b>feed-forward</b> <b>neural</b> <b>network</b> with given experimental data obtained for the designed system. The second stage is simulation with the neural network model. The variant of the method for <b>feed-forward</b> <b>neural</b> <b>network</b> training known as “backpropagation ” is developed. The effective realization in matrix form of the proposed algorithm is given. This approach is successfully applied to the design problem in time domain for the recursive digital filter. Some results and graphical representations for lowpass recursive digital filters are given...|$|E
30|$|The {{most common}} neural network {{architecture}} is the <b>feed-forward</b> <b>neural</b> <b>network.</b> Feed-forward network is the network structure {{in which the}} information or signals will propagate only in one direction, from input to output. A three layered <b>feed-forward</b> <b>neural</b> <b>network</b> with back propagation algorithm can approximate any nonlinear continuous function to an arbitrary accuracy (Brown and Harris 1994; Hornick et al. 1989).|$|E
30|$|ELM {{has good}} {{generalization}} performance. And the experimental {{results show that}} the ELM can achieve good generalization performance in most cases and can learn faster than <b>feed-forward</b> <b>neural</b> <b>networks</b> [21].|$|R
40|$|<b>Feed-forward</b> <b>neural</b> <b>networks</b> have {{recently}} been applied in situations where an analysis based on the logistic regression model {{would have been a}} standard statistical approach; direct comparisons of results, however, are seldomly attempted. We therefore present a comparative investigation of both logistic regression models and <b>feed-forward</b> <b>neural</b> <b>networks</b> including some extensions. The theoretical features and properties are reviewed and illustrated in two examples, also discussing practical problems with their application. In Part II of the paper some further important aspects of approximation, overfitting and model selection are investigated in more detail both analytically and by means of simulation studies...|$|R
30|$|In {{this work}} we test two {{different}} predictive systems {{based on the}} model proposed in (Mooney et al. 2011). This model is a N-to- 1 <b>neural</b> <b>network,</b> or N 1 -NN, composed by a sequence of two two-layered <b>feed-forward</b> <b>neural</b> <b>networks.</b>|$|R
40|$|Optimization {{algorithms}} {{are normally}} influenced by meta-heuristic approach. In recent years several hybrid methods for optimization are developed {{to find out}} a better solution. The proposed work using meta-heuristic Nature Inspired algorithm is applied with back-propagation method to train a <b>feed-forward</b> <b>neural</b> <b>network.</b> Firefly algorithm is a nature inspired meta-heuristic algorithm, and it is incorporated into back-propagation algorithm to achieve fast and improved convergence rate in training <b>feed-forward</b> <b>neural</b> <b>network.</b> The proposed technique is tested over some standard data set. It is found that proposed method produces an improved convergence within very few iteration. This performance is also analyzed and compared to genetic algorithm based back-propagation. It is observed that proposed method consumes less time to converge and providing improved convergence rate with minimum <b>feed-forward</b> <b>neural</b> <b>network</b> design. Comment: 9 pages, 10 figures, Published with International Journal of Computer Applications (IJCA...|$|E
40|$|The outputs of {{non-linear}} <b>feed-forward</b> <b>neural</b> <b>network</b> are positive, {{which could}} be treated as probability when they are normalized to one. If we take Entropy-Based Principle into consideration, the outputs for each sample could be represented as the distribution of this sample for different clusters. Entropy-Based Principle is the principle with which we could estimate the unknown distribution under some limited conditions. As this paper defines two processes in <b>Feed-Forward</b> <b>Neural</b> <b>Network,</b> our limited condition is the abstracted features of samples which are worked out in the abstraction process. And the final outputs are the probability distribution for different clusters in the clustering process. As Entropy-Based Principle is considered into the <b>feed-forward</b> <b>neural</b> <b>network,</b> a clustering method is born. We have conducted some experiments on six open UCI datasets, comparing with a few baselines and applied purity as the measurement. The results illustrate that our method outperforms all the other baselines that are most popular clustering methods. Comment: This paper has been published in ICANN 201...|$|E
40|$|Abstract — Neural {{network is}} one of the {{successful}} methods for protein secondary structure prediction. Day to day this technology is modified, improved, even other methods also combined with it to get better result. In this paper we trained <b>feed-forward</b> <b>neural</b> <b>network</b> with proteins for secondary structure prediction. Using Java Object Oriented Neural Engine (JOONE) our achieved accuracy for helix prediction is 71 % and for sheet prediction is 65 %. This paper is expected to benefit researchers in proteomics by presenting a summary of developments of neural network in this area. Index Terms — α-helix, β-sheet, bioinformatics, <b>feed-forward</b> <b>neural</b> <b>network.</b> ...|$|E
40|$|Parkinson’s {{disease is}} a {{worldwide}} frequent neurodegenerative disorder with increasing incidence. Speech disturbance appears during the progression of the disease. UPDRS is a gold standard tool for diagnostic and follow up of the disease. We aim at estimating the UPDRS score based on biomedical voice recordings. In this paper, we study the hubness phenomenon in context of the UPDRS score estimation and propose hubness-aware error correction for <b>feed-forward</b> <b>neural</b> <b>networks</b> {{in order to increase}} the accuracy of estimation. We perform experiments on publicly available datasets derived form real voice data and show that the proposed technique systematically increases the accuracy of various <b>feed-forward</b> <b>neural</b> <b>networks...</b>|$|R
40|$|The paper {{presents}} {{the possibility of}} the design of <b>feed-forward</b> <b>neural</b> <b>networks</b> with pre-processing inputs with learning algoritm Back-propagation in predicing of gross domestic product development. For design models are exploited index of leading economic indicators and diffusive index of leading economic indicators as a representative of leading indexes which enabled predicting break-even points. The paper {{presents the}} possibility of the design of <b>feed-forward</b> <b>neural</b> <b>networks</b> with pre-processing inputs with learning algoritm Back-propagation in predicing of gross domestic product development. For design models are exploited index of leading economic indicators and diffusive index of leading economic indicators as a representative of leading indexes which enabled predicting break-even points...|$|R
3000|$|Artificial <b>neural</b> <b>networks</b> {{are based}} on this idea, since they were {{designed}} to mimic the biological <b>neural</b> <b>networks</b> found in the human brain. They are formed of groups of artificial neurons connected together {{in much the same way}} as the brains neurons. Connections between artificial neurons are determined by learning processes, similarly as connections between human brain neurons are determined and modified by learning and experience acquired during time. Commonly, <b>feed-forward</b> <b>neural</b> <b>networks</b> are employed, in which neurons are connected using a [...] "feed-forward" [...] structure that allows signals to travel from input to output only. Recurrent <b>neural</b> <b>networks,</b> including loop connections that allow signals to travel both directions, are potentially very powerful and more biologically plausible than <b>feed-forward</b> <b>neural</b> <b>networks.</b> This encourages the application of recurrent <b>neural</b> <b>networks</b> to complex recognition tasks.|$|R
30|$|In this research, we {{proposed}} a PV yield prediction system based on multiple <b>feed-forward</b> <b>neural</b> <b>network</b> irradiance forecast models. The PV power forecast based on this system outperforms the PV persistence power forecast model.|$|E
40|$|In {{this paper}} we {{introduce}} two {{artificial neural network}} formulations {{that can be used}} to assess the preference ratings from the pairwise comparison matrices of the Analytic Hierarchy Process. First, we introduce a modified Hopfield network that can determine the vector of preference ratings associated with a positive reciprocal comparison matrix. The dynamics of this network are mathematically equivalent to the power method, a widely used numerical method for computing the principal eigenvectors of square matrices. However, this Hopfield network representation is incapable of generalizing the preference patterns, and consequently is not suitable for approximating the preference ratings if the pairwise comparison judgments are imprecise. Second, we present a <b>feed-forward</b> <b>neural</b> <b>network</b> formulation that does have the ability to accurately approximate the preference ratings. We use a simulation experiment to verify the robustness of the <b>feed-forward</b> <b>neural</b> <b>network</b> formulation with respect to imprecise pairwise judgments. From the results of these experiment, we conclude that <b>feed-forward</b> <b>neural</b> <b>network</b> formulation appears to be a powerful tool for analyzing discrete alternative multicriteria decision problems with imprecise or fuzzy ratio-scale preference judgments...|$|E
40|$|Abstract. Thermal {{deformation}} is a nonlinear dynamic {{phenomenon and}} is one of the significant factors for the accuracy of machine tools. In this study, a dynamic <b>feed-forward</b> <b>neural</b> <b>network</b> model is built to predict the thermal deformation of machine tool. The temperatures and thermal deformations data at present and past sampling time interval are used train the proposed neural model. Thus, it can model dynamic and the nonlinear relationship between input and output data pairs. According to the comparison results, the proposed neural model can obtain better predictive accuracy than that of some other neural model. Keywords: <b>Feed-forward</b> <b>neural</b> <b>network,</b> Thermal deformation, Neural Prediction mode...|$|E
40|$|Some of {{the main}} results in the {{mathematical}} evaluation of <b>neural</b> <b>networks</b> as information processing systems are discussed. The basic operation of feedback and <b>feed-forward</b> <b>neural</b> <b>networks</b> is described. Their memory capacity and computing power are considered. The concept of learning by example {{as it applies to}} <b>neural</b> <b>networks</b> is examined...|$|R
40|$|A new {{learning}} algorithm for <b>feed-forward</b> <b>neural</b> <b>networks</b> {{based on}} linear program-ming is introduced. This alternative to back-propagation gives {{faster and more}} reliable learning on reasonably sized examples. Extensions of the method for efficient (approxi-mate) implementations in large networks are considered...|$|R
50|$|Extreme Learning Machines (ELM) is {{a special}} case of single hidden layer <b>feed-forward</b> <b>neural</b> <b>networks</b> (SLFNs) where in the input weights and the hidden node biases can be chosen at random. Many {{variants}} and developments are made to the ELM for multiclass classification.|$|R
40|$|Abstract: An {{evolutionary}} {{neural network}} modeling approach for software cumulative failure prediction based on <b>feed-forward</b> <b>neural</b> <b>network</b> is proposed. A real coded genetic algorithm {{is used to}} optimize the mean square of the error produced by training a neural network established by Aljahdali S. [3]. In this paper we present a real coded genetic algorithm that uses the appropriate operators for this encoding type to train <b>feed-forward</b> <b>neural</b> <b>network.</b> We describe the genetic algorithm and we also experimentally compare our approach with the back propagation learning algorithm for the regression model order 4. Numerical results show that both the goodness-of-fit and the next-step-predictability of our proposed approach have greater accuracy in predicting software cumulative failure compared to other approaches...|$|E
40|$|Abstract: In modern {{protection}} relays, {{the accurate}} and fast fault location {{is an essential}} task for transmission line protection {{from the point of}} service restoration and reliability. The applications of neural networks based fault location techniques to transmission line are available in many papers. However, almost all the studies have so far employed the FNN (<b>feed-forward</b> <b>neural</b> <b>network)</b> trained with back-propagation algorithm (BPNN) which has a better structure and been widely used. But there are still many drawbacks if we simply use <b>feed-forward</b> <b>neural</b> <b>network,</b> such as slow training rate, easy to trap into local minimum point, and bad ability on global search. In this paper, <b>feed-forward</b> <b>neural</b> <b>network</b> trained by PSO (particle swarm optimization) algorithm is proposed for fault location scheme in 500 kV transmission system with distributed parameters presentation. The purpose is to simulate distance protection relay. The algorithm acts as classifier which requires phasor measurements data {{from one end of the}} transmission line and DFT (discrete Fourier transform). Extensive simulation studies carried out using MATLAB show that the proposed scheme has the ability to give a good estimation of fault location under various fault conditions...|$|E
30|$|In {{the present}} work, we propose ICA for {{optimizing}} the weights of <b>feed-forward</b> <b>neural</b> <b>network.</b> Then simulation results demonstrate the effectiveness and {{potential of the}} new proposed network for asphaltene precipitation prediction compared with scaling model (Hu and Guo 2001) using the same data.|$|E
40|$|In {{this paper}} we propose and examine new {{approaches}} in smoothing transition autoregressive (STAR) models. Firstly, a new STAR function is proposed, which is the hyperbolic tangent sigmoid function. Secondly, we propose <b>Feed-Forward</b> <b>Neural</b> <b>Networks</b> Smoothing Transition Autoregressive (FFNN-STAR) models. We examine the stock returns of US S&P 500, FTSE- 100 in UK stock index, DAX index in Germany and CAC- 40 in France and we apply bootstrapping ordinary least squares simulated regressions, while also GARCH models with bootstrapping simulations can be applied as well. The results {{are in favor of}} <b>neural</b> <b>networks,</b> while in almost all cases the forecasting performance of <b>Feed-Forward</b> <b>Neural</b> <b>Networks</b> STAR models is superior to conventional STAR models. This paper can be a guide and set up the fundamentals for further advanced research in econometrics and time-series analysis...|$|R
40|$|Various techniques, {{including}} <b>feed-forward</b> <b>neural</b> <b>networks,</b> {{are applied}} to the time series prediction problem. The forecasting of occupancy on a telephone trunk group is taken as a case study. The relative performances of the techniques are reported. Theoretical justifications are provided for the results. ...|$|R
40|$|Proper {{initialization}} {{is one of}} {{the most}} important prerequisites for fast convergence of <b>feed-forward</b> <b>neural</b> <b>networks</b> like high order and multilayer perceptrons. This publication aims at determining the optimal value of the initial weightvariance (or range), which is the principal parameter of random weight initialization methods for both types of <b>neural</b> <b>networks...</b>|$|R
