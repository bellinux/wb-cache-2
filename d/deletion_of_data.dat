58|10000|Public
5|$|Prior to the 1980s crimes {{involving}} {{computers were}} dealt with using existing laws. The first computer crimes were {{recognized in the}} 1978 Florida Computer Crimes Act, which included legislation against the unauthorized modification or <b>deletion</b> <b>of</b> <b>data</b> on a computer system. Over {{the next few years}} the range of computer crimes being committed increased, and laws were passed to deal with issues of copyright, privacy/harassment (e.g., cyber bullying, cyber stalking, and online predators) and child pornography. It was not until the 1980s that federal laws began to incorporate computer offences. Canada was the first country to pass legislation in 1983. This was followed by the US Federal Computer Fraud and Abuse Act in 1986, Australian amendments to their crimes acts in 1989 and the British Computer Misuse Act in 1990.|$|E
25|$|Note {{also that}} any {{shareholder}} who ever has {{enough information to}} decrypt the content at any point is able to take and store a copy of X. Consequently although tools and techniques such as Vanish can make data irrecoverable within their own system after a time, {{it is not possible}} to force the <b>deletion</b> <b>of</b> <b>data</b> once a malicious user has seen it. This is one of the leading conundrums of Digital Rights Management.|$|E
5000|$|... people {{registered}} in files must {{be informed of}} their rights, for example, for rectification and <b>deletion</b> <b>of</b> <b>data</b> on demand; ...|$|E
40|$|Data mining {{algorithms}} {{have been}} the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and <b>deletion</b> <b>of</b> blocks <b>of</b> <b>data.</b> Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and <b>deletions</b> <b>of</b> <b>data</b> records...|$|R
40|$|Data mining {{algorithms}} {{have been}} the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and <b>deletion</b> <b>of</b> blocks <b>of</b> <b>data.</b> Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and <b>deletions</b> <b>of</b> <b>data</b> records. In this paper, we consider a dynamic environment that evolves through systematic addition or <b>deletion</b> <b>of</b> blocks <b>of</b> <b>data.</b> We introduce a new dimension called the data span dimension, which allows userdefined selections of a temporal subset of the database. Taking this new degree of freedom into account, we describe efficient model maintenance algorithms for frequent itemsets and clusters. We then describe a generic algorithm that takes any traditional incremental model maintenance algorithm and transforms it into an algorithm that allows restrictions on the data span di [...] ...|$|R
40|$|Data {{from the}} Austrian Library Network's Aleph 500 union {{catalogue}} are exported incrementally {{in order to}} feed external applications such as search engines and other catalogues. This presentation deals with the problems connected with changes and <b>deletions</b> <b>of</b> <b>data,</b> particularly when exporting local holdings information. Solutions already implemented include work flows for exporting data for several search engines (Tri-National Catalogue, eDOC, Google) ...|$|R
5000|$|Deletion anomaly. Under certain circumstances, <b>deletion</b> <b>of</b> <b>data</b> {{representing}} certain facts necessitates <b>deletion</b> <b>of</b> <b>data</b> representing {{completely different}} facts. The [...] "Faculty and Their Courses" [...] relation {{described in the}} previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, we must delete {{the last of the}} records on which that faculty member appears, effectively also deleting the faculty member, unless we set the Course Code to null. This phenomenon is known as a deletion anomaly.|$|E
50|$|The {{capability}} of handling a base relation or a derived relation {{as a single}} operand applies {{not only to the}} retrieval of data but also to the insertion, update and <b>deletion</b> <b>of</b> <b>data.</b>|$|E
50|$|A {{part of any}} {{effective}} {{data retention}} policy is the permanent deletion of the retained data; achieving secure <b>deletion</b> <b>of</b> <b>data</b> by encrypting the data when stored, and then deleting the encryption key after a specified retention period. Thus, effectively deleting the data object and its copies stored in online and offline locations.|$|E
40|$|AbstractÐData mining {{algorithms}} {{have been}} the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and <b>deletion</b> <b>of</b> blocks <b>of</b> <b>data.</b> Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and <b>deletions</b> <b>of</b> <b>data</b> records. In this paper, we consider a dynamic environment that evolves through systematic addition or <b>deletion</b> <b>of</b> blocks <b>of</b> <b>data.</b> We introduce a new dimension, called the data span dimension, which allows user-defined selections of a temporal subset of the database. Taking this new degree of freedom into account, we describe efficient model maintenance algorithms for frequent itemsets and clusters. We then describe a generic algorithm that takes any traditional incremental model maintenance algorithm and transforms it into an algorithm that allows restrictions on the data span dimension. We also develop an algorithm for automatically discovering a specific class of interesting block selection sequences. In a detailed experimental study, we examine the validity and performance of our ideas on synthetic and real datasets. Index TermsÐData Mining, dynamic databases, evolving data, trends. ...|$|R
5000|$|Update - Insertion, modification, and <b>deletion</b> <b>of</b> {{the actual}} <b>data.</b>|$|R
40|$|International audienceThis paper {{presents}} a new, simple, and efficient data structure, namely, the o̱rdered s̱egment (OS) : a mono dimensional string array {{that we have}} been using in our classification <b>of</b> big <b>data.</b> The essential idea in construction of OS is {{to make use of the}} redundancies that abound user-data. OS enables us to performs efficient retrieval, insertions and <b>deletions</b> <b>of</b> <b>data.</b> The theoretical and experimental observations show that the method presented is more practical than existing ones considering the use of dynamic string sets for the classifications of huge user-files...|$|R
50|$|Many {{application}} developers compose SQL {{statements by}} concatenating strings {{and do not}} use prepared statement; {{in this case the}} application is susceptible to a SQL injection attack. The technique transforms an application SQL statement from an innocent SQL call to a malicious call that can cause unauthorized access, <b>deletion</b> <b>of</b> <b>data,</b> or theft of information.|$|E
50|$|The Gutmann {{method is}} an {{algorithm}} for securely erasing {{the contents of}} computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper Secure <b>Deletion</b> <b>of</b> <b>Data</b> from Magnetic and Solid-State Memory in July 1996, it involved writing a series of 35 patterns over the region to be erased.|$|E
50|$|As for secure <b>deletion</b> <b>of</b> <b>data,</b> {{using the}} ATA Secure Erase command is recommended, as the drive itself knows the most {{effective}} method to truly reset its data. A program such as Parted Magic {{can be used for}} this purpose. In 2014, Asus was the first company to introduce a Secure Erase feature built into the UEFI of its Republic of Gamers series of PC motherboards.|$|E
50|$|Additionally, {{any person}} may ask in writing a company (managing data files) the {{correction}} or <b>deletion</b> <b>of</b> any personal <b>data.</b> The company must respond within thirty days.|$|R
40|$|In {{a variety}} of applications, {{we need to keep}} track of the {{development}} <b>of</b> a <b>data</b> set over time. For maintaining and querying this multi version data I/O-efficiently, external memory data structures are required. In this paper, we present a probabilistic self-balancing persistent data structure in external memory called the persistent buffer tree, which supports insertions, updates and <b>deletions</b> <b>of</b> <b>data</b> items at the present version and range queries for any version, past or present. The persistent buffer tree is I/O-optimal in the sense that the expected amortized I/O performance bounds are asymptotically the same as the deterministic amortized bounds of the (single version) buffer tree in the worst case. Comment: 11 pages with no figures, unpublishe...|$|R
5000|$|Another {{method for}} {{eliminating}} spurious data is called Peirce's criterion. It was developed {{a few years}} before Chauvenet's criterion was published, and it is a more rigorous approach to the rational <b>deletion</b> <b>of</b> outlier <b>data.</b> [...] Other methods such as Grubbs' test for outliers are mentioned under the listing for Outlier.|$|R
50|$|Note {{also that}} any {{shareholder}} who ever has {{enough information to}} decrypt the content at any point is able to take and store a copy of X. Consequently although tools and techniques such as Vanish can make data irrecoverable within their own system after a time, {{it is not possible}} to force the <b>deletion</b> <b>of</b> <b>data</b> once a malicious user has seen it. This is one of the leading conundrums of Digital Rights Management.|$|E
5000|$|Error {{correction}} {{and loss}} of information: The most challenging problem within data cleansing remains the correction of values to remove duplicates and invalid entries. In many cases, the available information on such anomalies is limited and insufficient to determine the necessary transformations or corrections, leaving the deletion of such entries as a primary solution. The <b>deletion</b> <b>of</b> <b>data,</b> though, leads to loss of information; this loss can be particularly costly {{if there is a}} large amount of deleted data.|$|E
5000|$|The Article 29 Data Protection Working Party {{delivered}} {{an opinion}} on April 13, 2016, stating that the Privacy Shield offers major improvements compared to the Safe Harbour decisions, but that three major points of concern still remain. They relate to <b>deletion</b> <b>of</b> <b>data,</b> collection of massive amounts of data, and clarification of the new Ombudsperson mechanism. The European Data Protection Supervisor issued {{an opinion on}} 30 May 2016 in which he stated that [...] "the Privacy Shield, as it stands, is not robust enough to withstand future legal scrutiny before the European Court".|$|E
30|$|Integrity mainly {{includes}} the storage integrity <b>of</b> the medical <b>data</b> administrator and the query {{integrity of the}} medical staff or patient personnel. Storage integrity means that the tampering, additions, and <b>deletions</b> <b>of</b> stored <b>data</b> by cloud service providers or other attackers are perceived by medical data administrators. Query integrity {{includes the}} correctness, completeness, and freshness of search results.|$|R
40|$|Abstract. In {{a variety}} of applications, {{we need to keep}} track of the {{development}} <b>of</b> a <b>data</b> set over time. For maintaining and querying these multiversion data efficiently, external storage structures are an absolute necessity. We propose a multiversion B-tree that supports insertions and <b>deletions</b> <b>of</b> <b>data</b> items at the current version and range queries and exact match queries for any version, current or past. Our multiversion B-tree is asymptotically optimal in the sense that the time and space bounds are asymptotically the same as those of the (single-version) B-tree in the worst case. The technique we present for transforming a (single-version) B-tree into a multiversion B-tree is quite general: it applies to a number of hierarchical external access structures with certain properties directly, and it can be modified for others...|$|R
40|$|Peer-to-peer (P 2 P) systems {{provide a}} robust, {{scalable}} and decentralized way {{to share and}} publish data. However, most existing P 2 P systems only provide a very rudimentary query facility; they only support equality or keyword search queries over files. We believe that future P 2 P applications, such as resource discovery on a grid, will require more complex query functionality. As a first step towards this goal, we propose a new distributed, fault-tolerant P 2 P index structure for resource discovery applications called the P-tree. Ptrees efficiently evaluate range queries in addition to equality queries. We describe algorithms to maintain a P-tree under insertions and <b>deletions</b> <b>of</b> <b>data</b> items/peers, and evaluate its performance using both a simulation and a real distributed implementation. Our results show the efficacy of our approach. 1...|$|R
50|$|Prior to the 1980s crimes {{involving}} {{computers were}} dealt with using existing laws. The first computer crimes were {{recognized in the}} 1978 Florida Computer Crimes Act, which included legislation against the unauthorized modification or <b>deletion</b> <b>of</b> <b>data</b> on a computer system. Over {{the next few years}} the range of computer crimes being committed increased, and laws were passed to deal with issues of copyright, privacy/harassment (e.g., cyber bullying, cyber stalking, and online predators) and child pornography. It was not until the 1980s that federal laws began to incorporate computer offences. Canada was the first country to pass legislation in 1983. This was followed by the US Federal Computer Fraud and Abuse Act in 1986, Australian amendments to their crimes acts in 1989 and the British Computer Misuse Act in 1990.|$|E
30|$|The linear data {{structure}} or abstractly a sequential collection {{is called a}} queue. The principal operations on the collection of data are the addition {{of them to the}} rear terminal position, known as enqueuing, and <b>deletion</b> <b>of</b> <b>data</b> from the front terminal position, known as dequeuing [20].|$|E
30|$|Real-world {{applications}} undergo tremendous modifications due to continuous addition and <b>deletion</b> <b>of</b> <b>data.</b> Handling dynamic databases is {{a gruesome}} task for pattern mining techniques as {{the data are}} continuously updated, generating new rules and invalidating the existing ones. Pattern mining techniques invest huge amount of time, processing the newly updated database.|$|E
40|$|In {{a variety}} of applications, weneedtokeep track of the {{development}} <b>of</b> a <b>data</b> set over time. For maintaining and querying these multiversion data e ciently, external storage structures are an absolute necessity. We propose a multiversion B-tree that supports insertions and <b>deletions</b> <b>of</b> <b>data</b> items at the currentversion, and range queries and exact match queries for any version, current or past. Our multiversion B-tree is asymptotically optimal {{in the sense that}} the time and space bounds are asymptotically the same as those of the (single version) B-tree in the worst case. The technique we present for transforming a (single version) B-tree into a multiversion B-tree is quite general: it applies to a number of hierarchical external access structures with certain properties directly, and it can be modi ed for others. y isys software gmbh, Ensisheimer Str. 2 a, D{ 79110 Freiburg i. Br...|$|R
40|$|We {{describe}} efficient {{methods for}} organizing and maintaining large multidimensional data sets in external memory. This is particular important as access to external memory is currently several order of magnitudes slower than access to main memory, and current technology advances {{are likely to}} make this gap even wider. We focus particularly on multidimensional data sets which must be kept simultaneously sorted under several total orderings: these orderings may be defined by the user, and may also be changed dynamically by the user throughout the lifetime <b>of</b> the <b>data</b> structures, according to the application at hand. Besides standard insertions and <b>deletions</b> <b>of</b> <b>data,</b> our proposed solution can perform efficiently split and concatenate operations on the whole data sets according to any ordering. This allows the user: (1) to dynamically rearrange any ordering of a segment <b>of</b> <b>data,</b> in a time that is faster than recomputing the new ordering from scratch; (2) to efficiently answer queries rel [...] ...|$|R
40|$|We propose an {{asymptotically}} optimal multiversion B-tree. In our setting, insertions and <b>deletions</b> <b>of</b> <b>data</b> {{items are}} allowed {{only for the}} present version, whereas range queries and exact match queries are allowed for any version, present or past. The technique we present for transforming a (usual single version) B-tree into a multiversion B-tree is more general: it applies {{to a number of}} spatial and non-spatial hierarchical external access structures with certain properties directly, and it can be modified for others. For the B-tree and several other hierarchical external access structures, multiversion capabilities come at no extra cost, neither for storage space nor for runtime, asymptotically in the worst case. The analysis of the behavior of the multiversion B-tree shows that the constant loss of efficiency is low enough to make our suggestion not only a theoretical, but also a practical one. 1 Introduction The importance <b>of</b> maintaining <b>data</b> not only in their latest version, but [...] ...|$|R
40|$|We firstly suggest new cache policy {{applying}} {{the duty to}} delete invalid cache data on Non-volatile Memory (NVM). This cache policy includes generating random data and overwriting the random data into invalid cache data. Proposed cache policy is more economical and effective regarding perfect <b>deletion</b> <b>of</b> <b>data.</b> It is ensure that the invalid cache data in NVM is secure against malicious hackers. Comment: 3 pages, 8 figure...|$|E
40|$|We firstly suggest privacy {{protection}} cache policy applying {{the duty to}} delete personal information on a hybrid main memory system. This cache policy includes generating random data and overwriting the random data into the personal information. Proposed cache policy is more economical and effective regarding perfect <b>deletion</b> <b>of</b> <b>data.</b> Comment: 2 pages, 3 figures, IEEE Transactions on Very Large Scale Integration Systems. arXiv admin note: text overlap with arXiv: 1707. 0284...|$|E
40|$|As {{the amount}} of digital data grows, so does the theft of {{sensitive}} data through the loss or misplacement of laptops, thumb drives, external hard drives, and other electronic storage media. Sensitive data may also be leaked accidentally due to improper disposal or resale of storage media. To protect the secrecy of the entire data lifetime, we must have confidential ways to store and delete data. This survey summarizes and compares existing methods of providing confidential storage and <b>deletion</b> <b>of</b> <b>data</b> in personal computing environments. 1...|$|E
50|$|<b>Deletion</b> <b>of</b> outlier <b>data</b> is a {{controversial}} practice frowned on by many scientists and science instructors; while Chauvenet's criterion provides an objective and quantitative method for data rejection, {{it does not}} make the practice more scientifically or methodologically sound, especially in small sets or where a normal distribution cannot be assumed. Rejection of outliers is more acceptable in areas of practice where the underlying model of the process being measured and the usual distribution of measurement error are confidently known.|$|R
40|$|The {{results of}} the space shuttle {{automatic}} landing support program studies performed from November 1, 1975 to March 31, 1976 were summarized. The following subjects were discussed: (1) software definition review (TAEM pre-final and Autoland only), (2) software definition review (KPIT scheduling), (3) <b>deletion</b> <b>of</b> air <b>data,</b> (4) June 1975 aero data update and check runs, (5) flat turn study, (6) guidance mode switching study, (7) flat turn study-additional data, (8) trajectory shaping study, (9) aero data tolerances, and (10) turbulence model study...|$|R
3000|$|... b {{for each}} of the five scans with the {{following}} five combinations: unmodified experimental IF with unmodified TAC or with TAC after <b>deletion</b> <b>of</b> every second <b>data</b> point or with smoothed TAC, as well as smoothed IF with unmodified TAC or with smoothed TAC. To visualize the effect of the chosen v [...]...|$|R
