249|160|Public
25|$|For example, {{the colors}} red, green, {{blue and yellow}} are hyponyms. They fall under the general term of color, which is the <b>hypernym.</b>|$|E
25|$|Today, {{owing to}} {{technological}} convergence, the word fluoroscopy is widely {{understood to be}} a <b>hypernym</b> of all the earlier names for moving pictures taken with X-rays, both live and recorded. Also owing to technological convergence, radiography, CT, and fluoroscopy are now all digital imaging modes using X-rays with image analysis software and easy data storage and retrieval. Just as movies, TV, and web videos are to a substantive extent no longer separate technologies but only variations on common underlying digital themes, so too are the X-ray imaging modes. And indeed, the term X-ray imaging is the ultimate <b>hypernym</b> that unites all of them, even subsuming both fluoroscopy and 4DCT. However, it may be many decades before the earlier hyponyms fall into disuse, not least because the day when 4D CT displaces all earlier forms of moving X-ray imaging may yet be distant.|$|E
2500|$|Many names {{exist in}} the medical {{literature}} for moving pictures taken with X-rays. They include fluoroscopy, fluorography, cinefluorography, photofluorography, fluororadiography, kymography (electrokymography, roentgenkymography), cineradiography (cine), videofluorography, and videofluoroscopy. Today the word fluoroscopy is widely understood to be a <b>hypernym</b> of all the aforementioned terms, which explains why {{it is the most}} commonly used and why the others are declining in usage. The profusion of names is an idiomatic artifact of technological change, as follows: ...|$|E
40|$|In this paper, we {{show for}} the first time how distributionally-induced {{semantic}} classes can be helpful for extraction of <b>hypernyms.</b> We present a method for (1) inducing sense-aware semantic classes using distributional semantics and (2) using these induced semantic classes for filtering noisy hypernymy relations. Denoising of <b>hypernyms</b> is performed by labeling each semantic class with its <b>hypernyms.</b> On one hand, this allows us to filter out wrong extractions using the global structure of the distributionally similar senses. On the other hand, we infer missing <b>hypernyms</b> via label propagation to cluster terms. We conduct a large-scale crowdsourcing study showing that processing of automatically extracted <b>hypernyms</b> using our approach improves the quality of the hypernymy extraction both in terms of precision and recall. Furthermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a benchmarking dataset. Comment: Submitted to the Conference on Language Resources and Evaluation (LREC 2018...|$|R
5000|$|... {{replacing}} {{less frequent}} terms with their <b>hypernyms</b> (generalization) from target lexicon.|$|R
40|$|Can {{a system}} that “learns from reading” figure out on it’s own the {{semantic}} classes of arbitrary noun phrases? This is essential for text understanding, given the limited coverage of proper nouns in lexical resources such as WordNet. Previous methods that use lexical patterns to discover <b>hypernyms</b> suffer from limited precision and recall. We present methods based on lexical patterns that find <b>hypernyms</b> of arbitrary noun phrases with high precision. This more than doubles the recall of proper noun <b>hypernyms</b> provided by WordNet at a modest cost to precision. We also present a novel method using a Hidden Markov Model (HMM) to extend recall further...|$|R
5000|$|... hypernyms: Y is a <b>hypernym</b> of X {{if every}} X is a (kind of) Y (canine is a <b>hypernym</b> of dog) ...|$|E
5000|$|In linguistics, is-a {{relations}} are called hyponymy. Words that describe categories are called hypernyms and {{words that are}} examples of categories are hyponyms. In the simple biology example dog is a <b>hypernym</b> and Fido {{is one of its}} hyponyms. A word can be both a hyponym and a <b>hypernym.</b> For example, dog is a hyponym of mammal and also a <b>hypernym</b> of Fido. In ATC programs {{one of the most important}} tasks is the discovery of <b>hypernym</b> and hyponym relations among words.|$|E
5000|$|... hypernym: {{the verb}} Y is a <b>hypernym</b> of the verb X if the {{activity}} X is a (kind of) Y (to perceive is an <b>hypernym</b> of to listen) ...|$|E
40|$|The {{ability of}} searching for <b>hypernyms</b> {{with a high degree}} of {{generality}} by persons with impaired eyesight and the blind is thoroughly described. The present paper concerns the hierarchically organised structure of the mental lexicon. The final elements of hierarchical series of <b>hypernyms</b> constructed for 75 Polish nouns by 116 subjects (58 blind people and 58 sighted people) were analysed and compared...|$|R
25|$|Hyponyms and <b>hypernyms</b> can be {{described}} by using a taxonomy, {{as seen in the}} example.|$|R
5000|$|... {{acquisition}} {{by direct}} Web searching (searches for monosemous synonyms, <b>hypernyms,</b> hyponyms, parsed gloss' words, etc.), ...|$|R
50|$|The {{connection}} of generalization to specialization (or particularization) {{is reflected in}} the contrasting words <b>hypernym</b> and hyponym. A <b>hypernym</b> as a generic stands for a class or group of equally ranked items - for example, tree stands for equally ranked items such as peach and oak, and ship stands for equally ranked items such as cruiser and steamer. In contrast, a hyponym is one of the items included in the generic, such as peach and oak which are included in tree, and cruiser and steamer which are included in ship. A <b>hypernym</b> is superordinate to a hyponym, and a hyponym is subordinate to a <b>hypernym.</b>|$|E
50|$|Both nouns and verbs are {{organized}} into hierarchies, defined by <b>hypernym</b> or IS A relationships. For instance, one {{sense of the}} word dog is found following <b>hypernym</b> hierarchy; the words at the same level represent synset members. Each set of synonyms has a unique index.|$|E
50|$|Hyponymy {{shows the}} {{relationship}} between a generic term (<b>hypernym)</b> and a specific instance of it (hyponym). A hyponym is a word or phrase whose semantic field is more specific than its <b>hypernym.</b> The semantic field of a <b>hypernym,</b> also known as a superordinate, is broader than that of a hyponym. An approach to {{the relationship between}} hyponyms and hypernyms is to view a <b>hypernym</b> as consisting of hyponyms. This, however, becomes more difficult with abstract words such as imagine, understand and knowledge. While hyponyms are typically used to refer to nouns, it can also be used on other parts of speech. Like nouns, hyponyms in verbs are words that refer to a broad category of actions. For example, verbs such as stare, gaze, view and peer can also be considered hyponyms of the verb look.|$|E
40|$|In this paper, {{a system}} for Named Entity Recognition in the Open domain (NERO) is described. It is {{concerned}} with recognition of various types of entity, types that will be appropriate for Information Extraction in any scenario context. The recognition task is performed by identifying normally capitalised phrases in a document and then submitting queries to a search engine to find potential <b>hypernyms</b> of the capitalised sequences. These <b>hypernyms</b> are then clustered to derive a typology of named entities for the document. The <b>hypernyms</b> of the normally capitalised phrases are used to classify them {{with respect to this}} typology. The method is tested on a small corpus and its classifications are evaluated. Finally, conclusions are drawn and future work considered. ...|$|R
50|$|Distinguo Index: a {{tool for}} {{expanding}} search keywords to include inflected forms, synonyms, <b>hypernyms,</b> hyponyms, and other words related by meaning.|$|R
50|$|The below {{fragment}} of text has been processed by the semantic compression. Words in bold {{have been replaced}} by their <b>hypernyms.</b>|$|R
5000|$|... #Caption: [...] An {{example of}} the {{relationship}} between hyponyms and <b>hypernym</b> ...|$|E
5000|$|... {{coordinate}} terms: those verbs {{sharing a}} common <b>hypernym</b> (to lisp and to yell) ...|$|E
50|$|Sensor, <b>hypernym</b> for {{devices that}} measure with little interaction, {{typically}} used in technical applications.|$|E
5000|$|Network Building: Involves {{grasping}} {{the connections between}} words: understanding that some words are opposite in meaning. E.g., understanding the relationship between <b>hypernyms</b> and hyponyms.|$|R
30|$|Based on the {{critical}} analysis of published literature, it is inferred that more than 60 % of clustering techniques is based on term frequencies. About 30 % of clustering techniques and annotation tools use synonyms and <b>hypernyms</b> for predicting the concepts. Moreover, the synonyms and <b>Hypernyms</b> are extracted by means of WordNet lexical database Miller (1995). Since scientific literature and many tracks of news documents consist of purely domain-specific technical terms, the performance of synonyms and <b>hypernyms</b> based clustering may not always yield better results. In order to {{enhance the quality of}} the cluster for the above mentioned document sets, the focus of the present study is on clustering the document based on terms and their technically related terms. In this regard, a domain- specific dictionary has been developed by the authors to extract the related terms as concepts.|$|R
40|$|Abstract. The aim of {{the paper}} is to discuss the balance between the useful and the useless in the actual lifelong learning, in a {{conceptual}} scheme that goes beyond purely economical or technical evaluations. The authors chose an exemplary field starting from last year’s paper on the exploration of nets. In order to delimitate the research field, the official city toponymy has been considered, both {{in the structure of}} <b>hypernyms</b> and in the choices of proper names. The analysis considers Italian city <b>hypernyms,</b> with some regional or local varieties, explaining the underlying historical and contact phenomena. The grouping of similar proper names is then analyzed, finding out segmentations and some strange aversions. The non correspondence between the conceptual hierarchy and the perceptual geographical hierarchy is highlighted. Useful practical reference systems are compared with official ones. Keywords: city toponimy, geographical <b>hypernyms,</b> landmark systems, network representation, useful vs useles...|$|R
5000|$|... {{techniques}} {{are used to}} extract relation signatures, often based on pattern-based or definition-based <b>hypernym</b> extraction techniques.|$|E
5000|$|... for a {{subsumption}} relation: a hyponym (subtype, subclass) has a type-of (is-a) {{relationship with}} its <b>hypernym</b> (supertype, superclass); ...|$|E
50|$|If the <b>hypernym</b> Z {{consists}} of hyponyms X and Y, X and Y {{are identified as}} co-hyponyms. Co-hyponyms are labelled as such when separate hyponyms share the same <b>hypernym</b> but are not hyponyms of one another, unless {{they happen to be}} synonymous. For example, screwdriver, scissors, knife, and hammer are all co-hyponyms of tool, but not hyponyms of one another: *‘A hammer is a type of knife’ is false.|$|E
50|$|Frequency lists, {{together}} with semantic networks, {{are used to}} identify the least common, specialized terms {{to be replaced by}} their <b>hypernyms</b> in a process of semantic compression.|$|R
40|$|We {{consider}} supervised document classification where {{a semantic}} network {{is used to}} augment document features with their <b>hypernyms.</b> A novel document representation is introduced in which {{the contribution of the}} <b>hypernyms</b> to document similarity is determined by semantic network edge weights. We argue that the optimal edge weights are not a static property of the semantic network, but should rather be adapted to the given classification task. To determine the optimal weights, we introduce an e#cient gradient descent method driven by the misclassifications of the k-nearest neighbor (kNN) classifier...|$|R
40|$|The {{motivation}} {{of this paper}} is to increase the user perceived precision of results of Content Based Information Retrieval (CBIR) systems with Query Refinement (QR), Visual Analysis (VA) and Relevance Feedback (RF) algorithms. The proposed algorithms were implemented as modules into K-Space CBIR system. The QR module discovers <b>hypernyms</b> for the given query from a free text corpus (Wikipedia) and uses these <b>hypernyms</b> as refinements for the original query. Extracting <b>hypernyms</b> from Wikipedia makes it possible to apply query refinement to more queries than in related approaches that use static predefined thesaurus such as Wordnet. The VA Module uses the K-Means algorithm for clustering the images based on low-level features. The RF Module uses the preference information expressed by the user to build user profiles by applying SOM-based supervised classification, which is further optimized by a hybrid Particle Swarm Optimization (PSO) algorithm. The experiments evaluating the performance of QR and VA modules show promising results. ...|$|R
5000|$|In linguistics, a hyponym (from Greek hupó, [...] "under" [...] and ónoma, [...] "name") {{is a word}} {{or phrase}} whose {{semantic}} field is included within that of another word, its hyperonym or <b>hypernym</b> (from Greek hupér, [...] "over" [...] and ónoma, [...] "name"). In simpler terms, a hyponym shares a type-of relationship with its <b>hypernym.</b> For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hyperonym); which, in turn, is a hyponym of animal.|$|E
50|$|For example, {{the colors}} red, green, {{blue and yellow}} are hyponyms. They fall under the general term of color, which is the <b>hypernym.</b>|$|E
5000|$|WordNet (a {{semantic}} lexicon for the English language, {{which puts}} words in semantic relations to each other, mainly {{by using the}} concepts <b>hypernym</b> and hyponym) ...|$|E
40|$|The {{ability to}} deal with a huge number of {{independent}} and heterogeneous repositories is the most critical problem in Global Information Systems. One approach to enable efficient query processing is by utilizing semantic descriptions (organized as ontologies) of such repositories whenever available. In this context semantic relationships among ontologies can be used by Query Processors. Three kind of relationships are considered: synonyms, hyponyms and <b>hypernyms.</b> Using synonyms the semantics of the query is preserved; however, when synonyms are not available and <b>hypernyms</b> or hyponyms are used there exists some loss of information that must be measured. ...|$|R
40|$|Incorporating {{semantic}} features {{from the}} WordNet lexical database is among {{one of the}} many approaches that have been tried to improve the predictive performance of text classification models. The intuition behind this is that keywords in the training set alone may not be extensive enough to enable generation of a universal model for a category, but if we incorporate the word relationships in WordNet, a more accurate model may be possible. Other researchers have previously evaluated the effectiveness of incorporating WordNet synonyms, <b>hypernyms,</b> and hyponyms into text classification models. Generally, they have found that improvements in accuracy using features derived from these relationships are dependent upon the nature of the text corpora from which the document collections are extracted. In this paper, we not only reconsider the role of WordNet synonyms, <b>hypernyms,</b> and hyponyms in text classification models, we also consider the role of WordNet meronyms and holonyms. Incorporating these WordNet relationships into a Coordinate Matching classifier, a Naive Bayes classifier, and a Support Vector Machine classifier, we evaluate our approach on six document collections extracted from the Reuters- 21578, USENET, and Digi-Trad text corpora. Experimental results show that none of the WordNet relationships were effective at increasing the accuracy of the Naive Bayes classifier. Synonyms, <b>hypernyms,</b> and holonyms were effective at increasing the accuracy of the Coordinate Matching classifier, and <b>hypernyms</b> were effective at increasing the accuracy of the SVM classifier...|$|R
3000|$|There {{are many}} {{existing}} clustering algorithms that take synonyms and <b>hypernyms</b> for vector representation. In this study, {{the authors have}} considered crtv as concepts for clustering to improve the efficiency of clustering the documents both statically and dynamically. The idea of considering terms and related terms as concepts based on semantic similarity {{has been carried out}} for extracting topic from the clustered documents Jayabharathy et al. (2011). The proposed technique CCMARDC takes this idea of considering crtv as concepts for static clustering and applies the same concept for clustering the document dynamically. Considering terms or synonyms and <b>hypernyms</b> for information extraction leads the following issues: [...]...|$|R
