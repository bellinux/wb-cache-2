148|10|Public
25|$|For the {{important}} {{case in which}} the data are hypothesized to follow the normal distribution, depending {{on the nature of the}} test statistic and thus the underlying hypothesis of the test statistic, different null hypothesis tests have been developed. Some such tests are z-test for normal distribution, t-test for Student's t-distribution, f-test for <b>f-distribution.</b> When the data do not follow a normal distribution, it can still be possible to approximate the distribution of these test statistics by a normal distribution by invoking the central limit theorem for large samples, as in the case of Pearson's chi-squared test.|$|E
2500|$|... has an <b>F-distribution</b> {{with the}} {{corresponding}} number of {{degrees of freedom}} in the numerator and the denominator, provided that the model is correct. If the model is wrong, then the probability distribution of the denominator is still as stated above, and the numerator and denominator are still independent. [...] But the numerator then has a noncentral chi-squared distribution, and consequently the quotient {{as a whole has}} a non-central <b>F-distribution.</b>|$|E
2500|$|<b>F-{{distribution}},</b> {{the distribution}} of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient) ...|$|E
40|$|Let X = [sigma]Z be {{the scale}} mixture of Z with the scale factor [sigma] > 0. We {{consider}} two type expansions G[delta],k(x) and [Phi][delta],k(x) as the approximations to the distribution function F(x) of X. In this paper we derive non-uniform error bounds in approximating F(x) by the asymptotic expansions G[delta],k(x) and [Phi][delta],k(x). The non-uniform bounds are improvements on the uniform bounds {{in the tail}} part of the distribution. The results are applied to the asymptotic expansions of t- and <b>F-distributions.</b> non-uniform error bounds asymptotic expansion distribution function scale mixture t- and <b>F-distributions...</b>|$|R
40|$|AbstractEuclidean distance-based {{classification}} {{rules are}} derived {{within a certain}} nonclassical linear model approach and applied to elliptically contoured samples having a density generating function g. Then a geometric measure theoretical method to evaluate exact probabilities of correct classification for multivariate uncorrelated feature vectors is developed. When doing this one has to measure suitably defined sets with certain standardized measures. The geometric key {{point is that the}} intersection percentage functions of the areas under investigation coincide with those of certain parabolic cylinder type sets. The intersection percentage functions of the latter sets can be described as threefold integrals. It turns out that these intersection percentage functions yield simultaneously geometric representation formulae for the doubly noncentral g-generalized <b>F-distributions.</b> Hence, we get beyond new formulae for evaluating probabilities of correct classification new geometric representation formulae for the doubly noncentral g-generalized <b>F-distributions.</b> A numerical study concerning several aspects of evaluating both probabilities of correct classification and values of the doubly noncentral g-generalized <b>F-distributions</b> demonstrates the advantageous computational properties of the present new approach. This impression will be supported by comparison with the literature. It is shown that probabilities of correct classification depend on the parameters of the underlying sample distribution through a certain well-defined set of secondary parameters. If the underlying parameters are unknown, we propose to estimate probabilities of correct classification...|$|R
40|$|Covariance {{structure}} {{analysis is}} often used for inference and for dimension reduction with high dimensional data. When data is not normally distributed, the asymptotic distribution free (ADF) method {{is often used}} to fit a proposed model. This approach uses a weight matrix based on the inverse of the matrix formed by the sample fourth moments and sample covariances. The ADF test statistic is asymptotically distributed as a chi-square variate, but its empirical performance rejects the true model too often at all but impractically large sample sizes. By comparing mean and covariance structure analysis with its peer in the multivariate linear model, we propose some modified ADF test statistics as F-tests whose distributions we approximate using <b>F-distributions.</b> Empirical studies show that the distributions of the new F-tests are more closely approximated by <b>F-distributions</b> than are the original ADF statistics when referred to chi-square distributions. Detailed analysis indicates why the AD [...] ...|$|R
2500|$|One {{uses this}} F-statistic {{to test the}} null {{hypothesis}} {{that there is no}} lack of linear fit. Since the non-central <b>F-distribution</b> is stochastically larger than the (central) <b>F-distribution,</b> one rejects the null hypothesis if the F-statistic is larger than the critical F value. [...] The critical value corresponds to the cumulative distribution function of the F distribution with x equal to the desired confidence level, and degrees of freedom d1=(n'p) and d2=(N'n). This critical value can be calculated using online tools or found in tables of statistical values.|$|E
5000|$|The doubly noncentral <b>F-distribution</b> {{simplifies}} to the <b>F-distribution</b> if ...|$|E
50|$|The noncentral <b>F-distribution</b> {{simplifies}} to the <b>F-distribution</b> if λ = 0.|$|E
40|$|The maximum {{absolute}} studentized residual {{is commonly}} used for testing for a single outlier in a linear regression model. This test statistic, however, is seldom discussed in a nonlinear regression setting. We simulate the critical values for the tests under various nonlinear models. The associated critical values {{are found to be}} very close to one another. Moreover, they are very well approximated using the critical values obtained from <b>F-distributions</b> based on the Bonferroni equations in linear models. The results are promising even in samples of size 6. link_to_subscribed_fulltex...|$|R
40|$|A natural {{generalization}} of the well-known F-statistic is introduced for testing one-sided hypotheses. The exact finite sample null distribution of this statistic {{is shown to}} be a weighted sum of <b>F-distributions.</b> This resembles the large sample null distribution of the likelihood ratio statistic, a chi-bar squared distribution, which is a weighted sum of [chi] 2 distributions. It turns out that the weights in these two distributions are identical; this is an important feature because we can use essentially the same computer programs for computing the p-values for the F- and the likelihood ratio statistics. A general stimulation procedure for computing the chi-bar-squared weights when the linear inequalities in the alternative hypothesis are not independent is introducedChi-bar-squared distributions Dependent inequality constraints Nonstandard conditions One-sided F-test...|$|R
40|$|Variance {{components}} estimation {{originated with}} estimating error variance in {{analysis of variance}} by equating error mean square to its expected value. This equating procedure was _then extended to random effects models, first for balanced data (for which minimum variance properties were subsequently established) and later for unbalanced data. Unfortunately, this ANOVA ·methodology yields no optimum properties (other than unbiasedness) for estimation from unbalanced data. Today it is being replaced by maximum likelihood (ML) and restricted maximum likelihood (REML) based on normality assumptions and involving nonlinear equations {{that have to be}} solved numerically. There is also minimum norm quadratic unbiased estimation (MINQUE) which is closely related to REML but with fewer advantages. ORIGINS The analysis of variance table, as developed by R. A. Fisher in the 1920 s, is a well-established summary of the arithmetic for testing hypotheses using ratios of mean squares which, under normality assumptions, have <b>F-distributions</b> (so named by Snedecor in honor of Fisher). This arithmetic was initially designed for what are now called fixed effects models, for which the F-statistics are suited t...|$|R
5000|$|From the {{relations}} between a beta and an <b>F-distribution,</b> Wilks' lambda {{can be related to}} the <b>F-distribution</b> when one of the parameters of the Wilks lambda distribution is either 1 or 2, e.g.,and ...|$|E
5000|$|... where F1,n &minus; 1 is the <b>F-distribution</b> with 1 and n &minus; 1 {{degrees of}} freedom (see also Student's t-distribution). The final step here is {{effectively}} {{the definition of a}} random variable having the <b>F-distribution.</b>|$|E
50|$|The <b>F-distribution.</b>|$|E
40|$|A {{generalization}} of Wilks's single-outlier test suitable for {{application to the}} many-outlier problem of detecting from 1 to k outliers in a multivariate data set is proposed and appropriate critical values determined. The method used follows that suggested by Rosner employing sequential application of the generalized extreme Studentized deviate to univariate samples of reducing size, in which the type I error is controlled both under the hypothesis of no outliers and under the alternative hypothesis of 1, 2, [...] ., k outliers. It is shown that critical values for the sequential application of Wilks's test to detect many outliers depend only on those for a single outlier test which may be approximated by percentage points from the <b>F-distributions</b> as tabulated by Wilks. Relationships between Wilks's test statistic, the Mahalanobis distance between the 'outlier' and the mean vector, and Hotelling's T 2 -test between the outlier {{and the rest of}} the data, are used to reduce the amount of computation involved in applying the sequential procedure. Simulations are used to show that the method behaves well in detecting multiple outliers in samples larger than about 25. Finally, an example with three dimensions is used to illustrate how the method is applied...|$|R
40|$|Systems with a {{long-term}} stationary state that possess as a spatio-temporally fluctuation quantity β {{can be described}} by a superposition of several statistics, a "super statistics". We consider first, the Gamma, log-normal and <b>F-distributions</b> of β. It is assumed that they depend only on p_l, the probability associated with the microscopic configuration of the system. For {{each of the three}} β-distributions we calculate the Boltzmann factors and show that they coincide for small variance of the fluctuations. For the Gamma distribution it is possible to calculate the entropy in a closed form, depending on p_l, and to obtain then an equation relating p_l with β E_l. We also propose, as other examples, new entropies close related with the Kaniadakis and two possible Sharma-Mittal entropies. The entropies presented in this work do not depend on a constant parameter q but on p_l. For the p_l-Gamma distribution and its corresponding B_p_l(E) Boltzmann factor and the associated entropy, we show the validity of the saddle-point approximation. We also briefly discuss the generalization of one of the four Khinchin axioms to get this proposed entropy. Comment: 13 pages, 3 figure...|$|R
40|$|AbstractWe {{consider}} {{the problem of}} discriminating, {{on the basis of}} random “training” samples, between two independent multivariate normal populations, Np(μ, Σ 1) and Np(μ, Σ 2), which have a common mean vector μ and distinct covariance matrices Σ 1 and Σ 2. Using the theory of Bessel functions of the second kind of matrix argument developed by Herz (1955, Ann. Math. 61, 474 – 523), we derive stochastic representations for the exact distributions of the “plug-in” quadratic discriminant functions for classifying a newly obtained observation. These stochastic representations involve only chi-squared and <b>F-distributions,</b> hence we obtain an efficient method for simulating the discriminant functions and estimating the corresponding probabilities of misclassification. For some special values of p, Σ 1 and Σ 2 we obtain explicit formulas and inequalities for the probabilities of misclassification. We apply these results to data given by Stocks (1933, Ann. Eugen. 5, 1 – 55) in a biometric investigation of the physical characteristics of twins, and to data provided by Rencher (1995, “Methods of Multivariate Analysis,” Wiley, New York) in a study of the relationship between football helmet design and neck injuries. For each application we estimate the exact probabilities of misclassification, {{and in the case of}} Stocks' data we make extensive comparisons with previously published estimates...|$|R
5000|$|... to the <b>F-distribution</b> with , [...] {{degrees of}} freedom. Using the <b>F-distribution</b> {{is a natural}} {{candidate}} because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled chi-squared distribution.|$|E
5000|$|... where [...] is the <b>F-distribution</b> with {{parameters}} p and n − p. In {{order to}} calculate a p-value (unrelated to the p variable here), divide the t2 statistic by the above fraction {{and use the}} <b>F-distribution.</b>|$|E
50|$|In a frequentist context, a scaled <b>F-distribution</b> {{therefore}} {{gives the}} probability p(s12/s22 | σ12, σ22), with the <b>F-distribution</b> itself, without any scaling, applying where σ12 {{is being taken}} equal to σ22. This is {{the context in which}} the <b>F-distribution</b> most generally appears in F-tests: where the null hypothesis is that two independent normal variances are equal, and the observed sums of some appropriately selected squares are then examined to see whether their ratio is significantly incompatible with this null hypothesis.|$|E
40|$|We {{consider}} {{the problem of}} discriminating, {{on the basis of}} random "training" samples, between two independent multivariate normal populations, Np([mu],Â [Sigma] 1) and Np([mu],Â [Sigma] 2), which have a common mean vector [mu] and distinct covariance matrices [Sigma] 1 and [Sigma] 2. Using the theory of Bessel functions of the second kind of matrix argument developed by Herz (1955, Ann. Math. 61, 474 - 523), we derive stochastic representations for the exact distributions of the "plug-in" quadratic discriminant functions for classifying a newly obtained observation. These stochastic representations involve only chi-squared and <b>F-distributions,</b> hence we obtain an efficient method for simulating the discriminant functions and estimating the corresponding probabilities of misclassification. For some special values of p, [Sigma] 1 and [Sigma] 2 we obtain explicit formulas and inequalities for the probabilities of misclassification. We apply these results to data given by Stocks (1933, Ann. Eugen. 5, 1 - 55) in a biometric investigation of the physical characteristics of twins, and to data provided by Rencher (1995, "Methods of Multivariate Analysis," Wiley, New York) in a study of the relationship between football helmet design and neck injuries. For each application we estimate the exact probabilities of misclassification, {{and in the case of}} Stocks' data we make extensive comparisons with previously published estimates. Bessel function of matrix argument biplot discriminant analysis dizygotic twins football helmet design misclassification probability monozygotic twins multivariate gamma function multivariate normality neck injuries stochastic representation stochastic ordering Wishart distribution...|$|R
5000|$|... has an <b>F-distribution</b> with n &minus; 1 and m &minus; 1 {{degrees of}} freedom if the null {{hypothesis}} of equality of variances is true. Otherwise it has a non-central <b>F-distribution.</b> The null hypothesis is rejected if F is either too large or too small.|$|E
50|$|When λ = 0, the noncentral <b>F-distribution</b> becomes theF-distribution.|$|E
5000|$|The {{mean and}} {{variance}} of the noncentral <b>F-distribution</b> areand ...|$|E
5000|$|<b>F-{{distribution}}</b> is {{a special}} case of type 6 Pearson distribution ...|$|E
5000|$|Equivalently, {{the random}} {{variable}} of the <b>F-distribution</b> {{may also be}} written ...|$|E
50|$|The Pearson type VI {{distribution}} is a beta prime distribution or <b>F-distribution.</b>|$|E
5000|$|... or F(&alpha;,&nu;1,&nu;2) for the <b>F-distribution</b> with &nu;1 and &nu;2 {{degrees of}} freedom ...|$|E
5000|$|Fishers {{z-distribution}} is {{the statistical}} distribution {{of half the}} logarithm of an <b>F-distribution</b> variate: ...|$|E
5000|$|The pdf of the noncentral <b>F-distribution</b> is a {{solution}} of the following differential equation: ...|$|E
5000|$|The {{probability}} density {{function of the}} <b>F-distribution</b> is a solution of the following differential equation: ...|$|E
5000|$|If , then [...] {{follows a}} noncentral <b>F-distribution</b> with [...] degrees of freedom, and non-centrality {{parameter}} [...]|$|E
5000|$|The {{cumulative}} distribution function for the noncentral <b>F-distribution</b> iswhere [...] is the regularized incomplete beta function.|$|E
5000|$|<b>F-distribution,</b> arises {{frequently}} as the null distribution {{of a test}} statistic, most notably {{in the analysis of}} variance ...|$|E
50|$|In {{probability}} theory and statistics, the noncentral <b>F-distribution</b> is a continuous probability distribution {{that is a}} generalization of the (ordinary) <b>F-distribution.</b> It describes {{the distribution of the}} quotient (X/n1)/(Y/n2), where the numerator X has a noncentral chi-squared distribution with n1 degrees of freedom and the denominator Y has a central chi-squared distribution n2 degrees of freedom. It is also required that X and Y are statistically independent of each other.|$|E
