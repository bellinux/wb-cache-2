6|33|Public
50|$|In Batman: Year One, it is {{depicted}} that Batman hid {{a few pieces}} of his arsenal in his leather boots, such as a blow gun (its length made it impossible to fit in Batman's belt compartment) with fast-acting anesthetic darts and an ultrasonic device built into his left heel. The basic design of the boots are modeled on tactical boots, but they are made from lightweight rubbers and are much more flexible to allow for full extension when kicking. The bottom is a <b>flexible</b> <b>split</b> sole design and is textured for a variety of surfaces. The boots also have steel toes, making them much more effective when on the offensive. Although Batman is already an elite swimmer, during the Batman: Hush storyline, it is revealed that he installed underwater propellers in the heels. In Batman Begins, a boot heel is revealed to contain an ultrasonic signaling device capable of calling live bats to it as a form of protection and cover for Batman during a getaway. This device was originally introduced in the Batman: Year One series.|$|E
40|$|International audienceThe {{evolution}} of LTE and advent of 5 G networks increases further the bandwidth requirements for Radio Access Network (RAN). In parallel, {{the deployment of}} Centralized RAN architecture raises new challenges on the FrontHaul network. The inflexibility of the legacy Common Public Radio Interface (CPRI) is the primary challenge to Virtualized RAN deployments, and there is currently a strong trend towards the use of packetized transport methods, together with <b>flexible</b> <b>split</b> RAN based architectures. Functional splits within the real-time functions of the RAN have very stringent requirements on latency and jitter. This paper analyzes the jitter produced in the switching nodes of the FrontHaul network, and proposes dimensioning rules...|$|E
40|$|International audienceCloud Radio Access Network (Cloud RAN) is novel {{mobile network}} architecture. It uses cloud {{computing}} capabilities {{to enhance the}} quality of service (QoS) in next generation 5 G networks. The basic concept of Cloud RAN is to separate the digital baseband processing units (BBUs) of conventional cell sites, from the Remote Radio Heads (RRHs), and move them to the cloud for centralized signal processing and management. In this paper, we highlight key technologies and tradeoffs that drive Cloud RAN. We propose a new model of C-RAN architecture which introduces a <b>flexible</b> <b>split</b> of RAN functionalities between an edge cloud, Cloud-RRH, and the central Cloud. We conclude with a performances comparison between the proposed model and different C-RAN architectures...|$|E
40|$|In {{their recent}} SIAM J. Control Optim. paper from 2009, J. Eckstein and B. F. Svaiter {{proposed}} a very general and <b>flexible</b> <b>splitting</b> framework for finding a zero of {{the sum of}} finitely many maximal monotone operators. In this short note, we provide a technical result that allows {{for the removal of}} Eckstein and Svaiter’s assumption that the sum of the operators be maximal monotone or that the underlying Hilbert space be finite-dimensional. 2000 Mathematics Subject Classification...|$|R
25|$|In 2011 United Future {{campaigned on}} income <b>splitting,</b> <b>flexible</b> superannuation and restricting asset sales.|$|R
30|$|FlexCRAN [61] {{incorporates}} {{an architectural}} framework that implements a <b>flexible</b> functional <b>split,</b> using Ethernet as fronthaul link. The authors also introduced the {{key performance indicators}} (KPIs) of a C-RAN and evaluated the proposed architecture through an OpenAirInterface-based implementation.|$|R
40|$|We {{consider}} the long-run odds that narcotics users remain abstinent after methadone treatment. A <b>flexible</b> <b>split</b> hazard specification {{that allows for}} individual-level differences in both the long-run probability of eventual relapse and the short-run timing of relapse is developed. The model is applied to a comprehensive data set involving individual drug abuse and treatment histories for over eight hundred addicts. Our findings indicate (1) that the short-run success of methadone programs does not automatically translate into long-run abstinence, (2) the importance of intervention programs aimed at youths, and (3) the possibility to identify high-risk groups, which may allow for a more effective targeting of drug prevention and treatment programs. Narcotics; Probability; Model; Data;...|$|E
40|$|Fog-aided network {{architectures}} for 5 G systems encompass {{wireless edge}} nodes, {{referred to as}} remote radio systems (RRSs), as well as remote cloud center (RCC) processors, which {{are connected to the}} RRSs via a fronthaul access network. RRSs and RCC are operated via Network Functions Virtualization (NFV), enabling a <b>flexible</b> <b>split</b> of network functionalities that adapts to network parameters such as fronthaul latency and capacity. This work focuses on uplink communications and investigates the cloud-edge allocation of two important network functions, namely the control functionality of rate selection and the data-plane function of decoding. Three functional splits are considered: (i) Distributed Radio Access Network (D-RAN), in which both functions are implemented in a decentralized way at the RRSs, (ii) Cloud RAN (C-RAN), in which instead both functions are carried out centrally at the RCC, and (iii) a new functional split, referred to as Fog RAN (F-RAN), with separate decentralized edge control and centralized cloud data processing. The model under study consists of a time-varying uplink channel in which the RCC has global but delayed channel state information (CSI) due to fronthaul latency, while the RRSs have local but more timely CSI. Using the adaptive sum-rate as the performance criterion, it is concluded that the F-RAN architecture can provide significant gains in the presence of user mobility. Comment: 28 pages, 11 figures. This manuscript was presented in part at arXiv: 1606. 0913...|$|E
40|$|Conference Theme: From Hemodialysis Unit to ICUWe {{report a}} {{hemodialysis}} patient {{who suffered from}} superior vena cava perforation after a cuffed tunneled catheter insertion using the right subclavian approach. A 59 -year-old lady suffered from end-stage renal failure due to unknown cause and was initiated on peritoneal dialysis. She was switched to hemodialysis via the right A-V fistula {{as a result of}} abdominal adhesions. Her right forearm A-V fistula was thrombosed 3 years after creation. The right internal jugular vein was thrombosed and there was a left carotid/jugular fistula as a result of previous traumatic puncture. She was therefore scheduled for a cuffed tunneled catheter insertion using the right subclavian approach. There was difficulty during the negotiation of the peel-away sheath and kinking was noted. The procedure was aborted and CT thorax showed extravasation of contrast in the mediastinum. Perforation of the superior vena cava was diagnosed and DDAVP was given to control the bleeding. The cardiothoracic surgeons suggested for conservative anagement in view of the stable hemodynamics. Repeat CT scan 1 week later showed no further extravasation or hematoma formation. The right subclavian vein was attempted again after 2 weeks with a split catheter using a double guidewire technique (Retrocath) and the procedure was uneventful. Because of the angulation between the right subclavian vein and superior vena cava, cuffed tunneled catheter insertion using the subclavian approach is of higher technical difficulty. Attempts should be made to use the jugular vein for cuffed tunneled catheter insertion if possible. The risk of perforation or vascular injury is increased especially when the catheter is inserted using a peel-away sheath that is nonflexible. We suggested that if the subclavian approach is used, one should attempt with a <b>flexible</b> <b>split</b> catheter using a double guidewire technique to minimize the chance of vascular injury. Link_to_subscribed_fulltex...|$|E
40|$|Today’s {{networks}} typically handle {{traffic engineering}} (e. g., tuning the routing-protocol parameters {{to optimize the}} flow of traffic) and failure recovery (e. g., pre-installed backup paths) independently. In this paper, we propose a unified way to balance load efficiently under {{a wide range of}} failure scenarios. Our architecture supports <b>flexible</b> <b>splitting</b> of traffic over multiple precomputed paths, with efficient pathlevel failure detection and automatic load balancing over the remaining paths. We propose two candidate solutions that differ in how the routers rebalance the load after a failure, leading to a trade-off between router complexity and load-balancing performance. We present and solve the optimization problems that compute the configuration state for each router. Our experiments with traffic measurements and topology data (including shared risks in the underlying transport network) from a large ISP identify a “sweet spot” that achieves near-optimal load balancing under a variety of failure scenarios, with a relatively small amount of state in the routers. We believe that our solution for joint traffic engineering and failure recovery will appeal to Internet Service Providers as well as the operators of data-center networks...|$|R
40|$|Abstract—Dynamic Time-division duplex (TDD) {{can provide}} {{efficient}} and <b>flexible</b> <b>splitting</b> {{of the common}} wireless cellular resources between uplink (UL) and downlink (DL) users. In this paper, the UL/DL optimization problem is formulated as a noncooperative game among the {{small cell base stations}} (SCBSs) in which each base station aims at minimizing its total UL and DL flow delays. To solve this game, a self-organizing UL/DL resource configuration scheme for TDD-based small cell networks is proposed. Using the proposed scheme, an SCBS is able to estimate and learn the UL and DL loads autonomously while optimizing its UL/DL configuration accordingly. Simulations results show that the proposed algorithm achieves significant gains in terms of packet throughput in case of asymmetric UL and DL traffic loads. This gain increases as the traffic asymmetry increases, reaching up to 97 % and 200 % gains relative to random and fixed duplexing schemes respectively. Our results also show that the proposed algorithm is well- adapted to dynamic traffic conditions and different network sizes, and operates efficiently in case of severe cross-link interference in which neighboring cells transmit in opposite directions. Keywords- Dynamic-TDD; small cells; reinforcement learning; self-organizing networks I...|$|R
40|$|Dynamic Time-division duplex (TDD) {{can provide}} {{efficient}} and <b>flexible</b> <b>splitting</b> {{of the common}} wireless cellular resources between uplink (UL) and downlink (DL) users. In this paper, the UL/DL optimization problem is formulated as a noncooperative game among the {{small cell base stations}} (SCBSs) in which each base station aims at minimizing its total UL and DL flow delays. To solve this game, a self-organizing UL/DL resource configuration scheme for TDD-based small cell networks is proposed. Using the proposed scheme, an SCBS is able to estimate and learn the UL and DL loads autonomously while optimizing its UL/DL configuration accordingly. Simulations results show that the proposed algorithm achieves significant gains in terms of packet throughput in case of asymmetric UL and DL traffic loads. This gain increases as the traffic asymmetry increases, reaching up to 97 % and 200 % gains relative to random and fixed duplexing schemes respectively. Our results also show that the proposed algorithm is well- adapted to dynamic traffic conditions and different network sizes, and operates efficiently in case of severe cross-link interference in which neighboring cells transmit in opposite directions. Comment: In the IEEE 11 th International Symposium on Wireless Communication Systems (ISWCS) 201...|$|R
30|$|Finally, it {{is worth}} {{mentioning}} that considering the manufacturing attributes (e.g., alternative process routing, purchasing machine, duplicate machines, machine depot, lot <b>splitting,</b> <b>flexible</b> cell configuration, and varying number of formed cells) brings the flexibility for the presented model to respond to {{any change in the}} part demand and product mix.|$|R
40|$|When evolved NodeB (eNB) <b>flexible</b> {{functional}} <b>split</b> {{is implemented}} in Cloud-Radio Access Network (Cloud-RAN) 5 G systems, fronthaul connectivity between the virtualized functions must be always guaranteed. This study proposes {{the utilization of}} Software Defined Networking (SDN) to control mobile fronthaul. In particular, this study investigates {{the ability of the}} SDN-based control of reconfiguring the fronthaul to maintain virtualized network function connectivity when cell and optical access turn into sleep mode (off mode) for energy efficiency purposes. The experiments in two federated testbeds show that, upon cell and optical access turning on and off, the fronthaul reconfiguration time is limited to few tens of milliseconds...|$|R
30|$|SDN [18] is {{a recent}} {{networking}} paradigm, which separates control and data planes to enhance flexibility and to achieve programmability of network technologies. SDN is a key enabler for converged FH/BH networks in 5 G. It {{will be used to}} virtualize the transport network in order to support slicing and to allow a flexible deployment of virtual functions in different places of the network, as is required for the support of <b>flexible</b> functional <b>splits.</b> In some cases, a separate out-of-band network for SDN signaling traffic may be too expensive to maintain, or reliability constraints may require a certain degree of distributed control to be kept in the network elements, thus, balancing between distributed and centralized control. Therefore, {{further research is needed to}} holistically apply the SDN paradigm to transport networks.|$|R
50|$|The dynamic Hilbert R-tree is {{suitable}} for dynamic databases where insertions, deletions, or updates may occur in real time. Moreover, dynamic Hilbert R-trees employ <b>flexible</b> deferred <b>splitting</b> mechanism to increase the space utilization. Every node has a well defined set of sibling nodes. By adjusting the split policy the Hilbert R-tree can achieve a degree of space utilization as high as is desired. This is done by proposing an ordering on the R-tree nodes. The Hilbert R-tree sorts rectangles according to the Hilbert value {{of the center of}} the rectangles (i.e., MBR). (The Hilbert value of a point is the length of the Hilbert curve from the origin to the point.) Given the ordering, every node has a well-defined set of sibling nodes; thus, deferred splitting can be used. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, other R-tree variants have no control over the space utilization.|$|R
40|$|International audienceWorking {{schedules}} and Society : what outcome and what evolution in Europe and in France? Abstract. Atypical working schedules have been expanding {{for several years}} and have become the "standard" in all the European countries. The great variety of working time organization (shift work, night work, weekend work, <b>flexible</b> working hours, <b>split</b> shifts, etc.) is quite different in the European countries, and has multiple impacts on health. These effects depend also on individual and occupational factors, as well as on living and social conditions. So, working time issues have a multidimensional character and need integration of diverse disciplines, such as medicine, physiology, psychology, ergonomics, sociology, economics, etc...|$|R
40|$|In this paper, {{we study}} the {{state-dependent}} two-user interference channel, {{where the state}} information is non-causally known at both transmitters but unknown to either of the receivers. We first propose two coding schemes for the discrete memoryless case: simultaneous encoding for the sub-messages in the first one and superposition encoding in the second one, both with rate splitting and Gel'fand-Pinsker coding. The corresponding achievable rate regions are established. Moreover, for the Gaussian case, {{we focus on the}} simultaneous encoding scheme and propose an active interference cancellation mechanism, which is a generalized dirty-paper coding technique, to partially eliminate the state effect at the receivers. The corresponding achievable rate region is then derived. We also propose several heuristic schemes for some special cases: the strong interference case, the mixed interference case, and the weak interference case. For the strong and mixed interference case, numerical results are provided to show that active interference cancellation significantly enlarges the achievable rate region. For the weak interference case, <b>flexible</b> power <b>splitting</b> instead of active interference cancellation improves the performance significantly. Comment: 32 pages, 8 figures, submitted to IEEE Transaction on Communication...|$|R
40|$|In {{this paper}} 1 x 4 fixed and {{flexible}} power splitters using multicore photonic crystal fiber(PCF) and photonic crystal waveguide array(PCWA) is proposed. Multicore PCF power splitter comprises of four identical cores surrounding the central core (Fixed power splitter). The central core {{is surrounded by}} non identical cores(Flexible power splitter). The optical power launched into the central core is equally divided into four neighbouring cores with 25 % coupling ratio(Fixed power splitter). An unequal distribution of power is observed when the diameters of the cores surrounding the central core are varied. The PCWA power splitter comprises of a rectangular array of dielectric rods in air. This array is integrated with multimode interference coupler (MMI). Fixed and <b>flexible</b> power <b>splitting</b> ratio is obtained by varying {{the diameter of the}} rods. The PCWA power splitters are compact(14 m) as compared to the PCF power splitters (57 m). These novel structures are investigated using Finite Difference Time Domain Method(FDTD). Coupled mode analysis is also carried out to understand the super mode patterns and coupling characteristics. The device size reduction compared with the conventional MMI power splitter is attributed to the large dispersion of the PCW and PCF...|$|R
40|$|In {{the context}} of Software Engineering, Software Process {{is a set of}} {{activities}} and its associated results. The Object Management Group (OMG) created the Software Process Engineering Metamodel (SPEM) in order to better repre-sent and design software process. However, the artifacts are built as an auto-sufficient monolithic piece of information. The challenge is to build software artifacts that can better be controlled and managed. In this paper we present ongo-ing work in SPEM extension which represents information as high granularity pieces of data. Nevertheless, redefining and extending the SPEM metamodel gives us a <b>flexible</b> way to <b>split</b> the software process metamodel artifacts. Moreover, we can manage data in an easy way, avoiding some of the redundancy and organizing the artifacts as a composite el-ement. Finally, this work presents a different perspective to artifacts using a rudimentary example. 1...|$|R
40|$|We {{analyze the}} {{investment}} decision {{of a firm}} that has an option to complete an investment project either in one lump or in two smaller parts at distinct points in time. The firm faces a trade-off between the cost savings that arise when the project is completed at once and the additional flexibility that arises when the firm is {{able to respond to}} resolving uncertainty by choosing optimal timing individually for each stage. We derive the optimal investment policy and show that, contrary to our initial presumption, higher uncertainty makes the lump investment more attractive relative to the apparently more <b>flexible</b> alternative of <b>splitting</b> the investment. JEL classification: C 61, D 81, G 31 We would like to thank seminar participants at Antwerp and Tilburg for helpful comments and suggestions. All remaining errors are ours. 1...|$|R
5000|$|Rice barns in Indonesia (known as lumbungs) {{are built}} on four poles, usually stand between 1½ metre and 2 metres up from ground level. The upper storage area often has a {{distinct}} omega shape created by bending <b>flexible</b> framing of <b>split</b> bamboo or betel nut trees to support the roof. The roof is generally covered with alang-alang grass and the sides are made of woven, split bamboo (called pagar). The pole support structure beneath the raised, enclosed rice barn is open with no walls. A floor or platform is constructed of wood and bamboo about 1/2 metre above the ground. This lower platform provides a convenient, shady place for people to sit and relax. In many traditional villages this lower sitting area is a meeting place for village residents where both business activities and social interaction commonly occur.|$|R
40|$|A {{web service}} is a {{programmable}} web application accessible using standard Internet protocols. A three-layer architecture has been suggested for web services: service providers, service brokers, and service requesters. We propose in this paper a SOAP-oriented component-based framework to support device-independent multimedia web services. Two intelligent agents are introduced and embedded into proxy server and service provider server, respectively. Separating metadata from multimedia content leads to enhance SOAP flexibility. Composite Capability / Preference Profiles-based user profile management provides an easy and <b>flexible</b> way to <b>split</b> and adapt multimedia services to appropriate composite devices as well as increases the flexibility for users to manage multi-devices. Together with XML/XSL strategy, we ensure the device independency of the system. Utilizing local caches, our agents enable the caching and streaming of multimedia transportation. In addition, our framework seamlessly incorporates cutting-edge technologies relating to web services: SOAP, XML/XSL, and CC/PP...|$|R
40|$|International audienceWeb service {{paradigm}} {{and related}} technologies have provided favorable {{means for the}} realization of collaborative business processes. From both conceptual and implementation points of view, the business processes {{are based on a}} centralized management approach. Nevertheless, it is very well known that the enterprise-wide process management where processes may span multiple organizational units requires particular considerations on scalability, heterogeneity, availability and privacy issues, that in turn, require particular consideration on decentralization. In a previous work, we have described a <b>flexible</b> methodology for <b>splitting</b> a centralized process specification into a form that is amenable to a distributed execution. The approach is based on the computation of very basic dependencies between process elements. In this paper, we extend this approach to support advanced patterns such as Loops, Multiple instances and Discriminator, and incorporate the necessary synchronization between the different processing entities. We also detail our interconnection mechanism and explain how to handle control and data dependencies between activities of the different partitions through asynchronous message exchanges. The proposed methodology preserves semantics of the centralized process with a peer-to peer interactions among the derived decentralized processes...|$|R
40|$|An {{electromagnetic}} catheter {{flow meter}} {{is described in}} which the magnetic field is generated by two parallel bundles of wire carrying equal currents in opposite directions. The electrodes are fixed centrally to the insulated wire bundles that generate the magnetic field. The flow sensor is <b>flexible,</b> resembling a <b>split</b> catheter. The flow transducer is designed to constrict as it is introduced through a branch artery and to expand in the main artery over the span of its diameter. The principle is suitable for branch flow measurement {{as well as for}} measurement of flow in a major artery or vein by the same transducer. A special method of guiding the electrode wires results in a zero base line at zero flow for the entire range of diameters accommodating the field generating coil. The electrodes could be used in this configuration with a magnetic field generated by coils external to the patient for blood flow measurements with a catheter of reduced gauge. The transducer can be made smaller in circumference than those employed in other electromagnetic flow measuring catheter devices. This feature is of special value for envisaged clinical uses (percutaneous introduction) to minimize surgical intervention...|$|R
40|$|Abstract—Web service {{paradigm}} {{and related}} technologies have provided favorable {{means for the}} realization of collaborative business processes. From both conceptual and implementation points of view, the business processes {{are based on a}} centralized management approach. Nevertheless, it is very well known that the enterprise-wide process management where processes may span multiple organizational units requires particular considerations on scalability, heterogeneity, availability and privacy issues, that in turn, require particular consideration on decentralization. In a previous work [10], we have described a <b>flexible</b> methodology for <b>splitting</b> a centralized process specification into a form that is amenable to a distributed execution. The approach is based on the computation of very basic dependencies between process elements. In this paper, we extend this approach to support advanced patterns such as Loops, Multiple instances and Discriminator, and incorporate the necessary synchronization between the different processing entities. We also detail our interconnection mechanism and explain how to handle control and data dependencies between activities of the different partitions through asynchronous message exchanges. The proposed methodology preserves semantics of the centralized process with a peer-to peer interactions among the derived decentralized processes. I...|$|R
40|$|In {{the latest}} Joint Video Exploration Team development, the {{quadtree}} plus binary tree (QTBT) block partitioning structure {{has been proposed}} for future video coding. Compared to the traditional quadtree structure of High Efficiency Video Coding (HEVC) standard, QTBT provides more <b>flexible</b> patterns for <b>splitting</b> the blocks, which results in dramatically increased combinations of block partitions and high computational complexity. In view of this, a confidence interval based early termination (CIET) scheme is proposed for QTBT to identify the unnecessary partition modes {{in the sense of}} rate-distortion (RD) optimization. In particular, a RD model is established to predict the RD cost of each partition pattern without the full encoding process. Subsequently, the mode decision problem is casted into a probabilistic framework to select the final partition based on the confidence interval decision strategy. Experimental results show that the proposed CIET algorithm can speed up QTBT block partitioning structure by reducing 54. 7 % encoding time with only 1. 12 % increase in terms of bit rate. Moreover, the proposed scheme performs consistently well for the high resolution sequences, of which the video coding efficiency is crucial in real applications...|$|R
30|$|In this paper, we {{consider}} a packetized network to analyze possible fronthaul capacity reduction. Recently, CPRI released the first eCPRI specification (1.0) [26] that enables use of packet-based transport {{technologies such as}} Ethernet and supports real-time traffic through different <b>flexible</b> functional <b>splits.</b> Various functional splits between BBU-RRU have been investigated, e.g., in [14, 15, 24] and {{the choice of a}} particular split is an application specific. Unlike packetized network, in traditional CPRI, FH data rate is always static and independent of the traffic load, i.e., full-FH data rate needs to be forwarded even when there is no user connected to the BS. However, with the appropriate RRU-BBU functional split such as Split C 1 [14], where resource mapping and precoding operations are executed at RRU instead of centrally at BBU, FH data rate can be made traffic dependent. This allows FH data rate more closely coupled with the actual user traffic i.e., the traffic will be lower due to low demand or due to unfavourable channel conditions. For simplicity, we assume that the RRU generates the beamforming weights locally at the RRU after having obtained perfect CSI from uplink pilots, which means there is generally no signalling overhead on the FH. Precoding at RRU enables to transmit one stream per user instead of one stream per transceiver 2. Performing precoding at RRU gives rise to two advantages: First, the number of streams will vary according to the users currently served, and hence, by allowing a certain outage probability within the limits of acceptable quality of service (QoS), i.e. dimensioning the FH capacity only for the 99 th percentile of the traffic distribution, the required FH capacity can be reduced considerably. Second, the variable streams of different RRUs can be combined in the aggregation segment, resulting in statistical multiplexing, which further lowers the required FH capacity. In Section 3, we explain this concept.|$|R
500|$|The {{focus of}} the NWP shifted to {{equality}} under the law, including equal employment opportunities, jury service, nationality for married women and any other provision which legally prohibited women from having full legal equality. In 1923, the Equal Rights Amendment was introduced by Daniel Read Anthony, Jr. and the women pushed for its passage, lobbying for support from both political parties. Stevens served as vice chair of NWP’s New York branch, spearheading the NWP Women for Congress campaign in 1924. Unable to run herself due to her having established a legal residence in France, Stevens worked toward the goal of securing the election of 100 women to Congress in states where female candidates were among contenders for office. The campaign had negligible results and the women shifted back to equality measures. Beginning in 1926, one of the proposals Stevens focused on {{for the next several}} years was the [...] "Wages for Wives" [...] marriage contract. Campaigning vigorously for its adoption, the [...] "Wages for Wives" [...] proposal called for a <b>flexible</b> contract which <b>split</b> marital assets 50-50 rather than treating married couples as a single entity and called for women to be paid a wage for domestic services and raising children as a protection for children's continuous support.|$|R
40|$|Abstract—Traffic Engineering (TE) leverages {{information}} of network traffic {{to generate a}} routing scheme optimizing the traffic distribution so as to advance network performance. However, optimize the link weights for OSPF to the offered traffic is an known NP-hard problem [16]. In this paper, motivated by the fairness concept of congestion control [10], we firstly propose a generic objective function, where various interests of providers can be extracted with different parameter settings. And then, we model the optimal TE as the utility maximization of multi-commodity flows with the generic objective function and theoretically show that any given set of optimal routes corresponding to a particular objective function {{can be converted to}} shortest paths with respect to a set of positive link weights. This can be directly configured on OSPF-based protocols. On these bases, we employ the Network Entropy Maximization(NEM) framework [20] and develop a new OSPF-based routing protocol, SPEF, to realize a <b>flexible</b> way to <b>split</b> traffic over shortest paths in a distributed fashion. Actually, comparing to OSPF, SPEF only needs one more weight for each link and provably achieves optimal TE. Numerical experiments have been done to compare SPEF with the current version of OSPF, showing the effectiveness of SPEF in terms of link utilization and network load distribution...|$|R
5000|$|The {{focus of}} the NWP shifted to {{equality}} under the law, including equal employment opportunities, jury service, nationality for married women and any other provision which legally prohibited women from having full legal equality. In 1923, the Equal Rights Amendment was introduced by Daniel Read Anthony, Jr. and the women pushed for its passage, lobbying for support from both political parties. Stevens served as vice chair of NWP’s New York branch, spearheading the NWP Women for Congress campaign in 1924. Unable to run herself due to her having established a legal residence in France, Stevens worked toward the goal of securing the election of 100 women to Congress in states where female candidates were among contenders for office. The campaign had negligible results and the women shifted back to equality measures. Beginning in 1926, one of the proposals Stevens focused on {{for the next several}} years was the [...] "Wages for Wives" [...] marriage contract. Campaigning vigorously for its adoption, the [...] "Wages for Wives" [...] proposal called for a <b>flexible</b> contract which <b>split</b> marital assets 50-50 rather than treating married couples as a single entity and called for women to be paid a wage for domestic services and raising children as a protection for children's continuous support.|$|R
40|$|In this dissertation, {{we study}} the {{state-dependent}} two-user interference channel, {{where the state}} information is non-causally known at both transmitters but unknown to either of the receivers. We first propose two coding schemes for the discrete memoryless case: simultaneous encoding for the sub-messages in the first one and super-position encoding in the second one, both with rate splitting and Gel'fand-Pinsker coding. The corresponding achievable rate regions are established. Moreover, for the Gaussian case, {{we focus on the}} simultaneous encoding scheme and propose an active interference cancellation mechanism, which is a generalized dirty-paper coding technique, to partially eliminate the state effect at the receivers. The corresponding achievable rate region is then derived. We also propose several heuristic schemes for some special cases: the strong interference case, the mixed interference case, and the weak interference case. For the strong and mixed interference case, numerical results are provided to show that active interference cancellation significantly enlarges the achievable rate region. For the weak interference case, <b>flexible</b> power <b>splitting</b> instead of active interference cancellation improves the performance significantly. Moreover, we focus on the simplest symmetric case, where both direct link gains are the same with each other, and both interfering link gains are the same with each other. We apply the above coding scheme with different dirty paper coding parameters. When the state is additive and symmetric at both receivers, we study both strong and weak interference scenarios and characterize the theoretical gap between the achievable symmetric rate and the upper bound, which is shown to be less than 1 / 4 bit for the strong interference case and less than 3 / 4 bit for the weak interference case. Then we provide numerical evaluations of the achievable rates against the upper bound, which validates the theoretical analysis for both strong and weak interference scenarios. Finally, we define the generalized degrees of freedom for the symmetric Gaussian case, and compare the lower bounds against the upper bounds for both strong and weak interference cases. We also show that our achievable schemes can obtain the exact optimal values of the generalized degrees of freedom, i. e., the lower bounds meet the upper bounds for both strong and weak interference cases...|$|R
40|$|Literature {{provided}} {{few examples}} of studies that applied embedded intergroup theory (a combination of social identity, differentiation of identity, social comparison, dynamic conservatism, structural encasement, and embeddedness theories) to large systems such as communities. This study contributed to theory development in a large-system setting by assessing the ability of embedded intergroup theory to explain the community dynamics surrounding a community development effort in Coatesville, Pennsylvania in the 1990 s. ^ This study used the extended case study model of ethnography. Analysis started with theory (macro), analyzed the data for unexplainable phenomena (micro), and returned to theory to enhance it (macro). Data was gathered through review of archival sources (newspaper, printed materials), focus group, interviews, and field observation. ^ Coatesville {{was one of the}} poorest communities in Pennsylvania, located in the wealthiest county in Pennsylvania. Coatesville 2 ̆ 7 s population was approximately half non-white, while the county was predominantly white. The intergroup dynamics of the community change effort were rooted in a lynching of an African American in 1911. Key to the dynamics was conflict that centered across two identity splits: “insider” versus “outsider,” and white versus black. ^ A group of business leaders (CAPP) successfully initiated a community change effort, despite strong resistance, with city leadership eventually assuming the lead in implementing a comprehensive community development plan. Indications were seen that the community change effort might have initiated conflict across a new class identity split. ^ The study generated a model of a community social system (a “Rubik 2 ̆ 7 s Cube”) that suggested two propositions: (1) <b>flexible</b> identity <b>splits</b> hold a community together and support evolving change; and (2) a balance of focus on intra and intergroup relationships maximizes a group 2 ̆ 7 s development and effectiveness as a system change agent. ^ A weakness of embedded group theory was that it provided no conceptual link to intragroup development theory. The internal development of two key change agents appeared linked to community dynamics, but no theoretical model exists to understand this interaction. This is a critical area for new research. ...|$|R
40|$|Topology {{optimization}} is at {{the highest}} level in the field of structural optimization. The introduction of the level set methods into this field has demonstrated several attractive advantages. However, a significant limit of the level set methods in topology optimization is that it can not create new holes in the design domain. Topological derivative approach is a relatively new procedure to overcome this problem. It can indicate the appropriate location to create new holes such that the strong dependency of the optimal topology on the initial design can be alleviated. In this paper, we investigated the use of topological derivative in combination with the level set methods for topology optimization of solid structures. By comparing the topological derivative value with the penalized volume constraint function value, we can determine whether and where the holes should be nucleated. In this paper we also developed an approach to evolving the level set function by replacing the gradient item by a Delta function in the standard Hamilton-Jacobi equation. We found that this handling can create new holes in the solid domain, grow a structure from an empty domain, and improve the convergence rate of the optimization process. We also discussed some issues in the implementation of our new approach. The success of our approach is demonstrated by several numerical examples. 2. Keywords: level set method, topology optimization, topological derivative The level set models are topologically <b>flexible,</b> and can <b>split</b> and merge as necessary in the course of deformation without the need for re-parametrization. Because of these advantages, level set methods are becoming a powerful mathematical tool to deal with shap...|$|R
30|$|Future work is {{suggested}} {{to go further}} in adding more shipping elements and rules so that tramp shipping models become more realistic. Elements such as <b>flexible</b> cargo sizes, <b>splitting</b> of loads, and different ship speed, although they affect profitability if formulated within the models, they can be handled instead by sensitivity and what-if analysis, giving other elements {{the chance to be}} formulated. Stochastic and profit-per-day models need more attention. Cargo transport demand needs more study on the construction of probability distribution of the transport demand for main types of cargo. OR-Based Decision Support Systems are used to integrate OR models into database management systems. It is highly recommended to build such systems for shipping so that OR methodologies become transparent to ship owners while being supportive at the same time. Moreover, these systems have to interact with the ship owner in friendlier sensitivity and what-if analysis sessions. Because hardware speed represents the prime limitation of the algorithm adopted in this paper, faster computer hardware and communication equipment must be used to enable ship owners take their decisions in the right time. Ship owners, operators of utilities, and researchers are encouraged to meet somewhere to discuss problems of mutual concern. It is highly recommended that workshops are to be considered as the places where all should meet to discuss case studies like the ones mentioned in this research paper. It is the role of international conferences to arrange such workshops in different places worldwide. The future work on tramp shipping should result in an impact on the logistic system in which transportation by ship is part of. An example of this impact is given by this research paper when it shows that shortening ship voyage time, to the extent ship owners can afford, is caused by a stochastic gross-profit-per-day objective. Finally, stochastic gross-profit-per-day objective may be used in other time-sensitive production systems. Examples are crop charts in agriculture, customized production line in industry, product maintenance schedule in services, project plan in construction, and logistics network in trade. It may be used as well in fixed-time production systems, before time being fixed, to determine the optimal amounts of factors of production employed in a multiple-products multiple-systems investment plan. Examples are crop harvesting in agriculture, car manufacturing and assembly lines in industry, port cargo handling in services, road paving in construction, and market control measurements in trade.|$|R
30|$|It {{was found}} that the OR model of Osman et al. (1993) and Christiansen et al. (2007) holds {{characteristics}} close to the tramp shipping characteristics mentioned {{at the beginning of this}} section. The model of either research paper is based on a network of multiple cargo flows. Each network node either represents a load or a discharge event for each cargo. Ships compete in carrying cargoes by following selected arcs in the network, beginning with a start node and ending with an end node. If a network arc is used by a ship, this arc is restricted for use by other ships. An arc is used by a ship if lay can of each arc node can be met and load available in each arc node is within remaining ship capacity. The model assigns network arcs to ships in an attempt to maximise total voyage gross profit for all ships. Both models are nonlinear. Hemmati et al. (2014) and Christiansen and Fagerholt (2014) have presented better tramp shipping characteristics. The former have used a linear objective but used heuristic algorithms to solve their problem. The latter have presented some linear and non-linear models; some handle flexible cargo sizes of what is called ‘more or less owner’s option’, some handle splitting of cargo loads, and some others handle varying ship speed. Most of these models use heuristic algorithms to solve the problem of concern. <b>Flexible</b> cargo sizes, <b>splitting</b> of loads, and different ship speed, although they have been formulated within the models; they could have been handled via sensitivity and what-if analysis after solution. This might help other important shipping elements to be formulated as well. Sensitivity and what-if analysis are necessary validation tools in tramp shipping to handle possible changes in cargo quantity and freight rate, cargo handling rate and charges, and ship speed and fuel consumption. Instead of ship full loads assumed in Brown et al. (1987) and Laake and Zhang (2013), Vilhelmsen et al. (2015) have developed a linear model to handle the case where multiple cargoes can be carried simultaneously on board each ship. The review of the previous models is reported in the follows items. The first is that the model objective maximises voyage gross profit, while in tramp shipping the objective has to maximise gross profit per day. The second is that transport demand is assumed deterministic. In shipping, some cargoes may have random demand. The third is that the model with non-linear objective or/and constraints call for software solutions usually less reliable and inefficient. The fourth is that the authors brought no evidence on the possibility of solving large problems when more cargoes and ships are added.|$|R
