2|30|Public
30|$|The {{visual and}} {{semantic}} modules {{can either be}} learned jointly with the core ZSL module in an <b>end-to-end</b> <b>procedure</b> by back-propagation of the error signal from the core ZSL module to the two lower modules, or they can be learned independently on unsupervised or auxiliary supervised tasks (e.g., pretraining the visual module on the ILSVRC classification task and pretraining the semantic module as an unsupervised word embedding model).|$|E
40|$|The UMTS {{appear to}} be the {{communication}} system of the next future. In this system all the communication services will be included. This means that the Quality of Service (QoS) will have a great importance. So is envisaged that the management of the system will take in account the momentary system capabilities continually determined in real time. Until now, the quality of the links is determined by end-to-end in line procedures based on Bit Error Rate (BER) measurements. It is clear that an <b>end-to-end</b> <b>procedure</b> cannot detect the worse section of the link. To overcome this inconvenient the more recent trend of QoS management is to realise a distributed monitoring of the whole system. Following this last trend this paper proposes a distributed off-line monitoring of the system based on the analysis of the Quality of Analog Signal (QoAS). This proposed analysis implies a high-speed analog to digital conversion of the Intermediate Frequency signal and a very complex processing. In this paper, the state-of-the-art researches performed in this perspective are described...|$|E
30|$|Signaling {{overhead}} can {{be reduced}} by designing more efficient protocols and by thinning out some protocol layers, e.g., for the simplification of <b>end-to-end</b> <b>procedures.</b> On the air interface, the signaling for random access, time adjustment, and/or resource assignment {{can be reduced}}, for example, by using contention-based access, waveforms that are more tolerant to timing mismatch as compared to orthogonal frequency division multiplexing (OFDM), and/or by resource reservation, making the air interface more flexible and suitable for small payload traffic. For instance, methods dedicated to contention-based access for massive number of devices without scheduled access and multi-user detection facilitated by compressive sensing are good candidates to reduce signaling.|$|R
40|$|The {{purpose of}} the study is to present an <b>end-to-end</b> {{experimental}} <b>procedure,</b> based on a polymer gel phantom, capable of assessing the total geometric uncertainty in GK radiosurgery applications, in which MR images are solely used for both target delineation and registration of patient image coordinates to the Leksell space. As a result the study aims to propose a time-efficient method, based on corresponding polymer gel results, which considerably improves the geometric accuracy in GK treatment delivery...|$|R
30|$|Results: We {{included}} 158 patients: 138 with ureterolysis and 20 {{patients with}} resection and <b>end-to-end</b> anastomosis. Associated <b>procedures</b> were: 13 patients - segmental bowel resection, 15 - shaving of recto-vaginal nodule and 23 - resection of bladder endometriosis nodule. We founded 9 major complications from which 4 uretero-vaginal fistula.|$|R
40|$|Hypothesis: End-to-side repair (ES) with {{ligation}} of the {{tracheoesophageal fistula}} (TEF) reduces {{the risks of}} stricture and gastroesophageal reflux disease requiring operation compared with the end-to-end repair of esophageal atresia and distal TEF. Design Case series with institutional and historical control subjects. Setting Referral children's hospital. Patients One hundred thirty-four infants diagnosed as having esophageal atresia and distal TEF between June 30, 1968, and July 1, 2003. Interventions Ninety-six infants having ES and 38 having end-to-end repair. Main Outcome Measures Patients were studied for overall survival, surgical complications, and well-being {{during the first year}} of life. Results: Survival was 95 % vs 90 % (patients undergoing ES vs end-to-end repair). Complications included anastomotic leak, 8 % vs 13 %; recurrent TEF, 7 % vs 3 %, with only 1 recurrence in the last 28 patients having ES; anastomotic stricture (requiring dilatation), 5 % vs 13 %; gastroesophageal reflux disease requiring operation, 6 % vs 18 %; and esophageal dysmotility, which was present following nearly all ES and <b>end-to-end</b> <b>procedures.</b> Tracheomalacia-related respiratory symptoms following ES decreased from 50 % to 11 % at 1 year of age. Age-appropriate diet following ES was achieved in 93 % by 1 year; 5 % experienced occasional dysphagia or choking episodes. Conclusions :The ES operation is accompanied by a reduced rate of stricture and gastroesophageal reflux disease requiring operation compared with end-to-end repair. Earlier concerns regarding an unacceptable risk of recurrent TEF were not substantiated...|$|R
40|$|For vehicle design, shield optimization, mission planning, and {{astronaut}} risk assessment, {{the exposure}} from {{galactic cosmic rays}} (GCR) poses a significant and complex problem both in low Earth orbit and in deep space. To address this problem, various computational tools {{have been developed to}} quantify the exposure and risk {{in a wide range of}} scenarios. Generally, the tool used to describe the ambient GCR environment provides the input into subsequent computational tools and is therefore a critical component of <b>end-to-end</b> <b>procedures.</b> Over the past few years, several researchers have independently and very carefully compared some of the widely used GCR models to more rigorously characterize model differences and quantify uncertainties. All of the GCR models studied rely heavily on calibrating to available near-Earth measurements of GCR particle energy spectra, typically over restricted energy regions and short time periods. In this work, we first review recent sensitivity studies quantifying the ions and energies in the ambient GCR environment of greatest importance to exposure quantities behind shielding. Currently available measurements used to calibrate and validate GCR models are also summarized within this context. It is shown that the AMS-II measurements will fill a critically important gap in the measurement database. The emergence of AMS-II measurements also provides a unique opportunity to validate existing models against measurements that were not used to calibrate free parameters in the empirical descriptions. Discussion is given regarding rigorous approaches to implement the independent validation efforts, followed by recalibration of empirical parameters...|$|R
40|$|JANUS is a multi-lingual speech-tospeech {{translation}} {{system designed}} to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain. In an attempt to achieve both robustness and translation accuracy we use two di erent translation components: the GLR module, designed to be more accurate, and the Phoenix module, designed to be more robust. We analyze {{the strengths and weaknesses}} of each of the approaches and describe our work on combining them. Another recent focus has been on developing a detailed <b>end-to-end</b> evaluation <b>procedure</b> to measure the performance and e ectiveness of the system. We present our most recent Spanish-to-English performance evaluation results...|$|R
40|$|The {{areas of}} machine {{learning}} and communication technology are converging. Today's communications systems generate {{a huge amount}} of traffic data, which can help to significantly enhance the design and management of networks and communication components when combined with advanced machine learning methods. Furthermore, recently developed <b>end-to-end</b> training <b>procedures</b> offer new ways to jointly optimize the components of a communication system. Also in many emerging application fields of communication technology, e. g., smart cities or internet of things, machine learning methods are of central importance. This paper gives an overview over the use of machine learning in different areas of communications and discusses two exemplar applications in wireless networking. Furthermore, it identifies promising future research topics and discusses their potential impact. Comment: 8 pages, 4 figure...|$|R
40|$|JANUS is a multi-lingual speech-to-speech {{translation}} {{system designed}} to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain. In this paper we describe our methodology for evaluating translation performance. Our current focus is on end-to-end evaluations - {{the evaluation of the}} translation capabilities of the system as a whole. The main goal of our <b>end-to-end</b> evaluation <b>procedure</b> is to determine translation accuracy on a test set of previously unseen dialogues. Other goals include evaluating the effectiveness of the system in conveying domain-relevant information and in detecting and dealing appropriately with utterances (or portions of utterances) that are out-of-domain. End-to-end evaluations are performed in order to verify the general coverage of our knowledge sources, guide our development efforts, and to track our improvement over time. We discuss our evaluation procedures, the criteria used for assigning scores to translations [...] ...|$|R
40|$|Developments in {{spaceborne}} Earth Observation (EO) {{sensor technology}} {{over the last}} decade, combined with well-tested physical models and multispectral data-processing techniques developed from the early 1980 s, have paved {{the way to the}} global monitoring of volcanoes by sensors of metric, decametric, kilometric and multi-kilometric spatial resolution. Such variable geometries provide for revisit intervals ranging from about monthly – at high-spatial resolution in Low-Earth Orbit – to less than 5 min – at low-spatial resolution, from geostationary platforms. There are currently about 20 spacecrafts available for carrying out 24 / 7 quantitative observations of volcanic unrest, at all resolutions and {{as close as possible to}} real-time. We show some successful examples of synergetic EO on volcanoes on three continents from 10 different payloads, automatically processed with three, <b>end-to-end</b> unsupervised <b>procedures,</b> on eight major eruptions and a lava lake between 2006 and 2014...|$|R
40|$|The {{evolution}} of third generation cellular network {{focuses on the}} provision of enriched multimedia services {{and the support of}} QoS (quality of service) guarantees. The IMS (IP multimedia subsystem) is specified as subsystem providing resource, admission and charging control. Enabling GPRS (general packet radio service) and UMTS (universal mobile telecommunications system) to support multicast and broadcast transmissions, 3 GPP (third generation partnership project) has recently standardised the MBMS (multimedia broadcast multicast services) framework. Up to now, IMS and MBMS are separated subsystems sharing no common interfaces in order to utilise each other. However, 3 GPP is working currently on release 7 to integrate IMS and MBMS. In this paper we present an efficient integration of IMS and MBMS which supports several phases of unicast, multicast and broadcast transmissions. Furthermore, an integrated solution framework is introduced. Several <b>end-to-end</b> signalling <b>procedures</b> are finally discussed...|$|R
40|$|Recognizing text in {{the wild}} is a really {{challenging}} task because of complex backgrounds, various illuminations and diverse distortions, even with deep neural networks (convolutional neural networks and recurrent neural networks). In the <b>end-to-end</b> training <b>procedure</b> for scene text recognition, the outputs of deep neural networks at different iterations are always demonstrated with diversity and complementarity for the target object (text). Here, a simple but effective deep learning method, an adaptive ensemble of deep neural networks (AdaDNNs), is proposed to simply select and adaptively combine classifier components at different iterations from the whole learning system. Furthermore, the ensemble is formulated as a Bayesian framework for classifier weighting and combination. A variety of experiments on several typical acknowledged benchmarks, i. e., ICDAR Robust Reading Competition (Challenge 1, 2 and 4) datasets, verify the surprised improvement from the baseline DNNs, {{and the effectiveness of}} AdaDNNs compared with the recent state-of-the-art methods...|$|R
40|$|We {{present a}} {{feasibility}} study of using TeraGrid resources for computing non-rigid registration (NRR) of brain MRI. The <b>end-to-end</b> NRR <b>procedure</b> we consider has previously undergone clinical validation at Brigham and Women’s Hospital, Boston. We consider {{the use of}} TeraGrid resources in two scenarios. First, we evaluate the feasibility of using a grid-enabled implementation to provide timely execution of the most time-consuming components of the registration. Second, we describe a workflow implementation to enable speculative computation of registration to improve confidence in the result and assist in retrospective evaluation of the method. Our initial results indicate that TeraGrid provides practical and cost-effective means to support IGNS image processing. At the same time, performance limitations of the intra-operative speculative NRR execution using conventional means of workflow scheduling prevent its intra-operative applications. The results of this paper motivate future work in highthroughput scheduling and execution models on TeraGrid, which can be leveraged by the workflow implementation we present. ...|$|R
40|$|Abstract. JANUS is a multi-lingual speech-to-speech {{translation}} {{system designed}} to facilitate communication between two parties engaged in a spontaneousconversation in a limited domain. In this paper we describe our methodology for evaluating translation performance. Our current focus is on end-to-end evaluations- {{the evaluation of the}} translation capabilities of the system as a whole. The main goal of our <b>end-to-end</b> evaluation <b>procedure</b> is to determine translation accuracy on a test set of previously unseen dialogues. Other goals include evaluating the effectiveness of the system in conveying the domain relevant information and in detecting and dealing appropriately with utterances (or portions of utterances) that are out-of-domain. End-toend evaluations are performed in order to verify the general coverage of our knowledge sources, guide our development efforts, and to track our improvement over time. We discuss our evaluation procedures, the criteria used for assigning scores to translations produced by the system, and the tools developed for performing this task. Our most recent Spanish-to-English performance evaluation results are presented as an example...|$|R
40|$|Recent {{advances}} in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, {{it is now}} well-known that the arithmetic operations of deep networks can be encoded down to 8 -bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3 -bit fixed-point results in significant losses in performance. In this {{paper we propose a}} new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage {{of the fact that the}} weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base- 2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an <b>end-to-end</b> training <b>procedure</b> that uses log representation at 5 -bits, which achieves higher final test accuracy than linear at 5 -bits. Comment: 10 pages, 7 figure...|$|R
40|$|Abstract—This paper {{describes}} an algorithm for recovering the rigid 3 -DOF transformation (offset and rotation) between pairs of sensors mounted rigidly {{in a common}} plane on a mobile robot. The algorithm requires only a set of sensor observations made as the robot moves along a suitable path. Our method does not require synchronized sensors; nor does it require complete metrical reconstruction of the environment or the sensor path. We show that incremental pose measurements alone are sufficient to recover sensor calibration through nonlinear least squares estimation. We use the Fisher Information Matrix to compute a Cramer-Rao lower bound (CRLB) for the resulting calibration. Applying the algorithm in practice requires a non-degenerate motion path, a principled procedure for estimating per-sensor pose displacements and their covariances, a way to temporally resample asynchronous sensor data, {{and a way to}} assess the quality of the recovered calibration. We give constructive methods for each step. We demonstrate and validate the <b>end-to-end</b> calibration <b>procedure</b> for both simulated and real LIDAR and inertial data, achieving CRLBs, and corresponding calibrations, accurate to millimeters and milliradians. Source code is available fro...|$|R
40|$|Recent {{work has}} shown deep neural {{networks}} (DNNs) {{to be highly}} susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100 % mis-classification for {{a state of the}} art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new <b>end-to-end</b> training <b>procedure</b> that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty...|$|R
40|$|Spectral {{clustering}} is {{a leading}} and popular technique in unsupervised data analysis. Two of its major limitations are scalability and generalization of the spectral embedding (i. e., out-of-sample-extension). In this paper we introduce a deep learning approach to spectral clustering that overcomes the above shortcomings. Our network, which we call SpectralNet, learns a map that embeds input data points into the eigenspace of their associated graph Laplacian matrix and subsequently clusters them. We train SpectralNet using a procedure that involves constrained stochastic optimization. Stochastic optimization allows it to scale to large datasets, while the constraints, which are implemented using a special-purpose output layer, allow us to keep the network output orthogonal. Moreover, the map learned by SpectralNet naturally generalizes the spectral embedding to unseen data points. To further {{improve the quality of}} the clustering, we replace the standard pairwise Gaussian affinities with affinities leaned from unlabeled data using a Siamese network. Additional improvement can be achieved by applying the network to code representations produced, e. g., by standard autoencoders. Our <b>end-to-end</b> learning <b>procedure</b> is fully unsupervised. In addition, we apply VC dimension theory to derive a lower bound on the size of SpectralNet. State-of-the-art clustering results are reported on the Reuters dataset. Our implementation is publicly available at [URL]...|$|R
40|$|While {{analyzing}} vehicular sensor data, {{we found}} that frequently occurring waveforms could serve as features for further analysis, such as rule mining, classification, and anomaly detection. The discovery of waveform patterns, also known as time-series motifs, has been studied extensively; however, available techniques for discovering frequently occurring time-series motifs were found lacking in either efficiency or quality: Standard subsequence clustering results in poor quality, {{to the extent that}} it has even been termed 'meaningless'. Variants of hierarchical clustering using techniques for efficient discovery of 'exact pair motifs' find high-quality frequent motifs, but at the cost of high computational complexity, making such techniques unusable for our voluminous vehicular sensor data. We show that good quality frequent motifs can be discovered using bounded spherical clustering of time-series subsequences, which we refer to as COIN clustering, with near linear complexity in time-series size. COIN clustering addresses many of the challenges that previously led to subsequence clustering being viewed as meaningless. We describe an <b>end-to-end</b> motif-discovery <b>procedure</b> using a sequence of pre and post-processing techniques that remove trivial-matches and shifted-motifs, which also plagued previous subsequence-clustering approaches. We demonstrate that our technique efficiently discovers frequent motifs in voluminous vehicular sensor data as well as in publicly available data sets. Comment: 13 pages, 8 figures, Technical Repor...|$|R
40|$|In this paper, {{we propose}} a {{probabilistic}} parsing model, which defines a proper conditional probability distribution over non-projective dependency trees {{for a given}} sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTM-CNNs which benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM and CNN. On top of the neural network, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. We evaluate our model on 17 different datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straight-forward <b>end-to-end</b> model training <b>procedure</b> via back-propagation. Our parser achieves state-of-the-art parsing performance on nine datasets. Comment: To appear in IJCNLP 201...|$|R
40|$|A {{clinical}} and body compositional {{study has been}} made of 150 patients with morbid obesity and their responses to four different dimensional alterations of jejunoileal bypass. Total body potassium was estimated by measuring 40 K with the whole body counter and total body water by tritiated water dilution. Body compositional data derived from these measurements were compared in the 4 groups during followup periods up to 4 years and related to clinical results. Initially, patients were two or more times overweight due to excess (60 to 65 %) body fat and increased hydration (21 %) of lean tissues. The 80 <b>end-to-end</b> jejunoileal bypass <b>procedures</b> of Groups 3 and 4 (30 cm jejunum to 15 or 20 cm ileum) had better weight losses and clinical results in followup were rated "good" in 60 % and 81 % respectively. These results were accompanied by a greater degree of improvement in body composition than was observed in the other groups under study...|$|R
40|$|We {{present an}} {{evaluation}} of aircraft observations of the carbon and greenhouse gases CO 2, CH 4, N 2 O, and CO using a direct-absorption pulsed quantum cascade laser spectrometer (QCLS) operated during the HIPPO and CalNex airborne experiments. The QCLS made continuous 1 Hz measurements with 1 σ Allan precisions of 20, 0. 5, 0. 09, and 0. 15 ppb for CO 2, CH 4, N 2 O, and CO, respectively, over > 500 flight hours on 79 research flights. The QCLS measurements are compared to two vacuum ultraviolet (VUV) CO instruments (CalNex and HIPPO), a cavity ring-down spectrometer (CRDS) measuring CO 2 and CH 4 (CalNex), two broadband non-dispersive infrared (NDIR) spectrometers measuring CO 2 (HIPPO), two onboard gas chromatographs measuring a variety of chemical species including CH 4, N 2 O, and CO (HIPPO), and various flask-based measurements of all four species. QCLS measurements are tied to NOAA and WMO standards using an in-flight calibration system, and mean differences when compared to NOAA CCG flask data over the 59 HIPPO research flights were 100, 1, 1, and 2 ppb for CO 2, CH 4, N 2 O, and CO, respectively. The details of the <b>end-to-end</b> calibration <b>procedures</b> and the data quality assurance and quality control (QA/QC) are presented. Specifically, we discuss our practices for the traceability of standards given uncertainties in calibration cylinders, isotopic and surface effects for the long-lived greenhouse gas tracers, interpolation techniques for in-flight calibrations, {{and the effects of}} instrument linearity on retrieved mole fractions...|$|R
40|$|International Telemetering Conference Proceedings / October 27 - 30, 1997 / Riviera Hotel and Convention Center, Las Vegas, NevadaThe Army’s {{continuing}} {{effort to}} develop self-guided, anti-tank weapon systems has been fueled by successful development of an earlier generation of smart weapons. These self-guided systems, also labeled “brilliant weapons,” will present a natural progression to “smart” weapons development and testing. What has been {{critical to the success}} of these smart weapons development programs has been an end-to-end testability provided by on-board telemetry methods. The <b>end-to-end</b> test <b>procedures</b> can be efficiently performed in a static laboratory environment where space is available. On board the smart weapon, end-to-end testability is less feasible due to space and bandwidth limitations. The STAFF XM 943 projectile development program makes use of a dual telemetry link to transmit an array of measurements which characterize the performance of the projectile in and end-to-end fashion. The dual telemetry systems provide analog measurement capability to evaluate system component-level functions and digital measurement capability to evaluate a system processor which lends itself to computer processing. The digital data is intrinsic to smart weapon systems since they typically employ embedded microprocessors for projectile system control. The STAFF XM 943 electronic system is controlled by a TMS 320 C 30 microprocessor. The dual telemetry system employs a traditional FM/FM technique for monitoring a number of analog functions and a “quasi-traditional” PCM/FM scheme for digital transmission. This paper discusses the implementation of this dual telemetry approach for the XM 943 Projectile System...|$|R
40|$|We {{investigate}} the scientific {{impact of the}} Wide Field X-ray Telescope mission. We present simulated images and spectra of X-ray sources as observed from the three surveys planned for the nominal 5 -year WFXT lifetime. The goal of these simulations is to provide WFXT images of the extragalactic sky in different energy bands based on accurate description of AGN populations, normal and star forming galaxies, groups and clusters of galaxies. The images are realized using a detailed PSF model, instrumental and physical backgrounds/foregrounds, accurate model of the effective area and the related vignetting effect. Thanks to this comprehensive modelization of the WFXT properties, the simulated images {{can be used to}} evaluate the flux limits for detection of point and extended sources, the effect of source confusion at very faint fluxes, and in general the efficiency of detection algorithms. We also simulate the spectra of the detected sources, in order to address specific science topics which are unique to WFXT. Among them, we focus on the characterization of the Intra Cluster Medium (ICM) of high-z clusters, and in particular on the measurement of the redshift from the ICM spectrum in order to build a cosmological sample of galaxy clusters. The <b>end-to-end</b> simulation <b>procedure</b> presented here, is a valuable tool in optimizing the mission design. Therefore, these simulations can be used to reliably characterize the WFXT discovery space and to verify the connection between mission requirements and scientific goals. Thanks to this effort, we can conclude on firm basis that an X-ray mission optimized for surveys like WFXT is necessary to bring X-ray astronomy {{at the level of the}} optical, IR, submm and radio wavebands as foreseen in the coming decade. Comment: "Proceedings of "The Wide Field X-ray Telescope Workshop", held in Bologna, Italy, Nov. 25 - 26 2009. To appear in Memorie della Societa Astronomica Italiana 2010 (arXiv: 1010. 5889) ...|$|R
40|$|BACKGROUND AND OBJECTIVES: Laser-based {{repairing}} techniques offer {{several advantages}} respect to standard suturing in microsurgery. In this work we evaluate the applicability and feasibility of two innovative laser-based approaches for microvascular repair and anastomoses: (1) laser-assisted vascular repair (LAVR); (2) laser-assisted end-to-end vascular anastomosis (LAVA). All these procedures have been {{executed by the}} use of diode laser irradiation and chitosan-patches infused with Indocyanine Green (ICG). STUDY DESIGN/MATERIALS AND METHODS: Experiments were performed on 30 rabbits. Twenty animals underwent LAVR and 10 <b>end-to-end</b> LAVA <b>procedures.</b> In the LAVR group, a 5 -mm longitudinal cut was performed on the common carotid artery (CCA), then an ICG-infused chitosan patch was topically applied and laser-soldered over the arterial lesion. In the LAVA group the end-to-end anastomosis was executed on CCA by means of application of the three interrupted sutures and subsequent laser soldering of the ICG-infused patch. Animals underwent different follow-up periods (2, 7, 30, and 90 days). At the end of every follow-up, the animals were re-anesthetized and a microdoppler analysis was performed in order to check patency of the treated vessels. Then soldered segments were excised and subjected to histological and ultrastructural evaluations. RESULTS: At the end of surgery no bleeding from the treated segment was observed; all the treated vessels were patent. At the end of follow-up periods, no signs of perivascular haemorrhage were found. An intraoperative microdoppler evaluation assessed the patency of all the treated vessels. Histology showed a good reorganization of the vascular wall structures and an early endothelial regeneration was observed by SEM. CONCLUSIONS: Our study demonstrated the efficacy of laser tissue soldering by means of ICG-infused chitosan patches for the in vivo repairing of microvascular lesions and end-to-end anastomoses. This approach offers several advantages over conventional suturing methods and is technically easy to perform, minimizing the surgical trauma to vessels...|$|R
30|$|In this paper, a novel Hybrid Redundant Macroblock and Intra {{macroblock}} Refreshment {{approach has}} been proposed to combat packet loss. In the proposed approach, redundant coding and/or intra coding are optimally allocated in macroblock level. Whether to use redundant coding and/or intra coding and the quantization parameter of the redundant coding is all determined in the <b>end-to-end</b> rate-distortion optimization <b>procedure.</b> It is worth mentioning that, in the proposed approach, only information from the previously encoded frames {{is used to calculate}} the end-to-end distortion in the RDO process; therefore, no additional delay is caused, making the proposed approach suitable for real-time applications such as video conference. Extensive experimental results show that the proposed method provides better performance than other error-resilient source coding approaches. The performance gap between the proposed approach and the Optimal Intra Refreshment is huge, and in some simulation environments, the proposed approach can provide 4 dB higher PSNR than the conventional Optimal Intra Refreshment with the same bitrate. Our future work is to calculate the end-to-end distortion in sub-pixel accuracy; therefore, more accurate end-to-end distortion would be available, which would eventually lead to better resource allocation.|$|R
40|$|The Thesis of the {{dissertation}} work {{present results}} of the accelerometer test operation on board of the Russian small spacecraft Universat – 2. This technical experiment was performed {{in the frame of}} the project TEASER (Technological Experiment And Space Environmental Resistance) financed by Ministry of Industry and Trade of the Czech Republic. The principal aim of the project was development and application of <b>end-to-end</b> tests <b>procedures</b> for MAC 04 TS. Generally, End-to-end Testing is intended for verification of the new product comprehensive operational sequence. In the context of the accelerometer this process includes following aspects: On-Ground Qualification of the instrument flight model and its integration with space platform, operation in orbit to verify the device functionality and performances. The MAC 04 TS is the new modification of the triaxial electrostatic high sensitive microaccelerometer MAC (or MACEK) designed for measurement of non-gravitational accelerations acting to orbiting spacecraft. Mentioned non-conservative perturbations are primary constraint for theoretical predictions of the spacecrafts orbit evolution. Problem of the precise orbit determination is important not only for ballistic mission analysis and support but at planning and realization of the science researches in Geophysics, Geodesy or in Space Physics branches… Presented text to examination is devoted to partial task of the MAC 04 TS end-to-end testing, especially, measurement and analysis of the micro-gravitational accelerations in the instrument position on board of the spacecraft. Solution of the problem is based on two methods. The first method was based on the calculation of the accelerations by telemetry information about satellite attitude motion. The second method consisted in direct measuring the accelerations by the triaxial low frequency accelerometer MAC 04 TS and subsequent smoothing measurement data. Comparison of the acceleration values, obtained by different ways, was carried out as a result of constructing the approximation of the measured acceleration values by their calculated values. The approximation was constructed by the least squares method. Both methods gave similar results. The received estimations of the accelerometer measurements can be used for the analysis of the accelerometer verification...|$|R
40|$|A {{number of}} image-processing {{problems}} can be formulated as optimization problems. The objective function typically contains several terms specifically designed for different purposes. Parameters in front of these terms are used to control the relative weights among them. It is of critical importance to tune these parameters, as quality of the solution depends on their values. Tuning parameter is a relatively straightforward task for a human, as one can intelligently determine the direction of parameter adjustment based on the solution quality. Yet manual parameter tuning is not only tedious in many cases, but becomes impractical when a number of parameters exist in a problem. Aiming at solving this problem, this paper proposes an approach that employs deep reinforcement learning to train a system that can automatically adjust parameters in a human-like manner. We demonstrate our idea in an example problem of optimization-based iterative CT reconstruction with a pixel-wise total-variation regularization term. We set up a parameter tuning policy network (PTPN), which maps an CT image patch to an output that specifies the direction and amplitude by which the parameter at the patch center is adjusted. We train the PTPN via an <b>end-to-end</b> reinforcement learning <b>procedure.</b> We demonstrate that {{under the guidance of}} the trained PTPN for parameter tuning at each pixel, reconstructed CT images attain quality similar or better than in those reconstructed with manually tuned parameters. Comment: 8 pages, 8 figures, 2 table...|$|R
40|$|Outside the factory, robots {{will often}} {{encounter}} mechanical systems {{with which they}} need to interact. The robot may need to open and unload a kitchen dishwasher or move around heavy construction equipment. Many of the mechanical systems encountered {{can be described as}} a series of rigid segments connected by joints. The pose of a segment places constraints on adjacent segments because they are mechanically 'connected. When modeling or perceiving the motion of such an articulated system, it is beneficial to make use of these constraints to reduce uncertainty. In this thesis, we examine two aspects of perception related to articulated structures. First, we examine the special case of a single segment and recover the rigid body transformation between two sensors mounted on it. Second, we consider the task of tracking the configuration of a multi-segment structure, given some knowledge of its kinematics. First, we develop an algorithm to recover the rigid body transformation, or extrinsic calibration, between two sensors on a link of a mobile robot. The single link, a degenerate articulated object, is often encountered in practice. The algorithm requires only a set of sensor observations made as the robot moves along a suitable path. Over-parametrization of poses avoids degeneracies and the corresponding Lie algebra enables noise projection to and from the over-parametrized space. We demonstrate and validate the <b>end-to-end</b> calibration <b>procedure,</b> achieving Cramer-Rao Lower Bounds. The parameters are accurate to millimeters and milliradians in the case of planar LIDARs data and about 1 cm and 1 degree for 6 -DOF RGB-D cameras. Second, we develop a particle filter to track an articulated object. Unlike most previous work, the algorithm accepts a kinematic description as input and is not specific to a particular object. A potentially incomplete series of observations of the object's links are used to form an on-line estimate of the object's configuration (i. e., the pose of one link and the joint positions). The particle filter does not require a reliable state transition model, since observations are incorporated during particle proposal. Noise is modeled in the observation space, an over-parametrization of the state space, reducing the dependency on the kinematic description. We compare our method to several alternative implementations and demonstrate lower tracking error for fixed observation noise. by Jonathan David Brookshire. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2014. Cataloged from PDF version of thesis. Includes bibliographical references (pages 155 - 161) ...|$|R
40|$|This {{dissertation}} {{is composed}} of two independent studies: Cellular research and WBAN (Wireless Body Area Network) research. Both investigations are directed towards improving the system performance in wireless communication systems in terms of Quality of Service (QoS) and system capacity. For the Cellular research part, this dissertation will present novel user-specific QoS requirements as defined by their respective Mean Opinion Score (MOS) formulas, and associated schedulers for wireless applications and systems that optimize spectral allocation. User-specific QoS requirements are defined and several methods {{to make use of}} such requirements to maximum the spectral utilization are presented. Five User-Specific QoS Aware (USQA) schedulers are proposed that consider the user-specific QoS requirements in the allocation of spectral resources. Schedulers are introduced that dynamically adapt to the user-specific QoS requirements to improve quality as measured by the MOS, or the system capacity, or can improve both the quality and system capacity. Due to the different cell deployment arrangements and inter-cell interference in heterogeneous networks in comparison to homogeneous networks, the USQA scheduling is also analyzed and the system performance is evaluated in such networks. Throughput improvements of File Transfer Protocol (FTP) applications benefiting from the rate adaptation and MAC (Media Access Control) scheduling algorithms for video applications that incorporate user-specific QoS requirements to improve system capacity are demonstrated. Another novel approach recognizes that the user-specific frequency sensitivity can be used to improve capacity. There is considerable variation in the audible range of frequencies that can be perceived by individuals, especially at the high frequency end, which is primarily affected by a gradual decline with age. This can be utilized to improve the system performance by personalizing the VoIP codecs and decreasing the user 2 ̆ 7 s source data rate for people from an older age group and thus increase the system capacity. Given the potentially substantial system performance gain resulting from the USQA schedulers, it is critical to analyze their feasibility and complexity in practical LTE (4 G cellular) and future wireless systems. From the LTE system perspective, LTE QoS <b>end-to-end</b> signaling <b>procedures</b> are addressed, and corresponding protocol adaptations are analyzed in order to support the USQA schedulers. In addition, the optimal scheduling period is analyzed that trades off between performance gain and implementation complexity. In the WBAN research, MIMO (Multiple Input Multiple Output) in vivo antenna technologies are introduced and are motivated by the high data rate requirements of wirelessly transmitted low-delay High Definition (HD) video during Minimally Invasive Surgery (MIS). MIMO in vivo technologies are proposed {{to be used in the}} in vivo environments to enhance and determine the maximum data transmission rate while satisfying the Specific Absorption Rate (SAR) power limitations. Various factors are considered in the MIMO in vivo study including antenna separation, antenna angular positions, human body size, and system bandwidth to determinate the maximum data rate that can be supported...|$|R

