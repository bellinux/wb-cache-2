5|1|Public
40|$|Conventional CD and <b>digital</b> <b>audiotape</b> (DAT) systems sample at 44. 1 kHz using {{pulse code}} {{modulation}} 1 (PCM) with a 16 -bit sample resolution. This results in a data rate of 705. 6 kbits per second (kb/s) for a monaural channel or 1. 41 Mbits per second (Mb/s) for stereo (Painter and Spanias, 2000). Though such high data rates are reasonable in audi...|$|E
40|$|Abstract. Seventy-six {{profoundly deaf}} {{children}} with cochlear implants completed a nonword repetition task. The children {{were presented with}} 20 nonword auditory patterns over a loudspeaker {{and were asked to}} repeat them aloud to the experimenter. All of the children were deaf before the age of 36 months. At the time of testing the children were between 8 and 9 years old. All but three of the children used a Nucleus 22 implant with the SPEAK coding strategy at the time of testing. Two of the children used a Clarion device and one child used a Nucleus 24 implant at the time of testing. The duration of implant use for all children was between 4 and 7. 5 years. All of the children in this study produced a repetition response to at least 15 of the 20 target nonwords. The children’s responses were recorded on <b>digital</b> <b>audiotape</b> and then played back to 240 normal-hearing adult listeners for accuracy judgments. Normal-hearing listeners rated the accuracy of the children’s imitation responses against the original target models using a 7 -point scale. The perceptual ratings of the children’s nonwords were strongly correlated with scores on separate independent measures of spoken word recognition, immediate memory span, an...|$|E
40|$|Background and Purpose—Many {{reports in}} the medical {{literature}} have proposed methods of differentiating between gaseous and particulate emboli detected {{with the use of}} transcranial Doppler ultrasound. The {{purpose of this study was}} to compare the previously published methods with our own sample volume length (SVL) parameter to assess the accuracy of each method in classifying emboli. Methods—A pure source of gaseous and particulate emboli was obtained from in vitro and in vivo studies, respectively, and recorded onto <b>digital</b> <b>audiotape</b> for off-line analysis. In total, 100 gaseous emboli and 215 particulate emboli were analyzed to measure four embolic parameters, namely, embolic duration, embolic velocity, relative signal intensity increase (measured embolic power [MEP]), and SVL of the embolic signal (�Duration�Velocity). Receiver operator characteristic analysis was used to assess the optimum threshold for each parameter to differentiate between particulate and gaseous emboli, and levels of sensitivity and specificity were calculated. Results—Embolic duration and velocity produced the poorest levels of sensitivity and specificity compared with the MEP and SVL parameters. The optimum thresholds for embolic duration and velocity were 35 ms and 1 m/s, respectively, which produced a sensitivity (specificity) of 85. 1 % (87 %) and 87 % (67 %), respectively. The optimum MEP and SVL thresholds were 30 dB and 12. 8 mm, respectively, which produced a sensitivity (specificity) of 86. 5 % (95 %) and 93 % (97 %), respectively. The SVL and MEP parameters were compared statistically (� 2) at chosen specificity values of 90 %...|$|E
50|$|Since 1934, {{copyright}} law has treated sound recordings (or phonograms) differently from musical works. Copyright, Designs and Patents Act 1988 defines a sound recording as (a) {{a recording of}} sounds, from which the sounds may be reproduced, or (b) a recording of the whole or any part of a literary, dramatic or musical work, from which sounds reproducing the work or part may be produced, regardless of the medium on which the recording is made or the method by which the sounds are reproduced or produced. It thus covers vinyl records, tapes, compact discs, <b>digital</b> <b>audiotapes,</b> and MP3s that embody recordings.|$|R
40|$|Background and Purpose—The {{detection}} of asymptomatic embolization {{with the use}} of Doppler ultrasound has a number of potential applications in patients with acute stroke. It may provide information on the stroke pathogenesis in individual cases, identify patients with continued embolization, and allow localization of the active embolic source. Methods—We recruited 119 patients with acute anterior circulation infarction within 72 hours of stroke onset. Transcranial Doppler recordings were possible in 100 (84. 0 %). Bilateral 1 -hour middle cerebral artery (MCA) recordings were made and saved on <b>digital</b> <b>audiotape</b> for blinded offline analysis. When embolic signals were detected during screening of the first recording, simultaneous recording was performed from the ipsilateral MCA and common carotid artery for an additional 30 minutes. In all patients with embolic signals at screening and in matched negative controls, recordings were repeated on days 4, 7, and 14. Results—Embolic signals were detected in the symptomatic MCA in 16 patients (16 %). They were more common in patients with carotid stenosis (P, 0. 0001), occurring in 50 % of this group. They were rare in patients with cardioembolic stroke (4. 5 %) and were not detected in patients with lacunar stroke. In the 16 patients with embolic signals, the proportion with embolic signals fell over time (P 50. 0025), but they were still present in a third at 2 weeks. In 10 patients, localization of the embolic source was possible by simultaneous recording from the MCA and the ipsilateral common carotid artery...|$|E
40|$|John Quiggin {{reports on}} the {{powerful}} interests trying to bring the internet to heel THE great paradox {{of the information age}} was apparently first summarised in 1984, by Stewart Brand, now with the Sante Fe Institute: “On the one hand information wants to be expensive, because it’s so valuable. The right information in the right place just changes your life. On the other hand, information wants to be free, because the cost of getting it out is getting lower and lower all the time. So you have these two fighting against each other. ” For years, internet enthusiasts focused on only one side of this dichotomy. “Information wants to be free” became first a slogan, then a cliche. In the late 1990 s, as the idealism of the net pioneers was succeeded by the financial madness of the internet bubble, the paradox intensified. Billions of dollars were sunk into dotcoms {{based on the belief that}} vast fortunes could be made out of giving away information or, in other words, that you could have your internet cake and eat it too. On the sidelines of the internet boom, though, were large and powerful groups who had never believed in free information. The US music and motion picture industry associations had a long history of resisting every new technology or social development that might challenge their control over the product. They had tried and failed to kill videotapes, but had won other battles such as the fight to suppress <b>digital</b> <b>audiotape.</b> They were part of a much larger movement pushing for the conversion of information and knowledge into “intellectual property”. This included the creation of new forms of patents, such as plant variety rights, and massive extensions in the scope and duration of existing systems of patent and copyright protection, as well as greater ease in getting new patents. As with so many other things, the internet bubble took the patenting phenomenon to new heights of absurdity. Amazon patented the idea of ordering books with a single click of the mouse. Reverse or “Dutch” auctions have been around for centuries, but the Priceline company got a patent for running them over the internet. Kozmo and Urban Fetch, two companies based on the idea of home delivery of groceries ordered over the internet, sued each other vigorously over the rights to this “business method” until both went bankrupt. (One wag called this the “Whose stupid idea was this?” lawsuit.) Nor was the madness confined to the US. According to the Age (2 July 2001), a Melbourne man, John Keogh, sought and successfully obtained a patent for the wheel from the Patent Office (now rolled into a larger organisation named, in typical bureaucratic newspeak, IP Australia). Behind these farcical occurrences was a growing concern that patents and other forms of intellectual property would stifle innovation and put giant monopolies like Microsoft in an impregnable position, even when their own activities were largely based on the appropriation of others’ ideas. THE first person to make any real sense of this confusion was Lawrence Lessig, of the Stanford Law School. In his first book, Code and Other Laws of Cyberspace, he argued against the idea, more popular a few years ago, that the very nature of cyberspace made it essentially immune to government regulation. On the contrary, Lessig argued, cyberspace depended on the code used to construct it. The code of the internet had been created in a way that facilitated communication and resisted control, but these were not immutable features. It would be quite feasible to modify the code of cyberspace in ways that would facilitate government regulation and the enforcement of property rights. In his new book, The Future of Ideas, Lessig develops this point further, pointing out that communications systems may be considered to have three layers: a content layer, consisting of the actual communication; a physical layer, representing the medium of communication; and a code layer, representing the rules under which communication takes place. Within this framework, Lessig describes the idea of an information “commons”, drawing on the work of scholars like Elinor Ostrom who have explored the workings of common property systems in a range of contexts. These scholars have refuted the idea popularised by Garret Hardin of an inevitable “tragedy of the commons”, and have shown how common and private property can be combined to enhance the benefits of both. One part of Lessig‘s argument is directed against advocates for enclosure of the information commons through the creation of unfettered rights to private “intellectual property”. This case is not too hard to make. The most striking point is that the leading advocates of intellectual property have also been the biggest beneficiaries of the information commons. Microsoft, for example, receives huge rents from its graphical operating system, its web browser, word processor, spreadsheet, presentation software and even from flight simulators. None of these types of software was invented by Microsoft, and, in most cases, the original innovators received few benefits. Disney has pillaged the public domain for source material (even Mickey Mouse in his original incarnation as Steamboat Willie was a copy of a Buster Keaton character, Steamboat Bill), but now demands permanent ownership of its own “creations”. The more difficult task is to convince supporters of free information that the victory of their cause is not guaranteed by the progress of technology. Lessig makes a number of strong points here, showing how easily the technology underlying the information commons can be turned into one of centralised control. Moreover, Lessig argues, the shift towards control is already well under way. The paradigmatic instance for Lessig is the decision to extend the term of copyright yet again, from 70 years after the author’s death to 99 years, a decision now being fought in the US Supreme Court. Lessig makes a good case for pessimism, but there are also reasons for optimism. First, the most vigorous defences of intellectual property apply to the most trivial instances. As Lessig notes, the best explanation for the evolution of US copyright law is that copyright is extended whenever Mickey Mouse is about to fall into the public domain. The only really successful attack on free information in recent years has been the demolition of Napster, which was basically devoted to disseminating pop songs. Conversely, the freedom to publish and reproduce serious ideas in a text format has never been greater than it is today. Quite literally, anyone with an idea and access to a computer can publish their thoughts to the entire world. I recently ran across a weblog (“blog” to the cognoscenti) maintained by a homeless man who does his publishing in a Nashville public library (the site is at [URL] Moreover, the legal freedom of information is well entrenched in the world of text. Copyright does not protect ideas, only the particular words used to express those ideas. Even with respect to expression, the rules of fair dealing and fair comment allow extensive quotation of the words of others, particularly in relation to debate about political and social issues. Again, the weblog phenomenon shows how this freedom has been expanded by the internet. Many postings on weblogs consist of reproductions of entire articles, interspersed with critical commentary, which ranges from sharp refutations to blustering abuse. Such postings are often referred to as “Fiskings”, after the British journalist Robert Fisk, a frequent target. The most sustained attempt to misuse intellectual property law has been that by the Church of Scientology which has repeatedly tried to use claims of copyright to silence critics who quote from church documents. The church has also used a variety of techniques aimed at reducing the visibility of its critics to search engines such as Google. Although the church has won some battles, it has lost the war. A Google search for “Scientology” now yields as many links to anti-Scientology sites as to the church‘s official sites. Moreover, the very attempt to suppress criticism has attracted the attention of many critics with no interest in the initial dispute. There has also been some collateral damage arising from attempts to protect copyright in such trivia as Mickey Mouse and Gone with the Wind. Many books that were never bestsellers, but would still attract a small group of readers, are out of print, but not out of copyright. Everyone, including the authors, would be better off if these books were in the public domain. The legal and administrative costs of securing the rights to republish such books are prohibitive. This is one reason that Project Gutenberg, an attempt to make public domain works accessible via the internet, has moved much of its activity to Australia where the copyright term remains the life of the author plus 50 years. The area where Lessig’s concerns carry most weight is that of software. Here the logic of technology works towards monopoly rather than competition. Although there is no general agreement among economists as to whether monopoly is conducive or hostile to innovation, experience in the information technology sector supports the pessimistic outlook. Microsoft 2 ̆ 7 s success in “embracing and extending” the work of others has produced huge profits, but offered little in the way of innovation. Even here, however, the appropriate remedy seems to lie in the sphere of antitrust and competition law rather than in the architecture of the internet. So the picture is decidedly mixed. The information commons is facing enclosure as regards music, video and software. But information in the ordinary sense of the term has never been as accessible or as easily published and republished. Lawrence Lessig’s The Future of Ideas: The Fate of the Commons in a Connected World is published by Random House Professor John Quiggin is an Australian Research Council Senior Fellow based at the Australian National University and Queensland University of Technology. His web site is at [URL] and his weblog is at [URL] This article originally appeared in the Australian Financial Review See also: The blog brings freedom to the net by Joanna Maret...|$|E

