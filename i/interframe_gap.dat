17|0|Public
50|$|Between {{successive}} frames {{a sequence}} of (at least) six primitives must be transmitted, sometimes called <b>interframe</b> <b>gap.</b>|$|E
5000|$|For Fibre Channel, {{there is}} a {{sequence}} of primitives between successive frames, sometimes called <b>interframe</b> <b>gap</b> as well. The minimum sequence consists of six primitives, [...] Each primitive consists of four channel words of 10 bits each for 8b/10b encoded variants (1-8 Gbit/s), equivalent to four data bytes.|$|E
50|$|Note {{that this}} example doesn't {{consider}} some additional Ethernet overhead, {{such as the}} <b>interframe</b> <b>gap</b> (a minimum of 96 bit times), or collisions (which have a variable impact, depending on the network load). TCP itself also adds the overhead of acknowledgements (which along with the round-trip delay time and the TCP window size in effect will rate-limit each individual TCP connection, see bandwidth-delay product). This example also does not consider the overhead of the HTTP protocol itself, which becomes relevant when transferring small files.|$|E
5000|$|Ethernet devices {{must allow}} a minimum idle period between {{transmission}} of Ethernet packets {{known as the}} interpacket gap (IPG), interframe spacing, or <b>interframe</b> <b>gap</b> (IFG). A brief recovery time between packets allows devices to prepare for reception of the next packet. While some physical layer variants literally transmit nothing during the idle period, most modern ones transmit a constant signal and send idle symbols. The standard minimum interpacket gap for transmission is 96 bit times (the {{time it takes to}} transmit 96 bits of raw data on the medium), which is ...|$|E
50|$|For example, {{the maximum}} frame size in Ethernet is 1526 bytes: up to 1500 bytes for the payload, eight bytes for the preamble, 14 bytes for the header, and four bytes for the trailer. An {{additional}} minimum <b>interframe</b> <b>gap</b> corresponding to 12 bytes is inserted after each frame. This {{corresponds to a}} maximum channel utilization of 1526 / (1526 + 12) × 100% = 99.22%, or a maximum channel use of 99.22 Mbit/s inclusive of Ethernet datalink layer protocol overhead in a 100 Mbit/s Ethernet connection. The maximum throughput or channel efficiency is then 1500 / (1526 + 12) = 97.5 Mbit/s, exclusive of the Ethernet protocol overhead.|$|E
5000|$|In the OSI {{model of}} {{computer}} networking, a frame is the {{protocol data unit}} at the data link layer. Frames {{are the result of}} the final layer of encapsulation before the data is transmitted over the physical layer. A frame is [...] "the unit of transmission in a link layer protocol, and consists of a link layer header followed by a packet." [...] Each frame is separated from the next by an <b>interframe</b> <b>gap.</b> A frame is a series of bits generally composed of framing bits, the packet payload, and a frame check sequence. Examples are Ethernet frames, Point-to-Point Protocol (PPP) frames, Fibre Channel frames, and V.42 modem frames.|$|E
50|$|Since {{the setup}} of HS {{transactions}} over an EfEx channel occurs between the HS transfers themselves, like the implementations of STANAG 3910 that {{wait for the}} preceding HS transfer to complete before initiating the next, the maximum bandwidth is necessarily less than 20 Mbps; though it is {{higher than that of}} this type of STANAG 3910 channel, because the HS control formats on the HS channel require less time than those on the LS channel. However, where a STANAG 3910 channel implementation performs the setup of an HS transfer in parallel with the preceding one, an implementation of STANAG 3910 could provide a very slightly higher throughput than an EfEX implementation, even allowing for the longest possible transmission of the HS message at the lowest possible data transmission rate. Also, assuming that the RTs met the requirements of the standard for a minimum 4 μs <b>interframe</b> <b>gap</b> time, this should have meant modifying only the BC to predict the end times of the HS messages, and initiate the HS control just before this; rather than modifying both the BC and multiple RTs to send and receive HS control formats on the HS channel.|$|E
50|$|The {{duration}} of an HS control format initiating an HS RT to HS RT transfer over the HS channel comprises {{a pair of}} 3838/1553B BC-RT transfers, including command words, data words (the HS action words themselves), LS status responses, LS RT response times, and an inter message gap (which is limited by, but {{is not necessarily the}} same as the 3838/1553B specified minimum intermessage gap of 4 μs). As a consequence, the {{duration of}} such a HS control format can be relatively long in comparison to the duration of the HS transfer that follows. This overhead is then compounded where the BC initiates an RT to BC transfer on the LS channel to, e.g., obtain the HS status word from the receiver. It is technically possible to begin the setup of the next HS transfer while the previous one is in progress, and thus achieve the minimum permitted HS <b>interframe</b> <b>gap</b> of 4 μs. However, it is common practice to wait for one HS transfer to end before beginning the LS channel transfers to set up the next, as predicting the timing of the end of a transmission is complicated by the possible variations in transmitter bit rates. Thus, while the theoretical throughput approaches 21 (20 + 1) Mbps, the actual throughput will be significantly less than 20 Mbps.|$|E
30|$|The {{measurements}} of throughput {{for a given}} frame length under different <b>interframe</b> <b>gap</b> are shown in Figure  10. It can be found that for a given frame length under a specified <b>interframe</b> <b>gap,</b> the throughput decreases clearly {{with the growth of}} <b>interframe</b> <b>gap.</b> It is obvious that the downlink throughput is never less than the upstream throughput. From the intersection between the downlink throughput and upstream throughput, the maximum throughput of the communication path can be determined. So, the accurate bottleneck bandwidth can be gained. The bottleneck bandwidth is 871, 910, and 931 Mbps when the data frame length is 256, 512 and 1, 024 bytes, respectively. Besides, the frame length under the specific throughput is approximately proportional to the <b>interframe</b> <b>gap,</b> which is drawn by the scope of the <b>interframe</b> <b>gap</b> under different frame length.|$|E
3000|$|... {{decreasing}} in turn. Therefore, it is theoretically {{proved that}} we can dynamically adjust the <b>interframe</b> <b>gap</b> to gradually approach to the maximum throughput, which enables us to implement the measurement of the bottleneck bandwidth.|$|E
30|$|For {{the data}} frame of {{different}} length under different <b>interframe</b> <b>gap,</b> it is sent for 20, 000 times. We {{compare it with}} the same one communication path for latency, throughput, and packet loss rate.|$|E
40|$|Abstract—Transmission timer {{framework}} is proposed for rate based pacing (RBP) TCP, {{in order to}} mitigate a burstiness in TCP slow-start. In this approach, host software specifies the time for each packet that should be sent out, and gives a precise <b>interframe</b> <b>gap</b> for the data stream. An RBP TCP implementation including a transmission timer aware network interface card is demonstrated. Also, a quantitative analysis of the burstiness in TCP slow-start is done with a burstiness index that we are proposed. I...|$|E
30|$|In this paper, an FPGA-based {{high-speed}} network performance measurement for RFC 2544 [11] is proposed. The active measurement method is employed {{to generate a}} lot of probe data frames set by the user. The passive measurement method is used to precisely count network traffic of each Ethernet interface and other related parameters through Register I/O in real time. According to the change of throughput, it dynamically adjusts the <b>interframe</b> <b>gap</b> to reach the limit of network performance. Our approach not only gets the latency that is accurate to the nanosecond but also can be applied in {{high-speed network}}.|$|E
30|$|In {{order to}} fully {{evaluate}} our approach, the network performance measurement system of our approach is compared with it using software to send probe data frames {{for the same}} one communication path. The software system operates at the host computer. It uses the libnet to cyclically encapsulate data frame according to the user’s configurations. Then, it forwards those data frames immediately. Meanwhile, in order to monitor the network traffic transmitted and received for each network interface, the above NetFPGA {{is also used to}} count the network traffic in the software system. Moreover, we do not wait for the <b>interframe</b> <b>gap</b> when sending those data frames so as to maximize the network performance, namely sending back-to-back data frames to reveal the network performance.|$|E
30|$|Aiming at {{the problem}} that {{existing}} network performance measurements have low accuracy for (Request for Comments) RFC 2544, this paper proposes a high-speed network performance measurement based on field-programmable gate array (FPGA). The active measurement method is used to generate probe data frames, and a passive measurement method is employed to count network traffic. According to the statistical laws based on throughput variation, interval stretching mechanism is used to dynamically adjust <b>interframe</b> <b>gap.</b> When our approach approaches the maximum throughput, the network performance parameters are achieved. A prototype based on NetFPGA is also implemented for evaluation. Experimental results show that our approach can be applied in high-speed network and the latency can be accurate to the nanosecond. Compared with network performance measurement using software to send probe data frames and a similar work based on FPGA, our approach can be more flexible and the evaluation data are more accurate.|$|E
40|$|Abstract—In this paper, {{we study}} time-triggered {{distributed}} systems where periodic application tasks are mapped onto different end stations (processing units) communicating over a switched Ethernet network. We {{address the problem}} of application level (i. e., both task- and network-level) schedule synthesis and optimization. In this context, most of the recent works [10], [11] either focus on communication schedule or consider a simplified task model. In this work, we formulate the co-synthesis problem of task and communication schedules as a Mixed Integer Programming (MIP) model taking into account a number of Ethernet-specific timing parameters such as <b>interframe</b> <b>gap,</b> precision and synchronization error. Our formulation is able to handle one or multiple timing objectives such as application response time, end-to-end delay and their combinations. We show the applicability of our formulation con-sidering an industrial size case study using a number of different sets of objectives. Further, we show that our formulation scales to systems with reasonably large size. I...|$|E
40|$|A single 802. 3 CatTier Sense Multiple Access/Collision Detection (CSMA/CD) {{segment is}} {{measured}} resulting in throughput, response times and workstation parameters for several network nodes. During the measurements, the network can'ied an artificial workload with {{the characteristics of}} a real-life workload. A simulation of the laboratory test is developed using the artificial workload parameters and the 802. 3 CSMA/CD standard of the Consultative Committee for International Telephony and Telegraphy (CCITT). The measurements show {{that it is possible to}} determine the workstation parameters with a great accuracy using simple throughput measurements on an otherwise empty network. It is then possible to isolate exact ethernet parameters during throughput measurements on a network with a known workload. The behaviour measured is reproduced in a simple simulation. The results of the simulation conform to the measured values. Some conclusions are that a 802. 3 CSMA/CD segment can be measured and simulated with accurate results. The simulation environment is used to model a real-life ethernet network in circumstances that can not be measured in a real-life situation. Parameters that can be used to fine-tune the simulation are the <b>interframe</b> <b>gap</b> time, and the workstation distance on a network...|$|E

