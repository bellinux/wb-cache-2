61|7|Public
5000|$|<b>Expressive</b> <b>timing</b> {{refers to}} the musical {{phenomenon}} whereby a performer introduces subtle temporal nuances to an otherwise metronomic ("perfectly" [...] timed) interpretation. This is {{also referred to as}} microtiming or microrhythm. For instance, a pianist might introduce a slight ritardando (not called for explicitly in the musical score) {{at the end of a}} phrase to convey a structural event (in this case, a phrase ending). <b>Expressive</b> <b>timing</b> has been shown to operate in different musical styles. In jazz, <b>expressive</b> <b>timing</b> plays an important role in how [...] "swing" [...] eighth-notes are timed.|$|E
50|$|Richard Beaudoin (born October 10, 1975) is an American {{composer}} {{of contemporary}} music. His music and writings explore compositional uses of <b>expressive</b> <b>timing,</b> or microtiming.|$|E
5000|$|Waadeland, C.H. (2001). [...] "It Don't Mean a Thing If It Ain't Got That Swing" [...] - Simulating <b>Expressive</b> <b>Timing</b> by Modulated Movements. Journal of New Music Research 30(1), pp. 23-37.|$|E
40|$|In {{this paper}} {{we present a}} {{preliminary}} version of the ACCompanion, an expressive accompaniment system for MIDI input. The system uses a probabilistic monophonic score follower to track {{the position of the}} soloist in the score, and a linear Gaussian model to compute tempo updates. The expressiveness of the system is powered by the Basis-Mixer, a state-of-the-art computational model of expressive music performance. The system allows for <b>expressive</b> dynamics, <b>timing</b> and articulation. Comment: Presented at the Late-Breaking Demo Session of the 18 th International Society for Music Information Retrieval Conference (ISMIR 2017), Suzhou, China, 201...|$|R
40|$|We {{investigate}} the <b>expressive</b> power of <b>timing</b> restrictions on labeled transition systems. In particular, we show how constraints on clock variables {{together with a}} uniform liveness condition [...] - the divergence of time [...] - can express Buchi, Muller, Streett, Rabin, and weak and strong fairness conditions on a given labeled transition system. We then consider the effect, on both timed and time-abstract expressiveness, of varying the following parameters: time domain (discrete or dense), number of clocks, number of states, and size of constants used in timing restrictions...|$|R
40|$|Abstract. Building {{animated}} conversational agents requires {{developing a}} fine-grained {{analysis of the}} motions and meanings available to interlocutors in face-to-face conversation and implementing strategies for using these motions and meanings to communicate effectively. In this paper, we sketch our efforts to characterize people’s facial displays of uncertainty in face-to-face conversation. We analyze empirical data from human–human conversation and extend our platform for conversational animation, including RUTH (the Rutgers University Talking Head), to simulate what we find. This methodology leads {{to a range of}} new insights into the structure, <b>timing,</b> <b>expressive</b> content and communicative function of facial actions...|$|R
5000|$|In this way, with {{suitable}} metronome techniques, {{use of a}} metronome {{helps you}} to improve your sense of time and exact timing without causing any of the expected issues for musicality and <b>expressive</b> <b>timing.</b> The thing {{to bear in mind}} all the way through is that you use the metronome to help with exact timing - but that the sense of rhythm and musically <b>expressive</b> <b>timing</b> is something that comes from yourself. Rhythm is natural to human beings and pervades our lives, though you may need help to bring that rhythm into music. As Andrew Lewis says in his book: ...|$|E
5000|$|Timing {{in music}} {{refers to the}} ability to [...] "keep time" [...] {{accurately}} and to synchronise to an ensemble, as well as to <b>expressive</b> <b>timing</b> - subtle adjustment of note or beat duration, or of tempo, for aesthetic effect.|$|E
50|$|Beaudoin's {{most widely}} {{performed}} works are {{those related to}} <b>expressive</b> <b>timing,</b> or microtiming. This process, which he developed in 2009 with the Swiss musicologist Olivier Senn, is based on millisecond-level microtemporal analyses of recorded performances. The timing measurements of every sound in a given recording are {{used to create a}} detailed transcription of the recording in musical notation, often in elongation.|$|E
40|$|Abstract. Building {{animated}} conversational agents requires {{developing a}} fine-grained {{analysis of the}} motions and meanings available to interlocutors in face-to-face conversation and implementing strategies for using these motions and meanings to communicate effectively. In this paper, we describe our research on signaling uncertainty on an animated face as an end-to-end case study of this process. We sketch our efforts to characterize people’s facial displays of uncertainty in face-to-face conversation in ways {{that allow us to}} simulate those behaviors in an animated agent. Our work has led to new insights into the structure, <b>timing,</b> <b>expressive</b> content and communicative function of facial actions that we must take into account to explain our empirical findings and to build agents that reproduce people’s effective use of the face in managing the dynamics of conversation. ...|$|R
40|$|Modelling {{expressive}} performance using consistent evolutionary regression trees Abstract. We {{present an}} evolutionary approach for building regression tree based {{models in the}} context of expressive music performance. We first review the benefits of using evolutionary computation techniques in this context. We then use the strongly-typed genetic programming framework and define the types and constraints that are needed for evolving efficiently multi-dimensional regression trees, and present two fitness functions for modelling <b>expressive</b> performance local <b>timing.</b> While the first fitness measurement is purely error-driven, the second also takes into account the balance of the evolved tree in terms of input space representation. Finally, we present the results of both learning and generalization experiments. For these experiments, we use a database of saxophone performance timing extracted from a set of acoustic recordings of jazz standards. The whole system is built into the Open Beagle [5] evolutionary computation framework. ...|$|R
40|$|This chapter explores current use of {{automated}} feedback techniques among musicians, and {{the anticipated}} usefulness of such systems. It examines purpose-designed software available for performers, and then summarizes {{the results of}} experimental investigations {{of the effectiveness of}} feedback systems in enhancing practicing to perform musical excerpts in various expressive manners. The methodological challenges of designing a program that can be applied in a general manner without biasing practice and performance are discussed. Promising avenues are suggested - for example, by making feedback summative rather than real-time, and based on probabilistic learning from target examples. In addition, the training may be to widely explore performance expression rather than to reinforce through imitation. The survey discussed indicates that if a user-friendly, reliable, and non-biasing product is realized, it is very likely to be adopted for a multitude of reasons, including feedback on ensemble <b>timing,</b> <b>expressive</b> interpretation, and aspects of performance control...|$|R
40|$|Though many past {{works have}} tried to cluster <b>expressive</b> <b>timing</b> within a phrase, {{there have been few}} {{attempts}} to cluster features of <b>expressive</b> <b>timing</b> with constant dimensions regardless of phrase lengths. For example, used as a way to represent <b>expressive</b> <b>timing,</b> tempo curves can be regressed by a polynomial function such that the number of regressed polynomial coefficients remains constant with a given order regardless of phrase lengths. In this paper, clustering the regressed polynomial coefficients is proposed for <b>expressive</b> <b>timing</b> analysis. A model selection test is presented to compare Gaussian Mixture Models (GMMs) fitting regressed polynomial coefficients and fitting <b>expressive</b> <b>timing</b> directly. As there are no expected results of clustering <b>expressive</b> <b>timing,</b> the proposed method is demonstrated by how well the <b>expressive</b> <b>timing</b> are approximated by the centroids of GMMs. The results show that GMMs fitting the regressed polynomial coefficients outperform GMMs fitting <b>expressive</b> <b>timing</b> directly. This conclusion suggests {{that it is possible to}} use regressed polynomial coefficients to represent <b>expressive</b> <b>timing</b> within a phrase and cluster <b>expressive</b> <b>timing</b> within phrases of different lengths...|$|E
40|$|We model <b>expressive</b> <b>timing</b> for {{a phrase}} in {{performed}} classical music as being dependent on two factors: the <b>expressive</b> <b>timing</b> {{in the previous}} phrase and {{the position of the}} phrase within the piece. We present a model selection test for evaluating candidate models that assert different dependencies for deciding the Cluster of <b>Expressive</b> <b>Timing</b> (CET) for a phrase. We use cross entropy and Kullback Leibler (KL) divergence to evaluate the resulting models: with these criteria we find that both the <b>expressive</b> <b>timing</b> in the previous phrase and the position of the phrase in the music score affect <b>expressive</b> <b>timing</b> in a phrase. The results show that the <b>expressive</b> <b>timing</b> in the previous phrase has a greater effect on timing choices than the position of the phrase, as the phrase position only impacts the choice of <b>expressive</b> <b>timing</b> in combination with the choice of <b>expressive</b> <b>timing</b> in the previous phrase...|$|E
40|$|This {{study is}} {{concerned}} with the question whether there is perceptual invariance of <b>expressive</b> <b>timing</b> under tempo-transformation in audio recordings. This was investigated by asking listeners to distinguish between an original recording and a time-stretched (i. e. tempotransformed) version. The original recordings were identified by a significant proportion of the participants. The results suggest that <b>expressive</b> <b>timing</b> can function as a clue in identifying a real performance. This is taken as evidence for the tempo-specific timing hypothesis, and counter evidence for the relational invariance hypothesis that predicts proportionally scaled <b>expressive</b> <b>timing</b> to be perceived natural as well. The results are discussed in the context of whether there is perceptual invariance of <b>expressive</b> <b>timing</b> under tempo transformation and possible improvements to state-of-the-art time-stretching algorithms. 1...|$|E
40|$|Whatever we do, {{we do it}} in our own way, and we {{recognise}} master artists {{by small}} samples of their work. This study investigates individuality of temporal deviations in musical scales in pianists in the absence of deliberate <b>expressive</b> intention. Note-by-note <b>timing</b> deviations away from regularity form a remarkably consistent "pianistic fingerprint". First, 8 professional pianists played C-major scales in two sessions, separated by fifteen minutes. Euclidian distances between deviation traces originating from different pianists were reliably larger than traces originating from the same pianist. As a result, a simple classifier that matched deviation traces by minimising their distance was able to recognise each pianist with 100 % accuracy. Furthermore, within each pianist, fingerprints produced by the same movements were more similar than fingerprints resulting in the same scale sound. This allowed us to conclude that the fingerprints are mostly neuromuscular rather than intentional or expressive in nature. However, human listeners were not able to distinguish the temporal fingerprints by ear. Next, 18 pianists played C-major scales on a normal or muted piano. Recognition rates ranged from 83 % to 100 %, further supporting the view that auditory feedback is not implicated {{in the creation of the}} temporal signature. Finally, 20 pianists were recognised 20 months later at above-chance level, showing signature effects to be long lasting. Our results indicate that even non-expressive playing of scales reveals consistent, partially effector-unspecific, but inaudible inter-individual differences. We suggest that machine learning studies into individuality in performance will need to take into account unintentional but consistent variability below the perceptual threshold...|$|R
40|$|We {{examined}} {{the effect of}} expressive intent on timing and movement in clarinet performance. Clarinetists ’ performances were recorded with motion capture while they performed with three expressive intents: expressive (normal) performance, exaggerated performance, and inexpressive performance. Acoustic measures (intensity, pitch height, duration) were compared with ancillary gestures (bell movement). The more expressive performances contained larger <b>expressive</b> <b>timing</b> measures and bell movement. Clarinetists marked phrase boundaries with increases in both <b>expressive</b> <b>timing</b> and clarinet motion. Neither pitch height nor sound intensity accounted for bell movements beyond <b>expressive</b> <b>timing.</b> These findings suggest that ancillary bell gestures are rule-governed and correlate with some acoustic features of musical expression...|$|E
40|$|I www. hum. uva. nl/mmm/hh/ Perceptual {{invariance}} of <b>expressive</b> <b>timing</b> 2 / 22 In several {{domains of}} cognition perceptual invariance {{has been studied}} and found, including the domains of speech, motor behavior, and object motion. In music perception, too, {{it has been the}} topic of several studies. However, with regard to the perceptual invariance of <b>expressive</b> <b>timing</b> under tempo transformation in music performance the existing perceptual studies present rather inconclusive evidence. This empirical study addresses the issue using commercially available recordings and an online internet experimental design [...] The results show that listeners can decide on what is a real performance when asked two compare two recordings one of which is tempo-transformed to make them similar in overall tempo. This result is taken as support for the tempo-specific timing hypothesis: <b>expressive</b> <b>timing</b> can function as a perceptual clue in identifying an original performance, and counter-evidence for the relational invariance hypothesis that predicts a tempo-transformed performance to sound equally natural. Perceptual invariance of <b>expressive</b> <b>timing</b> 3 / 2...|$|E
40|$|<b>Expressive</b> <b>timing,</b> through {{temporal}} {{coordination and}} expressivity, {{is an essential}} constituent of the communication between a conductor and ensemble musicians, {{and it has been}} studied both from a conductor’s perspective (e. g. in terms of physical parameters of conducting gestures) and from an observer’s perspective (e. g. in terms of evaluation of communicated musical expressivity). However, more insight is needed into the role of different factors that influence the conductor’s communication of <b>expressive</b> <b>timing.</b> In the current study, we investigated four factors that influence the sensitivity to the <b>expressive</b> <b>timing</b> communicated by a conductor; namely the experience in performing under the lead of a conductor, the ability to move along with the conductor, the role of visual and auditory modalities, and the tempo of <b>expressive</b> <b>timing.</b> We applied a repeated measures sensorimotor paradigm, based on a beat synchronization task. Participants (non-musicians, musicians, and musicians experienced in performing under the direction of a conductor) were asked to identify a musical beat by pressing a button while (1) listening to a musical piece (auditory condition), (2) observing the conductor directing the musical piece (visual condition), and (3) listening to the music and looking at the conductor’s gestures directing the music (audio-visual condition). Performing the task, participants were asked to either (1) move their body along with the music or the conductor (‘movement’ condition), or (2) stay still (‘no movement’ condition). The results reveal that conductor’s communication of <b>expressive</b> <b>timing</b> depends on an interconnection of experiences, tempi, and modalities of the beat. The ability to move along with the conductor did not significantly improve the participants’ <b>expressive</b> <b>timing</b> performance, which requires further research...|$|E
40|$|The {{organization}} of sound into meaningful units {{is fundamental to}} the processing of auditory information such as speech and music. In expressive music performance, structural units or phrases may become particularly distinguishable through subtle timing variations highlighting musical phrase boundaries. As such, <b>expressive</b> <b>timing</b> may support the successful parsing of otherwise continuous musical material. By means of the event-related potential technique (ERP), we investigated whether <b>expressive</b> <b>timing</b> modulates the neural processing of musical phrases. Musicians and laymen listened to short atonal scale-like melodies that were presented either isochronously (deadpan) or with <b>expressive</b> <b>timing</b> cues emphasizing the melodies ’ two-phrase structure. Melodies were presented in an active and a passive condition. <b>Expressive</b> <b>timing</b> facilitated the processing of phrase boundaries as indicated by decreased N 2 b amplitude and enhanced P 3 a amplitude for target phrase boundaries and larger P 2 amplitude for non-target boundaries. When timing cues were lacking, task demands increased especially for laymen as reflected by reduced P 3 a amplitude. In line, the N 2 b occurred earlier for musicians in both conditions indicating general faster target detection compared to laymen. Importantly, the elicitation of a P 3 a-like response to phrase boundaries marked by a pitch leap during passive exposure suggests that <b>expressive</b> <b>timing</b> information is automatically encoded and may lead to an involuntary allocation of attention towards significant events within a melody. We conclude that subtle timing variations in music performance prepare the listener for musical key events by directing an...|$|E
40|$|<b>Expressive</b> <b>timing</b> {{is vital}} for the {{aesthetic}} quality that makes us appreciate performed music. It is a largely tacit skill that musicians acquire by practice. A long-standing intuition is that <b>expressive</b> <b>timing</b> {{is closely related to}} the concept of motion. This view leads naturally to the adoption of a dynamical systems approach to the study of <b>expressive</b> <b>timing.</b> A well-known visualization technique from dynamical systems theory is the phase-plane representation. The application of this technique, that highlights the dynamic aspects of the data, is demonstrated in a case study on the final ritard in performances of Schumann’s Träumerei. We argue that expressive gestures are visible in a clear and intuitive manner in the phase-plane representations. Another striking aspect of the phase-plane trajectories is their suggestion of human gestural motion. I. INTRODUCTION AND RELATED WOR...|$|E
40|$|This {{study is}} {{concerned}} with the question whether, and to what extent, listeners ’ previous exposure to music in everyday life, and expertise as a result of formal musical training, play a role in making <b>expressive</b> <b>timing</b> judgments in music. This was investigated by using a Web-based listening experiment in which listeners {{with a wide range of}} musical backgrounds were asked to compare 2 recordings of the same composition (15 pairs, grouped in 3 musical genres), 1 of which was tempo-transformed (manip-ulating the <b>expressive</b> <b>timing).</b> The results show that <b>expressive</b> <b>timing</b> judgments are not so much influenced by expertise levels, as is suggested by the expertise hypothesis, but by exposure to a certain musical idiom, as is suggested by the exposure hypothesis. As such, the current study provides evidence for the idea that some musical capabilities are acquired through mere exposure to music, and that these abilities are more likely enhanced by active listening (exposure) than by formal musical training (expertise) ...|$|E
40|$|This paper {{addresses}} {{the problem of}} determining tempo and timing data {{from a list of}} beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing variations due to expressively timed events, phase shifts and errors in the annotation times. These deviations tend to propagate into the tempo graph and so tempo analysis methods tend to average over recent inter-beat intervals. However, whilst this minimises the effect such timing deviations have on the local tempo estimate, it also obscures the <b>expressive</b> <b>timing</b> devices used by the performer. Here we propose a more formal method for calculation of the optimal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and <b>expressive</b> <b>timing.</b> 1...|$|E
40|$|An {{approach}} of parsing piano music interpretation is presented. We focus mainly on quantifying <b>expressive</b> <b>timing</b> activities. A {{small number of}} different <b>expressive</b> <b>timing</b> behaviors (constant, slowing down, speeding up, accent) are defined in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent {{the evolution of the}} discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely configuration of the discrete behaviors and the hidden continuous variable tempo. This configuration represent a “smoothed ” version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method. 1...|$|E
40|$|This paper {{presents}} a calculus that enables <b>expressive</b> <b>timing</b> {{to be transformed}} {{on the basis of}} the structural aspects of the music. Expression within a unit is defined as the deviations of its parts with respect to the norm set by the unit itself. The behaviour of musical material under expressive transformations is determined uniquely by its structural description and the type of expression. Although the calculus separates different kinds of behaviour, it entails no musical knowledge of the transformations themselves and it also does not model music cognition. The algorithmic simplicity of the calculus combined with its elaborate knowledge representation mirrors the common hypothesis that the complex <b>expressive</b> <b>timing</b> profiles found in musical performances can be explained as the product of a small collection of simple rules linked to a relatively complex structure. The calculus will hopefully prove to be a solid basis for formalised theories of music cognition...|$|E
40|$|We {{formulate}} tempo tracking in a Bayesian framework where a tempo tracker {{is modeled}} as a stochastic dynamical system. The tempo is modeled as a hidden state variable {{of the system}} and is estimated from a MIDI performance by Kalman filtering and smoothing. We also introduce the Tempogram representation, a wavelet-like multiscale expansion of a real performance, on which the Kalman filter operates. 1 Introduction An important and interesting subtask in automatic music transcription is tempo tracking: how to follow the tempo in a performance that contains <b>expressive</b> <b>timing</b> and tempo variations. When these tempo fluctuations are correctly identified it becomes much easier to separate the continuous <b>expressive</b> <b>timing</b> from the discrete note categories (i. e. quantization). The sense of tempo seems to be carried by the beats and thus tempo tracking is related to the study of beat induction, the perception of beats or pulse while listening to music (see Desain and Honing (1994)). However, [...] ...|$|E
40|$|We {{propose a}} new method for rhythm {{quantisation}} and measurement of <b>expressive</b> <b>timing.</b> This paper {{focuses on the}} automatic quantisation and rhythmic transcription of syncopated rhythms and baroque ornaments, e. g. appogiaturas, mordants and trills from time-tagged audio recordings without knowing the score in advance. We demonstrate the transcription of the Aria of J. S. Bach’s Goldberg Variations, BWV 988, recorded by Glenn Gould in 195...|$|E
40|$|This article reviews {{a family}} of {{computational}} models (e. g., Sundberg and Verillo 1980; Feldman, Epstein, and Richards 1992; Todd 1992; Friberg and Sundberg 1999) that do make the relation between motion and music explicit and therefore can be tested and validated on real performance data. These kinematic models attempt to predict the timing patterns found in musical performances (generally referred to as <b>expressive</b> <b>timing).</b> Most of these studies focus on modeling the final ritard: the typical slowing {{down at the end}} of a music performance, especially in music from the Western Baroque and Romantic periods. But this characteristic slowing down can also be found in, for instance, Javanese gamelan music or some pop and jazz genres. In this kinematic approach, one looks for an explanation in terms of the rules of mechanics: that is, how <b>expressive</b> <b>timing</b> might relate to, or can be explained by, models of physical motion that deal with force, mass, and movement. A discussion of these kinematic models is presented below in the form of a story (see Figure 1), with three fictitious characters who represent the different disciplines involved in this research (psychology, mathematics, and musicology). The story is a continuation of Desain and Honing (1993; see also [URL] for additional sound examples), an article that dealt with the state of the art in <b>expressive</b> <b>timing</b> research some ten years ago. In addition, it brought forward a critique on the usefulness of the tempo curve (a continuous function of time or score position) as the underlying representation of several computational models (including most computer music software at that period). The main point of critique was that the predictions made by models using this representation are insensitive to [...] ...|$|E
40|$|Automatically {{following}} rhythms by beat tracking is by {{no means}} a solved problem, especially when dealing with varying tempo and <b>expressive</b> <b>timing.</b> This paper presents a connectionist machine learning approach to expressive rhythm prediction, based on cognitive and neurological models. We detail a multi-layered recurrent neural network combining two complementary network models as hidden layers within one system. The first layer is a Gradient Frequency Neural Network (GFNN), a network of nonlinear oscillators which acts as an entraining and learning resonant filter to an audio signal. The GFNN resonances are used as inputs to a second layer, a Long Short-term Memory Recurrent Neural Network (LSTM). The LSTM learns the long-term temporal structures present in the GFNN's output, the metrical structure implicit within it. From these inferences, the LSTM predicts when the next rhythmic event is likely to occur. We train the system on a dataset selected for its <b>expressive</b> <b>timing</b> qualities and evaluate the system on its ability to predict rhythmic events. We show that our GFNN-LSTM model performs as well as state-of-the art beat trackers and has the potential to be used in real-time interactive systems, following and generating expressive rhythmic structures...|$|E
40|$|A column (the {{second of}} a series of three) {{constitutes}} an abridged and adapted version of Tempo curves considered harmful. M (an amateur mathematician) and P (a would-be psychologist) incorporated some generative models for <b>expressive</b> <b>timing</b> in their sequencer program. This proved partially succesful but it missed the point of applying sensible transformations to an already recorded performance (of the theme from the six variations over the duet Nel cor piò non mi sento by Ludwig van Beethoven). Disappointed with the results, they retreated in the library...|$|E
40|$|International audienceThis {{longitudinal}} study compared the temporal characteristics of maternal singing at 3 {{and then at}} 6 months. Infant-directed (ID) singing is claimed to have different functions in preverbal communication. However few {{studies have focused on}} the specific characteristics of ID singing that change across the first months of life. We aimed to explore these changes between 3 and 6 months because musical routines become prominent in the repertoire of games parents and infants spontaneously play during a period referred to as 'the period of games'. We focused specifically on <b>expressive</b> <b>timing</b> because it reflects how mothers dynamically adapt their singing to their infant's states of attention and involvement. We aimed to determine whether the <b>expressive</b> <b>timing</b> cues of maternal singing would be different at 3 and then at 6 months. To this end, the interactions of 18 mother-infant dyads were recorded while mothers were singing a popular French playsong for their infant at 3 and then at 6 months. Acoustic analyses revealed that mothers showed final-lengthening and tempo slowing for both age groups, but marked the ends of the hierarchical structural units of the song more saliently with their 6 -month-olds. Unexpectedly, infant sex was also found to affect maternal singing: more exaggerated phrase-lengthening patterns were observed in singing to girls...|$|E
40|$|Background {{in music}} {{performance}} research For {{the past few}} decades there has been considerable scientific interest in expression in music performance (Gabrielsson, 2003). A particularly relevant aspect of music performance is <b>expressive</b> <b>timing,</b> that is, the fluctuations of tempo during a performance. Accordingly, <b>expressive</b> <b>timing</b> {{has been one of the}} major topics in music performance research. As an expressive parameter, timing is used to clarify the musical structure of the piece (Clarke, 1988), among other things. Background in computing, mathematics, and statistics A common situation in the natural sciences is the limited observability of complex systems, e. g. the weather. The observed variables typically exhibit little regularity due to the influence of unknown and interacting underlying factors. A helpful method in the study of such systems is the graphical presentation of typical trajectories through its phase-space, or state-space, the space of all the possible states the system can be in. The system state is represented by state variables. The complexity of the system is intrinsically related to the number of state variables that are necessary to completely describe its behaviour. The phase-plane is a two-dimensional plot of two state variables against each other. Such plots can give insights in the temporal behaviour of the system, and sometimes allow for a qualitativ...|$|E
40|$|Beat {{induction}} is {{the perceptual}} and cognitive {{process by which}} humans listen to music and perceive a steady pulse. Computationally modelling beat induction is important for many Music Information Retrieval (MIR) methods and is in general an open problem, especially when processing <b>expressive</b> <b>timing,</b> e. g. tempo changes or rubato. A neuro-cognitive model has been proposed, the Gradient Frequency Neural Network (GFNN), which can model the perception of pulse and metre. GFNNs have been applied successfully {{to a range of}} ‘difficult’ music perception problems such as polyrhythms and syncopation. This thesis explores the use of GFNNs for expressive rhythm perception and modelling, addressing the current gap in knowledge for how to deal with varying tempo and <b>expressive</b> <b>timing</b> in automated and interactive music systems. The cannonical oscillators contained in a GFNN have entrainment properties, allowing phase shifts and resulting in changes to the observed frequencies. This makes them good candidates for solving the <b>expressive</b> <b>timing</b> problem. It is found that modelling a metrical perception with GFNNs can improve a machine learning music model. However, it is also discovered that GFNNs perform poorly when dealing with tempo changes in the stimulus. Therefore, a novel Adaptive Frequency Neural Network (AFNN) is introduced; extending the GFNN with a Hebbian learning rule on oscillator frequencies. Two new adaptive behaviours (attraction and elasticity) increase entrainment in the oscillators, and increase the computational efficiency of the model by allowing for a great reduction {{in the size of the}} network. The AFNN is evaluated over a series of experiments on sets of symbolic and audio rhythms both from the literature and created specifically for this research. Where previous work with GFNNs has focused on frequency and amplitude responses, this thesis considers phase information as critical for pulse perception. Evaluating the time-based output, it was found that AFNNs behave differently to GFNNs: responses to symbolic stimuli with both steady and varying pulses are significantly improved, and on audio data the AFNNs performance matches the GFNN, despite its lower density. The thesis argues that AFNNs could replace the linear filtering methods commonly used in beat tracking and tempo estimation systems, and lead to more accurate methods...|$|E
40|$|Motivated by {{previous}} work aimed at developing mathematical models to describe <b>expressive</b> <b>timing</b> in music, and specifically the final ritardandi, using measured kinematic data, we further investigate the linkage of locomotion and timing in music. The natural running behavior of four subjects is {{measured with a}} wearable sensor prototype and analyzed to create normalized tempo curves. The resulting curves are then used to modulate the final ritard of MIDI scores, which are also performed by an expert musician. A Turing-inspired listening test is conducted to observe a human listener’s ability to determine {{the nature of the}} performer...|$|E
40|$|This paper proposes novel {{computer-based}} interfaces for piano practicing. They {{are designed}} to display in real time certain well-defined sub-aspects of piano playing. They are intelligent and unobtrusive in that they adjust automatically {{to the needs of}} the practitioner so that no other interaction is needed than moving the piano keys. They include 1) a pattern display, finding recurring pitch patterns and displaying <b>expressive</b> <b>timing</b> and dynamics thereof, 2) a chord display, showing timing asynchronies and tone intensity variations of chords tones, and 3) an acoustic piano roll display that visually models the acoustic piano tone from MIDI data...|$|E
40|$|Phrasing is {{a primary}} concern for {{performers}} {{in the process of}} interpretation, because its structure is associated with the music’s formal designs; many empirical researchers have therefore considered the relationship between timing and dynamic in performance and phrase structure (see Todd 1992). Performers’ tendency towards dynamic modification in phrase boundary is most often discussed in relation to timing fluctuation in performance (e. g. Todd 1992; Dunsby 1995). For instance, Todd (1992) creates the algorithmic model of tempo and dynamics through a series of filters. He calls the relationship between <b>expressive</b> <b>timing</b> and dynamic the ‘motor action’. Previous empirical studies using Todd’s model of performance (1992) includ...|$|E
