0|10000|Public
40|$|System requirements, {{software}} elements, {{and hardware}} equipment required for an IPAD system are defined. An IPAD conceptual design was evolved, a potential user survey was conducted, and work loads for {{various types of}} interactive terminals were projected. Various features of major <b>host</b> <b>computing</b> <b>systems</b> were compared, and target systems were selected {{in order to identify}} the various elements of software required...|$|R
40|$|The <b>computing</b> <b>system</b> {{design of}} IPAD is {{described}} and the requirements which {{form the basis}} for the system design are discussed. The system is presented in terms of a functional design description and technical design specifications. The functional design specifications give the detailed description of the system design using top-down structured programming methodology. Human behavioral characteristics, which specify the system design at the user interface, security considerations, and standards for system design, implementation, and maintenance are also part of the technical design specifications. Detailed specifications of the two most common <b>computing</b> <b>system</b> types in use by the major aerospace companies which could support the IPAD system design are presented. The report of a study to investigate migration of IPAD software between the two candidate 3 rd generation <b>host</b> <b>computing</b> <b>systems</b> and from these systems to a 4 th generation system is included...|$|R
40|$|The {{scaling of}} the {{computing}} infrastructure in distribution and heterogeneity is correlated to increasing complexity in navigation and {{management at the}} execution environment level. In 2 K, which is a user-centric, distributed and adaptable operating system, computation occurs in environments that are customizable to the user requirements and to {{the characteristics of the}} <b>host</b> <b>computing</b> <b>systems.</b> A 2 K environment is a container of components, devices and conguration parameters and provides an execution context for users within the distributed system. The management of 2 K execution environments contributes to the user-centrism of the system, by aggregating all the computing contexts of a user in a composite structure, accessible anywhere in the scope of 2 K. The structures of composite environments interact within themselves and with one another, and are correlated since actions in certain environments imply actions in others. In this paper, we describe event management in aggregated st [...] ...|$|R
40|$|Computer {{users are}} {{increasingly}} multi-device, and {{are no longer}} sedentary. <b>Computing</b> <b>systems</b> have evolved from single-user and time-shared <b>computing</b> to complex <b>systems</b> spanning distributed heterogeneous hardware and software platforms. Applications require not only a set of computing and communication resources but also sustainable levels of quality of service. It is desirable that computation occur in environments that are at once customizable to the user preferences, to {{the characteristics of the}} <b>host</b> <b>computing</b> <b>systems,</b> and to the applications service requirements. This paper describes a framework for managing execution environments in 2 K, an adaptable, distributed, network-centric, user- and application-oriented operating system aimed at accommodating change. A 2 K environment is a container of components, devices and con guration parameters and provides an execution context for users within the 2 K distributed system. A user has an execution environment that consists of several subenvironments running on different platforms, possibly suspended to be resumed later. The management of the execution environments is designed to provide a user-centric view of the system; it facilitates user mobility, by liberating users from the restriction of being explicitly attached to specific platforms, and by seamlessly recruiting resources where they are available. Mechanisms for distribution and interaction of environments are provided by event management in aggregated structures, and protocols for update and consistency of replicated structures...|$|R
40|$|MobiRing is a compact, finger-worn {{wireless}} motion-tracking {{platform that}} can be reprogrammed {{for a wide variety}} of features to augmented mobile computing. First, it works as a wireless mouse in the air without requiring a surface for a mouse or a trackpad, making it suitable for augmented-reality or virtual-reality systems. Second, it supports collection of stroke-in-the-air data {{that can be}} interpreted as gestures for commands, text input, or raw strokes. An advanced version can combine input from additional units on multiple fingers for richer possibilities. Third, the ability to quickly switch pairing among multiple <b>host</b> <b>computing</b> <b>systems</b> enables MobiRing to support drag-and-drop-like operations across systems such as PCs, smartphones, tablets, display walls, and even displayless embedded systems, where a wired connection may be inconvenient if not infeasible. MobiRing has been prototyped with an ultra-compact wireless sensing platform with an on-board triaxial accelerometer and a short-range wireless transceiver with an optional second accelerometer for enhanced accuracy. The prototype has been evaluated quanti-tatively for accuracy and qualitatively for usability by asking ten volunteer novice users to try a variety of the proposed operations. The results show that the MobiRing system to be intuitive for mouse-like functions and the recognition accuracy of each gesture is over 90 %. I...|$|R
40|$|This paper {{presents}} a <b>computing</b> <b>hosting</b> <b>system</b> to provide virtual computing laboratories for learning activities. This system {{is based on}} hosting and virtualization technologies. All the components used in its development are free software tools. The computing lab model provided by the system is a more sustainable and scalable alternative than the traditional academic computing lab, and it requires lower costs of installation and operation...|$|R
50|$|A {{relatively}} new method of centralized <b>computing,</b> <b>hosted</b> <b>computing,</b> solves {{many of the}} problems associated with traditional distributed <b>computing</b> <b>systems.</b> By centralizing processing and storage on powerful server hardware located in a data center, rather than in a local office, it relieves organizations of the many responsibilities in owning and maintaining an information technology system. These services are typically delivered on a subscription basis by an application service provider (ASP).|$|R
40|$|Over {{the last}} decade, Internet-based services, such as electronic-mail, music-on-demand, and social-network services, {{have changed the}} ways we {{communicate}} and access information. Usually, the key functionality of such a service is in backend components, which are located in a data center, a facility for <b>hosting</b> <b>computing</b> <b>systems</b> and related equipment. This thesis focuses on two fundamental problems related to the management, dimensioning, and provisioning of such backend components. The first problem centers around resource allocation for a large-scale cloud environment. Data centers have become very large; they often contain {{hundreds of thousands of}} machines and applications. In such a data center, resource allocation cannot be efficiently achieved through a traditional management system that is centralized in nature. Therefore, a more scalable solution is needed. To address this problem, we have developed and evaluated a scalable and generic protocol for resource allocation. The protocol is generic {{in the sense that it}} can be instantiated for different management objectives through objective functions. The protocol jointly allocates CPU, memory, and network resources to applications that are hosted by the cloud. We prove that the protocol converges to a solution, if an objective function satisfies a certain property. We perform a simulation study of the protocol for realistic scenarios. Simulation results suggest that the quality of the allocation is independent of the system size, up to 100, 000 machines and applications, for the management objectives considered. The second problem is related to performance modeling of a distributed key-value store. The specific distributed key-value store we focus on in this thesis is the Spotify storage system. Understanding the performance of the Spotify storage system is essential for achieving a key quality of service objective, namely that the playback latency of a song is sufficiently low. To address this problem, we have developed and evaluated models for predicting the performance of a distributed key-value store for a lightly loaded system. First, we developed a model that allows us to predict the response time distribution of requests. Second, we modeled the capacity of the distributed key-value store for two different object allocation policies. We evaluate the models by comparing model predictions with measurements from two different environments: our lab testbed and a Spotify operational environment. We found that the models are accurate in the sense that the prediction error, i. e., the difference between the model predictions and the measurements from the real systems, is at most 11 %. QC 20131001 </p...|$|R
40|$|High Speed {{computing}} meets {{ever increasing}} real-time computational demands through the leveraging of flexibility and parallelism. The flexibility is achieved when computing platform designed with heterogeneous resources to support multifarious tasks of an application where as task scheduling brings parallel processing. The {{efficient task scheduling}} is critical to obtain optimized performance in heterogeneous <b>computing</b> <b>Systems</b> (HCS). In this paper, we brought a review of various application scheduling models which provide parallelism for homogeneous and heterogeneous <b>computing</b> <b>systems.</b> In this paper, we made a review of various scheduling methodologies targeted to high speed <b>computing</b> <b>systems</b> and also prepared summary chart. The comparative study of scheduling methodologies for high speed <b>computing</b> <b>systems</b> {{has been carried out}} based on the attributes of platform & application as well. The attributes are execution time, nature of task, task handling capability, type of <b>host</b> & <b>computing</b> platform. Finally a summary chart has been prepared and it demonstrates that the need of developing scheduling methodologies for Heterogeneous Reconfigurable <b>Computing</b> <b>Systems</b> (HRCS) which is an emerging high speed computing platform for real time applications. Comment: 12 page...|$|R
40|$|To {{engineers}} {{involved in}} designing, developing, or operating control systems for practical applications, research in control may seem {{an exercise in}} mathematical abstractions. As an engineering discipline, however, the connection with the physical world is intrinsic to control. Interfacing with sensors and actuators, implementing advanced algorithms on real-time platforms, dealing with sampling time issues, and other such pragmatic matters {{may seem to be}} taken for granted in much of advanced control, but in fact there is an extensive body of research that is concerned with these very topics All advanced control algorithms today are <b>hosted</b> on digital <b>computing</b> <b>systems,</b> and any discussion of real-time control applications must address the specific issues and challenges associated with digital implementation. The benefits of digital realization are numerous: software-based computing allows more sophisticated control laws; updates and maintenance are rendered easier in many cases; control systems can be made more compact; and control can more readily be integrated with ancillary functions such as information display, system health management, and data recording. But the digital world brings complications too. For example, the continuous variables of a physical system must now be discretely sampled. Variations in the sampling rate are generally assumed to be negligible, but this is not always the case and a significant adverse impact on control quality can result...|$|R
40|$|The {{optimization}} of <b>computing</b> <b>systems</b> <b>hosted</b> on Boolean-circuit-based <b>computing</b> equipment must {{be expressed}} {{at some level}} in Boolean behaviors and operations. Boolean behaviors and operations {{are part of a}} larger family of logics [...] the logic of sentences, also known as the "sentential calculus". Two logics are implicationally equivalent if the axioms and inference rules of each imply the axioms of the other. Characterizing the inferential equivalences of various formulations of the sentential calculi is thus foundational to the optimization of Boolean <b>computing</b> <b>systems.</b> Using an automated deduction system, I show that the sentential calculus of the Principia Mathematica (PM) can be derived from Łukasiewicz's CN; the proof appears to be novel. The proofs variously demonstrate, furthermore, a natural proving order...|$|R
40|$|Abstract — The {{rapid growth}} of {{handheld}} computing devices such as mobile phones, PDAs or palmtops is {{paving the way for}} the emergence of pervasive <b>computing</b> <b>systems.</b> Just {{as in the case of}} traditional <b>computing</b> <b>systems,</b> perva-sive <b>computing</b> <b>systems</b> need to be tested in the large before they can be deployed in the field. As opposed to traditional <b>computing</b> <b>systems,</b> however, large-scale testing of pervasive <b>computing</b> <b>systems</b> requires the presence of dozens (perhaps hundreds) of physical devices, arranged together in a network, executing a vari-ety of complex scenarios. In order to reduce the cost of such testing, it would be better to simulate the operation of a pervasive <b>computing</b> <b>system</b> using well-known techniques from multi-agent simulation, by representing each (hardware or software) component of the system as a software agent. In this paper we de-scribe our ongoing work, where we extend our earlier work on multi-agent simu-lation, for pervasive <b>computing</b> <b>systems.</b> Since adaptation in pervasive <b>computing</b> <b>systems</b> is expected to be common, we also show that our simulation technique can model adaptation. ...|$|R
40|$|This paper {{deals with}} {{analysis}} of existing approaches to tasks mapping on reconfigurable <b>computing</b> <b>systems</b> with special {{attention paid to}} mapping methods for coarse grained reconfigurable <b>computing</b> <b>systems.</b> The purpose and objectives of a new heuristic method for tasks mapping on coarse grained reconfigurable <b>computing</b> <b>systems</b> are produced {{on the base of}} the carried out analysis. This novel method for tasks mapping on coarse grained reconfigurable <b>computing</b> <b>systems</b> is based on the method of graph partitioning with pushing vertices, graph covering algorithm, a heuristic approach to optimizing and packaging for a particular graph on coarse grained reconfigurable <b>computing</b> <b>system</b> and displaying methods of data flow graph on resources of coarse grained reconfigurable <b>computing</b> <b>system.</b> The simulation was conducted on system model with coarse grained reconfigurable hardware accelerator MATRIX. Experimental results are given. They prove the effectiveness of the proposed approach as compared with the widely used methods for tasks mapping on coarse grained reconfigurable <b>computing</b> <b>systems</b> and the ability to use dynamic functional parameters of coarse grained reconfigurable <b>computing</b> <b>system</b> to further improvement of the mapping results...|$|R
40|$|<b>Computing</b> <b>Systems</b> have a {{tremendous}} impact on everyday life in all domains, from the Internet to consumer electronics, transportation to manufacturing, medicine, energy, and scientific computing. In the future, <b>computing</b> <b>systems</b> will continue to be one of our most powerful tools for taking on the societal challenges shaping Europe, its values, and its global competitiveness. The FP 7 HiPEAC network of excellence is Europe’s premier organization for coordinating research, improving mobility, and enhancing visibility in the <b>computing</b> <b>system</b> field. HiPEAC covers all computing market segments: embedded <b>systems,</b> general purpose <b>computing</b> <b>systems,</b> data centers and high performance computing. Created in 2004, HiPEAC today gathers over 250 leading European academic and industrial <b>computing</b> <b>system</b> research- ers from about 100 universities and 50 companies in one virtual centre of excellence. To encourage <b>computing</b> <b>systems</b> innovation in Europe, HiPEAC provides collaboration grants, internships, sabbaticals, and improves networking through the yearly HiPEAC conference, ACACES summer school, and the semiannual <b>computing</b> <b>systems</b> week. In this roadmap document, HiPEAC leverages the broad expertise of its members to identify and analyze the key challenges for <b>computing</b> <b>systems</b> in Europe over the next decade. While advances in <b>computing</b> <b>systems</b> have been consistent and dra- matic over the past fifty years, its future today is not as certain. To continue to be a tool for providing new and innovative solu- tions, the <b>computing</b> <b>systems</b> community must face serious challenges in efficiency, complexity, and dependability. upmarchipea...|$|R
40|$|Abstract- In {{this paper}} we {{described}} four layer architecture of Grid <b>Computing</b> <b>System,</b> analyzes security requirements and problems existing in Grid <b>Computing</b> <b>System.</b> This paper presents a new approch of five layer security architecture of Grid <b>Computing</b> <b>System,</b> defines {{a new set}} of security policies & gives the representation...|$|R
40|$|Subject of investigation: <b>computing</b> <b>systems</b> {{and methods}} to ensure failure {{stability}} of the parallel real-time <b>computing</b> <b>systems.</b> Purpose of the work: development of methodological and practical recommendations for the analysis and maintenance of the required levels of the failure {{stability of the}} parallel real-time <b>computing</b> <b>systems</b> under the conditions of limitations for the structural redundancy and use of built-in control facilities. Methods and a mathematical model complex are developed {{for the analysis of}} the reliability of the parallel <b>computing</b> <b>systems</b> with active protection. An engineering method and software are developed for calculating the reliability of the <b>computing</b> <b>systems.</b> Technical concepts are proposed on the construction of the failure-stable parallel <b>computing</b> <b>systems.</b> The ingineering method of calculation of the <b>computing</b> <b>system</b> reliability is put into practice in the scientific and research centre of the computer equipment and is included in the standards of the intergovernmental commission on the computer equipment. Field of application: microprocessor computing systemsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Two {{design rules}} which aid the {{construction}} of distributed <b>computing</b> <b>systems</b> {{and the provision of}} fault tolerance are described, namely that: (i) a distributed <b>computing</b> <b>system</b> should be functionally equivalent to the individual <b>computing</b> <b>systems</b> of which it is composed, and (ii) fault tolerant systems should be constructed from generalised fault tolerant components. The reasonin...|$|R
40|$|Recently, {{applications}} using Peer-to-Peer and Grid-computing {{techniques have}} been gaining much attention {{due to the}} growing requirement for data processing. In this work we focus on public-resource <b>computing</b> <b>systems</b> known also as global <b>computing</b> <b>systems</b> or peer-to-peer <b>computing</b> <b>systems.</b> We {{address the problem of}} data transmission in overlay-based public-resource <b>computing</b> <b>systems</b> and examine two approaches: peer-to-peer (P 2 P) and unicast. As the objective we use cost of the system. Optimization model of public-resource <b>computing</b> <b>system</b> formulated as Integer Programming problem is applied to obtain optimal solutions and approximate results of effective heuristics and random approaches. Extensive simulations show that P 2 P approach enables significant reduction of the system cost. 1...|$|R
40|$|Purpose of the work: {{investigation}} of approaches {{and methods of}} organization of synchronization algorithms and development of algorithms for the models and trunk-module <b>computing</b> <b>systems.</b> The synchronization algorithms are classified and compared. An algorithm is developed for the models and for the trunk-module <b>computing</b> <b>systems.</b> The distributed simulation modelling system is implemented for trunk-module <b>computing</b> <b>system</b> "Stand". Field of application: software for the distributed <b>computing</b> <b>systems,</b> distributed simulation modelling, half-natural modellingAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Abstract. Logging is {{a central}} service in <b>computing</b> <b>systems.</b> It lays the {{foundation}} for accountability and audit services in <b>computing</b> <b>systems,</b> {{as well as for}} other accessory services. While providing logging services in traditional <b>computing</b> <b>systems</b> is a relatively smooth process, it turns to an intricate task in pervasive <b>computing</b> <b>systems.</b> In this context, we present two contributions addressing this problem. First, we develop an approach to securely log information in marginally trusted collectors. Second, we investigate {{the question of how to}} securely delegate our logging protocol to a relay equipped with trusted-computing modules. ...|$|R
40|$|To {{build on}} my {{extensive}} experience in research on the security mechanisms associated with cloud <b>computing</b> <b>system,</b> which {{would allow me to}} take advantage of my recent research experience in cloud and mobile <b>computing</b> <b>system</b> SUMMARY OF QUALIFICATIONS ▸Deep understanding of system-level performance improvement techniques toward secure cloud and mobile <b>computing</b> <b>systems</b> (High-Performance outsourced storage, communication, and audit systems) ▸Problem description and solving skills based on strong system and security background as well as the actual system implementation and subsequent evaluation in a real <b>computing</b> <b>system</b> ▸Execellent project leading experiences and research proposal writing experienc...|$|R
40|$|International audienceComputational {{science is}} placing new demands on {{distributed}} <b>computing</b> <b>systems</b> as {{the rate of}} data acquisition is far outpacing the improvements in processor speed. Evolutionary algorithms provide efficient means of optimizing the increasingly complex models required by different scientific projects, which can have very complex search spaces with many local minima. This work describes different validation strategies used by MilkyWay@Home, a volunteer computing project created to address the extreme computational demands of 3 -dimensionally modeling the Milky Way galaxy, which currently consists of over 27, 000 highly heterogeneous and volatile <b>computing</b> <b>hosts,</b> which provide a combined computing power of over 1. 55 petaflops. The validation strategies presented form a foundation for efficiently validating evolutionary algorithms on unreliable or even partially malicious <b>computing</b> <b>systems,</b> and have significantly reduced the time taken to obtain good fits of MilkyWay@Home's astronomical models...|$|R
40|$|Recently, Peer-to-Peer <b>computing</b> <b>systems</b> {{have been}} gaining much {{attention}} {{due to the}} growing requirement for data processing. We focus on the optimization problem related to computations and data transmission in overlay-based P 2 P <b>computing</b> <b>systems</b> with the system cost objective. An effective heuristic algorithm is proposed and compared against optimal results. Key words P 2 P, <b>computing</b> <b>systems,</b> overlay, optimization...|$|R
50|$|The need of {{constant}} user interaction in interactive <b>computing</b> <b>systems</b> makes it different {{in many ways}} from batch processing systems. Thus different aspects of <b>computing</b> <b>systems</b> are significantly different for interactive <b>computing</b> <b>systems</b> {{and they have been}} focused on different research. The design of a different programming model has been discussed. Another article describes the importance of security and reliability in interactive computing.|$|R
50|$|High Productivity <b>Computing</b> <b>Systems</b> (HPCS) is a DARPA {{project for}} {{developing}} {{a new generation of}} economically viable high productivity <b>computing</b> <b>systems</b> for national security and industry in the 2002-10 timeframe.|$|R
40|$|Projet REFLECSThe {{report issued}} by the Inquiry Board in charge of {{inspecting}} the Ariane 5 flight 501 failure concludes that causes of the failure are rooted into poor S/W Engineering practice. From the failure scenario described in the Inquiry Board report, {{it is possible to}} infer what, in our view, are the real causes of the 501 failure. We develop arguments to demonstrate that the real causes of the 501 failure are neither S/W specification errors nor S/W design errors. Real causes of the failure are faults in the capture of the overall Ariane 5 application/environment requirements, and faults in the design and the dimensioning of the Ariane 5 on- board <b>computing</b> <b>system.</b> These faults result from not following a rigorous System Engineering approach, such as applying a proof-based System Engineering method. What is proof-based <b>System</b> Engineering for <b>Computing</b> <b>Systems</b> is also briefly presented. Key-words: Ariane 5, spaceborne <b>computing</b> <b>system,</b> embedded system, fault, error, failure, method for the engineering of <b>computing</b> <b>systems,</b> real-time system, software engineering, system engineering, user requirements capture, <b>computing</b> <b>system</b> design, <b>computing</b> <b>system</b> dimensioning, design proof, dimensioning proof...|$|R
5000|$|The <b>Computing</b> <b>Systems</b> Technology Office {{combined}} {{functions of}} the old Information Sciences and Tactical Technology office. The office [...] "will work scalable parallel and distributed heterogeneous <b>computing</b> <b>systems</b> technologies," [...] DoD said.|$|R
40|$|Cluster <b>computing</b> <b>systems</b> {{are widely}} used in {{parallel}} and distributed computing research. Besides performance, energy cost and security should also be carefully concerned in large scale cluster <b>computing</b> <b>systems</b> to reduce budget and to avoid information leak. In this dissertation, I proposed a Time Aware Dynamic Voltage Scaling scheduling algorithm to conserve energy cost of processors in parallel <b>computing</b> <b>systems</b> and a design of an energyefficient I/O System with write buffer disks to conserve energy cost of large scale storage systems. To explain when the energy consumption could be reduced in cluster <b>computing</b> <b>systems,</b> I analyzed the CPU and I/O system performance in a security-aware storage system. Security is another issue {{which has not been}} well explored in cluster <b>computing</b> <b>systems.</b> I implemented a transparent encryption/decryption layer in a popular Message Passing Interface implementation: MPICH 2. Then I quantitatively evaluate the system performance on two cluster <b>computing</b> <b>systems.</b> ii Acknowledgments It is a great pleasure to thank those who made this dissertation possible. First and foremost, I am heartily thankful to my advisor, Dr. Xiao Qin, whose encouragement...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedWith {{the advent of}} distributed <b>computing</b> <b>systems,</b> the problem of deadlock, which has been essentially solved for centralized <b>computing</b> <b>systems,</b> has reappeared. Existing centralized deadlock detection techniques are either too expensive or they do not work correctly in distributed <b>computing</b> <b>systems.</b> Although several algorithms have been developed specifically for distributed systems, {{the majority of them}} have also been shown to be inefficient or incorrect. Additionally, although fault-tolerance is usually listed as an advantage of distributed <b>computing</b> <b>systems,</b> little has been done to analyze the fault tolerance of these algorithms. This thesis analyzes four published deadlock detection algorithms for distributed <b>computing</b> <b>systems</b> with respect ot their performance in the presence of certain faults. A new deadlock detection algorithm is then proposed whose efficiency and fault tolerance are adjustable. [URL] Commander, United States Nav...|$|R
40|$|Abstract—Autonomic <b>computing</b> <b>systems</b> are a {{promising}} technology for bending the cost curve associated with information and communi-cation technology (ICT) service management and for aiding {{the growth and}} evolution of complex <b>computing</b> <b>systems.</b> Indeed, this has motivated {{a significant amount of}} research. However, a central plank to achieving fully-fledged autonomic <b>computing</b> <b>systems</b> is missing i. e., the ability to certify these systems. The certification process will provide a basis; for assessing the quality of autonomic systems with similar functionalities, for assessing the current capability of the system and its suitability to the problem, {{to assess the impact of}} a certified component on a system and to resolve legal liability, if the autonomic <b>computing</b> <b>systems</b> were to fail. In this second part of a two-part paper, several steps to rate or certify autonomic <b>computing</b> <b>systems</b> within the context of the targeted application domain are proposed. In the first instance, the autonomi...|$|R
40|$|Recently, {{distributed}} <b>computing</b> <b>system</b> {{have been}} gaining much attention {{due to a}} growing demand for various kinds of effective computations in both industry and academia. In this paper, we focus on Peer-to-Peer (P 2 P) <b>computing</b> <b>systems,</b> also called public-resource <b>computing</b> <b>systems</b> or global <b>computing</b> <b>systems.</b> P 2 P <b>computing</b> <b>systems,</b> contrary to grids, use personal computers and other relatively simple electronic equipment (e. g., the PlayStation console) to process sophisticated computational projects. A significant example of the P 2 P computing idea is the BOINC (Berkeley Open Infrastructure for Network Computing) project. To improve {{the performance of the}} <b>computing</b> <b>system,</b> we propose to use the P 2 P approach to distribute results of computational projects, i. e., results are transmitted in the system like in P 2 P file sharing systems (e. g., BitTorrent). In this work, we concentrate on offline optimization of the P 2 P <b>computing</b> <b>system</b> including two elements: scheduling of computations and data distribution. The objective is to minimize the system OPEX cost related to data processing and data transmission. We formulate an Integer Linear Problem (ILP) to model the system and apply this formulation to obtain optimal results using the CPLEX solver. Next, we propose two heuristic algorithms that provide results very close to an optimum and can be used for larger problem instances than those solvable by CPLEX or other ILP solvers...|$|R
30|$|In the literature, several {{simulators}} {{have been}} developed for performance analysis of cloud <b>computing</b> <b>systems,</b> such as GridSim, MicroGrid, GangSim and CloudSim [32]. The first three focus on Grid <b>computing</b> <b>systems</b> to evaluate costs of executing distributed applications in Cloud infrastructures. However, CloudSim is a generalized and extensible simulation toolkit and application which enables seamless modeling, simulation, and experimentation of emerging cloud <b>computing</b> <b>system,</b> infrastructures and application environments for single and internetworked clouds [33, 34, 35].|$|R
50|$|Some {{hardware}} can {{be attached}} to a <b>computing</b> <b>system</b> in a daisy chain configuration by connecting each component to another similar component, rather than directly to the <b>computing</b> <b>system</b> that uses the component. Only the last component in the chain directly connects to the <b>computing</b> <b>system.</b> For example, chaining multiple components that each have a UART port to each other. The components must also behave cooperatively. e.g., only one seizes the communications bus at a time.|$|R
40|$|Novel <b>computing</b> <b>systems</b> are {{increasingly}} being composed {{of large numbers of}} heterogeneous components, each with potentially different goals or local perspectives, and connected in networks which change over time. Management of such systems quickly becomes infeasible for humans. As such, future <b>computing</b> <b>systems</b> should be able to achieve advanced levels of autonomous behaviour. In this context, the system's ability to be self-aware and be able to self-express becomes important. This paper surveys definitions and current understanding of self-awareness and self-expression in biology and cognitive science. Subsequently, previous efforts to apply these concepts to <b>computing</b> <b>systems</b> are described. This has enabled the development of novel working definitions for self-awareness and self-expression within the context of <b>computing</b> <b>systems...</b>|$|R
40|$|Due to {{the rapid}} {{development}} of VLSI technology, com-puting systems can be made much more distributed and to match the structures of the problems for various applica-tions. Because the characteristics of distributed <b>computing</b> <b>systems</b> are significantly {{different from those of}} central-ized <b>computing</b> <b>systems,</b> different a,pproaches are needed to effectively address t,he design issues related to the char-acteristics of software for distributed <b>computing</b> <b>systems.</b> In this pa. per, the methods currently used to develop the software for distributed <b>computing</b> <b>systems</b> are classified into three categories: dataflow-oriented, communication-oriented, and object-oriented. Their applicabilities are conpa. red, and the future trends of software design ap-proaches are discussed. Object-oriented software design methodoiogy is identified as a more promising approac...|$|R
40|$|A {{distributed}} <b>computing</b> <b>system</b> {{consists of}} processing elements, communication links, memory units, data "les, and programs. These resources are interconnected via {{a communication network}} and controlled by a distributed operating system. The distributed program reliability in a distributed <b>computing</b> <b>system</b> is {{the probability that a}} program which runs on multiple processing elements and needs to retrieve data "les from other processing elements will be executed successfully. This reliability varies according to (1) the topology of the distributed <b>computing</b> <b>system,</b> (2) the reliability of the communication edges, (3) the data "les and programs distribution among processing elements, and (4) the data "les required to execute a program. In this paper, we show that computing the distributed program reliability on the star distributed <b>computing</b> <b>systems</b> is NP-hard. We also develop an e$ciently solvable case to compute distributed program reliability when some additional "le distribution is restricted on the star topology. Scope and purpose Recent advances in VLSI circuitry have a tremendous impact on the price-performance revolution in microelectronics. This development has led to an increased use of workstations connected {{in the form of a}} powerful distributed <b>computing</b> <b>system.</b> Potential bene"ts o!ered by such distributed <b>computing</b> <b>system...</b>|$|R
