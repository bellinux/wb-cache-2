25|52|Public
2500|$|The {{goals of}} DevOps span the entire <b>delivery</b> <b>pipeline.</b> They include: ...|$|E
5000|$|Tests {{needing a}} human oracle: Not all quality {{attributes}} can be verified with automation. These attributes require humans in the loop, {{slowing down the}} <b>delivery</b> <b>pipeline.</b>|$|E
50|$|Continuous {{testing is}} the process of {{executing}} automated tests as part of the software <b>delivery</b> <b>pipeline</b> to obtain immediate feedback on the business risks associated with a software release candidate.|$|E
50|$|Xebialabs then {{produced}} XL Release {{to serve}} as a management system for entire continuous <b>delivery</b> <b>pipelines,</b> again extensible to existing tooling with the ability to automate tasks. In 2015, XebiaLabs released XL Satellite as an extension of XL Deploy for easy global deployments to cloud or on premise data centers, and has begun offering free community versions of XL Deploy and XL Release.|$|R
5000|$|Fuel {{and energy}} {{transportation}}: natural gas <b>delivery</b> by <b>pipelines,</b> oil product <b>delivery</b> {{and other related}} transportation ...|$|R
40|$|This paper {{examines}} a new dispatching {{alternative for}} the truckload trucking industry involving {{the use of}} delivery 'pipelines' with dense flow volumes. Drivers and loads are partitioned into two sets; those that utilize pipelines via a series of 'dray' moves and line-hauls, and the remaining set of random over-the-road (OTR) drivers that are dispatched by traditional methods. Alternative methods are presented to determine where <b>delivery</b> <b>pipelines</b> should be located and how they should be operated. The effects of the pipelines on the remaining OTR fleet are also examined. Results indicate that the new dispatching alternative is feasible. Dispatching Driver management Truckload trucking...|$|R
50|$|The goal of {{continuous}} testing {{is to provide}} fast and continuous feedback regarding the level of business risk in the latest build or release candidate. This information can then {{be used to determine}} if the software is ready to progress through the <b>delivery</b> <b>pipeline</b> at any given time.|$|E
50|$|Continuous {{testing is}} the process of {{executing}} automated tests as part of the software <b>delivery</b> <b>pipeline</b> to obtain immediate feedback on the business risks associated with a software release candidate. For Continuous Testing, the scope of testing extends from validating bottom-up requirements or user stories to assessing the system requirements associated with overarching business goals.|$|E
5000|$|Continuous {{testing is}} the process of {{executing}} automated tests as part of the software <b>delivery</b> <b>pipeline</b> to obtain immediate feedback on the business risks associated with a software release candidate. [...] Continuous testing includes the validation of both functional requirements and non-functional requirements; the scope of testing extends from validating bottom-up requirements or user stories to assessing the system requirements associated with overarching business goals.|$|E
50|$|Indirect potable reuse (IPR) {{has been}} {{considered}} for regional communities in Goulburn, NSW, the Australian Capital Territory (ACT) and Toowoomba, Queensland (2008). The Western Corridor Recycled Water Scheme in South East Queensland was designed and built to produce drinking quality water suitable for release into the Wivenhoe Dam, Brisbane's principal water storage. The advanced WWTP incorporated MF and RO followed by an advanced oxidation system using UV-light and hydrogen peroxide to remove specific disinfection by-products and non-specific low molecular weight organics. The project had a production capacity of 232,000 m3 per day and over 200 km of interconnecting and product water <b>delivery</b> <b>pipelines.</b>|$|R
40|$|Due to the {{complexity}} of present day supplychains it is important to select the simplest supplychain scheduling decision support system (DSS) which will determine and place orders satisfactorily. We propose to use a generic design framework, termed the explicitfilter methodology, to achieve this objective. In doing so we compare the explicitfilter approach to the implicitfilter approach utilised in previous OR research the latter focusing on minimising a cost function. Although the eventual results may well be similar with both approaches it is much clearer to the designer, both why and how, an ordering system will reduce the Bullwhip effect via the explicitfilter approach. The “explicitfilter” approach produces a range of DSS designs corresponding to best practice. These may be “mixed and matched” to generate a number of competitive <b>delivery</b> <b>pipelines</b> to suit the specific business scenario...|$|R
50|$|May 3: Trans-Arabian <b>Pipeline</b> <b>delivery</b> from Saudi Arabia to the Mediterranean {{interrupted}} in Syria, driving {{oil tanker}} rates to all time highs from June to December.|$|R
5000|$|XebiaLabs' {{first product}} was an {{application}} called [...] "Deployit", specializing in software deployment automation, the automatic spinning-up of environments, {{and ease of}} use across a range of platform middleware including Websphere, Weblogic, [...]NET/IIS, Glassfish {{and a number of other}} Java EE based providers as well as compatibility with Linux, Unix, and Windows. Deployit was later re-branded as XL Deploy to conform with the creation of other products in the stable, including a solution for continuous <b>delivery</b> <b>pipeline</b> orchestration with XL Release.|$|E
5000|$|In the 2010s, {{software}} {{has become}} a key business differentiator. [...] As a result, organizations now expect software development teams to deliver more, and more innovative, software within shorter delivery cycles. To meet these demands, teams have turned to lean approaches, such as Agile, DevOps, and Continuous Delivery, to try {{to speed up the}} SDLC. After accelerating other aspects of the <b>delivery</b> <b>pipeline,</b> teams typically find that their testing process is preventing them from achieving the expected benefits of their SDLC acceleration initiative. Testing and the overall quality process remain problematic for several key reasons.|$|E
5000|$|Organizations {{that have}} adopted agile {{software}} development are seeing much higher quantities of releases. With the increasing popularity of agile development {{a new approach}} to software releases known as Continuous delivery is starting to influence how software transitions from development to a release. [...] One goal of Continuous Delivery and DevOps is to release more reliable applications faster and more frequently. The movement of the application from a “build” through different environments to production as a “release” is part of the Continuous <b>Delivery</b> <b>pipeline.</b> Release managers are beginning to utilize tools such as application release automation and continuous integration tools to help advance the process of Continuous Delivery and incorporate a culture of DevOps by automating a task {{so that it can be}} done more quickly, reliably, and is repeatable. More software releases have led to increased reliance on release management and automation tools to execute these complex application release processes.|$|E
50|$|Sauce Labs offers {{automated}} testing for Continuous Integration (CI) and Continuous <b>Delivery</b> (CD) <b>pipelines,</b> and provides plugin integrations {{with the many}} CI platforms including: Jenkins, Bamboo, Travis CI, Circle CI and TeamCity. According to Forbes, Sauce Labs currently supports more than 500 browser, operating system, and device platform combinations. The company recently announced a plugin for Atlassian's JIRA project and issue tracking software.|$|R
40|$|The {{combination}} of increasingly international operations, the continuing domestic recession, and the gradual {{opening of the}} Japanese economy to foreign investment {{might be expected to}} change many features of Japanese multinational companies. In particular, the level and mode of central control of overseas units, especially their reliance on expatriate managers, could change as firms become more willing to use foreign subsidiaries as sources of innovation and learning rather than as <b>delivery</b> <b>pipelines</b> for domestically designed and developed products. These changes are more likely in sectors dominated by non-Japanese firms, such as international financial services. Interviews with managers of 14 Japanese manufacturing and financial service firms in the UK and Japan confirmed substantial differences in their internationalization patterns in the 1990 s, both between and within sectors. Car manufacturers in particular invested major resources in transferring significant features of their domestic operations to UK units, usually with expatriate managers, and one was investing in developing an international cadre of managers. Copyright Blackwell Publishing Ltd 2003. ...|$|R
50|$|After the Bolshevik Revolution, {{kerosene}} <b>deliveries</b> {{through the}} <b>pipeline</b> were relaunched in March 1921 and on 20 May 1921, the first delivery of kerosene arrived at Batum. After 1936 Batum was renamed to Batumi ref.11.|$|R
5000|$|The Klappan Coalbed Methane Project is a {{proposal}} by Shell Canada {{to develop a}} coalbed methane project in the Sacred Headwaters. In 2004, the British Columbia government granted Royal Dutch Shell (which is now a parent company of Shell Canada) a 400000 ha tenure for coalbed methane development. It is accessed by road via the abandoned BC Rail grade, which intersects British Columbia Highway 37 south of Iskut. As of summer 2008, Shell's project was in the exploration phase. Shell drilled three exploratory wells in 2004 and was preparing to drill an additional 14 wells in 2008, 8 of which were proposed for {{the headwaters of the}} Skeena River. If developed, Shell's project will entail a network of gas wells connected by roads and pipelines, as well as a pipeline to deliver the gas to market. Shell has disclosed neither how many wells will be necessary to make the project economically viable nor route options for the <b>delivery</b> <b>pipeline.</b> The Klappan Coalbed Methane Project has been met with opposition by both First Nations groups and non-governmental organizations. The Pembina Institute, an environmentalist think-tank, released a report on the potential impacts of the Klappan Coalbed Methane Project on wild salmon, calling it a [...] "risky experiment" [...] as commercial coalbed methane production has never been attempted in a salmon-bearing watershed. On December 18, 2012, the B.C. government announced that Shell Canada would relinquish its tenure on the land, and that oil and gas development would be banned in the Sacred Headwaters.|$|E
5000|$|The unit held a Parliamentary Reception in the House of Commons in May 2009 {{to raise}} {{awareness}} of the work carried out by the NHPAU, which was opened by David Drew. A review of the unit in autumn 2009 found that the organisation was performing well, and suggested it should also consider conducting research for local authorities into the housing market, improve the level of accessibility of the unit's publication and work closer with local bodies. In response to the review of the unit, and also to their paper published in February 2010 entitled Evaluating the requirements for market and affordable housing [...] - [...] which provides an alternative to expensive local income surveys, by providing model guidance on Strategic Housing Market Assessments [...] - [...] on 1 April 2010 a ministerial statement from the DC&LG announced that the NHPAU had a new, extended remit. The remit was expanded to enable the unit to work closer with local authorities, at local and sub-regional levels, and a new project was announced which will research whether the availability of low-cost housing could be affected by activities other than simply property building. Then Minister of State for Housing and Planning John Healey said that the changes will ensure [...] "the unit will help {{to make sure that we}} build more homes and in the right places", and then chair of the unit Dr. Peter Williams went on to say this: [...] The local area is where sites are identified, planning decisions are made and the <b>delivery</b> <b>pipeline</b> managed. In short it is where homes are built. It is also where economic theory and demographic projections meet practical reality.The overriding aim of our Board and expert team will be to help local authorities and sub-regional bodies make the planning and delivery system for housing work in a way which makes a difference to housing need. This is vital if everyone is to have a home that meets their needs at a price they can afford Dr. Peter Williams, Chair of the National Housing and Planning Advice Unit ...|$|E
40|$|This poster {{describes}} the <b>delivery</b> <b>pipeline</b> in the Bagadus soccer analysis system. The <b>delivery</b> <b>pipeline</b> takes a real-time stitched panorama video, and generates a personal virtual camera {{that can be}} controlled by the clients (end-users). An important component in this pipeline is the H. 264 encoding of the personalized virtual view before delivery. By using Nvidia’s NVENC hardware encoder, {{we are able to}} maintain the same visual quality as the software x 264 encoder with a reduction in both CPU utilization and encode latency...|$|E
5000|$|On 21 May 2006, the {{commissioning}} gas was pumped to {{the pipeline}} from the Sangachal Terminal. [...] First <b>deliveries</b> through the <b>pipeline</b> commenced on 30 September 2006. Deliveries of gas from Shah Deniz gas field started on 15 December 2006.|$|R
40|$|In {{traditional}} IT environments, it {{is common}} for software updates and new releases to take up to several weeks or even months to be eventually available to end users. Therefore, many IT vendors and providers of software products and services face the challenge of delivering updates considerably more frequently. This is because users, customers, and other stakeholders expect accelerated feedback loops and significantly faster responses to changing demands and issues that arise. Thus, taking this challenge seriously is of utmost economic importance for IT organizations if they wish to remain competitive. Continuous software delivery is an emerging paradigm adopted by an increasing number of organizations in order to address this challenge. It aims to drastically shorten release cycles while ensuring the delivery of high-quality software. Adopting continuous delivery essentially means to make it economical to constantly deliver changes in small batches. Infrequent high-risk releases with lots of accumulated changes are thereby replaced by a continuous stream of small and low-risk updates. To gain from the benefits of continuous delivery, a high degree of automation is required. This is technically achieved by implementing continuous <b>delivery</b> <b>pipelines</b> consisting of different application-specific stages (build, test, production, etc.) to automate most parts of the application delivery process. Each stage relies on a corresponding application environment such as a build environment or production environment. This work presents concepts and approaches to implement continuous <b>delivery</b> <b>pipelines</b> based on systematically gathered solutions to be used and orchestrated as building blocks of application environments. Initially, the presented Gather'n'Deliver method is centered around a shared knowledge base to provide the foundation for gathering, utilizing, and orchestrating diverse solutions such as deployment scripts, configuration definitions, and Cloud services. Several classification dimensions and taxonomies are discussed in order to facilitate a systematic categorization of solutions, in addition to expressing application environment requirements that are satisfied by those solutions. The presented GatherBase framework enables the collaborative and automated gathering of solutions through solution repositories. These repositories are the foundation for building diverse knowledge base variants that provide fine-grained query mechanisms to find and retrieve solutions, for example, to be used as building blocks of specific application environments. Combining and integrating diverse solutions at runtime is achieved by orchestrating their APIs. Since some solutions such as lower-level executable artifacts (deployment scripts, configuration definitions, etc.) do not immediately provide their functionality through APIs, additional APIs need to be supplied. This issue is addressed by different approaches, such as the presented Any 2 API framework that is intended to generate individual APIs for such artifacts. An integrated architecture in conjunction with corresponding prototype implementations aims to demonstrate the technical feasibility of the presented approaches. Finally, various validation scenarios evaluate the approaches within the scope of continuous delivery and application environments and even beyond...|$|R
40|$|Continuous {{delivery}} is a software development methodology {{that aims to}} reduce development cycle time by putting {{a strong emphasis on}} automation, quality and rapid feedback. This thesis develops an automated method for detecting performance regressions as part of a continuous <b>delivery</b> deployment <b>pipeline.</b> The chosen method is based on control charts, a tool commonly used within statistical process control. This method is implemented as part of a continuous <b>delivery</b> deployment <b>pipeline</b> and its ability to detect performance regressions is then evaluated by injecting various performance bottlenecks in a sample application. The results from this thesis show that using a control chart based approach is a viable option when trying to automate verification of load test results in the context of continuous delivery. Kontinuerlig leverans är en utvecklingsmetodik för mjukvara med målet att reducera ledtid genom att fokusera på automatisering, kvalitet och snabb återkoppling. I det här examensarbetet utvecklas en automatiserad metod för att upptäcka försämringar i prestanda i en deployment pipeline för kontinuerlig leverans. Den valda metoden baseras på kontrolldiagram, ett verktyg som ofta används inom statistisk processkontroll. Metoden implementeras som en del av en deployment pipeline för kontinuerlig leverans och dess förmåga att upptäcka prestandaförsämringar utvärderas genom att olika prestandarelaterade flaskhalsar implementeras i en testapplikation. Resultaten från arbetet visar att en metod baserad på kontrolldiagram är ett tänkbart alternativ för att automatisera verifiering lasttestresultat inom kontinuerlig leverans. ...|$|R
40|$|The {{task of the}} diploma thesis ‚‚Adjustment of the {{pumping station}} and <b>delivery</b> <b>pipeline</b> of the raw water to {{location}} of JETE for considered building up of new nuclear sources‘‘ {{is to figure out}} a capability of the pumping station Hněvkovice to draw a sufficient capacity of a raw water to the Temelín nuclear power plant. In case of necessity, it designs useful adjustment of a pumping station and deliver pipeline for increasing the raw water delivery rate...|$|E
40|$|In {{the recent}} years, DevOps methodologies have been {{introduced}} to extend the traditional agile principles which have brought up on us a paradigm shift in migrating applications towards a cloud-native architecture. Today, microservices, containers, and Continuous Integration/Continuous Delivery have become critical to any organization’s transformation journey towards developing lean artifacts {{and dealing with the}} growing demand of pushing new features, iterating rapidly to keep the customers happy. Traditionally, applications have been packaged and delivered in virtual machines. But, with the adoption of microservices architectures, containerized applications are becoming the standard way to deploy services to production. Thanks to container orchestration tools like Marathon, containers can now be deployed and monitored at scale with ease. Microservices and Containers along with Container Orchestration tools disrupt and redefine DevOps, especially the <b>delivery</b> <b>pipeline.</b> This Master’s thesis project focuses on deploying highly scalable microservices packed as immutable containers onto a Mesos cluster using a container orchestrating framework called Marathon. This is achieved by implementing a CI/CD pipeline and bringing in to play some of the greatest and latest practices and tools like Docker, Terraform, Jenkins, Consul, Vault, Prometheus, etc. The thesis is aimed to showcase why we need to design systems around microservices architecture, packaging cloud-native applications into containers, service discovery and many other latest trends within the DevOps realm that contribute to the continuous <b>delivery</b> <b>pipeline.</b> At BetterDoctor Inc., it is observed that this project improved the avg. release cycle, increased team members’ productivity and collaboration, reduced infrastructure costs and deployment failure rates. With the CD pipeline in place along with container orchestration tools it has been observed that the organisation could achieve Hyperscale computing as and when business demands...|$|E
40|$|Internet of Things {{has moved}} from being a 2 -tier server-client into a 3 -tier server-gateway-client {{architecture}}. The gateway plays {{a vital role in}} this 3 -tier architecture with intelligence being built into it. With no proper standardization and with more vendors having proprietary apps, which are shared in this multi-tenant gateway, it demands sandboxing and isolation of apps at the gateway. My thesis explores light weight LXD System containers and state of the art configuration management tools like Chef, to build an architecture, leveraging Infrastructure as a Code, creating an app <b>delivery</b> <b>pipeline</b> to deploy apps in jailed environments at an IoT Gateway while maintaining a minimal overhead. The framework also provides ways to automate tests for deployment validation...|$|E
50|$|In 2011, {{a dispute}} rose over {{payments}} for oil <b>delivery</b> through the <b>pipeline.</b> While Transneft has charged CNPC with violating their supply contract, CNPC is not acknowledging these claims. The contract stipulate the monthly volumes of oil {{according to the}} agreed price formula, which {{is less than the}} spot price of oil.|$|R
40|$|Purpose - To {{exploit the}} {{benefits}} from locating the material flow decoupling point (DCP) of the supply chain. Design/methodology/approach - Describes the DCP as the location where items of stock are stored as a deliberate part of supply strategy, and as a particularly effective technique where modularisation {{is an integral part}} in achieving mass customization. Illustrates some well-established standard alternative locations for the DCP, covering a wide spectrum of delivery strategies ranging from buying materials-to-order right through to complete manufacture. Explains that the DCP signifies the boundary between push tasks and pull tasks, and that delaying final manufacture and/or delivery by exploiting the DCP can facilitate the availability of goods to the end customer at reasonable cost and within a reasonable time, and limits the danger of product obsolescence. Uses the example of bespoke fashion garments and personalized computers to illustrate that, respectively, the optimum location of the DCP would be inventories close to the end customer, and towards the sales end of the manufacturing process where modules are withdrawn from stock and immediately assembled. Maps the material DCP on to Pagh and Cooper's (1997) postponement matrix to demonstrate the location of make-to-stock, deliver-to-order, make-to-stock, and finalise-to-order strategies. Demonstrates how the DCP can be exploited in a lean/agile supply chain. Contends that the DCP is an essential feature of modern <b>delivery</b> <b>pipelines</b> and a critical component in pipelines delivering high variety at economic cost. Originality/value - Explains the DCP concept and the opportunities that can be gained from its effective positioning...|$|R
40|$|This TIR {{describes}} the INS/CDBS Team’s responsibilities. It defines the standard {{procedures for the}} <b>delivery</b> of calibration <b>pipeline</b> and SYNPHOT reference files to the Data Management Systems (DMS). It provides guidelines for test and validation of reference files by the INS/CDBS Team. This is an update to TIR CDBS 2005 - 02 A. It clarifies procedures to deliver all type of SYNPHOT files, including Atlases and bandpasses...|$|R
40|$|Transportation {{of carbon}} dioxide (CO{sub 2 }) for {{enhanced}} oil recovery is a mature technology, with operating experience dating from the mid- 1980 s. Because of this maturity, recent sequestration studies for the US Department of Energy's National Energy Technology Laboratory {{have been able to}} incorporate transportation into overall energy-cycle economics with reasonable certainty. For these studies, two different coal-fueled plants are considered; the first collects CO{sub 2 } from a 456 -MW integrated coal gasification combined-cycle plant, while the second employs a 353 -MW pulverized-coal boiler plant retrofitted for flue-gas recycling (Doctor et al. 1999; MacDonald and Palkes 1999). The pulverized-coal plant fires a mixture of coal in a 33 % O{sub 2 } atmosphere, the bulk of the inert gas being made up to CO{sub 2 } to the greatest extent practical. If one power plant with one pipe feeds one sequestration reservoir, projected costs for a 500 -km <b>delivery</b> <b>pipeline</b> are problematic, because when supplying one reservoir both plant availability issues and useful pipeline life heavily influence capital recovery costs. The transportation system proposed here refines the sequestration scheme into a network of three distinctive pipelines: (1) 80 -km collection pipelines for a 330 -MW pulverized-coal power plant with 100 % CO{sub 2 } recovery; (2) a main CO{sub 2 } transportation trunk of 320 km that aggregates the CO{sub 2 } from four such plants; and (3) an 80 -km distribution network. A 25 -year life is assumed for the first two segments, but only half that for the distribution to the reservoir. Projected costs for a 500 -km <b>delivery</b> <b>pipeline,</b> assuming an infrastructure, are $ 7. 82 /tonne ($ 17. 22 / 10 {sup 3 } Nm{sub 3 } CO{sub 2 } or $ 0. 49 / 10 {sup 3 } scf CO{sub 2 }), a savings of nearly 60 % with respect to base-case estimates with no infrastructure. These costs are consistent only with conditioned CO{sub 2 } having low oxygen and sulfur content; they do not include CO{sub 2 } recovery, drying, and compression...|$|E
40|$|Purpose – Aims to {{show that}} {{material}} flow concepts developed and successfully applied to commercial products and services can form equally well the architectural infrastructure of effective healthcare delivery systems. Design/methodology/approach – The methodology {{is based on the}} “power of analogy” which demonstrates that healthcare pipelines may be classified via the Time-Space Matrix. Findings – A small number (circa 4) of substantially different healthcare delivery pipelines will cover the vast majority of patient needs and simultaneously create adequate added value from their perspective. Research limitations/implications – The emphasis is firmly placed on total process mapping and analysis via established identification techniques. Healthcare delivery pipelines must be properly engineered and matched to life cycle phase if the service is to be effective. Practical implications – This small family of healthcare delivery pipelines needs to be designed via adherence to very specific-to-purpose principles. These vary from “lean production” through to “agile delivery”. Originality/value – The proposition for a strategic approach to healthcare <b>delivery</b> <b>pipeline</b> design is novel and positions much currently isolated research into a comprehensive organisational framework. It therefore provides a synthesis of the needs of global healthcare...|$|E
40|$|Automated {{tests have}} always been {{essential}} for changing a piece of software. They let developers detect and locate faults early on and provide confidence in the product’s quality. With the rise of Continuous Delivery (CD) in software development, changes are being deployed multiple times a day. Maintaining a high quality test suite has therefore never been more important. Based on the CD practices at ING and its <b>delivery</b> <b>pipeline</b> we have created Spectata, a tool that helps development teams assure {{the quality of their}} unit test code. Every time a change in the code is made Spectata calculates metrics that reflect the adequacy, fault finding effectiveness and maintainability of the test suite. It provides a quality verdict on those aspects, recommended refactorings and a detailed comparison with the quality of prior builds. To evaluate Spectata we perform four case studies in which we compare Spectata’s verdict and recommendations to ones given by the developers and ourselves. The results demonstrate that the testing quality verdict aligns with our own opinion and that the tool is able to help development teams assess, maintain and improve the quality of their test suites. Electrical Engineering, Mathematics and Computer ScienceSoftware Technolog...|$|E
40|$|The {{rapid growth}} of {{offshore}} oil production and undersea oil <b>delivery</b> <b>pipelines</b> {{increases the risk of}} underwater oil spill. In this study, a model based on the Lagrangian particle tracking method is developed to simulate the spreading of oil and gas in an underwater oil spill, which is helpful to estimate the environmental impact and to find effective measures for preventing the spreading of oil. The oil droplets and gas bubbles released from the leakage point are modeled by a large number of representative particles, which are divided into several groups to simulate different components of oil and gas leaked from the underwater blowout. The movement of each particle in one time step includes two components, a mean movement and a random walk. The mean movement is computed by combining the effect of surrounding marine hydrodynamic, the buoyant jet flow near the leakage point and the rise velocity of representative oil droplets or gas bubbles. The random walk method is used to simulate the turbulent diffusion. The compressibility and dissolution of gas are also considered, which {{play an important role in}} deepwater. Comparing with the previous models for underwater oil spill based on the integral Lagrangian control volume method, the present model is more flexible in simulating the crude oil which has complex components. The model is validated by several experiment cases and successfully applied to simulate the DeepSpill field expreiment, and good agreement between the calculation and the observation is obtained. The fractionation of different gas bubbles or oil droplets is considered and significant differences in the underwater distribution of oil droplets and gas bubbles with different sizes are clearly shown in the simulated results...|$|R
40|$|Hydrogen {{and fuel}} cells {{combination}} {{is the most}} viable answer to the antithetic problems of growing energy demand and environmental pollution reduction. Due to the well note difficulties in H 2 transport and storage, distributed H 2 production results the most promising solution, so very compact and small size production plants are required. To this goal, hydrocarbons ATR reaction assures a self-sustaining process and high reactor compactness, resulting as the best method for distributed H 2 production to couple to a fuel cell system. In spite of the increasing interest in renewable sources, due to the low costs and the widespread existing <b>delivery</b> <b>pipelines,</b> fossil fuels reforming still remain the best choice in a transition period towards hydrogen based economy. In this work a compact catalytic reactor was analyzed for the ATR of CH 4 as natural gas surrogate. Structured catalysts (commercial honeycomb and foam monoliths) performances in CH 4 processing were studied. In reactor design, great {{attention has been paid}} to the thermal integration, in order to obtain a total self-sustainability of the process avoiding additional external heat sources, and improve the plant compactness. Through an heat exchange system integrated in the reactor, water and air stream are preheated by exploiting the heat from exhaust stream, allowing to feed reactants at room temperature as well as cooling product stream at a temperature suitable for further purification stages (WGS, PROX). In order to have a very comprehensive process analysis, temperatures and composition were monitored in 6 point along the catalytic bed. The influence of catalytic system geometry as well as thermal conductivity in the process performances was also analysed. Preliminary tests showed high thermal system efficiency, with a good hydrocarbon conversion at different operating conditions for both catalyst typologies. Copyright © 2012, AIDIC Servizi S. r. l...|$|R
40|$|Hydrogen {{fuel cells}} seem the most viable {{solution}} to the pollution reduction and the energy growing demand. Very compact and small size production plant for distribute H 2 production may reduce hydrogen transport and storage difficulties. Due to the high reactor compactness and thermal self-sustainability, the auto-thermal reforming (ATR) reaction of gaseous and liquid hydrocarbons can be the optimal solution. Fossil hydrocarbons like methane, gasoline and diesel still remain the favourite feed for catalytic auto-thermal reformer, due to the widespread existing <b>delivery</b> <b>pipelines</b> and the high energy density. Unfortunately, due to the different characteristics of liquid and gaseous fuels, it's very difficult to realize a multi-fuel processor characterized by high performances in terms of thermal efficiency and hydrogen yield, and, up to now, very low number of papers dealing with multi-fuel reformers {{is present in the}} literature. In this work, a catalytic reactor for the auto-thermal reforming of gaseous and liquid hydrocarbons was developed. An high pressure feed system, based on the “common rail” technology, was adopted for liquid fuel, allowing the formation of micro-droplets, assuring a very quick liquid fuel vaporization, an uniform mixing with other reactants, avoiding the coke formation, and improving the hydrogen yield and thermal efficiency. A commercial monolith structured catalysts was used and the influence of catalyst configuration was analysed. In order to obtain a total self-sustainable process and a very compact system, a heat exchanger was integrated in the reactor to preheat water and air streams by exploiting the heat from exhaust stream. The process is monitored by following temperatures and compositions along the catalytic bed. Preliminary tests showed high thermal system efficiency, with a good hydrocarbon conversion at different operating conditions. The low start-up times make the system extremely versatile, and suitable for batch operations...|$|R
