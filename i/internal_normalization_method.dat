0|1294|Public
40|$|Isotopic {{analyses}} of a total-spiked sample by multiple-collector {{inductively coupled plasma}} mass spectrometry (MC-ICP-MS) are often corrected for mass fractionation by doping the sample with an external tracer element of known isotopic composition and similar mass. However, {{because of the potential}} for elemental fractionation on the ICP-MS, <b>internal</b> <b>normalization</b> <b>methods</b> should be used whenever possible. Here, we present a weighted non-linear inversion of the mass spectra of a total-spiked sample, using linear, power, and exponential laws. This method was applied to the calibration of an Osmium tracer solution enriched in 190 Os, and the results agree with iterative unmixin...|$|R
40|$|Background: Real-time {{quantitative}} Reverse Transcriptase Polymerase Chain Reaction (qRT-PCR) is recently {{used for}} characterization and expression analysis of miRNAs. The data from such experiments need effective analysis methods to produce reliable and high-quality data. For the miRNA prostate cancer qRT-PCR data {{used in this}} study, standard housekeeping <b>normalization</b> <b>method</b> fails due to non-stability of endogenous controls used. Therefore, identifying appropriate <b>normalization</b> <b>method(s)</b> for data analysis based on other data driven principles is {{an important aspect of}} this study. Results: In this study, different <b>normalization</b> <b>methods</b> were tested, which are available in the R packages Affy and qpcrNorm for normalization of the raw data. These methods reduce the technical variation and represent robust alternatives to the standard housekeeping <b>normalization</b> <b>method.</b> The performance of different <b>normalization</b> <b>methods</b> was evaluated statistically and compared against each other {{as well as with the}} standard housekeeping <b>normalization</b> <b>method.</b> The results suggest that qpcrNorm Quantile <b>normalization</b> <b>method</b> performs best for all methods tested. Conclusions: The qpcrNorm Quantile <b>normalization</b> <b>method</b> outperforms the other <b>normalization</b> <b>methods</b> and standard housekeeping <b>normalization</b> <b>method,</b> thus proving the hypothesis of the study. The data driven methods used in this study can be applied as standard procedures in cases where endogenous controls are not stable...|$|R
40|$|For Hyperspectral image (HSI) datasets, {{each class}} have their salient feature and {{classifiers}} classify HSI datasets {{according to the}} class's saliency features, however, there will be different salient features when use different <b>normalization</b> <b>method.</b> In this letter, we report the effect on classifiers by different <b>normalization</b> <b>methods</b> and recommend the best <b>normalization</b> <b>methods</b> for classifier after analyzing the impact of different <b>normalization</b> <b>methods</b> on classifiers. Pavia University datasets, Indian Pines datasets and Kennedy Space Center datasets will apply to several typical classifiers in order to evaluate and analysis the impact of different <b>normalization</b> <b>methods</b> on typical classifiers. Comment: 6 pages. 1 figure, 4 table...|$|R
30|$|A new {{simple and}} {{effective}} <b>normalization</b> <b>method</b> is {{proposed in the}} PNAE algorithm that improves the <b>normalization</b> <b>method</b> of SDRCLCE in both enhancement effect and calculation efficiency.|$|R
40|$|In {{this thesis}} {{the effect of}} <b>normalization</b> <b>methods</b> on the {{identification}} of differentially expressed genes is investigated. A zebrafish microarray dataset called Swirl {{was used in this}} thesis work. First the Swirl dataset was extracted and visualized to view if the robust spline and print tip loess <b>normalization</b> <b>methods</b> are appropriate to normalize this dataset. The dataset was then normalized with the two <b>normalization</b> <b>methods</b> and the differentially expressed genes were identified with the LimmaGUI program. The results were then evaluated by investigating which genes overlap after applying different <b>normalization</b> <b>methods</b> and which ones are identified uniquely after applying the different methods. The results showed that after the <b>normalization</b> <b>methods</b> were applied the differentially expressed genes that were identified by the LimmaGUI program did differ to some extent but the difference was not considered to be major. Thus the main conclusion is that the choice of <b>normalization</b> <b>method</b> does not have a major effect on the resulting list of differentially expressed genes...|$|R
40|$|Background: Most currently-used <b>normalization</b> <b>methods</b> for miRNA {{array data}} {{are based on}} methods {{developed}} for mRNA arrays despite fundamental differences between the data characteristics. The application of conventional quantile normalization can mask important expression differences by ignoring demographic and environmental factors. We present a generalization of the conventional quantile <b>normalization</b> <b>method,</b> making use of available subject-level covariates in a colorectal cancer study. Results: In simulation, our weighted quantile <b>normalization</b> <b>method</b> is shown to increase statistical power {{by as much as}} 10 % when relevant subject-level covariates are available. In application to the colorectal cancer study, this increase in power is also observed, and previously-reported dysregulated miRNAs are rediscovered. Conclusions: When any subject-level covariates are available, the weighted quantile <b>normalization</b> <b>method</b> should be used over the conventional quantile <b>normalization</b> <b>method...</b>|$|R
40|$|Abstract Background DNA {{methylation}} plays a {{very important}} role in the silencing of tumor suppressor genes in various tumor types. In order to gain a genome-wide understanding of how changes in methylation affect tumor growth, the differential methylation hybridization (DMH) protocol has been developed and large amounts of DMH microarray data have been generated. However, it is still unclear how to preprocess this type of microarray data and how different background correction and <b>normalization</b> <b>methods</b> used for two-color gene expression arrays perform for the methylation microarray data. In this paper, we demonstrate our discovery of a set of internal control probes that have log ratios (M) theoretically equal to zero according to this DMH protocol. With the aid of this set of control probes, we propose two LOESS (or LOWESS, locally weighted scatter-plot smoothing) <b>normalization</b> <b>methods</b> that are novel and unique for DMH microarray data. Combining with other <b>normalization</b> <b>methods</b> (global LOESS and no normalization), we compare four <b>normalization</b> <b>methods.</b> In addition, we compare five different background correction methods. Results We study 20 different preprocessing methods, which are the combination of five background correction <b>methods</b> and four <b>normalization</b> <b>methods.</b> In order to compare these 20 methods, we evaluate their performance of identifying known methylated and un-methylated housekeeping genes based on two statistics. Comparison details are illustrated using breast cancer cell line and ovarian cancer patient methylation microarray data. Our comparison results show that different background correction methods perform similarly; however, four <b>normalization</b> <b>methods</b> perform very differently. In particular, all three different LOESS <b>normalization</b> <b>methods</b> perform better than the one without any normalization. Conclusions It is necessary to do within-array normalization, and the two LOESS <b>normalization</b> <b>methods</b> based on specific DMH internal control probes produce more stable and relatively better results than the global LOESS <b>normalization</b> <b>method.</b> </p...|$|R
40|$|Abstract—Robust face {{recognition}} under various illumination environments {{is very difficult}} {{and needs to be}} accomplished for successful commercialization. In this paper, we propose an improved illumination <b>normalization</b> <b>method</b> for {{face recognition}}. Illumination normalization algorithm based on anisotropic smoothing is well known to be effective among illumination <b>normalization</b> <b>methods</b> but deteriorates the intensity contrast of the original image, and incurs less sharp edges. The proposed method in this paper improves the previous anisotropic smoothing-based illumination <b>normalization</b> <b>method</b> so that it increases the intensity contrast and enhances the edges while diminishing the effect of illumination variations. Due to the result of these improvements, face images preprocessed by the proposed illumination <b>normalization</b> <b>method</b> becomes to have more distinctive feature vectors (Gabor feature vectors) for face recognition. Through experiments of face recognition based on Gabor feature vector similarity, the effectiveness of the proposed illumination <b>normalization</b> <b>method</b> is verified. Keywords—Illumination Normalization, Face Recognition, Anisotropic smoothing, Gabor feature vector...|$|R
40|$|URL {{normalization}} is {{a process}} of transforming URL strings into canonical form. Through this process, duplicate URL representations for web pages can be reduced significantly. There are a number of <b>normalization</b> <b>methods.</b> In this paper, we describe four metrics for evaluating <b>normalization</b> <b>methods.</b> The reliability and consistency of a URL is also considered in our evaluation. With the metrics proposed, we evaluate seven <b>normalization</b> <b>methods.</b> The evaluation results on over 25 million URLs, extracted from the web, are reported in this paper. 1...|$|R
3000|$|... {{denotes the}} {{normalized}} {{value for the}} output value of Iin(x, y). Though quite simple, the proposed <b>normalization</b> <b>method</b> is still an effective way and has a higher computational efficiency than the <b>normalization</b> <b>method</b> in SDRCLCE, which is confirmed in our experiments in Section  3.|$|R
30|$|Step 2 <b>Normalization</b> <b>method</b> of personal-best fitness.|$|R
25|$|On-going <b>normalization</b> <b>method</b> {{in which}} each unit is treated {{separately}} {{and the problem is}} continuously normalized as the solution develops. This approach, which is widely described in classical texts, is best suited for manual calculations. An example of the ongoing <b>normalization</b> <b>method</b> as applied to addition is shown below.|$|R
40|$|The goal of {{collaborative}} filtering {{is to make}} recommendations for a test user by utilizing the rating information of users who share interests similar to the test user. Because ratings are determined not only by user interests but also the rating habits of users, {{it is important to}} normalize ratings of different users to the same scale. In this paper, we compare two different normalization strategies for user ratings, namely the Gaussian <b>normalization</b> <b>method</b> and the decoupling <b>normalization</b> <b>method.</b> Particularly, we incorporated these two rating <b>normalization</b> <b>methods</b> into two collaborative filtering algorithms, and evaluated their effectiveness on the EachMovie dataset. The experiment results have shown that the decoupling <b>method</b> for rating <b>normalization</b> is more effective than the Gaussian <b>normalization</b> <b>method</b> in improving the performance {{of collaborative}} filtering algorithms...|$|R
40|$|Abstract Background MiR arrays {{distinguish}} {{themselves from}} gene expression arrays by their more {{limited number of}} probes, and the shorter and less flexible sequence in probe design. Robust data processing and analysis methods tailored to the unique characteristics of miR arrays are greatly needed. Assumptions underlying commonly used <b>normalization</b> <b>methods</b> for gene expression microarrays containing tens of thousands or more probes may not hold for miR microarrays. Findings from previous studies have sometimes been inconclusive or contradictory. Further studies to determine optimal <b>normalization</b> <b>methods</b> for miR microarrays are needed. Methods We evaluated many different <b>normalization</b> <b>methods</b> for data generated with a custom-made two channel miR microarray using two data sets that have technical replicates from several different cell lines. The impact of each <b>normalization</b> <b>method</b> was examined on both within miR error variance (between replicate arrays) and between miR variance to determine which <b>normalization</b> <b>methods</b> minimized differences between replicate samples while preserving differences between biologically distinct miRs. Results Lowess normalization generally did not perform {{as well as the}} other <b>methods,</b> and quantile <b>normalization</b> based on an invariant set showed the best performance in many cases unless restricted to a very small invariant set. Global median and global mean methods performed reasonably well in both data sets and have the advantage of computational simplicity. Conclusions Researchers need to consider carefully which assumptions underlying the different <b>normalization</b> <b>methods</b> appear most reasonable for their experimental setting and possibly consider more than one normalization approach to determine the sensitivity of their results to <b>normalization</b> <b>method</b> used. </p...|$|R
40|$|This paper {{discusses}} {{characteristics of}} dye biases in microarray data that the conventional <b>normalization</b> <b>methods</b> do not handle, and proposes a new <b>normalization</b> <b>method</b> involving {{a mixture of}} splines model. We also develop a test for between-group comparisons of each gene {{that is designed to}} be used with our proposed method. ...|$|R
40|$|Time-consuming {{external}} standard-based calibration {{methods are}} usually performed for hydrocarbon group type analysis (HGTA) of fossil fuels, {{regardless of the}} instrumental chromatographic technique. In this work, HGTA of a broad variety of coal and petroleum products is performed using a modern thin-layer chromatography-flame ionization detection (TLC-FID) system and a rapid <b>method</b> based on <b>internal</b> <b>normalization.</b> Repeatability, linear intervals, and sample load ranges for quantitative application of this method are detailed for different products that cover {{a broad range of}} boiling points and chemical functionalities in the field of fossil fuels: a heavy oil and its derived hydrocracked products, raw and chemically-modified petroleum asphaltenes, a coal-tar pitch, several coal extracts, and coal hydroliquefaction products. Results from external standard calibration and a <b>normalization</b> <b>method</b> (both obtained by TLC-FID) are in agreement, and they are validated using TLC-ultraviolet scanning. The use of the latter demonstrates that TLC-FID can also be applied to products such as coal extracts and hydroliquefaction products, despite these products being more volatile than petroleum asphaltenes or heavy oils. For preparative purposes when external calibration is necessary, preparative TLC and SPE are less time-consuming alternatives to MPLC fractionation, providing similar results...|$|R
40|$|Motivation: Gene {{expression}} microarrays {{are currently}} being applied {{in a variety of}} biomedical applications. This paper considers {{the problem of how to}} merge data sets arising from different gene-expression studies of a common organism and phenotype. Of particular interest is how to merge data from different technological platforms. Results: The paper makes two contributions to the problem. The first is a simple cross-study <b>normalization</b> <b>method,</b> which is based on linked gene/sample clustering of the given data sets. The second is the introduction and description of several general validation measures that can be used to assess and compare cross-study <b>normalization</b> <b>methods.</b> The proposed <b>normalization</b> <b>method</b> is applied to three existing breast cancer data sets, and is compared to several competing <b>normalization</b> <b>methods</b> using the proposed validation measures. Availability: The Supplementary Materials and XPN Matlab code are publicly available at website...|$|R
30|$|As {{was shown}} by Ganeeva et al. (2014), the n-alkanes {{dominate}} the chromatograms of oils, waxes, and asphaltenes; therefore, further we discuss only the molecular mass distribution (MMD) of n-alkanes in waxes, {{estimated by the}} <b>method</b> of <b>internal</b> <b>normalization.</b>|$|R
40|$|Mass {{spectrometry}} (MS) -based proteomics {{has seen}} significant technical advances {{during the past}} two decades and mass spectrometry has become a central tool in many biosciences. Despite the popularity of MS-based methods, the handling of the systematic non-biological variation in the data remains a common problem. This biasing variation can result from several sources ranging from sample handling to differences caused by the instrumentation. Normalization is the procedure which aims to account for this biasing variation and make samples comparable. Many <b>normalization</b> <b>methods</b> commonly used in proteomics have been adapted from the DNA-microarray world. Studies comparing <b>normalization</b> <b>methods</b> with proteomics data sets using some variability measures exist. However, a more thorough comparison looking at the quantitative and qualitative differences of the performance of the different <b>normalization</b> <b>methods</b> and at their ability in preserving the true differential expression signal of proteins, is lacking. In this thesis, several popular and widely used <b>normalization</b> <b>methods</b> (the Linear regression normalization, Local regression normalization, Variance stabilizing normalization, Quantile-normalization, Median central tendency normalization and also variants of some of the forementioned methods), representing different strategies in normalization are being compared and evaluated with a benchmark spike-in proteomics data set. The <b>normalization</b> <b>methods</b> are evaluated in several ways. The performance of the <b>normalization</b> <b>methods</b> is evaluated qualitatively and quantitatively on a global scale and in pairwise comparisons of sample groups. In addition, it is investigated, whether performing the normalization globally on the whole data or pairwise for the comparison pairs examined, affects the performance of the <b>normalization</b> <b>method</b> in normalizing the data and preserving the true differential expression signal. In this thesis, both major and minor differences in the performance of the different <b>normalization</b> <b>methods</b> were found. Also, the way in which the normalization was performed (global normalization of the whole data or pairwise normalization of the comparison pair) affected the performance of some of the methods in pairwise comparisons. Differences among variants of the same methods were also observed...|$|R
5000|$|Acceptable in most {{journals}} as a valid <b>normalization</b> <b>method</b> {{for western}} blot.|$|R
40|$|International audienceWe {{introduce}} a new shape <b>normalization</b> <b>method</b> based on implicit shape representations. The proposed method is robust with respect to deformations and invariant to similarity transformations (translation, isotropic scaling and rotation). The new method has been tested and compared to the classical shape <b>normalization</b> <b>method</b> and previous work in terms of aligning groups of shapes with deformations...|$|R
40|$|We have {{assessed}} {{the performance of}} seven <b>normalization</b> <b>methods</b> for single cell RNA-seq using data generated from dilution of RNA samples. Our analyses showed that methods considering spike-in External RNA Control Consortium (ERCC) RNA molecules significantly outperformed those not considering ERCCs. This work provides a guidance of selecting <b>normalization</b> <b>methods</b> to remove technical noise in single cell RNA-seq data...|$|R
40|$|Abstract Background In the {{microarray}} experiment, many undesirable systematic variations {{are commonly}} observed. Normalization {{is the process}} of removing such variation that affects the measured gene expression levels. Normalization {{plays an important role in}} the earlier stage of microarray data analysis. The subsequent analysis results are highly dependent on normalization. One major source of variation is the background intensities. Recently, some methods have been employed for correcting the background intensities. However, all these methods focus on defining signal intensities appropriately from foreground and background intensities in the image analysis. Although a number of <b>normalization</b> <b>methods</b> have been proposed, no systematic methods have been proposed using the background intensities in the normalization process. Results In this paper, we propose a two-stage method adjusting for the effect of background intensities in the normalization process. The first stage fits a regression model to adjust for the effect of background intensities and the second stage applies the usual <b>normalization</b> <b>method</b> such as a nonlinear LOWESS method to the background-adjusted intensities. In order to carry out the two-stage <b>normalization</b> <b>method,</b> we consider nine different background measures and investigate their performances in normalization. The performance of two-stage normalization is compared to those of global median normalization as well as intensity dependent nonlinear LOWESS normalization. We use the variability among the replicated slides to compare performance of <b>normalization</b> <b>methods.</b> Conclusions For the selected background measures, the proposed two-stage <b>normalization</b> <b>method</b> performs better than global or intensity dependent nonlinear LOWESS <b>normalization</b> <b>method.</b> Especially, when there is a strong relationship between the background intensity and the signal intensity, the proposed method performs much better. Regardless of background correction methods used in the image analysis, the proposed two-stage <b>normalization</b> <b>method</b> can be applicable as long as both signal intensity and background intensity are available. </p...|$|R
40|$|Producción CientíficaGene-expression data {{obtained}} from high throughput technologies are subject to various sources of noise and accordingly the raw data are pre-processed before formally analyzed. Normalization of the data is a key pre-processing step, since it removes systematic variations across arrays. There are numerous <b>normalization</b> <b>methods</b> available in the literature. Based on our experience, {{in the context of}} oscillatory systems, such as cell-cycle, circadian clock, etc., the choice of the <b>normalization</b> <b>method</b> may substantially impact the determination of a gene to be rhythmic. Thus rhythmicity of a gene can purely be an artifact of how the data were normalized. Since the determination of rhythmic genes is an important component of modern toxicological and pharmacological studies, it is important to determine truly rhythmic genes that are robust to the choice of a <b>normalization</b> <b>method.</b> In this paper we introduce a rhythmicity measure and a bootstrap methodology to detect rhythmic genes in an oscillatory system. Although the proposed methodology can be used for any high throughput gene expression data, in this paper we illustrate the proposed methodology using a publicly available circadian clock microarray gene-expression data. We demonstrate that the choice of <b>normalization</b> <b>method</b> has very little effect on the proposed methodology. Specifically, for any pair of <b>normalization</b> <b>methods</b> considered in this paper, the resulting values of the rhythmicity measure are highly correlated. Thus it suggests that the proposed measure is robust to the choice of a <b>normalization</b> <b>method.</b> Consequently, the rhythmicity of a gene is potentially not a mere artifact of the <b>normalization</b> <b>method</b> used. Lastly, as demonstrated in the paper, the proposed bootstrap methodology can also be used for simulating data for genes participating in an oscillatory system using a reference dataset. Estadística e Investigación OperativaMINECO grant MTM 2015 - 71217 -RMinisterio de Educación, Cultura y Deporte grant FPU 14 / 0453...|$|R
40|$|AbstractThere is {{a variety}} of MCDM methods of solving {{decision}} related problems that can by used. Only a combination of <b>normalization</b> and calculation <b>methods</b> is feasible if problems are solved which are to become a foundation of a rational decision. <b>Normalization</b> <b>methods</b> are used in decision-making processes in various fields. Many researchers apply <b>normalization</b> <b>methods</b> in civil and construction engineering and management. Reviews of <b>normalization</b> <b>methods</b> used in construction engineering and management, and their applications there are presented in the paper, {{as well as a}} review of research collaboration, achievements, academic activities, and information about {{one of the founders of}} the German-Lithuanian-Polish colloquium - Professor Friedel Peldschus. Information about Professor Friedel Peldschus, presented in the paper, has been taken from different date bases which evaluate research work...|$|R
40|$|Normalization of gene {{expression}} data refers {{the process of}} minimizing non biological variation in measured probe intensity levels so that biological differences in {{gene expression}} can be appropriately detected. Several linear normalization within arrays approaches has already been proposed. Recently, use of non-linear methods has been gained quite attention. In this study, our objective is to formulate non-linear <b>normalization</b> <b>methods</b> using support vector regression (SVR) and support vector machine quantile regression (SVMQR) approaches, more easier way and, assess the consistency of these methods with respect to other standard <b>normalization</b> <b>methods</b> for further application in gene expression data. SVR and SVMQR <b>normalization</b> <b>methods</b> have been implemented and their performance have been evaluated with respect to other standard <b>normalization</b> <b>methods</b> namely, locally weighted scatter plot smoothing and Kernel regression. It {{has been found that}} the normalized data based on proposed methods are capable of producing minimum variances within replicate groups and also able to detect truly expressible significant genes with respect to above mentioned other normalized data...|$|R
40|$|Abstract Background Microarray {{technology}} {{allows the}} monitoring of expression levels {{for thousands of}} genes simultaneously. This novel technique helps us to understand gene regulation as well as gene by gene interactions more systematically. In the microarray experiment, however, many undesirable systematic variations are observed. Even in replicated experiment, some variations are commonly observed. Normalization {{is the process of}} removing some sources of variation which affect the measured gene expression levels. Although a number of <b>normalization</b> <b>methods</b> have been proposed, it has been difficult to decide which <b>methods</b> perform best. <b>Normalization</b> {{plays an important role in}} the earlier stage of microarray data analysis. The subsequent analysis results are highly dependent on normalization. Results In this paper, we use the variability among the replicated slides to compare performance of <b>normalization</b> <b>methods.</b> We also compare <b>normalization</b> <b>methods</b> with regard to bias and mean square error using simulated data. Conclusions Our results show that intensity-dependent normalization often performs better than global <b>normalization</b> <b>methods,</b> and that linear and nonlinear <b>normalization</b> <b>methods</b> perform similarly. These conclusions are based on analysis of 36 cDNA microarrays of 3, 840 genes obtained in an experiment to search for changes in gene expression profiles during neuronal differentiation of cortical stem cells. Simulation studies confirm our findings. </p...|$|R
40|$|Background: MiR arrays {{distinguish}} {{themselves from}} gene expression arrays by their more {{limited number of}} probes, and the shorter and less flexible sequence in probe design. Robust data processing and analysis methods tailored to the unique characteristics of miR arrays are greatly needed. Assumptions underlying commonly used <b>normalization</b> <b>methods</b> for gene expression microarrays containing tens of thousands or more probes may not hold for miR microarrays. Findings from previous studies have sometimes been inconclusive or contradictory. Further studies to determine optimal <b>normalization</b> <b>methods</b> for miR microarrays are needed. Methods: We evaluated many different <b>normalization</b> <b>methods</b> for data generated with a custom-made two channel miR microarray using two data sets that have technical replicates from several different cell lines. The impact of each <b>normalization</b> <b>method</b> was examined on both within miR error variance (between replicate arrays) and between miR variance to determine which <b>normalization</b> <b>methods</b> minimized differences between replicate samples while preserving differences between biologically distinct miRs. Results: Lowess normalization generally did not perform {{as well as the}} other <b>methods,</b> and quantile <b>normalization</b> based on an invariant set showed the best performance in many cases unless restricted to a very small invariant set. Global median and global mean methods performed reasonably well in both data sets and have th...|$|R
40|$|This thesis compares vowel <b>normalization</b> <b>methods</b> {{from two}} approaches: {{statistics}} and cognitive sciences. It introduces an automaton {{model to the}} human speech perception mechanism, and discusses several dominant vowel <b>normalization</b> <b>methods</b> with this model. In order to evaluate the efficiency of each <b>normalization</b> <b>methods,</b> several experiments are conducted, {{and the results are}} processed with statistic methods. The most effective method evaluated is adopted in processing data from six different dialects. This thesis also investigates the mechanism of human speech perception. An ANN model is constructed to simulate six virtual speakers of different dialects and compute their mutual intelligibility. Through the simulation this thesis tries to find out the role played by the phonemes in human vowel recognition...|$|R
40|$|Abstract Background It is {{well known}} that the {{normalization}} step of microarray data makes a difference in the downstream analysis. All <b>normalization</b> <b>methods</b> rely on certain assumptions, so differences in results can be traced to different sensitivities to violation of the assumptions. Illustrating the lack of robustness, in a striking spike-in experiment all existing <b>normalization</b> <b>methods</b> fail because of an imbalance between up- and down-regulated genes. This means it is still important to develop a <b>normalization</b> <b>method</b> that is robust against violation of the standard assumptions Results We develop a new algorithm based on identification of the least-variant set (LVS) of genes across the arrays. The array-to-array variation is evaluated in the robust linear model fit of pre-normalized probe-level data. The genes are then used as a reference set for a non-linear <b>normalization.</b> The <b>method</b> is applicable to any existing expression summaries, such as MAS 5 or RMA. Conclusion We show that LVS normalization outperforms other <b>normalization</b> <b>methods</b> when the standard assumptions are not satisfied. In the complex spike-in study, LVS performs similarly to the ideal (in practice unknown) housekeeping-gene normalization. An R package called lvs is available in [URL]. </p...|$|R
30|$|Using the Schmidt <b>normalization</b> <b>method,</b> {{the column}} vectors of the random matrix Θ are {{orthogonal}} to each other.|$|R
30|$|As in Section  3, we {{employ a}} <b>normalization</b> <b>method</b> {{to obtain an}} {{eigenvalue}} result for generalized pseudomonotone operators.|$|R
40|$|Abstract: An {{overview}} of the classification of ecological <b>normalization</b> <b>methods</b> is presented to facilitate the evaluation of alternatives. An historical review is given {{of the development of}} several ecological <b>normalization</b> <b>methods</b> such as Life Cycle Assessment and the International Organization for Standardization’s Eco-Management and Audit Scheme and the like in the former Union of Soviet Socialist Republics (USSR), in the European mainland and in the United States of America. Mathematical models together with medical laboratory experiments were generally used in the former USSR to establish pollution permission levels, whereas environmental management tools are more emphasized in the Western world as ecological <b>normalization</b> <b>methods</b> for firms. Perspectives on the future development of these methods are given. It is concluded that the application of <b>methods</b> for ecological <b>normalization</b> {{is one of the most}} efficient ways of managing environmental matters today and this promotes human health protection. It is also concluded that the movement towards an increasing accord in quality standards among various countries using a complex approach will result in the continuous development of ecological <b>normalization</b> <b>methods...</b>|$|R
40|$|Normalization of cDNA and {{oligonucleotide}} microarray {{data has}} become a standard procedure to offset non-biological differences between two samples for accurate identification of differentially expressed genes. Although there are many normalization techniques available, their ability to accurately remove systematic variation has not been sufficiently evaluated. In this study, we performed experimental validation of various <b>normalization</b> <b>methods</b> {{in order to assess}} their ability to accurately offset non-biological differences (systematic variation). The limitations of many existing <b>normalization</b> <b>methods</b> become apparent when there are unbalanced shifts in transcript levels. To overcome this limitation, we have proposed a novel <b>normalization</b> <b>method</b> that uses a matching algorithm for the distribution peaks of the expression log ratio. The robustness and effectiveness of this method was evaluated using both experimental and simulated data...|$|R
40|$|The {{advent of}} large {{electronic}} text corpora has generated {{a range of}} technologies for their search and interpretation. Variation in document length {{can be a problem}} for these technologies, and several <b>normalization</b> <b>methods</b> for mitigating its effects have been proposed. This paper assesses the effectiveness of such methods in specific relation to exploratory multivariate analysis. The discussion is in four main parts. The first part states the problem, the second describes some <b>normalization</b> <b>methods,</b> the third identifies poor estimation of the population probability of variables as a factor that compromises the effectiveness of the <b>normalization</b> <b>methods</b> for very short documents, and the fourth proposes elimination of data matrix rows representing documents which are too short to be reliably normalized and suggests ways of identifying the relevant documents...|$|R
3000|$|... {{values of}} PNAE {{are similar to}} those of SDRCLCE on the whole because of the only {{difference}} on the <b>normalization</b> <b>method.</b>|$|R
