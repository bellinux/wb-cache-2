0|19|Public
5000|$|Some episodes did {{not have}} proper {{telecine}} encoding and when viewed on an HDTV some <b>interlacing</b> <b>artifacts</b> are visible. All four seasons were re-released on DVD by ABC Studios on April 7, 2009 in [...] "slimmer" [...] packaging. These region 1 releases have been discontinued and are now out of print.|$|R
50|$|Most modern displays, such as LCD, DLP and plasma displays, are {{not able}} to work in {{interlaced}} mode, because they are fixed-resolution displays and only support progressive scanning. In order to display interlaced signal on such displays, the two interlaced fields must be converted to one progressive frame with a process known as de-interlacing. However, when the two fields taken at different points in time are re-combined to a full frame displayed at once, visual defects called <b>interlace</b> <b>artifacts</b> or combing occur with moving objects in the image. A good deinterlacing algorithm should try to avoid <b>interlacing</b> <b>artifacts</b> as much as possible and not sacrifice image quality in the process, which is hard to achieve consistently. There are several techniques available that extrapolate the missing picture information, however they rather fall into the category of intelligent frame creation and require complex algorithms and substantial processing power.|$|R
40|$|Abstract — In {{this paper}} we {{introduce}} an algorithm – {{commonly known as}} a film mode detector – for separating progressive source video from interlaced source video. Due to <b>interlacing</b> <b>artifacts</b> {{in the presence of}} motion, a difference in isophote curvature can be measured and a threshold for effective classification can be set. This {{can be used in a}} video converter to ensure high quality output. We study two approaches. I...|$|R
50|$|When 576i {{is used to}} {{transmit}} content that was originally composed of 25 full progressive frames per second, the odd field of the frame is transmitted first. This {{is the opposite of}} NTSC. Systems which recover progressive frames, or transcode video should ensure that this field order is obeyed, otherwise the recovered frame will consist of a field from one frame and a field from an adjacent frame, resulting in 'comb' <b>interlacing</b> <b>artifacts.</b>|$|R
5000|$|The main {{advantage}} with {{progressive scan}} is that motion appears smoother and more realistic. There is {{an absence of}} visual <b>artifacts</b> associated with <b>interlaced</b> video of the same line rate, such as interline twitter. Frames have no <b>interlace</b> <b>artifacts</b> and can be captured for use as still photos. With progressive scan there is no necessity in intentional blurring (sometimes referred to as anti-aliasing) of video to reduce interline twitter and eye strain.|$|R
50|$|Interestingly, the Hanabi games can {{actually}} be played in both PAL60 (480i) and 480p modes. This makes these releases look significantly better on progressive displays such as LCD TVs. The fast moving sprites in NES and SNES games generally create {{a significant amount of}} <b>interlace</b> <b>artifacts</b> on such displays that the 480p option resolves. However Hanabi Mega Drive titles still run in 50 Hz with the usual PAL conversion problems, despite not been released in PAL.|$|R
5000|$|This {{has also}} become a hazard where non NTSC {{progressive}} video is transcoded to interlaced and vice versa. Systems that recover progressive frames or transcode video should ensure that the [...] "Field Order" [...] is obeyed, otherwise the recovered frame will consist of a field from one frame and a field from an adjacent frame, resulting in [...] "comb" [...] <b>interlacing</b> <b>artifacts.</b> This can often be observed in PC based video playing utilities if an inappropriate choice of de-interlacing algorithm is made.|$|R
50|$|Most video formats {{including}} professional ones utilize chroma subsampling {{to reduce}} amount of chroma {{information in a}} video, {{taking advantage of the}} human visual system's lower acuity for color differences than for luminance. Such a reduction improves compression of the video signal, which is always desirable because of storage and transmission limitations. To ensure compatibility with interlaced-based systems, chroma information in PsF video is sometimes recorded in interlaced format, despite that the content is progressive. This may result in <b>interlaced</b> <b>artifacts</b> being noticeable on colored objects.|$|R
5000|$|Progressive scan video must be {{properly}} deinterlaced to achieve full vertical resolution {{and to avoid}} <b>interlace</b> <b>artifacts.</b> 25P and 30P video must be deinterlaced with [...] "weave" [...] or [...] "no deinterlacing" [...] algorithm, which means joining two fields of each frame together into one progressive frame. This operation {{can be done in}} most editing tools simply by changing project properties from interlaced to progressive. 24P video must go through film-mode deinterlacing also known as inverse telecine, which throws out judder frames and restores original 24-frame/s progressive video. Many editing tools cannot perform film-mode deinterlacing, requiring usage of a separate converter.|$|R
5000|$|It {{converts}} {{the first}} frame into two fields, the second into three fields, the third into three fields, {{and the fourth}} into two fields. It then repeats this pattern for every group of four frames that follows. This pulldown pattern is used to avoid segmenting a 24p frame into two different 60i fields that exist in two different 60i frames. When a 24p frame is split up and recorded into separate 60i fields, <b>interlacing</b> <b>artifacts</b> can exist in the 60i [...] "frames" [...] (i.e. two fields). These artifacts decrease the compression efficiency of DV and can result in cycles of efficient compression followed by less-efficient compression. The advanced pulldown scheme avoids this as every 24p frame can be found intact within the resulting sequence of 60i frames, yet the compression efficiency remains the same as with 3:2 pulldown.|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. Issued also on microfiche from Lange Micrographics. This thesis presents an algorithm which processes a sequence of digital images to look like a sequence of film-originated images. The algorithm alters frame rate by compressing the original thirty frame per second sequence into a twenty-four frame per second sequence and can, if the user wishes, add grain and/or filmic artifacts such as scratches, bumps, and flicker. <b>Interlacing</b> <b>artifacts</b> associated with video are removed through the use of blurring. The algorithm is tested on a computer-generated sequence of images and a digitized video sequence. The results are then analyzed based on the author's subjective criteria...|$|R
2500|$|Image {{processing}} {{to remove}} periodic or anisotropic artifacts such as jaggies from <b>interlaced</b> video, strip <b>artifacts</b> from strip aerial photography, or wave patterns from {{radio frequency interference}} in a digital camera; ...|$|R
50|$|PAL {{usually has}} 576 visible lines {{compared}} with 480 lines with NTSC, meaning that PAL has a 20% higher resolution, {{in fact it}} even has a higher resolution than Enhanced Definition standard (854x480). Most TV output for PAL and NTSC use interlaced frames meaning that even lines update on one field and odd lines update on the next field. Interlacing frames gives a smoother motion with half the frame rate. NTSC is used with a frame rate of 60i or 30p whereas PAL generally uses 50i or 25p; both use a high enough frame rate to give the illusion of fluid motion. This {{is due to the}} fact that NTSC is generally used in countries with a utility frequency of 60 Hz and PAL in countries with 50 Hz, although there are many exceptions. Both PAL and NTSC have a higher frame rate than film which uses 24 frames per second. PAL has a closer frame rate to that of film, so most films are sped up 4% to play on PAL systems, shortening the runtime of the film and, without adjustment, slightly raising the pitch of the audio track. Film conversions for NTSC instead use 3:2 pull down to spread the 24 frames of film across 60 interlaced fields. This maintains the runtime of the film and preserves the original audio, but may cause worse <b>interlacing</b> <b>artifacts</b> during fast motion.|$|R
40|$|The {{opening and}} closing of the vocal folds (plica vocalis) at high {{frequencies}} is {{a major source of}} sound in human speech. Videoky-mography [Svec and Schutte 1995] is a technique for visualizing the motion of the vocal folds for medical diagnosis: The vibrating folds are filmed with an endoscopic camera pointed into the larynx. The camera records at a high framerate to capture vocal fold vibration (see fig. 1 for example frames). The kymogram used for medical diagnosis is a time-slice image, i. e. an X-t-cut through the X-Y-t image cube of the endoscopic video (fig. 2). The quality and di-agnostic interpretability of a kymogram deteriorates significantly if the camera moves relative to the scene as this motion interferes with the vibratory motion of the vocal fold in the kymogram. Therefore, we propose an approach to stabilizing the motion of endoscopic video for kymography. This motion compensation problem is challenging and different from deshaking handheld video (e. g. [Liu et al. 2009]) in several respects: Firstly, the camera motion to be eliminated may be signif-icantly larger than a typical camera shake due to the short distance between camera and scene. Secondly, not only the camera and the vocal folds move but the entire scene may be highly nonrigid. Fi-nally, the image quality of the input material can be challenging due to high noise levels, areas of saturated highlights, <b>interlacing</b> <b>artifacts,</b> etc. The proposed algorithm deviates from the typical feature-based ap-proaches to motion compensation, but is nevertheless parallelizable and realtime capable even on the CPU. We use an image-based in-verse mesh warping approach similar to [Hilsmann et al. 2010] that can be stated as an optimization problem and solved efficiently in a robust Gauss-Newton framework. Our method is described in more detail in [Schneider et al. 2011]. Mesh-based warping is a standard approach to computing com-plex image deformations by deforming a control mesh in the image plane. The inverse problem, i. e. solving for a control mesh defor-mation given two images, can be stated as an energy minimization task: Define the residual for pixel P as rP = I (xP) −K xP +...|$|R
30|$|<b>Interlacing</b> and deinterlacing <b>artifacts</b> {{can occur}} when a scene {{contains}} edges that move within the frame. Moving diagonal edges are particularly noticeable. These artifacts become particularly visible, and thus objectionable, on moving diagonal edges. The traditional deinterlacing detection sequence is a Silicon Optix test disca sequence showing an American flag waving in the breeze. Deinterlacing artifacts are easily {{visible on the}} high-contrast edges of the red and white stripes. The pictures {{in the work of}} Jung et al. [7] and Koo et al. [8] show the impact of deinterlacing problems using this flag sequence. Although any content with moving diagonal edges may be used, interlacing problems are more easily seen on strong contrast edges, which may trigger additional impairments.|$|R
50|$|Panasonic {{created a}} {{complete}} line of cameras that support recording in 24p, {{which is an}} analog of how film cameras record frames, for independent film production. 24p stands for 24 frames per second progressive: a frame rate which is commonly used in motion picture production, and progressive scan which avoids <b>interlacing</b> to give <b>artifact</b> free frames. These features give the recordings a film quality appearance. The original AG-DVX100 nor the later AG-DVX100A (announced on 2003.11.19) and B revisions cannot record in HD. The original Panasonic AG-DVX100 is a 4:3 aspect ratio SD camera only. Difference in original model and A revision are small. The B revision introduced the ability to properly monitor 16:9 aspect ratio, but still only in standard definition. The B revisions CCD sensors have a native aspect ratio of 4:3.|$|R
30|$|As {{screenshots}} of copyrighted video content {{are spreading}} through the Internet without any regulation, cases of copyright infringement have been observed. Further, {{it is difficult}} to use existing forensic techniques for determining whether or not a given image was captured from a screen. Thus, we propose a screenshot identification scheme using the trace of screen capture. Since most television systems and camcorders use interlaced scanning, many screenshots are taken from interlaced videos. Consequently, these screenshots contain the trace of <b>interlaced</b> videos, combing <b>artifacts.</b> In this study, we identify a screenshot using the characteristics of combing artifacts that appear to be shaped like horizontal jagged noise and can be found around the edges. To identify a screenshot, the edge areas are extracted using the gray level co-occurrence matrix (GLCM). Then, the amount of combing artifacts is calculated in the extracted edge areas by using the similarity ratio (SR), the ratio of the horizontal noise to the vertical noise. By analyzing the directional inequality of noise components, the proposed scheme identifies the source of an input image. In the experiments conducted, the identification accuracy is measured in various environments. The results prove that the proposed identification scheme is stable and performs well.|$|R

