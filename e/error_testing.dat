43|2776|Public
50|$|The {{best way}} to build up {{conditional}} statements is step by step composing followed by trial and <b>error</b> <b>testing</b> and refining code.|$|E
50|$|He failed an out-of-competition {{drug test}} in January 2012, {{because of a}} medical <b>error,</b> <b>testing</b> {{positive}} for nandrolone, and received a two-year ban.|$|E
5000|$|... [...] {{which makes}} trial and <b>error</b> <b>testing</b> of the {{arrangement}} of the disks perfectly impractical to perform by hand; computers would make such a break near trivial for 10 disks, but not for the 36 disks that Jefferson used, as 36! ≈ 2138.|$|E
5000|$|The signal quality <b>error</b> <b>test</b> detects silent {{failures}} in the circuitry.|$|R
40|$|In this paper, Short Term Load Forecasting (STLF) can {{be applied}} using Generalized Neuron Model (GNM) for under sum square error {{gradient}} function for different learning rates, with various training epochs and constant leaning rate, by having 30, 000 training epochs. The simulation results were the root mean square <b>testing</b> <b>error,</b> maximum <b>testing</b> <b>error,</b> minimum <b>testing</b> <b>error</b> were predicted...|$|R
5000|$|Both Signal quality <b>error</b> <b>test</b> and Link {{integrity}} functions {{assist in}} fault isolation.|$|R
5000|$|... ePSXe {{is able to}} run most PlayStation games accurately. Few games run {{flawlessly}} without extensive configuration {{and trial}} by <b>error</b> <b>testing.</b> In the case that a game does not run successfully, patches written for the game in question can be used, though few games have patches available.|$|E
50|$|Chemical paint removers {{work only}} on {{certain types of}} finishes, and when {{multiple}} types of finishes {{may have been used}} on any particular surface, trial and <b>error</b> <b>testing</b> is typical to determine the best stripper for each application. Two basic categories of chemical paint removers are caustic and solvent.|$|E
50|$|Zamben {{tries to}} {{reconstruct}} the fan's lost code book by trial and <b>error,</b> <b>testing</b> various combinations of taps with the fan and noting what he gets each time. A succession of past victims are restored thus, among them Yaebu — and the dragon once fanned away by Wangerr of Gwoling! The dragon promptly eats Zamben, fan and all, and bursts from the palace.|$|E
40|$|We provide {{sufficient}} conditions for linear properties {{to be hard}} to test, {{and in the course of}} the proof include a couple of observations which are of independent interest. 1. In the context of linear property testing, adaptive 2 -sided <b>error</b> <b>tests</b> have no more power than non-adaptive 1 -sided <b>error</b> <b>tests.</b> 2. Random linear LDPC codes with linear distance and constant rate are very far from being locally testable...|$|R
40|$|This paper {{considers}} the specification <b>error</b> <b>tests</b> for omitted nonlinearity in the long-run {{relationship of the}} vector error correction model with a null of the standard error correction model. We develop the test statistics for neglected nonlinearity by adding nonlinear functions of the regressors to the long-run relationship. Our test statistics can be calculated using the standard error correction model and its parameter estimates. The test statistics are shown to have the asymptotic standard distribution, and thus the proposed specification <b>error</b> <b>tests</b> {{can be carried out}} without inferential difficulty. The alternative distribution of the specification <b>error</b> <b>tests</b> is explored under the local drift and the smooth functional form. The Monte Carlo simulation reveals that the proposed tests have moderate finite sample performance in detecting nonlinear functional form. An economic application of the stock price-dividend relation is provided...|$|R
40|$|It {{is argued}} that, when {{researchers}} wish {{to carry out}} a Chow test of the significance of prediction errors, it is necessary to assume homoskedasticity because standard results on heteroskedasticity-robust tests are not available. The effects of heteroskedasticity on the Chow prediction <b>error</b> <b>test</b> are examined. The implementation of tests for heteroskedasticity is discussed, with the case in which the regressors include dummy variables for prediction <b>error</b> <b>tests</b> receiving special attention. Monte Carlo results are reported. Copyright (c) Blackwell Publishing Ltd and the Department of Economics, University of Oxford, 2008. ...|$|R
50|$|Hyperclycinemia {{was first}} found in 1957, but it wasn't {{diagnosed}} until 1961 at Johns Hopkins hospital in Philadelphia. Eric, {{the little boy}} who led to the discover of the condition was born he began experiencing {{a number of different}} severe health conditions. Each of these conditions caused him to have excess acidity and ketones in his blood, and luckily for him he responded well to intravenous fluids. The physicians and other medical care givers who were taking care of him were using all the tools they had to help figure out what was causing him to be so ill. One of the tests they used on Eric was an amino acid analysis. This analysis showed the medical providers that the little boy had high levels of amino acid glycine in the blood. The doctors at Johns Hopkins were still unaware of what these high levels of glycine meant, so Eric ended up moving to New York with his parents to be closer to Johns Hopkins. Eric, became a regular patient at Johns Hopkins due to his worsening condition. However, after a lot of trial and <b>error</b> <b>testing</b> Dr. Barton Childs realized that Eric became worse after given these five specific amino acids leucine, isoleucine, valine, theronine, and methionine. When Eric was consuming a diet with any of these amino acids his glycine levels dropped. Finally, after extensive research the medical team at Johns Hopkins published Eric's condition in 1961.|$|E
5000|$|When Team Ninja {{were first}} {{involved}} with the project, they performed lots of trial and <b>error</b> <b>testing</b> to find a gameplay style best suited to the game's tone. When the project was given to Team Ninja, Shibusawa told them [...] "to complete the mission of creating Nioh". The decision to give the project to Team Ninja was heavily influenced {{by the success of}} Dark Souls and other similar titles, dubbed by some as [...] "Masocore" [...] due to their difficult, yet rewarding action gameplay. Many at Team Ninja were fans of the Souls series, and credited their surge in popularity with saving Nioh from possible cancellation and allowing progress for development of the game. Other influences included Bloodborne, Ninja Gaiden, Onimusha and Diablo. The main aim for the developers was to emulate the tough gameplay of both the Souls series and their earlier work on Ninja Gaiden while also making it accessible, fair and rewarding for players. While the combat was extensively influenced by Souls games, Team Ninja's use of loot was more heavily influenced by the Diablo series, as they wanted combat to revolve around player skill rather than gear acquired through combat. The gameplay incorporated elements of samurai combat from popular culture. Historical accuracy when it came to weapons, armor and fighting styles dominated the gameplay design, which resulted in shields not being added as they were not used in combat by samurai. Each boss, from yokai to human enemies, had their own appearances and tactics. The yokai were all drawn from Japanese folklore, although their designs underwent slight alterations from their original forms. A recurring element for the yokai bosses was how they were designed: first they decided the initial form and impression, then the developers added an element which would catch players off guard: for instance, if a yokai appeared beautiful, they would become ugly {{at some point during the}} battle.|$|E
40|$|Compliance {{with the}} Daubert ruling [1] {{requires}} {{the evaluation of}} all medico-legal methods and techniques employed in forensic anthropology. Among other requirements, Daubert calls for estimates of reliability of methods through interobserver <b>error</b> <b>testing.</b> Reliability is best appreciated {{as a measure of}} the consistency of recorded observations. It is a necessary component of validity (the strength of the connection betwee...|$|E
2500|$|The {{purpose of}} testing {{is to achieve}} {{organizational}} acceptance that the solution satisfies the recovery requirements. Plans may fail to meet expectations due to insufficient or inaccurate recovery requirements, solution design flaws or solution implementation <b>errors.</b> <b>Testing</b> may include: ...|$|R
40|$|In this study, a new {{empirical}} model is proposed for estimating daily {{global solar radiation}} on a horizontal surface by {{the day of the}} year. The performance of the proposed model is validated by comparing with three trigonometric correlations at nine representative stations of China using statistical <b>error</b> <b>tests</b> such as the mean absolute percentage error (MAPE), mean absolute bias error (MABE), root mean square error (RMSE) and correlation coefficients (r). The results show that the new model provides better estimation and has good adaptability to highly variable weather conditions. Then the application of the methodology is performed for the other 70 meteorological stations across China. Global solar radiation Empirical model Statistical <b>error</b> <b>test</b> China...|$|R
40|$|In {{this paper}} we present the {{architecture}} of an intelligent <b>test</b> <b>error</b> detection agent that is able to independently supervise the test process. By means of rationally applied bin and cause specific retests it should detect and correct the majority of <b>test</b> <b>errors</b> with minimal additional test effort. To achieve this, the agent utilizes <b>test</b> <b>error</b> models learned from historical example data to rate single wafer runs. The resulting run specific <b>test</b> <b>error</b> hypotheses are sequentially combined with information gained from regular and ordered retests in order to infer and update a global <b>test</b> <b>error</b> hypothesis. Based on this global hypothesis the agent decides if a <b>test</b> <b>error</b> exists, what its most probable cause is and which bins are affected. Consequently, {{it is able to}} initiate proper retests to check the inferred hypothesis and if necessary correct the affected test runs. The paper includes a description of the general architecture and discussions about possible <b>test</b> <b>error</b> models, the inference approach to generate the <b>test</b> <b>error</b> hypotheses from the given information and a possible set of rules to act upon the inferred hypothesis...|$|R
40|$|AbstractData for {{calibration}} and out-of-sample <b>error</b> <b>testing</b> of {{option pricing}} models are provided alongside {{data obtained from}} optimization procedures in "On calibration of stochastic and fractional stochastic volatility models" [1]. Firstly we describe testing data sets, further calibration data obtained from combined optimizers is visually depicted – interactive 3 d bar plots are provided. The data is suitable for a further comparison of other optimization routines and also to benchmark different pricing models...|$|E
40|$|Previous {{work has}} shown that it is {{feasible}} to implement a fully digital test evaluation function to realise partial self-test on an automatic gain control circuit (AGC). This paper extends the technique to INL, DNL, offset & gain <b>error</b> <b>testing</b> of analogue to digital converters (ADC's). It also shows how the same function {{can be used to}} test an AGC / ADC pair. An extension to full self-test is also proposed by the on-chip generation of input stimuli through reconfiguration of existing functions...|$|E
40|$|The report {{presents}} {{a series of}} numerical experiments concerning application of Support Vector Machines for the two class spatial data classification. The main {{attention is paid to}} the variability of the results by changing hyperparameters: bandwidth of the radial basis function kernel and C parameter. Training <b>error,</b> <b>testing</b> error and number of support vectors are plotted against hyperparameters. Number of support vectors is minimal at the optimal solution. Two real case studies are considered: Cd contamination in the Leman Lake, Briansk region radionuclides soil contamination. Structural analysis (variography) is used for the description of the spatial patterns obtained and to monitor the performance of SVM...|$|E
40|$|Bergstrom {{showed that}} a {{necessary}} condition for a Pareto optimum with non-paternalistic altruism is classification as a selfish Pareto optimum. This paper shows that Bergstrom’s result does not generalize to the benefit-cost analysis of generic changes in public goods. There may exist good projects that will be rejected by a selfish-benefit cost test, a selfish <b>test</b> <b>error.</b> Selfish <b>test</b> <b>error</b> is linked to preference interdependence between public goods and income distribution, the same condition Musgrave identified as problematic for optimal public goods provision without altruism. Transferable selfish utilities provide freedom from selfish <b>test</b> <b>error...</b>|$|R
40|$|The {{rolling bearing}} {{friction}} torque is forecasted {{by means of}} the grey dynamic model GM (1, 1). Residual <b>test</b> and posteriori <b>error</b> <b>test</b> are conducted to verify the reliability of the results of prediction. The experiment shows that the method proposed has the high precision and satisfy the engineering demand...|$|R
40|$|An L 2 error {{between an}} {{estimated}} function and a true function {{can be a}} good means of checking lack of fit of a regression model. Kuchibhatla and Hart (1995) propose an L 2 <b>error</b> <b>test</b> based on Fourier series whose order is selected by an AIC type criterion. We call this test the KH test. When the sample size is small and the true function has high frequency behavior, the AIC type criterion often fails in choosing the correct order of a truncated Fourier series regression estimator. This failure results in low power for the KH test. Fan (1996) proposes L 2 <b>error</b> <b>tests</b> based on a threshold estimator. These tests do not perform well for low frequency alternatives in comparison to the KH test. As a compromise between the two types of tests and {{to make up for the}} failure of the AIC type criterion, we propose a new test which consists of a sum of L 2 error of a truncated estimator (KH test) and L 2 error of a threshold estimator for higher order Fourier coefficients. The proposed test has similar power to the KH test for low frequency alternatives, and better power than the KH test for high frequency alternatives. Fourier series estimator L 2 <b>error</b> <b>test</b> Neyman smooth test Thresholding test AIC type criterion Portmanteau test...|$|R
40|$|This Master’s Thesis is {{dedicated}} to the investigation and testing conventional and nonconventional Kramers-Kronig relations on simulated and experimentally measured spectra. It is done for both linear and nonlinear optical spectral data. Big part of attention is paid to the new method of obtaining complex refractive index from a transmittance spectrum without direct information of the sample thickness. The latter method is coupled with terahertz tome-domain spectroscopy and Kramers-Kronig analysis applied for testing the validity of complex refractive index. In this research precision of data inversion is evaluated by root-mean square <b>error.</b> <b>Testing</b> of methods is made over different spectral range and implementation of this methods in future is considered...|$|E
30|$|The {{history of}} the study of the machine tool thermal error is close to a century long. There is still no {{solution}} to the thermal error problem with modern high precision CNC machine tools. Most research about the machine tool thermal error has focused on establishing the relationship between the temperature field and thermal error of machine tools, but no solutions have presented themselves well in industry application. Since there have been no new technological breakthroughs in the experimental studies on the thermal error, traditional electrical testing and laser measurement technology are commonly used. The research objects in thermal <b>error</b> <b>testing</b> are usually small and medium-sized CNC machine tools. There is relatively less research on heavy-duty CNC machine tools.|$|E
40|$|Based on the {{electric}} power consumption data in 2001 - 2010, this paper discusses GM (1, 1) model and its improved model {{in the application of}} power consumption forecasting. Due to the traditional Grey Model itself has certain defects, we grouped the original sequence according to the degree of deviation first, and then combined with nonlinear GM (1, 1, α) to improve the traditional GM (1, 1) model. Through the relative <b>error</b> <b>testing</b> and the posterior testing, this paper made a comparative analysis to the traditional GM (1, 1) model and the improved GM. Example of Beijing shows that the improved model had good accuracy; it had a good application value in the actual prediction system...|$|E
40|$|Given n-copies {{of unknown}} bipartite (possiblly mixed) state, our {{task is to}} test whether the state is a pure state of not. Allowed to use the global operations, optimal one-sided <b>error</b> <b>test</b> is the {{projection}} onto the symmetric subspace, obviously. Is it possible to approximate the globally optimal measurement by LOCC when n is large...|$|R
40|$|FIGURE 16. Relationships of {{the tribe}} Trypetini {{inferred}} from neighbor-joining tree based on Kimura two parameter distances (1159 bp after gaps and sites with missing data removed). The first number is the Pc value from the standard <b>error</b> <b>test</b> (higher than 90 %), and the second number is the Pb from the bootstrap test (2000 replications) ...|$|R
40|$|New {{algorithms}} for parallel one-dimensional globally adaptive quadrature are developed. The algorithms {{are implemented}} on a Kendall Square Research KSR- 1 parallel computer and numerical results are presented. The most successful algorithm gives significant speedups {{on a range}} of hard problems, including ones with singular integrands. Keywords: Quadrature, numerical integration, parallel algorithm, globally adaptive algorithm. 1 Introduction We consider the problem of approximating, on a parallel computer, the definite integral I = Z b a f(x) dx; to some given absolute accuracy ffl. We restrict our attention to the finite interval [a; b], although much of our work is applicable also to a semi-infinite or infinite range of integration. Further we assume that the accuracy requirement is an absolute <b>error</b> <b>test,</b> noting that it would be straightforward to incorporate a relative <b>error</b> <b>test.</b> The conventional approach to this problem is to use a quadrature rule (or more correctly a sequence [...] ...|$|R
40|$|Mass {{spectrometry}} {{coupled with}} gas chromatography (GC/ MS) {{is a powerful}} analytical tool combining analytical sen-sitivity and specificity. Its successful application to toxicol-ogy and in-born <b>error</b> <b>testing</b> is well known. In the current healthcare environment, clinical laboratories must identify means to reduce costs and improve productivity, including workstation consolidation (1). This applies equally well to the utilization of specialized tools such as GC/MS, {{and the ability to}} maximize the productivity of a GC/MS set-up should enhance its economic value to the laboratory. We have had in use in our laboratory a GC/MS method for the measurement of methylmalonic acid (MMA) in plasma, based on solid phase anion-exchange extraction, derivatization with cyclohexanol, and separation with quan-tification using GC/MS (2). Gabapentin (Neurontin) is...|$|E
40|$|Despite {{the fact}} that {{numerous}} Christian denominations in America condemn or condone the death penalty, extant research {{on the effects of}} religiosity on citizens' support for capital punishment has generated ambiguous results of denominational affiliation. This empirical ambiguity {{may be the result of}} measurement <b>error.</b> <b>Testing</b> data from the General Social Survey, this study employs a historically and theologically grounded measure of religious tradition affiliation to contrast to past research. Controlling for religious beliefs, religious behaviors, and race, the results indicate that affiliation with any Christian denomination increases the likelihood that an individual will support the death penalty compared to nonreligious individuals. In contrast, members of different Christian religious traditions are no more or less likely to favor capital punishment than other Christian affiliates. ...|$|E
40|$|In {{a two-way}} {{deterministic}} {{quantum key distribution}} (DQKD) protocol, Bob randomly prepares qubits in one of four states and sends them to Alice. To encode a bit, Alice performs an operation on each received qubit and returns it to Bob. Bob then measures the backward qubits to learn about Alice's operations and hence the key bits. Recently, we proved the unconditional security of the final key of this protocol in the ideal device setting. In this paper, we prove that two-way DQKD protocols are immune to all detector side channel attacks at Bob's side, while we assume ideal detectors at Alice's side for <b>error</b> <b>testing.</b> Our result represents a step forward in making DQKD protocols secure against general detector side channel attacks. Comment: Published versio...|$|E
50|$|Simulates the Cosmic ray induced neutron field. Designed for Single Event Effects/Soft <b>Error</b> Rate <b>testing.</b>|$|R
5000|$|Even {{the models}} with very high climate {{sensitivity}} {{were found to}} be [...] "as realistic as other state-of-the-art climate models". The test of realism was done with a root mean square <b>error</b> <b>test.</b> This does not check on realism of seasonal changes and it is possible that more diagnostic measures may place stronger constraints on what is realistic. Improved realism tests are being developed.|$|R
30|$|In this study, {{the results}} are {{compared}} {{on the basis of}} the nine above-mentioned statistical <b>error</b> <b>tests,</b> and the accuracy of the estimated data of the databases is determined using these tests. For better data modeling, the ideal values of statistical tests such as MPE, MAPE, MBE, MABE, RMSE, tsta, NSE, and e(%) should be closer to zero, but R 2 should approach 1 as closely as possible [25].|$|R
