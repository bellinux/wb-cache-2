1394|1618|Public
25|$|Process {{activities}} are knowledge and collaborative activities that result due to organizational context such as errors/rework, manual <b>data</b> <b>transformation,</b> stress, politics, etc.|$|E
25|$|During {{the design}} phase of Unix, programmers decided to model every {{high-level}} device as a file, {{because they believed}} the purpose of computation was <b>data</b> <b>transformation.</b>|$|E
25|$|APL is {{well known}} for its use of a set of non-ASCII symbols, which are an {{extension}} of traditional arithmetic and algebraic notation. Having single character names for SIMD vector functions is one way that APL enables compact formulation of algorithms for <b>data</b> <b>transformation</b> such as computing Conway's Game of Life in one line of code. In nearly all versions of APL, it is theoretically possible to express any computable function in one expression, that is, in one line of code.|$|E
30|$|Using {{simultaneously}} {{input and}} output <b>data</b> <b>transformations</b> with four <b>data</b> buses which reduce <b>data</b> <b>transformations</b> time by 75 %.|$|R
40|$|Relational Database Systems often support {{activities}} like data warehousing, cleaning and integration. All these activities require performing {{some sort of}} <b>data</b> <b>transformations.</b> Since <b>data</b> often resides on relational databases, <b>data</b> <b>transformations</b> are often specified using SQL, which is based of relational algebra. However, many useful <b>data</b> <b>transformations</b> cannot be expressed as SQL queries due to limited expressive power of relational algebra. In particular, an important class of <b>data</b> <b>transformations</b> that produces several output tuples for a single input tuple cannot be expressed in that way. In this report, we analyze alternatives to process one-to-many <b>data</b> <b>transformations</b> using Relational Database Systems, and compare {{them in terms of}} expressiveness, optimizability and performanc...|$|R
50|$|E-LT Architecture:Traditional ETL tools perform complex <b>data</b> <b>transformations</b> using proprietary, middle-tier ETL engines. Instead, Sunopsis {{uses the}} E-LT (Extract - Load & Transform) approach, wherein all <b>data</b> <b>transformations</b> are {{executed}} by the existing RDBMS engine(s).|$|R
25|$|Innovative Routines International (IRI), Inc. is an American {{software}} company first known for bringing mainframe sort merge functionality into open systems. IRI {{was the first}} vendor to develop a commercial replacement for the Unix sort command, and combine <b>data</b> <b>transformation</b> and reporting in Unix batch processing environments. In 2007, IRI's coroutine sort ("CoSort") became the first product to collate and convert multi-gigabyte XML and LDIF files, join and lookup across multiple files, and apply role-based data privacy functions (including AES-256 encryption) for fields within sensitive files.|$|E
2500|$|... <b>data</b> <b>transformation</b> (incorporating aspects such as data {{normalization}} and data analysis) – for example {{principal components analysis}} dimensionality reduction, mean calculation ...|$|E
2500|$|Variance-stabilizing transformation: When a {{variable}} is Poisson distributed, its square root is approximately normally distributed with expected value of about [...] and variance of about 1/4. Under this transformation, the convergence to normality (as λ increases) is far {{faster than the}} untransformed variable. Other, slightly more complicated, variance stabilizing transformations are available, {{one of which is}} Anscombe transform. See <b>Data</b> <b>transformation</b> (statistics) for more general uses of transformations.|$|E
40|$|This paper {{addresses}} {{several issues}} associated with distribution and interpolation of time series, including model selection and various <b>data</b> <b>transformations.</b> Monte Carlo experiments are performed, which suggest that failure to account for these <b>data</b> <b>transformations</b> may lead to serious errors in estimation...|$|R
40|$|Shared {{data models}} provide {{leverage}} for reusable <b>data</b> <b>transformations.</b> Common modelling patterns and data structures can make <b>data</b> <b>transformations</b> applicable to diverse datasets. Similarly to data models, reusable <b>data</b> <b>transformations</b> promote separation of concerns, prevent duplication of effort, {{and reduce the}} time spent processing data. However, unlike data models, which can be shared as RDF vocabularies or ontologies, there is no well-established way of sharing <b>data</b> <b>transformations.</b> We propose a way to share <b>data</b> <b>transformations</b> as 'pipeline fragments' for LinkedPipes ETL (LP-ETL), which is an RDF-based data processing tool focused on RDF data. We describe the features of LP-ETL that enable development of reusable transformations as pipeline fragments. Pipeline fragments are represented in RDF as JSON-LD files that can be shared directly or via dereferenceable IRIs. We demonstrate the use of pipeline fragments on <b>data</b> <b>transformations</b> for fiscal <b>data</b> described by the Data Cube Vocabulary (DCV). We cover both generic transformations for any DCV-compliant data, such as DCV validation or DCV to CSV conversion, and transformations specific for the fiscal data used in the OpenBudgets. eu (OBEU) project, including conversion of Fiscal Data Package to RDF or normalization of monetary values. The applicability of these transformations is shown on concrete use cases serving {{the goals of the}} OBEU project...|$|R
40|$|Abstract Visualization {{methods are}} used to {{discover}} simplest <b>data</b> <b>transformations</b> implemented by constructive neural networks, revealing hidden data structures. In this way meta-learning, based on search for simplest models {{in the space of}} all <b>data</b> <b>transformations,</b> is facilitated. Key words: Meta-learning, constructive neural networks, projection pursuit, visualization...|$|R
2500|$|Most ESB {{implementations}} {{contain a}} facility called [...] "mediation". For example, mediation flows {{are part of}} the WebSphere enterprise service bus intercept. Mule also supports mediation flows. Mediation flows modify messages that are passed between existing services and clients that use those services. A mediation flow mediates or intervenes to provide functions, such as message logging, <b>data</b> <b>transformation,</b> and routing, typically the functions can be implemented using the Interception Design Pattern.|$|E
5000|$|... #Subtitle level 4: <b>Data</b> <b>transformation,</b> mapping, and {{translation}} ...|$|E
5000|$|<b>Data</b> <b>transformation</b> or data {{mediation}} {{between a}} data source and a destination ...|$|E
5000|$|Data sampling, binning, discretization, {{and other}} <b>data</b> <b>transformations.</b>|$|R
5000|$|<b>Data</b> Transformations: <b>transformations</b> {{allow for}} the mapping of user data into a more {{desirable}} form {{to be used by}} the mining model. PMML defines several kinds of simple <b>data</b> <b>transformations.</b>|$|R
40|$|Optimizing array accesses is {{extremely}} critical in embedded computing as many embedded applications {{make use of}} arrays (in form of images, video frames, etc). Previous research considered both loop and <b>data</b> <b>transformations</b> for improving array accesses. However, <b>data</b> <b>transformations</b> considered were mostly limited to linear <b>data</b> <b>transformations</b> and array interleaving. In this paper, we introduce two data transformations: array decomposition (breaking up a large array into multiple smaller arrays) and array composition (combining multiple small arrays into a single large array). This paper discusses that it is feasible to implement these optimizations within an optimizing compiler. 1...|$|R
5000|$|AWK - one of {{the oldest}} and popular textual <b>data</b> <b>transformation</b> language; ...|$|E
5000|$|Bidji is an Apache Ant {{project for}} code {{generation}} and <b>data</b> <b>transformation.</b>|$|E
5000|$|TXL - {{prototyping}} language-based descriptions, {{used for}} source code or <b>data</b> <b>transformation.</b>|$|E
40|$|This {{paper is}} {{concerned}} with integrating global <b>data</b> <b>transformations</b> and local loop transformations in order to minimise overhead on distributed shared memory machines such as the SGi Origin 2000. By first developing an extended algebraic transformation framework, a new technique to allow the static application of global <b>data</b> <b>transformations,</b> such as partitioning, to reshaped arrays is presented, {{eliminating the need for}} expensive temporary copies and hence eliminating any communication and synchronisation. In addition, by integrating loop and <b>data</b> <b>transformations,</b> any introduced poor spatial locality and expensive array subscripts can be eliminated. A specific performance improving algorithm is implemented giving significant improvements in execution time...|$|R
40|$|<b>Data</b> <b>transformations</b> are {{fundamental}} operations in legacy data migration, data integration, data cleaning, and data warehousing. These operations are often implemented as relational queries that aim at leveraging the optimization capabilities of most DBMSs. However, relational query languages like SQL are not expressive enough to specify one-to-many <b>data</b> <b>transformations,</b> an important class of <b>data</b> <b>transformations</b> that produce several output tuples {{for a single}} input tuple. These transformations are required for solving several types of data heterogeneities, like those that occur when the source data represents aggregations of the target data. This thesis proposes a new relational operator, named data mapper, as an extension to the relational algebra to address one-to-many <b>data</b> <b>transformations</b> and focus on its optimization. It also provides algebraic rewriting rules and execution algorithms for the logical and physical optimization, respectively. As a result, queries may be expressed as a combination of standard relational operators and mappers. The proposed optimizations have been experimentally validated and the key factors that influence the obtained performance gains identified...|$|R
40|$|<b>Data</b> <b>transformations</b> are {{commonly}} used tools that can serve many functions in quantitative [...] analysis of data. The goal {{of this paper is}} to focus on the use of three <b>data</b> <b>transformations</b> most [...] commonly discussed in statistics texts (square root, log, and inverse) for improving the [...] normality of variables. While these are important options for analysts, they do fundamentally [...] transform the nature of the variable, making the interpretation of the results somewhat more [...] complex. Further, few (if any) statistical texts discuss the tremendous influence a distribution's [...] minimum value has on the efficacy of a transformation. The goal {{of this paper is to}} promote [...] thoughtful and informed use of <b>data</b> <b>transformations...</b>|$|R
5000|$|Tasks: A task is {{an atomic}} work unit that {{performs}} some action. There {{are a couple}} of dozen tasks that ship in the box, ranging from the file system task (that can copy or move files) to the <b>data</b> <b>transformation</b> task. The <b>data</b> <b>transformation</b> task actually copies data; it implements the ETL features of the product ...|$|E
50|$|<b>Data</b> <b>transformation</b> through matrix decomposition: DAAL {{provides}} Cholesky, QR, and SVD decomposition algorithms.|$|E
5000|$|... ° Data Staging Component 1. Data Extraction 2. <b>Data</b> <b>Transformation</b> 3. Data Loading ...|$|E
40|$|International audienceWhen {{developing}} <b>data</b> <b>transformations</b> [...] -a task omnipresent in applications like data integration, data migration, data cleaning, {{or scientific}} data processing [...] -developers quickly face {{the need to}} verify the semantic correctness of the transformation. Declarative specifications of <b>data</b> <b>transformations,</b> e. g., SQL or ETL tools, increase developer productivity but usually provide limited or no means for inspection or debugging. In this situation, developers today {{have no choice but}} to manually analyze the transformation and, in case of an error, to (repeatedly) fix and test the transformation. The goal of the Nautilus project is to semi-automatically support this analysis-fix-test cycle. This demonstration focuses on one main component of Nautilus, namely the Nautilus Analyzer that helps developers in understanding and debugging their <b>data</b> <b>transformations.</b> The demonstration will show the capabilities of this component for <b>data</b> <b>transformations</b> specified in SQL on scenarios from different domains that are based on real-world data. We provide an overview the Nautilus Analyzer, discuss components and implementation techniques, and outline our demonstration plan. The Nautilus website (...|$|R
40|$|In our {{previous}} work, we proposed wavelet shrinkage estimation (WSE) for nonhomogeneous Poisson process (NHPP) -based software reliability models (SRMs), where WSE is a data-transform-based nonparametric estimation method. Among many variance-stabilizing <b>data</b> <b>transformations,</b> the Anscombe transform and the Fisz transform were employed. We {{have shown that}} it could provide higher goodness-of-fit performance than the conventional maximum likelihood estimation (MLE) and the least squares estimation (LSE) in many cases, {{in spite of its}} non-parametric nature, through numerical experiments with real software-fault count data. With the aim of improving the estimation accuracy of WSE, in this paper we introduce other three <b>data</b> <b>transformations</b> to preprocess the software-fault count data and investigate the influence of different <b>data</b> <b>transformations</b> to the estimation accuracy of WSE through goodness-of-fit test...|$|R
5000|$|<b>Data</b> <b>transformations</b> {{are often}} used to convert an {{equation}} into a linear form. For example, the Cobb-Douglas function—often used in economics—is nonlinear: ...|$|R
50|$|The Bidji {{project is}} a set of tools for <b>data</b> <b>transformation</b> and code generation.|$|E
5000|$|OpenRefine - a {{standalone}} {{open source}} desktop application for data clean-up and <b>data</b> <b>transformation</b> ...|$|E
5000|$|XSLT - the {{standard}} XML <b>data</b> <b>transformation</b> language (suitable by XQuery in many applications); ...|$|E
40|$|Abstract. Application {{scenarios}} such as legacy-data migration, ETL processes, {{data cleaning}} and data-integration require {{the transformation of}} input tuples into output tuples. Traditional approaches for implementing these <b>data</b> <b>transformations</b> enclose solutions as Persistent Stored Modules (PSM) executed by an RDBMS or transformation code using a commercial ETL tool. Neither of these solutions is easily maintainable or optimizable. To {{take advantage of the}} optimization capabilities of RDBMSs, <b>data</b> <b>transformations</b> are often expressed as relational queries. However, the limited expressive power of relational query languages like SQL hinder this approach. In particular, an important class of <b>data</b> <b>transformations</b> that produce several output tuples for a single input tuple cannot be expressed as a relational query. In this paper, we present the formal definition of a new operator named data mapper operator as an extension to the relational algebra to address this important class of <b>data</b> <b>transformations.</b> We demonstrate that relational algebra extended with the mapper operator is more expressive than standard relational algebra. Furthermore, we investigate several properties of the operator and supply a set of algebraic rewriting rules that enable the logical optimization of expressions that combine standard relational operators with mappers and present their proofs of correctness. 1...|$|R
40|$|Abstract. The paper {{provides}} {{a brief overview}} of the HyperMeData language specifically designed to support data interchange among heterogeneous information systems, and pays attention to meta-level transformation descriptions. The language is sufficiently complex and powerful to catch both intra- and inter-data schema relationships (i. e., to describe both data schemas and <b>data</b> <b>transformations),</b> nonetheless, its routine use requires employing a set of metalevel transformation rules to handle typical schematic differences among semantically similar database objects. The classification of schematic heterogeneities in multidatabases is used as the basis for proposing inter-attribute correspondences (meta-level transformation rules) and the respective translations to HyperMeData descriptions of <b>data</b> <b>transformations</b> (transformation rules). ...|$|R
5000|$|Migrations can {{be written}} in SQL (database-specific syntax (such as PL/SQL, T-SQL, ...) is supported) or Java (for {{advanced}} <b>data</b> <b>transformations</b> or dealing with LOBs).|$|R
