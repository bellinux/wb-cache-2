0|48|Public
50|$|The {{earliest}} EMC {{issue was}} lightning strike (lightning electromagnetic pulse, or LEMP) on buildings. Lightning rods or lightning conductors {{began to appear}} in the mid-18th century. With the advent of widespread electricity generation and power supply lines from the late 19th century on, problems also arose with equipment short-circuit failure affecting the power supply, and with local fire and shock hazard when the power line was struck by lightning. Power stations were provided with output circuit breakers. Buildings and appliances would soon be provided with <b>input</b> <b>fuses,</b> and later in the 20th century miniature circuit breakers (MCB) would come into use.|$|R
30|$|As {{shown in}} Fig.  1, several column ELM-AEs are {{performed}} on the same <b>input</b> data and <b>fused</b> to form the multi-column ELM-AE (MC-ELM-AE) algorithm. Since multi-column features are generated from MC-ELM-AE, the feature concatenation is {{the simplest way to}} fuse these features.|$|R
40|$|In {{this paper}} {{the use of}} a B-spline neuro-fuzzy model for {{different}} tasks such as vision-based fine-positioning using uncalibrated cameras and force control is presented. It is shown that neuro-fuzzy controllers can be used not only for low-dimensional problems like force control but also for high-dimensional problems like vision-based sensorimotor control and for <b>fusing</b> <b>input</b> from different sensors. Con-trollers of this type can be modularly combined to solve a given assembly problem. ...|$|R
40|$|Abstract. In this paper, a new metric for {{evaluating}} {{the performance of the}} combinative pixel-level image fusion is defined based on an image feature measurement, i. e. phase congruency and its moments, which provide an absolute measurement of image features. By comparing the local cross-correlation of corresponding feature maps of <b>input</b> images and <b>fused</b> output, the quality of the fused result is assessed without a reference. The experimental results on multi-focused image pairs demonstrate the efficiency of the proposed approach...|$|R
40|$|Abstract. In this paper, we {{describe}} a near-complete Pocket PC imple-mentation of a Mobile Multi-Modal Interaction (M 3 I) platform for pe-destrian navigation. The platform {{is designed to}} easily support indoor and outdoor navigation tasks, and uses the combination of several mo-dalities for presentation output and user input. Whereas 2 D/ 3 D-graphics and synthesized speech are used to present useful information on routes and places, <b>fused</b> <b>input</b> from embedded speech and gesture recognition engines allow for situated user interaction. ...|$|R
40|$|In a {{world that}} is {{becoming}} increasingly dependent on In- ternet communication, Denial of Service (DoS) attacks have evolved into a major security threat which is easy to launch but difficult to defend against. In order for DoS countermea- sures to be effective, the attack must be detected early and accurately. In this paper we propose a DoS detection tech- nique based on observation of the incoming traffic and a com- bination of traditional likelihood estimation with a recurrent random neural network (r-RNN) structure. We select input features that describe essential information on the incoming traffic and evaluate the likelihood ratios for each <b>input,</b> to <b>fuse</b> them with a r-RNN. We evaluate the performance of our method in terms of false alarm and correct detection rates with experiments on a large networking testbed, for a variety of input traffic...|$|R
40|$|NOTE: Text or symbols not renderable {{in plain}} ASCII are {{indicated}} by [ [...] . ]. Abstract is included in. pdf document. Memory {{is a key}} component of intelligence. In the human brain, physical structure and functionality jointly provide diverse memory modalities at multiple time scales. How could we engineer artificial memories with similar faculties? In this thesis, we attack both hardware and algorithmic aspects of this problem. A good part is devoted to holographic memory architectures, because they meet high capacity and parallelism requirements. We develop and fully characterize shift multiplexing, a novel storage method that simplifies disk head design for holographic disks. We develop and optimize the design of compact refreshable holographic random access memories, showing several ways that 1 Tbit can be stored holographically in volume less than 1 [ [...] . ], with surface density more than 20 times higher than conventional silicon DRAM integrated circuits. To address the issue of photorefractive volatility, we further develop the two-lambda (dual wavelength) method for shift multiplexing, and combine electrical fixing with angle multiplexing to demonstrate 1, 000 multiplexed fixed holograms. Finally, we propose a noise model and an information theoretic metric to optimize the imaging system of a holographic memory, in terms of storage density and error rate. Motivated by the problem of interfacing sensors and memories to a complex system with limited computational resources, we construct a computer game of Desert Survival, built as a high-dimensional non-stationary virtual environment in a competitive setting. The efficacy of episodic learning, implemented as a reinforced Nearest Neighbor scheme, and the probability of winning against a control opponent improve significantly by concentrating the algorithmic effort to the virtual desert neighborhood that emerges as most significant at any time. The generalized computational model combines the autonomous neural network and von Neumann paradigms through a compact, dynamic central representation, which contains the most salient features of the sensory <b>inputs,</b> <b>fused</b> with relevant recollections, reminiscent of the hypothesized cognitive function of awareness. The Declarative Memory is searched both by content and address, suggesting a holographic implementation. The proposed computer architecture may lead to a novel paradigm that solves "hard" cognitive problems at low cost. ...|$|R
40|$|Auditory speech {{perception}} can {{be altered}} by concurrent visual information. The superior temporal cortex is an important combining site for this integration process. This area was previously found {{to be sensitive to}} audiovisual congruency. However, the direction of this congruency effect (i. e., stronger or weaker activity for congruent compared to incongruent stimulation) has been more equivocal. Here, we used fMRI to look at the neural responses of human participants during the McGurk illusion?in which auditory /aba/ and visual /aga/ <b>inputs</b> are <b>fused</b> to perceived /ada/?in a large homogenous sample of participants who consistently experienced this illusion. This enabled us to compare the neuronal responses during congruent audiovisual stimulation with incongruent audiovisual stimulation leading to the McGurk illusion while avoiding the possible confounding factor of sensory surprise that can occur when McGurk stimuli are only occasionally perceived. We found larger activity for congruent audiovisual stimuli than for incongruent (McGurk) stimuli in bilateral superior temporal cortex, extending into the primary auditory cortex. This finding suggests that superior temporal cortex prefers when auditory and visual input support the same representation...|$|R
40|$|This work {{describes}} {{a method of}} 3 D model reconstruction from images that takes provided disparity maps as an <b>input</b> and <b>fuses</b> them into a consistent 3 D model. Camera parameters are re-estimated in the procedure as well. Occlusion boundary artifacts are reduced and holes in disparity maps interpolated over. The result of stereo matching is a disparity map created from correspondences between images. Every map holds information only from the overlapping regions of source image pair. If all maps from the view of one camera are compared, it shows that for some points in the image the values from different pairs are not consistent. This is caused by noise in the images, inaccurately estimated parameters of the cameras and errors in the stereo algorithm. The goal of this work is {{to make use of}} the information fusion and find the optimal depth maps. These depth maps are chosen as the representation of surface for the output 3 D photography. On real-world scenes is shown that this representation outperforms the currently used “fish-scales ” in continuous and more detailed surface...|$|R
30|$|Image fusion {{generally}} involves {{selecting the}} most informative areas {{from the source}} images and blending these local areas to get the fused output images. Among the various methods of image fusion, multi-resolution (MR)-based approaches are widely used in practice. The MR-based image fusion techniques are motivated {{by the fact that}} the HVS is more sensitive to local contrast changes (such as edges) and MR decompositions provide convenient space-scale localization of these changes. A generic MR fusion scheme uses fusion rules to construct a composite MR representation from the MR representations of the different <b>input</b> images. The <b>fused</b> image is constructed by applying an inverse decomposition.|$|R
3000|$|... [...]) (Liu and Laganiere 2007) {{are widely}} used in {{evaluating}} the performance of fusion methods. A fused image with maximum number of measures achieving their desirable value {{is considered to be}} a better quality of fused image. Many objective measures have been developed in literature for assessing the performance of image fusion algorithms but none of the measure has been considered as a standard measure. The main reason of not defining a proper quality measure for image fusion is the difficulty in defining an ideal fused image. The measures generally used for evaluating the performance of fusion algorithms are based on the amount of information that has been transferred from the <b>input</b> images into <b>fused</b> image.|$|R
30|$|The {{performance}} and visual quality of image is retained using discrete cosine harmonic wavelet (DCHWT) based image fusion with reduced computation (Kumar 2013). A fused image with {{maximum number of}} measures achieving their desirable value {{is considered to be}} a better quality of fused image. Many objective measures have been developed in literature for assessing the performance of image fusion algorithms. The measures generally used for evaluating the performance of fusion algorithms are based on the amount of information that has been transferred from the <b>input</b> images into <b>fused</b> image (Kotwal and Chaudhuri 2013; Haghighat et al. 2011; Arathi and Soman 2009; Wang et al. 2004; Zhang et al. 2011; Liu and Laganiere 2007).|$|R
40|$|Image Fusion is a {{technique}} that integrates complementary information from multiple images such that the new images {{are more suitable for}} processing tasks. Image fusion combines perfectly registered images toproduce a high quality fused image with spatial and spectral information. It integrates complementary information to give a better visual picture of a scenario, suitable for processing. Image Fusion produces a single image from a set of <b>input</b> images. The <b>fused</b> image has more complete information which is useful for human or machine perception. The fused image with such rich information will improve the performance of image analysis algorithms. In this paper, we propose wavelet based image fusion using pixel based maximum selection rule algorithm...|$|R
40|$|Animals use {{information}} from multiple sensory organs to generate appropriate behavior. Exactly how these different sensory <b>inputs</b> are <b>fused</b> at the motor {{system is not}} well understood. Here we study how fly neck motor neurons integrate information from two well characterized sensory systems: visual information from the compound eye and gyroscopic information from the mechanosensory halteres. Extracellular recordings reveal that a subpopulation of neck motor neurons display "gating-like" behavior: they do not fire action potentials in response to visual stimuli alone but will do so if the halteres are coactivated. Intracellular recordings show that these motor neurons receive small, sustained subthreshold visual inputs in addition to larger inputs that are phase locked to haltere movements. Our {{results suggest that the}} nonlinear gating-like effect results from summation of these two inputs with the action potential threshold providing the nonlinearity. As a result of this summation, the sustained visual depolarization is transformed into a temporally structured train of action potentials synchronized to the haltere beating movements. This simple mechanism efficiently fuses two different sensory signals and may also explain the context-dependent effects of visual inputs on fly behavior. ...|$|R
40|$|Likely {{outcomes}} of a collision between two objects are annihilation, reflection or fusion. We show how {{to construct a}} one-bit adder with pattern that fuse on impact. A fusion gate has two inputs and three outputs. When a signal is generated on a single input the object propagates along its own output trajectory. When both inputs are active the objects collide at a junction of <b>input</b> trajectories, <b>fuse</b> and propagate along dedicated output trajectory. Thus two outputs produce conjunction of one signal with negation of another signal; and, third output produces conjunction of input signals. By merging two outputs in one we make a one-bit half adder: one output is the conjunction of input signals, another output is the exclusive disjunction of the signals. We discuss blue-prints of the half-adders realised with two types of physical signal careers [...] - wave-fragments in excitable medium and high-velocity jet streams. We also propose an electrical circuits analogous of a fusion half-adder. By running fusion half-adders in reverse we find that, despite realising the same functions when in a straight mode, all devices implement different functions when their inputs swapped with outputs...|$|R
40|$|The human senses|evolved in {{primitive}} times {{primarily for}} survival|serve modern man as exquisitely-developed channels for communication and information exchange. Because the sensory modalities are highly learned and natural, {{we seek to}} endow machines with the ability tocommunicate in these terms. Complex machines can thereby be brought to serve human needs easier and more widely. Sensory realism, similar to face-to-face communication among humans, is the long-range objective. Of the senses, sight and sound have been exploited to the greatest extent for human/machine communication. Technologies for image processing and voice interaction are deploying rapidly. But, understanding of the touch modality is advancing, as tactile and manual interfaces develop. The dimensions of taste and smell are yet to be harnessed broadly. Advocates of Virtual Reality are sure {{to be at the}} forefront of research in this direction, as the search for sensory realism progresses. The human is adept at integrating sensory <b>inputs,</b> and <b>fusing</b> data to meet needs of the moment. Machines, to date, are less able to emulate this ability. This issue is central to current research inmultimedia information systems. But, the human ability to proces...|$|R
40|$|Electrical {{equipment}} {{to be protected}} from overvoltage is connected with a possible source of overvoltage via an <b>input</b> conductor. A <b>fuse</b> is connected in series with the input conductor. A spark gap is connected between the input conductor and ground for conducting the overvoltage current to ground and for blowing the fuse to open the circuit to the electrical equipment. A pulse attenuator network is provided between the spark gap and the electrical {{equipment to}} be protected for attenuating the pulse of energy passing through the fuse and spark gap prior to blowing of the fuse. The pulse attenuator network includes additional shunt spark gaps, series inductance, and a series connection of a twisted shielded pair of conductors having low-voltage insulation...|$|R
40|$|Here, a {{versatile}} data-driven application independent method {{to extend the}} depth of field is presented. The principal contribution in this effort {{is the use of}} features extracted by Empirical Mode Decomposition, namely Intrinsic Mode Images, for fusion. The input images are decomposed into intrinsic mode images and fusion is performed on the extracted oscillatory modes, by means of weighing schemes that allow emphasis of focused regions in each <b>input</b> image. The <b>fused</b> image unifies information from all focal planes, while maintaining the verisimilitude of the scene. In order to validate the fusion performance of our method, we have compared our results with those of region-based and multiscale decomposition based fusion techniques. Several illustrative examples and objective comparisons are provided. 1...|$|R
40|$|The {{relationship}} between thought and language and, in particular, {{the issue of}} whether and how language influences thought is still a matter of fierce debate. Here we consider a discrimination task scenario to study language acquisition in which an agent receives linguistic input from an external teacher, in addition to sensory stimuli from the objects that exemplify the overlapping categories that make up the environment. Sensory and linguistic <b>input</b> signals are <b>fused</b> using the Neural Modeling Fields (NMF) categorization algorithm. We find that the agent with language is capable of differentiating object features that it could not distinguish without language. In this sense, the linguistic stimuli prompt the agent to redefine and refine the discrimination capacity of its sensory channels...|$|R
40|$|The Spectral Edge {{method of}} image fusion <b>fuses</b> <b>input</b> image details, while {{maintaining}} natural color. It is a derivative-based technique, {{based on the}} structure tensor, and lookup-table-based gradient field reconstruction. It has many applications, including RGB-NIR image fusion and remote sensing. In this paper, we propose adding an iterative step to the method. We use the output Spectral Edge image as the putative color image for another fusion step, and repeat this for several iterations. We show that this creates an output image with a structure tensor field closer {{to that of the}} high-dimensional input than the output of the original method. We perform a psychophysical experiment using the iterative Spectral Edge method for RGB-NIR image fusion, which shows that the result of multiple iterations is preferred...|$|R
40|$|This paper {{presents}} our {{techniques used}} and their {{analysis for the}} runs made and the results submitted by the CINDI group for {{the task of the}} image retrieval and automatic annotation of ImageCLEF 2006. For the ah-hoc image retrieval from both the photographic and medical image collections, we have experimented with cross-modal (image and text) interaction and integration approaches based on the relevance feedback in the form of textual query expansion and visual query point movement with adaptive similarity matching functions. Experimental results show that our approaches performed well compared to initial visual or textual only retrieval without any user interactions or feedbacks. We are ranked first and second and achieved the highest MAP score (0. 3850) for the ad-hoc retrieval in the photographic collection (IAPR) among all the submissions. For the automatic annotation tasks for both the medical (IRMA) and object collections (LTU), we have experimented with a classifier combination approach, where several probabilistic multi-class SVM classifiers with features at different levels as <b>inputs</b> are <b>fused</b> with several combination rules to predict the final probability score of each category as image annotation. Analysis {{of the results of the}} different runs we made for both the image retrieval and annotation tasks are reported in this paper...|$|R
40|$|Urban {{areas are}} rapidly {{changing}} {{all over the}} world and therefore provoke the necessity to update urban maps frequently. Remote sensing has been used for many years to monitor these changes. With the availability of multi-sensor, multi-temporal, multi-resolution and multi-frequency image data from operational Earth observation satellites the fusion of digital image data has become a valuable tool in remote sensing image evaluation. Therefore, the goal of an image fusion algorithm is to integrate the redundant and complementary information obtained from the source images in order to form a new image which provides a better description of the scene for human or machine perception. In this paper, an attempt has been made to design an automatic change detection solution. The approach presented here takes advantage of fusion in two levels. That is, Feature-level fusion, which uses attributes (or features) extracted from the raw data as inputs and uses them into new features, or feature map and Decision-level fusion, that takes the decisions from each image as <b>inputs</b> and <b>fuses</b> them to obtain a global decision. The purpose of this paper is to reveal urban changes. Using multitemporal Landsat MSS and Landsat TM images, changes in New Delhi city in India is detected...|$|R
40|$|International audienceIn {{this paper}} we propose a novel detail-enhancing {{exposure}} fusion approach using nonlinear translation-variant filter (NTF). With the captured Standard Dynamic Range (SDR) images under different exposure settings, first the fine details are extracted based on guided filter. Next, the base layers (i. e., images obtained from NTF) across all <b>input</b> images are <b>fused</b> using multiresolution pyramid. Exposure, contrast, and saturation measures are considered to generate a mask that guides the fusion process of the base layers. Finally, the fused base layer is combined with the extracted fine details to obtain detail-enhanced fused image. The goal is to preserve details in both very dark and extremely bright regions without High Dynamic Range Image (HDRI) representation and tone mapping step. Moreover, we have demonstrated that the proposed method is also suitable for the multifocus image fusion without introducing artifacts...|$|R
40|$|Modern IP-based wide-area {{surveillance}} systems often {{build on}} networks of multi-modal, intelligent and mobile sensor units. Detection of complex events is performed on intelligent sensors and <b>fusing</b> <b>input</b> in the sensor units or centralized control room components. The domain of surveillance and public safety creates requirement for robustness and fault-tolerance. This article will present an automated intelligence architecture for mobile surveillance, which provides capabilities for combining on-board event detection in sensor units, centralized decision making on the server side, and automated exploitation of mobile surveillance unit positioning data. This architecture {{must be very}} reliable to provide services {{in the face of}} challenges such as natural disasters and fire, potentially damaging the infrastructure of the surveillance system. To increase its reliability and robustness, we study the introduction of a self-healing system into the architecture and examine the combined system's operation in three case studies...|$|R
40|$|In {{this paper}} we propose a novel detail-enhancing {{exposure}} fusion approach using nonlinear translation-variant filter (NTF). With the captured Standard Dynamic Range (SDR) images under different exposure settings, first the fine details are extracted based on guided filter. Next, the base layers (i. e., images obtained from NTF) across all <b>input</b> images are <b>fused</b> using multiresolution pyramid. Exposure, contrast, and saturation measures are considered to generate a mask that guides the fusion process of the base layers. Finally, the fused base layer is combined with the extracted fine details to obtain detail-enhanced fused image. The goal is to preserve details in both very dark and extremely bright regions without High Dynamic Range Image (HDRI) representation and tone mapping step. Moreover, we have demonstrated that the proposed method is also suitable for the multifocus image fusion without introducing artifacts...|$|R
40|$|In {{this paper}} {{the use of}} a B-spline neuro-fuzzy model for {{different}} tasks such as vision-based fine-positioning using uncalibrated cameras and force control is presented. It is shown that neuro-fuzzy controllers can be used not only for low-dimensional problems like force control but also for high-dimensional problems like vision-based sensorimotor control and for <b>fusing</b> <b>input</b> from different sensors. Controllers of this type can be modularly combined to solve a given assembly problem. 1 Introduction It is well-known that general fuzzy rule descriptions of systems with a large number of input variables suffer from the problem of the "curse of dimensionality. " In many realworld applications it is difficult to identify the decisive input parameters and thus {{to reduce the number of}} input variables to the minimum. A general solution to building fuzzy models is not only interesting from a theoretical point, it may also extend the range of applications of fuzzy control to more complex intel [...] ...|$|R
40|$|Abstract Background Prostate {{cancer is}} the single most {{prevalent}} cancer in US men whose gold standard of diagnosis is histologic assessment of biopsies. Manual assessment of stained tissue of all biopsies limits speed and accuracy in clinical practice and research of prostate cancer diagnosis. We sought to develop a fully-automated multimodal microscopy method to distinguish cancerous from non-cancerous tissue samples. Methods We recorded chemical data from an unstained tissue microarray (TMA) using Fourier transform infrared (FT-IR) spectroscopic imaging. Using pattern recognition, we identified epithelial cells without user <b>input.</b> We <b>fused</b> the cell type information with the corresponding stained images commonly used in clinical practice. Extracted morphological features, optimized by two-stage feature selection method using a minimum-redundancy-maximal-relevance (mRMR) criterion and sequential floating forward selection (SFFS), were applied to classify tissue samples as cancer or non-cancer. Results We achieved high accuracy (area under ROC curve (AUC) > 0. 97) in cross-validations on each of two data sets that were stained under different conditions. When the classifier was trained on one data set and tested on the other data set, an AUC value of ~ 0. 95 was observed. In the absence of IR data, the performance of the same classification system dropped for both data sets and between data sets. Conclusions We were able to achieve very effective fusion of the information from two different images that provide very different types of data with different characteristics. The method is entirely transparent to a user and does not involve any adjustment or decision-making based on spectral data. By combining the IR and optical data, we achieved high accurate classification. </p...|$|R
40|$|Copyright © 2014 Harbinder Singh et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. In this {{paper we propose a}} novel detail-enhancing exposure fusion approach using nonlinear translation-variant filter (NTF). With the captured Standard Dynamic Range (SDR) images under different exposure settings, first the fine details are extracted based on guided filter. Next, the base layers (i. e., images obtained from NTF) across all <b>input</b> images are <b>fused</b> using multiresolution pyramid. Exposure, contrast, and saturation measures are considered to generate a mask that guides the fusion process of the base layers. Finally, the fused base layer is combined with the extracted fine details to obtain detail-enhanced fused image. The goal is to preserve details in both very dark and extremely bright regions without High Dynamic Range Image (HDRI) representation and tone mapping step. Moreover, we have demonstrated that the proposed method is also suitable for the multifocus image fusion without introducing artifacts. 1...|$|R
40|$|THIS thesis {{addresses}} {{the issues of}} multi-sensor image systems and its surveillance applications. The advanced surveillance systems incorporate multiple imaging modalities for an improved and more reliable performance under various conditions. The so-called image fusion technique {{plays an important role}} to process multi-modal images. The use of image fusion techniques has been found {{in a wide range of}} applications. The fusion operation is to integrate features from multiple <b>inputs</b> into the <b>fused</b> result. The image fusion process consists of four basic steps, i. e. preprocessing, registration, fusion, and post-processing or evaluation. This thesis focuses on the last three topics. The first topic is the image registration or alignment, which is to associate corresponding pixels in multiple images to the same physical point in the scene. The registration of infrared and electro-optic video sequences is investigated in this study. The initial registration parameters are derived from the match of head top points across the consecutive video frames. Further refinement is implemented with the maximum mutual information approach. Instead of doing the foreground detection, the frame difference, from which the head to...|$|R
40|$|The usual {{clinical}} {{procedure for}} {{early detection of}} cancer in the urinary bladder is the visual inspection with an endoscope (cystoscope). The very limited field of view provided by the cystoscope impedes orientation for the surgeon, thus making it challenging to ensure, that the interior bladder wall has been examined completely. Panorama imaging techniques {{can be used to}} assist the surgeon and provide a larger view field. Creating endoscopic panorama images by means of stitching techniques require image blending to provide smooth transition between <b>fused</b> <b>input</b> frames. Several blending algorithms have been proposed in the past with the goal to suppress hard transitions between images {{and at the same time}} keep a maximum amount of image structure. In this contribution, we discuss several state of the art blending techniques applied to cystoscopy images and present a new approach based on a local entropy measure. We show that local entropy is a suitable measure to achieve a good balance between transition smoothness and structure preservation. A quantitative comparison with two well-established methods shows the efficacy and efficiency of the proposed method...|$|R
40|$|The {{method of}} {{combining}} important details from {{two or more}} source images into a final fused image is known as Image Fusion. When compared {{to any of the}} other <b>input</b> images, our <b>fused</b> output image will have more detailed information in it. The objective of image fusion is to obtain the most desirable data from each image. Multi sensor image fusion algorithm based on three different fusion techniques have been discussed in this paper. Those are “Pixel Level Iteration”, “Directional Discrete Cosine Transform (DDCT) ”, and “Discrete Wavelet Transform (DWT) ”. The outcomes are additionally outfitted in picture and table organization for near examination of above methods. This paper shows the three distinctive picture combination strategies and there relative results, as the routine combination methods Direct Pixel Iteration and Discrete Cosine Transform has a few downsides. The similar study presumes that Discrete Wavelet Transform {{is one of the best}} and most effective algorithm for Image Fusion. In this thesis, Discrete Wavelet Transform based two calculations are proposed, these are Maximum Intensity Replacement and Band Averaging Method...|$|R
40|$|In this paper, {{the authors}} report recent results on {{automatic}} classification of free text documents into a given number of categories. The method uses multiple sensors to derive informative clues about patterns {{of interest in}} the <b>input</b> text, and <b>fuses</b> this information using a neural network. Encouraging preliminary results were obtained by applying this approach to a set of free text documents from the Associated Press (AP) news wire. New free text documents have been made available by the Reuters news agency. The advantages of this collection compared to the AP data are that the Reuters stories were already manually classified, and included sufficiently high numbers of stories per category. The results indicate the usefulness of the new method: after the network is fully trained, if data belonging to only one category are used for testing, correctness is about 90 %, representing nearly 15 % over the best results for the AP data. Based on the performance of the method with the AP and the Reuters collections they now have conclusive evidence that the approach is viable and practical. More work remains to be done for handling data belonging to the multiple categories...|$|R
40|$|This paper {{describes}} a world model that combines {{a variety of}} sensed inputs and a priori information and is used to generate on-road and off-road autonomous driving behaviors. The system is designed {{in accordance with the}} principles of the 4 D/RCS architecture. The world model is hierarchical, with the resolution and scope at each level designed to minimize computational resource requirements and to support planning functions for that level of the control hierarchy. The sensory processing system that populates the world model <b>fuses</b> <b>inputs</b> from multiple sensors and extracts feature information, such as terrain elevation, cover, road edges, and obstacles. Feature information from digital maps, such as road networks, elevation, and hydrology, is also incorporated into this rich world model. The various features are maintained in different layers that are registered together to provide maximum flexibility in generation of vehicle plans depending on mission requirements. The paper includes discussion of how the maps are built and how the objects and features of the world are represented. Functions for maintaining the world model are discussed. The world model described herein is being developed for the Army Research Laboratory's Demo III Autonomous Scout Vehicle experiment...|$|R
40|$|We {{present a}} novel deep {{learning}} architecture for fusing static multi-exposure images. Current multi-exposure fusion (MEF) approaches use hand-crafted features to <b>fuse</b> <b>input</b> sequence. However, the weak hand-crafted representations are not robust to varying input conditions. Moreover, they perform poorly for extreme exposure image pairs. Thus, {{it is highly}} desirable to have a method that is robust to varying input conditions and capable of handling extreme exposure without artifacts. Deep representations have known to be robust to input conditions and have shown phenomenal performance in a supervised setting. However, the stumbling block in using deep learning for MEF {{was the lack of}} sufficient training data and an oracle to provide the ground-truth for supervision. To address the above issues, we have gathered a large dataset of multi-exposure image stacks for training and to circumvent the need for ground truth images, we propose an unsupervised deep learning framework for MEF utilizing a no-reference quality metric as loss function. The proposed approach uses a novel CNN architecture trained to learn the fusion operation without reference ground truth image. The model fuses a set of common low level features extracted from each image to generate artifact-free perceptually pleasing results. We perform extensive quantitative and qualitative evaluation and show that the proposed technique outperforms existing state-of-the-art approaches for a variety of natural images. Comment: ICCV 201...|$|R
40|$|Background: The {{process of}} medical image fusion is {{combining}} {{two or more}} medical images such as Magnetic Resonance Image (MRI) and Positron Emission Tomography (PET) and mapping them to a single image as fused image. So purpose of our study is assisting physicians to diagnose and treat the diseases in the least of the time. Methods: We used Magnetic Resonance Image (MRI) and Positron Emission Tomography (PET) as <b>input</b> images, so <b>fused</b> them based on combination of two dimensional Hilbert transform (2 -D HT) and Intensity Hue Saturation (IHS) method. Evaluation metrics that we apply are Discrepancy (Dk) as an assessing spectral features and Average Gradient (AGk) as an evaluating spatial features and also Overall Performance (O. P) to verify properly of the proposed method. Results: In this paper we used three common evaluation metrics like Average Gradient (AGk) and the lowest Discrepancy (Dk) and Overall Performance (O. P) to evaluate the performance of our method. Simulated and numerical results represent the desired performance of proposed method. Conclusions: Since that {{the main purpose of}} medical image fusion is preserving both spatial and spectral features of input images, so based on numerical results of evaluation metrics such as Average Gradient (AGk), Discrepancy (Dk) and Overall Performance (O. P) and also desired simulated results, it can be concluded that our proposed method can preserve both spatial and spectral features of input images...|$|R
50|$|In {{more recent}} {{calorimeter}} designs, the whole bomb, pressurized with excess pure oxygen (typically at 30atm) and containing a weighed mass {{of a sample}} (typically 1-1.5 g) and a small fixed amount of water (to saturate the internal atmosphere, thus ensuring that all water produced is liquid, and removing the need to include enthalpy of vaporization in calculations), is submerged under a known volume of water (ca. 2000 ml) before the charge is electrically ignited. The bomb, with the known mass of the sample and oxygen, form a closed system - no gases escape during the reaction. The weighed reactant put inside the steel container is then ignited. Energy is released by the combustion and heat flow from this crosses the stainless steel wall, thus raising {{the temperature of the}} steel bomb, its contents, and the surrounding water jacket. The temperature change in the water is then accurately measured with a thermometer. This reading, along with a bomb factor (which is dependent on the heat capacity of the metal bomb parts), is used to calculate the energy given out by the sample burn. A small correction is made to account for the electrical energy <b>input,</b> the burning <b>fuse,</b> and acid production (by titration of the residual liquid). After the temperature rise has been measured, the excess pressure in the bomb is released.|$|R
