0|16|Public
50|$|DEC invented Digital Linear <b>Tape</b> (<b>DLT),</b> {{formerly}} known as CompacTape, which began as a compact backup medium for MicroVAX systems, and later grew to capacities of 800 gigabytes.|$|R
50|$|Unlike some {{competing}} {{technologies such}} as Digital Linear <b>Tape</b> (<b>DLT)</b> and Linear Tape-Open (LTO), Travan technology does not automatically verify data after writing. Data verification must be done separately by the computer operator, to verify data was written successfully. If a separate verify operation is not performed after each backup, {{it is possible for}} backups to be found to be corrupt and unusable when the tapes need to be used.|$|R
5|$|A major {{complication}} in remastering {{the original}} work was having {{many of the}} critical game files go missing or on archaic formats. A large number of backup files were made on Digital Linear <b>Tape</b> (<b>DLT)</b> which Disney/LucasArts {{had been able to}} recover for Double Fine, but the company had no drives to read the tapes. Former LucasArts sound engineer Jory Prum had managed to save a DLT drive and was able to extract all of the game's audio development data from the tapes.|$|R
50|$|The tape {{inside an}} LTO {{cartridge}} is wound around a single reel. The {{end of the}} tape {{is attached to a}} perpendicular leader pin that is used by an LTO drive to reliably grasp the end of the tape and mount it in a take-up reel inside the drive. Older single-reel tape technologies, such as 9 track <b>tape</b> and <b>DLT,</b> used different means to load tape onto a take-up reel.|$|R
5000|$|... 4. Geno-informatics Lab.- A high {{capacity}} Sun TCF server, six SUN ultra10 servers, a backup <b>DLT</b> <b>tape</b> library, FOC broadband connectivity and 15 SUN ray thin clients {{has been established}} for genome analysis work and is being extensively used.|$|R
5000|$|A later {{version of}} tape backup for the Corvus Omninet was called [...] "The Bank" [...] {{and was a}} {{standalone}} Omninet connected device that used custom backup tape media that were very similar in shape and size to today's <b>DLT</b> <b>tapes.</b>|$|R
40|$|International Telemetering Conference Proceedings / October 26 - 29, 1998 / Town & Country Resort Hotel and Convention Center, San Diego, CaliforniaAs the {{performance}} of inexpensive commercial off-the-shelf (COTS) data storage devices continues to increase, the temptation to use them {{as the basis for}} data capture products for military and industrial applications becomes ever more compelling. For example, the Digital Linear <b>Tape</b> (<b>DLT)</b> format now offers a 270 Gigabits per cassette capacity at a sustained transfer rate of 40 Mbits/s – performance which would have cost tens or even {{hundreds of thousands of dollars}} per system just a few years ago. But to transplant such a device from its benign office habitat into a data capture product which will function reliably and consistently in a wide range of field and platform environments is an engineering task fully as difficult and complex as designing an environmentally robust recorder from scratch. This paper discusses the problems which typically have to be overcome; environmental protection, reliability, data integrity, power supplies, software issues, control and data interfacing, etc., citing practical examples of analog and digital DLT-based data recorders which are now entering service for telemetry, intelligence gathering, anti-submarine warfare and related application...|$|R
40|$|The Goddard Space Flight Center (GSFC) Version 0 (V 0) Distributed Active Archive Center (DAAC) {{has been}} {{developed}} to support existing and pre Earth Observing System (EOS) Earth science datasets, facilitate the scientific research, and test Earth Observing System Data and Information System (EOSDIS) concepts. To ensure that no data is ever lost, each product received at GSFC DAAC is archived on two different media (VHS and Digital Linear <b>Tape</b> (<b>DLT)).</b> The first copy is made on VHS tape and is {{under the control of}} UniTree. The second and third copies are made to DLT and VHS media under a custom built software package named "Archer". While Archer provides only a subset of the functions available with commercial software like UniTree, it supports migration between near-line and off-line media and offers much greater performance and flexibility to satisfy the specific needs of a Data Center. Archer is specifically designed to maximize total system throughput, rather than focusing on the turn-around time for individual files. The Commercial Off the Shelf Software (COTS) Hierarchical Storage Management (HSM) products evaluated were mainly concerned with transparent, interactive, file access to the end-user, rather than as a batch-oriented, optimizable (based on known data file characteristics) data archive and retrieval system. This is critical to the distribution requirements of the GSFC DAAC where orders for 5000 or more files at a time are received. Archer has the ability to queue many thousands of file requests and to sort these requests into internal processing schedules that optimize overall throughput. Specifically, mount and dismount, tape load and unload cycles, and tape motion are minimized. This feature {{did not seem to be}} available in many COTS packages. Archer also [...] ...|$|R
50|$|Degaussing often renders hard disks inoperable, as it erases {{low-level}} formatting {{that is only}} done at the factory during manufacturing. In some cases, {{it is possible to}} return the drive to a functional state by having it serviced at the manufacturer. However, some modern degaussers use such a strong magnetic pulse that the motor that spins the platters may be destroyed in the degaussing process, and servicing may not be cost-effective. Degaussed computer <b>tape</b> such as <b>DLT</b> can generally be reformatted and reused with standard consumer hardware.|$|R
40|$|ITC/USA 2006 Conference Proceedings / The Forty-Second Annual International Telemetering Conference and Technical Exhibition / October 23 - 26, 2006 / Town and Country Resort & Convention Center, San Diego, CaliforniaThe {{transition}} of PCM recording from analog to digital recorders was completed at many test ranges {{more than a}} decade ago as marked by delivery of data on S-VHS tape, CD-ROM, DVD, ZIP disc, JAZ disc, 8 mm <b>tape</b> and <b>DLT</b> <b>tape</b> for low rate data and D- 1 cassettes for high rate data. Data then quickly began distribution via the internet and other networks. Analog recorders have remained a necessary legacy for the long transition to convert from analog to digital (PCM) data transmission from the test vehicles. However, the new digital recorder capabilities have removed this requirement to convert the transmissions from the test vehicle. Analog signal and predetection recording on digital recorders has been successfully demonstrated at costs below the existing analog recorders. Application of new techniques in a methodical transition program to the new digital recorders has proven the many benefits of recording wider bandwidths with excellent repeatability. Repeatability issues are primarily in the very low error sources of the processing system because the major analog error sources of the analog tape recorders, analog time code readers, analog demodulators, etc have been greatly reduced. This paper provides test results of recording higher signal rates and bandwidths of the new programs and describes the techniques and implementation through procedures of the Western Range transition from analog to digital recorders. Surprising results show predetection and analog signal recording costs are nearly the same as PCM recording costs due to the price of deliverable media with respect to mission recording requirements...|$|R
40|$|Abstract- In {{order to}} {{advance in the}} firing angle control of {{thyristors}} in the single phase fully controlled bridge rectifiers through the actual technology this paper presents a method of obtaining a regulated dc voltage through <b>digital</b> control. This <b>lineal</b> dc voltage is obtained through the control of phase angle of thyristors with a PIC microcontroller of low gamma. The algorithm control is programmed in C language {{and the results are}} verified for different types of load. Index Terms- C language, fully controlled bridge rectifier,Microcontroller, Phase Angle...|$|R
50|$|A system {{dedicated}} to offsite offline data processing {{was set up}} at the Stony Brook University in Stony Brook, NY to process raw data sent from Kamioka. Most of the reformatted raw data is copied from system facility in Kamioka. At Stony Brook, a system was set up for analysis and further processing. At Stony Brook the raw data were processed with a multi-tape DLT drive. The first stage data reduction processes were done for the high energy analysis and for the low energy analysis. The data reduction for the high energy analysis was mainly for atmospheric neutrino events and proton decay search while the low energy analysis was mainly for the solar neutrino events. The reduced data for the high energy analysis was further filtered by other reduction processes and the resulting data were stored on disks. The reduced data for the low energy were stored on <b>DLT</b> <b>tapes</b> and sent to University of California, Irvine for further processing.|$|R
40|$|We {{describe}} {{the architecture of}} the data archiving and distribution of the Virgo antenna for gravitational wave detection. The main characteristic of this system is the modularity of the architecture. This solution allows system upgrades without dramatic changes of the hardware and software components. The main performances are: 1. Maximum sustained data flow of 10 Mbyte/s on <b>DLT</b> <b>tapes</b> (35 / 70 Gbyte) for the raw data archiving; 2. Up to 1 Tbyte data archiving capacity on disk at a maximum sustained data flow of 25 Mbyte/s for the online data distribution; 3. Up to 10 Mbyte/s data retrieval flow for the on-line data distribution. The basic architecture of the system consists of two sections: an acquisition and storage section and a data management section. The former is a LynxOS based system with the disks directly connected to the CPU slave boards, the latter is a DEC-Unix Alpha Server (Data Server), NFS mounting the LynxOS disks through a Fast Ethernet networ...|$|R
40|$|The Wide-Field Infrared Explorer (WIRE) Archive and Research Facility (WARF) is {{operated}} and {{maintained by the}} Department of Physics, USAF Academy. The lab is located in Fairchild Hall, 2354 Fairchild Dr., Suite 2 A 103, USAF Academy, CO 80840. The WARF {{will be used for}} research and education in support of the NASA Wide Field Infrared Explorer (WIRE) satellite, and for related high-precision photometry missions and activities. The WARF will also contain the WIRE preliminary and final archives prior to their delivery to the National Space Science Data Center (NSSDC). The WARF consists of a suite of equipment purchased under several NASA grants in support of WIRE research. The core system consists of a Red Hat Linux workstation with twin 933 MHz PIII processors, 1 GB of RAM, 133 GB of hard disk space, and DAT and <b>DLT</b> <b>tape</b> drives. The WARF is also supported by several additional networked Linux workstations. Only one of these (an older 450 Mhz PIII computer running Red Hat Linux) is currently running, but the addition of several more is expected over the next year. In addition, a printer will soon be added. The WARF will serve as the primary research facility for the analysis and archiving of data from the WIRE satellite, together with limited quantities of other high-precision astronomical photometry data from both ground- and space-based facilities. However, the archive to be created here will not be the final archive; rather, the archive will be duplicated at the NSSDC and public access to the data will generally take place through that site...|$|R
40|$|The Mediterrean Network (MedNet) {{presently}} comprises 22 operating broadband seismic stations installed {{and maintained}} in cooperation with 13 geophysical institutions in Italy and {{in most of the}} countries adjacent to Mediterranean Sea. The number of stations may vary as stations are opened or sometimes closed due to different reasons like political, technical, etc., but usually temporarily. All the stations are equipped with Quanterra digitizers and Streckeisen sensors, mostly STS 2 with a few STS 1. Aim of the network is to contribute to monitoring {{of one of the most}} active seismic regions of the World in terms of providing high quality real-time broadband data to the seismological community. Operations started with off-line field data collection and dial-up capabilities were later added at selected sites. At present these have been replaced with more efficient TCP connections that provide for real-time data collection over the whole network. This important technological upgrade allows a prompt contribution to the seismic monitoring of Italy and of most countries bordering the Mediterranean Sea, since data are exchanged in real-time with other seismological observatories. SeedLink protocol has been adopted for transmission. As for data archiving and distribution, a fast system for retrieving data has been developed. Continuous data streams, collected both from field data tapes and from real time transfer, are stored at the MedNet Data Center and are directly available at users’ request by the standard AutoDRM and NetDC protocols (in GSE and SEED formats respectively). Station metadata and continuous waveforms are archived in a MySQL database on RAID systems and backed up on <b>DLT</b> <b>tapes.</b> Presently, fully automatic network functions include: daily monitoring of state of health; triggered retrieval of event waveforms (with magnitude- and region- specific selection criteria), local and surface wave magnitude determination, and update of web pages ([URL] for events and station information. Rapid semiautomatic moment tensor solutions are calculated by means of a modified Harvard technique, which lowers the Mw threshold down to 4. 5 for regional events in those areas with proper station coverage. For smaller earthquakes in Italy a new approach to moment tensor estimation, based on higher signal frequencies, is now being developed. Preliminary tests on earthquake recordings (not only MedNet stations) from the 2002 Molise, South Italy sequence have proved very successful...|$|R

