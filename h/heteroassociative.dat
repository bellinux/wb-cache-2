51|0|Public
50|$|Bidirectional {{associative}} memories (BAM) are artificial {{neural networks}} {{that have long}} been used for performing <b>heteroassociative</b> recall.|$|E
5000|$|<b>Heteroassociative</b> memories, on {{the other}} hand, can recall an {{associated}} piece of datum from one category upon presentation of data from another category. Hopfield networks [...] have been shown [...] to act as autoassociative memory since {{they are capable of}} remembering data by observing a portion of that data.|$|E
40|$|Background Recent {{research}} suggests that the CA 3 subregion of the hippocampus has properties of both autoassociative network, due to its ability to complete partial cues, tolerate noise, and store associations between memories, and <b>heteroassociative</b> one, due to its ability to store and retrieve sequences of patterns. Although there are several computational models of the CA 3 as an autoassociative network, more detailed evaluations of its <b>heteroassociative</b> properties are missing. Methods We developed a model of the CA 3 subregion containing 10, 000 integrate-and-fire neurons with both recurrent excitatory and inhibitory connections, and which exhibits coupled oscillations in the gamma and theta ranges. We stored thousands of pattern sequences using a <b>heteroassociative</b> learning rule with competitive synaptic scaling. Results We showed that a purely <b>heteroassociative</b> network model can (i) retrieve pattern sequences from partial cues with external noise and incomplete connectivity, (ii) achieve homeostasis regarding the number of connections per neuron when many patterns are stored when using synaptic scaling, (iii) continuously update the set of retrievable patterns, guaranteeing that the last stored patterns can be retrieved and older ones can be forgotten. Discussion <b>Heteroassociative</b> networks with synaptic scaling rules seem sufficient to achieve many desirable features regarding connectivity homeostasis, pattern sequence retrieval, noise tolerance and updating of the set of retrievable patterns...|$|E
40|$|In this paper, an {{algorithm}} is suggested to train {{a single layer}} feedforward neural network {{to function as a}} <b>heteroassociative</b> memory. This algorithm enhances the ability of the memory to recall the stored patterns when partially described noisy inputs patterns are presented. The algorithm relies on adapting the standard delta rule by introducing new terms, first order term and second order term to it. Results show that the <b>heteroassociative</b> neural network trained with this algorithm perfectly recalls the desired stored pattern when 1. 6 % and 3. 2 % special partially described noisy inputs patterns are presented. General terms Soft computin...|$|E
40|$|Associations in {{episodic memory}} are formed between items {{presented}} close together in time. The temporal context model (TCM) hypothesizes that this contiguity {{effect is a}} consequence of shared temporal contexts rather than temporal proximity per se. Using double function lists of paired associates, which include chains of pairs (e. g. A-B, B-C), we examined associations between items that were not presented close together in time but were presented in similar temporal contexts. For instance A and C do not appear together, but both occur in the context of B. Although within-pair associations (e. g. A-B) were asymmetric, across-pair associations (e. g. A-C) showed no evidence for asymmetry. We attempted to describe these transitive associations using two models. One was a <b>heteroassociative</b> model in which the A-C associations resulted from mediated chaining as a result of “stepping through ” the links in the chain. Although this <b>heteroassociative</b> model and TCM make identical predictions regarding simple contiguity effects, the <b>heteroassociative</b> model had great difficulty accounting for the form of transitive associations between items. TCM provided an excellent fit to the data. These data raise the surprising possiblity that episodic contiguity effects do not reflect direct associations between items but rather a process of binding, encoding and retrieval of a gradually-changin...|$|E
30|$|A {{new method}} for image {{compression}} based on morphological associative memories (MAMs) is presented. We used the MAM {{to implement a}} new image transform and applied it at the transformation stage of image coding, thereby replacing such traditional methods as the discrete cosine transform or the discrete wavelet transform. Autoassociative and <b>heteroassociative</b> MAMs {{can be considered as}} a subclass of morphological neural networks. The morphological transform (MT) presented in this paper generates <b>heteroassociative</b> MAMs derived from image subblocks. The MT is applied to individual blocks of the image using some transformation matrix as an input pattern. Depending on this matrix, the image takes a morphological representation, which is used to perform the data compression at the next stages. With respect to traditional methods, the main advantage offered by the MT is the processing speed, whereas the compression rate and the signal-to-noise ratio are competitive to conventional transforms.|$|E
40|$|This {{paper will}} explore the dichotomous issue of {{increasing}} the controllability and improving the reliability of neural architectures, by introducing the new concept of a validation sub-network. One example of enhancing common neural networks (the <b>heteroassociative</b> memory) by using validation sub-networks is discussed. The concepts are verified by using this enhanced neural network as a component in a roadsign recognition system. The neural components is an image recognition unit based on the <b>heteroassociative</b> memory. 1. Introduction This paper introduces a general means of improving the reliability and the controllability of already existing neural architectures by using the new concept of validation subnetworks [1]. A validation subnetwork is an add-on feature to an already existing neural network. Its purpose is to observe both the input and {{the output of the}} network and to verify whether the output is correct or not, considering the input. Therefore, while the original net [...] ...|$|E
40|$|Abstract. Morphological Neural Networks (MNN) {{have been}} {{proposed}} as an alternative neural computation paradigm. In this paper we explore the potential of <b>Heteroassociative</b> MNN (HMNN) for a vision based practical task, that of self-localization in a vision-based navigation framework for mobile robots. HMNN have a big potential for real time application because its recall process is very fast. We present some experimental results that illustrate the proposed approach. 1...|$|E
40|$|Abstract [...] This article {{introduces}} a neural architecture termed Adaptive Resonance Associative Map (ARAM) that extends unsupervised Adaptive Resonance Theory (ART) systems for rapid, yet stable, <b>heteroassociative</b> learning. ARAM can be visualized as two overlapping ART networks sharing a single category field. Although ARAM is simpler in architecture than another class of supervised ART models known as ARTMAP, it produces classification performance {{equivalent to that}} of ARTMAP. As ARAM network structure and operations are symmetrical, associative recall can be performed in both directions. With maximal vigilance settings, ARAM encodes pattern pairs explicitly as cognitive chunks and thus guarantees perfect storage and recall of an arbitrary number of arbitrary pattern pairs. Simulations on an iris plant and a sonar return recognition problems compare ARAM classification perfor-mance with that of counterpropagation network, K-nearest neighbor system, and back-propagation network. Asso-ciative recall experiments on two pattern sets show that, besides the advantages of fast learning, guaranteed perfect storage, and full memory capacity, ARAM produces a stronger noise immunity than Bidirectional Associative Mem-ory (BAM). Keywords [...] Self-organization, Neural network architecture, Associative memory, <b>Heteroassociative</b> recall, Super-vised learning...|$|E
40|$|A {{new method}} for image {{compression}} based on morphological associative memories (MAMs) is presented. We used the MAM {{to implement a}} new image transform and applied it at the transformation stage of image coding, thereby replacing such traditional methods as the discrete cosine transform or the discrete wavelet transform. Autoassociative and <b>heteroassociative</b> MAMs {{can be considered as}} a subclass of morphological neural networks. The morphological transform (MT) presented in this paper generates <b>heteroassociative</b> MAMs derived from image subblocks. The MT is applied to individual blocks of the image using some transformation matrix as an input pattern. Depending on this matrix, the image takes a morphological representation, which is used to perform the data compression at the next stages. With respect to traditional methods, the main advantage offered by the MT is the processing speed, whereas the compression rate and the signal-to-noise ratio are competitive to conventional transforms. Copyright © 2008 Enrique Guzmán et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1...|$|E
40|$|A <b>heteroassociative</b> memory {{network for}} image {{recognition}} is constructed {{with the aid}} of the method in the paper [5]. This network is a three layered neural network which consists of an input layer, a hidden layer and an output layer. A feature of the network is to contain a sigmoid function only in the hidden units. Images to be stored in the network are real valued vectors. Weights and threshold values connecting the input layer with the hidden layer are determined such that for input reference images, a sufficiently small positive number E or 1 - E is output at each unit in the hidden layer. Interconnection weights between the hidden and output layers are determined so as to reconstitute the input reference images. This approach makes possible the contraction mapping analysis for the network. As done in [5], domains of attraction in the network are sought. Regions of attraction larger than these domains are also found using the smallness of E. Furthermore, a certain <b>heteroassociative</b> memory model is designed based on the shape of the fundamental domains of attraction, and successfully applied to recognition of facial images. ...|$|E
30|$|The {{bidirectional}} {{associative memory}} (BAM) neural network which was first introduced by Kosko in 1987 [1, 2] is formed by neurons arranged in two layers. The neurons in one layer are fully interconnected to the neurons in the other layer, {{while there are no}} interconnections among neurons in the same layer. Through iterations of forward and backward information flows between the two layers, it performs a two-way associative search for stored bipolar vector pairs and generalizes the single-layer autoassociative hebbian correlation to a two-layer pattern matched <b>heteroassociative.</b>|$|E
40|$|We {{investigate}} associative {{memories for}} memristive neural networks with deviating argument. Firstly, {{the existence and}} uniqueness of the solution for memristive neural networks with deviating argument are discussed. Next, some sufficient conditions for this class of neural networks to possess invariant manifolds are obtained. In addition, a global exponential stability criterion is presented. Then, analysis and design of autoassociative memories and <b>heteroassociative</b> memories for memristive neural networks with deviating argument are formulated, respectively. Finally, several numerical examples are given to demonstrate {{the effectiveness of the}} obtained results...|$|E
40|$|Best Paper AwardInternational audienceNeuronal {{models of}} {{associative}} memories are recurrent networks {{able to learn}} quickly patterns as stable states of the network. Their main acknowledged weakness is related to catastrophic interference when too many or too close examples are stored. Based on biological data we have recently proposed a model resistant to some kinds of interferences related to <b>heteroassociative</b> learning. In this paper we report numerical experiments that highlight this robustness and demonstrate very good performances of memorization. We also discuss convergence of interests for such an adaptive mechanism for biological modeling and information processing {{in the domain of}} machine learning...|$|E
40|$|A new {{mathematical}} {{approach for}} deriving learning algorithms for various neural network models including the Hopfield model, Bidirectional Associative Memory, Dynamic <b>Heteroassociative</b> Neural Memory, and Radial Basis Function Networks is presented. The mathematical approach {{is based on}} the relaxation method for solving systems of linear inequalities. The newly developed learning algorithms are fast and they guarantee convergence to a solution in a finite number of steps. The new algorithms are highly insensitive to choice of parameters and the initial set of weights. They also exhibit high scalability on binary random patterns. Rigorous mathematical foundations for the new algorithms and their simulation studies are included...|$|E
40|$|In many {{problems}} {{in science and}} engineering, it {{is often the case}} that there exist a number of computational models to simulate the problem at hand. These models are usually trade-offs between accuracy and computational expense. Given a limited computation budget, there is need to develop a framework for selecting between different models in a sensible fashion during the search. The method proposed here is based on the construction of a <b>heteroassociative</b> mapping to estimate the differences between models, and using this information to guide the search. The proposed framework is tested on the problem of minimizing the transmitted vibration energy in a satellite boo...|$|E
40|$|The hippocampal regions CA 3 and CA 1 {{have long}} been {{proposed}} to be autoand <b>heteroassociative</b> memories, respectively (Marr, 1971; McNaughton and Morris, 1987; Treves and Rolls, 1994), for the storage of declarative information. An autoassociative memory is formed when a set of neurons are recurrently connected by modifiable synapses, whereas a <b>heteroassociative</b> memory is formed through modifiable connections from an input layer of neurons to an output layer. Associative memory storage simply requires a Hebbian strengthening of connections between neurons that are coactive (Amit, 1989; Hopfield, 1982; Willshaw et al., 1969). Recall proceeds from a cue activity pattern across neurons that is a partial or noisy version of a previously stored pattern. A suitable firing threshold on each neuron that receives input from already active neurons ensures that neural activity evolves towards the stored pattern. This may happen with {{only one or two}} updates of each neuron’s activity. Accurate recall is obtainable provided not too many patterns have been stored, otherwise recall is poor, or even impossible. Network models of spiking neurons can be used to explore the dynamics of storage and recall in such memory networks. Here we introduce a recurrent network model based on hippocampal area CA 3 and a feedforward network model for area CA 1. Cells are simplified compartmental models with complex ion channel dynamics. In addition to pyramidal cells, one or more types of interneuron are present. We investigate, in particular, the roles of these interneurons in setting the appropriate threshold for memory recall...|$|E
40|$|Abstract. This paper proposes novel {{hierarchical}} self-organizing {{associative memory}} architecture for machine learning. This memory architecture is characterized with sparse and local interconnections, self-organizing processing elements (PE), and probabilistic synaptic transmission. Each PE {{in the network}} dynamically estimates its output value from the observed input data distribution and remembers the statistical correlations between its inputs. Both feed forward and feedback signal propagation is used to transfer signals and make associations. Feed forward processing is used to discover relationships in the input patterns, while feedback processing is used to make associations and predict missing signal values. Classification and image recovery applications are used to demonstrate {{the effectiveness of the}} proposed memory for both <b>heteroassociative</b> and auto-associative learning...|$|E
30|$|As is well known, the {{bidirectional}} {{associative memory}} (BAM) neural networks were originally introduced by Kosko [1 – 3], {{and they are a}} class of two-layer <b>heteroassociative</b> networks, which are composed of neurons arranged in two layers, the U-layer and the V-layer. Generally speaking, the neurons in one layer are fully interconnected to the neurons in the other layer. Moreover, there may be no interconnection among neurons in the same layer. In addition, the addressable memories or patterns of BAM neural networks can be stored with a two-way associative search. Owing to these reasons, the BAM neural network has been widely studied both in theory and applications; see [4 – 13]. Therefore, it is meaningful and important to study the BAM neural network.|$|E
40|$|International audienceThe {{development}} of an integrated MultiSpectral Imaging (MSI) system yielding hyperspectral cubes by means of artificial neural networks is described. The MSI system {{is based on a}} CCD camera, a rotating wheel bearing a set of seven interference filters, a light source and a computer. The resulting device has been elaborated for in vivo imaging of skin lesions. It provides multispectral images and is coupled with a software reconstructing hyperspectral cubes from multispectral images. Reconstruction is performed by a neural network-based algorithm using <b>heteroassociative</b> memories. The resulting hyperspectral cube provides skin optical reflectance spectral data combined with bidimensional spatial information. This combined information will hopefully improve diagnosis and follow-up in a range of skin disorders from skin cancer to inflammatory diseases...|$|E
40|$|This paper {{introduces}} {{a new class}} of predictive ART architectures, called Adaptive Resonance Associative Map (ARAM) which performs rapid, yet stable <b>heteroassociative</b> learning in real time environment. ARAM can be visualized as two ART modules sharing a single recognition code layer. The unit for recruiting a recognition code is a pattern pair. Code stabilization is ensured by restricting coding to states where resonances are reached in both modules. Simulation results have shown that ARAM is capable of self-stabilizing association of arbitrary pattern pairs of arbitrary complexity appearing in arbitrary sequence by fast learning in real time environment. Due to the symmetrical network structure, associative recall can be performed in both directions. Air Force Office of Scientific Research (90 - 0128...|$|E
40|$|Abstract The {{associative}} net model of <b>heteroassociative</b> memory with binaryvalued synapses {{has been extended}} to include recent experimental data that indicates that in the hippocampus one form of synaptic modification {{is a change in}} the probability of synaptic transmission [2]. Pattern pairs are stored in the net by a version of the Hebbian learning rule that changes the probability of transmission at synapses where the presynaptic and postsynaptic units are simultaneously active from a low, base value to a high, modified value. Numerical calculations of the expected recall response have been used to assess the performance for different values of the base and modified probabilities. If there is a cost incurred with generating the difference between these probabilities, then the optimal difference is around 0. 4. Performance can be greatly enhanced by using multiple cue presentations during recall...|$|E
40|$|A {{model of}} the {{hippocampus}} as a "cognitive graph" is proposed. It essentially considers the hippocampus as an <b>heteroassociative</b> network that learns temporal sequences of visited places and stores a topological representation of the environment. Using place cells, head-direction cells, and "goal cells", we propose a biologically plausible way of exploiting such a spatial representation for navigation, which does not require complicated graph search algorithms. Simulations show that the resulting animat is able to navigate in continuous environments that contain obstacles. Furthermore, we make experimental predictions on simultaneous recordings of multiple cells in the rat hippocampus. 1. Introduction The discovery of place cells in areas CA 3 and CA 1 of the rat hippocampus (O'Keefe and Dostrovsky, 1971) [...] cells that discharge selectively when the rat is in restricted regions of the environment (their place fields) [...] led {{to the idea that}} the hippocampus functions as a cognitive map of [...] ...|$|E
40|$|Numerical {{calculations}} {{have been}} used to assess the performance of three different winners-take-all recall strategies for the associative net model of <b>heteroassociative</b> memory. Two strategies designed to improve recall when the net is partially connected or the input cues are noisy are shown to provide significantly greater capacity and information efficiency under these conditions. Estimates are made for the capacity of nets that are the size of the CA 3 region of the rat hippocampus. These indicate that thousands of patterns can be stored, with the exact number highly dependent on pattern coding rates and the recall strategy. Analysis of nets with different types of structure shows that the capacity is much more sensitive to net size than to connectivity level. This has implications for neurobiology where there are examples of environmentally produced changes in both the size and connectivity of various brain regions...|$|E
40|$|Papers on {{optical pattern}} {{recognition}} are presented, covering {{topics such as}} the estimation of satellite pose and motion parameters using a neural net tracker, associative memory, optical implmentation of programmable neural networks, optoelectronic neural networks, dynamic autoassociative neural memory, <b>heteroassociative</b> memory, bilinear pattern recognition processors, optical processing of optical correlation plane data, and a synthetic discriminant function-based nonlinear optical correlator. Other topics include an interactive optical-digital image processor, geometric transformations for video compression and human teleoperator display, quasiconformal remapping for compensation of human visual field defects, hybrid vision for automated spacecraft landing, advanced symbolic and inference optical correlation filters, and a rotationally invariant holographic tracking system. Additional topics include the detection of rotational and scale-varying objects with a programmable joint transform correlator, a single spatial light modulator binary nonlinear optical correlator, optical joint transform correlation, linear phase coefficient composite filters, and binary phase-only filters...|$|E
40|$|Background. Temporal {{sequence}} {{represents the}} main principle underlying episodic memory. The storage of temporal sequence information {{is thought to}} involve hippocampus-dependent memory systems, preserving temporal structure possibly via chaining of sequence elements in <b>heteroassociative</b> networks. Converging evidence indicates that sleep enhances the consolidation of recently acquired representations in the hippocampus-dependent declarative memory system. Yet, it is unknown if this consolidation process comprises strengthening of the temporal sequence structure of the representation as well, or is restricted to sequence elements independent of their temporal order. To address this issue we tested the influence of sleep {{on the strength of}} forward and backward associations in word-triplets. Methodology/Principal Findings. Subjects learned a list of 32 triplets of unrelated words, presented successively (A-B-C) {{in the center of a}} screen, and either slept normally or stayed awake in the subsequent night. After two days, retrieval was assessed for the triplets sequentially either i...|$|E
40|$|We {{address the}} problem of {{training}} relaxation labeling processes, a popular class of parallel iterative procedures widely employed in pattern recognition and computer vision. The approach discussed here is based on a theory of consistency developed by Hummel and Zucker, and contrasts with a recently introduced learning strategy which can be regarded as <b>heteroassociative,</b> i. e., what is actually learned is the association between patterns rather than the patterns themselves. The proposed learning model is instead autoassociative and involves making a set of training patterns consistent, in the sense rigorously defined by Hummel and Zucker; this implies that they become local attractors of the relaxation labeling dynamical system. The learning problem is formulated in terms of solving a system of linear inequalities, and a straightforward iterative algorithm is presented to accomplish this. The learning model described here allows one to view the relaxation labeling process as a kind of [...] ...|$|E
40|$|A new {{associative}} memory model is proposed {{on the basis}} of a nonlinear transformation in the Fourier domain of the data. The Moore-Penrose pseudoinverse is used to compute the optimal leastsquares solution. Computer simulations, using onedimensional speech and two-dimensional images, are presented. Comparison of the new model with the classical optimal linear {{associative memory}} and an optimal nonlinear (polynomial) memory is presented. 1 Introduction Associative recall (memory) may be understood as an operation or transformation with a set of input signals or other items considered as keys, and some sort of outcome which constitutes the recall. The most characteristic property of associative recall is the following: if the totality of the input signal, is stored as such in a memory, it is retrievable by a part of the key. A recall is autoassociative if the pattern is retrievable on a basis a fragment of the key. In the <b>heteroassociative</b> mode of operation, an outcome which structural [...] ...|$|E
40|$|Computer {{simulation}} of a CA 1 hippocampal pyramidal cell {{is used to}} estimate the effects of synaptic and spatio-temporal noise on signal integration. Comparison is made between the pattern recognition ability of the cell {{in the presence of}} this noise and that of a computing unit in an artificial neural network model of an <b>heteroassociative</b> memory. The results indicate that the pattern recognition performance of the pyramidal cell is within an order of magnitude of that of the noise-free computing unit. Synaptic noise may be the most serious form of noise from those considered. 1 Introduction Biological neurons contain many sources of noise that are not generally included in neural network models even when they are designed to mimic the behaviour of a part of the nervous system. This study continues work examining the effects of neurobiological forms of noise on the capacity of associative memory networks. Previously we have determined the impact of partial connectivity and probabilist [...] ...|$|E
40|$|Single-pattern real-valued {{spectral}} associative memories (SAMs) {{are proposed}} for coding and recalling sampled analog or multi-valued data patterns over noisy channels. SAMs are frequencydomain formulations of associative memory that combine the extrinsic redundancy of neural networks with the in-phase and quadrature modulation schemes of telecommunications. Data patterns, or "codewords", are encoded into an attractor wave by associative amplitude modulation (AAM), {{which may be}} recalled by recurrent associative amplitude demodulation (AAD). Because no attractors are actually stored in the decoder, single patterns may be transmitted, {{one at a time}} over a noisy channel with spurious-free recall. Long-range connectivity is made virtually in the frequency domain, allowing both encoder and decoder to scale linearly with pattern dimension. In-phase and quadrature coding schemes are presented with band structures and anti-aliasing constraints for auto- and <b>heteroassociative</b> memory formation and recall. Simulation results are provided that show the accuracy of recall for various Butterworth and Chebyshev filters...|$|E
40|$|Pattern {{recognition}} {{techniques are}} associated a symbolic identity {{with the image}} of the pattern. In this work we will analyze different neural network methods in pattern recognition. This problem of replication of patterns by machines (computers) involves the machine printed patterns. The pattern recognition is better known as optical pattern recognition. Since, it deals with recognition of optically processed patterns rather then magnetically processed ones. A neural network is a processing device, whose design was inspired by the design and functioning of human brain and their components. There is no idle memory containing data and programmed, but each neuron is programmed and continuously active. Neural network has many applications. The most likely applications for the neural networks are (1) Classification (2) Association and (3) Reasoning. One of the applications of neural networks is in the field of pattern recognition. The Bidirectional associative memory does <b>heteroassociative</b> processing in which, association between pattern pairs is stored. The Bidirectional Associative has capacity limitations. It can store and correctly recognize only six characters, with the condition that the characters should be slightly similar in shape...|$|E
40|$|Abstract—Typical {{bidirectional}} associative memories (BAM) use an offline, one-shot learning rule, have poor {{memory storage}} ca-pacity, {{are sensitive to}} noise, and are subject to spurious steady states during recall. Recent work on BAM has improved network performance in relation to noisy recall {{and the number of}} spurious attractors, but at the cost of an increase in BAM complexity. In all cases, the networks can only recall bipolar stimuli and, thus, are of limited use for grey-level pattern recall. In this paper, we introduce a new bidirectional <b>heteroassociative</b> memory model that uses a simple self-convergent iterative learning rule and a new nonlinear output function. As a result, the model can learn online without being subject to overlearning. Our simulation results show that this new model causes fewer spurious attractors when compared to others popular BAM networks, for a comparable performance in terms of tolerance to noise and storage capacity. In addition, the novel output function enables it to learn and recall grey-level pat-terns in a bidirectional way. Index Terms—Associative memories, bidirectional associative memories (BAM), learning, neural networks. I...|$|E
40|$|The storage {{capacity}} of holographic associative memories is estimated. An argument {{based on the}} available degrees of freedom shows {{that the number of}} patterns that can be stored is limited by the space-bandwidth product of the hologram divided by the number of pixels in each pattern. A statistical calculation shows that if we attempt to store associations by multiply exposing the hologram, the cross talk among the stored items severely degrades the output fidelity. This confirms the {{storage capacity}} predicted by the degrees-of-freedom argument. An associative memory internally stores a set of distinct output signal vectors gm, i = 1, 2, [...] ., Min a oneto-one association with a second set of stimulus signals fmn {{in such a way as}} to make selective recall possible. That is, a signal g 9 is recalled by presenting its associated stimulus, f 4, as the input. Such a memory is sometimes described as being <b>heteroassociative,</b> of which autoassociative schemes in which the stimulus and stored signals are the same form a special case. The analogy between associative memories and holograph...|$|E
40|$|Transitive {{associations}} 2 In {{episodic memory}} tasks, associations are formed between items presented close together in time. The temporal context model (TCM) hypothesizes that this contiguity {{effect is a}} consequence of shared temporal context rather than temporal proximity per se. Using double function lists of paired associates (e. g. A-B, B-C) presented in a random order, we examined associations between items that were not presented close together in time but were presented in similar temporal contexts. For instance A and C do not appear in close temporal proximity, but both occur in the context of B. After learning long double-function lists, across-pair associations fell off with distance in the list, as if participants were able to integrate the disparate experiences with the items into a coherent memory structure. Within-pair associations (e. g. A-B) were strongly asymmetric favoring forward transitions; across-pair associations (e. g. A-C) showed no evidence for asymmetry. While this pattern of results presented a stern challenge for a <b>heteroassociative</b> mediated chaining model, TCM provided an excellent fit to the data...|$|E
40|$|The paper {{deals with}} the Sparse {{distributed}} memory (SDM). The SDM can be seen either {{as an extension of}} a RAM or as a neural network and it may be used either as an autoassociative or as a <b>heteroassociative</b> memory. The basic principle, topology, data storing and retrieving, and some practical experience with simple program models of the SDM are described here. Keywords: Neural Networks, Associative Memories, Sparse Distributed Memory. 1 Introduction Associative memory is a memory that can recall data when a reference address is sufficiently close (not only exact equal as in random-access memories) to the address at which the data were stored. It is very useful if the reference address is corrupted by random noise or outright errors or if this address is only partially specified. Suppose L pairs of vectors f(~x 1; ~y 1); (~x 2; ~y 2); :::; (~x L; ~y L) g; where ~x i fflR n x j = f 0; 1 g ~y i fflR m y j = f 0; 1 g Three types of associative memories can be distinguished: 1. [...] ...|$|E
40|$|Nonlinear {{spectral}} associative {{memories are}} proposed as quantized frequency domain formulations of nonlinear, recurrent associative memories in which volatile network attractors are instantiated by attractor waves. In contrast to conventional associative memories, attractors encoded {{in the frequency}} domain by convolution {{may be viewed as}} volatile on-line inputs, rather than nonvolatile, off-line parameters. Spectral memories hold several advantages over conventional associative memories, including decoder/attractor separability and linear scalability, which make them especially well suited for digital communications. Bit patterns may be transmitted over a noisy channel in a spectral attractor and recovered at the receiver by recurrent, spectral decoding. Massive nonlocal connectivity is realized virtually, maintaining high symbol-to-bit ratios while scaling linearly with pattern dimension. For-bit patterns, autoassociative memories achieve the highest noise immunity, whereas <b>heteroassociative</b> memories offer the added flexibility of achieving various code rates, or degrees of extrinsic redundancy. Due to linear scalability, high noise immunity and use of conventional building blocks, spectral associative memories hold much promise for achieving robust communication systems. Simulations are provided showing bit error rates (BERs) for various degrees of decoding time, computational oversampling, and signal-to-noise ratio (SNR) ...|$|E
