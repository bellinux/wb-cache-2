35|53|Public
40|$|We {{present a}} new {{approach}} to estimating the distribution of free speeds based on the method of censored observations. The original <b>distribution-free</b> <b>method</b> of Kaplan-Meier is generalized to include partially censored data, i. e. observations that are censored with a certain probability. This is achieved using a composite time headway distribution model that is estimated as well. Using synthetic data, the method is validated. The method is applied using real life data collected at different two-lane rural roads in the Netherlands to establish free speed distributions and the differences between different vehicle-types. ...|$|E
40|$|Robust {{methods are}} useful in making {{reliable}} statistical inferences when there are small deviations from the model assumptions. The widely used method of the generalized estimating equations can be "robustified" by replacing the standardized residuals with the M-residuals. If the Pearson residuals {{are assumed to be}} unbiased from zero, parameter estimators from the robust approach are asymptotically biased when error distributions are not symmetric. We propose a <b>distribution-free</b> <b>method</b> for correcting this bias. Our extensive numerical studies show that the proposed method can reduce the bias substantially. Examples are given for illustration...|$|E
40|$|Background and objective: This {{study was}} {{conducted}} to define normal reference values and lower limits of normal (LLN) for single-breath carbon monoxide diffusing capacity (DLco) and DLco per unit of alveolar volume (Kco) for Chinese adults in Hong Kong. Methods: Healthy non-smoking men and women aged 18 - 80 years were recruited by random digit dialling. DLco and Kco were measured according to American Thoracic Society standards. Reference equations were obtained by multiple linear regression; LLN were derived by <b>distribution-free</b> <b>method</b> for estimation of age-related centiles. Results: Tests from 568 subjects (259 men, 309 women) were analysed. DLco declined with age in both genders, and increased with height and the interaction term of height and age in men and women, respectively. Considering Hb values did not improve the reference equations. Kco declined with age and increased with weight in both genders, while height and its interaction term with age were additional determinants in women. The reference DLco was lower than some Caucasian values, and was only explained partially by a smaller body size and alveolar volume in Chinese. The <b>distribution-free</b> <b>method</b> yielded better overall approximation to the fifth percentile compared with the traditional method of determining LLN. Conclusions: The equations for reference values and LLN of diffusing capacity derived in this study are of clinical relevance to Chinese subjects. Â© 2007 The Authors. link_to_subscribed_fulltex...|$|E
50|$|Parameter {{estimation}} is done {{by comparing}} the actual covariance matrices representing the relationships between variables and the estimated covariance matrices of the best fitting model. This is obtained through numerical maximization of a fit criterion as provided by maximum likelihood estimation, quasi-maximum likelihood estimation, weighted least squares or asymptotically <b>distribution-free</b> <b>methods.</b> This is often accomplished by using a specialized SEM analysis program, of which several exist.|$|R
40|$|Nonparametric {{statistical}} {{tests are}} robust to disturbances of the as-sumptions on the underlying distri-butions. However, {{in the presence}} of vague data, modelled by fuzzy sets, the problem of robustness is much more complicated. Here not only <b>distribution-free</b> <b>methods</b> but tests which are also not sensitive to the particular choice of the member-ship functions applied for modelling vague concepts would be desirable. A construction of such bi-robust test is suggested in the paper...|$|R
40|$|While {{preserving}} the clear, accessible style of previous editions, Applied Nonparametric Statistical Methods, Fourth Edition reflects {{the latest developments}} in computer-intensive methods that deal with intractable analytical problems and unwieldy data sets. Reorganized and with additional material, this edition begins with a brief summary of some relevant general statistical concepts and an introduction to basic ideas of nonparametric or <b>distribution-free</b> <b>methods.</b> Designed experiments, including those with factorial treatment structures, are now the focus of an entire chapter. The text also...|$|R
40|$|We {{propose a}} simple, <b>distribution-free</b> <b>method</b> for pooling {{synthetic}} control case studies using the mean percentile rank. We also test for heterogeneous treatment effects using {{the distribution of}} estimated ranks, which has a known form. We propose a cross-validation based procedure for model selection. Using 29 cases of state minimum wage increases between 1979 and 2013, we find a sizable, positive and statistically {{significant effect on the}} average teen wage. We do detect heterogeneity in the wage elasticities, consistent with differential bites in the policy. In contrast, the employment estimates suggest a small constant effect not distinguishable from zero...|$|E
40|$|AbstractAn {{expression}} for the stress-strength reliability R=P(X 1 <X 2) is obtained when the vector (X 1, X 2) follows a general bivariate distribution. Such distribution includes bivariate compound Weibull, bivariate compound Gompertz, bivariate compound Pareto, among others. In the parametric case, the maximum likelihood {{estimates of the}} parameters and reliability function R are obtained. In the non-parametric case, point and interval estimates of R are developed using Govindarajulu's asymptotic <b>distribution-free</b> <b>method</b> when X 1 and X 2 are dependent. An example is given when the population distribution is bivariate compound Weibull. Simulation is performed, based on different sample sizes to study the performance of estimates...|$|E
40|$|Biomarkers {{abound in}} many areas of {{clinical}} research, and often investigators are interested in combining them for diagnosis, prognosis and screening. In many applications, the true positive rate for a biomarker combination at a prespecified, clinically acceptable false positive rate is the most relevant measure of predictive capacity. We propose a <b>distribution-free</b> <b>method</b> for constructing biomarker combinations by maximizing the true positive rate while constraining the false positive rate. Theoretical results demonstrate good operating characteristics for the resulting combination. In simulations, the biomarker combination provided by our method demonstrated improved operating characteristics in a variety of scenarios when compared with more traditional methods for constructing combinations...|$|E
40|$|Objective: To derive {{reliable}} {{estimates of}} the sensitivity of HIV- 1 DNA polymerase chain reaction (PCR) in the neonatal period and to quantify the relative contributions of intra-uterine and intra-partum transmission. Methods: After reviewing studies on the early diagnosis of HIV- 1 infection, investigators {{were asked to provide}} published and unpublished PCR test results on prospectively followed, non-breastfed, vertically infected children. Age-specific {{estimates of the}} sensitivity of PCR were derived using <b>distribution-free</b> <b>methods</b> for interval-censored data. Results: Data on 271 infected children were combined for analysis. PCR detected HIV- 1 DNA in an estimated 38...|$|R
40|$|Cost {{data that}} arise in the {{evaluation}} of health care technologies usually exhibit highly skew, heavy-tailed and, possibly, multi-modal distributions. <b>Distribution-free</b> <b>methods</b> for analysing these data, such as the bootstrap, or those based on the asymptotic normality of sample means, may often lead to inefficient or misleading inferences. On the other hand, parametric models that fit the data (or a transformation of the data) equally well can produce very different answers. We consider a Bayesian approach, and model cost data with a distribution composed of a piecewise constant density up to an unknown endpoint, and a generalised Pareto distribution for the remaining tail...|$|R
40|$|Maximum {{likelihood}} estimation in {{confirmatory factor}} analysis requires large sample sizes, normally distributed item responses, and reliable indicators of each latent construct, but these ideals are rarely met. We examine alternative strategies for dealing with non-normal data, particularly when the sample size is small. In two simulation studies, we systematically varied: the degree of non-normality; the sample size from 50 to 1000; the way of indicator formation, comparing items versus parcels; the parcelling strategy, evaluating uniformly positively skews and kurtosis parcels versus those with counterbalancing skews and kurtosis; and the estimation procedure, contrasting maximum likelihood and asymptotically <b>distribution-free</b> <b>methods.</b> We evaluated the convergence behaviour of solutions, {{as well as the}} systematic bias and variability of parameter estimates, and goodness of fit...|$|R
40|$|An {{expression}} for the stress-strength reliability R=P(X 1 <X 2) is obtained when the vector (X 1, X 2) follows a general bivariate distribution. Such distribution includes bivariate compound Weibull, bivariate compound Gompertz, bivariate compound Pareto, among others. In the parametric case, the maximum likelihood {{estimates of the}} parameters and reliability function R are obtained. In the non-parametric case, point and interval estimates of R are developed using Govindarajulu's asymptotic <b>distribution-free</b> <b>method</b> when X 1 and X 2 are dependent. An example is given when the population distribution is bivariate compound Weibull. Simulation is performed, based on different sample sizes to study the performance of estimates...|$|E
40|$|Genetic {{mapping of}} {{quantitative}} trait loci (QTLs) is performed typically {{by using a}} parametric approach, {{based on the assumption}} that the phenotype follows a normal distribution. Many traits of interest, however, are not normally distributed. In this paper, we present a nonparametric approach to QTL mapping applicable to any phenotypic distribution. The method is based on a statistic Z(w), which generalizes the nonparametric Wilcoxon rank-sum test to the situation of whole-genome search by interval mapping. We determine the appropriate significance level for the statistic Z(w), by showing that its asymptotic null distribution follows an Ornstein-Uhlenbeck process. These results provide a robust, <b>distribution-free</b> <b>method</b> for mapping QTLs...|$|E
40|$|We {{investigate}} the operating {{characteristics of the}} Benjamini-Hochberg false discovery rate procedure for multiple testing. This is a <b>distribution-free</b> <b>method</b> that controls the expected fraction of falsely rejected null hypotheses among those rejected. The paper provides a framework for understanding more about this procedure. We first study the asymptotic properties of the `deciding point' "D" that determines the critical "p"-value. From this, we obtain explicit asymptotic expressions for a particular risk function. We introduce the dual notion of false non-rejections and we consider a risk function that combines the false discovery rate and false non-rejections. We also consider the optimal procedure {{with respect to a}} measure of conditional risk. Copyright 2002 Royal Statistical Society. ...|$|E
40|$|In Phase I analysis, {{data are}} used {{retrospectively}} for checking process stability and defining the incontrol state. Most Phase I control charts {{are based on}} the assumption of normally distributed observations. However, <b>distribution-free</b> <b>methods</b> appear to be ideal candidates for Phase I applications. Indeed, because little information is available, it is difficult to validate a distributional assumption in Phase I or at least at its beginning stage. In addition, as has been noted in the literature, this assumption cannot be checked before process stability is established. In this article, we propose a new distribution-free Phase I procedure for univariate observations. The suggested method, based on recursive segmentation and permutation, detects single or multiple mean and/or scale shifts. A simulation study shows that our method compares favorably with parametric control charts when the process is normally distributed and performs better than other nonparametric control charts when the process distribution is skewed or heavy tailed. An R package {{can be found in the}} supplemental materials...|$|R
30|$|A common {{method to}} verify {{assumptions}} that simplify the interference analysis is a comparison against Monte Carlo simulations. In order {{to quantify the}} goodness of fit between the actual interference distribution and its approximation, non-parametric tests, also known as <b>distribution-free</b> inferential <b>method,</b> are commonly applied [152, 153]. Some {{of the most frequently}} used approaches include the Anderson-Darling test, the CramÃ©r-von Mises criterion, and the Kolmogorov-Smirnov test, among others.|$|R
5000|$|Non-parametric (or <b>distribution-free)</b> {{inferential}} statistical <b>methods</b> are mathematical {{procedures for}} statistical hypothesis testing which, unlike parametric statistics, make no {{assumptions about the}} probability distributions of the variables being assessed. The most frequently used tests include ...|$|R
40|$|Contemporary {{studies of}} cost {{efficiency}} {{in the insurance}} industry have predominantly focused on large economies. This paper investigates scale economies and cost efficiency in the Irish life insurance industry by using the translog cost function and <b>distribution-free</b> <b>method.</b> Increasing returns to scale {{are found in the}} industry but the magnitude of cost economies varies with firm size. This study indicates that firms with larger market shares are more likely to exhibit cost efficiency. Furthermore, this study shows that bancassurance firms are more cost efficient than other types of insurers in the Irish life insurance industry, which provides new insights into the cost efficiency of bancassurance. bancassurance; cost efficiency; economies of scale; firm size; life insurance; Ireland. ...|$|E
40|$|The article {{demonstrates}} how the <b>distribution-free</b> <b>method</b> of bootstrapping {{can be applied}} to the construction of confidence intervals for forecasts generated by a dynamic econometric model. Because the exogenous variables must be forecast, the forecasts of the dependent variable are functions of stochastic forecast-period exogenous variables. A dynamic model of pork supply is used to illustrate the procedure. Key words: bootstrapping, confidence intervals, dynamic econometric models, forecasting. The purpose of this article is to apply a rela-tively new nonparametric statistical procedure known as bootstrapping to the problem of con-structing confidence intervals for the point forecasts generated by econometric models. It is well known that the point forecasts obtained from such models are normally distribute...|$|E
40|$|We {{develop a}} finite-sample {{procedure}} {{to test for}} mean-variance efficiency and spanning without imposing any parametric assumptions {{on the distribution of}} model disturbances. In so doing, we provide an exact <b>distribution-free</b> <b>method</b> to test uniform linear restrictions in multivariate linear regression models. The framework allows for unknown forms of non-normalities, and time-varying conditional variances and covariances among the model disturbances. We derive exact bounds on the null distribution of joint F statistics {{in order to deal with}} the presence of nuisance parameters, and we show how to implement the resulting generalized non-parametric bounds tests with Monte Carlo resampling techniques. In sharp contrast to the usual tests that are not computable when the number of test assets is too large, the power of the new test procedure potentially increases along both the time and cross-sectional dimensions...|$|E
40|$|The {{aggregate}} supply function {{developed by}} Lucas (1973) {{predicts that the}} short-run effects of monetary disturbances on real output are negatively related to the variability of such disturbances. This paper assesses the empirical relevance of the Lucas supply function for a large sample of developing countries by using <b>distribution-free</b> statistical <b>methods.</b> The negative relationship {{seems to be a}} robust feature of developing country data and holds true for almost all of the analytical subgroups examined. ...|$|R
30|$|Landslide hazard and {{susceptibility}} zonation mapping {{have been}} carried out by using various methods and techniques using different scales based on the requirement of the end user and the rationale of the investigation [26]. Different landslide hazards and susceptibility mapping methods described by Mantovani et al. [39] include distribution analysis [16, 22, 78], qualitative analysis [17, 41, 43], statistical analysis [53, 55, 67], deterministic analysis [1, 6, 44, 68], landslide frequency analysis [12, 32, 40, 42], and <b>distribution-free</b> <b>methods</b> such as fuzzy logic [34, 36, 52, 53, 54, 69] and artificial neural network (ANN) models [13, 15, 51, 80]. Many researchers adopted the Bureau of Indian Standard [BIS 14496 (Part 2): 1998] guidelines to prepare the landslide hazard zonation mapping [5]. The BIS guidelines [11] were originally proposed by Anbalagan [3], which suggest a quantitative method based on conventional field surveys called landslide hazard evaluation factor (LHEF) rating scheme for Himalaya region. Number of researchers have carried out the landslide hazard zonation mapping based on LHEF rating scheme on different scales using varying number of parameters with some revision for different terrains [4, 5, 33, 35, 60, 61, 62, 65].|$|R
40|$|Biomarker use in {{exposure}} assessment is increasingly common and consideration of related issues is of growing importance. Exposure quantification may be compromised when measurement {{is subject to}} a lower threshold. Statistical modeling of such data requires a decision regarding the handling of such readings. Various authors have considered this problem. In the context of linear regression analysis, Richardson and Ciampi proposed replacement of data below a threshold by a constant equal to the expectation for such data to yield unbiased estimates. Use of such an imputation has some limitations; distributional assumptions are required, and bias reduction in estimation of regression parameters is asymptotic, thereby presenting concerns to small studies. In this paper the authors propose <b>distribution-free</b> <b>methods</b> for managing values below detection limits and evaluate the biases that may result when exposure measurement is constrained by a lower threshold. The authors utilize an analytical approach {{as well as a}} simulation study to assess the effects of the proposed replacement method on estimates. These results may inform decisions regarding analytical plans for future studies as well as provide possible explanation for some amount of discordance seen in extant literature...|$|R
40|$|Wavelet {{thresholding}} generally assumes independent, identically distributed normal errors when estimating {{functions in}} a nonparametric regression setting. VisuShrink and SureShrink {{are just two}} of the many common thresholding methods based on this assumption. When the errors are not normally distributed, however, few methods have been proposed. A <b>distribution-free</b> <b>method</b> for thresholding wavelet coefficients in nonparametric regression is described, which unlike some other non-normal error thresholding methods, does not assume {{the form of the}} non-normal distribution is known. Improvements are made to an existing even-odd cross-validation method by employing block thresholding and level dependence. The efficiency of the proposed method on a variety of non-normal errors, including comparisons to existing wavelet threshold estimators, is shown on both simulated and real data. Comment: 27 pages, 12 figures. The code included in the ancillary materials was updated from the previous versio...|$|E
40|$|We {{study the}} correct {{estimation}} of the true variance of the predictor in stochastic Kriging (SK). First, we obtain macroreplications for a SK metamodel that approximates a single-server simulation model; these macroreplications give independently and identically distributed predictions. This simulation may use common random numbers (CRN). From these macroreplications we conclude that the usual plug-in estimator of the variance signicantly underestimates the true variance. Because macroreplications of practical simulation models are computationally expensive, we next formulate two bootstrap methods that use a single macroreplication: (i) a <b>distribution-free</b> <b>method</b> that resamples simulation replications (within the single macroreplication), and (ii) a parametric method that assumes a Gaussian distribution for the SK predictor, and estimates the (hyper) parameters of that distribution from the single macroreplication. Altogether we recommend distribution-free bootstrapping for the {{estimation of the}} SK predictor variance in practical simulation experiments...|$|E
40|$|We {{apply the}} {{synthetic}} control approach {{in a setting}} with multiple cases and recurring treatments. Using minimum wage changes as an application, we propose a simple <b>distribution-free</b> <b>method</b> for pooling across cases using mean percentile ranks, which have desirable small sample properties. We invert the mean rank statistic in order to construct a confidence interval for the pooled estimate, and we test for the heterogeneity of the treatment effect using the distribution of estimated ranks. We also offer guidance on model selection and match qualityâ issues that are of practical concern in the synthetic control approach generally and when pooling across many cases. Using 32 cases of state minimum wage increases between 1979 and 2013, we do not find a statistically significant effect on teen employment, with the mean elasticity close to zero. There is also no indication of heterogeneous treatment effects. Finally, we discuss some important practical challenges, including the ability to find close matches and the choice of predictors used for constructing a synthetic control...|$|E
40|$|In FY 01 {{we learned}} that {{hardware}} reliability models need substantial changes to account for differences in software, thus making software reliability measurements more effective, accurate, and easier to apply. These reliability models are generally based on familiar distributions or parametric methods. An obvious question is 'What new statistical and probability models can be developed using non-parametric and <b>distribution-free</b> <b>methods</b> instead of the traditional parametric method?" Two approaches to software reliability engineering appear somewhat promising. The first study, begin in FY 01, is based in hardware reliability, a very well established science that has many aspects {{that can be applied}} to software. This research effort has investigated mathematical aspects of hardware reliability and has identified those applicable to software. Currently the research effort is applying and testing these approaches to software reliability measurement, These parametric models require much project data that may be difficult to apply and interpret. Projects at GSFC are often complex in both technology and schedules. Assessing and estimating reliability of the final system is extremely difficult when various subsystems are tested and completed long before others. Parametric and distribution free techniques may offer a new and accurate way of modeling failure time and other project data to provide earlier and more accurate estimates of system reliability...|$|R
40|$|In this lecture {{the general}} {{statistical}} procedure and the principles underlying {{the application of}} the theory to practical problems were considered critically. After a general discussion of the theory of statistical inference and testing of hypotheses, the conditions underlying the application to practical problems were emphasized. The statician always has to find a compromise between his mathematical and his social conscience, because he hardly ever would be allowed to apply the (mathematical) theory to practical problems if he would consult this mathematical conscience only. The correct application of the theory requires for instance that only one statistical test be applied to each set of observations, whereas practice often requires that more than one (dependent) statistical test be applied to the same set of observations. It is however not justified to test hypotheses suggested by a given set of observations, {{on the basis of these}} observations themselves. Finally, the importance of distribution free methods, which are more generally valid thant the so-called parametric methods, was pointed out. It is especially in the development of the theory of <b>distribution-free</b> <b>methods</b> the staff and students of the Department of Statistics of this University hope to participate. Item was scanned at 300 dpi. Scanner used HP Scanjet 5590 P[URL]...|$|R
40|$|Diameter {{distribution}} {{is used in}} most forest management planning packages for predicting stand volume, timber volume and stand growth. The prediction of diameter distribution can be based on parametric distribution functions, <b>distribution-free</b> parametric prediction <b>methods</b> or purely non-parametric methods. In the fi rst case, the {{distribution is}} obtained by predicting the parameters of some probability density function. In a <b>distribution-free</b> percentile <b>method,</b> the diameters at certain percentiles of the distribution are predicted with models. In non-parametric methods, the predicted distribution is a linear combination of similar measured stands. In this study, the percentile based diameter distribution is compared to the results obtained with the Weibull method in four independent data sets. In the case of Scots pine, the other methods are also compared to k-nearest neighbour method. The comparison was made {{with respect to the}} accuracy of predicted stand volume, saw timber volume and number of stems. The predicted percentile and Weibull distributions were calibrated using number of stems measured from the stand. The information of minimum and maximum diameters were also used, for re-scaling the percentile based distribution or for parameter recovery of Weibull parameters. Th...|$|R
40|$|The {{coefficient}} of L-variation (L-CV) {{is commonly used}} in statistical hydrology, in particular in regional frequency analysis, {{as a measure of}} steepness of the frequency curve. The aim of this work is to infer the full frequency distribution of the sample L-CV (and, consequently, its confidence intervals) for small samples and without making assumptions on the underlying parent distribution of the hydrological variable of interest. Several two-parameters candidate distributions are compared {{for a wide range of}} cases using Monte-Carlo simulations. A <b>distribution-free</b> <b>method,</b> recently proposed to estimate the variance structure of sample L-moments, is used to provide the parameters for the candidate distributions. It is shown that the log-Student t distribution approximates best, in most of the cases, the distribution of the sample L-CV and that a simple correction of the bias for the sample L-CV and its variance improves the fit. Also, the parametric method proposed here is demonstrated to perform better than the non-parametric bootstrap. An example of how this result could be used in hydrology is presented, namely in the comparison of methods for regional flood frequency analysis...|$|E
40|$|In {{this paper}} {{the concept of}} 'rank-interaction' is {{introduced}} and a <b>distribution-free</b> <b>method</b> for testing against the presence of 'rank-interaction' is suggested {{in the case of}} a two-way layout (classification) with m (2 Ì 6 gt; I) observations per cell. Roughly speaking rank-interaction can be understood as the phenomenon at which the ranks of the levels of some relevant variable arc different for different classes of the other factor. TIle exaet null distribution of the test statistic has been computed in some cases. The asymptotic distribution under the null hypothesis has been derived. A test suggested by J. V. BRADLEY in his book 'Distribution-free Statistical Tests' (2] is discussed. In the opinion of the authors it is doubtful whether the asymptotic distribution of the test statistic under the nuU hypothesis,asgiven by BRADLEY, is correct. The test of BRADLEY was intended to be sensitive to the presence of interactions defined in the usual way and hence not only to 'rank-interaction'. The same applies to methods proposed by some other authors. We claim that situations exist where one should test against rank-interaction and not against the usual more general alternative...|$|E
40|$|Costs {{associated}} with the evaluation of biomarkers can restrict the number of relevant biological samples to be measured. This common problem has been dealt with extensively in the epidemiologic and biostatistical literature that proposes to apply different cost-efficient procedures, including pooling and random sampling strategies. The pooling design has been widely addressed as a very efficient sampling method under certain parametric assumptions regarding data distribution. When cost is not a main factor {{in the evaluation of}} biomarkers but measurement is subject to a limit of detection, a common instrument limitation on the measurement process, the pooling design can partially overcome this instrumental limitation. In certain situations, the pooling design can provide data that is less informative than a simple random sample; however this is not always the case. Pooled-data-based nonparametric inferences have not been well addressed in the literature. In this article, a <b>distribution-free</b> <b>method</b> based on the empirical likelihood technique is proposed to substitute the traditional parametric-likelihood approach, providing the true coverage, confidence interval estimation and powerful tests based on data obtained after the cost-efficient designs. We also consider several nonparametric tests to compare with the proposed procedure. We examine the proposed methodology via a broad Monte Carlo study and a real data example. ...|$|E
40|$|We develop semiparametric {{tests for}} {{conditional}} independence in time series models of causal effects. Our approach {{is motivated by}} empirical studies of monetary policy effects and is semiparametric {{in the sense that}} we model the process determining the distribution of treatmentâthe policy propensity scoreâbut leave the model for outcomes unspecified. A conceptual innovation is that we adapt the cross-sectional potential outcomes framework to a time series setting. We also develop root-T consistent <b>distribution-free</b> inference <b>methods</b> for full conditional independence testing, appropriate for dependent data and allowing for first-step estimation of the (multinomial) propensity score. Â© 2011 The President and Fellows of Harvard College and the Massachusetts Institute of Technology. ...|$|R
40|$|The {{purpose of}} {{sufficient}} dimension reduction (SDR) {{is to find}} a low-dimensional expression of input features that is sufficient for predicting output values. In this paper, we propose a novel <b>distribution-free</b> SDR <b>method</b> called sufficient component analysis (SCA), which is computationally more efficient than existing methods. In our method, a solution is computed by iteratively performing dependence estimation and maximization: Dependence estimation is analytically carried out by recently-proposed least-squares mutual information (LSMI), and dependence maximization is also analytically carried out by utilizing the Epanechnikov kernel. Through large-scale experiments on real-world image classification and audio tagging problems, the proposed method is shown to compare favorably with existing dimension reduction approaches...|$|R
40|$|Using the <b>distribution-free</b> chain ladder <b>method</b> we {{estimate}} the total ultimate claim amounts at time I and after updating the information at time I + 1. The observable claims development result at time I + 1 for accounting year (I, I + 1] is then defined {{to be the}} difference between these two successive best estimate predictions for the ultimate claim. We analyze the uncertainty of this observable claims development result. ...|$|R
