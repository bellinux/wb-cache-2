1|33|Public
40|$|Abstract. This paper {{presents}} {{the result of}} our continued work on a further enhancement to our previous proposed algorithm. Moving beyond the extraction of word groups {{and based on the}} same irregular pyramid structure the new proposed algorithm groups the extracted words into sentences. The uniqueness of the algorithm is in its ability to process text of a wide variation in terms of size, <b>font,</b> <b>orientation</b> and layout on the same document image. No assumption is made on any specified document type. The algorithm is based on the irregular pyramid structure with the application of four fundamental concepts. The first is the inclusion of background information. The second is the concept of closeness where text information within a group is close to each other, in terms of spatial distance, as compared to other text areas. The third is the “majority win ” strategy that is more suitable under the greatly varying environment than a constant threshold value. The final concept is the uniformity and continuity among words belonging to the same sentence. ...|$|E
50|$|A {{related and}} more {{flexible}} program is FIGlet, which can display text in different <b>fonts</b> and <b>orientations.</b>|$|R
50|$|Windows Phone 8.1 {{adds the}} ability for OEMs and {{individual}} apps to customize their custom lock screen themes even further by skinning the <b>font</b> and <b>orientation</b> of time, date, and notification text.|$|R
40|$|Text {{information}} in natural scene images serves as important clues for many computer vision {{applications such as}} content-based image retrieval, tourist translator, and assistive navigation. Extraction of such information from natural scene images, involves number of sub stages represented by text information extraction (TIE) system. However, performance of such system is greatly influenced by text localization module. Lots of work {{has been reported in}} this field, but it still remained as a challenging problem, due to two main issues: different variety of text patterns like sizes, <b>fonts,</b> <b>orientations,</b> colors, and presence of background outliers similar to text characters, such as windows, bricks. The {{purpose of this paper is}} to study and surveyed existing text localization method and challenges for the same...|$|R
5000|$|Level 4 adds new {{features}} of the IBM LaserPrinter 4029 to the previous PPDS levels. These features include compression, scalable <b>fonts,</b> and enhanced <b>orientations.</b>|$|R
40|$|Garment {{information}} tracking {{is required}} for clean room garment management. In this paper, we present a camera-based robust system with implementation of Optical Character Reconition (OCR) techniques to fulfill garment label recognition. In the system, a camera is used for image capturing; an adaptive thresholding algorithm is employed to generate binary images; Connected Component Labelling (CCL) is then adopted for object detection in the binary image {{as a part of}} finding the ROI (Region of Interest); Artificial Neural Networks (ANNs) with the BP (Back Propagation) learning algorithm are used for digit recognition; and finally the system is verified by a system database. The system has been tested. The results show that it is capable of coping with variance of lighting, digit twisting, background complexity, and <b>font</b> <b>orientations.</b> The system performance with association to the digit recognition rate has met the design requirement. It has achieved real-time and error-free garment information tracking during the testing. 1...|$|R
40|$|Text line {{segmentation}} is {{an important}} step because inaccurately segmented text lines will cause errors in the recognition stage [...] The nature of handwriting makes the process of text line segmentation very challenging. Text characteristics can vary in <b>font,</b> size, <b>orientation,</b> alignment, color, contrast, and background information. These variations turn the process of word detection complex and difficult. Since handwritten text can vary greatly depending on the user skills, disposition and cultural background. In this work we have proposed the method which works on the different intensity values for extracting the text-lines...|$|R
40|$|Extracting text {{objects from}} the PDF images is a {{challenging}} problem. The text data present in the PDF images contain certain useful information for automatic annotation, indexing etc. However variations of the text {{due to differences in}} text style, <b>font,</b> size, <b>orientation,</b> alignment as well as complex structure make the problem of automatic text extraction extremely difficult and challenging job. This paper presents two techniques under block-based classification. After a brief introduction of the classification methods, two methods were enhanced and results were evaluated. The performance metrics for segmentation and time consumption are tested for both the models. Comment: 5 pages, 5 figure...|$|R
40|$|Text line {{segmentation}} is {{an important}} step because inaccurately segmented text lines will cause errors in the recognition stage. Text line segmentation of the handwritten documents {{is still one of the}} most complicated problems in developing a reliable OCR. The nature of handwriting makes the process of text line segmentation very challenging. Text characteristics can vary in <b>font,</b> size, <b>orientation,</b> alignment, color, contrast, and background information. These variations turn the process of word detection complex and difficult. Since handwritten text can vary greatly depending on the user skills, disposition and cultural background. The technique of Piece-wise projection alongwith contour tracing to segment a handwritten document into distinct lines of text is presented. The proposed method is robust to handle line fluctuatio...|$|R
40|$|The mere {{exposure}} effect, {{positive affect}} elicited {{by exposure to}} a previously unfamiliar stimulus, {{is considered one of}} the most well established findings in the psychological literature. Yet its mechanism remains unknown. In Experiments 1 - 5, memory encoding was examined to determine whether the mere exposure effect was a form of conceptual or perceptual implicit priming, and, if not either, whether cardiovascular psychophysiology could reveal its nature. Experiment 1 examined the effects of study phase level of processing on recognition, the mere exposure effect, and word identification implicit priming. Deep relative to shallow processing improved recognition, but did not influence the mere exposure effect or word identification implicit priming. Experiments 2 and 3 examined the effect of study-test changes in <b>font</b> and <b>orientation,</b> respectively, on the mere exposure effect and word identification implicit priming. Different study-test <b>font</b> and <b>orientation</b> reduced word identification implicit priming, but had no influence on the mere exposure effect. The combined results from Experiments 1 - 3 suggested that conceptual and perceptual processing do not drive the mere exposure effect. Experiments 4 and 5 developed and used, respectively, an innovative cardiovascular psychophysiological implicit priming paradigm to examine whether stimulus-specific cardiovascular reactivity at study predicted the mere exposure effect at test. At encoding, stimulus-specific peripheral vasodilatation had predictive value for the mere exposure effect, but not for word identification implicit priming. Experiments 6 and 7 examined whether sustained or transitory anxiety (i. e., trait or state, respectively) would influence the mere exposure effect. Greater trait and state anxiety reduced the mere exposure effect. Together, the findings from these experiments (N = 362) identify a novel affective mechanism of implicit priming that is influenced by cardiovascular psychophysiology and variations in trait and state anxiety...|$|R
40|$|Text Extraction plays a {{major role}} in finding vital and {{valuable}} information. Text extraction involvesdetection, localization, tracking, binarization, extraction, enhancement and recognition of the text from the given image. These text characters are difficult to be detected and recognized due to their deviation of size, <b>font,</b> style, <b>orientation,</b> alignment, contrast, complex colored, textured background. Due to rapid growth of available multimedia documents and growing requirement for information, identification, indexing and retrieval, many researches have been done on text extraction in images. Several techniqueshave been developed for extracting the text from an image. The proposed methods were based on morphological operators, wavelet transform, artificial neural network,skeletonization operation,edge detection algorithm, histogram technique etc. All these techniques have their benefits and restrictions. This article discusses various schemes proposed earlier for extracting the text from an image. This paper also provides the performance comparison of several existing methods proposed by researchers in extracting the text from an image...|$|R
40|$|International audienceThe use of {{ontology}} {{is common}} for many domains such as semantic web, e-commerce, artificial intelligence and geographical information systems. Besides, the vocabulary used in ontologies is always textual; concepts and relations are identified and labelled by words. However, some concepts include a visual aspect especially in the cartographic domain. For example, in a cartographic legend, a point of interest symbol is identified as a concept with its icon and/or abbreviation, color, texture, <b>font</b> style, <b>orientation</b> or number. The overall concerns of our research are to integrate different legends as visual ontologies towards a unique reference one (base map and symbols from many providers). The objective of our article is to 1) propose {{a new type of}} ontology where the concepts are visual and to apply it to cartographic symbols then to 2) develop an application prototype in order to handle these kinds of ontologies with an extension of Web Ontology Language (CartOWL) ...|$|R
40|$|Abstract — Text in {{an image}} may contain {{important}} information. This paper proposes a nearest Neighbors based method to recognize text in images. The proposed method uses color space processing. Histogram analysis and geometrical properties {{are used for}} edge detection. Character recognition is done through OCR which accepts the input in form of text boxes, which are generated through text detection and localization stages. Proposed method is robust {{with respect to the}} <b>font</b> size, color, <b>orientation,</b> and style. Results of the proposed algorithm, by taking real scenes, including indoor and outdoor images, show that this method efficiently extract and localize the scene text...|$|R
40|$|International audienceTopographic paper {{maps are}} a common support for {{geographical}} information. In the field of document analysis {{of this kind of}} support, this paper proposes an automatic approach to extract and recognize toponyms. We present a technique based on image segmentation and connected component processing. Different filtering stages ensure the consistency of plausible characters and strings. Detected text areas are used to feed an OCR software and the recognized words are analyzed and corrected. The main advantage of our technique is that no assumption is made about the character <b>font,</b> size or <b>orientation.</b> Experimental results obtained are encouraging in term of recognition efficiency...|$|R
40|$|This paper {{proposes a}} new {{algorithm}} to detect word groups in imaged documents, using irregular pyramid. The uniqueness of this algorithm is its inclusion of strategic background {{information in the}} analysis where most techniques have discarded. Both foreground (i. e. text area) and portion of background (i. e. white area) regions are examined. The fundamental of the algorithm {{is based on the}} concept of “closeness ” where text information within a group is close to each other, in terms of spatial distance, as compared to other text areas. The result produced by the algorithm is encouraging with the ability to correctly group words of different sizes, <b>fonts,</b> arrangements and <b>orientations.</b> 1...|$|R
40|$|The {{detection}} and extraction of scene text from document images {{is one of}} the challenging research areas. Many researchers have detected and extracted the text from plain text background. But the multi-oriented scene text detection {{is one of the}} complex problems due to multi-oriented texts which have different <b>orientation,</b> <b>font</b> size, colors etc. In this work, we have proposed a new algorithm to detect and extract the multi-oriented scene text. Experiments have been carried out to find the robustness on the proposed method, by conducting various experiments on heterogeneous datasets. The proposed method achieves selectively high detection rate of 88. 43 % on the multi-oriented scene text...|$|R
30|$|Text {{detection}} {{in natural}} scene images {{is a very}} challenging task, far from being completely solved. Complex backgrounds, uneven illumination, and presence of almost unlimited number of text <b>fonts,</b> sizes, and <b>orientations</b> pose great difficulties even to state-of-the-art text detection methods. Unlike document images, where text is usually superimposed on either blank or complex backgrounds and is therefore more distinct [1 – 3], natural scene images deal with scene text, which is already {{a part of the}} captured scene and is often much less distinct. Nevertheless, text detection has become a very popular research area due to its enormous potential in many applicative areas such as sign translation, content-based web image searching, and assisting the visually impaired.|$|R
40|$|Abstract — Scene text {{contains}} {{significant and}} beneficial information. Extraction and localization of scene text {{is used in}} many applications. In this paper, we propose a connected component based method to extract text from natural images. The proposed method uses color space processing. Histogram analysis and geometrical properties are used for edge detection. Character recognition is done through OCR which accepts the input in form of text boxes, which are generated through text detection and localization stages. Proposed method is robust {{with respect to the}} <b>font</b> size, color, <b>orientation,</b> and style. Results of the proposed algorithm, by taking real scenes, including indoor and outdoor images, show that this method efficiently extract and localize the scene text...|$|R
40|$|We {{propose a}} system that reads the Kannada text {{encountered}} in natural scenes with the aim to provide assistance to the visually impaired persons of Karnataka state. This paper describes the system design and standard deviation based Kannada text extraction method. The proposed system contain three main stages text extraction, text recognition and speech synthesis. This paper concentrated on text extraction from images/videos. In this paper: an efficient algorithm which can automatically detect, localize and extract Kannada text from images (and digital videos) with complex backgrounds is presented. The proposed approach {{is based on the}} application of a color reduction technique, a standard deviation base method for edge detection, and the localization of text regions using new connected component properties. The outputs of the algorithm are text boxes with a simple background, ready tobe fed into an OCR engine for subsequent character recognition. Our proposal is robust with respect to different font sizes, <b>font</b> colors, <b>orientation,</b> alignment and background complexities. The performance of the approach is demonstrated by presenting promising experimental results for a set of images taken from different types of video sequences...|$|R
40|$|The text data {{present in}} images and video contain certain useful {{information}} for automatic annotation,indexing, and structuring of images. However {{variations of the}} text {{due to differences in}} text style, <b>font,</b> size, <b>orientation,</b> alignment as well as low image contrast and complex background make the problem of automatic text extraction extremely difficult and challenging job. A large number of techniques have been proposed to address this problem and {{the purpose of this paper}} is to design algorithms for each phase of extracting text from a video using java libraries and classes. Here first we frame the input video into stream of images using the Java Media Framework (JMF) with the input being a real time or a video from the database. Then we apply pre processing algorithms to convert the image to gray scale and remove the disturbances like superimposed lines over the text, discontinuity removal, and dot removal. Then we continue with the algorithms for localization, segmentation and recognition for which we use the neural network pattern matching technique. The performance of our approach is demonstrated by presenting experimental results for a set of static images...|$|R
40|$|This paper {{proposed}} a new algorithm to perform text extraction from image document. The paper focused in {{the extraction of}} word group. Irregular pyramid structure is {{used as the basis}} of the algorithm. The uniqueness of this algorithm is its inclusion of strategic background information in the analysis where most techniques have discarded. Both foreground (i. e. text area) and portion of background (i. e. white area) regions are examined. The fundamental of the algorithm is based on the concept of “closeness ” where text information within a group is closed to each other, in terms of spatial distance, as compare to other text area. The result produced by the algorithm is encouraging with the ability to correctly group words of different size, <b>font,</b> arrangement and <b>orientation...</b>|$|R
40|$|Text {{within a}} camera grabbed image can contain {{a huge amount}} of meta data about that scene. Such meta data can be useful for identification, {{indexing}} and retrieval purposes. Detection of coloured scene text is a new challenge for all camera based images. Common problems for text extraction from camera based images are the lack of prior knowledge of any kind of text features such as colour, <b>font,</b> size and <b>orientation.</b> In this paper we propose a new algorithm for the extraction of text from an image which can overcome these problems. In addition, problems due to an unconstrained complex background in the scene has also been addressed. Here a new technique is applied to determine the discrete edges around the text boundaries. A novel methodology is also proposed to extract the text exploiting its appearance in terms of colour and spatial distribution...|$|R
40|$|The diploma thesis {{presents}} a description {{and implementation of}} some modern techniques and methods for optical character recognition in images of natural scenes. When choosing methods, we focused on speed and accuracy. As a basis we have chosen the method of directional segment features with nonlinear mesh since it was developed for the mobile platform and thus meets the criteria of speed. Also, the method comparable to other methods reaches very good results. The proposed method was further upgraded with some other popular features extraction methods and classifiers. Optical recognition of text in images of natural scenes is very problematic, because in them the text appears {{in a variety of}} sizes, colors, <b>fonts</b> and <b>orientations.</b> Also pictures of natural scenes typically have lower quality and contain complex background, which greatly complicates the process of recognition. Similar to the classic optical character recognition the systems for optical character recognition in the images of natural scenes typically consist of four steps: preprocessing, segmentation, feature extraction and classification. Preprocessing phase is designed to improve image quality, in the segmentation stage only the pixels that belong to each character are chosen in the picture. Both steps are due to the aforementioned problems of natural scenes extremely important. During the feature extraction phase the characteristics of a segmented character are calculated, which are user for further classification of the character corresponding class. All implemented methods were tested on image databases ICDAR, CVL OCR DB, and a hybrid collection that we have generated from the two mentioned databases. Improved method presented in the thesis has achieved good results and is, in conjunction with the relevant text detection in images of natural scenes, suitable for migration and the use on the mobile platform...|$|R
40|$|This paper {{deals with}} the problem of extracting the text {{information}} from complex ground from color document images. Developing general framework for separating the foreground text and background information from complex document image is still a challenging problem because of its high unpredictability and complexity. In this paper a new interval type- 2 fuzzy based thresholding method is proposed for processing color document images. The proposed method is experimented with varying background of multiple colors and texture and foreground text in any color, <b>font,</b> size and <b>orientation.</b> Experimental results show that the performance of the proposed approach is better than the existing ones. the development of new algorithms because they are nonlinear knowledge based methods. It can be able to remove grayness ambiguities in a robust way. Zadeh[2] first introduced the Type- 1 Fuzzy Set(T 1 FS) theory in 1965 and has been successfully applied in many areas including image processing, modeling and control, data mining, timeseries prediction, etc. An example of a T 1 FS is shown in Fig. 1 1 µA (x) µA 1 µA (x) X Keywords- Document image preprocessing, Interval type- 2 fuzzy, thresholding, binarization 1...|$|R
40|$|POSTSCRIPT is a page {{description}} language {{which is used}} to transmit printing information from a host computer (i. e. Apple Macintosh) to a printer (i. e. Apple LaserWriter Plus). It has the ability to describe pages consisting of text, vector graphics, and scanned bit-map images. Printing text is the area of concentration for this thesis. Specifically several variables that affect the printing speed of a common POSTSCRIPT printer, the Apple LaserWriter Plus, are looked at when printing text in a variety of <b>fonts,</b> sizes, and <b>orientations.</b> The variables that affect printer performance include: - use of outline vs. bit-map fonts; - the outline font rasterization process; - the use of pre-cached bit-map fonts; - background outline font rasterization; - arbitrary scaling and rotation; - downloading host-resident fonts; - Adobe and Third Party host-resident downloadable fonts vs. printer-resident fonts; - Appletalk vs. RS- 232 communications interfaces; - use of the POSTSCRIPT show, ashow, and widthshow instructions; - targeting the POSTSCRIPT instructions at a particular engine resolution; - print engine overhead A sequence of POSTSCRIPT files were transmitted to the Apple LaserWriter Plus printer. The experiments were carefully constructed to exercize each of the variables listed above. Performance measurements were carefully recorded and analyzed. Where applicable, improvements were proposed to improve printer performance...|$|R
40|$|Fig. 1 : Comparison of axis {{labeling}} algorithms. Our {{extension of}} Wilkinson’s optimization approach produces nice labelings while maintaining good visual {{density of the}} labels and coverage of the data. Abstract—The non-data components of a visualization, such as axes and legends, can often be {{just as important as}} the data itself. They provide contextual information essential to interpreting the data. In this paper, we describe an automated system for choosing positions and labels for axis tick marks. Our system extends Wilkinson’s optimization-based labeling approach [11] to create a more robust, full-featured axis labeler. We define an expanded space of axis labelings by automatically generating additional nice numbers as needed and by permitting the extreme labels to occur inside the data range. These changes provide flexibility in problematic cases, without degrading quality elsewhere. We also propose an additional optimization criterion, legibility, which allows us to simultaneously optimize over label formatting, <b>font</b> size, and <b>orientation.</b> To solve this revised optimization problem, we describe the optimization function and an efficient search algorithm. Finally, we compare our method to previous work using both quantitative and qualitative metrics. This paper is a good example of how ideas from automated graphic design can be applied to information visualization. Index Terms—Axis labeling, nice numbers. ...|$|R
40|$|Extraction of {{foreground}} contents {{in complex}} background document images {{is very difficult}} as background texture, color and foreground font, size, color, tilt are not known in advance. In this work, we propose a RGB color model for the input of complex color document images. An algorithm to detect the text regions using Gabor filters followed by extraction of text using color feature luminance is developed too. The proposed approach consists of three stages. Based on the Gabor features, the candidate image segments containing text are detected in stage- 1. Because of complex background, certain amount of high frequency non-text objects in the background are also detected as text objects in stage- 1. In stage- 2, certain amount of false text objects is dropped by performing the connected component analysis. In stage- 3, the image segments containing textual information, which are obtained from the previous stage are binarized to extract the foreground text. The color feature luminance is extracted from the input color document image. The threshold value is derived automatically using this color feature. The proposed approach handles both printed and handwritten color document images with foreground text in any color, <b>font,</b> size and <b>orientation.</b> For experimental evaluations, we have considered a variety of document images having non-uniform/uniform textured and multicolored background. Performance of segmentation of foreground text is evaluated on a commercially available OCR. Evaluation results show better recognition accuracy of foreground characters in the processed document images against unprocessed document images...|$|R
40|$|Text {{detection}} and recognition is a challenging problem in document analysis due 10 {{the presence of}} the unpredictable nature of video texts, such as the variations of <b>orientation,</b> <b>font</b> and size, illumination effects, and even different 20 / 30 text shadows. In this paper, we propose a novel horizontal and vertical symmetry feature by calculating the gradient direction and the gradient magnitude of each text candidate, which results in Potential Text Candidates (PTCs) after applying the k-means clustering algorithm on the gradient image of each input frame to verify PTC, we explore temporal information of video by proposing an iterative process that continuously verifies the PTCs of the first frame and the successive frames, until the process meets the converging criterion. This outputs Stable Potential Text Candidates (SPTCs). For each, PTC, the method obtains text representatives {{with the help of the}} edge image of the input frame. Then for each text representative, we divide it into four quadrants and check a new Mutual Nearest Neighbor Symmetry (MNNS) based on the dominant stroke width distances of the four quadrants. A voting method is finally proposed to clasify each text block as either 2 D or 3 D by counting the text representatives that satisfy MNNS. Experimental results on clasifying 2 D and 3 D text images are promising, and the result re further validated by text {{detection and}} recognition before clasification and after clasification with the exiting methods, respectively...|$|R
40|$|Abstract—Text {{detection}} and recognition is a challenging problem in document analysis {{due to the}} presence of the unpredictable nature of video texts, such as the variations of <b>orientation,</b> <b>font</b> and size, illumination effects, and even different 2 D/ 3 D text shadows. In this paper, we propose a novel horizontal and vertical symmetry feature by calculating the gradient direction and the gradient magnitude of each text candidate, which results in Potential Text Candidates (PTCs) after applying the k-means clustering algorithm on the gradient image of each input frame. To verify PTCs, we explore temporal information of video by proposing an iterative process that continuously verifies the PTCs of the first frame and the successive frames, until the process meets the converging criterion. This outputs Stable Potential Text Candidates (SPTCs). For each SPTC, the method obtains text representatives {{with the help of the}} edge image of the input frame. Then for each text representative, we divide it into four quadrants and check a new Mutual Nearest Neighbor Symmetry (MNNS) based on the dominant stroke width distances of the four quadrants. A voting method is finally proposed to classify each text block as either 2 D or 3 D by counting the text representatives that satisfy MNNS. Experimental results on classifying 2 D and 3 D text images are promising, and the results are further validated by text {{detection and}} recognition before classification and after classification with the exiting methods, respectively. Keywords—Video text frames, Horizontal and vertical symmetry, Video potential text candidates, Dominant potential text candidates, 2 D and 3 D text video classification I...|$|R
40|$|Printed {{schedules}} {{are critical}} to mass transit mobility, perhaps no more so than to bus transit users who often embark from locations where information is not provided. For economic reasons, they also rely heavily on transit. Schedules are their lifeline. After becoming concerned with the readability of its bus schedules, New Jersey Transit (NJT) enlisted an interdisciplinary research and design team from the New Jersey Institute of Technology (NJIT) to analyze, redesign, and test the agency’s bus timetables over an 18 -month period beginning in 2003. The process included precedent research, community outreach, graphic design, laboratory testing, and survey methods. It began with a literature survey and review of timetables produced by other agencies. Two focus groups were convened to incorporate user viewpoints. Based on these methods and acknowledging the institutional and production constraints of the agency, two prototype timetables were designed {{for one of the}} agency’s most complex bus routes. The prototypes and the current schedule for the route were time-tested in a laboratory with 30 participants. A survey was given to the same participants. The analysis of the experimental data was partially inconclusive due to high error rates for all schedules tested. However, in the survey, a majority of participants showed preference for aspects developed in the prototypes, offering the agency important production recommendations regarding <b>font</b> sizes, text <b>orientation</b> and graphic display methods, as well as institutional directives regarding data transfer, 61 Journal of Public Transportation, Vol. 9, No. 4, 2006 maps, zone designations, passenger information, and telephone contacts. This article recounts this process and offers to the larger transit community the conclusions of this interdisciplinary approach, not combined in this manner before, to make bus transit more attractive and efficient...|$|R
40|$|Often we {{encounter}} documents with text printed on complex color background. Readability of textual contents in such documents is very poor due to {{complexity of the}} background and mix up of color(s) of foreground text with colors of background. Automatic segmentation of foreground text in such document images is very much essential for smooth reading of the document contents either by human or by machine. In this {{paper we propose a}} novel approach to extract the foreground text in color document images having complex background. The proposed approach is a hybrid approach which combines connected component and texture feature analysis of potential text regions. The proposed approach utilizes Canny edge detector to detect all possible text edge pixels. Connected component analysis is performed on these edge pixels to identify candidate text regions. Because of background complexity {{it is also possible that}} a non-text region may be identified as a text region. This problem is overcome by analyzing the texture features of potential text region corresponding to each connected component. An unsupervised local thresholding is devised to perform foreground segmentation in detected text regions. Finally the text regions which are noisy are identified and reprocessed to further enhance the quality of retrieved foreground. The proposed approach can handle document images with varying background of multiple colors and texture; and foreground text in any color, <b>font,</b> size and <b>orientation.</b> Experimental results show that the proposed algorithm detects on an average 97. 12 % of text regions in the source document. Readability of the extracted foreground text is illustrated through Optical character recognition (OCR) in case the text is in English. The proposed approach is compared with some existing methods of foreground separation in document images. Experimental results show that our approach performs better...|$|R
40|$|Text is {{no longer}} {{confined}} to scanned pages and often appears in camera-based images originating from text on real world objects. Unlike the images from conventional flatbed scanners, which have a controlled acquisition environment, camera-based images pose new challenges such as uneven illumination, blur, poor resolution, perspective distortion and 3 D deformations that can severely affect the performance of any optical character recognition (OCR) system. Due to the variations in the imaging condition {{as well as the}} target document type, traditional OCR systems, designed for scanned images, cannot be directly applied to camera-captured images and a new level of processing needs to be addressed. In this thesis, we study some of the issues commonly encountered in camera-based image analysis and propose novel methods to overcome them. All the methods make use of color connected components. 1. Connected component descriptor for document image mosaicing Document image analysis often requires mosaicing when {{it is not possible to}} capture a large document at a reasonable resolution in a single exposure. Such a document is captured in parts and mosaicing stitches them into a single image. Since connected components (CCs) in a document image can easily be extracted regardless of the image rotation, scale and perspective distortion, we design a robust feature named connected component descriptor that is tailored for mosaicing camera-captured document images. The method involves extraction of a circular measurement region around each CC and its description using the angular radial transform (ART). To ensure geometric consistency during feature matching, the ART coefficients of a CC are augmented with those of its 2 nearest neighbors. Our method addresses two critical issues often encountered in correspondence matching: (i) the stability of features and (ii) robustness against false matches due to multiple instances of many characters in a document image. We illustrate the effectiveness of the proposed method on camera-captured document images exhibiting large variations in viewpoint, illumination and scale. 2. Font and background color independent text binarization The first step in an OCR system, after document acquisition, is binarization, which converts a gray-scale/color image into a two-level image -the foreground text and the background. We propose two methods for binarization of color documents whereby the foreground text is output as black and the background as white regardless of the polarity of foreground-background shades. (a) Hierarchical CC Analysis: The method employs an edge-based connected component approach and automatically determines a threshold for each component. It overcomes several limitations of existing locally-adaptive thresholding techniques. Firstly, it can handle documents with multi-colored texts with different background shades. Secondly, the method is applicable to documents having text of widely varying sizes, usually not handled by local binarization methods. Thirdly, the method automatically computes the threshold for binarization and the logic for inverting the output from the image data and does not require any input parameter. However, the method is sensitive to complex backgrounds since it relies on the edge information to identify CCs. It also uses script-specific characteristics to filter out edge components before binarization and currently works well for Roman script only. (b) Contour-based color clustering (COCOCLUST) : To overcome the above limitations, we introduce a novel unsupervised color clustering approach that operates on a ‘small’ representative set of color pixels identified using the contour information. Based on the assumption that every character is of a uniform color, we analyze each color layer individually and identify potential text regions for binarization. Experiments on several complex images having large variations in <b>font,</b> size, color, <b>orientation</b> and script illustrate the robustness of the method. 3. Multi-script and multi-oriented text extraction from scene images Scene text understanding normally involves a pre-processing step of text detection and extraction before subjecting the acquired image for character recognition task. The subsequent recognition task is performed only on the detected text regions so as to mitigate the effect of background complexity. We propose a color-based CC labeling for robust text segmentation from natural scene images. Text CCs are identified using a combination of support vector machine and neural network classifiers trained on a set of low-level features derived from the boundary, stroke and gradient information. We develop a semiautomatic annotation toolkit to generate pixel-accurate groundtruth of 100 scenic images containing text in various layout styles and multiple scripts. The overall precision, recall and f-measure obtained on our dataset are 0. 8, 0. 86 and 0. 83, respectively. The proposed method is also compared with others in the literature using the ICDAR 2003 robust reading competition dataset, which, however, has only horizontal English text. The overall precision, recall and f-measure obtained are 0. 63, 0. 59 and 0. 61 respectively, which is comparable to the best performing methods in the ICDAR 2005 text locating competition. A recent method proposed by Epshtein et al. [1] achieves better results but it cannot handle arbitrarily oriented text. Our method, however, works well for generic scene images having arbitrary text orientations. 4. Alignment of curved text lines Conventional OCR systems perform poorly on document images that contain multi-oriented text lines. We propose a technique that first identifies individual text lines by grouping adjacent CCs based on their proximity and regularity. For each identified text string, a B-spline curve is fitted to the centroids of the constituent characters and normal vectors are computed along the fitted curve. Each character is then individually rotated such that the corresponding normal vector is aligned with the vertical axis. The method has been tested on a data set consisting of 50 images with text laid out in various ways namely along arcs, waves, triangles and a combination of these with linearly skewed text lines. It yields 95. 9 % recognition accuracy on text strings, where, before alignment, state-of-the-art OCRs fail to recognize any text. The CC-based pre-processing algorithms developed are well-suited for processing camera-captured images. We demonstrate the feasibility of the algorithms on the publicly-available ICDAR 2003 robust reading competition dataset and our own database comprising camera-captured document images that contain multiple scripts and arbitrary text layouts...|$|R

