1298|67|Public
25|$|It can {{be shown}} that for the output of Markov {{information}} sources, <b>Kolmogorov</b> <b>complexity</b> {{is related to the}} entropy of the information source. More precisely, the <b>Kolmogorov</b> <b>complexity</b> of the output of a Markov information source, normalized by the length of the output, converges almost surely (as the length of the output goes to infinity) to the entropy of the source.|$|E
25|$|Algorithmic {{information}} theory {{is the area}} of computer science that studies <b>Kolmogorov</b> <b>complexity</b> and other complexity measures on strings (or other data structures).|$|E
25|$|Theorem: There exist {{strings of}} {{arbitrarily}} large <b>Kolmogorov</b> <b>complexity.</b> Formally: for each n ∈ ℕ, {{there is a}} string s with K(s) ≥ n.|$|E
40|$|AbstractIntuitively, {{the program}} size {{complexity}} of a binary string measures {{the amount of}} information in the string. Researchers have formalized this notion {{in a number of different}} ways. Here, we demonstrate similarities between some of these formulations. We also investigate in some detail the properties of <b>Kolmogorov's</b> <b>complexity</b> measure...|$|R
40|$|International audienceInspired by genetic {{programming}} (GP), we study iterative algorithms for non-computable tasks and {{compare them to}} naive models. This framework justifies many practical standard tricks from GP and also provides complexity lower-bounds which justify the computational cost of GP thanks {{to the use of}} <b>Kolmogorov's</b> <b>complexity</b> in bounded time...|$|R
40|$|Kolmogorov's goal in proposing his {{complexity}} {{conception of}} probability {{was to provide}} a better foundation for the applications of probability (as opposed to the theory of probability; he believed that his 1933 axioms were sufficient for the theory of probability). The complexity conception was a natural development of Kolmogorov's earlier frequentist conception combined with (a) his conviction that only finite data sequences are of any interest in the applications of probability, and (b) Turing's discovery of the universal computing device. Besides the complexity conception itself, its developments by Martin-Lof, Levin et al will be briefly discussed; I will also list some advantages and limitations of <b>Kolmogorov's</b> <b>complexity</b> conception and the algorithmic theory of randomness in general. Introduction 1 Theory Applications Kolmogorov's axiomatic conception of probability Modern algorithmic theory of randomness <b>Kolmogorov's</b> <b>complexity</b> conception of probability Conditi [...] ...|$|R
25|$|The {{notion of}} <b>Kolmogorov</b> <b>complexity</b> {{can be used}} to state and prove impossibility results akin to Cantor's {{diagonal}} argument, Gödel's incompleteness theorem, and Turing's halting problem.|$|E
25|$|More formally, the {{complexity}} of a string is {{the length of the}} shortest possible description of the string in some fixed universal description language (the sensitivity of complexity relative to the choice of description language is discussed below). It can be shown that the <b>Kolmogorov</b> <b>complexity</b> of any string cannot be more than a few bytes larger than the length of the string itself. Strings like the abab example above, whose <b>Kolmogorov</b> <b>complexity</b> is small relative to the string's size, are not considered to be complex.|$|E
25|$|In particular, {{for almost}} each object {{it is not}} {{possible}} to compute even a lower bound for its <b>Kolmogorov</b> <b>complexity</b> (Chaitin 1964), let alone its exact value.|$|E
40|$|C. Calude, A. Nies, L. Staiger, and F. Stephan {{posed the}} {{following}} question about the relation between plain and prefix <b>Kolmogorov</b> <b>complexities</b> (see their paper in DLT 2008 conference proceedings) : does the domain of every optimal decompressor contain the domain of some optimal prefix-free decompressor? In this paper we provide a negative answer to this question...|$|R
40|$|A 1976 theorem of Chaitin {{can be used}} to {{show that}} {{arbitrarily}} dense sets of lengths n have a paucity of trivial strings (only a bounded number of strings of length n having trivially low plain <b>Kolmogorov</b> <b>complexities).</b> We use the probabilistic method to give a new proof of this fact. This proof is much simpler than previously published proofs, and it gives a tighter paucity bound. ...|$|R
40|$|AbstractGiven a n×n {{positive}} semidefinite matrix A and a subspace S of Cn, Σ(S,A) {{denotes the}} shorted matrix of A to S. We consider the notion of spectral shorted matrixρ(S,A) =limm→∞Σ(S,Am) 1 /m. Wecompletely characterize this martix in terms of S and the spectrum and the eigenspaces of A. We show the relation of this notion with the spectral order of matrices and the <b>Kolmogorov’s</b> <b>complexity</b> of A to a vector ξ∈Cn...|$|R
25|$|There {{are several}} other {{variants}} of <b>Kolmogorov</b> <b>complexity</b> or algorithmic information. The {{most widely used}} one is based on self-delimiting programs, and is mainly due to Leonid Levin (1974).|$|E
25|$|An axiomatic {{approach}} to <b>Kolmogorov</b> <b>complexity</b> based on Blum axioms (Blum 1967) {{was introduced by}} Mark Burgin in the paper presented for publication by Andrey Kolmogorov (Burgin 1982).|$|E
25|$|The Kolmogorov {{structure}} {{function is}} used in the algorithmic information theory, also known as the theory of <b>Kolmogorov</b> <b>complexity,</b> for describing the structure of a string by use of models of increasing complexity.|$|E
40|$|Computability in {{the limit}} {{represents}} the non-plus-ultra of constructive describability. It {{is well known}} that the limit computable functions on naturals are exactly those computable with the oracle for the halting problem. However, prefix (<b>Kolmogorov)</b> <b>complexities</b> defined with respect to these two models may differ. We introduce and compare several natural variations of prefix complexity definitions based on generalized Turing machines embodying the idea of limit computability, as well as complexities based on oracle machines, for both finite and infinite sequences...|$|R
40|$|Abstract The {{very first}} Kolmogorov's paper on {{algorithmic}} information theory [1] was entitled &quot;Three {{approaches to the}} definition of the quantity of information&quot;. These three approaches were called combinatorial, probabilistic and algorithmic. Trying to establish formal connections between combinatorial and algorithmic approaches, we prove that every linear inequality including <b>Kolmogorov</b> <b>complexities</b> could be translated to an equivalent combinatorial statement. Entropy (comlexity) proofs of combinatorial inequalities given in [5] and [2] can be considered as a special cases (and a natural starting points) for this translation...|$|R
40|$|This is an expository talk {{written for}} the Bourbaki Seminar. After a brief introduction, Section 1 {{discusses}} in the categorical language {{the structure of the}} classical deterministic computations. Basic notions of complexity icluding the P/NP problem are reviewed. Section 2 introduces the notion of quantum parallelism and explains the main issues of quantum computing. Section 3 is devoted to four quantum subroutines: initialization, quantum computing of classical Boolean functions, quantum Fourier transform, and Grover's search algorithm. The central Section 4 explains Shor's factoring algorithm. Section 5 relates <b>Kolmogorov's</b> <b>complexity</b> to the spectral properties of computable function. Appendix contributes to the prehistory of quantum computing. Comment: 27 pp., no figures, amste...|$|R
25|$|A related {{approach}} to Hutter's prize which appeared much {{earlier in the}} late 1990s is the inclusion of compression problems in an extended Turing Test. or by tests which are completely derived from <b>Kolmogorov</b> <b>complexity.</b>|$|E
25|$|In 1973 Kolmogorov {{proposed}} a non-probabilistic approach to statistics and model selection. Let each datum be a finite binary string {{and a model}} be finite sets of binary strings. Consider model classes consisting of models of given maximal <b>Kolmogorov</b> <b>complexity.</b>|$|E
25|$|One {{possible}} conclusion from {{mixing the}} concepts of <b>Kolmogorov</b> <b>complexity</b> and Occam's razor is that an ideal data compressor {{would also be a}} scientific explanation/formulation generator. Some {{attempts have been made to}} re-derive known laws from considerations of simplicity or compressibility.|$|E
40|$|This paper {{investigates the}} {{distribution}} and nonuniform complexity of problems that are complete or weakly complete for ESPACE under nonuniform reductions that are computed by polynomial-size circuits (P/Poly-Turing reductions and P/Poly-many-one reductions). A tight, exponential lower bound on the space-bounded <b>Kolmogorov</b> <b>complexities</b> of weakly P/PolyTuring -complete problems is established. A Small Span Theorem for P/Poly-Turing reductions in ESPACE is proven and used to show that every P/Poly-Turing degree [...] - including the complete degree [...] - has measure 0 in ESPACE. (In contrast, {{it is known that}} almost every element of ESPACE is weakly P-many-one complete.) Every weakly P/Poly-many-one-complete problem is shown to have a dense, exponential, nonuniform complexity core. More importantly, the P/Poly-many-one-complete problems are shown to be unusually simple elements of ESPACE, {{in the sense that they}} obey nontrivial upper bounds on nonuniform complexity (size of nonuniform complexit [...] ...|$|R
40|$|AbstractBased on <b>Kolmogorov's</b> idea, <b>complexity</b> of {{positive}} definite matrices {{with respect to}} a unit vector is defined. We show that the range of the complexity coincides with the logarithm of its spectrum and the order induced by the complexity is equivalent to the spectral one. This order implies the reversed one induced by the operator entropy...|$|R
40|$|AbstractWe {{initiate}} {{the theory of}} communication complexity of individual inputs held by the agents. This contrasts with the usual communication complexity model, where one counts the amount of communication for the worst-case or the average-case inputs. The individual communication complexity gives more information (the worst-case and the average-case {{can be derived from}} it but not vice versa) and may in some cases be of more interest. It is given in terms of the <b>Kolmogorov</b> <b>complexities</b> of the individual inputs. There are different measures of communication complexity depending on whether the protocol is guaranteed to be correct for all inputs or not, and whether there's one-way or two-way communication. Bounds are provided for the communication of specific functions and connections between the different communication measures are shown. Some counter-intuitive results: for deterministic protocols that need to communicate Bob's input to Alice they need to communicate all of Bob's input (rather than the information difference with Alice's input), and there are so-called “non-communicable” inputs...|$|R
25|$|In {{algorithmic}} {{information theory}} (a subfield of computer science and mathematics), the <b>Kolmogorov</b> <b>complexity</b> of an object, {{such as a}} piece of text, is the length of the shortest computer program (in a predetermined programming language) that produces the object as output.|$|E
25|$|There {{are various}} papers in {{scholarly}} journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, {{and using it}} {{to come up with}} criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and <b>Kolmogorov</b> <b>complexity.</b>|$|E
25|$|It {{states that}} the {{shortest}} program that reproduces X and Y {{is no more than}} a logarithmic term larger than a program to reproduce X and a program to reproduce Y given X. Using this statement, one can define an analogue of mutual information for <b>Kolmogorov</b> <b>complexity.</b>|$|E
40|$|In {{this paper}} we present an {{unconventional}} image segmentation approach which is devised {{to meet the}} requirements of image understanding and pattern recognition tasks. Generally image understanding assumes interplay of two sub-processes: image information content discovery and image information content interpretation. Despite of its widespread use, the notion of "image information content" is still ill defined, intuitive, and ambiguous. Most often, it is used in the Shannon's sense, which means information content assessment averaged over the whole signal ensemble. Humans, however,rarely resort to such estimates. They are very effective in decomposing images into their meaningful constituents and focusing attention to the perceptually relevant image parts. We posit that following the latest findings in human attention vision studies and the concepts of <b>Kolmogorov's</b> <b>complexity</b> theory an unorthodox segmentation approach can be proposed that provides effective image decomposition to information preserving image fragments well suited for subsequent image interpretation. We provide some illustrative examples, demonstrating effectiveness of this approach. Comment: 14 th Scandinavian Conference on Image Analysis (SCIA 2005...|$|R
40|$|Abstract. We {{consider}} a methodology {{for the development}} and application of a class of generators that are externally parameterized tools enabling to generate Web component instances on demand {{depending on the context}} of use. Such generators are generalized entities of conventional Web components that indeed are lower-level generators for the portal domain. We use one-stage heterogeneous metaprogramming techniques for implementing the externally parameterized metaprograms as a specification of the generators. The first our contribution is a systemized process to create the externally parameterized metaprograms for building Web domain generators. The process describes a logical linking into the coherent structure of the following entities: semantic model for change, program generator model, Web component instance model, and given metalanguages. The second our contribution is the complexity estimation of Web component generators that were developed and used for generating Web component instances to incorporate them into real portal settings. The complexity is estimated using the <b>Kolmogorov’s</b> <b>complexity</b> measures and Cyclomatic Complexity. We analyze also specific features and characteristics of the developed generators...|$|R
40|$|Abstract. Image {{understanding}} and image semantics processing have recently {{become an issue}} of critical importance in computer vision R&D. Biological vision has always considered them as an enigmatic mixture of perceptual and cognitive processing faculties. In its impetuous and rash development, computer vision without any hesitations has adopted this stance. I will argue that such a segregation of image processing faculties is wrong, both for the biological and the computer vision. My conjecture is that images contain only one sort of information – the perceptual (physical) information, which can be discovered in an image and elicited for further processing. Cognitive (semantic) information is {{not a part of}} image-conveyed information. It belongs to a human observer that acquires and interprets the image. Relying on a new definition of “information”, which can be derived from <b>Kolmogorov’s</b> <b>complexity</b> theory and Chaitin’s notion of algorithmic information, I propose a unifying framework for visual information processing, which explicitly accounts for perceptual and cognitive image processing peculiarities. I believe, it would provide better scaffolding for modeling visual information processing in human brain. ...|$|R
25|$|The {{field of}} <b>Kolmogorov</b> <b>complexity</b> and {{algorithmic}} randomness was {{developed during the}} 1960s and 1970s by Chaitin, Kolmogorov, Levin, Martin-Löf and Solomonoff (the names are given here in alphabetical order; {{much of the research}} was independent, and the unity of the concept of randomness was not understood at the time). The main idea is to consider a universal Turing machine U and to measure the complexity of a number (or string) x as the length of the shortest input p such that U(p) outputs x. This approach revolutionized earlier ways to determine when an infinite sequence (equivalently, characteristic function of a subset of the natural numbers) is random or not by invoking a notion of randomness for finite objects. <b>Kolmogorov</b> <b>complexity</b> became not only a subject of independent study but is also applied to other subjects as a tool for obtaining proofs.|$|E
25|$|Entropy {{effectively}} bounds {{the performance}} of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. See also <b>Kolmogorov</b> <b>complexity.</b> In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.|$|E
25|$|A quite {{different}} approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence. Examples {{of these kinds}} of tests start in the late nineties devising intelligence tests using notions from <b>Kolmogorov</b> <b>complexity</b> and data compression. Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.|$|E
40|$|I {{proposed}} rate tolerance and discussed {{its relation to}} rate distortion in my book "A Generalized Information Theory" published in 1993. Recently, I examined the structure function and the complexity distortion based on <b>Kolmogorov's</b> <b>complexity</b> theory. It is my understanding now that complexity-distortion is only a special case of rate tolerance while constraint sets change from fuzzy sets into clear sets that look like balls with the same radius. It is not true that the complexity distortion is generally equivalent to rate distortion as claimed by the researchers of complexity theory. I conclude that a rate distortion function can only be equivalent to a rate tolerance function {{and both of them}} can be described by a generalized mutual information formula where P(Y|X) is equal to P(Y|Tolerance). The paper uses GPS as an example to derive generalized information formulae and proves the above conclusions using mathematical analyses and a coding example. The similarity between the formula for measuring GPS information and the formula for rate distortion function can deepen our understanding the generalized information measure. Comment: 6 pages, 4 figure...|$|R
40|$|The article further {{develops}} <b>Kolmogorov's</b> algorithmic <b>complexity</b> theory. The {{definition of}} randomness {{is modified to}} satisfy strong invariance properties (conservation inequalities). This allows definitions of concepts such as mutual information in individual infinite sequences. Applications to several areas, like probability theory, theory of algorithms, intuitionistic logic are considered. These theories are simplified substantially with the postulate that the objects they consider are independent of (have small mutual information with) any sequence specified by a mathematical property...|$|R
40|$|The paper {{introduces}} the recent results {{related to an}} entropy functional on trajectories of a controlled diffusion process, and the information path functional (IPF), analyzing their connections to the <b>Kolmogorov's</b> entropy, <b>complexity</b> and the Lyapunov's characteristics. Considering the IPF's essentials and specifics, the paper studies the singularities of the IPF extremal equations and the created invariant relations, which both are useful for the solution of important mathematical and applied problems. Keywords: Additive functional; Entropy; Singularities, Natural Border Problem; Invarian...|$|R
