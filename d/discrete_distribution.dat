727|1980|Public
25|$|The mode is not {{necessarily}} unique to a given <b>discrete</b> <b>distribution,</b> since the probability mass function may take the same maximum value at several points x1, x2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently.|$|E
25|$|In {{computer}} science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. Two different distributions are commonly used: binary trees formed by inserting nodes {{one at a}} time according to a random permutation, and binary trees chosen from a uniform <b>discrete</b> <b>distribution</b> in which all distinct trees are equally likely. It is also possible to form other distributions, for instance by repeated splitting. Adding and removing nodes directly in a random binary tree will in general disrupt its random structure, but the treap and related randomized binary search tree data structures use the principle of binary trees formed from a random permutation in order to maintain a balanced binary search tree dynamically as nodes are inserted and deleted.|$|E
500|$|In {{probability}} theory and statistics, the Dirac delta function {{is often used}} to represent a <b>discrete</b> <b>distribution,</b> or a partially discrete, partially continuous distribution, using a probability density function (which is normally used to represent fully continuous distributions). [...] For example, the probability density function f(x) of a <b>discrete</b> <b>distribution</b> consisting of points x = {x1, ..., xn}, with corresponding probabilities p1, ..., pn, can be written as ...|$|E
50|$|Classification of <b>discrete</b> <b>distributions</b> by variance-to-mean ratio; see cumulants of some <b>discrete</b> {{probability}} <b>distributions</b> for details.|$|R
40|$|This paper {{proposes a}} nonparametric Poisson kernel density {{estimation}} technique for <b>discrete</b> <b>distributions.</b> Economists {{have been using}} continuous kernels to approximate <b>discrete</b> <b>distributions.</b> This work introduces a discrete kernel as more appropriate for approximating <b>discrete</b> <b>distributions.</b> Simulation results are presented to compare with standard parametric approaches. We apply our discrete Poisson kernel estimator to approximate the distribution of coal mine wildcat strikes in the United States. ...|$|R
40|$|Abstract: Central {{moments are}} usually {{calculated}} {{in terms of}} non central moments. This may be difficult in many <b>discrete</b> <b>distributions.</b> In this paper, the factorial moments are simply calculated. Then a closed form is deduced for the central moments. A new number is produced to simplify calculations of the required moments. Central moments are calculated for some <b>discrete</b> <b>distributions.</b> Key Words: central moments, factorial moments, <b>discrete</b> <b>distributions</b> 1...|$|R
2500|$|The Poisson {{distribution}} with parameter [...] is a <b>discrete</b> <b>distribution</b> for [...] Its {{probability mass}} function is given by ...|$|E
2500|$|Equivalently, if the {{situation}} {{is characterized by a}} <b>discrete</b> <b>distribution</b> function fk (k=0,...,W) where [...] fk is the fraction of the population with income k and W = N is the total income, then [...] and the Theil index is: ...|$|E
2500|$|The {{linear search}} {{problem for a}} general {{probability}} distribution is unsolved. However, there exists a dynamic programming algorithm that produces a solution for any <b>discrete</b> <b>distribution</b> [...] and also an approximate solution, for any probability distribution, with any desired accuracy.|$|E
5000|$|There {{are only}} three <b>discrete</b> <b>distributions</b> that satisfy the full form of this relationship: the Poisson, {{binomial}} and negative binomial distributions. These are also the three <b>discrete</b> <b>distributions</b> among the six members of the natural exponential family with quadratic variance functions (NEF-QVF).|$|R
40|$|This text {{presents}} an overview on multivariate <b>discrete</b> <b>distributions</b> {{such as the}} multivariate Poisson or Pascal distributions. The paper summarizes uniform representations by probability generating functions to obtain similar expressions {{as compared to the}} univariate <b>distribution</b> analogues. Multivariate <b>discrete</b> <b>distributions</b> Probability generating functions Dependence structure...|$|R
40|$|The {{negative}} binomial and {{the geometric}} are characterized {{in terms of}} the necessary and sufficient conditions for the infinite and finite divisibility of <b>discrete</b> <b>distributions.</b> The Poisson is characterized {{in terms of the}} necessary and sufficient conditions for the finite divisibility of <b>discrete</b> <b>distributions.</b> Finally, the binomial, Bernoulli and uniform distributions are characterized in terms of the necessary and sufficient conditions for the noninfinite divisibility of <b>discrete</b> <b>distributions.</b> The significance of these characterizations is briefly discussed. necessary and sufficient conditions induction combinatorial properties infinite vectors and matrices...|$|R
2500|$|Intuitively, a {{continuous}} random variable is the one which can take {{a continuous}} range of values—as opposed to a <b>discrete</b> <b>distribution,</b> where the set of possible values for the random variable is at most countable. While for a <b>discrete</b> <b>distribution</b> an event with probability zero is impossible (e.g., rolling 3 on a standard dice is impossible, and has probability zero), {{this is not so}} {{in the case of a}} {{continuous random variable}}. For example, if one measures the width of an oak leaf, the result of 3½cm is possible; however, it has probability zero because uncountably many other potential values exist even between 3cm and 4cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval [...] is nonzero. This apparent paradox is resolved by the fact that the probability that X attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.|$|E
2500|$|... where m is {{the sample}} maximum and k is the sample size, {{sampling}} without replacement (though this distinction almost surely makes no difference for a continuous distribution). This follows {{for the same reasons}} as estimation for the <b>discrete</b> <b>distribution,</b> and {{can be seen as a}} very simple case of maximum spacing estimation. This problem is commonly known as the German tank problem, due to application of maximum estimation to estimates of German tank production during World War II.|$|E
2500|$|Computationally, {{this method}} {{involves}} computing the quantile {{function of the}} distribution — in other words, computing the cumulative distribution function (CDF) of the distribution (which maps a number in the domain to a probability between 0 and 1) and then inverting that function. This {{is the source of}} the term [...] "inverse" [...] or [...] "inversion" [...] in most of the names for this method. Note that for a <b>discrete</b> <b>distribution,</b> computing the CDF is not in general too difficult: we simply add up the individual probabilities for the various points of the distribution. For a continuous distribution, however, we need to integrate the probability density function (PDF) of the distribution, which is impossible to do analytically for most distributions (including the normal distribution). As a result, this method may be computationally inefficient for many distributions and other methods are preferred; however, it is a useful method for building more generally applicable samplers such as those based on rejection sampling.|$|E
40|$|Properties of the {{generalized}} hypergeometric series functions are employed {{to get the}} recurrence relation for inverse moments and inverse factorial moments of some <b>discrete</b> <b>distributions.</b> Meanwhile, with {{the existence of the}} recurrence relations, the accurate value for inverse moment of <b>discrete</b> <b>distributions</b> can thus be obtained...|$|R
50|$|Other {{examples}} of <b>discrete</b> univariate <b>distributions</b> include the binomial, geometric, negative binomial, and Poisson distributions. At least 750 univariate <b>discrete</b> <b>distributions</b> {{have been reported}} in the literature.|$|R
5000|$|... #Subtitle level 3: <b>Discrete</b> <b>distributions</b> with {{specified}} mean ...|$|R
2500|$|Another (agnostic) way {{to analyze}} the {{observed}} mutational spectra and DNA sequence context of mutations in tumors involves pooling all mutations of different types and contexts from cancer samples into a <b>discrete</b> <b>distribution.</b> If multiple cancer samples are available, their context-dependent mutations can be represented {{in the form of}} a nonnegative matrix. This matrix can be further decomposed into components (mutational signatures) which ideally should describe individual mutagenic factors. Several computational methods have been proposed for solving this decomposition problem. The first implementation of Non-negative Matrix Factorization (NMF) method is available in Sanger Institute Mutational Signature Framework {{in the form of a}} MATLAB package. On the other hand, if mutations from a single tumor sample are only available, the DeconstructSigs R package and [...] may provide the identification of contributions of different mutational signatures for a single tumor sample. In addition, MutaGene server provides mutagen or cancer-specific mutational background models and signatures that can be applied to calculate expected DNA and protein site mutability to decouple relative contributions of mutagenesis and selection in carcinogenesis.|$|E
5000|$|Another way {{to define}} a {{unimodal}} <b>discrete</b> <b>distribution</b> is by the occurrence of sign changes in the sequence of differences of the probabilities. A <b>discrete</b> <b>distribution</b> with a probability mass function, , is called unimodal if the sequence [...] has exactly one sign change (when zeroes don't count).|$|E
5000|$|... #Subtitle level 3: <b>Discrete</b> <b>distribution,</b> {{continuous}} {{parameter space}} ...|$|E
40|$|In {{this paper}} we {{consider}} some widely utilized classes of <b>discrete</b> <b>distributions</b> and aim {{to provide a}} systematic overview about their preservation under convolution. This paper {{will serve as a}} detailed reference for the study and applications of the preservation of the discrete NBU(2), NBUCA classes of <b>discrete</b> <b>distributions...</b>|$|R
40|$|The Sibuya {{distribution}} {{plays an}} important role in considering several <b>discrete</b> self-decomposable <b>distributions.</b> Here we will consider several properties of the Sibuya distribution. The main results will concern the discrete self-decomposability and infinite divisibility of the scaled Sibuya distribution in dependence of the scale parameter. Discrete self-decomposability Infinite divisibility Sibuya <b>distributions</b> <b>Discrete</b> Linnik <b>distributions</b> <b>Discrete</b> stable <b>distributions...</b>|$|R
5000|$|... #Subtitle level 3: Likelihoods for mixed {{continuous}} - <b>discrete</b> <b>distributions</b> ...|$|R
5000|$|Golomb coding is {{the optimal}} prefix code for the {{geometric}} <b>discrete</b> <b>distribution.</b>|$|E
5000|$|In {{probability}} theory and statistics, the Dirac delta function {{is often used}} to represent a <b>discrete</b> <b>distribution,</b> or a partially discrete, partially continuous distribution, using a probability density function (which is normally used to represent fully continuous distributions). For example, the probability density function f(x) of a <b>discrete</b> <b>distribution</b> consisting of points x = {x1, ..., xn}, with corresponding probabilities p1, ..., pn, can be written as ...|$|E
5000|$|There {{does not}} exist a Van Houtum {{distribution}} for every combination of [...] and [...] By using {{the fact that for}} any real mean [...] the <b>discrete</b> <b>distribution</b> on the integers that has minimal variance is concentrated on the integers [...] and , it is easy to verify that a Van Houtum distribution (or indeed any <b>discrete</b> <b>distribution</b> on the integers) can only be fitted on the first two moments if ...|$|E
40|$|Type I {{error and}} power of the {{standard}} independent samples t-test were compared with the trimmed and Winsorized t-test with respect to continuous <b>distributions</b> and various <b>discrete</b> <b>distributions</b> known to occur in applied data. The continuous and <b>discrete</b> <b>distributions</b> were generated with similar levels of skew and kurtosis but the <b>discrete</b> <b>distributions</b> had a variety of structural features not reflected in the continuous distributions. The results showed that the Type I error rates of the t-tests were not seriously affected, but the power rate of the trimmed and Winsorized t-test varied greatly across the considered distributions...|$|R
40|$|Characterizations of {{measures}} of concordance depend on an ordering of bivariate distributions with fixed marginals {{or use the}} concept of a copula to define a ordering without fixed but with continuous marginals. Ordinal variables with a fixed number of categories have <b>discrete</b> bivariate <b>distributions.</b> But for <b>discrete</b> <b>distributions</b> copulas are not unique. Therefore, Scarsini (1984) propose an ordering for <b>discrete</b> <b>distributions.</b> But {{it is not possible to}} check whether a measure of concordance hold this ordering. Following Scarsini we consider for <b>discrete</b> <b>distributions</b> a special copula and define an ordering based on this special copula. We give formulas for the traditional measures of concordance (like Kendall's Pi and Spearman's p) which hold the ordering and are only slight modifications of the well-known formulas. [...] ...|$|R
50|$|For <b>discrete</b> <b>distributions,</b> {{there is}} no {{universal}} agreement on selecting the quartile values.|$|R
5000|$|The Poisson {{distribution}} with parameter [...] is a <b>discrete</b> <b>distribution</b> for [...] Its {{probability mass}} function is given by ...|$|E
5000|$|... {{for which}} k &ge; &phi; = 1.61803398879&hellip;, the golden ratio, or, more generally, for any <b>discrete</b> <b>distribution</b> for which ...|$|E
50|$|In {{probability}} theory, a continuity correction is {{an adjustment}} {{that is made}} when a <b>discrete</b> <b>distribution</b> is approximated by a continuous distribution.|$|E
40|$|If X 1 ([theta]), [...] .,Xn([theta]) are {{independent}} random variables having <b>discrete</b> <b>distributions</b> depending on a positive parameter [theta][set membership, variant](0,[rho]) and satisfying appropriate assumptions, the expected extreme order statistics EXn:n([theta]) and EX 1 :n([theta]) have nice monotonicity properties. Such properties extend to random extreme order statistics, but not to intermediate order statistics. <b>Discrete</b> <b>distributions</b> One-parameter family Order statistics Expected value Monotonicity properties...|$|R
25|$|<b>Discrete</b> <b>distributions</b> {{and some}} {{continuous}} distributions (like the Cantor distribution) do not admit such a density.|$|R
40|$|As {{probabilistic}} computations play {{an increasing}} role in solving various problems, researchers have designed probabilistic languages to facilitate their modeling. Most {{of the existing}} probabilistic languages, however, focus only on <b>discrete</b> <b>distributions,</b> {{and there has been}} little effort to develop probabilistic languages whose expressive power is beyond <b>discrete</b> <b>distributions.</b> This dissertation presents a probabilistic language, called PTP (ProbabilisTic Programming), which supports all kinds of probability distributions...|$|R
