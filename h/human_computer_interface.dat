337|8686|Public
2500|$|A system {{architecture}} primarily {{concentrates on the}} internal interfaces among the system's [...] or subsystems, and on the interface(s) [...] between the system and its external environment, especially the user. [...] (In the specific case of computer systems, this latter, special, interface {{is known as the}} computer human interface, AKA <b>human</b> <b>computer</b> <b>interface,</b> or CHI; formerly called the man-machine interface.) ...|$|E
5000|$|... #Subtitle level 2: <b>Human</b> <b>Computer</b> <b>Interface</b> (HCI) {{control input}} ...|$|E
5000|$|... {{the user}} {{interface}} shell (the <b>human</b> <b>computer</b> <b>interface)</b> and the database ...|$|E
5000|$|Illiteracy: Text user {{interfaces}} {{do not work}} very well, innovative <b>Human</b> <b>Computer</b> <b>Interfaces</b> (see <b>Human</b> <b>Computer</b> Interaction) are required.|$|R
40|$|Metaphors {{and visual}} formalisms are {{commonly}} seen as two different and separate concepts {{for the design}} of <b>human</b> <b>computer</b> <b>interfaces.</b> This article describes both concepts and their advantages and disadvantages. We claim that thorough consideration of the use of these concepts enhances the design of <b>human</b> <b>computer</b> <b>interfaces.</b> We also claim that strict separation of these concepts is not always possible or useful. Practical examples are given to support our claims and to demonstrate the problems as well as the possibilities of metaphors and visual formalisms in interface design...|$|R
5000|$|The institutes work in {{the field}} of Usability largely focuses on <b>human</b> <b>computer</b> <b>interfaces</b> (HCI), {{information}} architecture and semantic web. Most of the projects are user-oriented and include the following: ...|$|R
5000|$|Head-mounted {{displays}} are {{not designed}} to be workstations, and traditional input devices such as keyboard and mouse {{do not support the}} concept of smartglasses. Instead <b>Human</b> <b>Computer</b> <b>Interface</b> (HCI) control input needs to be methods lend themselves to mobility and/or hands-free use are good candidates. A wide body of literature in <b>human</b> <b>computer</b> <b>interface</b> can be classified into three main categories, which are hand-held, touch, and touchless input [...] The examples are listed as follows.|$|E
5000|$|Standards are {{associated}} with technologies, systems, systems nodes, and data, and refer to technical standards for information processing, information transfer, data, security, and <b>human</b> <b>computer</b> <b>interface.</b>|$|E
50|$|Face {{detection}} {{is used in}} biometrics, {{often as}} a part of (or together with) a facial recognition system. It is also used in video surveillance, <b>human</b> <b>computer</b> <b>interface</b> and image database management.|$|E
40|$|<b>Human</b> <b>computer</b> <b>interfaces</b> still {{lack the}} ability to {{identify}} and respond to usersâ€™ emotional and mental states, despite evidence that such knowledge may improve interaction (Picard, 1997). Human communication often supplements verbal messages with other channels, such as vocal nuances, facial expressions, postur...|$|R
40|$|In this paper, {{we present}} an {{overview}} of research in our laboratories on Multimodal <b>Human</b> <b>Computer</b> <b>Interfaces.</b> The goal for such interfaces is to free <b>human</b> <b>computer</b> interaction from the limitations and acceptance barriers due to rigid operating commands and keyboards as only/main I/O-device. Instead we move to involve all available human communication modalities. These human modalities include Speech, Gesture and Pointing...|$|R
40|$|An {{international}} journal on software components, large-scale software, software correctness and security, object-oriented techniques, programming paradigms, multilanguage programming, multithreading and distributed applications, high-performance computing, web services and virtual machines, algorithms, data structures and techniques, <b>human</b> <b>computer</b> <b>interfaces</b> with. NET, related projects with. NET, educational software and teaching object-oriented paradigms with. NE...|$|R
5000|$|Though the {{previous}} level specifies {{details of the}} implementation, the outputs of this stage are implementation-independent and concentrate on the requirements for the <b>human</b> <b>computer</b> <b>interface.</b> The logical design specifies the main methods of interaction in terms of menu structures and command structures.|$|E
50|$|Incorporating three {{fundamental}} {{pillars of}} rebooting computing, including energy efficiency, security, and <b>Human</b> <b>Computer</b> <b>Interface</b> (HCI), the initiative seeks to overcome setbacks and challenges {{relating to the}} deceleration of computational power and capacity. In turn, these efforts may also be applied in other technology sectors, such as the Internet of Things.|$|E
50|$|In {{the field}} of {{engineering}} and medicine ergonomics and <b>human</b> <b>computer</b> <b>interface</b> are to key areas where design theory plays an important role. Within {{the field of}} economics, marketing affects {{the parameters of the}} design. In the 1930s, Henry Dreyfuss developed the field of ergonomic design through his application of anthropometrics.|$|E
50|$|Screen {{generating}} {{programs are}} also commonly used and they enable prototypers to show user's systems {{that do not}} function, but show what the screens may look like. Developing <b>Human</b> <b>Computer</b> <b>Interfaces</b> can sometimes be the critical part of the development effort, since to the users the interface essentially is the system.|$|R
40|$|Wearable {{computing}} and Augmented Reality applications {{call for}} less obtrusive and more intuitive <b>human</b> <b>computer</b> <b>interfaces</b> than keyboards and mice. One way to realize such interfaces is using gestures, e. g., for pointing {{in order to}} replace the mouse. The less obtrusive way of gesture recognition is to use computer vision based methods...|$|R
40|$|International audienceThis paper {{focuses on}} the formal {{validation}} and verification of multi-modal <b>human</b> <b>computer</b> <b>interfaces.</b> It describes part of the obtained results of the French RNRT VERBATIM project whose purpose is the Multimodal Interfaces BIformal Verification and Test Automation. This project {{focuses on the}} application of a formal technique, namely the event B method. This approach is based on a proof technique and therefore it does not suffer from the state number explosion problem occurring in classical model checking. We outline the capability of this technique to support the design of multi-modal <b>human</b> <b>computer</b> <b>interfaces,</b> in particular, the capability to support the expression and the verification of properties issued from the CARE family. The proposed approach uses notations and semi-formal techniques issued from the HCI design area. We apply our approach on a case study called "CLIPS Yellow Pages"...|$|R
5000|$|A system {{architecture}} primarily {{concentrates on the}} internal interfaces among the system's [...] or subsystems, and on the interface(s) between the system and its external environment, especially the user. (In the specific case of computer systems, this latter, special, interface {{is known as the}} computer human interface, AKA <b>human</b> <b>computer</b> <b>interface,</b> or CHI; formerly called the man-machine interface.) ...|$|E
50|$|Recent {{exhibitions}} and books, including Dominic Lopes' A Philosophy of Computer Art(2009) {{have sought to}} examine the integral role of coding in contemporary art beyond that of <b>Human</b> <b>Computer</b> <b>Interface</b> (HCI). Criticising Lopes however, Juliff and Cox argue that Lopes continues to privilege interface and user {{at the expense of}} the integral condition of code in much computer art. Arguing for a more nuanced appreciation of coding, Juliff and Cox set out contemporary creative coding as the examination of code and intentionality as integral to the users understanding of the work. ibid.|$|E
50|$|Upon {{graduation}} from University of Pennsylvania in 1985, Hirschberg joined AT&T Bell Labs as a Member of Technical {{staff in the}} Linguistics Research Department, where she worked on improving prosody assignment for Text-to-Speech Synthesis (TTS) in the Bell Labs TTS system. She was promoted to Department Head in 1994 when she created a new <b>Human</b> <b>Computer</b> <b>Interface</b> Research Lab. She and her department remained at Bell Labs until 1996 when they moved to AT&T Labs Research {{as part of a}} corporate reorganization. In 2002, she joined the Columbia University faculty as a Professor in the Department of Computer Science.|$|E
40|$|With {{the advance}} of {{information}} technology capabilities, {{and the importance of}} <b>human</b> <b>computer</b> <b>interfaces</b> within society there has been a significant increase in research activity within the field of <b>human</b> <b>computer</b> interaction (HCI). This paper summarizes some of the work undertaken to date, paying particular attention to methods applicable to on-line control and monitoring systems such as those employed by The National Grid Company plc...|$|R
40|$|Controlling {{appliances}} in home environments by gestures is a {{step towards}} more intuitive and natural <b>human</b> <b>computer</b> <b>interfaces.</b> A brief overview on an existing vision based gesture recognition system and its architecture and details on ergonomic remote control of devices by gestures are clarified. The {{focus is on the}} motion detection, object normalization and identification, the modelling and the prediction of motion by the Kalman Filter. The initialization problem of the Kalman Filter of a vision based system for human motion tracking differs from initialization for physical systems, where manuals report measurement errors. A main interest was to develop the initialization and adequate Kalman model for human motion. Most aspects mentioned in this report were implemented in the ARGUS prototype. 1 Introduction <b>Human</b> <b>computer</b> <b>interfaces</b> based on visual input (video) have found growing interest during the last few years. One reason may be the continuously falling expense of hardware for [...] ...|$|R
40|$|Virtual {{presence}} and cross-reality interactions. Enhanced techniques for HMD-based virtual reality systems. Virtual Reality, Augmented Reality, Mixed Reality systems and applications. 3 D <b>Human</b> <b>Computer</b> <b>interfaces</b> and 3 D UI metaphors. Motion representation and editing. Shape representation, level of detail. Implicit surfaces and convolution surfaces: modeling, rendering, animation. Modeling of natural phenomena with implicit surfaces. Physically-based modeling and simulation. High-quality rendering...|$|R
50|$|A {{hardware}} {{architecture is}} {{primarily concerned with}} the internal electrical (and, more rarely, the mechanical) interfaces among the system's components or subsystems, and the interface between the system and its external environment, especially the devices operated by or the electronic displays viewed by a user. (This latter, special interface, {{is known as the}} computer human interface, AKA <b>human</b> <b>computer</b> <b>interface,</b> or HCI; formerly called the man-machine interface.) Integrated circuit (IC) designers are driving current technologies into innovative approaches for new products. Hence, multiple layers of active devices are being proposed as single chip, opening up opportunities for disruptive microelectronic, optoelectronic, and new microelectromechanical hardware implementation.|$|E
5000|$|Ubiquitous {{computing}} {{research has}} focused on building an environment in which computers allow humans to focus attention on select aspects of the environment and operate in supervisory and policy-making roles. Ubiquitous computing emphasizes the creation of a <b>human</b> <b>computer</b> <b>interface</b> that can interpret and support a user's intentions. For example, MIT's Project Oxygen seeks to create a system in which computation is as pervasive as air:In the future, computation will be human centered. It will be freely available everywhere, like batteries and power sockets, or oxygen in the air we breathe...We will not need to carry our own devices around with us. Instead, configurable generic devices, either handheld or embedded in the environment, will bring computation to us, whenever we need it and wherever we might be. As we interact with these [...] "anonymous" [...] devices, they will adopt our information personalities. They will respect our desires for privacy and security. We won't have to type, click, or learn new computer jargon. Instead, we'll communicate naturally, using speech and gestures that describe our intent...|$|E
5000|$|The final BEA report {{points to}} the <b>Human</b> <b>Computer</b> <b>Interface</b> (HCI) of the Airbus as a {{possible}} factor contributing to the crash. It provides an explanation {{for most of the}} pitch-up inputs by the pilot flying (PF), left unexplained in the Popular Mechanics piece: namely that the Flight Director (FD) display was misleading. [...] The pitch-up input {{at the beginning of the}} fatal sequence of events appears to be the consequence of an altimeter error. The investigators also pointed to the lack of a clear display of the airspeed inconsistencies even though the computers had identified them. Some systems generated failure messages only about the consequences but never mentioned the origin of the problem. The investigators recommended that a blocked pitot tube should be clearly indicated as such to the crew on the flight displays. The Daily Telegraph pointed out the absence of angle of attack information, which is so important in identifying and preventing a stall. The paper stated that [...] "though angle of attack readings are sent to onboard computers, there are no displays in modern jets to convey this critical information to the crews." [...] Der Spiegel indicated the difficulty the pilots faced in diagnosing the problem: [...] "One alarm after another lit up the cockpit monitors. One after another, the autopilot, the automatic engine control system, and the flight computers shut themselves off." [...] Against this backdrop of confusing information, difficulty with aural cognition (due to heavy buffeting from the storm as well as the stall) and zero external visibility, the pilots had less than three minutes to identify the problem and take corrective action. The Spiegel report asserts that such a crash [...] "could happen again." ...|$|E
40|$|Part of the Graphics and <b>Human</b> <b>Computer</b> <b>Interfaces</b> Commons, and the Library and Information Science Commons This Article {{is brought}} to you for free and open access by the VCU Libraries at VCU Scholars Compass. It has been {{accepted}} for inclusion in VCU Libraries Faculty and Staff Publications by an authorized administrator of VCU Scholars Compass. For more information, please contac...|$|R
40|$|Almost all <b>Human</b> <b>Computer</b> <b>Interfaces</b> involve {{vision and}} Pedagogical {{research}} encourages {{the use of}} multiple modalities including vision. The combination of visual and other modalities, as well as the many submodalities of vision, has both advantages and pitfalls. The work presented here connects psychological research into human cognitive and perceptual processes and limitations, to evaluation and optimization of multimodal HCI...|$|R
40|$|This paper {{describes}} a novel {{form of display}} using crossmodal output. A crossmodal icon is an abstract icon that can be instantiated {{in one of two}} equivalent forms (auditory or tactile). These can be used in interfaces as a means of non-visual output. This paper discusses how crossmodal icons can be constructed and the potential benefits they bring to mobile <b>human</b> <b>computer</b> <b>interfaces...</b>|$|R
40|$|ABSTRACT: The most {{important}} desire {{of this paper}} is to provide the symposium on <b>Human</b> <b>Computer</b> <b>Interface</b> (HCI). HCI is also called as Man Machine Interaction (MMI) or Computer Human Interaction (CHI). Effective evaluation techniques of HCI cause how the users can operate the systems more effortless and security. It encompasses Introduction design process, star life cycle, user interface management system (UIMS), user experience in <b>human</b> <b>computer</b> <b>interface,</b> evaluation techniques and applications of HCI. Persistent use of the star life cycle, processors an...|$|E
40|$|A Gaming-Simulation Environment for {{teaching}} and learning that incorporates Intelligent Tutoring Support {{also serves as a}} supporting tool for the <b>Human</b> <b>Computer</b> <b>Interface.</b> The <b>Human</b> <b>Computer</b> <b>Interface</b> to this environment will affect how players interact with the domain that is both being tutored upon and is being the object of game play. The development of this interface is subject to a variety of constraints. This paper suggests a Hypertext Approach to developing this Integrated Environment and thus the Interface that promises to oust or minimise these constraints...|$|E
40|$|The {{purpose of}} the {{research}} was to examine <b>human</b> <b>computer</b> <b>interface</b> design within an Irish context. The primary objective was to identify and examine the critical issues of <b>human</b> <b>computer</b> <b>interface</b> design, and their application in an Irish context, as focus is shifting from stand-alone alone applications towards web-based applications. For the {{purpose of the}} study it was decided to e-mail a questionnaire to a random sample of five hundred organisations in order to obtain the information required. In total, sixty completed and valid questionnaires were returned. The overall response rate was 12...|$|E
40|$|We {{argue that}} {{cognitive}} models {{should be used}} in analysing the usability of multi-modal <b>human</b> <b>computer</b> <b>interfaces</b> and further, that formal methods can be advantageously applied to such analysis. In pursuing this objective we specify the Interacting Cognitive Subsystems model formally using the process calculus LOTOS and then we verify that it satisfies certain behavioural goals formulated in the interval temporal logic Mexitl...|$|R
50|$|Iterative {{design is}} {{commonly}} used {{in the development of}} <b>human</b> <b>computer</b> <b>interfaces.</b> This allows designers to identify any usability issues that may arise in the user interface before it is put into wide use. Even the best usability experts cannot design perfect user interfaces in a single attempt, so a usability engineering lifecycle should be built around the concept of iteration.|$|R
40|$|Abstract. This paper {{presents}} {{two kinds}} of novel haptic mouse systems as new <b>human</b> <b>computer</b> <b>interfaces,</b> which have a force and tactile feedback capability. The first one can reflect 1 dof grabbing force as well as 2 dof translation force. Five-bar mechanism has been adapted to realize the 2 dof translation force feedback, and double prismatic joint mechanism {{has been used to}} implement the grabbing force feedback. This system helps the user to feel grabbing force, contact force and weight while picking up and moving an object in virtual envi-ronment. The second system can simulate the surface roughness as well as con-tact force. This system consists of two parts: a 2 DOF force feedback device for kinesthetic display and a tactile feedback unit for displaying the normal stimu-lation to the skin and the skin stretch. The proposed systems are expected to be used as new <b>human</b> <b>computer</b> <b>interfaces</b> by presenting realistic haptic interac-tion in e-commerce or VR environment. ...|$|R
