11|53|Public
50|$|Bebugging (or {{fault seeding}} or <b>error</b> <b>seeding)</b> {{is a popular}} {{software}} engineering technique used in the 1970s to measure test coverage. Known bugs are randomly added to a program source code and the programmer is tasked to find them. The percentage of the known bugs not found gives {{an indication of the}} real bugs that remain.|$|E
40|$|Previous {{spreadsheet}} inspection {{experiments have}} had human subjects look for seeded errors in spreadsheets. In this study, subjects attempted to find errors in human-developed spreadsheets {{to avoid the}} potential artifacts created by <b>error</b> <b>seeding.</b> Human subject success rates were compared to the successful rates for error-flagging by spreadsheet static analysis tools (SSATs) applied to the same spreadsheets. The human error detection results were {{comparable to those of}} studies using <b>error</b> <b>seeding.</b> However, Excel Error Check and Spreadsheet Professional were almost useless for correctly flagging natural (human) errors in this study. Comment: 14 Pages, 4 Figure...|$|E
40|$|A {{previously}} reported experiment in <b>error</b> <b>seeding</b> {{as a program}} validation technique is summarized. The experiment was designed to test the validity of three assumptions on which the alleged effectiveness of <b>error</b> <b>seeding</b> is based. Errors were seeded into 17 functionally identical but independently programmed Pascal programs {{in such a way}} as to produce 408 programs, each with one seeded error. Using mean time to failure as a metric, results indicated that it is possible to generate seeded errors that are arbitrarily but not equally difficult to locate. Examination of indigenous errors demonstrated that these are also arbitrarily difficult to locate. These two results support the assumption that seeded and indigenous errors are approximately equally difficult to locate. However, the assumption that, for each type of error, all errors are equally difficult to locate was not borne out. Finally, since a seeded error occasionally corrected an indigenous error, the assumption that errors do not interfere with each other was proven wrong. <b>Error</b> <b>seeding</b> can be made useful by taking these results into account in modifying the underlying model...|$|E
40|$|Abstract. <b>Error</b> 1 <b>seeding</b> is a technique, {{which can}} be used to {{evaluate}} the amount of residual errors after the system software test phase. This method helps to decide how to release the software depending on results that fault-seeding technique can provide. The advantages and disadvantages of the method are discussed, and various strategies for <b>seeding</b> <b>errors</b> are presented. The lessons learnt from experiences performed at Motorola Global Software Group Argentina are summarized, and suggestions for further research are offered. ...|$|R
40|$|An {{efficiency}} {{comparison is}} made in estimating the number of errors in a system using a recapture approach and a removal with <b>seeded</b> <b>errors</b> approach. We compare the Fisher information of the two methods derived from an optimal estimating function. A simulation study is given. The performance of the removal with <b>seeded</b> <b>errors</b> approach is shown {{to be better than}} the recapture approach especially when capture proportion is small. link_to_subscribed_fulltex...|$|R
40|$|Intuitively we know, some {{software}} {{errors are}} more complex than others. If the error can be fixed by changing one faulty statement, {{it is a simple}} error. The more substantial the fix must be, the more complex we consider the error. In this work, we formally define and quantify the com-plexity of an error w. r. t. the complexity of the error’s least complex, correct fix. As a concrete measure of complexity for such fixes, we introduce Cyclomatic Change Complexity which is inspired by existing program complexity metrics. Moreover, we introduce CoREBench, a collection of 70 regression errors systematically extracted from several open-source C-projects and compare their complexity with that of the <b>seeded</b> <b>errors</b> in the two most popular error bench-marks, SIR and the Siemens Suite. We find that seeded er-rors are significantly less complex, i. e., require significantly less substantial fixes, compared to actual regression errors. For example, among the <b>seeded</b> <b>errors</b> more than 42 % are simple compared to 8 % among the actual ones. This is a concern for the external validity of studies based on <b>seeded</b> <b>errors</b> and we propose CoREBench for the controlled study of regression testing, debugging, and repair techniques...|$|R
40|$|Papers are {{presented}} {{on the following}} topics: measurement of software technology, recent studies of the Software Engineering Lab, software management tools, expert systems, <b>error</b> <b>seeding</b> as a program validation technique, software quality assurance, software engineering environments (including knowledge-based environments), the Distributed Computing Design System, and various Ada experiments...|$|E
40|$|Synopses {{are given}} for NASA {{supported}} work {{in computer science}} at the University of Virginia. Some areas of research include: <b>error</b> <b>seeding</b> as a testing method; knowledge representation for engineering design; analysis of faults in a multi-version software experiment; implementation of a parallel programming environment; two computer graphics systems for visualization of pressure distribution and convective density particles; task decomposition for multiple robot arms; vectorized incomplete conjugate gradient; and iterative methods for solving linear equations on the Flex/ 32...|$|E
40|$|This paper {{describes}} {{an experiment in}} which simple syntactic alterations were introduced into program text in order to evaluate the testing strategy known as error seed-ing. The experiment’s goal {{was to determine if}} randomly placed syntactic manipulations can produce failure charac-teristics similar to those of indigenous errors found within unseeded programs. As a result of a separate experiment, several programs were available, all of which were written to the same specifications and thus were intended to be functionally equivalent. The use of functionally equivalent programs allowed the influence of individual programmer styles to be removed as a variable from the <b>error</b> <b>seeding</b> experiment. Each of six different syntactic manipulations were introduced into each program and the mean times to failure for the seeded errors were observed. The seeded errors were found to have a broad spectrum of mean times to failure independent of the syntactic altera-tion used. We conclude {{that it is possible to}} seed errors using only simple syntactic techniques that are arbitrarily di 5 culty to locate. In addition, several unexpected results indicate that some issues involved in <b>error</b> <b>seeding</b> have not been addressed previously. 1...|$|E
40|$|A new, {{efficient}} procedure {{estimates the}} number of errors in a system. A known number of <b>seeded</b> <b>errors</b> are inserted into a system. The failure intensities of the <b>seeded</b> and real <b>errors</b> are allowed to be different and time dependent. When an error is detected during the test, it {{is removed from the}} system. The testing process is observed for a fixed amount of time τ. Martingale theory is used to derive a class of estimators for {{the number of}} <b>seeded</b> <b>errors</b> in a continuous time setting. Some of the estimators and their associated standard deviations have explicit expressions. An optimal estimator among the class of estimators is obtained. A simulation study assesses the performance of the proposed estimators. published_or_final_versio...|$|R
40|$|State-based {{testing is}} a new method for testing {{object-oriented}} programs. The information stored {{in the state of}} an object is of two kinds: control-information and data-storage. The control-information transitions are modelled as a finite state automaton. Every operation of the class under test is considered as a mapping from starting states to a finishing states dependent upon the parameters passed. The possible parameter values are analysed for significant values which combined with the invocation of an operation can be used to represent stimuli applied to an object under test. State-based testing validates the expected transformations that can occur within a class. Classes are modelled using physical values assigned to the attributes of the class. The range of physical values is reduced by the use of a technique based on equivalence partitioning. This approach has a number of advantages over the conceptual modelling of a class, in particular the ease of manipulation of physical values and the independence of each operation from the other operations provided by an object. The technique when used in conjunction with other techniques provides an adequate level of validation for object-oriented programs. A suite of prototype tools that automate the generation of state-based test cases are outlined. These tools are used in four case studies that are presented as an evaluation of the technique. The code coverage achieved with each case study is analysed for the factors that affect the effectiveness of the state-based test suite. Additionally, <b>errors</b> have been <b>seeded</b> into 2 of the classes to determine the effectiveness of the technique for detecting errors on paths that are executed by the test suite. 92. 5 % of the <b>errors</b> <b>seeded</b> were detected by the state-based test-suite...|$|R
5000|$|Algis Budrys {{declared}} Stardance to be [...] "a reading experience which genuinely {{evokes a}} basic human feeling ... that {{within each of}} us dwells something glorious that is beyond mortal <b>error,</b> is the <b>seed</b> of an angel," [...] concluding that [...] "Stardance sweeps over the reader with the uncommon power attainable only by the social extrapolations of SF, and then rarely." ...|$|R
40|$|The {{assessment}} of a testing strategy and the comparison of different testing strategies is a crucial part in current re-search on software testing. Often, manual <b>error</b> <b>seeding</b> is used to generate faulty programs. As a consequence, the re-sults obtained from the examination of these programs are often not reproducible and likely to be biased. In this paper, a flexible approach to the benchmarking of testing strategies is presented. The approach utilizes well-known results from mutation analysis to construct an ob-jective effectiveness measure for test oracles. This measure allows to draw conclusions {{not only on the}} effectiveness of a single testing strategy but also to compare different testing strategies by their effectiveness measure. ...|$|E
40|$|AbstractSafety plays a {{key role}} in the safe {{operation}} of any safety critical control systems. Safety in such systems depends on the correct operation of the software meant for the safety purpose. Thorough testing of software is required to avoid the catastrophic accidents or to minimize the failure. As a case study, benchmark problem is tested against the C code according to the required specification of the system. Due to complexity involved in the control system there is a need to create a set of test inputs automatically. This paper describes the generation of optimized test cases to ensure block coverage metrics using Genetic Algorithm and results are compared with the Taguchi design of experiments. Random <b>error</b> <b>seeding</b> is carried out into the code to study the efficacy of the test cases...|$|E
40|$|In {{earlier work}} we have {{demonstrated}} that GA can successfully identify error prone paths that have been weighted according to our weighting scheme. In this paper we investigate whether the depth of strata in the software affects {{the performance of the}} GA. Our experiments show that the GA performance changes throughout the paths. It performs better in the upper, less in the middle and best in the lower layer of the paths. Although various methods have been applied for detecting and reducing errors in software, little research has been done into partitioning a system into smaller, error prone domains for Software Quality Assurance. To identify error proneness in software paths is important because by identifying them, they can be given priority in code inspections or testing. Our experiments observe to what extent the GA identifies errors seeded into paths using several <b>error</b> <b>seeding</b> strategies. We have compared our GA performance with Random Path Selection. Griffith Sciences, School of Information and Communication TechnologyFull Tex...|$|E
40|$|Digital Flight Control System (DFCS) {{software}} {{was used as}} a test case for assertion testing. The assertions were written and embedded in the code, then <b>errors</b> were inserted (<b>seeded)</b> one at a time and the code executed. Results indicate that assertion testing is an effective and efficient method of detecting errors in flight software. Most errors are eliminate at an earlier stage in the development than before...|$|R
40|$|Potency of {{pertussis}} component, {{present in}} DPT-vax, is evaluated by Kendrick assay. Strain Bordetella pertussis 18323 is recommended by WHO, {{which should be}} stored {{in a way that}} ensures its viability, purity and virulence. As part of the assay, the concentration of a strain suspension should be calculated, which opacity is compared at first sight with the Fifth Reference International Preparation of Opacity. This method is very inexact and frequently leads to <b>errors.</b> A <b>seed</b> lot from Bordetella. pertussis 18323 was elaborated and characterized in this study and the concentration of the strain using a calibration curve was calculated. The lot was used in eight potency assays and they complied with established validity criterion...|$|R
40|$|This paper {{presents}} a methodology for random testing of software models. Random testing tools {{can be used}} very effectively early in the modeling process, e. g., while writing formal requirements specification for a given system. In this phase users cannot know whether a correct operational model is being built or whether the properties that the model must satisfy are correctly identified and stated. So it is very useful to have tools to quickly identify errors in the operational model or in the properties, and make appropriate corrections. Using Lurch, our random testing tool for finite-state models, we evaluated the effectiveness of random model testing by detecting manually <b>seeded</b> <b>errors</b> in an SCR specification of a real-world personnel access control system. Having detected over 80 % of <b>seeded</b> <b>errors</b> quickly, our results appear to be very encouraging. We further defined and measured test coverage metrics {{with the goal of}} understanding why some of the mutants were not detected. Coverage measures allowed us to understand the pitfalls of random testing of formal models, thus providing opportunities for future improvement. Categories and Subject Descriptor...|$|R
40|$|Although various {{methods have}} been applied for {{detecting}} and reducing errors in software, {{little research has been}} done into partitioning a system into smaller, error prone domains for Software Quality Assurance. In this paper we demonstrate {{that it is possible to}} identify error prone parts in programs. This is important because by identifying them, they can be given priority in code inspections or testing. We use a scheme for allocating a potential Source of Error to the code instructions. Then we use Genetic Algorithms to find the most error prone parts in that software. We do this by observing to what extent the Genetic Algorithms identifies the heaviest error seeded paths by applying clustered <b>error</b> <b>seeding</b> strategies. We have compared the performance of the Genetic Algorithms in identifying error prone paths with random path selection. Results from our experiments show that the Genetic Algorithms can identify the most potentially error prone paths that would contain at least 85 % of the clustered seeded errors. Griffith Sciences, School of Information and Communication TechnologyNo Full Tex...|$|E
40|$|Aspect-oriented {{software}} design (AOSD) enables {{better and more}} complete separation of concerns in software-intensive systems. By extracting aspect code and relegating crosscutting functionality to aspects, software engineers can improve the maintainability of their code by reducing code tangling and coupling of code concerns. Further, the number of software defects {{has been shown to}} correlate with the number of non- encapsulated nonfunctional crosscutting concerns in a system. Aspect-mining is a technique that uses data mining techniques to identify existing aspects in legacy code. Unfortunately, {{there is a lack of}} suitably-documented test data for aspect- mining research and none that is fully representative of large-scale legacy systems. Using a new technique called concern seeding [...] based on the decades-old concept of <b>error</b> <b>seeding</b> [...] a tool called AspectAssay (akin to the radioimmunoassay test in medicine) was developed. The concern seeding technique allows researchers to seed existing legacy code with nonfunctional crosscutting concerns of known type, location, and quantity, thus greatly increasing the pool of available test data for aspect mining research. Nine seeding test cases were run on a medium-sized codebase using the AspectAssay tool. Each test case seeded a different concern type (data validation, tracing, and observer) and attempted to achieve target values for each of three metrics: 0. 95 degree of scattering across methods (DOSM), 0. 95 degree of scattering across classes (DOSC), and 10 concern instances. The results were manually verified for their accuracy in producing concerns with known properties (i. e., type, location, quantity, and scattering). The resulting code compiled without errors and was functionally identical to the original. The achieved metrics averaged better than 99. 9...|$|E
40|$|Abstract: The image {{segmentation}} algorithm based on facet model fitting is proposed, we firstly employ the facet model {{to fit the}} image intensity, and then calculate the fitting <b>error.</b> After acquiring <b>seed</b> segmentation region from the fitting error distribution, the region growing algorithm is implemented to enlarge the seed region to some region boundary. Finally, a new region merging algorithm is implemented to merge adjacent regipons into some large regions. Experiment results intestify the correctness of our proposed segmentation algorithm. 1...|$|R
40|$|Several short {{summaries}} {{of the work}} performed during this reporting period are presented. Topics discussed in this document include: (1) resilient <b>seeded</b> <b>errors</b> via simple techniques; (2) knowledge representation for engineering design; (3) analysis of faults in a multiversion software experiment; (4) implementation of parallel programming environment; (5) symbolic execution of concurrent programs; (6) two computer graphics systems for visualization of pressure distribution and convective density particles; (7) design of a source code management system; (8) vectorizing incomplete conjugate gradient on the Cyber 203 / 205; (9) extensions of domain testing theory and; (10) performance analyzer for the pisces system...|$|R
40|$|This study {{reports the}} results of an {{experiment}} that examines the impact of electronic work environments on auditor performance. It builds on re-search that has found that electronic work environments are more cog-nitively demanding than traditional paper environments and, thus, negatively impact performance. Forty-eight auditors with an average of 78 months of audit experience from one Big Four accounting Jim partic-ipated. Auditors in the electronic work environment were less able to iden-tlfy <b>seeded</b> <b>errors</b> and to use them properly in evaluating loan covenants than auditors in the traditional paper environment. Given these jindings, suggestions are made as to how to maximize auditor performance in an electronic work environment. 1...|$|R
40|$|With {{present day}} grain drills {{the number of}} plants per unit area is {{controlled}} via the seed-mass per unit area in kg/ha. This method causes substantial deviations from the target. Instead of the seed-mass the seed-numbers should be controlled. This can be attained by recording the number of seeds passing through the seed tube. A method is presented which includes a compensating program for recording <b>errors</b> due to <b>seed</b> clusters falling through the tube. It is shown that the actual deviations {{in the number of}} seeds recorded with the present day seed rates can be kept below 2, 5 %. The method is suitable for automatic closed loop computer control and for side specific sowing...|$|R
40|$|Individual plant care {{cropping}} systems, {{embodied in}} precision farming, {{may lead to}} new opportunities in agricultural crop management. The objective {{of the project was}} to provide high accuracy seed position mapping of a field of sugar beet. An RTK GPS was retrofitted on to a precision seeder to map the seeds as they were planted. The average <b>error</b> between the <b>seed</b> map and the actual plant map was about 32 mm to 59 mm. The results showed that the overall accuracy of the estimated plant positions is acceptable for the guidance of vehicles and implements. For subsequent individual plant care, the deviations were not, in all cases, small enough to ensure accurate individual plant targeting...|$|R
40|$|The {{intent of}} the {{workshop}} was to start moving research on the verification and validation (V&V) of knowledge based systems (KBSs) {{in the direction of}} providing tangible 'products' that a KBS developer could use. In the near term research will focus on identifying the kinds of experiences encountered during KBS development of 'real' KBSs. These will be stored in a repository and will serve as the foundation {{for the rest of the}} activities described here. One specific approach to be pursued is 'benchmarking'. With this approach, a KBS developer can use either 'canned' KBSs with <b>seeded</b> <b>errors</b> or existing KBSs with known errors to evaluate a given tool's ability to satisfactorily identify errors...|$|R
40|$|Auditors may {{initially}} generate {{and test}} a small or {{a large number of}} hypotheses when performing various tasks. However, {{little is known about the}} relative efficiencies of testing a different number of hypotheses. This study uses an analytical review framework to examine the impact of testing different hypothesis set sizes on auditors 2 ̆ 7 decision accuracy and information search. ^ Professional auditors were given information which indicated a fluctuation in a client 2 ̆ 7 s financial statements. The auditors were divided into four groups and asked to either generate a specific number of hypotheses (one, three, or six) that may explain the deviations, or to test any number desired (no-restriction group). They then analyzed a computerized detailed information set which provided data that could rule out all but a specific <b>error</b> <b>seeded</b> in the financial statements. The auditors either decided that one of the hypothesized causes explain the deviation, or continued generating and testing additional hypotheses. The specific variables examined were the time taken to perform the task, decision accuracy, and the amount and type of information searched. ^ The results indicated that auditors testing three hypotheses at a time spent less time on the decision task than subjects in the other groups. In addition, the three hypotheses group was as accurate as the one hypothesis group, and more accurate than the six hypotheses and the no-restriction groups. As compared to the three hypotheses group, the one hypothesis group spent significantly more time on the evaluation of each hypothesis, and on the generation of subsequent hypotheses. To minimize their effort, auditors chose to test the smallest total number of hypotheses in the single hypothesis group, which would likely have reduced their decision accuracy. The six hypotheses group spent more time generating the initial set of hypotheses, and considered the largest total number of hypotheses. However, these auditors spent the same amount of time actually evaluating information as the one and three hypothesis size groups and therefore they may have failed to conclusively test the large hypothesis size. The lack of constraints on the hypothesis set size did not benefit the time efficiency or the accuracy of the no-restriction group. ^ These results suggest that, in tasks similar to the one investigated here, a moderate hypothesis set size (e. g., three hypotheses size) appears to enhance the time efficiency of hypothesis testing, while modestly improving its accuracy. Implications for audit judgment and decision making research are discussed. ...|$|R
30|$|In a {{practical}} marketing context, early stage {{investigation of the}} success of spread at different levels of peer exposure (and variability across individuals) may critically inform the optimal level of investment a company should make in improving link-prediction <b>error</b> and what <b>seeding</b> algorithms should be applied in observed or estimated networks. In considering strategic levels of investment in link prediction, the planner should also consider their budget, b. The size of cascades being planned appears to strongly impact the value of good link prediction under the Uniform Threshold Model: in key parameter ranges, large premiums in cascade size may be gained by investing in improved link prediction. In other ranges, OAS performance appears quite insensitive to improvements in link prediction: such investments would be wasted.|$|R
40|$|We {{show that}} any {{distribution}} on - 1, 1 ^n that is k-wise independent fools any halfspace h with error for k = O(^ 2 (1 /) /^ 2). Up to logarithmic factors, our result matches a lower bound by Benjamini, Gurel-Gurevich, and Peled (2007) showing that k = Ω(1 /(^ 2 ·(1 /))). Using standard constructions of k-wise independent distributions, we obtain the first explicit pseudorandom generators G: - 1, 1 ^s [...] > - 1, 1 ^n that fool halfspaces. Specifically, we fool halfspaces with <b>error</b> eps and <b>seed</b> length s = k n = O(n ·^ 2 (1 /) /^ 2). Our approach combines classical tools from real approximation theory with structural results on halfspaces by Servedio (Computational Complexity 2007) ...|$|R
40|$|Valid {{data are}} {{required}} to make climate assessments and to make climate-related decisions. The objective {{of this paper is}} threefold: to introduce an explicit treatment of Type I and Type II errors in evaluating the performance of quality assurance procedures, to illustrate a quality control approach that allows tailoring to regions and subregions, and to introduce a new spatial regression test. Threshold testing, step change, persistence, and spatial regression were included in a test of three decades of temperature and precipitation data at six weather stations representing different climate regimes. The magnitude of thresholds was addressed in terms of the climatic variability, and multiple thresholds were tested to determine the number of Type I errors generated. In a separate test, random <b>errors</b> were <b>seeded</b> into the data and the performance of the tests was such that most Type II errors were made in the range of � 1 �C for temperature, not too different from the sensor field accuracy. The study underscores the fact that precipitation is more difficult to quality control than temperature. The new spatial regression test presented in this document outperformed all the other tests, which together identified only a few errors beyond those identified by the spatial regression test. 1...|$|R
40|$|We {{present a}} new {{approach}} to constructing pseudorandom generators that fool lowdegree polynomials over finite fields, based on the Gowers norm. Using this approach, we obtain the following main constructions of explicitly computable generators G: F s → F n that fool polynomials over a finite field F: 1. a generator that fools degree- 2 (i. e., quadratic) polynomials to within error 1 /n, with seed length s = O(log n), 2. a generator that fools degree- 3 (i. e., cubic) polynomials to within <b>error</b> ɛ, with <b>seed</b> length s = O(log |F | n) + f(ɛ, F) where f depends only on ɛ and F (not on n), 3. assuming the “inverse conjecture for the Gowers norm, ” for every d a generator that fools degree-d polynomials to within <b>error</b> ɛ, with <b>seed</b> length s = O(d · log |F | n) + f(d, ɛ, F) where f depends only on d, ɛ, and F (not on n). We stress that the results in (1) and (2) are unconditional, i. e. do not rely on any unproven assumption. Moreover, the results in (3) rely on a special case of the conjecture which may be easier to prove. Our generator for degree-d polynomials is the component-wise sum of d generators for degree- 1 polynomials (on independent seeds). Prior to our work, generators with logarithmic seed length were only known for degree- 1 (i. e., linear) polynomials (Naor and Naor; SIAM J. Comput., 1993). In fact, over small fields such as F 2 = { 0, 1 }, our results constitute the first progress on these problems since the long-standing generator by Luby, Veličković and Wigderson (ISTCS 1993), whose seed length is much bigger: s = exp (Ω (√ log n)), even for the case of degree- 2 polynomials over F 2...|$|R
40|$|Dilution {{water demand}} (DWD) {{can cause a}} {{positive}} error when the dilution biochemical oxygen demand (BOD) method is used. Dilution water demand {{may be attributed to}} oxidation of organic impurities in the dilution water and nitrification of ammonia added as a nutrient. To minimize the error associated with these sources, the standard BOD method requires that DWD be less than 0. 2 mg/L in 5 days and does not allow correction for DWD when calculating test results. This study derives a set of theoretical equations to analyze the uncorrected errors with and without seeding. The authors concluded that DWD can be completely corrected if seeded dilution water is used for the sample dilution. When seeding individual bottles, the uncorrected error approaches 8. 3 to approximately 8. 8 % at a 5 -day depletion of 2 mg/L for a typical secondary effluent. Tests without seeding show an almost 1 % higher uncorrected <b>error</b> than <b>seeded</b> tests. The analysis also suggests that these errors can be effectively reduced to less than 3 % when the 5 -day depletion approaches 6 mg/L, even for 5 -day biochemical oxygen demand concentrations exceeding 1 X 10 (4) mg/L. Further analysis indicates that, if not inhibited, the ammonium added to dilution water as a nutrient may contribute additional error due to nitrification...|$|R
40|$|Individual plant care {{may well}} become {{embodied}} in precision farming {{in the future}} and will lead to new opportunities in agricultural crop management. The objective of this project was to develop and evaluate a data logging system attached to a precision seeder to enable high accuracy seed position mapping of a field of sugar beet. A Real Time Kinematic Global Positioning System (RTK GPS), optical seed detectors and a data logging system were retrofitted on to a precision seeder to map the seeds as they were planted. The average <b>error</b> between the <b>seed</b> map and the actual plant map was about 16 – 43 mm depending on vehicle speed and seed spacing. The results showed that the overall accuracy of the estimated plant positions was acceptable for the guidance of vehicles and implements as well as potential individual plant treatments...|$|R
40|$|Mainstream {{adoption}} of physiologically-relevant three-dimensional models {{has been slow}} in the last 50 years due to long, manual protocols with poor reproducibility, high price and closed commercial platforms. This chapter describes high-throughput, low-cost, open methods for spheroid viability assessment which use readily-available reagents and open-source software to analyse spheroid volume, metabolism and enzymatic activity. We provide two ImageJ macros for automated spheroid size determination - for both single images and for images in stacks. We also share an Excel template spreadsheet allowing users to rapidly process spheroid size data, analyse plate uniformity (such as edge effects and systematic <b>seeding</b> <b>errors),</b> detect outliers and calculate dose-response. The methods {{would be useful to}} researchers in preclinical and translational research planning to move away from simplistic monolayer studies and explore 3 D spheroid screens for drug safety and efficacy without substantial investment in money or time...|$|R
40|$|Abstract. Individual plant care {{may well}} become {{embodied}} in precision farming {{in the future}} and will lead to new opportunities in agricultural crop management. The objective of this project was to develop and evaluate a data logging system attached to a precision seeder to enable high accuracy seed position mapping of a field of sugar beet. A Real Time Kinematic Global Positioning System (RTK GPS), optical seed detectors and a data logging system were retrofitted on to a precision seeder to map the seeds as they were planted. The average <b>error</b> between the <b>seed</b> map and the actual plant map was about 16 – 43 mm depending on vehicle speed and seed spacing. The results showed that the overall accuracy of the estimated plant positions was acceptable for the guidance of vehicles and implements as well as potential individual plant treatments...|$|R
40|$|In {{interactive}} medical image segmentation, anatomical {{structures are}} extracted from reconstructed volumetric images. The first iterations of user interaction traditionally consist of drawing pictorial hints as an initial {{estimate of the}} object to extract. Only after this time consuming first phase, the efficient selective refinement of current segmentation results begins. Erroneously labeled seeds, especially near {{the border of the}} object, are challenging to detect and replace for a human and may substantially impact the overall segmentation quality. We propose an automatic seeding pipeline as well as a configuration based on saliency recognition, in order to skip the time-consuming initial interaction phase during segmentation. A median Dice score of 68. 22 % is reached before the first user interaction on the test data set with an <b>error</b> rate in <b>seeding</b> of only 0. 088 %. Comment: Medical Imaging Conference (MIC) 201...|$|R
