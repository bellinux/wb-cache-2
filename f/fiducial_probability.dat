14|2|Public
5000|$|The {{interpretation}} of probability {{has not been}} resolved (but <b>fiducial</b> <b>probability</b> is an orphan).|$|E
5000|$|Lindley {{showed that}} <b>fiducial</b> <b>{{probability}}</b> lacked additivity, {{and so was}} not a probability measure. Cox {{points out that the}} same argument applies to the so-called [...] "confidence distribution" [...] associated with confidence intervals, so the conclusion to be drawn from this is moot. Fisher sketched [...] "proofs" [...] of results using <b>fiducial</b> <b>probability.</b> When the conclusions of Fisher's fiducial arguments are not false, many have been shown to also follow from Bayesian inference.|$|E
5000|$|Fisher {{admitted}} that [...] "fiducial inference" [...] had problems. Fisher wrote to George A. Barnard {{that he was}} [...] "not clear in the head" [...] about one problem on fiducial inference, and, also writing to Barnard, Fisher complained that his theory seemed to have only [...] "an asymptotic approach to intelligibility". Later Fisher confessed that [...] "I don't understand yet what <b>fiducial</b> <b>probability</b> does. We shall {{have to live with}} it a long time before we know what it's doing for us. But it should not be ignored just because we don't yet have a clear interpretation".|$|E
40|$|International audienceP. R. Killeen's (2005 a) [Killeen, P. R. - An {{alternative}} to null-hypothesis significance tests, Psychological Science, 2005, 16, 345 – 353] probability of replication (prep) of an experimental {{result is the}} <b>fiducial</b> Bayesian predictive <b>probability</b> of finding a same-sign effect in a replication of an experiment. prep is now routinely reported in Psychological Science and has also begun to appear in other journals. However, there is little concrete, practical guidance for use of prep and the procedure has not received the scrutiny that it deserves. Furthermore, only a solution that assumes a known variance has been implemented. A practical problem with prep is identified: in many articles prep appears to be incorrectly computed, due to the confusion between 1 -tailed and 2 -tailed p-values. Experimental findings reveal the risk of misinterpreting prep as the predictive probability of finding a same-sign and significant effect in a replication (p srep). Conceptual and practical guidelines are given to avoid these pitfalls. They include the extension {{to the case of}} unknown variance. Moreover, other uses of <b>fiducial</b> Bayesian predictive <b>probabilities,</b> for analyzing, designing ("how many subjects?") and monitoring ("when to stop?") experiments, are presented. Concluding remarks emphasize the role of predictive procedures in statistical methodology...|$|R
50|$|Concerning the {{identification}} of the parameters of a distribution law, the mature reader may recall lengthy disputes in the mid 20th century about the interpretation of their variability in terms of <b>fiducial</b> distribution , structural <b>probabilities</b> , priors/posteriors , and so on. From an epistemology viewpoint, this entailed a companion dispute as to the nature of probability: is it a physical feature of phenomena to be described through random variables or a way of synthesizing data about a phenomenon? Opting for the latter, Fisher defines a fiducial distribution law of parameters of a given random variable that he deduces from a sample of its specifications. With this law he computes, for instance “the probability that μ (mean of a Gaussian variable - our note) is less than any assigned value, or the probability that it lies between any assigned values, or, in short, its probability distribution, {{in the light of the}} sample observed”.|$|R
5000|$|Fiducial {{inference}} was {{an approach}} to statistical inference based on <b>fiducial</b> <b>probability,</b> {{also known as a}} [...] "fiducial distribution". In subsequent work, this approach has been called ill-defined, extremely limited in applicability, and even fallacious. However this argument is the same as that which shows [...] that a so-called confidence distribution is not a valid probability distribution and, since this has not invalidated the application of confidence intervals, it does not necessarily invalidate conclusions drawn from fiducial arguments. An attempt was made to reinterpret the early work of Fisher's fiducial argument as a special case of an inference theory using Upper and lower probabilities.|$|E
40|$|The Behrens [...] Fisher problem {{concerns}} {{finding an}} interval estimate {{for the difference}} between the means of two normal populations, without making any assumption about the variances. At present, statisticians cannot agree on its solution. In this paper, a credible and sharp solution is described. It is compared with the solutions of Behrens (see Fisher, 1956), Welch (1947) and Wilkinson, Venables and James (1979). <b>Fiducial</b> <b>probability</b> reference set confidence statement relevant subset two-means problem Behrens problem parameter of interest...|$|E
30|$|In {{the first}} stage of SVM training, it is {{necessary}} to ensure the accuracy rate is as high as possible, and to reduce the proportion of samples into the second stage to improve the efficiency of security regions analysis. In this model, Grid Search is adopted first to optimize the parameters of the SVM model, and the SVM model with the best classification accuracy is obtained. The probability output of the SVM is then used to judge the <b>fiducial</b> <b>probability</b> of the sample in the SVM model in {{the first stage}}.|$|E
30|$|Through this study, the {{probability}} output of SVM can effectively distinguishes the SVM model in gray space. The <b>fiducial</b> <b>probability</b> output of classification results {{can make the}} classification results of the vague samples to be effectively recognized for subsequent processing. The classification accuracy of the samples in the gray space can be effectively improved by using the clustering analysis with the SVM model. Using the penalty factor adjustment method can reduce the proportion of unstable results mistaken as stable ones. In addition, this method can effectively deal with each cluster after being clustered into two samples due to unbalanced training difficulties, so as to improve the accuracy of classification.|$|E
40|$|By {{discussing}} several examples, {{the theory}} of generalized functional models is shown to be very natural for modeling some situations of reasoning under uncertainty. A generalized functional model is a pair (f, P) where f is a function describing the interactions between a parameter variable, an observation variable and a random source, and P is a probability distribution for the random source. Unlike traditional functional models, generalized functional models do not require {{that there is only}} one value of the parameter variable that is compatible with an observation and a realization of the random source. As a consequence, the results of the analysis of a generalized functional model are not expressed in terms of probability distributions but rather by support and plausibility functions. The analysis of a generalized functional model is very logical and is inspired from ideas already put forward by R. A. Fisher in his theory of <b>fiducial</b> <b>probability.</b> Comment: Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI 1997...|$|E
40|$|The {{geometric}} {{formulation of}} <b>fiducial</b> <b>probability</b> employed {{in this paper}} is an improvement over the usual pivotal quantity formulation. For a single parameter and single observation, the new formulation {{is based on the}} geometric properties of an ordinary two variable function and its surface representation. The following theorem is proved: A fiducial distribution for the continuous parameter θ exists if and only if (i) the continuous random probability distributions of x for different θ's are non-intersecting, and (ii) the random distributions are complete, i. e. at the extreme values of θ the limiting probability distributions are zero and one for all x. The proof yields also a complete characterization of random distributions that lead to fiducial distributions. The paper also treats intersecting distributions and non-intersecting incomplete distributions. The latter, which are frequently encountered in a null hypothesis, are shown to be associated with intersecting "composite" distributions. An appendix compares the pivotal and geometric formulations...|$|E
40|$|Estimation of {{an effect}} size or other {{parameter}} of interest (POI), such as an average of differential abundance levels of metabolites or of the differential expression levels of genes, may be improved by shrinking toward a null-hypothesis value {{to the extent of}} a probability that the null hypothesis is true. For example, the local false discovery rate (LFDR) is a null-hypothesis posterior probability that is estimable via empirical Bayes methods without specifying the hyperprior distributions needed for a hierarchical Bayesian approach. We compared the following (estimated) null-hypothesis probabilities as degrees of shrinkage in order to improve POI interval estimates (IEs) and POI point estimates (PEs) : a histogram-based estimator (HBE) of the LFDR, a binomial-based estimator of the LFDR, a maximum-likelihood estimator of the LFDR, an expected LFDR (ELFDR), and a <b>fiducial</b> <b>probability</b> (FP). In multiple-hypothesis testing, the ELFDR yields reliable IEs while the HBE gives the best PEs. For single-hypothesis testing, the FP generates an IE that outperforms the confidence interval and generates a PE performing at least as well as the unbiased estimator. We apply these POI estimators to the abundance levels of 20 plasma proteins in women with breast cancer...|$|E
40|$|Reacting {{against the}} {{limitation}} of statistics to decision procedures, R. A. Fisher proposed for inductive reasoning {{the use of the}} fiducial distribution, a parameter-space distribution of epistemological probability transferred directly from limiting relative frequencies rather than computed according to the Bayes update rule. The proposal is developed as follows using the confidence measure of a scalar parameter of interest. (With the restriction to one-dimensional parameter space, a confidence measure is essentially a <b>fiducial</b> <b>probability</b> distribution free of complications involving ancillary statistics.) A betting game establishes a sense in which confidence measures are the only reliable inferential probability distributions. The equality between the probabilities encoded in a confidence measure and the coverage rates of the corresponding confidence intervals ensures that the measure's rule for assigning confidence levels to hypotheses is uniquely minimax in the game. Although a confidence measure can be computed without any prior distribution, previous knowledge can be incorporated into confidence-based reasoning. To adjust a p-value or confidence interval for prior information, the confidence measure from the observed data can be combined with one or more independent confidence measures representing previous agent opinion. (The former confidence measure may correspond to a posterior distribution with frequentist matching of coverage probabilities.) The representation of subjective knowledge in terms of confidence measures rather than prior probability distributions preserves approximate frequentist validity. Comment: major revisio...|$|E
40|$|The article {{deals with}} the {{estimation}} algorithm for functional system reliability influence on the aircraft technical maintenance. The research results which allow to estimate the functional system reliability influence on the aircraft tech- nical maintenance process are provided. The aircraft maintenance process effectiveness implies the maintenance procedures including pre-flight preparation, the flight itself, technical maintenance and storage. The research was aimed at forecasting the aircraft technical maintenance process considering the functional system reliability, with special attention payed to failure-free operation. The term failure-free operation stands for the ability to execute a required function within a given time interval at given conditions. The objects of maintenance -aircraft functional systems and their components have a great degree of failure-free operation, {{which has been a}} design requirement and is supported at aircraft maintenance and operation. Failures and mal- functions are seldom events. This fact leads to a limited statistic maintenance surveillance database. Classical methods of statistical analyses do not provide the sufficient <b>fiducial</b> <b>probability</b> for functional system failure-free operation estimation. Thus these methods do not allow to estimate its influence on the aircraft technical maintenance process effectiveness. In order to solve this problem it is suggested to use the method of flow diagrams and random sample statistical method for failure-free operation estimation-statistical modeling (Monte-Carlo Method). Reliability flow diagram method allowed to determine the functional system failure-free operation random sample dependence on the technical maintenance frequency. The initial data were: the functional system flow diagram, the list of the main equipment, functional system equipment failure random sample data. The use of statistical modeling allowed to estimate the dependence of functional system reliability (failure-free operation) on the technical maintenance frequency for expanded statistical database (a num- ber of functional system maintenance and operation procedures) ...|$|E
40|$|Motivation. This {{version is}} {{based solely on}} the {{calculus}} of probability, excluding any statistical principle. "Location measurement" means the pdf of the error is known. When the datum is obtained, intuition suggests something like a pdf for the parameter; here we attempt a critical examination of its meaning. Summary. In default of prior probability the parameter is not defined as a random variable, hence there can be no genuine prior-free parametric inference. Nevertheless prior-free predictive inference regarding any future datum is generated directly from the datum of a location measurement. Such inference turns out as if obtained from a certain pdf ("fiducial") indirectly associated with the parameter. This false pdf can expedite predictive inference, but is inappropriate in the analysis of combined measurements (unless they all are location measurements of the same parameter). Also it has the same distribution as the ostensible Bayesian posterior from a uniform "prior". However, if any of these spurious entities is admitted in the analysis, inconsistent results follow. When we combine measurements, we find that the quantisation errors, inevitable in data recording, must be taken into consideration. These errors cannot be folded into predictive inference in an exact sense; that is, we cannot render a predictive distribution of a future datum except as an approximation. Keywords: location measurement; combination of observations; parametric inference; predictive inference; prior-free inference; quantisation error; digitisation; frequentist interpretation; the fiducial argument; fiducial probability; pivotal inference; intuitive assessment; prior-free assessment Former abstract: The existence of a default "prior" probability density is established in a special case: if the experimental error, as a random variable, is of known density, independently of the true value. In such cases the prior can be thought of in terms of the experimental error, assuming that an admissible prior for the main parameter is not available. That is, we obtain a plain Bayesian interpretation of Fisher's <b>fiducial</b> <b>probability.</b> There is also a corresponding frequentist interpretation. In a generic case, the default "prior" is demonstrated to be equivalent to that induced by the information metric; it has been used e. g. by V. Balasubramanian (1996), following Jeffreys' final approach (1961; {{not to be confused with}} his earlier ones). Consequently, prejudice can be eliminated in Bayesian analysis. The Likelihood "Principle" is violated by Jeffrey's "prior". In particular, changing the stopping rule of an experiment can affect the default "prior"...|$|E
40|$|This paper {{describes}} {{the analysis of}} data from a three-year gross time study of skyline, balloon, and helicopter yarding systems operating in Western Oregon. Data collection activities were designed and supervised by the Pacific Northwest Forest and Range Experiment Station, U. S. D. A. Forest Service. The specific logging systems studied were running skyline, North Bend standing skyline, long-span standing skyline, balloon (inverted skyline, highlead, inverted skyline yo-yo, and highlead yo-yo), medium helicopter, and heavy helicopter. These systems were observed under {{a wide range of}} silvicultural and landscape design prescriptions, timber type, terrain, and weather conditions. The objective {{of this study was to}} develop yarding production equations, to sumarize delays, road change times,and landing change times, and to compare yarding production estimates made from both gross and detailed time study data. This kind of information is useful for the comparison of alternative logging methods in environmentally sensitive, landscape-designed harvest units. The data were segregated according to the individual logging systems and analyzed via multiple regression. Then individual system data were combined into the categories of short-span skyline, long-span skyline, balloon, and helicopter. These combined data were also analyzed via multiple regression. Chi-square tests were performed to determine whether the equations developed from the combined data were significantly different from the set of equations developed from the segregated data. The results of these tests support a conclusion, at the 95 percent level of <b>fiducial</b> <b>probability,</b> that the equations developed from the combined data are as adequate for predicting yarding production rates for these logging systems as the equations developed from the individual-system data. The variables shown statistically to influence yarding production rates for all logging systems studied were yarding distance and number of logs per turn. In addition, helicopter yarding productivity was also found to be influenced by the type of cutting prescription, and short-span skyline yarding, by chordslope. A variable combining aspect and the season of work was found to be significant for both the running skyline and the heavy helicopter. Yarding delays were found to be affected by yarder, landing size, season, and crews' experience. In order to compare similar systems' delays, it was found important to segregate out weather-related delays. In a separate study, detailed time studies were made on four of the yarding systems analyzed in this paper. This allowed a comparison between the measurements of yarding production rates made during the detailed time study and those made during the gross tinte study. The gross time study rates were consistently lower than the detail time study rates. This suggests that the detailed method does not reflect the total downtime as accurately as the gross method. Thus the gross method appears better suited for developing information that is useful for appraisal purposes and the detailed method is better suited for evaluation of system efficiency...|$|E

