81|827|Public
5000|$|... 2016, <b>Data</b> <b>Lake</b> Architecture - Keeping Your <b>Data</b> <b>Lake</b> from Turning into a Garbage Dump, Technics Publications, NJ, 2016 ...|$|E
50|$|One other {{criticism}} about the <b>data</b> <b>lake</b> {{is that the}} concept is fuzzy and arbitrary. It refers to any tool or data management practice that does not fit into the traditional data warehouse architecture. The <b>data</b> <b>lake</b> {{has been referred to}} as a technology such as Hadoop. The <b>data</b> <b>lake</b> has been labeled as a raw data reservoir or a hub for ETL offload. The <b>data</b> <b>lake</b> has been defined as a central hub for self-service analytics. The concept of the <b>data</b> <b>lake</b> has been overloaded with meanings, which puts the usefulness of the term into question.|$|E
50|$|The <b>data</b> <b>lake</b> allows an {{organization}} to shift its focus from centralized control to a shared model {{to respond to}} the changing dynamics of information management. This enables quick segregation of data into the <b>data</b> <b>lake,</b> thereby reducing the overhead time.|$|E
5000|$|James Dixon, then chief {{technology}} officer at Pentaho allegedly coined the term to contrast it with data mart, which is a smaller repository of interesting attributes extracted from raw data. He argued that data marts have several inherent problems, and promoted <b>data</b> <b>lakes.</b> These problems are {{often referred to as}} information siloing. PricewaterhouseCoopers said that <b>data</b> <b>lakes</b> could [...] "put an end to data silos. In their study on <b>data</b> <b>lakes</b> they noted that enterprises were [...] "starting to extract and place data for analytics into a single, Hadoop-based repository." ...|$|R
5000|$|In a {{blog post}} Damon Feldman {{writes about the}} {{differences}} between <b>Data</b> Hubs, <b>Data</b> <b>Lakes</b> and Federations: ...|$|R
50|$|Attribute-based {{access control}} {{can also be}} applied to Big Data systems like Hadoop. Policies {{similar to those used}} {{previously}} can be applied when retrieving <b>data</b> from <b>data</b> <b>lakes.</b>|$|R
50|$|Azure <b>Data</b> <b>Lake</b> Analytics uses Apache YARN, {{the central}} part of Apache Hadoop to govern {{resource}} management and deliver operations across the Hadoop clusters. Microsoft Azure <b>Data</b> <b>Lake</b> Store supports any application that uses the open Apache Hadoop Distributed File System (HDFS) standard.|$|E
50|$|An earlier <b>data</b> <b>lake</b> (Hadoop 1.0) {{had limited}} {{capabilities}} with its batch oriented processing (Map Reduce) {{and was the}} only processing paradigm associated with it. Interacting with the <b>data</b> <b>lake</b> meant {{you had to have}} expertise in Java with map reduce and higher level tools like Pig & Hive (which by themselves were batch oriented). With the dawn of Hadoop 2.0 and separation of duties with Resource Management taken over by YARN (Yet Another Resource Negotiator), new processing paradigms like streaming, interactive, on-line have become available via Hadoop and the <b>Data</b> <b>Lake.</b>|$|E
5000|$|Azure <b>Data</b> <b>Lake</b> Analytics is a {{parallel}} on-demand job service. The parallel processing system {{is based on}} the Microsoft Dryad solution. [...] Dryad can represent arbitrary Directed Acyclic Graphs (DAGs) of computation. [...] <b>Data</b> <b>Lake</b> Analytics provides a distributed infrastructure that can dynamically allocate or de-allocate resources so customers pay for only the services they use.|$|E
40|$|The {{heterogeneity}} of sources in Big Data systems requires new integration approaches which {{can handle the}} large volume of the data {{as well as its}} variety. <b>Data</b> <b>lakes</b> have been proposed to reduce the upfront integration costs and to provide more flexibility in integrating and analyzing information. In <b>data</b> <b>lakes,</b> <b>data</b> from the sources is copied in its original structure to a repository; only a syntactic integration is done as data is stored in a common semi-structured format. Metadata plays an important role, as the source data is not loaded into an integrated repository with a unified schema; the data has to come with its own metadata. This paper presents GEMMS, a Generic and Extensible Metadata Management System for <b>data</b> <b>lakes</b> which extracts metadata from the sources and manages the structural and semantical information in an extensible metamodel. The system has been developed with a focus on scientific data management in the life sciences which is often only file-based with limited query function...|$|R
50|$|In this blog post, Kurt Cagle {{focuses on}} <b>Data</b> <b>Lakes</b> (simple storage in Hadoop) vs. Data Hubs (more complex transformation, {{security}} and semantics using a hub product - {{in this case}} MarkLogic).|$|R
50|$|Zaloni, Inc. is {{a privately}} owned, software, and {{services}} company headquartered in Durham, North Carolina. Zaloni provides data management software and solutions for big data scale-out architectures, such as Apache Hadoop, Amazon S3. The company focus on management of <b>data</b> <b>lakes</b> with 2 products: Bedrock and Mica.|$|R
50|$|ELT is an {{alternative}} to Extract, transform, load (ETL) used with <b>data</b> <b>lake</b> implementations.In ELT models the data is not processed on entry to the <b>data</b> <b>lake</b> which enables faster loading times. But does require sufficient processing within the data processing engine {{to carry out the}} transform on demand and return the results to the consumer in a timely manner.Since the data is not processed on entry to the <b>data</b> <b>lake</b> the query and schema {{do not need to be}} defined a-priori (often the schema will be available during load since many data sources are extracts from databases or similar structured data systems and hence have an associated schema).|$|E
50|$|Her {{work today}} {{is focused on}} <b>data</b> <b>lake</b> architectures, {{metadata}} management and information governance.|$|E
50|$|In 2012, Zaloni {{released}} {{their first}} software product, the Bedrock <b>Data</b> <b>Lake</b> Management platform.|$|E
50|$|The {{recipients}} {{could be}} individuals, such as data architects or data scientists who will investigate the data further, business users who will consume the data directly in reports, or systems that will further process {{the data and}} write it into targets such as <b>data</b> warehouses, <b>data</b> <b>lakes</b> or downstream applications.|$|R
5000|$|... In August 2015, STT {{made its}} first {{investment}} in emerging technology verticals, leading a US$40 million financing round in Datameer, a big data analytics and visualization company based in San Francisco, California. Datameer is a big data analytics platform that helps companies create and extract value from enterprise <b>data</b> <b>lakes.</b>|$|R
50|$|Isilon {{provides}} multi-protocol {{access to}} files using NFS or SMB/CIFS or FTP. In addition, Isilon supports HDFS as a protocol allowing Hadoop analytics {{to be performed}} on files resident on the storage. Data can be stored using one protocol and accessed using another protocol. The key building blocks for Isilon include the OneFS operating system, the NAS architecture, the scale-out <b>data</b> <b>lakes,</b> and other enterprise features.|$|R
50|$|The Azure <b>Data</b> <b>Lake</b> {{service was}} {{released}} on November 16th, 2016. Azure <b>Data</b> <b>Lake</b> is built on the learnings and technologies of COSMOS, Microsoftâ€™s internal big data system. COSMOS is used to store and process data for applications such as Azure, AdCenter, Bing, MSN, Skype and Windows Live. COSMOS features a SQL-like query engine called SCOPE upon which U-SQL was built.|$|E
50|$|One {{example of}} a <b>data</b> <b>lake</b> is the {{distributed}} file system used in Apache Hadoop.|$|E
50|$|A {{data hub}} differs from a <b>data</b> <b>lake</b> by {{homogenizing}} data and possibly serving data in multiple desired formats, {{rather than simply}} storing it in one place, and by adding other value to the data such as de-duplication, quality, security, and a standardized set of query services. A <b>Data</b> <b>Lake</b> tends to store data in one place for availability, and allow/require the consumer to process or add value to the data.|$|E
50|$|Recent deals between EMC and Cloudera {{will allow}} the Cloudera Enterprise Hadoop kit to be sold {{directly}} from EMC and its channel partners. The deal is expected to benefit the thousands of EMC Isilon customers with existing <b>data</b> <b>lakes</b> by providing a base for running analytic processes on their data, giving them access to Impala, Cloudera's open source, massively parallel processing SQL query engine that runs on Hadoop.|$|R
40|$|Double-differential cross {{sections}} (energy spectral) for proton and deuteron emission in fast neutron induced reactions on oxygen are reported for nine incident neutron energies between 25 and 65 MeV. Angular distributions were measured at 15 laboratory? angles between 20 degrees and 160 degrees. Procedures for <b>data</b> <b>laking</b> and <b>data</b> reduction are presented. Deduced energy-differential, angle-differential and total {{cross sections}} are also reported. Experimental cross sections are compared with existing data and with theoretical model calculations...|$|R
50|$|A {{summary of}} {{geographical}} and limnological <b>data</b> for <b>lake</b> Todos los Santos {{is found in}} the database of the International Lake Environment Committee.|$|R
50|$|A <b>data</b> <b>lake</b> is {{a method}} of storing data within a system or repository, in its natural format, that {{facilitates}} the collocation of data in various schemata and structural forms, usually object blobs or files. The idea of <b>data</b> <b>lake</b> {{is to have a}} single store of all data in the enterprise ranging from raw data (which implies exact copy of source system data) to transformed data which is used for various tasks including reporting, visualization, analytics and machine learning. The <b>data</b> <b>lake</b> includes structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and even binary data (images, audio, video) thus creating a centralized data store accommodating all forms of data.|$|E
5000|$|A data swamp is a {{deteriorated}} <b>data</b> <b>lake,</b> that is {{inaccessible to}} its intended users and provides little value.|$|E
5000|$|Azure <b>Data</b> <b>Lake</b> is a {{scalable}} {{data storage}} and analytics service. The service is hosted in Azure, Microsoft's public cloud.|$|E
40|$|The {{vertical}} {{temperature profile}} {{is a poor}} indicator of sratification in most tropical <b>lake.</b> <b>Data</b> from <b>Lake</b> Bunyonyi, Uganda, showed, however, that the density profile highlighted discontinuity layers whieh were not obvious from the temperature profile, It is suggested that the vertical density profile and the region of maximum discontinuity, the pycnocline, could be adopted as a useful index of stratification especially in tropical lakes...|$|R
50|$|Due {{to a lack}} of {{historic}} <b>data</b> for <b>Lake</b> of the Woods, the natural conditions of the lake cannot be established. However, the lake's trophic state is in the oligotrophic to mesotrophic range.|$|R
40|$|By January 2004, <b>data</b> on 314 <b>lakes</b> from 34 types were {{submitted}} by 25 countries. From the 68 class boundaries (two for each type) 25 were {{represented by a}} sufficient number of lakes five or more. Mostly data on hydrochemistry (263 cases) and phytoplankton (214 cases) were used in the preliminary quality assessment, followed by data on angiosperms (87), benthic invertebrates (76) and fish (29). These numbers reflect also the availability of <b>data</b> for <b>lakes.</b> JRC. H. 5 -Rural, water and ecosystem resource...|$|R
50|$|Franz Inc. is an American owned {{technology}} company, {{focused on}} developing and deploying Graph Database solutions, Common Lisp, and Semantic <b>Data</b> <b>Lake</b> technologies.|$|E
50|$|On September 8, 2016, NetApp {{announced}} {{a partnership with}} Zaloni on a mid-tier storage solution for complete lifecycle management of the <b>data</b> <b>lake.</b>|$|E
5000|$|Many {{companies}} have now entered into this space: Google, Microsoft, Zeloni, Teradata, Cloudera, and Amazon all have <b>data</b> <b>lake</b> offerings {{to name a}} few.|$|E
25|$|Below are {{the climate}} <b>data</b> for Manzanita <b>Lake,</b> {{located near the}} National Park entrance.|$|R
40|$|A {{cooperative}} program between the Wisconsin Department of Natural Resources and the University of Wisconsin-Madison {{resulted in the}} assessment of the trophic condition of approximately 3, 000 significant inland lakes in Wisconsin. The feasibility of using both photographic and digital representations of LANDSAT multispectral scanner <b>data</b> for <b>lake</b> classification was investigated. The result was the development of a nearly automated system which, with minimal human interaction, locates and extracts the <b>lake</b> <b>data,</b> then corrects the data for atmospheric effects, and finally classifies all the significant lakes in the state as to trophic condition...|$|R
50|$|In 2006, {{according}} to Statistics Canada census <b>data,</b> Granville <b>Lake</b> had {{a population of}} 98 living in 16 dwellings, a 42.0% increase from 2001. The Indian Settlement has a land area of 2.33 km2 and a population density of 42.0 /km2 /sq mi.|$|R
