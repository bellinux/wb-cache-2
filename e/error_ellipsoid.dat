28|25|Public
40|$|Remote sensing radar {{satellites}} cover wide {{areas and}} provide spatially dense measurements, {{with millions of}} scatterers. Knowledge of the precise position of each radar scatterer is essential to identify the corresponding object and interpret the estimated deformation. The absolute position accuracy of synthetic aperture radar (SAR) scatterers in a 2 D radar coordinate system, after compensating for atmosphere and tidal effects, is {{in the order of}} centimeters for TerraSAR-X (TSX) spotlight images. However, the absolute positioning in 3 D and its quality description are not well known. Here, we exploit time-series interferometric SAR to enhance the positioning capability in three dimensions. The 3 D positioning precision is parameterized by a variance–covariance matrix and visualized as an <b>error</b> <b>ellipsoid</b> centered at the estimated position. The intersection of the <b>error</b> <b>ellipsoid</b> with objects in the field is exploited to link radar scatterers to real-world objects. We demonstrate the estimation of scatterer position and its quality using 20 months of TSX stripmap acquisitions over Delft, the Netherlands. Using trihedral corner reflectors (CR) for validation, the accuracy of absolute positioning in 2 D is about 7 cm. In 3 D, an absolute accuracy of up to ? 66 cm is realized, with a cigar-shaped <b>error</b> <b>ellipsoid</b> having centimeter precision in azimuth and range dimensions, and elongated in cross-range dimension with a precision in the order of meters (the ratio of the ellipsoid axis lengths is 1 / 3 / 213, respectively). The CR absolute 3 D position, along with the associated <b>error</b> <b>ellipsoid,</b> is found to be accurate and agree with the ground truth position at a 99 % confidence level. For other non-CR coherent scatterers, the <b>error</b> <b>ellipsoid</b> concept is validated using 3 D building models. In both cases, the <b>error</b> <b>ellipsoid</b> not only serves as a quality descriptor, but can also help to associate radar scatterers to real-world objects. Geoscience and Remote SensingCivil Engineering and Geoscience...|$|E
40|$|The use of {{three-dimensional}} (3 D) data in {{the industrial}} measurement field is becoming increasingly popular because of the rapid development of laser scanning techniques based on the time-of-flight principle. However, the accuracy and uncertainty {{of these types of}} measurement methods are seldom investigated. In this study, a mathematical uncertainty evaluation model for the diameter measurement of standard cylindroid components has been proposed and applied to a 3 D laser radar measurement system (LRMS). First, a single-point <b>error</b> <b>ellipsoid</b> analysis for the LRMS was established. An <b>error</b> <b>ellipsoid</b> model and algorithm for diameter measurement of cylindroid components was then proposed based on the single-point <b>error</b> <b>ellipsoid.</b> Finally, four experiments were conducted using the LRMS to measure the diameter of a standard cylinder in the laboratory. The experimental results of the uncertainty evaluation consistently matched well with the predictions. The proposed uncertainty evaluation model for cylindrical diameters can provide a reliable method for actual measurements and support further accuracy improvement of the LRMS...|$|E
40|$|Primary and {{secondary}} covariances combined and projected into conjunction plane (plane perpendicular to relative velocity vector at TCA) Primary placed on x-axis at (miss distance, 0) and represented by circle of radius equal to sum of both spacecraft circumscribing radiiZ-axis perpendicular to x-axis in conjunction plane Pc is portion of combined <b>error</b> <b>ellipsoid</b> that {{falls within the}} hard-body radius circl...|$|E
40|$|We {{present an}} {{approach}} for elastic registration of 3 D tomographic images {{which is based}} on a set of corresponding anatomical point landmarks and takes into account anisotropic landmark localization errors in form of 3 D <b>error</b> <b>ellipsoids.</b> The 3 D <b>error</b> <b>ellipsoids</b> are directly estimated from the image data. The performance of our approach is demonstrated for 2 D and 3 D MR images of the human brain. 1...|$|R
40|$|BHB (+ blue straggler) {{catalogue}} with {{distances and}} proper motions (updated using Sergey's database), and BHB probabilities using random forest classifier trained on SDSS spectroscopic sample (labelled using their log g). TGAS+RAVE-on {{with a few}} cuts and angles+frequencies computed. Only appropriate for investigating large scale trends because I have not sampled from the <b>error</b> <b>ellipsoids</b> for each target. Also, I used 1 /parallax (sorry), but will update soon using better distance estimates. I can provide the SQL queries I used to get these samples. NB both catalogs will probably be changed {{throughout the course of}} the week so check with me before you finalise results...|$|R
40|$|The {{random error}} pattern of point clouds has {{significant}} effect on the quality of final 3 D model. The magnitude and distribution of random errors should be modelled numerically. This work aims at developing such an anisotropic point error model, specifically for the terrestrial laser scanner (TLS) acquired 3 D point clouds. A priori precisions of basic TLS observations, which are the range, horizontal angle and vertical angle, are determined by predefined and practical measurement configurations, performed at real-world test environments. A priori precision of horizontal () and vertical () angles are constant for each point of a data set, and can directly be determined through the repetitive scanning of the same environment. In our practical tests, precisions of the horizontal and vertical angles were found as =± 36. 6 and =± 17. 8, respectively. On the other hand, a priori precision of the range observation () is assumed to be a function of range, incidence angle of the incoming laser ray, and reflectivity of object surface. Hence, it is a variable, and computed for each point individually by employing an empirically developed formula varying as =± 2 − 12 for a FARO Focus X 330 laser scanner. This procedure was followed by the computation of <b>error</b> <b>ellipsoids</b> of each point using the law of variance-covariance propagation. The direction and size of the <b>error</b> <b>ellipsoids</b> were computed by the principal components transformation. The usability and feasibility of the model was investigated in real world scenarios. These investigations validated the suitability and practicality of the proposed method...|$|R
40|$|This paper {{describes}} the measurement principle, the mathematical model, the calibration procedure, and the error {{model of a}} range sensor based on the Coded-Light Approach. The sensor consists of a CCD camera and a stripe projector. 3 -D measurements are realized by the intersection of camera rays with the planes generated by the stripe projection. Using projective geometry renders camera and projector model linear. A calibration procedure based on general least squares estimation is proposed. The estimation model consists of the analytic and stochastic model of the range sensor, which yields estimations of the model parameters {{as well as their}} accuracies. With introduction of unknown 3 -D check points in the estimation model, the accuracy of the range measurements is obtained as the main result of this work. The confidence interval of the range measurements is represented as an <b>error</b> <b>ellipsoid.</b> It turned out that the longest axis of the <b>error</b> <b>ellipsoid</b> is almost parallel to the projector [...] ...|$|E
30|$|An {{arbitrary}} <b>error</b> <b>ellipsoid</b> {{is assumed}} with dimensions [along track, cross track, radial] = [2 km, 2 km, 1 km] for all objects. These values are deemed descriptive of {{the error of}} a TLE state, and no considerations are made for nonlinear error state transition. For the calculation of the breakup time, only valid TLEs with epochs after the approximate breakup time are used to propagate backwards to estimate {{the time of the}} event.|$|E
40|$|The {{dynamics}} of error growth in a two-layer nonlinear quasi-geostrophic {{model has been}} studied to {{gain an understanding of}} the mathematical theory of atmospheric predictability. The growth of random errors of varying initial magnitudes has been studied, and the relation between this classical approach and the concepts of the nonlinear dynamical systems theory has been explored. The local and global growths of random errors have been expressed partly in terms of the properties of an <b>error</b> <b>ellipsoid</b> and the Liapunov exponents determined by linear error dynamics. The local growth of small errors is initially governed by several modes of the evolving <b>error</b> <b>ellipsoid</b> but soon becomes dominated by the longest axis. The average global growth of small errors is exponential with a growth rate consistent with the largest Liapunov exponent. The duration of the exponential growth phase depends on the initial magnitude of the errors. The subsequent large errors undergo a nonlinear growth with a steadily decreasing growth rate and attain saturation that defines the limit of predictability. The degree of chaos and the largest Liapunov exponent show considerable variation with change in the forcing, which implies that the time variation in the external forcing can introduce variable character to the predictability...|$|E
40|$|Two riveted antenna {{panels on}} rings number 3 and 9 {{were removed from}} the 34 m antenna at DSS- 15, fixed in the leveled {{position}} and the surface was photographed indoors. The results from this pilot photogrammetric demonstration and diagnostics of panel surface contours, are presented. The photogrammetric network for each panel incorporated eight photographs, two from each of four camera stations and observed over 200 targets. The accuracy (1 sigma) of the XYZ coordinates for the <b>error</b> <b>ellipsoids</b> was + or - 0. 013 mm (0. 0005 inch). This level of precision relative to the object size corresponds roughly to 1 part in 250, 000 which is superior to conventional dial sweep-arm template techniques by at least a factor of 4...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedExtended Kalman filtering {{is applied to}} the PLRS (Position Locating Reporting System). Here the nonlinearity to the filter enters through the measurement (range only). The nonlinearity being the relationship between range and the Cartesian coordinate states of the filter. Filter covariances of error are portrayed as <b>error</b> <b>ellipsoids.</b> these are used to determine which one should be used to update a unit when there are several other units available for ranging. One should attempt to make the range measurement is the same direction of the major axis of error associated with the unit to be updated. The filtering techniques are evaluated using static units and high speed maneuvering aircraft. Naval Electronics System Command - Project Number N 0003975 PO 52945 [URL] United States Marine Corp...|$|R
40|$|The present article {{deals with}} three-variant {{processing}} and finding estimates of unknown parameters in a geodetic network by {{the technology of}} global navigation satellite systems in 2004, 2008 and 2011. The assessment {{of the impact of}} a used method of adjustment on the estimation of parameters of the first and second order of the geodetic network and the presentation of results of a deformation analysis with graphical visualisation of individual processing and analyses are the objectives of this paper. An LSM method and robust M-estimates according to Huber and Hampel were used for the processing and adjustment of observations. All three processing methods showed a displacement of point No. 5005 in the epoch 04 - 08 in the analysis of the stability of points, which is also confirmed by graphical visualisations using confidence <b>error</b> <b>ellipsoids...</b>|$|R
40|$|Aims. We study degeneracies between cosmological {{parameters}} and measurement errors from cosmic shear surveys. We simulate realistic survey topologies with non-uniform sky coverage, and quantify {{the effect of}} survey geometry, depth and noise from intrinsic galaxy ellipticities on the parameter errors. This analysis allows us to optimise the survey geometry. Methods. We carry out a principal component analysis of the Fisher information matrix to assess the accuracy with which linear combinations of parameters can be determined. Using the shear two-point correlation functions and the aperture mass dispersion, which can directly be measured from the shear maps, we study various degeneracy directions in a multi-dimensional parameter space spanned by Ωm, ΩΛ, σ 8, the shape parameter Γ, the spectral index ns, along with parameters that specify the distribution of source galaxies. Results. A principal component analysis is an effective tool to probe the extent and dimensionality of the <b>error</b> <b>ellipsoid.</b> If only three parameters are to be obtained from weak lensing data, a single principal component is dominant and contains all information about the main parameter degeneracies and their errors. For four or more free parameters, the first two principal components dominate the parameter errors. The degeneracy directions are insensitive against variations in the noise or survey geometry. The variance of the dominant principal component of the Fisher matrix, however, scales with the noise. Further, it shows a minimum for survey strategies which have small cosmic variance and measure the shear correlation up to several degrees. This minimum is less pronounced if external priors are added, rendering the optimisation less effective. The minimisation of the Fisher <b>error</b> <b>ellipsoid</b> can lead to slightly different results than the principal component analysis...|$|E
40|$|Abstract. Recent use of Type Ia supernovae {{to measure}} {{acceleration}} of the universe has motivated questions regarding their optimal use to constrain cosmological parameters ΩM, ΩΛ and wQ. In this work we address the question: what is the optimal distribution of supernovae in redshift in order to best constrain the cosmological parameters? The {{solution to this problem}} is not only of theoretical interest, but can be useful in planning supernova searches. Using the Fisher matrix formalism we show that the <b>error</b> <b>ellipsoid</b> corresponding to N parameters (for N = 1, 2, 3) has the smallest volume if the supernovae are located at N discrete redshifts, with equal number of supernovae at each redshift and with one redshift always being the maximum one probed. Including marginalization over the “nuisance parameter ” M changes this result only trivially. ...|$|E
40|$|A new {{derivation}} of an algorithm which fuses {{the outputs}} of two Kalman filters is presented {{within the context}} of previous research in this field. Unlike works from different authors, this derivation clearly shows the combination of estimates to be optimal, minimizing the trace of the fused covariance matrix. The algorithm assumes that the filters use identical models, and are stable and operating optimally with respect to their own local measurements. Evidence is presented which indicates that the <b>error</b> <b>ellipsoid</b> derived from the covariance of the optimally fused estimate is contained within the intersections of the error ellipsoids of the two filters being fused. Modifications which reduce the algorithm's data transmission requirements are also presented, including a scalar gain approximation, a cross-covariance update formula which employs only the two contributing filters ' autocovariances, and a form of the algorithm which can be used to reinitializ...|$|E
40|$|We {{study how}} direct {{detection}} of the inflationary gravitational wave background constrains inflationary parameters and complements CMB polarization measurements. The <b>error</b> <b>ellipsoids</b> calculated using the Fisher information matrix approach with Planck and the direct detection experiment, BBO (Big Bang Observer), show different directions of parameter degeneracy, and the degeneracy is broken {{when they are}} combined. For a slow-roll parameterization, we show that BBO could significantly improve the constraints on the tensor-to-scalar ratio compared with Planck alone. We also look at a quadratic and a natural inflation model. In both cases, if the temperature of reheating is also treated as a free parameter, then the addition of BBO can significantly improve the error bars. In the case of natural inflation, {{we find that the}} addition of BBO could even partially improve the error bars of a cosmic variance-limited CMB experiment. Comment: 12 pages, 5 figures; matches version to appear in PRD; typos correcte...|$|R
30|$|To {{assess the}} {{reliability}} of the relocated hypocenters, we applied a statistical resampling approach termed as bootstrap method to the events. We calculated synthetic arrival time data by adding random Gaussian noises {{with a standard deviation of}} 0.05  s (average of 0  s) to the observed arrival time data. We then relocated events to determine the shift in location based on the corrupted arrival times. The process was repeated 500 times with new Gaussian noise generated for each iteration. Based on the cumulative results, the average horizontal and depth errors are 160 and 340  m, respectively. The <b>error</b> <b>ellipsoids</b> of each hypocenter on the horizontal surface are slightly elongated along the NE–SW directions. The hypocenter accuracy of the LP events are not so well compared with that of the VT events, due to a shortage of high-frequency components at onsets of P- and S-wave arrivals. Furthermore, absolute locations were sensitive to the assumed velocity structure, but the relative distribution of earthquake was less sensitive. For an example, relocated hypocenters tend to be deeper and more clustered for an assumed velocity model with a small velocity gradient near the surface.|$|R
40|$|Navigating {{underground}} when {{drilling for}} oil and gas has become more challenging as companies try to hit smaller targets in reservoirs already congested with existing wells. One widely used method is Measurement While Drilling (MWD) using magnetic survey tools to direct the drill head. The provision of accurate geomagnetic field values with verifiable estimates of uncertainty is of utmost importance as the estimates help mitigate the risk of collision or missing a target. The BGS offers three options to the oil industry depending on accuracy required: the basic option is to use estimates of the geomagnetic field from the annually updated BGS Global Geomagnetic Model (BGGM); the second and more accurate option, In-Field Referencing (IFR), includes estimates of the local crustal magnetic field; the third and most accurate option, Interpolation In-Field Referencing (IIFR), includes estimates of the rapidly time-varying magnetic field at the oil field. The estimates of uncertainty in the geomagnetic field values supplied under each of these options are almost as mportant as the values themselves because they are incorporated into company software which derives positional <b>error</b> <b>ellipsoids</b> along the well-path. We describe work done over several years on the derivation and communication of geomagnetic field uncertainty for the oil industry...|$|R
40|$|The robust Capon {{beamformer}} {{has been}} shown to alleviate the problem of signal cancellation resulting from steering vector errors, caused, e. g., by calibration and/or angle- of-arrival errors, which would otherwise seriously deteriorate the performance of an adaptive beamformer. Here, we examine robust Capon beamforming of multi-dimensional arrays, where robustness to angle-of-arrival errors is needed in both azimuth and elevation. It is shown that the commonly used spherical uncertainty sets are unable to control robustness in each of these directions independently. Here, we instead propose the use of flat ellipsoidal sets to control the angle-of-arrival un- certainty. To also allow for other errors, such as calibration errors, we combine these flat ellipsoids with a higher-dimension <b>error</b> <b>ellipsoid.</b> Computationally efficient automatic techniques for estimating the necessary uncertainty sets are derived, and the proposed methods are evaluated using both simulated data and experimental underwater acoustics measurements, clearly showing the benefits of the technique...|$|E
40|$|With the {{continuous}} {{development of the}} terrestrial laser scanning (TLS) technique, the precision of the laser scanning has been improved which makes it possible that TLS {{could be used for}} high-precision deformation monitoring. A deformation monitorable indicator (DMI) should be determined to distinguish the deformation from the error of point cloud and {{plays an important role in}} the deformation monitoring using TLS. After the DMI determined, a scheme of the deformation monitoring case could be planned to choose a suitable instrument, set up a suitable distance and sampling interval. In this paper, the point error space and the point cloud error space are modelled firstly based on the point <b>error</b> <b>ellipsoid.</b> Secondly, the actual point error is derived by the relationship between the actual point cloud error space and the point error space. Then, the DMI is determined using the actual point error. Finally, two sets of experiments is carried out and the feasibility of the DMI is proved...|$|E
40|$|Artículo de publicación ISIThe triple {{radio source}} in the Serpens star {{formation}} region consists of a central object and two lobes with spectral indices characteristic of nonthermal emission. Measurements made at the Arecibo Observatory and the VLA indicate that this triple source is probably associated with the Serpens star-forming region. A weak, circularly polarized 1667 MHz OH maser detected at Arecibo has been observed with the VLA and found to coincide in position to within 1 arcsec with the central source. The position of this central source also falls within the position <b>error</b> <b>ellipsoid</b> of IRAS 18273 + 0113. VLA observations at 6 cm clearly show that the outer components are {{moving away from the}} central source. The angular motion of each component with respect to the central one is 1. 3 + or - 0. 2 arcsec, implying velocities of 300 + or - 100 km/s at a distance of 500 + or - 200 pc...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedExtended Kalman filtering is applied {{as an extension}} of the Position Location Reporting System (PLRS) to track a moving target in the XY plane. The application uses four sets of observables which correspond to inputs from a fused-sensor array where the sensors employed are acoustic, seismic, or radar. The nonlinearities to the Kalman filter occur through the measured observables which are: bearings to the target only, ranges to the target only, bearings and ranges to the target, and a Doppler-shifted frequency accompanied by the bearing to that frequency. The observables are nonlinear in their relationships to the Cartesian coordinate states of the filter. Filter error covariances are portrayed as <b>error</b> <b>ellipsoids</b> about the laser target estimate made by the filter. Rotation of the ellipsoids is accomplished to avoid the cross correlation of the coordinates. The ellipsoids employed are one standard of deviation in the rotated coordinate system and correspond to a constant of probability of target location about the latest Kalman target estimate. Filtering techniques are evaluated for both stationary and moving observers with arbitrarily moving targets. The objective of creating a user-friendly, personal computer based tracking algorithm is also discussed. [URL] United States Marine Corp...|$|R
40|$|Laser {{scanners}} {{are used}} {{more and more}} in mobile mapping systems. They provide 3 D point clouds that are used for object reconstruction and registration of the system. For both of those applications, uncertainty analysis of 3 D points is of great interest but rarely investigated in the literature. In this paper we present a complete pipeline that takes into account all the sources of uncertainties and allows to compute a covariance matrix per 3 D point. The sources of uncertainties are laser scanner, calibration of the scanner in relation to the vehicle and direct georeferencing system. We suppose that all the uncertainties follow the Gaussian law. The variances of the laser scanner measurements (two angles and one distance) are usually evaluated by the constructors. This is also the case for integrated direct georeferencing devices. Residuals of the calibration process were used to estimate the covariance matrix of the 6 D transformation between scanner laser and the vehicle system. Knowing the variances of all sources of uncertainties, we applied uncertainty propagation technique to compute the variance-covariance matrix of every obtained 3 D point. Such an uncertainty analysis enables to estimate the impact of different laser scanners and georeferencing devices on the quality of obtained 3 D points. The obtained uncertainty values were illustrated using <b>error</b> <b>ellipsoids</b> on different datasets...|$|R
40|$|The {{well-known}} {{problem of}} estimating an unknown deterministic parameter vector over a linear system subject to additive Gaussian noise is studied {{from the perspective}} of minimizing total sensor measurement cost under a constraint on the log volume of the estimation <b>error</b> confidence <b>ellipsoid.</b> A convex optimization problem is formulated for the general case, and a closed form solution is provided when the system matrix is invertible. Furthermore, effects of system matrix uncertainty are discussed by employing a specific but nevertheless practical uncertainty model. Numerical examples are presented to discuss the theoretical results in detail. © 2012 IEEE...|$|R
40|$|Contents 1 Normal random {{variables}} 2 2 Normal random vectors 7 2. 1 Particularization for second order.................. 9 2. 2 Locus of constant probability.................... 14 3 Properties 22 4 Covariance matrices and <b>error</b> <b>ellipsoid</b> 24 Normal {{random variables}} A random variable X {{is said to}} be normally distributed with mean and variance if its probability density function (pdf) is f X (x) = < x < #. (1. 1) Whenever there is no possible confusion between the random variable X and the real argument, x, of the pdf this is simply represented by f(x) omitting the explicit reference to the random variable X in the subscript. The Normal or Gaussian distribution of X is usually represented by,),). The Normal or Gaussian pdf (1. 1) is a bell-shaped curve that is symmetric about the mean and that attains its maximum value of 0. 399 at x = as represented in Figure 1. 1 for = 2 and # = 1. 5. The Gaussian pd...|$|E
40|$|Recent use of Type Ia supernovae {{to measure}} {{acceleration}} of the universe has motivated questions regarding their optimal use to constrain cosmological parameters Omega_M, Omega_Λ and w_Q. In this work we address the question: what is the optimal distribution of supernovae in redshift in order to best constrain the cosmological parameters? The {{solution to this problem}} is not only of theoretical interest, but can be useful in planning supernova searches. Using the Fisher matrix formalism we show that the <b>error</b> <b>ellipsoid</b> corresponding to N parameters (for N= 1, 2, 3) has the smallest volume if the supernovae are located at N discrete redshifts, with equal number of supernovae at each redshift and with one redshift always being the maximum one probed. Including marginalization over the "nuisance parameter" M changes this result only trivially. Comment: 12 pages, 6 eps figures. To appear in Proceedings of "Sources and Detection of Dark Matter/Energy in the Universe", Marina Del Rey, Feb. 200...|$|E
40|$|Collision {{analysis}} and mitigation performed for the QB 50 mission. The {{aim of this}} thesis report is to identify which mitigation strategies are most suitable for a network of uncontrollable satellites. Furthermore, {{the aim is to}} set-out a method to determine the collision probability for a network of uncontrollable satellites, and identify the parameters that influence the collision probability. These methods are applied to the QB 50 mission; to find a scenario where the collision probability is lowest. An alternative method is developed by the author to calculate the Gaussian probability, which is applicable for small satellites. As the size of the satellites decreases relative to the <b>error</b> <b>ellipsoid,</b> the probability at a certain moment in time becomes more equal to the probability {{at the center of the}} combined sphere (assuming spherical satellites). Now, instead of dealing with a cumbersome volume integral through the combined <b>error</b> <b>ellipsoid,</b> the collision probability can be approached by a line-integral times the area of the combined satellites’ bodies. Four ideal deployment angles for the QB 50 mission were found located in a plane of zero pitch and at yaw angles of 34 ?, 146 ?, 248 ?, 326 ? measured from velocity vector of the upperstage. Deploying at zero pitch has the effect that the phase between the cross-track and radial separation is half the orbital period. This has the consequence that, when either the cross-track separation is zero, the radial separation has its maximum and visa versa. This can also be identified as (anti-) parallel alignment of relative eccentricity and inclination vectors. Synchronization of the motion at times where the deployment is half or equal to the orbital period should be avoided. For these satellites the amplitude radial and cross-track separations are small. Furthermore, the relative perturbations between these satellites is large, decreasing the offset of the radial motion. This causes the radial and cross-track separation reach zero at equal time. Synchronized satellite increase the collision probability significantly. Both Patera’s method and the line-integral method are applied to a full-scale simulation for the QB 50 mission. Multiple scenario’s are chosen for the full-scale simulation. The two scenario’s with the lowest probability are sequential deployment at one of the ideal angles and alternating deployment between two opposite ideal angles. These fall below the threshold value of 10 ? 4, a value used by the German Space Operations Center (GSOC). Space Systems EngineeringSpace FlightAerospace Engineerin...|$|E
40|$|This paper {{develops}} {{an extended}} ellipsoidal outer-bounding set-membership estimation (EEOB-SME) algorithm with high accuracy and efficiency for nonlinear discrete-time systems under unknown-but-bounded (UBB) disturbances. The EEOB-SME linearizes the first-order terms {{about the current}} state estimations and bounds the linearization <b>errors</b> by <b>ellipsoids</b> using interval analysis for nonlinear equations of process and measurement equations, respectively. It has been demonstrated that the EEOB-SME algorithm is stable and the estimation errors of the EEOB-SME are bounded when the nonlinear system is observable. The EEOB-SME decreases the computation load and the feasible sets of EEOB-SME contain more true states. The efficiency of the EEOB-SME algorithm has been shown by a numerical simulation under UBB disturbances...|$|R
40|$|Fig. 1. Uncertainty flow showing {{variations}} of uncertainty along different analysis processes with four major steps: opinion mining, brushing in items space, correlation analysis, and combined analysis. Uncertainty arises during opinion mining, and increases, decreases, spits, or merges in subsequent steps. (a) and (b) show that uncertainty increases and decreases, respectively. (c) and (d) show two different results obtained by merging {{the results of}} correlation analysis. (c) reveals that female customers complain more while male customers complain less, whereas (d) only reveals that female customers complain more. Compared with (c), (d) is more reliable as it excludes a highly uncertain result of correlation analysis for the combined analysis. Abstract—Uncertainty can arise in any stage of a visual analytics process, especially in data-intensive applications with a sequence of data transformations. Additionally, throughout the process of multidimensional, multivariate data analysis, uncertainty due to data transformation and integration may split, merge, increase, or decrease. This dynamic characteristic along with other features of uncertainty pose a great challenge to effective uncertainty-aware visualization. This paper presents a new framework for modeling uncertainty and characterizing {{the evolution of the}} uncertainty information through analytical processes. Based on the framework, we have designed a visual metaphor called uncertainty flow to visually and intuitively summarize how uncertainty information propagates over the whole analysis pipeline. Our system allows analysts to interact with and analyze the uncertainty information at different levels of detail. Three experiments were conducted to demonstrate the effectiveness and intuitiveness of our design. Index Terms—Uncertainty visualization, uncertainty quantification, uncertainty propagation, <b>error</b> <b>ellipsoids,</b> uncertainty fusion. ...|$|R
40|$|A. 2 D {{scatter plot}} showing each fly as a dot, {{and the mean}} and {{standard}} error of the factor loadings as bars. Abscissa is the first principal component (PC 1). Ordinate is PC 2 on the upper panel and PC 3 on the lower panel. B. Snapshot of the corresponding 3 D representation plot. Each dot corresponds to a fly coordinate, bars are means and standard <b>errors,</b> and the <b>ellipsoids</b> represent the 80...|$|R
40|$|Lauer & Postman (LP) {{observe that}} all Abell {{clusters}} with redshifts less than 15, 000 {{appear to be}} participating in a bulk flow of 689 km s^- 1 {{with respect to the}} Cosmic Microwave Background. We find this result difficult to reconcile with all popular models for large-scale structure formation that assume Gaussian initial conditions. This conclusion is based on Monte-Carlo realizations of the LP data, drawn from large Particle-Mesh N-body simulations. We have taken special care to treat properly the longest-wavelength components of the power spectra. Bulk flows with amplitude as large as that reported by LP are not uncommon in the Monte-Carlo datasets. However, the χ^ 2 of the observed bulk flow, taking into account the anisotropy of the <b>error</b> <b>ellipsoid,</b> is much more difficult to match in the simulations. The models examined are ruled out at confidence levels between 94 % and 98 %. Any model that has intrinsic flows of less than 480 on the scales probed by LP scales can be ruled out at a similar level. Comment: Submitted to ApJ. 31 pages of uuencoded compressed postscript (810 kbytes); figures included. Also available via anonymous ftp to eku. ias. edu in /pub/strauss/warpfire/warpfire. ps. ...|$|E
40|$|We {{perform a}} {{detailed}} analysis of all existing data (CELLO, CLEO, BaBar) on the pion-photon transition form factor by means of light-cone sum rules in which we include the NLO QCD radiative corrections and the twist-four contributions. The NNLO radiative correction together with the twist-six contribution are also taken into account in terms of theoretical uncertainties. Keeping only the first two Gegenbauer coefficients a_ 2 and a_ 4, we show that the 1 σ error ellipse of all data up to 9 GeV^ 2 greatly overlaps with the set of pion distribution amplitudes obtained from nonlocal QCD sum rules [...] -within the range of uncertainties due to twist-four. This remains valid also for the projection of the 1 σ <b>error</b> <b>ellipsoid</b> on the (a_ 2,a_ 4) plane when including a_ 6. We argue that {{it is not possible to}} accommodate the high-Q^ 2 tail of the BaBar data with the same accuracy, despite opposite claims by other authors, and conclude that the BaBar data still pose a challenge to QCD. Comment: 10 pages, 2 Tables, 4 figures. In v 2 some explanations and references added. In v 3 some clarifications added and references updated; matches version to appear in Phys. Rev. ...|$|E
40|$|Context. Weak {{gravitational}} lensing is {{a powerful}} probe of large-scale structure and cosmology. Most commonly, second-order correlations of observed galaxy ellipticities are expressed as a projection of the matter power spectrum, corresponding to the lowest-order approximation between the projected and 3 d power spectrum. Aims. The dominant lensing-only contribution beyond the zero-order approximation is the reduced shear, which takes into account not only lensing-induced distortions but also isotropic magnification of galaxy images. This involves an integral over the matter bispectrum. We provide a fast and general way to calculate this correction term. Methods. Using {{a model for the}} matter bispectrum, we fit elementary functions to the reduced-shear contribution and its derivatives with respect to cosmological parameters. The dependence on cosmology is encompassed in a Taylor-expansion around a fiducial model. Results. Within a region in parameter space comprising the WMAP 7 68 % <b>error</b> <b>ellipsoid,</b> the total reduced-shear power spectrum (shear plus fitted reduced-shear correction) is accurate to 1 % (2 %) for l< 10 ^ 4 (l< 2 x 10 ^ 5). This corresponds to a factor of four reduction of the bias compared to the case where no correction is used. This precision is necessary to match the accuracy of current non-linear power spectrum predictions from numerical simulations. Comment: 7 pages, 3 figures. A&A in press. Revised version with minor change...|$|E
40|$|In {{order to}} obtain high quality {{positions}} from navigation satellites, range errors have to be identified and either modelled or estimated. This thesis focuses on satellite clock errors, which are needed to be known because satellite clocks are not perfectly synchronised with navigation system time. A new approach, invented at UCL, for the simultaneous estimation, in a single epoch, of all satellite clock offsets within a Global Navigation Satellite System (GNSS) from range data collected at {{a large number of}} globally distributed ground stations is presented. The method was originally tested using only data from a limited number of GPS satellites and ground stations. In this work a total of 50 globally distributed stations and the whole GPS constellation are used in order to investigate more fully the capabilities of the method, in terms of both accuracy and reliability. A number of different estimation models have been tested. These include those with different weighting schemes, those with and without tropospheric bias parameters and those that include assumptions regarding prior knowledge of satellite orbits. In all cases conclusions have been drawn based on formal error propagation theory. Accuracy has been assessed largely through the sizes of the predicted satellite clock standard deviations and, in case of simultaneously estimating satellite positions, their <b>error</b> <b>ellipsoids.</b> Both internal and external reliability have been assessed as there are important contributions to integrity, something that is essential for many practical applications. It has been found that the accuracy and reliability of satellite clock offsets are functions of the number of known ground station clocks and distance from them, quality of orbits and quality of range measurement. Also the introduction of tropospheric zenith delay parameters into the model reduced both accuracy and reliability by amounts depending on satellite elevation angles...|$|R
40|$|Visualization of {{uncertainty}} or error in astrophysical data is seldom available in simulations of astronomical phenomena, and yet almost all rendered attributes possess some degree {{of uncertainty}} due to observational error. Uncertainties associated with spatial location typically vary significantly with scale and thus introduce further complexity {{in the interpretation of}} a given visualization. This paper introduces effective techniques for visualizing uncertainty in large-scale virtual astrophysical environments. Building upon our previous transparently scalable visualization architecture, we develop tools that enhance the perception and comprehension of uncertainty across wide scale ranges. Our methods include a unified color-coding scheme for representing log-scale distances and percentage <b>errors,</b> an <b>ellipsoid</b> model to represent positional uncertainty, an ellipsoid envelope model to expose trajectory uncertainty, and a magic-glass design supporting the selection of ranges of log-scale distance and uncertainty parameters, as well as an overview mode and a scalable WIM tool for exposing the magnitudes of spatial context and uncertainty...|$|R
40|$|New {{visualization}} trends {{bring new}} {{light to the}} way environmental monitoring data is exploited today. The presented application is an enhancement of the user experience for the visualization {{and analysis of the}} Argos service for animal tracking. Argos is a satellite-based system that collects data from Platform Terminal Transmitters, and distributes sensor and location data {{in order to understand the}} distribution of wildlife in a given territory. Virtual globes constitute a visualization platform of geospatial information projected onto the Earth Digital Elevation Model (DEM) in three dimensions, users interactively rotate, zoom and pan the available data, usually projected on a given background that provides a geographic context. The 3 D platform allows access to animal tracking data. It also permits the extraction of specific information, path animation and data download from a dedicated and secure platform, with the objective of facilitating the extensive use of this source of information. The animal tracking 3 D desktop application has been built with the World Wind SDK, an open source virtual globe built in Java/OpenGL and developed by the National Aeronautics and Space Administration (NASA). The user can combine tracking data with layers from any public server implementing the WMS (Web Mapping Service) protocol of the OGC (Open Geospatial Consortium) through a tab menu able to handle the layers and opacity. Animal tracks can be animated individually alongside the elevation profile of the trajectory with the <b>error</b> <b>ellipsoids</b> of each position of the animal. As for the input/output capabilities of the tool, the CSV format is used as default in order to be compatible with existing operational services. KML is also available to enhance interoperability within GIS communities. The application allows the possibility to query directly from the WebServices that hold the animal data through a REST API using SOAP (Simple Object Access Protocol) with authentication. The new 3 D approach to the way data is visualized and the possibility to overlay a wide range of data from open WMS servers, allows users of the service to have a more realistic and integrated vision of their animal positioning data...|$|R
