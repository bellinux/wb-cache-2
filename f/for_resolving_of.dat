14|10000|Public
50|$|Meanwhile, a {{completely}} unrelated bill (H.R. 3979), which clarified federal law so that emergency services volunteers {{were not considered}} employees under the Patient Protection and Affordable Care Act, was moving through Congress. The House approved this bill on March 11, 2014. The U.S. Senate approved the bill, with amendments, on April 7, 2014, and sent the bill back to the House <b>for</b> <b>resolving</b> <b>of</b> differences.|$|E
40|$|This work {{makes an}} attempt at {{theoretical}} summary of the major {{issues related to the}} essence, types, forms and procedures applied <b>for</b> <b>resolving</b> <b>of</b> collective bargain disputes and the major goals and principles of the negotiations intended for dispute resolving. The work gives reason to the methodological basis of the potential for mathematical formalization of labor conflicts from the viewpoint of the theory for regulating the dialectical contradictions. Concrete patterns are suggested to serve as prove for the usability of the game theory to analyze collective labor disputes in antagonistic and cooperative aspect. ...|$|E
40|$|Abstract – The paper {{describes}} technology {{aspects of}} proposed solution <b>for</b> <b>resolving</b> <b>of</b> state explosion problem. The main point is in usage of guides {{which can be}} created both manually and automatically and allow to reduce state space during trace generation process. The following techniques is described: traceability tracking of requirements, guides generation based on selected criterion, guides analysis in case of problems with traces generation. Usage of described techniques in verification and testing phases of software development allow to resolve problem of state exposure for industrial projects. Key words — design specification, state explosion problem, requirements, behavioral tree, guides, guide formalization. I...|$|E
30|$|Today, {{applications}} of modeling and regression {{methods have been}} widely spread in different fields of the science. These methods were applied for increasing the quality of forecasts using the sample data sets from experiments and different investigations. Appropriate use of these methods is very useful <b>for</b> <b>resolve</b> <b>of</b> uncertainties and results of predictions, when obtain of information is difficult and sometimes impossible.|$|R
5000|$|Four Steps for Handling a Problem- A Proposition <b>for</b> <b>Resolving</b> the Difficulties <b>of</b> Life: ...|$|R
5000|$|C3 linearization, a {{computer}} science algorithm <b>for</b> <b>resolving</b> the order <b>of</b> method resolution in multiple inheritance conditions ...|$|R
40|$|Bc. Veronika Turková Improvement of nonconformities {{control system}} Master´s thesis, Institute of {{metrology}} and quality assurance, Brno University of technology This thesis {{deals with the}} issue of improving the management system of nonconformities control in a company ALSTOM Power, s. r. o. ALSTOM Group, the Division of boilers production, located in Brno. This thesis contains an analysis of current status of nonconformities control process and designs a new nonconformity control process using a new internal database <b>for</b> <b>resolving</b> <b>of</b> nonconformities. These measures should deal with the efficiency of the management of nonconformities process and reduce current company pains...|$|E
40|$|A new {{approach}} is developed here <b>for</b> <b>resolving</b> <b>of</b> the Poisson equations in case the components of angular velocity of rigid body rotation could be considered as the functions of time-parameter t only. Fundamental solution is presented by the analytical formulae in dependence on two time-dependent, the real-valued coefficients. Such coefficients as above are {{proved to be the}} solutions of mutual system of 2 Riccati ordinary differential equations (which has no analytical solution in general case). All in all, the cases of analytical resolving of Poisson equation are quite rare (according to the cases of exact resolving of the aforementioned system of Riccati ODEs). So, the system of Euler-Poisson equations is proved to have the analytical solutions (in quadratures) only in classical simplifying cases: 1) Lagrange case, or 2) Kovalevskaya case or 3) Euler case or other well-known but particular cases (where the existence of particular solutions depends on the choosing of the appropriate initial conditions). Comment: 11 pages, Keywords: Euler equations (rigid body dynamics), Poisson equations, principal moments of inertia, Riccati equation, Acta Mechanica (2017...|$|E
40|$|Abstract:- The {{computation}} {{of stress}} and strain conditions for tunnel profile excavation is performed by the Boundary Element Method (BEM). It is a numerical method performed in geotechnics <b>for</b> <b>resolving</b> <b>of</b> the stress and strain distribution in a rock massif where the change of stress condition occurs during some operations. The method is timesaving procedure for the problem definition in design process, it offers flexibility in assumptions on input parameters characterising the massif, and in observing the influence of changes on the stress and strain conditions. A computer programme has been developed for the purpose. This programme facilitates computation {{to the influence of}} different primary support systems on the stress and strain condition. In this paper, the stress and strain conditions for a tunnel profile excavation are compared with the stress and strain conditions for shotcrete lining supported excavation profile. Analogously to the treatment of reinforced concrete, the ultimate bearing capacity of the concrete and steel are considered. The flexibility of the primary support structure is important. The shotcrete lining shall be slender in order to avoid bending moments...|$|E
5000|$|Poorly chosen {{upstream}} frequency <b>for</b> white modems, <b>resolved</b> <b>for</b> deployment <b>of</b> black modems ...|$|R
50|$|De auctoritate praesidendi in concilio generali (1434), a {{proposal}} <b>for</b> <b>resolving</b> the question <b>of</b> presidency over the deliberations of the Council of Basil.|$|R
5000|$|Payne N, Snelling E, Fitzpatrick R, Seymour J, Courtney R, Barnett A, Watanabe Y, Sims D, Squire L & Semmens J (2015) A {{new method}} <b>for</b> <b>resolving</b> {{uncertainty}} <b>of</b> energy requirments in large water breathers: the ‘mega-flume’ seagoing swim-tunnel respirometer. Methods in Ecology and Evolution. Doi:10.11111/20410210S.12358 ...|$|R
40|$|The lack of {{knowledge}} and standard guidelines <b>for</b> <b>resolving</b> <b>of</b> structural failure cases has contributed to several problems in construction industry especially in making claims. This phenomenon often leads to many difficulties for the involved parties in the incidents of structural failure. This study attempted to explore the available law and contract provision that can be referred in managing structural failure cases. The study also tries to determine the common parties that claim {{to be responsible for}} this failure cases. Methodology adopted for this study involved interviews with professional and questionnaire survey. Findings of the study indicate that the most common laws referred for failure cases and claims are contract law and tort. Common offences inflicted normally related to breach of contract and negligence. The study also identified the common contract provisions that are being violated in failure cases as well the responsible parties that often being blamed for structural failure cases. Eventually, this study has made several proposals for the industry to improve the process of managing structural failure cases...|$|E
40|$|This work called The Federal Republic of Germany as an Object of the United States Foreign Policy 1949 - 1955 try to {{describe}} and summarize key moments of development U. S. - German relations {{during the period of}} the fifthies. The aim of the work is description and analysis of basic lines US foreign policy toward Germany and political soulutions <b>for</b> <b>resolving</b> <b>of</b> the so-called German question. Main focus of the work is oriented on guestion of persistence of the US German-policy basic direction and intentions and next problems as: range of German influence on US policy making; possibility for eventual American support German unification; influence of the so-called Stalin-note, Stalin's death and Winston Churchill détente policy on US policy decision making; continuum or discontinuum of the US German-policy compared Democratic and Repulican administrations; and comparing achievements and nonsuccesses of US German- and European-policy. The German question in the 1949 - 1955 period is very interesting, and in light of current problems of international policy even actual topic. Germany problem was only one part of the global controversy two world political systems. Germany question was expression of failure agreement on European- and World order. But Germany thanks its economic and military potential played a key role [...] ...|$|E
40|$|Goal of {{this paper}} is to notify of the {{research}} which was focused on demands in the area of knowledge potential development of firms. Knowledge as a part of companies’ and the whole society intangible assets have been getting fundamental importance during last years as they have been perceived to be important source of the competitive advantages The main interest is focused on „people oriented knowledge management“ in this paper. Possibilities for development were explored in methods and practices of strategic human resource management. There was revealed what kind of procedures, activities and processes are being perceived as important and useful to develop. The respondents assigned Teams, Cooperation, Activity, Culture and Strategy as the domains being the most important for knowledge potential development in the future. Next data were acquired by comparing the evaluation of the present situation and the future needs. There was found a difference between the present estate and the future estate (chance for future) and between the present importance and present use of opportunities (chance at present). Methods and activities of human resource management could be used <b>for</b> <b>resolving</b> <b>of</b> actual problems and for future knowledge potential development. The knowledge potential development can be also perceived as application field, where specific instruments of knowledge management as well as instruments of human resources management can be put into effect...|$|E
40|$|We give a new {{algorithm}} <b>for</b> <b>resolving</b> singularities <b>of</b> plane curves. The {{algorithm is}} polynomial {{time in the}} bit complexity model, does not require factorization, and works over Q or finite fields. 1 Introduction Resolving singularities is a central problem in computational algebraic geometry. In this paper we describe a new algorithm <b>for</b> <b>resolving</b> singularities <b>of</b> irreducible plane curves. The algorithm runs in polynomialtime in the bit complexity model, does not require polynomial factorization, and works over Q or any finite field. Classical algorithms <b>for</b> <b>resolving</b> singularities [2, 15, 7] {{use a combination of}} methods involving [...] the Newton polygon, a polygon in Z 2 whose vertices are the exponents of terms in f; [...] Puiseux series, power series with fractional exponents. These algorithms take polynomial time if we assume efficient factorization over algebraic extensions of the base field and unit-time arithmetic these extensions. Teitelbaum [13] establishes bounds on the d [...] ...|$|R
40|$|Serialization, the {{traditional}} method <b>of</b> <b>resolving</b> concurrent interactions, is often inappropriate; when interactions {{are dependent on}} each other, other policies <b>for</b> <b>resolving</b> them may be more suitable. We use semantic information to help categorize common interactions encountered in the modeling and simulation domain. This categorization enables us to suggest reasonable policies <b>for</b> <b>resolving</b> the effects <b>of</b> concurrent interactions. 1...|$|R
5000|$|From a pragma-dialectical perspective, {{in order}} to get an {{overview}} of those aspects in argumentative discourse that are crucial <b>for</b> <b>resolving</b> a difference <b>of</b> opinion, the following analytical operations are carried out: ...|$|R
40|$|Abstract. «Efficacy» of {{pathogens}} {{interaction with}} the immunity system is manifested by broad spreading of many bacterial infections including tuberculosis first of all and in activation of known and emergent pathogens. The refined mechanisms of avoiding of bacteria from recognizing by immune system as creation of obstacles for phagocytosis and intracellular killing, using of secretory systems like “syringe” for inoculation into host cells deregulated substances, suppression or enhancing of inflammatory response, activation of inhibitory receptors to suppress respiratory explosion in phagosome, decreasing of synthesis of proinflammatory cytokines by influences to inflammasomes, stimulation of cytokines production suppres sed of innate response, damage of key molecules on intracellular signal routes, manipulation with apoptosis and auto phagia {{with the aim of}} surviving and replication inside the host cells, blocking of processing and presentation of bacterial antigens have been evolutionary developed. The study of interaction between host and parasite allows to understand new facts characterized “logic of live being” on the pathogen level and to use their mechanisms of evasion <b>for</b> <b>resolving</b> <b>of</b> actual problems raised in human society, for example, development of original vaccines and principally new drugs for immune system correction in case of diseases such as oncogenic tumors, autoimmune and allergic diseases as well as infectious diseases which are difficult to prevent and treat. Moreover, it was proved that permanent interaction with microorganisms including pathogenic ones is useful for human being because bacterial substances “train” immune system of people and assist its evolutionary improvement. </p...|$|E
40|$|Abstract—It {{is known}} that {{residual}} welding deformations give negative effect to processability and operational quality of welded structures, complicating their assembly and reducing strength. Therefore, selection of optimal technology, ensuring minimum welding deformations, {{is one of the}} main goals in developing a technology for manufacturing of welded structures. Through years, JSC SSTC has been developing a theory for estimation of welding deformations and practical activities for reducing and compensating such deformations during welding process. During long time a methodology was used, based on analytic dependence. This methodology allowed defining volumetric changes of metal due to welding heating and subsequent cooling. However, dependences for definition of structures deformations, arising as a result of volumetric changes of metal in the weld area, allowed performing calculations only for simple structures, such as units, flat sections and sections with small curvature. In case of complex 3 D structures, estimations on the base of analytic dependences gave significant errors. To eliminate this shortage, it was suggested to use finite elements method <b>for</b> <b>resolving</b> <b>of</b> deformation problem. Here, one shall first calculate volumes of longitudinal and transversal shortenings of welding joints using method of analytic dependences and further, with obtained shortenings, calculate forces, which action is equivalent to the action of active welding stresses. Further, a finite-elements model of the structure is developed and equivalent forces are added to this model. Having results of calculations, an optimal sequence of assembly and welding is selected and special measures to reduce and compensate welding deformations are developed and taken. Keywords—Finite elements method, modeling, expected welding deformations, welding, assembling. I...|$|E
40|$|Extension of the {{production}} facilities life, focus on increased oil recovered, development of satellite reservoirs tied to the main facilities and innovation and advancement in the SAS technologies have led to an increased demands on the modifications and updating existing SAS and replacement of obsolete SAS technologies used on {{the production}} facilities. The SAS has {{a significant role in}} the petroleum processes because the failures in SAS can pose serious hazards for people and the environment. The SAS helps to detect conditions that signal potentially hazardous disturbances, and assist the operators of the system in the control and elimination of those disturbances. Use of SAS in the petroleum industry provides better protection and control solutions, real time performance required to meet reliability demands, industrial productivity and energy efficiency. At the same time SAS system has to be regularly updated and modified to take into account various factors described earlier which might impact on the safety of process. The challenges of SAS project implementation are investigated in the current master thesis. The main objective of this master thesis is to evaluate current SAS installations in the Norwegian Continental Shelf and its compliance with an industrial standards and regulations. Furthermore the thesis reviews development history of current SAS installations in NCS, evaluates possible future industrial demands and its impact on the future SAS technology, compares three SAS vendors which have current installation on the NCS and provides technical characteristics cross analysis of these systems. In the last part, the numbers of proposals have been made for future SAS developments and innovations under industrial demands. Recommendations have been proposed <b>for</b> <b>resolving</b> <b>of</b> gaps and challenges of current SAS installations in NCS...|$|E
50|$|Francine Shapiro is an American {{psychologist}} and educator who originated and developed Eye movement desensitization and reprocessing (EMDR), a form <b>of</b> psychotherapy <b>for</b> <b>resolving</b> the symptoms <b>of</b> traumatic and other disturbing life experiences.|$|R
40|$|This bachelor's thesis aims to {{evaluate}} the financial situation of the company SQS spol. s. r. o. during the period 2009 - 2013. The theoretical part of this thesis {{is focused on the}} theory of financial analysis. In the second part I will analyze and evaluate the company. In conclusion there are suggestions <b>for</b> <b>resolving</b> outputs <b>of</b> the thesis...|$|R
40|$|The {{goal of the}} bachelor's thesis {{concentrates}} on financial analysis of ELMATSERVIS s. r. o. company. The first part Theoretical grounds describes basic terms, formulas and procedures relating financial health of the company. The next part of the thesis compiles financial analysis for years 2006 - 2010. In conclusion there are suggestions <b>for</b> <b>resolving</b> outputs <b>of</b> the thesis...|$|R
40|$|Master's thesis in Industrial asset managementExtension of the {{production}} facilities life, focus on increased oil recovered, development of satellite reservoirs tied to the main facilities and innovation and advancement in the SAS technologies have led to an increased demands on the modifications and updating existing SAS and replacement of obsolete SAS technologies used on {{the production}} facilities. The SAS has {{a significant role in}} the petroleum processes because the failures in SAS can pose serious hazards for people and the environment. The SAS helps to detect conditions that signal potentially hazardous disturbances, and assist the operators of the system in the control and elimination of those disturbances. Use of SAS in the petroleum industry provides better protection and control solutions, real time performance required to meet reliability demands, industrial productivity and energy efficiency. At the same time SAS system has to be regularly updated and modified to take into account various factors described earlier which might impact on the safety of process. The challenges of SAS project implementation are investigated in the current master thesis. The main objective of this master thesis is to evaluate current SAS installations in the Norwegian Continental Shelf and its compliance with an industrial standards and regulations. Furthermore the thesis reviews development history of current SAS installations in NCS, evaluates possible future industrial demands and its impact on the future SAS technology, compares three SAS vendors which have current installation on the NCS and provides technical characteristics cross analysis of these systems. In the last part, the numbers of proposals have been made for future SAS developments and innovations under industrial demands. Recommendations have been proposed <b>for</b> <b>resolving</b> <b>of</b> gaps and challenges of current SAS installations in NCS...|$|E
40|$|Research topic A {{problem of}} {{fundamental}} interest is semantic video understanding, particularly {{in terms of}} human location and identity. There are several reasons why one is interested in this research topic. First of all, a semantic video description can provide the basis for a video indexing system, which can be used to process video signals so as to make possible search requests on a natural language. A second reason why face detection and recognition in video is important is that they are potentially capable of letting us learn {{a great deal about the}} entire video without having to have semantically understand it on a very deep level for majority of applications (i. e. humans are the key objects of interest in semantic video analysis that capture significantly higher degree of importance then any other objects in a general case). This property is especially important when the cost of getting semantic meaning from video is very high. Finally, the most important reason why face detection and recognition in video are important is that they often work extremely well in practice, and enable us to realize accurate and reliable practical systems, in a very efficient manner. In this work we concern ourselves strictly with face detection and recognition problem being a detached part of more general semantic video understanding problem. Thus we didn’t explore possibilities of better video comprehension by integration of multiple types of semantic descriptors together. We have focused our attention on the solution that is accurate and reliable enough to work more effective than a human expert in video analysis applications. Particularly we are interesting in all types of video, where human individuals can be found. Problems and Objective Neither the theory of face detection and recognition in video nor its applications to pattern recognition is new. Many research papers were published staring from the 1970 th. However, widespread application of the theory of face detection and recognition to semantic video understanding has occurred only within the past decade. There are several reasons why this has been the case. First, the growth of the telecommunication industry reaches the barriers of human-wise media information management and the need of automatic video understanding and management systems became critical for further development. The second reason was that the original applications of the face detection and recognition, mainly in security domain, didn’t provide sufficient performance for most semantic video analysis demands. As a result, the fundamentals of the face detection and recognition theory, has provided a sufficient level of details for a number of research labs to begin to work on further development in the direction discussed above. Therefore the objective of this work can be summarized as follows: to propose face detection and recognition in video solution that is enough fast, accurate and reliable to be implemented in the semantic video understanding system that is capable of replacing human expert in a variety of multimedia indexing applications. Meanwhile we assume that the research results that were raised during this work are complete enough to be adapted or modified as a part of other image processing, pattern recognition and video indexing and analysis systems. Our Approach A video signal can be represented by a series of layers, each containing one semantic object. The only difference between semantically uncovered signal and a standard video is that all layers are mixed in the last case. Given the above scenario, the problem of interest is how do we decompose the video signal into layers to isolate semantic objects in it. Because of unique nature, human face was used as a token to solve the semantic video understanding issue (and particularly understanding of human individuals). The problems one faces are mainly reflected by variability of human appearance (expression, orientation, scale, etc.) and photometric conditions within the video (lighting conditions, video quality, duration and others). Unlike many problems in image processing and pattern recognition for which an exact solution can be given, there are several possible ways of solving face detection and recognition problem associated with the given semantic video understanding issue. The difficulty lies with the optimal choice of a general architecture and basic principles of such a system. For some of these implementation issues we can prescribe exact analytical solutions; for other issue, mainly concerning the architecture fundamentals, we can only provide some experience gained from working with the problem of interest from the scientific society. Considering the polar concepts of a human-wise data comprehension and a computer-wise, which is fast but very simple in nature we have to benefit from both approaches to meet real-world demands. According to the hypothesis presented above, our approach consists of choosing computationally-wise core with simple instructions, which are adapted by human knowledge in order to empower the entire solution, thus solve the issue. Contributions In order to illustrate all contributions made in this work we first present our general concept of face detection and recognition as a part of the semantic video understanding problem, then we separately discuss face annotating, detection, matching and recognition problems and our particular solutions to them. Face detection and recognition as a part of the semantic video understanding problem: in this part of the thesis we point out that each particular classification/analysis solution for any semantic object has theoretical limits on its learning capacity. However each of these solutions can be trained to accurately and reliably solve a partial part of the entire issue. Therefore we conclude that the face detection and recognition must be decomposed into a series of connected sub-problems that could be efficiently solved. Thus, this architecture relies on human knowledge on the subject of interest in order to decompose and connect separate solutions. The same time it is grounded on fast, accurate and reliable image processing and pattern recognition methods <b>for</b> <b>resolving</b> <b>of</b> particular sub-issues. Face annotating: this section presents a method for semi-automatic ground truth segmentation for benchmarking of face detection and recognition in video. We aim to illustrate the solution to the issue where an image processing and pattern recognition expert is able to segment and annotate facial patterns in video sequences at the rate of 7500 frames per hour. Evaluation criteria are discussed within different aspects of manual face segmentation. We extend these ideas to the semi-automatic face segmentation methodology, where all facial patterns are categorized into 4 classes in order to increase flexibility of evaluation results analysis. We present a strict guide how to speedup manual segmentation process up to 30 times and illustrate it with the sample test video sequences that consists of more than 90000 frames, 800 individuals and 50000 facial images. Experimental evaluation of the face detection using the ground truth data, that was semi-automatically segmented, demonstrates effectiveness of current approach both for learning and test stages. Face detection: given the form of face detection and recognition in semantic video understanding, there are basic problems of interest that have been solved in this thesis. The first problem is the integration problem, namely given a set of methods for face detection, how do we combine their results together in the most efficient way. Our solution utilizes five independent face localizers in order to select potential regions of interest; next these regions are classified using data-mining, which produces an optimal detection strategy. Following this strategy we search potential facial regions by a neural network classifier in order to localize facial position with minimum computational expenses, preserving high accuracy and reliability. The second problem is the one in which we attempt to construct face/non face classifier with high accuracy (> 0. 7) and extremely high reliability (10 E- 8 <). The presented solution is based on series of cascades of three different kinds of classifier. The first set of cascade is the modified version of the state-of-the art classifier that uses integral facial features. It is followed by a set of neural network cascades on integral facial features. The last type of classifiers is intensity-based neural networks. Although this solution is computationally more expensive comparing to the reference works it provides higher reliability that is the most significant factor for the issue of interest. Finally we consider the problem of adapting face detection to current video stream conditions, so that it provides better results comparing to a fixed solution. The application of interest is media streaming on mobile phones. For selected cases, for example videoconferences, we were able to achieve frame rate face detection on a standard smartphone. Face Matching: the issue of face detection and recognition problem has leads to many schools of though. Our idea is to let some face with known parameters correspond roughly to a probe face that is found by a face detector and must be recognized by a face recognizer. Thus the recognition process will benefit an advantage of a priory available probe characteristics such as lighting conditions, orientation, expression, etc. Furthermore we restrict each detected facial image to have a corresponding best match; this implies that the models used for recognition will work best when they are properly adjusted. The face matching requires a large gallery of training facial images to correctly approximate practically infinite variability of probe images. The concept of a compact facial model, a facial image that is encoded and can be decoded by a 3 d face modeling engine when needed, and fast search methods were used in this work in order to solve the issue. The very first important issue that rises face matching is the accuracy of a probe image. This entirely depends on the face detector. The way in which the face matching was solved refers to a 60 bit encoded compact facial model and ability to correctly match probe facial image with varying in-plane/out-plane orientations, head mesh transforms, emotions, texture details, lighting conditions, scale and translations. Face Recognition: this process places constraints on face detection and matching parts of the entire solution so that the probe facial image located by the detector has to be described by the compact facial model in the face matching unit. Genetic algorithms are used for matching on the learning stage because of higher accuracy comparing to the fast search methods used in the previous case (the drawback is significantly higher computational complexity). The probe facial image is reconstructed into a normalized texture by 3 d face modeling engine that is installed with compact facial model parameters. Then the facial feature matching finds the most suitable templates for eyes, eyebrows, nose and mouse by using fast matching methods. Such a facial image can again be represented by a set of facial feature templates and their locations. This information is used for preliminary classification, where only top-N individuals are selected for further recognition. Finally a set of neural network cascades based on the encoded facial description (that is independently trained for each individual) is used for choosing the appropriate response. This again serves to make the recognition task more accurate and reliable, thus leads to higher performance of the system. The results of this work were applied into the multimedia indexing system CINDI in a framework of RNRT Cyrano project on personal media distribution and management. Fast search methods were used in biomedical images indexing project B- 705. A series of face detection and tracking methods was integrated into a commercial video assets management system...|$|E
5000|$|O. Scherzer, [...] "Limitations <b>for</b> the <b>resolving</b> power <b>of</b> {{electron}} microscopes", Proceedings ICEM-9 Volume 3, 123-9 (1978) {{as cited}} in Peter Hawkes - The Long Road to Spherical Aberration Correction.|$|R
40|$|Serialization, the {{traditional}} method <b>of</b> <b>resolving</b> concurrent interactions, is often inappropriate; when interactions {{are dependent on}} each other, other policies <b>for</b> <b>resolving</b> them may be more suitable. We use semantic information to help categorize common interactions encountered in the modeling and simulation domain. This categorization enables us to suggest reasonable policies <b>for</b> <b>resolving</b> the effects <b>of</b> concurrent interactions. 1. Introduction One {{of the most significant}} challenges facing the simulation community is Multi-Representation Modeling (MRM) [...] - the joint execution of multiple models of the same object or process [8]. The crux of the challenge is resolving concurrent interactions on the representations in the different models [17]. Many systems either serialize concurrent interactions or avoid them by restricting the interactions that can co-occur. However, serialization and avoidance are insufficient <b>for</b> <b>resolving</b> the effects <b>of</b> concurrent interactions in the general c [...] ...|$|R
40|$|The general regularities {{detecting}} in {{the psychological}} development of person is modern theoretical problem of psychology. So the computer-aided informational system was made as the psychological item <b>for</b> the <b>resolving</b> <b>of</b> this problem. The system stores empirical data, makes processing and generalization of the results, creates the new psychological tests and compares {{the results in}} Kamchatka region with other regions’ results by the identical parameter...|$|R
40|$|As agents {{begin to}} perform complex tasks {{alongside}} humans as collaborative teammates, it becomes crucial that the resulting humanmultiagent teams adapt to time-critical domains. In such domains, adjustable autonomy has proven useful by {{allowing for a}} dynamic transfer of control of decision making between human and agents. However, existing adjustable autonomy algorithms commonly discretize time, which not only results in high algorithm runtimes but also translates into inaccurate transfer of control policies. In addition, existing techniques fail to address decision making inconsistencies often encountered in human multiagent decision making. To address these limitations, we present novel approach <b>for</b> <b>Resolving</b> Inconsistencies in Adjustable Autonomy in Continuous Time (RIAACT) that makes three contributions: First, we apply continuous time planning paradigm to adjustable autonomy, resulting in high-accuracy transfer of control policies. Second, our new adjustable autonomy framework both models and plans <b>for</b> the <b>resolving</b> <b>of</b> inconsistencies between human and agent decisions. Third, we introduce a new model, Interruptible Action Time-dependent Markov Decision Problem (IA-TMDP), which allows for actions to be interrupted {{at any point in}} continuous time. We show how to solve IA-TMDPs efficiently and leverage them to plan <b>for</b> the <b>resolving</b> <b>of</b> inconsistencies in RIAACT. Furthermore, these contributions have been realized and evaluated in a complex disaster response simulation system. 1...|$|R
3000|$|... in {{collaboration}} with ONERA, we devised a concept for integrating high-sensitivity inertial sensors to gain sufficient control of the spacecraft attitude and to monitor non-inertial movements of the spacecraft. This is essential <b>for</b> <b>resolving</b> the position <b>of</b> the test particle with sufficient accuracy.|$|R
40|$|In this paper, we {{introduce}} a method <b>for</b> <b>resolving</b> singularities <b>of</b> Galois closure covers for 5 -fold covers between smooth surfaces. Applying this method, we determine types of singular fibers {{of a family}} of Galois closure curves for plane sextic curves. Consequently,we obtain an explicit construction of smooth projective minimal surfaces of general type with positive indices obtained as families of Galois closure curves of smooth plane sextic curves...|$|R
40|$|This report {{describes}} techniques <b>for</b> <b>resolving</b> systems <b>of</b> polynomial equations and inequalities. The general {{technique used}} is cylindrical algebraic decomposition, which decomposes space {{into a number}} of regions, on each of which the equations and inequalities have the same sign. Most of the report is spent describing the algebraic and algorithmic pre-requisites (resultants, algebraic numbers, Sturm sequences etc.), and then describing the method, rst in two dimensions and then in an arbitrary number of dimensions. ...|$|R
40|$|Regulation {{procedures}} and water management that incorporate projected hydrological changes with related uncertainties become extremely important {{in order to}} prevent degradation of water ecosystems. Ensuring real time water management and optimization becomes mandatory <b>for</b> <b>resolving</b> the constraints <b>of</b> water supply/demand and to comply with biodiversity requirements...|$|R
40|$|Here {{we present}} {{collective}} Thomson scattering measurements of 1 D fast-ion velocity distribution functions in neutral beam heated TEXTOR plasmas with sawtooth oscillations. Up to 50 % {{of the fast}} ions in the centre are redistributed {{as a consequence of}} a sawtooth crash. We resolve various directions to the magnetic field. The fast-ion distribution is found to be anisotropic as expected. <b>For</b> a <b>resolved</b> angle <b>of</b> 39 ¿ to the magnetic field we find a drop in the fast-ion distribution of 20 – 40 %. <b>For</b> a <b>resolved</b> angle <b>of</b> 83 ¿ to the magnetic field the drop is no larger than 20 %. (Some figures in this article are in colour only in the electronic version...|$|R
40|$|Permitting {{modification}} of all mortgages in bankruptcy {{would create a}} low-cost, effective, fair, and immediately available method <b>for</b> <b>resolving</b> much <b>of</b> the current foreclosure crisis without imposing costs on taxpayers, creating a moral hazard for borrowers or lenders, or increasing mortgage credit costs or decreasing mortgage credit availability. As the foreclosure crisis deepens, bankruptcy modification presents the best and least invasive method of stabilizing the housing market and is a crucial step in stabilizing financial markets...|$|R
