1074|92|Public
25|$|MIPS has 32 floating-point {{registers}}. Two registers are paired for <b>double</b> <b>precision</b> numbers. Odd numbered registers {{cannot be}} used for arithmetic or branching, just {{as part of a}} <b>double</b> <b>precision</b> register pair, resulting in 16 usable registers for most instructions (moves/copies and loads/stores were not affected).|$|E
25|$|Any integer with {{absolute}} value less than 224 can be exactly {{represented in the}} single precision format, and any integer {{with absolute}} value less than 253 can be exactly represented in the <b>double</b> <b>precision</b> format. Furthermore, {{a wide range of}} powers of 2 times such a number can be represented. These properties are sometimes used for purely integer data, to get 53-bit integers on platforms that have <b>double</b> <b>precision</b> floats but only 32-bit integers.|$|E
25|$|D {{programming}} language implements real using largest floating point size implemented in hardware, 80 bits for x86 CPUs or <b>double</b> <b>precision,</b> whichever is larger.|$|E
40|$|This manual {{describes}} {{the library of}} conjugate gradients codes CCGPAK, which solves system of complex linear system of equations. The library is written in FORTRAN 90 and is highly portable. The codes are general and provide mechanism for matrix times vector multiplication which is separated from the conjugate gradient iterations itself. It is simple to switch between single and <b>double</b> <b>precisions.</b> All codes follow the same naming conventions...|$|R
50|$|Prior to JVM 1.2, floating-point {{calculations}} were strict; that is, all intermediate floating-point {{results were}} represented as IEEE single or <b>double</b> <b>precisions</b> only. As a consequence, errors of calculation (round-off errors), overflows and underflows, would occur with greater frequency than in architectures which did intermediate calculations in greater precision. The arithmetic issues were {{a real problem}} on early Java VMs, and many other solutions besides {{the use of this}} instruction were proposed.|$|R
5000|$|As {{a rule of}} thumb, {{iterative}} refinement for Gaussian elimination {{produces a}} solution correct to working <b>precision</b> if <b>double</b> the working <b>precision</b> {{is used in the}} computation of , e.g. by using quad or <b>double</b> extended <b>precision</b> IEEE 754 floating point, and if [...] is not too ill-conditioned (and the iteration and the rate of convergence are determined by the condition number of [...] ).|$|R
25|$|A 64-bit significand {{provides}} sufficient precision {{to avoid}} loss of precision when {{the results are}} converted back to <b>double</b> <b>precision</b> format in the vast number of cases.|$|E
25|$|IBM's {{supercomputer}}, IBM Roadrunner, is {{a hybrid}} of General Purpose CISC Opteron as well as Cell processors. This system assumed the #1 spot on the June 2008 Top 500 list as the first supercomputer to run at petaFLOPS speeds, having gained a sustained 1.026 petaFLOPS speed using the standard Linpack benchmark. IBM Roadrunner uses the PowerXCell 8i version of the Cell processor, manufactured using 65nm technology and enhanced SPUs that can handle <b>double</b> <b>precision</b> calculations in the 128-bit registers, reaching <b>double</b> <b>precision</b> 102GFLOPs per chip.|$|E
25|$|The {{choice of}} STEP {{depends on the}} {{threshold}} of overflow. For <b>double</b> <b>precision</b> floating point format, the threshold is near e700, so 500 shall be a safe STEP.|$|E
25|$|Both {{shifting}} and <b>doubling</b> the <b>precision</b> {{are important for}} some multiplication algorithms. Note that unlike addition and subtraction, width extension and right shifting are done differently for signed and unsigned numbers.|$|R
5000|$|ArcCos (Calculates inverse cosinus. Overloaded {{versions}} for Single, <b>Double</b> and Extended <b>precision.)</b> ...|$|R
50|$|The 4A Engine {{implementation}} of Metro 2033 features volumetric fog, <b>double</b> PhysX <b>precision,</b> object blur, sub-surface scattering for skin shaders, parallax mapping on all surfaces and greater geometric detail {{with a less}} aggressive LOD(s).|$|R
25|$|In conclusion, {{the exact}} number of bits of {{precision}} needed in the significand of the intermediate result is somewhat data dependent but 64 bits is sufficient to avoid precision loss {{in the vast majority of}} exponentiation computations involving <b>double</b> <b>precision</b> numbers.|$|E
25|$|The brand became AMD FireStream {{with the}} second {{generation}} of stream processors in 2007, based on the RV650 chip with new unified shaders and <b>double</b> <b>precision</b> support. Asynchronous DMA also improved performance by allowing a larger memory pool without the CPU's help. One model was released, the 9170, for the initial price of $1999. Plans included {{the development of a}} stream processor on an MXM module by 2008, for laptop computing, but was never released.|$|E
25|$|It was {{possible}} to wire the carry of one accumulator into another accumulator to perform <b>double</b> <b>precision</b> arithmetic, but the accumulator carry circuit timing prevented the wiring of 3+ for even higher precision. ENIAC used 4 of the accumulators (controlled by a special multiplier unit) to perform up to 385 multiplication operations/second; 5 of the accumulators were controlled by a special divider/square-rooter unit to perform up to 40 division operations/second or 3 square root operations/second.|$|E
5000|$|... 65-point auto-focus system, all cross-type. Center {{point is}} high <b>precision,</b> <b>double</b> cross-type with −3 EV {{sensitivity}} ...|$|R
5000|$|... 19 point auto-focus system, all cross-type. Center {{point is}} high <b>precision,</b> <b>double</b> cross-type at f/2.8 or faster ...|$|R
5000|$|... 9 points AF system, all cross-type at f/5.6. Center {{point is}} high <b>precision,</b> <b>double</b> cross-type at f/2.8 ...|$|R
25|$|The arithmetical {{difference}} between two consecutive representable floating-point numbers {{which have the}} same exponent is called a unit in the last place (ULP). For example, {{if there is no}} representable number lying between the representable numbers 1.45a70c22hex and 1.45a70c24hex, the ULP is 2×16−8, or 2−31. For numbers with a base-2 exponent part of 0, i.e. numbers with an absolute value higher than or equal to 1 but lower than 2, an ULP is exactly 2−23 or about 10−7 in single precision, and exactly 2−53 or about 10−16 in <b>double</b> <b>precision.</b> The mandated behavior of IEEE-compliant hardware is that the result be within one-half of a ULP.|$|E
25|$|DV (divide): Divide the {{contents}} of register A by the data at the referenced memory address. Store the quotient in register A and the absolute value of the remainder in register Q. Unlike modern machines, fixed-point numbers were treated as fractions (notional decimal point just to right of the sign bit), so you could produce garbage if the divisor was not larger than the dividend; there was no protection against that situation. In the Block II AGC, a double-precision dividend started in A and L (the Block II LP), and the correctly signed remainder was delivered in L. That considerably simplified the subroutine for <b>double</b> <b>precision</b> division.|$|E
25|$|TS (transfer to storage): Store {{register}} A at {{the specified}} memory address. TS also detects, and corrects for, overflows {{in such a}} way as to propagate a carry for multi-precision add/subtract. If the result has no overflow (leftmost 2 bits of A the same), nothing special happens; if there is overflow (those 2 bits differ), the leftmost one goes the memory as the sign bit, register A is changed to +1 or −1 accordingly, and control skips to the second instruction following the TS. Whenever overflow is a possible but abnormal event, the TS was followed by a TC to the no-overflow logic; when it is a normal possibility (as in multi-precision add/subtract), the TS is followed by CAF ZERO (CAF = XCH to fixed memory) to complete the formation of the carry (+1, 0, or −1) into the next higher-precision word. Angles were kept in single precision, distances and velocities in <b>double</b> <b>precision,</b> and elapsed time in triple precision.|$|E
5000|$|... 19 AF points, all cross-type at f/5.6. Center {{point is}} high <b>precision,</b> <b>double</b> cross-type at f/2.8 or faster ...|$|R
40|$|This paper {{presents}} the first {{deployment of the}} Fast Multipole Method on the Cell processor (PowerXCell 8 i). We rely on the matrix formulation with BLAS routines of the FMB code (Fast Multipole with BLAS) in order to directly and efficiently offload the most time consuming operators of both far field and near field computations on the Cell heterogeneous cores. We detail the difficulties {{that had to be}} solved first, and we finally obtain a deployment in single and <b>double</b> <b>precisions,</b> which scales linearly on several Cell blades and which is able to handle both uniform and non-uniform distributions of particles. We also present our performance results and comparisons with multicore CPUs, as well as the limitations of our deployment on the Cell processor...|$|R
5000|$|... 19-point AF System, all cross-type at f/5.6. Center {{point is}} high <b>precision,</b> <b>double</b> cross-type at f/2.8 or faster. (9 points, all cross-type on 60D) ...|$|R
25|$|The AGC {{also had}} a {{sophisticated}} software interpreter, developed by the MIT Instrumentation Laboratory, that implemented a virtual machine with more complex and capable pseudo-instructions than the native AGC. These instructions simplified the navigational programs. Interpreted code, which featured <b>double</b> <b>precision</b> trigonometric, scalar and vector arithmetic (16 and 24-bit), even an MXV (matrix × vector) instruction, could be mixed with native AGC code. While the execution time of the pseudo-instructions was increased (due {{to the need to}} interpret these instructions at runtime) the interpreter provided many more instructions than AGC natively supported and the memory requirements were much lower than in the case of adding these instructions to the AGC native language which would require additional memory built into the computer (at that time the memory capacity was very expensive). The average pseudo-instruction required about 24 ms to execute. The assembler and version control system, named YUL for an early prototype Christmas Computer, enforced proper transitions between native and interpreted code.|$|E
2500|$|The {{number of}} bits needed for the {{exponent}} of the extended precision format follows from the requirement that the product of two <b>double</b> <b>precision</b> numbers should not overflow when computed using the extended format. [...] The largest possible exponent of a <b>double</b> <b>precision</b> value is 1023 so the exponent of the largest possible product of two <b>double</b> <b>precision</b> numbers is 2047 (an 11-bit value). [...] Adding in a bias to account for negative exponents means that the exponent field {{must be at least}} 12 bits wide.|$|E
2500|$|... {{then the}} {{algorithm}} becomes numerically stable and can compute to full <b>double</b> <b>precision.</b>|$|E
50|$|To extract all {{the complex}} {{solutions}} from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, <b>doubling</b> the <b>precision</b> each time, until solutions remain stable, as {{the substitution of}} the roots in the equations of the input variables can be highly unstable.|$|R
30|$|For {{the thermal}} error {{detecting}} of {{the cutting tool}} tip, three categories of sensors are mainly used, that are non-contact displacement detection sensors, high <b>precision</b> <b>double</b> ball gauge, and laser interferometer. The non-contact displacement detection sensors utilized in machine tools include eddy current transducers, capacitive transducers and laser displacement sensors. Though their sensing principles are different, their installation and error detection method are consistent with each other. The high <b>precision</b> <b>double</b> ball gauge and laser interferometer are mainly used to detect the dynamic geometric error of the machine tool, and {{they can also be}} competent at thermal error detecting.|$|R
5000|$|The 400 gr g bullet at 2400 ft/s is the {{industry}} performance {{standard for the}} [...]416 Remington Magnum. Very few ammunition manufacturers offer bullet weights other than the 400 gr g bullet. Currently Conley <b>Precision,</b> <b>Double</b> Tap, Federal, Hornady, Norma, Remington and Winchester produce ammunition for rifles chambered for this cartridge. Currently only Conley <b>Precision,</b> <b>Double</b> Tap and Norma produce ammunition loaded with bullet weighing something other than 400 gr g. However, the ammunition produced by these companies are generally available only through mail order (Conley Precision) or not commonly found throughout North America (Norma and Double Tap).|$|R
2500|$|While the Cell chip {{can have}} a number of {{different}} configurations, the basic configuration is a multi-core chip composed of one [...] "Power Processor Element" [...] ("PPE") (sometimes called [...] "Processing Element", or [...] "PE"), and multiple [...] "Synergistic Processing Elements" [...] ("SPE"). The PPE and SPEs are linked together by an internal high speed bus dubbed [...] "Element Interconnect Bus" [...] ("EIB"). Due to the nature of its applications, Cell is optimized towards single precision floating point computation. The SPEs are capable of performing <b>double</b> <b>precision</b> calculations, albeit with an order of magnitude performance penalty. [...] New chips expected mid-2008 are rumored to boost SPE <b>double</b> <b>precision</b> performance as high as 5x over pre-2008 designs. In the meantime, there are ways to circumvent this in software using iterative refinement, which means values are calculated in <b>double</b> <b>precision</b> only when necessary. Jack Dongarra and his team [...] a 3.2GHz Cell with 8 SPEs delivering a performance equal to 100GFLOPS on an average <b>double</b> <b>precision</b> Linpack 4096x4096 matrix.|$|E
2500|$|Taking the log of this {{representation}} of a <b>double</b> <b>precision</b> number and simplifying results in the following: ...|$|E
2500|$|FORTRAN (at {{least since}} FORTRAN IV as of 1961) also uses [...] "D" [...] to signify <b>double</b> <b>precision</b> numbers.|$|E
5000|$|The series A is convergent, {{and may be}} {{truncated}} {{to obtain}} an approximation with the desired precision. By choosing an appropriate g (typically a small integer), only some 5-10 terms of the series are needed to compute the Gamma function with typical single or <b>double</b> floating-point <b>precision.</b> If a fixed g is chosen, the coefficients can be calculated in advance and the sum is recast into the following form: ...|$|R
40|$|Artificial Intelligence Lab, Department of MIS, University of ArizonaResearch {{has shown}} that most usersâ online {{information}} searches are suboptimal. Query optimization based on a relevance feedback or genetic algorithm using dynamic query contexts can help casual users search the Internet. These algorithms can draw on implicit user feedback based on the surrounding links and text in a search engine result set to expand user queries with a variable number of keywords in two manners. Positive expansion adds terms to a userâ s keywords with a Boolean â and,â negative expansion adds terms to the userâ s keywords with a Boolean â not. â Each algorithm was examined for three user groups, high, middle, and low achievers, who were classified according to their overall performance. The interactions of users with different levels of expertise with different expansion types or algorithms were evaluated. The genetic algorithm with negative expansion tripled recall and <b>doubled</b> <b>precision</b> for low achievers, but high achievers displayed an opposed trend {{and seemed to be}} hindered in this condition. The effect of other conditions was less substantial...|$|R
50|$|Small {{family farms}} still used towed behind harvesters, these are either single chop, <b>double</b> chop or <b>precision</b> chop. Older {{machines}} were operated by cables, {{then they were}} operated by hydraulics and the newer types are operated by electronics.|$|R
