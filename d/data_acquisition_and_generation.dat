13|10000|Public
40|$|We have immense {{computing}} power {{available at the}} national supercomputer centers and local clusters of fast PCs. We also have had a proliferation of <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> through the deployment of sophisticated new generations of sensors. The lack of coordination between current computational capacity and sensor technology impairs our ability to effectively utilize the continuous flood of information. This is...|$|E
40|$|In the Big Data era, {{workflow}} systems must embrace data {{parallel computing}} techniques for efficient data analysis and analytics. Here, an easy-to-use, scalable approach {{is presented to}} build and execute Big Data applications using actor-oriented modeling in data parallel computing. Two bioinformatics use cases for next-generation sequencing data analysis demonstrate the approach’s feasibility. A s the Internet of Things 1 and other <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> technologies advance, the amount of data being gener-ated is growing at an exponential rate at all scales in many online and scientific platforms. This mostly unstructured and variable data {{is often referred to}} as Big Data. The amount of potentially valuable information buried in Big Data is of inter-est to many data science applications, ranging from the natural sciences to marketing research. To ana...|$|E
40|$|With {{the advance}} of new <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> technologies, the {{biomedical}} domain is becoming increasingly data-driven. Thus, understanding the information in large and complex data sets {{has been in the}} focus of several research fields such as statistics, data mining, machine learning, and visualization. While the first three fields predominantly rely on computational power, visualization relies mainly on human perceptual and cognitive capabilities for extracting information. Data visualization, similar to Human–Computer Interaction, attempts an appropriate interaction between human and data to interactively exploit data sets. Specifically within the analysis of complex data sets, visualization researchers have integrated computational methods to enhance the interactive processes. In this state-of-the-art report, we investigate how such an integration is carried out. We study the related literature with respect to the underlying analytical tasks and methods of integration. In addition, we focus on how such methods are applied to the biomedical domain and present a concise overview within our taxonomy. Finally, we discuss some open problems and future challenges...|$|E
30|$|Detector-based {{spectral}} CT (SDCT; Philips Healthcare, Cleveland, OH, USA) {{offers a}} novel approach to dual-energy imaging where the spectral separation occurs {{at the level of}} the detectors. In this article, we describe <b>data</b> <b>acquisition</b> <b>and</b> image <b>generation</b> with SDCT <b>and</b> the clinical applications of this technology.|$|R
40|$|A simple {{database}} management system has been developed {{for use by the}} Cardiac Catheterization Laboratory at University of Iowa Hospitals and Clinics. The system was developed with limited resources in a period of eight months. Major functions provided by the DBMS are <b>data</b> <b>acquisition,</b> report <b>generation</b> <b>and</b> selective retrieval by diagnosis. The modular design of the system provides for growth in the database without modification of existing programs. The project was divided into four stages: data specification, design, development and implementation...|$|R
40|$|Volumetric data {{rendering}} is CPU- and memory-intensive. Technological {{innovations in}} <b>data</b> <b>acquisition</b> <b>and</b> <b>data</b> <b>generation</b> will probably {{continue to grow}} at a rate such that the data produced are orders of magnitude too big to be handled/analyzed/rendered on an affordable desktop computer. Addressing these demands, the San Diego Supercomputer Center Networked Volume Renderer (SDSC_NetV) efficiently manages and gives access to several direct volume rendering algorithms running on advanced computing rendering engines. SDSC_NetV's most innovative aspects are the easy-to-use graphical interface (UIM), which hides all notion of a network, and the server manager (SMM), which assigns advanced computing servers based on the user's rendering request and {{on the set of}} currently available resources. The SMM, acting as a render broker for big rendering jobs, works in unison with the UIM to give the user several of the most effective features from turnkey, application builder, and advanced archite [...] ...|$|R
40|$|This article {{deals with}} the {{construction}} of a vehicle driven by electric motors and that is automated, that is, that can move anywhere without human intervention. The control was done using the software Labview, with <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> of control signs. The vehicle has an infrared sensors system that indicates the existence of an obstacle ahead the vehicle, informing it that it should stop and bypass the obstacle. The program is the responsible for the engine control, making it possible for the prototype to run and bypass the objects that block its way. The possibility of remote-controlling a vehicle is very important is risky situations for human beings, for example in radioactive places. The main advantage of this system is the total flexibility for making alterations in the control software, without being necessary to touch the physical part of the prototype. The conclusion of this work is that the system is efficient and able to move in a room with objects without touching them...|$|E
40|$|Digitization of {{specimens}} {{is becoming}} an ever more important part of palaeontology, both for archival and research purposes. The advent of mainstream hardware containing depth sensors and RGB cameras, used primarily for interacting with video games, in conjunction with an open platform used by developers, {{has led to an}} abundance of highly affordable technology with which to digitize specimens. Here, the Microsoft® Kinect™ is used to digitize specimens of varying sizes in order to demonstrate the potential applications of the technology to palaeontologists. The resulting digital models are compared with models produced using photogrammetry. Although the Kinect™ generally records morphology at a lower resolution, and thus captures less detail than photogrammetric techniques, it offers advantages in speed of <b>data</b> <b>acquisition,</b> <b>and</b> <b>generation</b> of a completed mesh in real time at the point of data collection. Whilst it is therefore limited in archival applications, the ease of use and low cost, driven by strong market competition, make this technology an enticing alternative for studies where rapid digitization of general morphology is desired...|$|E
40|$|The US Centers for Disease Control and Prevention define syndromic {{surveillance}} as “an investigational approach where {{health department}} staff, assisted by automated <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> of statistical alerts, monitor disease indicators in real-time or near real-time to detect outbreaks of disease earlier than {{would otherwise be}} possible with traditional public health methods. ”[1] Syndromic surveillance initially came into prominence as a bioterrorist surveillance methodology primarily following the events of September 11 th, where a rapidly evolving emergency situation required regular and timely access to epidemiologic information to foresee and plan for the allocation and use of limited and stressed resources. It has subsequently evolved into a sub-discipline of epidemiologic surveillance beyond the exclusive scope of bioterrorism preparedness[2 - 4] to include, among others, pandemic preparedness[5, 6], West Nile surveillance[7, 8], and as a potential tool to enhance routine surveillance systems commonly used {{within the field of}} public health[9]. A significant benefit of syndromic surveillance systems is that they characteristically rely on the use of pre-existing datasets, thereby foregoing the challenges associated with th...|$|E
40|$|Over {{the last}} couple of years the usage of small UAV (unmanned aerial vehicles) in various fields of {{civilian}} use has grown extensively. Photogrammetric image acquisition with small unmanned aerial vehicles allows us to acquire high spatial resolution images of small areas, which can be used for the production of photogrammetric point clouds and orthophotos. In this graduation thesis we have examined the usefulness of a UAV system for <b>data</b> <b>acquisition</b> <b>and</b> the <b>generation</b> of a geodetic plan of a small urban area. We were focused to find out which details for the geodetic plan can be obtained from the data acquired by UAV, as well as to investigate the strengths, the limits and potential problems of the used method. The detail was acquired from the photogrammetric point cloud with the RiSCAN PRO software in two steps. In the first step, the detail was acquired only from the photogrammetric point cloud, in the second step, the orthophoto has been additionally used for interpretation of the contents. The comparison between the detail obtained from the photogrammetric point cloud and the data acquired by terrestrial laser scanning was done as well. After all these steps the geodetic plan was accomplished. Final observations and the evaluation of the usefulness of close-range aerial images for the generation of the geodetic plan are given in the conclusion...|$|R
40|$|When formulating {{explanations}} for the events we witness in the world, temporal dynamics govern the hypotheses we generate. In our view, temporal dynamics influence beliefs over three stages: <b>data</b> <b>acquisition,</b> hypothesis <b>generation,</b> <b>and</b> hypothesis maintenance and updating. This paper presents experimental and computational evidence for the influence of temporal dynamics on hypothesis generation through dynamic working memory processes during <b>data</b> <b>acquisition</b> in a medical diagnosis task. We show that increasing the presentation rate of a sequence of symptoms leads to a primacy effect, which is predicted by the dynamic competition in working memory that dictates the weights allocated to individual data in the generation process. Individual differences observed in hypothesis generation are explained by differences in working memory capacity. Finally, in a simulation experiment we show that activation dynamics at <b>data</b> <b>acquisition</b> also accounts for results from a related task previously held to support capacity-unlimited diagnostic reasoning...|$|R
40|$|Abstract Large-scale {{simulations}} of blood flow {{allow for the}} optimal evaluation of endothelial shear stress for real-life case studies in cardiovascular pathologies. The procedure for anatomic <b>data</b> <b>acquisition,</b> geometry, <b>and</b> mesh <b>generation</b> are particularly favorable if {{used in conjunction with}} the Lattice Boltzmann method and the underlying Cartesian mesh. The methodology allows to accommodate red blood cells in order {{to take into account the}} corpuscular nature of blood in multiscale scenarios and its complex rheological response, in particular, in proxim-ity of the endothelium. Taken together, the Lattice Boltzmann framework has become a reality for studying sections of the human circulatory system in physio-logical conditions. Blood flow simulations constitute a rapidly growing field for the medical, engineer-ing, and basic sciences communities. The study of blood in the macrovasculature, as much as in capillaries, has deep implications in understanding and prevention of the most common cardiovascular pathologies, with atherosclerosis being perhap...|$|R
40|$|The {{objective}} of this thesis is design of the universal USB measuring module for virtual instrumentation. The module is connected to PC via USB interface. The module have analog inputs and analog outputs. The inputs have some switchable ranges, adjustable high-pass and low-pass fiters. Also the module have the digital input-output unit with 8 channels with configurable data direction and voltage level. Physical layer for digital serial interfaces SPI/UART and I 2 C {{is also part of}} this unit. The module is able to communicate with LabVIEW enviroment. The teoretical part contains survey of the commercial USB modules for <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> from various manufacturers with its basic parameters. Some selected modules are described in detail. Furthermore the thesis deals with possibilities of connecting the USB module with the LabVIEW enviroment. The practical part deals with the design of input-output circuits and with design of the module itself. Furthemore, tho communication protocol, utility library and demonstration LabVIEW software are described in this part. At the end of this part, there are results of verification measurements. The result of this thesis is completed universal USB measuring module with utility library and demonstration program...|$|E
40|$|With the {{increase}} in mobile broadband services the operators are gaining profits by providing high speed Internet access over the mobile network. On the other side they are also facing challenges to give QoS guarantee to the customers. In this thesis we investigate the impact of data rate and payload size on One Way Delay (OWD) and packet loss over TCP performance in 3 G networks. Our goal is to evaluate the OWD and packet loss characteristics with respect to payload size and data rate from the collected network level traces. To collect these traces an experimental testbed is setup with Endace <b>Data</b> <b>Acquisition</b> <b>and</b> <b>Generation</b> (DAG) cards, for accurate measurements Endace DAG cards together with Global Positioning System (GPS) synchronization is implemented. The experiments are conducted for three different Swedish mobile operator networks and further the statistics of OWD measurements and packet loss for different data rates and payload sizes are evaluated. Our {{results indicate that the}} minimal OWD occurred at higher data rates and also shows a high delay variability. The packet loss has much impact on higher data rates and larger payload sizes, as the packet loss increases with {{the increase}} in data rate and payload size...|$|E
40|$|Abstract—Opaque traffic, i. e., {{traffic that}} is {{compressed}} or encrypted, incurs particularly high overhead for deep packet inspection engines and often yields {{little or no}} useful information. Our experiments indicate that an astonishing 89 % of payload-carrying TCP packets — and 86 % of bytes transmitted — are opaque, forcing us to consider the challenges this class of traffic presents for network security, both in the short-term and, as the proportion of opaque traffic continues to rise, for the future. We provide {{a first step toward}} addressing some of these challenges by introducing new techniques for accurate real-time winnowing, or filtering, of such traffic based on the intuition that the distribution of byte values found in opaque traffic will differ greatly from that found in transparent traffic. Evaluation on traffic from two campuses reveals that our techniques are able to identify opaque data with 95 % accuracy, on average, while examining less than 16 bytes of payload data. We implemented our most promising technique as a preprocessor for the Snort IDS and compared the performance to a stock Snort instance by running both instances live, on identical traffic streams, using a <b>Data</b> <b>Acquisition</b> <b>and</b> <b>Generation</b> (DAG) card deployed within a campus network. Winnowing enabled Snort to handle a peak load of 1. 2 Gbps, with zero percent packet loss, and process almost one hundred billion packets over 24 hours — a 147 % increase over the number processed by the stock Snort instance. This increase in capacity resulted in 33, 000 additional alerts which would otherwise have been missed. I...|$|E
40|$|The Landsat Program has {{continuously}} provided data {{to a broad}} user community since 1972. With {{the successful}} launch of Landsat 7 on April 15, 1999 the availability of Landsat data will be continued and essentially extended by its Enhanced Thematic Mapper Plus (ETM+) instrument. The German Remote Sensing Data Center (DFD) will operate a Landsat 7 International Ground Station in Europe {{as part of the}} Earthnet Programme of the European Space Agency ESA and at other locations world-wide. The paper gives an overview on the present status of the Landsat 7 ground segment at the DFD. The main mission and sensor characteristics are briefly explained. The role of the DFD as Landsat 7 ground station including <b>data</b> <b>acquisition,</b> product <b>generation</b> <b>and</b> distribution is outlined and the DFD's data utilization concept is described. The Scientific Data Pool as central part of the DFD strategy to support research and pilot applications is explained. 1 INTRODUCTION Dating back to the launch of Landsat [...] ...|$|R
40|$|We present g-PRIME, a {{software}} based tool for physiology <b>data</b> <b>acquisition,</b> analysis, <b>and</b> stimulus <b>generation</b> in education <b>and</b> research. This software {{was developed in}} an undergraduate neurophysiology course and strongly influenced by instructor and student feedback. g-PRIME is a free, stand-alone, windows application coded and “compiled ” in Matlab (does not require a Matlab license). g-PRIME supports many <b>data</b> <b>acquisition</b> interfaces from the PC sound card to expensive high throughput calibrated equipment. The program is designed as {{a software}} oscilloscope with standard trigger modes, multi-channel visualization controls, and data logging features. Extensive analysis options allow real time and offline filtering of signals, multi-parameter threshold-and-window based event detection, and two-dimensional display {{of a variety of}} parameters including event time, energy density, maximum FFT frequency component, max/min amplitudes, and inter-event rate and intervals. The software also correlates detected events with another simultaneously acquired source (event triggered average) in real time or offline. g-PRIME supports parameter histogram production and a variety of elegant publication quality graphics outputs. A major goal of this software is to merge powerful engineering <b>acquisition</b> <b>and</b> analysis tools with a biological approach to studies of nervous system function. Key words: physiology software, data analysis, dat...|$|R
40|$|The work {{reported}} in this thesis {{has been carried out}} within the Trigger and Data Ac- quisition (TDAQ) working group of the CERN NA 62 experiment, and focused on the development of a trigger strategy for collecting rare kaon decays. The main aim of the NA 62 experiment is the study of the ultra rare decay K + ! + in order to provide a stringent test of the Standard Model. The theoretical framework of the K + ! + decay, the present experimental status of the measurement of the Branching Ratio and the NA 62 experimental strategy are described in chapter 1. Chapter 2 provides a description of the NA 62 experimental setup. The first part of the work concerned the design, development and assessment of the common Trigger <b>and</b> <b>Data</b> <b>Acquisition</b> system for the majority of detectors in NA 62, a high-speed integrated <b>data</b> <b>acquisition</b> <b>and</b> trigger <b>generation</b> system based on digital high resolution time measurements. The architecture of the boards, {{a detailed description of the}} devel- oped firmwares and some results obtained during the NA 62 TDAQ commissioning phase are presented in chapter 3. The second part of the work focused on Monte Carlo studies of hardware low-level (L 0) trigger schemes with the purpose of selecting interesting decays. The dependence of the L 0 output trigger rate and of the trigger efficiency upon the variation of several trigger parameters has been studied. The optimization of the L 0 trigger scheme for the selection of K + ! + decays is described in chapter 4. In addition, a L 0 trigger selection for the search for a hypotheti- cal dark photon (U) in 0 decays is discussed in chapter 5. After a general introduction about the searches for rare and forbidden kaon decays at NA 62, the physics motivations and the experimental status are described. Finally, the NA 62 expected upper limits on the Branching Ratio of the 0 ! U;U ! e +...|$|R
40|$|Doctoral Thesis. Submitted in partial {{fulfillment}} of the requirements for the award of Doctor of Philosophy of Loughborough University. 3 -D computational modelling of the human spine provides a sophisticated and cost-effective medium for bioengineers, researchers, and ergonomics designers in order to study the biomechanical behaviour of the human spine under different loading conditions. Developing a generic parametric computational human spine model to be employed in biomechanical modelling introduces a considerable potential to reduce the complexity of implementing and amending the intricate spinal geometry. The main objective {{of this research is}} to develop a 3 -D parametric human spine model generation framework based on a command file system, by which the parameters of each vertebra are read from the database system, and then modelled within commercial 3 -D CAD software. A novel <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> system was developed {{as a part of the}} framework for determining the unknown vertebral dimensions, depending on the correlations between the parameters estimated from existing anthropometrical studies in the literature. The data acquisition system embodies a predictive methodology that comprehends the relations between the features of the vertebrae by employing statistical and geometrical techniques. Relations amongst vertebral parameters such as golden ratio were investigated and successfully implemented into the algorithms. The validation of the framework was carried out by comparing the developed 3 -D computational human spine models against various real life human spine data, where good agreements were achieved. The constructed versatile framework possesses the capability to be utilised as a basis for quickly and effectively developing biomechanical models of the human spine such as finite element models...|$|E
40|$|Due to the {{unprecedented}} technology development of sensors, platforms and algorithms for 3 D <b>data</b> <b>acquisition</b> <b>and</b> <b>generation,</b> 3 D spaceborne, airborne and close-range data, {{in the form}} of image based, Light Detection and Ranging (LiDAR) based point clouds, Digital Elevation Models (DEM) and 3 D city models, become more accessible than ever before. Change detection (CD) or time-series data analysis in 3 D has gained great attention due to its capability of providing volumetric dynamics to facilitate more applications and provide more accurate results. The state-of-the-art CD reviews aim to provide a comprehensive synthesis and to simplify the taxonomy of the traditional remote sensing CD techniques, which mainly sit within the boundary of 2 D image/spectrum analysis, largely ignoring the particularities of 3 D aspects of the data. The inclusion of 3 D data for change detection (termed 3 D CD), not only provides a source with different modality for analysis, but also transcends the border of traditional top-view 2 D pixel/object-based analysis to highly detailed, oblique view or voxel-based geometric analysis. This paper reviews the recent developments and applications of 3 D CD using remote sensing and close-range data, in support of both academia and industry researchers who seek for solutions in detecting and analyzing 3 D dynamics of various objects of interest. We first describe the general considerations of 3 D CD problems in different processing stages and identify CD types based on the information used, being the geometric comparison and geometric-spectral analysis. We then summarize relevant works and practices in urban, environment, ecology and civil applications, etc. Given the broad spectrum of applications and different types of 3 D data, we discuss important issues in 3 D CD methods. Finally, we present concluding remarks in algorithmic aspects of 3 D CD...|$|E
40|$|Streetscapes have {{presented}} a long-standing interest in many fields. Recently, {{there has been}} a resurgence of attention on streetscape issues, catalyzed in large part by computing. Because of computing, there is more understanding, vistas, data, and analysis of and on streetscape phenomena than ever before. This diversity of lenses trained on streetscapes permits us to address long-standing questions, such as how people use information while mobile, how interactions with people and things occur on streets, how we might safeguard crowds, how we can design services to assist pedestrians, and how we could better support special populations as they traverse cities. Amid each of these avenues of inquiry, computing is facilitating new ways of posing these questions, particularly by expanding the scope of what-if exploration that is possible. With assistance from computing, consideration of streetscapes now reaches across scales, from the neurological interactions that form among place cells in the brain up to informatics that afford real-time views of activity over whole urban spaces. For some streetscape phenomena, computing allows us to build realistic but synthetic facsimiles in computation, which can function as artificial laboratories for testing ideas. In this paper, I review the domain science for studying streetscapes from vantages in physics, urban studies, animation and the visual arts, psychology, biology, and behavioral geography. I also review the computational developments shaping streetscape science, with particular emphasis on modeling and simulation as informed by <b>data</b> <b>acquisition</b> <b>and</b> <b>generation,</b> data models, path-planning heuristics, artificial intelligence for navigation and way-finding, timing, synthetic vision, steering routines, kinematics, and geometrical treatment of collision detection and avoidance. I also discuss the implications that the advances in computing streetscapes might have on emerging developments in cyber-physical systems and new developments in urban computing and mobile computing...|$|E
40|$|The paper {{describes}} {{the application of}} computer <b>data</b> <b>acquisition</b> <b>and</b> control techniques to several high pressure hydraulic systems and {{draws attention to the}} problems of sampling <b>and</b> rapid <b>data</b> <b>acquisition.</b> The general requirements for <b>data</b> <b>acquisition</b> <b>and</b> control systems, using computers, are discussed and future trends in microcomputers and minicomputers are reviewe...|$|R
30|$|LH: made a {{substantial}} {{contribution to the}} research design, <b>data</b> <b>acquisition</b> <b>and</b> interpretation and writing the first manuscript. SR: made {{a substantial}} contribution to the research design, data analysis and interpretation and critical revisions of the manuscript. PBJ: made a substantial contribution to the research design, <b>data</b> <b>acquisition</b> <b>and</b> critical revisions of the manuscript. BMK: made a substantial contribution to the research design <b>and</b> <b>data</b> <b>acquisition.</b> BK: made a substantial contribution to the data analysis and interpretation and critical revisions of the manuscript. MS: made a substantial contribution to the research design, <b>data</b> <b>acquisition</b> <b>and</b> interpretation and critical revisions of the manuscript. All authors read and approved the final manuscript.|$|R
40|$|In recent years, {{due to the}} {{proliferation}} of sensor networks, {{there has been a}} genuine need of researching techniques for sensor <b>data</b> <b>acquisition</b> <b>and</b> management. To this end, a large number of techniques have emerged that advocate model-based sensor <b>data</b> <b>acquisition</b> <b>and</b> management. These techniques use mathematical models for performing various, day-to-day tasks involved in managing sensor data. In this chapter, we survey the state-of-the-art techniques for model-based sensor <b>data</b> <b>acquisition</b> <b>and</b> management. We start by discussing the techniques for acquiring sensor data. We, then, discuss the application of models in sensor data cleaning; followed by a discussion on model-based methods for querying sensor data. Lastly, we survey model-based methods proposed for data compression <b>and</b> synopsis <b>generation.</b> ...|$|R
40|$|A presente investigaÃÃo apresenta as geotecnologias como instrumentaÃÃo capaz de tornar mais eficiente a gestÃo pÃblica. O municÃpio de IcapuÃ foi o cenÃrio local escolhido para a aplicaÃÃo da proposta. Objetivando mostrar que informaÃÃes sistematizadas e espacialmente referenciadas, atravÃs da representaÃÃo de cartografia temÃtica, e de sistemas de informaÃÃes, contribuem positivamente para a gestÃo pÃblica, durante o desenvolvimento da tese, foi construÃdo e publicado o Atlas de IcapuÃ, e projetado o Sistema de InformaÃÃes GeogrÃficas â SIG para o municÃpio. A cartografia foi elaborada atravÃs da vetorizaÃÃo manual de imagens com resoluÃÃo de 50 cm, detalhadamente, permitindo a plotagem de mapas temÃticos do municÃpio como um todo, e das comunidades, em escalas ampliadas. As escalas dos mapas produzidos variam entre 1 : 5. 000 e 1 : 150. 000. Dados SRTM foram utilizados para a obtenÃÃo de dados altimÃtricos e geraÃÃo de MDT, e a Geologia e Geomorfologia foram representadas com base na interpretaÃÃo das imagens e em estudos desenvolvidos por MEIRELES & SANTOS (2011), que tratam da evoluÃÃo geomorfolÃgica da planÃcie costeira de IcapuÃ. A tese de que as geotecnologias configuram importante ferramenta para a gestÃo pÃblica, procurou evidenciar, atravÃs da cartografia produzida e do SIG, os ecossistemas costeiros, setores em processo erosivo, Ãreas de risco potencial, zonas adequadas Ã expansÃo urbana, Ãreas de preservaÃÃo, e identificar as intervenÃÃes que provocam perdas de biodiversidade e as aÃÃes de conservaÃÃo que podem ajudar na recuperaÃÃo de ambientes degradados. This {{research}} {{presents the}} Geotechnology as instrumentation {{capable of making}} more efficient public management. The municipality of IcapuÃ was the local scenario chosen {{for the implementation of}} the proposal. Aiming to show that systematized information and spatially referenced through the thematic cartography, and representation of information systems, contribute positively to the public administration, during the development of the thesis, was built and published the Atlas of IcapuÃ, and designed the geographic information System â GIS for the municipality. Cartography was prepared through the manual vectorization of images with a resolution of 50 cm, detail, enabling plotting of thematic maps of the municipality as a whole, and communities, in enlarged scales. The scales of maps produced vary between 1 : 5, 000 and 1 : 150, 000. SRTM data were used for the altimetrics <b>data</b> <b>acquisition</b> <b>and</b> <b>generation</b> of MDT, and the Geology and Geomorphology were represented {{on the basis of the}} interpretation of images and in studies developed by MEIRELES & SANTOS (2011), dealing with the geomorphological evolution of the coastal plain of IcapuÃ. The thesis that the Geotechnologies configure important tooling for the public administration, sought to highlight, through cartography produced and GIS, the coastal ecosystems, sectors in erosive process, areas of potential risk, appropriate urban expansion areas, preservation areas, and identify interventions that lead to loss of biodiversity and the conservation actions that can help in the recovery of degraded environments...|$|E
30|$|Errors also existed during <b>data</b> <b>acquisition</b> <b>and</b> processing.|$|R
5000|$|IEST-RP-DTE012.2: Handbook for Dynamic <b>Data</b> <b>Acquisition</b> <b>and</b> Analysis ...|$|R
50|$|Bustec is {{a company}} that designs and {{manufactures}} instrumentation for high-performance <b>data</b> <b>acquisition</b> <b>and</b> instrument control. The company's products serve applications that include engine testing, automotive and missile testing, wind tunnel <b>data</b> <b>acquisition</b> <b>and</b> control, acoustics, vibration applications, aircraft component testing and more. Headquarters is located in Shannon, Co. Clare, Ireland.|$|R
40|$|Abstract In recent years, {{due to the}} {{proliferation}} of sensor networks, {{there has been a}} genuine need of researching techniques for sensor <b>data</b> <b>acquisition</b> <b>and</b> manage-ment. To this end, a large number of techniques have emerged that advocate model-based sensor <b>data</b> <b>acquisition</b> <b>and</b> management. These techniques use mathematical models for performing various, day-to-day tasks involved in man-aging sensor data. In this chapter, we survey the state-of-the-art techniques for model-based sensor <b>data</b> <b>acquisition</b> <b>and</b> management. We start by discussing the techniques for acquiring sensor data. We, then, discuss the application of models in sensor data cleaning; followed by a discussion on model-based meth- 2 ods for querying sensor data. Lastly, we survey model-based methods proposed for data compression <b>and</b> synopsis <b>generation...</b>|$|R
40|$|Abstract. This paper {{discusses}} {{a system}} of <b>data</b> <b>acquisition</b> <b>and</b> processing based on radio frequency technology and labview. It is designed for <b>data</b> <b>acquisition</b> <b>and</b> transmission of UAV (Unmanned Aerial Vehicle) test system. It makes full use of the usefulness of labview in measurement field and the flexibility of wireless data transmission. Firstly this paper introduces the hardware and software of wireless <b>data</b> <b>acquisition</b> <b>and</b> transmission device. Then it introduces the design of labview software. Finally UAV test system uses this device to collect and analysis data, it provided a lot of useful information for the UAV flight test...|$|R
5000|$|Develop new <b>data</b> <b>acquisition</b> <b>and</b> {{processing}} {{techniques for}} expanded data quality.|$|R
5000|$|<b>Data</b> <b>acquisition</b> <b>and</b> {{reduction}} {{software for}} the UKIRT and the JCMT.|$|R
40|$|<b>Data</b> <b>acquisition</b> systems used in NASA's {{wind tunnels}} from the 1950 's through {{the present time}} are {{summarized}} as a baseline for assessing the impact of minicomputers <b>and</b> microcomputers on <b>data</b> <b>acquisition</b> <b>and</b> <b>data</b> processing. Emphasis {{is placed on the}} cyclic evolution in computer technology which transformed the central computer system, and finally the distributed computer system. Other developments discussed include: medium scale integration, large scale integration, combining the functions of <b>data</b> <b>acquisition</b> <b>and</b> control, and micro and minicomputers...|$|R
5000|$|AcqKnowledge - Physiology <b>data</b> <b>acquisition</b> <b>and</b> {{analysis}} software with automated HRV analysis ...|$|R
30|$|The <b>data</b> <b>acquisition</b> <b>and</b> {{processing}} software was SurfaceLab 6.2 (ION-TOF GmbH, Münster, Germany).|$|R
40|$|Abstract:- The need {{to improve}} the {{software}} engineering {{in the development of}} complex <b>data</b> <b>acquisition</b> <b>and</b> control systems with reusable components designed with graphical programming languages leads to represent the knowledge acquired in the development of these systems. Software for <b>data</b> <b>acquisition</b> <b>and</b> control systems with graphical programming is playing an important role in industry and research, graphical programming languages as Lab VIEW has a variety of reusable graphical objects applicable in this kind of systems. In this paper we present an effort to represent the knowledge in the development of <b>data</b> <b>acquisition</b> <b>and</b> control systems with graphical programming, modeling the software process by UML notation...|$|R
