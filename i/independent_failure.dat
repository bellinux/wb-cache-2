82|265|Public
2500|$|Distributed {{computing}} studies {{distributed system}}s. A distributed {{system is a}} software system in which components located on networked computers communicate and coordinate their actions by passing messages. The components {{interact with each other}} in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and <b>independent</b> <b>failure</b> of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to [...] peer-to-peer applications.|$|E
2500|$|Distributed {{computing}} [...] is [...] a {{field of}} computer science that studies distributed systems. A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages. The components {{interact with each other}} in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and <b>independent</b> <b>failure</b> of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.|$|E
2500|$|The {{use of a}} {{post and}} core does not {{strengthen}} the tooth prior to restoration with a crown; rather, it {{may contribute to the}} weakening of the tooth structure, as the forces placed upon the future prosthetic crown and core are now transmitted along virtually {{the entire length of the}} brittle, endodontically treated tooth. This inherent drawback is taken into account when the prognosis of the finished restoration is determined and explained to the patient prior to the onset of treatment. [...] It is because of this increased risk of failure inherent in the use of post and core restorations that, when all of the <b>independent</b> <b>failure</b> rates of the many procedures needed for the restoration of the tooth are considered together (endodontic treatment, crown lengthening (when indicated), post and core and prosthetic crown), the patient is sometimes advised to have the tooth extracted and an implant placed.|$|E
50|$|The {{alternative}} is {{defense in depth}} - such as reinforced or double insulation - where multiple <b>independent</b> <b>failures</b> must occur to expose a dangerous condition.|$|R
5000|$|<b>Independent</b> homelands: <b>Failure</b> of {{a fiction}} : 1979 presidential address (1979) ISBN B0006E8KNO ...|$|R
40|$|The {{oxidation}} {{behavior of}} a slip cast, yttria-doped, sintered reaction-bonded silicon nitride after 'flash oxidation' was investigated. It was found that both the static oxidation resistance and flexural stress rupture life (creep deformation) were improved at 1000 C in air compared {{to those of the}} same material without flash oxidation. Stress rupture data at high temperatures (1000 to 1200 C) are presented to indicate applied stress levels for oxidation-dependent and <b>independent</b> <b>failures...</b>|$|R
50|$|In {{fault-tolerant}} design, MTTR {{is usually}} considered to {{also include the}} time the fault is latent (the time from when the failure occurs until it is detected). If a latent fault goes undetected until an <b>independent</b> <b>failure</b> occurs, the system {{may not be able}} to recover.|$|E
5000|$|Distributed {{computing}} studies {{distributed system}}s. A distributed {{system is a}} software system in which components located on networked computers communicate and coordinate their actions by passing messages. The components {{interact with each other}} in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and <b>independent</b> <b>failure</b> of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to [...] peer-to-peer applications.|$|E
5000|$|Distributed {{computing}} [...] is a {{field of}} computer science that studies distributed systems. A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages. The components {{interact with each other}} in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and <b>independent</b> <b>failure</b> of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to [...] peer-to-peer applications.|$|E
40|$|Abstract—The paper {{presents}} a BIST-based scheme for fault diagnosis {{that can be}} used to identify permanent and address <b>independent</b> <b>failures</b> in embedded read-only memories. The proposed approach offers a simple test flow and does not require intensive interactions between a BIST controller and a tester. The scheme rests on partitioning of rows and columns of the memory array by employing low cost test logic. It is designed to meet requirements of at-speed test thus enabling detection of time-related faults...|$|R
40|$|ORDER-c, an {{algorithm}} for enumerating, {{in order}} of nonincreasing probability, the states of systems with dependent failures and multimode components is presented. The dependencies and degradations are modeled by the cause-based multimode model, which is reviewed. The ORDER-b algorithm is also reviewed, and the ORDER-b and ORDER-c algorithms are compared. The ORDER-algorithm is shown to be more flexible than the ORDER-b algorithm {{because it can be}} based on any state enumeration algorithm for <b>independent</b> <b>failures</b> and multimode components. Experimental results show that the ORDER-c algorithm requires less memory and runs faster. link_to_subscribed_fulltex...|$|R
40|$|Details on Sockets {{later on}} by Jan Stöss For a better understanding: read Coulouris et al: “Distributed Systems: Concepts and Design”, ch. 4 + 5 © 2003 Universität Karlsruhe, System Architecture Group 2 Distributed Objects Objects that can receive remote method invocations are called “remote objects ” and they {{implement}} a remote interface Due to possibility of <b>independent</b> <b>failures</b> of invoker and invoked object RMIs differ from local invocations Code for marshalling etc. {{can be generated}} automatically by an IDL compiler from {{the definition of the}} remote interface...|$|R
50|$|The {{use of a}} {{post and}} core does not {{strengthen}} the tooth prior to restoration with a crown; rather, it {{may contribute to the}} weakening of the tooth structure, as the forces placed upon the future prosthetic crown and core are now transmitted along virtually {{the entire length of the}} brittle, endodontically treated tooth. This inherent drawback is taken into account when the prognosis of the finished restoration is determined and explained to the patient prior to the onset of treatment. It is because of this increased risk of failure inherent in the use of post and core restorations that, when all of the <b>independent</b> <b>failure</b> rates of the many procedures needed for the restoration of the tooth are considered together (endodontic treatment, crown lengthening (when indicated), post and core and prosthetic crown), the patient is sometimes advised to have the tooth extracted and an implant placed.|$|E
40|$|Abstract-The {{power system}} need to sustain high {{reliability}} {{due to its}} complexity and security. The reliability prediction method is usually based on <b>independent</b> <b>failure.</b> However, the common cause failure affects many components simultaneously in a system, {{and it turns out}} the system collapse seriously in a wide range. Therefore, to improve the reliability of the power system practically, the analysis using the common cause failure is required. This paper describes the common cause failure modeling combined with <b>independent</b> <b>failure.</b> Using the dynamic fault tree, the incorporated <b>independent</b> <b>failure</b> and common cause failure are proposed and analyzed, and it is applied to the power substation in order to examine the method. I...|$|E
40|$|Abstract: Using a {{conditional}} method, explicit formulae for computing quantiles {{pertinent to}} prediction intervals for future Weibull order statistics are developed for two cases: when only previous <b>independent</b> <b>failure</b> data are available, and when both previous <b>independent</b> <b>failure</b> data and early-failure data in current experiment are available. The second case includes {{the case when}} only current early-failure data are available. Comparisons of interval widths are made for different estimators of parameters and different ways of forming prediction intervals...|$|E
40|$|Abstract—In this paper, logically and {{spatially}} correlated failures affecting a distributed-computing system (DCS) {{have been}} modeled in a stochastic manner {{by means of}} a Markov random field (MRF) approach. The MRF is induced by the topology of the communication network, and is specified locally by the reliability of each node and the degree of interaction between a node and its nearest neighbors. Thus, the MRF introduces a global probability distribution function for the failure patterns of nodes in the DCS, which is parameterized using n values per node, where n is the number of nodes in the DCS. The statistical analysis conducted on test networks has shown that, compared to <b>independent</b> <b>failures,</b> correlated failures increase: (i) the average number of failed nodes due to failures propagate among the nodes; and (ii) the probability of observing a large fraction of failed computing nodes...|$|R
40|$|Earlier this year, Australia’s {{most recent}} independently-owned news and {{features}} magazine, The Eye, ceased publication. It {{was just one}} publication in {{a long list of}} <b>independent</b> <b>failures</b> in Australia - recent history would indicate that only publications owned by the major chains are able to survive in the contemporary news media environment. But not all countries in the Western world experience the same dearth of independently-owned news media. This paper draws on research conducted in 2000 on the magazine and newspaper subsidies systems in The Netherlands and Scandinavia, and looks at the benefits of such a policy environment to the independent news media...|$|R
50|$|Zonal Safety Analysis (ZSA) is one {{of three}} {{analytical}} methods which, taken together, form a Common Cause Analysis (CCA) in aircraft safety engineering under SAE ARP4761. The other two methods are Particular Risks Analysis (PRA) and Common Mode Analysis (CMA). Aircraft system safety requires the independence of failure conditions for multiple systems. <b>Independent</b> <b>failures,</b> represented by an AND gate in a fault tree analysis, have a low probability of occurring in the same flight. Common causes result in the loss of independence, which dramatically increases probability of failure. CCA and ZSA are used to find and eliminate or mitigate common causes for multiple failures.|$|R
40|$|Two <b>independent</b> <b>failure</b> mechanisms, mainly {{induced by}} temperature, were {{observed}} in unpassivated, Au-metallised, commercially available power MESFETs. They result in a high gate/source (drain) leakage and {{in an increase in}} the open channel resistance, and may explain the burn-out of the devices in practical applications...|$|E
40|$|We {{present a}} new {{improvement}} upon the classical Bonferroni inequalities and show how this improvement can be utilized in bounding {{the reliability of}} a communication network whose nodes are perfectly reliable and whose edges are subject to random and <b>independent</b> <b>failure.</b> (orig.) SIGLEAvailable from TIB Hannover: RR 2036 (114) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|E
40|$|The key {{concepts}} and techniques {{used to build}} high availability computer systems are (1) modularity, (2) fail-fast modules, (3) <b>independent</b> <b>failure</b> modes, (4) redundancy, and (5) repair. These ideas apply to hardware, to design, and to software. They also apply to tolerating operations faults and environmental faults. This article explains these ideas and assesses highavailability system trends...|$|E
30|$|Deb ([1995]) has {{explained}} the optimization techniques {{and how they}} can be used in the engineering problems. Tewari et al. ([2000, 2005]) dealt with the development of decision support system of refining system of sugar plant. They determined the availability for the refining system with elements exhibiting <b>independent</b> <b>failures</b> and repairs or the operation with standby elements for sugar industry. They also dealt with mathematical modeling and behavioral analysis for a refining system of a sugar industry using genetic algorithm. Ying-Shen et al. ([2008]) proposed a genetic algorithm-based optimization model to optimize the availability for a series–parallel system. The objective is to determine the most economical policy of component’s MTBF and mean time to repair.|$|R
40|$|Reliability Analysis of nonrestorable {{redundant}} power Systems {{of industrial}} plants and other consumers of electric energy was carried out. The main attention {{was paid to}} numbers failures influence, caused by failures of all elements of System due to one general reason. Noted the main possible reasons of common failures formation. Two main indicators of reliability of non-restorable systems are considered: average time of no-failure operation and mean probability of no-failure operation. Modeling of failures were carried out by mean of division of investigated system into two in-series connected subsystems, one of them indicated <b>independent</b> <b>failures,</b> but the other indicated common failures. Due to joined modeling of single and common failures resulting intensity of failures is the amount incompatible components: intensity statistically <b>independent</b> <b>failures</b> and intensity of common failures of elements and system in total. It is shown the influence of common failures of elements on average time of no-failure operation of system. There is built the scale of preference of systems according to criterion of  average time maximum of no-failure operation, depending on portion of common failures. It is noticed that such common failures don’t influence on the scale of preference, but  change intervals of time, determining the moments of systems failures and excepting them from the number of comparators. There were discussed two problems  of conditionally optimization of  systems’  reservation choice, taking into account their reliability and cost. The first problem is solved due to criterion of minimum cost of system providing mean probability of no-failure operation, the second problem is solved due to criterion of maximum of mean probability of no-failure operation with cost limitation of system...|$|R
40|$|Abstract — In a grid {{environment}} {{there are}} thousands of resources, services and applications that need to interact in order to make possible the use of the grid [1] as an execution platform. Since these elements are extremely heterogeneous, volatile and dynamic, there are many failure possibilities, including not only <b>independent</b> <b>failures</b> of each element, but also those resulting from interactions between them. Because of the inherent instability of grid environments, fault-detection and recovery is another critical component that must be addressed. The need for fault-tolerance is especially sensitive for large parallel applications since the failure rate grows with the number of processors and the duration of the computation. In this paper we will discuss the various fault management stratigies that will help to achieve the fault tolerance and is good reference to researcher...|$|R
40|$|AbstractThe {{assessment}} and modeling of the maintenance effect {{is an active}} research topic. In this paper, we proposed a failure- counting-based maintenance effect assessment model (abbreviated as FCME). Specifically speaking, this model assumes that a system failure process during a preventive maintenance interval {{can be described as}} an <b>independent</b> <b>failure</b> path. The initial <b>independent</b> <b>failure</b> path can be denoted as the inherent failure path. Furthermore, the current failure can be presented as the sum of the last failure path and the increment that is proportional to the difference between the inherent and last failure path. This proportion is used to represent the preventive maintenance effect. Moreover, the failure path is modeled by a random failure point process model. In order to illustrate the effectiveness of the proposed method, a numerical example is presented based on the data referenced from previous literatures. Then the model is used to assess the preventive maintenance effect of a bus fleet with the real world operating data...|$|E
40|$|An {{increased}} use of software systems in safety [...] critical applications has made the estimation of the software probability of failure on demand (pfd) an issue of great importance. In this paper, estimation of the system pfd {{is based on the}} decomposition of the system into a set of sub [...] systems with <b>independent</b> <b>failure</b> behaviour. Structures which fork and then join again can pose a problem when it comes to defining independently failing sub-systems...|$|E
40|$|Transactions {{can improve}} the {{performance}} and consistency of file system applications. The traditional file system provides weak consistency through a lightweight interface, but a heavy-weight interface providing strong, fine-grained consistency {{can be used to}} achieve higher performance by improving application behavior. Transactions also provide extensibility, allowing <b>independent</b> <b>failure</b> modes among composed file system modules. A system will be presented that provides delta compressed storage using a portable library file system interface. It outperforms RCS, using transactions to enable an inexpensive atomic update of its structures, validating the claim that transactions can improve application performance...|$|E
40|$|A {{fault-tolerant}} scheme, called dual homing, {{is generally}} used in IP-based access networks {{to increase the}} survivability of the network. However, dual homing itself cannot provide survivability with respect to possible failures in the Wavelength Division Multiplexed (WDM) core network. To provide survivability in the core network, protection and restoration techniques must be used. In the past, dual homing architecture and protection are studied separately. In this paper, we observe that the dual homing architecture introduces new issues for protection and restoration design, especially when providing survivability against two <b>independent</b> <b>failures,</b> one in the access network {{and the other in}} the core network. This paper provides an integrated solution and studies the protection design problem in the WDM core network, given a dual-homing infrastructure in the access network. Several algorithmic solutions are proposed, and performance of the solutions is compared...|$|R
40|$|International audienceThis paper {{presents}} {{an approach to}} assess the effects of common cause failures (CCF) on dependability of digital systems. <b>Independent</b> <b>failures</b> of system components and partial or lethal shocks are considered in a global CCF model, the Atwood model. The Coloured Petri nets (CPN) are used to modelize the digital system and the common cause failures. Based on the CPN, the parameters of Atwood model are estimated analytically and by Monte-Carlo simulation. Thereafter, the Atwood model of CCF is modified in order to represent the dominant failures propagation on some system components {{in the case of}} partial shocks. The assessment of system dependability, in the presence of CCF failures, becomes possible. This approach is applied to a representative instrumentation and control system of a nuclear power plant. The system is large {{with a high level of}} redundancy...|$|R
40|$|Abstract—Fault tolerance, {{reliability}} and availability in Cloud computing are critical to ensure correct and continuous system operation also {{in the presence of}} failures. In this paper, we present an approach to evaluate fault tolerance mechanisms that use the virtualization technology to transparently increase the {{reliability and}} availability of applications deployed in the virtual machines in a Cloud. In contrast to several existing solutions that assume <b>independent</b> <b>failures,</b> we take into account the failure behavior of various server components, network and power distribution in a typical Cloud computing infrastructure, the correlation between individual failures, and the impact of each failure on user’s applications. We use this evaluation to study fault tolerance mechanisms under different deployment contexts, and use it as the basis to develop a methodology for identifying and selecting mechanisms that match user’s fault tolerance requirements...|$|R
40|$|Three {{fundamentally}} different failure theories for homogeneous and isotropic materials are examined {{in both the}} ductile and brittle ranges of behavior. All three theories are calibrated by just two <b>independent</b> <b>failure</b> properties. These three are the Coulomb-Mohr form, the Drucker-Prager form, and a recently derived theory involving a quadratic representation along with a fracture restriction. The three theories are given a detailed comparison and evaluation. The Coulomb-Mohr form and the Drucker-Prager form are found to predict physically unrealistic behavior in some important cases. The present form meets the consistency requirements...|$|E
40|$|A {{model for}} the {{reliability}} and performance analysis of systems where components can degrade in a statistically dependent manner is presented. This cause-based multimode model {{is based on the}} idea that deviations of components from the up state have underlying physical causes which can be explicitly identified and are statistically independent. The effects of several causes can be combined in a flexible manner. System reliability and performance measures can be computed approximately by considering the most probable states. Such states can be efficiently generated by algorithms developed for the earlier multimode, statistically <b>independent</b> <b>failure</b> model. link_to_subscribed_fulltex...|$|E
40|$|Generally, {{the methods}} for the {{calculation}} of the system reliability assume independent components with <b>independent</b> <b>failure</b> modes. For mechanical systems in a realistic situation there might be dependencies between {{the components of the}} system. Therefore, existing approaches to consider dependencies for {{the calculation of}} the system reliability are summarized and extended in our paper. Most of these methods are restricted to small system structures and assume exponential distributions to describe the failure mode, i. e. constant failure rates. It is shown how the models can be used for time-dependent failure rates (e. g. Weibull distribution). We describe how to apply the models to bigger system structures...|$|E
40|$|International audienceIn telecommunications, {{power supply}} systems and {{transportation}} systems, among other areas, it is of interest to compute {{the probability that a}} set of nodes is connected within a network subject to failure of edges. This can be done by considering a non-oriented graph G for which links mayfail, with probability qi for link i. Since the exact evaluation is an NP-hard problem in general with respect {{to the size of the}} graph, Monte Carlo simulation techniques are generally needed, and rare-event simulation techniques when dealing with low probabilities of disconnection. Two typesof methods can be applied, namely splitting and importance sampling. In a first part of the talk, we review our existing works on zero-variance importance sampling approximation techniques applied to this context. The methods have been applied to the context of <b>independent</b> <b>failures</b> between links. The general idea is to generate the link states one by one, using a sampling strategy that approximates an ideal zero-variance importance sampling scheme. The approximation is based on minimal cuts in subgraphs, and the relative error of the resulting estimator is shown to stay bounded as individual link failure probabilities tend to zero, and even to converge to 0 under some conditions. The method can be sped up by applying series-parallel reductions of the graph each time a link state is sampled. Another extension is to try improve the mincut-based approximation by using a convex linear combination of this approximation and a minpath-based one. In a second part of the talk, we relax the assumption of <b>independent</b> <b>failures</b> of links by the realistic Marshall-Olkin copula model for failures, so that the links can fail in groups (called shocks) with certain probabilities for the groups. We describe how, instead of sampling links sequentially, we can sample shocks sequentially, and we discuss the application of zero-variance importance sampling to this context. Depending on time, we will also present a splitting algorithm recently designed for this type of model...|$|R
40|$|In this paper, {{we propose}} and study {{analytical}} models of self-repairing peer-to-peer storage systems subject to failures. The failures {{correspond to the}} simultaneous loss of multiple data blocks due to the definitive loss of a peer (or following a disk crash). In the system we consider that such failures happen continuously, hence {{the necessity of a}} self-repairing mechanism (data are written once for ever). We show that, whereas stochastic models of <b>independent</b> <b>failures</b> similar to those found in the literature give a correct approximation of the average behavior of real systems, they fail to capture their variations (e. g. in bandwidth needs). We propose to solve this problem using a new stochastic model based on a fluid approximation and we give a characterization of the behavior of the system according to this model (expectation and standard deviation). This new model is validated using comparisons between its theoretical behavior and computer simulations...|$|R
40|$|The {{reliability}} and income analyses of newly developed $k-out-of-(n + m) :G$ (or $n, m, k$) type redundant systems {{subject to a}} combination of common-cause <b>failures</b> and <b>independent</b> <b>failures</b> are presented. The global goal was to evaluate the impacts of the standby activation policy and the system repair times on such relevant system performance indices as the reliability, long-run availability, mean time to failure, variance of time to failure and net income. To facilitate this investigation several possible repair policies are developed. Results obtained using typical and practical values of basic system variables indicate that the governing standby activation policy as well as the system repair time distribution affect profoundly the values of the afore-mentioned system performance indices. In addition, newly developed mathematical relationships that would enable the net incomes of $k-out-of-(n + m) :G$ type systems to be maximized by the adoption of certain system repair rates are presented...|$|R
