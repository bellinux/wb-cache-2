147|10000|Public
50|$|With rate = 1 using four antennas, data is {{transmitted}} four times per symbol, where {{each time the}} data is conjugated and/or inverted. This {{does not change the}} data rate, but does give the signal more robustness and avoids sudden increases <b>in</b> <b>error</b> <b>rates.</b>|$|E
50|$|Kaplan {{observed}} Massar and {{his teammates}} playing for a weekend in Atlantic City. He noted {{that each of the}} players used a different, and overcomplicated, card counting strategy. This resulted <b>in</b> <b>error</b> <b>rates</b> that undermined the benefits of the more complicated strategies. Upon returning to Cambridge, Kaplan detailed the problems he observed to Massar.|$|E
5000|$|Whole genomic DNA {{amplification}} [...] The bisulfite treated DNA is {{subjected to}} whole genome amplification (WGA) via random hexamer priming and Phi29 DNA polymerase, which has a proofreading activity resulting <b>in</b> <b>error</b> <b>rates</b> 100 times lower than the Taq polymerase. The products are then enzymatically fragmented, purified from dNTPs, primers and enzymes, and applied to the chip.|$|E
40|$|This paper applies inter-session {{variability}} modelling {{and joint}} factor analysis to face authentication using Gaus-sian mixture models. These techniques, originally devel-oped for speaker authentication, aim to explicitly model and remove detrimental within-client (inter-session) variation from client models. We apply the techniques to face authen-tication on the publicly-available BANCA, SCface and MO-BIO databases. We propose a face authentication protocol for the challenging SCface database, {{and provide the}} first results on the MOBIO still face protocol. The techniques provide relative reductions <b>in</b> <b>error</b> <b>rate</b> of up to 44 %, us-ing only limited training data. On the BANCA database, our results represent a 31 % reduction <b>in</b> <b>error</b> <b>rate</b> when benchmarked against previous work. 1...|$|R
40|$|We {{evaluate}} {{the performance of}} several feature sets on the Aurora task as defined by ETSI. We show that after a non-linear transformation, a number of features can be effectively used in a HMM-based recognition system. The non-linear transformation is computed using a neural network which is discriminatively trained on the phonetically labeled (forcibly aligned) training data. A combination of the non-linearly transformed PLP (perceptive linear predictive coefficients), MSG (modulation filtered spectrogram) and TRAP (temporal pattern) features yields a 63 % improvement <b>in</b> <b>error</b> <b>rate</b> as compared to baseline me frequency cepstral coefficients features. The use of the non-linearly transformed RASTA-like features, with system parameters scaled down {{to take into account}} the ETSI imposed memory and latency constraints, still yields a 40 % improvement <b>in</b> <b>error</b> <b>rate...</b>|$|R
30|$|We {{demonstrated}} that incorporation of motion-related information into user-specific offset models {{can improve the}} accuracy of typing input. Including gait phase or acceleration data as an additional input to a Gaussian Process offset model gave a 1 % absolute improvement <b>in</b> <b>error</b> <b>rate</b> over a position-only model.|$|R
5000|$|As of October 2016, over 3,000 MinIONs {{have been}} shipped. PromethION {{has started to}} ship in early access. [...] GridION {{has not yet been}} brought to market. In a paper {{published}} in November 2014, one of the MAP participants wrote, [...] "The MinION is an exciting step in a new direction for single-molecule sequencing, though it will require dramatic decreases <b>in</b> <b>error</b> <b>rates</b> before it lives up to its promise.". By August 2016, bioinformatician Jared Simpson noted that 99.96% consensus accuracy was generated using the nanopolish tool after raw accuracy had been improved with the new R9 nanopore.|$|E
50|$|Foldi et al. used a multi-target visual {{cancellation}} task {{to examine}} visual selective attention {{in patients with}} AD compared to healthy controls. Researchers found slower performance and completion times compared to those without AD, and {{this can be seen}} as a more inefficient and severe form of the attentional processes found in normal aging. Patients with AD scored twice as long on each target or cancellation, irrespective of the type of cancellation, compared to that of the age-matched healthy controls. The control group was also found to slow their search times consistent with a speed-accuracy trade off. That is, when the similarity between targets and distractors increased, and it became more difficult to distinguish the target. However, they maintained detection accuracy with no increase <b>in</b> <b>error</b> <b>rates.</b> This finding suggests, in contrast to patients with AD’s performance, that the control group slowed their search only to avoid responses to distractors, and therefore to avoid errors and demonstrate the speed-accuracy trade off. Patients with AD slowed their search as the similarity between target and distractor increased, however they still made more errors and missed targets. This performance did not demonstrate the speed-accuracy trade-off. This suggests that, in AD patients as opposed to healthy controls, the degree of similarity between search items affects the ability to selectively attend to relevant targets. Similar results were observed in Schaefer, where a study of visual selective attention in AD was conducted by varying the physical characteristics of a selective cancellation task.|$|E
5000|$|According to rapid-chase theory, both primes and targets elicit {{feedforward}} sweeps that {{traverse the}} visuomotor system {{in rapid succession}} until they reach motor areas of the brain. There, motor processes are elicited automatically and {{without the need for}} a conscious representation. Because the prime signal has a head-start over the target signal, primes and targets are engaged in a [...] "rapid chase" [...] through the visuomotor system. Because the prime signal reaches the motor cortex first, it is able to activate the motor response assigned to it. The shorter the prime-target SOA, the quicker the target can start the pursuit. When the target signal finally arrives at the motor cortex, it can continue the response process elicited by the prime (if prime and target are consistent) or redirect the response process (if prime and target are inconsistent). This explains why response priming effects increase with prime-target SOA: the longer the SOA, the more time for the prime signal to control the response on its own, and the further the response activation process can proceed {{in the direction of the}} prime. Under some circumstances, the prime can also provoke a response error (leading to the characteristic priming effects <b>in</b> <b>error</b> <b>rates).</b> Such a time-course of sequential motor control by primes and targets was described in 2003 by Dirk Vorberg and coworkers in a mathematical model and fits the time-course of primed motor potentials in the EEG.|$|E
40|$|This paper investigates fitness {{sharing in}} genetic programming. Implicit fitness sharing {{is applied to}} populations of programs. Three {{treatments}} are compared: raw fitness, pure fitness sharing, and a gradual change from fitness sharing to raw fitness. The 6 - and 11 -multiplexer problems are compared. Using the same population sizes, fitness sharing shows a large improvement <b>in</b> the <b>error</b> <b>rate</b> for both problems. Further experiments compare the treatments on learning recursive list membership functions; again, there are dramatic improvements <b>in</b> <b>error</b> <b>rate.</b> Conversely, fitness sharing runs achieve comparable results to raw fitness using populations two to three time...|$|R
40|$|We discuss an {{interactive}} approach to robust, interpretation {{in a large}} scale speech-to-speech translation system. Where other interactive approaches to robust interpretation have depended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37 % reduction <b>in</b> <b>error</b> <b>rate</b> over a corpus of noisy sentences...|$|R
5000|$|When {{applied to}} facial recognition, CNNs {{achieved}} a large decrease <b>in</b> <b>error</b> <b>rate.</b> Another paper reported a 97.6 percent recognition rate on [...] "5,600 still images {{of more than}} 10 subjects". CNNs {{were used to assess}} video quality in an objective way after manual training; the resulting system had a very low root mean square error.|$|R
30|$|Systems {{to improve}} patient safety, such as checklists, are only {{effective}} when implemented within a receptive and positive safety climate. A positive safety climate {{is associated with}} a measurable decrease <b>in</b> <b>error</b> <b>rates</b> 2.|$|E
40|$|Objective: Antisaccade {{errors are}} {{consistently}} increased in schizophrenia. As {{they have been}} demonstrated only in cross sectional studies, {{it is unclear how}} they vary longitudinally or with different medications. In a previous cross sectional study, we reported a trend towards a reduction <b>in</b> <b>error</b> <b>rates</b> in a patient group treated with risperidone, compared with clozapine and sulpiride treated groups...|$|E
40|$|In {{the present}} experiment, {{cognitive}} control under stress was investigated using a real-life paradigm, namely an evaluation flight for military student pilots. The magnitude of cognitive interference on color-word, numerical and emotional Stroop paradigms was studied during a baseline recording and {{right before the}} test flight. Cardio-respiratory parameters were simultaneously assessed during rest {{and the performance of}} the Stroop tasks. Cognitive data suggested a different speed/accuracy trade-off under stress, and no modulation of the interference effect for color words or numerical stimuli. However, we observed a major increase <b>in</b> <b>error</b> <b>rates</b> for specific emotional stimuli related to the evaluation situation in the stress condition. The increase in cognitive interference from emotional stimuli, expressed as an increase <b>in</b> <b>error</b> <b>rates,</b> was correlated to the decreased cardiac reactivity to challenge in the stress situation. This relationship is discussed in the framework of Sanders' (1983) model of stress and performance. In terms of future research, this link warrants a fruitful lead to be followed for investigating the causal mechanism of performance decrements under the influence of stress. © 2013 Elsevier B. V. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|In {{very large}} {{vocabulary}} hypothesis-verification systems, the fine acoustic matcher {{is usually the}} most time consuming, so that the main concern is reducing the preselection list length as much as possible. Traditionally, these systems use a too high fixed preselection list length, increasing computational demands over the really needed. The idea we are proposing is estimating a different preselection list length for every utterance, {{so that we can}} lower the average computational effort needed for the recognition process. As we will show, it’s even possible that the resulting system outperforms the fixed length one <b>in</b> <b>error</b> <b>rate,</b> even when reducing computational cost. This paper presents a detailed study on a NN based approach to variable preselection list length estimation. The main achievement has been a relative decrease <b>in</b> <b>error</b> <b>rate</b> of up to 40 %, while getting a relative decrease in average preselection list length of up to 31 %. 1...|$|R
50|$|Useful as {{constituent}} code <b>in</b> low <b>error</b> <b>rate</b> turbo {{codes for}} {{applications such as}} satellite links. Also suitable as SCCC outer code.|$|R
30|$|Participants entered text while sitting, {{standing}} and walking. <b>Error</b> <b>rates</b> <b>in</b> the walking condition {{were significantly higher}} than either of the static conditions. However, although accelerometer data were captured in this study, the correction technique did not make use of these. The goal of our gait phase analysis is to identify factors contributing to the observed differences <b>in</b> <b>error</b> <b>rate</b> when walking. Note that we discarded the data for one subject because accelerometer data were incomplete.|$|R
40|$|Two {{mobile device}} text entry methods were evaluated. The {{well-known}} Multitap method {{was compared to}} our RollPad method on a new device utilizing a tactile touchpad {{in place of a}} keypad. RollPad was well liked by participants. KSPC (keystrokes per character) was significantly lower: 1. 42 compared to 2. 13 with Multitap. However, no significant difference was found <b>in</b> <b>error</b> <b>rates</b> or entry speed, with speed measured at about 7. 3 wpm for both methods...|$|E
40|$|International audienceDespite {{years of}} speech {{recognition}} research, {{little is known}} about which words tend to be misrecognized and why. Previous work has shown that errors increase for infrequent words, short words, and very loud or fast speech, but many other presumed causes of error (e. g., nearby disfluencies, turn-initial words, phonetic neighborhood density) have never been carefully tested. The reasons for the huge differences found <b>in</b> <b>error</b> <b>rates</b> between speakers also remain largely mysterious. Using a mixed-effects regression model, we investigate these and other factors by analyzing the errors of two state-of-the-art recognizers on conversational speech. Words with higher error rates include those with extreme prosodic characteristics, those occurring turn-initially or as discourse markers, and : acoustically similar words that also have similar language model probabilities. Words preceding disfluent interruption points (first repetition tokens and words before fragments) also have higher error rates. Finally, even after accounting for other factors, speaker differences cause enormous variance <b>in</b> <b>error</b> <b>rates,</b> suggesting that speaker error rate variance is not fully explained by differences in word choice, fluency, or prosodic characteristics. We also propose that doubly confusable pairs, rather than high neighborhood density, may better explain phonetic neighborhood errors in human speech processing...|$|E
40|$|We {{present a}} method of {{transforming}} an extract-based DNA computation that is error-prone into one that is relatively error-free. These improvements <b>in</b> <b>error</b> <b>rates</b> are achieved without the supposition of any improvements in {{the reliability of the}} underlying laboratory techniques. We assume that only two types of errors are possible: a DNA strand may be incorrectly processed or it may be lost entirely. We show how to deal with each of these errors individually and then analyze the tradeoff when both must be optimized simultaneously...|$|E
40|$|Recently two new multiplicative weight-updating {{algorithms}}, "Shifting Winnow" and "Tracking the Best Expert" were published. These algorithms {{were designed}} {{in such a}} way as to be able to adapt to a changing concept with minimal amount of loss either <b>in</b> <b>error</b> <b>rate</b> or by number of mistakes made throughout the run of the algorithm. This paper discuss experimental results of implementations of these algorithms. The algorithms were subjected to two different types of datasets, with experiments on the best parameters to set for each of them and comparison of these results to the their respective theoretical bounds. It was found that on a real world data set of an oscillating value with varying frequency, "Tracking the Best Expert" was able to predict far into the future with little increase <b>in</b> <b>error</b> <b>rate</b> whereas "Shifting Winnow" did not do as well using binary features, but was very competitive using real-valued features. On an artificial data set of changing disjunctive concepts, "Shifting [...] ...|$|R
40|$|A new {{approach}} for speaker adaptation consisting of MLLR adaptation enriched {{by a special}} weighting scheme followed by MAP adaptation is presented. While the standard MLLR approach increases the <b>error</b> <b>rate</b> for the considered small amounts of adaptation data in on-line, unsupervised adaptation, our approach can reduce the error by up to 30 %. This result can further be improved by switching to MAP adaptation, yielding a final reduction <b>in</b> <b>error</b> <b>rate</b> of 38. 6 % compared to the speaker independent (SI) system...|$|R
40|$|We {{consider}} {{the problem of}} decoding First-Order Reed-Muller codes efficiently. We give an algorithm that implicitly adapts to the noise conditions, runs significantly faster than known maximum-likelihood algorithms, and yields an <b>error</b> <b>rate</b> that {{is very close to}} optimal. When applied to CCK demodulation (used in the 802. 11 b standard for Wireless Local Area Networks), the algorithm runs up to 4 times faster than a decoder based on the Fast Hadamard Transform, with a loss of at most dB <b>in</b> <b>error</b> <b>rate.</b> We show analytically that the <b>error</b> <b>rate</b> of our adaptive algorithm is, where is the length of a codeword...|$|R
40|$|Mitigating {{the effects}} of interruptions is {{important}} for tackling {{the increasing number of}} possible disrup-tions at home, at work, and online. Previous work has shown that the benefits of practice can decrease the amount of time it takes to resume a task after an interruption. This paper demonstrates that the same benefit can be extended to error rates at the post-completion step showing that a general increase in interruptions leads to a decrease <b>in</b> <b>error</b> <b>rates</b> for the last step of a form-filling task...|$|E
40|$|We {{present a}} simple but {{efficient}} model for object segmentation in video scenes that integrates motion and color information in a joint probabilistic framework. Optical flow is modeled using parametric motion with Gaussian noise. The color distribution of foreground and background is described by histograms or Gaussian mixture models. Optimization is carried out using an efficient graph cut algorithm. In quantitative experiments {{on a variety of}} video data, we demonstrate that the proposed approach leads to significant reductions <b>in</b> <b>error</b> <b>rates</b> compared to a state-of-the-art motion-only segmentation. 1...|$|E
40|$|At {{the outset}} we should {{emphasize}} that nothing in Hayward and Tarr’s commentary {{speaks to the}} main results of the Biederman and Bar (1999) report: Slight costs of rotation in detecting geon differences when matching a sequential pair of novel objects (3. 3 % in-crease <b>in</b> <b>error</b> <b>rates),</b> but massive costs (46. 2 % increase <b>in</b> <b>error</b> <b>rates)</b> in the detection of metric differences. Their comments are all addressed to our {{review of the literature}} of the relatively small costs (versus zero costs) that have been observed when geon differences were, presumably, available. Although they characterize some of Biederman and Bar’s arguments as ‘incorrect’, in fact these arguments can readily be defended. Given the sensitivity that prompted these authors to write their comments, it is disappointing that their own characteri-zation of many of Biederman and Bar’s points are inaccurate. It is not completely clear why the origin of these relatively small rotation costs are an important theoret-ical issue to Hayward and Tarr insofar as they have now ‘eschewed ’ mental rotation mechanisms (their note 2). Instead, viewpoint costs are now regarded by these authors as reflecting changes in information and that ‘there is no directly causal relation between the magni-tude of a change in viewpoint of an object, and the magnitude of the associated cost in recognition ’ (note 2). This has been our position all along (Biederman...|$|E
40|$|A new {{transformation}} matrix technique {{for reducing the}} complexity of a multiuser receiver for DS-CDMA system is presented. The reduction in complexity of multiuser receiver would result <b>in</b> better bit <b>error</b> <b>rate</b> (BER) performance. The reduction <b>in</b> <b>error</b> <b>rate</b> {{would allow us to}} maximize the data throughput of a communication network by minimizing the packet loss. Our simulation results demonstrate that the proposed technique successfully reduces the computational complexity of an optimal multiuser receiver for the DS-CDMA systems. The complexity of the proposed technique is not polynomial in the number of users, but it still gives comparatively reduced complexity {{that can be used to}} achieve optimum performance in terms of a reduced BER and increased network data throughput. [URL]...|$|R
40|$|Abstract- A new {{transformation}} matrix technique {{for reducing the}} complexity of a multiuser receiver for DS-CDMA system is presented. The reduction in complexity of multiuser receiver would result <b>in</b> better bit <b>error</b> <b>rate</b> (BER) performance. The reduction <b>in</b> <b>error</b> <b>rate</b> {{would allow us to}} maximize the data throughput of a communication network by minimizing the packet loss. Our simulation results demonstrate that the proposed technique successfully reduces the computational complexity of an optimal multiuser receiver for the DS-CDMA systems. The complexity of the proposed technique is not polynomial in the number of users, but it still gives comparatively reduced complexity {{that can be used to}} achieve optimum performance in terms of a reduced BER and increased network data throughput. Keywords- DS-CDMA, bit <b>error</b> <b>rate,</b> data throughput, multiuser communications, packet loss I...|$|R
40|$|Improved {{acoustic}} modeling {{can significantly}} decrease the <b>error</b> <b>rate</b> <b>in</b> large-vocabulary speech recognition. Our {{approach to the}} problem is twofold. We first propose a scheme that optimizes the degree of mixture tying for a given amount of training data and computational resources. Experimental results on the Wall Street Journal (WSJ) Corpus show that this new form of output distri-bution achieves a 25 % reduction <b>in</b> <b>error</b> <b>rate</b> over typical tied-mixture systems. We then show that an additional improvement {{can be achieved by}} modeling local time correlation with linear discriminant features. 1...|$|R
40|$|Button {{bars are}} a {{relatively}} new interaction method intended to speed up application use as compared to pull-down menus. This exploratory study compares three command selection methods: pull-down menus, button bars, and user choice of pull-down menus or button bars. Effectiveness was measured in two ways: speed of selection and error rate. 15 participants performed 15 word processor related tasks. Results show that in frequently used functions, such as character attribute selection (bold, italic, u nderline, etc.), button bars are faster. There were {{no statistically significant differences}} <b>in</b> <b>error</b> <b>rates</b> between the three interaction methods. (Also cross-referenced as CAR-TR- 764...|$|E
40|$|This paper {{deals with}} random forest {{regression}} based acoustic event detection (AED) by combining acoustic features with bottleneck features (BN). The bottleneck features {{have a good}} reputation of being inherently discriminative in acoustic signal processing. To deal with the unstructured and complex real-world acoustic events, an acoustic event detection system is constructed using bottleneck features combined with acoustic features. Evaluations were carried out on the UPC-TALP and ITC-Irst databases which consist of highly variable acoustic events. Experimental results demonstrate {{the usefulness of the}} low-dimensional and discriminative bottleneck features with relative 5. 33 % and 5. 51 % decreases <b>in</b> <b>error</b> <b>rates</b> respectively...|$|E
40|$|Abstract. Designing {{software}} for exploring hierarchical data sets is challenging because users can easily become lost in large hierarchies. We present a novel interface, the hoptree, to assist users with navigating large hierarchies. The hoptree preserves navigational history and context and allows one-click navigation to recently-visited locations. We describe {{the design of}} hoptrees and an implementation that we created for a tree exploration application. We discuss the potential for hoptrees {{to be used in}} a wide variety of hierarchy navigation scenarios. Through a controlled experiment, we compared the effectiveness of hoptrees to a breadcrumb navigation interface. Study participants overwhelmingly preferred the hoptree, with improved time-on-task with no difference <b>in</b> <b>error</b> <b>rates...</b>|$|E
40|$|In this paper, {{we propose}} a {{recognition}} scheme for the Indian script of Devanagari. Recognition accuracy of Devanagari script {{is not yet}} comparable to its Roman counterparts. This is mainly due {{to the complexity of}} the script, writing style etc. Our solution uses a Recurrent Neural Network known as Bidirectional Long-Short Term Memory (BLSTM). Our approach does not require word to character segmentation, {{which is one of the}} most common reason for high word <b>error</b> <b>rate.</b> We report a reduction of more than 20 % <b>in</b> word <b>error</b> <b>rate</b> and over 9 % reduction <b>in</b> character <b>error</b> <b>rate</b> while comparing with the best available OCR system...|$|R
40|$|This work is {{concerned}} with distinguishing different semantic relations which exist between distributionally similar words. We compare a novel approach based on training a linear Support Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional similarity. We show that the new supervised approach does better even when there is minimal information about the target words in the training data, giving a 15 % reduction <b>in</b> <b>error</b> <b>rate</b> over unsupervised approaches...|$|R
40|$|We {{evaluate}} a new filterbank structure, yielding the harmonic structure cepstral coefficients (HSCCs), on a mismatchedsession closed-set speaker classification task. The novelty of the filterbank {{lies in its}} averaging of energy at frequencies related by harmonicity rather than by adjacency. Improvements are presented which achieve a 37 %rel reduction <b>in</b> <b>error</b> <b>rate</b> under these conditions. The improved features are combined with a similar Mel-frequency cepstral coefficient (MFCC) system to yield <b>error</b> <b>rate</b> reductions of 32 %rel, suggesting that HSCCs offer information which is complimentary to that available to today’s MFCC-based systems. Index Terms: speaker recognition, signal processing, harmonic strucure, spectral analysi...|$|R
