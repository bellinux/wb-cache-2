2599|2404|Public
25|$|The {{original}} maximum-margin hyperplane algorithm {{proposed by}} Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested {{a way to}} create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product {{is replaced by a}} nonlinear <b>kernel</b> <b>function.</b> This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.|$|E
25|$|The {{choice of}} the <b>kernel</b> <b>function</b> K is not crucial to the {{accuracy}} of kernel density estimators, so we use the standard multivariate normal kernel throughout: , where H plays the role of the covariance matrix. On the other hand, the {{choice of the}} bandwidth matrix H {{is the single most important}} factor affecting its accuracy since it controls the amount and orientation of smoothing induced. That the bandwidth matrix also induces an orientation is a basic difference between multivariate kernel density estimation from its univariate analogue since orientation is not defined for 1D kernels. This leads to the choice of the parametrisation of this bandwidth matrix. The three main parametrisation classes (in increasing order of complexity) are S, the class of positive scalars times the identity matrix; D, diagonal matrices with positive entries on the main diagonal; and F, symmetric positive definite matrices. The S class kernels have the same amount of smoothing applied in all coordinate directions, D kernels allow different amounts of smoothing in each of the coordinates, and F kernels allow arbitrary amounts and orientation of the smoothing. Historically S and D kernels are the most widespread due to computational reasons, but research indicates that important gains in accuracy can be obtained using the more general F class kernels.|$|E
2500|$|K is the <b>kernel</b> <b>function</b> {{which is}} a {{symmetric}} multivariate density; ...|$|E
3000|$|... in {{synaptic}} coupling is of {{some general}} types of functions. Not only three typical types of <b>kernel</b> <b>functions</b> but also oscillatory <b>kernel</b> <b>functions</b> within certain range of model parameters in synaptic coupling are considered. First, we give some assumptions for the <b>kernel</b> <b>functions</b> [...]...|$|R
3000|$|Next, we quantitatively {{verify the}} {{performance}} of the proposed method. In this simulation, we verify the effectiveness of our <b>kernel</b> <b>functions.</b> In the proposed method, we define two types of <b>kernel</b> <b>functions,</b> LCSS <b>kernel</b> and spectrum intersection kernel, for human motions and music pieces. Thus, we experimentally compare our two newly defined <b>kernel</b> <b>functions.</b> Using combinations of the <b>kernel</b> <b>functions,</b> we prepared four simulations [...] "Simulation 1 "-"Simulation 4 ", as follows: [...]...|$|R
40|$|A {{comparative}} study of <b>kernel</b> <b>functions</b> for primal-dual interior-point algorithms in linear optimization. (English summary) SIAM J. Optim. 15 (2004), no. 1, 101 – 128 (electronic). This paper studies {{a new class of}} <b>kernel</b> <b>functions,</b> each of which forms a building block of an interior-point method. More specifically, seven <b>kernel</b> <b>functions</b> are proposed and analyzed. The authors establish complexity estimates for the so-called small-update and large-update methods for each of these functions...|$|R
2500|$|In this formula, [...] {{stands for}} gravity anomalies, {{differences}} between true and normal (reference) gravity, and S is the Stokes function, a <b>kernel</b> <b>function</b> derived by Stokes in closed analytical form.|$|E
2500|$|Suppose {{now that}} we would like to learn a {{nonlinear}} classification rule which corresponds to a linear classification rule for the transformed data points [...] Moreover, we are given a <b>kernel</b> <b>function</b> [...] which satisfies [...]|$|E
2500|$|Another related {{concept is}} the {{representation}} of probability distributions as elements of a reproducing kernel Hilbert space via the kernel embedding of distributions. [...] This framework {{may be viewed as}} a generalization of the characteristic function under specific choices of the <b>kernel</b> <b>function.</b>|$|E
3000|$|... -HLCP. In Section  3 we {{introduce}} new classes of eligible <b>kernel</b> <b>functions</b> and their technical properties. Finally, we derive {{the framework for}} analyzing the iteration bounds and the complexity results of the algorithms based on these <b>kernel</b> <b>functions</b> in Section  4.|$|R
40|$|Abstract. A {{computational}} model of cognitive inductive reasoning {{that accounts for}} risk context effects is proposed. The model {{is based on a}} Support Vector Machine (SVM) that utilizes the <b>kernel</b> method. <b>Kernel</b> <b>functions</b> within the model are assumed to represent the functions of similarity computations based on distances between premise entities and conclusion entities in inductive reasoning arguments. Multipliers related to the <b>kernel</b> <b>functions</b> have the role of adjusting similarities and can explain rating shifts between two different risk contexts. Model fitting data supports the SVM-based model with <b>kernel</b> <b>functions</b> as a model of inductive reasoning in risk contexts. Finally, the paper discusses how the multipliers for <b>kernel</b> <b>functions</b> provide a satisfactory cognitive theoretical account of similarity adjustment...|$|R
3000|$|Motivated {{by these}} works, we {{introduce}} new classes of eligible <b>kernel</b> <b>functions,</b> which {{are different from}} known <b>kernel</b> <b>functions</b> in [3, 6, 7] and have the exponential power of exponential barrier term, and propose a complexity analysis of the IPMs for [...]...|$|R
2500|$|The {{method of}} invoking the <b>kernel</b> <b>function</b> varies from kernel to kernel. If memory {{isolation}} is in use, {{it is impossible}} for a user process to call the kernel directly, because that would be a violation of the processor's access control rules. A few possibilities are: ...|$|E
2500|$|In {{statistics}} and machine learning, some models have loss functions of a form {{similar to that}} of the gravitational potential: a sum of kernel functions over all pairs of objects, where the <b>kernel</b> <b>function</b> depends on the distance between the objects in parameter space. Example problems that fit into this form include all-nearest-neighbors in manifold learning, kernel density estimation, and kernel machines. Alternative optimizations to reduce the [...] time complexity to [...] have been developed, such as dual tree algorithms, that have applicability to the gravitational -body problem as well.|$|E
2500|$|Digital Rights Management in the Apple-Intel {{architecture}} is accomplished via the Dont Steal Mac OS X.kext, {{sometimes referred to}} as DSMOS or DSMOSX, a file present in Intel-capable versions of the Mac OS X operating system. [...] Its presence enforces a form of Digital Rights Management, preventing Mac OS X being installed on stock PCs. The name of the kext is a reference to the Mac OS X license conditions, which allow installation on Apple hardware only. According to Apple, anything else is [...] stealing Mac OS X. The kext is located at /System/Library/Extensions on the volume containing the operating system. The extension contains a <b>kernel</b> <b>function</b> called page_transform (...) which performs AES decryption of [...] "apple-protected" [...] programs. A system lacking a proper key {{will not be able to}} run the Apple-restricted binaries, which include Dock, Finder, loginwindow, SystemUIServer, mds, ATSServer, backupd, fontd, translate, or translated.|$|E
3000|$|Motivated {{by their}} {{exciting}} pioneering works [12 – 17], in this paper, {{we aim to}} study the existence and uniqueness of the wave front solutions of IDE (1.1) with more general <b>kernel</b> <b>functions.</b> The main idea in this paper is employing the speed index functions (the main idea in [12, 17] and other pioneering works) and the principle of linear superposition. It {{is easy to see}} that the <b>kernel</b> <b>functions</b> that were studied before are included in our study. For example, if the <b>kernel</b> <b>functions</b> [...]...|$|R
40|$|We {{introduce}} a new family of positive-definite <b>kernel</b> <b>functions</b> that mimic the computation in large, multilayer neural nets. These <b>kernel</b> <b>functions</b> {{can be used in}} shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these <b>kernel</b> <b>functions</b> on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets. ...|$|R
40|$|In this paper, {{we study}} the {{accuracy}} of Irving-Kirkwood type of formulas for the approximation of continuum quantities from atomistic simulations. Such formulas are derived by expressing the displacement, deformation gradient and stress in terms of certain <b>kernel</b> <b>functions.</b> We propose two criteria for choosing the <b>kernel</b> <b>functions</b> to significantly improve the sampling accuracy. We present a simple procedure to construct <b>kernel</b> <b>functions</b> that meet these criteria. Further, numerical tests on homogeneous and non-homogeneous systems provide validations for our analysis. Comment: 16 pages, 8 figure...|$|R
2500|$|Whereas the {{original}} {{problem may be}} stated in a finite dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that {{the original}} finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily {{in terms of the}} variables in the original space, by defining them in terms of a <b>kernel</b> <b>function</b> [...] selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters [...] of images of feature vectors [...] that occur in the data base. With this choice of a hyperplane, the points [...] in the feature space that are mapped into the hyperplane are defined by the relation: [...] Note that if [...] becomes small as [...] grows further away from , each term in the sum measures the degree of closeness of the test point [...] to the corresponding data base point [...] In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in {{one or the other of}} the sets to be discriminated. Note the fact that the set of points [...] mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets which are not convex at all in the original space.|$|E
5000|$|The <b>kernel</b> <b>function</b> is scale-invariant. Equally scaling all {{elements}} of the sample [...] does not alter {{the values of the}} <b>kernel</b> <b>function.</b>|$|E
5000|$|For 2-D and {{axisymmetric}} configurations, the <b>kernel</b> <b>function</b> can be analytically integrated {{along the}} z or θ direction. The {{integration of the}} <b>kernel</b> <b>function</b> is ...|$|E
40|$|We obtain <b>kernel</b> <b>functions</b> {{associated}} with the quantum relativistic Toda systems, both for the periodic version and for the nonperiodic version with its dual. This involves taking limits of previously known results concerning <b>kernel</b> <b>functions</b> for the elliptic and hyperbolic relativistic Calogero-Moser systems. We show that the special <b>kernel</b> <b>functions</b> at issue admit a limit that yields generating functions of Bäcklund transformations for the classical relativistic Calogero-Moser and Toda systems. We also obtain the nonrelativistic counterparts of our results, which tie in with previous results in the literature. Comment: 76 page...|$|R
40|$|An integral-differential model {{equation}} {{arising from}} neuronal networks with very general <b>kernel</b> <b>functions</b> is considered in this paper. The <b>kernel</b> <b>functions</b> we study here include pure excitations, lateral inhibition, lateral excitations, and more general synaptic couplings (e. g., oscillating <b>kernel</b> <b>functions).</b> The main {{goal of this}} paper is to prove the existence and uniqueness of the traveling wave front solutions. The main idea we apply here is to reduce the nonlinear integral-differential equation into a solvable differential equation and test whether the solution we get is really a wave front solution of the model equation...|$|R
40|$|In this paper, we {{introduce}} {{the class of}} semi-separable <b>kernel</b> <b>functions</b> for use in constructing Lyapunov functions for distributed-parameter systems such as delaydifferential equations. We then consider the subset of semiseparable <b>kernel</b> <b>functions</b> defined by polynomials. We show that the set of such kernels which define positive forms can be parameterized by positive semidefinite matrices. In the particular case of linear time-delay systems, we show how to construct the derivative of Lyapunov functions defined by piecewise continuous semi-separable kernels and give numerical examples which illustrate some advantages over standard polynomial <b>kernel</b> <b>functions.</b> © 2008 IEEE...|$|R
5000|$|The <b>kernel</b> <b>function</b> is location-invariant. If we add or {{subtract}} {{any value}} to each {{element of the}} sample , the corresponding values of the <b>kernel</b> <b>function</b> do not change.|$|E
50|$|Kernel {{functions}} {{provide a}} way to manipulate data {{as though it were}} projected into a higher dimensional space, by operating on it in its original space. So that data in higher-dimensional space become more easily separable. <b>Kernel</b> <b>function</b> is also used in integral equation for surface radiation exchanges. <b>Kernel</b> <b>function</b> relates to both the geometry of the enclosure and its surface properties. <b>Kernel</b> <b>function</b> depends on geometry of the body.|$|E
5000|$|As we can {{see from}} the figure above, the <b>kernel</b> <b>function</b> indeed {{suppress}} the interference which is away from the origin, but for the cross-term locates on the [...] and [...] axes, this <b>kernel</b> <b>function</b> can do nothing about it.|$|E
40|$|A common {{approach}} in structural pattern classification is {{to define a}} dissimilarity measure on patterns and apply a distance-based nearest-neighbor classifier. In this paper, we introduce an alternative method for classification using <b>kernel</b> <b>functions</b> based on edit distance. The proposed approach is applicable to both string and graph representations of patterns. By means of the <b>kernel</b> <b>functions</b> introduced in this paper, string and graph classification can be performed in an implicit vector space using powerful statistical algorithms. The validity of the kernel method cannot be established for edit distance in general. However, by evaluating theoretical criteria we show that the <b>kernel</b> <b>functions</b> are nevertheless suitable for classification, and experiments on various string and graph datasets clearly demonstrate that nearest-neighbor classifiers can be outperformed by support vector machines using the proposed <b>kernel</b> <b>functions...</b>|$|R
40|$|Kernel-based methods first {{appeared}} in the form of support vector machines. Since the first Support Vector Machine (SVM) formulation in 1995, we have seen how the number of proposed <b>kernel</b> <b>functions</b> has quickly grown, and how these kernels have approached a wide range of problems and domains. The most common and direct applications of these methods are focused on continuous numeric data, given that SVMs at the end involves the solution of an optimization problem. Additionally, some <b>kernel</b> <b>functions</b> have been oriented to more symbolic data, in problems like text analysis, or hand-written digits recognition. But surprisingly, there is a gap in the area of <b>kernel</b> <b>functions</b> devoted to handle datasets with qualitative variables. One of the most common practices to overcome this lack consists on recoding the source qualitative information, making them suitable for applying numeric <b>kernel</b> <b>functions.</b> This thesis presents the development of new <b>kernel</b> <b>functions</b> that can better model symbolic information presented as categorical variables, in a direct way, and without the need of data preprocessing methods. The proposition is based on the use of probabilistic information (probability mass distribution) to compare the different modalities of a variable. Additionally, the idea is formulated through a modular schema, combining a set of components to obtain the <b>kernel</b> <b>functions,</b> facilitating the modification and extension of single components. The experimental results suggest an slightly improvement with respect to traditional <b>kernel</b> <b>functions,</b> in the accuracy obtained on classification problems. This progress is clearer on datasets with known probabilistic structure...|$|R
5000|$|Theta <b>functions,</b> <b>kernel</b> <b>functions</b> and abelian integrals, AMS 1972 ...|$|R
5000|$|More generally, the <b>kernel</b> <b>function</b> has the {{following}} properties ...|$|E
50|$|There exist {{a unique}} {{solution}} for a chosen <b>kernel</b> <b>function.</b>|$|E
5000|$|<b>Kernel</b> <b>function</b> {{for solving}} {{integral}} equation of surface radiation exchanges ...|$|E
40|$|This Research {{presents}} {{the effects of}} interaction between various <b>Kernel</b> <b>functions</b> and different Feature Selection Techniques for improving the learning capability of Support Vector Machine (SVM) in detecting email spams. The interaction of four <b>Kernel</b> <b>functions</b> of SVM i. e. “Normalised Polynomial Kernel (NP) ”, “Polynomial Kernel (PK) ”, “Radial Basis <b>Function</b> <b>Kernel</b> (RBF) ”, and “Pearson VII Function-Based Universal Kernel (PUK) ” with three feature selectio...|$|R
30|$|We obtain {{some useful}} {{reproducing}} <b>kernel</b> <b>functions</b> in this section.|$|R
30|$|The {{first class}} {{consists}} of nonnegative <b>kernel</b> <b>functions</b> (pure excitation).|$|R
