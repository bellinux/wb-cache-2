128|3525|Public
25|$|The {{definition}} of range is further confounded by how the total realized range size is measured. There {{are two types}} of measurements commonly in use, the extent of occurrence (EOO) and the area of occupancy (AOO) (see also the Scaling pattern of occupancy). The EOO can best be thought of as the minimum convex polygon encompassing all known normal occurrences of a particular species, and is the measure of range most commonly found in field guides. The AOO is the subset of the EOO where the species actually occurs. In essence, the AOO acknowledges that there are holes in the distribution of a species within its EOO, and attempts to correct for these vacancies. A common way to describe the AOO of a species is to divide the study region into a matrix of cells and record if the species is present in or absent from each cell. For example, in describing O–A relationships for common British birds, Quinn et al. found that the occupancy at the <b>finest</b> <b>resolution</b> (10 x 10km squares) best explained abundance patterns. In a similar manner, Zuckerberg et al. used Breeding Bird Atlas data measured on cells 5×5km to describe breeding bird occupancy in New York State.|$|E
50|$|Annually {{laminated}} deposits or varves are rhythmites {{with annual}} periodicity: annual layers of sediment or sedimentary rock are laid down through seasonal variations {{that result from}} precipitation, or from temperature, which influences precipitation rates and debris loads in runoff. Of the many rhythmites found in the geological record, varves {{are among the most}} important and illuminating to studies of past climate change. Varves are amongst the <b>finest</b> <b>resolution</b> events easily recognised in stratigraphy.|$|E
5000|$|In a {{marketing}} brochure Spansion has {{claimed that the}} processing cost of a MirrorBit NOR flash wafer is {{lower than that of}} a conventional floating gate wafer since there are 10% fewer photolithography mask steps, and 40% fewer [...] "critical" [...] steps (those requiring the <b>finest</b> <b>resolution,</b> and therefore the most expensive photolithographic equipment.)Infineon's marketing materials showed that 15% fewer mask steps were required to make charge trapping NAND flash than to manufacture the equivalent floating gate product.|$|E
5000|$|Improved PWM {{control line}} means <b>finer</b> <b>resolution</b> for {{movement}} control.|$|R
40|$|In wavelet based {{electron}} structure calculations {{introducing a}} new, <b>finer</b> <b>resolution</b> level is usually an expensive task, {{this is why}} often a two-level approximation is used with very <b>fine</b> starting <b>resolution</b> level. This process results in large matrices to calculate with {{and a large number}} of coefficients to be stored. In our previous work we have developed an adaptively refining solution scheme that determines the indices, where refined basis functions are to be included, and later a method for predicting the next, <b>finer</b> <b>resolution</b> coefficients in a very economic way. In the present contribution we would like to determine, whether the method can be applied for predicting not only the first, but also the other, higher resolution level coefficients. Also the energy expectation values of the predicted wave functions are studied, as well as the scaling behaviour of the coefficients in the <b>fine</b> <b>resolution</b> limit...|$|R
40|$|Recovering {{method of}} missing data {{based on the}} {{proposed}} modified Kalman filter for {{the case that the}} time series of mean data is know is proposed. There are some cases of which although a portion of data is missing, mean value of the time series of data is known. For instance, although coarse resolution of imagery data are acquired every day, <b>fine</b> <b>resolution</b> of imagery data are missing sometimes. In other words, coarse resolution of imaging sensor has wide swath width while <b>fine</b> <b>resolution</b> of imaging sensor has narrow swath, in general. Therefore, coarse resolution of sensor data can be acquired every day while <b>fine</b> <b>resolution</b> of sensor data can be acquired not so frequently. It would be nice to become able to create frequently acquired <b>fine</b> <b>resolution</b> of sensor data (every day) using the previously acquired <b>fine</b> <b>resolution</b> of sensor data together with the coarse resolution of sensor data. The proposed method allows creation of <b>fine</b> <b>resolution</b> sensor data with the aforementioned method based on a modified Kalman filter. As an example of the proposed method, prediction of missing ASTER/VNIR data based on Kalman filter using simultaneously acquired MODIS data as a mean value of time series data in revision of filter status is attempted together with a comparative study of prediction errors for both conventional Kalman filter and the proposed modified Kalman filter which utilizes mean value of time series data derived from the other sources. Experimental data shows that 4 to 111 % of prediction error reduction can be achieved by the proposed modified Kalman filter in comparison to the conventional Kalman filter. It is found that the reduction rate depends on the mean value accuracy of time series data derived from the other data sources. The experimental results with remote sensing satellite imagery data show a validity of the proposed metho...|$|R
50|$|Wind flow {{modeling}} methods, {{described in}} the following section, named 'Wind flow modeling', provide insights into very high-resolution wind flow behavior, often, at horizontal resolution finer than 100-m. Because of such <b>finest</b> <b>resolution</b> computational fluid dynamics (CFD) modeling application, the typical model domains used by these small-scale models have a few kilometers in the horizontal direction and several hundred meters in the vertical direction. The above-mentioned model domain limitations by small-scale CFD models, {{need to be addressed}} and are often addressed by atmospheric CFD models, that could cover horizontal model domains on the order of hundreds of kilometers and vertical domain depths of tens of kilometers. In other words, any atmospheric processes that occur within such large-scale atmospheric model domains, that will influence site-specific wind and its temporal variation should be captured for successful wind resource assessment. This class of atmospheric CFD models and their contribution has not been fully explored and adopted yet, although the aforementioned atmospheric influences captured by such models are highly relevant and critical for the overall wind resource assessment efforts. There are a few such atmospheric CFD models being applied for wind resource assessments today.|$|E
50|$|The {{definition}} of range is further confounded by how the total realized range size is measured. There {{are two types}} of measurements commonly in use, the extent of occurrence (EOO) and the area of occupancy (AOO) (see also the Scaling pattern of occupancy). The EOO can best be thought of as the minimum convex polygon encompassing all known normal occurrences of a particular species, and is the measure of range most commonly found in field guides. The AOO is the subset of the EOO where the species actually occurs. In essence, the AOO acknowledges that there are holes in the distribution of a species within its EOO, and attempts to correct for these vacancies. A common way to describe the AOO of a species is to divide the study region into a matrix of cells and record if the species is present in or absent from each cell. For example, in describing O-A relationships for common British birds, Quinn et al. found that the occupancy at the <b>finest</b> <b>resolution</b> (10 x 10 km squares) best explained abundance patterns. In a similar manner, Zuckerberg et al. used Breeding Bird Atlas data measured on cells 5 × 5 km to describe breeding bird occupancy in New York State.|$|E
3000|$|... k= 129, 513, 2049, 8193, 32, 719 are {{the leading}} indices from {{resolution}} level (L −  1) to level 1 (<b>finest</b> <b>resolution</b> level). There is {{a total of five}} combined level of arrangement in eight MRI slices, where each of 128  ×  128 resolution.|$|E
40|$|This paper {{presents}} {{a framework for}} using real-time big-data to inform a transport Agent Based Model (ABM) {{for a range of}} scenario testing applications. Computational advances have enabled for increasingly complex, bottom-up, <b>fine</b> <b>resolution</b> simulations to be carried out over long time horizons at fine spatial and temporal resolution. This has hinted at the possibility of connecting scales of what has been historically been <b>fine</b> <b>resolution</b> operational models and coarse resolution strategic models. The value of any <b>fine</b> <b>resolution</b> dynamic model is limited by the quality of its inputs. The wave of new geospatially connected devices has enabled the harvesting of <b>fine</b> <b>resolution</b> spatial and temporal data on travellers’ and even the infrastructure itself. This crowd-sourced data can be used to inform dynamic models with real-world and real-time data, bypassing the need for generalised functions and/or expensive survey data. In this paper, Google Directions API data and Transport for London data feeds are presented in a framework for London. The use of decentralised data structures is also presented and comment is made on the possibilities of using parallel computing advances in Computer Science to scaling up <b>fine</b> <b>resolution</b> scenario testing transportation models and enabling support for a range of agent decision making methodologies. Such data structures offer performance improvements in the storing of dynamic data that may be manipulated in order to simulate local and global hard infrastructure scenarios alone or in tandem with traditional policy or dynamic policy making scenarios...|$|R
40|$|Super-{{resolution}} mapping (SRM) is {{a method}} to produce a <b>fine</b> spatial <b>resolution</b> land cover map from coarse spatial resolution remotely sensed imagery. A popular approach for SRM is a two-step algorithm, which first increases the spatial resolution of coarse fraction images by interpolation, and then determines class labels of <b>fine</b> <b>resolution</b> pixels using the maximum a posteriori (MAP) principle. By constructing a new image formation process that establishes the relationship between observed coarse resolution fraction images and the latent <b>fine</b> <b>resolution</b> land cover map, {{it is found that}} the MAP principle only matches with area-to-point interpolation algorithms, and should be replaced by de-convolution if an area-to-area interpolation algorithm is to be applied. A novel iterative interpolation de-convolution (IID) SRM algorithm is proposed. The IID algorithm first interpolates coarse resolution fraction images with an area-to-area interpolation algorithm, and produces an initial <b>fine</b> <b>resolution</b> land cover map by de-convolution. The <b>fine</b> spatial <b>resolution</b> land cover map is then updated by re-convolution, back-projection and de-convolution iteratively until the final result is produced. The IID algorithm was evaluated with simulated shapes, simulated multi-spectral images, and degraded Landsat images, including comparison against three widely used SRM algorithms: pixel swapping, bilinear interpolation, and Hopfield neural network. Results show that the IID algorithm can reduce the impact of fraction errors, and can preserve the patch continuity and the patch boundary smoothness, simultaneously. Moreover, the IID algorithm produced <b>fine</b> <b>resolution</b> land cover maps with higher accuracies than those produced by other SRM algorithms...|$|R
40|$|To perform <b>fine</b> <b>resolution</b> SAR imaging, {{the radar}} is {{required}} to image the area of interest {{for a long period}} of time. In this paper, the signal processing aspects of processing such SAR data is described. The signal processing challenges are to estimate the large motion errors contributed by the long imaging time and to limit the phase errors due to approximation in the signal modeling for <b>fine</b> <b>resolution</b> image formation. This paper presents the implementation of a sub-spot Polar Format technique with autofocus to process data collected with long integration time. The sub-spot technique developed produced sub-spot images of the radar footprint. The full SAR image is formed by mosaic the individual sub-spots. This technique allows the formation of a large scene image at <b>fine</b> <b>resolution</b> while utilizing th...|$|R
40|$|In wavelet deconvolution, the <b>finest</b> <b>resolution</b> {{level is}} a key {{parameter}} which needs to be chosen carefully. In this paper a data-driven method is presented that selects the <b>finest</b> <b>resolution</b> level using a blockwise thresholding method in the Fourier domain. In particular, we present a method that applies to the general multichannel model whereby a practitioner observes many box-car convolutions of a signal of interest (with possible different levels of box-car 'blur') with additive long memory noise. The box-car functions governing the blur are assumed to have Badly Approximable (BA) width. To {{the best of the}} author's knowledge, no automatic fine resolution selection method exists for the box-car wavelet deconvolution paradigm. We present a method that selects the optimal level that is adaptive to box-car width and noise levels and conduct a short numerical study to supplement the findings. 9 page(s...|$|E
40|$|The wavelet {{deconvolution}} method WaveD using band-limited wavelets offers both {{theoretical and}} computational advantages over traditional compactly supported wavelets. The translation-invariant WaveD with a fast algorithm improves further. The twofold cross-validation method for choosing the threshold parameter and the <b>finest</b> <b>resolution</b> level in WaveD is introduced. The algorithm’s performance is {{compared with the}} fixed constant tuning and the default tuning in WaveD...|$|E
40|$|Wavelet {{deconvolution}} in {{a periodic}} setting using cross-validation Abstract — Wavelet deconvolution method WaveD using bandlimited wavelets offer both theoretical and computational advantage over traditional compactly-supported wavelets. The translation invariant WaveD with fast algorithm improves further. The twofold cross-validation method for choosing the threshold parameter and the <b>finest</b> <b>resolution</b> level in WaveD is introduced. The algorithm’s performance is {{compared with the}} fixed constant tuning and the default tuning in WaveD. Index Terms — Wavelet deconvolution, cross-validation. I...|$|E
40|$|Moderate Resolution Imaging Spectroradiometer (MODIS) {{data are}} {{effective}} and efficient for monitoring urban dynamics such as urban cover change and thermal anomalies, but the spatial resolution provided by MODIS data is 500 m (for most of its shorter spectral bands), which results in difficulty in detecting subtle spatial variations within a coarse pixel—especially for a fast-growing city. Given that the historical land use/cover products and satellite data at <b>finer</b> <b>resolution</b> are valuable to reflect the urban dynamics with more spatial details, <b>finer</b> spatial <b>resolution</b> images, as well as land cover products at previous times, are exploited in this study to improve the change detection capability of coarse resolution satellite data. The proposed approach involves two main steps. First, pairs of coarse and <b>finer</b> <b>resolution</b> satellite data at previous times are learned and then applied to generate synthetic satellite data with <b>finer</b> spatial <b>resolution</b> from coarse resolution satellite data. Second, a land cover map was produced at a <b>finer</b> spatial <b>resolution</b> and adjusted with the obtained synthetic satellite data and prior land cover maps. The approach was tested for generating <b>finer</b> <b>resolution</b> synthetic Landsat images using MODIS data from the Guangzhou study area. The <b>finer</b> <b>resolution</b> Landsat-like data were then applied to detect land cover changes with more spatial details. Test {{results show that the}} change detection accuracy using the proposed approach with the synthetic Landsat data is much better than the results using the original MODIS data or conventional spatial and temporal fusion-based approaches. The proposed approach is beneficial for detecting subtle urban land cover changes with more spatial details when multitemporal coarse satellite data are available...|$|R
50|$|Current ANSI {{identification}} {{is based on}} ecoregions and ecodistricts, and <b>finer</b> <b>resolution</b> is used to determine particular features and areas, such as a landform.|$|R
40|$|An {{adaptive}} mesh {{solute transport}} model is presented. The adaptive mesh scheme is implemented in a one-way multiply nested {{version of the}} hydrobiological model DIVAST (Depth Integrated Velocity and Solute Transport). The scheme allows the inner meshes of <b>finer</b> <b>resolution</b> to follow physical features such as solute plumes, thereby minimising the <b>fine</b> <b>resolution</b> coverage and thus the computational cost. If a physical feature moves {{during the course of}} a simulation, the <b>fine</b> <b>resolution</b> meshes move with it. Mesh movement can be either specified or automatic. The model was tested using a model of Galway Bay to simulate the discharge of a conservative tracer from a wastewater treatment plant. Results show that the model is capable of predicting solute transport to a high degree of accuracy and that adaptive meshing provides an efficient alternative to the classical zoom nesting techniques...|$|R
40|$|This paper {{deals with}} phase {{preserving}} focusing for very low resolution ScanSAR. Conventional techniques get ScanSAR focusing by exploiting the SAR matched reference, and compensate scalloping by an inverse antenna weighting. Yet, this approach introduces a space-variant distortion in the focused impulse response (IRF). A rather different focusing technique is then proposed, where {{the set of}} space-variant focusing kernels is computed by means of Wiener deconvolution. They perform ScanSAR focusing and descalloping at one time, achieving the <b>finest</b> <b>resolution</b> and without distorting the impulse respons...|$|E
40|$|AbstractBacteriophage T 4 was imaged by atomic force {{microscopy}} {{with the}} <b>finest</b> <b>resolution</b> to date with a clear image of tail fibers of an estimated diameter of 2 – 3 nm. T 4 phages were spread on a clean surface of silicon wafer and dried under air before observation with an atomic force microscope. The head, tail and tail fibers were routinely imaged with somewhat distorted dimensions. The ease of imaging isolated phage particles with a good resolution raised our expectation for the further use of AFM in biomedical applications...|$|E
40|$|Abstract. We have {{performed}} high resolution 3 D simulations of the Local Bubble (with 1. 25 pc <b>finest</b> <b>resolution)</b> in a realistic background ISM, jointly with the dynamical {{evolution of the}} neighbouring Loop I superbubble. We can reproduce (i) {{the size of the}} bubbles (in contrast to similarity solutions), (ii) the interaction shell with Loop I, discovered with ROSAT, (iii) predict the merging of the two bubbles in about 3 Myr, when the interaction shell starts to fragment, and, (iv) the generation of blobs like the Local Cloud as a consequence of a dynamical instability...|$|E
40|$|Estimation of {{evapotranspiration}} (ET) from {{remote sensing}} based energy balance models have evolved as a promising {{tool in the}} field of water resources management. Performance of energy balance models and reliability of ET estimates is decided by the availability of remote sensing data at high spatial and temporal resolutions. However huge tradeoff in the spatial and temporal resolution of satellite images act as major constraints in deriving ET at fine spatial and temporal resolution using remote sensing based energy balance models. Hence a need exists to derive <b>finer</b> <b>resolution</b> data from the available coarse resolution imagery, which could be applied to deliver ET estimates at scales to the range of individual fields. The current study employed a spatio-temporal disaggregation method to derive <b>fine</b> spatial <b>resolution</b> (60 m) images of NDVI by integrating the information in terms of crop phenology derived from time series of MODIS NDVI composites with <b>fine</b> <b>resolution</b> NDVI derived from a single AWiFS data acquired during the season. The disaggregated images of NDVI at <b>fine</b> <b>resolution</b> were used to disaggregate MODIS LST data at 960 m resolution to the scale of Landsat LST data at 60 m resolution. The robustness of the algorithm was verified by comparison of the disaggregated NDVI and LST with concurrent NDVI and LST images derived from Landsat ETM+. The results showed that disaggregated NDVI and LST images compared well with the concurrent NDVI and LST derived from ETM+ at <b>fine</b> <b>resolution</b> with a high Nash Sutcliffe Efficiency and low Root Mean Square Error. The proposed disaggregation method proves promising in generating time series of ET at <b>fine</b> <b>resolution</b> for effective water management. * Corresponding author. This is useful to know for communication with the appropriate person in cases with more than one author...|$|R
40|$|Abstract—Recovering {{method of}} missing data {{based on the}} {{proposed}} modified Kalman filter for {{the case that the}} time series of mean data is know is proposed. There are some cases of which although a portion of data is missing, mean value of the time series of data is known. For instance, although coarse resolution of imagery data are acquired every day, <b>fine</b> <b>resolution</b> of imagery data are missing sometimes. In other words, coarse resolution of imaging sensor has wide swath width while <b>fine</b> <b>resolution</b> of imaging sensor has narrow swath, in general. Therefore, coarse resolution of sensor data can be acquired every day while <b>fine</b> <b>resolution</b> of sensor data can be acquired not so frequently. It would be nice to become able to create frequently acquired <b>fine</b> <b>resolution</b> of sensor data (every day) using the previously acquired <b>fine</b> <b>resolution</b> of sensor data together with the coarse resolution of sensor data. The proposed method allows creation of <b>fine</b> <b>resolution</b> sensor data with the aforementioned method based on a modified Kalman filter. As an example of the proposed method, prediction of missing ASTER/VNIR data based on Kalman filter using simultaneously acquired MODIS data as a mean value of time series data in revision of filter status is attempted together with a comparative study of prediction errors for both conventional Kalman filter and the proposed modified Kalman filter which utilizes mean value of time series data derived from the other sources. Experimental data shows that 4 to 111 % of prediction error reduction can be achieved by the proposed modified Kalman filter in comparison to the conventional Kalman filter. It is found that the reduction rate depends on the mean value accuracy of time series data derived from the other data sources. The experimental results with remote sensing satellite imagery data show a validity of the proposed method Keywords—Kalman filter; nremote sensing satellite image; time series analysis I...|$|R
50|$|The Haar {{transform}} can {{be thought}} of as a sampling process in which rows of the transformation matrix act as samples of <b>finer</b> and <b>finer</b> <b>resolution.</b>|$|R
40|$|An {{implicit}} structured-adaptive-mesh-refinement (SAMR) solver for 2 D reduced magnetohydrodynamics (MHD) is described. The time-implicit discretization {{is able to}} {{step over}} fast normal modes, while the spatial adaptivity resolves thin, dynamically evolving features. A Jacobian-free Newton-Krylov method {{is used for the}} nonlinear solver engine. For preconditioning, we have extended the optimal “physics-based ” approach developed in [11] (which employed multigrid solver technology in the preconditioner for scalability) to SAMR grids using the well-known Fast Adaptive Composite grid (FAC) method [35]. A grid convergence study demonstrates that the solver performance is independent of the number of grid levels and only depends of the <b>finest</b> <b>resolution</b> considered, and that it scales well with grid refinement. The study of error generation and propagation in our SAMR implementation demonstrates that high-order (cubic) interpolation during regridding, combined with a robustly damping second-order temporal scheme such as BDF 2, is required to minimize impact of grid errors at coarse fine interfaces on the overall error of the computation for this MHD application. We also demonstrate that our implementation features the desired property that the overall numerical error level is dependent only on the <b>finest</b> <b>resolution</b> level considered, and not on the base-grid resolution or on the number of refinement levels present during the simulation. We demonstrate the effectiveness of the tool on several challenging problems. Key words: Adaptive mesh refinement, Newton-Krylov, implicit methods, magnetohydrodynamics, multilevel solver...|$|E
30|$|The {{population}} totals {{and demographic}} distributions were first derived using Oak Ridge National Labs LandScan 2008 population data (ORNL 2008) and Venezuela census 2011 (INE 2011) data that {{were assigned to}} each census tract by following the procedure suggested by Hansen et al. (2010). LandScan is the <b>finest</b> <b>resolution</b> global population distribution dataset containing granular population data down to the square kilometer level. It {{was developed by the}} Oak Ridge National Laboratory (ORNL) for the US Department of Defense. The LandScan algorithm uses spatial data and imagery analysis technologies and a multivariable dasymetric modeling approach to disaggregate census counts within an administrative boundary (ORNL 2008).|$|E
40|$|For plane-wave and many-spiral {{states of}} the {{experimentally}} based Luo-Rudy 1 model of heart tissue in large (8 cm square) domains, we show that an explicit space-time-adaptive time-integration algorithm can achieve {{an order of magnitude}} reduction in computational effort and memory - but without a reduction in accuracy - when compared to an algorithm using a uniform space-time mesh at the <b>finest</b> <b>resolution.</b> Our results indicate that such an explicit algorithm can be extended straightforwardly to simulate quantitatively large-scale three-dimensional electrical dynamics over the whole human heart. Comment: 4 pages (LaTeX), 3 color PostScript figure files, submitted to PRL; Changed content of one paragraph; Correcting corrupt fig 2...|$|E
40|$|For two centuries, Koch’s postulates {{have set}} the gold {{standard}} for establishing the microbiological etiology of infection and disease. Genomic sequencing now brings <b>finer</b> <b>resolution</b> to both bacterial strain variation and the host genetic state that may predispose to disease. In this issue of the JID, Fitz-Gibbons and colleagues present strain-based resolution of Propionibacterium acnes and its association with the common teenage malady acne vulgaris. Here I examine how Koch’s postulates were envisioned and incorporate this <b>finer</b> <b>resolution</b> of both host and microbial states...|$|R
3000|$|... [...]. Note, {{that the}} {{interpolation}} and averaging approximation strategies have the disadvantage {{that they are}} qualitatively different. Thus, momentum and energy conservation are only ensured for very <b>fine</b> <b>resolutions.</b>|$|R
3000|$|... {{than the}} one {{provided}} in Theorem 1.3 and therefore, a <b>finer</b> <b>resolution</b> can be achieved in practice. Additional theoretical investigations are necessary to refine the value of C [...]...|$|R
40|$|We {{present an}} {{efficient}} multi-resolution approach to segment a 3 D point cloud into planar components. In {{order to gain}} efficiency, we process large point clouds iteratively from coarse to fine 3 D resolutions: At each resolution, we rapidly extract surface normals to describe surface elements (surfels). We group surfels that cannot be associated with planes from coarser resolutions into co-planar clusters with the Hough transform. We then extract connected components on these clusters and determine a best plane fit through RANSAC. Finally, we merge plane segments and refine the segmentation on the <b>finest</b> <b>resolution.</b> In experiments, we demonstrate the efficiency and quality of our method and compare it to other state-of-the-art approaches...|$|E
40|$|Abstract—In EBCOT, {{the context}} {{modeling}} process takes excessive calculation time and this paper proposed {{a method to}} reduce this calculation time. That is, if the <b>finest</b> <b>resolution</b> coefficient {{is less than a}} pre-defined transfer factor the coefficient and its descendents skip the context modeling process. There is a trade-off relationship between the calculation time and the image quality or the amount of output data such that as this threshold value increases, the calculation time and the amount of output data decreases, but the image degradation increases. The experimental results showed that in this range the resulting reduction rate in calculation time was from 3 % to 64 % in average, the reduction rate in output data was from 32 % to 73 % in average Index Terms—JPEG 2000, EBCOT, context modeling...|$|E
40|$|In this paper, we {{formulate}} {{and develop}} an approach which integrates different modules (feature extractor, matching and interpolation) involved in stereo. We study the integration {{process at the}} <b>finest</b> <b>resolution</b> when (i) the precomputed edge map is the only line filed driving the model, (ii) the line field are computed interactively by the feature extracting module of the model, and (iii) when both the interactive line field computation module and the precomputed line field modules are present. The integration process being computationally intensive, we develop a multiresolution stereo integration approach. The energy function for each module at different resolutions is constructed and minimized in an integrated manner yielding a dense disparity map. A new energy function for the matching module is proposed. Experimental results are presented to illustrate our approach...|$|E
40|$|We here {{concentrate}} on available π^± and K^± ALICE preliminary R_AA data in central 2. 76 TeV Pb+Pb collisions at LHC. These data show an interesting <b>fine</b> <b>resolution</b> hierarchy, i. e. the measured K^± data have consistently lower suppression compared to π^± measurements. We here ask whether theoretical predictions based on energy loss in dynamical QCD medium can quantitatively and qualitatively explain such <b>fine</b> <b>resolution.</b> While our suppression calculations agree {{well with the}} data, we find that qualitatively explaining the fine hierarchy critically depends on the choice of fragmentation functions. While {{the most widely used}} fragmentation functions lead to the reversal of the observed hierarchy, a more recent version correctly reproduce the experimental data. We here point to the reasons behind such discrepancy in the predictions. Our results argue that accuracy of the theoretical predictions reached a point where comparison with <b>fine</b> <b>resolution</b> data at LHC can generate useful understanding. Comment: 4 pages, 3 figure...|$|R
50|$|In film cameras, copy stands are {{traditionally}} used with slide film. The <b>fine</b> <b>resolution</b> of slide film allows the images to be reproduced with high fidelity {{when they are}} projected.|$|R
40|$|Aim: To {{investigate}} {{the effect of}} using <b>fine</b> <b>resolution</b> (200 m) multi-beam bathymetry data to calculate the percentage representation of deep-sea communities within an existing MPA network using predictive habitat modelling methods, therefore identifying how model resolution influences the assessment of existing area closures {{for the protection of}} listed habitats. Location: The NE Atlantic deep sea (UK and Irish extended continental shelf limits). Methods: Predicted habitat distribution maps are built using MaxEnt modelling methods for three listed deep-sea habitats (Scleractinian (Vaughan & Wells, 1943) cold-water coral reef, (SclerReef), Pheronema carpenteri (WyvilleThomson, 1869) aggregations (PcAggs) and Syringammina fragilissima (Brady, 1883) aggregations (SfAggs)) using <b>fine</b> <b>resolution</b> bathymetric data. Pre-selection of environmental variables (bathymetry-derived) by generalized additive modelling is used to select the most ecologically sensible combination to best predict habitat suitability. Models are evaluated using repeated 75 / 25 training/test data partitions using AUC and threshold-dependent assessment methods. The percentage representation of each habitat within an existing MPA network is quantified and compared to that calculated from distribution maps built from coarse resolution (750 m – GEBCO) models. Results: Percentage representation of SclerReef, PcAggs, and SfAggs within the MPA network all increased when modelled using <b>fine</b> <b>resolution</b> bathymetry data. Distinct differences in predicted habitat distribution were observed for all three habitats. Spatial extent for SclerReef and SfAggs decreased with an increase in model resolution, whereas PcAggs total suitable area increased. <b>Fine</b> <b>resolution</b> models for all three habitats are considered ‘excellent’ when assessed using AUC. Main conclusions: MPA assessment based on coarse resolution models result in more conservative percentage estimates. Predictive habitat modelling based on <b>fine</b> <b>resolution</b> data provides more accurate distribution data on which conservation strategy should be established. Coarse resolution data however is fit for the purpose of assessing MPA network effectiveness against policy driven conservation targets, is more readily available, and cheaper...|$|R
