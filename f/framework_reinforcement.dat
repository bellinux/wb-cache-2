9|151|Public
40|$|This paper {{presents}} an adaptive learning model for market-making under the reinforcement learning <b>framework.</b> <b>Reinforcement</b> {{learning is a}} learning technique in which agents aim to maximize the long-term accumulated rewards. No knowledge of the market environment, such as the order arrival or price process, is assumed. Instead, the agent learns from realtime market experience and develops explicit market-making strategies, achieving multiple objectives including the maximizing of profits and minimization of the bid-ask spread. The simulation results show initial success in bringing learning techniques to building marketmaking algorithms...|$|E
40|$|This chapter gives a compact, self{contained {{tutorial}} {{survey of}} reinforcement learn-ing, {{a tool that}} is increasingly nding application {{in the development of}} intelligent dynamic systems. Research on reinforcement learning during the past decade has {{led to the development of}} a variety of useful algorithms. This chapter surveys the literature and presents the algorithms in a cohesive <b>framework.</b> <b>Reinforcement</b> Learning (RL), a term borrowed from animal learning literature by Min-sky (1954, 1961), refers to a class of learning tasks and algorithms in which the learning system learns an associative mapping, : X! A by maximizing a scalar evaluation (re-inforcement) of its performance from the environment (user). Compared to supervise...|$|E
40|$|Although behaviour-based {{robotics}} {{has been}} successfully used to develop autonomous mobile robots {{up to a certain}} point, further progress may require the integration of a learning model into the behaviour-based <b>framework.</b> <b>Reinforcement</b> learning is a natural candidate for this because it seems well suited to the problems faced by autonomous agents. However, previous attempts to use reinforcement learning in behaviour-based mobile robots have been simple combinations of these two methodologies rather than full integrations, and have suffered from severe scaling problems that appear to make them infeasible. Furthermore, the implicit assumptions that form the basis of reinforcement learning theory were not developed with the problems faced by autonomous agents in complex environments in mind...|$|E
50|$|Onderdonk engaged Irishman Michael James Haney, a {{superior}} foreman. Haney saw {{the problem of}} the rivers, gorges, and cliffs and realized that Onderdonk would need extensive bridging. To solve this problem, Haney built a steam-powered sawmill at Haney to cut standardized timbers. Crews also cut bridge bents (<b>framework</b> <b>reinforcements),</b> so the advance crews only had to submit measurements to Heney and he would custom-build a woodwork for that mile marker of track. The prime Douglas Fir of the Fraser Valley provided ideal timber. Lumber for ties was also cut at Texas Creek, and timber was cut at the Chehalis river near Kilby. Other crews were engaged around the province cutting and squaring crossties by the million.|$|R
40|$|Given the scarce {{bibliography}} dealing explicitly with robot pain, {{this chapter}} has enriched its review with related research works about robot behaviours and capacities in which pain {{could play a}} role. It is shown that all such roles ¿ranging from punishment to intrinsic motivation and planning knowledge¿ can be formulated within the unified <b>framework</b> of <b>reinforcement</b> learning. Peer ReviewedPostprint (author's final draft...|$|R
30|$|For {{all these}} {{therapeutic}} goals, specific contents were developed (collective rule making, <b>reinforcement</b> <b>framework,</b> teacher communication envelope, self-grading and grading therapists, organizational mural, monthly calendar) {{as well as}} techniques such as dramatization, diaphragmatic breathing, and problem-solving.|$|R
40|$|Abstract. Being {{of a high}} complexity, most multi-agent {{systems are}} {{difficult}} to deal with by a hand-coded approach to decision making. In such complicated environments in which decision making processes should be controlled from both the individuals points ' of view and the whole team, the common approach to the subject is the Reinforcement Learning (RL) method which is mainly based on learning the optimal policy through mapping this task to an episodic reinforcement learning <b>framework.</b> <b>Reinforcement</b> learning is the problem of generating optimal behavior in a sequential decision making environment given the opportunity of interacting with it. Since the Robocop domain is a multi-agent dynamic environment, with notable features making it outstanding for multi-agent simulation benchmarks, it has been largely used as a basis for international multi-agent simulation competitions and research challenges. For this purpose, reinforcement learning problems should be made “understandable ” for “agents ” which are Robocop players in this case. To make the agents “aware ” of what they are intended to do, this paper will provide an overview of different methods and alternative approaches {{as a starting point for}} robocupers who are not familiar with reinforcement learning problems in the Robocop domain. 1...|$|E
40|$|One {{objective}} of artificial intelligence is {{to model the}} behavior of an intelligent agent interacting with its environment. The environment's transformations could be modeled as a Markov chain, whose state is partially observable to the agent and affected by its actions; such processes are known as partially observable Markov decision processes (POMDPs). While the environment's dynamics are assumed to obey certain rules, the agent does not know them and must learn. In this dissertation {{we focus on the}} agent's adaptation as captured by the reinforcement learning <b>framework.</b> <b>Reinforcement</b> learning means learning a policy [...] -a mapping of observations into actions [...] -based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. The set of policies being searched is constrained by the architecture of the agent's controller. POMDPs require a controller to have a memory. We investigate various architectures for controllers with memory, including controllers with external memory, finite state controllers and distributed controllers for multi-agent system. For these various controllers we work out the details of the algorithms which learn by ascending the gradient of expected cumulative reinforcement. Building on statistical learning theory and experiment design theory, a policy evaluation algorithm is developed for the case of experience re-use. We address the question of sufficient experience for uniform convergence of policy evaluation and obtain sample complexity bounds for various estimators. Finally, we demonstrate the performance of the proposed algorithms on several domains, the most complex of which is simulated adaptive packet routing in a telecommunication network...|$|E
40|$|This thesis {{presents}} a novel hierarchical learning <b>framework,</b> <b>Reinforcement</b> Learning Optimal Control, for controlling nonlinear dynamical systems with continuous states and actions. The adapted approach mimics the neural computations that allow our brain to {{bridge across the}} divide between symbolic action-selection and low-level actuation control by operating at two levels of abstraction. First, current findings demonstrate that {{at the level of}} limb coordination human behaviour is explained by linear optimal feedback control theory, where cost functions match energy and timing constraints of tasks. Second, humans learn cognitive tasks involving learning symbolic level action selection, in terms of both model-free and model-based reinforcement learning algorithms. We postulate that the ease with which humans learn complex nonlinear tasks arises from combining these two levels of abstraction. The Reinforcement Learning Optimal Control framework learns the local task dynamics from naive experience using an expectation maximization algorithm for estimation of linear dynamical systems and forms locally optimal Linear Quadratic Regulators, producing continuous low-level control. A high-level reinforcement learning agent uses these available controllers as actions and learns how to combine them in state space, while maximizing a long term reward. The optimal control costs form training signals for high-level symbolic learner. The algorithm demonstrates that a small number of locally optimal linear controllers can be combined in a smart way to solve global nonlinear control problems and forms a proof-of-principle to how the brain may bridge the divide between low-level continuous control and high-level symbolic action selection. It competes in terms of computational cost and solution quality with state-of-the-art control, which is illustrated with solutions to benchmark problems. Open Acces...|$|E
50|$|Among his {{research}} contributions were {{the invention of}} error-correcting output coding to multi-class classification, the formalization of the multiple-instance problem, the MAXQ <b>framework</b> for hierarchical <b>reinforcement</b> learning, {{and the development of}} methods for integrating non-parametric regression trees into probabilistic graphical models.|$|R
40|$|Abstract. The {{scenario}} {{of this work}} {{is defined by the}} need of many Machine Learning algorithms to tune a number of parameters that define its behavior; the resulting performance can be evaluated with different indices. The relationship between parameters and performance may be neither linear nor straightforward. This work proposes a qualitative approach to the afore-mentioned relationship by using Self-Organizing Maps due to their visual information processing. The approach is evaluated in the <b>framework</b> of <b>Reinforcement</b> Learning algorithms. ...|$|R
40|$|Abstract. A {{wide variety}} of {{function}} approximation schemes have been applied to reinforcement learning. However, Bayesian filtering approaches, which have been shown efficient in other fields such as neural network training, have been little studied. We propose a general Bayesian filtering <b>framework</b> for <b>reinforcement</b> learning, {{as well as a}} specific implementation based on sigma point Kalman filtering and kernel machines. This allows us to derive an efficient off-policy model-free approximate temporal differences algorithm which will be demonstrated on two simple benchmarks. ...|$|R
40|$|It {{is often}} assumed that animals and people adjust their {{behavior}} to maximize reward acquisition. In visually cued reinforcement schedules, monkeys make errors in trials {{that are not}} immediately rewarded, despite having to repeat error trials. Here we show that error rates are typically smaller in trials equally distant from reward but belonging to longer schedules (referred to as "schedule length effect"). This violates the principles of reward maximization and invariance and cannot be predicted by the standard methods of Reinforcement Learning, such as the method of temporal differences. We develop a heuristic model that accounts {{for all of the}} properties of the behavior in the reinforcement schedule task but whose predictions are not {{different from those of the}} standard temporal difference model in choice tasks. In the modification of temporal difference learning introduced here, the effect of schedule length emerges spontaneously from the sensitivity to the immediately preceding trial. We also introduce a policy for general Markov Decision Processes, where the decision made at each node is conditioned on the motivation to perform an instrumental action, and show that the application of our model to the reinforcement schedule task and the choice task are special cases of this general theoretical framework. Within this <b>framework,</b> <b>Reinforcement</b> Learning can approach contextual learning with the mixture of empirical findings and principled assumptions that seem to coexist in the best descriptions of animal behavior. As examples, we discuss two phenomena observed in humans that often derive from the violation of the principle of invariance: "framing," wherein equivalent options are treated differently depending on the context in which they are presented, and the "sunk cost" effect, the greater tendency to continue an endeavor once an investment in money, effort, or time has been made. The schedule length effect might be a manifestation of these phenomena in monkeys...|$|E
40|$|Although behaviour-based {{robotics}} {{has been}} successfully used to develop autonomous mobile robots {{up to a certain}} point, further progress may require the integration of a learning model into the behaviour-based <b>framework.</b> <b>Reinforcement</b> learning is a natural candidate for this because it seems well suited to the problems faced by autonomous agents. However, previous attempts to use reinforcement learning in behaviour-based mobile robots have been simple combinations of these two methodologies rather than full integrations, and have suffered from severe scaling problems that appear to make them infeasible. Furthermore, the implicit assumptions that form the basis of reinforcement learning theory were not developed with the problems faced by autonomous agents in complex environments in mind. This dissertation introduces a model of reinforcement learning that is designed specifically for use in behaviour-based robots, taking the conditions faced by situated agents into account. The model layers a distributed and asynchronous reinforcement learning algorithm over a learned topological map and standard behavioural substrate to create a reinforcement learning complex. The topological map creates a small and task-relevant state space that aims to make reinforcement learning feasible, while the distributed and asynchronous nature of the model makes it compatible with behaviour-based design principles. The model is then validated through an experiment that requires a mobile robot to perform puck foraging in three separate artificial arenas. The development of Dangerous Beans, a mobile robot that is capable of building a distributed topological map of its environment and performing reinforcement learning over it is described, along with the results of its use to test three control strategies (random decision making, a standard reinforcement learning algorithm layered on top of a topological map, and the full model developed in this dissertation) in the arenas. The results show that the model developed in this dissertation is able to learn rapidly in a real environment, and outperforms both the random strategy and the layered standard reinforcement learning algorithm. Following this, a discussion of the implications of these results is given, which suggests that situated learning and the integration of behaviour-based methods and layered learning models merit further study...|$|E
40|$|This thesis studies three {{problems}} in sequential decision making across two different frameworks. The first framework we consider is online learning: for each {{round of a}} T round repeated game, the learner makes a prediction, the adversary observes this prediction and reveals the true outcome, and the learner suffers some loss based on {{the accuracy of the}} prediction. The learner's aim is to minimize the regret, which is defined to be the difference between the learner's cumulative loss and the cumulative loss of the best prediction strategy in some class. We study the minimax strategy, which guarantees the lowest regret against all possible adversary strategies. In general, computing the minimax strategy is exponential in T; we focus on two setting where efficient algorithms are possible. The first is prediction under squared Euclidean loss. The learner predicts a point in ^d and the adversary is constrained to respond with a point in some compact set. The regret is with respect to the single best prediction in the set. We compute the minimax strategy and the value of the game for any compact set and show that the value {{is the product of a}} horizon-dependent constant and the squared radius of the smallest enclosing ball of the set. We also present the optimal strategy of the adversary for two important sets: ellipsoids and polytopes that intersect their smallest enclosing ball at all vertices. The minimax strategy can be cast as a simple shrinkage of the past data towards the center of this minimum enclosing ball, where the shrinkage factor can be efficiently computed before the start of the game. Noting that the value does not have any explicit dimension dependence, we then extend these results to Hilbert space, finding, once again, that the value is proportional to the squared radius of the smallest enclosing ball. The second setting where we derive efficient minimax strategies is online linear regression. At the start of each round, the adversary chooses and reveals a vector of covariates. The regret is defined with respect to the best linear function of the covariates. We show that the minimax strategy is an easily computed linear predictor, provided that the adversary adheres to some natural constraints that prevent him from misrepresenting the scale of the problem. This strategy is horizon-independent: regardless of the length of the game, this strategy incurs no more regret than any strategy that has knowledge of the number of rounds. We also provide an interpretation of the minimax algorithm as a follow-the-regularized-leader strategy with a data-dependent regularizer and obtain an explicit expression for the minimax regret. We then turn to the second <b>framework,</b> <b>reinforcement</b> learning. More specifically, we consider the problem of controlling a Markov decision process (MDP) with a large state-space. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a low-dimensional family of policies. Specifically, we restrict the variables of the dual linear program to lie in some low-dimensional subspace, and show that we can find a policy that performs almost as well as the best policy in this class. We derive separate results for the average cost and discounted cost cases. Most importantly, the complexity of our method depends on the size of the comparison class but not the size of the state-space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application...|$|E
40|$|The exploration-exploitation dilemma {{has been}} an {{intriguing}} and unsolved problem within the <b>framework</b> of <b>reinforcement</b> learning. "Optimism {{in the face of}} uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants. Comment: Extended version of the homonymous ICML' 08 paper, with proof...|$|R
40|$|Abstract. We {{propose a}} new turn-taking {{framework}} for spoken dialogue systems in which conversants {{bid for the}} turn. This differs from most current turn-taking approaches, where the turn only changes after the holder has released it. Our new <b>framework</b> uses <b>Reinforcement</b> Learning to choose appropriate turn bids, which indirectly estimates the utterance importance. We evaluate this approach in an artificial task-oriented domain and we find that it outperforms conventional release-turn methods in a relatively realistic environment. Current performance is explained and future implications are discussed. ...|$|R
40|$|This paper explores a new <b>framework</b> for <b>reinforcement</b> {{learning}} {{based on}} online convex optimization, in particular mirror descent and related algorithms. Mirror descent {{can be viewed}} as an enhanced gradient method, particularly suited to minimization of convex functions in highdimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent {{can be viewed as}} a proximal algorithm where the distance generating function used is a Bregman divergence. A new class of proximal-gradient based temporaldifferenc...|$|R
40|$|We {{address the}} problem of single-agent, {{autonomous}} sequential decision making. We assume that some controllers or behavior policies are given as prior knowledge, and the task of the agent is to learn how to switch between these policies. We formulate the problem using the <b>framework</b> of <b>reinforcement</b> learning and options (Sutton, Precup & Singh, 1999; Precup, 2000). We derive gradient-based algorithms for learning the termination conditions of options, with the goal of optimizing the expected long-term return. We incorporate the proposed approach into policy-gradient methods with linear function approximation...|$|R
40|$|Research in analogical {{reasoning}} suggests that higher-order cognitive {{functions such as}} abstract reasoning, far transfer, and creativity are founded on recognizing structural similarities among relational systems. Here we integrate theories of analogy with the computational <b>framework</b> of <b>reinforcement</b> learning (RL). We propose a psychology theory that is a computational synergy between analogy and RL, in which analogical comparison provides the RL learning algorithm {{with a measure of}} relational similarity, and RL provides feedback signals that can drive analogical learning. Simulation results support the power of this approach. Comment: 20 pages, 7 figure...|$|R
40|$|This paper {{presents}} {{control framework}} based on multi-agent reinforcement approach for building intellectual steering software for multi-wheeled mobile robots. The <b>framework</b> uses modified <b>reinforcement</b> learning approach based on special multi-agent structure with virtual leader. The framework application example {{will be shown}} with real multi-wheeled mobile platform. The experiments were performed in simulation environment with accurate virtual model...|$|R
40|$|AbstractWe {{examine a}} model of human causal cognition, which {{generally}} deviates from normative systems such as classical logic and probability theory. For two-armed bandit problems, we demonstrate the efficacy of our loosely symmetric model (LS) and its implementation of two cognitive biases peculiar to humans: symmetry and mutual exclusivity. Specifically, we use LS as a simple value function within the <b>framework</b> of <b>reinforcement</b> learning. The resulting cognitively biased valuations precisely describe human causal intuitions. We further show that operating LS under the simplest greedy policy yields superior reliability and robustness, even managing to overcome the usual speed-accuracy trade-off, and effectively removing the need for parameter tuning...|$|R
40|$|This paper {{introduces}} a teacher-student <b>framework</b> for <b>reinforcement</b> learning. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, {{the teacher may}} only give such advice {{a limited number of}} times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two experimental domains: Mountain Car and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations...|$|R
40|$|Programming mobile robots {{can be a}} long, {{time-consuming}} process. Specifying the low-level mapping from {{sensors to}} actuators is prone to programmer misconceptions, and debugging such a mapping can be tedious. The {{idea of having a}} robot learn how to accomplish a task, rather than being told explicitly is an appealing one. It seems easier and much more intuitive for the programmer to specify what the robot should be doing, and to let it learn the fine details of how to do it. In this paper, we introduce a <b>framework</b> for <b>reinforcement</b> learning on mobile robots and describe our experiments using it to learn simple tasks...|$|R
40|$|Abstract — Programming mobile robots {{can be a}} long, timeconsuming process. Specifying the {{low-level}} mapping from {{sensors to}} actuators is prone to programmer misconceptions, and debugging such a mapping can be tedious. The {{idea of having a}} robot learn how to accomplish a task, rather than being told explicitly is an appealing one. It seems easier and much more intuitive for the programmer to specify what the robot should be doing, and to let it learn the fine details of how to do it. In this paper, we introduce a <b>framework</b> for <b>reinforcement</b> learning on mobile robots and describe our experiments using it to learn simple tasks. Index Terms — Mobile robots, machine learning, reinforcement learning, learning from demonstration...|$|R
40|$|We {{consider}} communication {{when there}} is no agreement about symbols and meanings. We treat it within the <b>framework</b> of <b>reinforcement</b> learning. We apply different reinforcement learning models in our studies and simplify the problem as much as possible. We show that the modelling of the other agent is insufficient in the simplest possible case, unless the intentions can also be modelled. The model of the agent and its intentions enable quick agreements about symbol-meaning association. We show that when both agents assume an `intention model' about the other agent then the symbol-meaning association process can be spoiled and symbol meaning association may become hard. Comment: 22 pages, 3 figures, New Ties ([URL] EU FP 6 discussion pape...|$|R
40|$|Computational {{learning}} {{models are}} critical for understanding mechanisms of adaptive behavior. However, the two major current <b>frameworks,</b> <b>reinforcement</b> learning (RL) and Bayesian learning, both have certain limitations. For example, many Bayesian models are agnostic of inter-individual variability and involve complicated integrals, making online learning difficult. Here, we introduce a generic hierarchical Bayesian framework for individual learning under multiple forms of uncertainty (e. g., environmental volatility and perceptual uncertainty). The model assumes Gaussian random walks of states at all but the first level, with the step size determined by the next higher level. The coupling between levels is controlled by parameters that shape the influence of uncertainty on learning in a subject-specific fashion. Using variational Bayes under a mean field approximation and a novel approximation to the posterior energy function, we derive trial-by-trial update equations which (i) are analytical and extremely efficient, enabling real-time learning, (ii) have a natural interpretation in terms of RL, and (iii) contain parameters representing processes which {{play a key role}} in current theories of learning, e. g., precision-weighting of prediction error. These parameters allow for the expression of individual differences in learning and may relate to specific neuromodulatory mechanisms in the brain. Our model is very general: it can deal with both discrete and continuous states and equally accounts for deterministic and probabilistic relations between environmental events and perceptual states (i. e., situations with and without perceptual uncertainty). These properties are illustrated by simulations and analyses of empirical time series. Overall, our framework provides a novel foundation for understanding normal and pathological learning that contextualizes RL within a generic Bayesian scheme and thus connects it to principles of optimality from probability theory...|$|R
40|$|Reinforcement {{learning}} {{has recently been}} receiving increased attention as a method for robot learning {{with little or no}} a priori knowledge and higher capability of reactive and adaptive behaviors. This paper presents a <b>framework</b> of the <b>reinforcement</b> learning, and several issues in applying the method to real robot tasks. Then, examples of real robot applications, especially vision-based reinforcement learning methods are introduced to show how they cope with these issues...|$|R
40|$|Motor control {{depends on}} sensory {{feedback}} in multiple modalities with different latencies. In this paper we consider within the <b>framework</b> of <b>reinforcement</b> learning how different sensory modalities {{can be combined}} and selected for real-time, optimal movement control. We propose an actor-critic architecture with multiple modules, whose output are combined using a softmax function. We tested our architecture in a simulation of a sequential reaching task. Reaching was initially guided by visual feedback with a long latency. Our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory. In simulations with different latencies for visual and somatosensory feedback, {{we found that the}} agent depended more on feedback with shorter latency. ...|$|R
40|$|Skills {{can often}} be {{performed}} in many different ways. In order to provide robots with human-like adaptation capabilities, it is {{of great interest to}} learn several ways of achieving the same skills in parallel, since eventual changes in the environment or in the robot can make some solutions unfeasible. In this case, the knowledge of multiple solutions can avoid relearning the task. This problem is addressed in this paper within the <b>framework</b> of <b>Reinforcement</b> Learning, as the automatic determination of multiple optimal parameterized policies. For this purpose, a model handling a variable number of policies is built using a Bayesian non-parametric approach. The algorithm is first compared to single policy algorithms on known benchmarks. It is then applied to a typical robotic problem presenting multiple solutions...|$|R
40|$|Abstract. The paper {{presents}} {{a method to}} guide the self-organised development of behaviours of autonomous robots. In earlier publications we demonstrated {{how to use the}} homeokinesis principle and dynamical systems theory to obtain self-organised playful but goal-free behaviour. Now we extend this <b>framework</b> by <b>reinforcement</b> signals. We validate the mechanisms with two experiment with a spherical robot. The first experiment aims at fast motion, where the robot reaches on average about twice the speed of a not reinforcement robot. In the second experiment spinning motion is rewarded and we demonstrate that the robot successfully develops pirouettes and curved motion which only rarely occur among the natural behaviours of the robot. Key words: autonomous robots, self-organised behaviour, reinforcement learning, developmental robotics, homeokinesis...|$|R
40|$|Editor’s Summary: Chapter?? {{introduced}} policy gradients {{as a way}} {{to improve}} on stochastic search of the policy space when learning. This chapter presents supervised actor-critic reinforcement learning as another method for improving the effectiveness of learning. With this approach, a supervisor adds structure to a learning problem and supervised learning makes that structure part of an actor-critic <b>framework</b> for <b>reinforcement</b> learning. Theoretical background and a detailed algorithm description are provided, along with several examples that contain enough detail to make them easy to understand and possible to duplicate. These examples also illustrate the use of two kinds of supervisors: a feedback controller that is easily designed yet sub-optimal, and a human operator providing intermittent control of a simulated robotic arm. 1. ...|$|R
40|$|This paper {{addresses}} {{the question of}} the functional role of the dual application of positive and negative Hebbian time dependent plasticity rules, in the particular <b>framework</b> of <b>reinforcement</b> learning tasks. Our simulations take place in a recurrent network of spiking neurons with inhomogeneous synaptic weights. The network spontaneously displays a self-sustained activity. A Spike-Timing Dependent Plasticity (STDP) rule is combined with its opposite, the anti-STDP. A local regulation mechanism moreover maintains the output neuron in the vicinity of a reference frequency, which forces the global dynamics to be maintained in a softly disordered regime. This approach is tested on a simple discrimination task which requires short-term memory: temporal pattern identification. We show that such temporal patterns can be categorized, and present tracks for future improvements...|$|R
40|$|Supervised {{learning}} {{is widely used}} in training autonomous driving vehicle. However, it is trained with large amount of supervised labeled data. Reinforcement learning can be trained without abundant labeled data, but we cannot train it in reality because it would involve many unpredictable accidents. Nevertheless, training an agent with good performance in virtual environment is relatively much easier. Because of the huge difference between virtual and real, how {{to fill the gap}} between virtual and real is challenging. In this paper, we proposed a novel <b>framework</b> of <b>reinforcement</b> learning with image semantic segmentation network to make the whole model adaptable to reality. The agent is trained in TORCS, a car racing simulator. Comment: arXiv admin note: text overlap with arXiv: 1704. 03952 by other author...|$|R
30|$|Adaptive optimization. CWNs {{offer the}} ability to {{adaptive}} optimize against the varying wireless environmental conditions using AI-based techniques. CWN typically utilizes adaptive <b>frameworks</b> such as <b>reinforcement</b> learning, learning automata, game theory, etc., for adaptively optimizing {{the parameters of the}} network. There are numerous optimization-based applications of CWNs including dynamic spectrum access[144], parameter optimization[145, 188], optimized MAC[146] and routing[61, 189], enhanced reliability[148] and security[149, 189], QoS assurance and management[190, 191], channel assignment[192], etc.|$|R
40|$|There is a {{long history}} of {{experiments}} in which participants are instructed to generate a long sequence of binary random numbers. The scope of this line of research has shifted over the years from identifying the basic psychological principles and/or the heuristics that lead to deviations from randomness, to one of predicting future choices. In this paper, we used generalized linear regression and the <b>framework</b> of <b>Reinforcement</b> Learning in order to address both points. In particular, we used logistic regression analysis in order to characterize the temporal sequence of participants' choices. Surprisingly, a population analysis indicated that the contribution of the most recent trial has only a weak effect on behavior, compared to more preceding trials, a result that seems irreconcilable with standard sequential effects that decay monotonously with the delay. However, when considering each participant separately, we found that the magnitudes of the sequential effect are a monotonous decreasing function of the delay, yet these individual sequential effects are largely averaged out in a population analysis because of heterogeneity. The substantial behavioral heterogeneity in this task is further demonstrated quantitatively by considering the predictive power of the model. We show that a heterogeneous model of sequential dependencies captures the structure available in random sequence generation. Finally, we show that the results of the logistic regression analysis can be interpreted in the <b>framework</b> of <b>reinforcement</b> learning, allowing us to compare the sequential effects in the random sequence generation task to those in an operant learning task. We show that in contrast to the random sequence generation task, sequential effects in operant learning are far more homogenous across the population. These results suggest that in the random sequence generation task, different participants adopt different cognitive strategies to suppress sequential dependencies when generating the "random" sequences...|$|R
