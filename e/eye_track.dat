16|3181|Public
5000|$|Blind <b>Eye</b> (<b>Track</b> 4) on {{the album}} First Strut Is The Deepest (2013) by Brother Strut ...|$|E
5000|$|Scanlan told Entertainment Weekly in November 2015 {{that the}} robot's design {{came from an}} {{original}} sketch by Abrams, saying [...] "It was a very simple sketch, beautiful in its simplicity of a ball with this little dome on top." [...] Of BB-8's design [...] "with differently shaped panels on each side to help the viewer’s <b>eye</b> <b>track</b> movement", Scanlan said [...] "If you had parallel patterns that ran around the circumference, they would be less informative as to the direction BB-8 was traveling than a slightly more chaotic pattern." [...] Calling the robot a [...] "Swiss Army Knife that shouldn’t be trusted", he noted that while each of the BB-8's panels has a specific purpose, like a port or tool, {{not all of them}} have been absolutely defined to leave options for future films. Abrams also named the robot, telling Entertainment Weekly in August 2015, [...] "I named him BB-8 because it was almost onomatopoeia. It was sort of how he looked to me, with the 8, obviously, and then the two B's." [...] The name was conceived early on in the film's production {{and was one of the}} few to remain unchanged.|$|E
40|$|A {{hierarchical}} framework suggesting how graph readers {{go beyond}} explicitly represented data to make inferences is presented. According to our hierarchical framework, graph readers use read-offs, integration and pattern extrapolation to make inferences. Verbal protocol data demonstrates highlevel {{differences in the}} way inferences are made and <b>eye</b> <b>track</b> data examines these processes at the perceptual level...|$|E
40|$|<b>Eye</b> <b>tracking</b> is {{a widely}} used {{research}} method, {{but there are many}} questions and misconceptions about how to effectively apply it. <b>Eye</b> <b>Tracking</b> the User Experience the first how-to book about <b>eye</b> <b>tracking</b> for UX practitioners offers step-by-step advice on how to plan, prepare, and conduct <b>eye</b> <b>tracking</b> studies; how to analyze and interpret eye movement data; and how to successfully communicate <b>eye</b> <b>tracking</b> findings...|$|R
40|$|<b>Eye</b> <b>Tracking</b> for User Experience Design {{explores the}} many {{applications}} of <b>eye</b> <b>tracking</b> {{to better understand}} how users view and interact with technology. Ten leading experts in <b>eye</b> <b>tracking</b> discuss how they {{have taken advantage of}} this new technology to understand, design, and evaluate user experience. Real-world stories are included from these experts who have used <b>eye</b> <b>tracking</b> during the design and development of products ranging from information websites to immersive games. They also explore recent advances in the technology which tracks how users interact with mobile devices, large-screen displays and video game consoles. Methods for combining <b>eye</b> <b>tracking</b> with other research techniques for a more holistic understanding of the user experience are discussed. This is an invaluable resource to those who want to learn how <b>eye</b> <b>tracking</b> can be used to better understand and design for their users. * Includes highly relevant examples and information for those who perform user research and design interactive experiences* Written by numerous experts in user experience and <b>eye</b> <b>tracking.</b> * Highly relevant to anyone interested in <b>eye</b> <b>tracking</b> & UX design * Features contemporary <b>eye</b> <b>tracking</b> research emphasizing the latest uses of <b>eye</b> <b>tracking</b> technology in the user experience industry...|$|R
30|$|We have {{categorised}} related studies {{under three}} categories: <b>eye</b> <b>tracking</b> for user classification, <b>eye</b> <b>tracking</b> and attention/affective state and <b>eye</b> <b>tracking</b> and graphics/visualisations. In the first category, <b>eye</b> <b>tracking</b> {{is used to}} directly augment the student model or understand different groups of students. We then outline the research work involving the use of <b>eye</b> <b>tracking</b> to understand or predict the students’ affective state, in particular to determine when students are struggling. A review of the research into using <b>eye</b> <b>tracking</b> {{to learn more about}} students’ behaviour when viewing visualisations and graphics follows, and a summary of the findings of the related work concludes this section.|$|R
40|$|Abstract — This study aims {{at finding}} a typical {{pattern in the}} eye movement, when human obtains an {{interpretation}} of a presented image. We conducted experiments where subjects looked at abstract artworks and commercial posters. We compared the <b>eye</b> <b>track</b> lines with subjects’ statements about {{the interpretation of the}} image. The results show the tendency that eyes draw lines in the velocity between saccade and pursuit eye-movement as we call slow saccade line (SSL), following objects in the image corresponding to the meaningful units (sentences) of subjects ’ statement about image interpretation. Furthermore, when they reached an insight, i. e., obtained a new interpretation after feeling difficulty in understanding the image, the <b>eye</b> <b>track</b> changed into SSLs after exploring in the velocity of fast saccade. We found this shift of velocity enables to acquire long-term memory of the obtained interpretation. Index Terms — Auxiliary lines, slow saccade line, image interpretation, insight I...|$|E
40|$|The {{process of}} {{resuming}} an interrupted task has been understood by task level goals (Altmann & Trafton, 2002). Recent empirical evidence has implicated spatial memory {{as a component}} of the resumption process suggesting that spatial level representations are important as well. We collected <b>eye</b> <b>track</b> data in an interruptions paradigm to examine the perceptual processes involved in resumption. Four models were created to illustrate the importance of the role of spatial representations and further, to demonstrate how the task level and spatial representations can be integrated...|$|E
40|$|Photography {{provides}} tangible and visceral mementos of im-portant experiences. Recent {{research in}} content-aware image processing to automatically improve photos {{relies heavily on}} automatically identifying salient areas in images. While au-tomatic saliency estimation has achieved estimable success, it will always face inherent challenges. Tracking the photogra-pher’s eyes allows a direct, passive means to estimate scene saliency. We show that saliency estimation is sometimes an ill-posed posed problem for automatic algorithms, made well-posed by the availability of recorded eye tracks. We instru-ment several content-aware image processing algorithms with <b>eye</b> <b>track</b> based saliency estimation, producing photos that ac-centuate {{the parts of the}} image originally viewed...|$|E
40|$|This work {{deals with}} <b>eye</b> <b>tracking</b> systems and {{possibility}} of using them {{for control of}} many software or hardware devices. Project explains eye anatomy and electrophysiology, types of eye trackers and methods of <b>eye</b> <b>tracking.</b> We studied methods of image processing and image analyzing in LabVIEW development system and IMAQ Vision subsystem. On the base of these studies, we created whole <b>eye</b> <b>tracking</b> system. This system {{can be used for}} both the <b>eye</b> <b>tracking</b> from video recording and real time <b>eye</b> <b>tracking.</b> This system can monitor position or trajectory of look...|$|R
40|$|<b>Eye</b> <b>tracking</b> {{research}} and research methodologies are becomingly increasingly common in many disciplines from psychology and marketing {{to education and}} learning. This is because <b>eye</b> <b>tracking</b> {{research and}} research methodologies offer new ways of collecting data, framing research questions, and thinking about how we view, see, and experience the world. Researchers are also making new findings about {{the way that the}} visual system works and the way it interacts with attention, cognition, and behaviour. As a result, research based on <b>eye</b> <b>tracking</b> research methods is increasing in every discipline. New studies using <b>eye</b> <b>tracking</b> technologies are continually being published and new applications of this innovative way of conducting research are being shared by researchers from every continent and country. Analysis of researchusing <b>eye</b> <b>tracking</b> methods is growing exponentially. Current Trends in <b>Eye</b> <b>Tracking</b> Research presents a range of new research studies using <b>eye</b> <b>tracking</b> research and research methods {{from a wide variety of}} disciplines. The research studies have been chosen to chronicle the wide applications and uses of <b>eye</b> <b>tracking</b> research. Current Trends in <b>Eye</b> <b>Tracking</b> Research is comprised of new and innovative studies using <b>eye</b> <b>tracking</b> research and research methods and showcases innovative ways of applying <b>eye</b> <b>tracking</b> technologies to interesting research problems. The book collects the research of over 55 researchers and academics currently using the <b>eye</b> <b>tracking</b> research and introduces the work of a number of <b>eye</b> <b>tracking</b> research laboratories and their key staff and research interests. Current Trends in <b>Eye</b> <b>Tracking</b> Research is designed to explore a broad range of applications of this emerging and evolving research technology and to open the research space for wider sharing of new research methods and research questions. The book incorporates a number of new studies and introduces a number of new researchers to the practitioners of <b>eye</b> <b>tracking</b> research. Current Trends in <b>Eye</b> <b>Tracking</b> Research also focuses on lessons learned in conducting eye movement research across multiple institutions, settings, and disciplines and innovative uses of existing technology as well as pioneering implementation of new technology in a range of research contexts and disciplines, key challenges, and important discoveries in moving from raw data to findings and challenges and opportunities related to situating individual research efforts in a larger research context. Current Trends in <b>Eye</b> <b>Tracking</b> Research is divided into four key sections. Each section provides a central theme that integrates the many chapters in that section. Part I is titled <b>Eye</b> <b>Tracking</b> and the Visual System and is concerned with research on the operation of the human visual system. The chapters in this section overview <b>eye</b> <b>tracking</b> and the human visual system research, and provide a series of chapters that examine how to explain the operation of the human visual system and fundamentalresearch on the use of <b>eye</b> <b>tracking</b> to deepen and strengthen our understanding of the complexity of visual processes. Part II is titled Aligning <b>Eye</b> <b>Tracking</b> and EEG Data and is concerned with research that reports on the alignment of EGG and <b>eye</b> <b>tracking</b> data. The chapters in this section overview fundamental research finding on how to link <b>eye</b> <b>tracking</b> and EEG data. The chapters in this section also address some critical research questions in integrating <b>eye</b> <b>tracking</b> data with other forms of data. The four chapters also overview current approaches to research on this alignment process. Part III is titled <b>Eye</b> <b>Tracking</b> and Marketing and Social Applications and is concerned with <b>eye</b> <b>tracking</b> based research in a range of social science and marketing disciplines. Each chapter provides a different application from a different discipline — from marketing to aging, from mental illness to evaluating forgeries to understanding what people see when they read financial reports. Each chapter provides a novel application of <b>eye</b> <b>tracking</b> research methodology in the social sciences. Part IV is titled <b>Eye</b> <b>Tracking</b> and Education and is concerned with research on learning using <b>eye</b> <b>tracking</b> methodologies. The five chapters focus on fundamental research problems in learning such as reading comprehension and the visual mechanics of comprehension, learning to read complex visual displays, and the development of student self-regulation skills. The section also explores the use of think aloud research protocols for multilingual learners...|$|R
50|$|Alternatively, {{it can be}} {{measured}} using <b>eye</b> <b>tracking</b> (see also <b>Eye</b> <b>tracking</b> on the ISS for an example).|$|R
40|$|Although {{automation}} has {{benefits for}} commercial aviation, {{it has led}} to undesirable consequences. One approach to understanding errors is the development and examination of cognitive models of the flying task. However, the construction of these models requires knowledge about the processes pilots use when they fly and how they acquire readings from their flight instruments. We explored this issue by collecting data from pilots interacting with a Boeing 747 - 400 desktop simulator. <b>Eye</b> <b>track</b> data provided information about where pilots were looking. This report describes the data obtained and provides suggestions for what these data mean in light of cognitive models...|$|E
40|$|Theories {{accounting}} for the process of primary task resumption following an interruption {{have focused on the}} suspension and retrieval of a specific goal (Altmann & Trafton, 2002). The ability to recall the spatial location of where in the task one was prior to being interrupted may also be important. We show that being able to maintain a spatial representation of the primary task facilitates task resumption. Participants were interrupted by an instant message window that either partially or fully occluded the primary task interface. Reaction time measures show that participants were faster at resuming in the partial occlusion condition. In addition, <b>eye</b> <b>track</b> data suggest that participants were more accurate at returning to where they left off, suggesting {{that they were able to}} maintain a spatial representation of the task and use this information to resume more quickly...|$|E
40|$|Since scalp EEG {{recordings}} {{are measured}} in microvolts, electrical signals may easily interfere during an experiment. As Spehlmann discusses, such interference may be introduced through {{the lights in}} the recording room, a nearby television, or even a computer monitor [Spehlmann, 1991]. Thus, when we consider performing EEG/EP/ERP experiments within a virtual reality helmet containing an eye tracker, electrical interference becomes a real possibility. We tested the effects of wearing a VR 4 virtual reality (VR) helmet containing an ISCAN eye tracker while asking subjects to do a continuous performance task. The results of this task were then analyzed in the frequency domain and compared to results from the same experiment while looking at a computer screen in two different environments. Results indicate that in an environment with other computers, the vertical refresh from the back of a nearby row of computer monitors added more noise to the signal than wearing the VR helmet and <b>eye</b> <b>track</b> [...] ...|$|E
40|$|<b>Eye</b> <b>tracking</b> {{methods are}} {{increasingly}} being used to analyze and evaluate e-learning systems. <b>Eye</b> <b>tracking</b> systems use the movements of eyes during visual exploration to identify the eye positions and gaze patterns. In e-learning, <b>eye</b> <b>tracking</b> {{can be used to}} validate learning preferences, content adaptation and presentation styles according to the cognitive styles, prior knowledge and computer competency of the learner. In this paper, we explore how <b>eye</b> <b>tracking</b> have been applied to enable effective user involvement and engagement in eLearning. Review of <b>eye</b> <b>tracking</b> applications in interactive and adaptive eLearning systems as well as in diagnostic applications are discusse...|$|R
40|$|Jarodzka, H. (2011, December). <b>Eye</b> <b>tracking</b> {{research}} at the OU. Free Hands-on <b>Eye</b> <b>Tracking</b> Workshop: SMI <b>Eye</b> <b>Tracking</b> Glasses on Tour in the Netherlands, Heerlen, The Netherlands. This presentation was in introduction to an eye trackign workshop organized together with SMI at the Open University in Heerlen...|$|R
40|$|This paper {{discusses}} how <b>eye</b> <b>tracking</b> {{can be used}} {{to supplement}} usability testing and intends to identify best practice about <b>eye</b> <b>tracking</b> in its scientific and practical sense, especially as applied to commercial usability engineering practice. Keywords: <b>Eye</b> <b>Tracking,</b> Usability Evaluation, User Interfaces, User-centered design. 1...|$|R
40|$|Exhibition at the Royal Society Summer Science Exhibition, {{accompanied}} by catalogue (containing essay by Tchalenko) and 15 minute DVD film “The Needle in a Haystack: Strategies of Visual Search in Art, Medicine and Daily Life. The Eye Control project (2001 - 2004) {{from which this}} research originated, was a Wellcome Trust funded project comparing visual search strategies in art (e. g. in observational drawing) and in medicine (e. g. in X-ray diagnosis). It was undertaken jointly with Imperial College Computing Department whose role was confined to providing <b>eye</b> <b>track</b> equipment and processing the medical part of the data. In parallel to addressing the specialist, our main emphasis was to generate enthusiasm amongst a younger generation of potential scientists and artists. This was achieved on the science side with the Royal Society exhibition display (see 1. 1) which was voted as the 2 nd most popular display of the show. On the art side, we were invited by David Hockney and Alan Jones to exhibit at the Royal Academy of Arts Summer Show 2004 examples of free-eye drawing (drawn with the eyes alone) by four international artists...|$|E
40|$|Abstract—Eyes {{blinking}} and its movement can portray {{many reasons}} {{of the body}} and health state. Eyes can blink intentionally and sometimes randomly even in sleeping mode. Thus, the aim {{of this paper is to}} discover and observe the relationship between the frequency of eye blink and the level of eye muscle stress. The <b>eye</b> <b>track</b> data is fed directly into the electroencephalogram (EEG) record for parameter classification and identification. The EEG signal might have an artifact that has been analyzed and converted the observation into the mathematical library and repository software (HPC). The artificial neural network (ANN) is integrated with EEG digital data by the derivation of the mathematical modelling. The function of ANN is to train a large sparse digital data for future prediction of eye condition associated with the stress level. In order to validate the model and simulation, the numerical analysis and performance evaluation are compared to the real data set of eye therapy industry, IC Herbz Sdn Bhd. A library and repository software of mathematical model using EEG record data is developed to integrate with wearable augmented reality (WAR) based on EEG sensor device for predicting and monitoring the real time eye blinks, movement and muscle stress...|$|E
40|$|Minimal access surgery, in {{particular}} endoscopy and including bronchoscopy and laparoscopy, {{is characterized by}} complex instrument control, lack of tactile perception and restriction in vision and mobility. Consequently, {{a high degree of}} operator skill is required and special emphasis on training. Many aspects of this skill are centered on eye-hand coordination tasks in which eye control plays an essential part. Eye control involves the ability to control the temporal and spatial characteristics of one's voluntary eye movements, {{in particular}} scanning speed and smoothness, and targeting precision. Different subjects have different degrees of eye control, as can be seen, for example, when they are asked to write their names in space with their eyes alone. The Eye Control project (2001 - 2004) from which this research originated, was a Wellcome Trust funded project comparing visual search strategies in art (e. g. in observational drawing) and in medicine (e. g. in X-ray diagnosis). It was undertaken jointly with Imperial College Computing Department whose role was confined to providing <b>eye</b> <b>track</b> equipment and processing the medical part of the data. In parallel to addressing the specialist, our main emphasis was to generate enthusiasm amongst a younger generation of potential scientists and artists. This was achieved on the science side with the Royal Society exhibition display (see 1. 1) which was voted as the 2 nd most popular display of the show. On the art side, we were invited by David Hockney and Alan Jones to exhibit at the Royal Academy of Arts Summer Show 2004 examples of free-eye drawing (drawn with the eyes alone) by four international artists...|$|E
40|$|<b>Eye</b> <b>tracking</b> {{have been}} around for many years now, but until {{recently}} the technology have been expensive and the performance too low for the mass market to pay any attention. But times are changing and the cost of eye trackers is going down and performance is getting better. <b>Eye</b> <b>tracking</b> will soon start appearing in more and more homes around the planet. With the advent of eye trackers in people‟s homes, it is time to start thinking about what users would want to use <b>eye</b> <b>tracking</b> for. A popular field that comes to mind is gaming. Adding <b>eye</b> <b>tracking</b> to games could change gaming significantly. Experiences could be enhanced and new genres created. A deep integration with the games would allow <b>eye</b> <b>tracking</b> to blend smoothly with the game; creating a natural and immersive experience. However, it will take a while for the game developers to make this come true. In the meantime something else is needed to fill the gap between now, when no game had <b>eye</b> <b>tracking</b> support, and the future when all games will have a deeply integrated <b>eye</b> <b>tracking</b> interaction. The product we came up with is a tool for non-intrusively integrating <b>eye</b> <b>tracking</b> into existing games. This means that the tool is able to add <b>eye</b> <b>tracking</b> support without the need to modify the game in any way...|$|R
50|$|On the {{hardware}} side {{the company has}} three main product lines: mobile <b>Eye</b> <b>Tracking</b> Glasses (ETG), remote <b>eye</b> <b>tracking</b> systems (RED) and tower-mounted systems (Hi-Speed).|$|R
40|$|Mobile <b>eye</b> <b>tracking</b> {{provides}} {{insights into}} cognitive processing of visual information while a learner moves around. This chapter presents {{a case study}} in a small museum exhibition that was conducted to explore the suitability of mobile <b>eye</b> <b>tracking</b> for researching mobile learning. The study showed both potentials and limitations of mobile <b>eye</b> <b>tracking</b> methodology for research on mobile learning in general and in science exhibitions in particular: Mobile <b>eye</b> <b>tracking</b> provides rich, non-reactive data from the learner’s perspective which can be further analysed qualitatively and quantitatively. Concerns were raised with respect to interrelations of object fixations and underlying cognitive processes. Limitations also include obtrusiveness, accuracy, selective sampling, ethical concerns, financial effort, and effort of data analysis. These limitations suggest that, to increase validity, <b>eye</b> <b>tracking</b> is best used in combination with other methods. Nonetheless, mobile <b>eye</b> <b>tracking</b> can be a powerful data collection method in research on mobile learning...|$|R
40|$|We {{investigated}} {{with the}} help of an eyetracker, movement sensor and close-up video filming a well-known painter, Humphrey Ocean, drawing portraits (Miall & Tchalenko, 2001). In the present study we report on a further study of this data concentrating on the painter’s eye-hand coordination. We observed that, in general, his eye closely followed the drawing hand, with fixations on, or very near, the line being drawn. But there were also frequent exceptions to this behaviour when the artist’s eye departed from the drawing hand to fixate other parts of the drawing or turned to the model. Examples are presented for each of these cases, illustrating the process of visual memory fading and refreshing, and the possible action of a motor memory component in the drawing method of this painter. The Eye Control project (2001 - 2004) from which this research originated, was a Wellcome Trust funded project comparing visual search strategies in art (e. g. in observational drawing) and in medicine (e. g. in X-ray diagnosis). It was undertaken jointly with Imperial College Computing Department whose role was confined to providing <b>eye</b> <b>track</b> equipment and processing the medical part of the data. In parallel to addressing the specialist, our main emphasis was to generate enthusiasm amongst a younger generation of potential scientists and artists. This was achieved on the science side with the Royal Society exhibition display (see 1. 1) which was voted as the 2 nd most popular display of the show. On the art side, we were invited by David Hockney and Alan Jones to exhibit at the Royal Academy of Arts Summer Show 2004 examples of free-eye drawing (drawn with the eyes alone) by four international artists...|$|E
40|$|Objective: This study aims {{to present}} {{literature}} providing researchers with insightson specific fields {{of research and}} highlighting the major issues in the researchtopics. A systematic review is suggested using content analysis on literatures regarding"visual search", "eye movement", and "eye track". Background: Literature review can be classified as "narrative" or "systematic"depending on its approach in structuring {{the content of the}} research. Narrativereview is a traditional approach that describes the current state of a study fieldand discusses relevant topics. However, since literatures on specific area cover abroad range, reviewers inherently give subjective weight on specific issues. On thecontrary, systematic review applies explicit structured methodology to observe thestudy trends quantitatively. Method: We collected meta-data of journal papers using three search keywords:visual search, eye movement, and <b>eye</b> <b>track.</b> The collected information contains anunstructured data set including many natural languages which compose titles andabstracts, while the keyword of the journal paper is the only structured one. Basedon the collected terms, seven categories were evaluated by inductive categorizationand quantitative analysis from the chronological trend of the research area. Results: Unstructured information contains heavier content on "stimuli" and"condition" categories as compared with structured information. Studies on visualsearch cover a wide range of cognitive area whereas studies on eye movement andeye track are closely related to the physiological aspect. In addition, experimentalstudies show an increasing trend as opposed to the theoretical studies. Conclusion: By systematic review, we could quantitatively identify the characteristicof the research keyword which presented specific research topics. We also foundout that the structured information was more suitable to observe the aim of theresearch. Chronological analysis on the structured keyword data showed that studieson "physical eye movement" and "cognitive process" were jointly studied in increasingfashion. Application: While conventional narrative literature reviews were largely dependenton authors' instinct, quantitative approach enabled more objective and macroscopicviews. Moreover, the characteristics of information type were specified by comparingunstructured and structured information. Systematic literature review also could beused to support the authors' instinct in narrative literature reviews. clos...|$|E
40|$|This study {{examines}} the morphodynamic response of a deltaic system to extreme weather events. The Wax Lake Delta (WLD) in Louisiana, USA, {{is used to}} illustrate the impact of extreme events (hurricanes) on a river-dominated deltaic system. Simulations using the open source Delft 3 D model reveal that Hurricane Rita, which made landfall 120 km {{to the west of}} WLD as a Category 3 storm in 2005, caused erosion on the right side and deposition {{on the left side of}} the hurricane <b>eye</b> <b>track</b> on the continental shelf line (water depth 10 m to 50 m). Erosion over a wide area occurred both on the continental shelf line and in coastal areas when the hurricane moved onshore, while deposition occurred along the Gulf coastline (water depth < 5 m) when storm surge water moved back offshore. The numerical model estimated that Hurricane Rita’s storm surge reached 2. 5 m, with maximum currents of 2. 0 m s– 1, and wave heights of 1. 4 m on the WLD. The northwestern-directed flow and waves induced shear stresses, caused erosion on the eastern banks of the deltaic islands and deposition in channels located west of these islands. In total, Hurricane Rita eroded more than 500, 000 m 3 of sediments on the WLD area. Including waves in the analysis resulted in doubling the amount of erosion in the study area, comparing to the wave-excluding scenario. The exclusion of fluvial input caused minor changes in deltaic morphology during the event. Vegetation cover was represented as rigid rods in the model which add extra source terms for drag and turbulence to influence the momentum and turbulence equations. Vegetation slowed down the floodwater propagation and decreased flow velocity on the islands, leading to a 47 % reduction in the total amount of erosion. Morphodynamic impact of the hurricane track relative to the delta was explored. Simulations indicate that the original track of Hurricane Rita (landfall 120 km west of the WLD) produced twice as much erosion and deposition at the delta compared to a hurricane of a similar intensity that made landfall directly on the delta. This demonstrates that the wetlands located on the right side of a hurricane track experience more significant morphological changes than areas located directly on the hurricane track...|$|E
40|$|Keywords: POP-II. B. program comprehension, POP-V. B. <b>eye</b> <b>tracking</b> <b>Eye</b> <b>tracking</b> {{can be used}} in {{measuring}} point of gaze data that provides information concerning subject’s focus of attention. The focus of subject’s attention can be used as supportive evidence in studying cognitive processes. Despite the potential usefulness of <b>eye</b> <b>tracking</b> in psychology of programming research, there exists only few instances where <b>eye</b> <b>tracking</b> has actually been used. This paper presents an experiment in which we used three <b>eye</b> <b>tracking</b> devices to record subjects’ points of gaze when they were studying short computer programs using a program animator. The results suggest that <b>eye</b> <b>tracking</b> can be used to collect relatively accurate data for the purposes of psychology of programming research. The results also revealed significant differences between the devices in the accuracy of the point of gaze data and in the times needed for setting up the monitoring process...|$|R
50|$|In 2013 TechViz {{integrated}} SMI’s 3D <b>Eye</b> <b>Tracking</b> Glasses with TechViz 3D visualization {{software to}} enable <b>eye</b> <b>tracking</b> {{in a virtual}} reality CAVE. The 3D <b>Eye</b> <b>Tracking</b> Glasses were developed in partnership with Volfoni. In the same year WorldViz started cooperating with SMI to enable calculation intersects of gaze vectors with 3D objects and saving the data in one common database for deeper analysis. German Research Center for Artificial Intelligence (DFKI) used the <b>Eye</b> <b>Tracking</b> Glasses to create Talking Places - the prototype of an interactive city guide.|$|R
40|$|The {{development}} of cheaper eye trackers and {{open source software}} for <b>eye</b> <b>tracking</b> and gaze interaction brings the possibility to integrate <b>eye</b> <b>tracking</b> into everyday use devices as well as highly specialized equipment. Apart from providing means for analyzing <b>eye</b> movements, <b>eye</b> <b>tracking</b> also offers {{the possibility of a}} natural user interaction modality. Gaze control interfaces are already used within assistive applications for disabled users. However, this novel user interaction possibility comes with its own set of limitations and challenges. The aim of this SIG is to provide a forum for Designers, Researchers and Usability Professionals to discuss the role of <b>eye</b> <b>tracking</b> as a user interaction method in the future as well as the technical and user interaction challenges that using <b>eye</b> <b>tracking</b> as an interaction method brings...|$|R
40|$|A major {{challenge}} in generating high-fidelity virtual environments {{for use in}} Virtual Reality (VR) {{is to be able}} to provide interactive rates of realism. The high-fidelity simulation of light and sound wave propagation is still unachievable in real-time. Physically accurate simulation is very computationally demanding. Only recently has visual perception been used in high-fidelity rendering to improve performance by a series of novel exploitations; to render parts of the scene that are not currently being attended by the viewer at a much lower quality with-out the difference being perceived. This thesis investigates the effect spatialized directional sounds, both discrete and converged and smells have on the visual attention of the user towards rendered scene images. These perceptual artefacts are utilised in selective rendering pipelines via the use of multi-modal maps. This work verifies the worth of investigating subliminal saccade shifts (fast movements of the eyes) from directional audio impulses via a pilot study to <b>eye</b> <b>track</b> participant's free viewing a scene with and without an audio impulse and with and without a congruency for that impulse. This experiment showed that even without an acoustic identifier in the scene, directional sound provides an impulse to guide subliminal saccade shifts. A novel technique for generating interactive discrete acoustic samples from arbitrary geometry is also presented. This work is extrapolated by investigating whether temporal auditory sound wave saliencies can be used as a feature vector in the image rendering process. The method works by producing image maps of the sound wave flux and attenuating this map via these auditory saliency feature vectors. Whilst selectively rendering, the method encodes spatial auditory distracters into the standard visual saliency map. Furthermore, this work investigates the effect various smells have on the visual attention of a user when free viewing a set of images whilst being eye tracked. This thesis explores these saccade shifts to a congruent smell object. By analysing the gaze points, the time spent attending a particular area of a scene is considered. The work presents a technique derived from measured data to modulate traditional saliency maps of image features to account for the observed results for smell congruences and shows that smell provides an impulse on visual attention. Finally, the observed data is used in applying modulated image saliency maps to address the additional effects cross-modal stimuli has on human perception when applied to a selective renderer. These multi-modal maps, derived from measured data for smells, and from sound spatialisation techniques attempt to exploit the extra stimuli presented in multi-modal VR environments and help to re-quantify the saliency map to account for observed cross-modal perceptual features of the human visual system. The multi-modal maps are tested through rigorous psychophysical experiments to examine their applicability to selective rendering algorithms, with a series of fixed cost rendering functions, and are found to perform better than image saliency maps that are naively applied to multi-modal virtual environments...|$|E
40|$|Visual {{categorization}} {{and learning}} of visual categories exhibit early onset, however {{the underlying mechanisms}} of early cat-egorization are not well understood. The main limiting factor for examining these mechanisms is the limited duration of in-fant cooperation (10 - 15 minutes), which leaves little room for multiple test trials. With its tight link to visual attention, <b>eye</b> <b>tracking</b> is a promising method for getting access to the mecha-nisms of category learning. But how should researchers decide which aspects of the rich <b>eye</b> <b>tracking</b> data to focus on? To date, <b>eye</b> <b>tracking</b> variables are generally handpicked, often re-sulting in biases of the <b>eye</b> <b>tracking</b> data. Here, we propose an automated method for selecting <b>eye</b> <b>tracking</b> variables based on their usefulness to discriminate learners from non-learners of visual categories. We presented infants and adults with a category learning task and <b>tracked</b> their <b>eye</b> movements. We then extracted an over-complete set of <b>eye</b> <b>tracking</b> variables encompassing durations, probabilities, latencies, and the order of fixations and saccadic eye movements. We applied the sta-tistical technique of ANOVA ranking to identifying those vari-ables among this large set that covaried with the learner/non-learner label. We {{were able to identify}} learners and non-learners above chance level using linear SVM and the top <b>eye</b> <b>tracking</b> variables...|$|R
40|$|During {{the last}} decade, <b>eye</b> <b>tracking</b> {{technology}} has undergone rapid development and growth that {{has increased its}} popularity amongst practitioners and researchers {{from a wide variety}} of disciplines. In spite of widespread applications of <b>eye</b> <b>tracking</b> in the computing and human factor domains, considerably less attention has been paid to the potential of this technology to improve the design and construction processes. Currently, the ability of using eye trackers in construction applications is limited due to the lack of knowledge about this technology and its potential benefits. This paper provides an overview and guidance for the implementation of <b>eye</b> <b>tracking</b> technology in the construction process. The description of the technology and its current applications are also provided, with a brief discussion of potential application and the limitations. A description of a use case example is provided to investigate the use of <b>eye</b> <b>tracking</b> technology to measure and analyze the end-user satisfaction in the design process. This study provides the construction industry with information about how to design an <b>eye</b> <b>tracking</b> experiment and analyze the data generated by the <b>eye</b> <b>tracking</b> tools...|$|R
40|$|From {{scientific}} research to commercial applications, <b>eye</b> <b>tracking</b> {{is an important}} tool across many domains. Despite its range of applications, <b>eye</b> <b>tracking</b> has yet to become a pervasive technology. We {{believe that we can}} put the power of <b>eye</b> <b>tracking</b> in everyone's palm by building <b>eye</b> <b>tracking</b> software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for <b>eye</b> <b>tracking,</b> containing data from over 1450 people consisting of almost 2 : 5 M frames. Using GazeCapture, we train iTracker, a convolutional neural network for <b>eye</b> <b>tracking,</b> which achieves a significant reduction in error over previous approaches while running in real time (10 - 15 fps) on a modern mobile device. Our model achieves a prediction error of 1. 71 cm and 2. 53 cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1. 34 cm and 2. 12 cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results...|$|R
40|$|Recent {{studies on}} gaze {{behaviours}} in individuals with autism spectrum disorder (ASD) have utilised “live <b>eye</b> <b>tracking.</b> ” Such {{research has focused}} on generating quantitative <b>eye</b> <b>tracking</b> measurements, which provide limited (if any) qualitative contextual details of the actual interactions in which gaze occurs. This article presents a novel methodological approach that combines live <b>eye</b> <b>tracking</b> with qualitative interaction analysis, multimodally informed conversation analysis. Drawing on <b>eye</b> <b>tracking</b> and wide-angle video recordings, this combination renders visible some of the functions, or what gaze “does,” in interactional situations. The participants include three children with ASD and their adult co-participants during body-movement gaming sessions. The article demonstrates how quantitative <b>eye</b> <b>tracking</b> research can be extended qualitatively using a microanalytic interaction analysis to recontextualise the gaze shifts identified. The findings in this article show that the co-participants treat a child’s gaze shifts differently depending on when these occur in a stream of other action. The study suggests that introducing this qualitative dimension to <b>eye</b> <b>tracking</b> research could increase its ecological validity and offer new insight into gaze behaviours in ASD...|$|R
40|$|International audienceThe {{accuracy}} of the <b>eye</b> <b>tracking</b> systems is a key indicator of data validity. Currently developed <b>eye</b> <b>tracking</b> systems can be configured {{to be used as}} remote wireless autonomous systems. In order to meet the required constraints of system's responsiveness and effectively use the available hardware, image compression techniques can be employed {{in order to reduce the}} amount of data needed to be sent via a physical transmission link. Since <b>eye</b> <b>tracking</b> systems are sensitive to input image details it is necessary to preserve as much details as possible. In this paper we are presenting results of accuracy gradation for an <b>eye</b> <b>tracking</b> system depending on the quality of the decompressed image. This image is used as input for the <b>eye</b> <b>tracking</b> system in order to find the minimal acceptable quality that leads to neglectable tracking systematic errors...|$|R
