0|5797|Public
50|$|After 1974, Austin {{was head}} of the <b>Subject</b> <b>System</b> Office, The British Library.|$|R
50|$|A common {{example of}} {{unavoidable}} bypass is a <b>subject</b> <b>system</b> {{that is required}} to accept secret IP packets from an untrusted source, encrypt the secret userdata and not the header and deposit the result to an untrusted network. The source lies outside the sphere of influence of the <b>subject</b> <b>system.</b> Although the source is untrusted (e.g. system high) it is being trusted {{as if it were}} MLS because it provides packets that have unclassified headers and secret plaintext userdata, an MLS data construct. Since the source is untrusted, it could be corrupt and place secrets in the unclassified packet header. The corrupted packet headers could be nonsense but it is impossible for the <b>subject</b> <b>system</b> to determine that with any reasonable reliability. The packet userdata is cryptographically well protected but the packet header can contain readable secrets. If the corrupted packets are passed to an untrusted network by the <b>subject</b> <b>system</b> they may not be routable but some cooperating corrupt process in the network could grab the packets and acknowledge them and the <b>subject</b> <b>system</b> may not detect the leak. This can be a large overt leak that is hard to detect. Viewing classified packets with unclassified headers as system high structures instead of the MLS structures they really are presents a very common but serious threat.|$|R
40|$|A {{number of}} {{approaches}} for spanning the requirements-archi-tecture gap {{have been published}} in recent years, and we sought to rigorously characterize the gap and to conduct a comparative evaluation of approaches to span the gap using a case study method on a realistic problem. However, our in-tentions were impeded by the problem of finding appropriate <b>subject</b> <b>systems</b> that included sufficient information in both requirements and architecture document. Most subject sys-tems that we found contained either detailed requirements or detailed architecture description, but not both. In this paper, we report on our search and the seventeen most suit-able <b>subject</b> <b>systems</b> with the hope of aiding others under-taking a similar study. We speculate on the reasons for the paucity of suitable <b>subject</b> <b>systems</b> and invite contributions and suggestions for our ongoing work. 1...|$|R
40|$|Visualizing the {{artifacts}} of a software system graphically {{has proven to}} improve the cognitive strategies {{and understanding of the}} <b>subject</b> <b>system</b> by programmers. This is more crucial when they need to maintain a software system with out-dated documentation or without system documentation at all. Many tools have emerged and they predominantly consist of a reverse engineering environment and a viewer to visualize software artifacts such as in the form of graphs. The tools also grant structural re-documentation of existing software system but they do not directly utilize document-like software visualization in their approaches. This paper proposes DocLike Modularized Graph (DMG) method that represents the software architectures of a reverse engineered <b>subject</b> <b>system</b> graphically in a modularized and standardized document-like manner. To realize this method, we have built a prototype tool called DocLike Viewer that enables a user to re-document, visualize and comprehend a <b>subject</b> <b>system</b> written in C language that is parsed by an existing parser. From the experiment conducted we found that our method managed to statistically improve cognition of a <b>subject</b> <b>system</b> in terms of productivity and quality to solve certain types of maintenance tasks. 1...|$|R
40|$|Abstract—Currently, {{the impacts}} of clones in {{software}} main-tenance activities are being investigated by different researchers in different ways. Comparative stability analysis of cloned and non-cloned regions of a <b>subject</b> <b>system</b> is a well-known way of measuring the impacts where the hypothesis is that, the more a region is stable the less it is harmful for maintenance. Each of the existing stability measurement methods lacks to address one important characteristic, dispersion, of the changes happening in the cloned and non-cloned regions of software systems. Change dispersion of a particular region quantifies {{the extent to which}} the changes are scattered over that region. The intuition is that, more dispersed changes require more efforts to be spent in the maintenance phase. Measurement of Dispersion requires the extraction of method genealogies. In this paper, we have measured the dis-persions of changes in cloned and non-cloned regions of several <b>subject</b> <b>systems</b> using a concurrent and robust framework for method genealogy extraction. We implemented the framework on Actor Architecture platform which facilitates coarse grained parallellism with asynchronous message passing capabilities. Our experimental results with 12 open-source <b>subject</b> <b>systems</b> written in three different programming languages (Java, C and C#) using two clone detection tools suggest that, the changes in cloned regions are more dispersed than the changes in non-cloned regions. Also, Type- 3 clones exhibit more dispersion as compared to the Type- 1 and Type- 2 clones. The <b>subject</b> <b>systems</b> written in Java and C show higher dispersions as well as increased maintenance efforts as compared to the <b>subject</b> <b>systems</b> written in C#...|$|R
40|$|Shimba, a {{prototype}} reverse engineering environment, {{has been built}} to support the understanding of Java software. Shimba uses Rigi and SCED to analyze, visualize, and explore the static and dynamic aspects, respectively, of the <b>subject</b> <b>system.</b> The static software artifacts and their dependencies are extracted from Java byte code and viewed as directed graphs using the Rigi reverse engineering environment. The static dependency graphs of a <b>subject</b> <b>system</b> can be annotated with attributes, such as software quality measures, and then be analyzed and visualized using scripts through the end-user programmable interface. Shimba has recently been extended with the Chidamber and Kemerer suite of object-oriented metrics. The metrics measure properties of the classes, the inheritance hierarchy, and the interaction among classes of a <b>subject</b> <b>system.</b> Since Shimba is primarily intended for the analysis and exploration of Java software, the metrics have been tailored to measure properties of softwa [...] ...|$|R
40|$|Report {{presents}} {{procedures and}} {{results of test}} of FCC baseboard system developed for use in commerical and residential applications. Mechanical, electrical, chemical, environmental, thermal, and analytical tests <b>subjected</b> <b>system</b> to conditions of greater severity than would be encountered in normal service; system withstood tests favorably...|$|R
40|$|Consideration {{is given}} to the System Diagnostic Builder (SDB), an {{automated}} knowledge acquisition tool using state-of-the-art AI technologies. The SDB employs an inductive machine learning technique to generate rules from data sets that are classified by a subject matter expert. Thus, data are captured from the <b>subject</b> <b>system,</b> classified, and used to drive the rule generation process. These rule bases are used to represent the observable behavior of the <b>subject</b> <b>system,</b> and to represent knowledge about this system. The knowledge bases captured from the Shuttle Mission Simulator can be used as black box simulations by the Intelligent Computer Aided Training devices. The SDB {{can also be used to}} construct knowledge bases for the process control industry, such as chemical production or oil and gas production...|$|R
40|$|Abstract: Understanding an {{existing}} software system to trace possible changes {{involved in a}} maintenance task can be time consuming especially if its design document is absence or out-dated. In this case, visualizing the software artefacts graphically may improve the cognition of the <b>subject</b> <b>system</b> by software maintainers. A number of tools have emerged and they generally consist of a reverse engineering environment and a viewer to visualize software artefacts {{such as in the}} form of graphs. The tools also grant structural re-documentation of existing software systems but they do not explicitly employ document-like software visualization in their methods. This paper proposes DocLike Modularized Graph method that represents the software artefacts of a reverse engineered <b>subject</b> <b>system</b> graphically, module-by-module in a document-like re-documentation environment. The method is utilized in a prototype tool named DocLike viewer that generates graphical views of a C language software system parsed by a selected C language parser. Two experiments were conducted to validate how much the proposed method could improve cognition of a <b>subject</b> <b>system</b> by software maintainers without documentation, in terms of productivity and quality. Both results deduce that the method has the potential to improve cognitive aspects of software visualization to support software maintainers in finding solutions of assigned maintenance tasks...|$|R
5000|$|Jack Eapen based the WebBiblio <b>Subject</b> Gateway <b>System</b> on OpenBiblio.|$|R
40|$|The {{landscape}} of reverse engineering {{is rich in}} tools for the recovery, quantification, and analysis of source code. Most of these tools, however, cover only a small slice of what the notion of reverse engineering promises: “a process of analyzing a <b>subject</b> <b>system</b> to (a) identify the system’s components and their interrelationship...|$|R
40|$|Object graphs {{help explain}} the runtime {{structure}} of a system. To make object graphs convey design intent, one insight is to use abstraction by hierarchy, i. e., to show objects that are implementation details as children of architecturally-relevant objects from the application domain. But additional information is needed to express this object hierarchy, using ownership type qualifiers in the code. Adding qualifiers after the fact involves manual overhead, and requires developers to switch between adding qualifiers in the code and looking at abstract object graphs to understand the object structures that the qualifiers describe. We propose an approach where developers express their design intent by refining an object graph directly, while an inference analysis infers valid qualifiers in the code. We present, formalize and implement the inference analysis. Novel features of the inference analysis compared to closely related work include a larger set of qualifiers to support less restrictive object hierarchy (logical containment) in addition to strict hierarchy (strict encapsulation), as well as object uniqueness and object borrowing. A separate extraction analysis then uses these qualifiers and extracts an updated object graph. We evaluate the approach on two <b>subject</b> <b>systems.</b> One of the <b>subject</b> <b>systems</b> is reproduced from an experiment using related techniques and another ownership type system, which enables a meaningful comparison. For the other <b>subject</b> <b>system,</b> we use its documentation to pick refinements that express design intent. We compute metrics on the refinements (how many attempts on each <b>subject</b> <b>system)</b> and classify them by their type. We also compute metrics on the inferred qualifiers and metrics on the object graphs to enable quantitative comparison. Moreover, we qualitatively compare the hierarchical object graphs with the flat object graphs and with each other, by highlighting how they express design intent. Finally, we confirm that the approach can infer from refinements valid qualifiers such that the extracted object graphs reflect the design intent of the refinements...|$|R
40|$|This paper {{describes}} a structured tool demonstration, a hybrid evaluation technique that combines elements from experiments, case studies, and technology demonstrations. Developers of program understanding tools {{were invited to}} bring their tools to a common location {{to participate in a}} scenario with a common <b>subject</b> <b>system.</b> Working simultaneously, the tool teams were given reverse engineering tasks and maintenance tasks to complete on an unfamiliar <b>subject</b> <b>system.</b> Observers were assigned to each team to find out how useful the observed program comprehension tool would be in an industrial setting. The demonstration was followed by a workshop panel where the development teams and the observers presented their results and findings from this experience. Keywords Empirical study, program comprehension, tool evaluation. 1. Introduction During the past decade, many tools have been developed both in industry and research to support reverse engineering and program understanding. There is no [...] ...|$|R
40|$|This paper {{describes}} {{our approach}} to creating higher-level abstract representations of a <b>subject</b> <b>system,</b> which involves the identification of related components and dependencies, the construction of layered subsystem structures, and the computation of exact interfaces among subsystems. We show how top-down decompositions of a <b>subject</b> <b>system</b> can be (re) constructed via bottom-up subsystem composition. This process involves identifying groups of building blocks (e. g., variables, procedures, modules, and subsystems) using composition operations based on software engineering principles such as low coupling and high cohesion. The result is an architecture of layered subsystem structures. The structures are manipulated and recorded using the Rigi system, which consists of a distributed graph editor and a parsing system with a central repository. The editor provides graph filters and clustering operations to build and explore subsystem hierarchies interactively. The paper concludes with a detailed, step-by-step analysis of a 30 -module software system using Rigi...|$|R
40|$|Code cloning is a {{controversial}} software engineering practice due to contradictory claims regarding {{its effect on}} software maintenance. Code stability is a recently introduced measurement technique {{that has been used}} to determine the impact of code cloning by quantifying the changeability of a code region. Although most of the existing stability analysis studies agree that cloned code is more stable than noncloned code, the studies have two major flaws: (i) each study only considered a single stability measurement (e. g., lines of code changed, frequency of change, age of change); and, (ii) {{only a small number of}} <b>subject</b> <b>systems</b> were analyzed and these were of limited variety. In this paper, we present a comprehensive empirical study on code stability using three different stability measuring methods. We use a recently introduced hybrid clone detection tool, NiCAD, to detect the clones and analyze their stability in four dimensions: by clone type, by measuring method, by programming language, and by system size and age. Our four-dimensional investigation on 12 diverse <b>subject</b> <b>systems</b> written in three programming languages considering three clone types reveals that: (i) Type- 1 and Type- 2 clones are unstable, but Type- 3 clones are not; (ii) clones in Java and C systems are not as stable as clones in C# systems; (iii) a system’s development strategy might play a key role in defining its comparative code stability scenario; and, (iv) cloned and non-cloned regions of a <b>subject</b> <b>system</b> do not follow a consistent change pattern...|$|R
5000|$|... 577 students. A <b>subject</b> electives <b>system</b> was {{introduced}} at Level 9 and 10.|$|R
5000|$|My LyceumAdvise: Online <b>Subject</b> Advising <b>System</b> of LPU By: Ms. Evelyn Red (2011) ...|$|R
40|$|Abstract: This paper compare several {{differences}} with other <b>subject</b> information gateway <b>systems,</b> such as embody scope, resource types, organization system, resource description, retrieval function, appreciation service, renewal and maintenance, cooperation model,amount of data etc. For the more, the paper put forward some good ideas of developing <b>Subject</b> Gateway <b>system...</b>|$|R
40|$|The X- 15 {{airplane}} {{performance and}} operational requirements define a landing-gear {{system that will}} be subjected to high temperatures and high landing speeds and that will expend a minimum of airplane space and weight. This paper is concerned primarily with the landing-gear design configuration concept, the reporting of several unique design features that were incorporated, and description of the developmental testing of the <b>subject</b> <b>system.</b> (author...|$|R
50|$|The BBC's Lonclass ("London Classification") is a <b>subject</b> {{classification}} <b>system</b> used internally at the BBC {{throughout its}} archives.|$|R
40|$|Whitesands is an under-described Oceanic {{language}} {{belonging to}} the Southern Vanuatu sub-family. Like other languages in the Southern Vanuatu family it utilises a system of `switch reference' that has been called "Echo Subject" (ES) (Lynch 1983, Crowely 2002). This paper investigates the underlying syntactic properties of the Echo <b>Subject</b> <b>system</b> in Whitesands. We conclude that there are features {{of the system that}} are formally complex but also subject to pragmatic influence...|$|R
40|$|This paper {{presents}} {{the conception of}} computing curricula in an African university with a further attempt at balancing computability and usability concerns in computing curricula. The key changes to the existing curricula in moving from a <b>Subject</b> <b>system</b> to a Semeterised Course system is presented. The alignment of the approach to computing curricula conception with the local needs is discussed. Key issues emanating from the case is examined from the operational, contextual and strategic perspectives...|$|R
40|$|Abstract. Software {{evolution}} involves {{different categories}} of interven-tions, having variable {{impact on the}} code. Knowledge about the expected impact of an intervention is fundamental for project planning and re-source allocation. Moreover, deviations from the expected impact may hint for areas of the system having a poor design. In this paper, we inves-tigate the relationship between evolution categories and impacted code {{by means of a}} set of metrics computed over time for a <b>subject</b> <b>system...</b>|$|R
5000|$|Organized complexity, in Weaver's view, {{resides in}} nothing else than the non-random, or correlated, {{interaction}} between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity vis-a-vis to other <b>systems</b> than the <b>subject</b> <b>system</b> {{can be said to}} [...] "emerge," [...] without any [...] "guiding hand".|$|R
5000|$|While {{proteomics}} {{is largely}} a discovery-based approach that is followed by other molecular or analytical techniques to provide a full picture of the <b>subject</b> <b>system,</b> it {{is not limited to}} simple cataloging of proteins present in a sample. With the combined capabilities of [...] "top-down" [...] and [...] "bottom-up" [...] approaches, proteomics can pursue inquiries ranging from quantitation of gene expression between growth conditions (whether nutritional, spatial, temporal, or chemical) to protein structural information.|$|R
40|$|Abstract—The {{impacts of}} clones on {{software}} maintenance is a long-lived debate on whether clones are beneficial or not. Some researchers argue that clones lead to additional changes during the maintenance phase and thus increase the overall maintenance effort. Moreover, {{they note that}} inconsistent changes to clones may introduce faults during evolution. On the other hand, other researchers argue that cloned code exhibits more stability than non-cloned code. Studies resulting in such contradictory outcomes may be a consequence of using different methodologies, using different clone detection tools, defining different impact assessment metrics, and evaluating different <b>subject</b> <b>systems.</b> In {{order to understand the}} conflicting results from the studies, we plan to conduct a comprehensive empirical study using a common framework incorporating nine existing methods that yielded mostly contradictory findings. Our research strategy involves implementing each of these methods using four clone detection tools and evaluating the methods on more than fifteen <b>subject</b> <b>systems</b> of different languages and of a diverse nature. We believe that our study will help eliminate tool and study biases to resolve conflicts regarding the impacts of clones on software maintenance. Keywords-–Clone Evolution; Code Stability; Experiment. I...|$|R
40|$|Although no {{standard}} definition exists, {{reverse engineering}} {{has traditionally been}} defined as a twostep process of information extraction followed by information abstraction. The first step analyzes the <b>subject</b> <b>system</b> to identify its components and their inter-relationships. The second step creates representations of the system in another form or at a higher abstraction level. Recently, {{a new approach to}} reverse engineering was advocated [Til 95] that refines this traditional two-step approach of information extraction and abstraction. The new three-step approach is as follows: Model: Construct domain-specific models of the application using conceptual modeling techniques. Extract: Gather the raw data from the <b>subject</b> <b>system</b> using the appropriate extraction mechanisms. Abstract: Create abstractions that facilitate program understanding and permit the navigation, analysis, and presentation of the resultant information structures. It is this definition reverse engineering is seen as an activity which does not change the subject system; it is a process of examination, not a process of alteration. It can facilitate the understanding process through the identification of artifacts, the discovery of their relationships, and the generation of abstractions. This process is dependent on one's cognitive abilities and preferences, on one'...|$|R
40|$|The study {{presented}} in this paper aims at analyzing empirically the quality of evolving software systems using metrics. We used a synthetic metric (Quality Assurance Indicator - Qi), which captures in an integrated way different object-oriented software attributes. We wanted to investigate if the Qi metric can be used to observe how quality evolves along the evolution of software systems. We consider software quality from an internal (structural) perspective. We used various object-oriented design metrics for measuring the structural quality of a release. We performed an empirical analysis using historical data collected from successive released versions of three open source (Java) software systems. The collected data cover, for each system, a period of several years (4 years for two systems and 7 years for the third one). We focused on three issues: (1) the evolution of the Qi metric along the evolution of the <b>subject</b> <b>systems,</b> (2) the class growth of the <b>subject</b> <b>systems,</b> and (3) the quality of added classes versus the quality of removed ones. Empirical results provide evidence that the Qi metric reflects properly the quality evolution of the studied software systems. </p...|$|R
40|$|The {{basic idea}} behind {{software}} reuse is to exploit similarities {{within and across}} software systems to avoid repetitive development work. Conventional reuse is based on components and architectures. We describe how reuse of structural similarities extends the benefits of conventional component reuse, and realization of the concept with a generative technique of XVCL 1. Structural similarities are repetition patterns in software of any type or granularity, from similar code fragments to recurring architecturelevel component configuration patterns. We represent any significant repetition pattern in <b>subject</b> <b>system(s)</b> with a generic, adaptable, XVCL meta-structure. We develop, reuse and evolve software {{at the level of}} meta-structures, deriving specific, custom systems from their meta-level representations. Lab studies and industrial applications of XVCL show that by doing that, on average, we raise reuse rates and productivity by 60 - 90 %, reducing cognitive program complexity and maintenance effort by similar rates. The approach scales to systems of any size. The benefits are proportional to system size, and to the extent of repetitions present in <b>subject</b> <b>system(s).</b> The main application of this reuse strategy is in supporting software Product Lines. 1...|$|R
40|$|In this paper, we analyse the {{exception}} handling mechanism of a state-of-the-art industrial embedded software system. Like many systems implemented in classic programming languages, our <b>subject</b> <b>system</b> uses the popular return-code idiom {{for dealing with}} exceptions. Our goal is to evaluate the fault-proneness of this idiom, and we therefore present a characterisation of the idiom, a fault model accompanied by an analysis tool, and empirical data. Our findings show that the idiom is indeed fault prone, but that a simple solution can lead to significant improvements. 1...|$|R
40|$|The {{purpose of}} this panel is to report on a {{structured}} demonstration for comparing program comprehension tools. Five program comprehension tool designers applied their tools {{to a set of}} maintenance tasks on a common <b>subject</b> <b>system.</b> By applying a variety of reverse engineering techniques to a predefined set of tasks, the tools can be compared using a common playing field. A secondary topic of discussion will address the development of “guinea pig ” systems and how to use them in a structured demonstration for evaluating software tools. ...|$|R
40|$|The {{cooling system}} for the X- 15 {{airplane}} incorporates insulation partial protection against high-temperature effects plus liquid and gaseous nitrogen to control specifically environmental and equipment temperatures. Electronic-equipment cooling represents the largest load; however, other items to be considered are the pressure-suit ventilation, aerodynamic-heating effects, sensor-head cooling, and windshield antifogging. A review of various types of cooling systems indicates that an expendable, stored, cooling system is the most efficient for short-duration missions. This paper presents a description including a simplified schematic diagram of the <b>subject</b> <b>system.</b> (author...|$|R
5000|$|The use of endings to show {{person on}} verbs in {{parallel}} with a pronominal <b>subject</b> <b>system,</b> thus [...] "I must" [...] is in Munster [...] as well as , while other dialects prefer [...] ( [...] means [...] "I"). [...] "I was and you were" [...] is [...] as well as [...] in Munster, but more commonly [...] in other dialects. Note that these are strong tendencies, and the personal forms Bhíos etc. {{are used in the}} West and North, particularly when the words are last in the clause.|$|R
40|$|International Telemetering Conference Proceedings / October 29 -November 02, 1990 / Riviera Hotel and Convention Center, Las Vegas, NevadaTelemetry system {{requirements}} {{are driven by}} technological developments in other areas, thus the capabilities of one are mirrored in {{the capabilities of the}} other. Contemporary systems typically involve two or more digital subsystems, each operating at a unique clock rate; an increase in complexity {{that needs to be addressed}} by the Telemetry system designer. Although the subsystems may be exchanging information, complete synchronization is seldom realized in discrete systems. Because the Telemetry system must provide information sufficient to isolate data/process corruption, it must accept data from the various subsytems at different rates and times. What is needed is a technique to de-couple the Telemetry system clock rate from that of the <b>Subject</b> <b>system</b> or any of its subsystems. This technique must bridge the gap between the synchronous data transmission fundamental to the Telemetry system and the asynchronous data transfer required by the various non integrated subsystems. This paper will discuss the design challenges offered by such a <b>Subject</b> <b>system</b> for both real time and post flight analysis. It will discuss how the restrictions imposed by the IRIG standards and anticipated mission requirements factored into developing the architecture for a Generic Multi-Port Digital Telemetry Interface...|$|R
40|$|The Master's {{thesis is}} focused on the Polythematic Structured <b>Subject</b> Heading <b>System,</b> an {{indexing}} language created at the State Technical Library and used for document indexing and retrieval both in this library and in other institutions. The objective of the thesis is to evaluate the history of the <b>subject</b> heading <b>system</b> and suggest ways how it can be further developed. First of all the history of the <b>subject</b> heading <b>system</b> is described, both in the context of indexing and classification of documents in the State Technical Library and trends in knowledge organization in general. Then the current situation of the system is analyzed both from the formal and factual point of view. Tools used for the maintenance and presentation of the system are highlighted. The thesis also includes the description of relevant processes and suggests ways how to optimize the processes. Updating, maintenance and marketing are especially emphasized. The possibilities to use modern information and communication technologies for the further development of the <b>subject</b> heading <b>system</b> are described in detail. Authority files, markup languages and on-line library catalogues are included in the analysis. A draft of a new development policy for the system and new maintenance and indexing rules are also part of the thesis...|$|R
40|$|<b>Subject</b> {{headings}} <b>systems</b> are {{tools for}} {{organization of knowledge}} {{that have been developed}} over the years by libraries. The SKOS Simple Knowledge Organization System provides a practical way to represent <b>subject</b> headings <b>systems,</b> and several libraries have taken the initiative to make these systems widely available as open linked data. Each individual subject heading describes a concept, however, in the majority of cases, one subject heading is actually a combination of several concepts, such as a topic bounded in geographical and temporal scopes. In these cases, the label of the concept actually contains several concepts which are not represented in structured form. This paper address the alignment of the geographic concepts described in <b>subject</b> headings <b>systems</b> with their correspondence in geographic ontologies. Our approach first recognizes the place names in the subject headings using entity recognition techniques and follows with the resolution of the place names in a target geographic ontology. The system is based on machine learning and was designed to be language independent {{so that it can be}} applied to the many existing <b>subject</b> headings <b>systems.</b> Our approach was evaluated on a subset of the Library of Congress Subject Headings, achieving an F 1 score of 93 %...|$|R
