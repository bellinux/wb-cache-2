72|98|Public
25|$|Turing {{thought that}} the speed {{and the size of}} {{computer}} memory were crucial elements, so he proposed a <b>high-speed</b> <b>memory</b> of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used Abbreviated Computer Instructions, an early form of programming language.|$|E
25|$|In 1989, Cray {{was faced}} with a repeat of history when the Cray-3 started to run into difficulties. An upgrade of the X-MP using <b>high-speed</b> <b>memory</b> from the Cray-2 was under {{development}} and seemed to be making real progress, and once again management {{was faced with}} two projects and limited budgets. They eventually decided to take the safer route, releasing the new design as the Cray Y-MP.|$|E
5000|$|Two-Channel Low-Latency DMA Controller for <b>High-Speed</b> <b>Memory</b> Transfers ...|$|E
5000|$|... {{to support}} {{a wide range of}} {{requirements}} from <b>high-speed</b> manual <b>memory</b> management, to complex garbage collection with many different types of reference.|$|R
50|$|Streaming Transformations for XML (STX) is an XML {{transformation}} language {{intended as}} a <b>high-speed,</b> low <b>memory</b> consumption alternative to XSLT version 1.0 and 2.0. Current work on XSLT 3.0 includes Streaming capabilities.|$|R
50|$|The chip {{added the}} ability to use all or half of its cache as <b>high-speed,</b> non-cached <b>memory</b> mapped to the processor's {{physical}} address space as desired. This feature was used by embedded systems vendors such as Mercury Computer Systems.|$|R
5000|$|XC APIs - the [...] "premium features" [...] for Flash Player, {{consisting}} of <b>high-speed</b> <b>memory</b> manipulation opcodes (known as [...] "Alchemy") and Stage3D APIs.|$|E
50|$|SMP {{systems have}} {{centralized}} shared memory called main memory (MM) operating {{under a single}} operating system with two or more homogeneous processors. Usually each processor has an associated private <b>high-speed</b> <b>memory</b> known as cache memory (or cache) {{to speed up the}} main memory data access and to reduce the system bus traffic.|$|E
50|$|Verigy designs, develops, manufactures, sells and {{services}} advanced semiconductor test {{systems for the}} flash memory, <b>high-speed</b> <b>memory</b> and system-on-chip (SoC) markets. Verigy’s products are used worldwide in design validation, characterization, and high-volume manufacturing test. The company began doing business as Verigy on June 1, 2006 with its global headquarters located in Singapore.|$|E
50|$|Several {{universities}} {{and a number}} of companies (Hewlett Packard, ZettaCore) have announced work on molecular memories, which some hope will supplant DRAM memory as the lowest cost technology for <b>high-speed</b> computer <b>memory.</b> NASA is also supporting research on non-volatile molecular memories.|$|R
2500|$|The DDR4 SDRAM is a <b>high-speed</b> dynamic random-access <b>memory</b> {{internally}} configured as sixteen-banks, 4 bank {{group with}} 4 ...|$|R
5000|$|... #Caption: The Cromemco XXU {{processor}} board, {{introduced in}} 1986. At 16.7 MHz {{it was the}} fastest CPU ever developed for the S-100 bus. It used a Motorola 68020 processor with 68881 co-processor and 16 Kbytes of <b>high-speed</b> cache <b>memory.</b> This CPU {{was used in the}} Cromemco CS-250 computer, widely deployed by the U.S. Air Force.|$|R
50|$|Turing {{felt that}} speed {{and size of}} memory were crucial and he {{proposed}} a <b>high-speed</b> <b>memory</b> of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used Abbreviated Computer Instructions, an early form of programming language.|$|E
50|$|When DVI was designed, most {{computer}} monitors were still of the {{cathode ray tube}} type that require analog video synchronization signals. The timing of the digital synchronization signals matches the equivalent analog ones, making the process of transforming DVI to and from an analog signal a process that does not require extra (<b>high-speed)</b> <b>memory,</b> expensive at the time.|$|E
50|$|In 1989, Cray {{was faced}} with a repeat of history when the Cray-3 started to run into difficulties. An upgrade of the X-MP using <b>high-speed</b> <b>memory</b> from the Cray-2 was under {{development}} and seemed to be making real progress, and once again management {{was faced with}} two projects and limited budgets. They eventually decided to take the safer route, releasing the new design as the Cray Y-MP.|$|E
5|$|Each {{background}} processor {{consisted of}} a computation section, a control section and local memory. The computation section performed 64-bit scalar, floating point and vector arithmetic. The control section provided instruction buffers, memory management functions, and a real-time clock. 16 kwords (128 kbytes) of <b>high-speed</b> local <b>memory</b> was incorporated into each background processor for use as temporary scratch memory.|$|R
30|$|The {{technology}} of mobile devices has evolved rapidly {{over the last}} decade. Today’s high-end mobile devices have <b>high-speed</b> processors, <b>memory</b> as large as 256 gigabytes, high-resolution monitors and an LTE or 4 G wireless network connection. Depending on the screen size, mobile devices can be classified as smartphones (small screen), tablets (large screen) and phablets (medium screen).|$|R
50|$|Each {{background}} processor {{consisted of}} a computation section, a control section and local memory. The computation section performed 64-bit scalar, floating point and vector arithmetic. The control section provided instruction buffers, memory management functions, and a real-time clock. 16 kwords (128 kbytes) of <b>high-speed</b> local <b>memory</b> was incorporated into each background processor for use as temporary scratch memory.|$|R
50|$|Most CPU designs {{include a}} small amount of very <b>high-speed</b> <b>memory</b> known as registers. Registers are used by the CPU in order to hold {{temporary}} values while working on longer strings of instructions. Considerable performance can be added to a design with more registers. However, since the registers are a visible piece of the CPU's instruction set, the number cannot typically be changed after the design has been released.|$|E
50|$|CISCs were {{believed}} to be the most powerful types of computers, because their microcode was small and could be stored in very <b>high-speed</b> <b>memory.</b> The CISC architecture also addressed the semantic gap as it was then perceived. This was a defined distance between the machine language, and the higher level programming languages used to program a machine. It was felt that compilers could do a better job with a richer instruction set.|$|E
50|$|Microcode {{typically}} {{resides in}} special <b>high-speed</b> <b>memory</b> and translates machine instructions, state machine data or other input into sequences of detailed circuit-level operations. It separates the machine {{instructions from the}} underlying electronics so that instructions can be designed and altered more freely. It also facilitates the building of complex multi-step instructions, while reducing the complexity of computer circuits. Writing microcode is often called microprogramming and the microcode in a particular processor implementation is sometimes called a microprogram.|$|E
40|$|Attention {{is given}} to general {{questions}} regarding television systems, aspects of image distortion in TV systems, approaches of image restoration, noise limitations, and the digital image integrator. The digital image recorder has been built around a <b>high-speed</b> digital <b>memory</b> system. It is shown that television systems {{can be used to}} augment man's capabilities in the control loop of remote manipulation systems...|$|R
40|$|Enlarged color {{photograph}} of the Intel® Pentium® II MPU die. The 7. 5 million-transistor Intel® Pentium® II processor incorporates Intel® MMX™ technology, which is designed specifically to process video, audio and graphics data efficiently. It was introduced in innovative Single Edge Contact (S. E. C) Cartridge that also incorporated a <b>high-speed</b> cache <b>memory</b> chip. 0. 25 µ. 300 - 450 MHz. External L 2 cache...|$|R
40|$|This paper {{focuses on}} the power {{delivery}} characterization of <b>high-speed</b> IC <b>memories</b> by means of on-chip measurements. A systematic analysis of the measurement setup, {{of the effects of}} chip biasing and of the use of the measured responses to develop models defined by simplified circuit equivalents is given. All the results collected in the paper are based on real measurements carried out on a commercial 90 nm flash memory...|$|R
50|$|Scratchpad memory (SPM), {{also known}} as scratchpad, scratchpad RAM or local store in {{computer}} terminology, is a high-speed internal memory used for temporary storage of calculations, data, and other work in progress. In reference to a microprocessor ("CPU"), scratchpad refers to a special <b>high-speed</b> <b>memory</b> circuit used to hold small items of data for rapid retrieval. It {{is similar to the}} usage and size of a scratchpad in life: a pad of paper for preliminary notes or sketches or writings, etc.|$|E
50|$|In 1986, {{she began}} working for Digital Equipment Corporation, where she spent 12 years, {{first at the}} Western Research Laboratory. While at Digital Equipment, she {{developed}} and patented a method for generating complete address traces for analyzing and designing <b>high-speed</b> <b>memory</b> systems. Her experience running the ever-expanding Systers mailing list, which she founded in 1987, led her to work in email communication. As a consultant engineer in the Network Systems Laboratory under Brian Reid, she developed MECCA, an email and Web-based system for communicating in virtual communities.|$|E
5000|$|In particular, the {{new design}} did not include many of the {{addressing}} modes that were intended to make programs smaller in memory, a technique that was widely used on other DEC machines and CISC designs in general. This would mean the machine would spend more time accessing memory, which would slow it down. However, the machine also extended the idea of multiple [...] "General Purpose Registers" [...] (GPRs), which gave the programmer flexibility to use these <b>high-speed</b> <b>memory</b> caches as they needed, potentially addressing the performance issues.|$|E
40|$|Per-flow {{network traffic}} {{measurements}} {{are needed for}} effective network traffic management, network performance assessment, and detection of anomalous network events such as incipient DoS attacks. Explicit measurement of per-flow traffic statistics is difficult in backbone networks because tracking the possibly {{hundreds of thousands of}} flows needs correspondingly large <b>high-speed</b> <b>memories.</b> To reduce the measurement overhead, many previous papers have proposed the use of random sampling and this is also used in commercial routers (Cisco's NetFlow). Our goal is to develop a new scheme that has very low memory requirements and has quick convergence to within a pre-specified accuracy. We achieve this by use of a novel approach based on sampling two-runs to estimate per-flow traffic. (A flow has a two-run when two consecutive samples belong to the same flow). Sampling two-runs automatically biases the samples towards the larger flows thereby making the estimation of these sources more accurate. This biased sampling leads to significantly smaller memory requirement compared to random sampling schemes. The scheme is very simple to implement and performs extremely well...|$|R
40|$|Using {{the cache}} memory {{structure}} {{as applied to}} a 12 -bit minicomputer (PDP- 8 /E), and the 100 -ns processor constructed of H and S series TTL gates, the resultant performance is 5 to 10 times the conventional implementation with a corresponding improvement:rt the perf ormance/cost ratio Implementation of a Buffer Memory in Minicomputers The cache memory concept was developed {{in an effort to}} improve the performance of large-scale computers. This has produced machines with the advantages of short cycle times without the added cost of all solid-state <b>high-speed</b> <b>memories.</b> The implementation of this scheme in a minicomputer is more difficult, because it requires a determination of the performance gained for each increase in cost and, therefore, a firm understanding of the schemes di-rectly usable in a minicomputer. Basically, the complexity added to a minicomputer by the use of a cache is a much larger portion of the machine's basic price than that of a large-scale machine. As a result, the cache scheme should not aim at finding 98 % of the addresse...|$|R
40|$|We {{report the}} {{realization}} of a completely controllable <b>high-speed</b> nanomechanical <b>memory</b> element fabricated from single-crystal silicon wafers. This element consists of a doubly-clamped suspended nanomechanical beam structure, which {{can be made to}} switch controllably between two stable and distinct states at a single frequency in the megahertz range. Because of their sub-micron size and high normal-mode frequencies, these nanomechanical memory elements offer the potential to rival the current state-of-the-art electronic data storage and processing...|$|R
50|$|Each {{of these}} steps {{increases}} {{the amount of}} memory needed to hold the resulting image. By the time it reaches {{the end of the}} pipeline the images are so large that typical graphics card designs often use specialized <b>high-speed</b> <b>memory</b> and a very fast computer bus to provide the required bandwidth to move the image {{in and out of the}} various sub-components of the pipeline. This sort of support is possible on dedicated graphics cards, but as power and size budgets become more limited, providing enough bandwidth becomes expensive in design terms.|$|E
50|$|The AMD RX Vega {{series is}} a series of {{consumer}} graphics processors produced by AMD. These cards are to use the Vega architecture, are to feature HBM2 <b>high-speed</b> <b>memory</b> and will continue to use the 14nm FinFET architecture. Vega {{will be the first to}} use the new Graphics Core Next 5th generation and will be succeeded by the Navi architecture in 2018. The lineup, releasing on the 14th of August 2017, will include the RX Vega 56 and the RX Vega 64, priced at $399 and $499 respectively.|$|E
5000|$|On November 20, 2009, Elpida Memory {{announced}} {{the opening of}} the company's Munich Design Center, responsible for Graphics DRAM (GDDR) design and engineering. Elpida received GDDR design assets from Qimonda AG in August 2009 after Qimonda's bankruptcy. The design center has approximately 50 employees and is equipped with <b>high-speed</b> <b>memory</b> testing equipment for use in the design, development and evaluation of Graphics memory. [...] On July 31, 2013, Elpida became a fully owned subsidiary of Micron Technology and based on current public LinkedIn professional profiles, Micron continues to operate the Graphics Design Center in Munich.|$|E
40|$|High-speed, {{environmentally}} hardened controller {{developed for}} use with commercially available system crates for both experiment control and data handling. Programable crate controller consists of three functional areas: control section utilizes <b>high-speed</b> bit-slice circuitry, <b>memory,</b> and data way interface...|$|R
40|$|Abstract—Memory-access speed {{continues}} {{falling behind}} the growing speeds of network transmission links. High-speed network links {{provide a means}} to connect memory placed in hosts, located in different corners of the network. These hosts are called storage system units (SSUs), where data can be stored. Cloud storage provided with a single server can facilitate large amounts of storage to a user, however, at low access speeds. A distributed approach to cloud storage is an attractive solution. In a distributed cloud, small <b>high-speed</b> <b>memories</b> at SSUs can potentially increase the memory access speed for data processing and transmission. However, the latencies of each SSUs may be different. Therefore, the selection of SSUs impacts the overall memory access speed. This paper proposes a latency-aware scheduling scheme to access data from SSUs. This scheme determines the minimum latency requirement for a given dataset and selects available SSUs with the required latencies. Furthermore, because the latencies of some selected SSUs may be large, the proposed scheme notifies SSUs {{in advance of the}} expected time to perform data access. The simulation results show that the proposed scheme achieves faster access speeds than a scheme that randomly selects SSUs and another hat greedily selects SSUs with small latencies. I...|$|R
40|$|Aims {{to provide}} an {{explanation}} of the state-of-the-art nanometer and sub- 1 -V memory LSIs that play decisive roles in power conscious systems. This book discusses the problems between the device, circuit, and system levels in terms of reliable <b>high-speed</b> operations of <b>memory</b> cells and peripheral logic circuits...|$|R
