21|14|Public
2500|$|... 1939: J. Barkley Rosser {{observes the}} <b>essential</b> <b>equivalence</b> of [...] "effective method" [...] defined by Gödel, Church, and Turing (Rosser in Davis, 1965, p.273, [...] "Informal Exposition of Proofs of Gödel's Theorem and Church's Theorem") ...|$|E
2500|$|Analyzing strategy, Schlesinger {{maintained}} that the {{theory and practice of}} the 1950s and 1960s had been overtaken by events, particularly the rise of the Soviet Union to virtual nuclear parity with the United States and the effect this development had on the concept of deterrence. Schlesinger believed that [...] "deterrence is not a substitute for defense; defense capabilities, representing the potential for effective counteraction, are the essential condition of deterrence." [...] He had grave doubts about the assured destruction strategy, which relied on massive nuclear attacks against an enemy's urban-industrial areas. Credible strategic nuclear deterrence, the secretary felt, depended on fulfilling several conditions: maintaining <b>essential</b> <b>equivalence</b> with the Soviet Union in force effectiveness; maintaining a highly survivable force that could be withheld or targeted against an enemy's economic base in order to deter coercive or desperation attacks against U.S. population or economic targets; establishing a fast-response force that could act to deter additional enemy attacks; and establishing a range of capabilities sufficient to convince all nations that the United States was equal to its strongest competitors.|$|E
5000|$|... 1939: J. Barkley Rosser {{observes the}} <b>essential</b> <b>equivalence</b> of [...] "effective method" [...] defined by Gödel, Church, and Turing (Rosser in Davis, 1965, p. 273, [...] "Informal Exposition of Proofs of Gödels Theorem and Churchs Theorem") ...|$|E
40|$|Hilsum-Skandalis maps, from {{differential}} geometry, are {{studied in}} the context of a cartesian category. It is shown that Hilsum-Skandalis maps can be represented as stably Frobenius adjunctions. This leads to a new and more general proof that Hilsum-Skandalis maps represent a universal way of inverting <b>essential</b> <b>equivalences</b> between internal groupoids. To prove the representation theorem, a new characterisation of the con- nected components adjunction of any internal groupoid is given. The charaterisation is that the adjunction is covered by a stable Frobenius adjunction that is a slice and whose right adjoint is monadic. Geometric morphisms can be represented as stably Frobenius adjunctions. As applications of the study we show how it is easy to recover properties of geometric morphisms, seeing them as aspects of properties of stably Frobenius adjunctions...|$|R
40|$|This paper shows <b>essential</b> <b>equivalences</b> {{among several}} methods of linearly {{constrained}} correspondence analysis. They include Fisher’s method of additive scoring, Hayashi’s {{second type of}} quantification method, ter Braak’s canonical correspondence analysis, Nishisato’s ANOVA of categorical data, correspondence analysis of manipulated contingency tables, B 6 ckenholt and B 6 ckenholt’s least squares canonical analysis with linear constraints, and van der Heijden and Meijerink’s zero average restrictions. These methods {{fall into one of}} two classes of methods corresponding to two alternative ways of imposing linear constraints, the reparametrization method and the null space method. A connection between the two is estab-lished through Khatri’s lemma. Key words: canonical correlation analysis, generalized singular value decomposition (GSVD), the method of additive scoring, the second type of quantification method (Q 2), canonical cor-respondence analysis (CCA), ANOVA of categorical data, canonical analysis with linear con-straints (CALC), zero average restrictions, Khatri’s lemma. 1...|$|R
50|$|Describing the {{subjects}} state or situation typically uses the normal VSO ordering with the verb bí. The copula is {{is used to}} state <b>essential</b> characteristics or <b>equivalences.</b>|$|R
5000|$|Analyzing strategy, Schlesinger {{maintained}} that the {{theory and practice of}} the 1950s and 1960s had been overtaken by events, particularly the rise of the Soviet Union to virtual nuclear parity with the United States and the effect this development had on the concept of deterrence. Schlesinger believed that [...] "deterrence is not a substitute for defense; defense capabilities, representing the potential for effective counteraction, are the essential condition of deterrence." [...] He had grave doubts about the assured destruction strategy, which relied on massive nuclear attacks against an enemy's urban-industrial areas. Credible strategic nuclear deterrence, the secretary felt, depended on fulfilling several conditions: maintaining <b>essential</b> <b>equivalence</b> with the Soviet Union in force effectiveness; maintaining a highly survivable force that could be withheld or targeted against an enemy's economic base in order to deter coercive or desperation attacks against U.S. population or economic targets; establishing a fast-response force that could act to deter additional enemy attacks; and establishing a range of capabilities sufficient to convince all nations that the United States was equal to its strongest competitors.|$|E
40|$|By using a fonctionelle of {{probability}} distributions, several different statistical physics including extensive and nonextensive statistics are unified {{in a general}} method. The <b>essential</b> <b>equivalence</b> between the MaxEnt process of the most probable probility distribution in these statistics and the famous thermodynamical relation dU=TdS is strictly proved without any additional assumption. Moreover, it is expounded that all the conclusions of these different statistics can be directly derived from the equivalent relation. Comment: 8 page...|$|E
40|$|For a large {{collection}} of random variables, pairwise conditional independence and mutual conditional independence are {{shown to be}} essentially equivalent. Unlike in the finite setting, a large {{collection of}} random variables remains essentially conditionally independent under further conditioning. The <b>essential</b> <b>equivalence</b> of pairwise and multiple versions of exchangeability also follows as a corollary. Our proof {{is based on an}} iterative extension of Bledsoe and Morse’s completion of a product measure on a pair of measure spaces...|$|E
40|$|In {{the last}} two decades a lot of matrix algehra optimal and superlinear preconditioners (those assuring a strong {{clustering}} at the unity) have been proposed for the solution of polynomially ill-conditioned Toeplitz linear systems. The corresponding generalizations to multilevel structures do not preserve optimality neither superlinearity. Regarding the notion of superlinearity, it has been recently shown that this is simply impossible. Here we propose some ideas and a proof technique for demonstrating that also the spectral <b>equivalence</b> and the <b>essential</b> spectral <b>equivalence</b> (up to a constant number of diverging eigenvalues) are impossible and therefore the search for optimal matrix algebra preconditioners in the multilevel setting cannot be successful...|$|R
40|$|AbstractWe examine {{different}} χ 2 statistics {{appropriate for}} high-level metrology. “Key” measurement comparisons often need statistics {{that can be}} used before a reference value is chosen. One such statistic is the pair-difference χ 2, presented here. This is also a natural way to examine bilateral <b>equivalences</b> <b>essential</b> for trade. Monte Carlo simulation is a practical means to extend rigor beyond conventional χ 2 testing, and permits the use {{of a wide variety of}} reference values for familiar null-hypothesis testing. Further, simulation enables the handling of measurements purportedly drawn from Student distributions or with reported inter-laboratory covariances...|$|R
40|$|AbstractIn {{the last}} decades several matrix algebra optimal and superlinear preconditioners (those assuring a strong {{clustering}} at the unity) have been proposed for the solution of polynomially ill-conditioned Toeplitz linear systems. The corresponding generalizations for multilevel structures are neither optimal nor superlinear (see e. g. Contemp. Math. 281 (2001) 193). Concerning the notion of superlinearity, it has been recently shown that the proper clustering cannot be obtained in general (see Linear Algebra Appl. 343 – 344 (2002) 303; SIAM J. Matrix Anal. Appl. 22 (1) (1999) 431; Math. Comput. 72 (2003) 1305). In this paper, by exploiting a proof technique previously proposed by the authors (see Contemp. Math. 323 (2003) 313), we prove that the spectral <b>equivalence</b> and the <b>essential</b> spectral <b>equivalence</b> (up to a constant number of diverging eigenvalues) are impossible too. In conclusion, optimal matrix algebra preconditioners in the multilevel setting simply do not exist in general and therefore the search for optimal iterative solvers should be oriented to different directions with special attention to multilevel/multigrid techniques...|$|R
40|$|I {{was most}} {{disappointed}} by the Science & Society article in your September issue[1]. By focusing only on the European Union’s (EU) oversight of transgenic plants, (that is, those modified by recombinant DNA techniques and containing heterologous DNA), the authors ignored the scientific consensus about the continuum, and <b>essential</b> <b>equivalence</b> with respect to risk considerations, between conventional and recombinant DNA-mediated genetic modification. By omitting any reference to the EU’s flawed choice of scope for its regulatory scheme, which is limited to recombinan...|$|E
40|$|Recent {{developments}} in the categorical foundations of universal algebra have given fresh impetus {{to an understanding of}} the lambda calculus coming from categorical logic: an interpretation is a semi-closed algebraic theory. Scott's representation theorem is then completely natural and leads to precise theorems showing the <b>essential</b> <b>equivalence</b> with more familiar notions. Simple abstract proofs of fundamental results in the semantics of the lambda calculus are given. Comment: 21 pages, submitted for 90 th Birthday of Corrado Bohm Second version accepted by MSCS. Material filled out at the request of referees. Now 28 pages but nothing essentially ne...|$|E
40|$|Codimension one foliated {{manifolds}} (M,F) admitting {{a closed}} 2 -form ω making each leaf symplectic are a natural generalization of 3 -dimensional taut foliations. Remarkably, on such closed foliated manifolds (M,F) {{there exists a}} class of 3 -dimensional transverse closed submanifolds W on which F induces a taut foliation F_W. Our main result says that the foliated submanifold (W,F_W) has the same transverse geometry as (M,F). More precisely, the inclusion induces an <b>essential</b> <b>equivalence</b> between the corresponding holonomy groupoids. The proof of our main result relies on a leafwise Lefschetz hyperplane theorem, which is of independent interest...|$|E
40|$|This paper {{presents}} a flexible and efficient approach to deriving indirect implications in logic circuits. Indirect implications are <b>essential</b> in ATPG, <b>equivalence</b> checking, and netlist optimization. Contrary to other methods, our approach {{is based on}} a graph model of a circuit's clause description called the implication graph. It combines both the flexibility of SAT-based techniques and high efficiency of structure based methods. As the proposed algorithms operate only on the implication graph, they are independent of the chosen logic. Computation of indirect implications is performed by simple and efficient graph algorithms. Experimental results for various applications relying on indirect implications demonstrate the efficiency of our approach...|$|R
30|$|However, in this paper, we only {{consider}} {{the situation where}} l_ 0 -minimization has an unique solution. The uniqueness assumption is vital for us to prove the main results. However, from Lemma 1 {{we see that the}} uniqueness assumption is equivalent to a certain double-inequality condition, which looks like RIP. The evident difference between them is in that the former possesses the homogeneity rather than the latter. This implies that, unlike RIP, the uniqueness assumption is not in <b>essential</b> conflict with <b>equivalence</b> of all linear systems λ Ax=λ x, λ∈R. Therefore, we think that the uniqueness assumption and, equivalently, the resulting double-inequality condition in Lemma 1 can replace the RIP in many cases.|$|R
40|$|It is {{proved that}} two {{different}} and independently derived integral representations of droplet size distribution moments {{encountered in the}} literature are equivalent and, moreover, consistent with the general dynamic equation that governs the droplet size distribution function. One of these representations consists of an integral over the droplet radius while the other representation consists of an integral over time. The proof is based on analytical solution of the general dynamic equation {{in the absence of}} coagulation but in the presence of both growth and nucleation. The solution derived is explicit in the droplet radius, which is in contrast with the literature where solutions are presented along characteristics. This difference is <b>essential</b> for the <b>equivalence</b> proof. Both the case of nonconvected vapor as well as the case of convected vapor are presented...|$|R
40|$|This article {{discusses}} the <b>essential</b> <b>equivalence</b> of second-order impedance control with force feedback and proportional gain explicit force control with force feedforward. This is first done analytically by reviewing each control method and showing how they mathematically correspond for constrained manipula-tor control. For stiff environments the correspondence is exact. However, even for softer environments, a similar {{response of the}} system is indicated. Next, the results of an implementation of these control schemes on the CMU DD Arm II are pre-sented, confirming the predictions of the analysis. These results experimentally demonstrate that proportional gain force control and impedance control, with and without dynamics compensa-tion, have equivalent response to commanded force trajectories. 1...|$|E
40|$|We {{study in}} this paper various ordinal ranks of (bounded) Baire class 1 {{functions}} and we show their <b>essential</b> <b>equivalence.</b> This leads to a natural classification {{of the class of}} bounded Baire class 1 functions B_ 1 in a transfinite hierarchy B^ξ_ 1 ξ < ω_ 1) of "small" Baire classes, for which (for example) an analysis similar to the Hausdorff-Kuratowski analysis of Δ^ 0 _ 2 sets via transfinite differences of closed sets can be carried out. The notions of pseudouniform convergence of a sequence of functions and optimal convergence of a sequence of continuous functions to a Baire class 1 function ƒ are introduced and used in this study...|$|E
40|$|We {{consider}} {{fundamental questions}} of arbitrage pricing arising when the uncertainty model incorporates volatility uncertainty. With a standard probabilistic model, <b>essential</b> <b>equivalence</b> between {{the absence of}} arbitrage {{and the existence of}} an equivalent martingale measure is a folk theorem, see Harrison and Kreps (1979). We establish a microeconomic foundation of sublinear price systems and present an extension result. In this context we introduce a prior dependent notion of marketed spaces and viable price systems. We associate this extension with a canonically altered concept of equivalent symmetric martingale measure sets, in a dynamic trading framework under absence of prior depending arbitrage. We prove the existence of such sets when volatility uncertainty is modeled by a stochastic di erential equation, driven by Peng's G-Brownian motion...|$|E
5000|$|The {{general theory}} of {{quadratic}} forms was initiated by Lagrange in 1775 in his Recherches d'Arithmétique. Lagrange {{was the first to}} realize that [...] "a coherent general theory required the simulatenous consideration of all forms." [...] He was the first to recognize the importance of the discriminant and to define the <b>essential</b> notions of <b>equivalence</b> and reduction, which, according to Weil, have [...] "dominated the whole subject of quadratic forms ever since". [...] Lagrange showed that there are finitely many equivalence classes of given discriminant, thereby defining for the first time an arithmetic class number. His introduction of reduction allowed the quick enumeration of the classes of given discriminant and foreshadowed the eventual development of infrastructure. In 1798, Legendre published Essai sur la théorie des nombres, which summarized the work of Euler and Lagrange and added some of his own contributions, including the first glimpse of a composition operation on forms.|$|R
40|$|Background: Understanding the {{relationship}} between organizational context and research utilization is key to reducing the research-practice gap in health care. This is particularly true in the residential long term care (LTC) setting where relatively little work has examined the influence of context on research implementation. Reliable, valid measures and tools are a prerequisite for studying organizational context and research utilization. Few such tools exist in German. We thus translated three such tools (the Alberta Context Tool and two measures of research use) into German for use in German residential LTC. We point out challenges and strategies for their solution unique to German residential LTC, and demonstrate how resolving specific challenges in the translation {{of the health care}} aide instrument version streamlined the translation process of versions for registered nurses, allied health providers, practice specialists, and managers. Methods: Our translation methods were based on best practices and included two independent forward translations, reconciliation of the forward translations, expert panel discussions, two independent back translations, reconciliation of the back translations, back translation review, and cognitive debriefing. Results: We categorized the challenges in this translation process into seven categories: (1) differing professional education of Canadian and German care providers, (2) risk that German translations would become grammatically complex, (3) wordings at risk of being misunderstood, (4) phrases/idioms non-existent in German, (5) lack of corresponding German words, (6) limited comprehensibility of corresponding German words, and (7) target persons’ unfamiliarity with activities detailed in survey items. Examples of each challenge are described with strategies that we used to manage the challenge. Conclusion: Translating an existing instrument is complex and time-consuming, but a rigorous approach is necessary to obtain instrument <b>equivalence.</b> <b>Essential</b> components were (1) involvement of and co-operation with the instrument developers and (2) expert panel discussions, including both target group and content experts. Equivalent translated instruments help researchers from different cultures to find a common language and undertake comparative research. As acceptable psychometric properties are a prerequisite for that, we are currently carrying out a study with that focus...|$|R
40|$|Time and {{precision}} {{of the results}} {{are the most important}} factors in any code used for nuclear calculations. Despite of the high accuracy of Monte Carlo codes, MCNP and Serpent, in many cases their relatively long computational time leads to difficulties in using any of them as the main calculation code. Usually, Monte Carlo codes are used only to benchmark the results. The deterministic codes, which are usually used in nuclear reactor’s calculations, have limited precision, due to the approximations in the methods used to solve the multi-group transport equation. Self- Shielding treatment, an algorithm that produces an average cross-section defined over the complete energy domain of the neutrons in a nuclear reactor, is responsible for the biggest error in any deterministic codes. There are mainly two resonance self-shielding models commonly applied: models based on equivalence and dilution and models based on subgroup approach. The fundamental problem with any self-shielding method is that it treats any isotope as there are no other isotopes with resonance present in the reactor. The most practical way to solve this problem is to use multi-energy groups (50 - 200) that are chosen in a way that allows us to use all major resonances without self-shielding. In this paper, we perform cell calculations, for a fresh seed fuel pin which is used in thorium/uranium reactors, by solving 172 energy group transport equation using the deterministic DRAGON code, for the two types of self-shielding models (equivalence and dilution models and subgroup models) Using WIMS-D 5 and DRAGON data libraries. The results are then tested by comparing it with the stochastic MCNP 5 code.   We also tested the sensitivity of the results to a specific change in self-shielding method implemented, for example the effect of applying Livolant-Jeanpierre Normalization scheme and Rimman Integration improvement on the equivalence and dilution method, and the effect of using Ribbon extended approach on sub-group method.  The results of K inf, in case of fresh seed fuel pin which is used in thorium/uranium PWR, show that a high accuracy is obtained by using some specific self-shielding modules.  It is also shown that for the implemented self-shielding models DRAGON library is more reliable than WIMS-D 5 library, and that applying Livolant-Jeanpierre Normalization scheme is <b>essential</b> with the <b>equivalence</b> and dilution self-shielding method.  </p...|$|R
40|$|This paper {{reveals the}} <b>essential</b> <b>equivalence</b> of second order {{impedance}} control and proportional gain explicit force control with feedforward. This is first done analytically by reviewing each control method and showing how they mathematically correspond. For stiff environments the correspondence is exact. However, even for softer environments similar {{response of the}} system is indicated. Next, the results of an implementation of these control schemes on the CMU DD Arm II are presented, confirming the predictions of the analysis. These results experimentally demonstrate that proportional gain force control and impedance control, with and without dynamics compensation, have equivalent response to commanded force trajectories. 1 Introduction There is a whole class of tasks that seem to implicitly require controlling the force of interaction between a manipulator and its environment: pushing, scraping, grinding, pounding, polishing, twisting, etc. Thus, force control of the manipulator bec [...] ...|$|E
40|$|To each entire {{function}} from the Polya class with real simple zeros, we associate conformal mappings {{from the plane}} without some slits onto the canonical domain, i. e., the plane without horizontal or vertical slits, crossing the points pi n, n 2 Z. We obtain basic properties of these mappings and {{entire function}}s, which are neccesary to describe the <b>essential</b> <b>equivalence</b> between domains, entire functions and Dirichlet integrals. The main results are devoted to a class of conformal mappings with exact asymptotics at i 1 and with bounded Dirichlet integral. In this case we describe the corresponding classes of entire functions (the smooth analogue of the Cartwright class) and classes of domains (slits). Moreover, we obtain two-sided estimates between the geometric parameters of the domains (slit lengths in l&sup 2;-norm), the Dirichlet integral and the parameters of entire functions...|$|E
40|$|This paper {{discusses}} the <b>essential</b> <b>equivalence</b> of second order impedance control with force feedback and proportional gain explicit force control with force feedforward. This is first done analytically by reviewing each control method and showing how they mathematically correspond for constrained manipulator control. For stiff environments the correspondence is exact. However, even for softer environments similar {{response of the}} system is indicated. Next, the results of an implementation of these control schemes on the CMU DD Arm II are presented, confirming the predictions of the analysis. These results experimentally demonstrate that proportional gain force control and impedance control, with and without dynamics compensation, have equivalent response to commanded force trajectories. 1 Introduction There is a whole class of tasks that implicitly require controlling the force of interaction between a manipulator and its environment: pushing, scraping, grinding, pounding, polishing, t [...] ...|$|E
40|$|Chapter 2 {{introduces}} the baseline {{version of the}} VAR model, with its basic statistical assumptions that we examine in the sequel. We first check whether the variables in the VAR can be transformed to meet these assumptions. We analyze the univariate characteristics of the series. Important properties are a bounded spectrum, the order of (seasonal) integration, linearity and normality after the appropriate transformation. Subsequently, these properties are contrasted with the properties of stochastic fractional integration. We suggest data-analytic tools to check the assumption of univariate unit root integration. In an appendix we give {{a detailed account of}} unit root tests for stochastic unit root nonstationarity versus deterministic nonstationarity at frequencies of interest. Chapter 3 first discusses local and global influence analysis, which should point out the observations with the most notable impact on the estimates of location and covariance parameters. The results from this analysis can be helpful in spotting the sources of possible problems with the baseline model. After the influence analysis we discuss the merits of various statistical diagnostic tests for the adequacy of the separate regression equations. After one has estimated the unrestricted VAR one should check some overall characteristics of the system. We present several suggestions on how to do this. Chapter 4 deals with common sources of misspecification stemming from problems with seasonality and seasonal adjustment in the multivariate model. We discuss a number of univariate unobserved component models for stochastic seasonality, giving additional insight into the properties of models with unit root nonstationarity. We also suggest a modification of a simple but quite robust seasonal adjustment procedure. Some new data-analytic tools are introduced to examine the seasonal component more closely. Appendix A 4. 1 discusses the limitations of deterministic modeling of seasonality. Appendix A 4. 2 treats aspects of backforecasting in models with nonstationarity in mean. Chapter 5 introduces outlier models. We develop a testing procedure to direct and evaluate the treatment of exceptional observations in the VAR. We illustrate its application on an artificial data set that contains important characteristics of macroeconomic time series. The effect of the outliers and the effectiveness of the testing procedure is also analyzed on a four-variate set of quarterly French data, which exhibits cointegration. We compare some ready-to-use outlier correction methods in the last section. Chapter 6 deals with restrictions on the VAR model. First we discuss a number of interesting reparameterizations of the VAR under unit root restrictions. The reparameterizations lead to different interpretations, which can help to assess the plausibility of empirical outcomes. We present some straightforward transformation formulae for a number of these parameterizations and show which assumptions are <b>essential</b> for the <b>equivalence</b> of these models. We illustrate this in simple numerical examples. Next we compare VAR based methods to estimate pushing trends and pulling equilibria in multivariate time series. The predictability approach of Box and Tiao receives special attention. Finally we discuss multivariate tests for unit roots and cointegration. Chapter 7 applies the methods described in the previous chapters to analyze gross fixed capital investment in the Netherlands from 1961 to 1988 in a six-variate system. We discuss a number of economic approaches to model macroeconomic investment series. We list a number of problems in empirical applications of these models. Section 7. 3 presents empirically relevant aspects of the measurement model for macroeconomic investment. Section 7. 4 applies the univariate techniques of Chapters 2, 3, 4 and 5 to the investment series and five other macroeconomic with a notable dynamic relationship with investment, viz. consumption, imports, exports, the terms of trade and German industrial production. The univariate analysis clearly shows the presence of nonstationary seasonal components in a number of the series. The model is extended with a structural break on the basis of results from the univariate analysis. The subsequent multivariate analysis confirms the need for a structural break in the model for the growth rates of the multivariate series. An empirically important equilibrium relation between investment, imports and exports is seen to remain stable over the entire sample period. The partial correlation of deviations from this equilibrium and growth rates of investment is large and stable...|$|R
40|$|Continuing {{the study}} of {{connections}} amongst Dense Model Theorem, Low Complexity Approximation Theorem and Hardcore Lemma initiated by Trevisan et al. [TTV 09], this thesis builds {{on the work of}} Barak et al., Impagliazzo, Reingold et al. and Zhang [BHK 09, Imp 09, RTTV 08 a, Zha 11] to show the <b>essential</b> <b>equivalence</b> of these three results. The first main result obtained here is a reduction from any of the standard black-box Dense Models Theorems to the Low Complexity Approximation Theorem. The next is the extension of Impagliazzo 2 ̆ 7 s reduction from Strong Hardcore Lemma to Dense Model Theorem. Then using Zhang 2 ̆ 7 s Dense Model Theorem algorithm we reduce Weak Hardcore Lemma to Strong Hardcore Lemma. Last we distill the methods of Barak et al. and Zhang to extract a single algorithm which yields uniform constructions for all three. Putting all this together demonstrates the three results are essentially equivalent...|$|E
40|$|The {{results of}} SCF-CI {{calculations}} of the intermolecular potential surface for CO and He are compared with pressure broadening data. Coupled states calculations were performed {{for both the}} electron gas and the more accurate SCF-CI potentials at collision energies from 15 to 1200 /cm, and Boltzmann averages over collision energy were taken to compare with experimental results at liquid nitrogen (77 K), dry ice (195 K) and room (295 K) temperatures. The cross sections from the SCF-CI potential are shown to be in slightly better agreement with experiment than those from the electron gas potential, although both {{are consistent with the}} data to within experimental uncertainties. Closer examination of the experimental results reveals that this <b>essential</b> <b>equivalence</b> is due not to any inherent insensitivity of the pressure broadening technique, but to the uncertainties in the experimental values. It is thus concluded that more accurate experimental values are required to place meaningful constraints on the intermolecular potential...|$|E
40|$|What is {{the optimal}} {{way to cut}} a convex bounded domain K in Euclidean space (R^n,|·|) into two halves of equal volume, so that the {{interface}} between the two halves has least surface area? A conjecture of Kannan, Lovász and Simonovits asserts that, if one does not mind gaining a universal numerical factor (independent of n) in the surface area, one might as well dissect K using a hyperplane. This conjectured <b>essential</b> <b>equivalence</b> between the former non-linear isoperimetric inequality and its latter linear relaxation, has been shown {{over the last two}} decades to be of fundamental importance to the understanding of volumetric and spectral properties of convex domains. In this work, we address the conjecture for the subclass of generalized Orlicz balls K = {x ∈R^n; ∑_i= 1 ^n V_i(x_i) ≤ E }, confirming its validity for certain levels E ∈R under a mild technical assumption on the growth of the convex functions V_i at infinity (without which we confirm the conjecture up to a (1 +n) factor). In sharp contrast to previous approaches for tackling the KLS conjecture, we emphasize that no symmetry is required from K. This significantly enlarges the subclass of convex bodies for which the conjecture is confirmed. Comment: 36 page...|$|E
40|$|Acknowledging {{the absence}} of {{up-to-date}} empirical data on the value retention, service life and annual use of chipping machinery, in 2017 the authors surveyed the records kept by 50 contractors offering biomass chipping services. The machine fleet and operations in this survey could be taken as representative for most of Europe, where the biomass sector is well established and is facing further expansion. Data collection included the whole chipping unit, comprised of chipper, carrier and loader. Manually-fed units {{were excluded from the}} survey. The data pointed at a service life up to and exceeding 10, 000 h and 10 years, which relieved any concerns about poor durability. Value retention was good, and may exceed that of other mainstream forestry equipment. Engine power was the main explanatory variable in any models to predict purchase price and productivity. The effect of this variable could explain most of the variability (> 80 %) in the purchase price and productivity data. Results also pointed at the <b>essential</b> <b>equivalence</b> in price and productivity between PTO-driven (i. e., tractor powered) and independent-engine chippers, once differences in engine power are accounted for. However, the distribution of purchase price between different components of the chipping unit was different between the two unit types, with the chipper accounting for a larger proportion of the total investment in independent-engine units. Machine power was also different, with most PTO-driven units being significantly smaller than independent-engine units, due to the limitations of existing tractors. Furthermore, half of the carriers assigned to a PTO-driven unit were subject to flexible use, i. e., they were not solely used for chipping work...|$|E
40|$|Rewriting logic expresses an <b>essential</b> <b>equivalence</b> between {{logic and}} computation. System states are in {{bijective}} correspondence with formulas, and concurrent computations are in bijective correspondence with proofs. Given this equivalence between computation and logic, a rewriting logic axiom {{of the form}} t Γ! t 0 has two readings. Computationally, it means that a fragment of a system 's state that is an instance of the pattern t can change to the corresponding instance of t 0 concurrently with any other state changes; logically, it just means that we can derive the formula t 0 from the formula t. Rewriting logic is entirely neutral about the structure and properties of the formulas/states t. They are entirely user-definable as an algebraic data type satisfying certain equational axioms. Because of this ecumenical neutrality, rewriting logic has, from a logical viewpoint, good properties as a logical framework, in which many other logics can be naturally represented. And, computationally, it has also good properties as a semantic framework, in which many different system styles and models of concurrent computation and many different languages can be naturally expressed without any distorting encodings. The goal {{of this paper is}} to provide a relatively gentle introduction to rewriting logic, and to paint in broad strokes the main research directions that, since its introduction in 1990, have been pursued by a growing number of researchers in Europe, the US, and Japan. Key theoretical developments, as well as the main current applications of rewriting logic as a logical and semantic framework, and the work on formal reasoning to prove properties of specifications are surveyed...|$|E
40|$|Introduction : Gait {{evaluation}} protocols using instrumented treadmills will {{be increasingly}} used in the near future. For this reason, it must be shown that using instrumented treadmills will produce measures of the ground reaction force adequate for inverse dynamic analysis, and differences between treadmill and overground gait must be well characterized. Methods : Overground walking kinetics were estimated with the subjects walking at their self-selected comfortable walking speed. For the treadmill gait trials, the subjects walked on two treadmills, such that heel-strike occurred on the forward treadmill and toe-off occurred on the trailing treadmill. The treadmill was set to the average overground walking speed. Overground and treadmill data were evaluated using Vicon Plugin Gait. The differences between the maxima and minima of kinematic and kinetic parameters for overground and treadmill gait were evaluated. Results : The kinematics of treadmill and overground gait were very similar. Twelve of 22 kinematic parameter maxima were statistically significantly different (p < 0. 05), but {{the magnitude of the}} difference was generally less than 28. All GRF maxima were found to be statistically significantly smaller for treadmill versus overground gait (p < 0. 05) as were 15 of 18 moment, and 3 of 6 power maxima. However, the magnitude of the differences was comparable to the variability in normal gait parameters. The sagittal plane ankle moments were not statistically different for treadmill and overground gait. Discussion : We have shown that treadmill gait is qualitatively and quantitatively similar to overground gait. Differences in kinematic and kinetic parameters can be detected inmatched comparisons, particularly in the case of kinetic parameters. However, the magnitudes of these differences are all within the range of repeatability of measured kinematic parameters. Thus, the mechanics of treadmill and overground gait are very similar. Clinical significance : Having demonstrated the <b>essential</b> <b>equivalence</b> of treadmill and overground gait, it is now possible for clinical movement analysis to take advantage of treadmill-based protocols...|$|E

