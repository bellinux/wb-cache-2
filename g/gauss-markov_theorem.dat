82|1|Public
25|$|If the {{assumption}} of normality is replaced by assumptions of homoscedasticity and uncorrelatedness of errors, and if one still assumes zero mean, then the <b>Gauss–Markov</b> <b>theorem</b> entails that the solution is the minimal unbiased estimator.|$|E
25|$|This {{estimator}} {{reaches the}} Cramér–Rao {{bound for the}} model, and thus is optimal {{in the class of}} all unbiased estimators. Note that unlike the <b>Gauss–Markov</b> <b>theorem,</b> this result establishes optimality among both linear and non-linear estimators, but only in the case of normally distributed error terms.|$|E
25|$|In 1810, {{after reading}} Gauss's work, Laplace, after proving the central limit theorem, {{used it to}} give a large sample {{justification}} for the method of least square and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal {{in the sense that}} in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the <b>Gauss–Markov</b> <b>theorem.</b>|$|E
40|$|Abstract—We {{introduce}} two {{new source}} coding problems: robust sequential coding and robust predictive coding. For the Gauss–Markov source model with the {{mean squared error}} distor-tion measure, we characterize certain supporting hyperplanes of the rate region of these two coding problems. Our investigation also reveals an information-theoretic minimax theorem and the associated extremal inequalities. Index Terms—Extremal inequality, <b>Gauss–Markov</b> source, min-imax <b>theorem,</b> predictive coding, saddle point, sequential coding...|$|R
2500|$|The <b>Gauss–Markov</b> <b>theorem</b> {{states that}} under the spherical errors {{assumption}} (that is, the errors should be uncorrelated and homoscedastic) the estimator [...] is efficient {{in the class of}} linear unbiased estimators. This is called the best linear unbiased estimator (BLUE). Efficiency should be understood as if we were to find some other estimator [...] which would be linear in y and unbiased, then ...|$|E
2500|$|Note {{that the}} {{original}} strict exogeneity assumption [...] implies a far richer set of moment conditions than stated above. In particular, this assumption implies that for any vector-function ƒ, the moment condition [...] will hold. However it can be shown using the <b>Gauss–Markov</b> <b>theorem</b> that the optimal choice of function ƒ is to take , which results in the moment equation posted above.|$|E
2500|$|The <b>Gauss–Markov</b> <b>theorem.</b> In {{a linear}} {{model in which}} the errors have {{expectation}} zero conditional on the independent variables, are uncorrelated and have equal variances, the best linear unbiased estimator of any linear combination of the observations, is its least-squares estimator. [...] "Best" [...] means that the least squares estimators of the parameters have minimum variance. The assumption of equal variance is valid when the errors all {{belong to the same}} distribution.|$|E
2500|$|The {{expressions}} given {{above are}} based on the implicit assumption that the errors are uncorrelated {{with each other and with}} the independent variables and have equal variance. The <b>Gauss–Markov</b> <b>theorem</b> shows that, when this is so, [...] is a best linear unbiased estimator (BLUE). If, however, the measurements are uncorrelated but have different uncertainties, a modified approach might be adopted. Aitken showed that when a weighted sum of squared residuals is minimized, [...] is [...] the BLUE if each weight is equal to the reciprocal of the variance of the measurement ...|$|E
5000|$|... #Subtitle level 2: <b>Gauss-Markov</b> <b>theorem</b> {{as stated}} in {{econometrics}} ...|$|E
5000|$|Concerning such [...] "best {{unbiased}} estimators", {{see also}} Cramér-Rao bound, <b>Gauss-Markov</b> <b>theorem,</b> Lehmann-Scheffé theorem, Rao-Blackwell theorem.|$|E
5000|$|The <b>Gauss-Markov</b> <b>theorem</b> in {{mathematical}} statistics (In this theorem, {{one does not}} assume the probability distributions are Gaussian.) ...|$|E
5000|$|The kriging {{error is}} given by:which {{leads to the}} {{generalised}} least squares version of the <b>Gauss-Markov</b> <b>theorem</b> (Chiles & Delfiner 1999, p. 159): ...|$|E
50|$|The {{generalized}} {{least squares}} (GLS), developed by Aitken, extends the <b>Gauss-Markov</b> <b>theorem</b> {{to the case}} where the error vector has a non-scalar covariance matrix. The Aitken estimator is also a BLUE.|$|E
50|$|If the {{assumption}} of normality is replaced by assumptions of homoscedasticity and uncorrelatedness of errors, and if one still assumes zero mean, then the <b>Gauss-Markov</b> <b>theorem</b> entails that the solution is the minimal unbiased estimator.|$|E
50|$|This has {{the effect}} of {{standardizing}} the scale of the errors and “de-correlating” them. Since OLS is applied to data with homoscedastic errors, the <b>Gauss-Markov</b> <b>theorem</b> applies, and therefore the GLS estimate is the best linear unbiased estimator for β.|$|E
50|$|A simple, very {{important}} {{example of a}} generalized linear model (also {{an example of a}} general linear model) is linear regression. In linear regression, the use of the least-squares estimator is justified by the <b>Gauss-Markov</b> <b>theorem,</b> which does not assume that the distribution is normal.|$|E
5000|$|In most {{treatments}} of OLS, the regressors {{in the design}} matrix [...] {{are assumed to be}} fixed in repeated samples. This assumption is considered inappropriate for a predominantly nonexperimental science like econometrics. Instead, the assumptions of the <b>Gauss-Markov</b> <b>theorem</b> are stated conditional on [...]|$|E
50|$|The <b>Gauss-Markov</b> <b>theorem</b> {{states that}} {{regression}} models which fulfill the classical linear regression model assumptions provide the best, linear and unbiased estimators. With respect to ordinary least squares, the relevant {{assumption of the}} classical linear regression model is that the error term is uncorrelated with the regressors.|$|E
50|$|This {{estimator}} {{reaches the}} Cramér-Rao {{bound for the}} model, and thus is optimal {{in the class of}} all unbiased estimators. Note that unlike the <b>Gauss-Markov</b> <b>theorem,</b> this result establishes optimality among both linear and non-linear estimators, but only in the case of normally distributed error terms.|$|E
5000|$|In {{the general}} case, suppose that , [...] is the {{covariance}} matrix relating the quantities , [...] {{is the common}} mean to be estimated, and [...] is the design matrix [...] (of length [...] ). The <b>Gauss-Markov</b> <b>theorem</b> states that the estimate of the mean having minimum variance is given by: ...|$|E
50|$|For example, {{it is easy}} to {{show that}} the {{arithmetic}} mean of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the <b>Gauss-Markov</b> <b>theorem</b> apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.|$|E
50|$|While WLS assumes {{independence}} of observations {{it does not}} assume equal variance and is therefore a solution for parameter estimation {{in the presence of}} heteroscedasticity. The <b>Gauss-Markov</b> <b>theorem</b> and Aitken demonstrate that the best linear unbiased estimator (BLUE), the unbiased estimator with minimum variance, has each weight equal to the reciprocal of the variance of the measurement.|$|E
5000|$|It {{is known}} that the least squares {{estimator}} minimizes the variance of mean-unbiased estimators (under {{the conditions of the}} <b>Gauss-Markov</b> <b>theorem).</b> In the estimation theory for statistical models with one real parameter, the reciprocal of the variance of an ("efficient") estimator is called the [...] "Fisher information" [...] for that estimator. Because of this reciprocity, minimizing the variance corresponds to maximizing the information.|$|E
5000|$|The <b>Gauss-Markov</b> <b>theorem</b> {{states that}} under the spherical errors {{assumption}} (that is, the errors should be uncorrelated and homoscedastic) the estimator [...] is efficient {{in the class of}} linear unbiased estimators. This is called the best linear unbiased estimator (BLUE). Efficiency should be understood as if we were to find some other estimator [...] which would be linear in y and unbiased, then ...|$|E
50|$|Traditionally, {{statistical}} methods {{have relied on}} mean-unbiased estimators of treatment effects: Under {{the conditions of the}} <b>Gauss-Markov</b> <b>theorem,</b> least squares estimators have minimum variance among all mean-unbiased estimators. The emphasis on comparisons of means also draws (limiting) comfort from the law of large numbers, according to which the sample means converge to the true mean. Fisher's textbook on the design of experiments emphasized comparisons of treatment means.|$|E
5000|$|Note {{that the}} {{original}} strict exogeneity assumption Eεi&thinsp;&thinsp;xi [...] 0 implies a far richer set of moment conditions than stated above. In particular, this assumption implies that for any vector-function ƒ, the moment condition Eƒ(xi)·εi [...] 0 will hold. However it can be shown using the <b>Gauss-Markov</b> <b>theorem</b> that the optimal choice of function ƒ is to take ƒ(x) [...] x, which results in the moment equation posted above.|$|E
5000|$|The <b>Gauss-Markov</b> <b>theorem.</b> In {{a linear}} {{model in which}} the errors have {{expectation}} zero conditional on the independent variables, are uncorrelated and have equal variances, the best linear unbiased estimator of any linear combination of the observations, is its least-squares estimator. [...] "Best" [...] means that the least squares estimators of the parameters have minimum variance. The assumption of equal variance is valid when the errors all {{belong to the same}} distribution.|$|E
50|$|The {{earliest}} form of regression was {{the method}} of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method {{to the problem of}} determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the <b>Gauss-Markov</b> <b>theorem.</b>|$|E
5000|$|The {{solutions}} to the MME, [...] and [...] are best linear unbiased estimates (BLUE) and predictors (BLUP) for [...] and , respectively. This {{is a consequence of}} the <b>Gauss-Markov</b> <b>theorem</b> when the conditional variance of the outcome is not scalable to the identity matrix. When the conditional variance is known, then the inverse variance weighted least squares estimate is BLUE. However, the conditional variance is rarely, if ever, known. So it is desirable to jointly estimate the variance and weighted parameter estimates when solving MMEs.|$|E
50|$|As used in {{describing}} {{simple linear regression}} analysis, one assumption of the fitted model (to ensure that the least-squares estimators are each a best linear unbiased estimator of the respective population parameters, by the <b>Gauss-Markov</b> <b>theorem)</b> is that the standard deviations of the error terms are constant and do {{not depend on the}} x-value. Consequently, each probability distribution for y (response variable) has the same standard deviation regardless of the x-value (predictor). In short, this assumption is homoscedasticity. Homoscedasticity is not required for the estimates to be unbiased, consistent, and asymptotically normal.|$|E
50|$|In 1810, {{after reading}} Gauss's work, Laplace, after proving the central limit theorem, {{used it to}} give a large sample {{justification}} for the method of least square and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal {{in the sense that}} in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the <b>Gauss-Markov</b> <b>theorem.</b>|$|E
50|$|Another {{assumption}} of linear regression {{is that the}} variance {{be the same for}} each possible expected value (this is known as homoscedasticity). Univariate normality is not needed for least squares estimates of the regression parameters to be meaningful (see <b>Gauss-Markov</b> <b>theorem).</b> However confidence intervals and hypothesis tests will have better statistical properties if the variables exhibit multivariate normality. This can be assessed empirically by plotting the fitted values against the residuals, and by inspecting the normal quantile plot of the residuals. Note that it is not relevant whether the dependent variable Y is marginally normally distributed.|$|E
5000|$|The {{expressions}} given {{above are}} based on the implicit assumption that the errors are uncorrelated {{with each other and with}} the independent variables and have equal variance. The <b>Gauss-Markov</b> <b>theorem</b> shows that, when this is so, [...] is a best linear unbiased estimator (BLUE). If, however, the measurements are uncorrelated but have different uncertainties, a modified approach might be adopted. Aitken showed that when a weighted sum of squared residuals is minimized, [...] is the BLUE if each weight is equal to the reciprocal of the variance of the measurementThe gradient equations for this sum of squares are ...|$|E
50|$|The {{basic idea}} of kriging is {{to predict the}} value of a {{function}} at a given point by computing a weighted average of the known values of the function in the neighborhood of the point. The method is mathematically closely related to regression analysis. Both theories derive a best linear unbiased estimator, based on assumptions on covariances, make use of <b>Gauss-Markov</b> <b>theorem</b> to prove independence of the estimate and error, and make use of very similar formulae. Even so, they are useful in different frameworks: kriging is made for estimation of a single realization of a random field, while regression models are based on multiple observations of a multivariate data set.|$|E
50|$|If the {{experimental}} errors, , are uncorrelated, have {{a mean of}} zero and a constant variance, , the <b>Gauss-Markov</b> <b>theorem</b> states that the least-squares estimator, , has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or optimal, estimator of the parameters. Note particularly that this property is independent of the statistical distribution function of the errors. In other words, the distribution function of the errors {{need not be a}} normal distribution. However, for some probability distributions, {{there is no guarantee that}} the least-squares solution is even possible given the observations; still, in such cases it is the best estimator that is both linear and unbiased.|$|E
5000|$|In statistics, the <b>Gauss-Markov</b> <b>theorem,</b> {{named after}} Carl Friedrich Gauss and Andrey Markov, states {{that in a}} linear {{regression}} model in which the errors have expectation zero and are uncorrelated and have equal variances, the best linear unbiased estimator (BLUE) of the coefficients is given by the ordinary least squares (OLS) estimator, provided it exists. Here [...] "best" [...] means giving the lowest variance of the estimate, as compared to other unbiased, linear estimators. The errors {{do not need to}} be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance). The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the James-Stein estimator (which also drops linearity) or ridge regression.|$|E
