23|441|Public
30|$|Admati and Pfleiderer (1988) {{examine whether}} an <b>{{information}}</b> <b>owner</b> sells information directly to investors or trades {{on the information}} by creating a mutual fund. The latter can control the effects of competition among these indirectly informed traders.|$|E
40|$|Abstract—This paper {{suggests}} a legalized P 3 P based approach for data privacy {{protection for the}} <b>Information</b> <b>owner.</b> The model creates a trust engine between the operating service and user’s data repository. The trust engine now parses the data read/write queries with the privacy policy of the user and releases data or rejects requests as per its decision. Prior to that, {{there has to be}} a agreement between the <b>Information</b> <b>Owner</b> and Service provider called “Legal Handshake ” upon only which e- contract is generated which is legally binding to both the parties. Any breach to the contract will attract the legal penalties as per laws defined in the chosen legislative drafted in Data Legal Dictionary...|$|E
40|$|This papers uses new micro data on vehicle {{pollution}} and merges on Census <b>information</b> <b>owner</b> income {{to study the}} relationship between air {{pollution and}} individual income. It is important to quantify how this externality varies with individual income when considering the equity and efficiency of stringent vehicle emissions testing or vehicle scrappage programs. ...|$|E
40|$|This work {{is aimed}} at {{supporting}} system and <b>information</b> <b>owners</b> in their mission to apply a proper remedy when a security flaw is discovered during system operation. A broad analysis of the different aspects of flaw remediation {{has resulted in a}} structured taxonomy that will guide the system and <b>information</b> <b>owners</b> through the remedy identification process. The information produced in the process will help to make decisions about changes to the system or procedures. A selected vulnerability that was able to be removed using three different remedies is used as an example...|$|R
25|$|The {{two sources}} for the table below (Lloyd's Register and the Register of Shipping) agree in broad outline, but there are discrepancies. The {{registers}} published at different times, and are only as accurate as the <b>information</b> <b>owners</b> cared to provide.|$|R
50|$|The {{information}} in Lloyd's Register {{is only as}} accurate and up-to-date as the <b>information</b> <b>owners</b> of vessels bothered to provide. One finds, therefore, information that is stale-dated and inaccurate (i.e., contradicted in later listings). In the listing below, there is clearly a confusion of names between 1784 and 1787.|$|R
40|$|While {{the nascent}} Cloud Computing {{paradigm}} supported by virtualization has the upward new notion of edges, it lacks proper security and trust mechanisms. Edges are like on demand scalability and infinite resource provisioning as per the `pay-as-you-go 2 ̆ 7 manner {{in favour of}} a single <b>information</b> <b>owner</b> (abbreviated as INO from now onwards) to multiple corporate INOs. While outsourcing information to a cloud storage controlled by a cloud service provider (abbreviated as CSP from now onwards) relives an <b>information</b> <b>owner</b> of tackling instantaneous oversight and management needs, a significant issue of retaining the control of that information to the <b>information</b> <b>owner</b> {{still needs to be}} solved. This paper perspicaciously delves into the facts of the Cloud Computing security issues and aims to explore and establish a secure channel for the INO to communicate with the CSP while maintaining trust and confidentiality. The objective of the paper is served by analyzing different protocols and proposing the one in commensurate with the requirement of the security property like information or data confidentiality along the line of security in Cloud Computing Environment (CCE). To the best of our knowledge, we are the first to derive a secure protocol by successively eliminating the dangling pitfalls that remain dormant and thereby hamper confidentiality and integrity of information that is worth exchanging between the INO and the CSP. Besides, conceptually, our derived protocol is compared with the SSL from the perspectives of work flow related activities along the line of secure trusted path for information confidentiality. <br /...|$|E
40|$|Abstract. Individuals {{today have}} no control over the way their per-sonal {{information}} is being used even though they are the ones to suffer the consequences of any unwanted uses of their information. We propose addressing this externality through the creation of a market for personal information, where licenses to access individuals ’ personal information will be voluntarily traded. Through this market, satisfactory compensa-tion to the <b>information</b> <b>owner</b> is provided, whilst personal information remains under the owner’s control. Using cryptographic tools and micro-payments we propose and develop a prototype for personal information trades where the above principles are implemented and tested...|$|E
40|$|We {{introduce}} {{three new}} mechanisms that allow trust {{to be built}} into cloud computing called the Private Virtual Infrastructure (PVI) Locator Bot (LoBot) Trusted Virtual Environment Module (TVEM) Trusted Environment Key (TEK) <<</ines trust from the <b>information</b> <b>owner</b> and the service provider to create a dual root of trust for the TVEM that is distinct for every virtual environment and separate from the host platform’s trust. PVI, Locator Bot, and TVEM can be used individually or combined to provide a foundation for trust in cloud computing. They enable organizations to maintain control of their information in the cloud and realize benefits of cloud computing...|$|E
5000|$|Queen Jool; <b>information</b> broker; <b>owner,</b> Rik's cantina (Hutt female) ...|$|R
5|$|Lloyd's Register {{is only as}} {{accurate}} as the <b>information</b> <b>owners</b> gave it, and there are discrepancies between the entries for vessels and information from other sources. That said, there is generally a strong correspondence between the information in Lloyd's Register, and other sources, at least with respect to Prince of Wales.|$|R
50|$|Unfortunately, {{information}} {{is only as}} correct and up-to-date as the <b>information</b> vessel <b>owners</b> provided.|$|R
40|$|Access {{control and}} privacy {{policies}} change {{during the course}} of collaboration. Information is often shared with collaborators outside of the traditional “perimeterized” organizational computer network. At this point the <b>information</b> <b>owner</b> (in the legal data protection sense) loses persistent control over their information. They cannot modify the policy that controls who accesses it, and have that enforced on the information wherever it resides. However, if patient consent is withdrawn or if the collaboration comes to an end naturally, or prematurely, the owner may be required to withdraw further access to their information. This paper presents a system that enhances the way access control technology is currently deployed so that information owners retain control of their access control and privacy policies, even after information has been shared...|$|E
40|$|Dear Sir or Madam: By {{the above}} {{referenced}} letter, Entergy Operations, Inc. (Entergy) proposed a {{change to the}} Grand Gulf Nuclear Station, Unit 1 (GGNS) Technical Specifications (TS) to extend the Local Power Range Monitor (LPRM) Calibration Frequency. Some of {{the information contained in}} the referenced letter was considered proprietary to AREVA NP Inc. An affidavit by the <b>information</b> <b>owner,</b> AREVA NP, was provided to support a request to withhold the proprietary information from public disclosure in accordance with 10 CFR 9. 17 (a) (4), 10 CFR 2. 390 (a) (4), and 10 CFR 2. 390 (b) (l). This supplement provides a revised affidavit. The need for the revised Ndavit was previously discussed with the NRR Project Manager. The response to the request for nondisclosure should be provided to...|$|E
40|$|Cloud {{computing}} places an organization’s {{sensitive data}} in the control of a third party, introducing a significant level of risk on the privacy and security of the data. We propose a new management and security model for cloud computing called the Private Virtual Infrastructure (PVI) that shares the responsibility of security in cloud computing between the service provider and client, decreasing the risk exposure to both. The PVI datacenter is under control of the <b>information</b> <b>owner</b> while the cloud fabric is under control of the service provider. A cloud Locator Bot pre-measures the cloud for security properties, securely provisions the datacenter in the cloud, and provides situational awareness through continuous monitoring of the cloud security. PVI and Locator Bot provide the tools that organizations require to maintain control of their information in the cloud and realize the benefits of cloud computing. ...|$|E
25|$|Aggregation {{does not}} {{automatically}} trigger an increase in protective marking. For instance, a database with thousands of records which are individually OFFICIAL should not be relabeled as a SECRET database. Instead, <b>information</b> <b>owners</b> are expected {{to make decisions about}} controls based on a risk assessment, and should consider what the aggregated information is, who needs to access it, and how.|$|R
50|$|Lloyd's Register for {{the years}} 1820 to 1823 does not show a voyage to India. However, the Register was only as {{accurate}} as the <b>information</b> <b>owners</b> chose to feed it. The Register of Shipping for 1821 however does show a voyage to India. It has Asia, Patterson, master, Chapman, owner, sailing from London to Bombay, and then London to Quebec.|$|R
5000|$|Provides <b>information</b> to <b>owners</b> of Victorian and Edwardian houses {{about how}} they can better look after their buildings.|$|R
40|$|The {{insider threat}} has proved a tough nut to crack. Previous {{work in this}} area has been {{dominated}} by efforts to model normal user behavior through statistical measures and then detect substantial anomalies. Unfortunately, while these methods have shown some ability in the detection of masqueraders, broader applications have proved ineffectual due to extremely high false alarm rates. In this paper we describe an alternative approach (SL-SAFE) that can achieve high levels of accuracy in detecting the unauthorized access and distribution of sensitive/proprietary information by insiders – the single most costly type of computer crime. SL-SAFE succeeds in this task by means of a stochastic sampling of bottlenecks through which information must flow in order to be useful to the malicious insider. Further, it achieves a low (and shrinking) false alarm rate by validating its suspicions through public information sources and eliciting feedback from the <b>information</b> <b>owner.</b> 1...|$|E
40|$|Abstract. In this paper, {{we argue}} that a new {{generation}} of Policy-Aware Web (PAW) technology can hold the key for providing open, distributed and scalable information access on the World Wide Web. Our approach provides for the publication of declarative access policies in a way that allows transparency for information sharing among parties. In addition, greater control over information release can be placed {{in the hands of the}} <b>information</b> <b>owner,</b> by employing rule-based discretionary access control. In this paper, we outline the kind of reasoning support which is needed to achieve these goals. Also, we present our initial steps in this direction. Our example application for this purpose is a calendar and photo sharing web site that uses a distributed policy framework (REIN) built on top of a rule-based reasoner (CWM). 1 Introduction/Motivation While Semantic Web (SW) technologies continue to increase in popularity and scope, certain data owners are still reluctant to make their data public. Th...|$|E
40|$|In {{history of}} mankind, new {{findings}} knowledgehas been transfer to present. Documentationstudies of this knowledge {{is essential for}} a missionof both <b>information</b> <b>owner</b> rights protection and andto {{be transferred to the}} future. These studies end withpatent document which consists information aboutright owner and all details of the invention. But priorto that result, documentation is an ongoing issue inapplication and admission proces. In Turkey, nationalauthorising office is Turkish Patent Institute (TPE) and the international patent offices carry out theirwork of this certification. Intellectual property rightsthat are the basis of patent is secured by national andinternational laws. This warranty is the basis of thescientific and technological development. In the study,it is empasized that Concept of patent, description ofthe procedures required to obtain the patent, role ofthe Information and Records Management science inthis process. Information and Records Managementscience’s joint efforts with interdisciplinary work forObtaining the patent document and establishing documentmanagement system will provide a significantcontribution to this field...|$|E
40|$|Recent {{advances}} in networking, handheld computing and sensors technologies {{have led to}} the emergence of context-aware systems. The vast amounts of personal information collected by such systems has led to growing concerns about the privacy of their users. Users concerned about their private information are likely to refuse participation in such systems. Therefore, it is quite clear that for any context-aware system to be acceptable by the users, mechanisms for controlling access to personal information are a necessity. According to Alan Westin "privacy is the claim of individuals, groups, or institutions to determine for themselves when, how and to what extent information is communicated to others" 1. Within this context we can classify users as either <b>information</b> <b>owners</b> or <b>information</b> receivers. It is also acknowledged that <b>information</b> <b>owners</b> are willing to disclose personal information if this disclosure is potentially beneficial. So, the acceptance of any context-aware system depends on the provision of mechanisms for fine-grained control of the disclosure of personal information incorporating an explicit notion of benefit...|$|R
50|$|Dealer {{plates are}} white {{background}} with red typeface, usually for vehicles yet to have legal and confirmed <b>information</b> and <b>owner.</b>|$|R
5000|$|Generic or test IDs {{must not}} be created or enabled on {{production}} systems unless specifically authorized by the relevant <b>Information</b> Asset <b>Owners.</b>|$|R
40|$|The {{business}} use of cloud computing services {{is motivated by}} the ease of use and the potential financial cost reductions. Service failure may occur when the service provider does not protect information or when {{the use of the}} services becomes overly complex and difficult. The benefits also bring optimisation challenges for the information owners who must assess the service security risk {{and the degree to which}} new human behaviours are required. In this research we look at the risk of identity theft when ease of service access is provided through a Single Sign On (SSO) authorisation and ask: What are the optimal behavioural expectations for a Cloud service <b>information</b> <b>owner?</b> Federated identity management is a well-developed design literature for solutions to optimising human behaviours in relation to the new technologies. We briefly review the literature and then propose a working solution that optimises the trade-off between disclosure risk, human user risk and service security. Both breech and non-use of a system are failures...|$|E
40|$|Searching for useful {{information}} {{is a difficult}} job by the virtue of information overloading problem. With the technological advances, notably World-Wide Web (WWW), it allows every ordinary <b>information</b> <b>owner</b> to offer information on line for others to access and retrieve. However, it also makes up a global information system that is extremely large-scale, diverse and dynamic. Internet agents and Internet search engines {{have been used to}} deal with such problems. But the search results are usually not quite relevant to what a user wants since most of them use simple keyword matching. In this paper, we propose a natural language processing based agent (NIAGENT) that understands a user's natural query. NIAGENT not only cooperates with a meta Internet search engine in order to increase recall of web pages but also analyzes the contents of the referenced documents to increase precision. Moreover, the proposed agent is autonomous, light-weighted, and multithreaded. The architectural design al [...] ...|$|E
40|$|Abstract — Cloud Computing is a {{paradigm}} shift from the mainframe to client-server in the early 1980 s. Cloud computing describes a new supplement, consumption and delivery model of IT services based on Internet and it typically evolves over the Internet provision of dynamically scalable and often virtualized resources. Cloud Computing places organization’s sensitive data in control to third party which introduces a significant level of risk on the privacy and security of sensitive data of organization. This research paper discusses a new management and security model of cloud computing called Private Virtual Infrastructure (PVI) whose main responsibility is to share the security of cloud computing between the service provider and client and elimination of risk exposure between the two. PVI datacenter is under the direct control of <b>information</b> <b>owner</b> whereas the cloud fabric comes {{under the control of}} Service Provider. A cloud locater Bot pre-ascertain the security properties of the cloud, security provisions and provides situational awareness through continuous monitoring of the cloud security. PVI and locater Bot provide the tools that organizations require to maintain control over their information in the cloud and retains the benefits of cloud computing...|$|E
40|$|Social context {{information}} has been used with encouraging results in developing socially aware applications in different domains. However, users' social context information is distributed over the Web and managed by many different proprietary applications, which is a challenge for application developers as they must collect information from different sources and wade {{through a lot of}} irrelevant information to obtain the social context information of interest. On the other hand, it is extremely hard for <b>information</b> <b>owners</b> to control how their information should be exposed to different users and applications. Combining the social context information from the diverse sources, incorporating richer semantics and preserving information owners' privacy could greatly assist the developers and as well as the <b>information</b> <b>owners.</b> In this paper, we introduce a social context information management system (SCIMS). It includes the ability to acquire raw social data from multiple sources; an ontology-based model for classifying, inferring and storing social context information, in particular, social relationships and status; an ontology-based policy model and language for owners to control access to their information; a query interface for accessing and utilizing social context information. We evaluate the performance of SCIMS using real data from Facebook, LinkedIn, Twitter, and Google Calendar and demonstrate its applicability through a socially aware phone call application...|$|R
50|$|This is a {{complexity}} often lost on <b>Information</b> Asset <b>Owners</b> previously {{used to the}} strictly hierarchical tiered rising {{structure of}} GPMS (e.g. UNCLASSIFIED, PROTECT, RESTRICTED, SECRET, TOP SECRET).|$|R
40|$|Digital rights {{management}} allows <b>information</b> <b>owners</b> {{to control}} the use and dissemination of electronic documents via a machine-readable licence. This paper describes the design and implementation of a system for creating and enforcing licences containing location constraints {{that can be used}} to restrict access to sensitive documents to a defined area. Documents can be loaded onto a portable device and used in the approved areas, but cannot be used if the device moves to another area. Our contribution includes a taxonomy for access control in the presence of requests to perform non-instantaneous controlled actions...|$|R
40|$|Nowadays the {{broadcasting}} {{of information}} through the Internet-based World Wide Web (WWW) is simple, fast and attractive. But {{it is also}} easy and fast to transmit this information with wrong data, or {{in such a way}} that the consumer acquires a wrong knowledge (the only ensurance about data correctness is the human author). The more distant is the service provider (the person who produces the WWW pages) from the <b>information</b> <b>owner</b> (the source of data) the more accute is this problem. In this context, it is usually required quality in the information dissemination process. But, actually, it is easier to state that than to understand deeply its meaning, and moreover than to assure error-safe document processing. In this paper we intend to discuss the concept of quality in the production of documents and its relationships with the different actors in the process, aiming at the proposal of therapeutics to avoid some kind of semantic erros. Namely, we will present some extension to the SGML document type definitions (DTD's) that will enable the inclusion of some semantic constraints useful to guarentee the information quality. [SGML Open...|$|E
40|$|Private Virtual Infrastructure is a {{security}} architecture for cloud computing {{which uses a}} new trust model to share the responsibility of security in cloud computing between the service provider and client, decreasing the risk exposure to both. Private Virtual Infrastructure is under control of the <b>information</b> <b>owner</b> while the cloud fabric is under control of the service provider. The Private Virtual Infrastructure architecture comprises a cluster of trusted computing fabric platforms that host virtual servers running an application server with a Locator Bot security service. The cloud Locator Bot pre-measures the cloud platform for security properties to determine the trustworthiness of the platform. The Locator Bot uses Trusted Execution Technology and virtual Trusted Platform Modules to pre-measure the target environment and securely provision the Private Virtual Infrastructure in the cloud thus protecting information by preventing data from being placed in malicious or untrusted environments. Private Virtual Infrastructure — including Locator Bot — provides organizations tools to maintain control of their information in the cloud and realize benefits of cloud computing, with assurance that their information is protected. This paper presents a cloud trust model, Private Virtual Infrastructure architecture, and a Locator Bot protocol in enough detail to support further analysis or implementation...|$|E
40|$|An <b>information</b> <b>owner,</b> {{possessing}} diverse data sources, {{might want}} to offer information services based on these sources to cooperation partners and to this end interact with these partners by receiving and sending messages, which the owner on his part generates by program execution. Independently from data representation or its physical storage, information release to a partner might be restricted by the owner's confidentiality policy on an integrated, unified view of the sources. Such a policy should even be enforced if the partner as an intelligent and only semi-honest attacker attempts to infer hidden information from message data, also employing background knowledge. For this problem of inference control, we present a framework for a unified, holistic control of information flow induced by program-based processing of the data sources to messages sent to a cooperation partner. Our framework expands on and combines established concepts for confidentiality enforcement and its verification and is instantiated in a Java environment. More specifically, as a hybrid control we combine gradual release of information via declassification, enforced by static program analysis using a security type system, with a dynamic monitoring approach. The dynamic monitoring employs flow tracking for generalizing values to be declassified under confidentiality policy compliance. Comment: 44 page...|$|E
2500|$|This is a {{complexity}} often lost on <b>Information</b> Asset <b>Owners</b> previously {{used to the}} strictly hierarchical tiered rising {{structure of}} GPMS [...] (e.g. UNCLASSIFIED, PROTECT, RESTRICTED, SECRET, TOP SECRET).|$|R
5000|$|... {{restrictions}} {{that do not}} leak <b>information</b> about process <b>owners</b> ...|$|R
5000|$|Assists {{citizens}} with proper tools and <b>information</b> for pet <b>owners</b> ...|$|R
