20|80|Public
30|$|The default rule {{is taken}} when all rules {{on the outer}} <b>false</b> <b>link</b> chain fail.|$|E
30|$|Defaults with {{priority}} min + 1 form a first <b>false</b> <b>link</b> chain on {{the first}} exception level in T. Order of rules in this chain is irrelevant because defaults have mutually exclusive prerequisites. Defaults with same priorities >min + 1 are also converted to rules in T, and they form separate <b>false</b> <b>link</b> chains of rules within subsequent exception levels of T(see step 1 in Fig.  4.3).|$|E
3000|$|... r is {{added as}} a new rule—i.e. r is {{attached}} to the outer <b>false</b> <b>link.</b> Hence, some conclusions of the default rule no longer apply.|$|E
40|$|Data {{collected}} using traceroute-based algorithms underpins {{research into}} the Internet’s router-level topology, though {{it is possible to}} infer <b>false</b> <b>links</b> from this data. One source of false inference is the combination of per-flow load-balancing, in which more than one path is active from a given source to destination, and classic traceroute, which varies the UDP destination port number or ICMP checksum of successive probe packets, which can cause per-flow load-balancers to treat successive packets as distinct flows and forward them along different paths. Consequently, successive probe packets can solicit responses from unconnected routers, leading to the inference of <b>false</b> <b>links.</b> This paper examines the inaccuracies induced from such false inferences, both on macroscopic and ISP topology mapping. We collected macroscopic topology data to 365 k destinations, with techniques that both do and do not try to capture load balancing phenomena. We then use alias resolution techniques to infer if a measurement artifact of classic traceroute induces a <b>false</b> router-level <b>link.</b> This technique detected that 2. 71 % and 0. 76 % of the links in our UDP and ICMP graphs were falsely inferred due to the presence of load-balancing. We conclude that most per-flow load-balancing does not induce <b>false</b> <b>links</b> when macroscopic topology is inferred using classic traceroute. The effect of <b>false</b> <b>links</b> on ISP topology mapping is possibly much worse, because the degrees of a tier- 1 ISP’s routers derived from classic traceroute were inflated by a median factor of 2. 9 as compared to those inferred with Paris traceroute...|$|R
50|$|The {{corresponding}} {{statement is}} <b>false</b> for <b>links.</b>|$|R
40|$|This paper {{proposes a}} {{simplified}} {{approach to the}} assembly of large physical genome maps. The approach focuses on two key problems: (i) the integration of diverse forms of data from numerous sources, and (ii) the detection and removal of errors and anomalies in the data. The approach simplifies map assembly by dividing it into three phases [...] -overlap, linkage and ordering. In the first phase, all forms of overlap data are integrated into a simple abstract structure, called clusters, where each cluster {{is a set of}} mutually-overlapping DNA segments. This phase filters out many questionable overlaps in the mapping data. In the second phase, clusters are linked together into a weighted intersection graph. <b>False</b> <b>links</b> between widely separated regions of the genome show up as crooked, branching structures in the graph. Removing these <b>false</b> <b>links</b> produces graphs that are straight, reflecting the linear structure of chromosomes. From these straight graphs, the third phase constructs a physical m [...] ...|$|R
40|$|In this research, {{we study}} the {{optimization}} challenges of MANET and cross-layer technique {{to improve its}} performance. We propose an adaptive retransmission limits algorithm for IEEE 802. 11 MAC to reduce the <b>false</b> <b>link</b> failures and predict the node mobility. We implemented cross layer interaction between physical and MAC layers. The MAC layer utilizes the physical layer information for differentiating <b>false</b> <b>link</b> failure from true link failure. The MAC layer adaptively selects a retransmission limit (short and long) based on the neighbour signal strength and sender node speed information from the physical layer. The proposed approach tracks the signal strength of each node in network and, while transmitting to a neighbour node, if it's received signal strength is high and is received recently then Adaptive MAC persists in its retransmission attempts. As there is high probability that neighbour node is still in transmission range and may be not responding due to some problems other then mobility. In this paper, we evaluate the performance of MANET and show that how our Adaptive MAC greatly improves it. The simulation is done using Network Simulator NS- 2...|$|E
40|$|Routing {{protocols}} {{based on}} the link-state paradigm notoriously suffer from the problem of stale information in highly dynamic traffic scenarios, leading to an overall loss in routing efficiency. Solutions {{are not easy to}} come by, since an increase in the frequency of link state advertisements is equally dangerous: route flapping (i. e., periodic route changes that force traffic to be routed through an alternately underloaded set of paths) {{is one of the main}} drawbacks. In this paper we propose a novel, yet simple link state mechanism that may deliberately advertise <b>false</b> <b>link</b> state information with the purpose of stabilizing the routing, while keeping a high network utilizatio...|$|E
40|$|Abstract — Routing {{protocols}} {{based on}} the link-state paradigm notoriously suffer from the problem of stale information in highly dynamic traffic scenarios, leading to an overall loss in routing efficiency. Solutions {{are not easy to}} come by, since an increase in the frequency of link state advertisements is equally dangerous: route flapping (i. e., periodic route changes that force traffic to be routed through an alternately underloaded set of paths) {{is one of the main}} drawbacks. In this paper we propose a novel, yet simple link state mechanism that may deliberately advertise <b>false</b> <b>link</b> state information with the purpose of stabilizing the routing, while keeping a high network utilization. Keywords: Link-state Routing, Traffic Engineering I...|$|E
25|$|Facebook and Twitter are {{not immune}} to {{messages}} containing spam links. Most insidiously, spammers hack into accounts and send <b>false</b> <b>links</b> {{under the guise of}} a user's trusted contacts such as friends and family. As for Twitter, spammers gain credibility by following verified accounts such as that of Lady Gaga; when that account owner follows the spammer back, it legitimizes the spammer and allows him or her to proliferate.|$|R
50|$|Upon the {{publication}} of Overworld, Kolb was again recruited, this time by the Department of Homeland Security, to help investigate two white collar criminals with connections to the CIA. Kolb's investigation of Robert Sensi and Richard Hirschfeld led him to discover and foil a conspiracy to smear the John Kerry 2004 presidential campaign with <b>false</b> <b>links</b> to Al Qaeda. This {{became the subject of}} his 2007 book America at Night, which was reviewed by The New York Times on January 25, 2007.|$|R
50|$|Finally, an {{important}} application that community detection has found in network science is {{the prediction of}} missing links and the identification of <b>false</b> <b>links</b> in the network. During the measurement process, some links may not get observed {{for a number of}} reasons. Similarly, some links could falsely enter into the data because of the errors in the measurement. Both these cases are well handled by community detection algorithm since it allows one to assign the probability of existence of an edge between a given pair of nodes.|$|R
40|$|Abstract: Due to the MAC (medium access control) {{protocol}}, {{routing protocol}} and TCP itself, the TCP flow is very unstable in the wireless Ad Hoc networks based on IEEE 802. 11. However, {{the main reasons}} that make the TCP flow unstable are the unfairness of MAC protocol and the <b>false</b> <b>link</b> failure message that causes the long route discovery procedure. Combining with the IEEE 802. 11 MAC protocol and DSR routing protocol, these causes are analyzed in detail and the enhanced algorithm to IEEE 802. 11 MAC protocol and DSR routing protocol is given in this paper. The simulational {{results show that the}} proposed algorithm can ultimately avoid the instability of TCP flow and increase its average throughput in the wireless Ad Hoc networks. Key words: wireless Ad Hoc network; MAC (medium access control) protocol; routing protocol; TCP flow...|$|E
30|$|It is {{well known}} that {{transmission}} control protocol (TCP) performance degrades severely in IEEE 802.11 -based wireless ad hoc networks. We first identify two critical issues leading to the TCP performance degradation: (1) unreliable broadcast, since broadcast frames are transmitted without the request-to-send and clear-to-send (RTS/CTS) dialog and Data/ACK handshake, so they are vulnerable to the hidden terminal problem; and (2) <b>false</b> <b>link</b> failure which occurs when a node cannot successfully transmit data temporarily due to medium contention. We then propose a scheme to use a narrow-bandwidth, out-of-band busy tone channel to make reservation for broadcast and link error detection frames only. The proposed scheme is simple and power efficient, because only the sender needs to transmit two short messages in the busy tone channel before sending broadcast or link error detection frames in the data channel. Analytical results show that the proposed scheme can dramatically reduce the collision probability of broadcast and link error detection frames. Extensive simulations with different network topologies further demonstrate that the proposed scheme can improve TCP throughput by 23 % to 150 %, depending on user mobility, and effectively enhance both short-term and long-term fairness among coexisting TCP flows in multihop wireless ad hoc networks.|$|E
40|$|Abstract, Satish K. Tripathi £ Mobility in ad hoc {{networks}} causes frequent link failures, {{which in}} turn causes packet losses. TCP attributes these packet losses to congestion. This incorrect inference results in frequent TCP retransmission time-outs and therefore a degradation in TCP performance even at light loads. We propose mechanisms {{that are based on}} signal strength measurements to alleviate such packet losses due to mobility. Our key ideas are (a) if the signal strength measurements indicate that a link failure is most likely due to a neighbor moving out of range, in reaction, facilitate the use of temporary higher transmission power to keep the link alive and, (b) if the signal strength measurements indicate that a link is likely to fail, initiate a route re-discovery proactively before the link actually fails. We make changes at the MAC and the routing layers to predict link failures and estimate if a link failure is due to mobility. We also propose a simple mechanism at the MAC layer that can help alleviate <b>false</b> <b>link</b> failures, which occur due to congestion when the IEEE 802. 11 MAC protocol is used. We compare the above proactive and reactive schemes and also demonstrate the benefits of using them together and along with our MAC layer extension. We show that...|$|E
40|$|This paper {{presents}} an algorithm that builds topological maps, using omnidirectional vision {{as the only}} sensor modality. Local features are extracted from images obtained in sequence, and are used both to cluster the images into nodes and to detect links between the nodes. The algorithm is incremental, reducing the computational requirements of the corresponding batch algorithm. Experimental results in a complex, indoor environment show that the algorithm produces topologically correct maps, closing loops without suffering from perceptual aliasing or <b>false</b> <b>links.</b> Robustness to lighting variations was further demonstrated by building correct maps from combined multiple datasets collected {{over a period of}} 2 month...|$|R
50|$|Facebook and Twitter are {{not immune}} to {{messages}} containing spam links. Most insidiously, spammers hack into accounts and send <b>false</b> <b>links</b> {{under the guise of}} a user's trusted contacts such as friends and family. As for Twitter, spammers gain credibility by following verified accounts such as that of Lady Gaga; when that account owner follows the spammer back, it legitimizes the spammer and allows him or her to proliferate.Twitter has studied what interest structures allow their users to receive interesting tweets and avoid spam, despite the site using the broadcast model, in which all tweets from a user are broadcast to all followers of the user.|$|R
40|$|This paper {{explores the}} {{effectiveness}} of network attack when the attacker has imperfect information about the network. For Erdős-Rényi networks, we observe that dynamical importance and betweenness centrality-based attacks are surprisingly robust {{to the presence of}} a moderate amount of imperfect information and are more effective compared with simpler degree-based attacks even at moderate levels of network information error. In contrast, for scale-free networks {{the effectiveness of}} attack is much less degraded by a moderate level of information error. Furthermore, in the Erdőos-Rényi case the effectiveness of network attack is much more degraded by missing links as compared with the same number of <b>false</b> <b>links...</b>|$|R
40|$|Traceroute [2] is used {{to learn}} the path between two {{machines}} in the internet. Uses range from the diagnosis of network problems to the assemblage of internet maps. Unfortunately, traceroute measurements can be inaccurate and incomplete when the measured route traverses a load balancer. Consider the example in Fig. 1, where L 1, L 2 and L 3 are load balancers. The left side represents the actual network topology. Routers are represented as circles. We also numbered each of their interfaces. Probes are represented by black squares, either above the topology if they traverse L 2, or below if they traverse L 3. The right side is a possible classic traceroute outcome. This example illustrates the two problems with classic traceroute under load balancing. First, the discovered path is inaccurate since it reports a <b>false</b> <b>link</b> (the link (L 20,C 0) does not exist is the real topology). Second, it is incomplete since traceroute missed half of the nodes and their respective links. As a result, one may diagnose a wrong path or build an incomplete map of the network. Recent work [1] proposes a new traceroute implementation called Paris traceroute 1, which, by controlling the probe 1 Paris traceroute is available for download a...|$|E
40|$|Abstract. Mobility in ad hoc {{networks}} causes link failures, {{which in}} turn result in packet losses. TCP attributes these losses to congestion. This results in frequent TCP retransmission timeouts and degradation in TCP performance even at light loads. We propose mechanisms {{that are based on}} signal strength measurements to alleviate such packet losses due to mobility at light loads. Our key ideas are (a) if the signal strength measurements indicate that a link failure is most likely due to a neighbor moving out of range, in reaction, facilitate the use of temporary high power to re-establish the link and, (b) if the signal strength measurements indicate that a link is likely to fail, initiate a route rediscovery proactively before the link actually fails. We make changes at the MAC and the routing layers to predict link failures and estimate if a link failure is due to mobility. We also propose a simple mechanism that can help alleviate <b>false</b> <b>link</b> failures that occur due to congestion when the IEEE 802. 11 MAC protocol is used. We compare the above proactive and reactive schemes and also demonstrate the benefits of using them together. We show that, in high mobility, the performance of a TCP session can increase by as much as 45 percent when our methods are incorporated. Keywords: Power Management, Ad Hoc Networks, TCP, Signal Strength, IEEE 802. 1...|$|E
40|$|Abstract — Place {{recognition}} for loop closure detection {{lies at the}} heart of every Simultaneous Localization and Mapping (SLAM) method. Recently methods that use cameras and describe the entire image by one holistic feature vector have experienced a resurgence. Despite the success of these methods, it remains unclear how a descriptor should be constructed for this particular purpose. The problem of choosing the right descriptor becomes even more pronounced in the context of life long mapping. The appearance of a place may vary considerably under different illumination conditions and over the course of a day. None of the handcrafted descriptors published in literature are particularly designed for this purpose. Herein, we propose to use a set of elementary building blocks from which millions of different descriptors can be constructed automatically. Moreover, we present an evaluation function which evaluates the performance of a given image descriptor for place recognition under severe lighting changes. Finally we present an algorithm to efficiently search the space of descriptors to find the best suited one. Evaluating the trained descriptor on a test set shows a clear superiority over its hand crafted counter parts like BRIEF and U-SURF. Finally we show how loop closures can be reliably detected using the automatically learned descriptor. Two overlapping image sequences from two different days and times are merged into one pose graph. The resulting merged pose graph is optimized and does not contain a single <b>false</b> <b>link</b> while at the same time all true loop closures were detected correctly. The descriptor and the place recognizer source code is pub-lished with datasets o...|$|E
40|$|Abstract—Requirements {{traceability}} {{ensures that}} source code {{is consistent with}} documentation and that all requirements have been implemented. During software evolution, features are added, removed, or modified, the code drifts away from its original requirements. Thus traceability recovery approaches becomes necessary to re-establish the traceability relations between requirements and source code. This paper presents an approach (Coparvo) complementary to existing traceability recovery approaches for object-oriented programs. Coparvo reduces <b>false</b> positive <b>links</b> recovered by traditional traceability recovery processes thus reducing the manual validation effort. Coparvo assumes that information extracted from different entities (e. g., class names, comments, class variables, or methods signatures) are different information sources; they may have different level of reliability in requirements traceability and each information source may act as a different expert recommending traceability links. We applied Coparvo on three data sets, Pooka, SIP Communicator, and iTrust, to filter out <b>false</b> positive <b>links</b> recovered via the information retrieval approach i. e., vector space model. The results show that Coparvo significantly improves the of the recovered links accuracy and also reduces up to 83 % effort required to manually remove <b>false</b> positive <b>links.</b> Keywords—Traceability, requirements, source code, experts, Object-Oriented, source code partitions. I...|$|R
40|$|Objective: To {{gain insight}} into the {{performance}} of deterministic record linkage (DRL) vs. probabilistic record linkage (PRL) strategies under different conditions by varying the frequency of registration errors {{and the amount of}} discriminating power. Study Design and Setting: A simulation study in which data characteristics were varied to create a range of realistic linkage scenarios. For each scenario, we compared the number of misclassifications (number of false nonlinks and <b>false</b> <b>links)</b> made by the different linking strategies: deterministic full, deterministic N- 1, and probabilistic. Results: The full deterministic strategy produced the lowest number of <b>false</b> positive <b>links</b> but at the expense of missing considerable numbers of matches dependent on the error rate of the linking variables. The probabilistic strategy outperformed the deterministic strategy (full or N- 1) across all scenarios. A deterministic strategy can match the performance of a probabilistic approach providing that the decision about which disagreements should be tolerated is made correctly. This requires a priori knowledge about the quality of all linking variables, whereas this information is inherently generated by a probabilistic strategy. Conclusion: PRL is more flexible and provides data {{about the quality of the}} linkage process that in turn can minimize the degree of linking errors, given the data provided. (C) 2011 Elsevier Inc. All rights reserve...|$|R
40|$|Network {{researchers}} commonly use reverse DNS lookups of router {{names to}} provide geographic or topological {{information that would}} otherwise be difficult to obtain. By systematically examining a large ISP, we find that a certain percentage of these names are incorrect. We develop techniques to automatically identify these misnamings, and determine the actual locations, which we validate against the configuration of the ISP’s routers. While the actual number of misnamings is small, these errors induce a large number of <b>false</b> <b>links</b> in the inferred connectivity graph. We also measure the effects on path inflation, and find that the misnamings make path inflation and routing problems appear much worse than they actually are. Finally, we discuss other metrics that may be affected, and avenues for future research in this area. ...|$|R
40|$|Traditional MAC and routing protocols, {{which are}} {{primarily}} designed for homogeneous networks wherein all nodes transmit {{with the same}} power, suffer performance degradations when employed in power heterogeneous networks. The observed degradations are due to link asymmetry, which arises as high power nodes that do not sense the transmissions of low power nodes can potentially initiate transmissions that interfere with the low power communications. Link layer asymmetry in power heterogeneous networks not only disrupts {{the functioning of the}} routing protocol in use, but also results in unfairness in medium access. In this paper, we develop a cross-layer framework to effectively address the link asymmetry problem at both the MAC and the routing layers. At the MAC layer, the framework intelligently propagates low power control messages to the higher power nodes, so as to preclude them from initiating transmissions while there are low power communications in progress within their sensing range. At the routing layer, the framework facilitates the efficient use of unidirectional links. We perform extensive simulations to study the performance of our proposed framework in various settings, and show that the overall throughput in power heterogeneous networks is enhanced by as much as 25 % over traditional layered approaches. In addition, we show that our schemes are also beneficial in power homogeneous settings, as they reduce the extent of <b>false</b> <b>link</b> failures that arise when the IEEE 802. 11 MAC protocol is used. In summary, our framework offers a simple yet effective and viable approach for medium access control and for supporting routing in power heterogeneous ad hoc networks...|$|E
40|$|Spam ’ is an {{electronic}} version of ‘junk mail ’ {{sent to a}} large number of people who do not request it, detailing products or services in which they may have no interest. Spam is sent by people who disguise their identity and whom it is difficult, if not impossible, to locate or deter. Senders of spam rely on the fact that, although most will reject the message, a minority of recipients will read and/or respond to it. Criminals have been quick to take advantage of this fact through activities such as ‘phishing ’ – where official-looking ‘spoofed ’ or trick emails (typically purporting to come from banks) attempt to persuade a user to click on a <b>false</b> <b>link</b> leading to a fraudulent web site. Once there, the user is asked to provide their online password, user name and/or other personal information in response to a fake but convincing security check. Mitigating the impact of spam involves a number of stakeholders including: 1. governments – by enacting legislation to prevent spam; 2. law enforcement agencies – by investigating spam; 3. internet service providers (ISPs) – by filtering all email to remove spam prior to releasing it to customers ’ inboxes; and 4. corporations – by filtering all email for spam and governing appropriate use of email via an applied email policy. Individuals can assist in the anti-spam effort by: » avoiding placing email address in a public domain, for example a chat room; » if placing an email address in a public domain, disguising it (so, for example...|$|E
40|$|Mobility in ad hoc {{networks}} causes frequent link failures, {{which in}} turn causes packet losses. TCP attributes these packet losses to congestion. This incorrect inference results in frequent TCP re-transmission time-outs and therefore a degradation in TCP performance even at light loads. We propose mechanisms {{that are based on}} signal strength measurements to alleviate such packet losses due to mobility. Our key ideas are (a) if the signal strength measurements indicate that a link failure is most likely due to a neighbor moving out of range, in reaction, facilitate the use of temporary higher transmission power to keep the link alive and, (b) if the signal strength measurements indicate that a link is likely to fail, initiate a route re-discovery proactively before the link actually fails. We make changes at theMACand the routing layers to predict link failures and estimate if a link failure is due to mobility. We also propose a simple mechanism at the MAC layer that can help alleviate <b>false</b> <b>link</b> failures, which occur due to congestion when the IEEE 802. 11 MAC protocol is used. We compare the above proactive and reactive schemes and also demonstrate the benefits of using them together and along with our MAC layer extension. We show that, in high mobility, the goodput of a TCP session can be improved by as much as 75 % at light loads (when there is only one TCP session in the network) when our methods are incorporated. When the network is heavily loaded (i. e., there are multiple TCP sessions in the network), the proposed schemes can improve the aggregate goodput of the TCP sessions by about 14 – 30 %, on average...|$|E
40|$|Consistency {{of naming}} forest plants was {{subjected}} to a field test in a rural community of Northeastern Thailand. Local experts supplied names {{for a set of}} trees and vines in a surveyed plot. Results showed a high level of agreement among the informants {{for more than half of}} the plants and less than 10 % of the plants were not named consistently by the majority of informants. Disagreement on names largely took the form of non-responses or degrees of specificity. In general, vines and immature understory plants produced the greatest diversity of opinion. Of the names collected, 53 % were recorded in standard botanical references but about half were linked with more than one Latin binomial, often in different families. Many <b>false</b> <b>links</b> could be quickly resolved if voucher specimens of the plants were compared with herbarium specimens...|$|R
40|$|Simulation of {{weightlessness}} is {{a desired}} replenishment {{for research in}} microgravity since access to space flights is limited. In real microgravity conditions, the human epidermoid cell line A 431 exhibits specific changes in the actin cytoskeleton resulting ultimately in the rounding-up of cells. This rounding of A 431 cells was studied in detail during exposure to Random Positioning Machine (RPM) rotation and magnetic levitation. Random rotation and magnetic levitation induced similar changes in the actin morphology of A 431 cells that were also described in real microgravity. A transient process of cell rounding and renewed spreading was observed in time, illustrated by a changing actin cytoskeleton and variation {{in the presence of}} focal adhesions. However, side effects of both methods easily can lead to <b>false</b> <b>linking</b> of cellular responses to simulated microgravity. Therefore further characterization of both methods is required...|$|R
25|$|Beginning in 1601, {{the couple}} ruled the Spanish Netherlands together, and after Albert's death Isabella was {{appointed}} Governor of the Netherlands {{on behalf of}} the King of Spain. A <b>false</b> anecdote <b>links</b> Isabella, the siege of Ostend, and the horse coat colour isabelline. The reign of Albert and Isabella is considered the Golden Age of the Spanish Netherlands.|$|R
40|$|Background: Functional {{connectivity}} {{analyses of}} multiple neurons provide a powerful bottom-up approach to reveal functions of local neuronal circuits by using simultaneous recording of neuronal activity. A statistical methodology, generalized linear modeling (GLM) of the spike response function, {{is one of}} the most promising methodologies to reduce <b>false</b> <b>link</b> discoveries arising from pseudo-correlation based on common inputs. Although recent advancement of fluorescent imaging techniques has increased the number of simultaneously recoded neurons up to the hundreds or thousands, the amount of information per pair of neurons has not correspondingly increased, partly because of the instruments' limitations, and partly because the number of neuron pairs increase in a quadratic manner. Consequently, the estimation of GLM suffers from large statistical uncertainty caused by the shortage in effective information. Results: In this study, we propose a new combination of GLM and empirical Bayesian testing for the estimation of spike response functions that enables both conservative false discovery control and powerful functional connectivity detection. We compared our proposed method's performance with those of sparse estimation of GLM and classical Granger causality testing. Our method achieved high detection performance of functional connectivity with conservative estimation of false discovery rate and q values in case of information shortage due to short observation time. We also showed that empirical Bayesian testing on arbitrary statistics in place of likelihood-ratio statistics reduce the computational cost without decreasing the detection performance. When our proposed method was applied to a functional multi-neuron calcium imaging dataset from the rat hippocampal region, we found significant functional connections that are possibly mediated by AMPA and NMDA receptors. Conclusions: The proposed empirical Bayesian testing framework with GLM is promising especially when the amount of information per a neuron pair is small because of growing size of observed network...|$|E
40|$|Abstract:- The {{performance}} of the IEEE 802. 11 MAC protocol {{has been shown to}} degrade considerably in an ad hoc network with nodes that transmit at heterogeneous power levels. The main cause of this degradation is the potential inability of high power nodes to hear the RTS/CTS exchanges between nodes when at least one node involved in communication is a low power node. The propagation of the CTS message beyond the one-hop neighborhood of two communicating low power nodes was considered in our prior work in an attempt to alleviate this effect. However, this resulted in excessive overhead and further degraded the performance at the MAC layer. In this paper we consider two techniques to reduce the overhead incurred due to the aforementioned propagation of the CTS message: (a) the use of an intelligent broadcast scheme and (b) the reservation of bandwidth for the sequential transmission of multiple data packets with a single RTS/CTS exchange (and propagation as needed). These techniques require changes only at the MAC layer. We find, by means of extensive simulations, that the techniques provide a significant improvement over the original 802. 11 MAC protocol in the considered power heterogeneous ad hoc network. The overall throughput improves by as much as 12 % and the throughput of the low power nodes improves by up to 14 % as compared to the IEEE 802. 11 MAC protocol. Furthermore, the schemes find applicability even in homogeneous networks as they reduce the number of <b>false</b> <b>link</b> failures that arise when the IEEE 802. 11 MAC protocol is used, by about 20 %. We conclude that the schemes together offer a simple yet effective and viable means of performing medium access control in power heterogeneous ad hoc networks. I...|$|E
30|$|A topic {{has been}} {{represented}} with many sub-topics {{and as the}} granularity of an event {{is too small to}} describe a topic, and a less granular event has been used to describe only a sub-topic in [73]. The event indexing models has been used for characterizing a topic or story narrative with a plan of event executions as has been described in [74]. A similarity measure between news articles has been defined using weighted similarity of the persons and keywords describing the articles [75]. The topic splitting and merging research where an event causally influences another for establishing links between stories or topics or storylines have been described in [18, 76 – 80]. There can be multiple storylines related to higher level topics, people, location and time [71]. A storyline has been described [71] on a subset of the relevant topics, for instance a storyline has been described as to cover only the political or the economic aspect of Lehman Brother collapse. A cost has been associated with detection of <b>false</b> <b>link</b> between document pairs where the strength of a link has been described as representative of a measure of cohesion between these documents [76] and this cost or reward value can be utilized for applying reinforcement learning approach. A topic {{has been described as a}} seminal event or activity alongside occurrences of other secondary events and activities [81]. The concept of evolutionary discovery of theme which has been interpreted as a semantically coherent topic or subtopic transition appears within [82]. Many themes can be active at an interval of time, and a theme evolution graphs has been represented with arcs connecting a theme to another across the time intervals [82]. The theme evolution arcs can represent threads of themes describing lessons from the event, aids, concerts for the event, personal experience from the event (s), or donation match [60], utilized for describing the topic of Asian Tsunami. Topic specific words can be identified by removing words which appear exceeding a threshold measure defined based on ratio of frequency count of documents in which this word appears and frequency count of documents describing the topic of interest [83]. The above description summarizes related work in the hierarchical topic sub topic and threads of themes or sub-topics representation property.|$|E
50|$|Developers can attach {{data flow}} {{archetype}}s to each logical actor. A data flow archetype explains how the children types {{of an actor}} type arrange themselves in a data flow. With the help of this information, one can infer a link between each actor of a source type and a destination type. For example, in the MapReduce architecture, the map actor type is the source for reduce, and vice versa. The system infers this from the data flow archetypes and duly links map instances with reduce instances. However, there may be several MapReduce jobs in the data flow, and linking all map instances with all reduce instances can create <b>false</b> <b>links.</b> To prevent this, such links are restricted to actor instances contained within a common actor instance of a containing (or parent) actor type. Thus, map and reduce instances are only linked to each other if they {{belong to the same}} job.|$|R
30|$|The whole deanonymization {{process is}} highly {{sensitive}} to any imperfection of the heuristics. The potential {{effect of a}} heuristic error is to infer a wrong grouping from some transaction, {{it could lead to}} collapse Bitcoin addresses of different users onto a single entity, with the risk of creating users that seem to control a huge number of Bitcoin addresses. Being aware of this problem, we tried to use the safest heuristics possible, even at the expense of discarding some true linking between Bitcoin addresses. As some <b>false</b> <b>linking</b> could anyway occurs, the timespan we use for the deanonymization starts to play a key role; the longer the period of analysis, the bigger the probability that errors can cause the appearance of big clusters of Bitcoin addresses. Reducing the interval of the analysis might lead to the identification {{of a large number of}} small groups of addresses, in other terms the same user might still be split in several group of addresses.|$|R
40|$|As part of {{developing}} a record linkage algorithm using de-identified patient data, we analyzed the performance of several demographic variables for making linkages between patient registry records from two hospital registries and the Social Security Death Master File. We analyzed samples from each registry totaling 6, 000 record-pairs to establish a linkage gold-standard. Using Social Security Number as the exclusive linkage variable resulted in substantial linkage error rates of 4. 7 % and 9. 2 %. The best single variable combination for finding links was Social Security Number, phonetically compressed first name, birth month, and gender. This found 87 % and 88 % of the <b>links</b> without any <b>false</b> <b>links.</b> We achieved sensitivities of 90 % to 92 % while maintaining 100 % specificity using combinations of social security number, gender, name, and birth date fields. This represents an accurate method for linking patient records to death data and {{is the basis for}} a more generalized de-identified linkage algorithm...|$|R
