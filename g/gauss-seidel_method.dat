266|217|Public
25|$|NLLSQ {{is usually}} an {{iterative}} process. The iterative process {{has to be}} terminated when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods, although problems {{with large numbers of}} parameters are typically solved with iterative methods, such as the <b>Gauss–Seidel</b> <b>method.</b>|$|E
25|$|Much {{effort has}} been put in the {{development}} of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, <b>Gauss–Seidel</b> <b>method,</b> successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.|$|E
2500|$|The <b>Gauss-Seidel</b> <b>method</b> (...) {{applied to}} the problem (...) takes the form ...|$|E
5000|$|Successive over-relaxation can {{be applied}} to either of the Jacobi and <b>Gauss-Seidel</b> <b>methods</b> to speed convergence.|$|R
50|$|The Jacobi and <b>Gauss-Seidel</b> <b>methods</b> {{for solving}} a linear system {{converge}} if the matrix is strictly (or irreducibly) diagonally dominant.|$|R
3000|$|... (ω). In practice, such {{symmetric}} coercive optimization problems, the SOR {{methods are}} much more efficient than the Jacobi and <b>Gauss-Seidel</b> <b>methods.</b> We will exploit L_SOR [...]...|$|R
2500|$|The <b>Gauss-Seidel</b> <b>method</b> can be {{represented}} in matrix form as a splitting ...|$|E
2500|$|A rough {{solution}} [...] {{can often}} be improved by an interval version of the <b>Gauss–Seidel</b> <b>method.</b>|$|E
2500|$|The {{first few}} iterates for {{equation}} (...) {{are listed in}} the table below, beginning with [...] [...] From the table one can see that the method is evidently converging to the solution (...) , slightly faster than the <b>Gauss-Seidel</b> <b>method</b> described above.|$|E
40|$|Abstract—We {{study the}} semiconvergence of <b>Gauss-Seidel</b> {{iterative}} <b>methods</b> {{for the least}} squares solution of minimal norm of rank deficient linear systems of equations. Necessary and sufficient conditions for the semiconvergence of the <b>Gauss-Seidel</b> iterative <b>method</b> are given. We also show that if the linear system of equations is consistent, then the proposed methods with a zero vector as an initial guess converge in one iteration. Some numerical results are given to illustrate the theoretical results. Keywords—rank deficient least squares problems, AOR iterative <b>method,</b> <b>Gauss-Seidel</b> iterative <b>method,</b> semiconvergence. I...|$|R
40|$|Traditional {{power flow}} {{analysis}} {{techniques such as}} the Newton-Raphson and the <b>Gauss-Seidel</b> <b>methods</b> are widely used in analyzing power transmission systems. However, they are inefficient and may diverge due to the different network characteristics of power distribution systems such as radial and high R/X ratio. Therefore, other techniques such as the voltage sweep algorithms are developed for power distribution systems. In this thesis, the forward and backward sweep algorithm is studied and validated in Java. The sweep algorithm can deal with balanced net-works which are radial or weakly meshed and contain distributed gener-ators. Networks with different topologies are implemented to assess the convergence behavior of the sweeping algorithm and comparisons with es-tablished methods in the open-source Java power flow package JPower (the Newton-Raphson and <b>Gauss-Seidel</b> <b>methods)</b> are made. Results show that the sweeping algorithm is more efficient in analyzin...|$|R
40|$|Instead of {{the usual}} basis, we use a {{generating}} system for the discretization of PDEs that contains not only the basis functions of the finest level of discretization but additionally the basis functions of all coarser levels of discretization. The Galerkin-approach now results in a semidefinite system of linear equations to be solved. Standard iterative GS-methods for this system {{turn out to be}} equivalent to elaborated multigrid methods for the fine grid system. Beside <b>Gauss-Seidel</b> <b>methods</b> for the level-wise ordered semidefinite system, we study block <b>Gauss-Seidel</b> <b>methods</b> for the point-wise ordered semidefinite system. These new algorithms show basically the same properties as conventional multigrid methods with respect to their convergence behavior and efficiency. Additionally, they possess interesting properties with respect to parallelization. Regarding communication, the number of setup steps is only dependent on the number of processors and not on the number of level [...] ...|$|R
5000|$|The <b>Gauss-Seidel</b> <b>method</b> (...) {{applied to}} the problem (...) takes the form ...|$|E
5000|$|The <b>Gauss-Seidel</b> <b>method</b> is an {{improvement}} upon the Jacobi method.|$|E
50|$|The <b>Gauss-Seidel</b> <b>method</b> {{sometimes}} converges even {{if these}} conditions are not satisfied.|$|E
30|$|In the multigrid method, the {{relaxation}} operator {{is an important}} operator. Its work is not to remove the errors, but to damp the high-frequency components of the errors on the present grid level. A simple smoother (<b>Gauss-Seidel</b> relaxation) <b>method</b> can efficiently remove the errors in all directions for simple isotropic problems [7, 20], but in case of anisotropic and boundary layer problems, the line Gauss-Seidel [19, 21] and alternating line <b>Gauss-Seidel</b> <b>methods</b> [1, 25 – 27] are shown to be more robust smoothers. In this paper, we use three relaxations to smooth the residuals on each coarse grid such as the line Gauss-Seidel relaxation, natural Gauss-Seidel relaxation, and Red-black Gauss-Seidel relaxation.|$|R
40|$|Abstract The paper {{describes}} {{the development and}} features of an MS-Excel Workbook (available at www. reseeds. com), which illustrates four methods of power system load flow analysis. Iterative techniques are represented by the Newton-Raphson and <b>Gauss-Seidel</b> <b>methods.</b> The Workbook also includes two search algorithms: genetic algorithms and simulated annealing. Keywords load flow; numerical methods; power system Load flow studies are used to ensure that electrical power transfer from generators to consumers through the grid system is stable, reliable and economic. 1, 2 Conven-tional techniques for solving the load flow problem are iterative, using the Newton-Raphson or the <b>Gauss-Seidel</b> <b>methods.</b> Recently, however, {{there has been much}} interest in the application of stochastic search methods, such as genetic algorithms, to solving power system problems. 3 – 5 The increasing presence of distributed alter-native energy sources, often in geographically remote locations, complicates load flow studies and has triggered a resurgence of interest in the topic. The principles of power system load flow studies are taught within elective modules in the later years of undergraduate electrical engineering courses, or a...|$|R
40|$|Abstract. The Q-learning is one {{of typical}} {{reinforcement}} learning methods. Since the Q-learning requires huge amounts of time to solve a problem, this study proposes acceleration methods. This study introduces two approaches based on the value iteration method of the dynamic programming to accelerate the learning. One {{is the use of}} proper estimation of the state transition probability model and the other is the application of iterative solving methods for an inverse matrix, e. g., Jacobi’s <b>method,</b> <b>Gauss-Seidel’s</b> <b>method,</b> SOR method, etc. Those allow us to determine the optimal learning factor and to make the learning more efficient than the Q-learning. Numerical simulations show that the proposed methods are effective...|$|R
5000|$|The <b>Gauss-Seidel</b> <b>method</b> can be {{represented}} in matrix form as a splitting ...|$|E
50|$|The <b>Gauss-Seidel</b> <b>method</b> is {{a useful}} {{numerical}} iterative method for solving linear systems.|$|E
50|$|The element-wise {{formula for}} the <b>Gauss-Seidel</b> <b>method</b> is {{extremely}} {{similar to that}} of the Jacobi method.|$|E
40|$|Abstract We {{present a}} library of PVS meta-theories {{that can be used}} to verify a class of {{distributed}} systems in which agent communication is via message-passing. The theoreti-cal work, as outlined in Chandy et al. (Form Aspect Comput 2011, to appear) consists of iterative schemes for solving systems of linear equations, such as message-passing exten-sions of the Gauss and <b>Gauss-Seidel</b> <b>methods.</b> We briefly review that work and discuss the challenges in formally verifying it...|$|R
40|$|The {{main goal}} {{of this paper is}} to generalize Jacobi and <b>Gauss-Seidel</b> <b>methods</b> for solving non-square linear system. Towards this goal, we present {{iterative}} procedures to obtain an approximate solution for non-square linear system. We derive sufficient conditions for the convergence of such iterative methods. Procedure is given to show that how an exact solution can be obtained from these methods. Lastly, an example is considered to compare these methods with other available method(s) for the same. Comment: 5 page...|$|R
40|$|AbstractIn 2002, H. Kotakemori et al. {{proposed}} the modified <b>Gauss–Seidel</b> (MGS) <b>method</b> for solving the linear {{system with the}} preconditioner P=I+Smax [H. Kotakemori, K. Harada, M. Morimoto, H. Niki, A comparison theorem for the iterative method with the preconditioner (I+Smax) J. Comput. Appl. Math. 145 (2002) 373 – 378]. Since this preconditioner is constructed by only the largest element on each row of the upper triangular part of the coefficient matrix, the preconditioning effect is not observed on the nth row. In the present paper, {{to deal with this}} drawback, we propose two new preconditioners. The convergence and comparison theorems of the modified <b>Gauss–Seidel</b> <b>methods</b> with these two preconditioners for solving the linear system are established. The convergence rates of the new proposed preconditioned methods are compared. In addition, numerical experiments are used to show the effectiveness of the new MGS methods...|$|R
5000|$|Smoothing - {{reducing}} {{high frequency}} errors, for example using a few iterations of the <b>Gauss-Seidel</b> <b>method.</b>|$|E
5000|$|The <b>Gauss-Seidel</b> <b>method</b> is an {{iterative}} {{technique for}} solving a square system of n linear equations with unknown x: ...|$|E
5000|$|To {{solve this}} {{non-linear}} system of algebraic equations, traditionalload-flow algorithms were developed based on three iterativetechniques: the <b>Gauss-Seidel</b> <b>method</b> ...|$|E
40|$|It is {{well known}} that as a famous type of {{iterative}} methods in numerical linear algebra, <b>Gauss-Seidel</b> iterative <b>methods</b> are convergent for linear systems with strictly or irreducibly diagonally dominant matrices, invertible $H-$matrices (generalized strictly diagonally dominant matrices) and Hermitian positive definite matrices. But, the same is not necessarily true for linear systems with nonstrictly diagonally dominant matrices and general $H-$matrices. This paper firstly proposes some necessary and sufficient conditions for convergence on <b>Gauss-Seidel</b> iterative <b>methods</b> to establish several new theoretical results on linear systems with nonstrictly diagonally dominant matrices and general $H-$matrices. Then, the convergence results on preconditioned <b>Gauss-Seidel</b> (PGS) iterative <b>methods</b> for general $H-$matrices are presented. Finally, some numerical examples are given to demonstrate the results obtained in this paper...|$|R
40|$|The {{paper is}} devoted on methods and {{algorithms}} for steady-state analysis of Markov chains. Basic, direct and iterative methods for steady-state analysis of Markov chains are concerned, where Gaussian Elimination method and Grassman method, {{as well as}} Power, Jacobis and <b>Gauss-Seidels</b> <b>methods</b> are implemented. Algorithms for computation of steady-state probability vector for finite Markov chains are developed. Comparison of numerical solutions to exact equilibrium solution for local-balance equation of Discrete-Time Markov Chain is given. Example and numerical results for feedback networks of Markovian queues are shown...|$|R
40|$|AbstractIn multigrid methods, it is {{preferred}} to employ smoothing techniques which are convergent. In practice, the standard Jacobi and <b>Gauss-Seidel</b> <b>methods</b> are the choices for “smoothers” However, {{it is known}} that if the spectral radius condition is violated, then convergence is not guaranteed for these methods. In this paper we develop a simple two-step Jacobi-type method which has better convergence properties and which can be employed as a convergent “smoother” wherever the standard iterative methods fail. We provide the convergence proofs and demonstrate the applicability of the method on a variety of problems...|$|R
5000|$|The {{convergence}} {{properties of}} the <b>Gauss-Seidel</b> <b>method</b> are dependent on the matrix A. Namely, the procedure is known to converge if either: ...|$|E
50|$|When {{approximating}} {{the constraints}} locally to first order {{this is the}} same as the <b>Gauss-Seidel</b> <b>method.</b> For small matrices it is known that LU decomposition is faster. Large systems can be divided into clusters (for example: each ragdoll = cluster). Inside clusters the LU method is used, between clusters the <b>Gauss-Seidel</b> <b>method</b> is used. The matrix code can be reused: The dependency of the forces on the positions can be approximated locally to first order, and the Verlet integration can be made more implicit.|$|E
5000|$|In SHAKE algorithm, {{the system}} of {{non-linear}} constraint equations is solved using the <b>Gauss-Seidel</b> <b>method</b> which approximates {{the solution of the}} linear system of equations using the Newton-Raphson method; ...|$|E
40|$|The {{objective}} {{of this paper is}} to analyse the application of the Half-Sweep <b>Gauss-Seidel</b> (HSGS) <b>method</b> by using the Half-sweep approximation equation based on central difference (CD) and repeated trapezoidal (RT) formulas to solve linear fredholm integro-differential equations of first order. The formulation and implementation of the Full-Sweep Gauss-Seidel (FSGS) and Half- Sweep <b>Gauss-Seidel</b> (HSGS) <b>methods</b> are also presented. The HSGS method has been shown to rapid compared to the FSGS methods. Some numerical tests were illustrated to show that the HSGS method is superior to the FSGS method...|$|R
40|$|We {{describe}} an iterative fixed point approach {{for the following}} stochastic optimization problem: given a multicast tree and probability distributions of user utilities, find an optimal posted price mechanism- i. e. compute prices to offer the users {{in order to maximize}} the expected profit of the service provider. We show that any optimum pricing is a fixed point of an efficiently computable function. We can then apply the non-linear Jacobi and <b>Gauss-Seidel</b> <b>methods</b> of coordinate descent. We provide proof of convergence to the optimum prices for special cases of utility distributions and tree edge costs. ...|$|R
40|$|A {{solution}} of radial slide bearing dynamics and tribology incorporating {{the influences of}} real surface roughness contacts or the influence of surface roughness on bearing lubrication is presented in this paper. Finite difference method for Reynolds equation discretization, finite element method for calculation of elastic deformations, <b>Gauss-Seidel’s</b> <b>method</b> for iterative {{solution of}} discretized equations or Newmark’s algorithm are the methods employed in the proposed solution approach. The coupled structural-fluid solver considering mixed lubrication conditions of the radial bearings is the result. The proposed algorithms are presented for highly loaded radial slide bearing of internal combustion engine...|$|R
