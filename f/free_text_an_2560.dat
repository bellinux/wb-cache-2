0|10000|Public
5000|$|The Struggle for Bread, 1913 (<b>free</b> <b>text</b> version); <b>a</b> reply {{under the}} {{pseudonym}} [...] "Rifleman" [...] to Norman Angell's The Great Illusion (1910).|$|R
40|$|Primary care {{databases}} are a {{major source}} of data for epidemiological and health services research. However, most studies are based on coded information, ignoring information stored in <b>free</b> <b>text.</b> Using the early presentation of rheumatoid arthritis (RA) as an exemplar, our objective was to estimate the extent of data hidden within <b>free</b> <b>text,</b> using <b>a</b> keyword search...|$|R
40|$|We {{developed}} a web-based framework, which enables serious game designers {{to create the}} Game Design Document (GDD) of a game. By combining predefined menus along with non-predefined elements, this framework aims at facilitating designers to design the GDD faster, with more structure {{and with all the}} necessary element a GDD should have. The contributions of this framework with respect to regular GDDs is that it offers a more agile method for developing the document, due to its semi-structured environment, which offers more flexibility than a structured environment and more rigour than <b>free</b> <b>text.</b> Additionally, it’s <b>a</b> model-driven environment, which means that its development is based on a conceptual model of educational serious games. Finally, it offers innovative features, like hyperlinks to connect objects from the database with <b>free</b> <b>text,</b> <b>a</b> progress page where the designer can keep track of each aspect of the document and structured menus that connect objects from the database between them...|$|R
40|$|In this paper, {{we present}} an {{iterative}} algorithm for Word Sense Disambiguation. It combines two sources of information: Word_Net and a semantic tagged corpus, {{for the purpose}} of identifying the correct sense of the words in <b>a</b> given <b>text.</b> It differs from other standard approaches in that the disambiguation process is performed in an iterative manner: starting from <b>free</b> <b>text,</b> <b>a</b> set of disambiguated words is built, using various methods; new words are sense tagged based on their relation to the already disambiguated words, and then added to the set. This iterative process allows us to identify, in the original <b>text,</b> <b>a</b> set of words which can be disambiguated with high precision; 55 % of the verbs and nouns are disambiguated with an accuracy of 92 %...|$|R
50|$|Machine {{extraction}} {{creates a}} triple {{consisting of a}} subject, predicate or relation, and object. Each attribute-value pair of the infobox is used to create an RDF statement using an ontology. This is facilated by the narrower gap between Wikipedia and an ontology than exists between unstructured or <b>free</b> <b>text</b> and <b>an</b> ontology.|$|R
40|$|CWM Global Search is a meta-search engine {{allowing}} chemists and biologists {{to search}} the major chemical and biological databases on the Internet, by structure, synonyms, CAS Registry Numbers and <b>free</b> <b>text.</b> <b>A</b> meta-search engine is a search tool that sends user requests to several other search engines and/or databases and aggregates the results into a single list or displays them according to their source [1]. CWM Global Search is a web application that has many {{of the characteristics of}} desktop applications (also known as Rich Internet Application, RIA), and it runs on both Windows and Macintosh platforms. The application {{is one of the first}} RIA for scientists. The application can be started using the URL [URL]...|$|R
40|$|Effective, {{computer-based}} {{representation of}} clinical observations requires balancing {{the advantages of}} structured, coded descriptions against those of <b>free</b> <b>text</b> narrative. An essential data set of relevant signs and symptoms was defined by a multidisciplinary group based on management goals published in a national guideline {{to meet the needs}} of clinicians in the Spina Bifida Clinic at Yale-New Haven Hospital. These core data elements are stored in a structured format. Additional material is stored as <b>free</b> <b>text.</b> <b>A</b> relational schema was devised that permits storage of both coded findings and narrative. Symptoms and signs are represented as subtypes of a supertype patient finding entity; they inherit common attributes and specialize others. The IVORY vocabulary was supplemented and modified to provide terms that describe relevant clinical observations. For this application, fields were added that enable predictive data entry of findings based on patient age and gender...|$|R
40|$|Assessing student {{understanding}} by evaluating their <b>free</b> <b>text</b> {{answers to}} posed questions {{is a very}} important task. However, manually, it is time-consuming and computationally, it is difficult. This paper details our shallow NLP approach to computationally assessing student <b>free</b> <b>text</b> answers when <b>a</b> reference answer is provided. For four out of the five test sets, our system achieved an overall accuracy above the median and mean. ...|$|R
50|$|In {{recent years}} <b>free</b> <b>text</b> search as <b>a</b> {{means of access}} to {{documents}} has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is indexed). Many {{studies have been done}} to compare the efficiency and effectiveness of <b>free</b> <b>text</b> searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.|$|R
40|$|The <b>free</b> <b>text</b> notes typed by {{physicians}} during patient consultations contain valuable {{information for the}} study of disease and treatment. These notes are difficult to process by existing natural language analysis tools since they are highly telegraphic (omitting many words), and contain many spelling mistakes, inconsistencies in punctuation, and non-standard word order. To support information extraction and classification tasks over such <b>text,</b> we describe <b>a</b> de-identified corpus of <b>free</b> <b>text</b> notes, <b>a</b> shallow syntactic and named entity annotation scheme for this kind of <b>text,</b> and <b>an</b> approach to training domain specialists with no linguistic background to annotate the text. Finally, we present a statistical chunking system for such clinical <b>text</b> with <b>a</b> stable learning rate and good accuracy, indicating that the manual annotation is consistent and that the annotation scheme is tractable for machine learning...|$|R
40|$|Text {{understanding}} {{systems are}} approaching {{the point of}} being a practical technology as long as the system is trained for a narrowly defined domain. Machine learning and statistical approaches can minimize the effort involved in adapting <b>a</b> <b>text</b> understanding system to a new domain. This paper presents a system whose goal is deep understanding, limited only by the necessity of designing a formal representation of the target concepts relevant to the domain. This system is an advance over previous machine learning based systems because of its richer output representation, and an advance over equally expressive text understanding systems because of its more extensive use of machine learning. 1 Information Extraction from <b>Free</b> <b>Text</b> <b>A</b> variety of systems have been developed in recent years that extract information from text. None of them attempts general-purpose understanding, but instead focus on narrowly defined information needs. A domain is defined as a collection of docum [...] ...|$|R
40|$|Abstract. This paper proposes {{the design}} and {{implementation}} of <b>a</b> context-based <b>free</b> <b>text</b> interpreter (CFTI), <b>a</b> computer-based natural language understanding (NLU) system that can handle text as generated and used by humans, within a given context. It takes advantage of tracking the contextual meaning of words and phrases during (and after) {{the development of an}} ontology for that context, and subsequently uses this information as a knowledge base for interpretation of <b>free</b> <b>text</b> sentences. The system incorporates components of a computer NLU system based on studies of the human understanding processes. Two existing language tools, Link Grammar and WordNet, are examined and incorporated into the system. The CFTI system is designed and implemented through the use of an expert system shell, CLIPS 6. 20, to demonstrate the capability of interpretation and representation of the meaning of <b>free</b> <b>text</b> sentences when <b>a</b> context model is provided. The resultant CFTI system successfully demonstrates the capability to interpret and represent the meaning of <b>free</b> <b>text</b> sentences based on a relatively small-sized context model...|$|R
40|$|Motivation: Figures {{and tables}} in {{biomedical}} literature record {{vast amounts of}} important experiment results. In scientific papers, for example, quantitative trait locus (QTL) information is usually pre-sented in tables. However, most of the popular text-mining methods focus on extracting knowledge from unstructured <b>free</b> <b>text.</b> <b>As</b> far as we know, there are no published works on mining tables in biomedi-cal literature. In this paper, we propose a method to extract QTL information from tables and plain text found in literature. Heteroge-neous and complex tables were converted into a structured data-base, combined with information extracted from plain text. Our method could greatly reduce labor burdens involved with database curation. Results: We applied our method on a soybean QTL database cura-tion, from which 2278 records were extracted from 228 papers with a precision rate of 96. 9 % and a recall rate of 83. 3 %.,F value for the method is 89. 6 %. Availability: QTLMiner is available at www. soyomics. com/qtlminer/. Contact...|$|R
40|$|The named-entity phrases in <b>free</b> <b>text</b> {{represent}} <b>a</b> formidable {{challenge to}} <b>text</b> analysis. Translat-ing <b>a</b> named-entity {{is important for}} the task of Cross Language Information Retrieval and Ques-tion Answering. However, both tasks are not easy to handle because named-entities found in <b>free</b> <b>text</b> are often not listed in a monolingual or bilingual dictionary. Although it is possible to iden-tify and translate named-entities on the fly without a list of proper names and transliterations, an extensive list certainly will ensure the high accuracy rate of text analysis. We use a list of proper names and transliterations to train a Machine Transliteration Model. With the model it is possi-ble to extract proper names and their transliterations in a bilingual corpus with high average pre-cision and recall rates. 1...|$|R
40|$|Background Primary care {{databases}} are a {{major source}} of data for epidemiological and health services research. However, most studies are based on coded information, ignoring information stored in <b>free</b> <b>text.</b> Using the early presentation of rheumatoid arthritis (RA) as an exemplar, our objective was to estimate the extent of data hidden within <b>free</b> <b>text,</b> using <b>a</b> keyword search. Methods We examined the electronic health records (EHRs) of 6, 387 patients from the UK, aged 30 years and older, with a first coded diagnosis of RA between 2005 and 2008. We listed indicators for RA which were present in coded format and ran keyword searches for similar information held in <b>free</b> <b>text.</b> The frequency of indicator code groups and keywords from one year before to 14 days after RA diagnosis were compared, and temporal relationships examined. Results One or more keyword for RA was found in the <b>free</b> <b>text</b> in 29...|$|R
40|$|We {{describe}} a librarian knowledge-based system that generates a search strategy from a query representation {{based on a}} user's information need. Together with the natural language parser AQUA, the system functions as a human/computer interface, which translates a user query from <b>free</b> <b>text</b> into <b>a</b> BRS Onsite search formulation, for searching the MEDLINE bibliographic database. In the system, conceptual graphs are used to represent the user's information need. The UMLS Metathesaurus and Semantic Net are used as the key knowledge sources in building the knowledge base...|$|R
40|$|Abstract. The {{detection}} of event mentions in <b>free</b> <b>text</b> is <b>a</b> {{key to a}} deeper automatic understanding of the text’s contents. In this paper we present ongoing work on mechanisms to detect events in German texts {{in the domain of}} cultural heritage documentation. A central role plays a hand-crafted mapping of CIDOC CRM 1 events to GermaNet synsets to ease the process of creating a lexicon for automatic event detection. We discuss two approaches and insights gained from the mapping process and correct modelling of event mentions. ...|$|R
40|$|A language-oriented, multi-dimensional {{database}} of the linguistic {{characteristics of the}} Hebrew text of the Old Testament can enable researchers to do ad hoc queries. XML is a suitable technology to transform <b>free</b> <b>text</b> into <b>a</b> database. A clause’s word order can be kept intact while other features such as syntactic and semantic functions can be marked as elements or attributes. The elements or attributes from the XML “database” can be accessed and proces sed by a 4 th generation programming language, such as Visual Basic. XML is explored as an option to build an exploitable {{database of}} linguistic data by representing inherently multi-dimensional data, including syntactic and semantic analyses of <b>free</b> <b>text...</b>|$|R
40|$|Electronic {{patient data}} are {{associated}} with many potential benefits, e. g. data sharing, quality assessment, research, and management of patient care. The degree to which patient data are currently available electronically varies. To harvest {{the potential benefits of}} electronic data, the data must also be available in a structured format to enable processing by computer applications. Narrative data are typically recorded as <b>free</b> <b>text.</b> <b>As</b> a result, researchers still have to perform the labor-intensive task of reading and interpreting <b>free</b> <b>text</b> in individual electronic medical records. Structuring the medical narrative poses a significant challenge: content and level of detail are often unpredictable and vary per domain (and even per clinician). In an attempt to support structured recording of medical narratives we have developed OpenSDE (SDE: structured data entry). OpenSDE is intended for use in both care and research. Therefore, OpenSDE is designed to accommodate the structured recording of data in settings where content and order of data entry can often not be predicted. The aim of this research project is to investigate the feasibility of using data recorded with OpenSDE, for research purposes. Consistency and accuracy of collected data are pivotal for research, and are especially challenging if data will be collected {{over long periods of time}} and by different users. This Ph. D. project, therefore, focuses on pitfalls for data extraction for research purposes, and aims to formulate strategies to improve uniformity in data entry to enhance the reliability of data retrieval. In this research project we studied: •	The possibility of extracting data recorded with OpenSDE and representing the extracted data in a manner suitable for research purposes. •	The uniformity of recorded data when OpenSDE is used to transcribe data from the same source. •	The origin of differences in representation of semantically identical information. •	Strategies that can improve uniformity in data entry...|$|R
40|$|Abstract—Emails {{and issue}} reports capture useful {{knowledge}} about development practices, bug fixing, and change activities. Extracting such a content is challenging, {{due to the}} mix-up of source code and natural language, unstructured text. In this paper we introduce an approach, based on Hidden Markov Models (HMMs), to extract coded information islands, such as source code, stack traces, and patches, from <b>free</b> <b>text</b> at <b>a</b> token level of granularity. We train a HMM for each category of information contained in the text, and adopt the Viterbi algorithm to recognize whether the sequence of tokens—e. g. ...|$|R
50|$|In {{a typical}} {{document}} classification task, the input {{to the machine}} learning algorithm (both during learning and classification) is <b>free</b> <b>text.</b> From this, <b>a</b> bag of words (BOW) representation is constructed: the individual tokens are extracted and counted, and each distinct token in the training set defines a feature (independent variable) {{of each of the}} documents in both the training and test sets.|$|R
40|$|The {{electronic}} storage of medical patient data {{is becoming a}} daily experience {{in most of the}} practices and hospitals worldwide. However, much of the available data is in <b>free</b> <b>text</b> form, <b>a</b> convenient way of expressing concepts and events but especially challenging if one wants to perform automatic searches, summarization or statistical analyses. Information Extraction can relieve some of these problems by offering a semantically informed interpretation and abstraction of the texts. MedInX, the Medical Information eXtraction system developed {{in the context of this}} PhD dissertation is designed to process textual clinical discharge records written in Portuguese and to perform automatic and accurate mapping of <b>free</b> <b>text</b> reports onto <b>a</b> structured representation. MedInX components are based on Natural Language Processing principles, and provide several mechanisms to read, process and utilize external resources, such as terminologies and ontologies. MedInX current practical applications include automatic code assignment and an audit system capable of systematically analyze the content and completeness of the clinical reports. The evaluation of the system on a set of authentic patient discharge letters indicate that the system performs with 95 % precision and recall. ...|$|R
40|$|Abstract. Clinicians have to {{deal with}} large amounts of textual data, searching for and {{navigating}} information that satisfies their informational needs. Clinical notes and Clinical Practice Guidelines (CPG) are textual resources that usually contain <b>free</b> <b>text.</b> <b>As</b> language use in the medical domain is rather specialized, generic information retrieval tools are suboptimal for such data. This calls for specialized information retrieval systems that incorporate domain-specific knowledge. Furthermore, information resources for relatively small languages are limited, so physicians speaking those languages often have to resort to in-formation sources in English or one of the other major languages spoken. In our setting, given a patient record written in Norwegian, a physician is looking for recommendations in CPGs written in English. The recommendations are to be retrieved and ranked according to their relevance to (specified parts of) the pa-tient’s medical history and possibly a user-supplied query. This means that the query language and the resource language differ, requiring some form of Cross-Lingual Information Retrieval. Most approaches rely on translating the inpu...|$|R
40|$|This study {{evaluated}} the domain completeness and expressiveness {{issues of the}} International Classification for Nursing Practice-based (ICNP) nursing data dictionary (NDD) through its application in an enterprise electronic medical record (EMR) system as a standard vocabulary at a single tertiary hospital in Korea. Data from 2, 262 inpatients obtained {{over a period of}} 9 weeks (May to July 2003) were extracted from the EMR system for analysis. Among the 530, 218 data-input events, 401, 190 (75. 7 %) were entered from the NDD, 20, 550 (3. 9 %) used only <b>free</b> <b>text,</b> and 108, 478 (20. 4 %) used a combination of coded data and <b>free</b> <b>text.</b> <b>A</b> content analysis of the free-text events showed that 80. 3 % of the expressions could be found in the NDD, whereas 10. 9 % were context-specific expressions such as direct quotations of patient complaints and responses, and references to the care plan or orders of physicians. A total of 7. 8 % of the expressions was used for a supplementary purpose such as adding a conjunction or end verb to make an expression appear as natural language. Only 1. 0 % of the expressions were identified as not being covered by the NDD. This evaluation study demonstrates that the ICNP-based NDD has sufficient power to cover most of the expressions used in a clinical nursing setting...|$|R
40|$|There is {{a wealth}} of {{information}} to be mined from narrative text on the World Wide Web. Unfortunately, standard natural language processing (NLP) extraction techniques expect full, grammatical sentences, and perform poorly on the choppy sentence fragments that are often found on web pages. This paper 1 introduces Webfoot, a preprocessor that parses web pages into logically coherent segments based on page layout cues. Output from Webfoot is then passed on to CRYSTAL, an NLP system that learns text extraction rules from example. Webfoot and CRYSTAL transform the <b>text</b> into <b>a</b> formal representation that is equivalent to relational database entries. This is a necessary first step for knowledge discovery and other automated analysis of <b>free</b> <b>text.</b> Information Extraction from the Web The World Wide Web contains <b>a</b> wealth of <b>text</b> information in the form of <b>free</b> <b>text.</b> Until <b>a</b> <b>text</b> extraction system transforms it into an unambiguous format, much of this information remains inaccessible to autom [...] ...|$|R
40|$|The aim of {{this study}} was to {{undertake}} a systematic analysis, using qualitative software, of the <b>free</b> <b>text</b> comments from <b>a</b> postal survey exploring women’s experiences of breast symptoms, their expectations of treatment, knowledge of breast cancer risk factors and perceptions of risk, in 34 group general practices in South Wales. The 959 women who returned the questionnaire, out of 1126 (response rate 85...|$|R
40|$|Background Primary care {{databases}} {{provide a}} unique resource for healthcare research, but most researchers currently use only the Read codes for their studies, ignoring {{information in the}} <b>free</b> <b>text,</b> which is much harder to access. Objectives To investigate how much information on ovarian cancer diagnosis is ‘hidden’ in the <b>free</b> <b>text</b> and the time lag between a diagnosis being described in the <b>text</b> or in <b>a</b> hospital letter and the patient being given a Read code for that diagnosis. Design Anonymised <b>free</b> <b>text</b> records from the General Practice Research Database of 344 women with a Read code indicating ovarian cancer between 01 June 2002 and 31 May 2008 were {{used to compare the}} date at which the diagnosis was first coded with the date at which the diagnosis was recorded in the <b>free</b> <b>text.</b> <b>Free</b> <b>text</b> relating to <b>a</b> diagnosis was identified (a) from the date of coded diagnosis and (b) by searching for words relating to the ovary. Results 90...|$|R
40|$|Software system {{documentation}} {{is almost always}} expressed informally, in natural language and <b>free</b> <b>text.</b> Examples include requirement specifications, design documents, manual pages, system development journals, error logs and related maintenance reports. We propose an approach {{to establish and maintain}} traceability links between source code and <b>free</b> <b>text</b> documents. <b>A</b> premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high level concepts with program concepts, and vice-versa. In this paper, the approach is applied to software written in an object-oriented language, namely Java, to trace classes to functional requirements...|$|R
40|$|Medical reports are, quite often, {{written and}} stored in {{computer}} systems in <b>a</b> non-structured <b>free</b> <b>text</b> form. <b>As</b> a consequence, {{the information contained in}} these reports is not easily available and {{it is not possible to}} take it into account by medical decision support systems. We propose a methodology to automatically process and analyze medical reports, identifying concepts and their instances, and populating a new ontology. This methodology is based in natural language processing techniques using linguistic and statistical information. The proposed system was applied successfully to a set of medical reports from the Veterinary Hospital of the University of Evora...|$|R
40|$|After Leukaemia, {{childhood}} brain tumours are {{the second}} most common form of cancer. The most accurate way to diagnosis a tumour is via a biopsy, but is not always possible. An alternative is Magnetic Resonance Spectroscopy (MRS) which analyses the chemical make-up of tissue and then used to make a diagnosis. A patient's medical records {{are an important part of}} treatment, used to communicate findings from medical images. However, these are written using unclear and ambiguous <b>free</b> <b>text.</b> <b>A</b> solution is to produce records using Structured Reporting. This has been incorporated into the Digital Communications in Medicine (DICOM) Standard for established imaging modalities, but not for MRS. An ontology was modelled to produce DICOM supported Structured Reports for MRS. Also, an algorithm to diagnosis different types of childhood brain cancer using MRS spectra was incorporated, allowing automated diagnostic support. A prototype Structured Reporting application was designed based on the ontology. The ontology was able to produce Structured Reports that successfully diagnosed certain childhood brain tumours based on the MRS readings. Usability testing and the diagnostic aspect of the ontology garnered positive feedback as MRS data is only currently used to diagnosis whether the tissue is cancerous or not...|$|R
40|$|Evaluation of {{the content}} of <b>free</b> <b>texts</b> is <b>a</b> {{challenging}} task for humans. Automation of this process is largely useful in order to reduce human related errors. We consider one instance of the “free texts ” assessment problems; automatic essay grading where the task is to grade student written essays automatically given course materials and a set of human-graded essays as training data. We use a Latent Semantic Analysis (LSA) -based methodology to accomplish this task. We experiment on a dataset obtained from an occupational therapy course and report the results. We also discuss our findings, analyze different problem areas and explain the potential solutions...|$|R
40|$|Language phrases <b>Free</b> <b>text</b> {{constitutes}} <b>a</b> overwhelming {{fraction of}} information {{available on the}} World Wide Web. Specifically, consider small chunks of natural language phrases frequently used by Web users to describe stuff relevant to them. For example, consider the following two posts on a classifieds site (which serves a small locality, say, a university campus) -” 2 Tickets for the prom tonight ” and ”Trade 2 extra passes for tonight’s Ball for $ 25 ”. For a human looking at these two posts, its trivial to conclude that he has found what he wanted. But when {{there are thousands of}} such posts and in the absence o...|$|R
30|$|To {{study the}} {{usefulness}} and qualitative {{characteristics of the}} output of social design feedback we explored such output in two ICT design cases (RQ 1). In both cases, the participants contributed design feedback as <b>free</b> <b>text</b> comments in <b>an</b> online environment. All comments were displayed in discussion threads adjacent to the visual representation of the design. The environment did not support annotations in the visual representation thereby avoiding problems with visual clutter.|$|R
40|$|We {{report on}} the results from an {{empirical}} study deal-ing with the semantic interpretation of prepositional phrases in medical <b>free</b> <b>texts.</b> We use <b>a</b> small number of semantic interpretation schemata only, which operate on well-defined configurations in dependency graphs. We provide a quantitative analysis {{of the performance of}} the semantic interpreter in terms of recall/precision data, and consider, in qualitative terms, the impact semantic interpretation patterns have on the construction of the underlying medical ontology...|$|R
40|$|This study {{presents}} the Chinese Open Relation Extraction (CORE) {{system that is}} able to extract entity-relation triples from Chinese <b>free</b> <b>texts</b> based on <b>a</b> series of NLP techniques, i. e., word segmentation, POS tagging, syntactic parsing, and extraction rules. We employ the proposed CORE techniques to extract more than 13 million entity-relations for an open domain question answering application. To our best knowledge, CORE is the first Chinese Open IE system for knowledge acquisition. ...|$|R
40|$|The {{generation}} of bimodal formal system specification documents that bi-directionally complement graphics from natural language {{and vice versa}} through ObjectProcess Methodology (OPM) is presented. <b>A</b> sample of <b>free</b> <b>text</b> paragraphs from <b>a</b> document that describes the concept of Free Flight {{as part of the}} National Airspace System are converted to OPM specification. We then discuss the requirements of porting the OPM application to a Web environment, the challenges and the expected benefits...|$|R
