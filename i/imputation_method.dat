492|622|Public
50|$|In statistics, {{data sets}} usually come from actual {{observations}} obtained by sampling a statistical population, and each row {{corresponds to the}} observations on one element of that population. Data sets may further be generated by algorithms {{for the purpose of}} testing certain kinds of software. Some modern statistical analysis software such as SPSS still present their data in the classical data set fashion. If data is missing or suspicious an <b>imputation</b> <b>method</b> may be used to complete a data set.|$|E
5000|$|The {{state of}} Utah then filed another lawsuit {{alleging that the}} {{statistical}} methods used in computing the state populations were improper and cost Utah the seat. The Bureau uses a method called imputation to assign a number of residents to addresses where residents cannot be reached after multiple efforts. While nationwide the <b>imputation</b> <b>method</b> added [...]4% to the population, the rate in Utah was [...]2%. The state challenged {{that the use of}} imputation violates the Census Act of 1957 and that it also fails the Constitution's requirement in Article I, Section 2 that an [...] "actual enumeration" [...] be used for apportionment. This case, Utah v. Evans, made it to the Supreme Court, but Utah was again defeated.|$|E
40|$|Abstract. In {{clinical}} {{case-based reasoning}} systems, missing {{values in the}} casebase pose a common but serious problem that impairs {{the performance of the}} system. Imputation of missing values is popular in many knowledge-based systems. However, applications need to be adjusted to take into account that imputed values are only estimates of original values. In this paper, we introduce an <b>imputation</b> <b>method</b> that exploits the correlation between clinical data by filtering the case-base according to attributes correlated to the missing attribute. We also present a framework that shows how any <b>imputation</b> <b>method</b> {{can be used in a}} case-based reasoning system to account for the inherent uncertainty of imputation and to reflect the quality of the <b>imputation</b> <b>method</b> in the similarity calculation. The filter <b>imputation</b> <b>method</b> and the imputation framework are evaluated using a case-based reasoning system for radiotherapy treatment planning for prostate cancer...|$|E
40|$|Missing {{data are}} often a problem in social science data. <b>Imputation</b> <b>methods</b> fill in the missing {{responses}} and lead, under certain conditions, to valid inference. This article reviews several <b>imputation</b> <b>methods</b> used {{in the social sciences}} and discusses advantages and disadvantages of these methods in practice. Simpler <b>imputation</b> <b>methods</b> as well as more advanced methods, such as fractional and multiple imputation, are considered. The paper introduces the reader new to the imputation literature to key ideas and methods. For those already familiar with <b>imputation</b> <b>methods</b> the paper highlights some new developments and clarifies some recent misconceptions in the use of <b>imputation</b> <b>methods.</b> The emphasis is on efficient hot deck <b>imputation</b> <b>methods,</b> implemented in either multiple or fractional imputation approaches. Software packages for using <b>imputation</b> <b>methods</b> in practice are reviewed highlighting newer developments. The paper discusses an example from the social sciences in detail, applying several <b>imputation</b> <b>methods</b> to a missing earnings variable. The objective is to illustrate how to choose between methods in a real data example. A simulation study evaluates various <b>imputation</b> <b>methods,</b> including predictive mean matching, fractional and multiple imputation. Certain forms of fractional and multiple hot deck methods are found to perform well with regards to bias and efficiency of a point estimator and robustness against model misspecifications. Standard parametric <b>imputation</b> <b>methods</b> are not found adequate for the application considered. [NCRM WP]item-nonresponse; imputation; fractional imputation; multiple imputation; estimation of distribution functions...|$|R
40|$|The Public Libraries Survey (PLS) imputes {{for missing}} data. Although the unit {{response}} rate is very high, {{there is still}} item nonresponse that requires imputation. PLS uses a variety of <b>imputation</b> <b>methods</b> depending on the item that is missing data. The {{purpose of this study}} is to evaluate current <b>imputation</b> <b>methods</b> along with some new alternatives. The <b>imputation</b> <b>methods</b> being evaluated are ratio, cell mean, adjusted cell mean, hotdeck, and combinations thereof. Another objective is to minimize the number of <b>imputation</b> <b>methods</b> being used. The study also compares current methods of determining the imputation cell boundaries to new methods using the population variable...|$|R
40|$|Abstract — Missing values, {{common in}} {{epidemiologic}} studies, {{are a major}} issue in obtaining valid estimates. Missing data are often a problem in social science data. <b>Imputation</b> <b>methods</b> fill in the missing responses and lead, under certain conditions, to valid inference. Simulation studies have suggested that multiple imputation is an attractive method for imputing missing values, but it is relatively complex and requires specialized software. This article reviews several <b>imputation</b> <b>methods</b> used in the social sciences and discusses advantages and disadvantages of these methods in practice. Simpler <b>imputation</b> <b>methods</b> as well as more advanced methods, such as fractional and multiple imputation, are discussed. The paper introduces the implementation in either multiple or fractional imputation approaches. Software packages for using <b>imputation</b> <b>methods</b> in practice are reviewed highlighting newer developments. The paper discusses an example from the social sciences in detail, applying several <b>imputation</b> <b>methods</b> to a missing earnings variable. The objective is to illustrate how to choose between methods in a real data example. A simulation study evaluates various <b>imputation</b> <b>methods,</b> including predictive mean matching, fractional and multiple imputation. This article reviews various <b>imputation</b> <b>methods</b> used within the social sciences to compensate for item-nonresponse bias, and provides the best result in usin...|$|R
40|$|Most multivariate {{statistical}} methods for gene expression data require a complete matrix of gene array values. In this paper, a <b>imputation</b> <b>method</b> based on least squares formulation is proposed to estimate missing values. It exploits local similarity {{structures in the}} data as well as least squares optimization process. The proposed local least squares <b>imputation</b> <b>method</b> (LLSimpute) represents a target gene that has missing values as a linear combination of similar genes. This algorithm showed better performance than the other imputation methods such as k-nearest neighbor imputation and an <b>imputation</b> <b>method</b> base on Bayesian principal component analysis. 1...|$|E
40|$|In this paper, {{we review}} the German {{practice}} of imputing {{the costs of}} owner-occupied housing by increasing the relative weight of actual rents in the CPI. As the structure of owner-occupied housing differs substantially from that of rental housing, this variant of the <b>imputation</b> <b>method</b> may cause a bias in the German CPI. For assessing {{the appropriateness of the}} German <b>imputation</b> <b>method,</b> we estimate alternative rental equivalent indices based on the GSOEP. We find some evidence of an understatement of the "true" rate of price increase, which is, however, not directly related to the <b>imputation</b> <b>method.</b> [...] Consumer Price Index,hedonics,housing,matched models...|$|E
40|$|Data were {{collected}} prospectively on parameters re-lated to first calving on 18 farms located in Northeast-ern Pennsylvania. This project {{was designed to}} study possible residual effects of calf management practices and events occurring during the first 16 wk of life on age, BW, skeletal growth, and body condition score at first calving. Multiple <b>imputation</b> <b>method</b> for handling missing data was incorporated in these analyses. This method has the advantage over ad hoc single imputa-tions because the appropriate error structure is main-tained. Much similarity {{was found between the}} multi-ple <b>imputation</b> <b>method</b> and a traditional mixed model analysis, except that some estimates from the multiple <b>imputation</b> <b>method</b> seemed more logical in their effect...|$|E
40|$|This study compares <b>imputation</b> <b>methods</b> (single and multiple) {{to examine}} the role of {{perceived}} stress {{in the relationship between}} social support and mood, and tested whether mediator effects influenced the relationship. The cross-sectional data reported here was collected in an experimental design with repeated measures with mothers of children who had been hospitalized in a child psychiatric unit. These <b>methods</b> included no <b>imputation,</b> single imputation, and multiple imputation for missing values. The results did not indicate any mediator effects for coping in the relationship between perceived stress and mood. These results were similar when imputation and no <b>imputations</b> <b>methods</b> were used. However, researchers should consider using imputation results were similar when imputation and no <b>imputations</b> <b>methods</b> were used. However, researchers should consider using <b>imputation</b> <b>methods</b> to help improve problems caused by missing values in the study...|$|R
40|$|Almost universally, forest {{inventory}} and monitoring databases are incomplete, ranging from missing data {{for only a}} few records and a few variables, common for small land areas, to missing data for many observations and many variables, common for large land areas. For a wide variety of applications, nearest neighbor (NN) <b>imputation</b> <b>methods</b> have been developed to fill in observations of variables that are missing on some records (Y-variables), using related variables that are available for all records (X-variables). This review attempts to summarize the advantages and weaknesses of NN <b>imputation</b> <b>methods</b> and to give an overview of the NN approaches that have most commonly been used. It also discusses some of the challenges of NN <b>imputation</b> <b>methods.</b> The inclusion of NN <b>imputation</b> <b>methods</b> into standard software packages and the use of consistent notation may improve further development of NN <b>imputation</b> <b>methods.</b> Using X-variables from different data sources provides promising results, but raises the issue of spatial and temporal registration errors. Quantitative measures of the contribution of individual X-variables to the accuracy of imputing the Y-variables are needed. In addition, further research is warranted to verify statistical properties, modify methods to improve statistical properties, and provide variance estimators...|$|R
3000|$|In general, we {{conclude}} that <b>imputation</b> <b>methods</b> can provide a suitable alternative to dummy coding, which is flawed by some conceptual problems and may cause biased mean proficiency estimates if the proportion of missing values on the background variables is substantial (Rutkowski [2011]). To date, multiple <b>imputation</b> <b>methods</b> per se are widely established in large-scale assessments to estimate θ [...]...|$|R
40|$|The final {{publication}} {{is available}} at IOS Press through [URL] This paper presents a comparative study over the respiratory pattern classification task involving three missing data imputation techniques, and four different machine learning algorithms. The main goal {{was to find a}} classifier that achieves the best accuracy results using a scalable <b>imputation</b> <b>method</b> in comparison to the method used in a previous work of the authors. The results obtained show that the Self-organization maps <b>imputation</b> <b>method</b> allows any classifier to achieve improvements over the rest of the imputation methods, and that the Feedforward neural network classifier offers the best performance regardless the <b>imputation</b> <b>method</b> used...|$|E
40|$|Missing {{data are}} {{prevalent}} in many public health studies for various reasons. For example, some subjects do not answer certain {{questions in a}} survey, or some subjects drop out of a longitudinal study prematurely. It is important to develop statistical methodologies to appropriately address missing data {{in order to reach}} valid conclusions. For regression analysis on data with missing values in the response variable, when data are not missing at random, usually the missing-data mechanism needs to be modeled. When the missingness only depends on the response variable, a pseudolikelihoodmethod that avoids modeling the nonignorable missing-data mechanism was developed in the past. A corresponding mean <b>imputation</b> <b>method</b> was used to impute the missing responses under this pseudolikelihood method. In this dissertation, we consider the inference on the moments of the response variable for missing data analyzed by this pseudolikelihood method. At first, we compared three methods: the delta method, the bootstrap method and a re-sampling method, for estimating the variance of the corresponding pseudolikelihood estimate in simulation studies. Second, we modified that mean <b>imputation</b> <b>method</b> and developed a corresponding stochastic <b>imputation</b> <b>method.</b> Multiple imputations were subsequently used to obtain estimates of the moments and the corresponding variance estimates. We compared the performance of these two imputation methods in simulation studies and illustrated them through analysis of the data from a Schizophrenia clinical trial. Compared to the mean <b>imputation</b> <b>method,</b> the stochastic <b>imputation</b> <b>method</b> leads to less and negligible bias...|$|E
40|$|Abstract: A {{comparative}} study over the respiratory pattern classification task, involving five missing data imputation techniques and several {{machine learning algorithms}} is presented in this paper. The main goal {{was to find a}} classifier that achieves the best accuracy results using a scalable <b>imputation</b> <b>method</b> in comparison to the method used in a previous work of the authors. The results obtained show that in general, the Self-organising map <b>imputation</b> <b>method</b> allows non-tree based classifiers to achieve improvements over the rest of the imputation methods in terms of the classification accuracy, and that the Feedforward neural network and the Random Forest classifiers offer the best performance regardless of the <b>imputation</b> <b>method</b> used. The improvements in terms of accuracy over the previous work of the authors are limited but the Feed Forward neural network model achieves promising results. Suggested Reviewers...|$|E
40|$|Deciphering {{important}} {{genes and}} pathways from incomplete gene expression data could facilitate {{a better understanding}} of cancer. Different <b>imputation</b> <b>methods</b> can be applied to estimate the missing values. In our study, we evaluated various <b>imputation</b> <b>methods</b> for their performance in preserving significant genes and pathways. In the first step, 5 % genes are considered in random for two types of ignorable and non-ignorable missingness mechanisms with various missing rates. Next, 10 well-known <b>imputation</b> <b>methods</b> were applied to the complete datasets. The significance analysis of microarrays (SAM) method was applied to detect the significant genes in rectal and lung cancers to showcase the utility of imputation approaches in preserving significant genes. To determine the impact of different <b>imputation</b> <b>methods</b> on the identification of important genes, the chi-squared test was used to compare the proportions of overlaps between significant genes detected from original data and those detected from the imputed datasets. Additionally, the significant genes are tested for their enrichment in important pathways, using the ConsensusPathDB. Our results showed that almost all the significant genes and pathways of the original dataset can be detected in all imputed datasets, indicating that there is {{no significant difference in the}} performance of various <b>imputation</b> <b>methods</b> tested. The source code and selected datasets are available on [URL]...|$|R
50|$|In addition, {{even when}} they do not add {{inappropriate}} bias, simple <b>imputation</b> <b>methods</b> overestimate the precision and reliability of the estimates {{and the power of the}} trial to assess the treatment. When data is missing, the sample size on which estimates are based is lowered. Simple <b>imputation</b> <b>methods</b> fail to account for this decrease in sample size, and hence tend to underestimate the variability of the results.|$|R
40|$|This paper {{describes}} the R package imputeTestbench {{that provides a}} testbench for comparing <b>imputation</b> <b>methods</b> for missing data in univariate time series. The imputeTestbench package {{can be used to}} simulate the amount and type of missing data in a complete dataset and compare filled data using different <b>imputation</b> <b>methods.</b> The user has the option to simulate missing data by removing observations completely at random or in blocks of different sizes. Several default <b>imputation</b> <b>methods</b> are included with the package, including historical means, linear interpolation, and last observation carried forward. The testbench {{is not limited to the}} default functions and users can add or remove additional methods using a simple two-step process. The testbench compares the actual missing and imputed data for each method with different error metrics, including RMSE, MAE, and MAPE. Alternative error metrics can also be supplied by the user. The simplicity of use and significant reduction in time to compare <b>imputation</b> <b>methods</b> for missing data in univariate time series is a significant advantage of the package. This paper provides an overview of the core functions, including a demonstration with examples...|$|R
40|$|Multiple <b>imputation</b> <b>method</b> is {{a widely}} used method in missing data analysis. The method {{consists}} of a three-stage process including imputation, analyzing and pooling. The number of imputations to be selected in the imputation step {{in the first stage}} is important. Hence, this study aimed to examine the performance of multiple <b>imputation</b> <b>method</b> at different numbers of imputations. Monotone missing data pattern was created in the study by deleting approximately 24 % of the observations from the continuous result variable with complete data. At the first stage of the multiple <b>imputation</b> <b>method,</b> monotone regression imputation at different numbers of imputations (m= 3, 5, 10 and 50) was performed. In the second stage, parameter estimations and their standard errors were obtained by applying general linear model to each of the complete data sets obtained. In the final stage, the obtained results were pooled and the effect of the numbers of imputations on parameter estimations and their standard errors were evaluated {{on the basis of these}} results. In conclusion, efficiency of parameter estimations at the number of imputation m= 50 was determined as about 99 %. Hence, at the determined missing observation rate, increase was determined in efficiency and performance of the multiple <b>imputation</b> <b>method</b> as the number of imputations increased...|$|E
40|$|Abstract: The hedonic <b>imputation</b> <b>method</b> {{can be used}} to {{construct}} price indexes over incompletely matched varieties of products. We show how use of the hedonic <b>imputation</b> <b>method</b> complicates the price index problem. In addition to choosing between different formulas such as Fisher and Törnqvist, it is necessary to choose between different varieties of each formula. This is because index compilers have a certain amount of discretion over which prices are imputed. The choice of price index formula is also affected by the functional form of the hedonic model. We show how some hedonic price indexes have a dual representation in product and characteristics space. The importance of these issues is illustrated in a housing context. We construct house price indexes for three regions in Sydney over a three year period and explore the sensitivity of the results to the way the hedonic <b>imputation</b> <b>method</b> is implemented...|$|E
40|$|In this paper, a data <b>imputation</b> <b>method</b> with a Support Vector Machine (SVM) is {{proposed}} {{to solve the}} issue of missing data in activity-based diaries. Here two SVM models are established to predict the missing elements of ‘number of cars’ and ‘driver license’. The inputs of the former SVM model include five variables (Household composition, household income, Age oldest household member, Children age class and Number of household members). The inputs of the latter SVM model include three variables (personal age, work status and gender). The SVM models to predict the ‘number of cars’ and ‘driver license’ can achieve accuracies of 69 % and 83 % respectively. The initial experimental results show that missing elements of observed activity diaries can be accurately inferred by relating different pieces of information. Therefore, the proposed SVM data <b>imputation</b> <b>method</b> serves as an effective data <b>imputation</b> <b>method</b> {{in the case of}} missing information. Peer reviewe...|$|E
40|$|A {{common problem}} {{encountered}} by many data mining techniques is the missing data. A missing data {{is defined as}} an attribute or feature in a dataset which has no associated data value. Correct treatment of these data is crucial, as they {{have a negative impact}} on the interpretation and result of data mining processes. Missing value handling techniques can be grouped into four categories, namely,complete case analysis, <b>Imputation</b> <b>methods,</b> maximum likelihood methods and machine learning methods. Out of these <b>imputation</b> <b>methods</b> are the widely used solution for handling missing values. However, there are situations when <b>imputation</b> <b>methods</b> might not work correctly. This study studies and analyzes the performance of two algorithms, one imputation based and another without imputation basedclassification on missing data...|$|R
40|$|Abstract—A {{common problem}} {{encountered}} by many data mining techniques is the missing data. A missing data {{is defined as}} an attribute or feature in a dataset which has no associated data value. Correct treatment of these data is crucial, as they {{have a negative impact}} on the interpretation and result of data mining processes. Missing value handling techniques can be grouped into four categories, namely, complete case analysis, <b>Imputation</b> <b>methods,</b> maximum likelihood methods and machine learning methods. Out of these <b>imputation</b> <b>methods</b> are the widely used solution for handling missing values. However, there are situations when <b>imputation</b> <b>methods</b> might not work correctly. This study studies and analyzes the performance of two algorithms, one imputation based and another without imputation based classification on missing data. Keywords-Missing Values, Imputation, Non-imputation, Classification with missing data. 1...|$|R
30|$|Wind speed data {{imputation}} necessitated scientifically imputing {{data gaps}} {{that may be}} at the beginning, within or {{at the end of the}} wind speed dataset. The power and accuracy levels of the <b>imputation</b> <b>methods</b> often vary and therefore, a survey of the most suitable approach was carried out to ensure validity of an imputed wind speed dataset. Although sometimes, <b>imputation</b> <b>methods</b> are capable of predicting future occurrences, necessary assumptions should be made to produce reliable results.|$|R
40|$|In a {{previous}} work, {{it was clearly}} shown that {{the performance of the}} very simple <b>imputation</b> <b>method</b> based on “Most Common Attribute Value ” called MC gave performance better than that of several complex imputation algorithms. And in that work [1] it was shown that the performance of MC was almost equal to that of best performing <b>imputation</b> <b>method</b> called “Event Covering” (EC). So in this work, It is tried to improve the performance of the simple <b>imputation</b> <b>method</b> MC and proposed a new algorithm. The performance of the proposed algorithm has been compared with the other simple and efficient imputation methods. The performance has been measured with respect to different rate or different percentage of missing values in the data set. To evaluate the performance, the standard WDBC data set has been used. The proposed algorithm performed very well and the arrived results were more significant and comparable. Keywords:Datamining, Preprocessing, Missing Value...|$|E
40|$|We {{develop a}} {{non-parametric}} <b>imputation</b> <b>method</b> for item non-response {{based on the}} well-known hot-deck approach. The proposed <b>imputation</b> <b>method</b> is developed for imputing numerical data that ensure that all record-level edit rules are satisfied and previously estimated or known totals are exactly preserved. We propose a sequential hot-deck imputation approach {{that takes into account}} survey weights. Original survey weights are not changed, rather the imputations themselves are calibrated so that weighted estimates will equal known or estimated population totals. Edit rules are preserved by integrating the sequential hot-deck imputation with Fourier-Motzkin elimination which defines the range of feasible values {{that can be used for}} imputation such that all record-level edits will be satisfied. We apply the proposed <b>imputation</b> <b>method</b> under different scenarios of random and nearest-neighbour hot-deck on two data sets: an annual structural business survey and a synthetically generated data set with a large proportion of missing data. We compare the proposed imputation methods to standard imputation methods based on a set of evaluation measures...|$|E
40|$|Missing {{values are}} {{ubiquitous}} in real-world datasets. In this work, we show {{how to handle}} them with heterogeneous ensembles of classifiers that outperform state-of-the-art solutions. Several approaches are compared using several different datasets. Some state-of-the-art classifiers, e. g., SVM and RotBoost, are tested first and coupled with the Expectation-Maximization (EM) <b>imputation</b> <b>method.</b> The classifiers are then combined to build ensembles. Using the Wilcoxon signed-rank test (reject the null hypothesis, level of significance 0. 05), we show that our best heterogeneous ensembles, obtained by combining a forest of decision trees (a method that does not require any dataset-specific tuning) with a cluster-based <b>imputation</b> <b>method,</b> outperforms two dataset-tuned solutions: a stand-alone SVM classifier and a random subspace of SVMs, both based on LibSVM, {{the most widely used}} SVM toolbox in the world. Our heterogeneous ensembles also exhibit better performance than a recent cluster-based <b>imputation</b> <b>method</b> for handling missing values (a method which has been shown to outperform several other state-of-the-art imputation approaches) when both the training set and the testing set contain 10 % missing values...|$|E
40|$|Missing data in {{clinical}} trials can ruin the significance of test results and violates the Intention To Treat principle. Therefore <b>imputation</b> <b>methods</b> are an important tool {{that is used to}} deal with this problem. In this essay two different Bayesian <b>imputation</b> <b>methods</b> will be compared to four different general <b>imputation</b> <b>methods.</b> The general methods are the LOCF, worst case, best case and a mean value method. The Bayesian methods are Empirical Bayes method and a method using the Expectation Maximization algorithm. From the results of analysis made on the data sets used here it can be concluded that the Bayesian methods give better estimates than general <b>imputation</b> <b>methods.</b> This conclusion is valid for different parameter values such as variance, percentage missing values and number of observations per patient. Acknowledgment I would like to thank my supervisor and examinator Silvelyn Zwanzig at the Department of Mathematics for answering questions and giving a lot of support. I also want to thank Ass. professor Johan Bring at Statisticon for introducing me to the interesting subject and for answering questions. Finally I want to thank my fellow student Martin Elfsberg for support and interesting discussions...|$|R
40|$|Abstract — Many {{attempts}} {{have been carried out}} to deal with missing values (MV) in microarrays data representing gene expressions. This is a problematic issue as many data analysis techniques are not robust to missing data. Most of the MV <b>imputation</b> <b>methods</b> currently being used have been evaluated only in terms of the similarity between the original and imputed data. While imputed expression values themselves are not interesting, rather whether or not the imputed expression values are reliable to use in subsequent analysis is the major concern. This paper focuses on studying the impact of different MV <b>imputation</b> <b>methods</b> on the classification accuracy. The experimental work was first subjected to implementing three popular <b>imputation</b> <b>methods,</b> namely Singular Value Decomposition (SVD), weighted K-nearest neighbors (KNNimpute), and Zero replacement. The robustness of the three methods to the amount of missing data was then studied. The experiments were repeated for datasets with different missing rates (MR) over the range of 0 - 20 % MR. In applying supervised two class classification we adopted a twofold approach, introducing all genes expressions to the classifiers as well as a subset of selected genes. The feature selection method used for gene selection is Fisher Discriminate Analysis (FDA), which improved noticeably the performance of the classifiers. The retained classifiers accuracies using imputed data after applying the three proposed <b>imputation</b> <b>methods</b> show slight variations over the specified range of MR. Thus, assessing that the three <b>imputation</b> <b>methods</b> in concern are robust. Keywords-microarrays; imputation; evaluation; classification I...|$|R
40|$|Missing {{data are}} a great concern in {{longitudinal}} studies, because few subjects will have complete data and missingness could be an indicator of an adverse outcome. Analyses that exclude potentially informative observations due to missing data can be inefficient or biased. To assess the extent of these problems {{in the context of}} genetic analyses, we compared case-wise deletion to two multiple <b>imputation</b> <b>methods</b> available in the popular SAS package, the propensity score and regression methods. For both the real and simulated data sets, the propensity score and regression methods produced results similar to case-wise deletion. However, for the simulated data, the estimates of heritability for case-wise deletion and the two multiple <b>imputation</b> <b>methods</b> were much lower than for the complete data. This suggests that if missingness patterns are correlated within families, then <b>imputation</b> <b>methods</b> that do not allow this correlation can yield biased results...|$|R
40|$|We {{investigate}} {{methods for}} penalized regression {{in the presence}} of missing observations. This paper introduces a method for estimating the parameters which compensates for the missing observations. We first, derive an unbiased estimator of the objective function with respect to the missing data and then, modify the criterion to ensure convexity. Finally, we extend our approach to a family of models that embraces the mean <b>imputation</b> <b>method.</b> These approaches are compared to the mean <b>imputation</b> <b>method,</b> one of the simplest methods for dealing with missing observations problem, via simulations. We also investigate the problem of making predictions when there are missing values in the test set...|$|E
40|$|Abstract Missing data {{imputation}} is {{an important}} issue in machine learning and data mining. In this paper, we propose a new and efficient <b>imputation</b> <b>method</b> for a kind of missing data: semi-parametric data. Our <b>imputation</b> <b>method</b> aims at making an optimal evaluation about Root Mean Square Error (RMSE), distribution function and quantile after missing-data are imputed. We evaluate our approaches using both simulated data and real data experimentally, and demonstrate that our stochastic semi-parametric regression imputation is much better than existing deterministic semi-parametric regression imputation in efficiency and effectiveness. This work is partially supported by Australian large ARC grants (DP 0449535, DP 0559536 and DP 0667060), a China NSF majo...|$|E
40|$|Background: Many QTL {{studies have}} two common features: (1) often there is missing marker information, (2) among many markers {{involved}} in the biological process only a few are causal. In statistics, the second issue falls under the headings "sparsity" and "causal inference". The goal of this work {{is to develop a}} two-step statistical methodology for QTL mapping for markers with binary genotypes. The first step introduces a novel <b>imputation</b> <b>method</b> for missing genotypes. Outcomes of the proposed <b>imputation</b> <b>method</b> are probabilities which serve as weights to the second step, namely in weighted lasso. The sparse phenotype inference is employed to select a set of predictive markers for the trait of interest. Results: Simulation studies validate the proposed methodology under a wide range of realistic settings. Furthermore, the methodology outperforms alternative imputation and variable selection methods in such studies. The methodology was applied to an Arabidopsis experiment, containing 69 markers for 165 recombinant inbred lines of a F 8 generation. The results confirm previously identified regions, however several new markers are also found. On the basis of the inferred ROC behavior these markers show good potential for being real, especially for the germination trait G(max). Conclusions: Our <b>imputation</b> <b>method</b> shows higher accuracy in terms of sensitivity and specificity compared to alternative <b>imputation</b> <b>method.</b> Also, the proposed weighted lasso outperforms commonly practiced multiple regression as well as the traditional lasso and adaptive lasso with three weighting schemes. This means that under realistic missing data settings this methodology can be used for QTL identification...|$|E
40|$|To {{estimate}} the population mean with imputation i. e. {{the technique of}} substituting missing data, {{there are a number}} of techniques available in literature like Ratio <b>method</b> of <b>imputation,</b> Compromised <b>method</b> of <b>imputation,</b> Mean <b>method</b> of <b>imputation,</b> Ahmed <b>method</b> of <b>imputation,</b> F-T <b>method</b> of <b>imputation,</b> and so on. If population mean of auxiliary information is unknown then these methods are not useful and the two-phase sampling is used to obtain the population mean. This paper presents some <b>imputation</b> <b>methods</b> of for missing values in twophase sampling. Two different sampling designs in two-phase sampling are compared under imputed data. The bias and m. s. e of suggested estimators are derived in the form of population parameters using the concept of large sample approximation. Numerical study is performed over two populations using the expressions of bias and m. s. e and efficiency compared with Ahmed estimators...|$|R
40|$|Background Policy makers need {{models to}} be able to detect groups at high risk of HIV infection. Incomplete records and dirty data are {{frequently}} seen in national data sets. Presence of missing data challenges the practice of model development. Several studies suggested that performance of <b>imputation</b> <b>methods</b> is acceptable when missing rate is moderate. One of the issues which was of less concern, to be addressed here, {{is the role of the}} pattern of missing data. Methods We used information of 2720 prisoners. Results derived from fitting regression model to whole data were served as gold standard. Missing data were then generated so that 10 %, 20 % and 50 % of data were lost. In scenario 1, we generated missing values, at above rates, in one variable which was significant in gold model (age). In scenario 2, a small proportion of each of independent variable was dropped out. Four <b>imputation</b> <b>methods,</b> under different Event Per Variable (EPV) values, were compared in terms of selection of important variables and parameter estimation. Results In scenario 2, bias in estimates was low and performances of all method for handing missing data were similar. All methods at all missing rates were able to detect significance of age. In scenario 1, biases in estimations were increased, in particular at 50 % missing rate. Here at EPVs of 10 and 5, <b>imputation</b> <b>methods</b> failed to capture effect of age. Conclusion In scenario 2, all <b>imputation</b> <b>methods</b> at all missing rates, were able to detect age as being significant. This was not the case in scenario 1. Our results showed that performance of <b>imputation</b> <b>methods</b> depends on the pattern of missing data...|$|R
40|$|BACKGROUND: Imputation {{techniques}} used to handle missing data {{are based on}} the principle of replacement. It is widely advocated that multiple imputation is superior to other <b>imputation</b> <b>methods,</b> however studies have suggested that simple methods for filling missing data can be just as accurate as complex methods. The objective {{of this study was to}} implement a number of simple and more complex <b>imputation</b> <b>methods,</b> and assess the effect of these techniques on the performance of undiagnosed diabetes risk prediction models during external validation. METHODS: Data from the Cape Town Bellville-South cohort served as the basis for this study. <b>Imputation</b> <b>methods</b> and models were identified via recent systematic reviews. Models’ discrimination was assessed and compared using C-statistic and non-parametric methods, before and after recalibration through simple intercept adjustment. RESULTS: The study sample consisted of 1256 individuals, of whom 173 were excluded due to previously diagnosed diabetes. Of the final 1083 individuals, 329 (30. 4 %) had missing data. Family history had the highest proportion of missing data (25 %). Imputation of the outcome, undiagnosed diabetes, was highest in stochastic regression imputation (163 individuals). Overall, deletion resulted in the lowest model performances while simple imputation yielded the highest C-statistic for the Cambridge Diabetes Risk model, Kuwaiti Risk model, Omani Diabetes Risk model and Rotterdam Predictive model. Multiple imputation only yielded the highest C-statistic for the Rotterdam Predictive model, which were matched by simpler <b>imputation</b> <b>methods.</b> CONCLUSIONS: Deletion was confirmed as a poor technique for handling missing data. However, despite the emphasized disadvantages of simpler <b>imputation</b> <b>methods,</b> this study showed that implementing these methods results in similar predictive utility for undiagnosed diabetes when compared to multiple imputation...|$|R
