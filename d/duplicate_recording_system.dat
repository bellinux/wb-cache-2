0|8888|Public
40|$|As {{research}} {{becomes more}} and more interdisciplinary, literature search from CD-ROM databases is often carried out on more than one CD-ROM database. This results in retrieving <b>duplicate</b> <b>records</b> due to same literature being covered (indexed) in more than one database. The retrieval software does not identify such <b>duplicate</b> <b>records.</b> Three different programs have been written to accomplish the task of identifying the <b>duplicate</b> <b>records.</b> These programs are executed from a shell script to minimize manual intervention. The various fields that have been used (extracted) to identify the <b>duplicate</b> <b>records</b> include the article title, year, volume number, issue number and pagination. The shell script when executed prompts for input file that may contain <b>duplicate</b> <b>records.</b> The programs identify the <b>duplicate</b> <b>records</b> and write them to a new file...|$|R
40|$|Abstract: <b>Duplicate</b> <b>record</b> {{detection}} {{is a key}} step in Deep Web data integration, but {{the existing}} approaches do not adapt to its large-scale nature. In this paper, a three-step automatic approach is proposed for <b>duplicate</b> <b>record</b> detection in Deep Web. It firstly uses cluster ensemble to select initial training instance. Then it utilizes tri-training classification to con-struct classification model. Finally, it uses evidence theory to combine the results of multiple classification models to con-struct the domain-level <b>duplicate</b> <b>record</b> detection model {{which can be used}} for large-scale <b>duplicate</b> <b>record</b> detection in the same domain. Experimental results show that the proposed approach is better than previous work and and the domain-level <b>duplicate</b> <b>record</b> detection model can get high performance...|$|R
40|$|The {{importance}} of supporting keyword searches on relations {{has been widely}} recognized. Different from the existing keyword search techniques on relations, this paper focuses on nearly <b>duplicate</b> <b>records</b> in relational databases due to abbreviation and typos. As a result, processing keyword searches with <b>duplicate</b> <b>records</b> involves many unique challenges. In this paper we discuss the motivation and present a system, RSEARCH, to show challenges in supporting keyword search using nearly <b>duplicate</b> <b>records</b> and key techniques including identifying nearly <b>duplicate</b> <b>records</b> and generating results efficiently. ...|$|R
40|$|Recent studies {{documented}} that survey data contain <b>duplicate</b> <b>records.</b> We assess how <b>duplicate</b> <b>records</b> affect regression estimates, {{and we evaluate}} the effectiveness of solutions to deal with <b>duplicate</b> <b>records.</b> Results show that the chances of obtaining unbiased estimates when data contain 40 doublets (about 5 % of the sample) range between 3. 5 % and 11. 5 % depending on the distribution of duplicates. If 7 quintuplets are present in the data (2 % of the sample), then the probability of obtaining biased estimates ranges between 11 % and 20 %. Weighting the <b>duplicate</b> <b>records</b> by the inverse of their multiplicity, or dropping superfluous duplicates outperform other solutions in all considered scenarios. Our results illustrate the risk of using data in presence of <b>duplicate</b> <b>records</b> and call for further research on strategies to analyze affected data. " (author's abstract...|$|R
40|$|Deduplication {{is the key}} {{operation}} in dataintegration from multiple data sources. To achieve higherquality information and more simplified data representation,data preprocessing is required. Data cleaning is one among thedata preprocessing steps. Data cleaning includes the process ofparsing, data transformation, duplicate elimination andstatistical methods. If two records represent the same realworld entity then it is called <b>duplicated</b> <b>records.</b> The problem ofdetecting and eliminating <b>duplicate</b> <b>records</b> is called recorddeduplication. This paper presents an analysis of recorddeduplication techniques and algorithms that detect andremove the <b>duplicate</b> <b>records...</b>|$|R
40|$|In {{real time}} applications, {{identification}} of records {{that represent the}} same real-world entity is a major challenge to be solved. Such records are termed to be <b>duplicate</b> <b>records.</b> This study presented a thorough analysis {{of the literature on}} <b>duplicate</b> <b>record</b> detection. The <b>duplicate</b> <b>record</b> detection is an important step for data integration. An overview of data deduplication issue is discussed in detail. This paper covered almost all the metrics that are commonly used to detect similar entries and a set of duplicate detection algorithms...|$|R
40|$|In recent years, the Web of Science Core Collection and Scopus {{databases}} {{have become}} primary sources for conducting studies that evaluate scientific investigations. Such studies require that <b>duplicate</b> <b>records</b> be excluded to avoid errors of overrepresentation. In this line, we identify <b>duplicate</b> <b>records</b> in Scopus and examine their origins. Identifying journals with <b>duplicate</b> <b>records</b> in Scopus, selecting and downloading bibliographic journal records, and identifying and analyzing the <b>duplicate</b> <b>records</b> is the methodology adopted. <b>Duplicate</b> <b>records</b> are found when {{articles published in}} a journal are incorrectly mapped by Scopus to this journal and to a different journal from the same publisher and when there are journal title changes, orthographic differences in the presentation of a journal name, and journal name variants. In these last three cases, one bibliographic <b>record</b> of each <b>duplicate</b> is mapped to Medline coverage of Scopus. Consequently, the identified duplicates and the {{significant differences in the}} number of citations received in duplicate articles may influence bibliometric studies. Thus, {{there is a need for}} rigorous quality control guidelines to govern database managers and editors to prevent the creation of duplicates. Peer Reviewe...|$|R
40|$|Detecting {{database}} records that are approximate duplicates, but not exact duplicates, {{is an important}} task. Databases may contain <b>duplicate</b> <b>records</b> concerning the same realworld entity because of data entry errors, because of unstandardized abbreviations, or because of differences in the detailed schemas of records from multiple databases, among other reasons. In this paper, we present an efficient algorithm for recognizing clusters of approximately <b>duplicate</b> <b>records.</b> Three key ideas distinguish the algorithm presented. First, {{a version of the}} Smith-Waterman algorithm for computing minimum edit-distance is used as a domainindependent method to recognize pairs of approximately <b>duplicate</b> <b>records.</b> Second, the union/find algorithm is used to keep track of clusters of <b>duplicate</b> <b>records</b> incrementally, as pairwise duplicate relationships are discovered. Third, the algorithm uses a priority queue of cluster subsets to respond adaptively to the size and homogeneity of the clusters discovered as [...] ...|$|R
40|$|Abstract. To {{solve the}} problem of {{attribute}} weight determination in the approximately <b>duplicate</b> <b>records,</b> we put forward a method based on fuzzy comprehensive evaluation to get attribute weight in data set. We first perform an analysis of the composition factors of attribute. Then we carry out an evaluation of their rank. Finally, we make a determination of the attribute weight using the fuzzy comprehensive evaluation method, on the basis of which the approximately <b>duplicate</b> <b>records</b> are detected. Theoretical analysis and experimental results show that the method can objectively determine all attributes weight, and effectively detect the approximately <b>duplicate</b> <b>records</b> in massive data set...|$|R
5000|$|Virus - The Agent That Shapes The Desert (<b>Duplicate</b> <b>Records,</b> 2010) ...|$|R
5000|$|... #Subtitle level 3: Incentives to make <b>duplicate</b> <b>recorded</b> {{versions}} of a song ...|$|R
40|$|Studies {{using the}} newly created Ontario Cancer Registry (OCR) {{identified}} a number of <b>duplicate</b> <b>records.</b> These could be traced to incorrect spelling of surnames which did not permit incoming records to be correctly compared during the routine computerized record linkage process. The method of elimination of the majority of these <b>duplicate</b> <b>records</b> and the results will be presented...|$|R
5000|$|Zhenya ( [...] P-Town Remix) ( [...] Compilation {{contribution}} to Kaffee Burger [...] (<b>Duplicate</b> <b>Records)</b> 2006) ...|$|R
5000|$|Überthrash II - 7" [...] Split, <b>Duplicate</b> <b>Records</b> (Split w. Audiopain / Aura Noir / Infernö) (2005) ...|$|R
40|$|Abstract: Problem statement: Record linkage is a {{technique}} {{which is used to}} detect and match <b>duplicate</b> <b>records</b> which are generated in data integration process. A variety of record linkage algorithms with different steps have been developed in order to detect such <b>duplicate</b> <b>records.</b> To find out whether two <b>records</b> are <b>duplicate</b> or not, supervised and unsupervised classification techniques are utilized in different studies. In order to utilize the supervised classification algorithms without consuming a lot of time for labeling data manually, a two step method which selects the training data automatically has been proposed in previous studies. However, the effectiveness of different classification techniques is the issue which should be taken into accounts in <b>record</b> linkage <b>systems</b> in order to classify records more accurately. Approach: To determine and compare the effectiveness of different supervised classification techniques in an unsupervised manner, some of the prominent classification methods are applied in <b>duplicate</b> <b>records</b> detection. <b>Duplicate</b> detection and classification of records in two real world datasets, namely Cora and Restaurant is experimented by Support Vector Machines, Naïve Bayes, Decision Tree and Bayesian Networks which are regarded as some prominent classification techniques. Results: As experimental results show, while Support Vector Machines outperforms with F-measure of 96. 27 % in Restaurant dataset, for Cora dataset, the effectiveness o...|$|R
5000|$|Überthrash - 7" [...] Split, <b>Duplicate</b> <b>Records</b> (Split w. Audiopain / Aura Noir / Infernö, Ltd. 500) (2004) ...|$|R
40|$|<b>Duplicate</b> <b>records</b> do {{not share}} a common key and/or they contain errors that make {{duplicate}} matching a difficult task. Errors are introduced {{as the result of}} transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately <b>duplicate</b> <b>records</b> in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area...|$|R
40|$|There is an {{increasing}} demand for systems that can provide secure data storage in a cost-effective manner. Having <b>duplicate</b> <b>records</b> occupies more space and even increases the access time. Thus {{there is a need}} to eliminate <b>duplicate</b> <b>records.</b> This sounds to be simple but requires an tedious work since <b>duplicate</b> <b>records</b> do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are also introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. Several approaches areproposed to eliminate duplicate data first at the file level and then at the chunk level to reduce the duplicatelookup complexity. In this paper,few of the methods are discussed with its advantages and disadvantages. And also a better solution is proposed...|$|R
5000|$|Oblivion Clock is an EP by the Norwegian avant-garde metal band Virus. It was {{released}} on 1 December 2012 by <b>Duplicate</b> <b>Records.</b>|$|R
40|$|Record {{matching}} is {{an important}} process in data integration and data cleaning. It involves identifying cases where multiple database entities correspond to the same realworld entity. Often, <b>duplicate</b> <b>records</b> do not share a common key and contain erroneous data that make record matching a difficult task. The quality of a <b>record</b> matching <b>system</b> highly depends on a good approach that is able to accurately detects duplicates in an efficient and effective way. Despite the many techniques that have been introduced over the decades, it is unclear which technique is the current state-of-the-art. Hence, the objectives of this project are: 1. Compare a few record matching techniques and evaluate their advantages and disadvantages. 2. Develop a technique that combines the best features from these techniques to produce an improved record matching technique. Currently, {{there are two main}} approaches for <b>duplicate</b> <b>record</b> detection, categorised into approaches that rely on training data, and approaches that rely on domain knowledge or distance metrics. This project focuses on comparisons between the Probabilisticbase...|$|R
40|$|Problem statement: Record linkage is a {{technique}} {{which is used to}} detect and match&# 13; <b>duplicate</b> <b>records</b> which are generated in data integration process. A variety of record linkage&# 13; algorithms with different steps have been developed in order to detect such <b>duplicate</b> <b>records.</b> To find&# 13; out whether two <b>records</b> are <b>duplicate</b> or not, supervised and unsupervised classification techniques are&# 13; utilized in different studies. In order to utilize the supervised classification algorithms without&# 13; consuming a lot of time for labeling data manually, a two step method which selects the training data&# 13; automatically has been proposed in previous studies. However, the effectiveness of different&# 13; classification techniques is the issue which should be taken into accounts in <b>record</b> linkage <b>systems</b> in&# 13; order to classify records more accurately. Approach: To determine and compare the effectiveness of&# 13; different supervised classification techniques in an unsupervised manner, some of the prominent&# 13; classification methods are applied in <b>duplicate</b> <b>records</b> detection. <b>Duplicate</b> detection and classification&# 13; of records in two real world datasets, namely Cora and Restaurant is experimented by Support Vector&# 13; Machines, Naïve Bayes, Decision Tree and Bayesian Networks which are regarded as some prominent&# 13; classification techniques. Results: As experimental results show, while Support Vector Machines&# 13; outperforms with F-measure of 96. 27 % in Restaurant dataset, for Cora dataset, the effectiveness of&# 13; Naive Bayes is the best and it leads to an improvement with F-measure of 89. 7 %. &# 13; Conclusion/Recommendation: The result of detecting <b>duplicate</b> <b>records</b> with different classification&# 13; techniques tends to fluctuate depending on the dataset which is used. Moreover, Support Vector&# 13; Machines and Naïve Bayes outperform other methods in our experiments...|$|R
40|$|Often, in {{the real}} world, {{entities}} have two or more representations in databases. <b>Duplicate</b> <b>records</b> do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced {{as the result of}} transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately <b>duplicate</b> <b>records</b> in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area...|$|R
40|$|<b>Duplicate</b> <b>records</b> {{detection}} is {{the process}} of identifying multiple records that refer to one unique real-world entity or object. However, <b>duplicate</b> <b>records</b> may do not share a common key and contain errors that make <b>duplicate</b> <b>records</b> detection a difficult task. By analyzing the MPN algorithm, it is clear that transitive closure in the merge step will cause higher false-positive rate. Our improved method treats a similar dataset as a complete sub-graph, and therefore the problem of <b>duplicate</b> <b>records</b> detection is converted to finding complete sub-graphs from an association graph where the vertexes represent data records and the edges reflect the similarity between records. In our proposed method, {{the first step is to}} build the association graph. Afterwards, it searches complete sub-graphs for each sliding window. It regards the corresponding vertex of the first record of current window as the first potential vertex of a complete sub-graph, and adds new vertexes into that sub-graph when new vertexes are adjacent to all the vertexes already in the sub-graph. These steps are repeated until all the records are checked. At the same time, our algorithm effectively avoids the redetection of some parts of an already detected sub-graph. Finally, the experimental results illustrate that the improved algorithm solves the problem of false cluster caused by transitive closure effectively...|$|R
50|$|The Agent That Shapes the Desert is {{the third}} studio album by Norwegian avant-garde metal band Virus. It was {{released}} on 14 February 2011 via <b>Duplicate</b> <b>Records.</b>|$|R
30|$|It can be {{seen from}} Fig.  4 that the {{accuracy}} of the pre-processed <b>duplicated</b> <b>recording</b> detection method is higher than that of the unpreprocessed <b>duplicated</b> <b>recording</b> detection method, but since the experimental data used is not large, the pre-processing cannot be fully displayed. In the two cases of ω= 16 and ω= 8, ωchooses 16 to be more accurate than 8 because the largest cluster in the experimental data contains 15 records. If ω= 8, it will result in some <b>duplicate</b> <b>records,</b> which cannot be detected, although ω= 16 when it is needed to do many unnecessary comparisons, increasing the amount of calculation, but it can guarantee a higher accuracy rate. In this experiment, the pre-processed ω= 16 ranking neighbor method {{is the same as the}} Canopy cluster’s replication record detection method, but the Canopy method has a higher recall rate, indicating that it can obtain more replication records. The algorithm is more efficient.|$|R
40|$|Abstract—Often, in {{the real}} world, {{entities}} have two or more representations in databases. <b>Duplicate</b> <b>records</b> do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced {{as the result of}} transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately <b>duplicate</b> <b>records</b> in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area. Index Terms—Duplicate detection, data cleaning, data integration, record linkage, data deduplication, instance identification, database hardening, name matching, identity uncertainty, entity resolution, fuzzy duplicate detection, entity matching. ...|$|R
40|$|Abstract — The {{problem of}} {{identifying}} approximately <b>duplicate</b> <b>record</b> in database {{is an essential}} step for data cleaning & data integration process. A dynamic web page is displayed to show the results {{as well as other}} relevant advertisements that seem relevant to the query. The real world entities have two or more representation in databases. When dealing with large amount of data it is important that there be a well defined and tested mechanism to filter out duplicate result. This keeps the result relevant to the queries. <b>Duplicate</b> <b>record</b> exists in the query result of many web databases especially when the duplicates are defined based on only some of the fields in a record. Using exact matching technique Records that are exactly same can be detected. The system that helps user to integrate and compares the query results returned from multiple web databases matches the different sources records that referred to the same real world entity. In this paper, we analyze the literature on <b>duplicate</b> <b>record</b> detection. We cover similarity metrics which are commonly used to detect similar field entries, and present an extensive set of duplicate detection algorithms that can detect approximately <b>duplicate</b> <b>records</b> in a database also the techniques for improving the efficiency and scalability of approximate duplicate detection algorithms are covered. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area...|$|R
40|$|Detecting {{database}} records that are approximate duplicates, but not exact duplicates, {{is an important}} task. Databases may contain <b>duplicate</b> <b>records</b> concerning the same real-world entity because of data entry errors, unstandardized abbreviations, or differences in the detailed schemas of records from multiple databases – such as what happens in data warehousing where records from multiple data sources are integrated into a single source of information – among other reasons. In this paper we review a system to detect approximate <b>duplicate</b> <b>records</b> in a database and provide properties that a pair-wise record matching algorithm must have {{in order to have}} a successful duplicate detection system. ...|$|R
40|$|Though {{data quality}} issues arise with ever-zooming {{quantity}} of data, it {{is a welcome}} sign that of late, significant improvement {{has been made in}} data engineering. Consequently, there have been significant investments from private and government organizations in developing methods for removing replicas from the data repositories. This phenomenon has caused a significant interest among researchers in developing efficient and effective duplicate detection strategy using modern and emerging techniques. In this paper, we have proposed accordingly. In the previous work <b>duplicate</b> <b>record</b> detection was done using Q-gram concept and the fuzzy classifier. Here, different set of features from the data is found out using the Q-gram concept that leads to computational complex environment. In order to reduce the computational task, a set of important Q-gram-based feature subsets is selected. With this intention, the overall steps of the proposed technique are carried out using three different steps, such as, 1) feature computation, 2) feature selection, and 3) detection. Initially, the features are computed using Q-gram concept and then, the subset of optimal feature sets is identified using particle swarm algorithm (PSO) {{which is one of the}} most effective optimization algorithms. Once we select the optimal features sets, the Naïve Bayes Classifier is utilized to detect the duplication records. There are two processes which characterize the proposed <b>Duplicate</b> <b>Record</b> Detection technique such as the training phase and the testing phase. The experimental results showed that the proposed <b>Duplicate</b> <b>Record</b> Detection technique has higher accuracy than that of the existing method. The accuracy obtained for the proposed <b>Duplicate</b> <b>Record</b> Detection is found to be 89 %...|$|R
40|$|Variation {{and noise}} in textual {{database}} entries can prevent text mining algorithms from discovering important regularities. We present two novel methods {{to cope with}} this problem: (1) an adaptive approach to "hardening" noisy databases by identifying <b>duplicate</b> <b>records,</b> and (2) mining "soft" association rules. For identifying approximately <b>duplicate</b> <b>records,</b> we present a domain-independent two-level method for improving duplicate detection accuracy based on machine learning. For mining soft matching rules, we introduce an algorithm that discovers association rules by allowing partial matching of items based on a textual similarity metric such as edit distance or cosine similarity. Experimental results on real and synthetic datasets show that our methods outperform traditional techniques for noisy textual databases...|$|R
50|$|Reunion 6 was {{announced}} on November 1998 {{and saw the}} genealogy software change to include pictures into the family card view and introduced the Match & Merge tool {{that can be used}} to detect and remove <b>duplicate</b> <b>records</b> in the family file.|$|R
50|$|<b>Duplicate</b> file <b>records</b> {{are to be}} {{used only}} to allow{{represent}}ing files with data extents that are largerthan 4GiB-2048. <b>Duplicate</b> file <b>records</b> {{are not to be}} usedto represent files with fragmented data. When duplicatefile records are used, the multi-extent flag must alsobe used as indicated in ISO-9660-1999 specification.Duplicate file records should not be created unless thetotal data size of the file is greater than 4Gib-2048.When <b>duplicate</b> file <b>records</b> exist for a file, all butthe last file record must have a data extent that isexactly 4Gib-2048 bytes in size.|$|R
40|$|The record {{deduplication}} is {{the task}} of identifying, in a data repository, records that refer to the same real world entity or object in spite of misspelling words, types, different writing styles or even different schema representations or data types. In existing system aims at providing Unsupervised Duplication Detection (UDD) method {{which can be used}} to identify and remove the <b>duplicate</b> <b>records</b> from different data sources. Starting from the non duplicate set, the two cooperating classifiers, a Weighted Component Similarity Summing Classifier (WCSS) and Support Vector Machine (SVM) are used to iteratively identify the <b>duplicate</b> <b>records</b> from the non <b>duplicate</b> <b>record</b> and present a genetic programming (GP) approach to record deduplication. Their GP-based approach is also able to automatically find effective deduplication functions. The genetic programming approach is time consuming task so we propose new algorithm KFINDMR(KFIND using Most Represented data samples) to find the most represented data samples to improve the accuracy of the classifier. The proposed system calculates the mean value of the most represented data samples in centroid of the record members; it selects the first most represented data sample that closest to the mean value calculates the minimum distance. The system Remove the duplicate dataset samples in the system and find the optimization solution to deduplication of records or data samples...|$|R
5000|$|Cloudingo - a {{cloud-based}} SaaS, connects to salesforce.com and allows system administrators to scan their entire database for similar or <b>duplicate</b> <b>records.</b> Cloudingo {{was launched in}} late 2011. It {{is well known for}} its ease-of-use and rich user experience. Cloudingo was recently featured at the Dreamforce 2012 conference.|$|R
5000|$|In SQL the '''''' clause {{combines}} {{the results of}} two SQL queries into a single table of all matching rows. The two queries must result in {{the same number of}} columns and compatible data types in order to unite. Any <b>duplicate</b> <b>records</b> are automatically removed unless [...] is used.|$|R
40|$|Abstract. Given {{the rapid}} growth of data, it is {{important}} to extract, mine and discover useful information from databases and data warehouses. The process of data cleansing is crucial because of the &quot;garbage in, garbage out &quot; principle. &quot;Dirty &quot; data les are prevalent because of incorrect or missing data values, inconsistent value naming conventions, and incomplete information. Hence, we mayhave multiple records refering to the same real world entity. In this paper, we examine the problem of detecting and removing <b>duplicating</b> <b>records.</b> We present several e-cient techniques to pre-process the records before sorting them so that potentially matching records will be brought to a close neighbourhood. Based on these techniques, we implement a data cleansing system which can detect and remove more <b>duplicate</b> <b>records</b> than existing methods. ...|$|R
