32|64|Public
25|$|These {{included}} {{reading or}} writing single characters {{to the system}} console and reading or writing a sector of data from the disk. The BDOS handled some of the buffering of data from the diskette, but before CP/M 3.0 it assumed a disk sector size fixed at 128 bytes, as used on single-density 8-inch floppy disks. Since most 5.25-inch disk formats used larger sectors, the blocking and deblocking and the management of a <b>disk</b> <b>buffer</b> area was handled by model-specific code in the BIOS.|$|E
50|$|The <b>disk</b> <b>buffer</b> is {{physically}} distinct from {{and is used}} differently from the page cache typically kept by the operating system in the computer's main memory. The <b>disk</b> <b>buffer</b> {{is controlled by the}} microcontroller in the hard disk drive, and the page cache is controlled by the computer to which that disk is attached. The <b>disk</b> <b>buffer</b> is usually quite small, ranging between 8 and 256 MiB, and the page cache is generally all unused main memory. While data in the page cache is reused multiple times, the data in the <b>disk</b> <b>buffer</b> is rarely reused. In this sense, the terms disk cache and cache buffer are misnomers; the embedded controller's memory is more appropriately called <b>disk</b> <b>buffer.</b>|$|E
5000|$|... #Caption: On {{this hard}} disk drive, the {{controller}} board contains a RAM integrated circuit {{used for the}} <b>disk</b> <b>buffer.</b>|$|E
5000|$|BUFFERS (DOS 2.0 and DR DOS 3.31 and higher; OS/2) : Specifies {{the number}} of <b>disk</b> <b>buffers</b> to allocate.|$|R
50|$|Starting with DOS 5, DOS could {{directly}} {{take advantage}} of the HMA by loading its kernel code and <b>disk</b> <b>buffers</b> there via the DOS=HIGH statement in CONFIG.SYS. DOS 5+ also allowed the use of available UMBs via the DOS=UMB statement in CONFIG.SYS.|$|R
50|$|No {{permanent}} hardware damage {{results from}} executing the F00F instruction on a vulnerable system. Depending on the file system, operating system, and other circumstances, {{it is possible}} for data loss to occur if the <b>disk</b> <b>buffers</b> have not been flushed, if drives were interrupted during a write operation, or if some other non-atomic operation was interrupted.|$|R
50|$|The {{speed of}} the disk's I/O {{interface}} to the computer almost never matches {{the speed at which}} the bits are transferred to and from the hard disk platter. The <b>disk</b> <b>buffer</b> is used so that both the I/O interface and the disk read/write head can operate at full speed.|$|E
50|$|Consider a {{transaction}} that transfers 10 from A to B. First it removes 10 from A, then it adds 10 to B. At this point, the user is told the transaction was a success, however {{the changes are}} still queued in the <b>disk</b> <b>buffer</b> waiting {{to be committed to}} disk. Power fails and the changes are lost. The user assumes (understandably) that the changes persist.|$|E
5000|$|Server-grade {{disk array}} {{controllers}} often contain onboard <b>disk</b> <b>buffer,</b> {{and provide an}} option for a [...] "backup battery unit" [...] (BBU) to maintain the contents of this cache after power loss. If this battery is present, disk writes can be considered completed when they reach the cache, thus speeding up I/O throughput by not waiting for the hard drive. This operation mode is called [...] "write-back caching".|$|E
40|$|DBE is an {{experimental}} tool designed for trace driven simulation of processor caches and <b>disk</b> <b>buffers.</b> Trace driven simulation is flexible and requires no special hardware, but generating traces {{can be too}} slow and the resulting traces too large to handle. To overcome this problem, DBE uses a compile-time trace compaction, which can yield smaller traces and faster run times...|$|R
40|$|A {{critical}} {{problem with}} parallel I/O systems {{is the fact}} that disks consume a significant amount of energy. To design economically attractive and environmentally friendly parallel I/O systems, we propose an energy-aware prefetching strategy (PRE-BUD) for parallel I/O systems with <b>disk</b> <b>buffers.</b> We introduce a new architecture that provides significant energy savings for parallel I/O systems using <b>buffer</b> <b>disks</b> while maintaining high performance. There are two <b>buffer</b> <b>disk</b> configurations: (1) adding an extra <b>buffer</b> <b>disk</b> to accommodate prefetched data, and (2) utilizing an existing <b>disk</b> as the <b>buffer</b> <b>disk.</b> PRE-BUD is not only able {{to reduce the number of}} power-state transitions, but also to increase the length and number of standby periods. As such, PRE-BUD conserves energy by keeping data disks in the standby state for increased periods of time. Compared with the first prefetching configuration, the second configuration lowers the capacity of the parallel disk system. However, the second configuration is more cost-effective and energy-efficient than the first one. Finally, we quantitatively compare PRE-BUD with both disk configurations against three existing strategies. Empirical results show that PRE-BUD is able to reduce energy dissipation in parallel disk systems by up to 50 percent when compared against a non-energy aware approach. Similarly, our strategy is capable of conserving up to 30 percent energy when compared to the dynamic power management technique...|$|R
40|$|Abstractâ€”Cluster storage {{systems are}} {{essential}} {{building blocks for}} many high-end computing infrastructures. Although energy conservation techniques have been intensively studied {{in the context of}} clusters and disk arrays, improving energy efficiency of cluster storage systems remains an open issue. To address this problem, we describe in this paper an approach to implementing an energyefficient cluster storage system or ECOS for short. ECOS relies on the architecture of cluster storage systems in which each I/O node manages multiple disks- one <b>buffer</b> <b>disk</b> and several data disks. Given an I/O node, the key idea behind ECOS is to redirect disk requests from data <b>disks</b> to the <b>buffer</b> <b>disk.</b> To balance I/O load among I/O nodes, ECOS might redirect requests from one I/O node into the others. Redirecting requests is a driving force of energy saving, and the reason is two-fold. First, ECOS makes an effort to keep <b>buffer</b> <b>disks</b> active while placing data disks into standby in a long time period to conserve energy. Second, ECOS reduces the number of disk spin downs/ups in I/O nodes. The idea of ECOS was implemented in a Linux cluster, where each I/O node contains one <b>buffer</b> <b>disk</b> and two data disks. Experimental results show that ECOS improves the energy efficiency of traditional cluster storage systems where <b>buffer</b> <b>disks</b> are not employed. Adding one extra <b>buffer</b> <b>disk</b> into each I/O node seemingly has negative impact on energy saving. Interestingly, our results indicate that ECOS equipped with extra <b>buffer</b> <b>disks</b> is more energy efficient than the same cluster storage system without the <b>buffer</b> <b>disks.</b> The implication of the experiments is that using existing data disks in I/O nodes to perform as <b>buffer</b> <b>disks</b> can achieve even higher energy efficiency. I...|$|R
50|$|In {{computer}} storage, <b>disk</b> <b>buffer</b> (often ambiguously called {{disk cache}} or cache buffer) is the embedded memory {{in a hard}} disk drive (HDD) acting as a buffer between {{the rest of the}} computer and the physical hard disk platter that is used for storage. Modern hard disk drives come with 8 to 256 MiB of such memory, and solid-state drives come with up to 1 GB of cache memory.|$|E
5000|$|While the <b>disk</b> <b>buffer,</b> {{which is}} an {{integrated}} part of the hard disk drive, is sometimes misleadingly referred to as [...] "disk cache", its main functions are write sequencing and read prefetching. Repeated cache hits are relatively rare, due to {{the small size of}} the buffer in comparison to the drive's capacity. However, high-end disk controllers often have their own on-board cache of the hard disk drive's data blocks.|$|E
50|$|Unfortunately, {{the early}} DOS File Manager subverted this {{efficiency}} by copying bytes read from or written to a file {{one at a}} time between a <b>disk</b> <b>buffer</b> and main memory, requiring more time and resulting in DOS constantly blowing revs when reading or writing files. Programs became available early on to format disks with modified sector interleaves; these disks give DOS more time between sectors to copy the data, ameliorating the problem.|$|E
50|$|QVD can run virtual {{machines}} using KVM or LXC. KVM {{allows for}} a complete isolation between the host and the guest virtual machines. LXC on the other hand, runs the virtual machined inside isolated containers inside the host. That greatly reduces the CPU and memory requirements per user session as the kernel is able to share resources (i.e. <b>disk</b> <b>buffers)</b> between the containers more effectively.|$|R
50|$|SRAM is {{also used}} in {{personal}} computers, workstations, routers and peripheral equipment: CPU register files, internal CPU caches and external burst mode SRAM caches, hard <b>disk</b> <b>buffers,</b> router buffers, etc. LCD screens and printers also normally employ static RAM to hold the image displayed (or to be printed). Static RAM {{was used for the}} main memory of some early personal computers such as the ZX80, TRS-80 Model 100 and Commodore VIC-20.|$|R
5000|$|First, the DR DOS kernel and {{structures}} such as <b>disk</b> <b>buffers</b> can {{be located in the}} High Memory Area (HMA), the first 64 KB of extended memory which are accessible in real mode due to an incomplete compatibility of the 80286 with earlier processors. This freed up the equivalent amount of critical [...] "base" [...] or conventional memory, the first 640 KB of the PC's RAM - the area in which all MS-DOS applications run.|$|R
50|$|When {{executing}} a {{read from}} the disk, the disk arm moves the read/write head to (or near) the correct track, and after some settling time the read head begins to pick up bits. Usually, the first sectors to be read are not {{the ones that have}} been requested by the operating system. The disk's embedded computer typically saves these unrequested sectors in the <b>disk</b> <b>buffer,</b> in case the operating system requests them later.|$|E
5000|$|Later, programmers outside Apple rewrote the File Manager {{routines}} {{to avoid}} making the extra copy for most sectors of a file; RWTS was instructed to read or write sectors directly to or from main memory rather than from a <b>disk</b> <b>buffer</b> whenever a full sector was to be transferred. An early [...] "patch" [...] to provide this functionality was published in Call-A.P.P.L.E.. Speedups in the LOAD command of {{three to five times}} were typical.|$|E
50|$|These {{included}} {{reading or}} writing single characters {{to the system}} console and reading or writing a sector of data from the disk. The BDOS handled some of the buffering of data from the diskette, but before CP/M 3.0 it assumed a disk sector size fixed at 128 bytes, as used on single-density 8-inch floppy disks. Since most 5.25-inch disk formats used larger sectors, the blocking and deblocking and the management of a <b>disk</b> <b>buffer</b> area was handled by model-specific code in the BIOS.|$|E
5000|$|<b>Disk</b> system <b>buffer</b> = 4 kB (8 Ã— 2114 SRAM chips)Commodore {{device number}} {{selectable}} between 8 - 11 (on IC 7H : 6532) ...|$|R
50|$|One lesser-known {{feature of}} NCQ is that, unlike its ATA TCQ predecessor, {{it allows the}} host to specify whether {{it wants to be}} {{notified}} when the data reaches the disk's platters, or when it reaches the <b>disk's</b> <b>buffer</b> (on-board cache). Assuming a correct hardware implementation, this feature allows data consistency to be guaranteed when the disk's on-board cache is used in conjunction with system calls like fsync. The associated write flag, which is also borrowed from SCSI, is called Force Unit Access (FUA).|$|R
40|$|Abstract â€” Parallel disk systems {{consume a}} {{significant}} amount of energy due to the large number of disks. To design economically attractive and environmentally friendly parallel disk systems, in this paper we design and evaluate an energy-aware prefetching strategy for parallel disk systems consisting of a small number of <b>buffer</b> <b>disks</b> and large number of data <b>disks.</b> Using <b>buffer</b> <b>disks</b> to temporarily handle requests for data disks, we can keep data disks in the low-power mode as long as possible. Our prefetching algorithm aims to group many small idle periods in data disks to form large idle periods, which in turn allow data disks to remain in the standby state to save energy. To achieve this goal, we utilize <b>buffer</b> <b>disks</b> to aggressively fetch popular data from regular data <b>disks</b> into <b>buffer</b> <b>disks,</b> thereby putting data disks into the standby state for longer time intervals. A centrepiece in the prefetcing mechanism is an energy-saving prediction model, based on which we implement the energy-saving calculation module that is invoked in the prefetching algorithm. We quantitatively compare our energy-aware prefetching mechanism against existing solutions, including the dynamic power management strategy. Experimental results confirm that the buffer-disk-based prefetching can significantly reduce energy consumption in parallel disk systems by up to 50 percent. In addition, we systematically investigate the energy efficiency impact that varying disk power parameters has on our prefetching algorithm. Keywords-storage systems; energy-efficiency;prefetching I...|$|R
50|$|The disk's {{embedded}} microcontroller may {{signal the}} main computer that a disk write is complete immediately {{after receiving the}} write data, before the data is actually written to the platter. This early signal allows the main computer to continue working even though the data has not actually been written yet. This can be somewhat dangerous, because if power is lost before the data is permanently fixed in the magnetic media, the data will be lost from the <b>disk</b> <b>buffer,</b> and the file system on the disk may be left in an inconsistent state.|$|E
50|$|When {{compared}} to main memory, {{hard disk drive}} read/write speeds are low and random accesses require expensive disk seeks; as a result, larger amounts of main memory bring performance improvements as more data can be cached in memory. Separate disk caching is provided on the hardware side, by dedicated RAM or NVRAM chips located either in the disk controller (in which case the cache is integrated into a hard disk drive and usually called <b>disk</b> <b>buffer),</b> or in a disk array controller. Such memory {{should not be confused}} with the page cache.|$|E
5000|$|In {{order to}} {{minimize}} {{the number of times}} received data is copied, the receive buffer for payload data is received directly into a page aligned <b>disk</b> <b>buffer.</b> If the connection is encrypted, the buffer is decrypted in-place. The buffer is then moved into the disk cache without being copied. Once all the blocks for a piece have been received, or the cache needs to be flushed, all the blocks are passed directly to writev (...) to flush them in a single syscall. This means a single copy into user space memory, and a single copy back into kernel memory.|$|E
5000|$|... {{interface}} {{to various}} hardware (mouse, keyboard, bitmap frame <b>buffer,</b> <b>disk,</b> printer, network interface) ...|$|R
40|$|Advances in robotic {{devices and}} storage media now make it {{possible}} to design near-line automated storage systems. These systems aim to provide responsive performance to users of tertiary storage devices. The Jaquith system is a prototype archive server that lets network users archive their own files using automated storage. It provides semi-interactive file access to its clients by combining a high-density robotic tape system with disk-based indexing. Jaquith presents an FTP interface whereby whole files are moved between the client and its storage archive. Each clientâ€™s archive is separately governed to provide independent namespaces, added security, and parallel operation. A wildcard query mechanism lets users manipulate arbitrary subsets of their files. Two important aspects of the query system are abstracts, text tags that can be associated with files, and versions, date-stamps that are applied to archived files. Jaquith throughput is about 135 KB/second when archiving small (10 KB) user files to <b>disk</b> <b>buffers.</b> The use of synchronous disk writes by the server to ensure durability of each user file degrades throughput to 40 KB/second. The performance when writing <b>disk</b> <b>buffers</b> to Exabyte or Metrum tape is severely limited by the time to write a hardware filemark. Consequently, it is important to write several megabytes of data between filemarks for good performance. Report Documentation Page Form ApprovedOMB No. 0704 - 0188 Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information...|$|R
40|$|For {{admission}} control {{in real time}} multimedia systems, <b>buffer</b> space, <b>disk</b> bandwidth and network bandwidth must be considered. The CBR based mechanisms do not use system resources effectively, since media data is usually encoded with VBR compression techniques. We propose an {{admission control}} mechanism based on a VBR data model, that has a dynamic period length. In our mechanism the period can be adaptively changed to maximize the performance considering both <b>disk</b> bandwidth and <b>buffer</b> space. To compare the performance, extensive simulations are conducted on RR, SCAN, and GSS schemes which have the dynamic period length and the static period length. Keywords : Admission control, video-on-demand systems, variable bit rate, <b>disk</b> scheduling, <b>buffer</b> management 1 Introduction Multimedia systems like VOD (Video-On-Demand) systems require considerable resources and have tight real-time constraints. Multimedia data {{to be sent to}} clients must be read from <b>disk</b> to memory <b>buffers</b> before [...] ...|$|R
5000|$|The first VTL {{solution}} was an IBM Virtual Tape Server (VTS) introduced in 1997. It was targeted for a mainframe market, where many legacy applications {{tend to use}} a lot of very short tape volumes. It used ESCON interface, and acted as a disk cache for 3494 tape library. A competitive offering from StorageTek (acquired in 2005 by SUN Microsystems, then subsequently by Oracle Corporation) was known as Virtual Storage Manager (VSM) which leveraged the market dominant STK Powderhorn library as a back store. Each product line has been enhanced to support larger <b>disk</b> <b>buffer</b> capacities, FICON, and more recently (c. 2010) [...] "tapeless" [...] disk-only environments.|$|E
5000|$|In 2001, Western Digital {{became the}} first {{manufacturer}} to offer mainstream ATA hard disk drives with 8 MiB of <b>disk</b> <b>buffer.</b> At that time, most desktop hard disk drives had 2 MB of buffer. WDC labeled these 8 MB models as [...] "Special Edition" [...] and distinguished them with the JB code (the 2 MB models had the BB code). The first 8 MB cache drive was the 100 GB WD1000JB, followed by other models starting with 40 GB capacity. WDC advertised the JB models for cost-effective file servers. In October 2001, WD restated its prior year results to reflect the adoption of SEC Staff Accounting Bulletin No.101 and the reclassification of Connex and SANavigator results as discontinued operations.|$|E
5000|$|There is a {{trade-off}} between {{the amount of}} time spent figuring out the best query plan and the quality of the choice; the optimizer may not choose the best answer on its own. Different qualities of database management systems have different ways of balancing these two. Cost-based query optimizers evaluate the resource footprint of various query plans and use this as the basis for plan selection. These assign an estimated [...] "cost" [...] to each possible query plan, and choose the plan with the smallest cost. Costs are used to estimate the runtime cost of evaluating the query, in terms of the number of I/O operations required, CPU path length, amount of <b>disk</b> <b>buffer</b> space, disk storage service time, and interconnect usage between units of parallelism, and other factors determined from the data dictionary. The set of query plans examined is formed by examining the possible access paths (e.g., primary index access, secondary index access, full file scan) and various relational table join techniques (e.g., merge join, hash join, product join). The search space can become quite large depending on the complexity of the SQL query. There are two types of optimization. These consist of logical optimizationâ€”which generates a sequence of relational algebra to solve the queryâ€”and physical optimizationâ€”which is used to determine the means of carrying out each operation.|$|E
5000|$|In SCSI and in SATA with Native Command Queuing (but not {{in plain}} ATA, even with TCQ) the host can specify whether {{it wants to}} be {{notified}} of completion when the data hits the disk's platters or when it hits the <b>disk's</b> <b>buffer</b> (on-board cache). Assuming a correct hardware implementation, this feature allows the disk's on-board cache to be used while guaranteeing correct semantics for system calls like [...] This hardware feature is called Force Unit Access (FUA) and it allows consistency with less overhead than flushing the entire cache as done for ATA (or SATA non-NCQ) disks. Although Linux enabled NCQ around 2007, it did not enable SATA/NCQ FUA until 2012, citing lack of support in the early drives.|$|R
5000|$|Test device read {{performance}} speed (-t for timing <b>buffered</b> <b>disk</b> reads) of {{the first}} hard drive: sudo hdparm -t /dev/sda ...|$|R
5000|$|Sixty 10-digit {{words of}} {{magnetic}} core memory at addresses 9000 to 9059; a small fast memory (this device gave a memory access time of 96Âµs, a 26-fold raw improvement {{relative to the}} rotating drum), needed for a tape and <b>disk</b> I/O <b>buffer.</b> (5 extra operation codes) ...|$|R
