2|10000|Public
40|$|Abstract — {{this paper}} {{presents}} a novel image feature extraction and recognition method two dimensional linear discriminant analysis (2 DLDA) in a much smaller subspace. Image representation and recognition based on the Fisher’s criterion is statistically dependent on {{the evaluation of the}} covariance matrices. Since the proposed approach computes the covariance matrices in a subspace of the input space, the optimal discriminant vectors are more accurately obtained. Furthermore, the proposed method is based on 2 D image matrices rather than 1 D vector so the image {{does not need to be}} transformed into a vector prior to feature extraction. This leads to the following benefits; the proposed method yields greater recognition accuracy while reduces the overall computational complexity. Finally, the effectiveness of the proposed algorithm is verified using the ORL database as a benchmark. The new algorithm achieves a recognition rate of 95. 50 % compared to the recognition rate of 90. 00 % for the Fisherface method. The experimental results also indicates that the <b>extraction</b> <b>of</b> <b>image</b> <b>feature</b> is approximately 84. 4 times computationally more efficient using the new method than the popular Fisherface. I...|$|E
40|$|With the {{popularity}} of geospatial applications, database updating is getting important due to the environmental changes over time. Imagery provides a lower cost and efficient way to update the database. Three dimensional objects can be measured by space intersection using conjugate image points and orientation parameters of cameras. However, precise orientation parameters of light amateur cameras are not always available due to their costliness and heaviness of precision GPS and IMU. To automatize data updating, the correspondence of object vector data and image may be built to improve the accuracy of direct georeferencing. This study contains four major parts, (1) back-projection of object vector data, (2) <b>extraction</b> <b>of</b> <b>image</b> <b>feature</b> lines, (3) object-image feature line matching, and (4) line-based orientation modeling. In order to construct the correspondence of features between an image and a building model, the building vector features were back-projected onto the image using the initial camera orientation from GPS and IMU. Image line features were extracted from the imagery. Afterwards, the matching procedure was done by assessing the similarity between the extracted image features and the back-projected ones. Then, the fourth part utilized line features in orientation modeling. The line-based orientation modeling was performed by the integration of line parametric equations into collinearity condition equations. The experiment data included images with 0. 06 m resolution acquired by Canon EOS Mark 5 D II camera on a Microdrones MD 4 - 1000 UAV. Experimental results indicate that 2. 1 pixel accuracy may be reached, which is equivalent to 0. 12 m in the object space...|$|E
30|$|In {{the last}} few years, the term “radiomics” has emerged in the imaging {{community}} as a novel field of research, defined by Lambin et al. as a “high-throughput <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> from radiographic images” [1].|$|R
30|$|Extending the MRF model, Kumar and Hebert {{proposed}} a discriminative random field (DRF) model that includes neighborhood interactions {{in the class}} labels, {{as well as at}} the observation level. They apply the DRF model to the segmentation of man-made structures in natural scenes [1], with an <b>extraction</b> <b>of</b> <b>images</b> <b>features</b> based on a grid of blocks that fully covers the image. The DRF model is trained on a set <b>of</b> manually segmented <b>images,</b> and then used to infer the segmentation into the two target classes.|$|R
30|$|Final output image time-frequency {{composite}} weighted image signalWuu(a,[*]b). Therefore, {{compared with}} the traditional time-domain, c <b>extraction</b> technique <b>of</b> <b>image</b> <b>features</b> can be better realized by the time-frequency composite weighting algorithm.|$|R
30|$|In this paper, the {{multimedia}} retrieval technology is studied {{with the image}} as the research object, {{as well as the}} wavelet decomposition <b>of</b> <b>images,</b> <b>extraction</b> <b>of</b> <b>image</b> <b>features,</b> comparison <b>of</b> the effect of different wavelet bases on the recognition results, and the effect of different decomposition layers on the recognition results in the retrieval and analysis process. Through the recognition results of face, vehicle, building, and landscape images, the optimal wavelet basis function and the optimal number of layers are selected, and an image retrieval model based on wavelet decomposition is established.|$|R
40|$|Abstract. In {{comparison}} with traditional types of databases, dealing with large multimedia databases brings many new challenges, like semantics assignment, {{the choice of}} an adequate similarity measure and data indexing. In this paper, we will discuss several approaches to the solution of these problems. In this respect, we will focus in particular on content-based image retrieval methods and their merits in {{comparison with}} the more traditional text-based approach. The main challenges include an improved <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> from the data and efficient indexing of the extracted high-dimensional features...|$|R
40|$|The paper {{presents}} algorithms for the <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> {{which are}} invariant {{with respect to}} the action of a transformation group on the image data. We focus on the parallelization of these algorithms. The occuring data dependencies are typical for a whole class of algorithms. The resulting communication demands can be characterized by a set of parameters. According to the values of these parameters it is possible to find well suited topologies for the communication network. We analyze embeddings of these topologies into a fixed network structure and devise suitable data distributions...|$|R
40|$|Detection of Antarctic clouds is {{important}} because of their strong radiation influence on energy balance of the snow and ice surface. In this paper, a method to classify cloud, sea ice and ground is proposed. This study is based upon analysis of the NOAA/AVHRR infrared images in Antarctica. The algorithm consists of two major approaches : <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> and a classification algorithm. A minimum distance classifier was used to classify that region into one of three categories using five <b>image</b> <b>features.</b> To reduce the error rate of the classification, threshold boundaries for minimum distance classifiers have been changed. Both classified and misclassified areas were decreased with increased threshold levels...|$|R
40|$|Satellite image {{processing}} {{is a complex}} task that has received considerable attention from many researchers. In this paper, an interactive image query system for satellite imagery searching and retrieval is proposed. Like most <b>image</b> retrieval systems, <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> {{is the most important}} step that has a great impact on the retrieval performance. Thus, a new technique that fuses color and texture features for segmentation is introduced. Applicability of the proposed technique is assessed using a database containing multispectral satellite imagery. The experiments demonstrate that the proposed segmentation technique is able to improve quality of the segmentation results as well as the retrieval performance. <br /...|$|R
40|$|Abstract- Thinning is {{essentially}} a “pre-processing ” step in many applications <b>of</b> digital <b>image</b> processing, computer vision, and pattern recognition. In many computer vision applications, the images interested in a scene can be characterized by structures composed of line or curve or arc patterns for shape analysis. It is used to compress the input data and expedite the <b>extraction</b> <b>of</b> <b>image</b> <b>features.</b> In this paper three different thinning algorithms are applied for MRI Brain Images to estimate performance evaluation metrics <b>of</b> thinned <b>images.</b> Image thinning reduces {{a large amount of}} memory usage for structural information storage. Experimental result shows the performance of the proposed algorithm. Index Terms- FCD, Parallel thinning algorithm, Skeleton, Performance Metrics, and MRI Images. ...|$|R
40|$|Abstract. <b>Extraction</b> <b>of</b> <b>image</b> <b>features</b> is {{a crucial}} step in many image {{analysis}} tasks. In feature extraction methods Gaussian derivative kernels are frequently utilized. Blurring <b>of</b> the <b>image</b> due to convolution with these kernels gives rise to feature measures different from the intended value in the original image. We propose {{to solve this problem}} by explicitly modeling the scale dependency of derivatives combined with measurement of derivatives at multiple scales. This approach is illustrated in methods for feature measurement in curvilinear structures. Results in 3 D Confocal Images confirm that modelling of scale behavior of derivatives results in improved methods for center line localization in curved line structures and enables curvature and diameter measurement...|$|R
40|$|Abstract: Edge {{detection}} {{refers to}} the process of locating edges in any given image. Edge detection is considered as primary image artifacts for the <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> and segmenting objects using low level image processing techniques. This step lays the foundation of computer vision and Image processing which are the most advanced branches of science in modern world. Due to this reason there always has been efforts to improve the techniques of edge detection. The edge detection is an important step to perform target tracking, object recognition, data compression, image reconstruction and segmentation. This paper includes the explanation of canny edge detectors and its comparison and preference over other edge detectors...|$|R
40|$|In this paper, two-dimensionalprincipal {{component}} analysis (2 DPCA) is used forimage representation and recognition. Compared to 1 D PCA, 2 DPCA {{is based on}} 2 D image matricesrather than 1 D vectors so the image matrix does notneed to {{be transformed into a}} vector prior to featureextraction. Instead, an image covariance matrix isconstructed directly using the original imagematrices, and its eigenvectors are derived for imagefeature extraction. In order to test the approach, wehave used ORL face database images. Therecognition rate across all trials was higher using 2 DPCA than PCA. The experimental results showsthat this approach <b>of</b> <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> iscomputationally more efficient using 2 DPCA thanPCA. It is also observed from the results that therecognition rate is high...|$|R
40|$|In this paper, {{we present}} a {{parallel}} implementation <b>of</b> an <b>image</b> <b>feature</b> <b>extraction</b> task on Connection Machine CM- 5. We show that, given a 2048 2048 grey level image as input, the <b>extraction</b> <b>of</b> <b>image</b> <b>features,</b> which includes edge detection, thinning, linking, and linear approximation, can be performed in less than 1. 2 seconds on a partition of CM- 5 having 512 processing nodes. A serial implementation on a Sun Sparc 400 takes more than 8 minutes. Experimental results on various sizes <b>of</b> <b>images</b> using various partitions of CM- 5 are alsoreported. The software hasbeen developed inamodular fashion to permit various techniques to be employed for the individual steps of the processing. Our technique starts by modeling the communication and computation features of the machine. Using this model, scalable algorithms are designed. ...|$|R
40|$|Abstract. With {{the rapid}} {{development}} of Internet, {{more and more}} enterprises establish business sites to achieve the purpose of online transactions. Taking taobao. com for example, {{hundreds of millions of}} goods trade on the trading platform. In front of the huge commodity <b>image</b> database, <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> is very convenient for people to find out <b>images</b> <b>of</b> user requirement. This paper focus mainly on the color <b>feature</b> <b>of</b> <b>images.</b> Firstly, we segment ROI <b>of</b> <b>images</b> using grabCut algorithm; secondly, we extract primary color <b>of</b> <b>images</b> by using dominant color descriptor of MPEG 7; Thirdly, we adopt RGB color quantization to quantize the primary color. Finally achieve the purpose <b>of</b> <b>image</b> color navigation. I have done experiment to compare with some other methods, and find that the algorithms I adopted make a better performance...|$|R
40|$|The tensor {{representation}} {{has proven}} a successful tool {{as a mean to}} describe local multi-dimensional orientation. In this respect, the tensor representation is a map {{from the local}} orientation to a second order tensor. This paper investigates how variations of the orientation are mapped to variation of the tensor, thereby giving an explicit equivariance relation. The results may be used in order to design tensor based algorithms for <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> defined in terms of local variations of the orientation, e. g. multidimensional curvature or circular symmetries. It is assumed that the variation of the local orientation can be described in terms of an orthogonal transformation group. Under this assumption a corresponding orthogonal transformation group, acting on the tensor, is constructed. Several correspondences between the two groups are demonstrated. ...|$|R
40|$|ABSTRACT—In this paper, {{two-dimensional}} {{principal component}} analysis (2 DPCA) is used for image representation and recognition. Compared to 1 D PCA, 2 DPCA is based on 2 D image matrices rather than 1 D vectors so the image matrix {{does not need to}} be transformed into a vector prior to feature extraction. Instead, an image covariance matrix is constructed directly using the original image matrices, and its eigenvectors are derived for <b>image</b> <b>feature</b> extraction. In order to test the approach, we have used ORL face database images. The recognition rate across all trials was higher using 2 DPCA than PCA. The experimental results shows that this approach <b>of</b> <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> is computationally more efficient using 2 DPCA than PCA. It is also observed from the results that the recognition rate is high. Index Terms—Principal Component Analysis (PCA), Eigenfaces, feature extraction, imag...|$|R
40|$|In this paper, a new {{technique}} coined two-dimensional principal component analysis (2 DPCA) is developed for image representation. As opposed to PCA, 2 DPCA is based on 2 D image matrices rather than 1 D vectors so the image matrix {{does not need to}} be transformed into a vector prior to feature extraction. Instead, an image covariance matrix is constructed directly using the original image matrices, and its eigenvectors are derived for <b>image</b> <b>feature</b> extraction. To test 2 DPCA and evaluate its performance, a series of experiments were performed on three face image databases: ORL, AR, and Yale face databases. The recognition rate across all trials was higher using 2 DPCA than PCA. The experimental results also indicated that the <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> is computationally more efficient using 2 DPCA than PCA. Department of Computin...|$|R
40|$|Image {{registration}} (IR) aims to geometrically match {{one image}} to another. It is extensively {{used in many}} imaging applications. Among many existing IR methods, one widely used group of methods are feature-based. By a feature-based method, a number <b>of</b> relevant <b>image</b> <b>features</b> are first extracted from the two images, respectively, and then a geometric matching transformation is found to best match {{the two sets of}} features. However, proper identification and <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> {{turns out to be a}} challenging task. Generally speaking, a good <b>image</b> <b>feature</b> extraction method should have the following two properties: (i) the identified <b>image</b> <b>features</b> should provide us proper information to approximate the geometric matching transformation accurately, and (ii) they should be easy to identify by a computer algorithm so that the entire feature extraction procedure is computer automatic. In this paper, a new type <b>of</b> <b>image</b> <b>features</b> is studied, which has the two properties described above. Together with the widely used thin platespline(TPS) geometrictransformationmodel,itisshownthatourfeature-basedIRmethod works effectively in various cases...|$|R
40|$|With {{respect to}} road crash statistics, {{on-board}} pedestrian detection {{is a key}} task for future advanced driver assistance systems. In this paper, we describe {{the implementation of a}} real-time pedestrian recognition system that combines FPGA-based <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> with a CPU-based object localization and classification framework. In terms of features, we have implemented the Histograms of Oriented Gradients (HOG) descriptor that is state-of-the-art in the field of human detection from a moving camera. While past HOG-related publications presented simplified FPGA-based HOG variants, often sacrificing classification performance, we implemented the original descriptor with minor modifications on dedicated hardware. Evaluation on the INRIA pedestrian database shows potential for deploying the system in practice. The descriptor computation runs on a PCIe frame grabber with embedded FPGA that can be directly integrated into an automotive computer of a test vehicle for evaluation purposes...|$|R
40|$|Abstract—In this paper, a new {{technique}} coined two-dimensional principal component analysis (2 DPCA) is developed for image representation. As opposed to PCA, 2 DPCA is based on 2 D imagematrices rather than 1 D vectors so the image matrix {{does not need to}} be transformed into a vector prior to feature extraction. Instead, an image covariance matrix is constructed directly using the original image matrices, and its eigenvectors are derived for <b>image</b> <b>feature</b> extraction. To test 2 DPCA and evaluate its performance, a series of experiments were performed on three face image databases: ORL, AR, and Yale face databases. The recognition rate across all trials was higher using 2 DPCA than PCA. The experimental results also indicated that the <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> is computationally more efficient using 2 DPCA than PCA. Index Terms—Principal Component Analysis (PCA), Eigenfaces, <b>feature</b> extraction, <b>image</b> representation, face recognition. ...|$|R
40|$|In this paper, {{we present}} a fast {{parallel}} implementation <b>of</b> an <b>image</b> <b>feature</b> <b>extraction</b> task on Connection Machine CM- 5. We show that, given a 2048 Θ 2048 grey level image as input, the <b>extraction</b> <b>of</b> <b>image</b> <b>features,</b> which includes edge detection, thinning, linking, and linear approximation, can be performed in less than 1. 2 seconds on a partition of CM- 5 having 512 processing nodes. A serial implementation written in C on a Sun Sparc 400 takes more than 8 minutes. Experimental results on various sizes <b>of</b> <b>images</b> using various partitions of CM- 5 are also reported. The software has been developed in a modular fashion to permit various techniques to be employed for the individual steps of the processing. Our technique starts by modeling the communication and computation features of the machine. Using this model, scalable algorithms are designed. 1 Introduction <b>Image</b> <b>feature</b> extraction is a fundamental process in vision. In general terms, the primary task <b>of</b> feature <b>extraction</b> is to ex [...] ...|$|R
40|$|This paper {{presents}} an original {{approach to the}} problem of camera calibration using a calibration pattern. It consists of directly searching for the camera parameters that best project three-dimensional points of a calibration pattern onto intensity edges in an <b>image</b> <b>of</b> this pattern, without explicitly extracting the edges. Based on a characterization <b>of</b> <b>image</b> edges as maxima of the intensity gradient or zero-crossings of the Laplacian, we express the whole calibration process as a one-stage optimization problem. A classical iterative optimization technique is used in order to solve it. Contrary to classical calibration techniques which involve two consecutive stages (<b>extraction</b> <b>of</b> <b>image</b> <b>features</b> and computation <b>of</b> the camera parameters), our approach does not require any customized feature extraction code. As a consequence, it can be directly used with any calibration pattern that produces image edges, and it is also more robust. First, we describe the details of the approach. The [...] ...|$|R
40|$|The Cresceptron is a multilayer, {{hierarchical}} network of retinotopic planes used for two-dimensional pattern recognition. The lower {{layers of the}} network recognize small features, and upper layers recognize successively larger combinations of these features, up to the size <b>of</b> the original <b>image.</b> Training is performed by hierarchically analyzing the input <b>image</b> for new <b>features</b> and creating new planes for these features at all layers of the network. Training for each input is completed in one pass. In our experiments, we studied {{the behavior of the}} Cresceptron through its application to character recognition. Specically, we examined the use of various transfer functions, the <b>extraction</b> <b>of</b> <b>image</b> <b>features,</b> network growth, and the reuse of existing feature planes. After examining oversaturation problems with similar inputs, we found in our experiments that Gaussian or triangle functions gave better classication results than sigmoidal or soft limiting saturation functions. In our study of [...] ...|$|R
40|$|The high {{historical}} and preciously wooden statues of Buddha, have received serious damages by cracking, cor-rosion, wormholes, etc., {{because of the}} quality of the mate-rials of woods. For the restoration of such damaged statues, the advance inspections of their interior conditions without destroying them are demanded. In this paper, we meet such a demand by taking the X-ray CT images and by applying some techniques <b>of</b> <b>image</b> analysis for <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> which are useful for the restoration. Especially, we focus on wormhole re-gions which are the most serious damages. We first present a method of the region segmentation and then consider ex-traction of a 3 D graph structure of wormhole regions. Since each node of the graph possesses the geometric informa-tion such as a location, and a diameter of each wormhole, we show that the graph structures are useful for visualizing wormholes. 1...|$|R
40|$|Shadows {{appear in}} remote sensing images due to {{elevated}} objects. Shadows cause hindrance to correct <b>feature</b> <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> like buildings,towers etc. {{in urban areas}} it may also cause false color tone and shape distortion of objects, which degrades the quality <b>of</b> <b>images.</b> Hence, {{it is important to}} segment shadow regions and restore their information for image interpretation. This paper presents an efficient and simple approach for shadow detection and removal based on HSV color model in complex urban color remote sensing images for solving problems caused by shadows. In the proposed method shadows are detected using normalized difference index and subsequent thresholding based on Otsu’s method. Once the shadows are detected they are classified and a non shadow area around each shadow termed as buffer area is estimated using morphological operators. The mean and variance of these buffer areas are used to compensate the shadow regions...|$|R
30|$|Battiato et al. [5] have {{proposed}} {{to deal with}} the problem of red eye detection by using the bag-of-keypoints paradigm. It involves <b>extraction</b> <b>of</b> local <b>image</b> <b>features,</b> quantization <b>of</b> the feature space into a codebook through clustering, and <b>extraction</b> <b>of</b> codeword distribution histograms. An SVM classifier has been used to decide to which class each histogram, thus each patch, belongs.|$|R
40|$|Image {{registration}} is used widely in applications for mapping one image to another. Existing image registration methods are either feature-based or intensity-based. Feature-based methods first extract relevant <b>image</b> <b>features,</b> {{and then find}} a geometrical transformation that best matches the two corresponding sets of features extracted from the two images. Because identification and <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> is often a challenging and time-consuming process, intensitybased image registration, by which the mapping transformation is estimated directly from the observed <b>image</b> intensities <b>of</b> the two <b>images,</b> has received much attention recently. In the literature, most existing intensity-based image registration methods require a parametric form of the mapping transformation, which is restrictive for certain applications. In this paper, we propose an intensity-based image registration method without requiring such a parametric form. By this method, the mapping transformation can be nonparametric, and it can even be discontinuous at certain places in the design space. Numerical examples show that it is effective in various applications...|$|R
40|$|Abstract. In this paper, {{we present}} an {{efficient}} approach for unsupervised segmenta-tion {{of natural and}} textural images based on the <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> and a fast active contour segmentation model. We {{address the problem of}} textures where neither the gray-level information nor the boundary information is adequate for object extraction. This is often the case <b>of</b> natural <b>images</b> composed <b>of</b> both homogeneous and textured regions. Because these images cannot be in general directly processed by the gray-level information, we propose a new texture descriptor which intrinsically defines the geometry of textures using semi-local image information and tools from differen-tial geometry. Then, we use the popular Kullback-Leibler distance to design an active contour model which distinguishes the background and textures of interest. The exis-tence of a minimizing solution to the proposed segmentation model is proven. Finally, a texture segmentation algorithm based on the Split-Bregman method is introduced to extract meaningful objects in a fast way. Promising synthetic and real-world results for gray-scale and color images are presented...|$|R
40|$|We {{propose a}} {{scalable}} and fexible hardware architecture for the <b>extraction</b> <b>of</b> <b>image</b> <b>features,</b> {{used in conjunction}} with an attentional cascade classifier for appearance-based object detection. Individual feature processors calculate feature-values in parallel, using parameter-sets and image data that is distributed via BRAM buffers. This approach can provide high utilization- and throughput-rates for a cascade classifier. Unlike previous hardware implementations, we are able to flexibly assign feature processors to either work on a single- or multiple image windows in parallel, depending on the complexity of the current cascade stage. The core of the architecture was implemented {{in the form of a}} streaming based FPGA design, and validated in simulation, synthesis, as well as via the use of a Logic Analyser for the verification of the on-chip functionality. For the given implementation, we focused on the design of Haar-like feature processors, but feature processors for a variety of heterogenous feature types, such as Gabor-like features, can also be accomodated by the proposed hardware architecture...|$|R
40|$|Feature {{analysis}} {{plays an}} important role in many multispectral image applications and scale invariant feature transform (SIFT) has been successfully applied for <b>extraction</b> <b>of</b> <b>image</b> <b>features.</b> However, the existing SIFT algorithms cannot extract <b>features</b> from multispectral <b>images</b> directly. This paper puts forward a novel algorithmic framework based on the SIFT for multispectral images. Firstly, with the theory of the geometric algebra (GA), a new representation <b>of</b> multispectral <b>image</b> including spatial and spectral information is put forward and discussed. Secondly, a new method for obtaining the scale space <b>of</b> the multispectral <b>image</b> is proposed. Thirdly, following the procedures of the SIFT, the GA based difference <b>of</b> Gaussian <b>images</b> are computed and the keypoints can be detected in the GA space. Fourthly, the feature points are finally detected and described in the mathematical framework of the GA. Finally, the comparison results show that the GA-SIFT outperforms some previously reported SIFT algorithms in the feature extraction from a multispectral image, and it is comparable with its counterparts in the <b>feature</b> <b>extraction</b> <b>of</b> color <b>images,</b> indicating good performance in various applications <b>of</b> <b>image</b> analysis. (C) 2013 Elsevier Inc. All rights reserved...|$|R
40|$|Abstract. Image {{processing}} and analysis based on the continuous or discrete image transforms are classic techniques. The image transforms are widely used in image filtering, data description, etc. Nowadays the wavelet theorems make up very popular methods <b>of</b> <b>image</b> processing, denoising and compression. Considering that the Haar functions are the simplest wavelets, these forms are used in many methods <b>of</b> discrete <b>image</b> transforms and processing. The image transform theory is a well known area characterized by a precise mathematical background, {{but in many cases}} some transforms have particular properties which are not still investigated. This paper for the first time presents graphic dependences between parts of Haar and wavelets spectra. It also presents a method <b>of</b> <b>image</b> analysis by means of the wavelets–Haar spectrum. Some properties of the Haar and wavelets spectrum were investigated. The <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> immediately from spectral coefficients distribution were shown. In this paper it is presented that two–dimensional both, the Haar and wavelets functions products man be treated as extractors <b>of</b> particular <b>image</b> <b>features.</b> Furthermore, it is also shown that some coefficients from both spectra are proportional, which simplify slightly computations and analyses...|$|R
40|$|Content-based video/image {{indexing}} and retrieval is {{of paramount}} importance in modern multimedia applications mainly due to the large amount of information involved. Querying by visual example, where the user provides an example image, and querying by sketch, employing user-constructed sketches, are important options in such systems. Prototype systems providing content-based image query and retrieval capabilities have been reported in literature, enabling efficient browsing in multimedia databases. In these systems, content information is modeled in terms of color, texture and shape attributes. However, existing techniques on image similarity retrieval, based on general color attributes, as well as localized color, have generally proved not to yield adequate results. Present work focuses on the <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> in terms <b>of</b> their color composition information using a graph-based approach. This approach inherently captures local color information, and provides improved results in both the case <b>of</b> seeking <b>images</b> similar to an example image and the case <b>of</b> <b>images</b> containing a part similar to the image at hand. Experiments have been included to verify the efficiency of the algorithm. The proposed technique is evaluated in the framework of the Esprit project under development DiVAN...|$|R
40|$|What is image restoration? Image Restoration {{refers to}} a class of methods that aim to remove or reduce the degradations that have {{occurred}} while the digital image was being obtained. All natural images when displayed have gone through some sort of degradation: during display mode acquisition mode, or processing mode The degradations {{may be due to}} sensor noise blur due to camera misfocus relative object-camera motion random atmospheric turbulence others In most <b>of</b> the existing <b>image</b> restoration methods we assume that the degradation process can be described using a mathematical model. How well can we do? Depends on how much we know about the original image the degradations (how accurate our models are) Image restoration and image enhancement-differences: Image restoration differs from image enhancement in that the latter is concerned more with accentuation or <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> rather than restoration <b>of</b> degradations. <b>Image</b> restoration problems can be quantified precisely, whereas enhancement criteria are difficult to represent mathematically. Image observation models Typical parts of an imaging system: image formation system, a detector and a recorder. A general model for such a system could be...|$|R
40|$|Edge {{detection}} is {{the most}} important <b>feature</b> <b>of</b> <b>image</b> processing for object detection, it is crucial to have a good understanding of edge detection algorithms/operators. Computer vision is rapidly expanding field that depends on the capability to perform faster segments and thus to classify and infer images. Segmentation is central to the successful <b>extraction</b> <b>of</b> <b>image</b> <b>features</b> and their ensuing classification. Powerful segmentation techniques are available; however each technique is ad hoc. In this paper, the computer vision investigates the sub regions <b>of</b> the composite <b>image,</b> brings out commonly used and most important edge detection algorithms/operators with a wide-ranging comparative along with the statistical approach. This paper implements popular algorithms such as Sobel, Roberts, Prewitt, Laplacian of Gaussian and canny. A standard metric is used for evaluating the performance degradation of edge detection algorithms as a function of Peak Signal to Noise Ratio (PSNR) along with the elapsed time for generating the segmented output image. A statistical approach to evaluate the variance among the PSNR and the time elapsed in output image is also incorporated. This paper provides a basis for objectively comparing the performance of different techniques and quantifies relative noise tolerance. Results shown allow selection of the most optimum method for application to image...|$|R
