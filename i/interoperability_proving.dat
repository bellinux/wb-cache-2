0|17|Public
5000|$|True and <b>proven</b> <b>interoperability</b> between {{manufacturers}} {{dictated by}} an open standard ...|$|R
40|$|MIFARE {{secure access}} module SAM AV 21. General {{description}} The NXP MIFARE SAM AV 2 hardware solution {{is the ideal}} add-on for reader devices offering additional security services. Supporting TDEA, AES and RSA capabilities, it offers secure storage and secure communication {{in a variety of}} infrastructures. Unlike other products in the field, MIFARE SAM AV 2 has <b>proven</b> <b>interoperability</b> with all o...|$|R
40|$|The {{development}} of critical systems {{is becoming more}} and more complex. The overall tendency is that development costs raise. In order to cut cost of development, companies are forced to build systems from proven components and larger new systems from smaller older ones. Respective reuse activities involve good number of people, tools and processes along different stages of the development lifecycle which involve large numbers of tools. Some development is directly planned for reuse. Planned reuse implies excellent knowledge management and firm governance of reusable items. According to the current state of the art, there are still practical problems in the two fields, mainly because the governance and knowledge management is fragmented over the tools of the toolchain. In our experience, the practical effect of this fragmentation is that involved ancestor and derivation relationships are often undocumented or not exploitable. Additionally, useful reuse is almost always dealing with heterogeneous content which must be transferred from older to newer development environments. In this process, <b>interoperability</b> <b>proves</b> either as biggest obstacle or greatest help. In this paper, authors connect the topics interoperability and knowledge management and propose to seek for ubiquitous reuse via advanced interoperability features. A single concept from a larger Technical Interoperability Concept (TIC) with the name bubble is presented. Bubbles are expected to overcome existing barriers to cost-efficient reuse in systems and software development lifecycles. That is why, the present paper introduces and defines bubbles by showing how they simplify application of repairs and changes and hence contribute to expansion of reuse at reduced cost. Comment: 24 pages, 18 figure...|$|R
50|$|WHATWG {{expects to}} {{maintain}} a living standard and stop using version numbers. W3C’s HTMLWG leaves the WHATWG spec in order to stick producing snapshots that provide implementors (like browser makers and web developers) with a stable, solid spec. Because the W3C version has more regulations (known as P3P) for implementation, by <b>proving</b> <b>interoperability,</b> it works much more slowly than the WHATWG. On the other hand, however, it ensures the accuracy and stability.|$|R
40|$|Abstract. Techniques {{based on}} sets have proved useful in {{many types of}} {{information}} systems. In databases the relational model has maintained wide dominance in business data processing. However, interoperability between different databases, even when based on the relational model, is proving a major problem. Sets satisfy first-order predicate logic which as consistent and complete has many advantages in practical application. Interoperability requires higher order logic as the arguments themselves are relations and functions. Higher order logic {{in the context of}} set theory behaves less satisfactorily according to Gödel’s theorems as such logic cannot satisfy all three of soundness, completeness and effectiveness. This may be a fundamental reason why <b>interoperability</b> is <b>proving</b> to be so difficult. This paper looks at underlying problems and suggests that they may be avoided by the use of categorial higher order logic. Cartesian categories are complete, consistent and decidable. They can be employed as an engineering technique to construct a general architecture of interoperability. ...|$|R
40|$|Abstract. Now {{dispatch}} centers {{needs to}} do a lot of work configuration and spend a lot of time to debug to communicate with substations. This is not only time-consuming but also error prone, so the alarm information needs to be directly sent. This paper presents the design and implementation of Alarm information directly transfers system, gives the system structure and software architecture of AIDT system used in UHVDC project; getting SOE based on distributed component Ice, realizing the protocol based on finite-state machine. And combined with the actual example, this paper provides a detailed explanation of the message. The program can be running on multiple operating systems including Debian Linux, KyLinux, HPUX and so on. Successful <b>interoperability</b> experiments <b>proved</b> that this system has good consistency, and running a long time in actual project proves the stability and reliability of the system. This system not only satisfies the need of UHVDC, but also has guiding significance on the research of AIDT system in UHVDC projects...|$|R
40|$|The {{increasing}} {{need for}} personalization forces developers to automatically adapt their applications to individual users. In order to realize this, an application needs {{a model of}} the user with as much and as accurate data as possible. However, users typically divide their time over many applications that individually are limited in their user modeling and therefore can gain from joining forces. This joining of forces boils down to establishing semantic interoperability of user models. Semantic <b>interoperability</b> already <b>proved</b> to be extremely hard in the past, but the emerging Semantic Web might o#er just the mechanisms that we need. From our ongoing research this paper presents the Generic User Model Component (GUC), a generic component that provides user model storage facilities for applications. Moreover, by utilizing Semantic Web technology (e. g. RDF(S), OWL) it also supports the exchange of user data between applications. GUC is {{one of a series of}} generic components for configuring a complete Adaptive Web Information System...|$|R
40|$|Abstract. The European {{knowledge-based}} economy has a complex prod-uct to market {{to the rest}} of the world. Techniques of the past, based on the closed-world assumption, have proved useful in many types of lo-cal information systems. However, theory and practice suggest that this approach may be inadequate for the infrastructure required. In data-bases the relational model through SQL has maintained wide dominance in business data processing. However, interoperability between different databases, even when based on the relational model, is proving a ma-jor problem. Predicate logic (consistent and complete to first-order) has many advantages for practical application. Interoperability however re-quires higher order logic as the arguments themselves are relations and functions. Higher order logic in the context of set theory behaves less satisfactorily according to Gödel’s theorems as such logic cannot sat-isfy all three of soundness, completeness and effectiveness. This may be a fundamental reason why <b>interoperability</b> is <b>proving</b> to be so difficult. This paper looks at underlying problems and suggests that they may be avoided by the use of categorial higher order logic. Cartesian categories are complete, consistent and decidable. They can be employed as an engi-neering technique to construct a general architecture of interoperability. ...|$|R
40|$|The CCSDS Mission Operations {{framework}} {{defined by}} the CCSDS Spacecraft Monitor & Control working group aims to provide a reusable, interoperable and distributed mission operations system based on a service-oriented architecture paradigm. Services are {{defined in terms of}} an abstract model called the “Message Abstraction Layer”, which is cast to a concrete technology by separately defined bindings. This work is concerned with prototype implementation of binding to the Space Packet Protocol. Challenges and solutions are outlined as well as next steps in terms of testing to <b>prove</b> <b>interoperability.</b> A second prototype implementing a core set of services – the Monitor and Control services – is discussed briefly, as well as a possible way of transition towards a Mission Operations framework based infrastructure at the German Space Operations Center GSOC. The work is put into context by pointing out how benefits for missions arise from the framework and its design...|$|R
40|$|Collaborative {{research}} poses important {{challenges and}} opportunities to scientific communities, such as the realization of effective interactions between different groups and organizations. Data sharing is growing increasingly important, as repositories are growing in size, number, and variety in the different domains. The underlying challenge is interoperability: between data, services, applications, models, and ultimately people. Abstraction and standardization (e. g. of data and services) have historically played {{an essential role in}} enabling <b>interoperability.</b> However, they <b>proved</b> not to be sufficient alone to let largescale collaborative research thrive. For different spheres (e. g. social/economic realms), intermediation approaches were successfully applied to pursue similar goals in the so-called brokering approach. This chapter argues that brokering is useful to enable collaborative research as well, both by addressing technological interoperability and supporting socio-organizational challenges. A technological brokering framework, implemented to help the earth system science collaborative research, is finally presented, along with success stories. JRC. B. 6 -Digital Econom...|$|R
40|$|Part 3 : Service Design and ImprovementInternational audienceA major {{challenge}} of interoperability projects and initiatives is to validate that different implementations {{work together and}} are compliant with underlying standard specifications. Interoperability testing can ensure that required end-to-end functionality is adequately fulfilled and all systems are implemented in conformity with existing standards. Adopters of standards have different methods to <b>prove</b> fulfillment of <b>interoperability</b> requirements. This often results in different efficiency, different quality {{and a lack of}} conformity. In this paper, we present the interoperability testing framework used in PEPPOL (Pan-European Public Procurement Online). The framework has supported the project well in establishing various production pilots that interoperate together in the field of e-procurement. It thereby aggregates different testing perspectives ranging from conformance testing to compliance testing and provides guidelines how to prove implementations through testing scenarios. Consequently, the key research question in this paper is how <b>interoperability</b> initiatives can <b>prove</b> that different implementations are compliant with underlying standard specifications and how interoperability can be ensured among different implementations beyond the technical approval mechanisms provided through conformance testing...|$|R
40|$|Virtual {{desktops}} {{are becoming}} increasingly desirable for organizations of all sizes {{that are trying to}} deliver IT more efficiently and to support more client devices, mobile workers, and “always-on ” expectations for service. However, successful deployment of virtual desktops can be elusive for a number of reasons. Traditionally, virtual desktop deployments require multiple components—including software, storage, servers, and more—to work together in a unified way. However, features and functions across products are often not fully compatible. And, when technical issues arise, it can sometimes be difficult to tell which product is causing the problem. The finger-pointing begins, delaying resolution and negatively impacting IT operations. Reference architectures have emerged as a best practice to provide a tested and extensible architecture with <b>proven</b> <b>interoperability</b> of technologies. But, not all reference architectures are created equal. There are big differences {{in the way that they}} are created and tested, which in turn impacts their applicability and reliability. Another important consideration is the requirement to provide predictable performance when rolling out thousands of virtual desktops with a multi-vendor solution. Because users expect their virtual desktops to perform as well as their physical desktops, a positive end-user experience is critical to successful virtual desktop deployment. Minimize Risk and Maximize Value with a Fast, Cost-Effective Fast Track Validated Reference Architectur...|$|R
40|$|So far {{research}} on modelling the travelling patterns of electric vehicles (EVs) lacks technical depth since {{the variability of}} individual travel behaviour lack spatial and temporal disaggregated details- this data is needed by operators (DNOs) to assess EV impact at a low voltage level. Thus, {{it is essential to}} integrate issues concerning the mobility of EVs with the operational issues facing DNOs. This paper discusses how a bottom-up agent-based modelling (ABM) approach, addressing the mobility of EVs, can be combined with power flow studies at different levels of abstraction. From the DNO’s perspective the fact that EVs can move around means loads disappear and may re-appear at a different location, which has consequences on the power flows. Hence the data collected from the EV driving patterns are quantified to monitor the state of charge of the batteries. Subsequently, the output collected from the ABM simulations are applied to power system studies by incorporating its data into a time coordinated optimal power flow (TCOPF) program. An example proof-of-concept case study is showcased to demonstrate the relevance of the ABM paradigm and the effectiveness of the TCOPF solver when they are merged for a small network; in this fashion <b>proving</b> <b>interoperability</b> between the models. Preliminary results illustrate the valuable operational information utilities can obtain regarding optimal EV charging strategies when considering an ABM approach to represent EVs in power flow studies...|$|R
40|$|The AT&T library {{organization}} {{has developed an}} interest in Z 39. 50 {{for a number of}} diverse reasons. It is hoped that eventually Z 39. 50 will help with or solve several classes of problems, ranging from behind the scenes issues resulting from distributed computing architectures to diversity of user interfaces. In addition to helping with known problems, we hope that Z 39. 50 will give us a flexibility required for a constantly evolving {{library organization}} in an international corporate environment. For Z 39. 50 to meet our needs the main requirement is that the protocol itself incorporate all the functionality of our existing information retrieval environment. The 1992 (version 2) version of the standard was a major start, but the newer version (1995 version 3) comes much closer to incorporating existing functionality. The next major requirement is <b>proven</b> <b>interoperability</b> and transparency of database provider to our users. Issues of indexing style, default operations and ways to override defaults, database coverage and loading characteristics become even more apparent in Z 39. 50 than in the traditional online world. Our end users, like users everywhere, are expecting interfaces integrated into their regular computing environment. A solution to this problem is a well accepted search and retrieval protocol. Z 39. 50 is well positioned to become this protocol, and in this belief we have focused our attention on developing a high-quality server for our internal resources. 1...|$|R
40|$|Despite {{the fact}} that an {{increasing}} number of researchers in the cultural heritage sector is recognising the advantages that could derive from the use of knowledge management methodologies and tools, a lack of awareness of the basic principles of this discipline is still rather evident. Key concepts like “knowledge representation”, metadata, conceptual modeling, syntactic or semantic <b>interoperability,</b> ontologies, can <b>prove</b> difficult to understand (and even more difficult to apply) for researchers with a background in the humanities. This contribution, therefore, aims at clarifying the theoretical reference framework through the concrete analysis of archaeological materials. In fact, while it seems easier to borrow definitions and theoretical concepts or to artificially create even very complex conceptual models (e. g. the CIDOC CRM, which has recently been recognised as an ISO standard), it is a lot harder to implement such principles onto real world objects analysis. According to this assessment, and to the need of going from theoretical to practical aspects, the paper is structured in three parts: the first offers a theoretical base that makes available, even for non-experts, the tools for addressing more operational aspects; the second describes, through practical examples, both the knowledge representation model and the software tool used for analysing a class of materials, the Etruscan urns, as shown in the third part. The final objective is, therefore, to provide a point of reference for facilitating the approach towards KM (Knowledge Management) and help clarifying the key elements of a discipline that is obtaining a growing success but, so far, still showing a high level of entropy...|$|R
40|$|For {{the purpose}} of pre Galileo system {{verification}} and testing, the Galileo Test Environment (GATE) is setup in the German Alps to reproduce a realistic test bed for the Galileo satellite navigation system. In fact, a complete miniature navigation system is built up consisting of a control, ground and pseudo-space segment. For system monitoring and testing a receiver is required, which is capable of processing all signals specified in the Galileo system specification. To <b>prove</b> <b>interoperability</b> and for reference purposes the receiver will also process Galileo and GPS signals on L 1 in a common processing chain. This paper describes a modular receiver design, developed for the use as a user receiver {{as well as a}} system monitor receiver. Detailed results from performance tests will be presented. Up to three single band receivers with radio frequency front-end and base band processor occupy up to nine slots of the PXI framework to serve any of the Galileo radio frequency ban ds E 5, E 6 and L 1. One base-band board and a front-end system consisting of two cPCI boards cover each frequency band. The front-end system consists of a digital signal conditioning board and an analog front-end board. As the base-band units are based on field programmable gate arrays (FPGA), the actual band specific functionality of the receiver is implemented in a HDL and uploaded to the system. This flexibility allows all base-band units to be designed identically. The three receiver front-ends similarly have an identical PCB-layout, which reduces costs. The digital signal conditioning boards are all identical. The analog front-end boards have identical PCB layout, but differ in the components fitted (filters, LO-frequencies). Receiving the whole E 5 band (i. e. E 5 a and E 5 b) including the first side-lobe (71 MHz) through one front-end system is the single most challenging requirement on the front-end system. To fulfill the required amplitude and group delay flatness in the pass-band is very challen...|$|R
40|$|This thesis {{answers the}} {{question}} whether {{it is possible to}} build a system for personal communication support, which is able to make automatic decisions within limits defined by the user, regarding the selection of communication terminals as well as the conversion of the communication medium. Therefore, the system relieves the user from predefining and foreseeing every possible situation in communication. Instead, the user can focus on his major goals in communication and general preferences in reachability management and let the system do the detailed work. Thereby, such a system can be considered as an 'electronic secretary'. The work is embedded in the concepts of Personal Communication Support. An appropriate prototype system has been built. It is able to evaluate the assumed outcome of such media conversions and to automatically propose the most suitable way of conversion and delivery, based on the reachability of the user as well as the technical quality and the semantics of the communication content. To reach this goal, a new model of media handling and conversion is proposed, and the impacts of media conversion on the semantic of information are discussed from the theoretical point of view. A new generic model of telecommunication services is proposed, a new concept of evaluating the quality of complete chains of concatenated conversion modules is applied. Methods from the area of automatic decision making are adapted to the specific problem, and algorithms for selecting and configuring chains of concatenated software and hardware modules and techniques for controlling and stream binding such modules are developed. Supporting complex kinds of media conversion, the system provides a generic interface for integrating third party products such as Text-to-Speech, Speech recognition, Optical Character Recognition. This concept of easy integration of conversion and filtering tools allows further enhancements. Beyond considering the technical quality of the message, content semantics (inteligent content handling), ranging from keyword recognition to multimedia content screening, can be used for evaluation. Closely following design principles of the Reference Model of Open Distributed Processing, and based on a vendor-open middleware using CORBA, the system <b>proves</b> <b>interoperability</b> between heterogeneous components, and is widely scalable, allowing tailoring for various purposes and environments...|$|R

