13|80|Public
30|$|Possible feature {{reduction}} {{techniques are}} {{techniques such as}} principle components, heuristic feature selection with wrapper method and feature selection with decision trees. Examples for case reduction techniques are <b>incremental</b> <b>samples,</b> average samples, increasing the sampling period and strategic sampling of key events. For value reduction prominent techniques are rounding, using k-means clustering and discretization using entropy minimization.|$|E
40|$|The forward-greedy {{algorithm}} {{based on}} the neighborhood rough set is a simple and effective method for the attribute reduction. However, it is non-incremental reduction method, which limits its application in the dynamic decision system. In this article, an improved incremental attribute reduction algorithm is proposed by introducing concept of the relative positive region, which can update the original reduction set quickly and handle both the incremental attributes and the <b>incremental</b> <b>samples.</b> Finally, the correctness and effectiveness of the proposed algorithm are demonstrated by examples and experiments on 5 standard data sets from UCI...|$|E
40|$|The {{heterogeneous}} three-dimensional {{spatial distribution}} of mycotoxins {{has proven to be}} one of the main limitations for the design of effective sampling protocols. Current sample collection protocols for mycotoxins have been designed to estimate the mean concentration and fail to characterise the {{spatial distribution of}} the mycotoxin concentration due to the aggregation of the <b>incremental</b> <b>samples.</b> Geostatistical techniques have been successfully applied to overcome similar problems in many research areas. However, little work has been developed on the use of geostatistics for the design of sampling protocols for mycotoxins. This paper focuses on the analysis of the two and three-dimensional spatial structure of fumonisins B 1 (FB 1) and B 2 (FB 2) in maize in a bulk store using a geostatistical approach and on how results help determine the number and location of <b>incremental</b> <b>samples</b> to be collected. The spatial correlation between FB 1 and FB 2, as well as between the number of kernels infected and the level of contamination was investigated. For this purpose, a bed of maize was sampled at different depths to generate a unique three-dimensional data set of FB 1 and FB 2. The analysis found no clear evidence of spatial structure in either the two- dimensional or three-dimensional analyses. The number of Fusarium infected kernels was not a good indicator for the prediction of fumonisin concentration and there was no spatial correlation between the concentrations of the two fumonisins...|$|E
40|$|<b>Incremental</b> <b>sampling</b> {{methodology}} (ISM) is {{a structured}} composite sampling and processing protocol having specific elements {{designed to reduce}} data variability and increase sample representativeness for a specified volume of soil under investigation. Variability in measured contaminant concentrations between discrete soil samples is due primarily to the particulat...|$|R
30|$|IRS: The <b>incremental</b> random <b>sampling</b> {{algorithm}} {{shown in}} algorithm 2.|$|R
40|$|Sampling {{can be a}} {{significant}} source of error in the measurement process. The characterization and cleanup of hazardous waste sites require data that meet site-specific levels of acceptable quality if scientifically supportable decisions are to be made. In support of this effort, the US Environmental Protection Agency (EPA) is investigating methods that relate sample characteristics to analytical performance. Predicted uncertainty levels allow appropriate study design decisions to be made, facilitating more timely and less expensive evaluations. Gy sampling theory can predict {{a significant}} fraction of sampling error when certain conditions are met. We report on several controlled studies of subsampling procedures to evaluate the utility of Gy sampling theory applied to laboratory subsampling practices. Several sample types were studied and both analyte and non-analyte containing particles were shown to play important roles affecting the measured uncertainty. Gy sampling theory was useful in predicting minimum uncertainty levels provided the theoretical assumptions were met. Predicted fundamental errors ranged from 46 to 68 % of the total measurement variability. The study results also showed sectorial splitting outperformed <b>incremental</b> <b>sampling</b> for simple model systems and suggested that sectorial splitters divide each size fraction independently. Under the limited conditions tested in this study, <b>incremental</b> <b>sampling</b> with a spatul...|$|R
40|$|Queries {{over large}} scale (petabyte) data bases often mean waiting {{overnight}} for a result to come back. Scale costs time. Such time {{also means that}} potential avenues of exploration are ignored because the costs {{are perceived to be}} too high to run or even propose them. With sampleAction we have explored whether interaction techniques to present query results running over only <b>incremental</b> <b>samples</b> can be presented as sufficiently trustworthy for analysts both to make closer to real time decisions about their queries and to be more exploratory in their questions of the data. Our work with three teams of analysts suggests that we can indeed accelerate and open up the query process with such incremental visualizations. Author Keywords Incremental visualizations, large data, exploratory dat...|$|E
40|$|The aim of {{this paper}} was to update the {{sampling}} plan for analysis of mycotoxins in grains, formerly published by the author. The proposed alterations were based on the acquired experience on its application and on FAO recommendations. This update restricts the scope of the former plan and establishes a sampling plan for analysis of aflatoxin in peanuts and corn, by means of modified formulas, the minimum number of sacks or points (when in bulk) from which <b>incremental</b> <b>samples</b> should be drawn to make a bulk sample. Fractional exponents (square roots) of the formulas proportionally decrease the number of sacks/points to be sampled as the lot size increases. Operating Characteristic (OC) curves developed for in-shell and shelled peanuts and corn as well as trend curves of the coefficient variation for different sample sizes (weights) are presented...|$|E
40|$|We present {{procedures}} for selecting the best or near-best of {{a finite number}} of simulated systems when best is defined by maximum or minimum expected performance. The procedures are appropriate when it is possible to repeatedly obtain small, <b>incremental</b> <b>samples</b> from each simulated system. The goal of such a sequential procedure is to eliminate, at an early stage of experimentation, those simulated systems that are clearly inferior, and thereby reduce the overall computational effort required to find the best. The procedures we present accommodate unequal variances across systems and the use of common random numbers. However, they are based on the assumption of normally distributed data, so we analyze the impact of batching (to achieve approximate normality or independence) on the performance of the procedures. Comparisons with existing procedures are also provided. Key Words: Output Analysis; Multiple Comparisons; Ranking and Selection; Variance Reduction 1 Introduction In a series of [...] ...|$|E
40|$|Abstract. The Hammersley and Halton point sets, two {{well known}} low {{discrepancy}} sequences, {{have been used}} for quasi-Monte Carlo integration in previous research. A deterministic formula generates a uniformly distributed and stochastic-looking sampling pattern, at low computational cost. The Halton point set is also useful for <b>incremental</b> <b>sampling.</b> In this paper, we discuss detailed implementation issues and our experience of choosing suitable bases of the point sets, not just on the 2 D plane, but also on a spherical surface. The sampling scheme is also applied to ray tracing, with a significant improvement in error. ...|$|R
40|$|Many {{people take}} photos and videos with {{smartphones}} {{and more recently}} with 360 -degree cameras at popular places and events, and share them in social media. Such visual content is produced in large volumes in urban areas, {{and it is a}} source of information that online users could exploit to learn what has got the interest of the general public {{on the streets of the}} cities where they live or plan to visit. A key step to providing users with that information is to identify the most popular k spots in specified areas. In this paper, we propose a clustering and <b>incremental</b> <b>sampling</b> (C&IS) approach that trades off accuracy of top-k results for detection speed. It uses clustering to determine areas with high density of visual content, and <b>incremental</b> <b>sampling,</b> controlled by stopping criteria, to limit the amount of computational work. It leverages spatial metadata, which represent the scenes in the visual content, to rapidly detect the hotspots, and uses a recently proposed Gaussian probability model to describe the capture intention distribution in the query area. We evaluate the approach with metadata, derived from a non-synthetic, user-generated dataset, for regular mobile and 360 -degree visual content. Our results show that the C&IS approach offers 2. 8 x- 19 x reductions in processing time over an optimized baseline, while in most cases correctly identifying 4 out of 5 top locations...|$|R
40|$|Abstract — This paper studies a {{class of}} approach-evasion {{differential}} games, in which one player aims to steer the state of a dynamic system to the given target set in minimum time, while avoiding some set of disallowed states, and the other player desires to achieve the opposite. We propose {{a class of}} novel anytime computation algorithms, analyze their convergence properties and verify their performance via a number of numerical simulations. Our algorithms significantly outperform the multi-grid method for the approach-evasion differential games both theoretically and numerically. Our technical approach leverages <b>incremental</b> <b>sampling</b> in robotic motion planning and viability theory. I...|$|R
40|$|We present proced 657 for {{selecting}} the best or near-best of {{a finite number}} of simulated systems when best isdz 389 by maximum or minimum expected performance. The proced 855 are appropriate when it is possible to repeated 3 obtain small, <b>incremental</b> <b>samples</b> from each simulated system. The goal of such a sequential proced 05 is to eliminate, at an early stage of experimentation, those simulated systems that are apparently inferior, and thereby red 46 the overall computational e#ortrequired tofind the best. The proced 668 we present accommodom unequal variances across systems and the use of common rand 5 numbers. However, they arebased on the assumption of normallydmallyz 244 dma so we analyze the impact of batching (to achieve approximate normality or ind 5 end 3945 on the performance of the proced 1723 Comparisons with some existing indingz 25126 zV 3 proced 533 are also provid 60 Key Word 9 Output Analysis; Multiple Comparisons; Rankingand Selection; Variance Redcez 42 1 IntroductiA In a series of p [...] ...|$|E
40|$|Mycotoxins in {{agricultural}} commodities are a hazard to {{human and animal}} health. Their heterogeneous spatial distribution in bulk storage or transport makes it particularly difficult to design effective and efficient sampling plans. There has been considerable emphasis on identifying the different sources of uncertainty associated with mycotoxin concentration estimations, but much less on identifying {{the effect of the}} spatial location of the sampling points. This study used a two-dimensional statistical modelling approach to produce detailed information on appropriate sampling strategies for surveillance of mycotoxins in raw food commodities. The emphasis was on deoxynivalenol (DON) and ochratoxin A (OTA) in large lots of grain in storage or bulk transport. The aim was to simulate a range of plausible distributions of mycotoxins in grain from a set of parameters characterising the distributions. For this purpose, a model was developed to generate data sets which were repeatedly sampled to investigate the effect that sampling strategy and the number of <b>incremental</b> <b>samples</b> has on determining the statistical properties of mycotoxin concentration. Results showed that, for most sample sizes, a regular grid proved to be more consistent and accurate in the estimation of the mean concentration of DON, which suggests that regular sampling strategies should be preferred to random sampling, where possible. For both strategies, the accuracy of the estimation of the mean concentration increased significantly up to sample sizes of 40 - 60 (depending on the simulation). The effect of sample size was small when it exceeded 60 points, which suggests that the maximum sample size required is of this order. Similar conclusions about the sample size apply to OTA, although the difference between regular and random sampling was small and probably negligible for most sample sizes...|$|E
40|$|In March 2013 a large {{shipment}} of maize, intended for feed was {{subject of an}} alert in the Rapid Alert System for Food and Feed of the European Commission (EC) because the aflatoxin B 1 (AFB 1) level in the load exceeded the EC regulated maximum level of 20 µg/kg. Since the shipment had passed import controls and was already distributed (mainly to German farms), a massive recall followed. The aim {{of the current study}} was to investigate questions, raised by authorities and industry, related to the effectivity of EU sampling procedures, the influence of sample homogenisation procedures and sample storage conditions on the test results, and fungal identification as unexpected mycotoxins were identified during this study. The Netherlands Food and Consumer Product Safety Authority seized a shipload of maize in July 2013, suspected to be contaminated with AFB 1. The shipload was sampled according to the 2009 and 2013 EC Sampling Regulations to compare the outcomes of both sampling protocols. Mycotoxin analysis of the <b>incremental</b> <b>samples</b> showed high mean levels of AFB 1, aflatoxin G 1 (AFG 1), and ochratoxin A (OTA). Also an extreme inhomogeneous distribution of aflatoxins and OTA was proven. Analysis of samples homogenised according to the slurry method showed improved performance as compared to samples homogenised through dry homogenisation. Sampling and sample homogenisation according to the Regulation from 2013 showed a closer estimate of the ‘true’ AFB 1 content as compared to sampling according to the Regulation from 2009. No influence of laboratory storage conditions on AFB 1 concentration could be determined. Fungal identification revealed Aspergillus flavus as the main source of AFB 1 in this shipment. Infrequent occurrence of Aspergillus parasiticus might have been the source of AFG 1. The occurrence of sometimes large amounts of OTA could not be explained, however it was suggested that Aspergillus welwitschiae might have played a role...|$|E
30|$|Algorithm 2 {{shows the}} {{procedure}} of <b>incremental</b> <b>sampling.</b> Firstly, we reuse the previous structure SW_i- 1 (Line 4). Then tuples, which {{are beyond the}} range of current window, are removed from SW_i (Line 6). Function remove_head is used to remove the first item in SW_i.indexarr and update SW_i.sh depending on how many tuples are removed. Then we use a temporary structure T_SW to store the indexes of newly sampled tuples {{in the range from}} SW_i.indexarr[SW_i.st] to W_i.t (Line 9 – 11). Next, we append the indexes in T_SW.indexarr to SW_i and update SW_i.st (Line 12). Finally, we aggregate the tuples in SW_i to estimate the final result (Line 13 – 14).|$|R
30|$|For many drug therapies, {{there is}} little {{knowledge}} about the ultimate distribution patterns of the compounds within tissue compartments following treatment. It is possible to label drugs with a tracer and follow their uptake using technologies such as positron-emission tomography (PET) and autoradiography. For both of these methods the physical manipulation of the compound by the labeling could change {{the properties of the}} compound. Over the last decade a method to identify unlabeled drugs in tissue has been under development using MALDI-MSI. With MALDI-MSI continual <b>incremental</b> <b>sampling</b> can be performed upon tissue taken directly from the body to identify cellular locations that contain the specific drug ion signatures of the compound in question [21].|$|R
40|$|Abstract — In this paper, the {{filtering}} {{problem for}} a large class of continuous-time, continuous-state stochastic dynam-ical systems is considered. Inspired by recent advances in asymptotically-optimal sampling-based motion planning algo-rithms, such as the PRM ∗ and the RRT∗, an incremen-tal sampling-based algorithm is proposed. Using <b>incremental</b> <b>sampling,</b> this approach constructs a sequence of Markov chain approximations, and solves the filtering problem, in an incremental manner, on these discrete approximations. It is shown that the trajectories of the Markov chain approximations converge in distribution to the trajectories of the original stochastic system; moreover, the optimal filter calculated on these Markov chains converges to the optimal continuous-time nonlinear filter. The convergence results are verified {{in a number of}} simulation examples. I...|$|R
40|$|This study {{examines}} the carbon and oxygen isotopic composition of bioapatite in equid tooth enamel {{as a potential}} record of environmental change in a north-central Mongolian sampling area between 51. 4 °N, 99. 0 °E and 44. 6 °N, 106. 9 °E (northwest to southeast). Mammal tooth enamel is useful as a palaeoclimate proxy {{because it is a}} durable material that directly reflects the isotopic composition of the body, and therefore organism diet and water intake. In addition, tooth enamel accumulates sequentially from crown to root over the period of months to years and often records seasonal variation. Thus, the inter- and intra-tooth variations in the stable carbon (ä 13 C) and oxygen (ä 18 O) isotopic composition of horse tooth enamel may provide a high-resolution record about climatic factors such as temperature and moisture availability as well as the composition and availability of forage during the tooth growth period. Sequential <b>incremental</b> <b>samples</b> from modern horses were analyzed to provide a record of the carbon and oxygen isotopic values preserved during tooth enamel formation and mineralization. To constrain the final composition of the bioapatite, modern enamel stable isotopic compositions were compared with the compositions of meteoric waters and plants from comparable localities. Modern teeth displayed a marked regional seasonal oscillation in ä 18 O and record a latitudinal shift observed in the ä 18 O of meteoric waters. In addition, bulk and sequentially sampled profiles from the modern teeth were used as a comparative set with samples from archaeological teeth (Bronze Age, ca. 1000 B. C.) and suggest that climatic patterns were roughly equivalent during both periods, with a similar plant communities, and similar summer precipitation/temperature patterns. However, seasonality may have been more intense in Bronze Age ca. 1000 B. C., with similar summer highs but more severe winters. The difference could be due to This work contributes to ongoing research into the climatic history of Central Asia and to the application of equid tooth enamel as an environmental proxy in this region...|$|E
40|$|In {{the eastern}} {{tropical}} Atlantic, the orangeback flying squid Sthenoteuthis pteropus (Steenstrup 1855) (Cephalopoda, Ommastrephidae) is a dominant {{species of the}} epipelagic nekton community. This carnivore squid has a short lifespan {{and is one of}} the fastest-growing squids. In this study, we characterise the role of S. pteropus in the pelagic food web of the eastern tropical Atlantic by investigating its diet and the dynamics of its feeding habits throughout its ontogeny and migration. During three expeditions in the eastern tropical Atlantic in 2015, 129 specimens were caught by hand jigging. Stomach content analyses (via visual identification and DNA barcoding) were combined with stable isotope data (∂ 15 N and ∂ 13 C) of muscle tissue to describe diet, feeding habits and trophic ecology of S. pteropus. Additionally, stable isotope analyses of <b>incremental</b> <b>samples</b> along the squid’s gladius—the chitinous spiniform structure supporting the muscles and organs—were carried out to explore possible diet shifts through ontogeny and migration. Our results show that S. pteropus preys mainly on myctophid fishes (e. g. Myctophum asperum, Myctophum nitidulum, Vinciguerria spp.), but also on other teleost species, cephalopods (e. g. Enoploteuthidae, Bolitinidae, Ommastrephidae), crustaceans and possibly on gelatinous zooplankton as well. The squid shows a highly opportunistic feeding behaviour that includes cannibalism. Our study indicates that the trophic position of S. pteropus may increase by approximately one trophic level from a mantle length of 15 cm to 47 cm. The reconstructed isotope-based feeding chronologies of the gladii revealed high intra- and inter-individual variability in the squid’s trophic position and foraging area. These findings are not revealed by diet or muscle tissue stable isotope analysis. This suggests a variable and complex life history involving individual variation and migration. The role of S. pteropus in transferring energy and nutrients from lower to higher trophic levels may be underestimated and important for understanding how a changing ocean impacts food webs in the eastern Atlantic...|$|E
40|$|Tillage erosion is {{increasingly}} recognised {{as an important}} soil erosion process on agricultural land. In view of its potential significance, {{there is a clear}} need to broaden the experimental database far the magnitude of tillage erosion to include a range of tillage implements and agricultural environments. The study discussed in this paper sought to address the need for such data by examining tillage erosion by a duckfoot chisel plough in stony soils on steep slopes in a semi-arid environment. Results of the investigation of coarse fraction (rock fragment) translocation by tillage in this environment have been presented elsewhere and the paper focuses on tillage translocation and erosion of the fine earth. Tillage translocation was measured at 10 sites, representing both upslope and downslope tillage by a duckfoot chisel plough on five different slopes, with tangents ranging from 0. 02 to 0. 41. A fine-earth tracer, comprising fine earth labelled with Cs- 134, was introduced into the plough layer before tillage. After a single pass of the plough, <b>incremental</b> <b>samples</b> of plough soil were excavated and sieved to separate the fine earth from the rock fragments,Translocation of the fine-earth tracer was established by analysing the 134 Cs content of the samples of fine earth. These data were used to establish translocation distances for each combination of slope and tillage direction. Translocation distances of the fine earth were not significantly different from translocation distances of the coarse fraction. For all sites, except uphill on the 0. 41 slope, translocation distances were found to be linearly related to slope tangent. The soil flux due to tillage for each site was calculated using the translocation distance and the mass per unit area of the plough layer. For slopes with tangents < 0. 25, the relationship between soil flux and tangent was linear and the soil flux coefficient derived was 520 - 660 kg m(- 1) per pass. This is much larger than the coefficients found in other studies and this high magnitude is attributed to the non-cohesive nature and high rock fragment content of the soil in this investigation. A second contrast with previous studies was found in non-linearity in the relationship between soil flux and tangent when steeper slopes were included. This was a product of variation in plough depth between the steepest slopes and the remainder of the study area. On the basis of the study it is suggested that an improved understanding of tillage erosion may be obtained by considering the dual processes of tillage detachment (mass per unit area of soil subject to tillage) and tillage displacement (equivalent to translocation distance per pass) in assessing, comparing and modelling tillage translocation. An improved model is proposed that recognises the complexity of soil redistribution by tillage, provides a framework for process-based investigation of the controls on tillage fluxes, and allows identification of potential self-limiting conditions for tillage erosion. (C) 1999 Elsevier Science B. V. All rights reserved. status: publishe...|$|E
40|$|We use isotopic {{analyses}} of authigenic siderite and calcite cements within Rosselia socialis burrows from shoreface deposits in the Upper Cretaceous Horseshoe Canyon Formation of Alberta, Canada, to re-veal the early cementation {{history of the}} burrow and geochemical conditions of the initial sedimentary environment. Within the Horse-shoe Canyon Formation, two forms of the Rosselia burrows are pres-ent: bulbous in situ burrows, and transported, spindlelike burrows, which display similar internal shaft diameters but smaller overall size compared to in situ forms. Transverse, <b>incremental</b> <b>sampling</b> of cal-cite and siderite cements in the Rosselia burrows reveals symmetrical isotopic deviation in 13 C and 18 O around the burrow core, rep-resenting accretionary records of evolving pore-water conditions. The number of isotopic deviations recorded in bulbous specimens is equal to those observed in spindle-shaped burrows, suggesting that in sit...|$|R
40|$|In this paper, the {{filtering}} {{problem for}} a large class of continuous-time, continuous-state stochastic dynamical systems is considered. Inspired by recent advances in asymptotically-optimal sampling-based motion planning algorithms, such as the PRM* and the RRT*, an incremental sampling-based algorithm is proposed. Using <b>incremental</b> <b>sampling,</b> this approach constructs a sequence of Markov chain approximations, and solves the filtering problem, in an incremental manner, on these discrete approximations. It is shown that the trajectories of the Markov chain approximations converge in distribution to the trajectories of the original stochastic system; moreover, the optimal filter calculated on these Markov chains converges to the optimal continuous-time nonlinear filter. The convergence results are verified {{in a number of}} simulation examples. United States. Army Research Office. Multidisciplinary University Research Initiative (Grant W 911 NF- 11 - 1 - 0046...|$|R
40|$|This paper {{focuses on}} a continuous-time, continuous-space {{formulation}} of the stochastic optimal control problem with nonlinear dynamics and observation noise. We lay the mathematical foundations to construct, via <b>incremental</b> <b>sampling,</b> an approximating sequence of discrete-time finite-state partially observable Markov decision processes (POMDPs), such that the behavior of successive approximations converges to {{the behavior of the}} original continuous system in an appropriate sense. We also show that the optimal cost function and control policies for these POMDP approximations converge almost surely to their counterparts for the underlying continuous system in the limit. We demonstrate this approach on two popular continuous-time problems, viz., the Linear-Quadratic-Gaussian (LQG) control problem and the light-dark domain problem. United States. Army Research Office. Multidisciplinary University Research Initiative (Grant W 911 NF- 11 - 1 - 0046...|$|R
40|$|Neural network plays a {{major role}} in the field of pattern recognition. For pattern recognition, a major {{drawback}} with traditional neural networks is that neural networks may easily be trapped in spurious states. Synergetic neural network (SNN) has been proposed in the literature to overcome this problem, however, when applying synergetic neural network on face recognition, the results are not satisfactory for large image databases due to low memory capacity. Therefore, the chaotic dynamic property is introduced to the conventional synergetic neural network in order to resolve the problem. In this paper, an additional control parameter is introduced to the chaotic synergetic neural network (CSNN) in order to terminate the recognition process whenever an image is recognized. This helps to alleviate processing memory demand which often accompanies such networks. Various imagery defects are tested and the accuracy of both methods is evaluated based on <b>incremental</b> <b>sample</b> siz...|$|R
30|$|This work {{develops}} {{several different}} sampling approaches, {{based on the}} sampling techniques in online aggregation, for executing window functions. The proposed sampling algorithms speed up query processing time by {{reducing the number of}} tuples used in window aggregation functions. And this work is an extension of our work [1], in which we proposed naive random <b>sampling</b> (NRS) and <b>incremental</b> random <b>sampling</b> (IRS) algorithms.|$|R
30|$|In {{this part}} we compare the error levels and the {{influence}} of the parameters of each algorithm. Sampling rate is a very important parameter in our methods. Since naive random <b>sampling</b> (NRS) and <b>incremental</b> random <b>sampling</b> (IRS) are similar in this respect, all the pictures in Fig.  8 show the results of IRS algorithm based on the row mode window.|$|R
40|$|Motion {{planning}} for robots with many {{degrees of freedom}} requires the exploration of an exponentially large configuration space. Single-query motion planners restrict exploration to regions of configuration space determined to be relevant to a particular planning query. The heuristics employed by existing single-query planners to estimate the relevance of a region, however, remain unchanged throughout the planning process. An incorrect estimate by the heuristic for a configuration space region will only be corrected by explicit exploration. As a result, unnecessary exploration is performed. In this paper we propose an alternative approach. We observe that every <b>incremental</b> <b>sample</b> improves the planner’s understanding of configuration space. This improved understanding can be exploited to inform the singlequery heuristic of a motion planner. We formalize the improvement in understanding by employing the notion of entropy from information theory and derive a principled method of configuration space exploration in the single-query setting. Experiments show that the proposed single-query entropy-guided motion planner outperforms existing single-query techniques. ...|$|R
40|$|Abstract — The Rapidly-exploring Random Tree (RRT) algorithm, {{based on}} <b>incremental</b> <b>sampling,</b> {{efficiently}} computes motion plans. Although the RRT algorithm quickly produces candidate feasible solutions, {{it tends to}} converge to a solution that is far from optimal. Practical applications favor “anytime” algorithms that quickly identify an initial feasible plan, then, given more computation time available during plan execution, improve the plan toward an optimal solution. This paper describes an anytime algorithm based on the RRT ∗ which (like the RRT) finds an initial feasible solution quickly, but (unlike the RRT) almost surely converges to an optimal solution. We present two key extensions to the RRT ∗, committed trajectories and branch-and-bound tree adaptation, that together enable the algorithm to make more efficient use of computation time online, resulting in an anytime algorithm for real-time implementation. We evaluate the method using a series of Monte Carlo runs in a high-fidelity simulation environment, and compare {{the operation of the}} RRT and RRT ∗ methods. We also demonstrate experimental results for an outdoor wheeled robotic vehicle. I...|$|R
40|$|Abstract — We {{study the}} problem of {{predicting}} the Residual Link Lifetime (RLL) in MANETs, where the nodes are able to measure the relative distances between them (e. g., by using the UWB technology). We propose a Mobile-Projected Trajectory (MPT) algorithm, whose input is periodically sampled, noisy range measurements between the two nodes of a link. It estimates a projected trajectory, which is then used to compute the predicted RLL. An enhancement technique, which we call <b>Incremental</b> <b>Sampling,</b> is proposed where the estimated trajectory is further refined to improve {{the accuracy of the}} RLL prediction. We have evaluated the performance of the MPT algorithm with two different mobility models and for different parameters, and have shown that MPT yields robust performance; i. e., the main strength of the MPT algorithm lies in its capability to accurately predict the RLL with limited range input data. For example, after a measurementacquisition time equal to 25 % of the link lifetime, the algorithm yields 90 % prediction accuracy; 80 % accuracy is achieved after 20 % of the link lifetime. After only 15 % of link lifetime, the algorithm still achieves 60 % prediction accuracy...|$|R
40|$|Abstract. Hierarchical {{modeling}} and reasoning are fundamental in ma-chine intelligence, {{and for this}} the two-parameter Poisson-Dirichlet Pro-cess (PDP) plays an important role. The most popular MCMC sampling algorithm for the hierarchical PDP and hierarchical Dirichlet Process is to conduct an <b>incremental</b> <b>sampling</b> based on the Chinese restaurant metaphor, which originates from the Chinese restaurant process (CRP). In this paper, with the same metaphor, we propose a new table repre-sentation for the hierarchical PDPs by introducing an auxiliary latent variable, called table indicator, to record which customer takes respon-sibility for starting a new table. In this way, the new representation allows full exchangeability that is an essential condition for a correct Gibbs sampling algorithm. Based on this representation, we develop a block Gibbs sampling algorithm, which can jointly sample the data item and its table contribution. We test this out on the hierarchical Dirichlet process variant of latent Dirichlet allocation (HDP-LDA) developed by Teh, Jordan, Beal and Blei. Experiment {{results show that the}} proposed algorithm outperforms their “posterior sampling by direct assignment” algorithm in both out-of-sample perplexity and convergence speed. The representation can be used with many other hierarchical PDP models...|$|R
40|$|A {{strategy}} is introduced for achieving high accuracy in {{synthetic aperture radar}} (SAR) automatic target recognition (ATR) tasks. Initially, a novel pose rectification process and an image normalization process are sequentially introduced to produce images with less variations prior to the feature processing stage. Then, feature sets that have a wealth of texture and edge information are extracted with the utilization of wavelet coefficients, where more effective and compact feature sets are acquired by reducing the redundancy and dimensionality of the extracted feature set. Finally, a group of discrimination trees are learned and combined into a final classifier {{in the framework of}} Real-AdaBoost. The proposed method is evaluated with the public release database for moving and stationary target acquisition and recognition (MSTAR). Several comparative studies are conducted {{to evaluate the effectiveness of}} the proposed algorithm. Experimental results show the distinctive superiority of the proposed method under both standard operating conditions (SOCs) and extended operating conditions (EOCs). Moreover, our additional tests suggest that good recognition accuracy can be achieved even with limited number of training images as long as these are captured with appropriately <b>incremental</b> <b>sample</b> step in target poses...|$|R
40|$|The Rapidly-exploring Random Tree (RRT) algorithm, {{based on}} <b>incremental</b> <b>sampling,</b> {{efficiently}} computes motion plans. Although the RRT algorithm quickly produces candidate feasible solutions, {{it tends to}} converge to a solution that is far from optimal. Practical applications favor “anytime” algorithms that quickly identify an initial feasible plan, then, given more computation time available during plan execution, improve the plan toward an optimal solution. This paper describes an anytime algorithm based on the RRT* which (like the RRT) finds an initial feasible solution quickly, but (unlike the RRT) almost surely converges to an optimal solution. We present two key extensions to the RRT*, committed trajectories and branch-and-bound tree adaptation, that together enable the algorithm to make more efficient use of computation time online, resulting in an anytime algorithm for real-time implementation. We evaluate the method using a series of Monte Carlo runs in a high-fidelity simulation environment, and compare {{the operation of the}} RRT and RRT* methods. We also demonstrate experimental results for an outdoor wheeled robotic vehicle. United States. Army. Logistics Innovation AgencyUnited States. Army Combined Arms Support CommandUnited States. Dept. of the Air Force (Air Force Contract FA 8721 - 05 -C- 0002...|$|R
50|$|<b>Incremental</b> Implementation: The <b>Sample</b> UHID can be {{implemented}} on an incremental basis. With the {{development and use of}} appropriate procedures and establishment of the necessary bidirectional mapping, both the Sample UHID and existing patient identifiers can co-exist during the time of transition.|$|R
40|$|We present {{algorithms}} {{for generating}} deterministic <b>sample</b> sequences using <b>incremental</b> grid-based <b>sampling.</b> Our algorithms {{are designed to}} generate dense sample sequences over spaces common in robotics, such as the unit cube, SO(3), and SE(3). Our sampling techniques provide the advantageous properties of uniformity, lattice structure, and incremental quality. In addition, the inherent structure of grid-based sequences not only enables them {{to be used in}} the place of other sampling techniques in existing algorithms, but also permits the development of new algorithms aimed at exploiting this structure...|$|R
40|$|We {{present the}} Macquarie/AAO/Strasbourg Hα Planetary Nebula Catalogue (MASH) of over 900 true, likely and {{possible}} new Galactic planetary nebulae (PNe) discovered from the AAO/UKST Ha {{survey of the}} southern Galactic plane. The combination of depth, resolution, uniformity and areal coverage of the Ha survey has opened up a hitherto unexplored region of parameter space permitting the detection of this significant new PN sample. Away from the Galactic bulge the new PNe are typically more evolved, of larger angular extent, of lower surface brightness and more obscured (i. e. extinguished) than those in most previous surveys. We have also doubled the number of PNe in the Galactic bulge itself and although most are compact, we have also found more evolved examples. The MASH catalogue represents {{the culmination of a}} seven-year programme of identification and confirmatory spectroscopy. A key strength is that the entire sample has been derived from the same, uniform observational data. The 60 per cent increase in known Galactic PNe represents the largest ever <b>incremental</b> <b>sample</b> of such discoveries and will {{have a significant impact on}} many aspects of PN research. This is especially important for studies at the faint end of the PN luminosity function which was previously poorly represented. © 2006 RAS. link_to_subscribed_fulltex...|$|R
