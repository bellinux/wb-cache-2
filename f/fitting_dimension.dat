1|26|Public
40|$|Description Functions, methods, and {{datasets}} for <b>fitting</b> <b>dimension</b> reduction regression, using slicing (methods SAVE and SIR), Principal Hessian Directions (phd, using residuals and the response), and an iterative IRE. Partial methods, {{that condition}} on categorical predictors are also available. A variety of tests, and stepwise deletion of predictors, is also included. Also included is code for computing permutation tests of dimension. Adding additional methods of estimating dimension is straightforward. For documentation, see the vignette in the package. With version 3. 0. 4, {{the arguments for}} dr. step have been modified...|$|E
5000|$|Allowance for {{the service}} at its widest point for spaces between pipe and duct runs, for insulation, {{standard}} <b>fitting</b> <b>dimensions,</b> and joint widths ...|$|R
2500|$|... 2002 – White Euro plates: N aa-nnn and N aa-nna {{new style}} of plates <b>fitting</b> European <b>dimensions</b> was {{introduced}} in April 2002. In these plates, the N is compulsory prefix then spaced combinations.|$|R
5000|$|ISO 6695: Cycles - Pedal axle and crank {{assembly}} with square end <b>fittings</b> - Assembly <b>dimensions.</b> International Organization for Standardization, Geneva, 1991. (also: British Standard BS 6102-14) ...|$|R
40|$|There {{have been}} an {{increasing}} number of applications where the number of predictors is large, meanwhile data are repeatedly measured at a sequence of time points. In this article we investigate how dimension reduction method can be employed for analyzing such high-dimensional longitudinal data. Predictor dimension can be effectively reduced while full regression means information can be retained during dimension reduction. Simultaneous variable selection along with dimension reduction is studied, and graphical diagnosis and model <b>fitting</b> after <b>dimension</b> reduction are investigated. The method is flexible enough to encompass a variety of commonly used longitudinal models. ...|$|R
3000|$|A CLASCAL {{analysis}} (see Appendix A) of {{the data}} yielded a 1 -latent class, 3 -dimensional space with specificities. Figures 13 (a) to 13 (c) represent the projections of the space, and Table 9 reports the correlation coefficients of the acoustic features best <b>fitting</b> the perceptual <b>dimensions.</b> The first dimension is correlated [[...]...|$|R
40|$|In {{order to}} realize the online {{measurement}} of lamp dimension, the bulb image dimension measurement based on vision (BIDMV) is proposed. The image of lamp is obtained by camera. After image processing, such as Otsu algorithm, median filter, ellipse fitting and envelope rectangle <b>fitting,</b> the <b>dimension</b> of lamp can be calculated. Based on this method, a non-contact real-time measurement system of the lamp 2 ̆ 7 s dimension is developed. The precision of the proposed method is 0. 07 mm, and it can satisfy the tolerance of the National Standard GB 15766. 1 - 2008. The experiment {{results show that the}} proposed method has a faster measuring speed and a higher precision compared with other measurement methods...|$|R
50|$|A {{limited number}} of buses used to carry cycle racks on the front but this service is no longer available. Bicycles are easy to {{transport}} on the CityCats, and many stops have bicycle racks. During peak hour, bicycles on Queensland Rail's (QR) City network services are restricted to counter-peak-flow services (i.e. You have to be travelling {{away from the city}} in the morning, and to the city in the afternoon). Exceptions to this are relatively uncommon such as fold up bicycles <b>fitting</b> within certain <b>dimensions.</b> Riding a bicycle on QR property is always forbidden.|$|R
40|$|A primary {{feature of}} a {{computer}} program is its quantitative performance characteristics: the amount of resources such as time, memory, and power the program needs to perform its task. Concrete resource bounds for specific hardware have many important applications in software development but their manual determination is tedious and error-prone. This dissertation studies the problem of automatically determining concrete worst-case bounds on the quantitative resource consumption of functional programs. Traditionally, automatic resource analyses are based on recurrence relations. The difficulty of both extracting and solving recurrence relations {{has led to the}} development of type-based resource analyses that are compositional, modular, and formally verifiable. However, existing automatic analyses based on amortization or sized types can only compute bounds that are linear in the sizes of the arguments of a function. This work presents a novel type system that derives polynomial bounds from first-order functional programs. As pioneered by Hofmann and Jost for linear bounds, it relies on the potential method of amortized analysis. Types are annotated with multivariate resource polynomials, a rich class of functions that generalize non-negative linear combinations of binomial coefficients. The main theorem states that type derivations establish resource bounds that are sound with respect to the resource-consumption of programs which is formalized by a big-step operational semantics. Simple local type rules allow for an efficient inference algorithm for the type annotations which relies on linear constraint solving only. This gives rise to an analysis system that is fully automatic if a maximal degree of the bounding polynomials is given. The analysis is generic in the resource of interest and can derive bounds on time and space usage. The bounds are naturally closed under composition and eventually summarized in closed, easily understood formulas. The practicability of this automatic amortized analysis is verified with a publicly available implementation and a reproducible experimental evaluation. The experiments with a wide range of examples from functional programming show that the inference of the bounds only takes a couple of seconds in most cases. The derived heap-space and evaluation-step bounds are compared with the measured worst-case behavior of the programs. Most bounds are asymptotically tight, and the constant factors are close or even identical to the optimal ones. For the first time we are able to automatically and precisely analyze the resource consumption of involved programs such as quick sort for lists of lists, longest common subsequence via dynamic programming, and multiplication of a list of matrices with different, <b>fitting</b> <b>dimensions...</b>|$|R
50|$|B16.22- Wrought Copper and Copper Alloy Solder Joint Pressure Fittings. This Standard {{establishes}} {{specifications for}} wrought copper and wrought copper alloy, solder-joint, seamless fittings, {{designed for use}} with seamless copper tube conforming to ASTM B88 (water and general plumbing systems), B280 (air conditioning and refrigeration service), and B819 (medical gas systems), as well as fittings intended to be assembled with soldering materials conforming to ASTM B32, brazing materials conforming to AWS A5.8, or with tapered pipe thread conforming to ASME B1.20.1. This Standard is allied with ASME B16.18, which covers cast copper alloy pressure fittings. It provides requirements for fitting ends suitable for soldering. This Standard covers pressure-temperature ratings, abbreviations for end connections, size and method of designating openings of <b>fittings,</b> marking, material, <b>dimensions</b> and tolerances, and tests.|$|R
5000|$|Best cast bullet {{accuracy}} in the [...]444 Marlin is attained when utilizing bullets sized to [...]432" [...] diameter, {{both in the}} older [...] "Micro-Grooved" [...] and the newer [...] "Ballard" [...] style barrels. This bullet diameter is dictated more by the large diameter of chamber throats than by groove diameter of the barrel. A projectile closely <b>fitting</b> the throat <b>dimensions</b> greatly enhances the cast bullet performance of this cartridge. Those writers and publications citing {{the inability of the}} [...]444 Marlin's Micro-Groove barrel to accurately shoot cast bullets driven over 1600 ft/s. are simply in error, in that those results were largely obtained using [...]429" [...] and [...]430" [...] diameter cast bullets. Full factory velocity handloads when assembled using hard-cast, gas-checked bullets of [...]432" [...] diameter will rival accuracy of any jacketed ammunition for this cartridge.|$|R
40|$|Most of {{the time}} series data mining tasks attempt to {{discover}} data patterns that appear frequently. Abnormal data is often ignored as noise. There are some data mining techniques based on time series to extract anomaly. However, most of these techniques cannot suit big unstable data existing in various fields. Their key problems are high <b>fitting</b> error after <b>dimension</b> reduction and low accuracy of mining results. This paper studies an approach of mining time series abnormal patterns in the hydrological field. The authors propose a new idea {{to solve the problem}} of hydrological anomaly mining based on time series. They propose Feature Points Symbolic Aggregate Approximation (FP-SAX) to improve the selection of feature points, and then measures the distance of strings by Symbol Distance based Dynamic Time Warping (SD-DTW). Finally, the distances generated are sorted. A set of dedicated experiments are performed to validate the authors' approach. The results show that their approach has lower fitting error and higher accuracy compared to other approaches. Department of Computin...|$|R
5000|$|Many modern {{implementations}} of GAMs {{and their}} extensions are {{built around the}} reduced rank smoothing approach, because it allows well founded estimation of the smoothness of the component smooths at comparatively modest computational cost, and also facilitates implementation {{of a number of}} model extensions {{in a way that is}} more difficult with other methods. At its simplest the idea is to replace the unknown smooth functions in the model with basis expansions where the [...] are known basis functions, usually chosen for good approximation theoretic properties (for example B splines or reduced rank thin plate splines), and the [...] are coefficients to be estimated as part of model <b>fitting.</b> The basis <b>dimension</b> [...] is chosen to be sufficiently large that we expect it to overfit the data to hand (thereby avoiding bias from model over-simplification), but small enough to retain computational efficiency. If [...] then the computational cost of model estimation this way will be [...]|$|R
40|$|ABSTRACT: Enamel matrix {{self-assembly}} {{has long}} been suggested as {{the driving force behind}} aligned nanofibrous hydroxyapatite formation. We tested if amelogenin, the main enamel matrix protein, can self-assemble into ribbon-like structures in physiologic solutions. Ribbons 17 nm wide were observed to grow several micrometers in length, requiring calcium, phosphate, and pH 4. 0 − 6. 0. The pH range suggests that the formation of ion bridges through protonated histidine residues is essential to self-assembly, supported by a statistical analysis of 212 phosphate-binding proteins predicting 12 phosphate-binding histidines. Thermophoretic analysis verified the importance of calcium and phosphate in self-assembly. X-ray scattering characterized amelo-genin dimers with <b>dimensions</b> <b>fitting</b> the cross-section of the amelogenin ribbon, leading to the hypothesis that antiparallel dimers are the building blocks of the ribbons. Over 5 − 7 days, ribbons self-organized into bundles composed of aligned ribbons mimicking the structure of enamel crystallites in enamel rods. These observations confirm reports of filamentous organic components in developing enamel and provide a new model for matrix-templated enamel mineralization...|$|R
40|$|I {{summarize}} {{the motivation for}} the effective field theory approach to nuclear physics, {{and some of its}} recent accomplishments. 1 Why effective field theory, why nuclear physics? Low energy data is generally insensitive to the details of interactions at short distance. It is therefore difficult to learn about short range interactions; yet by the same token, complete knowledge of the physics at short distances is not required for an accurate understanding of experiments. Effective field theory exploits this fact. The effects of nonlocal interactions at short distance may be represented in terms of local operators in a derivative expansion — the effective Lagrangian. The higher an operator’s dimension, the smaller the effect it has on low energy physics, and hence one can obtain a useful phenomenological theory by retaining operators only up to some <b>dimension,</b> <b>fitting</b> their coefficients to data. Some effective theories are quite useful, such as chiral perturbation theory; some are wildly successful, such as the standard model of particle physics. In this talk I will discuss a new application currentl...|$|R
40|$|International audienceBeing concern by {{scaling down}} thermoacoustic coolers to provide {{practical}} solutions for thermal heat management, especially in microcircuits, a current architecture {{has been proposed}} recently [Acta Acustica 97 (6), 926 - 932, November 2011]. A non-resonant small cavity <b>fitting</b> the stack <b>dimensions</b> is driven by two loudspeakers coupled through the stack. One of them creates the acoustic pressure field inside the stack while the other one creates the particle velocity field. This cooler has both advantages of being compact and flexible, as the acoustic field in the stack can be controlled to access the optimal field which optimizes thermoacoustic effects. Moreover, the working frequency {{is not related to}} resonance conditions, therefore either a quasi-isothermal stack (regenerator) or a quasi-adiabatic stack can be used. Experimental results, which validates theoretical ones [1], are presented to illustrate the thermal behaviour of a stack and a regenerator in this device. The performances compared with those of classical devices having equivalent stack (standing wave or Stirling devices) show the potentiality of this compact thermoacoustic cooler...|$|R
40|$|In {{this paper}} we draw on impact {{assessment}} {{work of the}} Australian Centre for International Agricultural Research (ACIAR) to present an example of meta-evaluation – an evaluation of evaluations – in an agricultural research, development and extension setting. We explore quality issues relating to evaluation studies {{in the context of}} government institutions. Program evaluation standards (PES) are divided into categories of utility, feasibility, propriety and accuracy to provide a framework for the meta-evaluation. The PES are presented as a universal measure of evaluation study quality. The intent of using them here is to judge the adequacy of PES as a universal quality measure or meta-evaluation base and to extract useful insights from ACIAR program evaluation activities when developing a meta-evaluation model for the Limpopo Department of Agriculture (LDA). Our meta-evaluation is undertaken of 63 impact assessment reports. First, the literature guiding the conduct of a meta-evaluation is reviewed. Second, an assessment (the meta-evaluation) of the evaluation studies is carried out for 19 sampled reports from a population of relevant reports <b>fitting</b> the <b>dimension</b> of the analysis, and results are presented and discussed. Also, lessons learned are presented, using the framework provided by the meta-evaluation criteria. Third, taking into account the lessons learned, implications are drawn for a proposed systematic meta-evaluation of the LDA. Finally, we conclude that all the PES cannot be equally emphasized in a meta-evaluation model. At ACIAR, 70 % of the standards were at least partially addressed. Therefore, we succeeded in using the PES in judging the ACIAR evaluation quality. As such, they can be an important base when developing an evaluation model but should be applied in a contextualized manner. Meta-evaluation, Evaluation Quality, Program Evaluation Standards, Evaluation Model, Australian Centre for International Agricultural Research, Limpopo Department of Agriculture (South Africa), International Development,...|$|R
40|$|A {{computer}} program, GelExplorer, {{which uses}} a new methodology for obtaining quantitative information about electrophoresis has been developed. It provides a straightforward, easy-to-use graphical interface, and includes a number of features which offer significant advantages over existing methods for quantitative gel analysis. The method uses curve fitting with a nonlinear least-squares optimization to deconvolute overlapping bands. Unlike most curve fitting approaches, the data is treated in two <b>dimensions,</b> <b>fitting</b> all the data across the entire width of the lane. This allows for accurate determination of the intensities of individual, overlapping bands, and in particular allows imperfectly shaped bands to be accurately modeled. Experiments described in this paper demonstrate empirically that the Lorentzian lineshape reproduces the contours of an individual gel band and provides a better model than the Gaussian function for curve fitting of electrophoresis bands. Results from several fitting applications are presented and {{a discussion of the}} sources and magnitudes of uncertainties in the results is included. Finally, the method is applied to the quantitative analysis of a hydroxyl radical footprint titration experiment to obtain the free energy of binding of the l repressor protein to the O R 1 operator DNA sequence...|$|R
40|$|Anthropometric {{knowledge}} is {{most frequently used}} by designers and product evaluators in the form one-dimensional data to verify whether the product <b>dimension</b> is <b>fitting</b> the human <b>dimension.</b> Several ways of how anthropometric data are used can be distinguished in this matter: - Ego-design: your own body dimension as a guide; - Average-design: body dimensions of the average as a guide; - Design for P 5 : body dimensions of the smallest person as a guide; - Design for P 95 : body dimensions of the largest person as a guide; - Design for P 5 -P 95; body dimensions of the smallest and largest person as a guide. This type is used most commonly and means that excluding 10 % is acceptable. - Design for all: implies the continuous effort during the design process to exclude as few persons as possible To make this anthropometric world easier to understand two tools are discussed. The tool ‘Ellipse’ will demonstrate {{how easy it is}} to analyse a fit-problem with multiple 2 D views. The tool ‘Persona’ will visualise the geometrical problems in the human-product-interaction with living persons or with digital models...|$|R
40|$|Advances {{in several}} key sensor {{technologies}} {{make it possible}} now to build a high performance, very compact and low cost imaging spectrometer for spacebourne terrestrial remote sensing. We describe an instrument based on a highly innovative optical design that incorporates {{state of the art}} focal plane arrays, electronics, focal plane cooling and dimensionally stable ceramics for the optical elements and structure. The instrument is optimized for viewing the earth 2 ̆ 7 s solid surface and adjacent coastal oceans. It has very high signal-to-noise performance over the full spectral range covered, from 400 to 2450 nanometers (nm). Spectral sampling is in 200 10 -nm wide, contiguous bands. The instrument combines a high spatial resolution panchromatic imaging system with a modest spatial resolution imaging spectrometer. It weighs 25 kg, requires less than 100 watts of power, and is approximately 30 by 20 by 10 cm in <b>dimension,</b> <b>fitting</b> well within the capacity of Pegasus-class small spacecraft missions. The instrument is well suited to support studies in earth system science as well as commercial remote sensing. Several applications in these areas will be highlighted to set in context the performance requirements that were used to define the sensor design and choice of sensor technologies...|$|R
40|$|Quantitative {{analysis}} of re-routing a high-head, low-flow, run-of-the-river, under- 100 kW {{hydroelectric power plant}} penstock and comparison with the original route for purposes of head loss estimation and available hydraulic power predictionPart of the infrastructure of a disused mill in Dee, Oregon is being converted into a hydroelectric power station. A turbine and generator combination must be selected to match the flow and pressure {{at the bottom of}} the penstock. One old penstock route is being abandoned and replaced with a new route. Inaccurate flow versus pressure measurements exist for the old penstock route. In order to estimate the available hydraulic power which can be expected from the new penstock route, both routes were mapped with special attention paid to elevation changes. Total head was found to be 473 feet over a distance of roughly 3. 8 miles. The effects of <b>fittings</b> and pipe <b>dimensions,</b> materials and ages on pressure dissipation were accounted for via minor loss correlations and both Darcy-Weisbach and Hazen-Williams equations. For each penstock route, total head loss was calculated at various flow rates from which total hydraulic power could be estimated. The head loss calculated in the old route was compared with the existing measurements. At the permitted water right flow rate of 2. 5 cubic feet per second, hydraulic power is predicted to be 93 kilo Watts using the new route and a Pelton turbine is recommended. While the original measurements corroborate this finding, corrections made to the measurements suggest that 80 kilo Watts is a more reasonable power estimate. It is recommended that new flow versus pressure measurements be taken before a final turbine selection is made...|$|R
40|$|BACKGROUND: The {{increased}} survival {{chances of}} extremely low-birthweight (ELBW) infants (weighing < 1000 g at birth) {{has led to}} concern about their behavioural outcome in childhood. In reports from several countries with different assessments at various ages, investigators have noted a higher frequency of behavioural problems in such infants, but cross-cultural comparisons are lacking. Our aim was to compare behavioural problems in ELBW children of similar ages from four countries. METHODS: We prospectively studied 408 ELBW children aged 8 - 10 years, whose parents completed the child behaviour checklist. The children came from the Netherlands, Germany, Canada, and USA. The checklist provides a total problem score consisting of eight narrow-band scales. Of these, two (aggressive and delinquent behaviour) give a broad-band externalising score, three (anxious, somatic, and withdrawn behaviour) give a broad-band internalising score, and three (social, thought, and attention problems) indicate difficulties <b>fitting</b> neither broad-band <b>dimension.</b> For each cohort we analysed scores in ELBW children and those in normal- birthweight controls (two cohorts) or national normative controls (two cohorts). Across countries, we assessed deviations of the ELBW children from normative or control groups. FINDINGS: ELBW children had higher total problem scores than normative or control children, but this increase was only significant in European countries. Narrow-band scores were raised only for the social, thought, and attention difficulty scales, which were 0. 5 - 1. 2 SD higher in ELBW children than in others. Except for the increase in internalising scores recorded for one cohort, ELBW children {{did not differ from}} normative or control children on internalising or externalising scales. INTERPRETATION: Despite cultural differences, types of behavioural problems seen in ELBW children were very similar in the four countries. This finding suggests that biological mechanisms contribute to behavioural problems of ELBW children...|$|R
40|$|International audienceThis work proposes new semi-analytical {{solutions}} for the interpretation of cross-borehole slug tests in fractured media. Our model {{is an extension of}} a previous work by Barker (1988) and Butler and Zhan (2004). It includes inertial effects at both test and observation wells and a fractional flow dimension in the aquifer. The model has 5 <b>fitting</b> parameters: flow <b>dimension</b> n, hydraulic conductivity K, specific storage coefficient Ss, and effective lengths of test well Le and of observation well Leo. The results of a sensitivity analysis show that the most sensitive parameter is the flow dimension n. The model sensitivity to other parameters may be ranked as follows: K > Le ~ Leo > Ss. The sensitivity to aquifer storage remains one or two orders of magnitude lower than that to other parameters. The model has been coupled to an automatic inversion algorithm for facilitating the interpretation of real field data. This inversion algorithm is based on a Gauss-Newton optimization procedure conditioned by re-scaled sensitivities. It has been used to interpret successfully cross-borehole slug test data from the Hydrogeological Experimental Site (HES) of Poitiers, France, consisting of fractured and karstic limestones. HES data provide flow dimension values ranging between 1. 6 and 2. 5, and hydraulic conductivity values ranging between 4. 4 x 10 - 5 and 7. 7 x 10 - 4 m. s- 1. These values are consistent with previous interpretations of single-well slug tests. The results of the sensitivity analysis are confirmed by calculations of relative errors on parameter estimates, which show that accuracy on n and K is below 20 % and that on Ss is about one order of magnitude. The K-values interpreted from cross-borehole slug tests are one order of magnitude higher than those previously interpreted from interference pumping tests. These findings suggest that cross-borehole slug tests focus on preferential flowpath networks made by fractures and karstic channels, i. e. the head perturbation induced by a slug test propagates only through those flowpaths with the lowest hydraulic resistance. As a result, cross-borehole slug tests are expected to identify the hydrodynamic properties of karstic channels and fracture flowpaths, and may be considered as complementary to pumping tests which more likely provide bulk properties of the whole fracture/karstic-channel/matrix system...|$|R
40|$|The goal of {{the first}} project was to {{evaluate}} strategies for determining the in vitro intrinsic clearance (CLint) of dextrorphan (DR) as metabolized by the UGT 2 B 7 enzyme to obtain dextrorphan glucuronide (DR-G). A direct injection liquid chromatography-mass spectrometry (LC-MS) method was used to monitor products using the pseudo-first-order (PFO) model. Standard enzymatic incubations were also quantified using LC-MS. These data were fit utilizing both PFO and Michaelis-Menten (MM) models to determine estimates of kinetic parameters. The CLint {{was determined to be}} 0. 28 (± 0. 08) µL/min/mg protein for a baculovirus insect cell-expressed UGT 2 B 7 enzyme. This is the first confirmation that dextrorphan is specifically metabolized by UGT 2 B 7 and the first report of these kinetic parameters. Simulated chromatographic data were used to determine the precision and accuracy in the estimation of peak volumes in comprehensive two-dimensional liquid chromatography (2 D-LC). Volumes were determined both by summing the areas in the second dimension chromatograms via the moments method and by <b>fitting</b> the second <b>dimension</b> areas to a Gaussian peak. When only two second dimension signals are substantially above baseline, the accuracy and precision are poor because the solution to the Gaussian fitting algorithm is indeterminate. The fit of a Gaussian peak to the areas of the second dimension peaks is better at predicting the peak volume when {{there are at least three}} second dimension injections above the limit of detection. Based on simulations where the sampling interval and sampling phase were varied, we conclude for well-resolved peaks that the optimum precision in peak volumes in 2 D separations will be obtained when the sampling ratio is approximately two. This provides an RSD of approximately 2 % for the signal-to-noise (S/N) used in this work. The precision of peak volume estimation for experimental data was also assessed, and RSD values were in the 4 - 5 % range. We conclude that the poorer precision found in the 2 D-LC experimental data as compared to 1 D-LC is due to a combination of factors, including variations in the first dimension peak shape related to undersampling and loss in S/N due to the injection of multiple smaller peaks onto the second dimension column...|$|R
40|$|The Group of Astrodynamics of University of Roma "La Sapienza" (GAUSS) {{established}} the UNISAT {{program at the}} Scuola di Ingegneria Aerospaziale of the University of Roma "La Sapienza". It is an educational project for designing, manufacturing and operating in orbit small microsatellites. The main goal of the UNISAT program is the education, {{with the aim of}} the participation of the students in all the phases of a real space program, from the initial mission concept to the operations in orbit. There is a strict time constraint on the mission development, which must fit in the two years didactical plan of the students. In the framework of this project four microsatellite have already been launched from Baikonour cosmodrome using the DNEPR launch vehicle, and UNISAT- 4 is scheduled to be launched on May 2006. The program has now achieved an experience of several years, in which the methods of hands-on education have been employed, along with traditional class teaching, to improve the students skills. Many program organization issues have been faced in these years, to improve and verify the didactical effectiveness of the teaching method followed. The paper describes briefly the program, focussing on the didactical aspects and describing the direct experience of some students, involved {{in the design of the}} satellite subsystems. Other goal of the UNISAT program is testing in orbit and space qualifying the terrestrial off the shelf commercial and industrial components and technologies, yielding a direct technological interest for the industries and contributing to keep low the program cost and affordable for the University research budget. The main microsatellite subsystems are described in this paper, showing the students contribution to the program and their involvement in the decision process of the design, manufacturing and testing. The design process includes the realization of a virtual "digital mock-up" of the satellite, in which all the parts are simulated to verify <b>dimensions,</b> <b>fitting</b> conflicts, and weight distributions. The electronic boards have been completely designed by the students. A significant effort has been devoted to the ground station automation, including remote operation by accredited users. In particular high school students are involved in an education program for space culture diffusion among young people...|$|R
40|$|INTRODUCTIONThis {{research}} {{investigates the}} assessment for {{the rehabilitation of}} the Pretoria State Garage (PSG) {{for the purpose of}} accommodating the Pretoria Technology Park (PTP). The Pretoria State Garage courtyard is comprised of industrial type of buildings, most of which stand obsolete due to a shift in the manufacturing process. These buildings are structurally sound and historically significant. They offer a major opportunity for conversion to attract business through providing relatively inexpensive commercial and industrial spaces to small and medium sized companies. The site is located in the southwest quadrant of Pretoria Central in which a number of sites and buildings are currently being downgraded in the urban revitalisation process, due to preferred, other technologies of construction. The assessment for the rehabilitation of the site evaluates the spatial qualities and the physical forms of buildings in relation to the new user, while the establishment of the PTP focuses on maintaining, elongating and innovating the industrial manufacturing tradition of the site. The incorporation of the PTP on the PSG site is implemented through the fitting process. RESEARCH AREAThe research focuses on the three main areas dealing with: Programming and planning of the Pretoria Technology Park Rehabilitation assessment of the Pretoria State Garage Architectural fitting process 1. Programming and planning of the Pretoria Technology Park The development methodology is employed {{for the establishment of a}} sustainable Pretoria Technology Park, which stems from local technology demand, supply and transfer. As a result, different centres are established to provide accommodation and services to major activities of the park dealing with administration, provision of advanced technology services and accommodation for technology-based firms. Individual centres are discussed according to their envisaged spatial qualities and design specifications. 2. Rehabilitation assessment of the Pretoria State Garage The rehabilitation assessment explores the historic significance and architectural values of the PSG. The outcome of the rehabilitation assessment defmes the manner and the degree in which various rehabilitation interventions could be executed. The process includes proposals for the demolition of unwanted structures and elements, investigating the long-term resilience of buildings to be retained and assessing negative physical features of the site. 3. Architectural <b>fitting</b> process Specific <b>dimensions</b> and spatial requirements of both the PSG (physical) and the PTP (intellectual) respectively are compared for the purpose of mutual fitting. Dissertation (MArch) [...] University of Pretoria, 2005. Architectureunrestricte...|$|R

