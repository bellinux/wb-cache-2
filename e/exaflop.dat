30|21|Public
50|$|The MRF's next goal is {{to achieve}} a machine capable of one <b>exaflop</b> (1018 floating-point {{operations}} per second) and then one zetaflop (1021). To achieve an <b>exaflop</b> machine by 2018, the NSA has proposed constructing two connecting buildings, totaling 260000 sq ft, called the Multiprogram Computational Data Center. The buildings will store dozens of computer cabinets that will comprise the <b>exaflop</b> machine. The facility will eventually use about 200 megawatts of power—enough to power around 200,000 homes—and will require 60000 ST of cooling equipment.|$|E
5000|$|Next {{generation}} supercomputers requiring drastically better {{energy efficiency}} to allow systems to scale to <b>exaflop</b> computing levels.|$|E
50|$|In 2008, James Bamford's book The Shadow Factory {{reported}} that NSA told the Pentagon {{it would need}} an <b>exaflop</b> computer by 2018.|$|E
5000|$|In June 2016, Japanese firm Fujitsu {{announced}} at the International Supercomputing Conference that its future exascale supercomputer will feature processors of its own design that implement the ARMv8 architecture. The Flagship2020 program, by Fujitsu for RIKEN plans to break the <b>exaflops</b> barrier by 2020 (and [...] "it looks like China and France {{have a chance to}} do so and that the United States is contentfor the moment at leastto wait until 2023 to break through the <b>exaflops</b> barrier.") These processors will also implement extensions to the ARMv8 architecture equivalent to HPC-ACE2 that Fujitsu is developing with ARM Holdings.|$|R
50|$|In July 2009, it was {{announced}} that C-DAC was developing a new high-speed PARAM. It was expected to be unveiled by 2012 and was expected to break the 1 PetaFLOPS barrier. In November 2014 it was reported that India is working on the Fastest supercomputer ever which is set to work at 132 <b>Exaflops</b> per second.|$|R
5000|$|BINA48 - A social robot (Breakthrough Intelligence via Neural Architecture 48 - 48 <b>exaflops</b> is {{the speed}} at which the human brain is purported to compute) that is an early {{demonstration}} of the potential for using [...] "mindfiles" [...] for transferring human consciousness information to new forms. TMF encourages public dialog about cyber-consciousness via public presentations and social media.|$|R
50|$|In 2012 the Indian Government has {{proposed}} to commit 2.5 billion USD to supercomputing research during the 12th five-year plan period (2012-2017). The {{project will be}} handled by Indian Institute of Science (IISc), Bangalore. Additionally, it was later revealed that India plans to develop a supercomputer with processing power in the <b>exaflop</b> range. It will be developed by C-DAC within the subsequent 5 years of approval.|$|E
40|$|Abstract—A {{recent study}} shows that {{computation}} per kilowatthour has doubled every 1. 57 years, akin to Moore’s Law. While this trend is encouraging, its implications to high-performance computing (HPC) are not yet clear. For instance, DARPA’s target of a 20 -MW <b>exaflop</b> system will require a 56. 8 -fold performance improvement with only a 2. 4 -fold increase in power consumption, which seems unachievable {{in light of the}} above trend. To provide a more comprehensive perspective, we analyze current trends in energy efficiency from the Green 500 and project expectations for the near future. Specifically, we first provide an analysis of energy efficiency trends in HPC systems from the Green 500. We then model and forecast the energy efficiency of future HPC systems. Next, we present exascalar – a holistic metric to measure the distance from the <b>exaflop</b> goal. Finally, we discuss our efforts to standardize power measurement methodologies in order to provide the community with reliable and accurate efficiency data. I...|$|E
40|$|Abstract—While the HPC {{community}} is working towards {{the development of}} the first <b>Exaflop</b> computer (expected around 2020), after reaching the Petaflop milestone in 2008 still only few HPC applications are able to fully exploit the capabilities of Petaflop systems. In this paper we argue that efforts for preparing HPC applications for Exascale should start before such systems become available. We identify challenges {{that need to be addressed}} and recommend solutions in key areas of interest, including formal modeling, static analysis and op-timization, runtime analysis and optimization, and autonomic computing. Furthermore, we outline a conceptual framework for porting HPC applications to future Exascale computing systems and propose steps for its implementation. I...|$|E
50|$|Exascale {{computing}} {{refers to}} computing systems capable {{of at least}} one exaFLOPS, or a billion billion calculations per second. Such capacity represents a thousandfold increase over the first petascale computer that came into operation in 2008. (One <b>exaflops</b> is a thousand petaflops or a quintillion, 1018, floating point operations per second.) At a supercomputing conference in 2009, Computerworld projected exascale implementation by 2018.|$|R
40|$|Fault {{tolerance}} overhead of {{high performance}} computing (HPC) applications is becoming {{critical to the}} efficient utilization of HPC systems at large scale. HPC applications typically tolerate fail-stop failures by checkpointing. Another promising method is in the algorithm level, called algorithmic recovery. These two methods can achieve high efficiency when the system scale is not very large, but will both lose their effectiveness when systems approach the scale of <b>Exaflops,</b> where the number of processors including in system is expected to achieve one million. This paper develops a new and efficient algorithm-based fault tolerance scheme for HPC applications. When failure occurs during the execution, we do not stop {{to wait for the}} recovery of corrupted data, but replace them with the corresponding redundant data and continue the execution. A background accelerated recovery method is also proposed to rebuild redundancy to tolerate multiple times of failures during the execution. To demonstrate the feasibility of our new scheme, we have incorporated it to the High Performance Linpack. Theoretical analysis demonstrates that our new fault tolerance scheme can still be effective even when the system scale achieves the <b>Exaflops.</b> Experiment using SiCortex SC 5832 verifies the feasibility of the scheme, and indicates that the advantage of our scheme can be observable even in a small scale. Comment: 11 pages, 8 figures, 1 table, submitted to conference SC 201...|$|R
40|$|Exascale computers, {{the next}} {{generation}} of high performance computers, are expected to process 1 <b>exaflops</b> around 2018. However the processor cores used in these systems are very likely to suffer from unpre- dictable high variability in performance. We built a prototype general- purpose reactive work rebalancer that handles such performance vari- ability with low overhead. We did an experimental validation by devel- oping a reactive rebalancer library in UPC, and using it in a 5 -point stencil (heat) simulation. The experiments show that our approach has very limited overhead that compensates for runtime processor speed variations, with or without simulated processor slowdowns. status: publishe...|$|R
40|$|Today’s {{supercomputers}} {{are built}} from the state-of-the-art components to extract as much performance {{as possible to}} solve the most computationally intensive problems in the world. Building {{the next generation of}} exascale supercomputers, however, would require re-architecting many of these components to extract over 50 x more performance than the current fastest supercomputer in the United States. To contribute towards this goal, two aspects of the compute node architecture were examined in this thesis: the on-chip interconnect topology and the memory and storage checkpointing platforms. As a first step, a skeleton exascale system was modeled to meet 1 <b>exaflop</b> of performance along with 100 petabytes of main memory. The model revealed that large kilo-core processors would be necessary to meet the <b>exaflop</b> performance goal; existing topologies, however, would not scale to those levels. To address this new challenge, we investigated and proposed asymmetric high-radix topologies that decoupled local and global communications and used different radix routers for switching network traffic at each level. The proposed topologies scaled more readily to higher numbers of cores with better latency and energy consumption than before. The vast number of components that the model revealed would be needed in these exascale systems cautioned towards better fault tolerance mechanisms. To address this challenge, we showed that local checkpoints within the compute node can be saved to a hybrid DRAM and SSD platform in order to write them faster without wearing out the SSD or consuming a lot of energy. A hybrid checkpointing platform allowed more frequent checkpoints to be made without sacrificing performance. Subsequently, we proposed switching to a DIMM-based SSD in order to perform fine-grained I/O operations that would be integral in interleaving checkpointing and computation while still providing persistence guarantees. Two more techniques that consolidate and overlap checkpointing were designed to better hide the checkpointing latency to the SSD...|$|E
40|$|While the HPC {{community}} is working towards {{the development of}} the first <b>Exaflop</b> computer (expected around 2020), after reaching the Petaflop milestone in 2008 still only few HPC applications are able to fully exploit the capabilities of Petaflop systems. In this paper we argue that efforts for preparing HPC applications for Exascale should start before such systems become available. We identify challenges {{that need to be addressed}} and recommend solutions in key areas of interest, including formal modeling, static analysis and optimization, runtime analysis and optimization, and autonomic computing. Furthermore, we outline a conceptual framework for porting HPC applications to future Exascale computing systems and propose steps for its implementation. Comment: 18 th International Conference on Network-Based Information Systems (NBiS 2015). 2 - 4 September 2015 in Tamkang, Taiwa...|$|E
40|$|High {{performance}} computing (HPC) has crossed the Petaflop mark and is reaching the <b>Exaflop</b> range quickly. The exascale system {{is projected to}} have millions of nodes, with thousands of cores for each node. At such an extreme scale, the substantial amount of concurrency can cause a critical contention issue for I/O system. The contention can destroy the request locality, increase the access latency, and waste the precious I/O interconnection bandwidth. This study proposes a dynamically coordinated I/O architecture for exascale systems. The fundamental idea is to coordinate I/O accesses according to access pattern, network topology, interconnection condition, and data distribution on storage devices to reduce the contention and regain the locality. The preliminary results have shown {{the promise of a}} dynamically coordinated I/O architecture. It has a potential to manage the substantial amount of I/O concurrency and provides a scalable I/O architecture for exascale systems...|$|E
40|$|Abstract—Traditional radio {{telescope}}s use large steel dishes to observe radio sources. The largest radio telescope in the world, LOFAR, uses {{tens of thousands}} of fixed, omni-directional antennas instead, a novel design that promises ground-breaking research in astronomy. Where traditional tele-scopes use custom-built hardware, LOFAR uses software to do signal processing in real time. This leads to an instrument that is inherently more flexible. However, the enormous data rates and processing requirements (tens to hundreds of teraflops) make this extremely challenging. The next-generation telescope, the SKA, will require <b>exaflops.</b> Unlike traditional instruments, LOFAR and SKA can observe in hundreds of directions simultaneously, using beam forming. This is useful, for example, to search the sky for pulsars (i. e. rapidly rotating highly magnetized neutron stars). Beam forming is an importan...|$|R
40|$|As an {{industrial}} user with very high {{stakes in the}} operation and maintenance of complex systems like nuclear power plants, EDF has been engaged into simulation for many years. We feel that <b>Exaflops</b> software should not only be thought {{as a way of}} tackling daunting research problems but should also take into account the sometimes equally daunting requirements that stem from {{an industrial}} usage perspective. We do feel that, whatever the hard changes that will probably have to be made on various software aspects, we should not loose sight that continuity paths have also to be found in order to make those big changes acceptable and profitable to many. We identified in this paper what we think are priority research themes that could benefit of an international collaboratio...|$|R
40|$|A recent {{development}} in radio astronomy is to replace traditional dishes with many small antennas. The signals are combined to form one large, virtual telescope. The enormous data streams are crosscorrelated {{to filter out}} noise. This is especially challenging, since the computational demands grow quadratically {{with the number of}} data streams. Moreover, the correlator is not only computationally intensive, but also very I/O intensive. The LOFAR telescope, for instance, will produce over 100 terabytes per day. The future SKA telescope will even require in the order of <b>exaflops,</b> and petabits/s of I/O. A recent trend is to correlate in software instead of dedicated hardware. This is done to increase flexibility and to reduce development efforts. Examples include e-VLBI and LOFAR. In this paper, we evaluate the correlator algorithm on multi-core CPUs and many-core architectures, such as NVIDIA and ATI GPUs...|$|R
40|$|Monte Carlo codes {{were adapted}} to vector {{computers}} in the 1980 s, clusters and parallel {{computers in the}} 1990 s, and teraflop systems in the 2000 s. Recent advances include hierarchical parallelism, combining threaded calculations on multicore processors with message-passing among different nodes. With the advances in computing, Monte Carlo codes have evolved with new capabilities and new ways of use. Production codes such as MCNP, MVP, MONK, TRIPOLI, MCU, and KENO are now 20 - 30 years old (or more) and are very rich in advanced features. The former “method of last resort ” has now become the first choice for many applications. Calculations are now routinely per-formed on office computers, not just on supercomputers. Current research and development efforts are investigating the use of Monte Carlo methods on FPGAs, GPUs, and many-core processors. Other far-reaching research is explor-ing ways to adapt Monte Carlo methods to future <b>exaflop</b> systems that may have 1 M or more concurrent computational processes...|$|E
40|$|Chemistry Climate Model (CCM) {{codes are}} {{important}} csusft 2 cbooatpfccm {{to understand how}} to mitigate global warming ifcoccmaapoape,pasgw. However, to produce meaningful results, CCM codes require performance in the scale of PetaFlop (and soon <b>ExaFlop)</b> eesiteri calculations. These scales have to be achieved within a reasonable power budget. It is therefore important to speedup as much as possible the execution of the already highly optimized state-of-the-art CCM codes. Fast-J tfs is a very important and widely used CCM code for simulations at different scales of magnitude, i. e., local, global and cosmic. Each scale of magnitude and its performance rely on a code, the Fast-J core code. In this thesis, we speedup the Fast-J core, selecting and implementing some high-level compiler transformations, which are not efficiently performed by CPU compilers. The optimizations consistently reach a performance speedup always greater than 4. 5 for each scale of simulation. To quantify this performance improvement, we compare the execution times of the new optimizations with the execution times of the state-of-the-art, already highly optimized, CPU multi-threading code...|$|E
40|$|During {{the last}} years, High Performance Computing (HPC) {{resources}} have undergone a dramatic transformation, with an explosion on the available parallelism {{and the use}} of special purpose processors. There are international initiatives focusing on redesigning hardware and software in order to achieve the <b>Exaflop</b> capability. With this aim, the HPC 4 E project is applying the new exascale HPC techniques to energy industry simulations, customizing them if necessary, and going beyond the state-of-the-art in the required HPC exascale simulations for different energy sources that are the present and the future of energy: wind energy production and design, efficient combustion systems for biomass-derived fuels (biogas), and exploration geophysics for hydrocarbon reservoirs. HPC 4 E joins efforts of several institutions settled in Brazil and Europe. The research leading to these results has received funding from the European Union's Horizon 2020 Programme (2014 - 2020) and from Brazilian Ministry of Science, Technology and Innovation through Rede Nacional de Pesquisa (RNP) under the HPC 4 E Project (www. hpc 4 e. eu), grant agreement n° 689772. Postprint (author's final draft...|$|E
40|$|Abstract—Modern {{scientific}} discovery {{is driven by}} an in-satiable demand for computing performance. The HPC com-munity is targeting development of supercomputers able to sustain 1 <b>ExaFlops</b> by the year 2020 and power consumption is the primary obstacle to achieving this goal. A combination of architectural improvements, circuit design, and manufacturing technologies must provide over a 20 × improvement in energy efficiency. In this paper, we present some of the progress NVIDIA Research is making toward the design of Exascale systems by tailoring features to address the scaling challenges of performance and energy efficiency. We evaluate several architectural concepts {{for a set of}} HPC applications demonstrating expected energy efficiency improvements resulting from circuit and packaging innovations such as low-voltage SRAM, low-energy signaling, and on-package memory. Finally, we discuss the scaling of these features with respect to future process technologies and provide power and performance projections for our Exascale research architecture. I...|$|R
40|$|Abstract: In the {{preprint}} {{a calculation}} methodology for the modeling of two phase flows at {{the scale of}} local equilibrium is presented. The described schematics {{is based on the}} DNS approach and is intended for the usage with the supercomputers (with the prospects of achieving of <b>exaflops</b> performance). The model is based on the usual concept of single-velocity continuum, where the phases differ in the density level. Also the model includes the EOS of Van der Waals type with the possibility to have the phase transition. At this such EOS allows performing the uniform calculations and including into the consideration various non-equilibrium processes. The Euler equations with heat conduction effects form the hydrodynamic background. It is demonstrated the capacity of proposed methodology to describe phase transitions realistically, to monitor the boundaries of phases’ separation clearly and to follow the processes like bubbles merging. Also such methodology in its basics does not depend on the dimensionality of problems. Note: Publication language:russia...|$|R
40|$|Due to {{processors}} {{reaching the}} maximum performance allowable by current technology, architectural trends for computer systems {{continue to increase}} the number of cores per processing chip to maximize system performance. Most estimates suggest massively parallel systems will be available within the decade, containing millions of cores and capable of <b>exaFlops</b> of performance. New models of execution are necessary to maximize processor utilization and minimize power costs for these exascale systems. ParalleX is one such execution model, which attempts to address inefficiencies of current execution models by exposing fine-grained parallelism, increasing system utilization using asynchronous workflow, and resolving resource contention through the use of adaptive and dynamic resource scheduling. A particularly important aspect of these exascale execution models is the design of the I/O subsystem, which has seen limited performance increases compared to processor and network technologies. Parallel file systems have been designed to help alleviate the poor performance of storage technologies by distributing file data acros...|$|R
40|$|Power {{consumption}} {{is a major}} obstacle for High Performance Computing (HPC) systems in their quest towards the holy grail of <b>ExaFLOP</b> performance. Significant advances in power efficiency {{have to be made}} before this goal can be attained and accurate modeling is an essential step towards power efficiency by optimizing system operating parameters to match dynamic energy needs. In this paper we present a study of power consumption by jobs in Eurora, a hybrid CPU-GPU-MIC system installed at the largest Italian data center. Using data from a dedicated monitoring framework, we build a data-driven model of power consumption for each user in the system and use it to predict the power requirements of future jobs. We are able to achieve good prediction results for over 80 % of the users in the system. For the remaining users, we identify possible reasons why prediction performance is not as good. Possible applications for our predictive modeling results include scheduling optimization, power-aware billing and system-scale power modeling. All the scripts used for the study have been made available on GitHub. Comment: 13 pages, 4 figures, 2 tables, Euro-Par 201...|$|E
40|$|Checkpointing is the {{predominant}} storage driver in today 2 ̆ 7 s petascale supercomputers {{and is expected}} to remain as such in tomorrow 2 ̆ 7 s exascale supercomputers. Users typically prefer to checkpoint into a shared file yet parallel file systems often perform poorly for shared file writing. A powerful technique to address this problem is to transparently transform shared file writing into many exclusively written as is done in ADIOS and PLFS. Unfortunately, the metadata to reconstruct the fragments into the original file grows with the number of writers. As such, the current approach cannot scale to <b>exaflop</b> supercomputers due to the large overhead of creating and reassembling the metadata. In this paper, we develop and evaluate algorithms by which patterns in the PLFS metadata can be discovered and then used to replace the current metadata. Our evaluation shows that these patterns reduce the size of the metadata by several orders of magnitude, increase the performance of writes by up to 40 percent, and the performance of reads by up to 480 percent. This contribution therefore can allow current checkpointing models to survive the transition from petato exascale...|$|E
40|$|Abstract—Checkpointing is the {{predominant}} storage driver in today’s petascale supercomputers {{and is expected}} to remain as such in tomorrow’s exascale supercomputers. Users typically prefer to checkpoint into a shared file yet parallel file systems often perform poorly for shared file writing. A powerful technique to address this problem is to transparently transform shared file writing into many exclusively written as is done in ADIOS and PLFS. Unfortunately, the metadata to reconstruct the fragments into the original file grows with the number of writers. As such, the current approach cannot scale to <b>exaflop</b> supercomputers due to the large overhead of creating and reassembling the metadata. In this paper, we develop and evaluate algorithms by which patterns in the PLFS metadata can be discovered and then used to replace the current metadata. Our evaluation shows that these patterns reduce the size of the metadata by several orders of magnitude, increase the performance of writes by up to 40 percent, and the performance of reads by up to 480 percent. This contribution therefore can allow current checkpointing models to survive the transition from peta- to exascale. I...|$|E
40|$|The ASCI {{supercomputing}} {{program is}} broadly defined as running physics simulations on progressively more powerful digital computers. What happens if we extrapolate the computer technology to its end? We {{have developed a}} model for key ASCI computations running on a hypothetical computer whose technology is parameterized in ways that account for advancing technology. This model includes technology information such as Moore’s Law for transistor scaling and developments in cooling technology. The model also includes limits imposed by laws of physics, such as thermodynamic limits on power dissipation, limits on cooling, and the limitation of signal propagation velocity to the speed of light. We apply this model and show that ASCI computations will advance smoothly for another 10 - 20 years to an “end game ” defined by thermodynamic limits and the speed of light. Performance levels at the end game will vary greatly by specific problem, but {{will be in the}} <b>Exaflops</b> to Zettaflops range for currently anticipate...|$|R
40|$|International audienceEnergy {{consumption}} {{has become}} one of the most critical issues in the evolution of High Performance Computing systems (HPC). Controlling the energy consumption of HPC platforms is not only a way to control the cost but also a step forward on the road towards <b>exaflops.</b> Powercapping is a widely studied technique that guarantees that the platform will not exceed a certain power threshold instantaneously but it gives no flexibility to adapt job scheduling to a longer term energy budget control. We propose a job scheduling mechanism that extends the backfilling algorithm to become energy-aware. Simultaneously, we adapt resource management with a node shutdown technique to minimize energy consumption whenever needed. This combination enables an efficient energy consumption budget control on a cluster during a period of time. The technique is experimented, validated and compared with various alternatives through extensive simulations. Experimentation results show high system utilization and limited bounded slowdown along with interesting outcomes in energy efficiency while respecting an energy budget during a particular time period...|$|R
40|$|Future {{large scale}} high {{performance}} supercomputer systems require {{high energy efficiency}} to achieve <b>exaflops</b> computational power and beyond. Despite the need to understand energy efficiency in high-performance systems, there are few techniques to evaluate energy efficiency at scale. In this paper, we propose a system-level iso-energy-efficiency model to analyze, evaluate and predict energy-performance of data intensive parallel applications with various execution patterns running on large scale power-aware clusters. Our analytical model can help users explore the effects of machine and application dependent characteristics on system energy efficiency and isolate efficient ways to scale system parameters (e. g. processor count, CPU power/frequency, workload size and network bandwidth) to balance energy use and performance. We derive our iso-energy-efficiency model {{and apply it to}} the NAS Parallel Benchmarks on two power-aware clusters. Our results indicate that the model accurately predicts total system energy consumption within 5 % error on average for parallel applications with various execution and communication patterns. We demonstrate effective use of the model for various application contexts and in scalability decision-making...|$|R
40|$|Exascale {{computing}} {{refers to}} a computing system which is capable {{to at least one}} <b>exaflop</b> in next couple of years. Many new programming models, architectures and algorithms have been introduced to attain the objective for exascale computing system. The primary objective is to enhance the system performance. In modern/super computers, GPU is being used to attain the high computing performance. However, it’s the objective of proposed technologies and programming models is almost same to make the GPU more powerful. But these technologies are still facing the number of challenges including parallelism, scale and complexity and also many more that must be fixed to achieve make computing system more powerful and efficient. In this paper, we have present a testing tool architecture for a parallel programming approach using two programming models as CUDA and OpenMP. Both CUDA and OpenMP could be used to program shared memory and GPU cores. The object of this architecture is to identify the static errors in the program that occurred during writing the code and cause absence of parallelism. Our architecture enforces the developers to write the feasible code through we can avoid from the essential errors in the program and run successfully...|$|E
40|$|Super {{computing}} {{is reaching}} out to <b>ExaFLOP</b> processing speeds, creating fundamental challenges for the way that computing systems are designed and built. One governing topic is the reduction of power used for operating the system, and eliminating the excess heat generated from the system. Current thinking sees optical interconnects on most interconnect levels to be a feasible solution {{to many of the}} challenges, although there are still limitations to the technical solutions, in particular with regard to manufacturability. This paper explores drivers for enabling optical interconnect technologies to advance into the module and chip level. The introduction of optical links into High Performance Computing (HPC) could be an option to allow scaling the manufacturing technology to large volume manufacturing. This will drive the need for manufacturability of optical interconnects, giving rise to other challenges that add to the realization of this type of interconnection. This paper describes a solution that allows the creation of optical components on module level, integrating optical chips, laser diodes or PIN diodes as components much like the well known SMD components used for electrical components. The paper shows the main challenges and potential solutions to this challenge and proposes a fundamental paradigm shift in the manufacturing of 3 -dimensional optical links for the level 1 interconnect (chip package) ...|$|E
40|$|Moderating {{the energy}} {{consumption}} and building eco-friendly computing infrastructure is of major {{concerns in the}} implementation of High Performance Computing (HPC) system, especially when a world- wide effort target the production of an <b>Exaflop</b> machine by 2020 within a power envelop of 20 MW. Tracking energy savings can be done at var- ious levels and in this paper, we investigate the automatic generation of energy aware software with the ambition to keep the same level of efficiency, testability, scalability and security. To this end, the Evo-LLVM framework is proposed. Based on the mod- ular LLVM Compiler Infrastructure and exploiting various evolutionary heuristics, our scheme is designed to optimize for a given input source code (written in C) the sequence of LLVM transformations that should be applied to the source code to improve its energy efficiency without degrading its other performance attributes (execution time, parallel or distributed scalability). Measuring this capacity is based on the combi- nation of several metrics optimized simultaneously with Multi-Objective Evolutionary Algorithms (MOEAs). In this position paper, the NSGA- II algorithm is implemented within the Evo-LLVM yet the analysis of more advanced heuristics is in progress. In all cases, the experimental validation of the framework over a pedagogical code sample reveal a drastic improvement of the energy consumed during the execution while maintaining (or even improving) the average execution time...|$|E
40|$|High {{performance}} computing (HPC) {{is experiencing a}} phase change with the challenges of programming and management of heterogeneous multicore sys-tems architectures and large scale system configu-rations. It is estimated {{that by the end}} of the next decade <b>Exaflops</b> computing systems requiring hun-dreds of millions of cores demanding multi-billion-way parallelism with a power budget of 50 Gflops/watt may emerge. At the same time, there are many scaling-challenged applications that al-though taking many weeks to complete, cannot scale even to a thousand cores using conventional distributed programming models. This paper de-scribes an experimental methodology, ParalleX, that addresses these challenges through a change in the fundamental model of parallel computation from that of the communicating sequential processes (e. g., MPI) to an innovative synthesis of concepts involving message-driven work-queue execution in the context of a global address space. The focus of this work is a new runtime system required to test, validate, and evaluate the use of ParalleX concepts for extreme scalability. This paper describes the ParalleX model and the HPX runtime system and discusses how both strategies contribute to the goal of extreme computing through dynamic asynchron-ous execution. The paper presents the first early experimental results of tests using a proof-of-concept runtime-system implementation. These results are very promising and are guiding future work towards a full scale parallel programming and runtime environment. 2...|$|R
30|$|In his book, Kurzweil [10] {{presents}} {{a series of}} figures that show exponential growth trends for memory capacity (DRAM in bits per dollar), microprocessor clock speed (Hz), transistors per chip, processor performance in million instructions per second (MIPS), and magnetic storage (bits per dollar). For example, the decrease in the ratio of cost per MIPS is about 8 million to 1 from 1967 up to 2004 [10]. In the same period, memory capacity improved approximately 2, 000 times. Saracco [11] argued that technological developments in digital storage and processing have been consistent in recent years. The number of hosts on the Internet has also been progressing exponentially, at least for now. High-performance computing based on supercomputers (or computer clusters) has already achieved petaflops (10 ^ 15) floating point operations per second [12] and evolution proceeds to <b>exaflops</b> (10 ^ 18). Moreover, display technology has advanced enormously in recent years, allowing ameliorated quality and larger screens, substantially {{improving the quality of}} experience and allowing new forms of digital interactivity [11]. The advance of consumer electronics in the form of handsets, laptops, HDTVs, e-books, video games, GPS, etc., pleads for exponential growth in these technologies. Besides being interesting, this estimatives are important to characterize how substrate technologies capacity will evolve in the next decades. In other words, they help to answer the question: which is the capacity available to the design of future ICT? To know the available capacity is important to any project.|$|R
40|$|A recent {{development}} in radio astronomy is to replace traditional disheswithmanysmallantennas. Thesignalsarecombinedtoform one large, virtual telescope. The enormous data streams are crosscorrelated {{to filter out}} noise. This is especially challenging, since the computational demands grow quadratically withthe number of data streams. Moreover, the correlator is not only computationally intensive, but also very I/O intensive. The LOFAR telescope, for instance, will produce over 100 terabytes per day. The future SKA telescope will even require {{in the order of}} <b>exaflops,</b> and petabits/s of I/O. A recent trend is to correlate in software instead of dedicated hardware. This is done to increase flexibility and to reduce development efforts. Examples include e-VLBIand LOFAR. In this paper, we evaluate the correlator algorithm on multi-core CPUsandmany-corearchitectures,suchasNVIDIAandATIGPUs, and the Cell/B. E. The correlator is a streaming, real-time application, and is much more I/O intensive than applications that are typically implementedon many-core hardware today. We compare withtheLOFARproductioncorrelatoronanIBMBlueGene/Psupercomputer. We investigate performance, power efficiency, and programmability. Weidentifyseveralimportant architecturalproblems which cause architectures to perform suboptimally. Our findings areapplicable todata-intensive applications ingeneral. The results show that the processing power and memory bandwidth of current GPUs are highly imbalanced for correlation purposes. WhiletheproductioncorrelatorontheBlueGene/Pachieves asuperb 96 %ofthetheoreticalpeakperformance, thisisonly 14 % on ATI GPUs, and 26 % on NVIDIA GPUs. The Cell/B. E. processor, in contrast, achieves an excellent 92 %. We found that the Cell/B. E. is also the most energy-efficient solution, it runs the correlator 5 - 7 timesmoreenergyefficientlythantheBlueGene/P. The research presented is an important pathfinder for next-generation telescopes. Categories andSubject Descriptors D. 1. 3 [Programming Techniques]: Concurrent programming...|$|R
