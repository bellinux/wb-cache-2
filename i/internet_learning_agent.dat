0|3462|Public
40|$|The {{following}} chapter explores <b>learning</b> <b>internet</b> <b>agents.</b> In recent years, {{with the}} massive {{increase in the}} amount of available information on the Internet, a need has arisen for being able to organize and access that data in a meaningful and directed way. Many well-explored techniques from the eld of AI and machine learning have been applied in this context. In this paper, special emphasis is placed on neural network approaches in implementing a <b>learning</b> <b>agent.</b> First, various important approaches are summarized. Then, an approach for neural <b>learning</b> <b>internet</b> <b>agents</b> is presented, one that uses recurrent neural networks for the learning of classifying a textual stream of information. Experimental results are presented showing that a neural network model based on a recurrent plausibility network can act as a scalable, robust and useful news routing agent...|$|R
40|$|In recent years, {{with the}} massive {{increase}} {{in the amount of}} available online information on the Internet, a need has arisen for being able to organize and access that data in a meaningful and directed way. In this paper, special emphasis is placed on neural network approaches in implementing a <b>learning</b> <b>agent</b> for text routing. A brief summary of the various important approaches used is presented. An approach for neural <b>learning</b> <b>internet</b> <b>agents</b> is outlined, one that uses recurrent neural networks for the learning of classifying a textual stream of news information. In particular, we analyze the context of the internal memory and how it is relevant in making classi cation decisions...|$|R
40|$|Abstract. The {{following}} chapter explores <b>learning</b> <b>internet</b> <b>agents.</b> In recent years, {{with the}} massive {{increase in the}} amount of available infor-mation on the Internet, a need has arisen for being able to organize and access that data in a meaningful and directed way. Many well-explored techniques from the eld of AI and machine learning have been applied in this context. In this paper, special emphasis is placed on neural network approaches in implementing a <b>learning</b> <b>agent.</b> First, various important approaches are summarized. Then, an approach for neural <b>learning</b> in-ternet <b>agents</b> is presented, one that uses recurrent neural networks for the learning of classifying a textual stream of information. Experimental results are presented showing that a neural network model based on a recurrent plausibility network can act as a scalable, robust and useful news routing agent. ...|$|R
40|$|<b>Learning</b> <b>agents</b> {{increase}} their team’s performance by learn-ing to coordinate better with their teammates, {{and we are}} in-terested in forming teams that contain such <b>learning</b> <b>agents.</b> In particular, we consider finite training instances for learn-ing agents to improve their coordination before the final team is formed. We formally define the <b>learning</b> <b>agents</b> team formation problem, and focus on <b>learning</b> <b>agent</b> pairs that improve their coordination. <b>Learning</b> <b>agent</b> pairs have het-erogeneous rates of improving coordination, and hence the allocation of training instances has a large impact {{on the performance of}} the final team...|$|R
40|$|We {{show that}} {{adaptive}} agents on the <b>Internet</b> can <b>learn</b> to exploit bidding agents who use a (limited) number of fixed strategies. These <b>learning</b> <b>agents</b> {{can be generated}} by adapting {{a special kind of}} finite automata with evolutionary algorithms (EAs). Our approach is especially powerful if the adaptive agent participates in frequently occurring micro-transactions, where there is sufficient opportunity for the <b>agent</b> to <b>learn</b> online from past negotiations. More in general, results presented in this paper provide a solid basis for the further development of adaptive agents for Internet applications...|$|R
40|$|We {{report a}} proof-of-principle {{experimental}} {{demonstration of the}} quantum speed-up for <b>learning</b> <b>agents</b> utilizing a small-scale quantum information processor based on radiofrequency-driven trapped ions. The decision-making process of a quantum <b>learning</b> <b>agent</b> within the projective simulation paradigm for machine learning is implemented {{in a system of}} two qubits. The latter are realized using hyperfine states of two frequency-addressed atomic ions exposed to a static magnetic field gradient. We show that the deliberation time of this quantum <b>learning</b> <b>agent</b> is quadratically improved with respect to comparable classical <b>learning</b> <b>agents.</b> The performance of this quantum-enhanced <b>learning</b> <b>agent</b> highlights the potential of scalable quantum processors taking advantage of machine learning. Comment: 11 pages incl. supplement, 7 figures. Author names now spelled correctly; sections rearranged; changes in the wording of the manuscrip...|$|R
40|$|Learning {{to solve}} {{problem-solving}} tasks is {{a hallmark of}} intelligence. Intelligent <b>agents</b> <b>learn</b> not only from their own experiences {{but also from the}} experiences of others. One would also like a computerized agent to do this: to exploit both its own experiences and those of other <b>agents</b> when <b>learning</b> to solve problem-solving tasks. To this end, we introduce a model of learner/trainer interaction that describes how a <b>learning</b> <b>agent</b> and training agent work together to help the <b>learning</b> <b>agent</b> <b>learn.</b> This proposed model presents the learner as an agent that must make decisions about how it is going to learn. For example, when should the <b>learning</b> <b>agent</b> ask the trainer for help? The training agent must also make decisions about how it interacts with the <b>learning</b> <b>agent.</b> For example, when the <b>learning</b> <b>agent</b> requests help, should the trainer provide it or ignore the request? These decisions drive the interactions, which are the mechanisms by which the training agent provides knowledg [...] ...|$|R
40|$|We {{attempt to}} achieve corporative {{behavior}} of autonomous decentralized agents constructed via Q-Learning, {{which is a}} type of reinforcement learning. As such, in the present paper, we examine the piano mover's problem. We propose a multi-agent architecture that has a training <b>agent,</b> <b>learning</b> <b>agents</b> and intermediate <b>agent.</b> <b>Learning</b> <b>agents</b> are heterogeneous and can communicate with each other. The movement of an object with three kinds of agent depends on the composition of the actions of the <b>learning</b> <b>agents.</b> By <b>learning</b> its own shape through the <b>learning</b> <b>agents,</b> avoidance of obstacles by the object is expected. We simulate the proposed method in a two-dimensional continuous world. Results obtained in the present investigation reveal the effectiveness of the proposed method...|$|R
40|$|Abstract. <b>Agents</b> can <b>learn</b> {{to improve}} their {{coordination}} with their teammates and increase team performance. We are interested in form-ing a team, i. e., selecting a subset of agents, that includes such <b>learning</b> <b>agents.</b> Before the team is formed, there are finite training instances that provide opportunities for the <b>learning</b> <b>agents</b> to improve. <b>Agents</b> <b>learn</b> at different rates, and hence, the allocation of training instances affects {{the performance of the}} team formed. We focus on allocating training instances to <b>learning</b> <b>agent</b> pairs, i. e., pairs that improve coordination with each other, with the goal of team formation. We formally define the <b>learning</b> <b>agents</b> team formation problem, and compare it with the multi-armed bandit problem. We consider <b>learning</b> <b>agent</b> pairs that improve linearly and geometrically, i. e., the marginal improvement decreases by a constant factor. We contribute algorithms that allocate the training instances, and compare against algorithms from the multi-armed bandit problem. In extensive simulations, we demonstrate that our algorithms perform similarly to the bandit algorithms in the linear case, and out-perform them in the geometric case, thus illustrating the efficacy of our algorithms. ...|$|R
40|$|Open Sesame! ® 1. 0 —released in 1993 —was the world’s first {{commercial}} user interface (UI) <b>learning</b> <b>agent.</b> The {{development of this}} agent involved a number of decisions about basic design issues {{that had not been}} previously addressed, including the expected types of agent and the preferred form and frequency of interaction. In the two years after shipping Open Sesame! 1. 0, we have compiled a rich database of customer feedback. Many of our design choices have been validated by the general approval of our customers while some were not received as favorably. Thanks to the overwhelming amount of feedback, we were able to substantially improve the design for Open Sesame! 2. 0, and develop a cross-platform learning engine- Learn Sesame- {{that can be used to}} add <b>learning</b> <b>agent</b> functionality to any third party application. In this paper, we present a summary of the lessons learned from customer feedback, an outline of resulting design changes, the details of the developed <b>learning</b> <b>agent</b> engine and planned research. 1 1. Background on User Interface <b>Learning</b> <b>Agents</b> In our user interface <b>learning</b> <b>agent</b> paradigm, a <b>learning</b> <b>agent</b> sits in the background and observes user actions, finds repetitive patterns, and automates them upon approval. The learnin...|$|R
40|$|Abstract. In this paper, {{we propose}} a {{reinforcement}} learning method called a fuzzy Q-learning where an agent determines its action {{based on the}} inference result by a fuzzy rule-based system. We apply the proposed method to a soccer agent that intercepts a passed ball by another agent. In the proposed method, the state space is represented by internal in-formation the <b>learning</b> <b>agent</b> maintains such as the relative velocity and the relative position of {{the ball to the}} <b>learning</b> <b>agent.</b> We divide the state space into several fuzzy subspaces. A fuzzy if-then rule in the proposed method represents a fuzzy subspace in the state space. The consequent part of the fuzzy if-then rules is a motion vector that suggests the mov-ing direction and velocity of the <b>learning</b> <b>agent.</b> A reward is given to the <b>learning</b> <b>agent</b> if the distance between the ball and the agent becomes smaller or if the agent catches up with the ball. It is expected that the <b>learning</b> <b>agent</b> finally obtains the efficient positioning skill. ...|$|R
50|$|A <b>learning</b> <b>agent</b> that {{actively}} {{investigates the}} consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps. By exploiting these learning opportunities the <b>learning</b> <b>agent</b> is able to learn beyond the explicit content of the new information.|$|R
40|$|As {{the global}} economy is {{increasingly}} becoming knowledge-based and knowledge-intensive, many experts and professionals predict {{that there will be}} a huge demand for educational products. Apparently, a major part of the demand is being met by the emergence of tens of thousands of electronic courses via the Internet provided by many education entities. Despite the huge number of these Internet courses, few researchers have addressed the students' perceptions or experiences in <b>Internet</b> <b>learning.</b> Therefore, a study as reported in this paper on the students' <b>Internet</b> <b>learning</b> experience is much needed. The results of this study have shown that both the students' competence in PC skills and their Internet surfing usages are significantly correlated with the students' usages of e-learning via the Internet. Additionally, the results have also shown that the e-learning usage is significantly correlated with the respondent' feelings of enjoyment for using the <b>Internet</b> <b>learning</b> materials. Alarmingly, the respondents agreed that <b>Internet</b> <b>learning</b> increased their workloads in studying. Practitioners in the relevant fields then can make use of these findings when developing their e-learning courses...|$|R
40|$|We {{propose a}} model of vicarious {{reinforcement}} in rule-based <b>learning</b> <b>agents.</b> The influence of this reinforcement is investigated in a population where a law is enforced ex ante. The norm-governed population of <b>learning</b> <b>agents</b> is formalised and simulated in an executable probabilistic rule-based argumentation framework. Vicarious experiences are expressed with rules and their learning effects are integrated into reinforcement <b>learning.</b> So, <b>agents</b> <b>learn</b> not only from their own experiences but also by {{taking into account the}} experiences of others. We show that simulation results differ from traditional calculus based on expected utilities...|$|R
5000|$|ALADDIN: {{autonomous}} <b>learning</b> <b>agents</b> for decentralised {{data and}} information networks (2005-2010) ...|$|R
40|$|We {{developed}} a reinforcement <b>learning</b> MDP <b>agent</b> and a genetic programming <b>learning</b> <b>agent</b> {{to play the}} game Super Mario Bros. Our results compare favorably with {{the current state of the}} art agents for this game. In particular, our genetic programming agent would be the top performing <b>learning</b> AI <b>agent</b> in the 2009 AI Mario competition...|$|R
40|$|This paper {{presents}} a mixed-initiative approach to rule refinement {{in which a}} subject matter expert collaborates with a <b>learning</b> <b>agent</b> to refine the agent’s knowledge base. This approach is implemented in the Disciple <b>learning</b> <b>agent</b> shell and has been evaluated in several agent training experiments performed by subject matter experts at the US Army War College. ...|$|R
50|$|In an {{interaction}} between <b>learning</b> <b>agents,</b> Pérez López finds three types of results.|$|R
5000|$|... 2007. PLOW: A Collaborative Task <b>Learning</b> <b>Agent.</b> (with Nathanael Chambers et al) AAAI'07 ...|$|R
40|$|This paper {{introduces}} {{a new type}} of intelligent agent called a constructive induction-based <b>learning</b> <b>agent</b> (CILA). This agent differs from other adaptive agents because it has the ability to not only learn how to assist a user in some task, but also to incrementally adapt its knowledge representation space to better fit the given <b>learning</b> task. The <b>agent's</b> ability to autonomously make problem-oriented modifications to the originally given representation space is due to its constructive induction (CI) learning method. Selective induction (SI) <b>learning</b> methods, and <b>agents</b> based on these methods, rely on a good representation space. A good representation space has no misclassification noise, inter-correlated attributes or irrelevant attributes. Our proposed CILA has methods for overcoming all of these problems. In agent domains with poor representations, the CIbased <b>learning</b> <b>agent</b> will <b>learn</b> more accurate rules and be more useful than an SI-based <b>learning</b> <b>agent.</b> This paper gives an archit [...] ...|$|R
40|$|AbstractAmong many {{reinforcement}} learning methods, FALCON is a machine learning method {{which is an}} extend fuzzy ART(Adaptive Resonance Theory), and can appropriately discretize a state space. FALCON is an on-line method proposed by Ah-Hwee Tan. It can discretize a state space and learn action rules simultaneously by learning relations among percepts, actions, and rewards. In this study, a <b>learning</b> <b>agent</b> using FALCON is interactively trained, and the learning effect is measured through experiments. In experiments, the <b>learning</b> <b>agent</b> <b>learns</b> by playing 50, 000 card games of “Hearts” against three rule-based agents. Then, the interface that agents can interactively play the game with human cooperators is made so that human cooperators can play the game against the <b>learning</b> <b>agent</b> to strengthen it. It continues learning during games. The effectiveness of interactive learning is ascertained through the experiments...|$|R
40|$|Abstract. Multiple devices, both {{hardware}} and software, may {{come and go}} {{at any time in}} a given room. Software controlling the behaviour of these devices must be able to adapt to encompass new devices or the removal of existing devices. This paper presents a model for curious, supervised <b>learning</b> <b>agents</b> that address the issue of adaptability at a behavioural level in an intelligent room. Curious, supervised <b>learning</b> <b>agents</b> comprise a curiosity module and a supervised learning algorithm. The curiosity module identifies interesting devices on which to focus the <b>agent’s</b> <b>learning.</b> The supervised learning component realises behaviours by observing, modelling and mimicking human actions. Our framework is demonstrated in a virtual meeting room in Second Life. We show that the curious <b>learning</b> <b>agent</b> can adapt its behaviour to identify new learning goals in response to new devices and activities...|$|R
40|$|Abstract — In {{this paper}} we {{advocate}} a paradigm of socially guided machine <b>learning,</b> designing <b>agents</b> that take better {{advantage of the}} situated aspects of learning. We augmented a standard Reinforcement <b>Learning</b> <b>agent</b> with the social mechanisms of attention direction and gaze. Experiments with an interactive computer game, deployed over the World Wide Web to over 75 players, show the positive impact of these social aspects. Allowing the human to direct the agent’s attention creates a more efficient exploration strategy. Additionally, gaze behavior lets the <b>learning</b> <b>agent</b> improve its own learning envi-ronment, using transparency to steer the human’s instruction. I...|$|R
40|$|We {{propose a}} general {{approach}} to safe reinforcement learning control based on Lyapunov design methods. In our approach, a Lyapunov function [...] -a special form of domain knowledge [...] -is used to formulate the action choices {{available to a}} reinforcement <b>learning</b> <b>agent.</b> A <b>learning</b> <b>agent</b> choosing among these actions provably enjoys performance guarantees, and satisfies safety constraints of various kinds. We demonstrate the general approach by applying it to several illustrative pendulum control problems...|$|R
5000|$|... 2003, CAREER {{award from}} the National Science Foundation for his {{research}} on <b>learning</b> <b>agents</b> in dynamic, collaborative, and adversarial multiagent environments.|$|R
40|$|We {{discuss a}} method of {{learning}} by practice {{based on the idea}} of determining classes of problems that can be solved in simplified ways, A description of a class is obtained by processes that hypothesize descriptions, generate and classify problem variations, and test the hypotheses against them. The approach has been implemented in a system that learns by practice in a domain of elementary physics. The system has two main components, a Problem Solver and a <b>Learning</b> <b>Agent.</b> The Problem Solver handles the problems in the domain and the <b>Learning</b> <b>Agent</b> does the actual learning. To perform its tasks the <b>Learning</b> <b>Agent</b> utilizes algorithms, heuristics, and domain knowledge, and for this reason it can be regarded as an expert system whose expertise resides in being able to learn by experimentation and generalization. 1...|$|R
40|$|We apply {{reinforcement}} learning {{to the problem}} of finding good policies for a fighting agent in a commercial computer game. The <b>learning</b> <b>agent</b> is trained using the SARSA algorithm for on-policy learning of an action-value function represented by linear and neural network function approximators. We discuss the selection and construction of features, actions, and rewards as well as other design choices necessary to integrate the learning process into the game. The <b>learning</b> <b>agent</b> is trained against the built-in AI of the game with different rewards encouraging aggressive or defensive behaviour. We show that the <b>learning</b> <b>agent</b> finds interesting (and partly near optimal) policies in accordance with the reward functions provided. We also discuss the particular challenges arising in the application of {{reinforcement learning}} to the domain of computer games...|$|R
40|$|This book {{contains}} the papers accepted for {{presentation at the}} 2011 edition of the Adaptive and <b>Learning</b> <b>Agents</b> (ALA) workshop. ALA {{is the result of}} the merger of the ALAMAS and ALAg workshops. ALAMAS was an annual European work-shop on Adaptive and <b>Learning</b> <b>Agents</b> and Multi-Agent Systems, held eight times. ALAg was the international workshop on Adaptive and <b>Learning</b> <b>agents,</b> typically held in conjunction with AAMAS. To increase the strength, visibility, and quality of the workshops, ALAMAS and ALAg were combined into the ALA workshop, and a steering committee was appointed to guide its development. The goal of the workshop is to increase awareness and interest in adaptive agent re-search, encourage collaboration, and give a representative overview of current research in the area of adaptive and <b>learning</b> <b>agents.</b> It aims at bringing together not only different areas of computer science (e. g., <b>agent</b> architectures, reinforcement <b>learning,</b> and evolutionary algorithms) but also from different fields studying similar concepts (e. g., game theory, bio-inspired control, and mechanism design). The workshop serves as an inclusive forum for the discussion of ongoing or completed work in adaptive an...|$|R
40|$|Reinforcement <b>learning</b> <b>agents</b> {{interacting}} with a complex environment {{like the real}} world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then {{it may be necessary}} for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the <b>learning</b> <b>agent</b> expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button— which is an undesirable outcome. This paper explores a way to make sure a <b>learning</b> <b>agent</b> will not <b>learn</b> to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal definition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement <b>learning</b> <b>agents</b> for (deterministic) general computable environments can be made safely interruptible...|$|R
40|$|Multiple devices, both {{hardware}} and software, may {{come and go}} {{at any time in}} a given room. Software controlling the behaviour of these devices must be able to adapt to encompass new devices or the removal of existing devices. This paper presents a model for curious, supervised <b>learning</b> <b>agents</b> that address the issue of adaptability at a behavioural level in an intelligent room. Curious, supervised <b>learning</b> <b>agents</b> comprise a curiosity module and a supervised learning algorithm. The curiosity module identifies interesting devices on which to focus the agentis learning. The supervised learning component realises behaviours by observing, modelling and mimicking human actions. Our framework is demonstrated in a virtual meeting room in Second Life. We show that the curious <b>learning</b> <b>agent</b> can adapt its behaviour to identify new learning goals in response to new devices and activities...|$|R
40|$|Given {{the cost}} and effort {{involved}} in investigating different methods or protocols for human collaborative learning, this paper proposes the use of software <b>learning</b> <b>agents</b> to simulate and test various protocols. The best performing protocols would then be tested on human subjects. The paper presents {{a new form of}} cooperative learning, called coactive learning. After arguing that humans could be taught to learn coactively, experiments are conducted with various coactive learning schemes. The experiments demonstrate situations in which machine <b>learning</b> <b>agents</b> using coactive <b>learning</b> can perform better than individual machine <b>learning</b> <b>agents.</b> Collaborative <b>learning</b> through coaction is shown to generate properties that individual learners can achieve only through computationally intensive strategies. It is suggested that human learners be trained to learn coactively using the most successful agent coacting protocols, and compared, on the same type of learning task, to individu [...] ...|$|R
5000|$|The last {{component}} of the <b>learning</b> <b>agent</b> is the [...] "problem generator". It is responsible for suggesting actions {{that will lead to}} new and informative experiences.|$|R
40|$|A major {{bottleneck}} {{for developing}} general reinforcement <b>learning</b> <b>agents</b> is determining rewards that will yield desirable behaviors under various circumstances. We introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels. In particular, we train a {{generative adversarial network}} to produce short sub-goals represented through motion templates. We demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions {{can be used to}} train reinforcement <b>learning</b> <b>agents.</b> Comment: Deep Reinforcement Learning Symposium, NIPS 201...|$|R
40|$|Deep {{reinforcement}} learning methods attain super-human {{performance in a}} wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement <b>learning</b> <b>agent</b> that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our <b>agent</b> <b>learns</b> significantly faster than other state-of-the-art, general purpose deep reinforcement <b>learning</b> <b>agents...</b>|$|R
40|$|We {{describe}} a statistical Natural Language Generation (NLG) method for summarisation of time-series {{data in the}} context of feedback generation for students. In this paper, we initially present a method for collecting time-series data from students (e. g. marks, lectures attended) and use example feedback from lecturers in a datadriven approach to content selection. We show a novel way of constructing a reward function for our Reinforcement <b>Learning</b> <b>agent</b> that is informed by the lecturers’ method of providing feedback. We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturerconstructed summaries and a Brute Force system. Our evaluation shows that the feedback generated by our <b>learning</b> <b>agent</b> is viewed by students {{to be as good as}} the feedback from the lecturers. Our findings suggest that the <b>learning</b> <b>agent</b> needs to take into account both the student and lecturers’ preferences. ...|$|R
40|$|Although {{building}} sophisticated <b>learning</b> <b>agents</b> {{that operate}} in complex environments will require learning to perform multiple tasks, most applications of reinforcement learning have focussed on single tasks. In this paper I consider {{a class of}} sequential decision tasks (SDTs), called composite sequential decision tasks, formed by temporally concatenating a number of elemental sequential decision tasks. Elemental SDTs cannot be decomposed into simpler SDTs. I consider a <b>learning</b> <b>agent</b> that has to learn to solve a set of elemental and composite SDTs. I assume that {{the structure of the}} composite tasks is unknown to the <b>learning</b> <b>agent.</b> The straightforward application of reinforcement learning to multiple tasks requires learning the tasks separately, which can waste computational resources, both memory and time. I present a new learning algorithm and a modular architecture that learns the decomposition of composite SDTs, and achieves transfer of learning by sharing the solutions of element [...] ...|$|R
