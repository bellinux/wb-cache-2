10000|528|Public
5|$|General {{relativity}} {{includes a}} dynamical spacetime, {{so it is}} difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation <b>invariance,</b> but general covariance makes translation <b>invariance</b> into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's presecriptions do not make a real tensor for this reason.|$|E
5|$|He {{articulated}} {{the principle of}} relativity. This was understood by Hermann Minkowski to be a generalization of rotational <b>invariance</b> from space to space-time. Other principles postulated by Einstein and later vindicated are the principle of equivalence, general covariance and the principle of adiabatic <b>invariance</b> of the quantum number.|$|E
5|$|Nikācanā (<b>invariance)</b> – For some sub-types, no {{variations}} or modifications are possible—the {{consequences are}} the same as were established at the time of bonding.|$|E
40|$|Abstract. Often, in pattern recognition, {{complementary}} {{knowledge is}} available. This {{could be useful}} to improve {{the performance of the}} recognition system. Part of this knowledge regards <b>invariances,</b> in particular when treating images or voice data. Many approaches have been proposed to incorporate <b>invariances</b> in pattern recognition systems. Some of these approaches require a pre-processing phase, others integrate the <b>invariances</b> in the algorithms. We present a unifying formulation of the problem of incorporating <b>invariances</b> into a pattern recognition classifier and we extend the SimpleSVM algorithm [Vishwanathan et al., 2003] to handle <b>invariances</b> efficiently. Keywords: SVM, <b>Invariances,</b> Classification, Active Constraints. ...|$|R
30|$|These {{findings}} show that the configural and metric <b>invariances</b> have been established across Facebook and other social media users, but the scalar and strict <b>invariances</b> have not.|$|R
40|$|It is {{commonly}} {{agreed that the}} use of relevant <b>invariances</b> as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate <b>invariances</b> into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, {{there is a need for}} methods to model and extract richer transformations that capture much higher-level <b>invariances.</b> To that end, we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear <b>invariances</b> of the data by visualizing their effect in the data space...|$|R
5|$|Logarithms {{have many}} {{applications}} {{inside and outside}} mathematics. Some of these occurrences {{are related to the}} notion of scale <b>invariance.</b> For example, each chamber of the shell of a nautilus is an approximate copy of the next one, scaled by a constant factor. This gives rise to a logarithmic spiral. Benford's law on the distribution of leading digits can also be explained by scale <b>invariance.</b> Logarithms are also linked to self-similarity. For example, logarithms appear in the analysis of algorithms that solve a problem by dividing it into two similar smaller problems and patching their solutions. The dimensions of self-similar geometric shapes, that is, shapes whose parts resemble the overall picture are also based on logarithms.|$|E
5|$|As it is {{constructed}} using tensors, general relativity exhibits general covariance: its laws—and further laws formulated within the general relativistic framework—take {{on the same}} form in all coordinate systems. Furthermore, the theory does not contain any invariant geometric background structures, i.e. it is background independent. It thus satisfies a more stringent general principle of relativity, namely that {{the laws of physics}} are the same for all observers. Locally, as expressed in the equivalence principle, spacetime is Minkowskian, and the laws of physics exhibit local Lorentz <b>invariance.</b>|$|E
5|$|Einstein {{contributed to}} these {{developments}} by linking {{them with the}} 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic <b>invariance</b> of a thermal equilibrium state allows all the blackbody curves at different temperature {{to be derived from}} one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics.|$|E
40|$|We study noncommutative {{deformations}} of Yang-Mills {{theories and}} show that these theories admit a infinite, continuous family of twisted star-gauge <b>invariances.</b> This family interpolates continously between star-gauge and twisted gauge transformations. The possible physical rôle of these start-twisted <b>invariances</b> is discussed...|$|R
40|$|International audienceA {{technique}} {{to describe the}} spatial / spectral features of hyperspectral images is introduced. These descriptors aim at representing {{the content of the}} image while considering <b>invariances</b> related to the texture and to its geometric transformations, so called spatial <b>invariances.</b> Moreover, we also consider spectral <b>invariances</b> which are related to the composition of the pixels. Our approach is based on the scattering transform, which provides an useful framework for deep learning classification. The goal through these descriptors is to improve pixel-wise classification of hyperspectral images...|$|R
50|$|Each of {{the four}} trichords (3-note sets) thus {{displays}} a relationship which can be made obvious by any {{of the four}} serial row operations, and thus creates certain <b>invariances.</b> These <b>invariances</b> in serial music are analogous to the use of common-tones and common-chords in tonal music.|$|R
5|$|In 1916, Einstein {{predicted}} gravitational waves, ripples in {{the curvature}} of spacetime which propagate as waves, traveling {{outward from the}} source, transporting energy as gravitational radiation. The existence of gravitational waves is possible under general relativity due to its Lorentz <b>invariance</b> which brings {{the concept of a}} finite speed of propagation of the physical interactions of gravity with it. By contrast, gravitational waves cannot exist in the Newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed.|$|E
5|$|In QFT, {{the demand}} for {{relativistic}} <b>invariance</b> enters, among other ways in that the S-matrix necessarily must be Poincaré invariant. This has the implication {{that there is one}} or more infinite-dimensional representation of the Lorentz group acting on Fock space. One way to guarantee the existence of such representations is the existence of a Lagrangian description (with modest requirements imposed, see the reference) of the system using the canonical formalism, from which a realization of the generators of the Lorentz group may be deduced.|$|E
5|$|The {{results of}} special {{relativity}} {{can be summarized}} by treating space and time as a unified structure known as spacetime (withc relating the units of space and time), and requiring that physical theories satisfy a special symmetry called Lorentz <b>invariance,</b> whose mathematical formulation contains the parameterc. Lorentz <b>invariance</b> is an almost universal assumption for modern physical theories, such as quantum electrodynamics, quantum chromodynamics, the Standard Model of particle physics, and general relativity. As such, the parameterc is ubiquitous in modern physics, appearing in many contexts that are unrelated to light. For example, general relativity predicts thatc is also the speed of gravity and of gravitational waves. In non-inertial frames of reference (gravitationally curved spacetime or accelerated reference frames), the local speed of light is constant and equal toc, but {{the speed of light}} along a trajectory of finite length can differ fromc, depending on how distances and times are defined.|$|E
40|$|We study pathwise <b>invariances</b> of centred random {{fields that}} can be {{controlled}} through the covariance. A result involving composition operators is obtained in second-order settings, and we show that various path properties including additivity boil down to <b>invariances</b> of the covariance kernel. These results are extended to a broader class of operators in the Gaussian case, via the Loève isometry. Several covariance-driven pathwise <b>invariances</b> are illustrated, including fields with symmetric paths, centred paths, harmonic paths, or sparse paths. The proposed approach delivers a number of promising results and perspectives in Gaussian process regression...|$|R
40|$|We study noncommutative {{deformations}} of Yang-Mills {{theories and}} show that these theories admit a infinite, continuous family of twisted star-gauge <b>invariances.</b> This family interpolates continuously between star-gauge and twisted gauge transformations. The possible physical role of these start-twisted <b>invariances</b> is discussed. Comment: 16 pages, no figures. v 2 : typos corrected and references added. To appear in Physics Letters...|$|R
50|$|<b>Invariances</b> is a 2001 book by Robert Nozick, {{his last}} book {{before his death}} in 2002.|$|R
5|$|While {{developing}} general relativity, Einstein became {{confused about}} the gauge <b>invariance</b> in the theory. He formulated {{an argument that}} led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.|$|E
25|$|In mathematics, scale <b>invariance</b> usually {{refers to}} an <b>invariance</b> of {{individual}} functions or curves. A closely related concept is self-similarity, where a function or curve is invariant under a discrete subset of the dilatations. It is also possible for the probability distributions of random processes to display this kind of scale <b>invariance</b> or self-similarity.|$|E
25|$|The gauge <b>invariance</b> and {{the metric}} <b>invariance</b> {{can be viewed}} as the <b>invariance</b> under the adjoint Lie group action in the Chern–Weil theory. The action {{integral}} (path integral) of the field theory in physics is viewed as the Lagrangian integral of the Chern–Simons form and Wilson loop, holonomy of vector bundle on M. These explain why the Chern–Simons theory is closely related to topological field theory.|$|E
40|$|Developed only recently, support vector {{learning}} machines achieve high generalization ability by minimizing a bound on {{the expected}} test error; however, {{so far there}} existed no way of adding knowledge about <b>invariances</b> of a classification problem at hand. We present a method of incorporating prior knowledge about transformation <b>invariances</b> by applying transformations to support vectors, the training examples most critical for determining the classification boundary...|$|R
50|$|Kumal et al. {{extended}} the algorithm to incorporate local <b>invariances</b> to multivariate polynomial transformations and improved regularization.|$|R
50|$|Categorical {{perception}} is {{the experience of}} percept <b>invariances</b> in sensory phenomena that can be varied along a continuum.|$|R
25|$|In {{classical}} field theory, scale <b>invariance</b> {{most commonly}} {{applies to the}} <b>invariance</b> of a whole theory under dilatations. Such theories typically describe classical physical processes with no characteristic length scale.|$|E
25|$|Any serious {{parameter}} optimization method {{should be}} translation invariant, but most methods do not exhibit all the above described <b>invariance</b> properties. A prominent example {{with the same}} <b>invariance</b> properties is the Nelder–Mead method, where the initial simplex must be chosen respectively.|$|E
25|$|Neutrino {{masses and}} CPT <b>invariance.</b>|$|E
40|$|Abstract—Representing {{transformation}} <b>invariances</b> in data {{is known}} to be valuable in many domains. We consider a method by which prior knowledge about the structure of such <b>invariances</b> can be exploited using a novel algorithm for sparse coding across a learned dictionary of atoms combined with a param-eterized deformation function that captures invariant structure. We demonstrate the value of this on both reconstructing signals, as well as improved unsupervised grouping based on invariant sparse representations. I...|$|R
5000|$|However, Jaynes did {{not just}} use <b>invariances</b> to accept or reject given methods: this {{would leave the}} {{possibility}} that there is another not yet described method that would meet his common-sense criteria. Jaynes used the integral equations describing the <b>invariances</b> to directly determine the probability distribution. In this problem, the integral equations indeed have a unique solution, and it is precisely what was called [...] "method 2" [...] above, the random radius method.|$|R
40|$|The {{choice of}} an SVM kernel {{corresponds}} to {{the choice of a}} representation of the data in a feature space and, to improve performance, it should therefore incorporate prior knowledge such as known transformation <b>invariances.</b> We propose a technique which extends earlier work and aims at incorporating <b>invariances</b> in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. ...|$|R
25|$|<b>Invariance</b> of {{the form}} of an {{equation}} under an arbitrary coordinate transformation is customarily referred to as general covariance and equations with this property are referred to as written in the covariant form. General covariance is a special case of gauge <b>invariance.</b>|$|E
25|$|<b>Invariance</b> {{properties}} imply uniform {{performance on}} a class of objective functions. They have been argued to be an advantage, because they allow to generalize and predict {{the behavior of the}} algorithm and therefore strengthen the meaning of empirical results obtained on single functions. The following <b>invariance</b> properties have been established for CMA-ES.|$|E
25|$|Indeed, it {{is enough}} to check <b>invariance</b> when g is {{sufficiently}} close to the identity.|$|E
40|$|Studies {{by various}} authors suggest that higher-order {{networks}} {{can be more}} powerful and are biologically more plausible {{with respect to the}} more traditional multilayer networks. These architectures make explicit use of nonlinear interactions between input variables in the form of higher-order units or product units. If it is known a priori that the problem to be implemented possesses a given set of <b>invariances</b> like in the translation, rotation, and scale invariant pattern recognition problems, those <b>invariances</b> can be encoded, thus eliminating all higher-order terms which are incompatible with the <b>invariances.</b> In general, however, it is a serious setback that the complexity of learning increases exponentially with the size of inputs. This paper reviews higher-order networks and introduces an implicit representation in which learning complexity is mainly decided by the number of higher-order terms to be learned and increases only linearly with the input size. 1...|$|R
40|$|Inferotemporal (IT) neurons in monkeys {{can respond}} to visual stimuli in a translation- and scaleinvariant manner. In a neural circuit model based on a {{recently}} reported form of gain modulation by attention in area V 4, the modulated visual responses of model V 4 neurons produce object-centered receptive fields further down the visual processing stream, accounting for <b>invariances</b> exhibited by IT neurons. Introduction The responses of inferotemporal (IT) neurons in monkeys are highly selective for certain types of visual images but, to a large degree, are insensitive to the exact location and size of those images [1, 2]. These <b>invariances</b> to translation and scale at the neuronal level parallel our ability to recognize objects independently of the location and size of their images on the retina. Although some ideas have been put forward regarding possible mechanisms for generating <b>invariances</b> [3, 4], these have not been confirmed neurophysiologically. Monkeys with lesions in area V 4 cannot [...] ...|$|R
40|$|Developed only recently, support vector {{learning}} machines achieve high generalization ability by minimizing a bound on {{the expected}} test error; however, {{so far there}} existed no way of adding knowledge about <b>invariances</b> of a classification problem at hand. We present a method of incorporating prior knowledge about transformation <b>invariances</b> by applying transformations to support vectors, the training examples most critical for determining the classification boundary. 1 Incorporating <b>Invariances</b> in: C. von der Malsburg, W. von Seelen, J. C. Vorbruggen, & B. Sendhoff (eds.) : Artificial Neural Networks [...] - ICANN' 96. Springer Lecture Notes in Computer Science, Vol. 1112, Berlin, 1996, 47 [...] 52 In many applications of learning procedures, prior knowledge about properties of the function to be learned is available (for a review, see Abu [...] Mostafa, 1995). For instance, certain transformations of the input could be known to leave function values unchanged. Mostly, two different ways of exploit [...] ...|$|R
