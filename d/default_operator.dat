11|22|Public
2500|$|Whitespace is used {{to specify}} logical AND, {{as it is the}} <b>default</b> <b>operator</b> for joining search terms: ...|$|E
50|$|Parentheses {{can be used}} in the expressions, {{but only}} to {{override}} the <b>default</b> <b>operator</b> precedence, for example, in 1024 = (4 − 2)10. Allowing parentheses without operators would result in trivial Friedman numbers such as 24 = (24). Leading zeros cannot be used, since that would also result in trivial Friedman numbers, such as 001729 = 1700 + 29.|$|E
40|$|The work {{provides}} {{an approach to}} default reasoning with imperfect information using extended logic programs in multivalued logics given by bilattices. The paper introduces the default semantics of an extended program using a set of operators with interesting properties elegantly {{expressed in terms of}} lattice theory, including the ultimate evaluation based consequence operator and the <b>default</b> <b>operator</b> corresponding to two types of inference in presence of imperfect information. We provide a computational approach for the default semantics and relate our framework to relevant work...|$|E
5000|$|... a {{container}} template which holds two member objects ( [...] and [...] ) of arbitrary type(s). Additionally, the header defines <b>default</b> relational <b>operators</b> for s which have both types in common.|$|R
30|$|To {{implement}} {{and evaluate}} our approach, we used three multi-objective algorithms that performed {{well in the}} SPL context: NSGA-II (Deb et al. 2002), SPEA 2 (Zitzler et al. 2001) and IBEA (Zitzler and Künzli 2004). They were implemented by configuring jMetal (Durillo and Nebro 2011). jMetal is a Java based framework that implements, among others, the three mentioned algorithms. It allows an easy integration with other tools. It includes <b>default</b> <b>operators</b> and solutions for known optimization problems, and can be instanced for new ones. In addition to this, it implements some quality indicators, such as hypervolume.|$|R
50|$|In {{addition}} to the LIKE operator, CUBRID provides the REGEXP operator for regular expression pattern matching. By <b>default,</b> the <b>operator</b> does a case insensitive matching on the input string, but the modifier BINARY {{can be used for}} case sensitive scenarios. An optional alias of REGEXP is RLIKE.|$|R
40|$|The {{performance}} of video tone-mapping operators is investigated in a rating experiment using two criteria: overall quality and fidelity to real-world experience. The study includes a tone-curve used in commercial cameras, rarely considered in tone-mapping evaluation studies. The quality is measured {{for a range}} of parameter settings, revealing the importance of parameter fine-tuning and often unsatisfactory results of the <b>default</b> <b>operator</b> parameters. In order to explain what makes best performing operators better, the results are analysed in relation to image statistics and the characteristics of the tone-mapping function. Our observations are: state-of-the-art tone mapping produces measurably better results than camera’s S-shaped curve for high dynamic range scenes with important content spanned across a wide dynamic range; differences in colour reproduction strongly affect the results; fidelity and quality criteria produce similar results when no reference is present; and state-of-the-art operators produce the results of comparable quality when their parameters are well selected...|$|E
40|$|Until now, {{the study}} of {{reasoning}} include inside interlocutions (language aspect of dialogs between 2 individuals or more) is made with pragmatic models emanated from linguistic and psychological approaches. The observation that we follow now {{in the framework of}} interlocution logic aims at deepening the assumption done about the way to proceed. In the particular case of this paper, we improve interlocution logic with {{the study of}} the cognitive domain of interlocution 1) by investigating conceivable reasonings done by the speakers to represent and understand the position they are, 2) by making clear the followed reasonings with the natural deduction procedure, 3) by adding an <b>default</b> <b>operator</b> to outstrip classic logics power, 4) and at the end by transposing these mecanisms to explain the behavior of speakers. Trognon Alain, Coulon Daniel. La modélisation des raisonnements générés dans les interlocutions. In: Langages, 35 ᵉ année, n° 144, 2001. Psycholinguistique et intelligence artificielle, sous la direction de Jean Vivier. pp. 58 - 77...|$|E
40|$|Web {{search engines}} face an {{extremely}} heterogeneous user population from web novices to highly skilled experts. Currently, the search {{strategies of the}} experienced web searchers are largely unknown and this paper addresses this issue by an observational study. Seven computer scientists were observed during and interviewed after performing their own work-related web search tasks. The observations indicated that experts have effective means for enhancing the searching, such as: using multiple search terms and operators, frequent query editing, using multiple windows, versatile result saving, and using the 'Find ' functionality. On the other hand, even the experts had misconceptions about the <b>default</b> <b>operator</b> and the ordering of the returned documents. Based on the results we suggest that search user interfaces should: 1) suggest alternative search terms, 2) explain search operators in natural language, 3) provide search history, and 4) facilitate users ’ orientation to the results. The suggestions are formulated as concrete solutions in a prototype user interface. The aim is to transfer the advanced strategies to the disposal of all users, not just experts...|$|E
5000|$|C++ {{objects in}} general behave like {{primitive}} types, so to copy a C++ object one {{could use the}} [...] (assignment) operator. There is a <b>default</b> assignment <b>operator</b> provided for all classes, but its effect may be altered {{through the use of}} operator overloading. There are dangers when using this technique (see slicing). A method of avoiding slicing can be implementing a similar solution to the Java [...] method for the classes, and using pointers. (Note that there is no built-in [...] method) ...|$|R
40|$|Abstract. Exponential {{crossover}} in Differential Evolution (DE), {{which is}} similar to 1 -point crossover in genetic algorithms, continues to be used today as a <b>default</b> crossover <b>operator</b> for DE. We demonstrate that expo-nential crossover exploits an unnatural feature of some widely used syn-thetic benchmarks such as the Rosenbrock function – dependencies be-tween adjacent variables. We show that for standard DE as well as state-of-the-art adaptive DE, exponential crossover performs quite poorly on benchmarks without this artificial feature. We also show that shuffled ex-ponential crossover, which removes this kind of search bias, significantly outperforms exponential crossover...|$|R
40|$|The {{past few}} years have seen a {{proliferation}} of search engines for the World Wide Web (WWW), {{as well as a}} growing number of specialized subject directories geared to the needs of health care professionals. Yet documentation on scope, coverage, and search features is often uneven at best; and even documented search features may not perform as advertised. This paper will present a group of sample searches to assist users in gauging database size, determining <b>default</b> search <b>operators,</b> and testing for the presence of advanced search features such as case sensitivity, stemming, and concept mapping for medical topics on English-language web sites...|$|R
40|$|In {{this paper}} we {{introduce}} DML: Default Modal Logic. DML is a logic endowed with a two-place modal connective {{that has the}} intended meaning of "If ff, then normally fi". On top of providing a well-defined tool for analyzing common default reasoning, DML allows nesting of the <b>default</b> <b>operator.</b> We present a semantic framework in which many of the known default proof systems can be naturally characterized, and prove soundness and completeness theorems for several such proof systems. Our semantics is a "neighbourhood modal semantics", and it allows for subjective defaults, that is, defaults may vary among different worlds within the same model. The semantics has an appealing intuitive interpretation and {{may be viewed as}} a set-theoretic generalization of the probabilistic interpretations of default reasoning. We show that our semantics is most general in the sense that any modal semantics that is sound for some basic axioms for default reasoning is a special case of our semantics. Such a generality result may serve to provide a semantical analysis of the relative strength of different proof systems and to show the nonexistence of semantics with certain properties. 2...|$|E
40|$|Abstract. Context aware {{ubiquitous}} technology {{provides us}} with much more conve-niences in our everyday life by dening contextual information in appropriate ways. Up to now, {{there has been a}} vast amount of researches and many context models proposed. Many characteristics have been incorporated into such models such as users, physical locations, available resources, etc. Some also take time factors into consideration. Most still focus on the interactions between context and users/groups rather than discussing follow-up operations deeply. In this study, we extend the scope of the context model to include metadata related to services and use it to describe the behaviours of general op-erations. Also, by further dividing the time factor into levels of importance, we can use time as the primary control factor to manipulate some specic operations so that oper-ations could be triggered by real time constraints and handled properly. Some critical operations within the whole system may need to be handled by specic users or groups. The occurrence of unexpected situations when the <b>default</b> <b>operator</b> cannot proceed with the corresponding operation on time must be taken care of by a pre-designed time-dependent multi-layer exception handling policy to maintain the integrity and security of the system. We also propose a critical operation scenario, describe the processes of our model and explain the importance of time driven events to prove its applicability...|$|E
40|$|The nonmonotonic logic Epistemic Default Logic (EDL) [Meyer and van der Hoek, 1993] {{is based}} on the metaphore of a meta-level architecture. It has already been {{established}} [Meyer and van der Hoek, 1993] how upward reflection can be formalized by a nonmonotonic entailment based on epistemic states, and the meta-level process by a (monotonic) epistemic logic. The meta-level reasoning at a given state can be viewed as the part of the reasoning pattern where it is determined what the candidates are for default assumptions to be made, depending on the knowledge and ignorance at that state. The outcome at the meta-level concerns default conclusions of the form Pφ, where φ is an object-level formula. In EDL, default conclusions are kept separate from the object level knowledge (they remain at the meta-level), by means of this explicit <b>default</b> <b>operator</b> P. If one wants to draw further conclusions from them using object level knowledge this should be done at the meta-level. Compared to a meta-level architecture, what is still missing in EDL is the step where the default assumptions are actually made, i. e., where such formulas φ are added to the object level knowledge. Here we actually ‘jump (down) to conclusions’. This is what should be achieved by the downward reflection step. In the current paper we introduce a formalization of this downward reflection step. Thus a formalization, called TED L, is obtained of the reasoning pattern as a whole...|$|E
40|$|ABSTRACT. Hybrid logic extends {{modal logic}} with support for {{reasoning}} about individual states, designated by so-called nominals. We study hybrid logic {{in the broad}} context of coalgebraic semantics, where Kripke frames are replaced with coalgebras for a given functor, thus covering {{a wide range of}} reasoning principles including, e. g., probabilistic, graded, <b>default,</b> or coalitional <b>operators.</b> Specifically...|$|R
40|$|Mutation {{testing is}} a {{fault-finding}} software testing technique that creates mutants by injecting a syntactic change into the source code. This technique operates by evaluating {{the ability of}} a testsuite to detect mutants, which is used to asses the quality of a test suite. The information from mutation testing can be used {{to improve the quality of}} a test suite by developing additional test cases. It is a promising technique, but the expensive nature of mutation testing has prevented its wide spread, practical use. One approach to make mutation testing more efficient is selective mutation testing. This approach proposes to use a subset of the available mutation operators, thereby reducing the number of mutants generated which will lead to a reduced execution time. According to theory many mutants are redundant and do not provide any useful information in the development of additional test cases. This study will evaluate PIT, a mutation testing tool, by using selective mutation testing on five programs, and compare the effectiveness of PIT with MuJava. The results showed that PIT’s <b>default</b> <b>operators</b> generated a small and effective set of mutants and at the same time giving a satisfying mutation score. However, there was no significant difference in mutation score produced by the different sets of operators. The test suites adequate for PIT managed to detect 75 - 85 % of MuJava’s mutants, which is relatively good when taken into consideration that PIT generated less than half as many mutants...|$|R
40|$|TPOT 0. 7 is now out, {{featuring}} multiprocessing {{support for}} Linux and macOS, customizable operator configurations, and more. TPOT now has multiprocessing support (Linux and macOS only). TPOT {{allows you to}} use multiple processes for accelerating pipeline optimization in TPOT with the n_jobs parameter in both TPOTClassifier and TPOTRegressor. TPOT now allows you to customize the operators and parameters explored during the optimization process. TPOT allows you to customize the list of operators and parameters in optimization process of TPOT with the config_dict parameter. The format of this customized dictionary {{can be found in}} the online documentation. TPOT now allows you to specify a time limit for evaluating a single pipeline (default limit is 5 minutes) in optimization process with the max_eval_time_mins parameter, so TPOT won't spend hours evaluating overly-complex pipelines. We tweaked TPOT's underlying evolutionary optimization algorithm to work even better, including using the mu+lambda algorithm. This algorithm gives you more control of how many pipelines are generated every iteration with the offspring_size parameter. Fixed a reproducibility issue where setting random_seed didn't necessarily result in the same results every time. This bug was present since version 0. 6. Refined the <b>default</b> <b>operators</b> and parameters in TPOT, so TPOT 0. 7 should work even better than 0. 6. TPOT now supports sample weights in the fitness function if some if your samples are more important to classify correctly than others. The sample weights option works the same as in scikit-learn, e. g., tpot. fit(x_train, y_train, sample_weights=sample_weights). The default scoring metric in TPOT has been changed from balanced accuracy to accuracy, the same default metric for classification algorithms in scikit-learn. Balanced accuracy can still be used by setting scoring='balanced_accuracy' when creating a TPOT instance...|$|R
40|$|This paper {{presents}} {{methods of}} default reasoning which {{allow us to}} draw negative conclusions that are not available {{in some of the}} models for inheritance reasoning. Some of these negative conclusions are shown to be logically required, while others result from an extension of the model to include the notion of a default negative assumption. The negative <b>default</b> assumption <b>operator</b> is exactly symmetrical to positive default assumption, and supports the drawing of extra negative conclusions. It is argued that in some domains negative conclusions are extremely important. An example is given from the medical domain to illustrate the usefulness of techniques for deducing negative facts. A formal definition of the inheritance model used, which in an earlier paper by the same author [Padgham 88] was shown to resolve a number of the classical problems in the literature on inheritance reasoning, is also given. 1...|$|R
40|$|Major {{changes from}} 1. 2. 0 to 1. 3. 0 OAI Publisher: fixed cache {{management}} fixed oai consistency (post feed) workflow branch fixed deletion of content when workflow of data sources are deleted D-Net enabling services: using cache for subscription access support only one subscription registry Mongo based services (mdstore, oaistore, wf logging) : using API of mongo-java-driver 3. 2. 2, removed usage of deprecated methods tracking {{the number of}} stored records to possibly highlight the collection of records with the same identifier GUI: enabling deletion of APIs via GUI enabling editing of metadata_identifier_path more info available in the datasource section removed map of data sources (TODO: adapt to the new google map API) Metadata collection: handling HTML illegal entities in collected XMLs Indexing: <b>default</b> query <b>operator</b> for "bag of words" queries set to AND instead of OR Workflow manager do not launch workflows that were scheduled for execution during a pause of the aggregation system ("prepare for shutdown"...|$|R
40|$|Default model {{theory is}} a nonmonotonic {{formalism}} for representing and reasoning about commonsense knowledge. Although this theory {{is motivated by}} ideas in Reiter's work on default logic, {{it is a very}} different, in some sense dual framework. We make Reiter's <b>default</b> extension <b>operator</b> into a constructive method of building models, not theories. Domain theory, which is a well established tool for partial information in the semantics of programming languages, is adopted as the basis for constructing partial models. One of the direct advantages of default model theory is that nonmonotonic reasoning can be conducted with monotonic logics, by using the method of model checking, instead of theorem proving. This paper reconsiders some of the laws of nonmonotonic consequence, due to Gabbay and to Kraus, Lehmann, and Magidor, in the light of default model theory. We remark that in general, Gabbay's law of cautious monotony is open to question. We consider an axiomatization of the nonmonotoni [...] ...|$|R
40|$|International audienceHybrid logic extends {{modal logic}} with support for {{reasoning}} about individual states, designated by so-called nominals. We study hybrid logic {{in the broad}} context of coalgebraic semantics, where Kripke frames are replaced with coalgebras for a given functor, thus covering {{a wide range of}} reasoning principles including, e. g., probabilistic, graded, <b>default,</b> or coalitional <b>operators.</b> Specifically, we establish generic criteria for a given coalgebraic hybrid logic to admit named canonical models, with ensuing completeness proofs for pure extensions on the one hand, and for an extended hybrid language with local binding on the other. We instantiate our framework with a number of examples. Notably, we prove completeness of graded hybrid logic with local binding...|$|R
500|$|One of {{the main}} reasons for writing a custom {{allocator}} is performance. Utilizing a specialized custom allocator may substantially improve the performance or memory usage, or both, of the program. [...] The <b>default</b> allocator uses <b>operator</b> new to allocate memory. This is often implemented as a thin layer around the C heap allocation functions, which are usually optimized for infrequent allocation of large memory blocks. This approach may work well with containers that mostly allocate large chunks of memory, like vector and deque. However, for containers that require frequent allocations of small objects, such as map and list, using the default allocator is generally slow. Other common problems with a malloc-based allocator include poor locality of reference, and excessive memory fragmentation.|$|R
40|$|Default {{domain theory}} is a {{framework}} for representing and reasoning about commonsense knowledge. Although this theory is motivated by ideas in Reiter's work on default logic, it is in some sense a dual framework. We make Reiter's <b>default</b> extension <b>operator</b> into a constructive method of building models, not theories. Domain theory, which is a well established tool for representing partial information in the semantics of programming languages, is adopted {{as the basis for}} constructing partial models. This paper considers some of the laws of nonmonotonic consequence, due to Gabbay and to Kraus, Lehmann, and Magidor, in the light of default domain theory. We remark that in some cases Gabbay's law of cautious monotony is open to question. We consider an axiomatization of the nonmonotonic consequence relation on prime open sets in the Scott topology [...] the natural logic [...] of a domain, which omits this law. We prove a representation theorem showing that such relations are in one to one correspondence with the consequence relations determined by extensions in Scott domains augmented with default sets. This means that defaults are very expressive: they can, in a sense, represent any reasonable nonmonotonic entailment. Results about what kind of defaults determine cautious monotony are also discussed. In particular, we show that the property of unique extensions guarantees cautious monotony, and we give several classes of default structures which determine unique extensions...|$|R
40|$|The {{goal of this}} {{document}} is to provide parameter settings for each tone mapping operator and all images used in our experiment. To fully {{understand the meaning of}} those parameters please refer to the respective original papers. A few restrictions were imposed to prepare images. We asked the respective tone mapping authors to apply such parameter values which lead to the best possible image accordingly to their judgement. For some tone mapping <b>operators</b> <b>default</b> values were chosen by their authors which also lead in such cases to very high quality images. Of course apart from the gamma correction (we fixed the gamma to 2. 2) no post processing such as sharpening for example were allowed. For Tumblin and Rushmeier [1993] we tried to follow the implementation of the foveal method [Tumblin et al. 1999] in which a user point to a region of interest with the mouse, the difference with the base operator is not only that the world adaptation is sampled in a constrained area, but also high and low luminance are remapped in a S-shaped curve to avoid cropping values to black and white. However, we had to sample world adaptation using the whole image which someho...|$|R
40|$|AbstractObjectivesThe {{purpose of}} this study was to assess whether the {{benefits}} conferred by radial access (RA) at an individual level are offset by a proportionally greater incidence of vascular access site complications (VASC) at a population level when femoral access (FA) is performed. BackgroundThe recent widespread adoption of RA for cardiac catheterization has been associated with increased rates of VASCs when FA is attempted. MethodsLogistic regression was used to calculate the adjusted VASC rate in a contemporary cohort of consecutive patients (2006 to 2008) where both RA and FA were used, and compared it with the adjusted VASC rate observed in a historical control cohort (1996 to 1998) where only FA was used. We calculated the adjusted attributable risk to estimate the proportion of VASC attributable to the introduction of RA in FA patients of the contemporary cohort. ResultsA total of 17, 059 patients were included. At a population level, the VASC rate was higher in the overall contemporary cohort compared with the historical cohort (adjusted rates: 2. 91 % vs. 1. 98 %; odds ratio [OR]: 1. 48, 95 % confidence interval [CI]: 1. 17 to 1. 89; p = 0. 001). In the contemporary cohort, RA patients experienced fewer VASC than FA patients (adjusted rates: 1. 44 % vs. 4. 19 %; OR: 0. 33, 95 % CI: 0. 23 to 0. 48; p < 0. 001). We observed a higher VASC rate in FA patients in the contemporary cohort compared with the historical cohort (adjusted rates: 4. 19 % vs. 1. 98 %; OR: 2. 16, 95 % CI: 1. 67 to 2. 81; p < 0. 001). This finding was consistent for both diagnostic and therapeutic catheterizations separately. The proportion of VASCs attributable to RA in the contemporary FA patients was estimated at 52. 7 %. ConclusionsIn a contemporary population where both RA and FA were used, the safety benefit associated with RA is offset by a paradoxical increase in VASCs among FA patients. The existence of this radial paradox should be taken into consideration, especially among trainees and <b>default</b> radial <b>operators...</b>|$|R
40|$|International audienceIn aspect-oriented {{programming}} (AOP) languages, advice {{evaluation is}} usually considered {{as part of}} the base program evaluation. This is also the case for certain pointcuts, such as if pointcuts in AspectJ, or simply all pointcuts in higher-order aspect languages like AspectScheme. While viewing aspects as part of base level computation clearly distinguishes AOP from reflection, it also comes at a price: because aspects observe base level computation, evaluating pointcuts and advice at the base level can trigger infinite regression. To avoid these pitfalls, aspect languages propose ad-hoc mechanisms, which increase the complexity for programmers while being insufficient in many cases. After shed- ding light on the many facets of the issue, this paper proposes to clarify the situation by introducing levels of execution in the programming language, thereby allowing aspects to observe and run at specific, possibly different, levels. We adopt a defensive default that avoids infinite regression, and gives advanced programmers the means to override this <b>default</b> using level-shifting <b>operators.</b> We then study execution levels both in practice and in theory. First, we study the relevance of the issues addressed by execution levels in existing aspect-oriented programs. We then formalize the semantics of execution levels and prove that the default semantics is indeed free of a certain form of infinite regression, which we call aspect loops. Finally, we report on existing implementations of execution levels for aspect-oriented extensions of Scheme, JavaScript and Java, discussing their implementation techniques and current applications...|$|R
40|$|E 2014 : 049, Increasing {{levels of}} Greenhouse gas {{emissions}} causing {{global warming and}} climate changes are major challenges in today’s society. One important contributor is the freight transport sector, which accounts for 20 % of all EU greenhouse gas emissions (Cefic, 2011). The automotive industry is meeting rising demands from external customers and public bodies as well as internal demands for accounting and reporting of emissions. These transports constitute an important role {{when it comes to}} minimizing environmental threats like global warming and air pollution. Especially since the amount of transports are not expected to decrease and the affects that our lifestyle have on the environment {{is becoming more and more}} apparent for everyone. This increases the need for companies to control their transports and to find ways to lower their emissions. Due to a recent reorganisation within the Volvo Car Group, the inbound and outbound logistics have become insourced within the company group. This resulted in a need for evaluation of each transport mode and the emissions deriving from each transport. The purpose of this thesis is to create a recommendation for emission reporting of greenhouse gases, sulphur oxide, nitrogen oxide and particulate matter to endeavour sustainable transports, within the Volvo Car Group. The recommendation is developed for quantification of emissions per each transported distance. Two examples of logistic chains are evaluated with the use of <b>default</b> values and <b>operator</b> specific values, hence a comparing analysis between the methods is made. Also the existing and future legal demands concerning each mode of transport are discussed. The research methodology is based on qualitative semi-structured interviews with the carriers of different modes of transport. Subsequently, literature studies were conducted to comprehend the methodologies used for emission reporting. The result from the interviews shows that the way of working with emission reporting varies in different companies and that some companies have well defined emission goals while others are more diffuse and visionary. The maturity level of emission reporting is closely linked to the enforcement of regulations from authorities. In the future, sea transports is the mode of transport with greatest challenges ahead, due to the Sulphur Emission Control Area (SECA) regulations coming into effect next year. In the comparison of calculating emissions based on values from Eco Transit, NTM and operator specific values, the result indicates that values from Eco Transit produce higher emission values compared to the values deriving from NTM and the transport operators. The generated recommendation for Volvo Cars suggests that emission reporting should be based on operator specific values in sea transports and default values in road, rail and air transports. The recommendation is to implement a strategy for improving the accuracy level from operator specific values on a yearly basis. For reporting greenhouse gases the recommendation is to use the CEN standard. For SOx, NOx and PM the recommendation is to use NTM emission factors. Legal demands will be the best way to decrease the amount of emissions from the transport sector, since it is competition neutral, and enforces organisations to step up without risking losing customers...|$|R

