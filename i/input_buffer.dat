189|299|Public
25|$|The {{word length}} for this {{computer}} Is 27 bits, of which 24 are used In computation. The remaining 3 bits are spare and synchronizing bits. The memory storage capability {{consists of a}} 6000 rpm magnetic disk with a storage capacity of 2985 words of which 2728 are addressable. The contents of memory include 20 cold-storage channels of 128 sectors (words) each, a hot-storage channel of 128 sectors, four rapid access loops (U,F,E,H) of 1, 4, 8, and 16 words respectively, four 1-word arithmetic loops (A, L,H,I), and a two 4-word <b>input</b> <b>buffer</b> input loops (V,R).|$|E
500|$|The {{unexpanded}} ZX81's tiny memory {{presents a}} major challenge to programmers. Simply displaying a full screen takes up to 793bytes, the system variables take up another 125bytes, and the program, <b>input</b> <b>buffer</b> and stacks need more memory on top of that. Nonetheless, ingenious programmers are able to achieve a surprising amount with just 1KB. One notable example is 1K ZX Chess by David Horne, which manages to include most {{of the rules of}} chess into only 672 bytes. The ZX81 conserves its memory to a certain extent by representing entire BASIC commands as one-byte tokens, stored as individual [...] "characters" [...] in {{the upper reaches of the}} machine's unique (non-ASCII) character set.|$|E
2500|$|Wubi {{distributes}} its characters very evenly {{and as such}} {{the vast}} majority of characters are uniquely defined by the 4 keystrokes discussed above. [...] One then types a space to move the character from the <b>input</b> <b>buffer</b> onto the screen. [...] In the event that the 4 letter representation of the character is not unique, one would type a digit to select the relevant character (for example, if two characters have the same representation, typing 1 would select the first, and 2 the second). [...] In most implementations, a space can always be typed and simply means 1 in an ambiguous setting. [...] Intelligent software will {{try to make sure that}} the character in the default position is the one desired.|$|E
5000|$|Read {{the first}} 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into <b>input</b> <b>buffers</b> in main memory and {{allocate}} the remaining 10 MB for an output buffer. (In practice, it might provide better performance {{to make the}} output buffer larger and the <b>input</b> <b>buffers</b> slightly smaller.) ...|$|R
3000|$|..., and in {{case there}} is still any {{unassigned}} MRU, these unallocated resources {{should be used to}} flush the <b>input</b> <b>buffers.</b> Since the minimum requirements for the SF have been already allocated, the spectral efficiency can be maximized by transmitting the data from those SFs associated to the best channel conditions. Considering that the status of the <b>input</b> <b>buffers</b> has been updated according to R [...]...|$|R
5000|$|The minimum {{buffer size}} {{for each of}} the 63 {{possible}} services (Service <b>Input</b> <b>Buffers)</b> is 128 bytes.|$|R
5000|$|Peekable reads (read without {{removing}} from pipe's <b>input</b> <b>buffer)</b> ...|$|E
5000|$|... 1. IBF (<b>Input</b> <b>Buffer</b> Full) - It is an output {{indicating}} that the input latch contains information.|$|E
50|$|In Unix the {{end-of-file}} character (by default EOT) {{causes the}} terminal driver {{to make available}} all characters in its <b>input</b> <b>buffer</b> immediately; normally the driver would collect characters until it sees an end-of-line character. If the <b>input</b> <b>buffer</b> is empty (because no characters have been typed since the last end-of-line or end-of-file), a program reading from the terminal reads a count of zero bytes. In Unix, such a condition is understood as having {{reached the end of}} the file.|$|E
40|$|This paper {{presents}} a simulation study of an <b>input</b> <b>buffered</b> Asynchronous Transfer Mode (ATM) switch running the Request-Grant-Status (RGS) iterative scheduling algorithm. The RGS algorithm {{can reduce the}} effects of head-of-line blocking on the throughput and delay performance of an <b>input</b> <b>buffered</b> ATM switch. During a cell slot interval, the RGS algorithm performs a fixed number (n) of iterations. At each iteration step, the RGS algorithm determines a match between buffered cells and corresponding idle output ports. After the last iteration step, all matched cells are routed through the switch fabric. The simulation {{results show that the}} RGS algorithm can significantly improve the throughput and delay performance of an <b>input</b> <b>buffered</b> ATM switch under a high load. When independent and identical Bernoulli traffic sources are used, the input queues of a 16 x 16 ATM switch, using a strict first-in first-out queueing discipline, saturate at an offered load of approximately 60 %. When the RGS algorithm is used with 5 iterations, the input queues of a 16 x 16 <b>input</b> <b>buffered</b> switch, under the same traffic conditions, d...|$|R
40|$|An <b>input</b> <b>buffered</b> {{packet switch}} called the odd-even {{multicast}} switch is proposed. The packet splitting probability {{of the proposed}} switch is derived and the packet output contention is resolved using the cyclic-priority reservation (CPR) algorithm. The throughput and mean packet delay of the proposed switch are compared with a simple <b>input</b> <b>buffered</b> switch. It is found that the proposed switch gives a significant performance improvement {{at the expense of}} extra packet splitting overhead. link_to_subscribed_fulltex...|$|R
40|$|To {{support the}} Internet's {{explosive}} growth and expansion into a true integrated services network, {{there is a}} need for cost-effective switching technologies that can simultaneously provide high capacity switching and advanced QoS. Unfortunately, these two goals are largely believed to be contradictory in nature. To support QoS, sophisticated packet scheduling algorithms, such as Fair Queueing, are needed to manage queueing points. However, the bulk of current research in packet scheduling algorithms assumes an output buffered switch architecture, whereas most high performance switches (both commercial and research) are <b>input</b> <b>buffered.</b> While output buffered systems may have the desired quality of service, they lack the necessary scalability. <b>Input</b> <b>buffered</b> systems, while scalable, lack the necessary quality of service features. In this paper, we propose the construction of switching systems that are both <b>input</b> and output <b>buffered,</b> with the scalability of <b>input</b> <b>buffered</b> switches and the r [...] ...|$|R
50|$|During the 1970s, Stuart {{created a}} version of the {{programming}} language FORTH, which he called LaFORTH and is notable for its implementation without an <b>input</b> <b>buffer.</b>|$|E
50|$|The {{purpose of}} this {{fallback}} was {{to ensure that the}} access method would allocate an <b>input</b> <b>buffer</b> set which was large enough to accommodate any and all of the specified datasets.|$|E
50|$|Incremental - The {{incremental}} inputs {{are basically}} independent of program control and consist of seven resolver type, two variable incremental type, and one pulse type. These inputs are accumulated {{in the two}} four-word <b>input</b> <b>buffer</b> loops (V&R).|$|E
40|$|Abstract. <b>Input</b> <b>Buffered</b> {{switches}} can {{not directly}} deal with variable size packets. The variable length packets {{need to be}} segmented into fixed length cells before scheduling. Then segmentation and reassembly (SAR) must be used. The traditional SAR scheme can lead to significant loss of fabric bandwidth due to the padding bytes, which requires speed up in switch fabric to compensate for this bandwidth loss. The improved scheme called cell merging can reduce the bandwidth loss greatly but it has low scalability {{and it is difficult}} to select suitable merging size. In this paper, we propose a new method of SAR for <b>Input</b> <b>buffered</b> switches using the queue information of switches and can adjust the segment size dynamically. New scheme is suitable for different traffic model and can provide an excellent delay performance. We evaluate DSAR scheme using simulation, the results show that it outperforms existing segmentation schemes in <b>Input</b> <b>Buffered</b> switches...|$|R
3000|$|... {{finishes}} decoding a codeword, it can {{be scheduled}} to decode a new codeword {{from one of the}} <b>input</b> <b>buffers,</b> or {{it can be}} scheduled to help another decoder [...]...|$|R
40|$|A {{conventional}} Network-on-Chip (NoC) router uses <b>input</b> <b>buffers</b> {{to store}} in-flight packets. These buffers improve performance, but consume significant power. It {{is possible to}} bypass these buffers when they are empty, reducing dynamic power, but static buffer power, and dynamic power when buffers are utilized, remain. To improve energy efficiency, buffer less deflection routing removes <b>input</b> <b>buffers,</b> and instead uses deflection (misrouting) to resolve contention. However, at high network load,deflections cause unnecessary network hops, wasting power and reducing performance. In this work, we propose a new NoC router design called the minimally-buffered deflection (MinBD) router. This router combines deflection routing with a small 2 ̆ 2 side buffer, 2 ̆ 2 which is much smaller than conventional <b>input</b> <b>buffers.</b> A MinBD router places some network traffic that would have otherwise been deflected in this side buffer, reducing deflections significantly. The router buffers {{only a fraction of}} traffic, thus making more efficient use of buffer space than a router that holds every flit in its <b>input</b> <b>buffers.</b> We evaluate MinBD against input-buffered routers of various sizes that implement buffer bypassing, a buffer less router, and a hybrid design, and show that MinBD is more energy efficient than all prior designs, and has performance that approaches the conventional input-buffered router with area and power close to the buffer less router...|$|R
50|$|The <b>input</b> <b>buffer</b> is a queue where {{events are}} stored (from keyboard, mouse etc.). The output buffer is a {{rectangular}} grid where characters are stored, {{together with their}} attributes. A console window may have several output buffers, only {{one of which is}} active (i.e. displayed) for a given moment.|$|E
5000|$|Some older {{computer}} games, {{especially those}} on 8-bit platforms, often had the combination IJKM {{used as the}} standard control key combination, which was more logically arranged, if far less ergonomic than an inverted-T. In addition, on the Apple II platform, special support existed in ROM for Escape mode. At the Applesoft BASIC prompt, using {{the right and left}} arrow keys to move the cursor would add/remove characters the cursor passed over to/from the <b>input</b> <b>buffer.</b> Pressing the Escape key entered a mode where pressing the , , [...] or [...] keys would move the cursor without altering the <b>input</b> <b>buffer.</b> After exiting this mode by pressing Escape again, normal behavior would resume. This made it easy to edit lines of BASIC code by listing them, then re-inputting them with edits interspersed.|$|E
50|$|Win32 console is a text user {{interface}} implementation {{within the system}} of Windows API, which runs console applications. A Win32 console has a screen buffer and an <b>input</b> <b>buffer,</b> and is available both as a window or in text mode screen, with switching back and forth available via Alt-Enter keys.|$|E
40|$|Abstract Fork/join {{stations}} model synchronization constraints in queuing network {{models of}} many manufacturing and computer systems. We consider a fork/join station with two <b>input</b> <b>buffers</b> and general <b>inputs</b> from finite populations and derive approximate expressions for throughput and mean queue lengths at the <b>input</b> <b>buffers.</b> We {{assume that the}} arrivals to the fork/join stations are renewal, but our approximations only use information about the first two moments of the inter-renewal distributions. Therefore the approximations can be use d to predict performance {{for a variety of}} systems. We verify the accuracy of these approximations against simulation and report sample results...|$|R
5000|$|A switch may be {{composed}} of <b>buffered</b> <b>input</b> ports, a switch fabric and buffered output ports. If first-in first-out (FIFO) <b>input</b> <b>buffers</b> are used, only the oldest packet is available for forwarding. More recent arrivals cannot be forwarded if the oldest packet cannot be forwarded because its destination output is busy.The output may be busy if: ...|$|R
40|$|Problem statement: Optical Packet Switching (OPS) and {{transmission}} networks based on Wavelength Division Multiplexing (WDM) have been increasingly {{deployed in the}} Internet infrastructure {{over the last decade}} {{in order to meet the}} huge increasing demand for bandwidth. Several different technologies have been developed for optical packet switching such as space switches, broadcast-and-select, <b>input</b> <b>buffered</b> switches and output buffered switches. These architectures vary based on several parameters such as the way of optical buffering, the placement of optical buffers, the way of solving the external blocking inherited from switching technologies in general and the components used to implement WDM. Approach: This study surveys most of the exiting optical packet switching architectures. A simulation-based comparison of <b>input</b> <b>buffered</b> and output buffered architectures were presented. Results: The performance analysis of the selected two architectures derived using simulation program and compared at different scenarios. We found that the output buffered architectures give better performance than <b>input</b> <b>buffered</b> architectures. Conclusion: The simulation results shows that the-broadcast-and-select architecture is attractive in terms that it has lees number of components compared to other switches. </P...|$|R
50|$|Shared individuals, {{states and}} events may look {{differently}} to the domains that share them. Consider for example an interface between {{a computer and}} a keyboard. When the keyboard domain sees an event Keyboard operator presses the spacebar the computer will see the same event as Byte hex("20") appears in the <b>input</b> <b>buffer.</b>|$|E
50|$|IO {{instructions}} left {{punctuation bits}} unchanged, reading or writing only data (and parity) bits into memory, and terminating on any record mark encountered. A record mark {{could be placed}} {{at the end of}} an <b>input</b> <b>buffer</b> to prevent any buffer overflow, a problem that was to persist in many other systems into the 21st century.|$|E
50|$|Implementations {{that rely}} on line-based input may require a newline {{character}} after each JSON object {{in order for the}} object to be emitted by the parser in a timely manner. (Otherwise the line may remain in the <b>input</b> <b>buffer</b> without being passed to the parser.) This is rarely recognised as an issue because terminating JSON objects with a newline character is very common.|$|E
40|$|Abstract—As {{the chip}} {{multiprocessor}} (CMP) design moves toward many-core architectures, communication delay in Network-on-Chip (NoC) {{has been a}} major bottleneck in CMP systems. Using high-density memories in <b>input</b> <b>buffers</b> helps to reduce the bottleneck through increasing throughput. Spin-Torque Transfer Magnetic RAM (STT-MRAM) can be a suitable solution due to its nature of high density and nearzero leakage power. But its long latency and high power consumption in write operations still need to be addressed. We explore the design issues in using STT-MRAM for NoC <b>input</b> <b>buffers.</b> Motivated by short intra-router latency, we use the previously proposed write latency reduction technique sacrificing retention time. Then we propose a hybrid design of <b>input</b> <b>buffers</b> using both SRAM and STT-MRAM to hide the long write latency efficiently. Considering that simple data migration in the hybrid buffer consumes more dynamic power compared to SRAM, we provide a lazy migration scheme that reduces the dynamic power consumption of the hybrid buffer. Simulation results show that the proposed scheme enhances the throughput by 21 % on average. Keywords-Network-on-Chip; STT-MRAM; router; input buffer; I...|$|R
40|$|Abstract: Problem statement: Optical Packet Switching (OPS) and {{transmission}} networks based on Wavelength Division Multiplexing (WDM) have been increasingly {{deployed in the}} Internet infrastructure {{over the last decade}} {{in order to meet the}} huge increasing demand for bandwidth. Several different technologies have been developed for optical packet switching such as space switches, broadcast-and-select, <b>input</b> <b>buffered</b> switches and output buffered switches. These architectures vary based on several parameters such as the way of optical buffering, the placement of optical buffers, the way of solving the external blocking inherited from switching technologies in general and the components used to implement WDM. Approach: This study surveys most of the exiting optical packet switching architectures. A simulation-based comparison of <b>input</b> <b>buffered</b> and output buffered architectures were presented. Results: The performance analysis of the selected two architectures derived using simulation program and compared at different scenarios. We found that the output buffered architectures give better performance than <b>input</b> <b>buffered</b> architectures. Conclusion: The simulation results shows that the-broadcast-and-select architecture is attractive in terms that it has lees number of components compared to other switches...|$|R
5000|$|... tri-state bufferUnlike {{the single}} <b>input</b> digital <b>buffer</b> which {{has only one}} <b>input,</b> Tri-state digital <b>buffer</b> has two inputs: a data input and a control input. (A control input is {{analogous}} to a valve, which controls the data flow.) When the control input is active, the output value is the input value, and the buffer is not different from the single <b>input</b> digital <b>buffer.</b>|$|R
50|$|Since SneakPeek accepts packets {{coming through}} an <b>input</b> <b>buffer</b> and {{delivers}} packets through an output buffer {{it can be}} easily connected to any standard I/O or network.The MIPI Alliance Specification for SneakPeek Protocol describes the basic concepts, the required infrastructure, the packets and the data flow. The last MIPI board adopted version of Specification for SneakPeek Protocol (SPPSM) is version 1.0 (August 2015).|$|E
50|$|Optimistic {{decompression}} is {{a digital}} forensics technique {{in which each}} byte of an <b>input</b> <b>buffer</b> is examined {{for the possibility of}} compressed data. If data is found that might be compressed, a decompression algorithm is invoked to perform a trial decompression. If the decompressor does not produce an error, the decompressed data is processed. The decompressor is thus called optimistically---that is, with the hope that it might be successful.|$|E
50|$|Volumetric {{lighting}} requires two {{components: a}} light space shadow map, and a depth buffer. Starting at the near clip {{plane of the}} camera, the whole scene is traced and sampling values are accumulated into the <b>input</b> <b>buffer.</b> For each sample, it is determined if the sample is lit by the source of light being processed using the shadow map as a comparison. Only lit samples will affect final pixel color.|$|E
50|$|Head-of-line {{blocking}} (HOL blocking) {{in computer}} networking is a performance-limiting phenomenon {{that occurs when}} a line of packets is held up by the first packet. Examples include <b>input</b> <b>buffered</b> network switches, out-of-order delivery and multiple requests in HTTP pipelining.|$|R
5000|$|This {{phenomenon}} {{limits the}} throughput of switches. For FIFO <b>input</b> <b>buffers,</b> a simple model of fixed-sized cells to uniformly distributed destinations, causes the throughput {{to be limited}} to 58.6% of the total as the number of links becomes large.|$|R
40|$|This {{paper is}} {{concerned}} with the evaluation of combined dispatching and routeing strategies on the performance of a flexible manufacturing system. Three routeing policies: no alternative routeings, alternative routeings dynamic and alternative routeings planned are considered with four dispatching rules with finite buffer capacity. In addition, the effect of changing part mix ratios is also discussed. The performance measures considered are makespan, average machine utilization, average flow time and average delay at local <b>input</b> <b>buffers.</b> Simulation results indicate that the alternative routeings dynamic policy gives the best results in three performance measures except for average delay at local <b>input</b> <b>buffers.</b> Further, the effect of changing part mix ratios is not significant. link_to_subscribed_fulltex...|$|R
