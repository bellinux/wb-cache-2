254|10000|Public
25|$|Mode: for a <b>discrete</b> <b>random</b> <b>variable,</b> {{the value}} with highest {{probability}} (the location {{at which the}} probability mass function has its peak); for a continuous random variable, a location at which the probability density function has a local peak.|$|E
25|$|In the {{continuous}} univariate case above, the reference measure is the Lebesgue measure. The {{probability mass function}} of a <b>discrete</b> <b>random</b> <b>variable</b> is the density {{with respect to the}} counting measure over the sample space (usually the set of integers, or some subset thereof).|$|E
2500|$|The entropy of a <b>discrete</b> <b>random</b> <b>variable</b> is a nonnegative number: ...|$|E
40|$|Available {{characterizations}} {{of the various}} notions of stochastic dominance concern continuous <b>random</b> <b>variables.</b> Yet, <b>discrete</b> <b>random</b> <b>variables</b> are often used either in pedagogical presentations of stochastic dominance or in experimental tests of this notion. This note provides complete {{characterizations of}} the various notions of stochastic dominance for <b>discrete</b> <b>random</b> <b>variables.</b> Stochastic Dominance; <b>Discrete</b> <b>Random</b> <b>Variables...</b>|$|R
40|$|In this study, the {{distributions}} of X_r:n&nbsp;order statistic of innid <b>discrete</b> <b>random</b> <b>variables</b> are obtained. In addition, the distributions are also {{expressed in the}} form of an integral. Then, the results related to pf and dfof minimum and maximum order statistics of innid <b>discrete</b> <b>random</b> <b>variables</b> are given...|$|R
5000|$|... #Subtitle level 2: Independent identically {{distributed}} <b>discrete</b> <b>random</b> <b>variables</b> ...|$|R
2500|$|Suppose X is a <b>discrete</b> <b>random</b> <b>variable</b> whose values {{lie in the}} set { 0, 1, 2, ... }.|$|E
2500|$|For a <b>discrete</b> <b>random</b> <b>variable</b> X, let u0, u1, ... be the [...] values it {{can take}} with non-zero probability. Denote ...|$|E
2500|$|... {{called a}} <b>discrete</b> <b>random</b> <b>variable</b> {{provided}} its {{image is a}} countable set and the pre-image of singleton sets are measurable, i.e., [...] for all [...]|$|E
40|$|In this study, the {{distributions}} of rth order statistic of innid <b>discrete</b> <b>random</b> <b>variables</b> are obtained. In addition, the distributions are also {{expressed in the}} form of an integral. Then, the results related to pf and df of minimum and maximum order statistics of innid <b>discrete</b> <b>random</b> <b>variables</b> are given...|$|R
40|$|In this paper, {{we propose}} a novel method for {{increasing}} the entropy of a sequence of independent, <b>discrete</b> <b>random</b> <b>variables</b> with arbitrary distributions. The method uses an auxiliary table and a novel theorem that concerns the entropy of a sequence in which the elements are a bitwise exclusive-or sum of independent <b>discrete</b> <b>random</b> <b>variables...</b>|$|R
30|$|All above results {{deal with}} {{continuous}} <b>random</b> <b>variable.</b> Finally, we give {{two examples of}} <b>discrete</b> <b>random</b> <b>variables.</b>|$|R
2500|$|A <b>discrete</b> <b>random</b> <b>variable</b> X {{is said to}} have a Poisson {{distribution}} with parameter λ>0, if, for k=0,1,2,..., {{the probability}} mass function of X is given by: ...|$|E
2500|$|Named after Boltzmann's Η-theorem, Shannon {{defined the}} entropy [...] (Greek capital letter eta) of a <b>discrete</b> <b>random</b> <b>variable</b> [...] with {{possible}} values [...] and {{probability mass function}} [...] as: ...|$|E
2500|$|In particular, if [...] is a <b>discrete</b> <b>random</b> <b>variable</b> {{assuming}} [...] with corresponding probability masses , then in {{the formula}} for total variance, the first term {{on the right-hand side}} becomes ...|$|E
25|$|Categorical distribution: for <b>discrete</b> <b>random</b> <b>variables</b> with {{a finite}} set of values.|$|R
5000|$|The joint {{probability}} mass function of two <b>discrete</b> <b>random</b> <b>variables</b> [...] is: ...|$|R
40|$|Authors study {{properties}} of linear optimization problems under probabilistic uncertainty while defining a problem {{based on the}} linear order {{on the set of}} <b>discrete</b> <b>random</b> <b>variables.</b> Properties of unconditional problem are established whose coefficients of the goal function or multiset?s elements (but not both simultaneously) are <b>discrete</b> <b>random</b> <b>variables.</b> Based on {{properties of}} the solution of an unconditional problem with deterministic coefficients, we prove solution?s properties for the problem with the goal function?s coefficients as <b>discrete</b> <b>random</b> <b>variables.</b> The scheme of the branch and bound method for solving the linear optimization problems on permutations under probabilistic uncertainty is proposed as well as rules of branching and truncation of sets. ??????????? ???????? ???????? ????? ??????????? ?? ??????????? ? ????????????? ?????????????????, ?????????? ??????? ???????????? ?? ?????? ???????? ????????? ??????? ?? ????????? ?????????? ????????? ???????. ??????????? ???????? ??????????? ??????, ? ??????? ???????????? ??????? ??????? ??? ???????? ??????????????? (?? ?? ?? ? ?????? ????????????) ???????? ??????????? ?????????? ??????????. ??????????? ?? ????????? ??????? ??????????? ?????? ? ?????????????????? ?????????????? ??????? ???????, ???????? ???????? ??????? ??? ??????, ? ??????? ???????????? ??????? ??????? ???????? ?????????? ??????????. ?????????? ????? ?????? ?????? ? ?????? ??? ??????? ???????? ????? ??????????? ?? ??????????? ? ????????????? ?????????????????, ? ??????? ????? ?????????? ??????? ????????? ? ????????? ????????...|$|R
2500|$|Intuitively, the entropy [...] of a <b>discrete</b> <b>random</b> <b>variable</b> [...] is {{a measure}} of the amount of {{uncertainty}} associated with the value of [...] when only its distribution is known.|$|E
2500|$|A {{discrete}} {{probability distribution}} is a probability distribution {{characterized by a}} probability mass function. [...] Thus, the distribution of a random variable X is discrete, and X is called a <b>discrete</b> <b>random</b> <b>variable,</b> if ...|$|E
2500|$|A six-sided fair die can be {{modelled}} with a <b>discrete</b> <b>random</b> <b>variable</b> with outcomes 1 through 6, {{each with}} equal probability [...] The expected value is [...] Therefore, the variance can be computed to be ...|$|E
5000|$|Probability mass, Probability mass function, p.m.f., Discrete {{probability}} distribution function: for <b>discrete</b> <b>random</b> <b>variables.</b>|$|R
30|$|In general, the {{distribution}} theory for order statistics is complex when the parent distribution is discrete. In this study, the joint distributions of order statistics of innid <b>discrete</b> <b>random</b> <b>variables</b> {{are expressed in}} the form of an integral. As far as we know, these approaches have not been considered in the framework of order statistics from innid <b>discrete</b> <b>random</b> <b>variables.</b>|$|R
5000|$|In recent years, {{the term}} [...] "{{algebraic}} statistics" [...] {{has been used}} more restrictively, to label the use of algebraic geometry and commutative algebra to study problems related to <b>discrete</b> <b>random</b> <b>variables</b> with finite state spaces. Commutative algebra and algebraic geometry have applications in statistics because many commonly used classes of <b>discrete</b> <b>random</b> <b>variables</b> {{can be viewed as}} algebraic varieties.|$|R
2500|$|... {{where the}} {{imaginary}} {{part of a}} complex number [...] is given by [...]The integral may be not Lebesgue-integrable; for example, when X is the <b>discrete</b> <b>random</b> <b>variable</b> that is always 0, it becomes the Dirichlet integral.|$|E
2500|$|An {{important}} {{example of}} the unilateral Z-transform is the probability-generating function, where the component x is {{the probability that a}} <b>discrete</b> <b>random</b> <b>variable</b> takes the value n, and the function X(z) is usually written as X(s), in terms of s = z−1. [...] The properties of Z-transforms (below) have useful interpretations in the context of probability theory.|$|E
2500|$|In {{classical}} mechanics, {{the center}} of mass is an analogous concept to expectation. For example, suppose X is a <b>discrete</b> <b>random</b> <b>variable</b> with values xi and corresponding probabilities pi. Now consider a weightless rod on which are placed weights, at locations xi along the rod and having masses pi (whose sum is one). The {{point at which the}} rod balances is E.|$|E
50|$|Bivariate {{marginal}} {{and joint}} probabilities for <b>discrete</b> <b>random</b> <b>variables</b> are often displayed as two-way tables.|$|R
2500|$|Probability mass, Probability mass function, p.m.f., Discrete {{probability}} distribution function: [...] for <b>discrete</b> <b>random</b> <b>variables.</b>|$|R
2500|$|Whereas the pdf {{exists only}} for {{continuous}} <b>random</b> <b>variables,</b> the cdf exists for all <b>random</b> <b>variables</b> (including <b>discrete</b> <b>random</b> <b>variables)</b> that take values in ...|$|R
2500|$|Equivalently to the above, a <b>discrete</b> <b>random</b> <b>{{variable}}</b> can {{be defined}} as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities—that is, its cdf increases only where it [...] "jumps" [...] to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.|$|E
2500|$|It is {{possible}} to represent certain discrete random variables as well as random variables involving both a continuous and a discrete part with a generalized probability density function, by using the Dirac delta function. For example, let us consider a binary <b>discrete</b> <b>random</b> <b>variable</b> having the Rademacher distributionthat is, taking −1 or 1 for values, with probability ½ each. The density of probability associated with this variable is: ...|$|E
2500|$|... as u {{runs through}} the set of all {{possible}} values of X. A <b>discrete</b> <b>random</b> <b>variable</b> can assume only a finite or countably infinite number of values. For the number of potential values to be countably infinite, even though their probabilities sum to 1, the probabilities have to decline to zero fast enough. For example, if [...] for n = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.|$|E
5000|$|Formally, {{the mutual}} {{information}} of two <b>discrete</b> <b>random</b> <b>variables</b> X and Y {{can be defined}} as: ...|$|R
30|$|We now {{establish}} three {{results for}} the df of single order statistic of innid <b>discrete</b> <b>random</b> <b>variables.</b>|$|R
50|$|In {{contrast}} to the conditional entropy for <b>discrete</b> <b>random</b> <b>variables,</b> the conditional differential entropy may be negative.|$|R
