68|7900|Public
50|$|The {{computational}} framework of label free approach includes detecting peptides, matching the corresponding peptides across multiple LC-MS <b>data,</b> <b>selecting</b> discriminatory peptides.|$|E
3000|$|...] and {{requires}} acute focus on <b>data</b> <b>selecting,</b> mapping, testing and determining accessibility roles {{in case of}} recovery process.|$|E
40|$|An {{automated}} circuit-modeling tool {{is developed}} for arbitrary passive components. The tool builds compact, parameterized, analytical models based on fullwave EM simulations. The scattering parameters (or {{the transmission line}} parameters) of the components are stored as a multidimensional function of frequency and geometrical parameters. The modeling algorithm combines adaptive <b>data</b> <b>selecting</b> and modeling techniques. The circuit models guarantee EM-accuracy and generality, and circuit simulation speed and flexibility...|$|E
5000|$|Half-life, spin, and isomer <b>data</b> <b>selected</b> {{from the}} {{following}} sources.|$|R
50|$|Cynergy <b>Data</b> <b>selects</b> TSYS as its {{preferred}} {{provider of}} authorization, settlement and dispute resolution services.|$|R
50|$|Jones sold part of Phones International Group, Wireless Logic, for £38 {{million in}} 2011, {{retaining}} the <b>Data</b> <b>Select</b> part.|$|R
40|$|The INLEN system {{combines}} database, knowledge base, {{and machine}} learning technologies {{to provide a}} user with an integrated system of tools for conceptually analyzing data and searching for interesting relationships and regularities in them. Machine learning techniques are used for determining general descriptions from facts, creating conceptual classifications of <b>data,</b> <b>selecting</b> the most relevant attributes, determining the most representative examples, and discovering equations governing numeric variables, as. well as formulating applicability conditions for these equations...|$|E
40|$|In this {{dissertation}} {{we present}} {{a search for the}} associated production of charginos and neutralinos, the supersymmetric partners of the Standard Model bosons. We analyze a data sample representing 745 pb - 1 of integrated luminosity collected by the CDF experiment at the pp¯ Tevatron collider. We compare the Standard Model predictions with the observed <b>data</b> <b>selecting</b> events with three leptons and missing transverse energy. Finding no excess, we combine the results of our search with similar analyses carried out at, CDF and set an upper limit, on the chargino mass in SUSY scenarios. ...|$|E
40|$|A conference poster {{presented}} at the American Medical Informatics Association (AMIA) 's 2008 Annual SymposiumHealth information exchange across multiple organizations requires a method or algorithm to optimally link records of the same individuals using demographic <b>data.</b> <b>Selecting</b> the best record linkage algorithm requires an evaluation to determine its sensitivity and specificity. This evaluation is facilitated by a large database test bed that closely reflects a real world population and {{takes into account the}} potential data entry errors that unfortunately occur in real-world databases. This study investigated the synthesis of such a database. Agency for Healthcare Research and Quality (AHRQ) grant number UC 1 HS 1615...|$|E
30|$|Category is a {{supplementary}} axis {{that enables}} personal <b>data</b> to be <b>selected.</b> A text tag is one {{item of information}} in a category. It is also useful for filtering large amounts of <b>data</b> <b>selected</b> with the above viewpoint.|$|R
2500|$|Burn - General purpose CD and DVD burning utility {{that can}} write AUDIO_TS <b>data.</b> <b>Select</b> [...] "Audio" [...] tab then [...] "DVD-Audio" [...] from the {{drop-down}} menu.|$|R
50|$|Clocking {{changes on}} TMS steps through a {{standardized}} JTAG state machine. The JTAG state machine can reset, access an instruction register, or access <b>data</b> <b>selected</b> by the instruction register.|$|R
40|$|With {{the rise}} of social {{networking}} sites user information is becoming increasingly complex and sophisticated. The needs, behaviours and preferences of users are dynamically changing, depending on their background knowledge, their current task, and many other parameters. Existing ontology models capture demographic information {{as well as the}} users’ activities and interactions in online communities. These vocabularies represent the raw data, but actionable knowledge comes from filtering these <b>data,</b> <b>selecting</b> useful features, and mining the resulting information to uncover the most salient preferences, behaviours and needs of the users. In this paper we propose reusing and reengineering ontological resources to provide a broader representation of users and the dynamics that emerge from the virtual social environments in which they participate...|$|E
40|$|Abstract—One {{of the key}} {{problems}} in path testing is building a path through specified set of stalemates particularly which contain loops. Traditional genetic algorithm has been successfully used in software testing activities such as finding test <b>data,</b> <b>selecting</b> test cases and test cases prioritization. In this paper, we introduce a new variable length genetic algorithm. Based on the new algorithm, we present a new strategy for automatically generating a set of basis test paths {{which can be used}} as testing paths in any basis path testing technique. We define all elements of genetic algorithm such as chromosome representation, crossover, mutation, and fitness function to be compatible with path generation. In addition, we present a case study to show the efficiency of our strategy...|$|E
40|$|This paper {{discusses}} {{the use of}} GAs (Genetic Algorithms) and TS (Tabu Search) to design NNCs (Neural Network Controllers) for Real-time control of flows in sewerage networks. Genetic algorithms evolve the weights for Neural Networks Controllers. We apply a modified Tabu Search algorithm in a novel fashion, to select the most relevant training data, {{in order to reduce}} the training time. The comparison between this approach and various fixed penstock control settings, and genetically-designed Neural Networks, is discussed. This paper reports experiments demonstrating that GAs are both effective and robust to design Neural Networks controllers in sewerage network control problems. To confirm whether the GA-Tabu training algorithm has statistically significant better performance than other <b>data</b> <b>selecting</b> algorithms, a t-test with a 5...|$|E
50|$|JCB {{licenses}} its brand for {{a series}} of rugged feature phones and smartphones targeted at construction personnel. The design and marketing contract was awarded to <b>Data</b> <b>Select</b> in 2010.|$|R
30|$|Enabling <b>data</b> {{sources to}} <b>select</b> <b>data</b> of {{interest}} and stream the result.|$|R
50|$|Landsat 7 {{continues}} to acquire {{data in this}} mode. Data products are available with the missing data optionally filled in using other Landsat 7 <b>data</b> <b>selected</b> by the user. In 2013, Landsat 7 was joined by Landsat 8.|$|R
40|$|Network {{forensic}} techniques help {{in tracking}} {{different types of}} cyber attack by monitoring and inspecting network traffic. However, with the high speed and large sizes of current networks, and the sophisticated philosophy of attackers, in particular mimicking normal behaviour and/or erasing traces to avoid detection, investigating such crimes demands intelligent network forensic techniques. This paper suggests a real-time collaborative network Forensic scheme (RCNF) that can monitor and investigate cyber intrusions. The scheme includes three components of capturing and storing network <b>data,</b> <b>selecting</b> important network features using chi-square method and investigating abnormal events using a new technique called correntropy-variation. We provide a case study using the UNSW-NB 15 dataset for evaluating the scheme, showing its high performance in terms of accuracy and false alarm rate compared with three recent state-of-the-art mechanisms...|$|E
40|$|Often in pattern {{classification}} problems, one {{tries to}} extract {{a large number}} offeatures and base the classifier decision on {{as much information as}} possible. This yields an array offeatures that are 'potentially ' useful. Most of the time however, large feature sets are sub-optimal in describing the samples since they tend to over-represent the data and model. noise along with the useful. information in the <b>data.</b> <b>Selecting</b> relevant features from the available set offeatures is, therefore, a challenging task. In this paper, we present an innovative feature sel. ection al. gorithm called Smart Beam Search (SBS). which is used with a Support Vector Machine based classifier for automatic defect classification. This feature selection approach not only reduces the dimensionality of the feature space substantially, but also improves the classifier performance. 1...|$|E
40|$|Abstract. Most {{adaptive}} multimedia multicast applications {{require the}} source {{to select the}} number of streams to transmit {{as well as the}} QoS parameters for each stream. If the receivers have different bandwidth limits for their devices and have various preferences for the quality of the <b>data,</b> <b>selecting</b> the QoS parameters that generate the best average satisfaction for all receivers is a challenging problem. In this paper, we developed a selection algorithm that is based on the user profiles and the device capabilities. Receivers are required to send their profiles and the bandwidth limitation on their devices to the source once before the session starts. To avoid the implosion problem and have a constant running time for the selection algorithm, we partition the receivers according to the bandwidth limit of their devices into classes and use a virtual representative for each class of receivers. ...|$|E
30|$|In the ANN models, the {{available}} database is generally {{divided into three}} subsets: training, validation and testing sets. In this study, 70  % of the 246 samples (172 randomly <b>selected</b> <b>data)</b> for training, 15  % of the total <b>data</b> (37 randomly <b>selected</b> <b>data)</b> for validation and also 15  % of the database (37 randomly <b>selected</b> <b>data)</b> for testing were used to predict the compressibility parameters.|$|R
50|$|Panopticon's {{technology}} {{relies on}} in-memory OLAP cubes, which are displayed {{through a series}} of visualizations including treemaps. This allows the user to load <b>data,</b> <b>select</b> variables and hierarchical structures, and navigate through the resultant visualization, filtering, zooming and drilling (sometimes called slicing and dicing), to identify outliers, correlations and trends.|$|R
30|$|Decision {{analyses}} {{using the}} normalized <b>data</b> at <b>selected</b> spatial scale(s).|$|R
40|$|To {{navigate}} {{and recognize}} where it is, a mobile robot {{must be able}} to identify its current location. In an unknown initial position, a robot needs to refer to its environment to determine its location in an external coordinate system. Even with a known initial position, drift in odometry causes the estimated position to deviate from the correct position, requiring correction. We show how to find landmarks without models. We use dense stereo data from our mobile robot's trinocular system to discover image regions that will be stable over widely differing viewpoints. We find image brightness "corners" in images and select those that do not straddle depth discontinuities in the stereo depth <b>data.</b> <b>Selecting</b> corners only in regions of nearly planar stereo data results in landmarks that can be seen in images taken from different viewpoints...|$|E
40|$|The {{main goal}} of my thesis {{is to create}} a bot for Unreal Tournament 2004 whose {{behaviour}} will be as similar as possible to human controlled players. At the beginning, I measured game play data from both humans and bots. Then, I chose one initial bot and I did five iterations of comparing bot data with humans <b>data,</b> <b>selecting</b> one difference, improving the bot with the aim to eliminate the difference and measuring data again. The successful improvements I made were removing rotations during running, adding reactions on hit and better weapon switching. The improvements that did not make a difference were adding a camping and roaming objective and evade using visibility module. In the end, believability of the bot was tested during a study with human players. The bot will take part in a human-like bot competition named the 2 K BotPrize...|$|E
40|$|Objectives: To compare {{treatment}} effect estimates {{obtained from}} a regression discontinuity (RD) design with results from an actual randomized controlled trial (RCT). Study Design and Setting: Data from an RCT (EVIDENT), which studied the effect of an Internet intervention on depressive symptoms measured with the Patient Health Questionnaire (PHQ- 9), were used to perform an RD analysis, in which treatment allocation was determined by a cutoff value at baseline (PHQ- 9 = 10). A linear regression model was fitted to the <b>data,</b> <b>selecting</b> participants above the cutoff who had received the intervention (n = 317) and control participants below the cutoff (n = 187). Outcome was PHQ- 9 sum score 12 weeks after baseline. Robustness of the effect estimate was studied; the estimate was compared with the RCT treatment effect. Results: The final regression model showed a regression coefficient of − 2. 29 [95...|$|E
50|$|Datawatch's {{technology}} {{relies on}} in-memory OLAP (Online Analytical Processing) cubes, which are displayed {{through a series}} of visualizations including treemaps. This allows the user to load <b>data,</b> <b>select</b> variables and hierarchical structures, and navigate through the resultant visualization, filtering, zooming and drilling (sometimes called slicing and dicing), to identify outliers, correlations and trends.|$|R
40|$|The OPTIM model {{helps to}} {{forecast}} each month {{the growth rate}} of French GDP and its main components for the coincident quarter and the quarter ahead. The model uses a wide range of monthly macroeconomic data and survey <b>data,</b> <b>selected</b> by an automatic statistical procedure. GDP forecasting, bridge model, general-to-specifi c approach (Gets). ...|$|R
5000|$|This <b>data</b> back {{imprints}} <b>selected</b> <b>data</b> {{clearly on}} the film from five recording modes: [...] (1) Year/Month/Day (2) Month/Day/Year (3) Day/Month/Year (4) Day/Hour/Minute or [...] (AM-PM/Hour/Minute) and (5) Off - no imprint. A simple push button control makes mode selection fast and easy, and the large LCD window displays the <b>selected</b> <b>data.</b>|$|R
40|$|Introduction The {{problem of}} {{estimating}} the dimensionality {{of a model}} occurs in various forms in applied statistics. There is estimating the number of factor in factor analysis, estimating the degree of a polynomial describing the <b>data,</b> <b>selecting</b> the variables to be introduced in a multiple regression equation, estimating the order of an AR or MA time series model, and so on. In factor analysis this problem was traditionally solved by eyeballing residual eigenvalues, or by applying {{some other kind of}} heuristic procedure. When maximum likelihood factor analysis became computationally feasible the likelihoods for different dimensionalities could be compared. Most statisticians were {{aware of the fact that}} comparison of successive chi squares was not optimal in any well defined decision theoretic sense. With the advent of the electronic computer the forward and backward stepwise selection procedures in multiple regression also became quite popular, but again there were plenty of e...|$|E
40|$|Introduction The recent {{popularity}} of applying machine learning methods to computational linguistics problems {{has given rise}} to a large supply of fully automated natural language processing systems. Enough systems have been created that almost every current problem is the focus of a rich set of system development efforts. The widespread proliferation of diverse systems that all address a single problem is not new. Even problems that are not current, and are considered "solved" have an array of off-the-shelf products or downloadable code that implement solutions. The systems addressing a single problem may differ in at least three significant ways. They may use different features to describe the space of input phenomena they observe, different learning methods, or different training <b>data.</b> <b>Selecting</b> and working out the details of combining the values for these three parameters is the human's {{part of the process of}} applying a machine learning method to natural language processing...|$|E
40|$|Privatization of data is an {{important}} technique {{that has been used}} by compilers to parallelize loops by eliminating storage-related dependences. When a compiler partitions computations based on the ownership of <b>data,</b> <b>selecting</b> a proper mapping of privatizable data is crucial to obtaining the benefits of privatization. This paper presents a novel framework for privatizing scalar and array variables {{in the context of a}} data-driven approach to parallelization. We show that there are numerous alternatives available for mapping privatized variables and the choice of mapping can significantly affect the performance of the program. We present an algorithm that attempts to preserve parallelism and minimize communication overheads. We also introduce the concept of partial privatization of arrays that combines data partitioning and privatization, and enables efficient handling of a class of codes with multidimensional data distribution that was not previously possible. Finally, we show how the i [...] ...|$|E
30|$|High-latitude three {{component}} vector <b>data</b> are <b>selected</b> at all local times.|$|R
40|$|A {{combination}} is presented of the inclusive diffractive cross section measurements {{made by the}} H 1 and ZEUS Collaborations at HERA. The analysis uses diffractive deep inelastic scattering <b>data</b> <b>selected</b> by means of proton spectrometers. Correlations of systematic uncertainties {{are taken into account}} by the combination method, resulting in improved precision. PoS(EPS-HEP 2011) 46...|$|R
40|$|The {{calibration}} data obtained {{during the fall}} 1978 Nimbus-G underflight mission with the scanning multichannel microwave radiometer (SMMR) simulator on board the NASA CV- 990 aircraft were analyzed and an interim calibration algorithm was developed. <b>Data</b> <b>selected</b> for the analysis consisted of in flight sky, first-year sea ice, and open water observations, as well as ground based observations of fixed targets with varied temperatures of selected instrument components. For most of the SMMR channels, a good fit to the <b>selected</b> <b>data</b> set was obtained with the algorithm...|$|R
