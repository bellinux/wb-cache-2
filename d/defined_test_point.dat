0|6377|Public
30|$|Without loss of generality, {{we assume}} that the MSs are {{homogeneously}} distributed in the area of interest (AOI), which yields a uniform distribution on the traffic demand in the AOI. We define that a site is covered under the C-RAN architecture if the received radio signal strength at the site is above a given threshold level (receiver sensitivity). Small-scale fading is not explicitly included in the system model since a long-term planning and design is targeted. To test the mobile coverage, we <b>define</b> <b>test</b> <b>points</b> (TPs) within the AOI. The TPs are also used for testing the received power intensity for MSs from the associated RRU. In Fig.  1, the locations of TPs and PSs for both OADM and RRU are also illustrated.|$|R
40|$|The primary {{objective}} {{for this work}} {{is to create a}} method to rapidly optimize interior permanent magnet (IPM) machines. For this research, a method was created called dimensionless optimization (DO). Eight dimensionless parameters determine nineteen dimensions of the machine based on design rules, stresses in the rotor and back EMF. Given a maximum torque specification and a local design space, a hypercube design of 64 machines is created using an 8 factor I-optimal design. Analysis is performed using a commercial program which incorporates electromagnetic FEA, and provides the current and phase angle requirements, and efficiency of the machines at the <b>defined</b> <b>test</b> <b>points.</b> Response surfaces are created for efficiency and current at each <b>test</b> <b>point.</b> The method determines the variance of the response surfaces, the optimum predicted design in the local design space, and the location of design spaces which may yield better designs. The method was validated by comparing 20 kW and 50 kW machines designed by an electric machines manufacturer, to machines designed using dimensionless optimization. The efficiencies of both machines were improved using dimensionless optimization...|$|R
40|$|Mean {{velocity}} (axial component) and turbulent velocity (axial component) {{measurements for}} thirty one selected flow conditions of six models were performed employing the Laser Doppler Velocimeter Aerodynamic conditions which <b>define</b> the <b>test</b> <b>points</b> are given. Tabulations which explain {{the scope of}} mean velocity traverses and turbulence histogram measurements are also presented. The actual LV position, the type of traverse, and measured mean and turbulent velocities along copies of the LV mean velocity traces are contained...|$|R
40|$|Carbopol {{is one of}} {{the most}} common {{thickening}} agent for water phases. It is used after neutralisation and its rheological properties in the aqueous medium are well known. The aim of this work was to investigate the gelation properties of Carbopol 971 e 974 polymeric systems in water-miscible cosolvents such as glycerine and PEG 400. Since in these cosolvents, carboxypolymethylene precipitates after neutralisation with a base, then the attention was pointed out of the gelation properties of the different systems at increasing temperature, in order to obtain Carbopols gels avoiding neutralisation and, at the same time, making possible the dissolution in these gels of insoluble or poorly soluble water drugs. Rheological properties of PEG 400 and glycerine samples were compared with similar systems in water by performing oscillatory analyses and measuring the main rheological parameters, G, G and δ. The results obtained showed that Carbopol 971 and 974 in PEG 400 gave rise after heating to gels that show a satisfactory rheological behaviour. The elastic modulus is greater than the viscous one showing a remarkable elastic character of these samples and the performed frequency sweeps show a typical spectrum of a “gel-like” structure. Being Carbopols well-known mucoadhesive polymers, gels adhesive properties were studied using the ex vivo method. Then, the possible cutaneous irritation were also tested using the in vivo method (Draize test). No signs of cutaneous irritation and good mucoadhesive properties were obtained for the PEG 400 and water gels of Carbopol 974 prepared by heating. After rheological and mucoadhesive properties were set, paracetamol as a model drug was then inserted in the composition of the gels and the release characteristics were <b>defined.</b> Dissolution <b>tests</b> <b>pointed</b> out the greater release control properties of PEG 400 -Carbopol 971 samples. These studies showed PEG 400 -Carbopol systems as a first-rate alternative to traditional water gels...|$|R
40|$|Abstract—An {{efficient}} {{method to}} select an optimum set of <b>test</b> <b>points</b> for dictionary techniques in analog fault diagnosis is proposed. This {{is done by}} searching for the minimum of the entropy index based on the available <b>test</b> <b>points.</b> First, the two-dimensional integer-coded dictionary is constructed whose entries are measurements associated with faults and <b>test</b> <b>points.</b> The problem of optimum <b>test</b> <b>points</b> selection is, thus, transformed to {{the selection of the}} columns that isolate the rows of the dictionary. Then, the likelihood for a column to be chosen based on the size of its ambiguity set is evaluated using the minimum entropy index of <b>test</b> <b>points.</b> Finally, the <b>test</b> <b>point</b> with the minimum entropy index is selected to construct the optimum set of <b>test</b> <b>points.</b> The proposed entropy-based method to select a local minimum set of <b>test</b> <b>points</b> is polynomial bounded in computational cost. The comparison between the proposed method and other reported <b>test</b> <b>points</b> selection methods is carried out by statistical experiments. The results indicate that the proposed method more efficiently and more accurately finds the locally optimum set of <b>test</b> <b>points</b> and is practical for large scale analog systems. Index Terms—Analog fault diagnosis, fault dictionary, rough set, <b>test</b> <b>point.</b> I...|$|R
40|$|Abstract—Test pointsprovide additionalcontrol todesignlogic and {{can improve}} circuit testability. Traditionally, <b>test</b> <b>points</b> are {{activated}} by a global test enable signal, and routing {{the signal to}} the <b>test</b> <b>points</b> can be costly. To address this problem, we propose a new <b>test</b> <b>point</b> structure that utilizes controllability don’t-cares to generate local <b>test</b> <b>point</b> activation signals. To support the structure, we propose new methods for extracting don’t-cares from assertions and finite state machines in the design. Our empirical evaluation shows that don’t-cares exist in many designs {{and can be used}} for reducing <b>test</b> <b>point</b> overhead. I...|$|R
40|$|By {{simplifying}} tolerance {{problem and}} treating faulty voltages on different <b>test</b> <b>points</b> as independent variables, integer-coded table technique is proposed {{to simplify the}} <b>test</b> <b>point</b> selection process. Usually, simplifying tolerance problem may induce a wrong solution while the independence assumption will result in over conservative result. To address these problems, the tolerance problem is thoroughly considered in this paper, and dependency relationship between different <b>test</b> <b>points</b> is considered at the same time. A heuristic graph search method is proposed to facilitate the <b>test</b> <b>point</b> selection process. First, the information theoretic concept of entropy is {{used to evaluate the}} optimality of <b>test</b> <b>point.</b> The entropy is calculated by using the ambiguous sets and faulty voltage distribution, determined by component tolerance. Second, the selected optimal <b>test</b> <b>point</b> is used to expand current graph node by using dependence relationship between the <b>test</b> <b>point</b> and graph node. Simulated results indicate that the proposed method more accurately finds the optimal set of <b>test</b> <b>points</b> than other methods; therefore, it is a good solution to minimize the size of the <b>test</b> <b>point</b> set. To simplify and clarify the proposed method, only catastrophic and some specific parametric faults are discussed in this paper...|$|R
40|$|Gels dosage {{forms are}} {{successfully}} used as drug delivery systems considering {{their ability to}} control drug release and to protect medicaments from an hostile environment. This study deals with the gelation properties of Carbopol 971 e 974 polymeric systems in tetraglycol, a water-miscible cosolvent. In this paper, the attention was noted of the thickening properties of the different Carbopol in tetraglycol solvent at increasing temperatures, {{in order to obtain}} gels avoiding neutralisation and, at the same time, to make possible the dissolution in these gels of insoluble or poorly soluble water drugs. Samples were prepared by simply dispersing different Carbopols amount (0. 5 - 4 %) were added to tetraglycol and different systems were prepared at room temperature and by heating at 70 °C. All these systems were then characterised rheologically. Frequency sweep, creep-recovery, temperature sweep and time sweep analyses outlined that Carbopol 971 and 974 in tetraglycol gave rise after heating to gels with satisfactory rheological behaviour: the elastic modulus was greater than the viscous one and a remarkable elastic character was found to be present. Systems obtained by heating procedure were examined also from a mechanical point of view using a texture profile analysis. Besides, being Carbopols well known mucoadhesive polymers, gels adhesive properties were also studied using the ex vivo method. Texture and adhesion characterisation confirmed rheological results pointing out a certain greater elasticity and adhesiveness of Carbopol 974 systems. Then, the possible cutaneous irritation was also tested using the in vivo method (Draize test). No signs of cutaneous irritation were obtained for all the samples that were analysed. After rheological and mucoadhesive properties were set, paracetamol as a model drug was inserted in the composition of the gels and the release characteristics were <b>defined.</b> Dissolution <b>tests</b> <b>pointed</b> out the greater release control properties of tetraglycol/Carbopol 971 samples. These studies showed tetraglycol/Carbopol systems as a first-rate alternative to traditional water gels when low water-soluble drugs have to be added...|$|R
40|$|Recently, Pomeranz and Reddy [7], {{presented}} a <b>test</b> <b>point</b> insertion method to improve path delay fault testability in large combinational circuits. A test application scheme was developed that allows <b>test</b> <b>points</b> to be utilized as primary inputs and primary outputs during testing. The placement of <b>test</b> <b>points</b> {{was guided by}} the number of paths and was aimed at reducing this number. Indirectly, this approach achieved complete robust path delay fault testability in very low computation times. In this paper, we use their test application scheme, however, we use more exact measures for guiding <b>test</b> <b>point</b> insertion like <b>test</b> generation and RD fault identification. Thus, we reduce the number of <b>test</b> <b>points</b> needed to achieve complete testability by ensuring that <b>test</b> <b>points</b> are inserted only on paths associated with path delay faults that are necessary to be tested and that are not robustly testable. Experimental results show that an average reduction of about 70 % in the number of <b>test</b> <b>points</b> ov [...] ...|$|R
3000|$|... are the ‘n’ <b>test</b> <b>points</b> [33]. The {{number of}} <b>test</b> <b>points</b> ‘n’, in each {{iteration}} step, determines {{the rate of}} convergence of the algorithm.|$|R
40|$|EP 0636895 (A 1) The present {{invention}} {{relates to}} a test grid for an unpopulated printed circuit, comprising <b>test</b> <b>points</b> (T) linked to measurement circuits. Each measurement circuit {{is linked to}} a group (G) of <b>test</b> <b>points,</b> the <b>test</b> <b>points</b> of each group being linked together by resistors (r) of non-zero values...|$|R
40|$|This paper {{presents}} an approach for embedding of test concepts into high-level synthesis. Symbolic <b>test</b> <b>points</b> declared by a test concept specification are mapped on physical registers. This {{is done by}} an assignment of <b>test</b> <b>points</b> to symbolic registers and by a usual register binding tool. The experiments show that this way of integration does not result in larger designs. 1 Introduction The use of built-in self test methods or partial scan paths requires the selection of <b>test</b> <b>points.</b> The selection of <b>test</b> <b>points</b> is a problem which is mostly considered after {{the design of the}} RT-structure. But it can be very difficult or even impossible to find a configuration of <b>test</b> <b>points,</b> which can realize a given test concept. In that cases, where it is impossible to find a configuration of <b>test</b> <b>points,</b> new registers are inserted in the design. These new registers are only used as <b>test</b> <b>points.</b> This paper {{presents an}} approach for embedding of test concepts using the design space. The test concep [...] ...|$|R
40|$|This paper {{presents}} an experimental investigation {{on the impact}} of <b>test</b> <b>point</b> insertion on circuit size and performance. Often <b>test</b> <b>points</b> are inserted into a circuit in order to improve the circuit’s testability, which results in smaller test data volume, shorter test time, and higher fault coverage. Inserting <b>test</b> <b>points</b> however requires additional silicon area and influences the timing of a circuit. The paper shows how placement and routing is affected by <b>test</b> <b>point</b> insertion during layout generation. Experimental data for industrial circuits show that inserting 1 % <b>test</b> <b>points</b> in general increases the silicon area after layout by less than 0. 5 % while the performance of the circuit may be reduced by 5 % or more. 1...|$|R
40|$|We {{propose a}} new {{algorithm}} for <b>test</b> <b>point</b> selection for scan-based BIST. The new algorithm combines {{the advantages of}} both explicit-testability-calculation and gradient techniques. The <b>test</b> <b>point</b> selection is guided bya cost function which is partially based on explicit testability recalculation and partially on gradients. With an event-driven mechanism, it can quickly identify a set of nodes whose testability need to be recalculated due to a <b>test</b> <b>point,</b> and then use gradients to estimate {{the impact of the}} rest of the circuit. In addition, by incorporating timing information into the cost function, timing penalty caused by <b>test</b> <b>points</b> can be easily avoided. We present the results to illustrate that high fault coverages for both area- and timing-driven <b>test</b> <b>point</b> insertions can be obtained with a small number of <b>test</b> <b>points.</b> The results also indicate a signi#cant reduction of computational complexity while the qualities are similar to the explicitly-testability-calculation method. 1 I [...] ...|$|R
3000|$|Because {{the optimal}} plan is not unique, {{to obtain a}} {{determined}} plan, one should restrict the arrangement mode of the stress level combinations (called <b>test</b> <b>points)</b> in the feasible region of the test (called test region), and restrict the sample location ratio on <b>test</b> <b>points.</b> Escobar and Meeker [44] proposed a method of obtaining the optimal non-degenerated plan (called splitting plan): find the <b>test</b> <b>point</b> ξ [...]...|$|R
5000|$|Pattern generator, which transmits a <b>defined</b> <b>test</b> {{pattern to}} the DUT or test system ...|$|R
40|$|This {{research}} {{was designed to}} determine a small set of low-beam <b>test</b> <b>points</b> for recommendation as common <b>test</b> <b>points</b> throughout the world. Our recommendation is a compromise among the following three set of inputs: (1) expert opinion, based on a worldwide survey of 119 experts in lighting and vision, (2) current practice, based on an analysis of candela matrices of 150 production low beams, and (3) scientific evidence concerning visibility and glare under nighttime driving conditions. Expert opinion and scientific evidence did not fully converge on the same <b>test</b> <b>points,</b> with the main difference being {{in the amount of}} light recommended for points at which objects need to be seen. While experts suggested light levels comparable to current production outputs, the recommendations based exclusively on scientific evidence would call for light levels of more than ten times the current levels. Therefore, the <b>test</b> <b>points</b> based exclusively on scientific evidence should be viewed only as ideal <b>test</b> <b>points,</b> but we should aim in the future to explore technologies that would make approximations to these <b>test</b> <b>points</b> feasible...|$|R
5000|$|After {{moving the}} <b>test</b> <b>points,</b> the linear {{equation}} part is repeated, {{getting a new}} polynomial, and Newton's method is used again to move the <b>test</b> <b>points</b> again. This sequence is continued until the result converges to the desired accuracy. The algorithm converges very rapidly.Convergence is quadratic for well-behaved functions—if the <b>test</b> <b>points</b> are within [...] of the correct result, they will be approximately within [...] of the correct result after the next round.|$|R
40|$|The <b>test</b> <b>point</b> {{insertion}} {{problem is}} that of select-ing t nodes in a combinational network as candi-dates for inserting observable <b>test</b> <b>points,</b> so as to minimize the number of test vectors needed to detect all single stuck-at faults in the network. In this paper we describe a dynamic programming approach to selecting the <b>test</b> <b>points</b> and provide an algorithm that inserts the <b>test</b> <b>points</b> optimally for fanout-free networks. We further extend this algo-rithm to general combinational networks with recon-vergent fanout. We also analyze the time complex-ity of the algorithm and show that it runs in O(n. t) time, where n {{is the size of}} the network and t is the number of <b>test</b> <b>points</b> to be inserted. As a side result we show that we can compute minimal test sets for a restricted class of networks that includes fanout. This extends previous results which were limited to fanout-free networks. 1...|$|R
5000|$|This {{intuitive}} {{approach can}} be made quantitative by defining the normalized distance between the <b>test</b> <b>point</b> and the set to be [...] By plugging this into the normal distribution we can derive the probability of the <b>test</b> <b>point</b> belonging to the set.|$|R
50|$|The ITU-T Y.156sam <b>defines</b> <b>test</b> streams with service {{attributes}} {{linked to the}} Metro Ethernet Forum (MEF) 10.2 definitions.|$|R
40|$|Golden section search {{strategies}} (GSSS), dichotomous search strategies (DSS), and Z-score strategies (ZSS) {{are simple}} and robust computerized adaptive testing strategies. GSSS, DSS, and {{one version of}} ZSS are similar in that statistical hypothesis testing occurs at each successive <b>testing</b> <b>point</b> in determining the current ability estimates. After each item is administered, the examinee 2 ̆ 7 s obtained score is compared with the expected score at successive <b>testing</b> <b>points.</b> If the examinee 2 ̆ 7 s obtained score does not exceed a confidence interval of an expected score at a <b>testing</b> <b>point,</b> the examinee 2 ̆ 7 s current ability estimate {{is assumed to be}} equal to that of the <b>testing</b> <b>point.</b> Otherwise, a hypothesis testing will be conducted at the next <b>testing</b> <b>point</b> and the process is continued until the examinee 2 ̆ 7 s current ability estimate is determined and the next item is selected. The three strategies differ in the successive <b>testing</b> <b>points</b> 2 ̆ 7 allocation. Each middle point of successive golden search regions is a <b>testing</b> <b>point</b> in GSSS; each middle point of successive dichotomous search regions is a <b>testing</b> <b>point</b> in DSS; each Z-score estimate evaluated at the previous <b>testing</b> <b>point</b> is the next <b>testing</b> <b>point</b> in ZSS. No hypothesis testing is involved in another version of ZSS in which the current ability estimate is the Z-score estimate evaluated at the previous ability estimate. Results of Monte Carlo studies in three hypothetical item pools and one SAT Verbal item pool showed that GSSS, DSS, and ZSS were computationally efficient, precise throughout the ability continuum, and robust against aberrant responses. Optimal measurement occurs using moderate size confidence intervals. Both versions of ZSS measured well under general conditions. GSSS, DSS, and ZSS provided more accurate and efficient ability estimates than did maximum likelihood estimate strategies (MLES) whenever guessing effect existed. GSSS, DSS, and ZSS were more efficient but not more accurate than MLES whenever guessing was not a factor. For GSSS, DSS, and ZSS, there were no differences in measurement accuracy, but occasional differences in measurement efficiency. GSSS, DSS and ZSS were more robust against guessing and inaccuracy of item parameters and took less time to execute than did MLES...|$|R
40|$|This paper {{presents}} a procedure for inserting <b>test</b> <b>points</b> at the outputs of scan {{elements of a}} full-scan circuit {{in such a manner}} that the peak power during scan testing is kept below a specified limit while maintaining the original fault coverage. If the power in a clock cycle during scan testing excee& a specified limit (which depen& on the peak power the chip has been designed to supply), a "peak power violation" is said to occur. Given a set of vectors, simulation is used to identify the cycles in which peak power violations occur (called "violating cycles"). For each violating cycle, the reduction in power caused by a control-O and control- 1 <b>test</b> <b>point</b> at each scan element is determined by simulation. The optimization problem then is to select as few <b>test</b> <b>points</b> as possible to eliminate all violating cycles. We present a heuristic procedure for minimizing the number of <b>test</b> <b>points</b> using integer linear programming techniques. The <b>test</b> <b>points</b> are activated and deactivated in a manner such that there is neither any loss in fault coverage nor peak power violations in the capture cycle. Experimental results indicate that the proposed procedure is very effective in controlling peak power during scan testing using a small number of <b>test</b> <b>points...</b>|$|R
50|$|Gray-box testers {{require both}} {{high-level}} and detailed documents describing the application, which they collect {{in order to}} <b>define</b> <b>test</b> cases.|$|R
5000|$|A {{technique}} {{has been described}} which opens up windows in the solder mask to create <b>test</b> <b>points</b> located directly on PCB tracks. This technique uses a conductive rubber tipped probe to contact the <b>test</b> <b>point</b> which could have a conductive Hot Air Solder Levelling (HASL) finish.|$|R
40|$|A {{method of}} testing the {{electrical}} functionality of an optically controlled switch in a reconfigurable antenna is provided. The method includes configuring one or more conductive paths between one or more feed points and one or more <b>test</b> <b>point</b> with switches in the reconfigurable antenna. Applying one or more test signals to {{the one or more}} feed points. Monitoring the one or more <b>test</b> <b>points</b> in response to the one or more test signals and determining the functionality of the switch based upon the monitoring of the one or more <b>test</b> <b>points...</b>|$|R
40|$|The set of test {{patterns}} {{applied to}} a circuit during built-in self-test (BIST) may not provide sufficiently high fault coverage due {{to the presence of}} hard-to-detect faults. This paper presents an innovative method for inserting <b>test</b> <b>points</b> in a way that complete (100 %) single stuck-at fault coverage is obtained for a specified set of test patterns. A very different approach is taken compared with previous <b>test</b> <b>point</b> insertion methods. Instead of using probabilistic techniques for <b>test</b> <b>point</b> placement, a path tracing procedure is used to place both control and observation points...|$|R
40|$|Abstract—This paper {{presents}} a novel <b>test</b> <b>point</b> insertion method for pseudorandom built-in self-test (BIST) {{to reduce the}} area overhead. The proposed method replaces dedicated flip-flops for driving control points by existing functional flip-flops. For each control point, candidate functional flip-flops are identified by using logic cone analysis that investigates the path inversion parity, logical distance, and reconvergence from each control point. Four types of new control point structures are introduced based on the logic cone analysis results to avoid degrading the testability. Experimental {{results indicate that the}} proposed method significantly reduces <b>test</b> <b>point</b> area overhead by replacing the dedicated flip-flops and achieves essentially the same fault coverage as conventional <b>test</b> <b>point</b> implementations using dedicated flip-flops driving the control points. Index Terms—Dedicated flip-flop, functional flip-flop, logic cone analysis, <b>test</b> <b>point</b> insertion. Ç...|$|R
40|$|AbstractWe {{consider}} {{the problem of}} verifying a simple polygon in the plane using “test points”. A <b>test</b> <b>point</b> is a geometric probe that takes as input a point in Euclidean space, and returns “+” if the point is inside the object being probed or “−” if it is outside. A verification procedure takes as input {{a description of a}} target object, including its location and orientation, and it produces a set of <b>test</b> <b>points</b> that are used to verify whether a test object matches the description. We give a procedure for verifying an n-sided, non-degenerate, simple target polygon using 5 n <b>test</b> <b>points.</b> This <b>testing</b> strategy works even if the test polygon has n + 1 vertices, and we show a lower bound of 3 n + 1 <b>test</b> <b>points</b> for this case. We also give algorithms using O(n) <b>test</b> <b>points</b> for simple polygons that may be degenerate and for test polygons that may have up to n + 2 vertices. All of these algorithms work for polygons with holes. We also discuss extensions of our results to higher dimensions...|$|R
40|$|In this paper, an {{automatic}} test pattern generator (ATPG) -based scan-path <b>test</b> <b>point</b> insertion technique, which can achieve high delay fault coverage for scan designs, is proposed. In the proposed technique, shift dependency between adjacent scan flip-flops that causes some delay faults to be untestable in standard scan environment, is broken by inserting <b>test</b> <b>points,</b> {{which can be}} combinational gates as well as flip-flops. Instead of topology-based approaches used in prior publications, the proposed technique uses a special ATPG to identify pairs of adjacent scan flip-flops between which <b>test</b> <b>points</b> are inserted to improve fault coverage. Since the proposed technique inserts <b>test</b> <b>points</b> only where they are necessary, it can drastically reduce hardware overhead compared to circuit topology-based techniques. 100 % transition delay coverage was attained for all ISCAS 89 benchmark circuits except one. This is achieved with very small numbers of <b>test</b> <b>points.</b> On an average, about 40 % reduction in scan chain length against a prior approach was achieved by the proposed method for benchmark circuits with default scan chain order...|$|R
40|$|While {{previous}} {{research has focused on}} deterministic testing of bridging faults, this paper studies pseudo-random testing of bridging faults and describes a means for achieving high fault coverage in a built-in self-test (BIST) environment. Bridging faults are generally more random pattern testable than shown to illustrate tha less random pattem tes method for identifying these random-pattem-resistant bridging faults is described. State-of-the-art <b>test</b> <b>point</b> insertion techniques, are based on the stuck-at fault model, are inadequa ta is presented which indicates that even after inserting <b>test</b> <b>points</b> that result in 100 % single stuck-ut fault coverage, many bridging faults are still not detected. A <b>test</b> <b>point</b> insertion procedure that targets both single stuck-at faults and non-feedback bridging faults is presented. It is shown that by considering both types of faults when selecting the location for <b>test</b> <b>points,</b> higher fault coverage can be obtained with little or no increase in overhead. Thus, the <b>test</b> <b>point</b> insertion procedure described here is a low-cost way {{to improve the quality of}} built-in self-test. 1...|$|R
50|$|Cucumber {{command line}} {{can be used to}} quickly run <b>defined</b> <b>tests.</b> It also {{supports}} running a subset of scenarios by filtering tags.|$|R
30|$|Our cube {{determination}} {{experiment was}} conducted in room 517, with <b>test</b> <b>points</b> 1 to 12. In each <b>test</b> <b>point,</b> the target broadcasts a cube determination request every six seconds and about 300 requests are sent in total. Note that anchors in neighboring places (e.g., rooms) do not reply to these requests.|$|R
5000|$|Note {{that the}} error graph does indeed {{take on the}} values [...] at the six <b>test</b> <b>points,</b> {{including}} the end points, but that those points are not extrema. If the four interior <b>test</b> <b>points</b> had been extrema (that is, the function P(x)f(x) had maxima or minima there), the polynomial would be optimal.|$|R
40|$|Noise {{test has}} been done for {{a certain type of}} diesel {{locomotive}} under different working conditions for outfield and cab, and analyzed the noise characteristic of each <b>test</b> <b>point.</b> Through the analysis of the data :The sound pressure level of point D, E, I, J of 12 <b>test</b> <b>points</b> of outfield <b>test</b> is bigger due to the effect of noise source; The changing of working condition of locomotive mainly makes <b>test</b> <b>points</b> of the outfield produce big change in the high frequency after 400 Hz; The opening of cooling fan makes sound pressure level of <b>test</b> <b>points</b> of outfield improved obviously; The noise of cab is mainly produced by combined action of vibration of locomotive wall and external noise source: External noise source influences the whole frequency band of the cab, vibration makes individual frequency have big increased...|$|R
5000|$|Nelder-Mead in n {{dimensions}} {{maintains a}} set of n+1 <b>test</b> <b>points</b> arranged as a simplex. It then extrapolates {{the behavior of the}} objective function measured at each <b>test</b> <b>point,</b> in order to find a new <b>test</b> <b>point</b> and to replace one of the old <b>test</b> <b>points</b> with the new one, and so the technique progresses. The simplest approach is to replace the worst point with a point reflected through the centroid of the remaining n points. If this point is better than the best current point, then we can try stretching exponentially out along this line. On the other hand, if this new point isn't much better than the previous value, then we are stepping across a valley, so we shrink the simplex towards a better point. An intuitive explanation of the algorithm is presented in ...|$|R
