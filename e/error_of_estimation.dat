194|10000|Public
25|$|The {{development}} of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the probability density for the errors and define a method of estimation that minimizes the <b>error</b> <b>of</b> <b>estimation.</b> For this purpose, Laplace used a symmetric two-sided exponential distribution we now call Laplace distribution to model the error distribution, and used the sum of absolute deviation as <b>error</b> <b>of</b> <b>estimation.</b> He felt these to be the simplest assumptions he could make, and {{he had hoped to}} obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.|$|E
25|$|In 1809 Carl Friedrich Gauss {{published}} his method of calculating {{the orbits of}} celestial bodies. In that work {{he claimed to have}} been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimizes the <b>error</b> <b>of</b> <b>estimation.</b> Gauss showed that arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.|$|E
500|$|Even before IQ {{tests were}} invented, there were {{attempts}} to classify people into intelligence categories by observing their behavior in daily life. Those {{other forms of}} behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of [...] "intelligence" [...] used in a particular case and on the reliability and <b>error</b> <b>of</b> <b>estimation</b> in the classification procedure.|$|E
30|$|The <b>error</b> <b>of</b> such <b>estimation</b> is {{bounded by}} Theorem 3, {{which is very}} small in practice.|$|R
40|$|This paper modifies the {{standard}} Hodrick Prescot Filter {{in order to}} reduce the problem of misleading predictive outcome when used with updated information. The modification allow for a more accurate <b>estimation</b> <b>of</b> output gap, as well as the introduction of confidence intervals that permit a better understanding of the uncertainty related to the <b>estimation</b> <b>of</b> the filter. Also improve the efficiency using a correction for autocorrelation in the <b>errors</b> <b>of</b> <b>estimation.</b> ...|$|R
40|$|The {{main purpose}} of {{microarray}} studies is screening to identify differentially expressed genes as candidates for further investigation. Because of limited resources in this stage, prioritizing or ranking genes is a relevant statistical task in microarray studies. In this article, we develop 3 empirical Bayes methods for gene ranking {{on the basis of}} differential expression, using hierarchical mixture models. These methods are based on (i) minimizing mean squared <b>errors</b> <b>of</b> <b>estimation</b> for parameters, (ii) minimizing mean squared <b>errors</b> <b>of</b> <b>estimation</b> for ranks <b>of</b> parameters, and (iii) maximizing sensitivity in selecting prespecified numbers of differential genes, with the largest effect. Our methods incorporate the mixture structures of differential and nondifferential components in empirical Bayes models to allow information borrowing across differential genes, with separation from nuisance, nondifferential genes. The accuracy of our ranking methods is compared with that of conventional methods through simulation studies. An application to a clinical study for breast cancer is provided...|$|R
500|$|As {{long ago}} as 1937, Lewis Terman pointed out that <b>error</b> <b>of</b> <b>estimation</b> in IQ scoring {{increases}} as IQ score increases, {{so that there is}} less and less certainty about assigning a test-taker to one band of scores or another as one looks at higher bands. Current IQ tests also have large error bands for high IQ scores. As an underlying reality, such distinctions as those between [...] "exceptionally gifted" [...] and [...] "profoundly gifted" [...] have never been well established. All longitudinal studies of IQ have shown that test-takers can bounce up and down in score, and thus switch up and down in rank order as compared to one another, over the course of childhood. Some test-givers claim that IQ classification categories such as [...] "profoundly gifted" [...] are meaningful, but those are based on the obsolete Stanford–Binet Third Revision (Form L-M) test. The highest reported standard score for most IQ tests is IQ 160, approximately the 99.997th percentile (leaving aside the issue of the considerable error in measurement at that level of IQ on any IQ test). IQ scores above this level are dubious as there are insufficient normative cases upon which to base a statistically justified rank-ordering. Moreover, there has never been any validation of the Stanford–Binet L-M on adult populations, and there is no trace of such terminology in the writings of Lewis Terman. Although two current tests attempt to provide [...] "extended norms" [...] that allow for classification of different levels of giftedness, those norms are not based on well validated data.|$|E
2500|$|... is {{the sample}} {{variance}} estimate of [...] That is, the <b>error</b> <b>of</b> <b>estimation</b> of [...] is (stochastically) of order [...]|$|E
2500|$|The {{standard}} <b>error</b> <b>of</b> <b>estimation</b> (SE) is {{the reciprocal}} {{of the test}} information of at a given trait level, is the ...|$|E
40|$|The {{exponential}} distribution and the Erlang distribution function are {{been used in}} numerous areas of mathematics, and specifically in the queueing theory. Such and similar appli-cations emphasize the importance <b>of</b> <b>estimation</b> <b>of</b> <b>error</b> <b>of</b> approximation by the Erlang distribution function. The article gives an analysis and technique <b>of</b> <b>error’s</b> <b>estimation</b> <b>of</b> an accuracy of such approximation, especially in some specific cases...|$|R
40|$|Computation <b>of</b> <b>error</b> in <b>estimation</b> <b>of</b> {{nonlinearity}} in ADC using histogram test {{are reported}} in this paper. <b>Error</b> determination in <b>estimation</b> <b>of</b> Differential Nonlinearity (DNL) and Integral Nonlinearity (INL) of an ADC is done by taking deviation of estimated value from actual value. Error in estimated INL and DNL is determined to check the usefulness of basic histogram test algorithm. Arbitrary error is introduced in ideal simulated ADC transfer characteristics and full scale simulated sine wave is applied to ADC for computation <b>of</b> <b>error</b> in <b>estimation</b> <b>of</b> transition levels and nonlinearity. Simulation results for 5 and 8 bit ADC are pre-sented which show effectiveness of the proposed method...|$|R
40|$|An {{important}} limitation In {{the study}} of tree roots has traditionally been the time necessary to excavate roots without injuring their fine structure. Thus, many root studies {{have been based on}} small samples and consequently contain large <b>errors</b> <b>of</b> <b>estimation.</b> This Note describes how large numbers of root systems from sapling-sized trees can be excavated rapidly and with minimum injury by placing patterned charges of high explosive in the soil. " [...] P. [1]...|$|R
2500|$|Historically, {{even before}} IQ tests were invented, there were {{attempts}} to classify people into intelligence categories by observing their behavior in daily life. Those {{other forms of}} behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of [...] "intelligence" [...] used in a particular case and on the reliability and <b>error</b> <b>of</b> <b>estimation</b> in the classification procedure.|$|E
5000|$|Then in {{the worst}} case the {{expected}} value of <b>error</b> <b>of</b> <b>estimation</b> is bound from below, ...|$|E
5000|$|... is {{the sample}} {{variance}} estimate of [...] That is, the <b>error</b> <b>of</b> <b>estimation</b> of [...] is (stochastically) of order [...]|$|E
40|$|Abstract. The paper {{proposes a}} new method for a target’s trajectory, {{assumed to be}} linear and uniform, based on the {{observation}} of its speed and bearings. After introducing the new method based on assumed model, the paper analyzes the relative <b>error</b> <b>of</b> range <b>estimation</b> caused by speed and bearings estimation’s relative error; The results of target motion analysis (TMA) is optimized by linearizing the model and using kalman filter. Pond test shows that relative <b>error</b> <b>of</b> range <b>estimation</b> calculated by this method is less than 1...|$|R
3000|$|... [...]. The {{distance}} consistency {{property of}} valid locators {{states that the}} mean square <b>error</b> <b>of</b> the location <b>estimation</b> based on the correct distance measurements is lower than a small threshold while the mean square <b>error</b> <b>of</b> the location <b>estimation</b> based on the distance measurements which contains some incorrect ones is not lower than the threshold.|$|R
30|$|Lets {{consider}} the gain <b>error</b> <b>estimation</b> <b>of</b> the conventional method.|$|R
5000|$|The {{standard}} <b>error</b> <b>of</b> <b>estimation</b> (SE) is {{the reciprocal}} {{of the test}} information of at a given trait level, is the ...|$|E
50|$|The {{development}} of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the probability density for the errors and define a method of estimation that minimizes the <b>error</b> <b>of</b> <b>estimation.</b> For this purpose, Laplace used a symmetric two-sided exponential distribution we now call Laplace distribution to model the error distribution, and used the sum of absolute deviation as <b>error</b> <b>of</b> <b>estimation.</b> He felt these to be the simplest assumptions he could make, and {{he had hoped to}} obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.|$|E
5000|$|Even before IQ {{tests were}} invented, there were {{attempts}} to classify people into intelligence categories by observing their behavior in daily life. Those {{other forms of}} behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of [...] "intelligence" [...] used in a particular case and on the reliability and <b>error</b> <b>of</b> <b>estimation</b> in the classification procedure.|$|E
40|$|In this paper, the {{statistical}} <b>estimation</b> <b>of</b> pulse transfer function of linear system is entered. The behaviour <b>of</b> an <b>error</b> estimate is studied. Conditions at {{which takes place}} asymptotic normality of finite dimensional distributions <b>of</b> the normalized <b>error</b> <b>of</b> an <b>estimation</b> <b>of</b> pulse transfer function are established...|$|R
40|$|Extended Kalman filter {{for target}} {{tracking}} due to Pulse-Doppler radar measurements is often divergent, {{which is caused}} by the presence <b>of</b> unaccounted <b>errors</b> which appear in the result of the equation of the range rate measurement linearization. In the article a simple way of the extended Kalman filter divergence elimination is suggested and consists in the variance <b>of</b> value measurement <b>error</b> increase by an amount proportional to the value extrapolation error variance. The coefficient of proportionality is determined by means of statistical modeling by testing the statistical hypothesis of Gaussian distribution <b>of</b> the <b>errors</b> <b>of</b> <b>estimation</b> <b>of</b> the target?s parameters vector. The suggested method analysis is carried out as an example of target tracking due to Pulse-Doppler radar measurements in the Cartesian coordinates using the statistical modeling. ??????????? ?????? ??????? ??? ????????????? ???? ?? ?????? ?????????-???????????? ??? ????? ???????? ????????????, ??? ??????????? ???????? ?????????? ??????, ???????????? ? ?????????? ???????????? ????????? ????????? ?????????? ????????. ????????? ??????? ?????? ?????????? ???????????? ???????????? ??????? ???????, ????????????? ? ?????????? ????????? ?????? ????????? ?????????? ???????? ?? ????????, ???????????????? ????????? ?????? ?? ?????????????. ???????? ???????????? ?????????????????? ???????????? ? ??????? ??????????????? ????????????? ????? ???????? ?????????????? ???????? ? ??????????? ?????? ????????????? ?????? ?????? ??????? ?????????? ???????? ????. ?????? ??????? ???????? ?? ??????? ????????????? ???? ?? ?????? ?????????-???????????? ??? ? ?????????? ??????? ????????? ? ??????? ??????????????? ????????????...|$|R
40|$|<b>Estimation</b> <b>of</b> the {{instantaneous}} frequency- and time-varying amplitude {{along with}} their derivatives is considered for a harmonic complex-valued signal given with an additive noise. Asymptotic minimax lower bounds are derived for the mean-squared <b>errors</b> <b>of</b> <b>estimation</b> provided that the phase and amplitude are arbitrary piece-wise differentiable functions of time. It is shown that these lower bounds are different only in constant factors from the optimal upper bounds <b>of</b> mean-squared <b>errors</b> <b>of</b> estimates given by the generalized local polynomial periodogram. The time-varying phase and amplitude are derived which are `worst', respectively, for <b>estimation</b> <b>of</b> the instantaneous frequency, amplitude and their derivative. These `worst' functions can be applied {{in order to test}} the accuracy of algorithms used for <b>estimation</b> <b>of</b> the instantaneous frequency and amplitude...|$|R
50|$|In 1809 Carl Friedrich Gauss {{published}} his method of calculating {{the orbits of}} celestial bodies. In that work {{he claimed to have}} been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimizes the <b>error</b> <b>of</b> <b>estimation.</b> Gauss showed that arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.|$|E
5000|$|As {{long ago}} as 1937, Lewis Terman pointed out that <b>error</b> <b>of</b> <b>estimation</b> in IQ scoring {{increases}} as IQ score increases, {{so that there is}} less and less certainty about assigning a test-taker to one band of scores or another as one looks at higher bands. Current IQ tests also have large error bands for high IQ scores. As an underlying reality, such distinctions as those between [...] "exceptionally gifted" [...] and [...] "profoundly gifted" [...] have never been well established. All longitudinal studies of IQ have shown that test-takers can bounce up and down in score, and thus switch up and down in rank order as compared to one another, over the course of childhood. Some test-givers claim that IQ classification categories such as [...] "profoundly gifted" [...] are meaningful, but those are based on the obsolete Stanford-Binet Third Revision (Form L-M) test. The highest reported standard score for most IQ tests is IQ 160, approximately the 99.997th percentile (leaving aside the issue of the considerable error in measurement at that level of IQ on any IQ test). IQ scores above this level are dubious as there are insufficient normative cases upon which to base a statistically justified rank-ordering. Moreover, there has never been any validation of the Stanford-Binet L-M on adult populations, and there is no trace of such terminology in the writings of Lewis Terman. Although two current tests attempt to provide [...] "extended norms" [...] that allow for classification of different levels of giftedness, those norms are not based on well validated data.|$|E
30|$|When the <b>error</b> <b>of</b> <b>estimation</b> {{is taken}} into account, {{introduction}} of two positive slack variables ζ and ζ* is {{to represent the}} distance between the actual value and the corresponding boundary values.|$|E
30|$|The {{following}} theorem {{gives the}} <b>error</b> <b>estimation</b> <b>of</b> the Laguerre wavelets expansion.|$|R
40|$|A formula <b>of</b> <b>error</b> <b>estimation</b> <b>of</b> Mann {{iteration}} {{is given}} {{in the case of}} strongly demicontractive mappings. Based on this <b>estimation,</b> a condition <b>of</b> strong convergence is obtained for the same class of mappings. T-stability for a particular case of strongly demicontractive mappings is proved. Some numerical examples showing the accuracy <b>of</b> the proposed <b>estimations</b> are also given. (C) 2014 Elsevier B. V. All rights reserved...|$|R
40|$|Failure of integrability {{is shown}} to cause path-dependence of {{willingness-to-pay}} measures of welfare change. Using the linear expenditure system, effects of failure of integrability are negligible (substantial) for estimating income (price) elasticities. For single price changes, Hausman's approach to calculating willingness to pay from ordinary demands becomes subject to excessive <b>errors</b> <b>of</b> <b>estimation.</b> For multiple price changes, calculations of willingness to pay become path dependent. The empirical approach of Vartia to calculation of willingness to pay for multiple price changes thus involves an arbitrary choice of path. Furthermore, the Willig results justifying consumer surplus approximation fail. Consumer/Household Economics, Demand and Price Analysis,...|$|R
40|$|By {{means of}} {{comparison}} of calculated and experimental durabilities estimations under service loading and using linear damage summation rule hypothesis a posterior {{estimation of the}} fatigue curve type (with-, or without horizontal part, broken type est.) in gigacycle region is estimated. Conception of acceleration coefficient is introduced to decrease the systematic <b>error</b> <b>of</b> <b>estimation...</b>|$|E
40|$|A {{problem of}} Bayesian {{sequential}} estimating an unknown parameter of an exponential distribution is considered. It is supposed {{that the loss}} associated with the <b>error</b> <b>of</b> <b>estimation</b> is asymmetric (LINEX) {{and the cost of}} observing the process is a linear function of time and the number of observations. A Bayes sequential procedure for estimating the unknown parameter is presented. ...|$|E
40|$|Tree species {{proportions}} of forest stands were estimated using ranging scatterometer called HUTSCAT. Employed estimation method was multilayer perceptron neural network with error backpropagation training algorithm. Different methods based on intensity and/or shape of measured profiles were tested. The best classification {{accuracy of the}} main tree species was about 88 % and the mean <b>error</b> <b>of</b> <b>estimation</b> for tree species proportions was 0. 26...|$|E
40|$|Background In {{nutritional}} epidemiology, it {{is common}} to fit models in which several dietary variables are included. However, with standard instruments for dietary assessment, not only are the intakes of many nutrients often highly correlated, but the <b>errors</b> in the <b>estimation</b> <b>of</b> the intake of different nutrients are also correlated. The effect <b>of</b> this <b>error</b> correlation on the results of observational studies has been little investigated. This paper describes the effect on multivariate regression coefficients of different levels of correlation, both between the variables themselves and between the <b>errors</b> <b>of</b> <b>estimation</b> <b>of</b> these variables. Methods Using a simple model for the multivariate error structure, we examine the effect on the estimates of bivariate linear regression coefficients of (1) differential precision of measurement of the two independent variables, (2) differing levels of correlation between the true values of the two variables, and (3) differing levels of correlation between the <b>errors</b> <b>of</b> measurement of the two variables. As an example, the prediction of plasma vitamin C levels by dietary intake variables is considered, using data from the European Prospective Investigation of Cancer (EPIC) Norfolk stud...|$|R
3000|$|... {{strictly}} upper-triangle feedback {{matrix of}} the DFE receiver. To minimize the <b>error</b> <b>of</b> the signal <b>estimation</b> in (11), we have [...]...|$|R
40|$|This paper {{develops}} an {{easy to use}} {{and cost}} efficient index for the assessment of stock viability in fisheries management and bioeconomic analysis. The index is predicated on biological and economic theory, and can be applied to different bioeconomic frameworks including different behavioural assumptions. Available time series data for the Canadian Atlantic cod, Danish North Sea cod and Norwegian cod fisheries are then assimilated into the model dynamics in order to estimate the index. To buttress the utility of this index, a jackknife technique is used to provide standard <b>errors</b> <b>of</b> <b>estimation.</b> It is found that for all the three fisheries, the imputed indices are low and consistent with observational data...|$|R
