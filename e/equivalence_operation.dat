4|32|Public
40|$|Complex {{orthogonal}} designs (CODs) {{are used}} to construct space-time block codes. COD O_z with parameter [p, n, k] is a p × n matrix, where nonzero entries are filled by ± z_i or ± z^*_i, i = 1, 2, [...] ., k, such that O^H_z O_z = (|z_ 1 |^ 2 +|z_ 2 |^ 2 + [...] . +|z_k|^ 2) I_n × n. Define O_z a first type COD {{if and only if}} O_z does not contain submatrix ± z_j & 0; 0 & ± z^*_j or ± z^*_j & 0; 0 & ± z_j. It is already known that, all CODs with maximal rate, i. e., maximal k/p, are of the first type. In this paper, we determine all achievable parameters [p, n, k] of first type COD, as well as all their possible structures. The existence of parameters is proved by explicit-form constructions. New CODs with parameters [p,n,k]=[nw- 1 +nw+ 1, n, nw], for 0 < w < n, are constructed, which demonstrate the possibility of sacrificing code rate to reduce decoding delay. It's worth mentioning that all maximal rate, minimal delay CODs are contained in our constructions, and their uniqueness under <b>equivalence</b> <b>operation</b> is proved...|$|E
40|$|Complex {{orthogonal}} designs (CODs) {{are used}} to construct space-time block codes. COD O_z with parameter [p, n, k] is a p× n matrix, where nonzero entries are filled by ± z_i or ± z^*_i, i = 1, 2, [...] ., k, such that O^H_z O_z = (|z_ 1 |^ 2 +|z_ 2 |^ 2 + [...] . +|z_k|^ 2) I_n × n. Adams et al. in "The final case of the decoding delay problem for maximum rate complex orthogonal designs," IEEE Trans. Inf. Theory, vol. 56, no. 1, pp. 103 - 122, Jan. 2010, first proved the nonexistence of [2 mm- 1, 2 m, 2 m- 1 m- 1], m odd, COD. Combining with the previous result that decoding delay should be an integer multiple of 2 mm- 1, they solved the final case n ≡ 2 4 of the decoding delay problem for maximum rate complex orthogonal designs. In this paper, we give another proof of the nonexistence of COD with parameter [2 mm- 1, 2 m, 2 m- 1 m- 1], m odd. Our new proof {{is based on the}} uniqueness of [2 mm- 1, 2 m- 1, 2 m- 1 m- 1] under <b>equivalence</b> <b>operation,</b> where an explicit-form representation is proposed to help the proof. Then, by proving it's impossible to add an extra orthogonal column on COD [2 mm- 1, 2 m- 1, 2 m- 1 m- 1] when m is odd, we complete the proof of the nonexistence of COD [2 mm- 1, 2 m, 2 m- 1 m- 1]...|$|E
40|$|Objective: To {{translate}} {{and culturally}} adapt the UK English Audit of Diabetes-Dependent Quality of Life (ADDQOL) into Chinese for Singapore. Methods: Translation was integrated into investigation of conceptual, item, semantic, and operation equivalence. Conceptual equivalence, item equivalence, and operation equivalence were assessed by literature review, expert judgment, and cognitive debriefing. Semantic equivalence was studied {{by using an}} optimized procedure including forward and backward translation, clinician review, and cognitive debriefings. Cognitive debriefings were done with five Chinese-speaking diabetic patients at polyclinics. Reliability, responsiveness, and construct validity tests were used to evaluate measurement equivalence. English- and Chinese-speaking diabetic patients by convenient sampling at a Diabetes Society of Singapore's public event were recruited for the measurement equivalence study. Mann-Whitney U tests, chi-square tests, and descriptive analyses were used for group comparisons and Spearman's correlation coefficients for construct validity tests. Results: Forty-two English-speaking and 26 Chinese-speaking diabetic patients (45. 5 % females) {{with a mean age}} of 54. 2 ± 10. 07 years were recruited. Chinese-speaking respondents were more likely than the English-speaking group to be unemployed, less educated, and with poorer family functioning (P < 0. 05). Conceptual equivalence, item <b>equivalence,</b> <b>operation</b> equivalence, and semantic equivalence were all demonstrated. Cronbach's alpha for internal consistency and intraclass correlation coefficient for test-retest reliability were 0. 94 and 0. 65, respectively. Distributions of responses were found to be similar except for some difference that can be justified by different demographic background. Convergent validity was suggested by weak to moderate correlations between “Present QOL” on the ADDQOL and EuroQol five-dimensional questionnaire (r = 0. 268; P = 0. 185) and six-dimensional health state short form (derived from short form 36 health survey) scores (r = 0. 351; P = 0. 078); divergent validity was shown by a weak correlation between ADDQOL average weighted impact (AWI) and ADDQOL “Present QOL” scores (r = 0. 027, P = 0. 896), a moderate correlation between ADDQOL AWI and six-dimensional health state short form (derived from short form 36 health survey) mental scores (r = 0. 247; P = 0. 224), and a positive correlation between ADDQOL AWI and family functioning scores (r = 0. 288; P = 0. 182). Conclusions: The ADDQOL has been translated and culturally adapted successfully into a Chinese version for Singapore. Our study provides justification for further research with large sample sizes among the Chinese-speaking population in Singapore...|$|E
25|$|Type {{dependent}} <b>equivalence</b> <b>operations</b> {{also exist}} in Scheme: string=? and string-ci=? compare two strings (the latter performs a case-independent comparison); char=? and char-ci=? compare characters; = compares numbers.|$|R
40|$|It {{is shown}} that <b>operations</b> of <b>equivalence</b> cannot serve for {{building}} algebras which would induce orthomodular lattices as {{the operations of}} implication can. Several properties of <b>equivalence</b> <b>operations</b> have been investigated. Distributivity of equivalence terms and several other 3 variable expressions involving equivalence terms have been proved to hold in any orthomodular lattice. Symmetric differences {{have been shown to}} reduce to complements of equivalence terms. Some congruence relations related to <b>equivalence</b> <b>operations</b> and symmetric differences have been considered. Comment: 13 pages, 1 figure, 1 table; To be published in International Journal of Theoretical Physics, Vol. 42, No. 12 (2003); Web page: [URL]...|$|R
40|$|One of {{the most}} {{promising}} structural approaches to resolving the Hadamard Conjecture uses the family of cocyclic matrices over Zt×Z 22. Two types of equivalence relations for classifying cocyclic matrices over Zt ×Z 22 have been independently found. Any cocyclic matrix equivalent by either of these relations to a Hadamard matrix will also be Hadamard. Bundle equivalence is based on algebraic relations between cocycles over any finite group. Diagram equivalence is based on geometric relations between diagrammatic visualisations of cocyclic matrices over the group Zt ×Z 22. Here we reconcile the two. We show the group Bund(t) generated by bundle <b>equivalence</b> <b>operations</b> is isomorphic to a subgroup of index 2 in the group Diag(t) generated by diagram <b>equivalence</b> <b>operations,</b> and that Diag(t) = &lt;Bund(t),T> where T is the geometric translation of matrix transposition...|$|R
5000|$|Matrix [...] {{contains}} the multipliers for the inter-industry inputs required to supply one unit of industry output. A certain total economic output [...] {{is required to}} satisfy a given level of final demand [...] This final demand may be domestic (for private households {{as well as the}} public sector) or foreign (exports) and can be written as an [...] vector. When this vector of final demand [...] is multiplied by the Leontief inverse , we obtain total output [...] [...] is the identity matrix so that the following matrix equation is the result of <b>equivalence</b> <b>operations</b> in our previous equation: ...|$|R
40|$|<b>Equivalence</b> <b>operations</b> for n-dimensional proper Hadamard {{matrices}} are defined. Their equivalence {{classes are}} investigated and bounds {{on the number}} of such equivalence classes are found {{to be associated with the}} number of equivalence classes of 2 -dimensional Hadamard matrices. Properties of planes (2 -dimensional sections) in an n-dimensional proper Hadamard matrix are investigated. It is shown that all planes of an n-dimensional proper Hadamard matrix are Hadamard equivalent to the 2 -dimensional Hadamard matrix from which it is constructed, regardless of whether the construction technique applied is Product Construction, Group Development or Relative Difference Set Construction. The relationship of the three constructions is demonstrated. ...|$|R
40|$|Student {{difficulties}} {{with the study}} of algebra have been well documented. The inability of many students to understand variables and formal symbolic manipulation act {{as a barrier to}} success in mathematics study. This report documents an intervention that uses a concrete approach to teaching algebra in a Year 9 class. Results indicate that much of the student struggle was associated with a lack of understanding of arithmetic concepts including those associated with <b>equivalence,</b> <b>operations</b> with negative integers, and the distributive law and fraction concepts. Once these difficulties were addressed through the explicit teaching of the links between materials and symbols, materials and language, language and symbols, students made considerable progress in writing, simplifying expressions, and solving equations with variables on both sides. Full Tex...|$|R
40|$|AbstractThis {{paper and}} its successor(s) aim to derive a {{mathematical}} description of bond graphs {{in general and}} of their junction structures in particular. It also introduces bond graphs to mathematicians that have no previous knowledge of them. In this introductory paper, a definition of bond graphs is given and the concept of acausal equivalence is introduced. Fifteen basic operations are defined and proved to be acausal <b>equivalence</b> <b>operations.</b> It is proved that these basic operations form a complete set, in the sense that, if two bond graphs are acausally equivalent, then each can be converted into the other by a sequence of these operations and their inverses. In the course of the proof it is shown that every bond graph is acausally equivalent to one in a standard form. These standard bond graphs are used to demonstrate various mathematical properties of bond graphs, and to derive a new procedure for testing whether or not a given set of input variables uniquely determines the corresponding set of output variables. This should be of interest to mathematicians and engineers alike...|$|R
40|$|In {{the past}} few decades, design theory has grown to {{encompass}} {{a wide variety of}} research directions. It comes as no surprise that applications in coding theory and communications continue to arise, and also that designs have found applications in new areas. Computer science has provided a new source of applications of designs, and simultaneously a field of new and challenging problems in design theory. In this paper, we revisit a construction for orthogonal designs using the multiplication tables of Cayley-Dickson algebras of dimension $ 2 ^n$. The desired orthogonal designs can be described by a system of equations {{with the aid of a}} Groebner basis computation. For orders greater than 16 the combinatorial explosion of the problem gives rise to equations that are unfeasible to be handled by traditional search algorithms. However, the structural properties of the designs make this problem possible to be tackled in terms of rewriting techniques, by equational unification. We establish connections between central concepts of design theory and equational unification where <b>equivalence</b> <b>operations</b> of designs point to the computation of a minimal complete set of unifiers. These connections make viable the computation of some types of orthogonal designs that have not been found before with the aforementioned algebraic modelling...|$|R
40|$|An {{introduction}} {{is given}} to the Littlewood-Richardson rule, and various combinatorial constructions related to it. We present a proof based on tableau switching, dual <b>equivalence,</b> and coplactic <b>operations.</b> We conclude with a section relating these fairly modern techniques to earlier work on the Littlewood-Richardson rule. Comment: 29 page...|$|R
40|$|Abstract: The {{notion of}} a {{functional}} relation among the attributes of a data set can be fruitfully applied in the structuring of an information system. These relations are meaningful both to the user of the system in his semantic understanding of the data, and to the designer in implementing the system. An important <b>equivalence</b> between <b>operations</b> with functional relations and operations with analogous Boolean functions is demonstrated in this paper. The equivalence is computationally helpful in exploring the properties of a given set of functional relations, {{as well as in}} the task of partitioning a data set into subfiles for efficient implementation. 1...|$|R
40|$|An {{equivalence}} relation {{is defined in}} the set of fuzzy numbers. In a particular <b>equivalence</b> class, arithmetic <b>operations</b> of fuzzy numbers are introduced. A fuzzy matrix {{with respect to a}} particular class and its associated crisp matrices are also introduced. The concept of equivalence class is applied in fuzzy decision-making problems and justified through a numerical example. Copyright © 2006 Hindawi Publishing Corporation. All rights reserved. 1...|$|R
40|$|Energy {{becomes the}} most {{critical}} aspect {{in the design of}} modern microelectronic devices, which is re-flected in the increasing number of conflicting requirements to the power characteristics of the circuits. How-ever, modern design tools do not consider dynamics of energy resource, do not have flexible notion of energy efficiency and limit power optimisation to few design-specific techniques. We propose to address these limita-tions through a fundamental token-based model of computation, extended with the concept of energy tokens to represent energy flows in the circuit. The energy token model will serve as a foundation for building systems {{in such a way that}} while maintaining functional <b>equivalence,</b> their <b>operation</b> will be altered to meet energy mode requirements under dynamically changing energy resource and operating conditions. Danil Sokolov, Alex Yakovlev: Task scheduling based on energy token mode...|$|R
3000|$|The tested {{algorithms}} in this article, {{as mentioned}} earlier, are categorized {{in the class}} of [...] "two-scan algorithms". In order to evaluate the performance of the proposed first image scan algorithm and then to avoid the effect of the <b>equivalences</b> resolution <b>operation,</b> this experiment was conducted by executing a variety of different algorithms (He et al. [8], Grana et al. [3], and our proposed method). In the equivalences resolution, we followed the Union-Find technique as presented by He et al. [11], which are the most advanced available techniques currently available. The Union-Find technique uses three array-based data structures that implement the algorithm in a very efficient way. During the second image scan, we only need to replace each provisional label by its representative label. As a result, all pixels belonging to a connected component will be assigned a unique label.|$|R
40|$|In {{this paper}} we study the {{relationship}} between forms of weak induction in theories of operations and numbers. Therefore, we investigate {{the structure of the}} natural numbers. Introducing a concept of N-strictness, we give a natural extension of the theory BON which implies the <b>equivalence</b> of <b>operation</b> and N-induction. In addition, we show that {{in the presence of the}} non-constructive ¯-operator the above equivalence is provable without this extension. 1 Introduction Applicative theories go back to Feferman's systems of explicit mathematics introduced in [Fef 75, Fef 79]. They are based on the basic theory of operations and numbers BON which is introduced in [FJ 93] as the classic version of Beeson's theory EON (cf. [Bee 85]) without induction. Combined with various induction principles, applicative theories provide a natural framework for constructive mathematics and functional programming. If they are strengthened by the so-called non-constructive ¯-operator, a predicatively acceptable [...] ...|$|R
40|$|A circle graph is the {{intersection}} graph {{of a family}} of chords on a circle. There is no known characterization of circle graphs by forbidden induced subgraphs that do not involve the notions of local <b>equivalence</b> or pivoting <b>operations.</b> We characterize circle graphs by a list of minimal forbidden induced subgraphs when the graph belongs to one of the following classes: linear domino graphs, P 4 -tidy graphs, and tree-cographs. We also completely characterize by minimal forbidden induced subgraphs the class of unit Helly circle graphs, which are those circle graphs having a model whose chords have all the same length, are pairwise different, and satisfy the Helly property...|$|R
40|$|We {{compare the}} {{activity}} of young children using a microworld and a JavaScript relational calculator with the literature on children using traditional calculators. We describe how the children constructed different meanings for the equal sign in each setting. It appears {{that the nature of}} the meaning constructed is highly dependent on specificities of the task design and the tools available. In particular, the microworld offers the potential for children to adopt a meaning of equivalence for the equal sign. CHILDREN’S PERCEPTIONS OF THE EQUAL SIGN There exist “two intuitive meanings of equality among preschoolers ” that correspond to <b>equivalence</b> and <b>operation</b> (Kieran, 1981). When conceived of as an operator the equal sign would be expected to generate an answer from a sum. In contrast, when conceived of as equivalence the equal sign would be regarded as a static relation between two expressions that are the same in value. However school children attend to the operator meaning more readily than equivalence (Behr et al, 1980). The reasons for this have been identified by Ginsburg (1977) as based in the way the equal sign is encountered in the classroom. Invariabl...|$|R
40|$|Abstract — In {{this paper}} we present the <b>equivalence</b> of the <b>operations</b> {{involved}} in DES and AES algorithm with operations of cellular automata. We identify all the permutation and substitution operations involved in DES and AES algorithm and compare these operations with the cellular automata rules. Then {{we find that}} permutation operations involved in DES and AES are equivalent to linear cellular automata rules providing diffusion property of cryptography whereas substitution operations involved in DES and AES are equivalent to non linear cellular automata rules providing the confusion property of cryptography. Hence instead of using operations involved in DES and AES algorithm, we can apply linear as well as non-linear cellular automata rules in cryptography for better security and parallel processing...|$|R
40|$|Linear {{array of}} {{permutations}} {{is hard to}} be factorized. However, by using a starter set, the process of listing the permutations becomes easy. Once the starter sets are obtained, the circular and reverse of circular operations are easily employed to produce distinct permutations from each starter set. However, a problem arises when the equivalence starter sets generate similar permutations and, therefore, {{will need to be}} discarded. In this paper, a new recursive strategy is proposed to generate starter sets that will not incur <b>equivalence</b> by circular <b>operation.</b> Computational advantages are presented that compare the results obtained by the new algorithm with those obtained using two other existing methods. The result indicates that the new algorithm is faster than the other two in time execution...|$|R
40|$|We {{present in}} this paper a new {{methodology}} for prioritizing threats rated with ordinal scale values while preserving the meaning of ordinal values and respecting the rules that govern ordinal scales. Our approach is quite novel because we present a formal algebraic system called the k/m algebra to derive the equivalence classes into which threats will be placed and define an operation called k/m dominance which orders the <b>equivalence</b> classes. The <b>operations</b> of our algebra always respect the rules that govern ordinal scales and preserve the meaning of ordinal values. We also describe and present the results from a preliminary case study where we applied our k/m algebra to prioritize threats ranked using data from an existing threat modeling system. Categories and Subject Descriptors D. 2. 8 [Software Engineering] Metrics – for threat modeling i...|$|R
40|$|Abstract Information flow {{security}} in a multilevel system aims at guaranteeing that no highlevel information is revealed to low level users, {{even in the}} presence of any possible malicious process. This requirement could be stronger than necessary when someknowledge about the environment (context) in which the process is going to run is available. To relax this requirement we introduce the notion of secure contexts fora class of processes. This notion is parametric with respect to both the observation <b>equivalence</b> and the <b>operation</b> used to characterize the low level view of a process. As observation equivalence we consider the cases of weak bisimulation and trace equivalence. We describe how to build secure contexts in these cases and we showthat two well-known security properties, named BNDC and NDC, are just special instances of our general notion...|$|R
40|$|Composition and lattice join (transitive {{closure of}} a union) of <b>equivalence</b> {{relations}} are <b>operations</b> taking pairs of decidable equivalence relations to relations that are semi-decidable, {{but not necessarily}} decidable. This article addresses the question, is every semi-decidable equivalence relation obtainable in those ways {{from a pair of}} decidable equivalence relations? It is shown that every semi-decidable equivalence relation, of which every equivalence class is infinite, is obtainable as both a composition and a lattice join of decidable equivalence relations having infinite equivalence classes. An example is constructed of a semi-decidable, but not decidable, equivalence relation having finite equivalence classes that can be obtained from decidable equivalence relations, both by composition and also by lattice join. Another example is constructed, in which such a relation cannot be obtained from decidable equivalence relations in either of the two ways. Comment: 14 page...|$|R
40|$|Information flow {{security}} in a multilevel system aims at guaranteeing that no high level information is revealed to low level users, {{even in the}} presence of any possible malicious process. This requirement could be stronger than necessary when some knowledge about the environment (context) in which the process is going to run is available. To relax this requirement we introduce the notion of secure contexts for a class of processes. This notion is parametric with respect to both the observation <b>equivalence</b> and the <b>operation</b> used to characterize the low level view of a process. As observation equivalence we consider the cases of weak bisimulation and trace equivalence. We describe how to build secure contexts in these cases and we show that two well-known security properties, named BNDC and NDC, are just special instances of our general notion. £This work has been partially supported by the EU Contract IST- 2001 - 32617 “Models and Types fo...|$|R
40|$|In {{this paper}} {{we present a}} general {{framework}} within which a study of networks of processes can be conducted. It {{is based upon the}} mathematical technique to abstract from irrelevant detail. We start out with a large class of objects and some operations upon them. Depending upon a correctness criterion to be imposed, some of these objects turn out to be equivalent. The resulting space of <b>equivalence</b> classes and <b>operations</b> upon them is, under certain conditions, the (fully) abstract space of interest for that particular correctness concern. We use this approach to study networks for which we assume the communications to be asymmetric and asynchronous. We impose the correctness criterion of absence of computation interference. The resulting abstract space {{turns out to be the}} space of delay-insensitive specifications. As operator we study composition of networks. The composition operator on the resulting space is shown to have a surprisingly simple factorization property, the prove of which turns out to be very simple due to the approach taken...|$|R
40|$|By {{shifting}} {{documents from}} display-centered media to database data SGML has raised new {{problems that the}} standard itself {{was not intended to}} solve. A database is an open world with many potential products, and most importantly, with the possibility of change. Database systems that manage change have several responsibilities. They must specify the operations that cause change and their logical properties, they must support the denition of constraints that identify valid changes, they must provide techniques for handling concurrent changes, and they must preserve the continuity of the data's essence while it changes. Handling these responsibilities constitutes the semantics of a database system. We discuss some issues that are 'meta-semantic': issues that lie behind every semantics, but that are peculiar to none. We discuss three issues - <b>equivalence,</b> redundancy, and <b>operations</b> - that are implicit in current systems and approaches. By making these issues explicit, we hope to provide the beginnings of a framework for comparison of the semantics of existing systems, and the development of more advanced ones...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. After many years of successful operation in military domains, Unmanned Aerial Systems (UASs) are generating significant interest amongst civilian operators in sectors such as law enforcement, search and rescue, aerial photography and mapping. To maximise the benefits brought by UASs to sectors such as these, {{a high level of}} autonomy is desirable to reduce the need for highly skilled operators. Highly autonomous UASs require a high level of situation awareness in order to make appropriate decisions. This is of particular importance to civilian UASs where transparency and <b>equivalence</b> of <b>operation</b> to current manned aircraft is a requirement, particularly in the terminal area immediately surrounding an airfield. This thesis presents an artificial situation awareness system for an autonomous UAS capable of comprehending both the current continuous and discrete states of traffic vehicles. This estimate forms the basis of the projection element of situation awareness, predicting the future states of traffic. Projection is subject to a large degree of uncertainty in both continuous state variables and in the execution of intent information by the pilot. Both of these sources of uncertainty are captured to fully quantify the future positions of traffic. Based upon the projection of future traffic positions a self separation system is designed which allows an UAS to quantify its separation to traffic vehicles up to some future time and manoeuvre appropriately to minimise the potential for conflict. A high fidelity simulation environment has been developed to test the performance of the artificial situation awareness and self separation system. The system has demonstrated good performance under all situations, with an equivalent level of safety to that of a human pilot...|$|R
40|$|AbstractA {{construction}} for Segal {{operations for}} K-theory of categories with cofibrations, weak equivalences and a biexact pairing is given and coherence {{properties of the}} operations are studied. The model for K-theory, which is used, allows coherence to be studied by means of (symmetric) monoidal functors. In the case of Waldhausen A-theory it is shown how to recover the operations used in Waldhausen (Lecture Notes in Mathematics, Vol. 967, Springer, Berlin, 1982, pp. 390 – 409) for the A-theory Kahn–Priddy theorem. The total Segal operation for A-theory, which assembles exterior power operations, is shown to carry a natural infinite loop map structure. The basic input is the un-delooped model for K-theory, which has been developed from a construction by Grayson and Gillet for exact categories in Gunnarsson et al. (J. Pure Appl. Algebra 79 (1992) 255), and Grayson's setup for operations in Grayson (K-theory (1989) 247). The relevant material from these sources is recollected followed by observations on equivariant objects and pairings. Grayson's conditions are then translated to the context of categories with cofibrations and weak <b>equivalences.</b> The power <b>operations</b> are shown to be well behaved w. r. t. suspension and are extended to algebraic K-theory of spaces. Staying close with the philosophy of Waldhausen (1982) Waldhausen's maps are found. The Kahn–Priddy theorem follows from splitting the “free part” off the equivariant theory. The treatment of coherence of the total operation in A-theory involves results from Laplaza (Lecture Notes in Mathematics, Vol. 281, Springer, Berlin, 1972, pp. 29 – 65) and restriction to spherical objects in {{the source of the}} operation...|$|R
40|$|The {{performance}} of dry, low NOx gas turbines, which employ lean premixed (or partially premixed) combustors, is often limited by combustor stability. To overcome this issue, a novel design, {{referred to as}} a Stagnation Point Reverse Flow (SPRF) combustor, has been recently demonstrated. The SPRF combustor has been shown to produce low NOx emissions with both gaseous and liquid fuels. The objective of this thesis is to elucidate the interactions between the flowfield and combustion processes in this combustor for gas- and liquid-fueled operation. This is achieved with experimental measurements employing various optical diagnostic techniques. These include Particle Image Velocimetry (PIV), chemiluminescence imaging, Planar Laser-Induced Fluorescence (PLIF) of OH radicals and laser scattering from liquid droplets. Velocity measurements in gas-fueled operation show that both nonreacting and reacting flows exhibit a stagnation region with low mean velocity and high turbulence intensities. The high shear between the forward and reverse flows causes significant recirculation resulting in enhanced entrainment and mixing of the returning product gases into the incoming reactant jet for the reacting flow cases, which enables stable operation of the combustor at very lean <b>equivalence</b> ratios. Nonpremixed <b>operation</b> produces a flowfield similar to premixed case except in the near-field region where high turbulence intensities result in significant fuel-air mixing before combustion occurs. Operation of the SPRF combustor with liquid Jet-A is also investigated experimentally. The results indicate that while the overall flow features are similar to the gas-fueled SPRF combustor, the combustion characteristics and NOx performance in liquid operation are strongly controlled by fuel dispersion and evaporation. Injecting the liquid at the exit of the air annulus results in a highly lifted flame, similar to nonpremixed gaseous operation. On the other hand, retracting the fuel injector well inside the air annulus produces a well-dispersed fuel pattern at the reactant inlet leading to a reduction of the equivalence ratio in the fuel consuming reaction zones. Since the effective Dahmkohler number increases with global equivalence ratio, the difference in NOx emissions is more pronounced at higher fuel-air ratios as the retracted injector lowers the relative mixing time compared to the flush case. Ph. D. Committee Chair: Seitzman, Jerry; Committee Member: Gaeta, Richard; Committee Member: Jagoda, Jeff; Committee Member: Neumeier, Yedidia; Committee Member: Yoda, Minami; Committee Member: Zinn, Be...|$|R
40|$|Relations are everywhere. In particular, {{we think}} and reason {{in terms of}} {{mathematical}} and English sentences that state relations. However, we teach our students much more about how to manipulate functions than about how to manipulate relations. Consider functions. We know how to combine functions to make new functions, how to evaluate functions efficiently, and how to think about compositions of functions. Especially {{in the area of}} boolean functions, we have become experts in the theory and art of designing combinations of functions to yield what we want, and this expertise has led to techniques that enable us to implement mind-bogglingly large yet efficient networks of such functions in hardware to help us with calculations. If we are to make progress in getting machines to be able to reason as well as they can calculate, we need to similarly develop our understanding of relations, especially their composition, so we can develop techniques to help us bridge between the large and small scales. There has been some important work in this area, ranging from practical applications such as relational databases to extremely theoretical work in universal algebra, and sometimes theory and practice manage to meet, such as in the programming language Prolog, or in the probabilistic reasoning methods of artificial intelligence. However, the real adventure is yet to come, as we learn to develop {{a better understanding of how}} relations can efficiently and reliably be composed to get from a low level representation to a high level representation, as this understanding will then allow the development of automated techniques to do this on a grand scale, finally enabling us to build machines that can reason as amazingly as our contemporary machines can calculate. This thesis explores new ground regarding the composition of relations into larger relational structures. First of all a foundation is laid by examining how networks of relations might be used for automated reasoning. We define exclusion networks, which have close connections with the areas of constraint satisfaction problems, belief propagation, and even boolean circuits. The foundation is laid somewhat deeper than usual, taking us inside the relations and inside the variables to see what is the simplest underlying structure that can satisfactorily represent the relationships contained in a relational network. This leads us to define zipper networks, an extremely low-level view in which the names of variables or even their values are no longer necessary, and relations and variables share a common substrate that does not distinguish between the two. A set of simple <b>equivalence</b> <b>operations</b> is found that allows one to transform a zipper network while retaining its solution structure, enabling a relation-variable duality as well as a canonical form on linear segments. Similarly simple operations allow automated deduction to take place, and these operations are simple and uniform enough that they are easy to imagine being implemented by biological neural structures. The canonical form for linear segments can be represented as a matrix, leading us to matrix networks. We study the question of how we can perform a change of basis in matrix networks, which brings us to a new understanding of Valiant's recent holographic algorithms, a new source of polynomial time algorithms for counting problems on graphs that would otherwise appear to take exponential time. We show how the holographic transformation can be understood as a collection of changes of basis on individual edges of the graph, thus providing a new level of freedom to the method, as each edge may now independently choose a basis so as to transform the matrices into the required form. Consideration of zipper networks makes it clear that "fan-out," i. e., the ability to duplicate information (for example allowing a variable to be used in many places), is most naturally itself represented as a relation along with everything else. This is a notable departure from the traditional lack of representation for this ability. This deconstruction of fan-out provides a more general model for combining relations than was provided by previous models, since we can examine both the traditional case where fan-out (the equality relation on three variables) is available and the more interesting case where its availability is sub ject to the same limitations as the availability of other relations. As we investigate the composition of relations in this model where fan-out is explicit, what we find is very different from what has been found in the past. First of all we examine the relative expressive power of small relations: For each relation on three boolean variables, we examine which others can be implemented by networks built solely from that relation. (We also find, in each of these cases, the complexity of deciding whether such a network has a solution. We find that solutions can be found in polynomial time for all but one case, which is NP-complete.) For the question of which relations are able to implement which others, we provide an extensive and complete answer in the form of a hierarchy of relative expressive power for these relations. The hierarchy for relations is more complex than Post's well-known comparable hierarchy for functions, and parts of it are particularly difficult to prove. We find an explanation for this phenomenon by showing that in fact, the question of whether one relation can implement another (and thus should be located above it in the hierarchy) is undecidable. We show this by means of a complicated reduction from the halting problem for register machines. The hierarchy itself has a lot of structure, as it is rarely the case that two ternary boolean relations are equivalent. Often they are comparable, and often they are incomparable—the hierarchy has quite a bit of width as well as depth. Notably, the fan-out relation is particularly difficult to implement; only a very few relations are capable of implementing it. This provides an additional ex post facto justification for considering the case where fan-out is absent: If you are not explicitly provided with fan-out, you are unlikely to be able to implement it. The undecidability of the hierarchy contrasts strongly with the traditional case, where the ubiquitous availability of fan-out causes all implementability questions to collapse into a finite decidable form. Thus we see that for implementability among relations, fan-out leads to undecidability. We then go on to examine whether this result might be taken back to the world of functions to find a similar difference there. As we study the implementability question among functions without fan-out, we are led directly to questions that are independently compelling, as our functional implementability question turns out to be equivalent to asking what can be computed by sets of chemical reactions acting on a finite number of species. In addition to these chemical reaction networks, several other nondeterministic systems are also found to be equivalent in this way to the implementability question, namely, Petri nets, unordered Fractran, vector addition systems, and "broken" register machines (whose decrement instruction may fail even on positive registers). We prove equivalences between these systems. We find several interesting results in particular for chemical reaction networks, where the standard model has reaction rates that depend on concentration. In this setting, we analyze questions of possibility as well as questions of probability. The question of the possibility of reaching a target state turns out to be equivalent to the reachability question for Petri nets and vector addition systems, which has been well studied. We provide a new proof that a form of this reachability question can be decided by primitive recursive functions. Ours is the first direct proof of this relationship, avoiding the traditional excursion to Diophantine equations, and thus providing a crisper picture of the relationship between Karp's coverability tree and primitive recursive functions. In contrast, the question of finding the probability (according to standard chemical kinetics) of reaching a given target state turns out to be undecidable. Another way of saying this is that if we wish to distinguish states with zero probability of occurring from states with positive probability of occurring, we can do so, but if we wish to distinguish low probability states from high probability states, there is no general way to do so. Thus, if we wish to use a chemical reaction network to perform a computation, then if we insist that the network must always get the right answer, we will only be able to use networks with limited computational power, but if we allow just the slightest probability of error, then we can use networks with Turing-universal computational ability. This power of probability is quite surprising, especially when contrasted with the conventional computational complexity belief that BPP = P. Exploring the source of this probabilistic power, we find that the probabilities guiding the network need to depend on the concentrations (or perhaps on time) —fixed probabilities aren’t enough on their own to achieve this power. In the language of Petri nets, if one first picks a transition at random, and then fires it if it is enabled, then the probability of reaching a particular target state can be calculated to arbitrary precision, but if one first picks a token at random, and then fires an enabled transition that will absorb that token, then the probability of reaching a particular target state cannot in general be calculated to any precision whatsoever. In short, what started as a simple thorough exploration of the power of composition of relations has led to many decidability and complexity questions that at first appear completely unrelated, but turn out to combine to paint a coherent picture of the relationship between relations and functions, implementability and reachability, possibility and probability, and decidability and undecidability...|$|R
40|$|This thesis {{examines}} specification refinement in {{the setting}} of polymorphic type theory and a complementary logic for relational parametricity. The starting point is the specification of abstract data types as done in the discipline of algebraic specification. Here, algebras are seen to match the standard notion of data type, i. e., a data representation together with operations on that data representation. An abstract data type is then a collection of data types sharing some well-defined abstract properties. In algebraic specification, these properties are specified algebraically by axioms in some suitable logic. Specification refinement then encompasses the idea that high-level specifications may be stepwise refined to executable programs that satisfy the initial specification; all in the framework of formal language and logic. This makes certain aspects of program development amenable to formal, computer-aided proofs of correctness. On the other hand, the discipline of type theory, lambda calculus, and its semantics is the prime field for research on programming languages. This framework is capable of characterising essentially any existing sequential programming-language feature, also advanced features such as recursive types, polymorphism and class-based object orientation. Furthermore, type theory provides a powerful framework for mechanised reasoning. This thesis is a contribution to lifting the idea of algebraic specification refinement into the more powerful domain of type theory and lambda calculus, thus giving the opportunity to expand in a sensible way a traditionally first order and functional framework to a wider range of programming aspects. We take a particular account of specification refinement and express it in a type-theoretic setting consisting of the polymorphic lambda calculus and a logic for relational parametricity. Key elements of algebraic specification are internalised in the syntax, e. g., data types viz. algebras are inhabitants of existential type, the latter providing essential data abstraction. For data types with only first-order operations, this setting automatically resolves certain issues of specification refinement, such as observational equivalence, stability and input sorts. After establishing a correspondence at first order, thus implanting the idea of algebraic specification refinement into the type-theoretic setting, the scene is set for lifting the idea of algebraic specification refinement to any number of programming features. In this thesis we focus on the generalisations to higher-order functions and to polymorphism. A simulation relation between two data types is a relation between their data representations that is preserved by their respective sets of operations. Using simulation relations is a classical way of explaining data refinement and observational equivalence. This combines with specification refinement to form specification refinement up to observational <b>equivalence.</b> With higher-order <b>operations,</b> however, we encounter in the logic a phenomenon related to what happens on the semantic level, i. e., the standard notion of refinement relation in the form of logical relations does not compose and the correspondence with observational equivalence is lost. In the logic {{it turns out that the}} standard notion of simulation relation fails to take into account a certain aspect of the abstraction barrier provided by existential types. We remedy this by proposing an alternative notion of simulation relation that observes this abstraction barrier more closely. We do this in two related ways; one relates to syntactic models while the other relates to a non-syntactic PER-model more apt for interpretive investigations. In algebraic specification, there is a universal proof method for specification refinement up to observational equivalence. This method can be imported soundly into the type-theoretic setting by asserting certain axioms. At first order, showing soundness for these axioms is straight-forward w. r. t. the standard parametric PER model for the logic. At higher order there are two problems. First, these axioms seemingly do not hold in the standard model. Secondly, the axioms speak in terms of simulation relations. At higher order, it is pertinent to have versions of the axioms featuring the abstraction barrier-observing simulation relations above, and to prove soundness for these poses an additional challenge. We show that the pure higher-order aspect of this problem can be solved by giving a setoid-based semantics. For the remaining task, we continue working from the observation that standard definitions do not observe abstraction barriers closely enough. Hence, we propose an alternative interpretation into the PER-model for data types that captures the abstraction barrier provided by existential types. The main contribution of this thesis is thus in generalising a prominent account of specification refinement to higher order and polymorphism via type theory incorporating relational parametricity. We also shed light on short-comings in the logic, as well as in the standard semantics, regarding the abstraction barrier provided by existential types. Two central contributions, namely abstraction barrier-observing simulation relations and abstraction barrier-observing semantics for data types, are the result of observing these short-comings. Finally, the work in this thesis also lays a foundation on which to adapt specification refinement to an object-oriented setting, because the theoretical concepts underlying object orientation can be seen as extensions of those for abstract data types...|$|R

