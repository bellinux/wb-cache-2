96|142|Public
25|$|Jet 3.0 {{included}} many enhancements, {{including a new}} index structure that reduced storage size and the time that was taken to create indices that were highly duplicated, the removal of read locks on index pages, a new mechanism for page reuse, a new compacting method for which compacting the database resulted in the indices being stored in a clustered-index format, a new page allocation mechanism to improve Jet's read-ahead capabilities, improved delete operations that speeded processing, multithreading (three threads were used to perform read ahead, write behind, and cache maintenance), implicit transactions (users {{did not have to}} instruct the engine to start manually and commit transactions to the database), a new sort engine, long values (such as memos or binary data types) were stored in separate tables, and dynamic buffering (whereby Jet's cache was dynamically allocated at start up and had no limit and which changed from a first in, first out (<b>FIFO)</b> <b>buffer</b> replacement policy to a least recently used (LRU) buffer replacement policy). Jet 3.0 also allowed for database replication.|$|E
5000|$|An on-chip <b>FIFO</b> <b>buffer</b> {{for both}} {{incoming}} and outgoing data; {{this gives the}} host system more time to respond to an interrupt generated by the UART, without loss of data.|$|E
50|$|The {{serial port}} is rudimentary, using {{programmed}} input/output only and lacking a <b>FIFO</b> <b>buffer.</b> However, virtually any bit rate can be selected, including all standard rates, MIDI rate, {{as well as}} extremely high custom rates.|$|E
40|$|A full duplex, {{high speed}} data link is {{described}} which comprises identical modules {{at each site}} for communication over coaxial cables. Each module having a digital frequency modulation (DFM) transmitter with an first-in-first-out (<b>FIFO)</b> data <b>buffer</b> for data from a digital system to be transmitted over a cable and a receiver with an <b>FIFO</b> data <b>buffer</b> for data received for a digital system {{at the other end}} of the cable. Data words are preceded by a sync word which enables the receiver. Every word transmitted, including the sync word is stored in the <b>FIFO</b> data <b>buffer.</b> This arrangement using <b>FIFO</b> <b>buffers</b> allows for asynchronous transmission of data with receipt of the data beginning with the very first bit of the sync word...|$|R
40|$|Message passing {{is one of}} {{the primary}} modes of interprocess {{communication}} in a distributed system. In this paper we investigate the possibility of characterizing and axiomatizing different message passing systems in temporal logic. Specifically, we consider <b>FIFO</b> <b>buffers</b> (queues), LIFO buffers (stacks) and unordered buffers (bags). We show that all bounded buffers are characterizable in propositional temporal logic (PTL) and so are axiomatizable. We prove that the theory of unbounded <b>FIFO</b> <b>buffers</b> is π 11 -complete and so is not axiomatizable. We also prove that the theories of unbounded LIFO and unordered buffers are decidable and hence are axiomatizable...|$|R
40|$|Previous work {{proposes a}} dependence-based {{scheduling}} technique {{in which the}} scheduling window is partitioned {{into a number of}} <b>FIFO</b> <b>buffers,</b> the heads of which constitute the set of instructions eligible for execution each cycle. In that work, empirical evaluation promises very modest IPC losses relative to a conventional scheduler, but we find in our own implementation that losses are severe — about sixfold higher than those originally reported. We show that those losses arise because the dependence-based steering policy, which is responsible for distributing instructions among the <b>FIFO</b> <b>buffers,</b> must frequently stall for lack of a suitable buffer into which the next instruction can be slotted. That problem, in turn, arises because dynamic dataflow is simply not of a shape that is amenable to being mapped onto just a few <b>FIFO</b> <b>buffers,</b> at least not by means of the proposed steering policy. We postulate that the original study produced good results because it evaluated the new scheduler in a machine whose performance bottleneck lay outside the execution core. The problems introduced by the steering logic were, as a result, hidden. ...|$|R
5000|$|The SPI 4.2 {{interface}} {{is composed}} of high speed clock, control, and data lines and lower speed <b>FIFO</b> <b>buffer</b> status lines. The high speed data line include a 16-bit data bus, a 1 bit control line and a double data rate (DDR) clock. The clock can run up to 500 MHz, supporting up to 1 GigaTransfer per second. The <b>FIFO</b> <b>buffer</b> status portion consists of a 2 bit status channel and a clock. SPI 4.2 supports a data width of 16 bits and can be PHY-link, link-link, link-PHY or PHY-PHY connection. The SPI 4.2 interface supports up to 256 port addresses with independent flow control for each.|$|E
50|$|The MOS Technology 8727 DMA {{was used}} on the A2090(A) ST-506/SCSI Controller and {{provides}} DMA management for the Konan DJC-002 (ST-506) and the WD33C93 SCSI controllers with byte to word funnelling and a 64 byte <b>FIFO</b> <b>buffer.</b>|$|E
50|$|The useful {{property}} of a circular buffer {{is that it}} does not need to have its elements shuffled around when one is consumed.(If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.)In other words, the circular buffer is well-suited as a <b>FIFO</b> <b>buffer</b> while a standard, non-circular buffer is well suited as a LIFO buffer.|$|E
40|$|We {{present a}} novel {{approach}} to model inter-processor communication in multi-DSP systems. In most multi-DSP systems, inter-processor communication is realized by transferring data over point-to-point links with hardware <b>FIFO</b> <b>buffers.</b> Direct memory access (DMA) is additionally used to concurrently transfer data to the <b>FIFO</b> <b>buffers</b> and perform computation. Our model accounts for the limited size of the communication buffers as well as concurrent DMA transfer. This novel communication model is applied in our rapid prototyping environment for optimizing multi-DSP systems. Given an extended data ow graph of the DSP application and {{a description of the}} target multi-processor system, our rapid prototyping environment automatically maps the DSP application onto the multi-processor system and generates a schedule for each processor...|$|R
30|$|It {{contains}} {{functions for}} {{reading and writing}} data to the configuration registers of Nordic RF module using SPI, setting the RF module as transmitter or receiver, flashing, and checking the status of internal Fast in, Fast out (<b>FIFO)</b> <b>buffers,</b> sending and receiving data packets to/from other RF module, taking the module to power down and up mode, etc.|$|R
40|$|In {{this paper}} {{we present a}} simple {{compositional}} Hoare logic for reasoning about the correctness of a certain class of distributed systems. We consider distributed systems composed of processes which interact asynchronously via unbounded <b>FIFO</b> <b>buffers.</b> The simplicity of the proof system {{is due to the}} restriction to local nondeterminism in the description of the sequential processes of a system. To illustrate the usefulness of the proof system we use PVS (Prototype Verification System, see [ORS 92]) to prove in a compositional manner the correctness of a heartbeat algorithm for computing the topology of a network. 1 Introduction In [dBvH 94] we have shown that a certain class of distributed systems composed of processes which communicate asynchronously via (unbounded) <b>FIFO</b> <b>buffers,</b> can be proved correct using a simple compositional proof system based on Hoare-logic. The class of systems introduced in [dBvH 94] is characterized by the restriction to deterministic control structures in the d [...] ...|$|R
5000|$|One {{drawback}} of {{the earlier}} 8250 UARTs and 16450 UARTs was that interrupts were generated for each byte received. This generated high rates of interrupts as transfer speeds increased. More critically, with only a 1-byte buffer there is a genuine risk that a received byte will be overwritten if interrupt service delays occur. To overcome these shortcomings, the 16550 series UARTs incorporated a 16-byte <b>FIFO</b> <b>buffer</b> with a programmable interrupt trigger of 1, 4, 8, or 14 bytes.|$|E
5000|$|It is a {{standard}} feature for a UART to store the most recent character while receiving the next. This [...] "double buffering" [...] gives a receiving computer an entire character transmission time to fetch a received character. Many UARTs have a small first-in, first-out <b>FIFO</b> <b>buffer</b> memory between the receiver shift register and the host system interface. This allows the host processor even more time to handle an interrupt from the UART and prevents loss of received data at high rates.|$|E
50|$|A traffic shaper {{works by}} delaying metered traffic such that each packet {{complies}} with the relevant traffic contract. Metering may be implemented with for example the leaky bucket or token bucket algorithms (the former typically in ATM and the latter in IP networks). Metered packets or cells are then stored in a <b>FIFO</b> <b>buffer</b> for each separately shaped class, until they can be transmitted {{in compliance with the}} prevailing traffic contract. This may occur immediately (if the traffic arriving at the shaper is already compliant), after some delay (waiting in the buffer until its scheduled release time) or never (in case of buffer overflow).|$|E
40|$|Many {{protocols}} {{are designed}} to operate correctly even in the case where the underlying communication medium is faulty. To capture the behaviour of such protocols, lossy channel systems (LCS) [AJ 96 b] have been proposed. In an LCS the communication channels are modelled as <b>FIFO</b> <b>buffers</b> which are unbounded, but also unreliable {{in the sense that}} they can nondeterministically lose messages. Recently, several attempts [...] ...|$|R
40|$|In this paper, {{we present}} two novel methodologies for testing the {{interconnect}} fabrics of network-on-chip (NoC) based chips. Both use {{the concept of}} recursive testing, with different degrees of parallelism in each case. Our test methodologies cover the logic switching blocks and the <b>FIFO</b> <b>buffers</b> that are the basic components of NoC fabrics. The paper concludes with test time evaluations for different NoC topologies and sizes. 1. Introduction an...|$|R
50|$|The two {{processors}} communicated {{through four}} pairs of <b>FIFO</b> <b>buffers</b> in the Tube ULA. Console input/output, error messages, data transfers and system calls each {{had their own}} pair of buffers, one for each direction. The queue capacity varied between 1 and 24 bytes, depending on the dedicated buffer function. Each buffer had a control register and status register to monitor its state and configure the raising of interrupts.|$|R
50|$|The leaky bucket as a queue is {{essentially}} a way of describing a simple <b>FIFO</b> <b>buffer</b> or queue that is serviced at a fixed rate to remove burstiness or jitter. A description of it is given by Andrew S. Tanenbaum, in his book Computer Networks as “The leaky bucket consists of a finite queue. When a packet arrives, if there is room on the queue it is appended to the queue; otherwise it is discarded. At every clock tick one packet is transmitted (unless the queue is empty)”. An implementation of the leaky bucket as a queue is therefore always a form of traffic shaping function.|$|E
50|$|Jet 3.0 {{included}} many enhancements, {{including a new}} index structure that reduced storage size and the time that was taken to create indices that were highly duplicated, the removal of read locks on index pages, a new mechanism for page reuse, a new compacting method for which compacting the database resulted in the indices being stored in a clustered-index format, a new page allocation mechanism to improve Jet's read-ahead capabilities, improved delete operations that speeded processing, multithreading (three threads were used to perform read ahead, write behind, and cache maintenance), implicit transactions (users {{did not have to}} instruct the engine to start manually and commit transactions to the database), a new sort engine, long values (such as memos or binary data types) were stored in separate tables, and dynamic buffering (whereby Jet's cache was dynamically allocated at start up and had no limit and which changed from a first in, first out (<b>FIFO)</b> <b>buffer</b> replacement policy to a least recently used (LRU) buffer replacement policy). Jet 3.0 also allowed for database replication.Jet 3.0 was replaced by Jet 3.5, which uses the same database structure, but different locking strategies, making it incompatible with Jet 3.0.|$|E
50|$|The company {{developed}} two {{chips to}} facilitate displaying the computer’s output {{on a standard}} color television set. The UM1 chip controlled sixteen rectangular objects on the screen that could be manipulated in size and shape, placement on the screen, and image within the rectangle. Software could designate {{the color of the}} image and of the remaining space within the rectangle, usually the background color of the display which was also software selectable. The UM1 was in turn controlled by an F8 processor. The UM1 fed a stream of pixels into a <b>FIFO</b> <b>buffer</b> which passed them along to be converted into signals that were delivered to the RF antenna input of a television set. Because the UM1 delivered the pixels needed for every line on the raster scan of the TV, rather than using the same pixel stream for every two or more adjacent lines as in then current video games, the VideoBrain produced finer display resolution than most television sets could support. The UM1 chip (Patent #4,232,374) was designed by John Cosley and Len Chen under the direction of Dr. Chung. A second chip, the UM2, was developed to serve as a clock for the entire system and to produce the NTSC (USA) and PAL (Europe) scanning video frames for the TV set. The PAL version of the UM2 was never manufactured or brought to market. Though a much simpler chip than the UM1, getting the UM2 into manufacturing was difficult because any flaw in timing, even once in millions of cycles, could bring the entire system down.|$|E
40|$|AbstractWe have {{previously}} proposed a clock distribution and data synchronization scheme {{to address a}} problem of jitter accumulated in Large Scale SFQ circuits such as Reconfigurable Data Paths processor (RDP). The RDP is divided into several stages clocked separately by an external jitter free system clock and <b>FIFO</b> <b>buffers</b> and clock controllers between the stages are used to synchronize data. In this paper we present architecture and experimental results of an RDP prototype that employed Operand Routing Network (ORN) and clock control and data synchronization scheme designed for ISTECSRL 10 kA/cm 2 advanced process. The circuit consisted of the ORN with 3 data inputs and maximum connection length equal to 1, three 8 -bit input and six 10 -bit output dual <b>FIFO</b> <b>buffers,</b> a 3 -bit controller, a ladder type high frequency clock signal generator and six 8 -bit output shift registers. Total it employed 6536 Josephson junctions and required a bias current of 0. 74 A. The prototype was successfully tested at the frequencies up to 48 GHz...|$|R
40|$|Abstract – In {{this paper}} {{we present a}} novel design {{approach}} that combines the advantages of on-chip switched networks (OCSNs) and the globally asynchronous, locally synchronous (GALS) design methodology using the mechanism of asynchronous <b>FIFO</b> <b>buffers.</b> Our proposed two GALS OCSN models were synthesized with 0. 25 µm Chip Express structured ASIC library. Comparative simulations were performed for these two platforms using three different mixed-clock test scenarios to show functional verification. Performance results are given...|$|R
40|$|We {{provide an}} {{analytical}} {{proof that the}} departure rate of a CBR flow at an overloaded link with <b>FIFO</b> <b>buffers</b> {{is proportional to the}} flow’s share of the total offered load at the link. This property of FIFO scheduling was recently validated in [1] in a series of traffic measurement experiments. An extension of the analysis to a multi-node scenario shows that the output rate of a flow in a network with many overloaded FIFO switches approaches the pessimistic values given by blind multiplexing...|$|R
30|$|The GM is {{responsible}} for the transformation and lighting, culling, and clipping of 3 D graphics geometry operations. The TDM is implemented using a tile-based concept [30] so as {{to reduce the number of}} memory accesses. The TDM creates tile list data in the 2 D vertex buffer for the RE. Output data is passed to the tile divider module, which builds a tiled triangle list for the RE. The GM has three pipeline stages, each about 16 cycles in length. A first-in-first-out (<b>FIFO)</b> <b>buffer</b> is needed between the GM and the TDM because of their different speeds. If the <b>FIFO</b> <b>buffer</b> becomes full, then the GM will stall.|$|E
30|$|When {{magnetic}} cancelation at {{the sensor}} is established, the output from the integrator represents the external magnetic field. The output is {{converted into a}} 20 -bit digital signal by the analog-to-digital conversion (ADC) circuit, which consists of a delta-sigma modulator and a digital filter programed in the FPGA. The FPGA stores the filtered magnetic field (mission) data in a first-in first-out (<b>FIFO)</b> <b>buffer</b> with time stamps. Sensor temperature data are obtained from a different ADC in the temperature measurement board. The FPGA transfers the mission data stored in the <b>FIFO</b> <b>buffer</b> to the CPU, together with the HK data. The HK data include the sensor temperature, parameters used to operate the magnetometer board (synchronous timing for the phase detector and range information), and Spacewire connection status.|$|E
3000|$|... [...]. Store each {{codeword}} in a first-in-first-out (<b>FIFO)</b> <b>buffer</b> {{of length}} n. A multiplexer {{is used to}} choose a symbol at each transmission time i ∈ [1, n] {{from one of the}} FIFO buffers according to the state s R,i(k). Then, the chosen symbol is transmitted.|$|E
40|$|Existing {{temporal}} {{analysis and}} buffer sizing techniques for real-time stream processing applications ignore that <b>FIFO</b> <b>buffers</b> bound interference between tasks {{on the same}} processor. By considering this effect it can be shown that a reduction of buffer capacities {{can result in a}} higher throughput. However, the relation between buffer capacities and throughput is non-monotone in general, which makes an exploitation of the effect challenging. In this paper a buffer sizing approach is presented which exploits that <b>FIFO</b> <b>buffers</b> bound interference between tasks on shared processors. The approach combines temporal analysis using a cyclic dataflow model with computation of buffer capacities in an iterative manner and thereby enables higher throughput guarantees at smaller buffer capacities. It is shown that convergence of the proposed analysis flow is guaranteed. The benefits of the presented approach are demonstrated using a WLAN 802. 11 p transceiver application executed on a multiprocessor system with shared processors. If buffers without blocking writes are used an up to 25 % higher guaranteeable throughput and up to 23 % smaller buffer capacities can be determined compared to existing approaches. For systems using buffers with blocking writes the guaranteeable throughput is even up to 43 % higher and buffer capacities up to 11 % smaller...|$|R
40|$|International audienceThis paper {{introduces}} a new semantics for <b>FIFO</b> <b>buffers</b> (more usually called channels) {{in a parallel}} programming language, B(PN) ². This semantics is given in terms of M-nets, which form an algebra of labelled high-level Petri nets. The proposed approach makes usage of asynchronous link operator, newly introduced in the algebra of M-nets, and repairs some drawbacks of the previous M-net semantics. Channels are now fully expressible within the algebra (it was not the case), they are significantly smaller (in number of places), and they offer several other advantages...|$|R
40|$|This paper {{presents}} {{a model for}} real-time computer vision applications based on the synchronous data flow (SDF) methodology. To accomplish the needs of computer vision applications (mainly access to pixel neighborhood and access to previous video frames) a new buffer concept called Structured <b>Buffers</b> replaces the <b>FIFO</b> <b>buffers</b> used in the SDF model. The model minimizes the system's latency and memory consumption and allows an execution time analysis which is an essential prerequisite for real-time applications. Finally a line-based edge vectorization is presented {{as an example of}} the usage of the presented model...|$|R
40|$|Variable-rate, {{asynchronous}} data signals from {{up to four}} measuring instruments or other sources combined in first-in/first-out (<b>FIFO)</b> <b>buffer</b> for transmission on single channel. Constructed in complementary metal-oxide-semiconductor (CMOS) logic, buffer consumes low power (only 125 mW at 5 V) and conforms to aerospace standards of reliability and maintainability...|$|E
30|$|First, the Win module {{sends the}} vector {{elements}} to the multiplier, and signals the SVM_v 2 module that the sparse vector operation is initiated. Next, {{if the results}} are valid, then the valid signal is asserted to start incrementing the counter, and to start loading the results to the <b>FIFO</b> <b>buffer.</b> In this case, the counter is incremented if the multiplier valid signal is asserted (high), the counter is decremented if the adder valid signal is asserted; and the counter is on hold if both the valid signals are asserted or de-asserted (low) simultaneously. The <b>FIFO</b> <b>buffer</b> is used to bridge the latency between the multiplier and the adder. If the count is 1, the SVM_v 2 module forwards the multiplication results to the output, by-passing the adder.|$|E
30|$|CTN: A CTN emulates a CAN-based device. Each of the CTN is {{realized}} {{by a small}} Nios II softcore CPU system and a CAN controller to generate or consume CAN messages. The 8 byte payload of each generated CAN message contains a timestamp that is set shortly before the CAN message is transferred to the CAN controller. In case a CAN message transmit attempt fails (i.e., another CAN message with a higher priority is sent simultaneously), the CAN controller will retry sending indefinitely until it succeeds. Each CAN controller has a transmit <b>FIFO</b> <b>buffer</b> with a length of 30 messages. In case the <b>FIFO</b> <b>buffer</b> is overrun, new messages are lost. All CTNs are also connected to the TTNoC; specifically they can receive configuration messages from the MU.|$|E
40|$|Technical report 2001 - 04, LACLThis paper {{introduces}} a new semantics for <b>FIFO</b> <b>buffers</b> (usually called channels) {{in a parallel}} programming language, B(PN) ². This semantics is given in terms of M-nets, which form an algebra of labelled high-level Petri nets. The proposed approach uses the asynchronous link operator, newly introduced in the algebra of M-nets, and repairs some drawbacks of the original M-net semantics. Channels are now fully expressible within the algebra (it was not the case), they are significantly smaller (in number of places), and they offer several other advantages...|$|R
50|$|Modems for {{personal}} computers that plug into a motherboard slot must {{also include the}} UART function on the card. The original 8250 UART chip shipped with the IBM personal computer had a one character buffer for the receiver and the transmitter each, which meant that communications software performed poorly at speeds above 9600 bits/second, especially if operating under a multitasking system or if handling interrupts from disk controllers. High-speed modems used UARTs that were compatible with the original chip but which included additional <b>FIFO</b> <b>buffers,</b> giving software additional time to respond to incoming data.|$|R
3000|$|... is {{the set of}} {{buffered}} tokens in the channel. Tokens in channels {{represent the}} flow of control as well as flow of data in the application. A token carries certain amount of data from task to another. This has two impacts. First, the load on the communication medium for {{the time of the}} transfer. Second, the execution load when the next task is triggered after reception. Latter enables data amount-dependent dynamic variations in execution of application tasks. Similar to traditional KPN model, channels between tasks (or processes) are uni-directional, unbounded <b>FIFO</b> <b>buffers</b> and tasks use a blocking read as a synchronization mechanism.|$|R
