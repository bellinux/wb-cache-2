12|214|Public
2500|$|When {{the user}} clicks the [...] "Install" [...] button {{in a typical}} MSI {{installation}} wizard, installation proceeds to the <b>Execute</b> <b>phase,</b> in which software components are actually installed. The <b>Execute</b> <b>phase</b> makes system changes, {{but it does not}} display any user interface elements.|$|E
50|$|Step 4 of the Instruction Cycle is the Execute Cycle. Here, the {{function}} of the instruction is performed. If the instruction involves arithmetic or logic, the Arithmetic Logic Unit is utilized. This is the only stage of the instruction cycle that is useful {{from the perspective of the}} end user. Everything else is overhead required to make the <b>execute</b> <b>phase</b> happen.|$|E
50|$|The {{construction}} of the new terminal, as well as runway expansion {{marked the end of}} Phase I in the project. AAI officials have announced that they are prepared to <b>execute</b> <b>Phase</b> 2 of the Kolkata Airport expansion plan. This primarily involves around the {{construction of}} an 157-meter ATC Tower to provide controllers with a better view of the planes at the new terminal. The building will be accompanied by a sprawling 4-storey office complex.|$|E
50|$|All {{three games}} {{share the same}} concept; turns are divided into a {{planning}} and an <b>executing</b> <b>phase.</b> While the planning phase can, in single player mode, {{last as long as}} the player needs to give orders to all their units, the <b>executing</b> <b>phase</b> always lasts 60 seconds of real-time. Both sides, either computer or another human, enter their orders before the execution phase takes place. This is known as the Wego system. During the execution phase, units carry out their orders, but the player cannot influence the result and is limited to watch, replay and move the camera. All games offer to play individual battles (ranging from 15 to 60 turns, or 120 turns in Barbarossa to Berlin and Afrika Korps) or operations, linking a series of battles. See Scenarios below for more information.|$|R
30|$|After {{training}} a TNFN, the <b>executing</b> <b>phase</b> {{of the proposed}} image alignment system merely consists of computing the WGOH descriptor and then feeding it into the DMELA-trained TNFN to get a scaling factor s, a rotation angle θ, and translation parameters (Δx, Δy). About this, the proposed system is simple and efficient.|$|R
50|$|Pipelining, {{in which}} {{different}} hardware in the CPU <b>executes</b> different <b>phases</b> of multiple instructions simultaneously.|$|R
30|$|At the <b>execute</b> <b>phase</b> {{we have a}} Firewall rule {{application}} engine component, {{which is}} responsible for effecting the new firewall rules on the servers. This component must take in consideration mechanisms for guaranteeing secure communication with each server, and configuration management techniques.|$|E
40|$|The end of Dennard scaling is {{expected}} to shrink the range of DVFS in future nodes, limiting the energy savings of this technique. This paper evaluates how much we can increase the effectiveness of DVFS by using a software decoupled access-execute approach. Decoupling the data access from execution allows us to apply optimal voltage-frequency selection for each phase and therefore improve energy efficiency over standard coupled execution. The underlying insight of our work is that by decoupling access and execute we {{can take advantage of}} the memory-bound nature of the access phase and the compute-bound nature of the <b>execute</b> <b>phase</b> to optimize power efficiency, while maintaining good performance. To demonstrate this we built a task based parallel execution infrastructure consisting of: (1) a runtime system to orchestrate the execution, (2) power models to predict optimal voltage-frequency selection at runtime, (3) a modeling infrastructure based on hardware measurements to simulate zero-latency, per-core DVFS, and (4) a hardware measurement infrastructure to verify our model's accuracy. Based on real hardware measurements we project that the combination of decoupled access-execute and DVFS has the potential to improve EDP by 25 % without hurting performance. On memory-bound applications we significantly improve performance due to increased MLP in the access phase and ILP in the <b>execute</b> <b>phase.</b> Furthermore we demonstrate that our method can achieve high performance both in presence or absence of a hardware prefetcher. LPGPU FP 7 -ICT- 288653 UPMAR...|$|E
40|$|Energy-efficiency plays a {{significant}} role given the battery lifetime constraints in embedded systems and hand-held devices. In this work we target the ARM big. LITTLE, a heterogeneous platform that is dominant in the mobile and embedded market, which allows code to run transparently on different microarchitectures with individual energy and performance characteristics. It allows to se more energy efficient cores to conserve power during simple tasks and idle times and switch over to faster, more power hungry cores when performance is needed. This proposal explores the power-savings and the performance gains that can be achieved by utilizing the ARM big. LITTLE core in combination with Decoupled Access-Execute (DAE). DAE is a compiler technique that splits code regions into two distinct phases: a memory-bound Access phase and a compute-bound <b>Execute</b> <b>phase.</b> By scheduling the memory-bound phase on the LITTLE core, and the compute-bound phase on the big core, we conserve energy while caching data from main memory and perform computations at maximum performance. Our preliminary findings show that applying DAE on ARM big. LITTLE has potential. By prefetching data in Access we can achieve an IPC improvement of up to 37 % in the <b>Execute</b> <b>phase,</b> and manage to shift {{more than half of the}} program runtime to the LITTLE core. We also provide insight into advantages and disadvantages of our approach, present preliminary results and discuss potential solutions to overcome locking overhead. Comment: Presented at HIP 3 ES, 201...|$|E
40|$|While {{developing}} {{a large scale}} distributed system aimed at automating supply chain distribution facility the author came across the problem of enabling sharing unique equipment between two simultaneously <b>executed</b> <b>phases</b> of the project [...] test and production. This article summarises the experience gained by the author {{in the area of}} integration heterogeneous distributed large-scale systems being constrained by time and some limitations of the systems to be integrated such as predefined synchronous model of communication...|$|R
50|$|HEFT <b>executes</b> in two <b>phases.</b>|$|R
30|$|As {{previously}} mentioned, we {{employ the}} Puppet configuration management language for describing servers’ configurations. Puppet provides tools for applying configurations, and for obtaining {{the current status}} of a host. A Puppet agent component runs on each host, and reports to (and receive commands from) the puppet-master component, which stores servers description into the Puppet catalog. Hence, Puppet agents fulfill the roles of sensor and effector of servers, while the Puppet master is responsible for the monitor and <b>execute</b> <b>phases</b> of the MAPE-K.|$|R
40|$|This work {{demonstrates}} {{the potential of}} hardware and software optimization to improve theeffectiveness of dynamic voltage and frequency scaling (DVFS). For software, we decouple data prefetch (access) and computation (execute) to enable optimal DVFS selectionfor each phase. For hardware, we use measurements from state-of-the-art multicore processors to accurately model the potential of per-core, zero-latency DVFS. We demonstrate that the combinationof decoupled access-execute and precise DVFS {{has the potential to}} decrease EDP by 25 - 30 % without reducing performance. The underlying insight in this work is that by decoupling access and execute we can take advantageof the memory-bound nature of the access phase and the compute-bound nature of the <b>execute</b> <b>phase</b> to optimize power efficiency. For the memory-bound access phase, where we prefetch data into the cachefrom main memory, we can run at a reduced frequency and voltage without hurting performance. Thereafter, the <b>execute</b> <b>phase</b> can run much faster, thanks to the prefetching of the access phase, and achieve higher performance. This decoupled program behavior allows us to achieve more effective use of DVFS than standard coupled executions which mix data access and compute. To understand the potential of this approach, we measure application performance and power consumption on a modern multicore system across a range of frequencies and voltages. From this data we build a model that allows us to analyze the effects of per-core, zero-latency DVFS. The results of this work demonstrate the significant potential for finer-grain DVFS in combination with DVFS-optimized software...|$|E
40|$|Abstract. The {{trend towards}} in-memory {{analytics}} and CPUs with {{an increasing number}} of cores calls for new algorithms that can efficiently utilize the available resources. This need is particularly evident in the case of CPU-intensive query operators. One example of such a query with applicability in data analytics is the skyline query. In this paper, we present APSkyline, a new approach for multicore skyline query processing, which adheres to the partition-execute-merge framework. Contrary to existing research, we focus on the partitioning phase to achieve significant performance gains, an issue largely overlooked in previous work in multicore processing. In particular, APSkyline employs an angle-based partitioning approach, which increases the degree of pruning that can be achieved in the <b>execute</b> <b>phase,</b> thus significantly reducing the number of candidate points that need to be checked in the final merging phase. APSkyline is extremely efficient for hard cases of skyline processing, as in the cases of datasets with large skyline result sets, where it is meaningful to exploit multicore processing. ...|$|E
40|$|The IVBSS {{program is}} a four-year, two phase {{cooperative}} research program being conducted by an industry team led by the University of Michigan Transportation Research Institute (UMTRI). The program began in November 2005 and will continue through December 2009 if results from vehicle verification tests conducted {{in the second year}} of the program indicate that the prototype system meets its performance guidelines and is safe for use by lay drivers in a field operational test planned for July 2008. The decision to <b>execute</b> <b>Phase</b> II of the program will take place in December 2007. The goal of the IVBSS program is to assess the safety benefits and driver acceptance associated with a prototype integrated crash warning system designed to address rear-end, road departure and lane change/merge crashes on light vehicles and heavy commercial trucks. This report describes accomplishments and progress made {{during the first year of}} the program (November 2005 -December 2006). Activities during the first year focused on system specification, design and development and construction of the prototype vehicles...|$|E
40|$|Introduction The {{most natural}} way of {{programming}} parallel machines is data-parallelism where programs <b>execute</b> <b>phases</b> of computations and communications on {{different sets of}} data and no overlap exists between communications and computations. Moreover, communication phases are synchronous, i. e. every processor <b>executes</b> these <b>phases</b> {{at the same time}} and waits until the last processor completes his communication phase. From the perspective of program correctness, these data-parallel programs are much more easier to prove than asynchronous CSP-based parallel programs [Hoa 85]. Unfortunately, performances of such programs are bounded by the communications cost. To avoid this problem, the solution is to overlap communications by computations. This is not always possible because of the dependences within the code. If data dependences prevent the use of simple overlap, a solution consists in using a pipelined data parallel algorithm by decreasing the grain of computations, and overlapp...|$|R
40|$|Abstract — Armed forces conduct {{operations}} within operational environments {{characterized by}} complexity, uncertainty, and continuous change. Military planners use planning and decision-making processes {{to cope with}} this confusion. This paper asserts that with changing operational environment {{there is a need to}} separate the planning in advance and planning in crises and <b>executing</b> <b>phase.</b> Our study states that the analytical process of decision-making does not provide the necessary means to respond effectively to crises response planning and decision-making and offers the implementation of intuitive process with some modifications...|$|R
5000|$|In September 1983, Chamberlain and Burchfield <b>executed</b> <b>Phase</b> V, [...] "Primary Light Documentation,” {{requiring}} 65 designated participants (plus backup) obtained 13 {{vehicles and}} a 30,000-watt generator, towed by a 40-foot flatbed truck. They moved this entire caravan down Laguna Canyon Road from 6 PM to 6 AM, while escorted by three different police agencies and Caltrans officials. The resulting images were printed onto a single print 3.5 inches wide by 516 feet long, depicting {{the entire length}} of the Northeast side of the road in kaleidoscopic color.|$|R
40|$|Decoupled Access-Execute (DAE) {{presents}} {{a novel approach}} to improve power efficiency {{with a combination of}} compile-time transformations and Dynamic Voltage Frequency Scaling (DVFS). DAE splits regions of the program into two distinct phases: a memory-bound access phase and a compute-bound <b>execute</b> <b>phase.</b> DVFS is used to run the phases at different frequencies, thus conserving energy while caching data from main memory and performing computations at maximum performance. This project analyses the power-savings and performance impact of DAE on the ARM architecture for the first time, a platform that is ominant in the mobile market where battery life is of particular significance. We target ARM big. LITTLE specifically, a heterogeneous hardware platform where code can not only be executed at different frequencies, but also transparently on different microarchitectures with individual energy and performance characteristics. As a result, this architecture enables hardware support for new DAE concepts that have not been previously available. As a first step towards new DAE compiler passes, we demonstrate the methodology for a thread-based DAE implementation that specifically targets the hardware features of ARM big. LITTLE. Finally, we manually apply it to a selection of benchmarks and analyse how this DAE implementation performs on the Samsung Exynos 5422 big. LITTLE SoC and identify the strengths and limitations of the implementation on current hardware...|$|E
40|$|Introduction to the Canadian Oil Sands”, “Canada’s Oil Sand Industry: An Overview”, “Heavy Oil Technologies”, and so {{many other}} topics about heavy oil have become the hotcakes in the oil industry. A number of new {{projects}} are in <b>Execute</b> <b>phase</b> for the development of heavy oil assets. This clearly shows the increasing demand for heavy oil. An oil industry is working hard to meet the world oil demand by developing deep water, HPHT, heavy oil, shale sands and all other non-conventional reservoirs but the main challenge is to develop and operate them in a risk free environment. Understanding the reservoir and fluid properties and developing new technologies help the industry to reduce the risk in developing non-conventional fields. A major problem in heavy oil field is to understand the behaviour of heavy oil. The viscous oil flows sluggishly in the formations and hence it is difficult to transport through unconsolidated formations and is very difficult to produce by conventional methods. Viscous oil recovery entails neatly designed enhanced oil recovery processes like Steam Assisted Gravity Drainage and the success of such technologies are critically dependent on accurate knowledge of reservoir, well and fluid properties of oil under variety of pressure and temperature conditions. This research project has provided some solutions to the challenges in heavy oil field development and can help the oil industry to optimise heavy oil production. Detailed experimental understanding of PVT properties has allowed this project to contribute to the knowledge. Reservoir, well and fluid properties were studied thoroughly and demonstrated the criticality of each parameter on the efficiency of Steam Assisted Gravity Drainage. An user friendly SAGD simulator is a big output of this research which allows the user to optimise the heavy oil recovery and enables to do risk assessments quickly during design phase of SAGD. A SAGD simulator is developed. Northern Research Partnership (NRP) and IDEAS...|$|E
40|$|Many {{scientific}} applications {{can benefit}} from pipelining computation and communication. Our aim is to provide compiler and runtime support for High Performance Fortran applications that could benefit from these techniques. This paper describes the integration of a library for pipelined computations in the runtime system. Results on some application kernels are given. 1 Introduction With the introduction of High Performance Fortran (HPF) [KLS + 94], {{it is possible to}} use the data parallel programming paradigm in a very convenient way for scientific applications. With current compilation technology, these programs will <b>execute</b> <b>phases</b> of computations and communications on differents sets of data and no overlap exists between communications and computations. Moreover, communication phases are synchronous, i. e. each processor <b>executes</b> these <b>phases</b> {{at the same time and}} waits until the last processor completes his communication phase. An important task of the HPF compiler is to detect th [...] ...|$|R
40|$|Metamodel {{approaches}} to building visual environments are becoming {{common in the}} field of domain specific visual languages, mainly focusing on the definition of visual editors and of simulation environments. Recent efforts tackle the generation of complex interaction management both in the editing and in the <b>executing</b> <b>phases.</b> We present an approach to interaction specification which takes into account metamodel information both on the objects that can be manipulated and on the spatial relations among them. Interaction dynamics are defined through a visual, declarative and formal notation based on graph grammar...|$|R
5000|$|Battle Dex {{employs a}} [...] "Wego" [...] {{simultaneous}} execution system where turns {{are divided into}} a planning and <b>executing</b> <b>phase.</b> While the planning phase can, in single player mode, {{last as long as}} the player needs to give orders to all their units, turns in multiplayer games are usually limited to two minutes. Both sides, either computer or another human, enter their orders during the planning phase. During the execution phase, units carry out their orders, but the player cannot influence the result and is limited to watch, replay and move the camera.|$|R
40|$|This paper {{describes}} the features {{and implementation of}} our automatic data distribution research tool. The tool (DDT) accepts programs written in Fortran 77 and generates HPF directives and executable statements. DDT works by identifying a set of computational phases (procedures and loops). The algorithm builds a search space of candidate solutions for these phases which is explored looking for their combination that minimize the overall cost; this cost includes movement cost and computation cost. The data movement cost includes the cost of <b>executing</b> each <b>phase</b> with a given mapping and the remapping costs {{that have to be}} paid in order to <b>execute</b> each <b>phase</b> with the mapping selected. The computation cost includes the cost of <b>executing</b> each <b>phase</b> in parallel according to the mapping selected and the owner computes rule. Control flow information is used to identify how phases are sequenced during the execution of the application. 1 Introduction Data distribution is one of the topics of [...] ...|$|R
50|$|As explained, the {{possibility}} to <b>execute</b> different <b>phases</b> of the implementation process iteratively enables the process to be executed by incrementally aligning the product to be implemented with the end-user (organization).|$|R
40|$|AbstractThis paper {{presents}} {{results of}} {{an analysis of the}} most relevant urban guided transportation projects of the last decades, to understand root causes of failures and relevant variances from previously estimated parameters. We collected information from all side of the project – history of the transportation network, social environment, technical choices – reading documentation and interviewing stakeholders. Major problems emerging during the <b>executing</b> <b>phases</b> turn out to be managerial and referable to project management areas. So we addressed to the modern theory of project management based on complexity, which seemed to be fit for urban transportation projects...|$|R
40|$|Dual-task {{paradigm}} {{was used}} to examine effects of task features (input, output and central processing) on integrated or separated operations. Results indicated that: (1) Performance of single task could be predicted by the principle of S-C-R compatibility, while performance of dual-task be predicted by the multiple resources theory of attention in tracking and spatial memory tasks. (2) Integration of the <b>executing</b> <b>phase</b> of the dual-task had little interference on performance compared to the condition in which the two tasks were executed separately. Possible mechanisms of the findings and effects of handedness on performance were discussed. IUPsy...|$|R
2500|$|Gas {{in there}} and <b>execute</b> the {{following}} <b>phases</b> of the project have been considered: ...|$|R
40|$|ABB Combustion Engineering, Inc. is one {{of three}} {{contractors}} <b>executing</b> <b>Phases</b> 1, 2 and 3 of the Department of Energy project entitled Engineering Development of Advanced Coal-Fired Low-Emission Boiler Systems (LEBS). Phase 1 has been completed and Phase 2 is scheduled for completion on September 30, 1996. The following major activities are being carried out in parallel in Phase 2 and this paper is a status report on this work: (1) in-furnace NOx reduction; (2) catalytic filter optimization; (3) add Kalina cycle to POCTF; and (4) POCTF design and licensing. The in-furnace NOx reduction work has been completed and, therefore, a description of this work comprises the major part of this paper...|$|R
40|$|Abstract Multi-processor {{system-on-chip}} (MPSoC) simulators {{are many}} {{orders of magnitude}} slower than the hardware they simulate due to increasing architectural com-plexity. In this paper, we propose a new application sampling technique to accelerate the simulation of MPSoC design space exploration (DSE). The proposed technique dy-namically combines simultaneously <b>executed</b> <b>phases,</b> thus generating a sampling unit. This technique accelerates the simulation by allowing the repeated combinations of parallel phases to be skipped. A complementary technique, called cluster synthesis, is also proposed to improve the simulation acceleration {{when the number of}} possible phase combinations increases. Our experimental results show that this technique can accelerate the simulation up to a factor of 800 with a relatively small estimation error. ...|$|R
50|$|Like the Hohmann {{transfer}}, both transfer orbits {{used in the}} bi-elliptic transfer constitute {{exactly one}} half of an elliptic orbit. This means that {{the time required to}} <b>execute</b> each <b>phase</b> of the transfer is half the orbital period of each transfer ellipse.|$|R
40|$|Provides {{methods for}} generating, {{exploring}} and <b>executing</b> seamless <b>Phase</b> II-III designs of Lai, La-vori and Shih using generalized likelihood ratio statistics. Includes pdf and source files that de-scribe the entire R implementation with the relevant mathematical details. Depends R (> = 3. 0), mvtnorm, surviva...|$|R
40|$|International audienceMulti-processor {{system-on-chip}} (MPSoC) simulators {{are many}} {{orders of magnitude}} slower than the hardware they simulate due to increasing architectural complexity. In this paper, we propose a new application sampling technique to accelerate the simulation of MPSoC design space exploration (DSE). The proposed technique dynamically combines simultaneously <b>executed</b> <b>phases,</b> thus generating a sampling unit. This technique accelerates the simulation by allowing the repeated combinations of parallel phases to be skipped. A complementary technique, called cluster synthesis, is also proposed to improve the simulation acceleration {{when the number of}} possible phase combinations increases. Our experimental results show that this technique can accelerate the simulation up to a factor of 800 with a relatively small estimation error. Keywords Simulation - MPSoC architectures - Application sampling - Performance evaluatio...|$|R
5000|$|Wasp (1957), {{novel by}} Eric Frank Russell. Earth, {{at war with}} the crypto-nazi Sirian Empire, is technologically {{superior}} but outnumbered and out-gunned by a factor of twelve-to-one. Virtuoso conniver James Mowry is recruited to infiltrate the Empire as a wasp: an undercover trickster whose pranks, ranging from the mischievous to the deadly, are to combine in a nine-phase plan that will first weaken and eventually destroy the enemy's morale and will to fight. Initiating his mission on the Sirian outpost world Jaimec ("From now on, he must be wholly a Sirian named Shir Agavan. Agavan was a forestry surveyor employed by the Jaimec Ministry of Natural Resources...") he <b>executes</b> <b>phase</b> one mildly enough, posting 80 stickers with subversive slogans all over town. But that's only the beginning...|$|R
5000|$|In Julia, Dylan and Fortress {{extensibility}} is the default, and the system's built-in {{functions are}} all generic and extensible. In Dylan, multiple dispatch is as fundamental {{as it is}} in Julia: all user-defined functions and even basic built-in operations like [...] are generic. Dylan's type system, however, does not fully support parametric types, which are more typical of the ML lineage of languages. By default, CLOS does not allow for dispatch on Common Lisp's parametric types; such extended dispatch semantics can only be added as an extension through the CLOS Metaobject Protocol. By convergent design, Fortress also features multiple dispatch on parametric types; unlike Julia, however, Fortress is statically rather than dynamically typed, with separate compiling and <b>executing</b> <b>phases.</b> The language features are summarized in the following table: ...|$|R
40|$|A {{project plan}} hardly {{reflects}} what actually happens during the project mainly because {{it tends to}} be static, while nowadays projects are extremely dynamic. Moreover, the strong integration between the executing and the planning phases makes the projects guidance very hard, especially under strict time boundaries. The existing project management techniques are quite inadequate to handle these features. For this reason, the paper proposes an innovative tool able to guarantee an improvement of projects guidance efficiency by introducing the simulation into the time-cost trade-off analysis. The model on the basis of which the tool has been developed uses in an integrated manner different operational software: Microsoft’s Project, Visual Basic for Application and Rockwell’s Arena. The tool has been tested on a construction project in progress and has already proved its usefulness in the planning {{as well as in the}} <b>executing</b> <b>phases...</b>|$|R
