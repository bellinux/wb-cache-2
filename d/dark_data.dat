30|186|Public
50|$|In an {{industrial}} context, <b>dark</b> <b>data</b> can include information gathered by sensors and telematics.|$|E
5000|$|... #Caption: It {{is claimed}} that the IBM Watson {{could be used for}} <b>dark</b> <b>data</b> {{analysis}} ...|$|E
5000|$|<b>Dark</b> <b>Data</b> (New)- Might {{not have}} noticed it because … it is ‘Dark Data’ ‘Big Data’ {{has begun to}} spin off its own superlatives.|$|E
30|$|The {{observational}} {{program of}} the photometer is written in C language. The observational procedure is as follows: (1) power on the photometer system with zero voltage applied to the PMTs and amplifiers, (2) obtain <b>dark</b> level <b>data</b> {{at the beginning of}} the observation for 20  min with high voltage (HV) loaded on the PMTs, (3) obtain sky data (i.e., measurements of auroral light), (4) once an hour, obtain <b>dark</b> level <b>data</b> for 10  s, (5) obtain <b>dark</b> level <b>data</b> for 10  min at the end of the observation, (6) apply zero voltage on the PMTs and amplifiers, and (7) power off the system. We obtained data at a 400  Hz sampling rate between September 6, 2017, and March 22, 2018, and at 20  Hz between February 1 and April 16, 2017. A shutter device was installed during the summer season (i.e., between May and August 2017); before then, we did not obtain <b>dark</b> level <b>data.</b>|$|R
5000|$|The DEAP-3600 {{detector}} {{started taking}} commissioning data in February 2015 with nitrogen gas purge in the detector. Liquid argon <b>dark</b> matter <b>data</b> {{is expected in}} early 2016.|$|R
5000|$|... #Caption: Cumulative energy {{released}} by all earthquakes in Southern California from 1932 through July 2014. Highest energy in red, lowest energy in <b>dark</b> blue. <b>Data</b> from the Southern California Earthquake Center. (Click on icon {{for a larger}} image.) ...|$|R
50|$|Organizations retain <b>dark</b> <b>data</b> for a {{multitude}} of reasons, and {{it is estimated that}} most companies are only analyzing 1% of their data. Often it is stored for regulatory compliance and record keeping. Some organizations believe that <b>dark</b> <b>data</b> could be useful to them in the future, once they have acquired better analytic and business intelligence technology to process the information. Because storage is inexpensive, storing data is easy. However, storing and securing the data usually entails greater expenses (or even risk) than the potential return profit.|$|E
5000|$|Many {{companies}} in the IT sector are looking at creating [...] "cognitive computer systems" [...] {{that are able to}} analyse unstructured <b>dark</b> <b>data.</b> The IBM Watson {{is considered to be a}} future system that would be able to analyse this unstructured data and be able to produce meaningful results that will utilise a lot of <b>dark</b> <b>data</b> that it is either practically impossible or very difficult to process at present. In terms of current systems, IBM have advertised the IBM Spark as a system that [...] "can extract insight from that information almost immediately. This enables businesses to build data rich products and services that use that information to transform the customer experience." [...] Furthermore, they also give an even broader definition of <b>dark</b> <b>data,</b> one that also includes data that is not currently processed by computing systems but could be, for example in law.|$|E
5000|$|A lot of <b>dark</b> <b>data</b> is unstructured, {{which means}} that the {{information}} is in formats that may be difficult to categorise, be read by the computer and thus analysed. Often the reason that business do not analyse their <b>dark</b> <b>data</b> is because of the amount of resources it would take and the difficulty of having that data analysed. According to Computer Weekly, 60% of organisations believe that their own business intelligence reporting capability is [...] "inadequate" [...] and 65% say that they have [...] "somewhat disorganised content management approaches".|$|E
40|$|The paper {{presents}} a new analytical method for extracting the diode ideality factor of a p-n junction device using Lambert W-function {{model and the}} <b>dark</b> current-voltage <b>data.</b> The extracted values {{are found to be}} in good agreement with those calculated experimentally from dark current-voltage charac-teristics. Key Words: Ideality factor; Lambert W-function; Solar cell. 1...|$|R
40|$|The {{commissioning}} of the VIRGO central interferometer occasioned {{the implementation}} and tests of various algorithms for {{the characterization of}} the non-Gaussianity, non-stationarity and non-linearity of the <b>dark</b> fringe <b>data.</b> This library of prototypes will serve as groundwork for the near commissioning of VIRGO (full scale). We make {{a summary of the}} activities on that subject including the description of the selected algorithms and some results obtained with the data of the engineering runs...|$|R
40|$|At the CHRIS/PROBA Workshop held {{last year}} in ESRIN a review was {{presented}} of various aspects associated with the CHRIS calibration [1]. The review addressed: first, the variation of the <b>dark</b> signal <b>data,</b> secondly, {{the use of the}} “generic ” dark-field data sets for correcting analogue electronic offsets and, thirdly, the influence of the platform temperature variations on instrument response and hence radiometric calibration. It was concluded, first, that the errors introduced by gradual increase in dark signal through the mission life were negligible. Secondly, the use of the “generic ” <b>dark</b> image <b>data</b> for correcting analogue offsets is probably acceptable for most land applications, where the scene albedo is relatively high. However, improvements in absolute offset errors may be worth pursuing for coastal scenes where the scene albedos are low, accepting that some images may experience saturation when imaging bright targets. Lastly, the temperature-induced pixel shifts significantly affect radiometric calibration and this effect required modelling within the image processing software. This paper reviews the changes that have been made to the data processing, following on from last year’s conclusions, and also addresses other information that has been added to the CHRIS data sets...|$|R
5000|$|It is {{generally}} considered that as more advanced computing systems for analysis of data are built, the higher the value of <b>dark</b> <b>data</b> will be. It has been noted that [...] "data and analytics will be {{the foundation of the}} modern industrial revolution". Of course, this includes data that is currently considered [...] "dark data" [...] since there are not enough resources to process it. All this data that is being collected can be used in the future to bring maximum productivity and an ability for organisations to meet consumers' demand. Furthermore, many organisations do not realise the value of <b>dark</b> <b>data</b> right now, for example in healthcare and education organisations deal with large amounts of data that could create a significant [...] "potential to service students and patients {{in the manner in which}} the consumer and financial services pursue their target population".|$|E
50|$|<b>Dark</b> <b>data</b> is data {{which is}} {{acquired}} through various computer network operations but {{not used in}} any manner to derive insights or for decision making. The ability of an organisation to collect data can exceed the throughput at which it can analyse the data. In some cases the organisation {{may not even be}} aware that the data is being collected. IBM estimate that roughly 90 percent of data generated by sensors and analog-to-digital conversions never get used.|$|E
5000|$|According to the New York Times, 90% {{of energy}} used by data centres is wasted. If data was not stored, energy costs could be saved. Furthermore, there are costs {{associated}} with the underutilisation of information and thus missed opportunities. According to Datamation, [...] "the storage environments of EMEA organizations consist of 54 percent <b>dark</b> <b>data,</b> 32 percent Redundant, Obsolete and Trivial data and 14 percent business-critical data. By 2020, this can add up to $891 billion in storage and management costs that can otherwise be avoided." ...|$|E
30|$|Following {{the data}} flow along the {{streaming}} pipeline, the starting trigger {{comes from the}} control interface which initiates a new ptychographic scan providing information about the scan (step size, scan pattern, number of scan points) and other relevant information (e.g., wavelength) to the back-end handler. Simultaneously, the control sends triggers to the scanning motors and the FCCD. A typical ptychographic scan combines the accumulation of a given number of dark frames together with scanning the sample in a region of interest. The frame-grabber, already waiting for raw data packets to arrive, assembles the data and sends it frame-by-frame to the back-end handler. When dealing with an acquisition control system that runs independently, the handler can distinguish between <b>dark</b> and <b>data</b> frames using counters. <b>Dark</b> and <b>data</b> frames are distributed to the corresponding workers. Having clean diffraction frames and an initial guess for the illumination ready, the SHARP worker is able to start the iterative reconstruction process. SHARP initializes and allocates space to hold all frames in a scan, computes a decomposition scheme, initializes the image and starts the reconstruction process. Unmeasured frames are either set to a bright-field frame (measured by removing the sample) or their weight is set to 0 until real data are received.|$|R
5000|$|... #Caption: Energy {{consumption}} in kilograms of oil equivalent (kgoe) {{per person per}} year per country (2001 <b>data).</b> <b>Darker</b> tones indicate larger consumption (dark grey areas are missing from the dataset). Red hue indicates increasing consumption, green hue indicates decreasing consumption, in the time between 1990 and 2001.|$|R
40|$|Radiochemical neutron {{concentration}} {{analysis was}} used to determine Ag, Au, Bi, Cd, Co, Cs, Ga, In, Rb, Sb, Se, Te, Tl, and Zn contents in consortium samples of solar gas and track-rich <b>dark</b> matrix. <b>Data</b> obtained confirmed that the matrix is chemically homogeneous. Most dark clasts had similar mean trace element concentrations as the matrix but were compositionally much more heterogeneous. It is concluded that material incorporated with soil and compacted to form the dark portion of Fayetteville included soil-melt, the odd C 2 M fragment, and dark and light clasts which escaped comminution...|$|R
5000|$|Steganography is a {{technique}} where information or files are hidden within another file {{in an attempt to}} hide data by leaving it in plain sight. [...] "Steganography produces <b>dark</b> <b>data</b> that is typically buried within light data (e.g., a non-perceptible digital watermark buried within a digital photograph)." [...] Some experts have argued that the use of steganography techniques are not very widespread and therefore shouldn't be given a lot of thought. Most experts will agree that steganography has the capability of disrupting the forensic process when used correctly.|$|E
5000|$|The [...] "lights-out" [...] data center, {{also known}} as a darkened or a <b>dark</b> <b>data</b> center, is a data center that, ideally, has all but {{eliminated}} the need for direct access by personnel, except under extraordinary circumstances. Because of the lack of need for staff to enter the data center, it can be operated without lighting. All of the devices are accessed and managed by remote systems, with automation programs used to perform unattended operations. In addition to the energy savings, reduction in staffing costs and the ability to locate the site further from population centers, implementing a lights-out data center reduces the threat of malicious attacks upon the infrastructure.|$|E
5000|$|Useful {{data may}} become <b>dark</b> <b>data</b> after it becomes irrelevant, {{as it is}} not {{processed}} fast enough. This is called [...] "perishable insights" [...] in [...] "live flowing data". For example, if the geolocation of a customer is known to a business, the business can make offer based on the location, however if this data is not processed immediately, it may be irrelevant in the future. According to IBM, about 60 percent of data loses its value immediately. Not analysing data immediately and letting it go 'dark' can lead to significant losses for an organisation in terms of not identifying fraud, for example, fast enough and then only addressing the issue when it is too late.|$|E
40|$|The XENON 100 {{experiment}} aims at detecting cold {{dark matter}} particles via their collisions with xenon nuclei in a two-phase time projection chamber filled {{with a total of}} 165 kg of ultra pure liquid xenon. The detector sensitive target mass is about 65 kg, surrounded by about 100 kg of active veto. The detector has been installed underground at the Gran Sasso National Laboratory (LNGS) since 2008 and after a successful calibration, <b>dark</b> matter <b>data</b> taking has started. The current status of the XENON 100 as well as future plans for the upgrade are presented...|$|R
40|$|Mongolian Gerbils {{are often}} used as a {{biological}} model, but it remainsunclear whether these rodents display nocturnal, diurnal,or crepuscular patterns of behavior. The experiments presentedbelow studied patterns of sleep-activity, feeding, and reproductivebehavior under 12 : 12 light <b>dark</b> cycles. All <b>data</b> from these experimentssuggest a nocturnal pattern of behavior in these rodents...|$|R
40|$|We {{extend the}} halo-independent method to compare direct <b>dark</b> matter {{detection}} <b>data,</b> so far {{used only for}} spin-independent WIMP-nucleon interactions, to any type of interaction. As an example we apply the method to magnetic moment interactions. Comment: 31 pages, 7 figures; v 2 : text and figures improved, accepted for publication in JCA...|$|R
50|$|The {{continuous}} {{storage of}} <b>dark</b> <b>data</b> can put an organisation at risk, especially if this data is sensitive. In {{the case of}} a breach, this can result in serious repercussions. These can be financial, legal and can seriously hurt an organisation's reputation. For example, a breach of private records of customers could result in the stealing of sensitive information, which could result in identity theft. Another example could be the breach of the company's own sensitive information, for example relating to research and development. These risks can be mitigated by assessing and auditing whether this data is useful to the organisation, employing strong encryption and security and finally, if it is determined to be discarded, then it should be discarded {{in a way that it}} becomes unretrievable.|$|E
40|$|One of {{the primary}} outputs of the {{scientific}} enterprise is data, but many institutions such as libraries that are charged with preserving and disseminating scholarly output have largely ignored this form of documentation of scholarly activity. This paper focuses on a particularly troublesome class of data, termed ???<b>dark</b> <b>data???.</b> ???<b>Dark</b> <b>data???</b> is not carefully indexed and stored so becomes nearly invisible to scientists and other potential users and therefore {{is more likely to}} remain underutilized and eventually lost. The article discusses how the concepts from long tail economics can be used to understand potential solutions for better curation of this data. The paper describes why this data is critical to scientific progress, some of the properties of this data, as well as some social and technical barriers to proper management of this class of data. Many potentially useful institutional, social and technical solutions are under development and are introduced in the last sections of the paper, but these solutions are largely unproven and require additional research and development. published or submitted for publicationnot peer reviewe...|$|E
40|$|Abstract—The OOPSLE {{workshop}} is a discussion-oriented {{and collaborative}} forum for formulating and addressing with open, unsolved and unsolvable problems in software language engineering (SLE), {{which is a}} research domain of systematic, disciplined and measurable approaches of development, evolu-tion and maintenance of artificial languages used in software development. OOPSLE aims {{to serve as a}} think tank in selecting candidates for the open problem list, as well as other kinds of unconventional questions and definitions that do not necessarily have clear answers or solutions, thus facilitating the exposure of <b>dark</b> <b>data.</b> We also plan to formulate promising language-related challenges to organise in the future...|$|E
40|$|We {{list the}} {{mathematical}} and physical arguments {{in favour of}} that numerous Cold <b>Dark</b> Matter <b>data,</b> obtained {{on the basis of}} the virial theorem in the Newton approximation, in fact, mean the cosmic evolution scenario of Galaxies. The cosmic evolution equation is formulated in the framework of the general relativity and the Standard Model in terms of holonomic coordinates and physical observables rescaled in relative units expanding together with the universe. The range of validity of the Newton approximation is estimated for the cosmological model compatible with the latest Supernova Data and primordial nucleosynthesis recalculated in relative units...|$|R
40|$|The paper {{presents}} {{a description of}} the work of detection and identification of frequency lines in the Virgo <b>dark</b> fringe <b>data</b> from run C 7. A number of methods are highlighted by which noise frequency lines are detected by data analysis and measurements in the laboratory. In this paper we give {{a description of the}} list of noise line candidates provided by the pulsar search analysis, the investigation of 10 Hz (and harmonics) noise, violin modes, noise from the end station buildings' air conditioners, sidebands in calibration lines and aliasing in the 4 kHz reconstructed data...|$|R
50|$|Dark {{globular}} cluster is a proposed type of globular star clusters {{that has an}} unusually high mass {{for the amount of}} stars within it. Proposed in 2015 on the basis of observational <b>data,</b> <b>dark</b> {{globular cluster}}s are believed to be populated by objects with significant dark matter components, such as central massive black holes.|$|R
40|$|Photovoltaic {{performance}} {{is shown to}} depend on ligand capping on PbS nanoparticle solar cells by varying the temperature between 140 K and 350 K. The thermal response of open‐circuit voltage, short‐circuit current density, fill‐factor and shunt resistance varies between the ligands. A large increase in short‐circuit current density at low temperatures is observed for 1, 2 ‐ethanedithiol and 3 ‐mercaptopropionic acid and a relatively constant shortcircuit current density is observed for the stiffer 1, 4 ‐benzenedithiol. <b>Dark</b> <b>data</b> provide evidence for tunnelling transport being the dominant charge conduction mechanism for all three ligand devices with recombination occurring within deep trap states. Under illumination, devices exhibit band‐to‐band recombination, indicated by an ideality factor of nearly unity...|$|E
30|$|In addition, we {{identified}} one locus, Os 05 g 28960, the two splice variants of which exhibit opposite differential expression {{both in the}} NSF 45 K light vs. <b>dark</b> <b>data,</b> and the Affymetrix M. grisea vs. mock data (Figs.  2 and 3). In the light vs. dark NSF 45 K array data, Os 05 g 28960.2 is induced in light and Os 05 g 28960.1 is repressed in light (Fig.  2). Similarly in the Affymetrix M. grisea treatment data, Os 05 g 28960.2 is induced at 4  dpi and Os 05 g 28960.1 is reduced relative to mock. The identification of opposite differential expression patterns in the same locus from both NSF 45 K and Affymetrix array platforms emphasizes the utility of current rice array data for studying alternative splicing events and provides independent experimental verification of both data sets.|$|E
40|$|This study {{represents}} {{the first report}} on chloroplast protein synthesis during the synchronous cell growth of a chromophytic (chlorophyll a,c) plant. When the unicellular alga Olisthodiscus luteus is maintained on a 12 -hour light: 12 -hour dark cycle, cell and chloroplast number double every 24 hours. A temporal separation between these two events occurs. Measurements of chloroplast and total cellular protein values suggest that polypeptide synthesis occurs mainly in the light portion of the cell cycle, and pulse chase studies demonstrate that chloroplast proteins made in the light are not degraded in the <b>dark.</b> <b>Data</b> support the following conclusions: (a) a similar complement of chloroplast DNA coded proteins is made at all phases of the light portion of the cell cycle, and (b) chloroplast protein synthesis is a light rather than a cell cycle mediated response...|$|E
40|$|Starting in 1994, all AVIRIS data {{distributions}} {{include a}} new product useful for quantification and modeling of the noise in the reported radiance data. The 'postcal' file contains approximately 100 lines of <b>dark</b> current <b>data</b> collected {{at the end of}} each data acquisition run. In essence this is a regular spectral-image cube, with 614 samples, 100 lines and 224 channels, collected with a closed shutter. Since there is no incident radiance signal, the recorded DN measure only the DC signal level and the noise in the system. Similar dark current measurements, made {{at the end of each}} line are used, with a 100 line moving average, to remove the DC signal offset. Therefore, the pixel-by-pixel fluctuations about the mean of this dark current image provide an excellent model for the additive noise that is present in AVIRIS reported radiance data. The 61, 400 dark current spectra can be used to calculate the noise levels in each channel and the noise covariance matrix. Both of these noise parameters should be used to improve spectral processing techniques. Some processing techniques, such as spectral curve fitting, will benefit from a robust estimate of the channel-dependent noise levels. Other techniques, such as automated unmixing and classification, will be improved by the stable and scene-independence noise covariance estimate. Future imaging spectrometry systems should have a similar ability to record <b>dark</b> current <b>data,</b> permitting this noise characterization and modeling...|$|R
30|$|Only five {{patients}} had charts with detailed case notes that provided presenting complaint, {{the nature and}} cause of wounds/injuries, treatment administered and outcome at discharge. However, only three patients received anti-tetanus serum (20, 000  IU). All patients were treated with antibiotics (metronidazole), diclofenac, diazepam, chlorpromazine, Nasal Gastric Tube for feeding and isolation in a <b>dark</b> room. <b>Data</b> on the type and cause of injuries were available in only four cases. The cause of injuries was a road traffic crash (RTC) and wounds located on the lower limbs. All the accidents involved the motorcycle ‘taxi’ (Boda-boda). Two elderly patients aged 59 and 63  years had diabetic foot ulcers.|$|R
40|$|Detailed {{characterization}} of deep-level defects {{and analysis of}} <b>dark</b> I-V <b>data</b> in 200 keV proton irradiated AlGaAs-GaAs solar cells {{have been carried out}} for several proton fluences (5 x 10 to the 11 th, 10 to the 12 th, and 10 to the 13 th P/sq cm), using DLTS, C-V, and I-V measurement techniques. To study the effect of low temperature thermal annealing on the deep-level defect properties, these irradiated samples were annealed in vacuum at 300 C for one hour. Comparison was then made on the measured defect parameters (i. e., defect energy levels and densities) and the dark I-V characteristics for both the annealed and unannealed samples...|$|R
