1729|2293|Public
25|$|In the 1990s, Holliday lost a {{substantial}} amount of weight and talked about her health struggles with depression during promotional interviews. Initially, the weight loss was attained strictly by diet. Eventually, in an effort to avoid regaining the weight, Holliday had gastric bypass surgery. After the <b>initial</b> <b>weight</b> loss, she released an LP and video titled I’m On Your Side. The video, unlike most videos, was recorded live. In 1995, Holliday released the gospel album On & On. In a March 2008 interview, she revealed that she was in the studio working on a new album, to be released later that year.|$|E
25|$|Over {{more than}} six months of {{sertraline}} therapy for depression, patients showed a nonsignificant weight increase of 0.1%. Similarly, a 30-month-long treatment with sertraline for OCD resulted in a mean weight gain of 1.5% (1kg). Although the difference did not reach statistical significance, the weight gain was lower for fluoxetine (Prozac) (1%) but higher for citalopram (Celexa), fluvoxamine (Luvox) and paroxetine (Paxil) (2.5%). Of the sertraline group, 4.5% gained a large amount of weight (defined as more than 7% gain). This result compares favorably with placebo, where, according to the literature, 3–6% of patients gained more than 7% of their <b>initial</b> <b>weight.</b> The large weight gain was observed only among female members of the sertraline group; the significance of this finding is unclear because of the small size of the group.|$|E
25|$|Another {{clarification}} {{needed is}} that the equivalence principle assumes a constant acceleration of 1g without considering the mechanics of generating 1g. If we do consider the mechanics of it, then we must assume the aforementioned windowless room has a fixed mass. Accelerating it at 1g means {{there is a constant}} force being applied, which = m*g where m is the mass of the windowless room along with its contents (including the observer). Now, if the observer jumps inside the room, an object lying freely on the floor will decrease in weight momentarily because the acceleration is going to decrease momentarily due to the observer pushing back against the floor in order to jump. The object will then gain weight while the observer is in the air and the resulting decreased mass of the windowless room allows greater acceleration; it will lose weight again when the observer lands and pushes once more against the floor; and it will finally return to its <b>initial</b> <b>weight</b> afterwards. To make all these effects equal those we would measure on a planet producing 1g, the windowless room must be assumed to have the same mass as that planet. Additionally, the windowless room must not cause its own gravity, otherwise the scenario changes even further. These are technicalities, clearly, but practical ones if we wish the experiment to demonstrate more or less precisely the equivalence of 1g gravity and 1g acceleration.|$|E
40|$|The {{success of}} {{learning}} {{as well as}} the learning speed of an artificial neural network (ANN) strongly depends on the <b>initial</b> <b>weights.</b> If problem or domain specific knowledge exists, it can be transferred to the ANN by means of a special choice of the <b>initial</b> <b>weights.</b> In this paper, we focus on the choice of a set of <b>initial</b> <b>weights,</b> well suited to fast and robust learning of all particular problems out of a class of related problems. Our evolutionary approach particularly takes the learning algorithm into consideration {{in the design of the}} <b>initial</b> <b>weights.</b> The superior properties of the <b>initial</b> <b>weights</b> resulting from this algorithm are corroborated using a class defined by solving a differential equation with variable boundary conditions...|$|R
30|$|Selection of the <b>initial</b> <b>weights</b> using small, {{positive}} values.|$|R
40|$|A {{competition}} {{which is}} {{based on the results of}} (partial) pairwise comparisons can be modelled by means of a directed graph. Given <b>initial</b> <b>weights</b> on the nodes in such digraph competitions, we view the measurement of the importance (i. e., the cardinal ranking) of the nodes as an allocation problem where we redistribute the <b>initial</b> <b>weights</b> on the basis of insights from cooperative game theory. After describing the resulting procedure of redistributing the <b>initial</b> <b>weights,</b> an iterative process is described that repeats this procedure: at each step the allocation obtained in the previous step determines the new input weights. Existence and uniqueness of the limit is established for arbitrary digraphs. Applications to the evaluation of, e. g., sport competitions and paired comparison experiments are discussed...|$|R
500|$|Beecher's cheeses {{differ from}} similar cheeses {{in that they}} mix cheese growth {{cultures}} in unusual ways. For example, their signature [...] "Flagship" [...] cheese includes cultures typically used for non-cheddar cheeses, such as Gruyère and Emmental, changing the nature, flavor, and texture of their cheddar. Flagship cheese is produced using a cheddaring process, but owing to a different taste, Beecher's does not call this cheese cheddar. The cheese {{has been described as}} having a [...] "sweet finish and creamy texture" [...] unlike the tangier cheddars, owing to this being one of the cheeses they create with a mixture of different cheese cultures. After being prepared in [...] blocks and aged for approximately one year, the Flagship—unlike cheddars—lacks a rind, is moister, resembles butter visually, and carries a milky aroma due to being aged in plastic bags. A variant called [...] "Flagship Reserve" [...] is aged in cheese cloth in [...] sizes on racks in open air, and is rubbed with butter while being turned daily. This preparation method causes the Reserve to lose up to 12% of its <b>initial</b> <b>weight</b> by the time it is completed. The Reserve is aged for a shorter amount of time, leading to a sharper, nuttier taste and texture, according to Food & Wine Magazine. Of the [...] of cheese they produce annually, approximately [...] will be Flagship, and only [...] will be Flagship Reserve.|$|E
2500|$|Though most of {{this work}} dealt with the {{theoretical}} and experimental relations between propellant, rocket mass, thrust, and velocity, a final section, entitled [...] "Calculation of minimum mass required to raise one pound to an 'infinite' altitude", discussed the possible uses of rockets, not only to reach the upper atmosphere, but to escape from Earth's gravitation altogether. He determined that a rocket with an effective exhaust velocity (see specific impulse) of 7000 feet per second and an <b>initial</b> <b>weight</b> of 602 pounds {{would be able to}} send a one-pound payload to an infinite height. Included as a thought experiment was the idea of launching a rocket to the moon and igniting a mass of flash powder on its surface, so as to be visible through a telescope. He discussed the matter seriously, down to an estimate of the amount of powder required; Goddard's conclusion was that a rocket with starting mass of 3.21 tons could produce a flash [...] "just visible" [...] from Earth, assuming a final payload weight of 10.7 pounds.|$|E
50|$|The rocket had 3 stages, 2 roll boosters, a 55 m height; and was liquid-fueled. Its <b>initial</b> <b>weight</b> was ~343 t.|$|E
5000|$|Using {{different}} training parameters with {{a single}} training method (e.g. using different <b>initial</b> <b>weights</b> for each neural network in an ensemble) ...|$|R
30|$|There {{have been}} many reports about setting <b>initial</b> <b>weights</b> in a {{training}} process [15]. Many researchers reported that <b>initial</b> <b>weights</b> may {{play a crucial role}} [16]. Different <b>initial</b> <b>weights</b> may induce different results when a learning process may not guarantee the convergence to a global optimal solution [17]. Therefore, one often tries to learn necessary parameters of a system with different sets of <b>initial</b> <b>weights</b> and combines results to get a final set of parameters [18]. When building a system of classifying a target object with cluttered background, one relies on training data with positive and negative samples. In general, negative samples could be quite different from each other and the variance of them can be very large, while positive samples may show somewhat common properties. Therefore, to increase the generality of the system, one should not count on a specific negative sample too much since the negative sample may reappear with little chance in real situation. On the other hand, one may need to treat the positive sample with more attention when the sample shows some common property of positive training data, since a target object may show such a property with a larger chance in real situation.|$|R
40|$|In this paper, it {{is found}} that the weights of a {{perceptron}} are bounded for all <b>initial</b> <b>weights</b> if there exists a nonempty set of <b>initial</b> <b>weights</b> that the weights of the perceptron are bounded. Hence, the boundedness condition of the weights of the perceptron is independent of the <b>initial</b> <b>weights.</b> Also, a necessary and sufficient condition for the weights of the perceptron exhibiting a limit cycle behavior is derived. The range of the number of updates for the weights of the perceptron required to reach the limit cycle is estimated. Finally, it is suggested that the perceptron exhibiting the limit cycle behavior can be employed for solving a recognition problem when downsampled sets of bounded training feature vectors are linearly separable. Numerical computer simulation results show that the perceptron exhibiting the limit cycle behavior can achieve a better recognition performance compared to a multilayer perceptro...|$|R
50|$|The season {{during which}} a birth takes place {{has been linked}} to the weight {{development}} of the infant as well as <b>initial</b> <b>weight.</b>|$|E
50|$|If each target {{object in}} set T {{belongs to a}} list of active features, link feature to target and set <b>initial</b> <b>weight</b> at the same time.|$|E
5000|$|The {{optimization}} {{takes as}} input {{a sequence of}} training examples [...] and produces a sequence of weights [...] starting from some <b>initial</b> <b>weight</b> , usually chosen at random.|$|E
40|$|The {{paper is}} based on feed forward neural network (FFNN) {{optimization}} by particle swarm intelligence (PSI) used to provide <b>initial</b> <b>weights</b> and biases to train neural network. Once the weights and biases are found using Particle swarm optimization (PSO) with neural network used as training algorithm for specified epoch, the same are used to train the neural network for training and classification of benchmark problems. Further the approach is tested for offline signature classifications. A comparison is made between normal FFNN with random weights and biases and FFNN with particle swarm optimized weights and biases. Firstly, the performance is tested on two benchmark databases for neural network, The Breast Cancer Database and the Diabetic Database. Result shows that neural network performs better with <b>initial</b> <b>weights</b> and biases obtained by Particle Swarm optimization. The network converges faster with PSO obtained <b>initial</b> <b>weights</b> and biases for FFNN and classification accuracy is increased...|$|R
40|$|We {{prove that}} the linearly edge {{reinforced}} random walk (LRRW) on any graph with bounded degrees is recurrent for sufficiently small <b>initial</b> <b>weights.</b> In contrast, we show that for non-amenable graphs the LRRW is transient for sufficiently large <b>initial</b> <b>weights,</b> thereby establishing a phase transition for the LRRW on non-amenable graphs. While we rely on {{the description of the}} LRRW as a mixture of Markov chains, the proof does not use the magic formula. We also derive analogous results for the vertex reinforced jump process. Comment: 30 page...|$|R
40|$|Abstract. In {{this paper}} we conduct a {{comparative}} study between hybrid methods to optimize multilayer perceptrons: {{a model that}} optimizes the architecture and <b>initial</b> <b>weights</b> of multilayer perceptrons; a parallel approach to optimize the architecture and <b>initial</b> <b>weights</b> of multilayer perceptrons; a method that searches for {{the parameters of the}} training algorithm, and an approach for cooperative co-evolutionary optimization of multilayer perceptrons. Obtained results show that a co-evolutionary model obtains similar or better results than specialized approaches, needing much less training epochs and thus using much less simulation time. ...|$|R
5000|$|Eman Ahmed Abd El Aty (born [...] 1980) is {{considered}} to be the heaviest living woman {{in the world and the}} second heaviest woman in history (after Carol Yager). Her <b>initial</b> <b>weight</b> was 504 kg.|$|E
50|$|She also {{initially}} {{was awarded the}} bronze medal in the 75 kg category at the 2008 Summer Olympics, {{with a total of}} 264 kg. She competed at the 2012 Summer Olympics, but failed to snatch the <b>initial</b> <b>weight.</b>|$|E
5000|$|Collé - [...] "stuck," [...] or [...] "glued," [...] is {{a stroke}} that begins from a heavily {{weighted}} bow resting motionless on the string. Ideally, the <b>initial</b> <b>weight</b> will be almost {{enough to cause}} an undesirable scratch sound.|$|E
40|$|This paper {{proposes a}} new version of a method (G-Prop, genetic backpropagation) that {{attempts}} {{to solve the problem of}} finding appropriate <b>initial</b> <b>weights</b> and learning parameters for a single hidden layer Multilayer Perceptron (MLP) by combining an evolutionary algorithm (EA) and backpropagation (BP). The EA selects the MLP <b>initial</b> <b>weights,</b> the learning rate and changes the number of neurons in the hidden layer through the application of specific genetic operators, one of which is BP training. The EA works on the <b>initial</b> <b>weights</b> and structure of the MLP, which is then trained using QuickProp; thus G-Prop combines the advantages of the global search performed by the EA over the MLP parameter space and the local search of the BP algorithm. The application of the G-Prop algorithm to several real-world and benchmark problems shows that MLPs evolved using G-Prop are smaller and achieve a higher level of generalization than other perceptron training algorithms, such as QuickProp [...] ...|$|R
3000|$|... pre is the <b>initial</b> {{quiescent}} <b>weights</b> of {{the main}} array constructed based on the presumed DOA of the desired signal. By substituting (19) into (7) and utilizing the <b>initial</b> quiescent <b>weights</b> w [...]...|$|R
30|$|Initialization <b>Initial</b> <b>weights</b> {{or start}} point of SQP {{algorithm}} is the global best chromosomes of GAs variants. The bounds, declarations and initial parameters {{are given in}} ‘optimset’ function such as number of iteration 500.|$|R
50|$|Marele câștigator is a Romanian reality {{game show}} that debuted on Antena 1 September 23, 2010. The show {{features}} obese people competing {{to win a}} cash prize by losing {{the highest percentage of}} weight relative to their <b>initial</b> <b>weight.</b>|$|E
50|$|Marele câștigător, is a Romanian reality {{game show}} and the {{romanian}} version of The Biggest Loser. The show features obese people competing to win a cash prize by losing {{the highest percentage of}} weight relative to their <b>initial</b> <b>weight.</b>|$|E
50|$|The {{extent of}} water loss during the salting and drying processes, whereby the product loses {{approximately}} half of its <b>initial</b> <b>weight,</b> is sufficient to confer excellent keeping qualities and a high nutritional value, {{without the need for}} any additional preservatives.|$|E
40|$|Abstract. This paper {{proposes a}} new version ofa method (G-Prop, genetic backpropagation) that {{attempts}} {{to solve the problem}} of ¢nding appropriate <b>initial</b> <b>weights</b> and learning parameters for a single hidden layer Multilayer Perceptron (MLP) by combining an evolutionary algorithm (EA) and backpropagation (BP). The EA selects the MLP <b>initial</b> <b>weights,</b> the learning rate and changes the number of neurons in the hidden layer through the application of speci¢c genetic operators, one of which is BP training. The EAworks on the <b>initial</b> <b>weights</b> and structure of the MLP, which is then trained using QuickProp; thus G-Prop combines the advantages of the global search performed by the EA over the MLP parameter space and the local search of the BP algorithm. The application of the G-Prop algorithm to several real-world and benchmark problems shows that MLPs evolved using G-Prop are smaller and achieve a higher level of generalization than other perceptron training algorithms, such as QuickPropagation or RPROP, and other evolutive algorithms. It also shows some improvement over previous versions of the algorithm. Mathematics Subject Classi¢cations (2000) : 68 T 0...|$|R
30|$|<b>Initial</b> <b>weights</b> {{for each}} run were picked {{independently}} from a Gaussian with mean and variance equal to 0.5. Euler’s method {{with a time}} step of 0.2 ms was used for numerically integrating the differential equations.|$|R
30|$|It {{is worth}} noticing that {{generally}} neural networks are not stable; {{each time a}} network is trained, the <b>initial</b> <b>weights</b> {{as well as the}} initial bias values are chosen randomly from the programme. This random selection strongly affects the training procedure and the final error of the network. The same network can achieve complete training for a specific set of <b>initial</b> <b>weights</b> and biases and afterwards can fail to be trained for another set of weight and bias values. For this reason, for each network examined, the network ran 15 times and then the most possible value one could take with only one “running” of the network was chosen.|$|R
5000|$|... where [...] is {{the weight}} of a given size {{interval}} of particles remaining in the screen at any given time [...] and [...] is the <b>initial</b> <b>weight</b> of the feed. Therefore, from equations (...) and (...) , the screening rate can be expressed as: ...|$|E
50|$|The Biggest Loser is an American {{competition}} {{reality show}} that debuted on NBC on October 19, 2004. The show features obese or overweight contestants competing {{to win a}} cash prize by losing {{the highest percentage of}} weight relative to their <b>initial</b> <b>weight.</b>|$|E
50|$|In 2006, Andrey Skoromnyy {{graduated}} from high school and moved to Moscow to study. At the age of 17, Andrey began attending the nearby weightlifting club and became extremely interested in bodybuilding. His <b>initial</b> <b>weight</b> was about 65 kg when he started weight training.|$|E
40|$|A {{method has}} been {{proposed}} for weight initialization in back-propagation feed-forward networks. Training data is analyzed {{and the notion of}} critical point is introduced for determining the <b>initial</b> <b>weights</b> and the number of hidden units. The proposed method has been applied to artificial data and the publicly available cancer database. The experimental results of artificial data show that the proposed method takes 1 / 3 of the training time required for standard back-propagation. In order to verify the effectiveness of the proposed method, standard back-propagation, where the learning starts with random <b>initial</b> <b>weights</b> was also applied to the cancer database. The experimental results indicate that the proposed weight initialization method results in better generalization...|$|R
40|$|In this paper, {{we present}} a {{comparative}} study of several methods that combine evolutionary algorithms and local search methods to optimize multilayer perceptrons: A method that optimizes the architecture and <b>initial</b> <b>weights</b> of multilayer perceptrons; another that searches for training algorithm parameters, and finally, a co-evolutionary algorithm, introduced in this paper, that handles the architecture, the network’s <b>initial</b> <b>weights</b> and the training algorithm parameters. Our aim is {{to determine how the}} co-evolutive method can obtain better results {{from the point of view}} of running time and classification ability. Experimental results show that the co-evolutionary method obtains similar or better results than the other approaches, requiring far less training epochs and thus, reducing running time...|$|R
40|$|The {{traditional}} Back Propagation (BP) {{has some}} significant disadvantages, such as training too slowly, easiness {{to fall into}} local minima, and sensitivity of the <b>initial</b> <b>weights</b> and bias. In order to overcome these shortcomings, an improved BP network that is optimized by Cuckoo Search (CS), called CSBP, is proposed in this paper. In CSBP, CS is used to simultaneously optimize the <b>initial</b> <b>weights</b> and bias of BP network. Wine data is adopted to study the prediction performance of CSBP, and the proposed method is compared with the basic BP and the General Regression Neural Network (GRNN). Moreover, the parameter study of CSBP is conducted {{in order to make}} the CSBP implement in the best way...|$|R
