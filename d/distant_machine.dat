9|23|Public
50|$|Mobile {{augmented}} reality applications are gaining popularity {{due to the}} wide adoption of mobile and especially wearable devices. However, they often rely on computationally intensive computer vision algorithms with extreme latency requirements. To compensate {{for the lack of}} computing power, offloading data processing to a <b>distant</b> <b>machine</b> is often desired. Computation Offloading introduces new constraints in applications, especially in terms of latency and bandwidth. Although there are a plethora of real-time multimedia transport protocols {{there is a need for}} support from network infrastructure as well.|$|E
5000|$|The usual {{method of}} {{operation}} was that the message would be prepared off-line, using paper tape. All common telex machines incorporated a 5-hole paper-tape punch and reader. Once the paper tape had been prepared, the message could be transmitted in minimum time. Telex billing was always by connected duration, so minimizing the connected time saved money. However, it was also possible to connect in [...] "real time", where the sender and the recipient could both type on the keyboard and these characters would be immediately printed on the <b>distant</b> <b>machine.</b>|$|E
50|$|Movement three {{suggests}} a motorcycle race through French Provence during a Christian procession. It includes more motorcycle sounds, {{this time with}} the soloist playing along with the entire trombone section. Eventually the upper winds start playing a religious-sounding melody, and the soloist plays very lightly in the background, the sound of a motorcycle. Though it is the sound of a <b>distant</b> <b>machine,</b> the tones harmonize with the procession music. Eventually the soloist plays a countermelody against the processional music. The movement ends with the soloist playing a very high and loud note until his air is gone. Then he sucks in with a deathly sound and begins the next movement.|$|E
40|$|International audienceIn this paper, {{we present}} {{cooperative}} virtual environments that allows two users to control {{different degrees of}} freedom of a virtual robot from two <b>distant</b> <b>machines</b> connected through LAN. In this context, we investigate the effect of tactile feedback and viewpoint on performance, awarness and user coordination. Ten volunteer subjects were instructed to cooperatively pick-and-place virtual objects using the Nintendo Wiimote TM. Results show that both viewpoint and tactile feedback signiﬁcantly enhance task performance, awarness and users coordination...|$|R
40|$|International audienceCAPE-stands for Checkpointing Aided Parallel Execution-has {{demonstrated}} as a {{high performance}} and compliant OpenMP implementation on distributed memory systems. CAPE {{is based on the}} use of checkpoints to automatically distribute jobs of OpenMP parallel structs to <b>distant</b> <b>machines</b> and to automatically collect calculated results on these machines to the master machine. However, on the current version, the data exchange on networks use manual sockets that take time to establish connections between machines for each parallel construct. Furthermore, this technique is not very reliable due to the risk of conflict of ports and the problems of data exchange using stream. This paper aims at presenting the approach of use MPI to improve the reliability and the performance of CAPE. We will analyze, discuss two methods and experiment to evaluate the performance of them...|$|R
40|$|International audienceCAPE - {{which stands}} for Checkpointing Aided Parallel Execution - has {{demonstrated}} to be a high-performance and compliant OpenMP implementation for distributed memory systems. CAPE {{is based on the}} use of checkpoints to automatically distribute jobs of OpenMP parallel constructs to <b>distant</b> <b>machines</b> and to automatically collect the calculated results on these machines to the master machine. However, on the current version, the data exchange on networks use manual sockets that require time to establish connections between machines for each parallel construct. Furthermore, this technique is not really reliable due to the risk of conflicts on ports and the problem of data exchange using stream. This paper aims at presenting the impact of using MPI to improve the reliability and the performance of CAPE. Both socket and MPI implementations are analyzed and discussed, and performance evaluations are provide...|$|R
50|$|While newsgroups {{were not}} created with the {{intention}} of distributing files such as pictures, sound and video, they have proven to be quite effective for this. Because newsgroups are widely distributed, a file uploaded once will be spread to many other servers and can then be downloaded by an unlimited number of users. More useful is that users download from a local news server, rather than from a more <b>distant</b> <b>machine</b> with perhaps limited connectivity, as may be the case with peer-to-peer technology. In fact, this is another benefit of newsgroups: it is usually not expected that users share. If every user makes uploads then the servers would be flooded; thus it is acceptable and often encouraged for users to just leech.|$|E
50|$|The adult white stork's main {{sound is}} noisy bill-clattering, {{which has been}} likened to <b>distant</b> <b>machine</b> gun fire. The bird makes these sounds by rapidly opening and closing its beak so that a {{knocking}} sound is made each time its beak closes. The clattering is amplified by its throat pouch, which acts as a resonator. Used {{in a variety of}} social interactions, bill-clattering generally grows louder the longer it lasts, and takes on distinctive rhythms depending on the situation—for example, slower during copulation and briefer when given as an alarm call. The only vocal sound adult birds generate is a weak barely audible hiss; however, young birds can generate a harsh hiss, various cheeping sounds, and a cat-like mew they use to beg for food. Like the adults, young also clatter their beaks. The up-down display is used for a number of interactions {{with other members of the}} species. Here a stork quickly throws its head backwards so that its crown rests on its back before slowly bringing its head and neck forwards again, and this is repeated several times. The display is used as a greeting between birds, post coitus, and also as a threat display. Breeding pairs are territorial over the summer, and use this display, as well as crouching forward with the tails cocked and wings extended.|$|E
40|$|The {{purpose here}} {{is to examine the}} concept of {{telepresence}} critically. To accomplish this goal, first, the assumptions that underlie telepresence and its applications are examined, and second, the issues raised by that examination are discussed. Also, these assumptions and issues are used as a means of shifting the focus in telepresence from development to user-based research. The most basic assumption of telepresence is that the information being provided to the human must be displayed in a natural fashion, i. e., the information should be displayed to the same human sensory modalities, and in the same fashion, as if the person where actually at the remote site. A further fundamental assumption for the functional use of telepresence is that a sense of being present in the work environment will produce superior performance. In other words, that sense of being there would allow the human operator of a <b>distant</b> <b>machine</b> to take greater advantage of his or her considerable perceptual, cognitive, and motor capabilities in the performance of a task than would more limited task-related feedback. Finally, a third fundamental assumption of functional telepresence is that the <b>distant</b> <b>machine</b> under the operator's control must substantially resemble a human in dexterity...|$|E
40|$|This paper {{presents}} an approach {{based on a}} user agent to permit a number of users connected to <b>distant</b> <b>machines</b> to access different information sources {{in order to satisfy}} their requests. This user agent permits the simplification of the information search from distributed sources by making them transparent to the users. The agent considers the specific needs of each user during the search and responds with reference to their profile. It also permits the processing of one or more information requests by one or more users, as well as concurrent responses to each of them. Moreover, the agent provides its users with a measure of interaction, in order to enhance {{the quality and quantity of}} the results obtained. As a result, the agent is endowed with the ability to filter and refine the search, thus improving its service to the users...|$|R
40|$|An {{increasing}} number of distributed data-driven applications are moving into shared public clouds. By sharing resources and operating at scale, public clouds promise higher utilization and lower costs than private clusters. To achieve high utilization, however, cloud providers inevitably allocate virtual machine instances noncontiguously, i. e., instances of a given application may end up in physically <b>distant</b> <b>machines</b> in the cloud. This allocation strategy can lead to large differences in average latency between instances. For a large class of applications, this difference can result in significant performance degradation, unless care is taken in how application components are mapped to instances. In this paper, we propose ClouDiA, a general deployment advisor that selects application node deployments minimizing either (i) the largest latency between application nodes, or (ii) the longest critical path among all application nodes. ClouDiA employs mixedinteger programming and constraint programming techniques to efficiently search the space of possible mappings of application nodes to instances. Through experiments with synthetic and real applications in Amazon EC 2, we show that our techniques yield a 15 % to 55 % reduction in time-to-solution or service response time, without any need for modifying application code. 1...|$|R
40|$|Article dans revue scientifique avec comité de lecture. International audienceThe main {{objective}} of an information retrieval system (IRS) {{is to provide}} relevant information {{in response to the}} user's query. On the one part, the relevance of a response concerns its exactness compared with the user's query. On the other part, it concerns its correspondence with the user's knowledge level and his preferences. One of the major contributions in this area of personalization of the system's response is by taking into consideration each user's specificity. We propose the use of explicit user model where the system's solution will be determined by the knowledge on the user. The user's activities are recorded as documents. The method we adopt for information retrieval combines query by criteria and information analysis. We have also proposed architecture for cooperative information retrieval. This architecture allows two users to share their experience in the process of information retrieval and for the interpretation of the system's result, on <b>distant</b> <b>machines.</b> The proposals were implemented in two systems : STREEMS and METIORE. STREEMS manages information on trees while METIORE manages information on bibliographic references...|$|R
40|$|Abstract—This study {{evaluates the}} idea that {{consciousness}} is creating an influence on the environment {{in a way that}} makes random systems (Random Number Generator – RNG) become more ordered. This evaluation is combined with an exploration of two, potentially, influencing factors: distance from the target and awareness of the target. The experimental procedure was based upon data derived from meditating groups where experimental data (during meditation) was compared with the control data (pre-meditation). Comparison was also made between awareness and non-awareness groups (participants were aware/non-aware of the RNG). This allowed the influencing factor of the participants ’ awareness to be tested. The factor of distance has been tested by applying a second RNG. The first machine (local) was positioned in the room with the meditators while the second machine (distant) was positioned away from that point. Results showed completely random data for the non-awareness group (both control and experimental periods) while the awareness group produced significant data in the experimental periods of the local machine and random data in the <b>distant</b> <b>machine.</b> It is suggested that awareness of the PK (psychokinesis) target and distance from the PK target could be influencing factors in the way consciousness might be influencing a random environment...|$|E
40|$|Narcisse is a {{graphics}} package {{developed by}} our French colleagues at Centre d`Etudes de Limeil Valenton of the Commissariat d`Energie Atomique. Narcisse is quite comprehensive; it can do two-, three-, and four-dimensional plots (the latter meaning that the surface is colored according {{to the values of}} an arbitrary function). One can open and send plots to a Narcisse window on a <b>distant</b> <b>machine.</b> Narcisse has a user-friendly graphical user interface (GUI) which, once a graph has appeared, allows the user to change its characteristics interactively. This enables one to find the best appearance for a particular plot without having to graph it repeatedly from the user program. Previously created files in various formats can also be imported directly into the Narcisse GUI and manipulated from there. Narcisse runs independently, as a graphics server. The user program communicates with Narcisse via Unix sockets. This communication is quite low level and very complex. The appearance of a plot is controlled by nearly 150 parameters for determining such things as the color palette, type of shading, axis scales, curve and surface labels, titles, angle and distance of view (for three- and four-dimensional graphs), hidden line removal, etc. Most end users do not wish to spend time learning the tedious details of such interfaces; they would just like to specify data and ask to have it plotted. This paper describes a high level, easy to use graphics interface which hides (as much as possible) the low level details of whatever graphics system is actually being used, so that the low level can be essentially ``plug-and-play. `` Then, whenever a better system becomes available, it should only be necessary to change low level interface routines not normally accessed by ordinary users. Python, with its easy extendability, was ideally suited for this job...|$|E
40|$|DANS CETTE THESE NOUS PROPOSONS UNE APPROCHE ET DES OUTILS POUR FACILITER LA MISE AU POINT ET L'UTILISATION DE TECHNIQUES D'INTERACTION AVANCEES AU SEIN D'APPLICATIONS GRAPHIQUES INTERACTIVES. NOUS PROPOSONS DE RESOUDRE LES EXIGENCES ANTITHETIQUES DE LA REUTILISATION, NECESSAIRE A LA FACTORISATION DES EFFORTS, ET DE L'INNOVATION, NECESSAIRE A L'ADAPTATION A DE NOUVEAUX CONTEXTES, EN FOURNISSANT UNE PYRAMIDE D'ABSTRACTIONS DE DIVERS NIVEAUX PERMETTANT LEUR RECOMBINAISON POUR S'ADAPTER FINEMENT AUX BESOINS SPECIFIQUES A CHAQUE USAGE. NOUS PROPOSONS EGALEMENT D'INTEGRER AUX LANGAGES IMPERATIFS UNE STRUCTURE DE CONTROLE BASEE SUR UN FORMALISME DE MACHINES A ETATS HIERARCHIQUES POUR FACILITER LA PROGRAMMATION DE COMPORTEMENTS DYNAMIQUES ET FAIRE DES INTERACTIONS DES OBJETS A PART ENTIERE DU VOCABULAIRE DES PROGRAMMEURS. NOUS MONTRONS PAR DES EXEMPLES COMME CES ELEMENTS PERMETTENT LA REPRODUCTION DE L'ETAT DE L'ART DES INTERACTIONS, TANT STANDARDS QU'AVANCEES, ET LA MISE AU POINT DE TECHNIQUES D'INTERACTION ORIGINALES ET PERFORMANTES. NOUS PRESENTONS EN PARTICULIER LA REALISATION D'APPLICATIONS GRAPHIQUES INTERACTIVES UTILISANT UNE ARCHITECTURE DISTRIBUEE PERMETTANT DE LOCALISER L'INTERACTION SUR LE SYSTEME LOCAL ET DE REPORTER LE NOYAU FONCTIONNEL SUR UNE MACHINE DISTANTE. NOUS PRESENTONS ENFIN UNE TECHNIQUE D'INTERACTION AVANCEE, LE POINTAGE SEMANTIQUE, QUI FACILITE LA TACHE ELEMENTAIRE DE SELECTION PAR POINTAGE EN PERMETTANT D'UTILISER DEUX TAILLES POUR LES OBJETS DE L'INTERFACE, L'UNE CHOISIE EN FONCTION DES INFORMATIONS QU'ILS PRESENTENT, L'AUTRE EN FONCTION DE LEUR IMPORTANCE POUR LA MANIPULATION. THIS THESIS PRESENTS AN APPROACH AND A SET OF TOOLS THAT FACILITATE THE DEVELOPMENT AND USE OF ADVANCED INTERACTION TECHNIQUES IN INTERACTIVE GRAPHICAL APPLICATIONS. WE SOLVE THE CONTRADICTORY CONSTRAINTS OF REUSABILITY, REQUIRED FOR FACTORING, AND INNOVATION, REQUIRED FOR ADAPTING APPLICATIONS TO NEW CONTEXTS OF USE, BY PROVIDING A PYRAMID OF LEVELS OF ABSTRACTIONS THAT CAN BE COMBINED IN VARIOUS WAYS TO ADAPT TO THE SPECIFIC NEEDS OF EACH APPLICATION. WE ALSO AUGMENT IMPERATIVE PROGRAMMING LANGUAGES WITH A NEW CONTROL STRUCTURED BASED ON HIERARCHICAL STATE MACHINES. THIS FACILITATES THE PROGRAMMING OF DYNAMIC BEHAVIOURS BY TURNING INTERACTIONS INTO FIRST-CLASS OBJECTS OF THE PROGRAMMING LANGUAGE. THROUGH A SET OF EXAMPLES, WE DEMONSTRATE HOW THIS APPROACH SUPPORTS THE IMPLEMENTATION OF BOTH CLASSICAL AND STATE-OF-THE-ART INTERACTIONS, AS WELL AS THE IMPLEMENTATION OF NOVEL AND EFFICIENT INTERACTION TECHNIQUES. IN PARTICULAR, WE DESCRIBE THE IMPLEMENTATION OF A DISTRIBUTED ARCHITECTURE FOR DEVELOPING INTERACTIVE GRAPHICAL APPLICATIONS WHERE ADVANCED INTERACTION AND RENDERING IS HANDLED ON THE LOCAL MACHINE WHILE THE FUNCTIONAL CORE RUNS ON A <b>DISTANT</b> <b>MACHINE.</b> WE ALSO DESCRIBE A NOVEL INTERACTION TECHNIQUE CALLED SEMANTIC POINTING THAT FACILITATES THE SELECTION OF OBJECTS WITH A POINTING DEVICE BY DECOUPLING THE VISUAL SIZE OF OBJECTS, DEFINED BY THEIR PRESENTATION REQUIREMENTS, FROM THEIR SIZE IN THE MOTOR SPACE, DEFINED BY THEIR INTERACTION REQUIREMENTS. ORSAY-PARIS 11 -BU Sciences (914712101) / SudocSudocFranceF...|$|E
40|$|Colloque avec actes et comité de lecture. International audienceFinding {{information}} may be compared to problem solving. A user of an Information Retrieval System (IRS) has a need that constitutes an objective for him. Unfortunately, this objective may not be well defined when the user starts the IRS. Whatever the degree of understanding of this objective, the user translates it into a set of queries that the system can use for calculating the relevant solutions. Knowing the problem to solve {{does not necessarily mean}} knowing how to solve it. Observing the dialog between a user and a librarian which often produce better relevant solutions and faster than asking the user to use an IRS himself we propose an IRS that may be used to provide cooperative information retrieval. The architecture provides three functioning modes: autonomy, observation and cooperative. In cooperative mode, two users participate in the process of information retrieval, in real time. The two of them work on <b>distant</b> <b>machines.</b> In observation mode, user sees what another user is doing but without the possibility of intervening in that process. In autonomous mode, a user uses the system alone without anybody seeing him and he does not see anybody. Our proposals are implemented in a prototype (METIORE) used for accessing and analyzing bibliographic references of our research laboratory cente...|$|R
5000|$|The song is an unmusical long, high-pitched reeling trill {{performed}} with beak held {{wide open and}} the whole body vibrating. It lasts for from {{a few seconds to}} two or three minutes with hardly a pause for breath. It varies in volume from a faint hum to a sound resembling a <b>distant</b> mowing <b>machine.</b> It is performed at any time of day from early morning until after the sun has set and is constantly to be heard from the arrival of the bird in spring until late July. The alarm call is a repeated ticking noise that has been rendered as [...] "twkit-twkit-twkit".|$|R
50|$|Structurally, {{the album}} has three {{distinct}} sections: the opening 18 minutes consist primarily of quiet, sparse acoustic guitar strums backed by <b>distant</b> whirs and <b>machine</b> noises; a lengthy crossfade shifts the album into drone doom metal, with Merzbow's accentuation via harsh noise layers; eventually, the loud half-hour settles through movements of noise into 6 {{minutes of a}} quiet electric guitar and ambience coda.|$|R
40|$|Thanks to {{incredible}} {{advances in}} instrumentation, surveys like the Sloan Digital Sky Survey {{have been able}} to find and catalog billions of objects, ranging from local M dwarfs to <b>distant</b> quasars. <b>Machine</b> learning algorithms have greatly aided in the effort to classify these objects; however, there are regimes where these algorithms fail, where interesting oddities may be found. We present here an X-ray bright quasar misidentified as a red supergiant/X-ray binary, and a subsequent search of the SDSS quasar catalog for X-ray bright stars misidentified as quasars. Comment: 4 pages, 2 figures. Submitted to the Proceedings of the 329 th International Astronomical Union Symposium: The Lives and Death-Throes of Massive Stars, X-ray Splinter Sessio...|$|R
40|$|An {{increasing}} number of distributed data-driven applications are moving into public clouds. By sharing resources and operating at large scale, public clouds promise higher utilization and lower costs than private clusters. Also, flexible resource allocation and billing methods offered by public clouds enable tenants to control response time or time-to-solution of their applications. To achieve high utilization, however, cloud providers inevitably place virtual machine instances non-contiguously, i. e., instances of a given application may end up in physically <b>distant</b> <b>machines</b> in the cloud. This allocation strategy leads to significant heterogeneity in average network latency between instances. Also, virtualization and the shared use of network resources between tenants results in network latency jitter. We observe that network latency heterogeneity and jitter in the cloud can greatly increase the time required for communication in these distributed data-driven applications, which leads to significantly worse response time. To improve response time under latency jitter, we propose a general parallel framework which exposes a high-level, data-centric programming model. We design a jitter-tolerant runtime that exploits this programming model to absorb latency spikes transparently by (1) carefully scheduling computation and (2) replicating data and computation. To improve response time with heterogeneous mean latency, we present ClouDiA, a general deployment advisor that selects application node deployments minimizing either (1) the largest latency between application nodes, or (2) the longest critical path among all application nodes. We also describe how to effectively control response time for interactive data analytics in public clouds. We introduce Smart, the first elastic cloud resource manager for in-memory interactive data analytics. Smart enables control {{of the speed of}} queries by letting users specify the number of compute units per GB of data processed, and quickly reacts to speed changes by adjusting the amount of resources allocated to the user. We then describe SmartShare, an extension of Smart that can serve multiple data scientists simultaneously to obtain additional cost savings without sacrificing query performance guarantees. Taking advantage of the workload characteristics of interactive data analysis, such as think time and overlap between datasets, we are able to further improve resource utilization and reduce cost...|$|R
50|$|The heavy {{machine gun}} was a {{specialist}} weapon, and in a static trench system was employed in a scientific manner, with carefully calculated fields of fire, so that at a moment's notice an accurate burst could be fired at the enemy's parapet or {{a break in the}} wire. Equally it could be used as light artillery in bombarding <b>distant</b> trenches. Heavy <b>machine</b> guns required teams of up to eight men to move them, maintain them, and keep them supplied with ammunition. This made them impractical for offensive manoeuvres, contributing to the stalemate on the Western Front.|$|R
30|$|Regarding the {{influence}} of magnification, Bourriau et al. [5] could not identify significant differences between equipment with a 4 -m <b>distant</b> cephalometric <b>machine</b> and a 1.5 -m distant cephalometric arm. Despite that, it should be considered that distance varying between the X-ray source and the image receptor will always cause a degree of magnification, the larger the distance, the lower the magnification. A focus object distance of 4 m in 2 D cephalometric equipment is usually favoured for the reduced radiation burden and lack of enlargement, while an equipment with 1.5 -m arm has a direct advantage of being compact and integrated in a multimodal system as well as having an increased resolution. On the other hand, panoramic equipment with a cephalometric arm at a 1.5 -m distance may present shortcomings in enlargement factors and superimposition of the bilateral structures more distant from the midsagittal plane, considering the less magnified structures on the side nearby the image receptor [30]. We {{were not able to}} identify studies correlating landmark identification errors in lateral cephalograms and their influence on the outcome of patient treatment.|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. With the emergence of advanced integrated services networks, the need for effective performance analysis techniques has become extremely important. Further advancements in these networks can only be possible if the practical performance issues of the existing networks are clearly understood. This thesis {{is concerned with the}} design and development of a measurement system which has been implemented on a large experimental network. The measurement system is based on dedicated traffic generators which have been designed and implemented on the Project Unison network. The Unison project is a multisite networking experiment for conducting research into the interconnection and interworking of local area network based multi-media application systems. The traffic generators were first developed for the Cambridge Ring based Unison network. Once their usefulness and effectiveness was proven, high performance traffic generators using transputer technology were built for the Cambridge Fast Ring based Unison network. The measurement system is capable of measuring the conventional performance parameters such as throughput and packet delay, and is able to characterise the operational performance of network bridging components under various loading conditions. In particular, the measurement system has been used in a 'measure and tune' fashion in order to improve the performance of a complex bridging device. Accurate measurement of packet delay in wide area networks is a recognised problem. The problem is associated with the synchronisation of the clocks between the <b>distant</b> <b>machines.</b> A chronological timestamping technique has been introduced in which the clocks are synchronised using a broadcast synchronisation technique. Rugby time clock receivers have been interfaced to each generator for the purpose of synchronisation. In order to design network applications, an accurate knowledge of the expected network performance under different loading conditions is essential. Using the measurement system, this has been achieved by examining the network characteristics at the network/user interface. Also, the generators are capable of emulating a variety of application traffic which can be injected into the network along with the traffic from real applications, thus enabling user oriented performance parameters to be evaluated in a mixed traffic environment. A number of performance measurement experiments have been conducted using the measurement system. Experimental results obtained from the Unison network serve to emphasise the power and effectiveness of the measurement methodology...|$|R
40|$|Cloud Computing {{has emerged}} as a {{successful}} computing paradigm for efficiently utilizing managed compute infrastructure such as high speed rack-mounted servers, connected with high speed net-working, and reliable storage. Usually such infrastructure is dedi-cated, physically secured and has reliable power and networking in-frastructure. However, much of our idle compute capacity is present in unmanaged infrastructure like idle desktops, lab <b>machines,</b> phys-ically <b>distant</b> server <b>machines,</b> and laptops. We present a scheme to utilize this idle compute capacity on a best-effort basis and provide high availability even in face of failure of individual components or facilities. We run virtual machines on the commodity infrastructure and present a cloud interface to our end users. The primary challenge is to maintain availability in the presence of node failures, net-work failures, and power failures. We run multiple copies of a Virtual Machine (VM) redundantly on geographically dispersed physical machines to achieve availability. If one of the running copies of a VM fails, we seamlessly switchover to another running copy. We use Virtual Machine Record/Replay capability to imple-ment this redundancy and switchover. In current progress, we have implemented VM Record/Replay for uniprocessor machines over Linux/KVM and are currently working on VM Record/Replay on shared-memory multiprocessor machines. We report initial experi-mental results based on our implementation. 1...|$|R
40|$|A {{low energy}} {{neutrino}} factory (LENF) is defined, {{for the purpose}} of this report, to accelerate a muon beam to a total energy in the range of 10 - 14 GeV, and store it in a decay ring directing a resulting neutrino beam to a detector 2200 - 2300 km <b>distant.</b> The <b>machine</b> should be ultimately capable of producing 10 {sup 21 } decays toward that detector per year of 10 {sup 7 } s. We consider such a neutrino factory to be the accelerator defined in the Interim Design Report (IDR) of the International Design Study for the Neutrino Factory (IDS-NF), modified to remove the final stage of acceleration, possibly modifying the remaining acceleration stages to adjust the final energy, and replacing the decay ring with one designed for the lower energy and shorter baseline. We discuss modifications to that design which would reduce the cost of the machine at the price of a reduction in neutrino production, down to as low as 10 {sup 20 } decays per year. These modifications will not preclude eventually upgrading the machine to the full production of 10 {sup 21 } decays per year. The eventual cost of a machine which achieves the full production through a series of lower-production stages should not exceed the cost of a machine which is immediately capable of the full production by more than a small fraction of the cost difference between the full production machine and the lowest production stage...|$|R
40|$|In {{this report}} I {{describe}} types of program annotations {{that could be}} used in C++ to enhance the testability, assurance, and overall quality of the code being developed. These annotations are formal, processable assertions which capture constraints and specifications which cannot be discerned from the program code itself. I first describe in some depth previous work in this area, and then try to apply this work to the C++ programming language. 1 Introduction Since their inception, programming languages and their design (and their designers) have taken the view that the programming language is meant to specify some behavior to a computing machine. Early programming "languages" were nothing more than a textual representation of the machine's instruction set. Though we now have languages that are much more <b>distant</b> from the <b>machine</b> in terms of their computational model, they still take a view of only providing the specification of behavior that the computational model is to undertake. This [...] ...|$|R
40|$|Tile {{production}} of photoneutrons by electron acceler-. ators is an unavoidable process at operating energies abi,ve about 10 MeV. These photoneutrons create problems in shielding, personnel protection, and induced radioactivity. This paper attempts {{to describe the}} neutron field spectral and intensity distributions. The initial spectrum and modifying effects of photon shielding materials and concrete room shielding are considered. The relative yields of neutrons and photons {{as a function of}} the primary electron energy are described. Both experimental and Monte Carlo results are presented. It is shown that the average energy of the neutron spectrum is a useful parameter for shielding calculations, fluence-to-dose equivalent conversions, and spectral degradation calculations. 4. The experimenter has measured fluence and then performed calculations to convert this fiuence into dose equivalent. An incorrect assurlption of the spectrum of the neutrons being measured will cause an error in the calculated dose equi. valent. 5. The experimenter has measured fluence or dose equivalent at some point <b>distant</b> from the <b>machine</b> and assumed an inverse square relationship. In the high-scattering geometry of a concrete room this assumption is invalid. It is the purpose of this paper to explore some of these problems experimentally and theoretically and find a better solution to this measurement problem...|$|R
40|$|Understanding energy {{consumption}} behavior provide an insightful knowledge to improve energy efficiency, promote energy conservation, and importantly sustain the human life. However, currently {{energy consumption}} data are being gathered by (smart) energy meters at the household level or through surveys. While gathering data using smart meter is highly reliable, it lacks semantic {{information about how}} energy is consumed (e. g. using appliance). On the other hand, survey allow to gather semantically rich data, but the acquisition of the data is labor-intensive. In this context, social media data data (e. g. twitter, instagram) which are semantically rich and publicly available {{can be used as}} an alternative source of data about energy consumption behavior. However, due to the noisy and ambiguous nature of social media data, the extraction of energy related information from micro posts is very challenging. The aim of this thesis is to introduce a general framework to discover knowledge about energy consumption behaviors from social media data. The framework explores the suitable of social media data as an alternative data source for capturing energy consumption behaviors, and thus to be used to complement conventional data sources. Using the state-of-the-art methods and approaches in social media data analytics field, we compose the framework which structured into three main stages: data collection, data enrichment & processing, and data analysis & visualization. To study the performance of our framework, we set up an experiment aiming at identifying energy consumption behavior patterns in two different world cities: Jakarta (Indonesia) and Amsterdam (The Netherlands). On data collection stage, we collected 1, 306, 336 tweets from both cities. Next, on data enrichment & processing stage, we pre-processed the collected tweets and conduct dictionary-based annotation using our 8, 329 energy consumption related terms. As a result, we identified 509, 471 tweets (39 %) of the corpus as energy consumption related tweets, which categorize into four different energy consumption behaviors: food, dwelling, mobility and leisure. Using the annotated streams as noisy datasets, we implement <b>distant</b> supervision <b>machine</b> learning technique using binomial classifier to identify energy consumption related tweets. Following this approach, we are able to achieve good classifier’s performance on identifying energy consumption related tweets. Finally, on data analysis & visualization stage, we conduct statistical analysis and found strong positive correlation (r = 0. 73) between energy consumption data extracted from social media and actual electricity load. Following this result, we show that social media data {{has the potential to be}} used as supplementary source of information for energy consumption studies...|$|R
40|$|The MES (manufacturing {{execution}} system) {{is mostly}} used from desktop based terminals in a factory. These terminals are <b>distant</b> from the <b>machines</b> and materials {{used on the}} factory floor. To access the information available through MES {{from anywhere in the}} factory floor, use of mobile terminal instead of desktop computers has been proposed. To evaluate two alternative implementation technologies, Web and native, we have de-veloped and compared two prototypes of the MES application. In addition, we have studied the advantages of native and Web approaches through the literature and survey. Mobile devices are categorized by its different platforms and screen sizes. Android, iOS, and Windows phone are most common among them. Mobile applications are platform dependent and an application made for one platform does not work on others. Web ap-plications are platform independent that work on all devices. HTML 5 has introduced some APIs through which a Web app can behave like a native app and can compete with the native app. So, in this thesis we have tried to compare Web and native app and tried to find out which is better for MES applications. A general answer to this question is native because of its better performance. In this thesis, we have analyzed some of the factors that are responsible for the performance difference between a Web app and native app. In addition to this, we have had an online survey to find out what developers think about the development, testing, maintenance and deployment of Web and native technologies. Based on all the data, i. e. literature review, some experiments, feedback from participants and online survey, we made a conclusion that native app is the best solution for mobile MES because native app is more responsive and more secure. However, native apps require more time, effort, cost and skills to be developed and maintained...|$|R
40|$|The {{principal}} {{aim of this}} thesis is to create and expound a distributed-telemanufacturing model (“DTMF” model, for short) {{that can be used}} for final-product realization, with “telemanufacturing” being the remote application of a layered-manufacturing machine and its software to create a model or product. It is envisaged (at no <b>distant</b> date) that <b>machines</b> will be made to be exponentially more accurate and that they will even be able to “create” models or products from a host of different materials and elements. The model to be expounded in the present thesis will, therefore, serve to address problems and issues relevant to such service, which service will, ultimately, be rendered mutually by businesses and − in the e-commerce scenario − by businesses to their customers. The DTMF model comprises an interface, a processing server, a locating agent and a manufacturing resource, with the interface being the overview that the user will have of the telemanufacturing process and the extent to which he/she will be able interactively to submit and glean information on and from a submission. The processing server, in its turn, prepares the submission in such a way as to allow its direct submission to a manufacturing resource, which resource consists of either a layered-manufacturing machine or any digital-input machine. The processing server, therefore, ensures that the submitted design be in the correct format to be interpreted by the manufacturing machine. The locating agent then takes the processed design and locates an appropriate manufacturing resource that matches the user’s specifications and meets the requirements of the processed design. On having received the processed design, the locating agent submits it to a queue at the manufacturing resource. The manufacturing resource is, therefore, controlled by the locating agent in that the locating agent calls up the available manufacturing methods through a web service at each machine. Next, the DTMF model is extended also to allow the use of a design repository, where a design can be searched for and retrieved. This enables a user to produce products on demand by retrieving a stored design and by applying customization, if necessary. The DTMF model, therefore, makes possible not only on-demand manufacturing for current machines but also music of the future, such as final-product realization. Ehlers, E. M., Prof...|$|R
40|$|Unravelling {{biological}} processes {{is dependent on}} the adequate modelling of regulatory mechanisms that determine the timing and spatial patterns of gene expression. In the last decade, a novel regulatory mechanism has been discovered and its biological importance has been increasingly recognised. This mechanism is mediated by RNA molecules named miRNAs that are the product of the maturation of non-coding gene transcripts and act post- transcriptionally usually to dampen or abolish the expression of protein-coding genes. Despite having eluded detection for such a long time, {{it is now clear that}} the elucidation of the expression pattern of many genes cannot be achieved without incorporating the effects of miRNA-mediated regulation. The technical difficulties that the experimental detection of these regulators entailed prompted the development of increasingly sophisticated computational approaches. Gene finding strategies originally developed for coding genes cannot be applied since these non- coding molecules are subject to very different sequence restraints and are too short to exhibit statistical properties that can be easily distinguished from the background. As a result, com- putational tools came to rely heavily on the identification of conserved sequences, <b>distant</b> homologs and <b>machine</b> learning techniques. Recent developments in sequencing technology have overcome some of the limitations of earlier experimental approaches, but pose new computational challenges. At present, the identification of new miRNA genes is therefore the result of the use of several approaches, both computational and experimental. In spite of the advancement that this research field has known in the last several years, we are still not able to formally and rigourously characterise miRNA genes in order to identify whichever sequence, structure or contextual requirements are needed to turn a DNA sequence into a functional miRNA. Efforts using computational algorithms towards the enumeration of the full set of miRNAs of an organism have been limited by strong reliance on arguments of precursor conservation and feature similarity. However, miRNA precursors may arise anew or be lost across the evolutionary history of a species and a newly-sequenced genome may be evolutionarily too distant from other genomes for an adequate comparative analysis. In addition, the learning of intricate classification rules based purely on features shared by miRNA precursors that are currently known may reflect a perpetuating identification bias rather than a sound means to tell true miRNAs from other genomic stem-loops. In this thesis, we present a strategy to sieve through the vast amount of stem-loops found in metazoan genomes in search of pre-miRNAs, significantly reducing the set of candidates while retaining most known miRNA precursors. Our approach relies on precursor properties derived from the current knowledge of miRNA biogenesis, analysis of the precursor structure and incorporation of information about the transcription potential of each candidate. i Our approach has been applied to the genomes of Drosophila melanogaster and Anophe- les gambiae, which has allowed us to show that there is a strong bias amongst annotated pre-miRNAs towards robust stem-loops in these genomes and to propose a scoring scheme for precursor candidates which combines four robustness measures. Additionally, we have identified several known pre-miRNA homologs in the newly-sequenced Anopheles darlingi and shown that most are found amongst the top-scoring precursor candidates for that or- ganism, with respect to the combined score. The structural analysis of our candidates and the identification of the region of the structural space where known precursors are usually found allowed us to eliminate several candidates, but also showed that there is a staggering number of genomic stem-loops which seem to fulfil the stability, robustness and structural requirements indicating that additional evidence is needed to identify functional precursors. To this effect, we have introduced different strategies to evaluate the transcription potential of the remaining candidates which vary according to the information which is available for the dataset under study...|$|R

