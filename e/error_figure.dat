16|553|Public
5000|$|A {{wattmeter}} is used {{to measure}} electrical power. Inexpensive plugin wattmeters, sometimes described as energy monitors, are available from prices of around US$10. Some more expensive models for home use have remote display units. In the US wattmeters can often also be borrowed from local power authorities or a local public library. [...] Although accuracy of measurement of low AC current and quantities derived from it, such as power, is often poor, these devices are nevertheless indicative of standby power, if sensitive enough to register it. Some home power monitors simply specify an <b>error</b> <b>figure</b> such as 0.2%, without specifying the parameter subject to this error (e.g., voltage, easy to measure), and without qualification. Errors of measurement at the low standby powers used from about 2010 (i.e., less than a few watts) may be a very {{large percentage of the}} actual value—accuracy is poor. Modification of such meters to read standby power has been described and discussed in detail (with oscilloscope waveforms and measurements). Essentially, the meter's shunt resistor, used to generate a voltage proportional to load current, is replaced by one of value typically 100 times larger, with protective diodes. Readings of the modified meter have to be multiplied by the resistance factor (e.g. 100), and maximum measurable power is reduced by the same factor.|$|E
40|$|The {{results of}} {{a study of the}} {{feasibility}} of an active method of surface error control using thermal elements are presented. It is shown that the control effort of the thermal elements is sufficient for the purpose, and that such benefits as low cost, low weight, and high reliability may be achieved in conjunction with a significant reduction in the mirror surface <b>error</b> <b>figure...</b>|$|E
30|$|An autoencoder {{is one of}} {{the deep}} {{learning}} algorithm which is basically a three layer neural network that tries to reconstruct the input with minimal <b>error.</b> <b>Figure</b> 1 depicts the working of an autoencoder. The network takes an input and constructs a hidden representation through hidden layer. This is an encoding phase. The output layer just tries to reconstruct the input so the hidden representation can be taken as a code to the original input. Input reconstruction is the decoding phase. This process is completely unsupervised.|$|E
40|$|An {{investigation}} {{is made of}} {{the sensitivity of the}} image quality for the proposed FUSE telescope to mirror misalignments and a wide spatial frequency range of <b>figure</b> <b>errors.</b> Representative <b>figure</b> <b>error</b> data was obtained for the analysis from measurements made on the SEUTS (Solar Extreme Ultraviolet Telescope Spectrograph) telescope mirrors. The tolerancing analysis was carried out with the aid of the Optical Surface Analysis Code (OSAC) program...|$|R
3000|$|Figures 4 and 5 {{give the}} {{comparison}} of the BER performance of SU and the interference power at PU-R between the proposed PC algorithm and the PC algorithm without sensing <b>errors.</b> <b>Figure</b> 4 shows the maximum BER performance of the selected SU link. The BER of proposed algorithm for the given IT level I [...]...|$|R
40|$|The {{marine area}} of the {{south-eastern}} tropical Atlantic (SEA) is associated with warm SST biases in GCM, linked with the equatorial region and impairing the simulated climate in climatically critical land areas s. a. the Amazon basin. Due {{to the presence of}} coastal and equatorial upwelling, the errors are sensitive to wind-stress <b>errors,</b> <b>Figures</b> (a 1 - 3), but equally to misrepresentation of the subsurface ocean waters, Figures (b 1 - 2) ...|$|R
30|$|After the {{conversion}} is done, {{we are able}} to decrease p values and make another evaluation like the one shown in Fig.  5 and compare them to decide if the E_max chosen fits well. In our tests we decided to decrease one bit of each variable between comparisons to increase the <b>error.</b> <b>Figure</b> 6 shows the evaluations obtained. Observing the graphics in Fig.  6 a new error is chosen and {{the conversion}} algorithm is applied again to find new suitable values for m and p.|$|E
40|$|Abstract. The {{measuring}} {{precision of}} error separation technology {{is directly related}} to error separation parameters selection. If the parameter selection rules aren’t followed correctly, the accuracy of error separation will be affected badly, and even the <b>error</b> <b>figure</b> got will be a distortion result. Through a discussion on the characters of error separation technology, eight rules for choosing error separation parameters of multi-probe method are made in this paper. These rules are regarded as the general laws to follow when measuring the geometrical error by multi-probe method. Based on the results of the online measurement experiment, the parameter selection rules are proved to be effective...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedRepeatable accuracy of hydrographic positioning was examined {{in terms of}} the two-dimensional normal distribution function which results in an elliptical <b>error</b> <b>figure.</b> The error ellipse was discussed, and two methods for conversion of elliptical errors to circular errors were given. These methods are "circle of equivalent probability" and "root mean square error" (d). Using the d error concept, repeatable accuracy of ranging, azimuthal, and hyperbolic systems was evaluated, and methods were developed to draw repeatability contours for those systems. A brief theoretical background was provided to explain the method of least squares and discuss its application to hydrographic survey positioning. For ranging, hyperbolic, azimuthal, sextant angle, and Global Positioning System the least squares observation equations were developed. Specific examples were constructed to demonstrate the capabilities of this data adjustment technique when applied to redundant position observations. [URL] Turkish Nav...|$|E
40|$|Figure S 1 - {{related to}} Figure 1 : Schematic {{diagrams}} of putative decision making processes {{used in this}} study and additional behavioral results Figure S 2 - related to Figure 2 : Additional results of the neural correlates of simulated other’s reward and action prediction <b>errors</b> <b>Figure</b> S 3 - related to Figure 4 : Reward prediction error signals in the ventral striatum (vStr) during the Control task. Table S 1 – related to Figure 1 : Best fitting parameter estimate...|$|R
3000|$|..., {{the system}} becomes more {{sensitive}} to the channel estimation <b>error.</b> The <b>figure</b> confirms that the BER performance with imperfect self-information removal for the higher [...]...|$|R
40|$|Edward H. Linfoot {{developed}} an integral expression for the irradiance {{in the image}} of a lens or mirror under the Foucault knife-edge test as a function of <b>figure</b> <b>error.</b> Samuel Katzoff developed a convenient method of inverting the linearized form of Linfoot's equation to express <b>figure</b> <b>error</b> in terms of the irradiance distribution in the image (Foucault pattern). This paper presents the results of an experimental study on a 20 -centimeter-diameter f/ 5 spherical mirror to complement the analytical work of Linfoot and Katzoff. The results clearly affirm the practicability of the Foucault test to quantitative evaluation of <b>figure</b> <b>errors</b> of near-diffraction-limited optical elements via the Linfoot/Katzoff formulation. The evaluation was based on a comparison of Foucault-test <b>figure</b> <b>error</b> data with parallel data from independent scatterplate interferometer measurements. The results are particaularly significant in that they reveal the fallacy of the widespread regard of the Foucault test as limited to qualications for the field of optical testing, since the test is basically simple, its implementation for quantitative <b>figure</b> <b>error</b> analysis is straightforward, and the associated experiemntal data processing is much simpler than that of interferometric testing methods. The question of potential use in <b>figure</b> <b>error</b> sensing in the planned large orbital telescope seems particularly pertinent...|$|R
40|$|Standard LANDSAT- 4 MSS {{digital image}} data were {{analyzed}} for geometric accuracy using two P-format (UTM projection) images of the Washington, D. C. area, scene day 109 (ID number 4010915140) and scene day 125 (ID number 4012515144). Both scenes were tested for geodetic registration accuracy (scene-to-map), temporal registration accuracy (scene-to-scene), and band-to-band registration accuracy (within a scene). The combined RMS error for geodetic registration accuracy was 0. 43 pixel (25. 51 meters), well within specifications. The comparison between the 2 scenes was made on a band-by-band basis. The 90 percent <b>error</b> <b>figure</b> for temporal registration was 0. 68 (57 x 57) pixel (38. 8 meters). Although this figure {{is larger than the}} specification, it can be considered excellent with respect to user application. The best case registration errors between bands 1 and 2, and 3 and 4 were 14. 2 m and 13. 7 m, respectively, both within specifications. The worst case registration error was 38. 0 m between bands 2 and 3...|$|E
40|$|Doctorate in Engineering Mechanics. Upon {{completion}} of the requirements for the Ph. D. in 1997, {{he returned to the}} Materials Directorate in Dayton, Ohio, where he is currently a civilian employee. 169 5000 10000 15000 20000 25000 30000 2 4 6 8 10 12 14 16 18 20 Measured Using Tangent Method (microstrain) Measured Tangent Angle (degrees) 1000 2000 3000 4000 5000 6000 40 60 80 100 120 140 Measured Using Finite-Increment Method (microstrain) Measured Delta Theta (pixels) Average Error Strain Average <b>Error</b> <b>Figure</b> B. 5 : Strain error caused by analysis measurement errors: (a) 2 pixels using the finite-increment method; (b) 1 using the tangent method. 168 Figure B. 5 a shows a plot of this situation. All data were calculated using typical values of strain and scale appropriate for comparison with the strain data presented in Chapter 3. Within this figure, the correct strain is plotted along with the magnitude of strain error caused by the average of 2 pixel errors in the fringe center mea...|$|E
40|$|Geographic routing is an {{attractive}} option for large scale wireless sensor networks (WSNs) because of its low overhead and energy expenditure, but is inefficient in realistic localization conditions. Positioning systems are inevitably imprecise because of inexact range measurements and location errors lead to poor performance of geographic routing in terms of packet delivery ratio (PDR) and energy efficiency. This paper proposes a novel, low-complexity, error-resilient geographic routing method, named conditioned mean square error ratio (CMSER) routing, intended to efficiently make use of existing network information and to successfully route packets when localization is inaccurate. Next hop selection {{is based on the}} largest distance to destination (minimizing the number of forwarding hops) and on the smallest estimated <b>error</b> <b>figure</b> associated with the measured neighbor coordinates. It is found that CMSER outperforms other basic greedy forwarding techniques employed by algorithms such as most forward within range (MFR), maximum expectation progress (MEP) and least expected distance (LED). Simulation results show that the throughput for CMSER is higher than for other methods, additionally it also reduces the energy wasted on lost packets by keeping their routing paths short...|$|E
30|$|Besides Binary {{consensus}} has zero percent <b>error</b> rate. <b>Figure</b> 6 defines binary consensus algorithm simulation for {{a single}} iteration. In algorithms mBX means mainEdges binary state X, BPId means binary protocol ID.|$|R
30|$|As it will {{be shown}} in the {{simulation}} results, the average BER of the LPIC detector exhibits the same semi-convergence behavior as for the norm of the relative <b>error</b> in <b>Figures</b> 6 and 7.|$|R
30|$|Finally, ranking is {{performed}} using our variant of dynamic programming which considers only a subset of possible matches thereby providing a considerable gain in performance {{for the same amount}} of <b>errors</b> (see <b>Figure</b> 12).|$|R
40|$|Coded-aperture {{systems are}} {{indirect}} imaging {{systems that have}} been used to image x-ray and (gamma) -ray sources. Coded aperture systems are also capable of recording tomographic information and because they involve no detector motion they are natural candidates for use in dynamic studies in nuclear medicine. Computer simulations suggest that an orthogonal-view coded-aperture system, which circumvents the problem of limited angular view, is capable of restoring clinically useful tomographic information. The restoration is performed with the aid of the iterative back-projection algorithm which is shown to yield the Moore-Penrose generalized inverse in the limit of many iterations. The convergence behavior of this algorithm is also examined. In order to improve reconstructions, the problems of optimizing coded aperture design is addressed. The concept of "alignment" is introduced in which the aperture parameters are adjusted until the system is tuned to measure well the object class of interest. A mean-square <b>error</b> <b>figure</b> of merit is derived that indicates the degree of alignment of a system. Aperture design may then be seen as a multidimensional optimization problem in which system parameters are adjusted in order to find a global minimum value for the figure of merit. The figure of merit presumes the use of an optimum restoration filter in the reconstruction process. Various restoration algorithms are suggested which fulfill this requirement. Finally, simple proof-of-principle simulations are given that demonstrate a degree of plausibility to the alignment approach...|$|E
40|$|Supplementary Information The S- and R- [4 - 3 H]-NADPH were {{consumed}} by the TM 0449 encoded FDTS under the reaction conditions described in the Materials and Methods section. All analytical procedures were described in detail elsewhere (1, 2). The reaction products were analyzed by reverse phase HPLC followed by a fraction-collector (1 min. per fraction) and the fractions were counted for 5 min. each by a LSC. The identity of the chromatographic peaks was determined using retention times of standards and their UV spectra. The elution order and retention times of relevant materials were: water (THO) (5 min.), dUMP (9 - 10 min., λmax = 263 nm), dTMP (15 - 16 min., λmax = 269 nm), NADP + (20 min., λmax = 258 nm), NADPH (25 - 26 min., λmax = 258 & 340 nm). Studies of S- [4 - 3 H]-NADPH: Consumption of S- [4 - 3 H]-NADPH by the FDTS showed that the pro-S tritium remained exclusively on the NADP + product (Figure 1). These experiments were conducted aerobically and some tritiated water was formed nonenzymatically. This was corrected for by conducting the same experiment without enzyme {{for the same amount}} of time, which resulted in the same amount of tritiated water (within experimental <b>error).</b> <b>Figure</b> 1 : S-[4 - 3 H]-NADPH reaction with FDTS: HPLC tritium (black solid trace) and 14 C (red dotted trace) radiograms of a. S-[4 - 3 H]-NADPH and [2 - 14 C]-dUMP before adding FDTS (see Methods for details). b. The same reaction 3 hours after adding 2. 5 µM of FDTS. Peak identities were determined by the relevant standards and UV spectra (see Methods). a...|$|E
40|$|The {{objective}} of this work {{was to develop a}} method to estimate the crop area of coffee, maize and soybean in the region of Cornélio Procópio, Paraná State, Brazil. This region comprises a total of 23 municipalities. The direct expansion method with regular segments of 1 km x 1 km was used to estimate the crop area. The stratification was carried out with remote sensing satellite images, digital image classification procedure, Geographic Information System (GIS), GPS (Global Positioning System) among others. The sampled area was 1. 26 % of the region which corresponds to 89 segments. These segments were visited during a field work from October 29 th to December 10 th of 2002. Comparison between the results of this research and the official estimates indicated that best results were obtained for soybean area estimation, which occupies 32. 1 % of the region. The soybean area estimation had a difference of 5. 9 % in relation to the official estimate and a coefficient of variation equal to 6. 6 %. The result was available one week {{after the end of the}} field work indicating that the method is fast. The conclusions of the work are: 1) the thematic map of the images makes it possible to stratify the region and can also be helpful to the subjective surveys; 2) the regular segments were easily located in the field, with the use of the GPS; 3) the crop area estimation was fast and met the opportunity criteria; 4) an <b>error</b> <b>figure</b> is associated to the estimates, attending the reliability criteria and; 5) the errors of the estimates varied in response to the cultivated area by each crop in the region...|$|E
40|$|The {{effects of}} {{alignment}} and surface <b>figure</b> <b>errors</b> and their compensation {{with each other}} in optical systems are analyzed based on computer simulations with exact ray tracing data. These effects are included in the prediction of system performance and the testing of optics. Several simple systems are used as examples. In the prediction of system performance, a Ritchey-Chretian telescope and a Reflaxicon system are studied. A correct alignment can be found to compensate certain surface <b>figure</b> <b>errors</b> in the system. This will allow larger surface <b>figure</b> <b>errors</b> to be tolerated in the system. In the testing of optics, a method to separate the <b>figure</b> <b>errors</b> from the alignment error contributions is discussed and an off-axis test configuration, the Ritchey-Common test, is studied thoroughly. A figure design approach is suggested and compared with other approaches for reduction of the measured wavefront data in the Ritchey-Common test...|$|R
2500|$|An {{independent}} {{analysis of}} Silver's state-by-state projections, assessing whether {{the percentages of}} votes that the candidates actually received fell within the [...] "margin of error" [...] of Silver's forecasts, found that [...] "Forty-eight out of 50 states actually fell within his margin of error, giving him a success rate of 96 percent. And assuming that his projected margin of <b>error</b> <b>figures</b> represent 95 percent confidence intervals, which it is likely they did, Silver performed just about exactly {{as well as he}} would expect to over 50 trials. Wizard, indeed". Additional tests of the accuracy of the electoral vote predictions were published by other researchers.|$|R
40|$|OPERATION ARGUMENT Read-args RESULT Read-result ERRORS { accessControlError, objectError, attributeError } ::= 7 Read-args ::= SET { name [0] ObjectName, targetAttributes [1] SET OF AttributeType } Read-result ::= SET { 8 name [0] DistinguishedName, targetAttributes [1] Attributes, <b>errors</b> [2] <b>Errors</b> <b>Figure</b> 3 : A Fragment of the ASN. 1 Formal Notation In {{addition}} to existing formal textual notations {{such as the}} one above, the current working documents have also developed a simple set of drawing conventions to aid the modeling process. These simple conventions are currently being included in the descriptive Information Model and are shown in figure 4 {{so that they can be}} used throughout the rest of the paper...|$|R
40|$|Abstract — This paper {{proposes a}} Contourlet Transform based {{approach}} to the segmentation of corneal blood vessels that is of clinical importance {{in the treatment of}} corneal neovascularisation. The quantification of blood vessels provides means for monitoring the affect of any treatment process being followed. The proposed approach initially uses semi-automated algorithm to detect the corneal area of a high quality colour image of an eye. Subsequently the difference image between the red and green colour planes is subjected to contrast adjustment followed by a novel contrast enhancement algorithm in the Contourlet Transform domain. The enhanced blood vessel images are finally thresholded to form binary images, using which a quantification is carried out based on a measure defined as a ratio of pixels belonging to blood vessels within the area of the cornea. We provide experimental results based on four practical data sets obtained from patients suffering from different levels of corneal neovascularization. damaged cornea with a clear cornea (corneal grafting). Thus timely and effective treatment of corneal neovascularization is important. Quantification of corneal neovascularization provides means to monitor the effectiveness of any treatment process. To this affect many corneal imaging devices are capable of providing medical experts with high quality images that may be manually inspected and quantified. However, in severe cases of corneal neovasculariation, manual inspection and quantification becomes an extremely tedious, time consuming task prone to human <b>error</b> (<b>figure</b> 1 Illustrates the difference between severe and minor cases in terms of number of the blood vessels growing within the cornea). Automated or semiautomated computer aided inspection and measurement provides a valuable and more accurate alternative approach. Index Terms — Contourlet Transform, contrast enhancement, corneal neovascularization, segmentation of blood vessels. ...|$|E
40|$|PSM {{cube map}} LogPSM PSM cube map error LogPSM <b>error</b> <b>Figure</b> 1 : Night-time scene of robots in a hangar with a point light. We compare our {{algorithm}} (LogPSM) to Kozlov’s improved perspective shadow map (PSM) algorithm. Both algorithms use a cube map {{with a total}} resolution of 1024 × 1024. The images have a resolution of 512 × 512. (Left) Compared to a standard cube map, the PSM cube map greatly reduces aliasing artifacts near the viewer, but some aliasing is still visible. The shadows are severely stretched on the back wall. LogPSMs provide higher quality both near the viewer and in the distance. The shadow map grid has been superimposed to aid visualization (grid lines every 20 texels). (Right) An error visualization for both algorithms. We use an error metric m that is essentially the maximum extent of a shadow map texel projected into the image. Green represents no aliasing (m = 1) and dark red (m> 10) represents high aliasing. LogPSMs provide significantly lower maximum error and the error is more evenly distributed. We present a novel shadow map parameterization to reduce perspective aliasing artifacts for both point and directional light sources. We derive the aliasing error equations for both types of light sources in general position. Using these equations we compute tight bounds on the aliasing error. From these bounds we derive our shadow map parameterization, which is a simple combination of a perspective projection with a logarithmic transformation. We then extend existing algorithms to formulate three types of logarithmic perspective shadow maps (LogPSMs) and analyze the error for each type. Compared with competing algorithms, LogPSMs can produce significantly less aliasing error. Equivalently, for the same error as competing algorithms, LogPSMs can produce significant savings in storage and bandwidth. We demonstrate the benefit of LogPSMs for several models of varying complexity. ...|$|E
40|$|This {{synopsis}} presents Harvey Mudd College’s {{entry into}} the 2005 AAAI scavenger hunt competition. We are submiting a lap-controlled robot which uses commodity parts and limited sensors to localize itself and perform arrow following and object recognition. Overview Harvey Mudd College’s {{entry into the}} 2005 AAAI robot scavenger hunt contest consists of a laptop robot, an Evolution Robotics ’ ER 1, with added sensors and software (Figure 1). The ER 1 is a differential-drive robot with a commodity laptop and web camera, IR sensors, and a home-brew sonar. Rather than use Evolution’s ERSP suite of robotic algorithms, we have chosen to implement all the processing we need ourselves. vision, and empirically derived triangular probability distributions of sensor and motion <b>error.</b> <b>Figure</b> 2 shows our map and the hypotheses being maintained during MCL. We developed a visual routine for finding red moldings that appear “naturally ” in our hallways to assist with MCL, but because these landmarks will not be present at the competition, the vision system now runs {{on top of the}} directed wandering routines, taking control as objects come into view. In the absence of target objects, the robot wanders according to Figure 3 ’s state machine. Figure 2 User interface for visualizing the results of MCL. The robot’s position hypotheses after an initial wandering leg and adjustment toward the wall are illustrated. Because of the initial ambiguity in pose, several mirror-symmetry hypotheses remain. Figure 1 Side and front views of our laptop-controlled ER 1 Low-level Control Developed as part of an undergraduate survey of mobile robotics, this entry’s basic operation consists of avoiding obstacles and exploring its environment. Layered atop these routines is an implementation of Monte Carlo Localization (MCL) (Thrun et al., 2001) based on sonar...|$|E
40|$|Quantitative {{analysis}} of serial ECG records {{is the most}} challenging requirement for computer electro-cardiography. A short review on ECG Variability is given. It is explained in which way the HES ECG analysis program has been extended to visualize and quantify ECG beat to beat and record variability. New features such as computation of standard <b>error</b> <b>figures</b> and confidence intervals for measurements have been embedded into the HES ECG analysis program. By means of these features data are available to support appropriate statistical testing of serial ECG recordings. Also, a statistical model is briefly described for estimation {{of the sources of}} variance in multiple serial recordings. 1...|$|R
30|$|The {{results showed}} that aspens were always clustered, and the {{diameter}} distributions indicated different stand structures in the three investigated forest stands. The reliability of the volume and number of large aspen trees varied from relative root mean square <b>error</b> <b>figures</b> above 50 % with fewer sample plots (5 – 10) to values of 25 %– 50 % with 10 or more sample plots. Stand level inventory estimates were also able to detect spatial pattern and {{the shape of the}} diameter distribution. In addition, ALS-based auxiliary information could be useful in guiding the inventories, but caution should be used when applying the ALS-supported inventory technique.|$|R
5000|$|An {{independent}} {{analysis of}} Silver's state-by-state projections, assessing whether {{the percentages of}} votes that the candidates actually received fell within the [...] "margin of error" [...] of Silver's forecasts, found that [...] "Forty-eight out of 50 states actually fell within his margin of error, giving him a success rate of 96 percent. And assuming that his projected margin of <b>error</b> <b>figures</b> represent 95 percent confidence intervals, which it is likely they did, Silver performed just about exactly {{as well as he}} would expect to over 50 trials. Wizard, indeed". Additional tests of the accuracy of the electoral vote predictions were published by other researchers.|$|R
30|$|The metrics also vary {{according}} to the synchronization error required into the mesh network. A high value for the synchronization error may entail that a node is sleeping when it receives a message from its predecessor node, provoking the loss of this message and its later retransmission, therefore increasing the delay. This issue is influenced by two different parameters, namely, the SR and the SI. Both parameters are interrelated because the number of regions into which the entire network is divided strongly affects the SI. Therefore, to avoid losing messages, the network needs frequent and accurate SIs, and obviously the selection of an appropriate number of hops by region. In this sense, a high duration of the SI, along with a high SR parameter, causes an increase in the clock’s drift among nodes of a same region. This causes a poor precision and a considerable synchronization <b>error.</b> <b>Figure</b> 3 also shows how the lack of synchronization results in a higher number of messages lost, thus worsening the throughput. It should be noted that values for SR greater than 5 are not depicted because they lead to a significant accumulative clock drift that affects all metrics very negatively. Intuitively, it may be inferred that, to relieve the loss of messages, the number of synchronization actions should increase, that is, the values for the SI and the SR should be lower. However, under these circumstances, a more often stopping of the dissemination of information occurs, increasing the latency and jitter, as depicted in Figure 3. This important drawback is carefully illustrated in Figure 4, where the latency of a routing node is shown for the set of values WO[*]=[*] 4, AO[*]=[*] 3, SI[*]=[*] 30, and SR[*]=[*] 3. From all data messages injected in the network, it will have some of their corresponding transmissions which may coincide temporarily with the synchronization period (SD). Low values of SI and SR involve a bigger number of undesired coincidences. When this happens, data messages are delayed by SES, because any transmission will not progress until the synchronization process finishes. On the contrary, data and synchronization messages might collide, implying later retransmissions. Retransmitting a synchronization message in a time different from the timestamp encapsulated in it provokes an inaccurate synchronization process. As a consequence, SES does not satisfy the strict delay requirements for data transmission in the reservation-based method, and therefore its main design goal is not accomplished.|$|E
40|$|Based on the <b>error</b> <b>figures</b> {{obtained}} after {{laboratory tests}} {{over a wide}} set of operational rain gauges from the network of the Liguria region, the bias introduced by systematic mechanical errors of tipping bucket rain gauges in the estimation of return periods and other statistics of rainfall extremes is quantified. An equivalent sample size {{is defined as a}} simple index that can be easily employed by practitioner engineers to measure the influence of systematic mechanical errors on common hydrological practice and the derived hydraulic engineering design. A few consequences of the presented results are discussed, with reference to data set reconstruction issues and the risk of introducing artificial climate trends in the observed rain records...|$|R
40|$|In {{this paper}} we discuss an {{innovative}} application of Remotely Piloted Aircraft Systems (RPAS) for the measurement of aviation-related atmospheric pollution. The proposed measurement system is conceived for the high-resolution characterisation {{in space and time}} domains of greenhouse and noxious gases as well as soot particulate. The system employs a Light Detection and Ranging (LIDAR) system performing differential absorption measurement in a bistatic layout. The airborne component consists of a tuneable laser emitter installed on a RPAS and the ground sub-system is composed by a target reference surface (calibrated for reflectance) and a differential transmittance measuring device based on a photo-camera calibrated for radiance. A preliminary assessment of the <b>error</b> <b>figures</b> associated with the proposed system layout is described...|$|R
40|$|We {{study the}} phase {{diagram of the}} BCSOS model with an {{extended}} interaction range using transfer matrix techniques, pertaining to the (100) surface of single component fcc and bcc crystals. The model shows a 2 x 2 reconstructed phase and a disordered flat phase. The deconstruction transition between these phases merges with a Kosterlitz-Thouless line, showing an interplay of Ising and Gaussian degrees of freedom. As in studies of the fully frustrated XY model, exponents deviating from Ising are found. We conjecture that tri-critical Ising behavior may be a {{possible explanation for the}} non-Ising exponents found in those models. Comment: 25 pages in RevTeX 3. 0, seven uuencoded postscript figures, REPLACED because of submission <b>error</b> (<b>figures</b> were not included...|$|R
40|$|This report {{describes}} the equipment, experimental methods, and first results {{at a new}} facility for interferometric measurement of cryogenically-cooled spherical mirrors at the Goddard Space Flight Center Optics Branch. The procedure, using standard phase-shifting interferometry, has an standard combined uncertainty of 3. 6 nm rms in its representation of the two-dimensional surface <b>figure</b> <b>error</b> at 80, and an uncertainty of plus or minus 1 nm in the rms statistic itself. The first mirror tested was a concave spherical silicon foam-core mirror, with a clear aperture of 120 mm. The optic surface was measured at room temperature using standard absolute techniques; and then the change in surface <b>figure</b> <b>error</b> from room temperature to 80 K was measured. The mirror was cooled within a cryostat. and its surface <b>figure</b> <b>error</b> measured through a fused-silica window. The facility and techniques {{will be used to}} measure the surface <b>figure</b> <b>error</b> at 20 K of prototype lightweight silicon carbide and Cesic mirrors developed by Galileo Avionica (Italy) for the European Space Agency (ESA) ...|$|R
40|$|The {{focus of}} this paper is {{end-to-end}} reliable transmission in UMTS environment where TCP, a reliable tranport protocol designed to retransmit information in case of loss, is present at the mobile station as well as the wired portion of the network. The mobile and wireless portion of the network, characterized by significant <b>error</b> <b>figures,</b> relies on ARQ to detect and retransmit corrupted frames. In this work, we propose an analytical model for the operation of TCP over ARQ. We investigate several error patterns corresponding to random, slow and fast fading channels. Our results quantify the enhanced performance of TCP over ARQ in terms of throughput as a function of both loss and error. Keywords end-to-end reliability, TCP over ARQ, UMTS I...|$|R
