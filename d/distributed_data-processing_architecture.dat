0|19|Public
50|$|Lambda <b>architecture</b> is a <b>data-processing</b> <b>architecture</b> {{designed}} to handle massive quantities of data {{by taking advantage of}} both batch- and stream-processing methods. This approach to architecture attempts to balance latency, throughput, and fault-tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of map-reduce.|$|R
50|$|An {{essential}} component of the experiment is the data acquisition (DAQ) system, which manages the data flow from the detector electronics. The requirement for the experiment is to acquire raw data {{at a rate of}} 18 GB/s. This is accomplished by employing parallel <b>data-processing</b> <b>architecture</b> using 24 high-speed GPUs (NVIDIA Tesla K40) to process data from 12 bit waveform digitisers. The set-up is controlled by the MIDAS DAQ software framework. The DAQ system processes data from 1296 calorimeter channels, 3 straw tracker stations, and auxiliary detectors (e.g. entrance muon counters). The total data output of the experiment is estimated at 2 PB.|$|R
40|$|Abstract — The {{design studies}} of the {{embedded}} systems for future <b>distributed</b> <b>data-processing</b> applications assess the cost and performance of the hardware infrastructure. Spec-ifications are derived from user requirements depending on system architecture; delivery time and enabling forecasted technology. Relating requirements to forecasted technology is complex because these aspects are much intertwined. We bridge the gap for the embedded system technology that are major parts {{of the next generation}} radio telescopes, LOFAR and SKA. We were able to take the technology advances into ac-count early in the project. This was possible using hierar-chical and stochastic performance models. Hence we could derive hardware specifications from astronomical applica-tions and assess cost-performance ratio for signal processing scenarios from specifications of forecasted technology...|$|R
40|$|MapReduce is {{a popular}} <b>distributed</b> <b>data-processing</b> system for {{analyzing}} big data in cloud environments. This platform is often used for critical data processing, e. g., {{in the context of}} scientific or financial simulation. Unfortunately, there is accumulating evidence of severe problems - including arbitrary faults and cloud outages - affecting the services that run atop cloud services. Faced with this challenge, we have recently explored multicloud solutions to increase the resilience and availability of MapReduce. Based on this experience, we present system design guidelines that allow to scale out MapReduce computation to multiple clouds in order to tolerate arbitrary and malicious faults, as well as cloud outages. Crucially, the techniques we introduce have reasonable cost and do not require changes to MapReduce or to the users’ code, enabling immediate deployment...|$|R
40|$|We {{consider}} {{a class of}} models for processing networks such as job shops or <b>distributing</b> <b>data-processing</b> systems. The defining features of this class of models are: (1) The network operates in discrete time, such that work is completed during fixed-length periods and work arrivals and transfers occur {{at the start of}} these periods. (2) Work arrivals are stochastic, characterized by a finite mean and variance. (3) Work flows are Markovian in that processing requirements do not depend on how the work got to a station. (4) Production at any station depends on work-in-queue levels through some production function. (5) We can write recursion equations (either exact or approximate) relating the moments of production and queue lengths in one period to the moments of production and queue lengths in the next period...|$|R
40|$|Introduction. The {{problem of}} {{creating}} a distributed measurement system designed to simplify {{the analysis of the}} organs of vision. Proposed the concept creating own flexible measuring system and management software for it. Main part. In this work we analyzed the problem of implementing a wireless distributed information-computational system for ophthalmic research using modern technical devices. Described to functional <b>distributed</b> <b>data-processing</b> system developed and its components. Shown the main characteristics of the developed software by which can control the system input and output information by operator, organizing communication between systems parts and automated information storage in the database After analyzing the characteristics of the system put forward its advantages over analog. Conclusions. It is shown that there is possible to create a measurement system for ophthalmologists using mobile handheld devices, wireless communication systems and a personal computer...|$|R
40|$|The MonALISA (Monitoring Agents using a Large Integrated Services Architecture) {{framework}} {{provides a}} distributed service system capable of controlling and optimizing large-scale, data-intensive applications. An {{essential part of}} managing large-scale, <b>distributed</b> <b>data-processing</b> facilities is a monitoring system for computing facilities, storage, networks, and the {{very large number of}} applications running on these systems in near realtime. All this monitoring information gathered for all the subsystems is essential for developing the required higher-level services—the components that provide decision support and some degree of automated decisions—and for maintaining and optimizing workflow in large-scale distributed systems. These management and global optimization functions are performed by higher-level agent-based services. We present several applications of MonALISA's higher-level services including optimized dynamic routing, control, data-transfer scheduling, distributed job scheduling, dynamic allocation of storage resource to running jobs and automated management of remote services among a large set of grid facilities...|$|R
50|$|SIS is {{controlled}} by an authority composed of representatives of the member nations. Personal data protection is a key responsibility. At a technical level, the participating countries adopted a <b>data-processing</b> star <b>architecture</b> {{made up of a}} central site containing the reference database, known as C-SIS, for which the responsibility is entrusted to the French Republic by the CAAS, and a site by country, known as N-SIS, containing a copy of the database.|$|R
40|$|International audienceThis paper {{describes}} {{the advantages and}} the main functionalities of the robot companion and <b>data-processing</b> <b>architecture</b> in the QuoVADis project. This project targets the people reached of cognitive troubles {{with the objective of}} their assistance at home. The robot is involved in the service which ensures the safety of the patient and plays a part in the cognitive stimulation. The stimulation of the residual cognitive capacities consists in helping the patient to locate himself in time and space, by making him safe in the event of wandering, of confusion and anguish, and by facilitating his communication with the ambient environment. The safety of the patient is based on a telecare system in the residence which aims to allow dependent or weakened people to remain at home and to be helped or "medicalized" remotely in a more reactive way in the event of emergency. The main originality of the QuoVADis project resides in the combination of a robot with a cognitive stimulation system and an integrated telecare system. The robot brings its mobility and multi-modal (audiovisual, tactile) interaction capabilities for improving the quality of the services given by these two systems...|$|R
40|$|Several {{high-throughput}} <b>distributed</b> <b>data-processing</b> applications require multi-hop {{processing of}} streams of data. These applications include continual processing on data streams originating from {{a network of}} sensors, composing a multimedia stream through embedding several component streams originating from different locations, etc. These data-flow computing applications require multiple processing nodes interconnected according to the data-flow topology of the application, for on-stream processing of the data. Since the applications usually sustain for a long period, {{it is important to}} optimally map the component computations and communications on the nodes and links in the network, fulfilling the capacity constraints and optimizing some quality metric such as end-to-end latency. The mapping problem is unfortunately NP-complete and heuristics have been previously proposed to compute the approximate solution in a centralized way. However, because of the dynamicity of the network, it is practically impossible to aggregate the correct state of the whole network in a single node. In this paper, we present a distributed algorithm for optimal mapping of the components of the data flow applications. We propose several heuristics to minimize the message complexity of the algorithm while maintaining the quality of the solution. 1...|$|R
40|$|This {{dissertation}} {{describes the}} development of a <b>distributed</b> data-capture and <b>data-processing</b> framework for use with a network-aware ground penetrating radar. The software that was developed addresses weakness in existingg data processing software, with the main focus being on the distributed capabilities of the framework...|$|R
40|$|Multi-Mission Automated Task Invocation Subsystem (MATIS) is {{software}} that establishes a <b>distributed</b> <b>data-processing</b> framework for automated generation of instrument data products from a spacecraft mission. Each mission may {{set up a}} set of MATIS servers for processing its data products. MATIS embodies lessons learned in experience with prior instrument- data-product-generation software. MATIS is an event-driven workflow manager that interprets project-specific, user-defined rules for managing processes. It executes programs in response to specific events under specific conditions according to the rules. Because requirements of different missions are too diverse to be satisfied by one program, MATIS accommodates plug-in programs. MATIS is flexible in that users can control such processing parameters as how many pipelines to run and on which computing machines to run them. MATIS has a fail-safe capability. At each step, MATIS captures and retains pertinent information needed to complete the step and start the next step. In {{the event of a}} restart, this information is retrieved so that processing can be resumed appropriately. At this writing, it is planned to develop a graphical user interface (GUI) for monitoring and controlling a product generation engine in MATIS. The GUI would enable users to schedule multiple processes and manage the data products produced in the processes. Although MATIS was initially designed for instrument data product generation...|$|R
40|$|The British Geological Survey (BGS) {{operates}} eight geomagnetic observatories {{around the}} world. The data from these observatories are transmitted to Edinburgh, {{where they are}} processed and subjected to rigorous quality control procedures. The data are then disseminated to the community over the internet via a number of channels, including the Edinburgh INTERMAGNET Geomagnetic Information Node (GIN), and BGS's own public website. Increasing demand for real-time or near-real-time observatory data means there is a requirement for institutes to have a robust and scalable <b>data-processing</b> <b>architecture</b> capable of delivering geomagnetic data products over the internet {{in a variety of}} commonly-used formats. BGS have spent the past year developing a new software system for the processing and distribution of our geomagnetic observatory data. The system provides a uniform interface for data access, available to any software capable of making HTTP requests, e. g. web-browsers, command-line tools, and web-enabled software packages such as MATLAB and R. The system provides data in common formats such as IAGA- 2002, WDC, NetCDF, XML and JSON, and enables third-parties to build applications which make use of the data in near-real-time, or to integrate our data into their own systems. We use the same web-service internally to access our observatory data, and have built a browser-based data exploration and quality control tool which we use to support daily observatory operations. ...|$|R
40|$|Monitoring the WLCG {{infrastructure}} {{requires the}} gathering {{and analysis of}} a high volume of heterogeneous data (e. g. data transfers, job monitoring, site tests) coming from different services and experiment-specific frameworks to provide a uniform and flexible interface for scientists and sites. The current architecture, where relational database systems are used to store, to process and to serve monitoring data, has limitations in coping with the foreseen increase in the volume (e. g. higher LHC luminosity) and the variety (e. g. new data-transfer protocols and new resource-types, as cloud-computing) of WLCG monitoring events. This paper presents a new scalable data store and analytics platform designed by the Support for Distributed Computing (SDC) group, at the CERN IT department, which uses a variety of technologies each one targeting specific aspects of big-scale <b>distributed</b> <b>data-processing</b> (commonly referred as lambda-architecture approach). Results of data processing on Hadoop for WLCG data activities monitoring are presented, showing how the new architecture can easily analyze {{hundreds of millions of}} transfer logs in a few minutes. Moreover, a comparison of data partitioning, compression and file format (e. g. CSV, Avro) is presented, with particular attention given to how the file structure impacts the overall MapReduce performance. In conclusion, the evolution of the current implementation, which focuses on data storage and batch processing, towards a complete lambda-architecture is discussed, with consideration of candidate technology for the serving layer (e. g. Elasticsearch) and a description of a proof of concept implementation, based on Apache Spark and Esper, for the real-time part which compensates for batch-processing latency and automates problem detection and failures...|$|R
40|$|Abstract Background Bioinformatic {{analyses}} typically {{proceed as}} chains of data-processing tasks. A pipeline, or 'workflow', is a well-defined protocol, {{with a specific}} structure defined by the topology of data-flow interdependencies, and a particular functionality arising from the data transformations applied at each step. In computer science, the dataflow programming (DFP) paradigm defines software systems constructed in this manner, as networks of message-passing components. Thus, bioinformatic workflows can be naturally mapped onto DFP concepts. Results To enable the flexible creation and execution of bioinformatics dataflows, we have written a modular framework for parallel pipelines in Python ('PaPy'). A PaPy workflow is created from re-usable components connected by data-pipes into a directed acyclic graph, which together define nested higher-order map functions. The successive functional transformations of input data are evaluated on flexibly pooled compute resources, either local or remote. Input items are processed in batches of adjustable size, all flowing one to tune the trade-off between parallelism and lazy-evaluation (memory consumption). An add-on module ('NuBio') facilitates the creation of bioinformatics workflows by providing domain specific data-containers (e. g., for biomolecular sequences, alignments, structures) and functionality (e. g., to parse/write standard file formats). Conclusions PaPy offers a modular framework for the creation and deployment of parallel and <b>distributed</b> <b>data-processing</b> workflows. Pipelines derive their functionality from user-written, data-coupled components, so PaPy also {{can be viewed as}} a lightweight toolkit for extensible, flow-based bioinformatics data-processing. The simplicity and flexibility of distributed PaPy pipelines may help users bridge the gap between traditional desktop/workstation and grid computing. PaPy is freely distributed as open-source Python code at [URL], and includes extensive documentation and annotated usage examples. </p...|$|R
40|$|Introduction : Trends of {{increase}} of ACS and AIS {{and their use}} in everyday life are discussed. The need a voice mode of human interaction with AIS is mentioned. Noticed that network integration of AIS allows to combine their resources and contributes to progress in speech recognition. The emergence of smart phones and their widespread use is the desire to use them as personal voice terminals for access to distributed information networks. Main part : Possibility of use of Android-based personal portable mobile devices (PPMD) like terminals and like autonomous units, as well as possibility of use of Windows-based stationary PC like servers of <b>distributed</b> <b>data-processing</b> system (DDPS) with voice control are considered. Criteria for selection of PPMD and OS of client terminals, as well as requirements DDPS and its structure are formulated. Concept of building of DDPS by "client - server" and "a lot of clients — many servers" technologies are submitted. Concept of a PPMD virtual interface and server virtual interface are offered. Communication between threads within {{the process of the}} PPMD virtual interface of client terminal and the interaction between the processes of the client and server in the autonomous mode, {{as well as in the}} DDPS mode are considered. The results of experimental tests of the prototype of DDPS when exchanging data between Windows and Android clients, and Windows Server are running; the accuracy and reliability of embedded solutions and scalability of DDPS are confirmed. Conclusions : Modern PPMD on Android OS with can be used as terminal devices for construction on the basis of their different specialized voice control DDPS with technology "client - server" and "a lot of customers - many servers". Unification APIs of PPMD with different OS can be done by implementing a virtual PPMD interface. Exchanging data between processes of DDPS better sell through technology Berkeley sockets, which are supported by most modern operating systems. Exchanging data between threads of individual processes better implement with technology of system messages. Application of these approaches allow to create a scalable DDPS with the number of concurrent clients 100 or more with server by PC with Intel Core i 3 CPU and OS Windows XP. </p...|$|R
40|$|Android ? ?????? ?????????? ?? ?????????? ??????, ? ????? ???????????? ?? ?? ???? ?? Windows ? ?????? ???????? ???????????? ????????????-?????????????? ??????? (????) ? ????????? ???????????. ????????????? ???????? ?????? ???? ?? ?? ??? ??????????? ??????????, ? ????? ?????? ?? ???? ?? ?? ?????????. ???????????? ????????? ???????? ???? ?? ??????????? ??????? ? ??????? ?? ??????? ???????? ? ?????? ?????????. ????????????? ????????? ???????????? ?????????? ???? ?? ???????????? ?????????? ???????. ?????????? ????????? ??? ???????? ? ????? ??????? ???????????? ?????????? ???? ???????????? ????????? ?? ????????? ??? ????????? ??????????? ? ????????? ??????? ?? ? ??????????? ????????, ??? ? ?????? ????. Introduction. Trends of {{increase}} of ACS and AIS {{and their use}} in everyday life are discussed. The need a voice mode of human interaction with AIS is mentioned. Noticed that network integration of AIS allows to combine their resources and contributes to progress in speech recognition. The emergence of smart phones and their widespread use is the desire to use them as personal voice terminals for access to distributed information networks. Main part. Possibility of use of Android-based personal portable mobile devices (PPMD) like terminals and like autonomous units, as well as possibility of use of Windows-based stationary PC like servers of <b>distributed</b> <b>data-processing</b> system (DDPS) with voice control are considered. Criteria for selection of PPMD and OS of client terminals, as well as require-ments DDPS and its structure are formulated. Concept of building of DDPS by "client - server" and "a lot of clients ? many servers" technologies are submitted. Concept of a PPMD virtual interface and server virtual interface are offered. Communication between threads within {{the process of the}} PPMD virtual interface of client terminal and the interaction be-tween the processes of the client and server in the autonomous mode, {{as well as in the}} DDPS mode are considered. The results of experimental tests of the prototype of DDPS when exchanging data between Windows and Android clients, and Windows Server are running; the accuracy and reliability of embedded solutions and scalability of DDPS are confirmed. Conclusions. Modern PPMD on Android OS with can be used as terminal devices for construction on the basis of their different specialized voice control DDPS with technology "client - server" and "a lot of customers - many servers". Unification APIs of PPMD with different OS can be done by implementing a virtual PPMD interface. Exchanging data between processes of DDPS better sell through technology Berkeley sockets, which are supported by most modern operating systems. Exchanging data between threads of individual processes better implement with technology of system messages. Application of these approaches allow to create a scalable DDPS with the number of concurrent clients 100 or more with server by PC with Intel Core i 3 CPU and OS Windows XP. ?????????-?? ??????????? ????????????? ???????????? ??????????? ????????? ????????? (????) ?? ???? ?? Android ? ???????? ?????????? ? ?????????? ?????, ? ????? ???????????? ?? ?? ???? ?? Windows ? ???????? ???????? ?????????????? ????????????? ?????????????? ??????? (????) ? ????????? ???????????. ?????????????? ???????? ?????? ???? ? ?? ??? ?????????? ??????????, ? ????? ?????????? ? ???? ? ?? ?????????. ???????????? ????????? ?????????? ???? ?? ?????????? ??????? ? ??????? ? ?????? ???????? ? ????? ?????????. ?????????? ????????? ???????????? ?????????? ???? ? ???????????? ?????????? ???????. ??????????? ?????????????? ????? ???????? ? ?????? ???????? ???????????? ?????????? ???? ??????????? ????????? ? ?????????????? ????? ?????????? ?????????? ? ????????? ??????, ??? ? ?????????? ????????, ??? ? ?????? ????...|$|R
40|$|The {{rapid pace}} of {{urbanization}} {{has an impact}} on climate change and other environmental issues. Currently, 54 % of the global population lives in cities accounting for two-thirds of global energy demand. Sustainable energy generation and consumption is the top humanity’s problem for the next 50 years. Faced with rising urban population and the need to achieve energy efficiency, urban planners are focusing on sustainable, smart energy systems. This has {{led to the development of}} Smart Grids (SG) that employs intelligent monitoring, control and communication technologies to enhance efficiency, reliability and sustainability of power generation and distribution networks. While energy utilities are optimizing energy generation and distribution, consumers play a key role in sustainable energy usage. Several energy services are provided to the consumers to know households' hourly energy consumption, estimate monthly electricity cost and recommendations to reduce energy consumption. Furthermore, advanced services such as demand response, can now control and influence energy demand at the consumer-end to reduce the overall peak demand and re-shape demand profiles. The effectiveness and adoption of these services highly depend on the consumers’ awareness, their participation and engagement. Current energy services seldomly consider consumer preferences such as their daily behavior, comfort level and energy-consumption pattern. In this thesis, we investigate development of personalized energy services that strive to achieve a balance between efficient-energy consumption and user comfort. Personalization refers to tailoring energy services based on individual consumers’ characteristics, preferences and behavior. To develop effective personalized energy services a set of challenges need to be tackled. First, fine-grained data collection at user and appliance level is required (data collection challenge). Mechanisms should be devised to collect fine-grained data at various levels in a non-intrusive way with minimal sensors. Second, personalized energy services require detailed user preferences such as their thermal comfort level, appliance usage behavior and daily habits (user preference challenge). Accurate learning models to derive user preferences with minimal training and intrusion are required. Third, energy services developed needs to be easily scalable, from one household to tens and thousands of households (scalability challenge). Mechanisms should be developed to tackle the deluge of data and support distributed storage and processing. Fourth, energy services should deliver real-time feedback or recommendations so that users can promptly act upon it (real time challenge). This calls for development of distributed and low complexity algorithms. This thesis moves away from traditional SG services [...] which hardly consider consumer preferences and comfort [...] and proposes a novel approach to develop effective personalized energy services. The proposed energy services provide actionable feedback, raise awareness and promote energy-saving behavior among consumers. In this thesis, we follow a bottom-up data-driven methodology to develop personalized energy services at various scales [...] (i) nano: individual households, (ii) micro: buildings and spaces, and (iii) macro: neighborhoods and cities. To this end, we present our approach [...] physical analytics for sustainable, smart energy systems [...] that combines IoT data, physical modeling and data analytics to develop intelligent, personalized energy services. Physical analytics fuses data from various Internet of Things (IoT) devices such as smart meters, smart phones and smart watches, along with physical information such as household type, demographics and occupancy to infer energy-usage patterns, user behavior and discover hidden patterns. This approach is used to learn and model user preferences and energy usage, subsequently, employed to develop personalized energy services. This thesis is organized into three parts. Part I describes how to derive fine-grained information with minimal sensors and intrusion. We present two novel algorithms viz., LocED and PEAT that derive fine-grained information from appliance and user level, respectively. This real-time information is used to raise awareness on energy-usage behavior among occupants. Part II presents personalized energy services targeted at households and buildings. We develop services that shift and/or reduce energy consumption and cost by considering individual consumers’ preferences and comfort. These energy services are aimed at providing actionable feedback to occupants towards sustainable energy usage. Part III presents energy services targeted at neighborhood and city level. These energy services aim to identify target consumers in a neighborhood based on their energy-usage pattern and preferences for various DR programs. Finally, we present <b>data-processing</b> <b>architectures</b> that investigate how to cope with the overwhelming data generated from smart meters towards design and development of sustainable, smart energy systems. This thesis advocates that the design and development of energy services should follow personalized approach with consumer preferences and comfort given paramount importance. Results show that the personalized energy services developed has significant potential to raise awareness, reduce energy consumption and improve user comfort in smart [...] homes, buildings and neighborhoods. Embedded Softwar...|$|R
40|$|For {{a growing}} number of {{biologists}} DNA or protein data are typically retrieved and managed on the Web, and not in the laboratory. A large number of bioinformatics datasets from primary and (thousands of) secondary databases are scattered on the Web in various formats. A biologist end-user might need to access and use tens of databases and tools every day. For this reason, the bioinformatics community is developing more and more service-oriented architectures (SOAs) : software architecture of loosely coupled software services that can be accessed without knowledge of, or control over, their internal <b>architecture.</b> <b>Data-processing</b> and analysis tasks can be automated by having free access to bioinformatics Web services (WSs) that are the building blocks of the SOAs. In this paper we introduce a new bioinformatics Web server, mepsMAP (mapping epitopes on protein surface: Mining Annotated Proteins), developed to identify the recognition sites between antibodies and their cognate antigens. In some cases, the recognition site is represented by a continuous segment of the antigen sequence, but much more often the epitope is "conformational," i. e., the antibody recognizes the location and type of exposed antigen side chains that are not necessarily contiguous in the antigen's sequence, but brought together by its three-dimensional structure. A facility on the server allows the user to search putative conformational epitopes on protein surface, querying the system for proteins with a given annotation. The mepsMAP server has been implemented as a SOA composed by a database and a set of four WSs. We present here the software architecture of the system with {{a detailed description of the}} WS dataflow that has been optimized to provide the best computing performance while maintaining the easiest end-user access to the system via a Web interface...|$|R

