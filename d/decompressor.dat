200|26|Public
25|$|It is {{straightforward}} to compute upper bounds for K(s)nbsp&– simply compress the string s with some method, implement the corresponding <b>decompressor</b> in the chosen language, concatenate the <b>decompressor</b> to the compressed string, {{and measure the}} length of the resulting stringnbsp&– concretely, the size of a self-extracting archive in the given language.|$|E
25|$|The {{system allows}} for the GPU to {{directly}} read data produced by the CPU without going to main memory. In this specific case of data streaming, called Xbox procedural synthesis (XPS), the CPU is effectively a data <b>decompressor,</b> generating geometry on-the-fly for consumption by the GPU 3D core.|$|E
2500|$|Images {{inserted}} into any Office 2004 application by using either {{cut and paste}} or drag and drop result in a file that does not display the inserted graphic when viewed on a Windows machine. Instead, the Windows user is told [...] "QuickTime and a TIFF (LZW) <b>decompressor</b> are needed to see this picture". Peter Clark of Geek Boy's Blog presented one solution in December 2004. However, this issue persists in Office 2008.|$|E
5000|$|The <b>decompressor's</b> {{state machine}} defines the {{following}} three states: ...|$|R
5000|$|All the Quantum {{compressors}} and <b>decompressors</b> require {{at least}} a 386 CPU in order to run.|$|R
40|$|Test data <b>decompressors</b> {{targeting}} {{low power}} scan testing introduce {{significant amount of}} correlation in the test data and thus they tend to adversely affect the coverage of unmodeled defects. In addition, low power decompression needs additional control data which increase the overall volume of test data to be encoded and inevitably increase the volume of compressed test data. In this paper we show that both these deficiencies can be efficiently tackled by a novel pseudorandom scheme and a novel encoding method. The proposed scheme can be combined with existing low power <b>decompressors</b> to increase unmodeled defect coverage and almost totally eliminate control data. Extensive experiments using ISCAS and IWLS benchmark circuits show {{the effectiveness of the}} proposed method when it is combined with state-of-the-art <b>decompressors...</b>|$|R
2500|$|The {{engine is}} a four stroke, 90° V twin cylinder, four valve, [...] {{powerplant}} that delivers {{a maximum of}} [...] at 7000 rpm and is capable of inverted flight as well as tractor and pusher configuration installations. It {{is equipped with a}} 2.95-to-1 reduction gearbox and has a dry weight of 67kg (147lbs). It incorporates an automatic <b>decompressor</b> starter installed on the exhaust valve camshaft. Fuel is delivered via electronic fuel injection. Turbocharging is envisioned for some future versions.|$|E
2500|$|The {{field of}} digital signal {{processing}} relies heavily on operations in the frequency domain (i.e. on the Fourier transform). For example, several lossy image and sound compression methods employ the discrete Fourier transform: the signal is cut into short segments, each is transformed, and then the Fourier coefficients of high frequencies, which {{are assumed to be}} unnoticeable, are discarded. The <b>decompressor</b> computes the inverse transform based on this reduced number of Fourier coefficients. (Compression applications often use a specialized form of the DFT, the discrete cosine transform or sometimes the modified discrete cosine transform.) ...|$|E
5000|$|QWIN.EXE is the Quantum <b>decompressor</b> for 16-bit Windows. It is {{much faster}} than the MS-DOS <b>decompressor</b> and has more {{features}} such as selective decompression.|$|E
40|$|Abstract This paper {{examines}} information-theoretic {{questions regarding}} the difficulty of compressing data versus the difficulty of decompressing data and the role that information loss plays in this interaction. Finite-state compression and decompression are shown to be of equivalent difficulty, even when the <b>decompressors</b> are allowed to be lossy...|$|R
40|$|A {{methodology}} {{is presented}} {{for improving the}} amount of compression achieved by continuous-flow <b>decompressors</b> by using multiple ratios of scan chains to tester channels (i. e., expansion ratios). The idea is {{to start with a}} higher expansion ratio than normal and then progressively reduce the expansion ratio to detect any faults that remain undetected. By detecting faults at the highest expansion ratio possible, the amount of compression can be significantly improved compared with conventional approaches. The expansion ratio is progressively reduced by concatenating scan chains together using MUXes to make fewer and longer scan chains. Selecting which scan chains to concatenate is done by using a dependency analysis procedure that takes into account structural dependencies among the scan chains as well as free-variable dependencies in the logic driving the scan chains to improve the probability of detecting faults. Results for applying the proposed approach to industrial designs using various types of <b>decompressors</b> indicate significant improvements in compression are possible. 1...|$|R
40|$|International audienceWe give a new text {{compression}} scheme {{based on}} Forbidden Words ("antidictionary"). We prove that our algorithms attain the entropy for balanced binary sources. They run in linear time. Moreover, {{one of the main}} advantages of this approach is that it produces very fast <b>decompressors.</b> A second advantage is a synchronization property that is helpful to search compressed data and allows parallel compression. Our algorithms can also be presented as "compilers" that create compressors dedicated to any previously fixed source. The techniques used in this paper are from Information Theory and Finite Automata...|$|R
50|$|In the Unidirectional mode of operation, packets {{are only}} sent in one direction: from {{compressor}} to <b>decompressor.</b> This mode therefore makes ROHC usable over links where a return path from <b>decompressor</b> to compressor is unavailable or undesirable. In order to handle potential decompression errors, the compressor sends periodic refreshes {{of the stream}} context to the <b>decompressor.</b>|$|E
5000|$|If an {{erroneous}} frame escapes detection, the <b>decompressor</b> will blindly use {{the frame}} data {{as if they}} were reliable, whereas in the case of detected erroneous frames, the <b>decompressor</b> can base its reconstruction on incomplete, but not misleading, data.|$|E
50|$|Both the {{compressor}} and the <b>decompressor</b> start in U-mode. They may then transition to O-mode if a usable return link is available, and the <b>decompressor</b> sends a positive acknowledgement, with O-mode specified, to {{the compressor}}. The transition to R-mode is {{achieved in the}} same way.|$|E
40|$|This paper proposes some {{modifications}} to reference {{models of the}} human visual system {{for the benefit of}} a best performing fingerprinting scheme inside wavelet <b>decompressors.</b> Two new algorithms have been developed, based on more accurate modeling for luminance masking and on experimental tests for texture masking. In order to manifest the improvements, the insertion gain of the masks relative to non-perceptual watermarking schemes is calculated and compared to the one obtained by the reference technique. Also, the masks' energy distribution is checked to validate them as weighting functions of the watermark. Anglai...|$|R
40|$|We give a new text {{compression}} scheme {{based on}} Forbidden Words ("antidictionary"). We prove that our algorithms attain the entropy for balanced binary sources. They run in linear time. Moreover, {{one of the main}} advantages of this approach is that it produces very fast <b>decompressors.</b> A second advantage is a synchronization property that is helpful to search compressed data and allows parallel compression. The techniques used in this paper are from Information Theory and Finite Automata. Keywords| Data Compression, Lossless compression, Information Theory, Finite Automaton, Forbidden Word, Pattern Matching. I...|$|R
40|$|Currently, {{there are}} two formats for {{graphics}} {{that are used in}} Web publications: GIF (officially pronounced "jif") and JPEG (also known as JPG, and pronounced "jay-peg"). Each of these standards takes a computer image and compresses it up to 100 times. Today's browsers have built-in <b>decompressors</b> for each format, so many Web page creators do not know which one to use. The common myth is that JPEG creates smaller files, but this is not always true. The intention {{of this article is to}} help Web page creators make an informed decision when selecting a format for each graphic in a Web publication...|$|R
5000|$|... xz, the {{command-line}} compressor and <b>decompressor</b> (analogous to gzip) ...|$|E
50|$|ROHC {{compresses}} these 40 bytes or 60 bytes of overhead typically into {{only one}} or three bytes, by placing a compressor before the link that has limited capacity, and a <b>decompressor</b> after that link. The compressor converts the large overhead to only a few bytes, while the <b>decompressor</b> does the opposite.|$|E
50|$|It is {{straightforward}} to compute upper bounds for K(s) - simply compress the string s with some method, implement the corresponding <b>decompressor</b> in the chosen language, concatenate the <b>decompressor</b> to the compressed string, {{and measure the}} length of the resulting string - concretely, the size of a self-extracting archive in the given language.|$|E
40|$|We give a new text {{compression}} scheme {{based on}} Forbidden Words ("antidictionary"). We prove that our algorithms attain the entropy for equilibrated binary sources. One of the main advantage {{of this approach is}} that it produces very fast <b>decompressors.</b> A second advantage is a synchronization property that is helpful to search compressed data and allows parallel compression. Our algorithms can also be presented as "compilers" that create compressors dedicated to any previously fixed source. The techniques used in this paper are from Information Theory and Finite Automata; as a consequence, this paper shows that Formal Language Theory (in particular Finite Automata Theory) can be useful in Data Compression...|$|R
40|$|Abstract — The {{constructions of}} optical buffers {{is one of}} the most {{critically}} sought after optical technologies in all-optical packet-switched networks, and constructing optical buffers directly via optical Switches and fiber Delay Lines (SDL) has received a lot of attention recently in the literature. A practical and challenging issue of the constructions of optical buffers that has not been addressed before is on the fault tolerant capability of such constructions. In this paper, we focus on the constructions of fault tolerant linear compressors and linear <b>decompressors.</b> The basic network element for our constructions is scaled optical memory cell, which is constructed by a 2 × 2 optical crossbar switch and a fiber delay line. We give a multistage construction of a self-routing linear compressor by a concatenation of scaled optical memory cells. We also show that if the delays, say d 1, d 2, [...] ., dM, of the fibers in the scaled optical memory cells satisfy a certain condition (specifically, the condition in (A 2) given in Section I), then our multistage construction can be operated as a self-routing linear compressor with maximum delay � M−F even after up to F of the M scaled optical memory cells fail to function properly, where 0 ≤ F ≤ M − 1. Furthermore, we prove that our multistage construction with the fiber delays d 1, d 2, [...] ., dM given by the generalized Fibonacci series of order F is the best among all constructions of a linear compressor that can tolerate up to F faulty scaled optical memory cells by using M scaled optical memory cells. Similarly results are also obtained for the constructions of fault tolerant linear <b>decompressors.</b> I...|$|R
40|$|Graphics Processing Units {{have become}} a booster for the {{microelectronics}} industry. However, due to intellectual property issues, {{there is a serious}} lack of information on implementation details of the hardware architecture that is behind GPUs. For instance, the way texture is handled and decompressed in a GPU to reduce bandwidth usage has never been dealt with in depth from a hardware point of view. This work addresses a comparative study on the hardware implementation of diﬀerent texture decompression algorithms for both conventional (PCs and video game consoles) and mobile platforms. Circuit synthesis is performed targeting both a reconﬁgurable hardware platform and a 90 nm standard cell library. Area-delay trade-oﬀs have been extensively analyzed, which allows us to compare the complexity of <b>decompressors</b> and thus determine suitability of algorithms for systems with limited hardware resources...|$|R
50|$|The Bidirectional Reliable mode differs in {{many ways}} from the {{previous}} two modes. The most important differences are a more intensive usage of the feedback channel, and a stricter logic at both the compressor and the <b>decompressor</b> that prevents loss of context synchronization between compressor and <b>decompressor,</b> except for very high residual bit error rates.|$|E
50|$|Self-extracting executables {{contain a}} {{compressed}} application and a <b>decompressor.</b> When executed, the <b>decompressor</b> transparently decompresses and runs the original application. This is especially {{often used in}} demo coding, where competitions are held for demos with strict size limits, as small as 1k.This type of compression is not strictly limited to binary executables, but can also be applied to scripts, such as JavaScript.|$|E
5000|$|Apache Commons Compress Apache Commons Compress project {{contains}} Java implementations of Bzip2 compressor and <b>decompressor.</b>|$|E
40|$|International audienceThree Dimensional Integrated Circuits are an {{important}} new paradigm in which different dies are stacked atop one another, and interconnected by Through Silicon Vias (TSVs). Testing 3 D-ICs poses additional challenges {{because of the need}} to transfer data to the non-bottom layers and the limited number of TSVs available in the 3 D-ICs for the data transfer. A novel test compression technique is proposed that introduces the ability to share tester data across layers using daisy-chained <b>decompressors.</b> This improves the encoding of test patterns substantially, thereby reducing the amount of test data required to be stored on the external tester. In addition, an inter-layer serialization technique is proposed, which further reduces the number of TSVs required, using simple hardware to serialize and deserialize the test data. Experimental results are presented demonstrating the efficiency of the technique proposed...|$|R
40|$|The {{constructions of}} optical queues {{is one of}} the most {{critically}} sought after optical technologies in all-optical packet-switched networks, and constructing optical queues directly via optical Switches and fiber Delay Lines (SDL) has received a lot of attention recently in the literature. A practical and challenging issue in the constructions of optical queues is on the fault tolerant capability of such constructions. In this paper, we focus on the constructions of fault tolerant optical linear compressors and linear <b>decompressors.</b> The basic network element for our constructions is scaled optical memory cell, which is constructed by a 2 × 2 optical crossbar switch and a fiber delay line. We first obtain a fundamental result on the minimum construction complexity of a linear compressor by using fiber delay lines as the storage devices for the packets queued in th...|$|R
40|$|In Digital Signal Processing (DSP), Field Programmable Gate Arrays (FPGAs) are {{becoming}} ubiquitous for their capability to process {{massive amount of}} data in parallel maintaining {{the flexibility of the}} software approach. FPGA chips of major vendors also support partial dynamic programming, namely the ability to change the functionality of portions of FPGA {{while the rest of the}} functionalities remain active. In this way, partial reconfiguration of the FPGA requires a fast reload of a partial bitstream. To this purpose, an improvement of the reconfiguration speed (with the contemporary reduction of the memory occupancy) is obtained by compressing the bitstreams. High performance on board <b>decompressors</b> are required to speed-up the reconfiguration operation. In this paper a new hardware oriented technique for the bitstream compression and decompression is proposed. This technique maintains good compression factors and correspond to a very simple and fast hardware architecture for the compressor block...|$|R
5000|$|Data {{compressors}} generally work {{in one of}} two ways. Either the <b>decompressor</b> can infer what codebook {{the compressor}} has used from previous context, or the compressor must tell the <b>decompressor</b> what the codebook is. Since a canonical Huffman codebook can be stored especially efficiently, most compressors start by generating a [...] "normal" [...] Huffman codebook, and then convert it to canonical Huffman before using it.|$|E
50|$|The {{engine was}} started up by a hand crank and {{helped by the}} use of a <b>decompressor.</b>|$|E
50|$|These {{compress}} {{the original}} script and output a new script {{that has a}} <b>decompressor</b> and compressed data.|$|E
40|$|We give a new text {{compression}} scheme {{based on}} Forbidden Words ("antidictionary"). We prove that our algorithms attain the entropy for balanced binary sources. They run in linear time. Moreover, {{one of the main}} advantages of this approach is that it produces very fast <b>decompressors.</b> A second advantage is a synchronization property that is helpful to search compressed data and allows parallel compression. Our algorithms can also be presented as " that create compressors dedicated to any previously xed source. The techniques used in this paper are from Information Theory and Finite Automata. Keywords| Data Compression, Lossless compression, Information Theory, Finite Automaton, Forbidden Word, Pattern Matching. 1 Introduction We present a simple text compression method called DCA (Data Compression with Antidictionaries) that uses some " information about the text, which is described in terms of antidictionaries. Contrary to other methods that make use, as [...] ...|$|R
40|$|A {{dimension}} extractor is an algorithm {{designed to}} increase the effective dimension – i. e., the amount of computational randomness – of an infinite binary sequence, in order to turn a “partially random ” sequence into a “more random ” sequence. Extractors are exhibited for various effective dimensions, including constructive, computable, space-bounded, time-bounded, and finite-state dimension. Using similar techniques, the Kučera-Gács theorem is examined {{from the perspective of}} decompression, by showing that every infinite sequence S is Turing reducible to a Martin-Löf random sequence R such that the asymptotic number of bits of R needed to compute n bits of S, divided by n, is precisely the constructive dimension of S, which is shown to be the optimal ratio of query bits to computed bits achievable with Turing reductions. The extractors and <b>decompressors</b> that are developed lead directly to new characterizations of some effective dimensions in terms of optimal decompression by Turing reductions. ...|$|R
40|$|One of {{the main}} {{problems}} in all-optical packet-switched networks {{is the lack of}} optical buffers, and one feasible technology for the constructions of optical buffers is to use optical crossbar Switches and fiber Delay Lines (SDL). In this two-part paper, we consider SDL constructions of optical queues with a limited number of recirculations through the optical switches and the fiber delay lines. Such a problem arises from practical feasibility considerations. In Part I, we have proposed a class of greedy constructions for certain types of optical queues, including linear compressors, linear <b>decompressors,</b> and 2 -to- 1 FIFO multiplexers, and have shown that every optimal construction among our previous constructions of these types of optical queues under the constraint of a limited number of recirculations must be a greedy construction. Specifically, given M ≥ 2 and 1 ≤ k ≤ M − 1, we have shown that to find an optimal construction, it suffices to find an optimal sequence d∗M 1 ∈ GM,k such that B(d∗M 1; k) = maxdM 1 ∈GM,k B(d M 1; k), where B(...|$|R
