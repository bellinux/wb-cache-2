9|0|Public
40|$|Despite {{impressive}} {{demonstrations of}} human infants ' intersensory capabilities {{over the past}} several decades, there has been little focus on the contributions of prenatal and postnatal experience or the specific developmental processes underlying the emergence of intersensory functioning. Research with nonhuman animals has, however, provided a number of advances in understanding early <b>intersensory</b> <b>perception.</b> The authors explore the value of a comparative, convergent-operations approach to the study of early <b>intersensory</b> <b>perception</b> and examine how this approach as highlighted the study of (a) prenatal factors, (b) brain-behavior relations, and (c) context and experience variables contributing to infants' intersensory esponsiveness. Examples of how human and animal research programs can cross-fertilize one another in their attempts o understand developmental processes underlying <b>intersensory</b> <b>perception</b> are considered. After all, it is only by comparing that we can judge, for our knowledge rests entirely on the relations that things have with others that are similar or different, and we should realize that if there were no animals, the nature of man would be even more incomprehensible. ~G. L. Buffon, Historie Naturell...|$|E
40|$|It is {{well known}} that {{simultaneous}} presentation of incongruent audio and visual stimuli can lead to illusory percepts. Recent data suggest that distinct processes underlie non-specific intersensory speech as opposed to non-speech perception. However, the development of both speech and non-speech <b>intersensory</b> <b>perception</b> across childhood and adolescence remains poorly defined. Thirty-eight observers aged 5 to 19 were tested on the McGurk effect (an audio-visual illusion involving speech), the Illusory Flash effect and the Fusion effect (two audio-visual illusions not involving speech) to investigate the development of audio-visual interactions and contrast speech vs. non-speech developmental patterns. Whereas the strength of audio-visual speech illusions varied as a direct function of maturational level, performance on non-speech illusory tasks appeared to be homogeneous across all ages. These data support the existence of independent maturational processes underlying speech and non-speech audio-visual illusory effects...|$|E
40|$|The {{present study}} {{examined}} whether infant-directed (ID) speech facilitates intersensory matching of audio [...] visual fluent speech in 12 -month-old infants. German-learning infantsâ audio [...] visual matching ability of German and French fluent speech {{was assessed by}} using {{a variant of the}} intermodal matching procedure, with auditory and visual speech information presented sequentially. In Experiment 1, the sentences were spoken in an adult-directed (AD) manner. Results showed that 12 -month-old infants did not exhibit a matching performance for the native, nor for the non-native language. However, Experiment 2 revealed that when ID speech stimuli were used, infants did perceive the relation between auditory and visual speech attributes, but only in response to their native language. Thus, the findings suggest that ID speech might have an influence on the <b>intersensory</b> <b>perception</b> of fluent speech and shed further light on multisensory perceptual narrowing...|$|E
40|$|The {{present study}} {{examined}} {{when and how}} the ability to cross-modally match audio-visual fluent speech develops in 4. 5 -, 6 - and 12 -month-old German-learning infants. In Experiment 1, 4. 5 - and 6 -month-old infants ’ audio-visual matching ability of native (German) and non-native (French) fluent speech was assessed by presenting auditory and visual speech information sequentially, that is, {{in the absence of}} temporal synchrony cues. The results showed that 4. 5 -month-old infants were capable of matching native as well as non-native audio and visual speech stimuli, whereas 6 -month-olds perceived the audio-visual correspondence of native language stimuli only. This suggests that intersensory matching narrows for fluent speech between 4. 5 and 6 months of age. In Experiment 2, auditory and visual speech information was presented simultaneously, therefore, providing temporal synchrony cues. Here, 6 -month-olds were found to match native as well as non-native speech indicating facilitation of temporal synchrony cues on the <b>intersensory</b> <b>perception</b> of non-native fluent speech. Intriguingly, despite the fact that audio and visual stimuli cohered temporally, 12 -month-olds matched the non...|$|E
40|$|International audienceThe {{present study}} {{examined}} {{when and how}} the ability to cross-modally match audio-visual fluent speech develops in 4. 5 -, 6 - and 12 -month-old German-learning infants. In Experiment 1, 4. 5 - and 6 -month-old infants' audio-visual matching ability of native (German) and non-native (French) fluent speech was assessed by presenting auditory and visual speech information sequentially, that is, {{in the absence of}} temporal synchrony cues. The results showed that 4. 5 -month-old infants were capable of matching native as well as non-native audio and visual speech stimuli, whereas 6 -month-olds perceived the audio-visual correspondence of native language stimuli only. This suggests that intersensory matching narrows for fluent speech between 4. 5 and 6 months of age. In Experiment 2, auditory and visual speech information was presented simultaneously, therefore, providing temporal synchrony cues. Here, 6 -month-olds were found to match native as well as non-native speech indicating facilitation of temporal synchrony cues on the <b>intersensory</b> <b>perception</b> of non-native fluent speech. Intriguingly, despite the fact that audio and visual stimuli cohered temporally, 12 -month-olds matched the non-native language only. Results were discussed with regard to multisensory perceptual narrowing {{during the first year of}} life...|$|E
40|$|Multiple-exemplar {{training}} with stimuli in four domains induced two new fill-based (A 1 ' and A 2 ') and satellite-image-based (B 1 ' and B 2 ') perceptual classes. Conditional discriminations were established between the endpoints of the A 1 ' and B 1 ' classes {{as well as}} the A 2 ' and B 2 ' classes. The emergence of linked perceptual classes was evaluated by the performances occasioned by nine cross-class probes that contained fill variants as samples and satellite variants as comparisons, along with nine other cross-class probes that consisted of satellite variants as samples and fill variants as comparisons. The 18 probes were first presented serially and then concurrently. Class-consistent responding indicated the emergence of linked perceptual classes. Of the linked perceptual classes, 70 % emerged during the initial serial test. An additional 20 % of the linked perceptual classes emerged during the subsequently presented concurrent test block. Thus, linked perceptual classes emerged on an immediate or delayed basis. Linked perceptual classes, then, share structural and fuctional similarities with equivalence classes, generalized equivalence classes, cross-modal classes, and complex maturally occurring categories, and may clarify processes such as <b>intersensory</b> <b>perception...</b>|$|E
40|$|Abstract—This paper {{discusses}} {{an integrated}} {{model of a}} robot’s sensory and perceptual capabilities based {{on one of the}} earliest forms of self-knowledge that humans develop, knowledge of the Ecological Self. The Ecological Self is a cohesive model of the body and senses learned through the experience of using them together. This unified model allows kinematic and sensory data to be combined, producing an <b>intersensory</b> <b>perception</b> grounded in both inputs. Taking inspiration from this Ecological Self, but building on modern engineering practices, this model allows a robot to learn the kinematics of its end-effector by witnessing its motion in its visual field. This property of adaptation through self-observation also allows the model to adapt to changes in the robot’s kinematic structure, {{as in the case of}} tool use. A final refinement is performed over the combined visual-kinematic model and is demonstrated to improve not only the accuracy of the kinematic model, but also the robot’s stereo vision calibration. This refinement is inspired by the hypothetical process by which infants learn about their selves. The system is demonstrated to require fewer than 200 motion samples to fully train, to predict end-effector position within 2. 29 mm (SD= 0. 10) and 2. 93 pixels (SD= 3. 83), to learn the lengths of the linkages in the robot’s arm to within 1. 1 mm, and to adapt to tool use after only 52 samples. I...|$|E
40|$|Models of Object Shape: The Role of Category Learning and Action Max Lungarella and Rolf Pfeifer (AI-Laboratory, University of Zurich, Switzerland) : Robots as Cognitive Tools Lorenzo Natale (Lira-Lab, University of Genoa, Italy) : Visuo-Acoustic Cue Integration in an Artificial Developing Agent Michael Thomas (Neurocognitive Development Unit, University College London, UK) : Developmental {{disorders}} as atypical {{trajectories of}} development: Empirical and computational approaches Mark Johnson (Centre for Brain and Cognitive Development, Birkbeck College, London, UK) :Interacting {{factors in the}} development of face processing Posters Luc Berthouze and Adriaan Tijsseling: Embodiment is Meaningless without Adequate Neural Dynamics Joanna Bryson: Embodiment vs. Memetics: Does Language Need a Physical Plant? Michael H. Coen: Issues in <b>Intersensory</b> <b>Perception</b> for Interactive Systems Steve R. Howell, Suzanna Becker and Damian Jankowicz: Modelling Language Acquisition: Lexical Grounding Through Perceptual Features Joris van Looveren: Self-Organization of a Lexicon in Embodied Agents Koji Morikawa, Sameer Agarwal, Charles Elkan and Garrison W. Cottrell: A Taxonomy of Computational and Social Learning Natsuki Oka, Koji Morikawa, Takanori Komatsu, Kentaro Suzuki, Kazuo Hiraki, Kazuhiro Ueda, Takashi Omori: Embodiment without a Physical Body Jochen Triesch: The Role of a priori Biases in Unsupervised Learning of Visual Representations: A Robotics Experiment (Will not be presented at the workshop.) Rachel Wood: The Brains Bodies Build: Some Preliminary Ideas on Allometry in Ontogenetic Adaptation Jelle Zuidema and Gert Westermann: Towards Formal Models of Embodiment and Selforganisation of Language Program Committee Rolf Pfeifer (co-chair, AI Lab, University of [...] ...|$|E

