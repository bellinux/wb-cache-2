73|25|Public
50|$|Software {{to manage}} data quality (e.g. for <b>duplicate</b> <b>removal</b> or {{handling}} of bad records) and uniformity {{may also be}} needed. In lieu of purchasing software, various companies provide an assortment of VDP-related print file, mailing and data services.|$|E
5000|$|The SQL [...] {{operator}} {{takes the}} distinct rows of one query and returns the rows {{that do not}} appear in a second result set. The [...] operator does not remove duplicates. For purposes of row elimination and <b>duplicate</b> <b>removal,</b> the [...] operator does not distinguish between [...]|$|E
5000|$|The SQL [...] {{operator}} {{takes the}} results of two queries and returns only rows that appear in both result sets. For purposes of <b>duplicate</b> <b>removal</b> the [...] operator does not distinguish between [...] The [...] operator removes duplicate rows from the final result set. The [...] operator does not remove duplicate rows from the final result set.|$|E
40|$|Abstract. In {{this paper}} we refine {{external}} exploration for explicit state model checking by a fusion with internal bitstate hashing. External A* provides {{a method to}} cope up with large state spaces by efficiently utilizing secondary storage devices like harddisk to maintain the open and closed lists. Duplicates are removed by a two-level refinement scheme that involves sorting {{a subset of the}} open list externally and subtracting a small subset of closed list from the open list. The bottleneck in External A * [7] is the <b>duplicates</b> <b>removal</b> phase that dominates the I/O complexity of External A*. Bitstate hashing provides a solution to faster <b>duplicates</b> <b>removal</b> by utilizing only few bits for each state. But bitstate hashing is faced with the problem of having no support for large open list and for solution reconstruction. We present a strategy to accelerate external search by using bitstate hashing for <b>duplicates</b> <b>removal.</b> Case studies with our experimental external explicit state model checker IO-HSF-SPIN illustrate the effectiveness of the proposed algorithms. ...|$|R
40|$|The {{effect of}} <b>duplicate</b> isolate <b>removal</b> {{strategies}} on Staphylococcal aureus susceptibility to oxacillin was com-pared by using antimicrobial test results for 14, 595 isolates from statewide surveillance in Hawaii in 2002. No removal {{was compared to}} most resistant and most susceptible methods at 365 days and to the National Committee for Clinical Laboratory Standards (NCCLS) and Cerner algo-rithms at 3 -, 10 -, 30 -, 90 -, and 365 -day analysis periods. Overall, no removal produced the lowest estimates of sus-ceptibility. Estimates with either NCCLS or Cerner differed by < 2 % when the analysis period was the same; with either method, the difference observed between a 90 - and a 365 -day period was < 1 %. The effect of <b>duplicate</b> isolate <b>removal</b> was greater for inpatient than outpatient settings. Considering the ease of implementation and comparabilit...|$|R
40|$|In our paper, {{we present}} main {{results of the}} Czech grant project Internet as a Language Corpus, whose aim was to build a corpus of Czech web texts and to develop and {{publicly}} release related software tools. Our corpus {{may not be the}} largest web corpus of Czech, but it maintains very good language quality due to high portion of human work involved in the corpus development process. We describe the corpus contents (2. 65 billions of words divided into three parts – 450 millions of words from news and magazines articles, 1 billion of words from blogs, diaries and other non-reviewed literary units, 1. 1 billion of words from discussions messages), particular steps of the corpus creation (crawling, HTML and boilerplate <b>removal,</b> near <b>duplicates</b> <b>removal,</b> language filtering) and its automatic language annotation (POS tagging, syntactic parsing). We also describe our software tools being released under an open source license, especially a fast linear-time module for removing near-duplicates on a paragraph level...|$|R
40|$|We {{present a}} sound and {{complete}} rule set for de-termining whether sorting and <b>duplicate</b> <b>removal</b> {{operations in the}} query plan of XPath expressions are unnecessary. Additionally we define a deter-ministic finite automaton that illustrates how these rules can be translated into an efficient al-gorithm. This work is {{an important first step}} in the understanding and tackling of XPath/XQuery optimization problems that are related to ordering and <b>duplicate</b> <b>removal.</b> ...|$|E
3000|$|... (S 2) <b>Duplicate</b> <b>Removal,</b> when {{publications}} indexed by {{more than}} one digital library were identified and the duplications were removed.|$|E
40|$|The {{relational}} data model {{is based on}} sets of tuples, i. e. it does not allow duplicate tuples in a relation. Many database languages and systems do require multi-set place, {{the high costs of}} <b>duplicate</b> <b>removal</b> in database operations is often prohibitive for the use of a data model that does not duplicates. semantics though, either becawe of functional require-ments or because of the high costs of <b>duplicate</b> <b>removal</b> in database operations. Several proposals have been presented that discuss multi-set semantics. As these proposals tend to be either rather practical, Iachng the formal background, or rather formal, lacking the connection to database practice, the gap between the-ory and practice has not been spanned yet. This pape...|$|E
40|$|Abstract- This paper {{describes}} pre-processing {{phase of}} ontology graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured {{representation of the}} input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, <b>removal</b> of <b>duplicate</b> terms, <b>removal</b> of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms...|$|R
40|$|Aim To {{evaluate}} {{the effects of}} systemic antibiotics in combination with scaling and root planing (SRP) on periodontal parameters, tooth loss and oral health-related {{quality of life in}} diabetes patients. Materials and Methods Two independent reviewers screened for controlled clinical trials with at least 6 -month fol-low-up in six electronic databases, registers of clinical trials, meeting abstracts and four major dental journals. After <b>duplicates</b> <b>removal,</b> electronic and hand searches yielded 1, 878 records; 18 full-text articles were independently read by two reviewers. To {{evaluate the}} additional effect of antibiotic usage, pooled weighted mean differences and 95 % confidence intervals were calculated using a fixed effects model. Results Five studies met the inclusion criteria, four of which were included in meta-analyses. The meta-analyses showed a significant effect favouring SRP plus antibiotic for reductions in mean probing depth (PD) (- 0. 22 mm [- 0. 34,- 0. 11]) and mean percentage of bleeding on probing (BoP) (4 % [- 7,- 1]). There was no significant effect for clinical attachment level gain and plaque index reduction. No study reported on tooth loss and oral health-related quality of life. Conclusion Adjunctive systemic antibiotic use in diabetic patients provides a small additional benefit in terms of reductions in mean PD and mean percentage of BoP...|$|R
40|$|This paper {{describes}} pre-processing {{phase of}} ontology graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured {{representation of the}} input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, <b>removal</b> of <b>duplicate</b> terms, <b>removal</b> of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms. Comment: 6 pages, 17 figures, 1 table, "Published with International Journal of Engineering Trends and Technology (IJETT) ...|$|R
40|$|In 1995 {{a method}} called Reversed <b>Duplicate</b> <b>Removal</b> (RDR) was {{introduced}} in Christensen and Ingwersen (1995), and it was further enhanced in Ingwersen and Christensen (1997). The RDR-method enables performing analyses of the overlap consisting of identical documents held in different files when performing online isolation and analyses of datasets...|$|E
40|$|In this paper, we {{describe}} the <b>duplicate</b> <b>removal</b> component of Infolab’s 1 question answering system that contributed to CSAIL’s entry of TREC- 15 2 Question Answering track. The goal of the Question Answering Track is to provide short, succinct answers to English sentences posed by users. In answering definition questions, we are asked to retrieve new and relevant information, {{in the form of}} short sentences or fragments from newswire text. Because many news articles overlap in content, we need to employ a <b>duplicate</b> <b>removal</b> method before presenting the results to the users. Here we present two different approaches to <b>duplicate</b> <b>removal.</b> Our first approach uses the BLEU score, a commonly used metric for machine translation evaluation, as the similarity metric between sentences. Our second approach takes a list of candidate answers, and clusters answers using word-level edit-distance as the similarity metric; the best answer from each cluster is chosen as the representative. In this paper we compare these two approaches and determine their relative performances in the duplicate detection task. 2. Background Determining if two sentences have the same meaning is important for almost all subfields of natural language research. 3 For example, if we are looking for the date of birth of Bill Gates and the document contains the sentence “William H. Gates was born in 1955 ”; a natural language system needs to understand that William H. Gates and Bil...|$|E
40|$|Motivation: Throughout {{the recent}} years, 454 {{pyrosequencing}} {{has emerged as}} an efficient alternative to traditional Sanger sequencing and is widely used in both de novo whole-genome sequencing and metagenomics. Especially the latter application is extremely sensitive to sequencing errors and artificially duplicated reads. Both are common in 454 pyrosequencing and can create a strong bias in the estimation of diversity and composition of a sample. To date, there are several tools that aim to remove both sequencing noise and duplicates. Nevertheless, <b>duplicate</b> <b>removal</b> is often based on nucleotide sequences {{rather than on the}} underlying flow values, which contain additional information. Results: With the novel tool JATAC, we present an approach towards a more accurate <b>duplicate</b> <b>removal</b> by analysing flow values directly. Making use of previous findings on 454 flow data characteristics, we combine read clustering with Bayesian distance measures. Finally, we provide a benchmark with an existing algorithm. Availability: JATAC is freely available under the General Publi...|$|E
40|$|Introduction: Hypertension {{is one of}} {{the major}} risk factors for {{cardiovascular}} disease. Coffee consumption has been associated with hypertension leading health professionals advise against him, it being one of the most consumed beverages in the world. Objectives: To evaluate the effect of coffee consumption in systolic and diastolic blood pressure in people over 18 years with hypertension. Methods: A systematic review following the principles proposed by the Cochrane Handbook was performed by a systematic search in CINAHL, MedicLatina, Cochrane Central Register of Controlled Trials, Cochrane Database of Systematic Reviews, Nursing & Allied Health Collection; Scielo; Elsevier; Pubmed; Google Scholar from January 2008 to May 2016. The inclusion criteria were: adults with 18 or more years with hypertension. Exclusion criteria: childs, pregnant women, diabetes, endocrine or metabolic disorders, cardiovascular disease, liver dysfunction, nephopathy, other serious disease, or caffeine/coffee hypersensitivity. Results: 5865 studies were identified, after applying the selection criteria and <b>duplicates</b> <b>removal</b> we stayed with 5 studies. For the critical appraisal we used the scale from the Centre for Evidence Based Medicine of FML and JBI Critical Appraisal Checklist for Cohort and Case-control studies. Preliminary results shows contradictory aspects because studies suggests that coffee consumption increases the risk of cardiovascular diseases in hypertension and by other hand hydroxyhydroquinonereduced coffee decreased blood pressure in subjects with mild hypertension. Conclusions: We found a shortness of quality studies available on this subject and the methodological limitations and differences in interventions make it difficult to compare study results. info:eu-repo/semantics/publishedVersio...|$|R
40|$|Introduction: Rheumatoid {{arthritis}} (RA) is {{a chronic}} autoimmune joint disease {{with an increased}} risk of joint damage and disability if disease control isn´t achieved and optimized from early on. Traditionally RA patients are managed by planned medical consultations every 3 – 12 months; however several recent studies have been demonstrated that this work can be effectively performed by trained nurses. Objectives: To determine the effectiveness of nurse led consultations in disease activity, fatigue, pain, quality of life, self-efficacy, treatment adherence in patients with RA when compared to rheumatologists. Methods: A systematic review following the principles proposed by the Cochrane Handbook was performed by a systematic search in PubMed, Embase and EBSCO from 2005 to 30 December 2015 (with monthly updates). Two reviewers independently selected articles, collected data from studies and carried out a manual search of the references of the included studies. For the critical appraisal of the studies we used the scale from Centre for Evidence Based Medicine of Faculty of Medicine Lisbon, and the primary outcome were disease activity, fatigue, pain, quality of life, self-efficacy, and treatment adherence. We excluded studies where the patients didn´t have RA in low disease activity. Results: 126 studies were identified and after <b>duplicates</b> <b>removal,</b> we proceeded with 100 studies. The selection by title and abstract selection leaves us with 11 studies for full reading. Our final selection included 13 (2 gathered by manual search). Preliminary results show that nursing consultations are a good alternative to rheumatologist consultations without deterioration of disease control. Conclusions: Nursing consultation can enhance patient’s self-efficacy, treatment adherence and pain control. info:eu-repo/semantics/publishedVersio...|$|R
40|$|Adds some {{miscellaneous}} improvements. Grouped plots {{now work}} better, see [URL] A datetime conversion dialog {{has been added}} as shown in [URL] Changes Added datetime conversion dialog Changes to make more compatible with matplotlib 2. 0 Can now plot grouped single plots for some plot types (line, bar, scatter) Fixed sharey, sharex for grouped plots Added ability to choose pre-defined mpl styles Fix to memory leak on redraws Changes to right click copy behaviour in table snapcraft. yaml to create snap package Fix for displaying small floats that were rounded to zero Added floating precision to table preferences Clipboard bug fix Improvement to merge dialog Fixes to widget styling issues on linux Added <b>duplicate</b> column <b>removal</b> in clean dat...|$|R
40|$|Our {{experience}} with the SIFT [YGM 95] information dissemination system (in use by over 7, 000 users daily) has identified an important and generic dissemination problem: duplicate information. In this paper we explain why duplicates arise, we quantify the problem, and we discuss why it impairs information dissemination. We then propose a <b>Duplicate</b> <b>Removal</b> Module (DRM) for an information dissemination system. The removal of duplicates operates on a per user, per document basis [...] each document read by a user generates a request, or a duplicate restraint. In wide-area environments, the number of restraints handled is very large. We consider {{the implementation of a}} DRM, examining alternative algorithms and data structures that may be used. We present a performance evaluation of the alternatives and answer important design questions such as: Which implementation is the best? With "best" scheme, how expensive will <b>duplicate</b> <b>removal</b> be? How much memory is required? How fast can restraints be p [...] ...|$|E
40|$|This work aims at {{addressing}} {{the question of}} whether the new CASAVA 1. 8, which boasts improvements such as local realignments of reads, is at par with the well accepted pipeline of BWA mapping, <b>duplicate</b> <b>removal,</b> local realignment, re-calibration and variant calling using GATK. We therefore compare the two methods on chromosome 21 of a Yoruba trio and compare the results to the genotype identified by the 1000 genomes project. |$|E
40|$|Summary: SEAL is a {{scalable}} {{tool for}} short read pair mapping and <b>duplicate</b> <b>removal.</b> It computes mappings {{that are consistent}} with those produced by BWA and removes duplicates according to the same criteria employed by Picard MarkDuplicates. On a 16 -node Hadoop cluster, it is capable of processing about 13 GB per hour in map+rmdup mode, while reaching a throughput of 19 GB per hour in mapping-only mode...|$|E
40|$|ABSTRACT: This paper {{presents}} and evaluates several schemes for handling duplicate tuple elimination during optimization {{and execution of}} large selectproject-join queries. The primary issues investigated are (1) precisely when to apply <b>duplicate</b> tuple <b>removal</b> during query evaluation, and (2) how an optimizer should predict the effects of removing duplicates. We also develop a realistic model of multiple join queries inspired by a proposed datamining application. Through experiments on this model, we find two critical techniques for high performance execution of select-project-join queries: First, the optimizer should decide where duplicates are removed within the query plan independent of the projections creating them. Second, join algorithms should remove duplicates when sorting or hashing their input, and the optimizer should be capable of predicting its effects. 1...|$|R
40|$|AIM: To {{perform a}} {{systematic}} review to establish whether blind injections of the gleno-humeral (GHJ) joint {{may be an}} accurate alternative to injections performed imaging guidance, considering multiple anatomical approaches. MATERIALS AND METHODS: Our search strategy yielded 478 articles for Scopus, 815 articles for MEDLINE, 128 articles for Cochrane Central Register of Controlled Trials and 555 articles for Embase until May 2016. One hundred and sixty-seven abstracts were retrieved after <b>duplicates</b> <b>removal.</b> Two readers independently reviewed all the 1067 abstracts. They selected for the full-text analysis only the abstracts in which the accuracy of intra-articular position of the needle was confirmed on imaging (humans) or by a surgical dissection (cadavers). Thirty-eight studies were eventually selected for the full-text reading and data extraction. The selected studies included a total of 2309 patients (2690 shoulders) and 195 cadavers (299 shoulders). To objectively assess the methodological quality of the present systematic review, "Assessment of Multiple Systematic Review" (AMSTAR) tool was used. RESULTS: The overall accuracy of the intra-articular injection in GHJ varied from 42 to 100 % in the 38 selected studies. Imaging guidance was used in 65 % of articles and the overall accuracy of guided GHJ injections was higher than blind injection. However, five articles in which blind injection the GHJ was used (159 shoulders) reported accuracy as high as 100 %. CONCLUSION: A comprehensive {{review of the literature}} confirms that guided injections of the GHJ have overall accuracy higher compared to blind injection. Nevertheless, in some studies, including a relatively large number of shoulders, blind injections have been proven to be 100 % accurate. Hence, blind injections of GHJ could be proposed a cost-effective alternative to imaging-guided injection. A large prospective randomized study is needed to gauge this hypothesis and compare the cost-effectiveness of these two techniques for the most common anatomical approaches. Peer reviewe...|$|R
40|$|Network traffic {{monitoring}} systems {{have to deal}} with a challenging problem: the traffic capturing process almost invariably produces duplicate packets. In spite of this, and in contrast with other fields, there is no scientific literature addressing it. This paper establishes the theoretical background concerning data duplication in network traffic analysis: generating mechanisms, types of duplicates and their characteristics are described. On this basis, a <b>duplicate</b> detection and <b>removal</b> methodology is proposed. Moreover, an analytical and experimental study is presented, whose results provide a dimensioning rule for this methodology. Comment: 7 pages, 8 figures. For the GitHub project, see [URL]...|$|R
40|$|Updates: make easy-deploy script more generic (less Broad specific) more {{documentation}} updates bugfixes to snakemake rules {{add more}} forgiveness in pre-Trinity filtration steps (<b>duplicate</b> <b>removal,</b> unpaired read removal) when read counts are below subsampling threshold. In particular, the last set of changes {{will increase the}} success rate of de novo assembly when viral genomes are degraded or have small fragment / segment sizes and have adapter read through issues in the second read...|$|E
40|$|Record linkage is {{a momentous}} process in data {{soundness}} {{which is used}} in combining, matching and <b>duplicate</b> <b>removal</b> from more than two databases that refer to the same entities. Deduplication {{is the process of}} taking off duplicate records in a united database. Now a day, data cleaning and standardization becomes a pompous process. Due to yielding capacity of today’s database, discovering matching records in united database is a crucial one. Indexing technique specifically suffix array is used to efficiently implement record linkage and deduplication...|$|E
30|$|The MAC-e/es {{protocol}} layers in the UE {{are responsible}} for HARQ and the transport format selection according to the scheduling grants. The created MAC-e PDU is transmitted over the air interface to the Node B. The MAC-e protocol layer in the Node B demultiplexes the MAC-e PDU to MAC-es PDUs which are transmitted over the TN to the SRNC. The MAC-es protocol layer in the SRNC handles {{the effect of the}} SHO by reordering, <b>duplicate</b> <b>removal</b> and macro combining to ensure in-sequence-delivery for the Radio Link Control (RLC) protocol layer.|$|E
25|$|Jet 3.0 {{included}} many enhancements, {{including a new}} index structure that reduced storage size and the time that was taken to create indices that were highly <b>duplicated,</b> the <b>removal</b> of read locks on index pages, a new mechanism for page reuse, a new compacting method for which compacting the database resulted in the indices being stored in a clustered-index format, a new page allocation mechanism to improve Jet's read-ahead capabilities, improved delete operations that speeded processing, multithreading (three threads were used to perform read ahead, write behind, and cache maintenance), implicit transactions (users {{did not have to}} instruct the engine to start manually and commit transactions to the database), a new sort engine, long values (such as memos or binary data types) were stored in separate tables, and dynamic buffering (whereby Jet's cache was dynamically allocated at start up and had no limit and which changed from a first in, first out (FIFO) buffer replacement policy to a least recently used (LRU) buffer replacement policy). Jet 3.0 also allowed for database replication.|$|R
40|$|The {{continuous}} {{increase in}} sequencing throughput imposes {{a new generation}} of tools for data processing. The alternative is to continue suffering scalability problems in processing workflows and IT infrastructure. We evaluate the advantages that the CRS 4 Sequencing and Genotyping Platform (CSGP), equipped with 6 Illumina sequencers, gained by replacing its conventional workflow with a new one based on Seal ([URL] and Hadoop. The former was a standard pipeline that demultiplexed samples, aligned reads with BWA, removed duplicates with Picard and recalibrated base qualities with GATK. It parallelized computation through concurrent jobs, using a centralized file system to share data. This implementation showed weaknesses as the workload increased: low parallelism; I/O bottleneck at central storage; failure of entire analyses due to node failures or transient cluster problems. The new workflow is a custom, distributed pipeline based on the open-source Seal suite, which provides a set of tools (including a distributed BWA aligner) that run on the Hadoop MapReduce framework, leveraging its functionality for genomic sequencing applications. By switching to a Seal-based workflow we have acquired computational scalability out-of-the-box. Therefore, we can now easily meet the demands imposed by the growing sequencing platform by adding more computing nodes. In addition, the much-increased parallelism has improved overall computational throughput by taking advantage of all available computing power. Notably, we drastically sped up alignment and <b>duplicates</b> <b>removal</b> by 5 x without adding computation nodes; adding nodes would result in additional throughput. Moreover, the effort required by our operators to run the analyses has been reduced, since Hadoop transparently handles most hardware and transient network problems and provides a friendly web interface to monitor job progress and logs. Finally, we eliminated the need for our expensive shared parallel storage devices. Our tests reveal that Seal is efficient, achieving close to 70 % of the theoretical maximum throughput per node (measured with a single-node version of the workflow on a small data set) and scales linearly at least up to 128 nodes. In summary, this case study suggests that the MapReduce programming model, Seal and Hadoop provide considerable benefits in the genomic sequencing domain. Seal now includes our new workflow as a downloadable sample application. 2011 - 10 - 11 Montreal - CanadaThe 12 TH International Congress Of Human Genetics & The American Society Of Human Genetics, 61 ST Annual Meeting, October 11 – 15, 2011 Montreal Canad...|$|R
50|$|Jet 3.0 {{included}} many enhancements, {{including a new}} index structure that reduced storage size and the time that was taken to create indices that were highly <b>duplicated,</b> the <b>removal</b> of read locks on index pages, a new mechanism for page reuse, a new compacting method for which compacting the database resulted in the indices being stored in a clustered-index format, a new page allocation mechanism to improve Jet's read-ahead capabilities, improved delete operations that speeded processing, multithreading (three threads were used to perform read ahead, write behind, and cache maintenance), implicit transactions (users {{did not have to}} instruct the engine to start manually and commit transactions to the database), a new sort engine, long values (such as memos or binary data types) were stored in separate tables, and dynamic buffering (whereby Jet's cache was dynamically allocated at start up and had no limit and which changed from a first in, first out (FIFO) buffer replacement policy to a least recently used (LRU) buffer replacement policy). Jet 3.0 also allowed for database replication.Jet 3.0 was replaced by Jet 3.5, which uses the same database structure, but different locking strategies, making it incompatible with Jet 3.0.|$|R
40|$|International audienceThis paper {{presents}} the approach {{used by the}} LIG-MRIM research group to {{the participation of the}} task 3 (TimeLine illustration based on Microblogs) for the CLEF of Cultural Microblog Contextualization track. This task deals with the retrieval of tweets related to cultural events (music festivals). For the content-based elements, we use the classical BM 25 model [4]. Then, we diversify the results based on <b>duplicate</b> <b>removal,</b> using tf-based representations of tweets. In a third step, we apply optional re-ranking related to time-line, activity and popularity of authors of tweets...|$|E
40|$|Aiming at the {{problems}} that existing Chinese webpage <b>duplicate</b> <b>removal</b> approaches are restricted by the page scale and the algorithm efficiency, we propose an efficient distributed parallel duplicates elimination approach based on Linux cluster. This paper not only solves {{the problems}} of memory limitations and large computation caused by the huge data scale, but also researches into the time-series problems in the distributed parallel computing and gives an effective solution. Experimental results on 10 million webpage dataset show that the proposed approach can deal with duplicates from massive web pages well and truly...|$|E
40|$|Throughout {{the recent}} years, 454 {{pyrosequencing}} {{has emerged as}} an efficient alternative to traditional Sanger sequencing and is widely used in both de novo whole-genome sequencing and metagenomics. Especially the latter application is extremely sensitive to sequencing errors and artificially duplicated reads. Both are common in 454 pyrosequencing and can create a strong bias in the estimation of diversity and composition of a sample. To date, there are several tools that aim to remove both sequencing noise and duplicates. Nevertheless, <b>duplicate</b> <b>removal</b> is often based on nucleotide sequences {{rather than on the}} underlying flow values, which contain additional information...|$|E
40|$|Background RNA-seq is {{a useful}} tool for {{analysis}} of gene expression. However, its robustness is greatly affected by a number of artifacts. One of them is the presence of duplicated reads. Results To infer the influence of different methods of <b>removal</b> of <b>duplicated</b> reads on estimation of gene expression in cancer genomics, we analyzed paired samples of hepatocellular carcinoma (HCC) and non-tumor liver tissue. Four protocols of data analysis were applied to each sample: processing without deduplication, deduplication using a method implemented in SAMtools, and deduplication based on one or two molecular indices (MI). We also analyzed the influence of sequencing layout (single read or paired end) and read length. We found that deduplication without MI greatly affects estimated expression values; this effect is the most pronounced for highly expressed genes. Conclusion The use of unique molecular identifiers greatly improves accuracy of RNA-seq analysis, especially for highly expressed genes. We developed a set of scripts that enable handling of MI and their incorporation into RNA-seq analysis pipelines. Deduplication without MI affects results of differential gene expression analysis, producing a high proportion of false negative results. The absence of <b>duplicate</b> read <b>removal</b> is biased towards false positives. In those cases where using MI is not possible, we recommend using paired-end sequencing layout...|$|R
40|$|Abstract Background Roche 454 {{pyrosequencing}} {{platform is}} often {{considered the most}} versatile of the Next Generation Sequencing technology platforms, permitting the sequencing of large genomes, the analysis of variations or the study of transcriptomes. A recent reported bias leads {{to the production of}} multiple reads for a unique DNA fragment in a random manner within a run. This bias has a direct impact {{on the quality of the}} measurement of the representation of the fragments using the reads. Other cleaning steps are usually performed on the reads before assembly or alignment. Findings PyroCleaner is a software module intended to clean 454 pyrosequencing reads in order to ease the assembly process. This program is a free software and is distributed under the terms of the GNU General Public License as published by the Free Software Foundation. It implements several filters using criteria such as read duplication, length, complexity, base-pair quality and number of undetermined bases. It also permits to clean flowgram files (. sff) of paired-end sequences generating on one hand validated paired-ends file and the other hand single read file. Conclusions Read cleaning has always been an important step in sequence analysis. The pyrocleaner python module is a Swiss knife dedicated to 454 reads cleaning. It includes commonly used filters as well as specialised ones such as <b>duplicated</b> read <b>removal</b> and paired-end read verification. </p...|$|R
40|$|Data {{intensive}} {{applications and}} computing {{has emerged as}} a central area of mod-ern research with the explosion of data stored world-wide. Applications involv-ing telecommunication call data records, web pages, online transactions, med-ical records, stock markets, climate warning systems, etc., necessitate efficient management and processing of such massively exponential amount of data from diverse sources. <b>Duplicate</b> detection and <b>removal</b> of redundancy from such multi-billion datasets helps in resource and compute efficiency for downstream process-ing. De-duplication or Intelligent Compression in streaming scenarios for ap-proximate identification and elimination of duplicates from such unbounded data stream is a greater challenge given the real-time nature of data arrival. Stable Bloom Filters (SBF) addresses this problem to a certain extent. However, SBF suffers from a high false negative rate and slow convergence rate, thereby render-ing it inefficient for applications with low false negative rate tolerances. ∗This work was completed at IBM Research, India...|$|R
