0|20|Public
40|$|Explored the {{possibility}} of quantitative assessment of risk factors of complications {{in the treatment of}} <b>diffuse</b> peritonitis. <b>Highlighted</b> 53 groups of features that are important in predicting the course of diffuse peritonitis. The proposed scheme of defining the risk of clinical course of diffuse peritonitis can quantify the severity of the source of patients and in most cases correctly predict the results of treatment of disease...|$|R
50|$|The {{statistical}} display (located {{below the}} numerical total) demonstrates {{the percentage of}} the normal population who measure below the patient's value at a specific retinal point. The probability display provides this percentage a key for interpreting the statistical display. For example, the darkest square in the key represents that <0.5% of the population would also attain this result, indicating that the vision loss is extensive. The Total Deviation plots <b>highlight</b> <b>diffuse</b> vision loss (i.e. the total departure from the age-norm).|$|R
40|$|The {{usefulness}} of a modulation wave approach to understanding and interpreting the highly structured continuous diffuse intensity distributions {{characteristic of the}} reciprocal spaces of the very large family of inherently flexible materials which exhibit ordered `disorder' is pointed out. It is shown that both longer range order and truly short-range order are simultaneously encoded in highly structured diffuse intensity distributions. The long-range ordered crystal chemical rules giving rise to such <b>diffuse</b> distributions are <b>highlighted,</b> along with the existence and {{usefulness of}} systematic extinction conditions in these types of structured diffuse distributions...|$|R
40|$|With {{the advent}} of desktop color {{scanners}} in the electronic prepress production environment, quality in color separations from transmissive originals has become erratic. The cause of this fluctuation is, in part, due to the transition from PMT to CCD-based scanning technology. Whereas, PMT scanners {{tend to have a}} broad dynamic range, that of CCD scanners is more limited. This characteristic adversely affects the quality of color separations by causing additional tone compression. An original transparency typically has a shadow density of 3. 00 and a <b>diffused</b> <b>highlight</b> density of 0. 30 for an overall density range of 2. 70. On a four-color heatset web press with coated stock, the maximum reproducible tonal range corresponds to a density of 1. 80. The difference in density of 0. 90 between the original and the press sheet is unable to fit through the printing window unless it undergoes considerable tone compression. This project was based on two hypotheses. The first was that the lower the tonal range of a transmissive original the more readily lower-midtone-to-shadow tonality could be retained in the separations produced by a CCD scanner and related equipment. The second was that exposure latitude in the separations would decrease with increasing tonal range. The first stage of production was to produce twelve test transparencies by photograph ing a still life set to four tonal ranges: 3. 5, 4. 5, 5. 5, and 6. 5 f/stops. Within each range, three images were selected to represent normal exposure, 1 / 2 f/stop overexposure, and 1 / 2 f/stop underexposure. Comparison of halftone proofs, made from separations of the normallyexposed transparencies, were later used for the first hypothesis. Proofs from the 1 / 2 f/stop over- and 1 / 2 f/stop underexposures were compared with the normal exposures to test the second hypothesis. vm The twelve test transparencies were first scanned on the Dainippon Screen SG- 608 to produce a set of best-of-kind reference separations and halftone proofs. Next, the trans parencies were scanned on two midrange 12 -bit CCD scanners, one a Pixelcraft CIS 4520 RS, the other an Agfa Horizon. Separations for both were produced with Color Access 1. 3. 3 software on a Macintosh Quadra 700 computer, linked to an Agfa SelectSet 5000 image-setter via an Agfa 5000 PS Star Plus RIP. The image files were placed in an 8 1 / 22 ̆ 2 X 112 ̆ 2 QuarkXPress page with a 20...|$|R
40|$|The {{exact nature}} of {{interstellar}} dust grains in the Galaxy remains mysterious, despite their ubiquity. Many viable models exist, based on available IR-UV data and assumed elemental abundances. However, the abundances, which {{are perhaps the most}} stringent constraint, are not well known: modelers must use proxies in the absence of direct measurements for the diffuse interstellar medium (ISM). Recent revisions of these proxy values have only added to confusion over which is the best representative for the <b>diffuse</b> ISM, and <b>highlighted</b> the need for direct, high signal-to-noise measurements from the ISM itself. The International X-ray Observatory's superior facilities will enable high-precision elemental abundance measurements. We ill show how these results will measure both the overall ISM abundances and challenge dust models, allowing us to construct a more realistic picture of the ISM...|$|R
40|$|During {{the last}} two decades, {{interpersonal}} regulation in natural and digital learning environments has gained importance. Ever since the first conceptual and methodological precisions regarding collaborative learning were made, educational psychology has focused its interest on analyzing collective regulation of motivation, cognition, and behavior. Despite {{the fact that the}} body of research on co-regulation has grown, emerging epistemological frameworks evidence a lack of conceptual and theoretical clarity. In response to this situation, the authors propose a conceptual approach in order to address interpersonal regulation in four aspects: first, they describe three learning theories which have been used to study co-regulation. Second, the authors recommend a conceptual delimitation of terms regarding the learning theories on social regulation. Third, they <b>highlight</b> <b>diffuse</b> boundaries between theoretical approaches and terms used in the literature on co-regulation. Finally, the authors suggest some challenges the researchers in this field face...|$|R
40|$|The {{latest round}} of share reform in China, which began in 2005, sets two related {{processes}} in motion: it increases the tradable share proportion and signals {{the start of a}} decline in the government-owned share proportion. This paper considers the effect these processes might have on firm performance in the future by analysing the impact the above share proportions had on firm performance immediately prior to reform commencing. The government-owned share proportion is found to exert a linear and positive impact on firm performance. Further, it is revealed that this impact is best explained by the high ownership concentration of government shareholdings. The policy implication is that simply making all shares tradable need not lead to better firm performance. Rather, a more pertinent consideration is whether shareholdings become more or less <b>diffuse,</b> and this <b>highlights</b> the importance of non-government institutional investors playing a more prominent role than they currently do. ...|$|R
40|$|International audienceWe {{present a}} novel {{technique}} called Radiance Scaling for {{the depiction of}} surface shape through shading. It adjusts reflected light intensities in a way dependent on both surface curvature and material characteristics. As a result, <b>diffuse</b> shading or <b>highlight</b> variations become correlated to surface feature variations, enhancing surface concavities and convexities. This approach is more versatile compared to previous methods. First, it produces satisfying results {{with any kind of}} material: we demonstrate results obtained with Phong and Ashikmin BRDFs, Cartoon shading, sub-Lambertian materials, and perfectly reflective or refractive objects. Second, it imposes no restriction on lighting environment: it does not require a dense sampling of lighting directions and works even with a single light. Third, it makes it possible to enhance surface shape through the use of precomputed radiance data such as Ambient Occlusion, Prefiltered Environment Maps or Lit Spheres. Our novel approach works in real-time on modern graphics hardware and is faster than previous techniques...|$|R
40|$|Extracellular amyloid {{deposits}} are {{a feature}} of both Alzheimer type dementia and the 'normal' aging process. Quantification of amyloid plaque deposits may well be useful in distinguishing between the senescent changes associated with 'normal' aging and the pathological processes underlying dementia. To determine the most reliable and reproducible method for visualisation of the amyloid we have compared conventional silver staining techniques with beta-amyloid immunocytochemistry on a large sample of post-mortem brain tissue from both demented (n = 15, age range 60 - 87) and non-demented (n = 65, age range 14 - 99) patients. The degree of amyloid deposition was rated on a four point scale and ratings for the two techniques were significantly correlated (P less than 0. 01). However, the immunocytochemical approach {{has a number of}} distinct advantages for quantification. The antibody to beta-amyloid is highly specific and does not stain neurofibrillary tangles or background features, it is considerably more sensitive than silver staining in <b>highlighting</b> <b>diffuse</b> amyloid deposits and, perhaps most importantly, it produces high contrast staining which allows easier image digitisation and subsequent computer image analysis...|$|R
40|$|International audienceBased on the {{observation}} that shading conveys shape information through intensity gradients, we present a new technique called Radiance Scaling that modifies the classical shading equations to offer versatile shape depiction functionalities. It works by scaling reflected light intensities depending on both surface curvature and material characteristics. As a result, <b>diffuse</b> shading or <b>highlight</b> variations become correlated to surface feature variations, enhancing concavities and convexities. The first advantage of such an approach is that it produces satisfying results {{with any kind of}} material for direct and global illumination: we demonstrate results obtained with Phong and Ashikmin-Shirley BRDFs, Cartoon shading, sub-Lambertian materials, perfectly reflective or refractive objects. Another advantage {{is that there is no}} restriction to the choice of lighting environment: it works with a single light, area lights, and inter-reflections. Third, it may be adapted to enhance surface shape through the use of precomputed radiance data such as Ambient Occlusion, Prefiltered Environment Maps or Lit Spheres. Finally, our approach works in real-time on modern graphics hardware making it suitable for any interactive 3 D visualization...|$|R
40|$|We {{present a}} new model for the {{calculation}} of the diffuse fraction of the global solar irradiance for solar system simulations. The importance of an accurate estimation of the horizontal <b>diffuse</b> irradiance is <b>highlighted</b> by findings that an inaccurately calculated diffuse irradiance can lead to significant over- or underestimations in the annual energy yield of a photovoltaic (PV) system {{by as much as}} 8 %. Our model utilizes a time series of global irradiance in one-minute resolution and geographical information as input. The model is validated by measurement data of 28 geographically and climatologically diverse locations worldwide with one year of one-minute data each, taken from the Baseline Surface Radiation Network (BSRN). We show that on average the mean absolute deviation of the modelled and the measured diffuse irradiance is reduced from about 12 % to about 6 % compared to three reference models. The maximum deviation is less than 20 %. In more than 80 % of the test cases, the deviation is smaller 10 %. The root mean squared error (RMSE) of the calculated diffuse fractions is reduced by about 18 %...|$|R
40|$|International audienceFor {{the past}} two decades, the need for 3 D Scanning of {{industrial}} objects has increased significantly. Therefore, many experimental techniques and commercial solutions have been proposed. However, difficulties remain for the acquisition of optically non-cooperative surfaces, such as transparent or specular surfaces. To address highly reflective metallic surfaces, we propose the extension of a technique that was originally dedicated to glass objects. In contrast to conventional active triangulation techniques that measure the reflection of visible radiation, we measure the thermal emission of a surface, which is locally heated by a laser source. Considering the thermophysical properties of metals, we present a simulation model of heat exchanges that are induced by the process, helping to demonstrate its feasibility on specular metallic surfaces and predicting the settings of the system. With our experimental device, we have validated the theoretical modeling and computed some 3 D point clouds from specular surfaces of various geometries. Furthermore, a comparison of our results with those of a conventional system on specular and <b>diffuse</b> parts will <b>highlight</b> that {{the accuracy of the}} measurement no longer depends on the roughness of the surface...|$|R
40|$|Abstract—Based on the {{observation}} that shading conveys shape information through intensity gradients, we present a new technique called Radiance Scaling that modifies the classical shading equations to offer versatile shape depiction functionalities. It works by scaling reflected light intensities depending on both surface curvature and material characteristics. As a result, <b>diffuse</b> shading or <b>highlight</b> variations become correlated to surface feature variations, enhancing concavities and convexities. The first advantage of such an approach is that it produces satisfying results {{with any kind of}} material for direct and global illumination: we demonstrate results obtained with Phong and Ashikmin-Shirley BRDFs, Cartoon shading, sub-Lambertian materials, perfectly reflective or refractive objects. Another advantage {{is that there is no}} restriction to the choice of lighting environment: it works with a single light, area lights, and inter-reflections. Third, it may be adapted to enhance surface shape through the use of precomputed radiance data such as Ambient Occlusion, Prefiltered Environment Maps or Lit Spheres. Finally, our approach works in real-time on modern graphics hardware making it suitable for any interactive 3 D visualization. Index Terms—Expressive rendering, NPR, shape depiction, shading, global illumination. ...|$|R
40|$|For {{the past}} two decades, the need for {{three-dimensional}} (3 -D) scanning of industrial objects has increased significantly and many experimental techniques and commercial solutions have been proposed. However, difficulties remain for the acquisition of optically non-cooperative surfaces, such as transparent or specular surfaces. To address highly reflective metallic surfaces, we propose the extension of a technique that was originally dedicated to glass objects. In contrast to conventional active triangulation techniques that measure the reflection of visible radiation, we measure the thermal emission of a surface, which is locally heated by a laser source. Considering the thermophysical properties of metals, we present a simulation model of heat exchanges that are induced by the process, helping to demonstrate its feasibility on specular metallic surfaces and predicting the settings of the system. With our experimental device, we have validated the theoretical modeling and computed some 3 -D point clouds from specular surfaces of various geometries. Furthermore, a comparison of our results with those of a conventional system on specular and <b>diffuse</b> parts will <b>highlight</b> that {{the accuracy of the}} measurement no longer depends on the roughness of the surface...|$|R
500|$|Although Wipeout 2048 and Wipeout HD have {{a shared}} shader program {{and did not}} require {{retooling}} for the Vita's architecture, Roberts said that a [...] "great deal" [...] of time and attention was spent fine-tuning the game's shader effects for the Vita's GPU. Lovegrove thought that the method of working on a PlayStation 3 and its handheld counterpart was identical (a sentiment generally shared by the team), and Roberts said that the systems' similarities helped the team [...] "get moving quickly". Roberts added that the game's lighting system was identical to that of Wipeout HD; both games' ships shared image-based lighting, with blended <b>diffuse</b> and specular <b>highlights</b> effects and the vertex-based lighting system used for weaponry. According to Roberts, {{the main difference between}} the games was PlayStation Vita's utilisation of the effects via the GPU; PlayStation 3 relied on SPUs. The team decided to use anti-aliased colour buffers for real-time shadow rendering, instead of depth buffers. This gave the game greater transparency effects, since the memory cost of anti-aliasing was 8 bits per pixel; therefore, 4x MSAA (multisample anti-aliasing) buffers contained the same amount of memory as a 32-bit depth buffer. Another feature which Roberts considered an improvement was tone mapping (partly due to Vita's superior support of buffer formats), which gave Wipeout 2048 better exposure control and bloom effects.|$|R
40|$|We {{consider}} real-time {{rendering of}} scenes in participating media, capturing {{the effects of}} light scattering in fog, mist and haze. While a number of sophisticated approaches based on Monte Carlo and finite element simulation have been developed, those methods do not work at interactive rates. The most common real-time methods are essentially simple variants of the OpenGL fog model. While {{easy to use and}} specify, that model excludes many important qualitative effects like glows around light sources, the impact of volumetric scattering on the appearance of surfaces such as the <b>diffusing</b> of glossy <b>highlights,</b> and the appearance under complex lighting such as environment maps. In this paper, we present an alternative physically based approach that captures these effects while maintaining realtime performance and the ease-of-use of the OpenGL fog model. Our method is based on an explicit analytic integration of the single scattering light transport equations for an isotropic point light source in a homogeneous participating medium. We can implement the model in modern programmable graphics hardware using a few small numerical lookup tables stored as texture maps. Our model can also be easily adapted to generate the appearances of materials with arbitrary BRDFs, environment map lighting, and precomputed radiance transfer methods, in the presence of participating media. Hence, our techniques can be widely used in real-time rendering. ...|$|R
5000|$|Although Wipeout 2048 and Wipeout HD have {{a shared}} shader program {{and did not}} require {{retooling}} for the Vita's architecture, Roberts said that a [...] "great deal" [...] of time and attention was spent fine-tuning the game's shader effects for the Vita's GPU. Lovegrove thought that the method of working on a PlayStation 3 and its handheld counterpart was identical (a sentiment generally shared by the team), and Roberts said that the systems' similarities helped the team [...] "get moving quickly". Roberts added that the game's lighting system was identical to that of Wipeout HD; both games' ships shared image-based lighting, with blended <b>diffuse</b> and specular <b>highlights</b> effects and the vertex-based lighting system used for weaponry. According to Roberts, {{the main difference between}} the games was PlayStation Vita's utilisation of the effects via the GPU; PlayStation 3 relied on SPUs. The team decided to use anti-aliased colour buffers for real-time shadow rendering, instead of depth buffers. This gave the game greater transparency effects, since the memory cost of anti-aliasing was 8 bits per pixel; therefore, 4x MSAA (multisample anti-aliasing) buffers contained the same amount of memory as a 32-bit depth buffer. Another feature which Roberts considered an improvement was tone mapping (partly due to Vita's superior support of buffer formats), which gave Wipeout 2048 better exposure control and bloom effects.|$|R
40|$|Ensuring global food {{security}} requires a comprehensive understanding of environmental pressures on food production, including {{the impacts of}} air quality. Surface ozone damages plants and decreases crop production; this effect has been extensively studied. In contrast, the presence of particulate matter (PM) in the atmosphere can be beneficial to crops given that enhanced light scattering leads to a more even and efficient distribution of photons which can outweigh total incoming radiation loss. This study quantifies the impacts of ozone and PM on the global production of maize, rice, and wheat in 2010 and 2050. We show that accounting for the growing season of these crops {{is an important factor}} in determining their air pollution exposure. We find that the effect of PM can offset much, if not all, of the reduction in yield associated with ozone damage. Assuming maximum sensitivity to PM, the current (2010) global net impact of air quality on crop production is positive (+ 6. 0  %, + 0. 5  %, and + 4. 9  % for maize, wheat, and rice, respectively). Future emissions scenarios indicate that attempts to improve air quality can result in a net negative effect on crop production in areas dominated by the PM effect. However, we caution that the uncertainty in this assessment is large due to the uncertainty associated with crop response to changes in <b>diffuse</b> radiation; this <b>highlights</b> that more detailed physiological study of this response for common cultivars is crucial...|$|R
40|$|International audienceGanglioglioma (GG) is a grade I tumor {{characterized}} by alterations in the MAPK pathway, including BRAF V 600 E mutation. Recently, diffuse midline glioma with an H 3 K 27 M mutation {{was added to}} the WHO 2016 classification as a new grade IV entity. As co-occurrence of H 3 K 27 M and BRAF V 600 E mutations has been reported in midline tumors and anaplastic GG, we searched for BRAF V 600 E and H 3 K 27 M mutations in a series of 54 paediatric midline grade I GG (midline GG) to determine the frequency of double mutations and its relevance for prognosis. Twenty-seven patients (50 %) possessed the BRAF V 600 E mutation. The frequency of the co-occurrence of H 3 F 3 A/BRAF mutations at diagnosis was 9. 3 %. No H 3 K 27 M mutation was detected {{in the absence of the}} BRAF V 600 E mutation. Double-immunostaining revealed that BRAF V 600 E and H 3 K 27 M mutant proteins were present in both the glial and neuronal components. Immunopositivity for the BRAF V 600 E mutant protein correlated with BRAF mutation status as detected by massARRAY or digital droplet PCR. The median follow-up of patients with double mutation was 4 years. One patient died of progressive disease 8 years after diagnosis, whereas the four other patients were all alive with stable disease at the last clinical follow-up (at 9 months, 1 year and 7 years) without adjuvant therapy. We demonstrate in this first series of midline GGs that the H 3 K 27 M mutation can occur in association with the BRAF V 600 E mutation in grade I glioneuronal tumors. Despite the presence of H 3 K 27 M mutations, these cases should not be graded and treated as grade IV tumors because they have a better spontaneous outcome than classic diffuse midline H 3 K 27 M-mutant glioma. These data suggest that H 3 K 27 M cannot be considered a specific hallmark of grade IV <b>diffuse</b> gliomas and <b>highlight</b> the importance of integrated histomolecular diagnosis in paediatric brain tumors...|$|R
40|$|The {{literature}} on technological innovation and technical change usually distinguishes between invention, innovation, diffusion and adoption. Diffusion isà defiedà asà theà processà yà hichà iovatiosà spreadà ithià adà acrossà ecooies. àAccording to some authors, {{in order to}} ensure successful technological diffusion, it is also necessary to have a range of systemic factors. In the political economy perspective, influences on technology adoption and diffusion can develop from government policies and incentives. Diffusion of innovation policies, in particular, aim at spreading technological capabilities throughout the industrial structure, facilitating the ongoing and the incremental adaptation to change. Economic {{literature on}} technology policies has focused predominantly on generation policies, underestimating the effects of policy interventions in the diffusion process. Nevertheless, {{it is also important to}} understand which policies need to be implemented to favour the diffusion of innovation because the adoption of a new technology creates positive economical and social effects. According to these considerations, the purpose of this paper is to carry out a review of the studies that have analyzed the policies able to support the diffusion of an innovation and to provide a framework that classifies these policies according to their effect on the diffusion of a new technology. We adopted an approach similar to the systematic reviews used in many studies, in which methodical searches and formal summaries of the literature are used to identify and classify results of all major studies on a particular topic. We searched the Scopus database for articles that had the word policyàadàtheàordsàdiffusio/adoptioàofàiovatio/techologyàiàtheàtopic field- title, key words and abstract. We therefore read through all the abstracts, or the full publication when we were unsure, to assess whether they dealt with the topic. This screening resulted in a short list of papers, then studied and assessed considering the conceptual framework. Considering the different variables that affect the diffusion process, theoretical contributions were classified in four research streams: country-focused, technology- focused, coherent adopter-focused and model-focused. The review reveals that only few papers are specifically focused on the role of policy to <b>diffuse</b> new technology, <b>highlighting</b> that the studies on the diffusion oriented policies are still few and fragmented. On the contrary, the most of the research in that field considers diffusion policies only in terms of practical implications. In order to overcome these gaps, the literature review provides a theoretical framework that links the policies to support the dissemination of an innovation to the barriers that hinders the diffusion of a new technology among a community of adopters. This framework it also useful to define a possible research agenda for future studies on the topic. For policy makers, the theoretical framework provides a conceptual tool to guide their choices in supporting the diffusion of relevant technologies, since policy intervention would depend on the nature of the barrier to diffusion of innovation...|$|R

