381|168|Public
25|$|Several {{optional}} extensions {{are also}} available, including MIPS-3D {{which is a}} simple set of floating-point SIMD instructions dedicated to common 3D tasks, MDMX (MaDMaX) which is a more extensive integer SIMD instruction set using the 64-bit floating-point registers, MIPS16e which adds compression to the <b>instruction</b> <b>stream</b> to make programs take up less room, and MIPS MT, which adds multithreading capability.|$|E
25|$|RISC {{designs are}} also more likely to feature a Harvard memory model, where the <b>instruction</b> <b>stream</b> and the data stream are conceptually separated; this means that modifying the memory where code is held might not have any effect on the {{instructions}} executed by the processor (because the CPU has a separate instruction and data cache), at least until a special synchronization instruction is issued. On the upside, this allows both caches to be accessed simultaneously, which can often improve performance.|$|E
500|$|A {{multi-core}} processor is {{a processor}} that includes multiple processing units (called [...] "cores") {{on the same}} chip. This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one <b>instruction</b> <b>stream</b> (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. IBM's Cell microprocessor, designed {{for use in the}} Sony PlayStation 3, is a prominent multi-core processor. Each core in a multi-core processor can potentially be superscalar as wellâ€”that is, on every clock cycle, each core can issue multiple instructions from one thread.|$|E
5000|$|... #Subtitle level 3: Multiple <b>instruction</b> <b>streams,</b> {{multiple}} data streams (MIMD) ...|$|R
5000|$|... #Subtitle level 3: Multiple <b>instruction</b> <b>streams,</b> single {{data stream}} (MISD) ...|$|R
5000|$|MIMD (Multiple <b>Instruction</b> <b>Streams,</b> Multiple Data Streams), {{which is}} {{sustained}} by commodity SMP.|$|R
2500|$|Sorting {{algorithms}} {{are also}} given for parallel computers. [...] These algorithms {{can all be}} run on a single <b>instruction</b> <b>stream</b> multiple data stream computer. Habermann's parallel neighbor-sort (or {{the glory of the}} induction principle) sorts k elements using k processors in k steps. This article introduces Optimal Algorithms for Paraller Computers where rk elements can be sorted using k processors in k steps.|$|E
50|$|In computing, SISD (single <b>instruction</b> <b>stream,</b> single data stream) is a {{computer}} architecture in which a single uni-core processor, executes a single <b>instruction</b> <b>stream,</b> to operate on data stored in a single memory. This corresponds to the von Neumann architecture.|$|E
5000|$|... #Subtitle level 3: Single <b>instruction</b> <b>stream</b> single {{data stream}} (SISD) ...|$|E
40|$|Intel's {{recently}} introduced Hyper-Threading Technology promises to increase application- and system-level performance through increased utilization of processor resources. It achieves this goal {{by allowing the}} processor to simultaneously maintain the context of multiple <b>instruction</b> <b>streams</b> and execute multiple <b>instruction</b> <b>streams</b> or threads. These multiple streams afford the processor added flexibility in internal scheduling, lowering the impact of external data latency, raising utilization of internal resources, and increasing overall performance...|$|R
40|$|This work {{presents}} several {{techniques for}} enlarging <b>instruction</b> <b>streams.</b> We call stream to {{a sequence of}} instructions from {{the target of a}} taken branch to the next taken branch, potentially containing multiple basic blocks. The long size of <b>instruction</b> <b>streams</b> makes it possible for a fetch engine based on streams to provide high fetch bandwidth, which leads to obtaining performance results comparable to a trace cache. The long size of streams also enables the next stream predictor to tolerate the prediction table access latency. Therefore, enlarging <b>instruction</b> <b>streams</b> will improve the behavior of a fetch engine based on streams. We provide a comprehensive analysis of dynamic <b>instruction</b> <b>streams,</b> showing that focusing on particular kinds of stream is not a good strategy due to Amdahl's law. Consequently, we propose the multiple stream predictor, a novel mechanism that deals with all kinds of streams by combining single streams into long virtual streams. We show that our multiple stream predictor is able to tolerate the prediction access latency without requiring the complexity caused by additional hardware mechanisms like prediction overriding. Postprint (published version...|$|R
40|$|An {{architecture}} {{for improving}} computer per-formance is presented and discussed. The main {{feature of the}} architecture is {{a high degree of}} decoupling between operand access and execution. This results in an implementation which has two separate <b>instruction</b> <b>streams</b> that communicate via queues. A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the <b>instruction</b> <b>streams.</b> This paper emphasizes implementation features that remove this burden from the programmer. Performance comparisons with a conventional scalar architecture are given, an...|$|R
5000|$|... #Subtitle level 3: Single <b>instruction</b> <b>stream,</b> {{multiple}} data streams (SIMD) ...|$|E
50|$|The 8061 had an interruptible-burst-mode 11-wire 8-bit memory {{interface}} bus {{called the}} M-Bus. This bus required a program counter and a data address register in each memory device. Each chip reset or branch instruction would update the program counter {{in the memory}} devices, after which <b>instruction</b> <b>stream</b> data would be read sequentially. The <b>instruction</b> <b>stream</b> could be interrupted to read or write data bytes and words using the memory's data address register while retaining the memory's program counter copy - allowing resumption of reading the <b>instruction</b> <b>stream</b> without having to re-send a program address after each data access.|$|E
50|$|In Flynn's taxonomy, a {{single-core}} superscalar processor {{is classified}} as an SISD processor (Single <b>Instruction</b> <b>stream,</b> Single Data stream), though many superscalar processors support short vector operations and so {{could be classified as}} SIMD (Single <b>Instruction</b> <b>stream,</b> Multiple Data streams). A multi-core superscalar processor {{is classified as}} an MIMD processor (Multiple Instruction streams, Multiple Data streams).|$|E
40|$|Fault-tolerance {{has become}} an {{essential}} concern for pro-cessor designers due to increasing transient and permanent fault rates. Executing <b>instruction</b> <b>streams</b> redundantly in chip multi processors (CMP) provides high reliability since it can detect both transient and permanent faults and silent data corruptions. However, comparing {{the results of the}} <b>instruction</b> <b>streams,</b> checkpointing the entire system and re-covering from the detected errors present high performance degradation in execution time. This paper presents FaulTM-multi, transactional memory based fault detection and re-covery scheme for multi threaded applications running on transactional memory hardware in order to reduce these per-formance degradations. 1...|$|R
40|$|The next stream {{predictor}} is {{an accurate}} branch predictor that provides stream level sequencing. Every stream prediction contains a full <b>stream</b> of <b>instructions,</b> that is, a sequence of instructions from {{the target of a}} taken branch to the next taken branch, potentially containing multiple basic blocks. The long size of <b>instruction</b> <b>streams</b> makes it possible for the stream predictor to provide high fetch bandwidth and to tolerate the prediction table access latency. Therefore, an excellent way for improving the behavior of the next stream predictor is to enlarge <b>instruction</b> <b>streams.</b> In this paper, we provide a comprehensive analysis of dynamic <b>instruction</b> <b>streams,</b> showing that focusing on particular kinds of stream is not a good strategy due to Amdahl's law. Consequently, we propose the multiple stream predictor, a novel mechanism that deals with all kinds of streams by combining single streams into long virtual streams. We show that our multiple stream predictor is able to tolerate the prediction table access latency without requiring the complexity caused by additional hardware mechanisms like prediction overriding, also reducing the overall branch predictor energy consumption. Postprint (published version...|$|R
50|$|As of 1996, a {{database}} benchmark {{study found that}} {{three out of four}} CPU cycles were spent waiting for memory.Researchers expect that increasing the number of simultaneous <b>instruction</b> <b>streams</b> with multithreading or single-chip multiprocessing will make this bottleneck even worse.|$|R
5000|$|The first {{implementation}} of tracing is Dynamo, [...] "a software dynamic optimization {{system that is}} capable of transparently improving the performance of a native <b>instruction</b> <b>stream</b> as it executes on the processor". To do this, the native <b>instruction</b> <b>stream</b> is interpreted until a [...] "hot" [...] instruction sequence is found. For this sequence an optimized version is generated, cached and executed.|$|E
5000|$|The {{degree of}} {{intrinsic}} parallelism in the <b>instruction</b> <b>stream</b> (instructions requiring the same computational {{resources from the}} CPU).|$|E
5000|$|The ILP wall; the {{increasing}} difficulty of finding enough parallelism {{in a single}} <b>instruction</b> <b>stream</b> to keep a high-performance single-core processor busy.|$|E
50|$|MPPA is a MIMD (Multiple <b>Instruction</b> <b>streams,</b> Multiple Data) architecture, with {{distributed}} memory accessed locally, not shared globally. Each processor is strictly encapsulated, accessing only its own code and memory. Point-to-point communication between processors is directly {{realized in the}} configurable interconnect.|$|R
40|$|An {{exponentially}} {{increasing demand}} for online services continues pushing server performance into {{the forefront of}} computer architecture. While the diversity and complexity of server workloads places demands on many aspects of server processors, the memory system {{has been among the}} key exposed bottlenecks. In particular, long-latency instruction accesses have long been {{recognized as one of the}} key factors limiting the performance of servers. Server workloads span multiple application binaries, shared libraries, and operating system modules which comprise hundreds of kilobytes to megabytes of code. While steady technological improvements have enabled growth in the total on-chip cache capacity, cache access latency constraints preclude building L 1 instruction caches large enough to capture the instruction working sets of server workloads, leaving L 1 instruction-cache misses as a major bottleneck. In this work, we make the observation that instruction-cache misses repeat in long recurring sequences that we call Temporal <b>Instruction</b> <b>Streams.</b> Temporal <b>instruction</b> <b>streams</b> comprise sequences of tens to thousands of instruction-cache blocks which recur frequently during program execution. The stability and length of the <b>instruction</b> <b>streams</b> lend themselves well to prediction, allowing accurate prediction of long sequences of upcoming instruction accesses once a previousl...|$|R
40|$|Decoupled access/execute {{architectures}} seek {{to maximize}} performance by dividing a given program into two separate <b>instruction</b> <b>streams</b> and executing the streams on independent cooperating processors. The <b>instruction</b> <b>streams</b> consist of those instructions involved in generating memory accesses (the Access stream) {{and those that}} consume the data (the Execute stream). If the processor running the access stream is able {{to get ahead of}} the execute stream, then dynamic pre-loading of operands will occur and the penalty due to long latency operations (such as memory accesses) will be reduced or eliminated. Although these architectures have been around for many years, the performance analysis performed have been incomplete for want of a compiler. Very little has been published on how to construct a compiler for such an architecture. In this paper we describe the partitioning method employed in Daecomp, a compiler for decoupled access/execute processors...|$|R
50|$|The SECD {{machine is}} stack-based. Functions take their {{arguments}} from the stack. The arguments to built-in instructions are encoded immediately after {{them in the}} <b>instruction</b> <b>stream.</b>|$|E
50|$|Intel's Itanium {{architecture}} (among others) {{solved the}} backward-compatibility {{problem with a}} more general mechanism. Within each of the multiple-opcode instructions, a bit field is allocated to denote dependency on the prior VLIW instruction within the program <b>instruction</b> <b>stream.</b> These bits are set at compile time, thus relieving the hardware from calculating this dependency information. Having this dependency information encoded in the <b>instruction</b> <b>stream</b> allows wider implementations to issue multiple non-dependent VLIW instructions in parallel per cycle, while narrower implementations would issue {{a smaller number of}} VLIW instructions per cycle.|$|E
5000|$|The Copper {{is another}} {{sub-component}} of Agnus; The name {{is short for}} [...] "co-processor". The Copper is a programmable finite state machine that executes a programmed <b>instruction</b> <b>stream,</b> synchronized with the video hardware.|$|E
40|$|Exploiting {{parallelism}} is {{an essential}} part of maximizing the performance of an application on a parallel computer. Parallelism is traditionally exploited at two granularities: individual operations are executed in parallel within a processor to exploit instruction-level parallelism and loop iterations or processes are executed in parallel on different processors to exploit loop-level parallelism and process-level parallelism. A new generation of architectures that execute multiple <b>instruction</b> <b>streams</b> on a single chip has the potential of significantly reducing the gap between communication costs within a processor and between processors. This means that parallelism of multiple granularities can be exploited between <b>instruction</b> <b>streams</b> by overlapping regions of code that range in granularity from a small set of instructions to basic blocks, conditionals, loop iterations, loop nests, procedure calls, and collections of such constructs. This opens the way to exploiting more parallelism i [...] ...|$|R
40|$|Reliability is an {{essential}} concern for processor designers due to increasing transient and permanent fault rates. Executing <b>instruction</b> <b>streams</b> redundantly in chip multi processors (CMP) provides high reliability since it can detect both transient and permanent faults. Additionally, it also minimizes the Silent Data Corruption rate. However, comparing {{the results of the}} <b>instruction</b> <b>streams,</b> checkpointing the entire system and recovering from the detected errors might lead to substantial performance degradation. In this study we propose FaulTM, an error detection and recovery schema utilizing Hardware Transactional Memory (HTM) in order to reduce these performance degradations. We show how a minimally modified HTM that features lazy conflict detection and lazy data versioning can provide low-cost reliability in addition to HTM's intended purpose of supporting optimistic concurrency. Compared with lockstepping, FaulTM reduces the performance degradation by 2. 5 X for SPEC 2006 benchmark. Postprint (published version...|$|R
40|$|For this study, we {{analyze the}} dynamic <b>instruction</b> <b>streams</b> of the SPEC 2000 integer {{benchmarks}} to find frequently occurring units of computation, or idioms. An idiom, {{in the broadest}} sense, is an interdependent piece of a computation dataflow. For example, a load-add-store idiom performs an increment operation {{through a set of}} three interdependent instructions...|$|R
50|$|Even {{though the}} <b>instruction</b> <b>stream</b> may contain no inter-instruction dependencies, a superscalar CPU must {{nonetheless}} check for that possibility, {{since there is}} no assurance otherwise and failure to detect a dependency would produce incorrect results.|$|E
50|$|Another {{strategy}} of achieving performance is to execute multiple threads or processes in parallel. This {{area of research}} is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple <b>instruction</b> <b>stream,</b> multiple data stream (MIMD).|$|E
50|$|Indexed {{addressing}} modes add a 16-bit extension {{word to the}} instruction. If both source and destination are indexed, the source extension word comes first. x refers to the next extension word in the <b>instruction</b> <b>stream</b> in the table below.|$|E
40|$|Deeply pipelined {{processors}} {{have relatively}} low issue rates due to dependencies between instructions. In this paper {{we examine the}} possibility of interleaving a second <b>stream</b> of <b>instructions</b> into the pipeline, which would issue instructions during the cycles the first stream was unable to. Such an interleaving {{has the potential to}} significantly increase the throughput of a processor without seriously imparing the execution of either process. We propose a dynamic interleaving of at most 2 <b>instructions</b> <b>streams,</b> which share the the pipelined functional units of a machine. To support the interleaving of 2 <b>instruction</b> <b>streams</b> a number of interleaving policies are described and discused. Finally, the amount of improvement in processor throughput is evaluated by simulating the interleaving policies for several machine variants. 1. Introduction An important metric for evaluating processor performance, especially in a multiprocessing context, is the throughput rate of a processor (defined as h [...] ...|$|R
50|$|TPF {{is capable}} of running on a multiprocessor, that is, on {{mainframe}} systems {{in which there is}} more than one CPU. Within the community, the CPUs are referred to as <b>Instruction</b> <b>Streams</b> or simply I-streams. On a mainframe or in a logical partition (LPAR) of a mainframe with more than one I-stream, TPF is said to be running tightly coupled.|$|R
40|$|We present Outrider, an {{architecture}} for throughput-oriented processors that exploits intra-thread memory-level parallelism (MLP) {{to improve performance}} efficiency on highly threaded workloads. Outrider enables a single thread of execution to {{be presented to the}} architecture as multiple decoupled <b>instruction</b> <b>streams,</b> consisting of either memory accessing or memory consuming instructions. The key insight is that by decoupling the <b>instruction</b> <b>streams,</b> the processor pipeline can expose MLP in a way similar to out-of-order designs while relying on a low-complexity in-order micro-architecture. Instead of adding more threads as is done in modern GPUs, Outrider can expose the same MLP with fewer threads and reduced contention for resources shared among threads. We demonstrate that Outrider can outperform single-threaded cores by 23 - 131 % and a 4 -way simultaneous multi-threaded core by up to 87 % in data parallel applications in a 1024 -core system. Outrider achieves these performance gains without incurring the overhead of additional hardware thread contexts, which results in improved efficiency compared to a multi-threaded core...|$|R
