3599|1|Public
5|$|Westland and IBM {{formed a}} {{consortium}} in 1991 {{to perform the}} helicopter's complex systems integration. The AW101 features a network of helicopter management and mission systems designed to reduce pilot workload and enable the helicopter to undertake {{a wide variety of}} missions. A digital automatic flight control system (AFCS) is employed by the AW101. The AFCS allows the operation of a four-axis (pitch, roll, yaw and collective) autopilot and the automatic stabilisation system, and is linked in with the aircraft's flight management systems. The AFCS, manufactured by Smiths Aerospace, is a dual-duplex system using two flight computers to provide redundancy and <b>fault-tolerance.</b>|$|E
25|$|There {{are also}} {{fundamental}} challenges that {{are unique to}} distributed computing. The first example is challenges {{that are related to}} <b>fault-tolerance.</b> Examples of related problems include consensus problems, Byzantine fault tolerance, and self-stabilisation.|$|E
25|$|The {{academic}} {{study of}} concurrent computing {{started in the}} 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving the mutual exclusion problem. He {{was also one of}} the early pioneers of the research on principles of distributed computing. His foundational work on concurrency, semaphores, mutual exclusion (mutex), deadlock (deadly embrace), finding shortest paths in graphs, <b>fault-tolerance,</b> self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. Shortly before his death in 2002, he received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.|$|E
25|$|Since {{hypotheses}} for {{the origins}} of sex are difficult to test experimentally (outside of Evolutionary computation), most current work {{has focused on the}} maintenance of sexual reproduction. Sexual reproduction must offer significant fitness advantages to a species because despite the two-fold cost of sex, it dominates among multicellular forms of life, implying that the fitness of offspring produced outweighs the costs. Sexual reproduction derives from recombination, where parent genotypes are reorganized and shared with the offspring. This stands in contrast to single-parent asexual replication, where the offspring is identical to the parents. Recombination supplies two <b>fault-tolerance</b> mechanisms at the molecular level: recombinational DNA repair (promoted during meiosis because homologous chromosomes pair at that time) and complementation (also known as heterosis, hybrid vigor or masking of mutations).|$|E
2500|$|The terms {{fault and}} failure are used here {{according}} to the standard definitions originally created by a joint committee on [...] "Fundamental Concepts and Terminology" [...] formed by the IEEE Computer Society's Technical Committee on Dependable Computing and <b>Fault-Tolerance</b> and IFIP Working Group 10.4 on Dependable Computing and Fault Tolerance. A version of these definitions is also described in the Dependability Wikipedia page.|$|E
2500|$|For RAID subsystems, data {{integrity}} and <b>fault-tolerance</b> requirements {{also reduce the}} realized capacity. For example, a RAID1 array has about half the total capacity {{as a result of}} data mirroring, while a RAID5 array with [...] drives loses [...] of capacity (which equals to the capacity of a single drive) due to storing parity information. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault tolerance. Most RAID vendors use checksums to improve {{data integrity}} at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes, or by using separate 512-byte sectors for the checksum data.|$|E
5000|$|<b>Fault-tolerance</b> â€” MooseFS uses replication, {{data can}} be {{replicated}} across chunkservers, the replication ratio (N) is set per file/directory. If (N-1) replicas fail the data will still be available. At the moment MooseFS does not offer any other technique for <b>fault-tolerance.</b> <b>Fault-tolerance</b> for very big files thus requires vast amount of space - N*filesize instead of filesize+(N*stripesize) as {{would be the case}} for RAID 4, RAID 5 or RAID 6. Version 4.x PRO of MooseFS will have RAID6.|$|E
5000|$|... #Subtitle level 3: State: Checkpoints, Savepoints, and <b>Fault-tolerance</b> ...|$|E
5000|$|Mobility (<b>Fault-tolerance</b> for mobile agents & {{personal}} computational grids) ...|$|E
5000|$|... #Subtitle level 3: Selection of {{communication}} paths, reservation and <b>fault-tolerance</b> ...|$|E
5000|$|IP network {{multipathing}} (IPMP), Solaris virtual IP implementation for <b>fault-tolerance</b> {{and load}} balancing ...|$|E
5000|$|<b>Fault-tolerance</b> - The {{software}} is resistant to {{and able to}} recover from component failure.|$|E
50|$|Virtual {{synchrony}} {{to learn}} about how event notification systems can offer stronger ordering and <b>fault-tolerance</b> properties.|$|E
5000|$|... resource-manager: {{distributes}} {{the load}} over the recservers as required to balance load {{and to provide}} <b>fault-tolerance.</b>|$|E
50|$|Under the hood, ProActive Parallel Suite {{relies on}} an Active_object-based Java {{framework}} to optimise task distribution and ensure <b>fault-tolerance.</b>|$|E
5000|$|One {{main point}} of TTEthernet is to {{integrate}} {{a model of}} <b>fault-tolerance</b> and failure management [...] Through a structure of evaluating the risk for each switch and message, it can implement a reliable redundancy management and virtual link integration to assure message transmission even {{in case of a}} switch failure. As it is not dynamically-computed routing like in classical Ethernet context, this <b>fault-tolerance</b> system integrated the statically-defined path mechanic found in real-time networks.|$|E
50|$|The {{parts of}} the ZPU that would be most aided by <b>fault-tolerance</b> are the address bus, stack pointer and program counter.|$|E
50|$|Joseph's {{joint work}} with Zhiming Liu on fault {{tolerance}} gives a formal model that precisely defines {{the notions of}} fault, error, failure and <b>fault-tolerance,</b> and their relationships. It also provided the properties that models fault-affected programs and fault-tolerant programs in terms of transformations. Together, they proposed a design process for fault-tolerant systems from requirement specifications and analysis, fault environment identification and analysis, specification of fault-affected design and verification of <b>fault-tolerance</b> for satisfaction of the requirements specification.|$|E
5000|$|Data is {{automatically}} replicated to multiple nodes for <b>fault-tolerance.</b> Replication across multiple data centers is supported. Failed nodes {{can be replaced}} with no downtime.|$|E
50|$|Replication in {{computing}} involves {{sharing information}} {{so as to}} ensure consistency between redundant resources, such as software or hardware components, to improve reliability, <b>fault-tolerance,</b> or accessibility.|$|E
50|$|The TSN technology, {{especially}} the time-aware scheduler according to IEEE 802.1Qbv, {{have been developed}} for use in mission-critical network environments. In these networks, not only timing guarantees are relevant, but <b>fault-tolerance</b> as well. Networks that support applications such as safety-relevant control loops or autonomous driving in vehicles have to be protected against faults in hardware or network media. The TSN task group is currently specifying the <b>fault-tolerance</b> protocol IEEE 802.1CB for this purpose. In addition to this protocol, existing high-availability protocols such as HSR or PRP that are specified in IEC 62439-3, can be utilized.|$|E
5000|$|<b>Fault-tolerance.</b> A {{group can}} easily support primary-backup forms of <b>fault-tolerance,</b> {{in which one}} process {{performs}} actions and a second one stands by as a backup. Even fancier is the [...] "coordinator/cohort" [...] model, in which each request is assigned to a different coordinator process. Other processes in the group are ranked {{to serve as a}} primary backup, secondary, etc. Since failures are rare, the effect is that a group with N members can potentially handle N times the compute load. Yet if a failure does occur, the group can transparently handle it.|$|E
5000|$|... (note: <b>fault-tolerance</b> can {{be allowed}} here: it's not {{required}} that all participants succeed in decrypting [...] {{as long as}} a qualified set of participants are successful to decrypt [...] ).|$|E
50|$|Doug Tougaw's {{contribution}} to the field {{has focused on the}} building of medium-scale integration components such as full-adders from basic QCA gates as well as <b>fault-tolerance</b> studies of QCA wires.|$|E
50|$|Windows Storage Server 2003 NAS {{equipment}} can be headless, {{which means}} that they are without any monitors, keyboards or mice, and are administered remotely. Such devices are plugged into any existing IP network and the storage capacity is available to all users. Windows Storage Server 2003 can use RAID arrays to provide data redundancy, <b>fault-tolerance</b> and high performance. Multiple such NAS servers can be clustered to appear as a single device, which allows responsibility for serving clients to be shared {{in such a way that}} if one server fails then other servers can take over (often termed a failover) which also improves <b>fault-tolerance.</b>|$|E
50|$|The {{flexibility}} {{associated with}} these limited forms of event reordering and optimistic early delivery permit virtual synchrony platforms to achieve extremely high data rates while still preserving very strong <b>fault-tolerance</b> and consistency guarantees.|$|E
50|$|There {{are also}} {{fundamental}} challenges that {{are unique to}} distributed computing. The first example is challenges {{that are related to}} <b>fault-tolerance.</b> Examples of related problems include consensus problems, Byzantine fault tolerance, and self-stabilisation.|$|E
50|$|A virtual IP address (VIP or VIPA) is an IP {{address that}} doesn't {{correspond}} to an actual physical network interface (port). Uses for VIPs include {{network address translation}} (especially, one-to-many NAT), <b>fault-tolerance,</b> and mobility.|$|E
50|$|Determinism is {{an ideal}} {{characteristic}} for providing <b>fault-tolerance.</b> Intuitively, if multiple copies of a system exist, a fault in one would be noticeable as {{a difference in the}} State or Output from the others.|$|E
50|$|Early NonStop {{applications}} {{had to be}} specifically coded for <b>fault-tolerance.</b> That requirement {{was removed}} in 1983 {{with the introduction of}} the Transaction Monitoring Facility (TMF), which handles the various aspects of fault tolerance on the system level.|$|E
50|$|George is {{also known}} for his {{contributions}} to the theoretical field of self-stabilization (a form of <b>fault-tolerance),</b> where he has helped (with various colleagues) pioneer several general techniques such as local checking, local correction, and counter flushing.|$|E
5000|$|Computer {{engineering}} for {{the design}} of the on-board computers and computer buses. This subsystem is mainly based on terrestrial technologies, but unlike most of them, it must: cope with space environment, be highly autonomous and provide higher <b>fault-tolerance.</b>|$|E
50|$|Dag Johansen, Keith Marzullo, Fred B. Schneider, Kjetil Jacobsen, and Dmitrii Zagorodnov. NAP: {{practical}} <b>fault-tolerance</b> for itinerant computations. Proceedings. 19th IEEE International Conference on Distributed Computing Systems (Cat. No.99CB37003). IEEE Computer Society 1999, pp. 180-9. Los Alamitos, CA, USA.|$|E
50|$|In {{computer}} storage, multipath I/O is a <b>fault-tolerance</b> and performance-enhancement {{technique that}} defines {{more than one}} physical path between the CPU in a computer system and its mass-storage devices through the buses, controllers, switches, and bridge devices connecting them.|$|E
50|$|Research {{continued}} {{within a}} joint industrial development between TTTech and Honeywell. TTEthernet was the industrial {{further development of}} the research. Objectives were extended towards scalable <b>fault-tolerance</b> and a finer classification of event-triggered messages into rate-constrained and best-effort traffic classes.|$|E
50|$|AdHoc, a {{hierarchical}} and fault-tolerant Distributed Shared Memory (DSM) system {{is used to}} interconnect streams of data between processing elements by providing a repository with: get/put/remove/execute operations. Research around AdHoc has focused on transparency, scalability, and <b>fault-tolerance</b> of the data repository.|$|E
