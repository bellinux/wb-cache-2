0|10000|Public
5000|$|The Bridge {{tried to}} build a comprehensive, {{illustrated}} encyclopedia on sheets <b>of</b> <b>standardized</b> <b>formats.</b> Its aim was to improve and organize scholarly information and communication. The following tasks were mentioned and (partially) carried out: ...|$|R
5000|$|... <b>of</b> each <b>item</b> <b>of</b> their <b>standardized</b> diving equipment. This {{is a part}} of the check.|$|R
40|$|Abstract—Electronic data {{interchange}} (EDI) {{is inevitable}} in enabling successful collaborations between different busi-ness partners. Exchanging information electronically requires <b>standardized</b> <b>formats</b> for information exchange. There exist {{a variety of}} standards including bottom-up standards as well as top-down standards. However, business partners may utilize different standards resulting {{in a loss of}} interoperability. To cope with the variety <b>of</b> <b>standardized</b> <b>formats</b> we propose the approach of mapping these formats to the common concept of core components introduced by the United Nations Center for Trade Facilitation and Electronic Business (UN/CEFACT). We evaluate the applicability of different strategies for mapping an arbitrary XML Schema based standard to core components by the example of ebInterface, an Austrian invoicing standard. The evaluation provides evidence that mapping of arbitrary standards to core components indeed provides substantial benefit in leveraging the interoperability between different business document standards. I...|$|R
50|$|In January 2010, the IETF {{chartered}} a new {{working group}} working towards the goal <b>of</b> <b>standardizing</b> the ARF <b>format.</b> The new WG is called Messaging Abuse Reporting Format WG or MARF.|$|R
40|$|The Open Access {{movement}} in scientific publishing and search engines like Google Scholar have made scientific articles more broadly accessible. During the last decade, {{the availability of}} scientific papers in full text has {{become more and more}} widespread thanks to the growing number of publications on online platforms such as ArXiv and CiteSeer. The efforts to provide articles in machine-readable formats and the rise of Open Access publishing have resulted in a number <b>of</b> <b>standardized</b> <b>formats</b> for scientific papers (such as NLM-JATS, TEI, DocBook). Our aim is to stimulate research at the intersection of Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scale text analytics and sense mining of scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing. Comment: 2 pages, paper accepted for the 15 th International Society of Scientometrics and Informetrics Conference (ISSI...|$|R
50|$|Beginning in the 1980s, WYSIWYG {{software}} such as Sibelius, MuseScore, and Finale {{first let}} musicians enter complex music notation {{on a computer}} screen, displaying it just as it will look when eventually printed. Such software stores the music in files <b>of</b> proprietary or <b>standardized</b> <b>formats,</b> usually not directly readable by humans.|$|R
40|$|World's {{second largest}} movie producer, Nollywood (United Nations News Center, 2009), is roughly {{divided into two}} equal parts– English and vernacular. The vernacular aspect {{comprises}} Yoruba (approximately 70 %, Hausa (20 %) and others. The increasing global visibility of the Yoruba genre and its large size elicit a dissection of subtitling, {{which is one of}} the daunting challenges it faces. This paper identifies, evaluates and discusses specific subtitling problems in the context of relevant theoretical framework, using a mixed method of analysis. It examines the issues relating to language, linguistics and understanding in respect of the genre. One significant finding was lack <b>of</b> <b>standardized</b> <b>format</b> <b>of</b> subtitling. What is more, practitioners were not even asking for one! Interestingly, the lackadaisical vitality arose out of non-existence of any ethical universal and a coherent policy on the industry especially from the statutory perspective. Practitioners need encouragement to get round this problem though sanctioning seems a more result-oriented strategy if solution is genuinely craved...|$|R
50|$|The PA formats {{did not end}} up in ISO 216, {{because the}} {{committee}} decided that the set <b>of</b> <b>standardized</b> paper <b>formats</b> should be kept to the minimum necessary. However, PA4 remains of practical use today. In landscape orientation, {{it has the same}} 4:3 aspect ratio as the displays of traditional TV sets, some computer displays (e.g. iPad) and data projectors. PA4, with appropriate margins, is therefore a good choice as the format of presentation slides.|$|R
40|$|The {{increasing}} use of simulators in {{the industry}} and various scientific facilities stressed the lack of simple and accessible content creation tools for these simulators. The goal of this thesis was to study the domain of driving simulators and produce an application, {{which can be used}} to generate logical and geometrical road data. This paper presents the details <b>of</b> the <b>standardized</b> <b>format</b> used to store the logical road description as well as the process and the problems encountered during the development of the application. N 2 COS Computer Science, Master´s Programm...|$|R
40|$|Archivists {{have been}} {{creating}} finding aids for generations, {{and in the}} last three decades they have done this work via a succession <b>of</b> <b>standardized</b> <b>formats.</b> However, like many other disciplines, they have carried out such work in violation of systems analysis. Although purporting to have the users of finding aids systems first and foremost in their mind, archivists have carried out their descriptive work apart from and with little knowledge of how researchers find and use archival sources. In this article, questions are raised about the utility of archival finding aids and how they will stand the test of time. Indeed, archivists, purportedly concerned with considering how records function and will be used over time, ought to apply the same kind of analysis and thinking to their finding aids. In this article, we explore three ways archival finding aids might be examined by outsiders, namely, those concerned with museum exhibitions, design experts, and accountability advocates. Doing this should assist archivists to reevaluate their next wave of experimentation with descriptive standards and the construction of finding aids. Archivists should expand the notion of what we are representing in archival representation. © 2007 by The Haworth Press...|$|R
40|$|ABSTRACT. Archivists {{have been}} {{creating}} finding aids for generations, {{and in the}} last three decades they have done this work via a succession <b>of</b> <b>standardized</b> <b>formats.</b> However, like many other disciplines, they have carried out such work in violation of systems analysis. Although purporting to have the users of finding aids systems first and foremost in their mind, archivists have carried out their descriptive work apart from and with little knowledge of how researchers find and use archival sources. In this article, questions are raised about the utility of archival finding aids and how they will stand the test of time. Indeed, archivists, purportedly concerned with considering how records function and will be used over time, ought to apply the same kind of analysis and thinking to their finding aids. In this article, we explore three ways archival finding aids might be examined by outsiders, namely, those concerned with museum exhibitions, design experts, and accountability advocates. Doing this should assist archivists to reevaluate their next wave of experimentation with descriptive standards and the construction of finding aids. Archivists should expand the notion of what we are representing in archival representation...|$|R
5000|$|... 3D {{films have}} existed {{in some form}} since 1915, but had been largely relegated to a niche in the motion picture {{industry}} because of the costly hardware and processes required to produce and display a 3D film, and the lack <b>of</b> a <b>standardized</b> <b>format</b> for all segments of the entertainment business. Nonetheless, 3D films were prominently featured in the 1950s in American cinema, and later experienced a worldwide resurgence in the 1980s and 1990s driven by IMAX high-end theaters and Disney themed-venues. 3D films {{became more and more}} successful throughout the 2000s, culminating in the unprecedented success of 3D presentations of Avatar in December 2009 and January 2010.|$|R
5000|$|AppleScript was {{designed}} to be used as an accessible end-user scripting language, offering users an intelligent mechanism to control applications, and to access and modify data and documents. AppleScript uses Apple events, a set <b>of</b> <b>standardized</b> data <b>formats</b> that the Macintosh operating system uses to send information to applications, roughly analogous to sending XPath queries over XML-RPC in the world of web services. Apple events allow a script to work with multiple applications simultaneously, passing data between them so that complex tasks can be accomplished without human interaction. For example, an AppleScript to create a simple web gallery might do the following: ...|$|R
5000|$|On September 21, 2015, Cadillac Fairview rebranded its {{shopping}} centre properties, adding the prefix “CF" [...] {{in front of}} each {{shopping centre}} name and phasing out individual mall logos in favour <b>of</b> a <b>standardized</b> logo <b>format</b> and image campaign. According to Cadillac Fairview, this change was intended to [...] "link the Corporation with a premium shopping experience in the minds of Canadian consumers".|$|R
40|$|In {{the wake}} of E-Business product data {{management}} is increasingly becoming important on a cross-company level. An organization's capability of making available high-quality and up-to-date electronic product and service data will constitute a crucial competitive factor. Only consequent application <b>of</b> <b>standardized</b> exchange <b>formats</b> and description models will allow automatic further processing {{of any kind of}} transmitted data. Being an E-Business service portal, the Product Data Clearing Center provides a decisive approach to deal with the tasks concerning electronic product information. A reference application for a practical and sector-wide realization and use of a Product Data Clearing Center is the ETIM project within the electrical industry sector in Germany...|$|R
40|$|Sorry, {{the full}} text of this article is not {{available}} in Huskie Commons. Please click on the alternative location to access it. 159 p. Educators face new challenges today. Swiftly advancing technology, globalization, and the individual's ready access to boundless amounts of information are changing the old way of doing things. Creativity can help prepare students for a life with these new challenges. Apparel design teachers and students were participants in this qualitative investigation into creativity. Interview and focus group techniques yielded data on teachers' and students' attitudes, perceptions, awareness, and fundamental knowledge of creativity. Data revealed that teachers and students had different perspectives concerning creativity. Teachers defined creativity in cognitive terms, most often as problem-solving. Students defined creativity in affective terms and they were emotionally attached to their products. Most participants had one-dimensional views of creativity and they were generally not aware of the complexities of the researched creativity construct. The teachers agree that creativity can be taught in the classroom, but many students contend that teaching creativity will cause a loss of their personal voice in favor <b>of</b> <b>standardized</b> <b>formats</b> and institutionalized concepts of creativity. The data allow for an understanding of a particular group of teachers' and students' perspectives on creativity. Findings highlight the complexities inherent in the construct and possibilities for misunderstandings in the classroom...|$|R
40|$|Many {{valuable}} earth science {{data are}} not available in a digital format. Manual entry of such information into databases is time consuming, unrewarding, and prone to introducing errors. Taxonomic descriptions of fossils are {{a good example of}} valuable data that are overwhelming and available only in printed volumes and journals, some of which are increasingly rare and inaccessible. The highly structured nature of taxonomic procedures and nomenclature means that many previously published data remain equally valid to the present day, and contain information that is currently not available on the World Wide Web; these data would be of great use {{to a wide variety of}} scientists and other end users in government, industry, academia and the general public. This paper describes an XML (extensible markup language) parsing technique that allows taxonomic descriptions to be fully digitized much more rapidly than would be possible by manual entry of the data into a database. The technique exploits the high degree of structure in taxonomic descriptions, which are written in a <b>standardized</b> <b>format,</b> to automate the processing of tagging separate sections of the text. Once tagged using XML, the data can be subjected to complex searches using queries written in any of the XML query standards. The XML-tagged data can potentially be imported into existing databases, in effect removing the necessity to manually enter the information, and hence overcoming the main bottleneck in generating digital data from printed material. Individual parsers can be tailored precisely to the nature of the text being analyzed, and once the underlying concepts and procedures are understood, those interested in acquiring and using digital data will be able to generate XML parsers dedicated to text with different styles <b>of</b> <b>standardized</b> <b>formatting...</b>|$|R
40|$|The European project Nature-SDIplus has {{developed}} data and metadata specifications for three INSPIRE Annex III themes: Habitat and Biotopes, Bio-geographical regions and Species distributions. These {{serves as a}} foundation for the thematic groups developing the corresponding INSPIRE specifications. The aim {{of this study is to}} test a data harmonization approach to make Swedish environmental geodata and metadata compliant with these specifications. In the harmonization process, we use offline transformations that are split into one spatial and one non-spatial part, and <b>standardized</b> <b>formats</b> to allow vendor neutrality. Moreover, we extend the compliance tests to the data and metadata specifications by validating against both eXtensible Markup Language (XML) -schema and Schematron. Finally, we identify harmonization processes that may be costly or have negative impacts on data quality. The harmonized data and metadata are thereafter published as network services compliant with OGC Web Service specifications. The output from our method is data and metadata that are valid to the Nature-SDIplus data specifications and metadata profiles. Although the usage <b>of</b> <b>standardized</b> <b>formats</b> facilitates vendor neutrality, the nonspatial transformation procedures expressed in interoperable languages seem to be insufficient to execute all the mapping rules. Therefore, some of these transformations cannot be executed in a vendor neutral environment without modifications. Furthermore, by splitting the harmonization into two manageable parts, we avoid some limitations about XML schema translations in existing spatial transformation tools. Additional findings are: (1) by extending the validation with Schematron tests, we find non-compliances that have been missed during the XML schema tests; (2) costly processes are identified, which are caused by missing elements and by unstructured information given as comments; and (3) degradation of the positional and thematic accuracy occur during the harmonization...|$|R
25|$|In {{the case}} of ODBC, the drivers {{encapsulate}} many functions that {{can be broken down}} into several broad categories. One set of functions is primarily concerned with finding, connecting to and disconnecting from the DBMS that driver talks to. A second set is used to send SQL commands from the ODBC system to the DBMS, converting or interpreting any commands that are not supported internally. For instance, a DBMS that does not support cursors can emulate this functionality in the driver. Finally, another set of commands, mostly used internally, is used to convert data from the DBMS's internal formats to a set <b>of</b> <b>standardized</b> ODBC <b>formats,</b> which are based on the C language formats.|$|R
40|$|The {{portability}} {{of cloud}} services {{has been the}} major issue of concern among the IT industries. The situations like vendorlock-in, lack <b>of</b> <b>standardized</b> data <b>formats</b> and complex service level agreements are still affecting the majority of IT sector from adapting this widely emerging technology. This paper suggests a new way in order to inculcate the portability among the cloud vendors and to maintain the consumer’s trust in cloud services by ensuring that consumer is the ultimate owner of the data throughout the services. The paper proposes introduction of reliable third party (mediator) between the cloud service provider and the cloud service consumer to remove the various portability issues encountered while switching among the clouds...|$|R
50|$|In {{the case}} of ODBC, the drivers {{encapsulate}} many functions that {{can be broken down}} into several broad categories. One set of functions is primarily concerned with finding, connecting to and disconnecting from the DBMS that driver talks to. A second set is used to send SQL commands from the ODBC system to the DBMS, converting or interpreting any commands that are not supported internally. For instance, a DBMS that does not support cursors can emulate this functionality in the driver. Finally, another set of commands, mostly used internally, is used to convert data from the DBMS's internal formats to a set <b>of</b> <b>standardized</b> ODBC <b>formats,</b> which are based on the C language formats.|$|R
40|$|During {{the last}} decade, {{a number of}} health {{initiatives}} have been undertaken in Australia. However, Australian medical systems still suffer from the chronic problem of inability to share information essential {{to the health and}} wellbeing of patients. The major causes for this are (1) the lack <b>of</b> a <b>standardized</b> <b>format</b> in which patient information is being kept, and (2) the lack of infrastructure to enable sharing of the information among different organizations and institutions. In this paper we propose the use of ontologies, to enable effective translation between different EHR formats, and use of web services to enable efficient information exchange and sharing. The proposed solution has the potential to greatly improve the way patient information is being used, and consequently reduce the associated costs in both human and financial terms...|$|R
5000|$|Even more {{problems}} are connected with complex file formats of various word processors, spreadsheets and graphics software. To alleviate the problem, many software companies distribute free file viewers for their proprietary file formats (one example is Adobe's Acrobat Reader). The other solution {{is the development}} <b>of</b> <b>standardized</b> non-proprietary file <b>formats</b> (such as HTML and OpenDocument), and electronic documents for specialized uses have specialized formats [...] - [...] the specialized electronic articles in physics use TeX or PostScript.|$|R
40|$|Mortality in National Heart, Lung and Blood Institute–sponsored {{clinical}} trials of treatments for acute lung injury (ALI) has decreased dramatically {{during the past}} two decades. As a consequence, design of such trials based on a mortality outcome requires ever-increasing numbers of patients. Recognizing that advances in clinical trial design might be applicable to these trials and might allow trials with fewer patients, the National Heart, Lung and Blood Institute convened a workshop of extramural experts from several disciplines. The workshop assessed the current state of clinical research addressing ALI, identified research needs, and recommended: (1) continued performance of trials evaluating treatments of patients with ALI; (2) development of strategies to perform ALI prevention trials; (3) observational studies of patients without ALI undergoing prolonged mechanical ventilation; and (4) development <b>of</b> a <b>standardized</b> <b>format</b> for reporting methods, endpoints, and results of ALI trials...|$|R
40|$|In {{order to}} develop a {{typology}} of indicators for long-term treatment in foster care, a cluster analysis {{has been carried out}} on the <b>items</b> <b>of</b> a <b>standardized</b> questionnaire which is used by foster care centres in the province of Zuid-Holland. Two types of indication for placement have emerged from the present study. In combination with the foster child's age, these two types discriminate effectively between unsuccessful, prematurely terminated placements after eighteen months and continued placements. It is claimed that the employment of this procedure can effect a {{reduction in the number of}} unsuccessful, prematurely terminated placements...|$|R
50|$|RPS is in {{many ways}} {{comparable}} to the electronic Common Technical Document. Ideally, the FDA would like to implement RPS as the next iteration of eCTD.The idea behind RPS and ICH’s eCTD is the same—the use <b>of</b> a <b>standardized</b> <b>format</b> for regulatory submissions, including PDF documents and SAS datasets. Although document contents are the same for eCTD and RPS, the internal XML structures are very different.RPS will offer two obvious advantages over eCTD. First, RPS will establish two-way communication between the submitter and all FDA-regulated product centers within the agency. Second, RPS will manage the life cycle of submissions by allowing cross-referencing of previously submitted information. This means that for electronic Investigational New Drug (IND) applications, New Drug Applications (NDA), and Biologic License Applications (BLAs), information need only be submitted once and previously submitted electronic documents can be applied to marketing applications. With RPS, archived electronic IND, NDA, and BLA submissions will be retrievable through standardized automated links. eCTD lacks this cross-referencing capability.|$|R
40|$|Space {{has been}} a central {{parameter}} in electroacoustic music composition and performance since its origins. Nevertheless, the design <b>of</b> a <b>standardized</b> interchange <b>format</b> for spatial audio performances is a complex task that poses a diverse set of constraints and problems. This position paper attempts to describe {{the current state of}} the art in terms of what can be called “easy ” today, and what areas pose as-yet unsolved technical or theoretical problems. The paper ends with a set of comments on the process of developing a widely useable spatial sound interchange format. 1...|$|R
40|$|In {{traditional}} lexicostatistics, {{distances between}} languages {{are determined by}} human expert cognacy judgements <b>of</b> <b>items</b> in <b>standardized</b> word lists, e. g., the Swadesh lists (Swadesh 1955). Recently, some researchers have turned to approaches more amenable to automation, hoping that large-scale automatic (lexicostatistic) language classification will thus become feasible...|$|R
40|$|In B 2 B {{relationships}} {{electronic product}} catalogs and the respective catalog data gain an important meaning {{as the starting}} point for procurement decisions. Suppliers have to provide catalog data for their customers and market places in <b>standardized</b> XML <b>formats</b> and defined quality. Contrary to B 2 C, catalog usage in B 2 B is characterized by the fact that data of the catalog-creating enterprise is imported into an information system (target system) of the catalog-receiving enterprise. Despite the application <b>of</b> <b>standardized</b> catalog <b>formats,</b> often a relevant amount of coordination and communication between the involved enterprises is necessary. Especially in the initialization phase, when the first exchange between two partners is established, a lot of adjustments regarding syntax, contents and quality of the transmitted data have to be made. A starting point for the improvement of exchange processes is extending the XML catalog standards so that they support the coordination and the exchange more widely by providing an appropriate process model and additional business messages. The paper pursues this approach by examining the catalog exchange processes for lacks and inadequacies, and developing a three-stage improvement concept {{that can be used for}} the extension of commercial XML catalog standards...|$|R
40|$|Network-based attacks pose {{a strong}} {{threat to the}} Internet landscape. There are {{different}} possibilities to encounter these threats. On the one hand attack detection operated at the end-users' side, {{on the other hand}} attack detection implemented at network operators' infrastructures. An obvious benefit of the second approach is that it counteracts a network-based attack at its root. It is currently unclear to which extent countermeasures are set up at Internet scale and which anomaly detection and mitigation approaches of the community may be adopted by ISPs. We present results of a survey, which aims at gaining insight in industry processes, structures and capabilities of IT companies and the computer networks they run. One result with respect to attack detection is that flow-based detection mechanisms are valuable, because those mechanisms could easily adapt to existing infrastructures. Due to the lack <b>of</b> <b>standardized</b> exchange <b>formats,</b> mitigation across network borders is currently uncommon...|$|R
30|$|High-throughput ‘omics’ {{technologies}} {{have become an}} integral part of medical research, thus paving the way towards Personalized Medicine or ‘Precision Medicine’. Accordingly, methods from Bioinformatics and Systems Biology need to be adapted (or even newly developed) for the context of clinical data [66]. The resulting new field, Systems Medicine [67], sets out to offer a multi-‘omics’ view on patient cohorts. Challenges to be addressed along this way include data integration (i.e., the cross-referencing of different types of patient-specific data and the development and use <b>of</b> <b>standardized,</b> universal data <b>formats)</b> and the capacity to analyze diverse data distributed over interdependent networks.|$|R
40|$|Part 3 : Security ManagementInternational audienceNetwork-based attacks pose {{a strong}} {{threat to the}} Internet landscape. There are {{different}} possibilities to encounter these threats. On the one hand attack detection operated at the end-users’ side, {{on the other hand}} attack detection implemented at network operators’ infrastructures. An obvious benefit of the second approach is that it counteracts a network-based attack at its root. It is currently unclear to which extent countermeasures are set up at Internet scale and which anomaly detection and mitigation approaches of the community may be adopted by ISPs. We present results of a survey, which aims at gaining insight in industry processes, structures and capabilities of IT companies and the computer networks they run. One result with respect to attack detection is that flow-based detection mechanisms are valuable, because those mechanisms could easily adapt to existing infrastructures. Due to the lack <b>of</b> <b>standardized</b> exchange <b>formats,</b> mitigation across network borders is currently uncommon...|$|R
40|$|Abstract. A course {{offering}} is {{a collection}} of learning objects composed together based on a syllabus. Syllabi define the contents of the course, as well as other information such as resources and assignments. Currently, there is no standard format for representing syllabi that can facilitate automatic processing of syllabi contents for various applications. In this paper, we report on the current practices in creating and publishing syllabi and present the motivation for a standardized syllabus schema. We report on our experiences obtaining and identifying syllabi published online by various institutions, and extracting syllabus data from them using genetic algorithms and other machine learning techniques. Finally, we describe the tools needed for working with syllabus schema, and applications that will be made possible with the availability <b>of</b> syllabi in <b>standardized</b> <b>formats.</b> ...|$|R
40|$|The MCP {{guidelines}} on {{the publication of}} the underlying data supporting proteomics papers (1) spearheaded a movement toward more structured sharing of (high quality) data. The original MCP guidelines discuss the criteria by which quality is judged. However, this is only one aspect of the process of publishing proteomics data. Upstream of the quality assignment are the use <b>of</b> <b>standardized</b> data <b>formats</b> and the fulfilling of minimal reporting requirements, such as those created by the Human Proteome Organization Proteomics Standards Initiative (HUPO-PSI) (2), first led by Rolf Apweiler and currently chaired by Henning Hermjakob with Rudi Aebersold as co-chair. It is interesting to see how these three aspects interact: the <b>standardized</b> data <b>formats</b> allow the uniform and effortless reading of data generated in different laboratories, whereas the minimal reporting requirements ensure that sufficient information is made available to perform the quality assignment according to a defined set of criteria. Obviously the undertaking of data sharing does not end with standards, reporting guidelines, or quality assignment because the data must ultimately be made publicly available. Availability, however, should be partnered with accessibility, implying a limited number of locations that are well stocked with data and offering powerful query abilities. These specific requirements are best satisfied by centralized data repositorie...|$|R
40|$|BACKGROUND: Pharmacovigilance {{involves}} the detection, assessment, understanding, {{and prevention of}} adverse drug reactions (ADRs), nationally and internationally. Effective communication, which relies increasingly on the Internet, is a crucial aspect of pharmacovigilance activities. AIM: The {{aim of this study}} was to perform an exploratory survey of national pharmacovigilance websites and compare their contents. METHODS: Of 99 international pharmacovigilance organizations known to us (listed in the Side Effects of Drugs Annual 30), 45 included website addresses and 35 provided some or all of the information in English. We reviewed 10 of these 35 websites in order to identify their contents. The 10 sites that we selected contained the most extensive information on pharmacovigilance of those that we were able to access. Reviewing these sites, we identified 32 <b>items</b> <b>of</b> information that we used to assess the scope of each website systematically, using a scoring system based on the presence or absence <b>of</b> those <b>items.</b> RESULTS: All the websites gave clear descriptions of national pharmacovigilance requirements and the reporting systems for ADRs, and all included devices. Beyond this, there was great variability in content from site to site. Few websites allowed access to raw pharmacovigilance data, such as individual case reports. CONCLUSIONS: Online drug safety communication from the selected national websites we examined is highly variable from site to site, although a wider study is needed to confirm this. Agreement on the key components of pharmacovigilance websites would facilitate the development <b>of</b> a <b>standardized</b> <b>format</b> to improve online communication...|$|R
40|$|The unique {{geographic}} {{features of}} Taiwan {{are attributed to}} the rich indigenous and endemic plant species in Taiwan. These plants serve as resourceful bank for biologically active phytochemicals. Given that these plant-derived chemicals are prototypes of potential drugs for diseases, databases connecting the chemical structures and pharmacological activities may facilitate drug development. To enhance {{the utility of the}} data, it is desirable to develop a database of chemical compounds and corresponding activities from indigenous plants in Taiwan. A database of anticancer, antiplatelet, and antituberculosis phytochemicals from indigenous plants in Taiwan was constructed. The database, TIPdb, is composed <b>of</b> a <b>standardized</b> <b>format</b> <b>of</b> published anticancer, antiplatelet, and antituberculosis phytochemicals from indigenous plants in Taiwan. A browse function was implemented for users to browse the database in a taxonomy-based manner. Search functions can be utilized to filter records of interest by botanical name, part, chemical class, or compound name. The structured and searchable database TIPdb was constructed to serve as a comprehensive and standardized resource for anticancer, antiplatelet, and antituberculosis compounds search. The manually curated chemical structures and activities provide a great opportunity to develop quantitative structure-activity relationship models for the high-throughput screening of potential anticancer, antiplatelet, and antituberculosis drugs...|$|R
