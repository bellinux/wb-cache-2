11|15|Public
40|$|Game {{traffic is}} often timing sensitive. • 802. 11 (WiFi) can have {{relatively}} limited resources. • Previously considered game server/client resource allocation for WiFi. •Now interested in split between game and other traffic. Aim: Want to prioritise time-sensitive game traffic over other traffic, while retaining good performance for other traffic. Observation: {{two types of}} congestion in WiFi — internal and external. • <b>Internal</b> <b>congestion</b> arises from shared buffering. • External congestion because medium is shared and MAC regulates access. • 802. 11 e provides useful tools: multiple queues and tuneable MAC. • 802. 11 e features we require are included in WME/WMM and 802. 11 n. • Interstation traffic usually through access point. ADRA Scheme The Application Divided Resource Allocation (ADRA) scheme: •Deal with <b>internal</b> <b>congestion</b> by using separate 11 e queues. •Deal with external congestion by assigning MAC parameters. •Can do this at all devices, including access point. •Classify packets into queues (e. g. packet size). listene...|$|E
40|$|Abstract: According to the European Commission, Svenska Kraftnät, the Swedish network operator, {{might have}} {{violated}} competition rules by limiting cross-border transmission capacity {{to relieve congestion}} within Sweden. Eventually, the case was settled and Svenska Kraftnät offered commitments to address the Commission’s concerns. As an interim remedy, it committed to reduce transmission flow of electricity on internal network bottlenecks primarily by introducing national measures and by not reducing interconnection capacity. As a final remedy, Svenska Kraftnät agreed to split the Swedish market into multiple price zones. Congestion within Sweden would then be solved by adjusting the prices of those zones. We analyse the economic effects of the alleged abuse and the remedy package. We make three observations. Firstly, it might be socially optimal to reduce cross-border capacity in response to <b>internal</b> <b>congestion.</b> Hence, without an in-depth economic analysis the Commission risked preventing efficient behaviour. Secondly, the interim remedy of handling <b>internal</b> <b>congestion</b> primarily by national measures is not socially optimal, and it cannot be ruled out that it reduces overall welfare. Thirdly, even though splitting the market into price zones may improve allocative efficiency within Sweden, it does not prevent Svenska Kraftnät from potential manipulation of cross-border transmission capacity. ...|$|E
40|$|This work {{analyses}} {{the problem}} of defining when to diploid a new distribution center. The decision model considers the usual logistic cost drivers such as shipping, inventory, infrastructure and administration, focusing in the first one. The shipping cost driver {{is determined by the}} client coverage of the centers, which is calculated using a first order condition heuristic that takes into account the facilies' <b>internal</b> <b>congestion.</b> The model is applied to a company that operating in Santiago de Chile which faces a highly seasonal demand. We show that by defining the coverage in a dynamic fashion that depends on the demand, it is possible to postpone the deployment of a new center so the company can save an estimate of 2 percent of the delivery cost. Distribution center, coverage, transportation, Chile...|$|E
40|$|The {{class of}} Timed Event Graphs (TEGs) has widely been studied {{for the last}} 30 years thanks to an {{algebraic}} approach known as the theory of Max-Plus linear systems. In particular, the modeling of TEGs via formal power series has led to input-output descriptions for which some model matching control problems have been solved. In the context of manufacturing applications, the controllers obtained by these approaches {{have the effect of}} regulating material flows in order to decrease <b>internal</b> <b>congestions</b> and intermediate stocks. The objective of this work is to extend the class of systems for which a similar control synthesis is possible. To this end, we define first a subclass of timed Petri nets that we call Balanced Timed and Weighted Event Graphs (B-TWEGs). B-TWEGs can model synchronisation and delays (B-TWEGs contains TEGs) and can also describe some dynamic phenomena such as batching and event duplications. Their behavior is described by some rational compositions of four elementary operators γ n, δ t, μm and βb on a dioid of formal power series. Then, we show that the series associated to B-TWEGs have a three dimensional graphical representation with a property of ultimate periodicity. This modeling allows us to show that B-TWEGs can be handled thanks to finite and canonical forms. Therefore, the existing results on control synthesis, in particular the model matching control problem, have a natural application in that framework...|$|R
40|$|The {{author has}} been doing its Dissertation Project at Strategic Business unit (SBU), {{which is one of}} the TELMEC Company’s Factory Units, that {{produces}} mainly domestics cooking pots and pans. The factory has been presenting some internal production performance issues. One of these issues which have been affecting the production is the excessive transportation of material handling caused by old production layout design, and the inadequate co-ordination of information process causing an increase amount of inventory and <b>internal</b> <b>congestions.</b> In order to improve the production performance, is necessary to create capacity of unification of the internal chain, having technology of information, excellent communication among departments, providing training program for workers to increase its work abilities and multifunctional skills. To satisfy these requirements the author has been implementing some elements of the Just in time System/Lean to solve efficiently these issues by focusing on the cellular flow production design and the total kanban pull system, which will be supported by the Systematic Layout Planning (SLP). As the Just in Time is a very complex system to implement, these techniques will be considered as the first steps for the implementation of its elements in the factory. The most significant impact will be on the internal production process giving a competitive advantage in reducing some of its non-added-value such as lead times, material handling, inventories and distance traveling in order to respond rapidly to customer’s demand, with right quantity, at the right place, at the right time, at the right quality, and reduced costs of performance...|$|R
500|$|Leg {{injuries}} {{that are not}} immediately fatal still may be life-threatening because a horse's weight must be distributed evenly on all four legs to prevent circulatory problems, laminitis, and other infections. If a horse loses the use of one leg temporarily, there is the risk that other legs will break down during the recovery period because they are carrying an abnormal weight load. While horses periodically lie down for brief periods of time, a horse cannot remain lying in {{the equivalent of a}} human's [...] "bed rest" [...] because of the risk of developing sores, <b>internal</b> damage, and <b>congestion.</b>|$|R
40|$|Abstract. Although optical {{technologies}} have been effectively employed {{to increase the}} capacity of communication links, it has proven difficult to apply these technologies towards increasing the capacity of Internet routers, which implement the central forwarding and routing functions of the Internet. Motivated by the need for future routers that will forward packets among several high-speed links, this work considers the design of an Internet router that can forward packets from a terabit-per-second link without <b>internal</b> <b>congestion</b> or packet loss. The router consists of an optical booster integrated with a conventional (mostly electronic) Internet router. The optical booster processes Internet Protocol packets analogously to the hosting router, but it can avoid the time-consuming lookup function and keep packets in an entirely optical format. An optically boosted router is an inexpensive, straightforward upgrade that can be deployed readily in a backbone IP network, and provides optical processing throughput even when not deployed ubiquitously. ...|$|E
40|$|International audienceMany-core {{architectures}} {{are more}} promising hardware to design real-time systems than multi-core systems as they should enable an easier mastered integration of a higher number of applications, potentially of different level of criticalities. However, the worst-case behavior of the Network-on-Chip (NoC) for both inter-core and core-to-Input/Output 1 (I/O) communications of critical applications must be established. The mapping over the NoC of both critical and non-critical applications {{has an impact on}} the network contention these critical communications exhibit. So far, all existing mapping strategies have focused on inter-core communications. However, many-cores in embedded real-time systems can be integrated within backbone Ethernet networks, as they mostly provide Ethernet controllers as I/O interfaces. In this work, we first show that Ethernet packets can be dropped due to an <b>internal</b> <b>congestion</b> in the NoC, if these coreto I/O communications are not taken into account while mapping applications. To solve this issue, we show on an avionic case study the benefits of the core-to-I/O contention-aware mapping strategy we propose...|$|E
40|$|This {{study was}} carried out to detect Lactococcus garvieae from {{experimentally}} infected rainbow trout (Oncorhynchus mykiss, W.) using bacteriological, serological, and histopathological techniques. Rainbow trout were experimentally infected with two Lactococcus garvieae strains (L 1 and L 2) by intraperitonal injection. These strains caused bilateral exophthalmia with periocular hemorrhage, corneal opacification, darkening of the skin, strong <b>internal</b> <b>congestion</b> in the visceral organs, and {{enlargement of the spleen}} in the infected rainbow trout. The two L. garvieae strains were re-isolated from experimentally infected fish groups and identified with standard biochemical methods and API 20 STREP system. The presence of L. garvieae in the tissue of experimentally infected fish was confirmed by Indirect Fluorescent Antibody Technique (IFAT). The humoral antibody response of the infected fish was confirmed by slide agglutination, IFAT, and Enzyme Linked Immunosorbent Assay (ELISA). Histopathologically, liquefactive necrosis in the liver, spleen, and kidney, epithelial cells hyperplasia, hemorrhages and telangiectasia of the gill filaments, and sub-retinal hemorrhages in the eyes were observed...|$|E
40|$|We develop {{theoretical}} models of airport congestion with non-atomistic traffic and implement them empirically {{using data from}} twenty-seven major US airports to determine whether dominant airlines internalize or ignore self-imposed congestion. Estimates of minute-by-minute delay patterns at each airport calibrate structural models of landing and takeoff queues as dynamic functions of traffic rates and airport capacities. These functions determine the <b>internal</b> and external <b>congestion</b> that aircraft impose on one another. Specification tests largely reject the internalization model. Optimal pricing values all time using non-dominant aircraft cost coefficients and treats all delays as external—i. e., fees equal opportunity costs of allocating peak capacity to dominant airlines. Hub-and-spoke airline networks, simulated annealing, commercial aviation, airline competition, airline mergers, airfares,...|$|R
40|$|Hydrogen {{is seen as}} an {{important}} energy carrier for the future, with a great benefit being carbon-free emissions at its point of use. A hydrogen transport system between manufacturing sites and end users is required, and one solution proposed is its addition to existing natural gas pipeline networks. A major concern with this approach is that the explosion hazard may be increased, relative to natural gas, should an accidental release occur. This paper describes a mathematical model of confined, vented explosions of mixtures of methane and hydrogen of value in performing consequence and risk assessments. The model is based on solutions of averaged forms of the Navier-Stokes equations, with the equation set closed using k-e{open} and second-moment turbulence models, and the turbulent burning velocity determined from correlations of data on CH-H mixtures reported in the literature. Predictions derived for explosions in a 70  m vessel, with and without <b>internal</b> pipe <b>congestion,</b> show reasonable agreement with available data, and demonstrate that hydrogen addition can {{have a significant effect on}} overpressure generation. Conclusions drawn from the calculations go some way to identifying safe operating limits for hydrogen addition...|$|R
40|$|With {{the growing}} share of wind production, {{understanding}} its impacts on electricity price and greenhouse gas (GHG) emissions becomes increasingly relevant, especially to design better wind-supporting policies. <b>Internal</b> grid <b>congestion</b> {{is usually not}} taken into account when assessing the price impact of fluctuating wind output. Using 2006 - 2011 hourly data from Ontario (Canada), we establish {{that the impact of}} wind output, both on price level and marginal GHG emissions, greatly differs depending on the congestion level. Indeed, from a 3. 3 % price reduction when wind production doubles, the reduction jumps to 5. 5 % during uncongested hours, but is only 0. 8 % when congestion prevails. Similarly, avoided GHG emissions due to wind are estimated to 331. 93 kilograms per megawatt-hour (kg/MWh) using all data, while for uncongested and congested hours, estimates are respectively 283. 49 and 393. 68 kg/MWh. These empirical estimates, being based on 2006 - 2011 Ontario data, cannot be generalized to other contexts. The main contribution {{of this paper is to}} underscore the importance of congestion in assessing the price and GHG impacts of wind. We also contribute by developing an approach to create clusters of data according to the congestion status and location. Finally, we compare different approaches to estimate avoided GHG emissions. ...|$|R
40|$|This paper {{presents}} two {{case studies}} where a macroscopic model-based approach for traffic state estimation, {{which we have}} recently developed, is employed and tested. The estimation methodology is developed for a "mixed" traffic scenario, where traffic is composed of both ordinary and connected vehicles. Only average speed measurements, which may be obtained from connected vehicles reports, and a minimum number (sufficient to guarantee observability) of spot sensor-based total flow measurements are utilised. In the first case study, we use NGSIM microscopic data {{in order to test}} the capability of estimating the traffic state on the basis of aggregated information retrieved from moving vehicles and considering various penetration rates of connected vehicles. In the second case study, a longer highway stretch with <b>internal</b> <b>congestion</b> is utilised, in order to test the capability of the proposed estimation scheme to produce appropriate estimates for varying traffic conditions on long stretches. In both cases the performances are satisfactory, and the obtained results demonstrate the effectiveness of the methodology, both in qualitative and quantitative terms. Comment: Submitted to the the 95 th Annual Meeting of the Transportation Research Board (TRB) and for publication to Transportation Research Record (TRR) on July 28, 201...|$|E
40|$|The {{continuous}} {{growth of}} network communication {{has been increasing}} the demands for high transmission capacity and quality of services (QoS) in networks. Optical networks are believed to solve the high transmission capacity problem in low cost with wavelength division multiplexing (WDM). However, no practical optical random access memory (RAM) makes buffering in switching a challenging problem. Many researches have been performed {{on the design of}} an optical switch with buffer using optical fiber delay lines together with switching fabrics. A switch for real network use should have the ability to handle bursty traffic and support QoS. However, most of the designs in literature are satisfactory for some aspects only. The thesis is focused on the design of an optical packet switch, which is practical in real networks. Through reviewing some designs in literature, it is found that a type of multistage feed-forward output buffering switch can be implemented through utilizing WDM such that it has packet drops only when buffer overflows under FIFO (first-in first-out) queueing discipline. Therefore, it likes SLOB, a switch proposed in another literature, that it can handle bursty traffic. The change from feed-forward to inter-connected stages architecture is done to adapt multi-classes switching and results in the switch proposed in this thesis. With the designed scheduling algorithm, the ability for handling bursty traffic with FIFO queueing discipline is kept. Simulations have been performed to study the ability of the proposed switch in QoS. It can be concluded that its <b>internal</b> <b>congestion</b> free property for FIFO queueing discipline allows a low packet loss rate for multi-classes switching when the difference of proportions of packets between different classes is large. When comparing with some other designs, the proposed switch shows its potential for real network use with the advantages in bursty traffic and QoS...|$|E
40|$|Many-core {{architectures}} {{are more}} promising hardware to design real-time systems than multi-core systems as they should enable an easier mastered integration of a higher number of applications, potentially of different level of criticalities. In embedded real-time systems, these architectures will be integrated within backbone Ethernet networks, as they mostly provide Ethernet controllers as Input/Output(I/O) interfaces. Thus, a number of applications of different level of criticalities could be allocated on the Network-on-Chip (NoC) and required to communicate with sensors and actuators. However, the worst-case behavior of NoC for both inter-core and core-to-I/O communications must be established. Several NoCs targeting hard real-time systems, made of specific hardware extensions, have been designed. However, none of these extensions are currently available in commercially available NoC-based many-core architectures, that instead rely on wormhole switching with round-robin arbitration. Using this switching strategy, interference patterns can occur between direct and indirect flows on many-cores. Besides, the mapping over the NoC of both critical and non-critical applications {{has an impact on}} the network contention these core-to-I/O communications exhibit. These core-to-I/O flows (coming from the Ethernet interface of the NoC) cross two networks of different speeds: NoC and Ethernet. On the NoC, the size of allowed packets is much smaller than the size of Ethernet frames. Thus, once an Ethernet frame is transmitted over the NoC, it will be divided into many packets. When all the data corresponding to this frame are received by the DDR-SDRAM memory on the NoC, the frame is removed from the buffer of the Ethernet interface. In addition, the congestion on the NoC, due to wormhole switching, can delay these flows. Besides, the buffer in the Ethernet interface has a limited capacity. Then, this behavior may lead to a problem of dropping Ethernet frames. The idea is therefore to analyze the worst case transmission delays on the NoC and reduce the delays of the core-to-I/O flows. In this thesis, we show that the pessimism of the existing Worst-Case Traversal Time (WCTT) computing methods and the existing mapping strategies lead to drop Ethernet frames due to an <b>internal</b> <b>congestion</b> in the NoC. Thus, we demonstrate properties of such NoC-based wormhole networks to reduce the pessimism when modeling flows in contentions. Then, we propose a mapping strategy that minimizes the contention of core-to-I/O flows in order to solve this problem. We show that the WCTT values can be reduced up to 50 % compared to current state-of-the-art real-time packet schedulability analysis. These results are due to the modeling of the real impact of the flows in contention in our proposed computing method. Besides, experimental results on real avionics applications show significant improvements of core-to-I/O flows transmission delays, up to 94 %, without significantly impacting transmission delays of core-to-core flows. These improvements are due to our mapping strategy that allocates the applications in such a way to reduce the impact of non-critical flows on critical flows. These reductions on the WCTT of the core-to-I/O flows avoid the drop of Ethernet frames...|$|E
40|$|Several European {{initiatives}} {{consider the}} electrical {{integration of the}} Euro-Mediterranean region a key priority for meeting future European Union (EU) energy policy goals. Ambitious plans include the development of Renewable Energy Sources (RES) {{in the region as}} well as transmission interconnectors between the two shores of the Mediterranean Sea. The success of such initiatives, in addition to several technoeconomic, political, environmental, regulatory and financial obstacles, depends on the ability of the European electricity network to suitably accommodate large electricity imports from North Africa. In order to address the issue, this paper, based on the combination of two methodologies, presents a first techno-economic analysis of the effects of electricity imports from North Africa on the European and the Italian power systems in 2030. Within a common framework, the adopted approach has proved its feasibility with coherent results showing a decrease in electricity prices in Europe. The European study shows how net electricity exchanges tend to follow the direction from South to North. The impact of North-African electricity on the Italian system is relevant. Also, Italy’s potential of becoming a Mediterranean electricity hub is emphasised. National <b>internal</b> grid <b>congestion</b> results to be a crucial issue for the Euro-Mediterranean electrical integration. JRC. F. 3 -Energy securit...|$|R
500|$|The {{level of}} {{treatment}} given to injured Thoroughbreds {{is often more}} intensive than for horses of lesser financial value [...] but also controversial, {{due in part to}} the significant challenges in treating broken bones and other major leg injuries. Leg injuries that are not immediately fatal still may be life-threatening because a horse's weight must be distributed evenly on all four legs to prevent circulatory problems, laminitis, and other infections. If a horse loses the use of one leg temporarily, there is the risk that other legs will break down during the recovery period because they are carrying an abnormal weight load. While horses periodically lie down for brief periods of time, a horse cannot remain lying in the equivalent of a human's [...] "bed rest" [...] because of the risk of developing sores, <b>internal</b> damage, and <b>congestion.</b>|$|R
40|$|Abstract—Increasing {{the number}} of {{processors}} in a single chip toward network-based many-core systems requires a run-time task allocation algorithm. We propose an efficient mapping algorithm that assigns communicating tasks of incoming applications onto resources of a many-core system utilizing Network-on-Chip paradigm. In our contiguous neighborhood allocation (CoNA) algorithm, we target at the reduction of both <b>internal</b> and external <b>congestion</b> due to detrimental impact of congestion on the network performance. We approach the goal by keeping the mapped region contiguous and placing the communicating tasks in a close neighborhood. A completely synthesizable simulation environment where none of the system objects {{are assumed to be}} ideal is provided. Experiments show at least 40 % gain in different mapping cost functions, as well as 16 % reduction in average network latency compared to existing algorithms. Keywords-Network-on-Chip; MPSoC; run-time; dynamic; task mapping; processor allocation; congestion; contiguous; latency; performance I...|$|R
40|$|Introduction. Heat stroke is {{the most}} {{dangerous}} among numerous disorders caused by elevated environmental temperature. It is characterized by an increased body temperature of over 40 °C, the dysfunction of the central nervous system and the development of multiple organ failure. The aim of this paper was to highlight problems in the clinical and post-mortal diagnosis of fatal heat stroke. Case Outline. A 20 -year-old male was found unconscious on the street; on admission at the Emergency Center, Clinical Center of Serbia, Belgrade, he was in a coma. The body temperature of 40 °C was maintained despite the applied therapy, meningeal signs were negative, tachycardia with gallop rhythm, hypotension, bleeding from the nose and mouth, and presence of skin bruises. Laboratory findings: highly elevated LDH and creatine kinase, elevated serum creatinine, AST, and signs of DIC. Lethal outcome occurred 6 hours after admission, and the case remained clinically unsolved. Autopsy showed signs of hemorrhagic diathesis, brain and pulmonary edema, and microscopic examination revealed general <b>congestion,</b> <b>internal</b> bleeding in various organs, cerebral edema, massive blood aspiration and pulmonary edema. Toxicological and bacteriological examinations were negative. Based on these findings and subsequently obtained data on the conditions at the workplace where the young man had a part-time job, it was concluded that the violent death was caused by heat stroke. Conclusion. Since heat stroke is associated with a high mortality rate and high incidence of serious and permanent organ damage in survivors, it is important to make the diagnosis of heat stroke as quickly as possible and apply appropriate treatment. Misdiagnosis of heat stroke, and consequently inadequate treatment, with a potential fatal outcome for the patient, can be the reason for blaming doctors for the legal offense of medical malpractice in failing to administer first aid...|$|R
40|$|Piroxicam is a benzothiazine {{compound}} with anti-inflammatory, antipyretic, and analgesic properties. Because of {{the very}} high efficacy of piroxicam and its increasing use {{in the treatment of}} carcinomas in dogs and cats, {{there is a need for}} acute toxicity study of piroxicam in monogastric animals and its potential for causing secondary poisoning in puppies. Piroxicam manufactured by Shanxi Federal Pharmaceutical Co, Ltd. was used for this study. Revised up-and-down procedure was used for the estimation of median lethal dose in mouse (259. 4 ± 51. 9 mg/kg), rat (259. 4 ± 69. 6 mg/kg), rabbit (707. 5 ± 130. 8 mg/kg), cat (437. 5 ± 128. 1 mg/kg), guinea pig (218. 7 ± 64. 1 mg/kg), monkey (733. 3 ± 83. 3 mg/kg), broiler (285. 3 ± 62. 5 mg/kg), hen (638. 3 ± 115. 4 mg/kg), turkey (707. 5 ± 130. 8 mg/kg), pigeon (375 ± 55. 9 mg/kg), and duck (311. 3 ± 46. 6 mg/kg). The acute toxicity signs of piroxicam at doses 207. 5 mg/kg and above observed in the animals are torticollis, opisthotonos, somnolence, lethargy, diarrhea, gastroenteritis, generalized <b>internal</b> bleeding, anemia, <b>congestion</b> of the lung and liver, flaccid paralysis, cheesy lung, urinary incontinence, engorged urinary bladder, convulsive jerking of the limbs, lying in ventral recumbency, gasping for air, roaring, and death. Three out of six puppies died after being fed the carcasses of poisoned turkey, duck, and hen administered piroxicam at doses of 1000, 415, and 1000 mg/kg, respectively. White flaky cheesy materials observed in turkeys were also observed in the gastrointestinal content of the puppies. Paleness of carcasses, watery crop content, dryness of pericardium, gastroenteritis, intestinal perforation, and whitish pericardium were observed in broilers. There were effusions in thoracic and abdominal cavities as seen in all other carcasses poisoned primarily by piroxicam. Administration of atropine (0. 02 mg/kg) led to survival of the remaining puppies. In conclusion, piroxicam is very to moderately toxic in monogastric animals...|$|R
40|$|In a liberalized {{electricity}} {{market with}} an economic merit based dispatch, “grid congestion” is talked {{about when the}} electricity flows, corresponding to the production and consumption schedules defined by the spot market and bilateral contracts, are incompatible with the transmission capacity allowed by the grid. In Italy, for the Daily and the Adjustment Market, the Discipline of the Electricity Market [1, 2] provides for the employment of a “zonal” technique to solve congestions, with the possible formation of market areas with different energy selling prices; from the demand side, a single national price is applied. An additional and specific market will manage the resolution of grid <b>congestions</b> <b>internal</b> to the zones. Congestion resolution techniques based on nodal prices are used in other electricity markets, where the liberalization process is at a more advanced stage (among these, some states in the north east of the U. S. A.). In such markets the operators that consume or produce electricity, respectively pay and receive prices that depend on the grid node they are connected to. The use of a nodal price technique allows collecting very punctual and localized economic signals; in addition, such a methodology limits the inevitable approximations typical of a predefinition of market zones, as {{it is based on}} a detailed representation of the grid. On the other hand, the use of a nodal technique instead of a zonal method implies a severe increase of computational times and a major discontinuity in operator treatment; this choice must therefore be suitably justified by a more precise incentive to the rational use of the grid. In this paper the nodal and zonal techniques for electricity price definition are described and compared from a qualitative and numerical point of view, both on a test network and a realistic case related to the Italian grid. The technical and economical results are analyzed in terms of sale revenues, customer costs, rules transparency, congestion fees and incentive to a market-based system development. The paper also examines the possibility, if a zonal technique is applied, of using the results of a nodal analysis to define with suitable criteria the grid zones...|$|R

