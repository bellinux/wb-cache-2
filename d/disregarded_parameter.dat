0|20|Public
40|$|This paper {{studies the}} optimal {{monetary}} {{policy in the}} presence of uncertainty about the natural rate and the short-run inflation-unemployment tradeoff. Two conflicting motives drive policy. In the static version of the model, uncertainty provides a motive for the policymaker to move cautiously. In the dynamic version, uncertainty motivates an element of experimentation. I find that the optimal policy that balances these motives typically still exhibits gradualism, i. e., is less aggressive than a policy that <b>disregards</b> <b>parameter</b> uncertainty. Exceptions occur when uncertainty is very high and inflation close to target. Monetary policy; Unemployment...|$|R
3000|$|Our {{approach}} also {{stems from}} {{the idea that there}} are families of malware such that two members of the same family share a common [...] "engine." [...] Moreover, there are malware generation utilities which use a common engine to create new malware instances; this engine may even be used to polymorph the threat as it propagates. When searching for such common engines among known malware, one must be aware that malware designers will attempt to hide such engines using a broad range of techniques. For example, these common engines may be located in varying locations inside the executables, and thus may be mapped to different addresses in memory or even perturbed slightly. To overcome such practices, we suggest <b>disregarding</b> any <b>parameters</b> of the OpCodes. We believe that <b>disregarding</b> the <b>parameters</b> would provide a more general representation of the files, which is expected to be more effective for purposes of classification into benign and malicious files.|$|R
40|$|Recent {{empirical}} research concerning {{the relationship between}} in ation and unemployment, a relationship that {{is central to the}} design of monetary policy, has been characterized by an active debate about the precision of relevant parameter estimates such as the estimated natural unemployment rate. This paper studies the optimal monetary policy in the presence of uncertainty about the natural rate and the short-run in ation-unemployment tradeo in a simple macroeconomic model. Two con icting motives drive the optimal policy. In the static version of the model, uncertainty provides a motive for the policymaker to move more cautiously than she would if she knew the true parameters. In the dynamic version, uncertainty also motivates an element of experimentation in policy. I nd that the optimal policy that balances the cautionary and activist motives typically exhibits gradualism, i. e. it is less aggressive than a policy that <b>disregards</b> <b>parameter</b> uncertainty. Exceptions occur when uncertainty isvery high and in ation close to target...|$|R
40|$|Inflation-targeting {{central banks}} have only {{imperfect}} {{knowledge about the}} effect of policy decisions on inflation. An important source of uncertainty is the relationship between inflation and unem-ployment. This paper studies the optimal monetary policy in the presence of uncertainty about the natural unemployment rate, the short-run inflation-unemployment tradeoff and the degree of inflation persistence in a simple macroeconomic model that incorporates rational learning by the central bank as well as market participants. Two conflicting motives drive the optimal policy. In the static version of the model, uncertainty provides a motive for the policymaker to move more cautiously than she would if she knew the true parameters. In the dynamic version, uncertainty also motivates an element of experimentation in policy. The optimal policy, which balances the cautionary and activist motives, is computed using empirical estimates of Phillips curve uncertainty as a benchmark. The effect due to experimentation is of quantitative relevance for moderate to high degrees of uncertainty. However, gradual inflation stabilization typically remains optimal, that is, the optimal policy response to inflation is still less aggressive than a policy that <b>disregards</b> <b>parameter</b> uncertainty. Exceptions occur when uncertainty is very high and inflation close to target...|$|R
40|$|When many {{fields of}} {{pedestrian}} and cyclist safety have been extensively studied, the surfacing {{has long been}} left unquestioned, despite being developed for another mode of transport and {{being one of the}} main causes for falls and fall injuries. In this project new surfacing materials for pedestrian and cyclist safety have been produced. Focusing on augmenting previously largely <b>disregarded</b> <b>parameters</b> as impact absorption, comfort and visibility {{at the same time as}} avoiding deteriorating of crucial parameters as friction and wear resistance. Rubber content, binder type, and pigment addition have been varied and evaluated. The results demonstrate that by increasing rubber content of the mixtures the head injury criterion (HIC) value and injury risk can be decreased while maintaining frictional properties according to existing criteria. Assembly of test-lanes demonstrate that some developed materials experience lower flow and component separation than standard materials due to rubber addition, calling for further optimisation of construction procedure linked to content development. Initial trials on the test-lanes indicate that a polyurethane (PU) based material has high cycling comfort, visibility and can be modified with phosphorescence properties. For standard asphalt, impact absorption might be inflicted by modification of bitumen alone but is mostly augmented by rubber addition. The results also indicate that rubber content can decrease ice formation on the materials...|$|R
40|$|We {{demonstrate}} {{the value in}} previously <b>disregarded</b> <b>parameters</b> in AIS data, and present a novel way of quickly identifying and characterizing potentially safety critical situations for vessels with a properly configured AIS transponder. The traditional approach of studying (near) collision situations, is through vessel conflict zones, based on vessel location and speed from low resolution AIS data. Our approach utilizes the rate of turn parameter in the AIS signal, at maximum time resolution. From collision investigation reports it is often seen that prior to or at collision navigators perform frenetic rudder actions in the hope to avoid collision in the last second. These hard maneuverings are easily spotted as non-normal rate of turn signals. An identified potential critical situation may then be further characterized by the occurring centripetal acceleration a vessel is exposed to. We {{demonstrate the}} novelty of our methodology in {{a case study of}} a real ship collision. As the rate of turn parameter is directly linkable to the navigator behavior it provides information about when and to what degree actions were taken. We believe our work will therefore inspire new research on safety and human factors as a risk profiles could be derived based on AIS data...|$|R
40|$|The {{purpose of}} the study is to make the T-T-T diagram {{available}} and to describe the X-ray diffraction techniques used to determine the fraction of transformed phase in the Ti- 15 Mo- 5 Zr- 3 Al metastable beta-alloy which decomposes into the hexagonal alpha-phase and the bcc beta-phase of a higher Mo concentration. The sheet material studied for this study is not ideal for an X-ray examination because of both a rolling and annealing texture as well as grain size problems. These difficulties are overcome using special X-ray techniques. Equations are given that indicate the approximations to be accepted if pole figure data are <b>disregarded.</b> Lattice <b>parameter</b> data from the beta phase enables one to obtain an approximate phase diagram...|$|R
3000|$|... th {{spectral}} band by the ij th photo-detector at the k th sample time. The terms a(i,j) and b(i,j) represent, respectively, all the multiplicative and additive noise sources corrupting {{the output of}} a PBHC. The additive term V(i,j,k) {{is known as the}} temporal white noise associated with the readout electronic for the ij th photo-detector. However, in most of the cameras the white noise V(i,j,k) is negligible compared to the term b(i,j), so it can be <b>disregarded.</b> Finally, the <b>parameter</b> r(λ [...]...|$|R
40|$|A {{key factor}} {{in the design of}} {{photovoltaic}} fields is their yearly productivity and normally the nominal power is the only module parameter taken into account in the productivity predictions. The present paper intends to demonstrate how this approach <b>disregards</b> an important <b>parameter,</b> the nominal fill factor, that may significantly affect module efficiency when working in realistic environmental conditions. A complete calculation method is presented to compute a module productivity in any operating condition, using data sheets information. As an example two commercial module characteristics with different fill factors are compared to illustrate the concept in quantitative terms...|$|R
40|$|Path finding {{solutions}} {{are becoming a}} major part of many GIS applications including location based services and web-based GIS services. Most traditional path finding {{solutions are}} based on shortest path algorithms that tend to minimize the cost of travel from one point to another. These algorithms make use of some cost criteria that is usually an attribute of the edges in the graph network. Providing one shortest path limits user’s flexibility when choosing a possible route, especially when more than one parameter is utilized to calculate cost (e. g., when length, number of traffic lights, and number of turns are used to calculate network cost.) K shortest path solutions tend to overcome this problem by providing second, third, and Kth shortest paths. These algorithms are efficient as long as the graphs edge weight does not change dynamically and no other parameters affect edge weights. In this paper we try to go beyond finding shortest paths based on some cost value, and provide all possible paths <b>disregarding</b> any <b>parameter</b> that may affect total cost. After finding all possible paths, we can rank the results by any parameter or combination of parameters, without a substantial increase in time complexity...|$|R
40|$|Classic {{inventory}} models use {{average cost}} functions. It is generally accepted that these models should {{account for the}} time value of money. They do so not by considering the timing of cash-flows, but by including opportunity costs. The Net Present Value (NPV) framework has long been used to compare these models with. We formalise NPV Equivalence Analysis (NPVEA) under various payment structures, {{and apply it to}} a few classic inventory models. While taking the linear approximation is typically part of the process to find equivalence, the essence is to <b>disregard</b> the <b>parameters</b> of a classic inventory model but instead start from cash-flow structures between firms. It is demonstrated how this leads to different plausible interpretations of, or variations to, classic inventory models, in particular for payment structures that differ from conventional assumptions. We identify situations with negative holding costs, which indicates that more features from the real world must be added into the decision model. We illustrate that in addition to capital costs, firms can enjoy capital rewards. These rewards may not always affect the firm's inventory decisions, but are in general useful for finding the impact of changes to various parameters on the firm's future profit...|$|R
40|$|The Shannon {{capacity}} of a graph G is the maximum asymptotic rate at which messages can be sent with zero probability of error through a noisy channel with confusability graph G. This extensively studied graph <b>parameter</b> <b>disregards</b> the fact that on atomic scales, Nature behaves in line with quantum mechanics. Entanglement, arguably the most counterintuitive feature of the theory, {{turns out to be}} a useful resource for communication across noisy channels. Recently, Leung, Mancinska, Matthews, Ozols and Roy [Comm. Math. Phys. 311, 2012] presented two examples of graphs whose Shannon capacity is strictly less than the capacity attainable if the sender and receiver have entangled quantum systems. Here we give new, possibly infinite, families of graphs for which the entangled capacity exceeds the Shannon capacity. Comment: 15 pages, 2 figure...|$|R
40|$|Biodiversity widely {{observed}} in ecological systems {{is attributed to}} the dynamical balance among the competing species. The time-varying populations of the interacting species are often captured rather well {{by a set of}} deterministic replicator equations in the evolutionary game theory. However, intrinsic fluctuations arisen from the discreteness of populations lead to stochastic derivations from the smooth evolution trajectories. The role of these fluctuations is shown to be critical at causing extinction and deteriorating the biodiversity of ecosystem. We use children's rock-paper-scissors game to demonstrate how the intrinsic fluctuations arise from the discrete populations and why the biodiversity of the ecosystem decays exponentially, <b>disregarding</b> the detail <b>parameters</b> for competing mechanism and initial distributions. The dissipative trend in biodiversity can be analogized to the gradual erosion of kinetic energy of a moving particle due to air drag or fluid viscosity. The dissipation-fluctuation theorem in statistical physics seals the fate of these originally conserved quantities. This concept in physics can be generalized to scrutinize the errors that might be incurred in the ecological, biological, and quantitative economic modeling for which the ingredients are all discrete in number. Comment: 15 pages, 4 figures and 1 tabl...|$|R
40|$|The {{assessment}} of robust CFD techniques is casting {{new light on}} the aerodynamics of airfoils rotating around an axis orthogonal to flow direction, with particular reference to flow curvature effects and stall mechanisms. In particular, Darrieus wind turbines' designers are taking profit from these new discovers to improve the aerodynamic design of the rotors, in view of an increase of the overall efficiency and a reduction of the structural stresses on the blades. A controversial design parameter for Darrieus turbines, especially in case of small-size rotors, is represented by the location of the blade-spoke connection along the chord. The most common solution is indeed to place the connection at approximately airfoil's quarter chord, i. e. where the pressure center is commonly located for low incidence angles. In some cases, however, the blade is connected at middle chord due to symmetry or aesthetic reasons. In some small turbines, innovative designs have even <b>disregarded</b> this <b>parameter.</b> Even if one can argue that the blade connection point is about to have some aerodynamic effects on the turbine's performance, the real impact of this important design parameter is often not fully understood. The present study makes use of extensive CFD simulations on a literature case study, using a NACA 0021 airfoil, to assess the influence of the blade-spoke connection point. In particular, the differences in terms of power coefficient curve of the turbine, optimal tip-speed ratio, torque profiles and stresses on the connection are analyzed and discussed. Detailed flow analyses are also shown for azimuthal positions of particular interest. Results on the selected case study showed that the middle-chord blade-spoke connection point seems to guarantee a higher performance of the rotor, even if additional solicitation is applied to the connection itself. It is further shown that the same performance can indeed be obtained with the airfoil attached at quarter chord and properly pitched. By doing so, the stresses are contained and the performance is maximized...|$|R
40|$|In my Bachelor Thesis I {{will deal}} with the {{evaluation}} of microclimatic conditions and artificial lighting in various rooms at two halls of residence. This evaluation concerns the following rooms: students' rooms, studies, halls, kitchens and bathrooms. Microclimate oriented measuring primarily includes ambient temperature, mean radiation temperature (globe temperature), air circulation rate and air humidity. I measured all these four parameters separately in each room. Upon {{the basis of these}} four main parameters and using a computer programme, I calculated other secondary microclimatic parameters which also participate in the overall microclimatic condition in a room and which must therefore by no means be <b>disregarded.</b> These particular <b>parameters</b> are stated below in the practical section as part of the individual evaluation of each measured room. The theoretical section of the Thesis also contains formulas which may be used for the calculation of the specific and exact value of a parameter which we may require by virtue of other data concerning or resulting from other microclimatic findings. In addition, I used the programme to calculate the recommended values (limits) of some of these parameters which should not be exceeded in any circumstances. In the event of their exceeding the set limits, the most [...] ...|$|R
40|$|We {{report the}} genetic {{analysis}} of 100 individuals {{of an elite}} breeding flock of Afshari sheep with a selected set of eighteen microsatellite markers. A full characterization of this set of eighteen loci was carried out generating allele frequency distributions {{that were used to}} estimate the genetic information content of these loci, including genetic variability, inbreeding, individual and parent verification <b>parameters.</b> <b>Disregarding</b> MCMA 26 monomorphic pattern, microsatellite loci showed moderate level of polymorphism, as such totally 102 alleles were detected with a mean number of 6 alleles per locus. The average expected heterozygosity was 0. 72 (SD = 0. 07) and the average Polymorphism Information Content (PIC) was 0. 67 (SD = 0. 08). Total value of inbreeding based on marker data was estimated as - 0. 02 so it indicates that inbreeding occurred less than would be expected at random. The overall probability of identity considering all twelve independent loci combined was 3. 148 E- 13 meaning lower than 1 in 31 trillions. The results of this study indicate, despite the selective breeding and closed flock system over a number of generations, a relatively high level of heterozygosity still exists in the representative sheep flock. The high degree of multiallelism and the clear and simple codominant Mendelian inheritance of the set of microsatellites used provide a powerful system for the unique identification of Afshari sheep individuals for fingerprinting purposes and parentage testing...|$|R
40|$|International audienceStrain {{localisation}} in continents is {{a general}} question tackled by specialists of various disciplines in Earth Sciences. Field geologists working at regional scale are able to describe the succession of events leading {{to the formation of}} large strain zones that accommodate large displacement within plate boundaries. On {{the other end of the}} spectrum, laboratory experiments provide numbers that quantitatively describe the rheology of rock material at the scale of a few mm and at deformation rates up to 8 - 10 orders of magnitude faster than in nature. Extrapolating from the scale of the experiment to the scale of the continental lithosphere is a considerable leap across 8 - 10 orders of magnitude both in space and time. It is however quite obvious that different processes are at work for each scale considered. At the scale of a grain aggregate diffusion within individual grains, dislocation or grain boundary sliding, depending on temperature and fluid conditions, are of primary importance. But at the scale of a mountain belt, a major detachment or a strike-slip shear zone that have accommodated tens or hundreds of kilometres of relative displacement, other parameters will take over such as structural softening and the heterogeneity of the crust inherited from past tectonic events that have juxtaposed rock units of very different compositions and induced a strong orientation of rocks. Once the deformation is localised along major shear zones, grain size reduction, interaction between rocks and fluids and metamorphic reactions and other small-scale processes tend to further localise the strain. Because the crust is colder and more lithologically complex this heterogeneity is likely much more prominent in the crust than in the mantle and then the relative importance of "small-scale" and "large-scale" parameters will be very different in the crust and in the mantle. Thus, depending upon the relative thickness of the crust and mantle in the deforming lithosphere, the role of each mechanism will have more or less important consequences on strain localisation. This complexity sometimes leads to <b>disregard</b> of experimental <b>parameters</b> in large-scale thermo-mechanical models and to use instead ad hoc "large-scale" numbers that better fit the observed geological history. The goal of the ERC RHEOLITH project is to associate to each tectonic process the relevant rheological parameters depending upon the scale considered, in an attempt to elaborate a generalized "Preliminary Rheology Model Set for Lithosphere" (PReMSL), which will cover the entire time and spatial scale range of deformation...|$|R
30|$|The {{easiest way}} to arrange {{aggregation}} would be to use contiguous component carriers within the same operating frequency band (as defined for LTE), the so called intra-band contiguous. However, in practice, such {{a large portion of}} continuous spectrum is rarely available. CA, where multiple CCs of smaller bandwidth are aggregated, is an attractive alternative to increase data rate. Additional advantages are offered by CA in terms of spectrum efficiency, deployment flexibility, backward compatibility and more. By aggregating non-contiguous carriers, fragmented spectrum can be more efficiently utilized [5]. For non-contiguous allocation, it could either be intra-band, i.e. the CCs belong to the same operating frequency band, but have a gap, or gaps, in between, or it could be inter-band, in which case the CCs belong to different operating frequency bands [6]. In [7, 8] and [9], the authors addressed {{the problem of how to}} optimize the resource allocation process in a multi-carrier system, while maintaining low complexity. Both simple theoretical and simulation results were obtained, which show that with low number of users and low percentage of LTE-A users, the load balancing method of round robin (RR) achieves better performance than the mobile hashing (MH) balancing. It was also found that using independent packet scheduling per CC suffers from poor coverage performance. In this context, the authors proposed a cross CC packet scheduler algorithm, which is a simple extension of the existing proportional fair scheduler. The cross CC algorithm is aware of the user throughput over all the aggregated CCs. As a result, it was shown that the cross CC algorithm maximizes the network utility even if users are provided with different number of CCs. This approach however only accounts for the cell throughput and <b>disregards</b> QoS <b>parameters,</b> e.g. delay and loss. Besides, full buffer traffic is addressed, which is not representative of nowadays and future cellular network traffic. In [10], a scheduling strategy for CA using pre-organized resource block (RB) sets was presented. An analytical evaluation framework was performed to determine the expected number of RBs required by users, based on a mapping of Channel Quality Indicator (CQI) values to data rates per RB and the statistical behaviour of the CQI. RBs can be grouped into sets based on the predefined maximum number of RBs and spectrum availability. By scheduling these RB sets, this scheme can help to reduce the scheduling delay. Nevertheless, this adds further scheduling complexity due to the RB pre-organization functionality. In this context, this work addresses LTE-A CA and proposes an updated integrated Common Radio Resource Management (iCRRM), from [11], that performs CC scheduling to satisfy the user’s quality of service (QoS) and experience (QoE) requirements while maximizing spectral efficiency. Two inter-band CA, band 7 (2.6 GHz) and band 20 (800 MHz), are considered. As stated above, large portions of continuous spectrum are rarely available and the aggregation of smaller bandwidths is an attractive solution to reduce spectrum under utilization. In 2011, the Portuguese telecommunications regulator, ANACOM, auctioned LTE bands 7 and 20. As a consequence, in Portugal, only 5 MHz (“lots”) can be made available for the considered CCs [12]. In South Korea, LG Uplus Corp, the third mobile carrier, created the LTE-A service by combining bandwidth of 40 MHz at 2.6 GHz, plus 20 -MHz bandwidths in the 800 -MHz and 2.1 -GHz bands [13]. Also in UK, Hong Kong and Singapore, late 2013 or in early 2014, EE, CSL or Sing Tel Mobile operators launched LTE-A carrier aggregation services, bringing together 20 MHz from 1, 800 -MHz spectrum band and 20 MHz from 2.6 -GHz spectrum band [14 - 16]. Since November 2014, Telekom Deutschland has been offering higher date rates by using the 2, 600 -MHz frequency band in combination with the 800 -MHz band, with 20 -MHz bandwidths [17]. Also, 5 -MHz and 20 -MHz bandwidth CCs are considered for this research while assuming inter-band carrier aggregation between the 800 -MHz and 2.6 -GHz frequency bands. Besides, following the forecast from [18], in this work, two different types of video traffic are addressed under the premise that in 2013 it represented more than half (53 %) of the total traffic and will reach 69 % of all worldwide mobile data traffic by 2018.|$|R
40|$|The {{comment by}} Nicholson (2011 a) {{questions}} the "consistency" of the "definition" of the "biological end-member" used by Kaiser (2011 a) in {{the calculation of}} oxygen gross production. "Biological end-member" refers to the relative oxygen isotope ratio difference between photosynthetic oxygen and Air-O 2 (abbreviated 17 dP and 18 dP for 17 O/ 16 O and 18 O/ 16 O, respectively). The comment claims that this leads to an overestimate of the discrepancy between previous studies and that the resulting gross production rates are " 30 % too high". Nicholson recognises the improved accuracy of Kaiser's direct calculation ("dual-delta") method compared to previous approximate approaches based on 17 O excess (17 ?) and its simplicity compared to previous iterative calculation methods. Although he correctly points out that differences in the normalised gross production rate (g) are largely due to different input parameters used in Kaiser's "base case" and previous studies, he does not acknowledge Kaiser's observation that iterative and dual-delta calculation methods give exactly the same g for the same input <b>parameters</b> (<b>disregarding</b> kinetic isotope fractionation during air-sea exchange). The comment is based on misunderstandings {{with respect to the}} "base case" 17 dP and 18 dP values. Since direct measurements of 17 dP and 18 dPdo not exist or have been lost, Kaiser constructed the "base case" {{in a way that was}} consistent and compatible with literature data. Nicholson showed that an alternative reconstruction of 17 dP gives g values closer to previous studies. However, unlike Nicholson, we refrain from interpreting either reconstruction as a benchmark for the accuracy of g. A number of publications over the last 12 months have tried to establish which of these two reconstructions is more accurate. Nicholson draws on recently revised measurements of the relative 17 O/ 16 O difference between VSMOW and Air-O 2 (17 dVSMOW; Barkan and Luz, 2011), together with new measurements of photosynthetic isotope fractionation, to support his comment. However, our own measurements disagree with these revised 17 dVSMOW values. If scaled for differences in 18 dVSMOW, they are actually in good agreement with the original data (Barkan and Luz, 2005) and support Kaiser's "base case" g values. The statement that Kaiser's g values are " 30 % too high" can therefore not be accepted, pending future work to reconcile different 17 dVSMOW measurements. Nicholson also suggests that approximated calculations of gross production should be performed with a triple isotope excess defined as 17 ?#= ln (1 + 17 d) –? ln(1 + 18 d), with ? = ?R = ln(1 + 17 ?R) / ln(1 + 18 ?R). However, this only improves the approximation for certain 17 dP and 18 dP values, for certain net to gross production ratios (f) and for certain ratios of gross production to gross Air-O 2 invasion (g). In other cases, the approximated calculation based on 17 ?† = 17 d – ? 18 d with ? = ?R = 17 ?R/ 18 ?R (Kaiser, 2011 a) gives more accurate results...|$|R

