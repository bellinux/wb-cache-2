57|10000|Public
30|$|This study {{clearly shows}} the {{potential}} of the ultra stable, regenerable SH-ePMO for removing mercury from aqueous solutions. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on optimisation of SH-ePMO to get the same performance of TP- 214 (high adsorption capacity[*]+[*]purification of water to ppt levels).|$|E
30|$|<b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on {{analysis}} of the optimal directional microphone beampattern and its influence, optimizing and adapting the temporal smoothing to the voice activity level, and combination with de-noising algorithms for integration in real-time quality monitoring systems with distributed microphone arrays.|$|E
3000|$|... both pose {{significant}} challenges, {{which go}} {{beyond the scope of}} this paper. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on these challenges by extending blind techniques for room reverberation time estimation [40]. Usage of the proposed Gaussian prior, which is also valid for rank- 1 mixing vectors, may also be explored in the context of FDICA, with the difficulty of translating this prior into a prior over the blocking vectors which are usually considered as parameters in this context instead.|$|E
30|$|From the {{simulation}} results, {{we find that}} the proposed classifier gives a good result compared with some works applied on Tennessee Eastman process. In addition, the proposed approach gives good results for each task in the process monitoring. In the case study, we have seen that some faults are difficult for detecting; our <b>future</b> <b>works</b> <b>will</b> <b>concentrate</b> {{on the development of the}} detection task. The developed reconfiguration agent realizes the reconfiguration tasks for known faults, and we will focus also on adding the reconfiguration plan in case when a new fault appear in the process.|$|R
40|$|Following a {{preliminary}} study of power generation processes consuming sugar-cane bagasse; this second round indicates {{the possibility of}} almost doubling the current efficiency presently obtained in conventional mills. A combined cycle uses highly pressurized fluidized bed boiler to provide steam above critical temperature to drive steam-turbine cycle while the flue-gas is injected into gas turbines. The present round also shows that gains over usual BIG/GT (Biomass In-tegrated Gasification/Gas Turbine) are very likely mainly due to the practicality of feeding the biomass as slurry that can be pumped into the pressurized boiler chamber. Such would avoid the cumbersome cascade feeding of the fibrous bio-mass, usually required by other processes. The present stage assumes slurry with 50 % added water. <b>Future</b> <b>works</b> <b>will</b> <b>concentrate</b> on thicker slurries, if those could be achieved. All studies apply a comprehensive simulator for boilers and gasifiers [CSFMB© or CeSFaMB™] and a process simulator (IPES) to predict the main features of the steam and gas tur-bine branches. © Souza-Santos and Chávez...|$|R
40|$|Nowadays, the {{improvement}} of communication technologies is widely applied to reduce energy use in cars. Several ecodriving application already appeared on the market. They consist in providing a feedback to the drivers describing their ecodriving behavior and they rely on embedded sensors signals (GPS speed and acceleration). However most of these applications {{does not take into}} account upcoming events such as curves, slopes or crossings to advise the driver on the best actions to undertake to lower energy consumption. Furthermore, they do not analyze data coming from vehicle sensors. In this paper, we present an application, developed within the FP 7 European project ecoDriver, that provides several innovative properties: advice according to upcoming events, a real time evaluation of the driving behavior, the analysis of past actions, an interface with OBD 2 connector, [...] . This paper further develops the complete architecture and links between each innovative function. <b>Future</b> <b>works</b> <b>will</b> <b>concentrate</b> on integrating image processing in this application in order to detect the possible presence of a front vehicle...|$|R
40|$|We {{developed}} and tested an inte-grated pest management (IPM) pro-gram {{for the key}} pests of cut roses, {{which was based on}} fixed precision sampling plans, thresholds, biological control, directed sprays of reduced-risk pesticides, and cultural control. This program represented the largest effort to date to implement an IPM program in U. S. floriculture. The bio-logical control of mites was successful at all locations, and pesticide use was generally lower in the IPM green-houses. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on reducing scouting time, improvin...|$|E
30|$|In this paper, we {{have studied}} the problem of top-k {{monitoring}} over distributed data streams in sliding window case. Based on two motivational observations, we have proposed a novel algorithm which reallocates numeric values of data objects among distributed monitoring nodes by assigning revision factors to deal with distributed top-k monitoring problem. We also have developed a hybrid framework and conducted our algorithm on top of Apache Storm. Furthermore, we have implemented two baseline algorithms and used two kinds of datasets to demonstrate the efficiency and scalability of our algorithm. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on monitoring other functions over distributed data streams.|$|E
40|$|This thesis {{presents}} {{the development of}} a Micro-Electro-Mechanical System (MEMS) monolithic in-plane tunable optical filter in both Indium Phosphide and Silicon. By placing one mirror of a waveguide-based Fabry-Perot interferometer on an electrostatically-actuated beam, a tunable filter is constructed. This is the first demonstration of a waveguide-coupled MEMS tunable optical filter in any material system. Filters with a tuning range of 40 nm from a wavelength 1550 nm with a linewidth of 35 nm are demonstrated. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on improving the filter's optical characteristics, limited by etch-induced facet roughness, and integration with active photonic devices...|$|E
40|$|The article {{presents}} the serviceplatform HeLv-Helpdesk for teachers for pupils with conduct disorder ([URL] HeLv offers {{easy access to}} its monitoring and coaching platform {{as well as a}} FAQ, contacts for networking with local institutions and a big offer of specialist literature. The focus of the article is on the genese of the service, actual developments and <b>future</b> <b>work</b> which <b>will</b> <b>concentrate</b> on better dissemination and awareness for the support being given by HeLv...|$|R
40|$|I am {{exploring}} {{an approach}} to developing services with multiple user interfaces based on a high level description of the service. The description is made using interaction acts, which are primitives for describing user-service interaction in a device independent way. Device adapted user interfaces are generated based on interaction acts in combination with device and service specific presentation information. As a proof of concept, the approach is implemented in a working prototype that handles graphical user interfaces, web user interfaces, and speech user interfaces for our sample services. <b>Future</b> <b>work</b> <b>will</b> mainly <b>concentrate</b> on evaluation of the generated user interfaces. 1...|$|R
40|$|Since many years, as a {{consequence}} of fossil fuels rarefaction and climate changes, ecology has become a major challenge of our society. In this field, many improvements could be realized on the transportation side and more precisely on passenger cars which are an important source of pollution. A quick and efficient solution to reduce fuel consumption and so, greenhouse gases emissions, is to adopt an ecological way of driving, called ecodriving. However, is ecodriving really efficient in terms of mobility and environment at a global point of view? The benefits of ecodriving have often been studied for an isolated vehicle and rarely for a whole network. The aim of this work is to estimate the effects of ecodriving on traffic congestion and fuel consumption according to the percentage of ecodrivers in the population. This has been achieved using a class of ecodriven vehicles with a car-following model (Intelligent Driver Model) and with a transport simulation software (Aimsun). Results show that the effect of ecodriving on the traffic congestion and pollution is not linearly linked to the proportion of ecodrivers and this effect varies according to the driving conditions. In some cases, ecodriving is cons-productive and fuel consumption increases. <b>Future</b> <b>works</b> <b>will</b> <b>concentrate</b> on experimental validation, on modeling the effect of ecodriving on road safety and on improving the different models...|$|R
40|$|This paper {{discusses}} {{the design of}} a control system. The controller is validated on 1 axis of an industrial hydraulic test rig which is used in the automotive industry for vibration comfort evaluations of new vehicle prototypes. The designed controller is a SISO (Single Input Single Output) feedback controller and is used in combination with an industrial available feed-forward controller. The experimental results show that the total controller (feed-forward + feedback) reduces the tracking error compared to the feed-forward controller only. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on the extension towards the MIMO (Multiple Input Multiple Output) case. 1...|$|E
40|$|Abstract—In {{this paper}} we build upon {{previous}} work {{to examine the}} efficacy of blending probabilities in asset-specific classifiers to improve diagnostic accuracy for a fleet of assets. In previous work we also introduced {{the idea of using}} split probabilities. We add environmental differentiation to asset differentiation in the experiments and assume that data is acquired in the context of online health monitoring. We hypothesize that overall diagnostic accuracy will be increased with the blending approach relative to the single aggregate classifier or split probability assetspecific classifiers. The hypothesis is largely supported by the results. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on improving the blending mechanism and working with small data sets...|$|E
40|$|The aim of {{this study}} is to develop a {{multi-level}} resilience analysis method (RAM) to assess risk and performance variability in current maritime socio-technical systems (STSs). The method integrates Hollnagel’s four resilience abilities to assess a system’s ability to effectively cope with disturbances and changes within the system. The RAM builds upon and extends existing methods. The method defines various system components of the STS on different hierarchical levels including control and feedback loops. It im-proves understanding of performance variability and associated strategies of all relevant users within the STS. A questionnaire assesses performance on the four resilience abilities. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on validat-ing the questionnaire and making the method applicable for prospective and design analysis as well. The method will be part of the future SEAHORSE virtual platform for assessing and improving resilience...|$|E
40|$|Department’s {{public library}} {{research}} programme has developed rapidly since 1978. Projects have included state-of-the-art reviews, preliminary or enabling studies, {{the evaluation of}} experimental services, user studies, descriptive and analytical studies and operational research projects. In addition, public library projects {{have been carried out}} by the research centres supported by the Department. <b>Future</b> <b>work</b> <b>will</b> probably <b>concentrate</b> more on the dissemination and promotion of research results. DEVELOPMENT OF THE PROGRAMME The British Library Research and Development Department’s public library research programme originated in a working party of the Library Advisory Council for England. The working party looked at the management of public librariesi and in doing so specified a number of areas where research was though...|$|R
40|$|AbstractEcodriving {{is known}} as a way to quickly and {{efficiently}} reduce fuel consumption for the concerned ecodriven vehicle. However, the impact of ecodriving on a whole road network at a large scale is unknown. In order to perform studies in a micro traffic simulation software, a fuel consumption model coupled to a gear behavior model are required. This study presents a gear shifting behavior model able to represent as well the variability of drivers as the difference between ecodriving and normal driving. This work, based on the evaluation of the real driver behaviors during 42 trips, has been partially validated with a result of 60 % of time spend in the correct gear. <b>Future</b> <b>works</b> <b>will</b> be <b>concentrated</b> on a detailed validation of this model and on its implementation with a fuel consumption model...|$|R
40|$|As neutron {{simulations}} {{packages are}} used for analysis of the expected performance for practically all newly built neutron instruments, possibilities for simulations with polarized neutrons have been relatively underdeveloped. During the last years we developed a new approach for the representation of time-dependent magnetic fields (both in magnitude and direction) for the VITESS simulation package. This allowed us to simulate the neutron spin dynamics in practically all polarized neutron devices (RF neutron flipper, adiabatic gradient RF flipper, the Drabkin resonator, etc.). In this article the above-mentioned VITESS instrument components (modules) will be presented and the simulated performance {{of a number of}} polarized neutron scattering instruments (NRSE, MIEZE, SESANS, etc.) will be demonstrated. Thus, we practically complete the polarized neutron suite of the VITESS, which seems sufficient for the simulation of performance of any existing polarized neutron scattering instrument. <b>Future</b> <b>work</b> <b>will</b> be <b>concentrated</b> on developments of dedicated sample modules (kernels) to allow for virtual experiments with VITESS. (C) 2010 Elsevier B. V. All rights reserved...|$|R
40|$|The {{administrative}} sources of data save time, money and provide valuable statistical data for different uses. This paper overviews {{the use of}} {{administrative sources}} for statistical purposes in Jordan. The use of administrative sources is utilized currently in different statistical fields. The use of administrative sources started in Environmental statistics and economical accounts with the cooperation with governmental bodies regarding these two targets. The administrative sources increased with time to improve the statistical performance in other fields such as foreign trading in cooperation with Jordanian Customs Directorate. These administrative sources are considered partners in these fields. Moreover, the civil registration records were used for statistical purposes. The <b>future</b> <b>work</b> <b>will</b> <b>concentrate</b> on restricting administrative sources national wise {{that can be used}} for statistical purposes to build national strategy for the use of these sources...|$|E
30|$|Finally, we {{acknowledge}} that this review cannot be {{claimed to be}} exhaustive, but it does provide a reasonable insight into the state-of-the-art on FCO research. Thus, {{it is hoped that}} this review will provide a source of reference for other researchers/readers interested toward FCO research and help stimulate further interest. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on the development of an appropriate information framework for FCO research in air transport. After that, this informational framework should be checked for reliability and validity. This leads {{to the development of a}} structural model of fuel consumption in the air transport industry and further knowing the relationships among the variables an optimization model will be constructed. Furthermore, this study will also provide the base for fuel conservation, energy efficiency, and emission reduction (As CO 2 emission are proportional to aircraft fuel burn) in the aviation sector.|$|E
40|$|Abstract — Previously, we {{demonstrated}} the potential value of constructing asset-specific models for fault diagnosis. We also {{examined the effects}} of using split probabilities where prior probabilities come from asset-specific statistics and likelihoods from fleet-wide statistics. In this paper, we build upon that work to examine the efficacy of smoothing probability distributions between asset-specific and fleet-wide distributions to improve diagnostic accuracy further. In the current experiments, we also add environmental differentiation to asset differentiation under an assumption that data is acquired in the context of online health monitoring. We hypothesize that overall diagnostic accuracy will be increased with the smoothing approach relative to a fleet-wide model or a set of asset-specific models. The hypothesis is largely supported by the results. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on improving the smoothing mechanism and in the context of small data sets. Index Terms—Diagnosis (fault), machine learning, Bayesian classifier, smoothing...|$|E
40|$|The normal prompt gamma-ray neutron {{activation}} analysis for either bulk or small beam samples inherently has a small signal-to-noise (S/N) ratio due primarily to the neutron source being present while the sample signal is being obtained. Coincidence counting offers the possibility of greatly reducing or eliminating the noise generated by the neutron source. The present report presents our results to date on implementing the coincidence counting PGNAA approach. We conclude that coincidence PGNAA yields: (1) a larger signal-to-noise (S/N) ratio, (2) more information (and therefore better accuracy) from essentially the same experiment when sophisticated coincidence electronics are used that can yield singles and coincidences simultaneously, and (3) a reduced (one or two orders of magnitude) signal from essentially the same experiment. In <b>future</b> <b>work</b> we <b>will</b> <b>concentrate</b> on: (1) modifying the existing CEARPGS Monte Carlo code to incorporate coincidence counting, (2) obtaining coincidence schemes for 18 or 20 of the common elements in coal and cement, and (3) optimizing {{the design of a}} PGNAA coincidence system for the bulk analysis of coal...|$|R
40|$|The {{hybrid rocket}} is reexamined {{in light of}} several {{important}} unanswered questions regarding its performance. The well-known heat transfer limited burning rate equation is quoted, and its limitations are pointed out. Several inconsistencies in the burning rate determination through fuel depolymerization are explicitly discussed. The resolution appears to be through the postulate of (surface) oxidative degradation of the fuel. Experiments are initiated to study the fuel degradation in mixtures of nitrogen/oxygen in the 99. 9 percent/ 0. 1 percent to 98 percent/ 2 percent range. The overall hybrid combustion behavior is studied in a 2 in-diameter rocket motor, where a PMMA tube is used as the fuel. The results include detailed, real-time infrared video images of the combustion zone. Space- and time-averaged images give a broad indication of the temperature reached in the gases. A brief outline is shown of <b>future</b> <b>work,</b> which <b>will</b> specifically <b>concentrate</b> on {{the exploration of the}} role of the oxidizer transport to the fuel surface, and the role of the unburned fuel that is reported to escape below the classical time-averaged boundary layer flame...|$|R
40|$|Abstract: The {{principle}} propose {{is to give}} powerful asset necessity expectation {{system for}} the input guided employment displaying instrument taking into account reproduction recognition. It additionally incorporates the proposed calculation for expectation in heterogeneous and various environments. We can develop our work via completing these investigations in a bigger domain {{and with all the}} more genuine codes including high calculations. Our <b>future</b> <b>work</b> <b>will</b> likewise <b>concentrate</b> on amplifying our proposed procedure and its usage to incorporate other programming dialects. Another critical angle is to actualize occupation booking taking into account this forecast calculation. The asset supplier is characterized as an operator that controls the asset. Work is allotted onto the most suitable asset supplier as per its necessities. Thus, asset prerequisite portrayal for a vocation is a vital assignment of asset administration. The runtime conduct of an occupation is for the most part not known previously. Along these lines, asset necessity determination characterized by the clients may prompt over-estimation or under-estimation of obliged assets for occupations. While underestimation frustrates execution of occupation, over-estimation prompts wastage of profitable assets. So it would be extremely helpful if the asset necessity estimation can be computerized in the lattice itself. For this, different approaches are developed and distinctive forecast modules are these days utilized {{as a part of the}} Resource Management Systems (RMS) of a lattice. Proficient asset necessity forecast arrangements additionally help to accomplish better nature of administration, due date planning, effective plate use and so forth. So looks into have been going ahead to devise exact asset prerequisite forecast strategies over the globe and it is a running issue in framework figuring today. [1] Keywords [...] Replica, RMS-resource management system, scheduling, automation. I...|$|R
40|$|OpenStreetMap (OSM) is {{a bottom}} up community-driven {{initiative}} {{to create a}} global map of the world. Yet the application of OSM to land use and land cover (LULC) mapping is still largely unexploited due to problems with inconsistencies in the data and harmonization of LULC nomenclatures with OSM. This chapter outlines an automated methodology for creating LULC maps using the nomenclature of two European LULC products: the Urban Atlas (UA) and CORINE Land Cover (CLC). The method is applied to two regions in London and Paris. The results show that LULC maps {{with a level of}} detail similar to UA can be obtained for the urban regions, but that OSM has limitations for conversion into the more detailed non-urban classes of the CLC nomenclature. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on developing additional rules to improve the accuracy of the transformation and building an online system for processing the data...|$|E
40|$|Abstract—Previously, we {{demonstrated}} the potential value of constructing asset-specific models for fault diagnosis. We also {{examined the effects}} of using split probabilities, where prior probabilities come from asset-specific statistics and likelihoods from fleet-wide statistics. In this paper, we build upon that work to examine the efficacy of smoothing probability distributions between asset-specific and fleet-wide distributions to further improve diagnostic accuracy. In the current experiments, we also add environmental differentiation to asset differentiation under the assumption that data are acquired in the context of online health monitoring. We hypothesize that the overall diagnostic accuracy will be increased with the smoothing approach relative to a fleet-wide model or a set of asset-specific models. The hypothesis is largely supported by the results. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on improving the smoothing mechanism and in the context of small data sets. Index Terms—Bayesian classifier, diagnosis (fault), machine learning, smoothing. I...|$|E
40|$|The {{molecular}} basis of gastrulation is poorly understood. In this {{paper we address}} this problem {{by taking advantage of}} the observation that the transcription activator Brachyury is essential for gastrulation movements in Xenopus and mouse embryos. We infer from this observation that amongst the target genes of Brachyury are some that are involved in the regulation of gastrulation. In the course of a screen for Brachyury targets we identified Xwnt 11. Use of a dominant-negative Xwntll construct confirms that signalling by this class of Wnts is essential for normal gastrulation movements, and further investigation suggests that Xwntll signals not through the canonical Wnt signalling pathway involving GSK- 3 and beta-catenin but through another route, which may require small GTPases such as Rho and Rac. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on elucidating the Xwnt 11 signal transduction pathway and on investigating its influence on cell shape and polarity during Xenopus gastrulation...|$|E
40|$|Measurements of {{geometric}} primitives are often noisy in real applications {{and we need}} to use statistics either to reduce the uncertainty (estimation), to compare measurements, or to test hypotheses. Unfortunately, {{geometric primitives}} often belong to manifolds that are not vector spaces. In previous works, we used invariance requirements to develop some basic probability tools on transformation groups and homogeneous manifolds that avoids paradoxes. In this paper, we consider the Riemannian metric as the basic structure for the manifold. Based on this metric, we develop the notions of mean value and covariance matrix of a random element, normal law, Mahalanobis distance and ^ 2 test. We provide a simple (but highly non trivial) characterization of Karcher means and an original gradient descent algorithm to efficiently compute them. The notion of Normal law we propose is based on the the minimization of the information knowing the mean and covariance of the distribution. The resulting family of pdfs spans the whole range from uniform (on compact manifolds) to the point mass distribution. Moreover, we were able to provide tractable approximations (with their limits) for small variances which show that we can effectively implement and work with these definitions. To come back to more practical cases, we then reconsider the case of connected Lie groups and homogeneous manifolds. In our Riemannian context, we investigate the use of invariance principles to choose the metric: we show that it can provide the stability of our statistical definitions w. r. t. geometric operations (composition, inversion and action of transformations). However, an invariant metric does not always exists for homogeneous manifolds, nor does a left and right invariant metric for non-compact Lie groups. In this case, we cannot guaranty the full consistency of geometric and statistical operations. Thus, <b>future</b> <b>work</b> <b>will</b> have to <b>concentrate</b> on constraints weaker than invariance...|$|R
40|$|The {{ultimate}} goal of speech synthesis {{is to build a}} system that could convert arbitrary written messages into intelligible and natural sounding speech. Such a system should also run on hardware platforms that we meet in everyday's life like a personal computer. The solutions that appeared in the last five decades can be divided into three different generations. Unfortunately, even the latest systems from the third generation are far from generating perfectly natural sounding speech. Currently, the best quality of the synthetic speech is obtained from the systems that belong to the group of Unit Selection Synthesis Systems. To build an adequate database of speech units a lot of work from trained engineers is required. The main objective of this Ph. D. thesis was to develop a system that could learn how to produce a high quality synthetic speech from the text and corresponding speech samples only, without requirements for skilled human labor or trained ASR (Automatic Speech Recognition) systems. The system should use statistical, machine learning techniques instead and algorithms for the automatic speech segmentation that do not require ASR. For the purposes of the thesis a prototype of the speech synthesis system named Learn to Speak by Yourself (LSY) was constructed. LSY belongs to the group of Unit Selection Synthesis Systems. The core of the LSY is made of the newly developed algorithm for the automatic speech segmentation that does not require the usage of an ASR system. The algorithm exploits the spectral differences between different phonemes (allophones) of a language. This approach is particularly useful for the Slovene or some other language with {{a relatively small number of}} speakers where it is more difficult to find skilled engineers or well trained ASR systems for the speech database construction. The system can start from scratch – i. e. no speech unit database is required. The database is automatically built during learning process. For generation of the speech samples the LSY uses a sinusoidal generator. The statistical results obtained from the listening tests show that synthetic speech produced by the generator in a synthesis by analysis process cannot be distinguished from a natural human speech. We may conclude that in theory a perfectly natural sounding synthetic speech can be produced by LSY. At this time the speech produced by a prototype version of the LSY is highly intelligible but not yet natural sounding. The main reason is the fact that only a few minutes of speech samples were fed to the prototype system while research results found in the literature recommend at least one hour of speech samples and even systems with five hours or more of speech samples are not uncommon. The <b>future</b> <b>work</b> <b>will</b> be <b>concentrated</b> on methods for the automatic extraction of prosody parameters from the speech samples. We would also like to improve the algorithm for the automatic speech segmentatio...|$|R
40|$|Proteins are {{composed}} of structural units, called domains, which can be detected using computational methods. In many families, proteins contain several domains, or several conserved sequence parts, called motifs. During the evolution, rearrangements (swaps or circular permutations) may alter the relative order of these units (domains or motifs) [Bornberg et al. 05], while tandem duplications can change their number. In a family, it results in a variable organization of the units along the chain. Profile HMMs (pHMMs) are the preferred models to represent motifs or domains. They serve to recognize new members of a protein family. Profile HMMs have a linear structure which can model a single unit, and if iterated they can detect repeats of this unit. To perform a "multiple tagging", that is to identify the sequence regions corresponding to each possible motif, one needs to combine the results of each pHMM. This was done following heuristic criteria (e. g., choosing the best scoring motif when the detections overlap). Up to now, we lack a method to perform a multiple tagging automatically and optimally according to a global criterion. To solve this problem, we generalize pHMMs to a novel structure called Cyclic Profile HMMs (CpHMMs), which can predict the most probable motif organization of a protein. It can cope with repeated units whose number varies, and with changes in the relative order of the units. We used CpHMMs to tag multiple motifs in the Pentatrico-Peptide Repeats (PPR) protein family of plants. PPR proteins contain tandem repeats of PPR motifs (named P, L, S) {{as well as other}} motifs (E, E+, Dyw). Half of the PPR proteins form the PPRP subfamily, while the other half is represented by the PCMP subfamily. PPRPs can be defined using a PROSITE like regular expression where letters represent motifs: (P*-S*) *, and the organization of PCMPs is described by: (P-L-S*) *-[E-[E+ -[Dyw]]] [Lurin et al. 04]. We used the motifs pHMMs as input to build a CpHMM that can process the entire motif sequence. It allows to obtain the globally optimal "multiple tagging" solution and also measures its statistical significance. Thus, we obtained an automatic, "multiple tagging" tool and applied it first to the Arabidopsis PCMP subfamily. We validated our results compared to the manual motif annotation available for the PCMPs of Arabidopsis and found less than 10 % discordance. We were able to retrieve the expected PCMP classes distribution [Rivals et al. 06]. After this validation, we used our tool to annotate the PPRP subfamily in Arabidopsis and to perform the first motif annotation of the complete PPR family of rice (522 proteins). This allows to compare the PPR motif organization between the model plants of monocotyledons and dicotyledons. In conclusion, we prove the capacity of the CpHMM structure {{to solve the problem of}} "multiple tagging" and verified its efficiency for a large number of sequences. CpHMMs are a versatile structure that adapts to new situations, like the PPR family of other species, or even to other protein families with complex unit organization (eg leucine rich repeats). In <b>future</b> <b>work,</b> we <b>will</b> <b>concentrate</b> on the fine identification of new subfamilies...|$|R
30|$|From Table  6 and Fig.  12, it {{is evident}} that the TTSD takes less {{computation}} time than NAFSWM, TBLI, ASM, and AFIDM and more computation time than ROAD, ROLD, and CBD. We also see that the computation time increases with the increase in the noise density in all methods. However, the rate of increase in computation time for TTSD is lower than the existing methods. To avoid miss detection, TTSD uses three levels of thresholds along with an auxiliary condition which involve various calculations. Therefore, the PSNR and SSIM values of TTSD are better than those of the existing methods and the computation time taken is higher than ROAD, ROLD, and CBD. This again demonstrates the effectiveness of the TTSD algorithm as against the existing methods. Our <b>future</b> <b>work</b> <b>will</b> <b>concentrate</b> on further reducing the computation time as well as reduction of the miss detection above 80 % RVIN. This can be achieved by use of parallel processing architect in place of series processing [31, 32].|$|E
40|$|A province-wide {{evaluation}} of the amount, distribution, source and control of damage to Ontario apple trees was initiated. Data we re obtained from 280 responses to a questionnaire distributed to growers across Ontario, and from trap censuses in three widely separated areas. Meadow voles (Microtus pennsylvanicus) damaged or destroyed 8, 423 trees in our questionnaire sample and other mammals damaged another 10, 307 trees. No relationship was found between application of rodenticide and levels of damage, but rodenticide-treated baits sharply reduced numbers of voles on our trap plots. Orchards {{with high levels of}} damage were on average only one-third as large as the average orchard in our total sample. All damage by voles appeared to be caused by meadow voles and no pine voles (M. pinetorum) were found. Numbers of voles varied greatly among our three study areas. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on factors causing high levels of damage and on the relationship between dispersal of voles and the effects of rodenticide treatment...|$|E
40|$|Increasing {{interest}} is being {{shown in the}} use of Java for large scale or Grande applications. This new use of Java places specific demands on the Java execution environments that could be tested and compared using a standard benchmark suite. EPCC has taken {{a leading role in the}} Java Grande Forum work to develop a framework and methodology for such a suite. Initial results presented here show interesting differences between JVMs, demonstrating the validity of the approach. <b>Future</b> <b>work</b> <b>will</b> <b>concentrate</b> on parallel benchmarks. 1 Introduction With the increasing ubiquity of Java comes a growing range of uses for the language that fall well outside its original design specifications. The use of Java for large scale applications with large memory, network or computational requirements, so called Grande applications, represent a clear example of this trend. Despite concerns about performance and numerical definitions an increasing number of users are taking seriously the possibility of usin [...] ...|$|E
40|$|Three-dimensional (3 D) {{reconstruction}} and characterization of external anatomical structures from images {{has been one}} of the major topics in Computer Vision. 3 D models of human external structures are normally built using 3 D scanners. Although frequently expensive, they are usually easy to use and can provide 3 D models of great accuracy. Recently, volumetric methods have been successfully used in 3 D reconstruction of objects with complex shapes. Comparing with stereo-based methods, they are more efficient in building 3 D models of smooth objects. They work in the object volumetric space and do not require a matching process between the images used, which is usually very complex with smooth objects. The work presented here is based on the Generalized Voxel Coloring (GVC) method. GVC does not impose any restriction on the object¿s shape or in the camera¿s displacement. Having as starting point a set of correctly calibrated images, GVC reconstructs the 3 D shape of the desired object and colorizes the reconstructed 3 D model¿s surface. First, in GVC, a 3 D volume of voxels surrounding the object to be built is defined. During the reconstruction process, inconsistent voxels are removed (carved). Consistency of a voxel is determined by analyzing the color standard deviation of the pixels that the same reproject in the images used. Finally, with the 3 D volumetric model built, it is possible to get a polygonal approach of the object¿s surface, using, for example, the Marching Cubes algorithm. In this work, a simple fixed off-the-shelf CCD camera is used. To calibrate it, a planar chessboard pattern is placed in front of the same in different orientations. On this first image sequence, Zhang¿s method is applied to obtain the camera¿s intrinsic parameters and radial and tangential distortion coefficients. After that, the object to be reconstructed is placed on a simple turntable device and under it the calibration pattern is positioned. Using again Zhang¿s method, this configuration allows us to obtain the camera¿s extrinsic parameters. During the reconstruction process, the intrinsic camera¿s parameters do not change. Background/object segmentation is performed on all images of the second image sequence, using usual image processing algorithms, like image binarization by threshold value. Having as inputs the second image sequence, the binary images associated and the calibration parameters, GVC is applied to obtain the 3 D model for the object to be reconstructed. Finally, the volumetric model obtained is polygonized and smoothed. Two objects were experimentally used: a hand and a human torso. The camera¿s calibration parameters were obtained with good precision and the 3 D reconstructions built were quite satisfactory as well. Some characteristic measurements were obtained from the models built: volume, centroid, extreme points, etc. With this work, we can conclude that the building of an accurate 3 D model of an external anatomical structure from images is difficult and complex. It was also verified that the errors obtained in the camera¿s calibration process have a strong influence on the considered reconstruction process. Thus, the <b>future</b> <b>work</b> <b>will</b> be <b>concentrated</b> in the improvement of the camera¿s calibration method used, as well as in the development of adequate photo-consistency criteria for objects with almost uniform colors on their surfaces, as is the case of human anatomical structures, in order to enhance the 3 D models obtained...|$|R
40|$|Precipitation water {{traveling}} through a catchment takes faster and slower flow paths {{to reach the}} outlet. The knowledge about the distribution of relevant flow paths in a catchment and their respective transit times of water is important when considering that water is the main transportation agent for pollutants and that anthropogenic impacts to natural systems can alter the hydrology dramatically, thus endangering water resources. However, the exact processes governing water transport through a catchment are unknown, as no measurement technology exists to capture them in situ. Tracers such as the stable isotopes of water ( 18 O and  2 H) are used to model these transport processes. The Transit Time Distribution (TTD) is a model estimate that integrates different flow paths of precipitation water through a catchment to the outlet. Due to different sources of uncertainties, e. g., the model structure, the estimates of TTDs are inherently uncertain. The conclusions of present day studies that want to elucidate the hydrological behavior of catchments, compare catchments or predict the hydrology of ungauged catchments from TTDs inherently suffer from these uncertainties. The {{aim of this study}} was to investigate spatiotemporal influences on the uncertainty of TTDs with the overall goal to ensure better estimates of TTDs. A simple, conceptual model was applied to two humid, small to medium scale catchments to investigate three hypothesis: that (1) heterogeneities of TTDs of a small catchment stem from different soil types, (2) canopy-induced changes in the tracer signal of stable isotopes of water due to interception will influence TTD estimates, and (3) a higher temporal resolution of tracer data will lead to differences in TTDs. The obtained results indicate that the soil types can indeed explain the spatial patterns of TTDs in a small scale catchment and could be used to limit uncertainty in e. g., ungauged catchments. When calculating TTD for forested catchments, interception must be considered, as it decreases the uncertainty of TTD estimates. Furthermore, a higher temporal resolution of tracer data led to drastically different estimates of TTDs, indicating that the usually applied weekly data is not enough to understand faster flow paths through a catchment. Thus, this study is a step forward in decreasing uncertainties in TTD estimates by considering canopy interception and arguing for higher resolution tracer data. <b>Future</b> <b>work</b> <b>will</b> have to <b>concentrate</b> on automatization of high-resolution measurements of tracer data to establish the data basis needed for less uncertain TTD estimates...|$|R
30|$|<b>Future</b> <b>work</b> <b>will</b> {{focus on}} {{refining}} RAP {{in a number}} of ways. Currently, RAP avoids over provisioning resources by allocating them in increments of one. <b>Future</b> <b>work</b> <b>will</b> address more explicit ways to specify resource constraints, e.g., enforcing a budget based on the costs for different types of resource flavours. We will also incorporate constraints that minimize the number of resource instance migrations across applications. In this paper, we do not address the objective of balancing the number of resources among the applications for the sake of fairness [44, 45]. Objective functions based on fairness measures will be investigated in <b>future</b> <b>work.</b> We <b>will</b> also modify RAP to accommodate multiple resource flavours for a given tier.Finally, <b>future</b> <b>work</b> <b>will</b> explore experimental validation using benchmark multi-tier applications.|$|R
