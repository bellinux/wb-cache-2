64|10000|Public
3000|$|There {{has been}} little prior work towards <b>emotion</b> <b>recognition</b> <b>using</b> both audio and visual cues in {{multimedia}} contents [16], [...]...|$|E
40|$|This paper {{introduces}} a first approach to <b>emotion</b> <b>recognition</b> <b>using</b> RAMSES, the UPC’s speech recognition system. The approach {{is based on}} standard speech recognition technology using hidden semi-continuous Markov models. Both the selection of low level features and {{the design of the}} recognition system are addressed. Results are given on speaker dependent <b>emotion</b> <b>recognition</b> <b>using</b> the Spanish corpus of INTERFACE Emotional Speech Synthesis Database. The accuracy recognising seven different emotions—the six ones defined in MPEG- 4 plus neutral style—exceeds 80 % using the best combination of low level features and HMM structure. This result is very similar to that obtained with the same database in subjective evaluation by human judges. 1...|$|E
40|$|This paper {{presents}} {{classification accuracy}} of neural network with {{principal component analysis}} (PCA) for feature selections in <b>emotion</b> <b>recognition</b> <b>using</b> facial expressions. Dimensionality reduction of a feature set is a common preprocessing step used for pattern recognition and classification applications. PCA {{is one of the}} popular methods used, and can be shown to be optimal using different optimality criteria. Experiment results, in which we achieved a recognition rate of approximately 85 % when testing six emotions on benchmark image data set, show that neural networks with PCA is effective in <b>emotion</b> <b>recognition</b> <b>using</b> facial expressions. Keywords: emotion recognition, feature selection, neural network, PCA. 2000 MSC: 68 T 45, 97 P 20, 97 R 40, 68 T 45...|$|E
40|$|The aim of {{this study}} is to apply a {{state-of-the-art}} speech <b>emotion</b> <b>recognition</b> engine on the detection of microsleep endangered sleepiness states. Current approaches in speech <b>emotion</b> <b>recognition</b> <b>use</b> low-level descriptors and functionals to compute brute-force feature sets. This paper describes a further enrichment of the temporal information, aggregating functionals and utilizing a broad pool of diverse elementary statistics and spectral descriptors. The resulting 45, 088 features were applied to speech samples gained from a car simulator based sleep deprivation study. After a correlation-filter based feature subset selection, which was employed on the feature space in an attempt to maximize relevance, several classification models were trained. The best model (Support Vector Machine, dot kernel) achieved 86. 1 % recognition rate in predicting microsleep endangered sleepiness stages 1...|$|R
40|$|The {{bio-inspired}} computation algorithms are {{the excellent}} tools for global optimization. These algorithms are {{very easy to}} understand and simple to implement. These algorithms especially dominate the classical optimization methods. Particle Swarm Optimization (PSO) is a global optimization algorithm that originally took its inspiration from the biological examples by swarming, flocking and herding phenomena in vertebrates. This paper presents facial <b>emotion</b> <b>recognition</b> system <b>using</b> PSO for different video clips with different cameras, at different distances and light intensity changes. The analysis is done for recognizing “Happy” emotion from other emotions. The comparison of facial <b>emotion</b> <b>recognition</b> system <b>using</b> other techniques and PSO is done. It is found that PSO has better results over other techniques in terms of accuracy and time required...|$|R
30|$|To fairly verify each {{recognition}} module and {{to simulate}} bimodal and multimodal <b>emotion</b> <b>recognition,</b> we <b>used</b> four kinds of emotions or musical moods in each experiment: neutral, happy, angry, and sad. We chose 'angry' and 'sad' {{as the most}} representative negative emotions, whereas 'neutral' and 'happy' were chosen as non-negative emotions.|$|R
40|$|The {{proceedings}} contain 142 papers. The topics discussed include: {{evaluating the}} consequences of affective feedback in intelligent tutoring systems; EEG-based <b>emotion</b> <b>recognition</b> <b>using</b> hybrid filtering and higher order crossings; multimodal real-time conversation analysis using a novel process engine; emotion detection in dialog systems: applications, strategies and challenges; affect sensing in speech: Studying fusion of linguistic and acoustic features; effects of emotional agents on human players in the public goods game; study of consumer&# 039;s emotion during product interviews; SentiFul: generating a reliable lexicon for sentiment analysis; EmoText: applying differentiated semantic analysis in lexical affect sensing; GraphLaugh: a tool for the interactive generation of humorous puns; stress and <b>emotion</b> <b>recognition</b> <b>using</b> log-Gabor filter analysis of speech spectrograms; and fundamental issues on the recognition of autonomic patterns produced by visual stimuli...|$|E
40|$|This paper {{discusses}} {{the application of}} feature extraction of facial expressions with combination of neural network for the recognition of different facial emotions (happy, sad, angry, fear, surprised, neutral etc [...] ). Humans are capable of producing thousands of facial actions during communication that vary in complexity, intensity, and meaning. This paper analyses the limitations with existing system <b>Emotion</b> <b>recognition</b> <b>using</b> brain activity. In this paper by using an existing simulator I have achieved 97 percent accurate results and it is easy and simplest way than <b>Emotion</b> <b>recognition</b> <b>using</b> brain activity system. Purposed system depends upon human face as we know face also reflects the human brain activities or emotions. In this paper neural network {{has been used for}} better results. In the end of paper comparisons of existing Human Emotion Recognition System has been made with new one...|$|E
40|$|Abstract — This paper {{discusses}} {{the application of}} feature extraction of facial expressions with combination of neural network for the recognition of different facial emotions (happy, sad, angry, fear, surprised, neutral etc [...] ). Humans are capable of producing thousands of facial actions during communication that vary in complexity, intensity, and meaning. This paper analyses the limitations with existing system <b>Emotion</b> <b>recognition</b> <b>using</b> brain activity. In this paper by using an existing simulator I have achieved 97 percent accurate results and it is easy and simplest way than <b>Emotion</b> <b>recognition</b> <b>using</b> brain activity system. Purposed system depends upon human face as we know face also reflects the human brain activities or emotions. In this paper neural network {{has been used for}} better results. In the end of paper comparisons of existing Human Emotion Recognition System has been made with new one. Index Terms — Emotions, Visible color difference, Mood, Brain activit...|$|E
40|$|The {{study of}} {{emotions}} in human-computer interaction {{is a growing}} research area. Focusing on automatic <b>emotion</b> <b>recognition,</b> work is being performed {{in order to achieve}} good results particularly in speech and facial gesture recognition. In this paper we present a study performed to analyze different machine learning techniques validity in automatic speech <b>emotion</b> <b>recognition</b> area. <b>Using</b> a bilingual affective database, different speech parameters have been calculated for each audio recording. Then, several machine learning techniques have been applied to evaluate their usefulness in speech <b>emotion</b> <b>recognition.</b> In this particular case, techniques based on evolutive algorithms (EDA) have been used to select speech feature subsets that optimize automatic <b>emotion</b> <b>recognition</b> success rate. Achieved experimental results show a representative increase in the abovementioned success rate. 1...|$|R
40|$|In this paper, {{we propose}} a novel scheme for speech <b>emotion</b> <b>recognition,</b> which <b>uses</b> decision-templates {{ensemble}} algorithm (DT) to combine base classifiers built on segment-level feature sets. Different feature sets from segments can provide sufficient diversity among base classifiers, {{which is known}} as a necessary condition for improvement in ensemble performance. Compared with those methods of majority voting ensemble and support vector machine, our ensemble scheme can achieve the highest performance at suitable segment levels. On the other hand, we investigate which segment-level and strategy of training base classifiers can provide potential performance in speech <b>emotion</b> <b>recognition,</b> in terms of diversity analysis...|$|R
40|$|Most {{previous}} research into <b>emotion</b> <b>recognition</b> <b>used</b> either a single modality or multiple modalities of physiological signal. However, the former method allows for limited enhancement of accuracy, {{and the latter}} has the disadvantages that its performance can be affected by head or body movements. Further, the latter causes inconvenience to the user due to the sensors attached to the body. Among various emotions, the accurate evaluation of fear is crucial in many applications, such as criminal psychology, intelligent surveillance systems and the objective evaluation of horror movies. Therefore, we propose a new method for evaluating fear based on nonintrusive measurements obtained using multiple sensors. Experimental results based on the t-test, the effect size and {{the sum of all}} of the correlation values with other modalities showed that facial temperature and subjective evaluation are more reliable than electroencephalogram (EEG) and eye blinking rate for the evaluation of fear...|$|R
40|$|<b>Emotion</b> <b>recognition</b> <b>using</b> {{physiological}} and speech signal in short-term observation / Jonghwa Kim and Elisabeth André. - In: Perception and interactive technologies : international tutorial and research workshop, PIT 2006, Kloster Irsee, Germany, June 19 - 21, 2006; proceedings / Elisabeth André [...] . (eds.). - Berlin [u. a. ] : Springer, 2006. - S. 53 - 64. - (Lecture notes in computer science; 4021 : Lecture notes in artificial intelligence...|$|E
40|$|Created {{intentionally}} or spontaneously, cyberworlds are information {{spaces and}} communities that immensely augment {{the way we}} interact, participate in business and receive information throughout the world. This paper reports position statements presented at the plenary panel of the 2015 th International Conference on Cyberworlds. First, the problems of enhancing creativity in cyberworlds using new interfaces are considered. It follows by the discussions on using biometric interfaces in on-line services. Finally, the challenges of using braincomputer interfaces and <b>emotion</b> <b>recognition</b> <b>using</b> electroencephalograms are considered...|$|E
40|$|The aim of {{this thesis}} work is to {{investigate}} the algorithm of speech <b>emotion</b> <b>recognition</b> <b>using</b> MATLAB. Firstly, five most commonly used features are selected and extracted from speech signal. After this, statistical values such as mean, variance will be derived from the features. These data along with their related emotion target will be fed to MATLAB neural network tool to train and test {{to make up the}} classifier. The overall system provides a reliable performance, classifying correctly more than 82 % speech samples after properly training. ...|$|E
40|$|Facial <b>emotion</b> <b>recognition</b> {{has been}} <b>used</b> as a {{representative}} pedestrian activity in studies examining the effect of changes in road lighting. Past studies have drawn conclusions using results averaged across performance with the six universally recognised expressions. This paper asks whether expression choice matters. A reanalysis of past data for each unique expression does not suggest {{a change in the}} conclusion that facial <b>emotion</b> <b>recognition</b> is not significantly affected by the spectral power distribution of the lighting...|$|R
40|$|In this paper, {{we propose}} a bimodal <b>emotion</b> <b>recognition</b> system <b>using</b> the {{combination}} of facial expressions and speech signals. The models obtained from a bimodal corpus with six acted emotions and ten subjects were trained and tested with different classifiers, such as Support Vector Machine, Naive Bayes and K-Nearest Neighbor. In order to fuse visual and acoustic information, two different approaches were implemented: feature level fusion and match score level fusion. Comparative studies reveal that the performance and the robustness of <b>emotion</b> <b>recognition</b> systems can be improved {{by the use of}} fusion-based techniques. Further, the fusion performed at the feature level showed better results than the one performed at the score level. 1...|$|R
40|$|Recently, {{different}} methods {{are provided in}} various articles for face <b>emotion</b> expression <b>recognition.</b> But none of the proposed methods is able to recognize face emotion expression in different illumination. In this paper, we have provided a new method for face <b>emotion</b> expression <b>recognition</b> <b>using</b> fuzzy inference system. The proposed method is able to detect the face emotion expression in different illumination conditions. The proposed method recognizes the hidden emotions in the image using the extracted features of the image and their classification by a fuzzy inference system. We used the images of RaFD database in order to evaluate the performance and efficiency of our algorithm. The experimental {{results show that the}} proposed algorithm can accurately recognize 89. 23 % of the face <b>emotion</b> expression <b>recognition...</b>|$|R
40|$|The {{most common}} {{approaches}} to automatic emotion recognition rely on utterance-level prosodic features. Recent {{studies have shown}} that utterance-level statistics of segmental spectral features also contain rich information about expressivity and emotion. In our work we introduce a more fine-grained yet robust set of spectral features: statistics of Mel-Frequency Cepstral Coefficients computed over three phoneme type classes of interest – stressed vowels, unstressed vowels and consonants in the utterance. We investigate performance of our features in the task of speaker-independent <b>emotion</b> <b>recognition</b> <b>using</b> two publicly available datasets. Our experimental results clearly indicate that indeed both the richer set of spectral features and the differentiation between phoneme type classes are beneficial for the task. Classification accuracies are consistently higher for our features compared to prosodic or utterance-level spectral features. Combination of our phoneme class features with prosodic features leads to even further improvement. Given the large number of class-level spectral features, we expected feature selection will improve results even further, but none of several selection methods led to clear gains. Further analyses reveal that spectral features computed from consonant regions of the utterance contain more information about emotion than either stressed or unstressed vowel features. We also explore how emotion recognition accuracy depends on utterance length. We show that, while there is no significant dependence for utterance-level prosodic features, accuracy of <b>emotion</b> <b>recognition</b> <b>using</b> class-level spectral features increases with the utterance length...|$|E
30|$|One of {{the major}} {{differences}} between our dataset and the DEAP dataset is the approach to annotation. Our EEG experiments allowed subjects to continuously report emotion in arousal-valence space; by contrast, subjects who produced the DEAP dataset could report only one perception for each music video watched. The temporal continuity of emotion reporting in our experiments led to a higher granularity in emotion capturing compared to the DEAP dataset, which could be the underlying reason why the <b>emotion</b> <b>recognition</b> <b>using</b> our dataset had achieved higher performance over the chance level than that using the DEAP dataset.|$|E
40|$|We {{propose a}} {{framework}} for multimodal sentiment analysis and <b>emotion</b> <b>recognition</b> <b>using</b> convolutional neural network-based feature extraction from text and visual modalities. We obtain a performance improvement of 10 % over {{the state of the}} art by combining visual, text and audio features. We also discuss some major issues frequently ignored in multimodal sentiment analysis research: the role of speaker-independent models, importance of the modalities and generalizability. The paper thus serve as a new benchmark for further research in multimodal sentiment analysis and also demonstrates the different facets of analysis to be considered while performing such tasks. Comment: Accepted in CICLing 201...|$|E
40|$|Most <b>emotion</b> <b>recognition</b> {{systems do}} not perform {{real-time}} <b>emotion</b> <b>recognition</b> due to latencies caused by phrase segmentation and resource-intensive feature acquisition, etc. To address this issue, we present an <b>emotion</b> <b>recognition</b> approach that can estimate speaker emotions with much lower latency. The proposed approach does {{not rely on}} phrase-level features to recognize speaker emotion; rather, it estimates the speaker’s emotional state {{over the course of}} the utterance incrementally, using a shifting n-word window on the basis of easily computable features. These features are obtained from three information streams, i. e. cepstral, prosodic and textual, at the wordlevel and combined at decision-level using a statistical framework. Our work shows that combining the three information streams yields higher <b>emotion</b> <b>recognition</b> accuracy than any single information stream. Using features extracted from n-word sequences rather than phrases provides for the low-latency capabilities of the proposed system, without any loss in utterance-level <b>emotion</b> <b>recognition</b> accuracy. The performance of the proposed system on a binary utterance-level <b>emotion</b> <b>recognition</b> task <b>using</b> an in-house database shows a relative improvement of 41 % over chance, compared to a relative improvement of 31. 82 % shown by the baseline phrase-level <b>emotion</b> <b>recognition</b> approach...|$|R
40|$|This paper {{describes}} an integrated system for human <b>emotion</b> <b>recognition,</b> which is <b>used</b> to provide feedback about the relevance or {{impact of the}} information that is presented to the user. Other techniques in this field extract explicit motion fields from the areas of interest and classify them with the help of templates or training sets; the proposed system, however, compare...|$|R
40|$|Deep brain {{stimulation}} of the subthalamic nucleus (STN-DBS) has acquired a relevant role {{in the treatment of}} Parkinson’s disease (PD). Despite being a safe procedure, it may expose patients to an increased risk to experience cognitive and emotional difficulties. Impair- ments in <b>emotion</b> <b>recognition,</b> mediated both by facial and prosodic expressions, have been reported in PD patients treated with such procedure. However, it is still unclear whether the STN per se is responsible for such changes or whether others factors like the microlesion produced by the electrode implantation may also play a role. In this study we evaluated facial emotions discrimination and <b>emotions</b> <b>recognition</b> <b>using</b> both facial and prosodic expressions in 12 patients with PD and 13 matched controls. Patients’ were tested in four conditions: before surgery, both in on and off medication, and after surgery, respectively few days after STN implantation before turning stimulator on and few months after with stimulation on. We observed that PD patients were impaired in discriminating and recognizing facial emotions, especially disgust, even before DBS implant. Microlesion caused by surgical procedure was found to influence patients’ performance on the discrimination task and recognition of sad facial expression while, after a few months of STN stimulation, impaired disgust recognition was again prominent. No impairment in emotional prosody recognition was observed both before and after surgery. Our study confirms that PD patients may experience a deficit in disgust recognition and provides insight into the differential effect of microlesion and {{stimulation of}} STN on several tasks assessing <b>emotion</b> <b>recognition...</b>|$|R
40|$|AbstractIn this paper, {{we propose}} {{a method for}} EEG emotion {{recognition}} which is tested based on 2 dimensional models of emotions, (1) the rSASM, and (2) the 12 -PAC model. EEG {{data were collected from}} 5 preschoolers aged 5 years old while watching emotional faces from the Radboud Faces Database (RafD). Features were extracted using KSDE and MFCC and classified using MLP. Results show that EEG <b>emotion</b> <b>recognition</b> <b>using</b> the 12 -PAC model gives the highest accuracy for both feature extraction methods. Results indicated that the accuracy of EEG emotion recognition is increased with the precision of the dimensional models...|$|E
40|$|In {{this paper}} we {{describe}} {{a solution to}} our entry for the emotion recognition challenge EmotiW 2017. We propose an ensemble of several models, which capture spatial and audio features from videos. Spatial features are captured by convolutional neural networks, pretrained on large face recognition datasets. We show that usage of strong industry-level face recognition networks increases the accuracy of <b>emotion</b> <b>recognition.</b> <b>Using</b> our ensemble we improve on the previous best result on the test set by about 1 %, achieving a 60. 03 % classification accuracy without any use of visual temporal information. Comment: 4 page...|$|E
40|$|AbstractIn the paper, {{modulation}} spectral features (MSFs) {{are proposed}} for the automatic emotional recognition for speech signal. The features are extracted from an auditory-inspired long-term spectro-temporal(ST) representation. On an experiment assessing classification of 4 emotion categories, the MSFs show promising performance in comparison with features {{that are based on}} mel-frequency cepstral coefficients and perceptual linear prediction coefficients, two commonly used short-term spectral representations. The MSFs further express a substantial improvement in recognition performance when used to augment prosodic features, which have been extensively used for speech <b>emotion</b> <b>recognition.</b> <b>Using</b> both types of features, an overall recognition rate of 91. 55 % is obtained for classifying 4 emotion categories...|$|E
40|$|International audienceIn {{the context}} of the very dynamic and {{challenging}} domain of affective computing, we adopt a software engineering point of view on <b>emotion</b> <b>recognition</b> in interactive systems. Our goal is threefold: first, developing an architecture model for <b>emotion</b> <b>recognition.</b> This architecture model emphasizes multimodality and reusability. Second, developing a prototype based on this architecture model. For this prototype we focus on gesture-based <b>emotion</b> <b>recognition.</b> And third, <b>using</b> this prototype for augmenting a ballet dance show. We hence describe an overview of our work so far, from the design of a flexible and multimodal <b>emotion</b> <b>recognition</b> architecture model, to a presentation of a gesture-based <b>emotion</b> <b>recognition</b> prototype based on this model, to a prototype that augments a ballet stage, taking emotions as inputs...|$|R
40|$|The main {{objective}} {{of this paper is}} to develop a speech <b>emotion</b> <b>recognition</b> system <b>using</b> residual phase and MFCC features with neural network. The speech <b>emotion</b> <b>recognition</b> system classifies the speech emotion into predefined categories such as anger, fear, happy, neutral or sad. The proposed technique for speech <b>emotion</b> <b>recognition</b> has two phases: Feature extraction, and Classification. Initially, speech signal is given to feature extraction phase to extract residual phase and MFCC features. Based on the feature vectors extracted from the training data, neural network are trained to classify the emotions into anger, fear, happy, neutral or sad. Using residual phase and MFCC features the performance of the proposed technique is evaluated. The rate of recognizing the emotion from the speech signal is about 88. 3 %. Experimental results show neural network, Classifier is better which offers a new efficient way of solving problems...|$|R
5000|$|<b>Emotion</b> <b>recognition</b> is <b>used</b> for {{a variety}} of reasons. Affectiva uses it to help {{advertisers}} and content creators to sell their products more effectively. Affectiva also makes a Q-sensor that gauges the emotions of autistic children. Emotient was a startup company which utilized artificial intelligence to predict [...] "attitudes and actions based on facial expressions". [...] Apple indicated its intention to buy Emotient in January 2016. ParallelDots have a emotion detction API to detect emotion from visual and textual data which uses convolutional neural networks for implementing emotion classification as it is a feature detection type of task. [...] nViso provides real-time <b>emotion</b> <b>recognition</b> for web and mobile applications through a real-time API. Visage Technologies AB offers emotion estimation as a part of their Visage SDK for marketing and scientific research and similar purposes. Eyeris is an <b>emotion</b> <b>recognition</b> company that works with embedded system manufacturers including car makers and social robotic companies on integrating its face analytics and <b>emotion</b> <b>recognition</b> software; as well as with video content creators to help them measure the perceived effectiveness of their short and long form video creative. <b>Emotion</b> <b>recognition</b> and <b>emotion</b> analysis are being studied by companies and universities around the world.|$|R
40|$|Both {{emotion and}} visual {{processing}} deficits are documented in schizophrenia, and preferential magnocellular visual pathway dysfunction {{has been reported}} in several studies. This study examined the contribution to emotion-processing deficits of magnocellular and parvocellular visual pathway function, based on stimulus properties and shape of contrast response functions. Experiment 1 examined the relationship between contrast sensitivity to magnocellular- and parvocel-lular-biased stimuli and <b>emotion</b> <b>recognition</b> <b>using</b> the Penn Emotion Recognition (ER- 40) and Emotion Differentiation (EMODIFF) tests. Experiment 2 altered the contrast levels of the faces themselves to determine whether emotion detec-tion curves would show a pattern characteristic of magno-cellular neurons and whether patients would show a deficit in performance related to early sensory processin...|$|E
40|$|Data sparseness is an ever {{dominating}} {{problem in}} automatic <b>emotion</b> <b>recognition.</b> <b>Using</b> artificially generated speech for training or adapting models could potentially ease this: though less natural than human speech, one could synthesize the exact spoken content in different emotional nuances- of many speakers {{and even in}} different languages. To investigate chances, the phonemisation components Txt 2 Pho and openMary are used with Emofilt and Mbrola for emotional speech synthesis. Analysis is realized with our Munich open Emotion and Affect Recognition toolkit. As test set we gently limit to the acted Berlin and eNTERFACE databases for the moment. In the result synthesized speech can indeed {{be used for the}} recognition of human emotional speech...|$|E
40|$|The {{purpose of}} this study was to examine the effects of shading parts of face on <b>emotion</b> <b>recognition</b> <b>using</b> methods of choice. The {{participants}} were 34 undergraduate students, whose ages ranged from 20 to 29 years. The stimulus materials comprised photographs of six basic emotions (happiness, surprise, fear, anger, sadness and, disgust), and copies of these photographs in which upper or lower parts of these six basic emotions were shaded. The models for the photographs were six Japanese males and six Japanese females. The task assigned to the participants was to select emotional words appropriate for each photograph. The rates of correct responses for the male and female photographs of happiness, surprise and sadness were > 90 %. In the male photographs, differences in the rates of correct responses between photographs with their upper parts shaded and those with their lower parts shaded were significant in happiness, anger, sadness and disgust. In the female photographs, differences in the rates of correct responses between photographs with their upper parts shaded and those with their lower parts shaded were significant in happiness, fear, anger and disgust. These results suggest the following; 1) Happiness can be recognised more precisely by viewing the lower parts of face regardless of the gender depicted in the photographs; 2) Anger can be recognised more precisely by viewing the upper parts of face regardless of the gender depicted in the photographs; 3) Fear can not be recognised precisely even in photographs that are not shaded; 4) In future studies, it is necessary to examine effects of shading parts of the face on <b>emotion</b> <b>recognition</b> <b>using</b> other sets of photographs, and to examine effects of action units peculiar to each emotion on emotion recognition...|$|E
40|$|Abstract — Speech {{is one of}} {{the most}} {{fundamental}} and natural means of communication between human beings. Human beings use emotions extensively for expressing their intentions through speech. Affection Recognition through speech signals is a current research topic in the field of human-computer interaction with wide range of applications. The proposed system recognizes the emotional state of a person based on gender. The system compose of two subsystems: Gender <b>Recognition</b> and <b>Emotion</b> <b>Recognition.</b> Here, a speech <b>emotion</b> <b>recognition</b> system <b>using</b> both the spectral & prosodic features is proposed. Since both the spectral & prosodic features contain emotion information, the combination of these features improves the performance of the system. The gender recognized speech and the features extracted are given to the <b>emotion</b> <b>recognition</b> subsystem, where the emotions are recognized based on two classifiers (i. e., two support vector machines) : the one trained on the basis of signals recorded by male speakers and the other one trained by that of female speakers...|$|R
40|$|In this paper, {{we present}} a hybrid speech <b>emotion</b> <b>recognition</b> system {{exploiting}} both spectral and prosodic features in speech. For capturing the emotional information in the spectral domain, we propose a new spectral feature extraction method by applying a novel non-uniform subband processing, instead of the mel-frequency subbands used in Mel-Frequency Cepstral Coefficients (MFCC). For prosodic features, a set of features that are closely correlated with speech emotional states are selected. In the proposed hybrid <b>emotion</b> <b>recognition</b> system, due to the inherently different characteristics of these two kinds of features (e. g., data size), the newly extracted spectral features are modeled by Gaussian Mixture Model (GMM) and the selected prosodic features are modeled by Support Vector Machine (SVM). The final result of the proposed <b>emotion</b> <b>recognition</b> system is obtained by combining the results from these two subsystems. Experimental results show that (1) the proposed non-uniform spectral features are {{more effective than the}} traditional MFCC features for emotion recognition; (2) the proposed hybrid <b>emotion</b> <b>recognition</b> system <b>using</b> both spectral and prosodic features yields the relative recognition error reduction rate of 17. 0 % over the traditional <b>recognition</b> systems <b>using</b> only the spectral features, and 62. 3 % over those using only the prosodic features...|$|R
40|$|Purpose - Previous {{research}} has demonstrated the importance of <b>emotion</b> <b>recognition</b> ability in negotiations and leadership, but scant {{research has}} investigated the role of <b>emotion</b> <b>recognition</b> ability in service contexts. The {{purpose of this paper}} is to propose and test a compensatory model in which service employees&# 039; <b>emotion</b> <b>recognition</b> ability helps enhance their job performance, particularly when employees score low on the agreeableness personality dimension or have low cognitive ability. Design/methodology/approach - With a two-wave multisource dataset collected from a service center of a large retail bank, multiple regression analysis was used to test the moderating roles of agreeableness and cognitive ability on the relationship between service employees&# 039; <b>emotion</b> <b>recognition</b> ability and their performance. Findings - Service employees&# 039; <b>emotion</b> <b>recognition</b> ability helped enhance their job performance. However, the positive effect of <b>emotion</b> <b>recognition</b> ability on job performance was only statistically significant when employees&# 039; agreeableness or cognitive ability was low. Practical implications - The findings have important implications for how service organizations select and recruit employees. In particular, service employees with low agreeableness or cognitive ability may still be able to perform well when possessing high <b>emotion</b> <b>recognition</b> ability. Therefore, <b>emotion</b> <b>recognition</b> ability should be considered in the selection and recruitment process. Originality/value - Going beyond self-report measures of <b>emotion</b> <b>recognition</b> and <b>using</b> a performance measure from organizational records, this study is one of the first to examine how <b>emotion</b> <b>recognition</b> ability interacts with personality and cognitive ability in predicting service employees&# 039; effectiveness in a service organization...|$|R
