4|10000|Public
40|$|The first tier of {{automotive}} manufacturers has faced to pressures about move, modify, updating tasks for manufacturing resources in production processes from demand response of production order sequence for motor company and process innovation purpose for productivity. For meets this requirements, {{it has to}} require absolutely lead time to re-wiring of physical interface for production equipment, needs for change existing program and test over again. For prepare this constraints, it needs studying an auto-configuration functions that build for both visibility and flexibility based on the 4 M (Man, Machine, Material, Method) group management which is supports from WSN (Wireless Sensor Network) of the open embedded device called M 2 M (Machine to Machine) and major functions of middleware including point manager for real-time device communication, real-time <b>data</b> <b>management,</b> <b>Standard</b> API (Application Program Interface) and application template management. To be application system to RMS (Reconfigurable Manufacturing System) for rapidly response from various orders and model from motor company that is beginning to establishing the mapping of manufacturing resources of 4 M using WSN...|$|E
40|$|Master data {{management}} (MDM) integrates data from mul-tiple structured data sources and builds a consolidated 360 -degree view of business entities such as customers and prod-ucts. Today’s MDM {{systems are not}} prepared to integrate information from unstructured data sources, such as news reports, emails, call-center transcripts, and chat logs. How-ever, those unstructured data sources may contain valuable information about the same entities known to MDM from the structured data sources. Integrating information from unstructured data into MDM is challenging as textual ref-erences to existing MDM entities are often incomplete and imprecise and the additional entity information extracted from text should not impact the trustworthiness of MDM data. In this paper, we present an architecture for making MDM text-aware and showcase its implementation as IBM Info-Sphere MDM Extension for Unstructured Text Correlation, an add-on to IBM InfoSphere Master <b>Data</b> <b>Management</b> <b>Standard</b> Edition. We highlight how MDM benefits from additional evidence found in documents when doing entity resolution and relationship discovery. We experimentally demonstrate the feasibility of integrating information from unstructured data sources into MDM. 1...|$|E
40|$|This thesis {{presents}} {{an extension of}} the contemporaryengineering design theory towards a unified view onsimultaneous development of products and manufacturing systems,i. e. concurrent engineering. The traditional engineering design theory explains therealization of a product design as a development of productstructure from four perspectives: technical process, function,technical solution, and physical embodiment. This thesisextends the engineering design theory with a set of definitionsand universal statements. These definitions and universalstatements describe manufacturing systems from same fourperspectives. In that context they also describe therelationship between a product and its manufacturing system. The thesis contributes {{to the creation of a}} single theoreticalsystem based on an integration of theories from two engineeringdesign schools, the WDK and the Axiomatic Design. WDKtheoriesare in this new context utilized for qualitative synthesis ofthe developed artifacts, while the Axiomatic Design is utilizedfor structuring and analyzing the corresponding quantitativeparameters. The definitions and universal statements describe thedevelopment structures for productsand manufacturing systems. This description is utilized for definition of a system fordevelopment of these structures, i. e. (i) a stage-gate-basedmanufacturing system development process, (ii) a developmentmethodology toolbox, and (iii) an information managementframework consisted of an information model harmonized with thesystems engineering <b>data</b> <b>management</b> <b>standard</b> STEP AP 233. The research has been carried out in a close collaborationwith Swedish manufacturing industry. The utilized researchmethodology is the hypothetic- deductive method, with casestudy as an observation method. Keywords:Concurrent Engineering, Engineering Design,Development Methods and Tools, Manufacturing System,Information Management...|$|E
5000|$|The PPDM Association is a global, not-for-profit {{organization}} that develops <b>data</b> <b>management</b> <b>standards</b> for the petroleum industry. See Professional Petroleum <b>Data</b> <b>Management</b> Association ...|$|R
50|$|Until 1996, the National Institute of Standards and Technology (NIST) <b>data</b> <b>management</b> <b>standards</b> program {{certified}} SQL DBMS {{compliance with}} the SQL standard. Vendors now self-certify the compliance of their products.|$|R
50|$|The Professional Petroleum <b>Data</b> <b>Management</b> Association (PPDM Association) is a global, not-for-profit {{organization}} that works collaboratively within the petroleum industry {{to create and}} promote standards and best practices for <b>data</b> <b>management.</b> The Association’s vision is the global adoption of <b>data</b> <b>management</b> <b>standards</b> and best practices throughout the upstream (exploration and production) petroleum industry.|$|R
40|$|Zhengwu Lu 1, Jing Su 21 Smith Hanley Consulting, Houston, Texas; 2 Department of Chemical Engineering, University of Massachusetts, Amherst, MA, USAAbstract: To {{maintain}} a competitive position, the biopharmaceutical {{industry has been}} facing the challenge of increasing productivity both internally and externally. As {{the product of the}} clinical development process, clinical data are recognized to be the key corporate asset and provide critical evidence of a medicine&rsquo;s efficacy and safety and of its potential economic value to the market. It is also well recognized that using effective technology-enabled methods to manage clinical data can enhance the speed with which the drug is developed and commercialized, hence enhancing the competitive advantage. The effective use of data-capture tools may ensure that high-quality data are available for early review and rapid decision-making. A well-designed, protocol-driven, standardized, site workflow-oriented and documented database, populated via efficient data feed mechanisms, will ensure regulatory and commercial questions receive rapid responses. When information from a sponsor&rsquo;s clinical database or data warehouse develops into corporate knowledge, the value of the medicine can be realized. Moreover, regulators, payer groups, patients, activist groups, patient advocacy groups, and employers are becoming more educated consumers of medicine, requiring monetary value and quality, and seeking out up-todate medical information supplied by biopharmaceutical companies. All these developments in the current biopharmaceutical arena demand that clinical data management (CDM) is at the forefront, leading change, influencing direction, and providing objective evidence. Sustaining an integrated database or data repository for initial product registration and subsequent postmarketing uses is a long-term process to maximize return on investment for organizations. CDM should be the owner of driving clinical data-cleaning process in consultation with other stakeholders, such as clinical operations, safety, quality assurance, and sites, and responsible for building a knowledge base to add potential value in assisting further study designs or clinical programs. CDM needs to draw on a broad range of skills such as technical, scientific, project management, information technology (IT), systems engineering, and interpersonal skills to tackle, drive, and provide valued service in managing data within the anticipated e-clinical age. Commitment to regulatory compliance is required in this regulated industry; however, a can-do attitude with strong willingness to change and to seek ways to improve CDM functions and processes proactively are essential to continued success and to ensure quality data-driven productivity. Keywords: clinical trials, <b>data</b> <b>management,</b> <b>standard,</b> efficacy, safety, clinical systems, clinical data, electronic data-capturin...|$|E
40|$|The {{purpose of}} this {{document}} is to provide background on <b>data</b> <b>management</b> <b>standards</b> and practices as they apply to bird monitoring data. The majority of text was taken directly from the information resources listed under the references. In some instances, text from these resources was modified to facilitate the document’s ease of reading or to reflect the compilers’ understanding of current <b>data</b> <b>management</b> practices. Suggested citation...|$|R
50|$|Ten countries, {{led by the}} Netherlands, Norway and the United Kingdom, formed NDR {{to share}} best {{practices}} and to formalize the development and deployment of <b>data</b> <b>management</b> <b>standards</b> for regulatory agencies. The other countries involved in the NDR Work Group’s formation are Australia, Canada, India, Kenya, New Zealand, South Africa and the United States.|$|R
40|$|Viewgraphs and {{discussion}} on <b>data</b> <b>management</b> <b>standards</b> in computer-aided acquisition and logistic support (CALS) are presented. CALS {{is intended to}} reduce cost, increase quality, and improve timeliness of weapon system acquisition and support by greatly improving the flow of technical information. The phase 2 standards, industrial environment, are discussed. The information resource dictionary system (IRDS) is described...|$|R
50|$|Global {{regulators}} of {{upstream oil}} and natural gas information, including seismic, drilling, production and reservoir data, formed the National Data Repository (NDR) Work Group in 2008 to collaborate on the development of <b>data</b> <b>management</b> <b>standards</b> and to assist emerging nations with hydrocarbon reserves to better collect, maintain and deliver oil and gas data to the public and to the industry.|$|R
50|$|NSSDC {{supports}} active space {{physics and}} astrophysics researchers. Web-based services allow the NSSDC {{to support the}} general public. This support {{is in the form}} of information about spacecraft and access to digital versions of selected imagery. NSSDC alsoprovides access to portions of their database contains information about data archived at NSSDC (and, in some cases, other facilities), the spacecraft which generate space science data and experiments which generate space science data. NSSDC services also included are <b>data</b> <b>management</b> <b>standards</b> and technologies.|$|R
50|$|With {{the growing}} {{complexity}} of supply chains {{as well as}} the increasing number of government regulations around importing and exporting, companies are realizing the need to proactively manage trade compliance data in order to minimize risk, such as the inaccurate classification of data. Providing classification data on imported and exported products is required, regardless of an organization's global location and classifying products is typically a manual and time-consuming process. Companies need to establish <b>data</b> <b>management</b> <b>standards</b> to help reduce inaccurate classification, strengthen the exchange of data between trading partners, and provide version control in order to track all changes that take place.|$|R
40|$|The {{explosive}} growth in wireless communications {{systems and the}} demand for advanced mobility features have created novel <b>data</b> <b>management</b> problems. Current schemes to address these problems rely on database organizations that have limited functionality and performance anomalies. We propose a new <b>data</b> <b>management</b> scheme that is flexible and scalable, and that is incrementally deployable so it can coexist with current <b>data</b> <b>management</b> <b>standards.</b> We compare our protocol against current standards and other suggested protocols using realistic calling and mobility patterns. To do so, we have built Pleiades, an extensible event driven simulator that is easily configurable to arbitrary geographies, networks, calling and mobility patterns, and <b>data</b> <b>management</b> schemes. We present, for the first time, models to closely approximate user calling and mobility patterns. These models are validated against real call traffic and urban vehicle data. Based on simulations for a representative 24 -hour period i [...] ...|$|R
40|$|Purpose: This poster {{examines}} the initial development {{and integration of}} instruction about data information literacy on a small, liberal-arts college campus {{in collaboration with the}} Institutional Review Board. Setting/Participants/Resources: The Jane Bancroft Cook Library at New College of Florida has a science librarian serving {{as a member of the}} Institutional Review Board. Brief Description: While serving as a member of the Institutional Review Board (IRB), the librarian noticed a lack of campus knowledge and consensus about <b>data</b> <b>management</b> <b>standards.</b> Partnering with fellow IRB members and the Office of Research Programs and Services, the librarian developed instruction for students and faculty about <b>data</b> <b>management.</b> This poster describes the librarian’s analysis of the issue, planning process, selection of methods, design of materials, and review of an in-person workshop. Results/Outcome: Collaborating with other members of the IRB, the librarian analyzed IRB proposals for lack of attention to <b>data</b> <b>management,</b> then developed materials and presented an in-person workshop based on this analysis. To further campus knowledge of <b>data</b> <b>management,</b> the librarian has developed a pre- and post-workshop survey for participants and will be developing an e-learning module for use on campus. Evaluation Method: To determine a need for data information literacy on campus, IRB proposals for twenty projects were evaluated with regard to their <b>data</b> <b>management</b> strategies. <b>Data</b> security, privacy, retention, sharing, and publication were considered. Fourteen of the twenty, or 70 %, of the proposals required revisions based on a lack of adequate attention to <b>data</b> <b>management...</b>|$|R
40|$|AbstractAdvances {{in medical}} care and {{computer}} technology in recent decades have expanded {{the parameters of the}} traditional domain of medical services. This scenario has created new opportunities for building applications to provide enterprise services in an efficient, diverse and highly dynamic environment. Yet the involvement of multiple factors in healthcare systems, such as diverse professionals and embedded devices, has made IT-based healthcare systems expensive, competitive and complex. The deployment of different programming languages, platforms and <b>data</b> <b>management</b> <b>standards</b> has led to restrictions in flawless exchange, integration and reuse of information across different systems. In this regard, service-oriented architecture (SOA) is an advanced methodology {{that can be used for}} developing loosely coupled, dynamic, flexible, distributed and cost-effective applications. SOA relies on services and can handle complexity and heterogeneity with the help of ontologies. In this paper, we show our development of domain ontology for the effective handling of IT-based healthcare system problems especially during an emergency...|$|R
40|$|Since the mid-eighties when Gemstone was {{introduced}} {{as the first}} object-oriented database management system (ODBMS), a dozen other commercial ODBMSs have joined the fierce competition in the market. Although we call them all ODBMSs, they differ in their system concepts and <b>data</b> <b>management</b> <b>standards.</b> Below, I discuss the past evolution and future prospects of those ODBMSs. Evolution of systems There have been three approaches to building an ODBMS: extending an object-oriented programming language (OOPL), extending a relational DBMS, and starting from the ground up. The first approach realizes an ODBMS by adding to an OOPL persistent storage for multiple concurrent accesses with transaction support. This extension {{has the advantage of}} reusing the type system of a programming language as a data model and thus achieves a seamless integration between programming language and database manipulation language. This approach has become the most popular in the commercial world so far and is represented by commercial ODBMSs such as ObjectStore, Versant, Objectivity, and O 2. In the second, extended relational approach, an ODBMS is built by enhancing an existin...|$|R
30|$|A set {{of basic}} {{requirements}} for a European Information System {{has been defined}} after analysing the needs of CC curators and CC users from the {{information provided by the}} MIRRI survey, and from discussions at the 2 nd WDCM symposium held in Beijing in 2011. As it {{can be seen from the}} options and solutions to user needs described here, harmonisation and common tools are required. Data standards and mechanisms for their adoption can be laid down in the MIRRI infrastructure partner charter ensuring conformity and delivery of the envisaged MIRRI Information System. There remain a number of steps to deliver the MIRRI Information System beginning with the evaluation of existing software and tools for its construction. Further work on the scope of the MIRRI Information System will examine interoperability between collections’ data and other relevant data sets outside collections. There is a clear requirement to follow common <b>data</b> <b>management</b> <b>standards,</b> including adopting common ontologies, or vocabularies, if the Information System construction is to be successful. CC staff have developed <b>standards</b> for <b>data</b> <b>management</b> in the EU-FP 7 project EMbaRC (European Consortium of Microbial Resource Centres) and in the GBRCN (Global Biological Resource Centre Network). These standards will be assessed in the context of the envisaged MIRRI Information System.|$|R
40|$|Geotechnical Engineering is {{classified}} by many mining companies {{as the highest}} corporate, investor and operational risk associated with the development and successful exploitation of a mineral resource. Given the shift in culture towards geotechnical engineering and the influx of new exploration projects, the quantity and complexity of geotechnical data is increasing at exponential rates. Unfortunately, in some cases, <b>data</b> <b>management</b> techniques have lagged behind data capture processes, resulting in relatively primitive technologies to store highly sensitive and costly data. Under these primitive systems, there is no quantifiable handling on the quantity or quality of geotechnical data. The rollover effects of poor <b>data</b> <b>management</b> <b>standards</b> are significant and in severe cases, areas require redrilling or revaluation to capture lost data. The aim of this project was to capture, extract and upload geotechnical data into an easily accessible, single source geotechnical database. Using Rio Tinto Coal Australia (RTCA) as a case study, the project formed a framework for future database implementations by outlining the systematic project progression from data extraction to population and application of the database. By providing a single source database, frequent engineering tasks at RTCA were automated which significantly increased engineering efficiency and accuracy. Additionally, comprehensive Quality Assurance and Quality Control (QAQC) checks improved overall data integrity, resulting in enhanced data confidence...|$|R
40|$|The {{purpose of}} this MOA is to {{establish}} the South Carolina Geographic Information Council to better coordinate GIS activities in the State including spatial <b>data</b> collection and <b>management,</b> <b>standards</b> development and <b>data</b> and information sharing. The SCGIC also {{will serve as the}} formal body to develop operational strategies and policies for GIS implementation among cooperating agencies in South Carolina...|$|R
40|$|The Testability of Interaction-Driven Manufacturing Systems project {{seeks to}} enhance the design-fortestability of {{specifications}} for manufacturing software interfaces, derive a test method that is usable for interaction-driven manufacturing systems in general, and foster the reuse of testing artifacts. For our first testability study we constructed some prototype conformance and interoperability tests for the Product <b>Data</b> <b>Management</b> Enablers <b>standard</b> from the Object Management Group. We reused test data developed for the Product <b>Data</b> <b>Management</b> Schema, a developing standard based on ISO 10303 (informally known as the Standard for Exchange of Product model data), and enumerated the lessons learned for testing and testability. We plan to reuse some of our new testing artifacts for testing an ISO 10303 Standard Data Access Interface to data based on the Product <b>Data</b> <b>Management</b> Schema...|$|R
40|$|For decades, the Codes of Fair Information Practice {{have served}} as a model for data privacy, {{protecting}} personal information collected by governments and corporations. But professional <b>data</b> <b>management</b> <b>standards</b> such as the Codes of Fair Information Practice do not take into account a world of distributed data collection, nor the realities of data mining and easy, almost uncontrolled, dissemination. Emerging models of information gathering create an environment where recording devices, deployed by individuals rather than organizations, disrupt expected flows of information in both public and private spaces. We suggest expanding the Codes of Fair Information Practice to protect privacy in this new data reality. An adapted understanding of the Codes of Fair Information Practice can promote individuals’ engagement with their own data, and apply not only to governments and corporations, but software developers creating the data collection programs of the 21 st century. To support user participation in regulating sharing and disclosure, we discuss three foundational design principles: primacy of participants, data legibility, and engagement of participants throughout the data life cycle. We also discuss social changes that will need to accompany these design principles, including engagement of groups and appeal to the public sphere, increasing transparency of services through voluntary or regulated labeling, and securing a legal privilege for raw location data...|$|R
40|$|Abstract: In the {{automotive}} and aerospace industry, millions of technical documents are generated {{during the development}} of complex engineering products. Particularly, the universal application of Computer Aided Design (CAD) {{from the very first}} design to the final documentation created the need for transactional, concurrent, reliable, and secure <b>data</b> <b>management.</b> The huge underlying CAD databases, occupying terabytes of distributed secondary and tertiary storage, are typically stored and referenced in Engineering <b>Data</b> <b>Management</b> systems (EDM) and organized by means of hierarchical product structures. Although most CAD files represent spatial objects or contain spatially related data, existing EDM systems do not efficiently support the evaluation of spatial predicates. In this paper, we introduce spatial database technology into the file-based world of CAD. As we integrate 3 D spatial <b>data</b> <b>management</b> into <b>standard</b> object-relational database systems, the required support for data independence, transactions, recovery, and interoperability can be achieved. Geometric primitives, transformations, and operations on threedimensional engineering data will be presented which are vital contributions t...|$|R
40|$|While {{the value}} of data for an {{individual}} study effort is well understood by the analytic community at large, aggregated worth of data is still astonishingly undervalued by {{many members of the}} OR study community. Data can be described as the fundamental elements of information and knowledge that comprises the corporate whole- consequently its aggregated value particularly when addressed in a context larger than an individual study is significantly greater than the sum of the parts. Obtaining data is indispensable. To be effective it must be a continuous process within every study and can be not only very time consuming but also a very expensive factor in the total cost of a study effort. With the aggregate of available data growing with every study the situation becomes even more complex and the case for agreed community wide <b>data</b> <b>management</b> <b>standards</b> and techniques is made even stronger. Without these standards the analyst’s ability to find the necessary data for an individual study effort by traditional means decreases exponentially and the ability to reuse existing data in future studies is reduced thereby increasing the cost of data. To help the analyst to face these challenges, the NATO Code of Best Practice for Assessment of Command and Control (COBP) introduced a Data Section. This section already defines the application domains of dat...|$|R
40|$|Earth System Science (ESS) {{has evolved}} into a new {{historic}} stage beyond Earth Science. ESS takes the whole earth systems and their interactive actions among spheres as its objective. This causes that its development need lots of multi-disciplinary, multisources, multi-type, integrated geosciences data resources support. According to this requirement, Data Sharing Network of Earth System Science (DSNESS) was established at the beginning of China Scientific Data Sharing Program (SDSP) launched in 2002. Data sharing in DSNESS need standards and specifications environment. Basic data sharing concept model are studied firstly for the requirement. According to 4 principles designed, ESS data sharing standards and specifications system and its relationship among related international and domestic standards system are studied. Designed standards and specifications system includes 4 main classes, i. e., mechanism and rules class, <b>data</b> <b>management</b> <b>standards</b> and specification class, platform development specification class, data service specification class. Total 18 standards, rules and specifications have been drawn up. Through almost 6 years research and application, nowadays all of these 18 standards and specifications have been used successfully in the distributed data sharing network of earth system science which include 1 general center and 13 sub-centers. In the near future, standards and specification environment of DSNESS will be further development two directions oriented, one direction is basic concept reference model research, and another direction is data fusion and assimilation standards and specifications study...|$|R
40|$|This paper proposes an {{extension}} of ODMG (Object <b>Data</b> <b>Management</b> Group) <b>standard</b> for the Object-Oriented Database Management Systems (OODBMS). The extension concentrates on composite objects, which provides a new paradigm, and also improves traditional OODBMS {{to meet the needs}} arising from the aggregation hierarchy. Currently in ODMG, the semantic of the aggregation relationship is explored at the modelling stage and is described in natural language. To formally specify and verify an aggregation relationship, the standardised ODL (Object Definition Language) and OIF (Object Interchange Format) must be utilised. Composite objects classes are defined in an extended ODL, and instance values are initialized with OIF. The proposed ODMG extension in this paper provides a formal foundation for the implementation of aggregation hierarchy in OODBMS. ...|$|R
40|$|Abstract- Environmental {{regulators}} {{must have}} confidence in the complete measurement sequence: from sample point to measurement report. Thus, they need to be confident not only in the measurement instruments that acquire the data but also in the <b>data</b> <b>management</b> software that processes, stores, and prepares reports. This paper describes a new <b>data</b> <b>management</b> software <b>standard</b> for the UK Environment Agency’s MCERTS scheme, which hitherto has only covered instruments. Our approach divides the standard into three parts: the generic quality of the software, which is not addressed here, because it concerns software engineering; the generic features of data management; and the application-specific features, referred to here as the ‘sector standard’. We focus on generic data management: the need for a mathematical specification of any signal processing and calibration; auditability and traceability; and reporting. We have used the standard to conduct audits of <b>data</b> <b>management</b> products; these audits and feedback from diverse industrial bodies who have reviewed the standard, together indicate that it is appropriate for the certification of environmental <b>data</b> <b>management</b> applications. Background Computers are now an integral part of how environmental data is generated, stored, manipulated and reported. Problems with <b>data</b> <b>management</b> can have a number of serious adverse effects on its usefulness t...|$|R
40|$|Abstract:- Recently, {{researches}} on {{relational database}} approaches to sensor networks are being tried. There occur, however, some problems in applying directly the traditional relational database concepts into a sensor network in that every database operation is performed {{only on the}} real existing data which are tuples in database relations. The reason is because in a sensor network viewpoint situations under which some operations should be performed on non-existing data may occur frequently. For instance, let us assume a sensor network that two different classes of nodes are randomly scattered in the same area. We cannot get join results to know the relationship between two different classes of sensing values because there might be no nodes at a exact same location. For a solution about the above described problem we propose in this paper new join operators. This new join operators can provide more effective <b>data</b> <b>management</b> and <b>standard</b> interfaces to application programs in sensor networks...|$|R
40|$|The {{spreading}} {{need for}} {{and use of}} configuration and <b>data</b> <b>management</b> (CDM) <b>standards</b> has highlighted a number of challenges to the companies that need to implement those standards. For companies and organizations that are new to CDM or have limited CDM capabilities, {{one of the major}} dilemmas faced is identifying how and where to start. In many cases {{there is a need to}} contend with a legacy of poorly identified items and information and an immature or non-existent CDM infrastructure (processes, procedures, people, and information systems). To the company management and CDM professional this poses a seemingly insurmountable task of putting in place a CDM infrastructure that provides the needed benefits while keeping within an acceptable cost and schedule. This paper deals with initially establishing the CDM infrastructure using the tools that a company already has available. The paper identifies features of common software applications that can be used to implement CDM principles...|$|R
40|$|Currently, {{power systems}} (PS) already {{accommodate}} a substantial penetration of distributed generation (DG) and operate in competitive environments. In the future, {{as the result}} of the liberalisation and political regulations, PS will have to deal with large-scale integration of DG and other distributed energy resources (DER), such as storage and provide market agents to ensure a flexible and secure operation. This cannot be done with the traditional PS operational tools used today like the quite restricted information systems Supervisory Control and Data Acquisition (SCADA) [1]. The trend to use the local generation in the active operation of the power system requires new solutions for <b>data</b> <b>management</b> system. The relevant standards have been developed separately in the last few years so there is a need to unify them in order to receive a common and interoperable solution. For the distribution operation the CIM models described in the IEC 61968 / 70 are especially relevant. In Europe dispersed and renewable energy resources (D&RER) are mostly operated without remote control mechanisms and feed the maximal amount of available power into the grid. To improve the network operation performance the idea of virtual power plants (VPP) will become a reality. In the future power generation of D&RER will be scheduled with a high accuracy. In order to realize VPP decentralized energy management, communication facilities are needed that have standardized interfaces and protocols. IEC 61850 is suitable to serve as a general standard for all communication tasks in power systems [2]. The paper deals with international activities and experiences in the implementation of a new <b>data</b> <b>management</b> and communication concept in the distribution system. The difficulties in the coordination of the inconsistent developed in parallel communication and <b>data</b> <b>management</b> <b>standards</b> - are first addressed in the paper. The upcoming unification work taking into account the growing role of D&RER in the PS is shown. It is possible to overcome the lag in current practical experiences using new tools for creating and maintenance the CIM data and simulation of the IEC 61850 protocol – the prototype of which is presented in the paper –. The origin and the accuracy of the data requirements depend on the data use (e. g. operation or planning) so some remarks concerning the definition of the digital interface incorporated in the merging unit idea from the power utility point of view are presented in the paper too. To summarize some required future work has been identified...|$|R
40|$|Abstract. To {{improve the}} {{performance}} <b>data</b> <b>management</b> efficiency of welded pipe {{used in the}} 2 nd west to east gas pipeline, a distributed performance <b>data</b> acquiring and <b>management</b> system for welded pipe was developed. The system was established by using two-level three-tier Client/Server model. Performance data for welded pipe from different factories was acquired at clients and input to local database. The data then can be analyzed at local, or {{be transferred to the}} server through networks for unified storage and analysis. The System has functions as <b>data</b> <b>management,</b> file transferring, <b>standard</b> technology condition <b>management,</b> statistical analysis, figure displaying and statistical analysis report generation, etc. The system was implemented using VC++. Oracle and Access is adopted as database for server and client respectively, XML is as the data encapsulation for transferring file. The developed system plays a significant role in analyzing and evaluating the whole quality of welded pipe, and is useful for pipeline quality control. 1...|$|R
40|$|In science, {{results that}} are not {{reproducible}} by peer scientists are valueless and of no significance. Good practices for reproducible science are to publish used codes under Open Source licenses, perform code reviews, save the computational environments with containers (e. g., Docker), use open data formats, use a <b>data</b> <b>management</b> system, and record the provenance of all actions. This talk shows how to record the provenance of code development, code execution, and <b>data</b> <b>management</b> using a <b>standard</b> format for provenance and accompanying Python libraries. In particular, how to gather the provenance of an development process based on Git, how to gather provenance of any Python script and of any IPython/Jupyter notebook, and how to gather provenance of a paper written in LaTeX. Finally, the talk shows how use Python to analyze and explore the provenance, which is stored in a graph database (Neo 4 J) ...|$|R
40|$|AbstractIntroductionThe PEI Programme in the WHO African region {{invested}} in recruitment of qualified staff in <b>data</b> <b>management,</b> developing <b>data</b> <b>management</b> system and <b>standards</b> operating systems since the revamp of the Polio Eradication Initiative in 1997 to cater for <b>data</b> <b>management</b> support {{needs in the}} Region. This support went beyond polio and was expanded to routine immunization and integrated surveillance of priority diseases. But {{the impact of the}} polio <b>data</b> <b>management</b> support to other programmes such as routine immunization and disease surveillance has not yet been fully documented. This is what this article seeks to demonstrate. MethodsWe reviewed how Polio <b>data</b> <b>management</b> area of work evolved progressively along with the expansion of the <b>data</b> <b>management</b> team capacity and the evolution of the <b>data</b> <b>management</b> systems from initiation of the AFP case-based to routine immunization, other case based disease surveillance and Supplementary immunization activities. ResultsIDSR has improved the data availability with support from IST Polio funded data managers who were collecting them from countries. The <b>data</b> <b>management</b> system developed by the polio team was used by countries to record information related to not only polio SIAs but also for other interventions. From the time when routine immunization data started to be part of polio <b>data</b> <b>management</b> team responsibility, the number of reports received went from around 4000 the first year (2005) to > 30, 000 the second year and to > 47, 000 in 2014. ConclusionPolio <b>data</b> <b>management</b> has helped to improve the overall VPD, IDSR and routine <b>data</b> <b>management</b> as well as emergency response in the Region. As we approach the polio end game, the African Region would benefit in using the already set infrastructure for other public health initiative in the Region...|$|R
40|$|Abstract – Currently, {{there is}} no global, open <b>data</b> or network <b>management</b> <b>standard</b> in the {{telecommunications}} power sector. There is a growing awareness that this represents a significant cost to telcos and asset managers. This cost may be measured both in capital, deployment and operating costs as well as indirect future costs. The cost-effective management of real time monitoring and asset information is a universal problem impacting all telcos. The paper will outline the kinds of information flow in telepower energy systems and how an open standard could provide cost savings and encourage wider adoption. It discusses what should be standardized and {{the process by which}} this might be achieved. The paper discusses common open protocols and data standards (e. g. SNMP, XML) and their application to a telepower standard. An outline of current initiatives towards an open standard is presented. The paper presents a case for an open standard in the area of information (<b>data)</b> <b>management,</b> remote (protocol) monitoring and configuration of telepower systems. This paper has been prepared collaboratively between Frank Bodi of Silcar and Paul Davis of Rectifier Technologies Pacific, out of a common technical interest. The Companies do not have commercial dealings with each other in this or other fields of business. I...|$|R
40|$|Compelled {{to improve}} {{information}} security by {{the introduction of}} personal data protection legislation, organizations worldwide are adopting standardized security management guidelines to inform their internal processes. This paper analyzes whether existing security <b>management</b> <b>standards</b> support process requirements for personal <b>data</b> <b>management,</b> drawing from experience with security policies in private organizations and through an analysis of current European and US legislation. Various aspects of personal <b>data</b> <b>management</b> not commonly addressed by security standards are identified, and a number of generally applicable enhancements are proposed to one common standard, IS 17799. The appropriateness of including data protection guidelines in security standards is discussed, showing how these enhancements could simplify the definition of personal <b>data</b> <b>management</b> procedures in organizations. Key words: personal data protection, privacy, information security management, IS 17799, multilateral security, HIPAA 1...|$|R
40|$|In June 2013, the University of British Columbia (UBC) Library {{completed}} a 6 -month project {{that involved the}} audit and analysis of over 700 licensing documents. Long overdue {{and the result of}} nearly a year of planning, the project brought about the successful implementation of an electronic resource management system, Serials Solutions’s 360 Resource Manager, and the reorganization of the UBC public-facing license permissions database. This paper follows the evolution of the UBC license reanalysis project from its inception to conclusion and emphasizes the influence of the changing copyright environment in Canada and institutional perceptions of risk. The development and deployment of a license management system is a fundamentally complex process; project <b>management,</b> <b>data</b> <b>standards,</b> and an iterative approach were all central to the project’s success...|$|R
