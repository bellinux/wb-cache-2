3|14|Public
40|$|This article {{intends to}} {{investigate}} {{the use of a}} Panorama multi-layer as a <b>hyper</b> <b>document</b> to broaden the understanding the history of the cities. We asume that a Panorama multi-layer, in this opportunity, can convey unusual interpretations with the add of important drawings and pictures made by the travelers wich arrived in Rio de Janjeiro in the beginning of 19 th century. This paper aims to demonstrate new possibilities which can be added into the digital and interactive panoramas, and a new way to develop a iconographic <b>hyper</b> <b>document</b> in the central area within the city of Rio de Janeiro...|$|E
40|$|For an {{effective}} process of distance learning {{it is necessary}} to assist teachers during the design of instructional material, helping them designing the material structure, navigation, organization and layout and considering pedagogical aspects that facilitate the student knowledge building. This article presented the development process of LIA’s learning action, focusing on how Ausubel and Gagne’s Theory supported this process. Also, the learning objects edition was discussed, considering a <b>hyper</b> <b>document</b> about environmental comfort designed with cognitive operators and a collaborative virtual environment to reinforce the concepts about functional comfort. 1...|$|E
40|$|MULTIMEDIA DATA IS PRODUCED BY VARIOUS APPLICATIONS THAT DO NOT COOPERATE. PARTS OF THE INFORMATION ARE DISPERSED ON MANY WORKSTATIONS OVER THE COMPUTER NETWORK. OUR TARGET IS TO CREATE A COHESIVE FRAMEWORK UNITING THOSE PARTS, SO THAT ITS ORGANIZATION REFLECTS THE LOGICAL RELATIONSHIP BETWEEN CONTENTS, RATHER THAN THEIR DIFFERING PHYSICAL PROPERTIES (FORMAT, MEDIA TYPE, STORAGE PLACEMENT). WE STUDY THE FUNCTIONALITY OF GENERAL PURPOSE, STATE-OF-THE-ART HYPERMEDIASYSTEMS AND PROPOSE A MATHEMATICAL, FORMAL MODEL FOR ACTIVE HYPERMEDIA THAT IS BASED ON PRODUCTION RULE SYSTEMS. THIS MODEL MAINTAINS PROPERTIES AND INTEGRITY CONSTRAINTS FOR HYPER DOCUMENTS AND IT SUPPORTS VERSIONS AND VIEWS. IT CAUSES THE AUTOMATIC EVOLUTION OF THE HYPER STRUCTURE TO HANDLE CONTINUOUSLY CHANGING INFORMATION OF THE <b>HYPER</b> <b>DOCUMENT.</b> HYPERARCHI IMPLEMENTS OUR MODEL. IT INTERCONNECTS PARTS FROM AN OPEN-ENDED SET OF APPLICATIONS. WE EXAMINE PRACTICALASPECTS, SUCH AS THE SEPARATION OF THE FUNCTIONALITY INTO A CLIENT AND A SERVER, AND THE AUTOMATIC LINK GENERATION FOR BIG TEXTUAL COLLECTIONS. WE STUDY THE ISSUES ABOUT THE INTERFACE TO THE END USER, HAVING TWO CUSTOM-MADE SYSTEMS AS CASE STUDIES. ...|$|E
40|$|In {{this paper}} {{attention}} {{is drawn to}} the rapidly increasing use of hyperdocuments to archive and access environmental data resources, and the spatial extensions required to explore and link together these data sets are discussed. To illustrate how such environmental <b>hyper</b> <b>documents</b> can be created, a prototype system called the Scolt Multimedia Project (SMP) is introduced and the tool used to explore it, called SMPviewer, is described. Also, the uses and applications of environmental hyperdocuments are presented and their merits vis-�-vis geographical information systems are discussed. ...|$|R
40|$|Distance Learning {{supported}} by the WEB is a reality which is growing fast and, like any technological or empirical innovation, it reveals positive and negative aspects. An important aspect is {{in relation to the}} monitoring of the activities done by the students since an accurate online assessment of the knowledge acquired is an open and, therefore, worrying subject. A way of minimizing such situation is by using technology to monitor the general performance of the student in the virtual learning environment. This work invests on the construction of the OAEditor framework which allows teachers to create learning objects in the form of educational <b>hyper</b> <b>documents</b> representing conceptual graphs that can adapt to the interaction of the students according to their performance, besides enabling a pedagogical monitoring based on self-evaluative questionnaires which are automatically generated, pondering the navigation and performance obtained...|$|R
40|$|Abstract — This paper {{presents}} Cognitor, a common-sense aided {{framework for}} the Cog-Learn Pattern Language that aims to help content editors create and contextualize e-learning content through the generation of <b>hyper</b> <b>documents</b> that comprise pedagogical issues, producing learning objects (LO) compatible with the SCORM standard. Cog-Learn have been developed from Learning and Human-Computer Interaction (HCI) Theories. The goal of Cognitor is {{to allow for the}} design of digital learning material with better organization of the content that will be explored by learners, aiming to make easier the interaction with it, addressing HCI issues, as well as the interaction between learners and teachers, addressing Human-Human Interaction (HHI) issues. To reach that, {{it is important that the}} LO created in Cognitor be easy to reuse, helping creators to fill out the metadata aiming at addressing cultural issues related to the context in which the LO can be adopted and reused. For that, suggestions coming from a common sense knowledge base automatically appear in some SCORM metadata fields in order to help the task of filling out culturally contextualized metadata...|$|R
40|$|How {{should one}} place oneself within the present iconosphere, {{at a time}} when {{everything}} seems to be <b>hyper</b> <b>documented?</b> What can be done with the plethora of images surrounding us? And is there anything left for us to read in these objects that have already been discussed over and over again? The filmmaker Harun Farocki hangs on to the belief that his documentaries can make images speak in new and different ways. Also, while traditional iconology sees the image as a crossword puzzle with only one possible interpretation, Georges Didi-Huberman argues for a critical montage aiming to restore the openness of the images in order to deploy their multiple meanings. What is at stake here is the very construction of "the eye of history," an alternative viewpoint allowing for a kind of reading that would free the document from the strictures of homogenizing interpretations. The montage as understood by Didi- Huberman restores the complexity of the visual, thus placing itself in stark opposition to traditional authoritarian ap- proaches. This paper explores the role of a "pedagogy of the document" in liberating knowledge from automatized modes of reading. Peer reviewe...|$|R
40|$|The {{continuous}} {{growth of}} the amount of available information in electronic support raises problems related to the conceptual modeling of hypertext systems, what hinders the process of browsing in <b>hyper</b> <b>documents.</b> This article results from the first reflections about the research that has being developed in UFMG´s Information Science Masters Program and deals with  the conceptual modeling hypertext systems according to the theories of thematic representation. It also presents {{a review of the literature}} on the subject and the applicability of concept’s theory in the process of identification of hypertextuals nodes and links determination. O crescimento contínuo da quantidade de informação disponível em suporte eletrônico traz consigo problemas relacionados à modelagem conceitual de sistemas de hipertextos, o que compromete o processo de folheio em hiperdocumentos. Esse artigo é resultado das reflexões iniciais da pesquisa desenvolvida no decorrer do Curso de Mestrado em Ciência da Informação da UFMG e propõe-se tratar a modelagem conceitual de sistemas de hipertextos do ponto de vista das teorias de representação temática. Inclui uma revisão bibliográfica sobre o tema e a aplicabilidade da teoria do conceito no processo de determinação de links e identificação de nós hipertextuais...|$|R
40|$|In the {{existing}} KBS-MEDIA (knowledgebased systems- media) environment demonstrators {{have been built}} to support different phases in the building process- City Advisor, Material and Vendor Information, Building Maintenance etc. In this environment new concepts are tried out in connection with using, building and maintaining the systems formed by advanced software and new media. New tools for building and using the systems have been defined, created and tested. The users have on application level access to the underlying facts bases (also audio/visual) and tool boxes through a context dependent interface. Background agents are created to help users/system builders to control the access and growth of the systems. Different representations are used (hypertext, analogical, relational databases, neural nets, decision trees, objectoriented, etc.) which are loosely linked and more or less formalizing our real world. The user interface has multimedia properties. The decision and knowledge transfer process is enhanced by the systems giving individuals and group of process participants access to adequate tools. The <b>hyper</b> <b>documents</b> which are created possess powerful man-machine interface and dynamic model building properties. The system which is shown at the conference forms a demonstrator environment used in different applications to capture, test and communicate ideas admitting fast prototyping. Keywords Agents; man-machine interaction; knowledge representation; hy-permedia; artificial intelligenc...|$|R
40|$|Part 1 : Long and Short PapersInternational audienceWe {{created and}} tested e-Rural, an {{approach}} to allow educators to dynamically adjust the target literacy level for their online learning content {{using a combination of}} three tools: PACO-T for planning, COGNITOR for editing <b>hyper</b> <b>documents</b> and Simplifica for text simplification. PACO-T and COGNITOR use the Brazilian Open Mind Common Sense knowledgebase (OMCS-Br) to provide access to commonly held understandings and beliefs on a diverse set of topics associated with a large range of Brazilian demographics, including, people with low literacy. We tested our experiment with 13 users that were creating hyperdocument-based learning content to describe important methods to milk production. We chose milk production as this is one of Brazil’s primary agricultural products and yet it has been established that there is a wide gap between the content from researchers with methods to greatly enhance the quality and economic power of milk production and the tacit knowledge and procedures of the farmers who actually produce the milk who are often at low literacy levels consistent with Brazil’s low literacy levels being around 75 % of the population. Our experiments reveal that educators are able to produce milk related learning content geared towards different literacy levels using our tools with a very satisfying efficacy and efficiency levels. Thus, we believe that the use of our approach that introduces demographically sensitive common sense holds promise {{to bridge the gap between}} high literacy researchers with evidence-based approach to milk production and tacitly-based, low-literacy milk producers to better develop the milk industry in Brazil...|$|R
40|$|Today Internet is {{becoming}} very wide and user wantto retrieve the exact information related with the query. Sometimes user query result return by the search engine is notexactly same. For {{this type of}} information retrieval problemmay be reduced by underlying Indexing Techniques namelyLatent Semantic Indexing, Latent Semantic analysis, PLSI, andLatent Dirichlet Allocation Model for provide semantic tools fortext and also describe some multimedia retrieval tools toprovide accurate precision and remove low recall rate. Here weintroduce a new model for linking document called as LTHMand Fuzzy Information Retrieval System(FIRS) for describe theweight based queries. Basically PLSI and LDA are used for thecontents of plaintext so for web text LTHM is used to describethe <b>hyper</b> linking of <b>document</b> with web. The prospective areaof information retrieval where FIRS is needed is described withjustification. FIRS are used to define the accuracy and precisionof web documents and queries...|$|R
40|$|Problem statement: The web content mining used {{to access}} lot of web pages, mining of web {{contents}} aims to extort positive information or awareness. Approach: There are several type of Web contents which can suggest valuable information to users are accessible in the Web, for instance graphical data, Extensible Markup Language <b>documents,</b> <b>Hyper</b> Text Markup Language documents and simple text. Here, only {{element of the}} information is useful for a testing purpose and the remaining information are noises. Results: In this research study, we propose an approach for removing the noises from a given web page which will get better the presentation of web content mining. At first, the web page information is divided into various blocks. Conclusion: From which, the duplicate blocks are removed using sketching. The performance of the proposed approach and results ensure {{the effectiveness of the}} proposed approach in classify the main blocks...|$|R
40|$|This {{paper is}} {{concerned}} with ranking model construction in document retrieval. Traditionally, the ranking model {{is defined as a}} function of a query and a document. In practice, many factors affecting ranking can and must be taken into consideration, for instance, similarities between <b>documents</b> and <b>hyper</b> links between <b>documents.</b> One needs to exploit a new ranking model which is a function of a query and the entire set of documents retrieved with the query. This paper names this new problem ‘global ranking of documents’, in contrast to traditional ‘local ranking of documents’. The paper proposes a novel learning to rank method to perform the task. The method employs Continuous Conditional Random Fields (CRF) as model, which is a conditional probability distribution representing the mapping relationship from the retrieved documents to their ranking scores. The model can naturally utilize as features the content information of documents as well as the relation information between documents for global ranking. A learning algorithm for creating Continuous CRF is also presented in the paper. Taking Pseudo Relevance Feedback and Topic Distillation as examples, this paper shows how the learning method can be applied to global ranking. Experimental results on benchmark data show that the proposed method outperforms the baseline methods. ...|$|R
40|$|OBJECTIVE: Sleep apnoea {{is common}} after stroke, and has {{adverse effects on}} the {{clinical}} outcome of affected cases. Its pathophysiological mechanisms are only partially known. Increases in brain connectivity after stroke might influence networks involved in arousal modulation and breathing control. The {{aim of this study}} was to investigate the resting state functional MRI thalamic hyper connectivity of stroke patients affected by sleep apnoea (SA) with respect to cases not affected, and to healthy controls (HC). PATIENTS AND METHODS: A series of stabilized strokes were submitted to 3 T resting state functional MRI imaging and full polysomnography. The ventral-posterior-lateral thalamic nucleus was used as seed. RESULTS: At the between groups comparison analysis, in SA cases versus HC, the regions significantly hyper-connected with the seed were those encoding noxious threats (frontal eye field, somatosensory association, secondary visual cortices). Comparisons between SA cases versus those without SA, revealed in the former group significantly increased connectivity with regions modulating the response to stimuli independently to their potentiality of threat (prefrontal, primary and somatosensory association, superolateral and medial-inferior temporal, associative and secondary occipital ones). Further significantly functionally <b>hyper</b> connections were <b>documented</b> with regions involved also in the modulation of breathing during sleep (pons, midbrain, cerebellum, posterior cingulate cortices), and in the modulation of breathing response to chemical variations (anterior, posterior and para-hippocampal cingulate cortices). CONCLUSIONS: Our preliminary data support the presence of functional hyper connectivity in thalamic circuits modulating sensorial stimuli, in patients with post-stroke sleep apnoea, possibly influencing both their arousal ability and breathing modulation during sleep...|$|R
40|$|The {{purpose of}} this thesis project was to test and to {{demonstrate}} the World Wide Web as a publishing vehicle by creating a Web presence for the School of Printing Management and Sciences. In order to reach this goal, {{a full understanding of}} the Hypertext Markup Language must first be realized. Once this is accomplished, issues regarding integration of mixed-media elements within an HTML document were investigated. Once a prototype of the HTML document was accomplished, the mixed-media elements were tested and evaluated for proper integration and contextual cohesiveness. Many issues regarding implementation of mixed-media elements, such as file size and file format were addressed upon testing. One of the additional goals of this project is a comprehensive description of the methodology for creating and maintaining a World Wide Web publishing presence. This addresses: navigational software, structuring HTML <b>documents,</b> <b>hyper</b> text linking, HTML style issues and limitations, effective integration of mixedmedia elements, inline and external image issues, testing documents, advertising documents, strategies for determining proper file sizes and formats of mixedmedia elements, integrating supplemental programs, World Wide Web Server issues, installing HTML and mixed-media files onto a World Wide Web Server, etc. The Web site located at ([URL] served as the vehicle for the investigation. Results of the study revealed the issues of providing data that services users across a wide range of computer systems, with different bandwidth restrictions, utilizing a myriad of computer software. Specific standards apply to An Investigation into World Wide Web Publishing with the Hypertext Markup Language alleviate much of the guesswork, however, publishing on the Internet remains to be as challenging as it is rewarding. The Web 2 ̆ 7 s format and the opportunity to reach millions of potential customers is creating new types of publishing ventures in true 2 ̆ 2 gold-rush 2 ̆ 2 fashion. The Web is being touted as the fourth medium, and some suggest it will have as great an impact on society as print, radio and television. The growth of the Web is explosive and will assuredly continue to blossom. Upon completion of this study, the author remains skeptical whether the World Wide Web is the medium of the future. It has, however, created a trend which will forever reshape the publishing world and the way information seekers receive their data. Publishing will change from a commodity based market where prices are based upon cost, and shift to a service market where prices are based upon the value of the information. Each reader requiring selected information tailored to their specific choice will pay for what they select no more paying for an entire magazine or newspaper and reading only one article. The future of information dissemination is electronic, interactive and selective. Whether the delivery mechanism will be the World Wide Web remains to be seen...|$|R
40|$|AbstractGraphs are well-known, well-understood, and {{frequently}} used means to depict networks of related items. They are successfully {{used as the}} underlying mathematical concept in various application domains. In all these domains tools are developed that store, retrieve, manipulate and display graphs as underlying data structures, despite {{of the fact that}} in most cases these graphs have a different name such as object diagrams, (meta) class diagrams, <b>hyper</b> <b>documents,</b> semantic webs etc. It is the purpose of this workshop to summarize {{the state of the art}} of graph-based tool development, bring together developers of graph-based tools in different application fields and to encourage new tool development cooperations. Motivation Graphs are an obvious means to describe structural aspects in various fields of computer science. They have been successfully used in application areas such as compiler compiler toolkits, constraint solving problems, generation of CASE tools, pattern recognition techniques, program analysis, software engineering, software evolution, software visualization and animation, and visual languages. In all these areas tools have been developed that use graphs as an important underlying data structure. Since graphs are a very general structure mechanism, it is a challenge to handle graphs in an effective way. Using graphs inside tools the following topics play an important role: efficient graph algorithms, empirical and experimental results on the scalability of graphs, reusable graph-manipulating software components, software architectures and frameworks for graph-based tools, standard data exchange formats for graphs, more general graph-based tool integration techniques, and meta CASE tools or generators for graph-based tools. The aim of the workshop on graph-based tools (GraBaTs) is to bring together developers of all kinds of graph-based tools in order to exchange their experiences, problems, and solutions concerning the efficient handling of graphs. The GraBaTs workshop is, therefore, of special relevance for the [URL] 1 st Intl. Conference on Graph Transformation (ICGT) which hosts GraBaTs as a satellite event: In many cases the application of graph transformation technology requires the existence of reliable, user-friendly and efficiently working graph transformation tools. These tools in turn have to be built on top of basic services or frameworks for graphs, which are the main topic of our workshop. Today, several graph transformation tool implementations have emerged which do not share any basic graph services (e. g. for graph pattern matching or graph layout purposes) and which implement rather different graph concepts and graph transformation approaches. Some of these tools - as a kind of survey of the state of the art - were presented in a special session, which is part of the main conference as well as of this satellite workshop. The presented tools are AGG, DiaGen, Fujaba, GenGED, and UPGRADE. The GraBaTs workshop was held for 				 1 					 12 				 days. Its schedule contained in addition to the afore-mentioned session on graph transformation tools, an invited talk by Tiziana Margaria (University of Dortmund, Germany) on ETI, an electronic tool integration platform where graph-based tools will play an important role. Apart from four sessions with presentations of 15 accepted papers (out of 19 submissions) on various graph-based tools and tool-relevant topics, a successful discussion ''Towards Standard Exchange Formats for Graph and Graph Transformation'' took place. Workshop Issues The workshop aims at bringing together tool developers from different fields, dealing with graphs from different perspectives. In the following, we give an overview on the most important perspectives. Meta-modeling by Graphs For a long time the syntax and static semantics of most visual modeling or programming languages was only defined by means of characteristic examples and informal descriptions. To improve this situation the visual language community invented grammar-based formalisms for the definition of the syntax of their languages, such as constraint grammars, graph grammars, relational grammars, etc. Unfortunately it turned out that the grammar-based definition of visual languages is rather complicated compared with the meta-modeling approach developed in parallel. The Meta-modeling approach for the definition of visual languages uses a combination of class diagrams (ER-diagrams, etc.) and predicate logic expressions (Z, OCL, etc.) to define the syntax and static semantics of visual languages. It became popular with the standardization of the OO-modeling language UML and is used by various meta-modeling (meta-CASE) tools which are able to generate domain-specific CASE tools. The so-called MOF approach (Meta-Object Facility) is one attempt to come up with a meta-modeling standard. Despite of its limited expressiveness (compared with ER diagrams or UML class diagrams) MOF builds the basis for the formal definition of UML and other visual languages. All meta-modeling approaches used nowadays have one common property: they offer graph-like diagrams for the definition of the structure (syntax) of graph-like diagram languages. Therefore, meta-modeling is in fact the formal definition of graph languages by means of graphs which are instances of “meta” graph languages. As a consequence, meta-CASE tools are a special class of graph-based tools, which need at least basic services for storing, visualizing, and analyzing graphs. Graph Visualization Facilities for visualizing graphs are needed by all kinds of graph-based tools, independent of the fact whether they are e. g. used for meta-modeling or rule-based programming purposes. Furthermore, graph visualization techniques are the most important means for visualizing various aspects of softwarearchitectures, the dynamic behavior of running systems, their evolution history, and so forth. Software components developed for these purposes usually have to deal with huge graphs and need services for making these graphs persistent, for introducing abstractions based on hierarchical graph models, for computing reasonable graph layouts (efficiently), and for displaying graphs effectively using “fish-eye-techniques” and the like. And last but not least, graph visualization techniques are often employed for teaching purposes in computer science courses on “data structures and (graph) algorithms”. To summarize, almost all kinds of graph-based tools urgently need efficiently and effectively working graph visualization services, whereas graph visualization tools may profit from research activities on graph query and graph transformation engines for the computation of graph abstractions or views. We, therefore, hope that this workshop encourages researchers to start new cooperations, such as adapting graph visualization tools to the needs of graph manipulation tools or exploiting graph manipulation and transformation techniques to compute sensible abstractions of huge graphs. Graph Queries and Graph Algorithms Most, if not all, graph-based tools use to a certain degree software components (libraries, subsystems, etc.) for executing graph queries and/or various kinds of standard graph algorithms. For example, graph transformation tools rely on rather sophisticated means for computing graph matches (rule occurrences) and graph-based reverse engineering tools need rather powerful query engines for determining critical substructures of software architectures. On the other hand, quite a number of database management systems have already been developed using graphs (networks of related objects) as the underlying data model and offering query languages based on graph path expressions or even graph transformations. Vice versa, graph transformation languages like PROGRES are not only used for specifying and visualizing graph algorithms, but incorporate many elements of database query languages such as means for the construction of indexes, the materialization and incremental update of views, etc. Therefore, we like to encourage tool developers again to start cooperating across the boundaries of research areas. Graph Transformation Graph transformation means the rule-based manipulation of graphs. Several graph transformation approaches have emerged which differ w. r. t. to the underlying kind of graphs as well as in the way how rules are applied to graphs, i. e. graph transformation takes place. The kind of graphs used by these tools include labeled, directed graphs, hypergraphs, and graph structures. Their rules, the basic means to manipulate graphs, differ w. r. t. to the formal definition of their semantics, the way how occurrences (matches) are searched for, and how matching rules are applied eventually. In tools, graph transformation is applied to visual languages, specification, code generation, verification, restructuring, evolution and programming of software systems, etc. Developers of graph transformation tools may profit from other workshop participants concerning more efficient realizations of basic functionality, while developers of other graph-based tools might find the graph transformation paradigm attractive to implement certain graph manipulations. The workshop may also provide insights to apply these tools to other application domains. Common Exchange Formats for Graphs and Graph Transformation To support interoperability between various graph-based tools, several initiatives on the development of common exchange formats for graphs have been founded. These formats are all based on the extensible markup language XML developed to interchange documents of arbitrary types. Preceding events like three subgroup meetings of the EU Working Group APPLIGRAPH, a Workshop on Standard Exchange Formats, and a satellite workshop of the 8 th Intl. Symposium on Graph Drawing (GD 2000) discussed various ideas which are currently converging to one format being GXL. During the GraBaTs workshop a further discussion round on this topic was organized focusing especially on graph layout and graph attributes. Another topic of interest for this discussion is an exchange format for graph transformation systems called GTXL, which is under development and which will be built on top of GXL. Workshop Organizers The Program Committee of the workshop consists of: 				Luciano Baresi (Italy) Giuseppe Di Battista (Italy) Ulrik Brandes (Germany) Scott Marshall (The Netherlands) Tom Mens (Belgium) (Co-chair) Andy Schürr (Germany) (Co-chair) Gabriele Taentzer (Germany) (Co-chair) Andreas Winter (Germany) Albert Zündorf (Germany) We are very grateful to Hartmut Ehrig for his help with the organization of the Workshop as satellite event of the 1 st Int. Conference on Graph Transformation (ICGT) and to Mike Mislove, one of the Managing Editors of the ENTCS series. Thanks are also due to Fernando Orejas and his local organizers at UPC in Barcelona who supplied preprints of this volume for all workshop participants...|$|R

