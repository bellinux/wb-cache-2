13|30|Public
50|$|There are {{existing}} <b>exascale</b> <b>computer</b> {{research programs}} in both China and Europe.|$|E
50|$|On 29 July 2015, President Obama {{signed an}} {{executive}} order creating a National Strategic Computing Initiative calling for the accelerated development of an exascale system and funding research into post-semiconductor computing. The Exascale Computing Project hopes to build an <b>exascale</b> <b>computer</b> by 2021.|$|E
50|$|China {{currently}} has the fastest two supercomputers in the world. China's first exascale supercomputer will enter service by 2020 {{according to the}} head of the school of computing at the National University of Defense Technology (NUDT). According to the national plan for the next generation of high performance computers, China will develop an <b>exascale</b> <b>computer</b> during the 13th Five-Year-Plan period (2016-2020). The government of Tianjin Binhai New Area, NUDT and the National Supercomputing Center in Tianjin are working on the project. The exascale supercomputer is planned to be named Tianhe-3.|$|E
40|$|<b>Exascale</b> <b>computers</b> are {{predicted}} to emerge {{by the end}} of this decade with millions of nodes and billions of concurrent cores/threads. One of the most critical challenges for exascale computing is how to effectively and efficiently maintain the system reliability. Checkpointing is the state-of-theart technique for high-end computing system reliability that has proved to work well for current petascale scales. This paper investigates the suitability of checkpointing mechanism for <b>exascale</b> <b>computers,</b> across both parallel filesystems and distributed filesystems. We built a model to emulate exascale systems, and developed a simulator, RXSim, to study its reliability and efficiency. Experiments show that the overall system efficiency and availability would go towards zero as system scales approach exascale with checkpointing mechanism on parallel filesystems. However, the simulations suggest that a distributed filesystem with local persistent storage would offer excellent scalability and aggregate bandwidth, enabling efficient checkpointing at exascale. 1...|$|R
50|$|The {{push for}} {{exascale}} computing {{beginning in the}} mid-2010s, as codified in the National Strategic Computing Initiative, {{was seen as an}} opening for superconducting computing research as <b>exascale</b> <b>computers</b> based on CMOS technology would be expected to require impractical amounts of electrical power. The Intelligence Advanced Research Projects Activity, formed in 2006, currently coordinates the U. S. Intelligence Community's research and development efforts in superconducting computing.|$|R
5000|$|If the {{simulation}} can be deployed on a supercomputing architecture, {{many of the}} modeling choices that are commonly adopted today (see above) might become obsolete.For instance, the future supercomputers {{might be able to}} [...] "move beyond the loosely coupled, forward-simulation paradigm". In particular, [...] "exascale computing will enable a more holistic treatment of complex problems". To exploit <b>exascale</b> <b>computers,</b> it will however be necessary to rethink the design of today's simulation algorithms.|$|R
50|$|Moving {{data from}} the {{collectors}} to the process facilities are traditionally bogged down due to high latency I/O, low bandwidth connections and data is often multiplied along the way {{due to lack of}} purposeful design of the communication network. This research project will try to reduce latency to a minimum and design the I/O systems so data will be written directly into the processing engines on an <b>exascale</b> <b>computer</b> design. The first phase will identify system bottlenecks, and investigate Remote direct memory access (RDMA). The second phase will investigate using standard RDMA technology onto interconnect networking. Phase three includes development of functional prototypes.|$|E
5000|$|The primary {{advantage}} of superconducting computing is improved power efficiency over conventional CMOS technology. Much {{of the power}} consumed, and heat dissipated, by conventional processors comes from moving information between logic elements rather than the actual logic operations. Because superconductors have zero electrical resistance, little energy is required to move bits within the processor. This is expected to result in power consumption savings of a factor of 500 for an <b>exascale</b> <b>computer.</b> For comparison, in 2014 {{it was estimated that}} a 1 exaFLOPS computer built in CMOS logic is estimated to consume some 500 megawatts of electrical power. [...] Superconducting logic can be an attractive option for ultrafast CPUs, where switching times are measured in picoseconds and operating frequencies approach 770 GHz. [...] However, since transferring information between the processor and the outside world does still dissipate energy, superconducting computing was seen as well-suited for communications-intensive tasks where the data largely stays in the cryogenic environment, rather than big data applications where large amounts of information are streamed from outside the processor.|$|E
40|$|Since 2006 JSC {{has held}} {{a series of}} well-received Extreme Scaling Workshops for its Blue Gene systems, and with the High-Q Club has {{documented}} 28 application codes that successfully scaled to exploit the full 28 racks (with 458752 cores capable of running over 1. 8 million threads) of its JUQUEEN Blue Gene/Q. We briefly review these activities and what might be lessons for future <b>exascale</b> <b>computer</b> systems...|$|E
40|$|<b>Exascale</b> <b>computers,</b> {{the next}} {{generation}} of high performance computers, are expected to process 1 exaflops around 2018. However the processor cores used in these systems are very likely to suffer from unpre- dictable high variability in performance. We built a prototype general- purpose reactive work rebalancer that handles such performance vari- ability with low overhead. We did an experimental validation by devel- oping a reactive rebalancer library in UPC, and using it in a 5 -point stencil (heat) simulation. The experiments show that our approach has very limited overhead that compensates for runtime processor speed variations, with or without simulated processor slowdowns. status: publishe...|$|R
40|$|Life {{sciences}} {{in general}} and brain research in particular are research areas with an increasing demand for high performance computing (HPC) resources. Some research goals, like the modeling of the human brain, are expected to require <b>exascale</b> <b>computers.</b> The applications from these areas often have significant different requirements then traditional HPC applications, as {{they tend to be}} unstructured and ask for data-driven approaches. In this talk we introduce a few applications from brain research, discuss their requirements and consider solutions to meet these requirements. Looking into the future we will describe the Human Brain Project, a new European flagship project for future and emerging technologies...|$|R
40|$|<b>Exascale</b> <b>computers</b> {{will enable}} the {{unraveling}} of significant scientific mysteries. Predictions are that 2019 will be the year of exascale, with millions of compute nodes and billions of threads of execution. The current architecture of high-end computing systems is decades-old and has persisted as we scaled from gigascales to petascales. In this architecture, storage is completely segregated from the compute resources and are connected via a network interconnect. This approach will not scale several orders of magnitude in terms of concurrency and throughput, and will thus prevent the move from petascale to exascale. At exascale, basic functionality at high concurrency levels will suffer poor performance, and combined with system mean-time-to-failure in hours, {{will lead to a}} performance collapse for large-scale heroic applications. Storage has the potential to b...|$|R
40|$|The {{confluence}} of emerging technologies and new data-centric workloads offers {{a unique opportunity}} to rethink traditional system architectures and memory hierarchies in future designs. What will future computing systems look like? We are entering an exciting era for systems design. Historically, the first computer to achieve terascale computing (1012, or one trillion operations per second) was demonstrated in the late 1990 s. In the 2000 s, the first petascale computer was demonstrated with a thousand-times better performance. Extrapolating these trends, we can expect the first <b>exascale</b> <b>computer</b> (with one million trillion operations per second) to appear around the end of this next decade...|$|E
40|$|Advances in {{supercomputers}} {{have come}} at a steady pace over the past 20 years. The next milestone is to build an <b>Exascale</b> <b>computer</b> however this requires not only speed improvement but also significant enhancements for energy efficiency and massive parallelism. This paper examines technological progress of supercomputer development to identify the innovative potential of three leading technology paths toward Exascale development: hybrid system, multicore system and manycore system. Performance measurement and rate of change calculation were made by technology forecasting using data envelopment analysis (TFDEA.) The results indicate that the current level of technology and rate of progress can achieve Exascale performance between early 2021 and late 2022 as either hybrid systems or manycore systems. clos...|$|E
40|$|Engineering {{researchers}} {{in the areas of}} Advanced Architectures and Critical Technologies for Exascale Computing. Multi institutional proposals with cohesive emphasis on transformational discoveries that address key barriers on the path to exascale computing are encouraged. Partnerships among academic institutions, National Laboratories, and industry are strongly encouraged. This program is managed in cooperation with NNSA and DARPA. Scientific challenges such as understanding the causes and potential impacts of climate change, improving the efficiency of combustion, and unraveling the mysteries of dark energy and dark matter, as well as a variety of national security challenges, require computational capabilities at extreme scale. At the same time, industry reports make it clear that the exponential growth in processor clock speeds that sustained increases in computational speed for more than 15 years has ended. Projections suggest that building an <b>exascale</b> <b>computer</b> from today's technology would cost approximately $ 1 billion, with power consumption of over a Gigawatt and a mean time between failures of only ten minutes. This Program Announcement invites proposals for basic and applied research to address fundamental challenges in the design of energy-efficient, resilient hardware and softwar...|$|E
30|$|J. Shalf, D. Quinlan and C. Janssen. Rethinking Hardware-Software Codesign for <b>Exascale</b> Systems. IEEE <b>Computer,</b> 44 (11), pages 22 – 30, November 2011.|$|R
40|$|Superconducting digital {{computing}} systems, primarily involving Josephson junctions are actively being pursued as high performance and low energy dissipating alternatives to CMOS-based technologies for petascale and <b>exascale</b> <b>computers,</b> although several challenges still exist in overcoming barriers to practically implement these technologies. In this paper, we present an alternative superconducting logic structure: quantized charge-based logic circuits using quantum phase-slip junctions, {{which have been}} identified as dual devices to Josephson junctions. Basic principles of logic implementation using quantum phase-slips are presented in simulations {{with the help of a}} SPICE model that has been developed for the quantum phase-slip structures. Circuit elements that form the building blocks for complex logic circuit design are introduced. Two different logic gate designs: OR gate and XOR gate are presented to demonstrate the usage of the building blocks introduced. Comment: 4 pages, 8 figures, EuCAS 201...|$|R
40|$|International audienceFailure free {{execution}} {{will become}} {{rare in the}} future <b>exascale</b> <b>computers.</b> Thus, fault tolerance is now an active field of research. In this paper, we study the impact of decomposing an application in much more parallelism that the physical parallelism on the rollback step of fault tolerant coordinated protocols. This over-decomposition gives the runtime a better opportunity to balance workload after failure without the need of spare nodes, while preserving performance. We show that the overhead on normal execution remains low for relevant factor of over-decomposition. With over-decomposition, restart execution on the remaining nodes after failures shows very good performance compared to classic decomposition approach: our experiments show that the execution time after restart can be reduced by 42 %. We also consider a partial restart protocol {{to reduce the amount}} of lost work in case of failure by tracking the task dependencies inside processes. In some cases and thanks to over-decomposition, this partial restart time can represent only 54 % of the global restart time...|$|R
40|$|The High-Q Club showcases {{applications}} which demonstrate scalability {{to effectively}} utilise all 458, 752 cores of the JUQUEEN 28 -rack Blue Gene/Q system at Jülich Supercomputing Centre. A {{broad spectrum of}} more than 24 application codes have qualified for membership, using up to 1. 8 million processes and/or threads. Seven application code-teams used the opportunity provided by the 2015 JUQUEEN Extreme Scaling workshop to (im) prove their code scalability [...] assisted by JUQUEEN and IBM technical support and experts from JSC Simulation Laboratories and Cross-sectional teams [...] and within the first 24 hours of dedicated access to the entire JUQUEEN resource, all 7 participating teams had adapted their codes and datasets to exploit the massive parallelism and restricted node memory for successful full-system executions. We compare {{the characteristics of the}} workshop and High-Q member codes, considering their strong and/or weak scaling, exploitation of hardware threading, and whether/how multi-threading is employed intra-node combined with message-passing. Scalability inhibitors such as inefficient use of limited compute node memory and file I/O are identified as key governing factors of applications on JUQUEEN which are expected to impact their ability to exploit expected <b>exascale</b> <b>computer</b> systems...|$|E
40|$|The {{advent of}} Advanced/Additive Manufacturing and the Materials Genome Initiative has placed {{significant}} emphasis on accelerating the qualification of new materials {{for use in}} real applications. Within these workflows lies both the engineering scale qualification through building and testing components at scale and full-scale modeling with integrated continuum computer codes and the materials scale qualification through revolutionary methods to nondestructively measure microstructure (3 DXRD) and physics specific experiments coupled with mesoscale mechanics simulations of the same physics specific experiment using the same microstructure. This {{is one of the}} use cases that drives the Exascale Materials Codesign Center (ExMatEx). The goal of the Codesign Center is very analogous to the acceleration of new materials deployment within the MGI, rather codesign accelerates the deploying of laboratory concepts for future computer components to enable a productive <b>exascale</b> <b>computer</b> system. To enable better mesoscale understanding in the continuum models, ExMatEx is creating a direct coupling between the continuum integrated code and direct numerical simulation of the mesoscale phenomena. Here, we review the ExMatEx project, its use cases and, in particular, the continuum 3 ̆c– 3 ̆e meso-scale coupling. *PIs: Jim Belak and Tim Germann [URL] [URL] Prepared by LLNL under Contract DE-AC 52 - 07 NA 27344...|$|E
40|$|The ASC Exascale Hardware Architecture {{working group}} is {{challenged}} to provide input {{on the following}} areas impacting the future use and usability of potential <b>exascale</b> <b>computer</b> systems: processor, memory, and interconnect architectures, {{as well as the}} power and resilience of these systems. Going forward, there are many challenging issues that will need to be addressed. First, power constraints in processor technologies will lead to steady increases in parallelism within a socket. Additionally, all cores may not be fully independent nor fully general purpose. Second, there is a clear trend toward less balanced machines, in terms of compute capability compared to memory and interconnect performance. In order to mitigate the memory issues, memory technologies will introduce 3 D stacking, eventually moving on-socket and likely on-die, providing greatly increased bandwidth but unfortunately also likely providing smaller memory capacity per core. Off-socket memory, possibly in the form of non-volatile memory, will create a complex memory hierarchy. Third, communication energy will dominate the energy required to compute, such that interconnect power and bandwidth will have a significant impact. All of the above changes are driven by the need for greatly increased energy efficiency, as current technology will prove unsuitable for exascale, due to unsustainable power requirements of such a system. These changes will have the most significant impact on programming models and algorithms, but they will be felt across all layers of the machine. There is clear need to engage all ASC working groups in planning for how to deal with technological changes of this magnitude. The primary function of the Hardware Architecture Working Group is to facilitate codesign with hardware vendors to ensure future exascale platforms are capable of efficiently supporting the ASC applications, which in turn need to meet the mission needs of the NNSA Stockpile Stewardship Program. This issue is relatively immediate, as there is only a small window of opportunity to influence hardware design for 2018 machines. Given the short timeline a firm co-design methodology with vendors is of prime importance...|$|E
40|$|My primary {{research}} interests lie in parallel I/O middleware, file systems and distributed systems. With the upcoming deployment of <b>Exascale</b> <b>computers,</b> a generic and intelligent high-level I/O interface are of pivotal importance for HPC scientific applications and the correspond-ing analysis and visualization tools. By identifying the existing problems with performance, scalability and usability of current I/O subsystems, my research {{efforts have been}} to provide software services for next generation HPC that can conveniently and efficiently interact with parallel storage systems such that computational scientists are not only able to delegate the I/O optimization to the high-level I/O interface during production cycle, but also quickly query and retrieve the data by using their analysis and visualization tools. The following sections outline my current research progress and future research directions. Current Research In the past two years, my research efforts have been dedicated on improving write performance for HPC scientific applications. We have explored different I/O optimization techniques, such as buffering and asynchronous function call for independent I/O operations. We have also developed various scheduling algorithms to improve average response time in collective I/O...|$|R
40|$|Climate change needs both {{high-resolution}} simulations {{for short-term}} prediction and the integration/coupling of several models for long-term climate projection. Exascale {{can be considered}} as a deep revolution on climate change applications, allowing highest resolutions of climate models that will surpass the resolution of today’s operational weather forecast models. This allows {{for the creation of a}} unified next-generation atmospheric simulation system, while sharing the expertise and know-how of both climate and weather communities. Climate and weather scientists can then collaborate on unified models, that strongly reduce uncertainty in climate prediction. However, legacy climate applications, in order to exploit exascale, require models re-engineering, improving computational kernels, new parallel model design and algorithms. Moreover, new models, dynamic grids and solution methods that have to be conceived on <b>exascale</b> <b>computers,</b> in charge of carrying out efficient operations. Climate change research Climate research community established that the configuration of a climate model consists in the definition of a trade-off among (i) complexity, as the range of model components involved (i. e., physical, chemical, and biological) and their interactions, (ii) prediction term/ensemble size and (iii) resolution (Figure 1) [1]...|$|R
40|$|For new <b>exascale</b> <b>computers</b> {{the degree}} of parallelismwill {{increase}} leading to architectures with more and morecores per node and multithreading and SIMD on each core. Dueto the limited memory per node the pure MPIparallelizing strategy may be no longer suited for these newarchitectures. For applications that already use a hybrid parallelization due tomemory limits, a pure MPI implementation will waste several cores forthe eigensolver unless a hybrid version can be used. Newly developed libraries for dense linear algebra computation such asthe ELPA library for the computation of {{a large part of}} theeigenspectrum of dense real symmetric and complex hermitean matricesthus offer versions with hybrid MPI and OpenMP parallelization. On Intel architectures {{it has been shown that}} this hybridparallelization can perform better than the pure MPI parallel version. OnBlueGene/Q the hybrid version is still experimental, but it showspromising results although the pure MPI version using all cores still performs better. In this talk we will present performance evaluation results of the two eigensolvers of the libraryELPA on BlueGene/Q and compare it to the results on the Intel Nehalemcluster JUROPA...|$|R
40|$|Development of new {{materials}} needs {{better understanding of}} the behavior of materials at nanoscale which involves accurate simulation of atomic and electronic interactions. Electronic structure is especially important when the atomic interactions involve breaking or formation of chemical bonds. When such interactions are present, first principles based ab-initio electronic structure calculations of atoms, which do not involve any empirical potentials, would be a suitable choice to study the behavior of materials at nanoscale. Such simulations involving many thousands of atoms are intractable by current software (especially for metals) due to their cubic scaling with respect to the system size. In this dissertation, the cubic scaling bottleneck is overcome by developing a linear scaling method amenable to massive parallelization. A linear scaling Density Functional Theory (DFT) framework has been developed using Clenshaw-Curtis Spectral Quadrature (SQ) method and implemented on massively parallel computers to simulate the electronic structure {{of hundreds of thousands of}} atoms. Finite difference representation has been employed in order to exploit the locality of electronic interactions in real space, enable systematic convergence and facilitate large-scale parallel implementation. In combination with linear scaling electrostatics, the electron density, energy and atomic forces can be calculated with effort that scales linearly with the number of atoms for both insulating and metallic systems. This method allows computation of the Γ-point and infinite-cell calculations without resorting to Brillouin zone integration or large supercells. The method is validated and systematic convergence of energy and forces to the exact diagonalization result is demonstrated. Convergence with respect to mesh size to established cubic scaling planewave results has also been shown. The efficiency and suitability of the method for high temperature calculations is also discussed. Energy and forces for systems with many thousands of atoms have been computed. The parallel scaling of the method to more than hundred thousand processors has been studied. The extreme parallelizability demonstrated by the method promises the potential to make use of the next generation <b>exascale</b> <b>computer</b> architectures for scientific simulations. In the spirit of massive parallelizability and efficiency, new extrapolation techniques have been developed to accelerate the convergence of fixed point iterations. These techniques when applied to basic iterative methods give rise to efficient solvers for linear systems of equations. Robust and efficient performance of these methods is demonstrated in acceleration of the non-linear fixed point iteration that is used to solve the electronic structure problem. The SQ method enables simulation of very large systems of both metals and insulators under a unified framework, at high temperatures. It also enables performing ab-initio molecular dynamics simulations at high temperatures which is impractical using cubic-scaling codes. This method also provides the basis on which an accurate simulation of the mechanics of materials at nanoscale can be performed in multi-scale modeling studies using coarse graining techniques. Ph. D...|$|E
40|$|The {{accurate}} {{modeling of}} complex physical phenomena, such as radiation transport {{in a laboratory}} experiment or a nuclear reactor, challenges the limits of modern computing resources and helps drive the requirement for next-generation <b>exascale</b> <b>computers.</b> The efficient and accurate propagation of uncertainty through these models {{is an area of}} ongoing research. Uncertainties in material properties contribute to the uncertainty in quantities of interest (QoIs) for which a problem is solved, but quantifying the uncertainty in a QoI is often prohibitively expensive. In this research an integrated approach to uncertainty quantification (UQ) for radiation transport problems with uncertain nuclear data is introduced. A novel dimension reduction method is applied to the nuclear data characterizing cross-section uncertainty. An adjoint-based sensitivity analysis is performed to yield sensitivity co-efficients for the QoI with respect to the reduced-dimensional space. Finally, response surfaces are constructed for the QoI over the reduced-dimensional input space. These surfaces yield information about the distribution of the QoI over the original uncertain input space. This multi-step approach is applied to several radiation transport problems for which traditional UQ methods are prohibitively expensive...|$|R
40|$|Aeronautics {{and space}} {{applications}} regarding for example aircraft, spacecraft or helicopter design on high performance computing (HPC) systems result in particular challenges regarding algorithm and software framework development. Algorithms must be robust, scalable {{and allow for}} highly accurate computations, in many cases for the solution of time-dependent and non-linear engineering problems. Software frameworks must exploit hardware parallelism on several levels, must be tunable for different, heterogeneous computing systems and must be easily to maintain. Engineering simulations often require coupling of several application components. This is a particular challenge for the underlying parallel algorithms, for example regarding convergence and communication behavior as well as load balancing. We presents a survey on aeronautics and space projects from German Aerospace Center (DLR), Simulation and Software Technology, which require use of HPC technology. These projects include solver development for quantum physics problems as well as pre- and postprocessing concepts for future <b>exascale</b> <b>computers,</b> helicopter and aircraft simulation as well a design, multi-disciplinary optimization for thermal management in spacecraft, experimental solution of aeronautic and space problems on adiabatic quantum annealers and application of HPC technology for space situational awareness (SSA) ...|$|R
40|$|This paper {{describes}} a methodology and tools {{to analyze and}} optimize the performance of task-based parallel applications. For illustrative purposes, a cutting-edge implementation of the Jacobi method aimed to address software challenges at <b>exascale</b> <b>computers</b> is evaluated. Specifically, the analysis was carried out on synchronous and asynchronous task-based implementations of the Jacobi method. The methodology consists of three basic steps: (i) performance analysis; (ii) prediction; and (iii) implementation. First, by instrumenting and tracing an application a general overview of its behavior can be obtained. The Paraver visualization tool enables the identification of performance bottlenecks or scalability problems. Secondly, {{with the help of}} prediction tools, such as Tareador and Dimemas, the inherent parallelism of the application is evaluated. Finally, the code is refactored to solve potential inefficiencies that prevent it to achieve higher performance. This final step is accomplished by using the OmpSs task-based parallel programming language. Results reported from using the methodology highlighted performance issues regarding to memory access, synchronization among the threads, and processors with long waiting periods. Additionally, the OmpSs implementation enabled the parallel execution of core functions of the application inside each thread, therefore obtaining a greater utilization of the computational resources...|$|R
40|$|This article {{provides}} background information about interconnection networks, {{an analysis of}} previous developments, and {{an overview of the}} state of the art. The main contribution of this article is to highlight the importance of the interpolation and extrapolation of technological changes and physical constraints in order to predict the optimum future interconnection network. The technological changes are related to three of the most important attributes of interconnection networks: topology, routing, and flow-control algorithms. On the other hand, the physical constraints, that is, port counts, number of communication nodes, and communication speed, determine the realistic properties of the network. We present the state-of-the-art technology for the most commonly used interconnection networks and some background related to often-used network topologies. The interconnection networks of the best-performing petascale parallel computers from past and present Top 500 lists are analyzed. The lessons learned from this analysis indicate that computer networks need better performance in future <b>exascale</b> <b>computers.</b> Such an approach leads to the conclusion that a high-radix topology with optical connections for longer links is set to become the optimum interconnect for a number of relevant application domains. Peer ReviewedPostprint (published version...|$|R
40|$|<b>Exascale</b> level <b>computers</b> {{might be}} {{available}} {{in less than a}} decade. Computer architects are already thinking of, and planning to achieve such levels of performance. It is reasonable to expect that researchers and engineers will carry out scientific and engineering computations more complex than ever before, and will attempt breakthroughs not possible today. If the size of the problems solved on such machines scales accordingly, we may face new issues related to precision, accuracy, performance, and programmability. The paper examines some relevant aspects of this problem. I...|$|R
40|$|Abstract. <b>Exascale</b> <b>computers</b> are {{expected}} to exhibit an unprecedented level of complexity, thereby posing significant challenges for porting applications to these new systems. One {{of the ways to}} support this transition is to create tools that allow their users to benefit from prior successful porting experiences. The key to such an approach is the manner in which we define source code similarity, and whether similar codes can be ported in the same way to a given system. In this paper, we propose a novel approach based on the notion of similarity that uses static and dynamic code features to check if two serial subroutines can be ported with the same OpenMP strategy. Our approach creates an annotated family distance tree based on the syntactic structure of subroutines, where subroutines that belong to the same syntactic family and share the similar code features have a greater potential to be optimized in the same way. We describe the design and implementation of a tool, based upon a compiler and performance tool, that is used to gather the data to build this porting planning tree. We then validate our approach by analyzing the similarity in subroutines of the serial version of the NAS benchmarks and comparing how they were ported in the OpenMP version of the suite. ...|$|R
40|$|To {{support the}} ever {{increasing}} demand of scientific computations, today’s High Performance Computing (HPC) systems have {{large numbers of}} computing elements running in parallel. Petascale computers, which are capable of reaching a performance in excess of one PetaFLOPS (1015 floating point operations per second), are successfully deployed and used {{at a number of}} places. <b>Exascale</b> <b>computers</b> with one thousand times the scale and computing power are projected to become available in less than 10 years. Reliability {{is one of the major}} challenges faced by exascale computing. With hundreds of thousands of cores, the mean time to failure is measured in minutes or hours instead of days or months. Failures are bound to happen during execution of HPC applications. Current fault recovery techniques focus on reactive ways to mitigate faults. Central to any kind of fault recovery method is the challenge of detecting faults and propagating this knowledge. The first half of this thesis work contributes to fault detection capabilities at the MPI-level. We propose two principle types of fault detection mechanisms: the first one uses periodic liveness checks while the second one makes on-demand liveness checks. These two techniques are experimentally compared for the overhead imposed on MPI applications. Checkpoint and restart (CR) recovery is one of the fault recovery methods which is used t...|$|R
40|$|High {{performance}} computing {{is becoming an}} essential tool for the development and optimal deployment of renewable energy generation and storage technologies. The Energy oriented Centre of Excellence (EoCoE) brings together four different scientific communities (meteo 4 energy, water 4 energy, materials 4 energy and fusion 4 energy) which already rely heavily on HPC to make advances in, for example, wind-, solar-, hydro- and geothermal power, nuclear fusion and high-capacity batteries. Harnessing its full potential, on the other hand, requires making efficient use of the upcoming <b>exascale</b> <b>computers</b> (hierarchical architectures {{with a large number}} of multicore nodes), and this in turn means that applications need to be significantly modified if they are to reach the expected performance level. A key paradigm in EoCoE is to foster close, sustainable collaborations between HPC experts and scientists from the four energy application domains. This is achieved through a combination specialized workshops, brokered bilateral links ('code teams') and shared collaborative platforms. In most cases this begins with a performance evaluation of the application using the in-house benchmarking package JUBE to automate the extraction of key metrics in a strictly reproducible manner. Performance can then be periodically re-evaluated after optimisation or more substantial refactoring - such as the introduction of a new linear algebra solver - in order to monitor the impact of these developments on the application in realistic production scenarios...|$|R
40|$|It is {{commonly}} agreed that highly parallel software on <b>Exascale</b> <b>computers</b> will suffer from many more runtime failures {{due to the}} decreasing trend in the mean time to failures (MTTF). Therefore, {{it is not surprising}} that a lot of research is going on in the area of fault tolerance and fault mitigation. Applications should survive a failure and/or be able to recover with minimal cost. MPI is not yet very mature in handling failures, the User-Level Failure Mitigation (ULFM) proposal being currently the most promising approach is still in its prototype phase. In our work we use GASPI, which is a relatively new communication library based on the PGAS model. It provides the missing features to allow the design of fault-tolerant applications. Instead of introducing algorithm-based fault tolerance in its true sense, we demonstrate how we can build on (existing) clever checkpointing and extend applications to allow integrate a low cost fault detection mechanism and, if necessary, recover the application on the fly. The aspects of process management, the restoration of groups and the recovery mechanism is presented in detail. We use a sparse matrix vector multiplication based application to perform the analysis of the overhead introduced by such modifications. Our fault detection mechanism causes no overhead in failure-free cases, whereas in case of failure(s), the failure detection and recovery cost is of reasonably acceptable order and shows good scalability...|$|R
40|$|<b>Exascale</b> <b>computers</b> {{will enable}} the {{unraveling}} of significant scientific mysteries. Predictions are that by 2019, supercomputers will reach exascales with millions of nodes and billions of threads of execution. Many-task computing (MTC) is a new viable distributed paradigm for extreme-scale supercomputing. The MTC paradigm can address {{four of the five}} major challenges of exascale computing, namely concurrency, resilience, heterogeneity, and I/O and memory; this work specifically addresses the first three major challenges. This paper presents a new light-weight and scalable discrete event simulator, SimMatrix, that enables the exploration of distributed scheduling for MTC workloads at exascale levels with up to 1 million nodes and 1 billion cores. SimMatrix is validated against real MTC workloads executed under Falkon at petascale levels, with 40 K nodes and 160 K-cores. Centralized scheduling is compared and contrasted to distributed scheduling; this work adopts work stealing, as an efficient and scalable approach to distributed load balancing. It explores a wide range of parameters important to understand work stealing at exascale levels, such as number of tasks to steal, number of neighbors of a node, static or dynamic neighbors, and different workloads. Experiment results show that the centralized scheduling saturates at small number of nodes, while the distributed scheduler configured with optimal parameters could scale up to 1 million nodes and 1 billion cores without any explicit upper bound. SimMatrix is light-weight and scalable, having been tested up to 1 billion cores and 10 billion tasks with modest resources (e. g. 200 GB of memory and 256 -core hours) ...|$|R
40|$|The {{transition}} from petascale to <b>exascale</b> <b>computers</b> {{is characterized by}} substantial changes in the computer architectures and technologies. The research community relying on computational simulations is being forced to revisit the algorithms for data generation and analysis due to various concerns, such as higher degrees of concurrency, deeper memory hierarchies, substantial I/O and communication constraints. Simulations today typically save all data to analyze later. Simulations at the exascale will require us to analyze data as it is generated and save only what is really needed for analysis, which must be performed predominately in-situ, i. e., executed sufficiently fast locally, limiting memory and disk usage, and avoiding the need to move large data across nodes. In this paper, we present a distributed method that enables in-situ data analysis for large protein folding trajectory datasets. Traditional trajectory analysis methods currently follow a centralized approach that moves the trajectory datasets to a centralized node and processes the data only after simulations have been completed. Our method, on the other hand, captures conformational information in-situ using local data only while reducing the storage space needed for {{the part of the}} trajectory under consideration. This method processes the input trajectory data in one pass, breaks from the centralized approach of traditional analysis, avoids the movement of trajectory data, and still builds the global knowledge on the formation of individual α-helices or β-strands as trajectory frames are generated. Comment: 40 pages, 15 figures, this paper is presently in the format request of the journal to which it was submitted for publicatio...|$|R
