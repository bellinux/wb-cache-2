964|76|Public
5|$|The {{physical}} and biological mechanism of LTP {{is still not}} understood, but some successful models have been developed. Studies of dendritic spines, protruding structures on dendrites that physically grow and retract {{over the course of}} minutes or hours, have suggested a relationship between the electrical resistance of the spine and the effective synapse strength, due to their relationship with intracellular calcium transients. Mathematical models such as BCM Theory, which depends also on intracellular calcium in relation to NMDA receptor voltage gates, have been developed since the 1980s and modify the traditional a priori <b>Hebbian</b> <b>learning</b> model with both biological and experimental justification. Still others have proposed re-arranging or synchronizing the relationship between receptor regulation, LTP, and synaptic strength.|$|E
25|$|The main {{categories}} of networks are acyclic or feedforward neural networks (where the signal passes {{in only one}} direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks {{can be applied to}} the problem of intelligent control (for robotics) or learning, using such techniques as <b>Hebbian</b> <b>learning,</b> GMDH or competitive learning.|$|E
25|$|Donald Olding Hebb FRS (July 22, 1904 – August 20, 1985) was a Canadian {{psychologist}} who was {{influential in the}} area of neuropsychology, where he sought to understand how the function of neurons contributed to psychological processes such as learning. He {{is best known for his}} theory of <b>Hebbian</b> <b>learning,</b> which he introduced in his classic 1949 work The Organization of Behavior. He has been described as the father of neuropsychology and neural networks. A Review of General Psychology survey, published in 2002, ranked Hebb as the 19th most cited psychologist of the 20th century.|$|E
50|$|<b>Hebbian</b> {{associative}} <b>learning</b> {{and memory}} maintenance depends on synaptic normalization mechanisms to prevent synaptic runaway. Where synaptic runaway describes overcrowding of dendritic associations, which reduce sensory or behavioural acuteness {{proportional to the}} level of synaptic runaway. Synaptic/neuronal normalization refers to synaptic competition, where the prosper of one synapse may weakening the efficacy of other nearby surrounding synapses with redundant neurotransmission.|$|R
40|$|<b>Hebbian</b> {{associative}} <b>learning</b> is {{a common}} form of neuronal adaptation {{in the brain and}} is important for many physiological functions such as motor learning, classical conditioning and operant conditioning. Here we show that a <b>Hebbian</b> associative <b>learning</b> synapse is an ideal neuronal substrate for the simultaneous implementation of high-gain adaptive control (HGAC) and model-reference adaptive control (MRAC), two classical adaptive control paradigms. The resultant dual adaptive control (DAC) scheme is shown to achieve superior tracking performance compared to both HGAC and MRAC, with increased convergence speed and improved robustness against disturbances and adaptation instability. The relationships between convergence rate and adaptation gain/error feedback gain are demonstrated via numerical simulations. According to these relationships, a tradeoff between the convergence rate and overshoot exists with respect to the choice of adaptation gain and error feedback gain. National Institutes of Health (U. S.) (HL 072849) National Institutes of Health (U. S.) (HL 067966) National Institutes of Health (U. S.) (EB 005460...|$|R
5000|$|A further {{application}} of physical neural network {{is shown in}} U.S. Patent No. 7,412,428 entitled [...] "Application of <b>hebbian</b> and anti-hebbian <b>learning</b> to nanotechnology-based physical neural networks," [...] which issued on August 12, 2008.|$|R
25|$|In 2009, {{a simple}} {{electronic}} circuit consisting of an LC network and a memristor {{was used to}} model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, {{in collaboration with the}} Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale neural network model to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011 they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from <b>Hebbian</b> <b>learning</b> rule.|$|E
2500|$|In {{the late}} 1940s, D.O. Hebb created a {{learning}} hypothesis {{based on the}} mechanism of neural plasticity that {{is now known as}} <b>Hebbian</b> <b>learning.</b> <b>Hebbian</b> <b>learning</b> is an unsupervised learning rule. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines [...]|$|E
2500|$|Earlier {{models of}} memory are {{primarily}} {{based on the}} postulates of <b>Hebbian</b> <b>learning.</b> Biologically relevant models such as [...] Hopfield net {{have been developed to}} address the properties of associative, rather than content-addressable, style of memory that occur in biological systems. These attempts are primarily focusing on the formation of medium- and long-term memory, localizing in the hippocampus. Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory.|$|E
40|$|Abstract—We {{present a}} class of {{time-delay}} anisotropic diffusion models for image restoration. These models lead to asymptotic states that are {{selected on the basis}} of a contrast parameter and bear some analogy with neural networks with <b>Hebbian</b> dynamical <b>learning</b> rules. Numerical examples show that these models are efficient in removing even high levels of noise, while allowing an accurate tracking of the edges. Index Terms — Adaptive neural network, image restoration, nonlinear diffusion, selective filter, time-delay regularization. I...|$|R
40|$|Abstract. We {{propose a}} network model of spiking neurons, without preimposed {{topology}} and driven by STDP (Spike-Time-Dependent Plasticity), a temporal <b>Hebbian</b> unsupervised <b>learning</b> mode, biologically observed. The model is further {{driven by a}} supervised learning algorithm, based on a margin criterion, that effects the synaptic delays linking the network to the output neurons, with classification as a goal task. The network processing and the resulting performance are completely explainable by the concept of polychronization, proposed by Izhikevich [1]. The model emphasizes the computational capabilities of this concept. ...|$|R
40|$|International audienceWe {{present the}} {{experimental}} realization of an adaptive polymeric system displaying a "learning behaviour". The system consists on a statistically organized networks of memristive elements (memory-resitors) based on polyaniline. In a such network the path {{followed by the}} current increments its conductivity, a property which makes the system able to mimic <b>Hebbian</b> type <b>learning</b> and have application in hardware neural networks. After discussing the working principle of single memristive element developed in our lab we present realization and the relative experimental results a statistically organized polymeric adaptive network...|$|R
2500|$|Criticism of Neural [...] "Darwinism" [...] {{was made}} by Francis Crick {{on the basis that}} {{neuronal}} groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. [...] A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended by Fernando in a later paper [...] In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding <b>Hebbian</b> <b>learning</b> to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.|$|E
5000|$|... #Subtitle level 2: <b>Hebbian</b> <b>learning</b> {{account of}} mirror neurons ...|$|E
5000|$|... #Subtitle level 3: <b>Hebbian</b> <b>learning</b> {{rule for}} Hopfield {{networks}} ...|$|E
40|$|International audienceWe {{propose a}} network model of spiking neurons, without preimposed {{topology}} and driven by STDP (Spike-Time-Dependent Plasticity), a temporal <b>Hebbian</b> unsupervised <b>learning</b> mode, biologically observed. The model is further {{driven by a}} supervised learning algorithm, based on a margin criterion, that has effect on the synaptic delays linking the network to the output neurons, with classification as a goal task. The network processing and the resulting performance are completely explainable by the concept of polychronization, proposed by Izhikevich (2006). The model emphasizes the computational capabilities of this concept...|$|R
40|$|This paper {{describes}} and motivates six {{principles for}} computational cognitive neuroscience models: biological realism, distributed representations, inhibitory competition, bidirectional activation propagation, errordriven task <b>learning,</b> and <b>Hebbian</b> model <b>learning.</b> Although these principles {{are supported by}} a number of cognitive, computational, and biological motivations, the prototypical neural network model (a feedforward backpropagation network) incorporates only two of them, and no widely used model incorporates all of them. This paper argues that these principles should be integrated into a coherent overall framework, and discusses some potential synergies and conflicts in doing so...|$|R
40|$|Abstract — The paper {{presents}} a neural model for learning sequences of relevant patterns embedded in distractors. A contextual episode is {{a sequence of}} relevant patterns – always {{in the same order}} – intermixed with distractors. By repeated presentations of all contextual episodes, the model discovers for each episode the set of relevant patterns and their order. The problem is solved in two stages: (a) by eliminating distractors, and (b) by learning the order between relevant patterns. The model uses the concept of latent attractors- essential in creating different neural representations for same patterns in distinct episodes. No external teacher and only <b>Hebbian</b> type <b>learning</b> rules are used. I...|$|R
50|$|In {{the late}} 1940s, D.O. Hebb created a {{learning}} hypothesis {{based on the}} mechanism of neural plasticity that {{is now known as}} <b>Hebbian</b> <b>learning.</b> <b>Hebbian</b> <b>learning</b> is an unsupervised learning rule. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.|$|E
50|$|The symmetric, {{midpoint}} {{version of}} GeneRec {{is equivalent to}} the contrastive <b>Hebbian</b> <b>learning</b> algorithm (CHL).|$|E
50|$|<b>Hebbian</b> <b>learning</b> is {{performed}} using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels.|$|E
40|$|Six {{principles}} for biologically based computational models of cortical cognition Randall C. O’Reilly This review describes and motivates six {{principles for}} computational cognitive neuroscience models: biological realism, distributed representations, inhibitory competition, bidirectional activation propagation, error-driven task <b>learning,</b> and <b>Hebbian</b> model <b>learning.</b> Although these principles {{are supported by}} a number of cognitive, computational and biological motivations, the prototypical neural-network model (a feedforward back-propagation network) incorporates only two of them, and no widely used model incorporates all of them. It is argued here that these principles should be integrated into a coherent overall framework, and some potential synergies and conflicts in doing so are discussed. A number of important principles have been develope...|$|R
40|$|An {{unsupervised}} learning model for developing L/M specific wiring at the ganglion cell level {{would support the}} research indicating L/M specific wiring at the ganglion cell level (Reid and Shapley, 2002). Removing the contributions to the surround from cells of the same cone type improves the signal-to-noise ratio of the chromatic signals. The {{unsupervised learning}} model used is <b>Hebbian</b> associative <b>learning,</b> which strengthens the surround input connections according to the correlation of the output with the input. Since the surround units of the same cone type as the center are redundant with the center, their weights end up disappearing. This process {{can be thought of}} as a general mechanism for eliminating unnecessary cells in the nervous system...|$|R
40|$|Spike Timing Dependent Plasticity (STDP) is a <b>Hebbian</b> like {{synaptic}} <b>learning</b> rule. The {{basis of}} STDP has strong experimental evidences and {{it depends on}} precise input and output spike timings. In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule. Comment: On Computational Neuroscience/ 11 page...|$|R
50|$|<b>Hebbian</b> <b>{{learning}}</b> {{is part of}} the framework, {{in which}} the event of learning physically alters neurons and connections, as learning takes place.|$|E
50|$|As can be {{seen this}} is very much like a small brain ruled by the theory of <b>Hebbian</b> <b>learning</b> (Kjellström, 1996, 1999 and 2002).|$|E
50|$|Work in the {{laboratory}} of Eric Kandel has provided evidence for the involvement of <b>Hebbian</b> <b>learning</b> mechanisms at synapses in the marine gastropod Aplysia californica.|$|E
40|$|Abstract. Taking {{a global}} analogy with the {{structure}} of perceptual biological systems, we present a system composed of two layers of real-valued sigmoidal neurons. The primary layer receives stimulating spatiotemporal signals, and the secondary layer is a fully connected random recurrent network. This secondary layer spontaneously displays complex chaotic dynamics. All connections have a constant time delay. We use for our experiments a <b>Hebbian</b> (covariance) <b>learning</b> rule. This rule slowly modifies the weights {{under the influence of}} a periodic stimulus. The effect of learning is twofold: (i) it simplifies the secondary-layer dynamics, which eventually stabilizes to a periodic orbit; and (ii) it connects the secondary layer to the primary layer, and realizes a feedback from the secondary to the primar...|$|R
40|$|Abstract. We {{propose a}} novel network model of spiking neurons, without preimposed {{topology}} and driven by STDP (Spike-Time-Dependent Plasticity), a temporal <b>Hebbian</b> unsupervised <b>learning</b> mode, based on biological observations of synaptic plasticity. The model is further {{driven by a}} supervised learning algorithm, based on a margin criterion, that has effect on the synaptic delays linking the network to the output neurons, with classification as a goal task. The network processing and the resulting performance are completely explainable by the concept of polychronization, recently introduced by Izhikevich [9]. On the one hand, our model {{can be viewed as}} a new machine learning concept for classifying patterns by means of spiking neuron networks. On the other hand, as a model of natural neural networks, it provides a new insight on cel...|$|R
40|$|In this work, {{we study}} the dynamic range in a {{neuronal}} network modelled by cellular automaton. We consider deterministic and non-deterministic rules to simulate electrical and chemical synapses. Chemical synapses have an intrinsic time-delay and {{are susceptible to}} parameter variations guided by <b>learning</b> <b>Hebbian</b> rules of behaviour. Our results show that chemical synapses can abruptly enhance sensibility of the neural network, a manifestation that can become even more predominant if learning rules of evolution are applied to the chemical synapses...|$|R
50|$|Some of the {{synaptic}} changes {{observed by}} Kandel's laboratory provide examples of Hebbian theory. One {{article describes the}} role of <b>Hebbian</b> <b>learning</b> in the Aplysia siphon-withdrawal reflex.|$|E
5000|$|P. Häfliger and M. Mahowald: [...] "Spike based {{normalizing}} <b>hebbian</b> <b>learning</b> in {{an analog}} VLSI artificial neuron", Analog Integrated Circuits and Signal Processing, 18:(2/3) 133-140, Feb, 1999 ...|$|E
50|$|The {{classical}} {{example of}} unsupervised {{learning in the}} study of both natural and artificial neural networks is subsumed by Donald Hebb's principle, that is, neurons that fire together wire together. In <b>Hebbian</b> <b>learning,</b> the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). <b>Hebbian</b> <b>Learning</b> has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.|$|E
50|$|Leabra {{stands for}} Local, Error-driven and Associative, Biologically Realistic Algorithm. It {{is a model}} of {{learning}} which is a balance between <b>Hebbian</b> and error-driven <b>learning</b> with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models.This algorithm is the default algorithm in emergent (successor of PDP++) when making a new project, and is extensively used in various simulations.|$|R
40|$|We {{propose a}} multi-timescale {{learning}} rule for spiking neuron networks, {{in the line}} of the recently emerging field of reservoir computing. The reservoir is a network model of spiking neurons, with random topology and driven by STDP (Spike-Time-Dependent Plasticity), a temporal <b>Hebbian</b> unsupervised <b>learning</b> mode, biologically observed. The model is further driven by a supervised learning algorithm, based on a margin criterion, that affects the synaptic delays linking the network to the readout neurons, with classification as a goal task. The network processing and the resulting performance {{can be explained by the}} concept of polychronization, proposed by Izhikevich (2006, Neural Computation, 18 : 2), on physiological grounds. The model emphasizes that polychronization can be used as a tool for exploiting the computational power of synaptic delays and for monitoring the topology and activity of a spiking neuron network...|$|R
40|$|A general {{approach}} for sparse signal decomposition is matching pursuit with an over-complete dictionary of features. It {{has been demonstrated}} that this gives efficient codes when applied to images and acoustic signals. This thesis implements an on-line version of matching pursuit with <b>Hebbian</b> dictionary <b>learning,</b> which is theoretically able to process vibration and acoustic emission signals for condition monitoring of, for example, bearings. The implementation is done in C and critical parts are identified and implemented in VHDL {{with the goal of}} synthesising those parts on an FPGA. The C implementation can handle data in two formats, both fixed and floating point and is verified to be functioning in both cases. The VHDL components are verified in simulation. The maximum processing speed possible for the designed system is expected be on the order of one million samples per second, 1 Msps. Validerat; 20130822 (global_studentproject_submitter...|$|R
