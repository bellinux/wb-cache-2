17|49|Public
50|$|RP-570 is a {{communications}} protocol used in industrial environments to communicate between a <b>front-end</b> <b>computer</b> and the substation to be controlled.|$|E
50|$|Computer-assisted {{telephone}} interviewing (CATI) is {{an interactive}} <b>front-end</b> <b>computer</b> {{system that allows}} interviewers to ask questions via computer rather than over the telephone. This system saves time, decreases costs, and helps guarantees high quality data.|$|E
5000|$|The Patriot Ledger {{was also}} among the first papers {{in the nation to}} {{establish}} zoned editions for local news and advertising, exchanging journalists with foreign countries, transmitting news copy and page layouts by facsimile, using a <b>front-end</b> <b>computer</b> editing system, installing a two-way radio system for spot news coverage, pioneering the use of 35-millimeter photography and setting up a [...] "little merchants" [...] carrier system.|$|E
40|$|The Thomas Jefferson National Accelerator Facility (Jefferson Lab) {{control system}} uses a nameserver to reduce system {{response}} time and to minimize the impact of client name resolution on <b>front-end</b> <b>computers.</b> The control system {{is based on the}} Experimental Physics and Industrial Control System (EPICS), which uses name-based broadcasts to initiate data communication. By default, when EPICS process variables (PV) are requested by client applications, all <b>front-end</b> <b>computers</b> receive the broadcasts and perform name resolution processing against local channel name lists. The nameserver is used to offload the name resolution task to a single node. This processing, formerly done on all <b>front-end</b> <b>computers,</b> is now done only by the nameserver. In a control system with heavily loaded <b>front-end</b> <b>computers</b> and high peak client connection loads, a significant performance improvement is seen. This paper describes the name server in more detail, and discusses {{the strengths and weaknesses of}} making name resolution a centralized service. Comment: ICALEPCS 200...|$|R
40|$|The Thomas Jefferson National Accelerator Facility {{control system}} uses a nameserver to reduce system {{response}} time and to minimize the impact of client name resolution on <b>front-end</b> <b>computers.</b> The control system {{is based on the}} Experimental Physics and Industrial Control System (EPICS), which uses name-based broadcasts to initiate data communication. By default, when EPICS process variables are requested by client applications, all <b>front-end</b> <b>computers</b> receive the broadcasts and perform name resolution processing against local channel name lists. The nameserver is used to offload the name resolution task to a single node. This processing, formerly done on all <b>front-end</b> <b>computers,</b> is now done only by the nameserver. In a control system with heavily loaded <b>front-end</b> <b>computers</b> and high peak client connection loads, a significant performance improvement is seen. This paper describes the name server in more detail, and discusses {{the strengths and weaknesses of}} making name resolution a centralized service. This work was supported by the U. S. DOE contract No. DE-AC 05 - 84 ER 40150...|$|R
40|$|A new {{generation}} of PowerPC VMEbus <b>front-end</b> <b>computers</b> is being introduced in the CERN accelerators and services control system infrastructure. This new technology is aimed at replacing existing PC based <b>front-end</b> <b>computers</b> and at offering a high performance microprocessor platform for present and future engineering developments for the LHC era. This paper describes the re-engineering strategy and the core architecture of the new systems. Special performance issues are also addressed in this paper...|$|R
50|$|CRC {{developed}} a unique object-oriented control algorithm building approach. Even though the Z80 microprocessor {{of the time}} ran at only 1 MHz, and had a memory addressing range of 64 Kbytes, the University of Melbourne system out-performed other automations systems of the time significantly. CRC created a <b>front-end</b> <b>computer</b> that used ten processors to share the task, having access to up to 256 pages of shared memory. Each SIU used two Z80 processors. This allowed the intelligence to be shared, and many decisions to be made locally thus further reducing the load {{on the front end}} and reducing communications.The University of Melbourne system was a leader in the development of distributed intelligence DDC system. Out of CRC Midac System Pty Limited was formed in 1982, and went on to develop and implement more leading DDC technology included systems where the <b>front-end</b> <b>computer</b> was a CPM operating system MZ3500 Sharp personal computer. The Sharp was chosen because of its much superior graphics compared with the early IBM PC of the early 1980s.|$|E
50|$|The initial model, the Cray-1A, weighed 5.5 tons {{including}} the Freon refrigeration system. Configured with 1 million words of main memory, {{the machine and}} its power supplies consumed about 115 kW of power; cooling and storage likely more than doubled this figure. A Data General SuperNova S/200 minicomputer served as the maintenance control unit (MCU), which was used to feed the Cray Operating System into the system at boot time, to monitor the CPU during use, and optionally as a <b>front-end</b> <b>computer.</b> Most, if not all Cray-1As were delivered using the follow-on Data General Eclipse as the MCU.|$|E
40|$|Abstract—The time {{synchronization}} in {{the virtual}} radio system is determined as one of thorny technical problems. Especially in TT&C (Telemetry、Tracking and Command) system, it requires incredible precision with time. A novel method for time synchronization by high-precision in a unified virtual radio model specially designed is derived based on AD/DA cache and software cache in the present study. The unified virtual radio model with distributed frame is designed for most applications of virtual radio. And on this basis, the time synchronization strategy is detailed on time synchronization strategy from two designs of daughter card and <b>front-end</b> <b>computer</b> software. Finally, it is also discussed preliminary that time delay of demodulation for this model {{have an effect on}} radio system (TT&C). Keywords-component; system of signal processing; Aerospace Measurement; Aerospace control; virtual radio; Time synchronizatio...|$|E
40|$|Today's {{control system}} {{architectures}} will be explained, using their historical development and advances in computer technologies as a guide. These developments will be documented with {{the example of}} CERN's accelerator control systems. Having an overview of current systems allows {{a closer look at}} the constituents: operator consoles, <b>front-end</b> <b>computers,</b> embedded processors and the networks and buses interconnecting them...|$|R
50|$|The {{experiment}} complexity {{ranges from}} test systems, where a single PC {{is connected to}} CAMAC via a PC-CAMAC interface, to experiments with several <b>front-end</b> <b>computers</b> and analysis nodes. The system currently runs under Linux, MS Windows, various versions of UNIX, VMS, VxWorks and MS-DOS and can be ported easily to virtually any operating system which supports TCP/IP sockets.|$|R
40|$|In {{this report}} an {{introduction}} to the Connection Machine model- 2 implementation and programming is given. The Connection Machine system is an integrated combination of hardware and software designed for high- speed data parallel computing. The hardware elements of the CM- 2 system include <b>front-end</b> <b>computers</b> that provide the development and execution environments for the system software, a parallel processing unit of 64 K (65536) processors that execute the data parallel operations, and a high- performance data parallel I/O system. Operations can take place in parallel on data in all the processors. The system software is based upon the operating system or environment of the <b>front-end</b> (host) <b>computer.</b> Users can program using familiar languages and programming constructs, with all the development tools provided by the front en...|$|R
40|$|The overall {{architecture}} of the multipurpose parallel-processing Navier-Stokes Computer (NSC) being developed by Princeton and NASA Langley (Nosenchuck et al., 1986) is described and illustrated with extensive diagrams, and the NSC implementation of an elementary multigrid algorithm for simulating isotropic turbulence (based on solution of the incompressible time-dependent Navier-Stokes equations with constant viscosity) is characterized in detail. The present NSC design concept calls for 64 nodes, each {{with the performance of}} a class VI supercomputer, linked together by a fiber-optic hypercube network and joined to a <b>front-end</b> <b>computer</b> by a global bus. In this configuration, the NSC would have a storage capacity of over 32 Gword and a peak speed of over 40 Gflops. The multigrid Navier-Stokes code discussed would give sustained operation rates of about 25 Gflops...|$|E
40|$|A {{data-driven}} {{method for}} error detection and fault diagnosis in processor arrays is proposed {{under the assumption}} that data streams can only be inserted and observed through the boundary processors. The method consists of attaching tags to data streams, thereby allowing the data items to carry their own control and error information. Our goal is to detect the malfunction of a specific processor at a specific time step. A tag, which initially contains control information to activate a testing process, is changed to indicate the occurrence of an error by a checking processor detecting an inconktency. To pinpoint a faulty processor, the <b>front-end</b> <b>computer</b> must go through the reverse process of identifying the processor that detected and signalled the inconsistency. Using two data streams, we can control every processor in the array and locate the faulty one. The resulting processor array is regular in structure, and the number of bits used to encode the control and error information i...|$|E
40|$|This {{paper will}} present the {{requirements}} and design of the Timing Synchronization System for the Continuous Electron Beam Accelerator Facility control system at Thomas Jefferson National Accelerator Facility. A clock module {{has been designed to}} reside in a VME crate with a master <b>front-end</b> <b>computer</b> and communicate with the Data Acquisition VME crates and their front-end computers via a serial fiber optic line. Configuration of the clock modules is jumper and software selectable. The application that motivated the development of the Timing Synchronization System, the Accelerator 30 Hz System, will also be presented. This system needs less than 1 ms time differential between the data acquisitions on the various DAQ front-end computers in order to gather correlated information. The development of and our operational experience with this application using the new timing synchronization system will be discussed. *This work was supported by the U. S. DOE contract No. DE-AC 05 - 84 -ER 4015...|$|E
40|$|The {{control system}} for the CERN 26 GEV Proton Synchrotron and its {{injectors}} is a generic system which {{can be adapted to}} other accelerators. Most configuration data are in a relational database. From these data we can generate object interfaces for equipment, configuration files for <b>front-end</b> <b>computers,</b> a read-only database for accelerator control interfacing, and full dynamic documentation on the Web. The database is also used in real time for runtime references and archives, and for the working data of several programs...|$|R
50|$|As COS {{was written}} by ex-Control Data employees, its command {{language}} and internal organization bore strong resemblance to the SCOPE operating system on the CDC 7600 and before that EXEC*8 from CDC's earlier ERA/Univac pedigree. User jobs were submitted to COS via <b>front-end</b> <b>computers</b> via a high-speed channel interface, and so-called station software. Front end stations were typically large IBM or Control Data mainframes. However the DEC VAX was also a very popular front-end. Interactive use of COS was possible through the stations, but most users simply submitted batch jobs.|$|R
40|$|The ELETTRA {{control system}} <b>front-end</b> <b>computers</b> are {{presently}} based on 68 k VME boards and the OS- 9 operating system. In {{view of the}} construction of the new booster injector and of a smooth upgrade of the existing control system, PowerPC VME boards running Linux have been adopted. The new platform provides reliability, performance and flexibility, while the RTAI (Real Time Application Interface) extension offers, where necessary, satisfying real-time capabilities that compete well with those of the most popular real-time operating systems. This article describes the main issues associated to the choices we made and presents an example of application...|$|R
40|$|The Beam Loss Monitor (BLM) System is {{designed}} to prevent the quenching of RHIC magnets due to beam loss, provide quantitative loss data, and the loss history {{in the event of}} a beam abort. The system uses 400 ion chambers of a modified Tevatron design. To satisfy fast (single turn) and slow (100 msec) loss beam criteria and provide sensitivity for studies measurements, a range of over 8 decades is needed. An RC pre-integrator reduces the dynamic range for a low current amplifier. This is digitized for data logging. The output is also applied to an analog multiplier which compensates the energy dependence, extending the range of the abort comparators. High and low pass filters separate the signal to dual comparators with independent programmable trip levels. Up to 64 channels, on 8 VME boards, are controlled by a micro-controller based VME module, decoupling it from the <b>front-end</b> <b>computer</b> (FEC) for real-time operation. Results with the detectors in the RHIC Sextant Test and the electronics in the AGS-to-RHIC (AtR) transfer line will be presented...|$|E
40|$|The new CPS {{controls}} system started {{operation in}} 1980 and is gradually being extended to control sev-eral accelerators and beam {{lines of the}} CPS complex. Part {{of the system and}} growing with it is a data-base driven alarm system. In each <b>front-end</b> <b>computer,</b> a program monitors the equipment, another the CAMAC in-terface and a third the beam currents. These programs send data to a central computer. In general-purpose consoles, a screen and touch-panel are dedicated to the alarm system. The alarms are displayed in various formats and levels of detail according to the wishes of the operator, who can also attempt automatic reset of individual faults or groups of related faults. Ex-cept for beam current monitoring, no details about the equipment are coded in the programs. The list of the equipment to be checked and information about it are stored in an off-line data base from which the necessary real-time files can be derived automatical-ly. The result is a powerful and unified alarm system which can be easily maintained and updated...|$|E
40|$|The {{interior}} point method (IPM) is {{now well}} {{established as a}} competitive technique for solving very large scale linear programming problems. The leading variant of the interior point method is the primal dual - predictor corrector algorithm due to Mehrotra. The main computational steps of this algorithm are the repeated calculation and solution of a large sparse positive definite system of equations. We describe an implementation of the predictor corrector IPM algorithm on MasPar, a massively parallel SIMD computer. At {{the heart of the}} implemen-tation is a parallel Cholesky factorization algorithm for sparse matrices. Our implementation uses a new scheme of mapping the matrix onto the processor grid of the MasPar, that results in a more efficient Cholesky factorization than previously suggested schemes. The IPM implementation uses the parallel unit of MasPar to speed up the factorization and other computationally intensive parts of the IPM. An impor-tant part of this implementation is the judicious division of data and computation between the <b>front-end</b> <b>computer,</b> that runs the main IPM algorithm, and the par-allel unit. Performanc...|$|E
40|$|The CERN PC-based ISOLDE {{control system}} has been {{installed}} at the SRS electron storage ring at Daresbury Laboratory. The use of Windows NT for the control consoles together with PC and VME <b>front-end</b> <b>computers</b> running under several operating systems {{has resulted in a}} flexible and reliable system for accelerator control. The implementation and philosophy of control application programs, based around a suite of Microsoft Visual Basic and Excel programs, is described. In particular, the use of Excel to provide adaptable programs online allows rapid generation of new control functions; orbit correction and servoing at the application level are described as examples of this...|$|R
40|$|Construction of the K 500 Super-conducting {{cyclotron}} {{facility at}} VECC, Kolkata, has progressed {{to the extent}} that the building, the Magnet yoke, the Helium-plant, the super-conducting coil winding on bobbin, the trim coils are in their advanced stages of completion. Requirement analysis of the control system is under continued development, based on a document depicting detailed specifications of the sub-systems, prepared after multi level and multi loop interactions with other subsystem implementing groups. The hardware of the control system is being implemented in a two layer architecture with PC's / Workstations connected through a Gigabit Control system Ethernet optical fibre LAN. The <b>front-end</b> <b>computers</b> are industrial PC's communicating throug...|$|R
40|$|A Post Mortem System was {{developed}} for the Relativistic Heavy Ion Collider at Brookhaven National Laboratory to provide a playback of the collider state {{at the time of}} a beam abort, quench, or other failure event. Post Mortem data is used to provide diagnostics about the failure and to improve future stores. This data is read from hardware buffers and is written directly to the main file system by Accelerator Device Objects in the <b>front-end</b> <b>computers.</b> The Post Mortem System has facilitated analysis of loss monitor and power supply data, such as beam loss during magnet quenches, dump kicker misfires and power supply malfunctions. System details and recent operating experience will be discussed. Comment: Poster paper for ICALEPCS 2001 conference, San Jose, USA, (THAP 063), 3 pages in PDF forma...|$|R
40|$|Recent {{development}} of the Caltech data acquisition system installed in 1981, which runs on a VAX- 11 / 750, a Peritek Q-bus network, LSI- 11 s, and CAMAC, is described. In this system, the DEC VMS and RT- 11 operating systems are supported on the VAX "host" and LSI- 11 "front-end" computers by a VMS device driver and network host program, and a bootable RT- 11 device driver. Network "utility" and "control" programs provide general purpose support for communication between front-end and host software. Data acquisition software tools are provided for writing programs to run nuclear physics experiments. A system similar to Caltech's was installed at the University of Rochester in 1982. The network has been tested for speed and real-time response. After including all software overhead required by data acquisition, {{it was found that}} the system could transfer buffers and acknowledge their receipt at a net speed of 127 KB per second with a 35 % load on the host computer. The network software is currently being rehosted on Ethernet hardware at Caltech in a multiple host - many <b>front-end</b> <b>computer</b> configuration. Compatibility with the current Peritek network software will be maintained...|$|E
40|$|Electrostatic septa (ZS) {{are used}} in the {{extraction}} of the particle beams from the CERN SPS to the North Area experimental zone. These septa employ high electric fields, generated from a 300 kV power supply, and are particularly prone to internal sparking around the cathode structure. This sparking degrades the electric field quality, consequently affecting the extracted beam, vacuum and equipment performance. To mitigate these effects, a Spark Detection System (SDS) has been realised, which is based on an industrial SIEMENS S 7 - 400 programmable logic controller and deported Boolean processor modules interfaced through a PROFINET fieldbus. The SDS interlock logic uses a moving average spark rate count to determine if the ZS performance is acceptable. Below a certain spark rate it is probable that the ZS septa tank vacuum can recover, thus avoiding transition into a state where rapid degradation would occur. Above this level an interlock is raised and the high voltage is switched off. Additionally, all spark signals acquired by the SDS are sent to a <b>front-end</b> <b>computer</b> to allow further analysis such as calculation of spark rates and production of statistical data...|$|E
40|$|The MEDEX {{equipment}} system implements {{an integrated}} centrally controlled interactive experiment package for medical investigations. A precursor version {{was used for}} the German-Russian MIR' 92 mission. Conserving the system concept of MEDEX, the equipment system was to be updated to the state of the art, and problems that occurred during the use of MEDEX were to be alleviated. Within the scope of the MEDEX project, the following tasks were to be accomplished: conception and setup of a control computer (stationary <b>front-end</b> <b>computer</b> with mobile terminal) which has sufficient performance in data acquisition, preprocessing, storage and presentation; development of a wireless data link on the basis of infrared data transmission between the instruments on the subject and the central data acquisition system; development of an electric power supply system on the basis of rechargeable batteries and an appropriate intelligent charging station; conception and manufacturing of a container system which can be easily adapted to different operational environments (Shuttle middeck, Spacelab, MIR). According to the specifications, the MEDEX system was set up and tested successfully. (orig.) SIGLEAvailable from TIB Hannover: F 95 B 1703 +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Forschung und Technologie (BMFT), Bonn (Germany); Deutsche Agentur fuer Raumfahrtangelegenheiten (DARA) GmbH, Bonn (Germany) DEGerman...|$|E
50|$|A {{database}} machines or {{back end}} processor {{is a computer}} or special hardware that stores and retrieves data from a database. It is specially designed for database access and is coupled to the main (<b>front-end)</b> <b>computer(s)</b> by a high-speed channel. The database machine is tightly coupled to the main CPU, whereas the database server is loosely coupled via the network. Database machines can transfer large packets of data to the mainframe using hundreds to thousands of microprocessors with database software. The front end processor receives the data and displays it. The back end processor {{on the other hand}} analyzes and stores the data from the front end processor. Back end processors result in higher performance, increasing host main memory, increasing database recovery and security, and decrease of cost to manufacture. The database machine contrasts with a database server, which is a computer in a local area network that holds a database.|$|R
40|$|APS) {{control system}} network {{was at the}} state-of-the-art. Different aspects of the system have been {{reported}} at previous meetings [1, 2]. As loads on the controls network have increased due to newer and faster workstations and <b>front-end</b> <b>computers,</b> we have found performance of the system declining and have implemented an upgraded network. There have been dramatic advances in networking hardware {{in the last several}} years. The upgraded APS controls network replaces the original FDDI backbone and shared Ethernet hubs with redundant gigabit uplinks and fully switched 10 / 100 Ethernet switches with backplane fabrics in excess of 20 Gbits/s (Gbps). The central collapsed backbone FDDI concentrator has been replaced with a Gigabit Ethernet switch with greater than 30 Gbps backplane fabric. Full redundancy of the system has been maintained. This paper will discuss this upgrade and include performance data and performance comparisons with the original network. ...|$|R
40|$|A {{new type}} of Input / Output Controller (IOC) has been {{developed}} based on F 3 RP 61, a CPU module of FA-M 3 Programmable Logic Controller (PLC). Since the CPU module runs Linux, it takes no special effort to run EPICS IOC core program on the CPU module. With the aid of wide variety of I/O modules of FA-M 3 PLC, the F 3 RP 61 -based IOC has various applications in accelerator controls, such as magnet power supply control, stepping motor control, data acquisition from beam monitors, monitoring interlock status and so forth. The adoption of the new IOC makes the architecture of accelerator control systems simpler by unifying the two layers of <b>front-end</b> <b>computers,</b> i. e., the IOC layer and the PLC layer, into one layer. We found that the simplification of the control system architecture helps us to reduce the time and cost for the development and maintenance of application software...|$|R
40|$|The ADP-system LIBRIS-II is {{described}} {{with emphasis on}} its network structure. The connection of 10 libraries within the organization under the National Swedish Board of Universities and Colleges (UHÄ) and four other libraries is outlined. The coverage of acquired literature in the connected llbrarles is discussed, {{as well as the}} extension of the coverage which is achieved by a decentralized reporting of accessions in a large number of other Swedish research and special libraries to the Royal Library which submits centralized input to the system. Three possible models are presented how to offer access for searches to LIBRIS outside the family of LIBRIS-libraries through 1) the existing programmable terminals, 2) dial-up ports in the <b>front-end</b> <b>computer,</b> 3) packet-switching networks such as SCANNET. Figures on data traffic in the network as weIl as error statistics are given, and possible conclusions are drawn. Variations in how the central processing unit is affected by LIBRIS usage are illustrated and analyzed. Response time and factors influencing on the response time are discussed. Facilities to indicate the system status at the terminals are described. The possibility to diminish (minimize) effects of system breakdown by indicating when the system is open again is emphasized. The expansion and the changeover to another computer and the problems involved in this work is also discussed...|$|E
40|$|In CERN accelerators {{control system}} several {{components}} are essential such as: Programmable Logic Controller (PLC), PCI Extensions for Instrumentation (PXI), and other micro-controller families. Together with their weaknesses and their strength points they typically present custom communication protocols {{and it is}} therefore difficult to federate them into the control system using a single communication strategy. Furthermore this dependency to the physical device interfaces and protocols makes most of the code not reusable and the replacement of old technology a difficult problem. The purpose of IEPLC ([1]) is to mitigate the communication issues given by this heterogeneity; it proposes a framework to define communication interfaces in a hardware independent manner. In addition it automatically generates all the resources needed on master side (typically represented by a FEC: <b>Front-End</b> <b>Computer)</b> and slave side (typically represented by the controller) to implement a common and generic Ethernet communication. The IEPLC framework is composed {{of a set of}} tools, and a C++ library. The configuration tool allows the definition of the data to be exchanged and their instantiation on different controllers within the control system. The scripts generate the resources necessary to the final communication while the library eventually allows the application on the master side to send and receive data to/from the different controllers. This paper describes the different components of this tool by focusing on its main objectives, namely: defining standard interconnection ways and clear communication interface between FECs and controllers; reducing user developments and configuration time...|$|E
40|$|About 90 stripline-type beam {{position}} monitors (BPMs) {{have been}} newly {{installed in the}} KEKB injector linac. These monitors easily reinforce handling beam orbits and measuring the charge of single-bunch electrons and positrons which are injected to the KEKB rings. The design value of the beam position resolution {{is expected to be}} less than 0. 1 mm. A new data-acquisition (DAQ) system has been developed in order to control these monitors in real time. The hardware and software of 18 <b>front-end</b> <b>computers</b> were tuned for the linac commissioning. This report describes the hardware and software system, the monitor calibration, and preliminary beam test results. INTRODUCTION The KEK B Factory (KEKB) project (1) is in progress in order to test CP violation in the decay of B mesons. KEKB is an asymmetric electron-positron collider comprised of 3. 5 GeV positron and 8 GeV electron rings. The PF 2. 5 GeV linac (2) is also being upgraded to the KEKB injector linac in order to inject single-b [...] ...|$|R
40|$|When it was installed,the Advanced Photon Source (APS) {{control system}} network {{was at the}} state-of-the-art. Different aspects of the system have been {{reported}} at previous meetings [1, 2]. As loads on the controls network have increased due to newer and faster workstations and <b>front-end</b> <b>computers,</b> we have found performance of the system declining and have implemented an upgraded network. There have been dramatic advances in networking hardware {{in the last several}} years. The upgraded APS controls network replaces the original FDDI backbone and shared Ethernet hubs with redundant gigabit uplinks and fully switched 10 / 100 Ethernet switches with backplane fabrics in excess of 20 Gbits/s (Gbps). The central collapsed backbone FDDI concentrator has been replaced with a Gigabit Ethernet switch with greater than 30 Gbps backplane fabric. Full redundancy of the system has been maintained. This paper will discuss this upgrade and include performance data and performance comparisons with the original network...|$|R
40|$|The {{software}} {{architecture of the}} control system of the CERN PS complex [1][2] is strongly based on object concepts. Equipment modules, designed and implemented during the late 70 s, introduced the concepts of abstraction and encapsulation, leading to an object oriented implementation during the 80 s. The current {{software architecture}}, set-up during the past 6 years, implements a well-defined object model in the <b>front-end</b> <b>computers</b> from which the console-level classes have been derived. In this context, {{the integration of the}} object oriented technologies is a natural and continuous process. This paper reports on the recent evolution of this architecture at the console level: the migration of the system libraries to C++, the introduction of object oriented Computer Aided Software Engineering (CASE) tools, the connection with CDEV [3] and the integration of Java. 1 Introduction 1. 1 Objects in the PS Control In the CERN-PS complex, control entities (e. g. device classes, d [...] ...|$|R
40|$|The {{control system}} for the CERN 26 Gev Proton Synchrotron and its {{injectors}} is a generic system which {{can be adapted to}} other accelerators. Most configuration data are in a relational database. From these data we can generate object interfaces for equipment, configuration files for <b>front-end</b> <b>computers,</b> a read-only database for accelerator control interfacing, and full dynamic documentation on the Web. The database is also used in real time for runtime references and archives, and for the working data of several programs. 1 Introduction The control {{system for the}} CERN PS accelerator complex (7 accelerators, not including SPS and LEP), is a generic system which can be adapted to other accelerators. It must steer the beams through the interconnected accelerators with up to 5 particle types accelerated in cycles grouped in a supercycle. This means that thousands of parameters must be changed each cycle of about 1. 2 seconds. Some help from a database is required. 2 The database 2. 1 Hist [...] ...|$|R
