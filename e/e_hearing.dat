34|183|Public
40|$|This {{preliminary}} study investigates lexical retrieval of written English with native English speakers (i. <b>e.,</b> <b>hearing)</b> and American Sign Language (ASL) signers (i. e., deaf) by using masked priming techniques. Repetition and pseudohomophone priming were tested. These types of priming {{were employed in}} order to investigate phonological and/or orthographic effects. A significant facilitative phonological effect for hearing participants and a significant inhibitory orthographic effect for deaf participants were found, showing clearly that the modality differences of participants who use sign or spoken languages are {{a factor in the}} lexical processing of written English...|$|E
40|$|Evidence {{indicates}} that adequate phonological abilities {{are necessary to}} develop proficient reading skills and that later in life phonology also has {{a role in the}} covert visual word recognition of expert readers. Impairments of acoustic perception, such as deafness, can lead to atypical phonological representations of written words and letters, which in turn can affect reading proficiency. Here, we report an experiment in which young adults with different levels of acoustic perception (i. <b>e.,</b> <b>hearing</b> and deaf individuals) and different modes of communication (i. <b>e.,</b> <b>hearing</b> individuals using spoken language, deaf individuals with a preference for sign language, and deaf individuals using the oral modality with less or no competence in sign language) performed a visual lexical decision task, which consisted of categorizing real words and consonant strings. The lexicality effect was restricted to deaf signers who responded faster to real words than consonant strings, showing over-reliance on whole word lexical processing of stimuli. No effect of stimulus type was found in deaf individuals using the oral modality or in hearing individuals. Thus, mode of communication modulates the lexicality effect. This suggests that learning a sign language during development shapes visuo-motor representations of words, which are tuned to the actions used to express them (phono-articulatory movements vs. hand movements) and to associated perceptions. As these visuo-motor representations are elicited during on-line linguistic processing and can overlap with the perceptual-motor processes required to execute the task, they can potentially produce interference or facilitation effects...|$|E
40|$|The {{complexity}} {{of caring for}} the ageing heart failure (HF) population is further complicated by concomitant chronic conditions (i. e., polypharmacy, depression), age related impairments (i. <b>e.,</b> <b>hearing,</b> visual and cognitive impairments, impairments in {{activities of daily living}} (ADL/IADL), and other issues (e. g., health illiteracy, lack of social support). This paper provides an overview of these risk factors, outlines how they individually and in interplay endanger favourable outcome by putting patients at risk for poor self-management. Moreover, suggestions are made on how these issues could be addressed and integrated in heart failure management by applying gerontological care principles in caring for the ageing heart failure population. status: publishe...|$|E
50|$|Standing {{between the}} robot's ears, Wile <b>E.</b> <b>hears</b> the Road Runner coming and orders the robot to ATTACK, but the robot's {{electric}} bolts from its ears burn {{him to a}} crisp.|$|R
500|$|In Philadelphia, Ed Jerse loses {{a divorce}} {{settlement}} to his ex-wife, who has sole {{custody of his}} children. After getting drunk at a bar, Ed wanders into a tattoo parlor and impulsively receives a tattoo depicting a Sailor Jerry-like pin-up girl with the words [...] "Never Again" [...] under her image. At work the next day, <b>Ed</b> <b>hears</b> a woman calling him a [...] "loser"; he has a violent confrontation with a female co-worker—who denies saying anything—and is subsequently subdued.|$|R
40|$|Abstract. Two {{different}} {{congenital malformations}} {{of the heart}} were seen in two calves and two ewe lambs. In all these malformations the left ventricle {{was more or less}} hypoplastic; it was small with a narrow lumen and a thick wall. In one malformation the aorta began as a cul-de-sac above the left ventricle; in the other, the aorta arose from the right ventricle, {{to the right of the}} supraventricular crest. The “left hypoplastic hear t ” a n d t h e “double out le t right ventricle with an intact interventr icular s e p t u m ” are well-known congeni ta l malformations of t h <b>e</b> <b>hear</b> t in man [l- 7, 9 - 12, 14 - 22], b u t less known in animals. Some have been repor ted i...|$|R
40|$|Methods {{developed}} for real-time time scale modification (TSM) of speech signal are presented. They {{are based on}} the non-uniform, speech rate depended SOLA algorithm (Synchronous Overlap and Add). Influence of the proposed method on the intelligibility of speech was investigated for two separate groups of listeners, i. <b>e.</b> <b>hearing</b> impaired children and elderly listeners. It was shown that for the speech with average rate equal to or higher than 6. 48 vowels/s, all of the proposed methods have statistically significant impact on the improvement of speech intelligibility for hearing impaired children with reduced hearing resolution and one of the proposed methods significantly improves comprehension of speech in the group of elderly listeners with reduced hearing resolution...|$|E
40|$|Abstract Methods {{developed}} for real-time time scale modification (TSM) of speech signal are presented. They {{are based on}} the non-uniform, speech rate depended SOLA algorithm (Synchronous Overlap and Add). Influence of the proposed method on the intelligibility of speech was investigated for two separate groups of listeners, i. <b>e.</b> <b>hearing</b> impaired children and elderly listeners. It was shown that for the speech with average rate equal to or higher than 6. 48 vowels/s, all of the proposed methods have statistically significant impact on the improvement of speech intelligibility for hearing impaired children with reduced hearing resolution and one of the proposed methods significantly improves comprehension of speech in the group of elderly listeners with reduced hearing resolution. Virtual slides [URL] </p...|$|E
40|$|The {{prevailing}} otolaryngologic {{approach to}} treatment of age-related hearing loss (ARHL), presbycusis, emphasizes compensation of peripheral functional deficits (i. <b>e.,</b> <b>hearing</b> aids and cochlear implants). This approach {{does not address}} adequately {{the needs of the}} geriatric population, one in five of whom is expected to consist of the “old old” in the coming decades. Aging affects both the peripheral and central auditory systems, and disorders of executive function become more prevalent with advancing age. Growing evidence supports an association between age-related hearing loss and cognitive decline. Thus, to facilitate optimal functional capacity in our geriatric patients, a more comprehensive management strategy of ARHL is needed. Diagnostic evaluation should go beyond standard audiometric testing and include measures of central auditory function including dichotic tasks and speech-in-noise testing. Treatment should include not only appropriate means of peripheral compensation, but also auditory rehabilitative training and counseling...|$|E
50|$|A {{couple on}} a {{motorcycle}} try to revive Owen with liquor, but they leave when he doesn't wake up. Police arrive and mistakenly conclude that Owen is drunk. At the station, night-shift captain <b>Ed</b> Bates <b>hears</b> the story and realizes that Liz {{is the daughter of}} the day-shift captain, Dan Taggart.|$|R
40|$|The AGC plays {{critical}} {{roles in}} many applications, <b>e.</b> g., <b>hearing</b> aids, hard disk drives, and particularly portable communication systems. As {{long as a}} receiver is required in a wireless system, the signal strength will be likely depend on {{the distance between the}} receiver and the transmitter which may cause receive...|$|R
40|$|Synaesthesia is a {{condition}} in which stimulation of one sensory modality (<b>e.</b> g. <b>hearing)</b> causes additional experiences in a second, unstimulated modality (e. g. seeing colours). The goal of this task is to explore the types (and incidence) of synaesthesia in different cultures. Two simple tests can ascertain the existence of synaesthesia in your community...|$|R
40|$|Historically, {{children}} with hearing loss {{have struggled to}} attain levels of literacy commensurate with typical hearing peers (Marschark, 2007), however, due {{to the use of}} advanced hearing technology (i. <b>e.,</b> <b>hearing</b> aids and cochlear implants), {{children with}} hearing loss have demonstrated improved literacy outcomes (Johnson 2 ̆ 6 Goswami, 2010). Standardized literacy, language, cognitive assessments and speech perception measures were administered to 11 preschool-age children using either hearing aids or cochlear implants. Descriptive analysis was provided regarding performance on each assessment. Correlations were made between early literacy and speech, language, and cognitive standardized test scores, speech perception measures, and hearing-related factors. Results indicated that preschool children with hearing loss are performing within the average range on early literacy measures. There is also variability among children with hearing loss on their early literacy performance. Auditory and visual cognitive processing is correlated with early literacy skills...|$|E
40|$|Statistical {{learning}} plays a {{key role}} in language processing, e. g., for speech segmentation. Older adults have been reported to show less statistical learning on the basis of visual input than younger adults. Given age-related changes in perception and cognition, we investigated whether statistical learning is also impaired in the auditory modality in older compared to younger adults and whether individual learning ability is associated with measures of perceptual (i. <b>e.,</b> <b>hearing</b> sensitivity) and cognitive functioning in both age groups. Thirty younger and thirty older adults performed an auditory artificial-grammar-learning task to assess their statistical learning ability. In younger adults, perceptual effort came at the cost of processing resources required for learning. Inhibitory control (as indexed by Stroop colornaming performance) did not predict auditory learning. Overall, younger and older adults showed the same amount of auditory learning, indicating that statistical learning ability is preserved over the adult life span...|$|E
40|$|Access to {{psychological}} therapies {{continues to be}} poor for people experiencing psychosis. To address this problem, researchers are developing brief interventions that address the specific symptoms associated with psychosis, i. <b>e.,</b> <b>hearing</b> voices. As part of the development work for a brief Cognitive Behaviour Therapy (CBT) intervention for voices we collected qualitative data from people who hear voices (study 1) and clinicians (study 2) on the potential barriers and facilitators to implementation and engagement. Thematic analysis of the responses from both groups revealed a number of anticipated barriers to implementation and engagement. Both groups believed the presenting problem (voices and psychosis symptoms) may impede engagement. Furthermore clinicians identified a lack of resources to be a barrier to implementation. The only facilitator to engagement was reported by people who hear voices who believed a compassionate, experienced and trustworthy therapist would promote engagement. The results are discussed in relation to how these barriers could be addressed {{in the context of}} a brief intervention using CBT techniques...|$|E
40|$|Number {{transcoding}} (e. g., writing 64 when hearing ‘sixty-four’) {{is a basic}} numerical skill; faultlessly {{performed in}} adults, but rather difficult for children. In the present study, children speaking Dutch (an inversed number language) and French (a non-inversed number language) wrote Arabic digits to dictation. We also tested their IQ and their phonological, visuospatial, and executive working memory. Although the amount of transcoding errors (<b>e.</b> g., <b>hearing</b> 46 but writing 56) was equal in both groups, the amount of inversion errors (<b>e.</b> g., <b>hearing</b> 46 but writing 64) was significantly higher in Dutch-speaking than in French-speaking children. Regression analyses confirmed that language was the only significant predictor of inversion errors. Working-memory components, in contrast, were the only significant predictors of transcoding errors. Executive resources were important in all children. Less-skilled transcoders also differed from more-skilled transcoders in that they used semantic rather than asemantic transcoding routes. Given the observed relation between number transcoding and mathematics grades, current findings may provide useful information for educational and clinical settings...|$|R
5000|$|A local gangster, Blacky Franchot, is {{the next}} person to contact Ed about Rosita. They arrange to meet, but Blacky is shot before Ed arrives. Blacky lives long enough for <b>Ed</b> to <b>hear</b> him utter the words [...] "I loved her" [...] before he dies. Ed reports his {{findings}} to the city editor, Gribbe, who writes a long gripping column about Rosita, making her life and fate sound sensational and mysterious.|$|R
40|$|It is {{estimated}} that one of every six Americans, people of both genders and {{of all ages and}} races, experiences some form of communication disorder (<b>e.</b> g., <b>hearing</b> impairment, dizziness, balance problems, smell and taste disorders, and voice, speech or language disturbances). Such disorders often compromise social, emotional, educational and vocational aspects of an individual's life. The cost of these disorders in terms of quality of life and unfulfilled potential is substantial...|$|R
40|$|Highly trained musicians {{not only}} play an {{instrument}} competently, but {{have been found to}} develop efficient skills related to music notation. For example, they can remember visually presented sequences of notes via musical imagery, and graphically represent in notation the pitches they imagine. Yet, the nature of this skill and encoding processes are not fully known, and while the experience seems to be one involving auditory processes of the inner ear (i. <b>e.,</b> <b>hearing</b> in the “mind’s ear”), these mental representations may actually be much more reliant on subvocal kinesthetic motor memory (i. e., covert rehearsal of the “mind’s voice”). Aims In a previous study, Brodsky et al (2003) demonstrated that phonatory interference rather than rhythmic distraction or auditory input obstructs notational audiation. Accordingly, it was suggested that score reading involves kinesthetic-like phonatory processes. The current study attempted to further explore this notion. Method The current study employed Brodsky’s embedded melody paradigm, but with added concurrent surface Audio/EMG monitoring of the vocal folds...|$|E
40|$|This report {{describes}} important {{physiological and}} anatomical {{aspects of human}} hearing, including instruments for hearing improvements, i. <b>e.</b> <b>hearing</b> aids. One common problem with hearing aids is acoustic feedback (howling). A new subband Adaptive Feedback Control (AFC) system for hearing aids is proposed. This system detects howling by calculating distances between zero crossings in the subband input signal. A stable subband zero crossing distance measure indicates that howling is present in a particular subband. This triggers an adaptation to estimate and attenuate the feedback channel. The adaptation is driven by a probe noise sequence constrained in both time and frequency. This constraint implies increased signal quality and that user discomfort due to emitted probe noise is reduced. Also, an effective and reliable adaptation is achieved. The method has proven to operate well in computer simulations, with speech as well as music as signal input. Initial simulations indicate that an increased hearing aid gain of at least 15 dB i...|$|E
40|$|The Ontario Infant Hearing Program (OIHP) {{provides}} early interventions (i. <b>e.,</b> <b>hearing</b> aids) {{to children}} who are hard of hearing (CHH) because research consistently demonstrates their benefit to language outcomes. The impact of pre-fitting language abilities on these outcomes are not well understood. This retrospective cohort analysis examined the performance of OIHP children on the Preschool Language Scale- 4 {{at the time of}} (n= 47), and after (n= 19), initial hearing aid intervention. Regression analyses revealed that, before amplification, hearing loss severity predicted language abilities. However, after amplification, severity of hearing loss did not uniquely predict language achievement, but rather was driven by its relationship with language at the time of amplification. These findings suggest that hearing aids fitted early may provide a preservation benefit to the language achievement of CHH, and that this benefit is greatest for children at highest risk (i. e., children with the weakest initial language, and most severe hearing loss) ...|$|E
40|$|Communication {{disorders}} occur {{across the}} lifespan and encompass {{a wide range}} of conditions that interfere with individuals’ abilities to <b>hear</b> (<b>e.</b> g., <b>hearing</b> loss), speak (<b>e.</b> g., voice disorders; motor speech disorders), and/or use language (e. g., specific language impairment; aphasia) to meet their communication needs. Such disorders often compromise the social, recreational, emotional, educational, and vocational aspects of an individual’s life. This research examines {{the development and implementation of}} new software that facilitates multi-syllabic speech production in children with autism and speech delays. The VocSyl software package utilizes a suite of audio visualizations that represent a myriad of audio features in abstract representations. The goal of these visualizations is to provide children with language impairments a new persistent modality in which to experience and practice speech-language skills. ACM Classification Keywords H 5. 2 [Information Interfaces and Presentation]: Screen design, Voice I/O. K 4. 2 [Social Issues]: Assistive technologies for person...|$|R
40|$|In {{the present}} study we {{investigate}} the role of attention in audiovisual semantic interference, by using an attentional blink paradigm. Participants were asked to make an unspeeded response to {{the identity of a}} visual target letter. This target letter was preceded at various SOAs by a synchronized audiovisual letter-pair, which was either congruent (<b>e.</b> g. <b>hearing</b> an " F" and viewing an " F") or incongruent (<b>e.</b> g. <b>hearing</b> an " F" and viewing a " Z"). In Experiment 1, participants were asked to match the members of the audiovisual letter-pair. In Experiment 2, participants were asked to ignore the synchronized audiovisual letter-pairs altogether and only report the visual target. In Experiment 3, participants were asked to identify only one of the audiovisual letters (identify the auditory letter, and ignore the synchronized visual letter, or vice versa). An attentional blink was found in all three experiments indicating that the audiovisual letter-pairs were processed. However, a congruency effect on subsequent target detection was observed in Experiments 1 and 3, but not in Experiment 2. The results indicate that attention to the semantic contents of at least one modality is necessary to establish audiovisual semantic interference. © 2010 Elsevier B. V...|$|R
40|$|This study {{investigated}} gamers’ auditory experiences as after effects of playing. This {{was done by}} classifying, quantifying, and analysing 192 experiences from 155 gamers collected from online videogame forums. The gamers’ experiences were classified as: (i) auditory imagery (<b>e.</b> g., constantly <b>hearing</b> the music from the game), (ii) inner speech (e. g., completing phrases in the mind), (iii) auditory misperceptions (e. g., confusing real life sounds with videogame sounds), and (iv) multisensorial auditory experiences (<b>e.</b> g., <b>hearing</b> music while involuntary moving the fingers). Gamers heard auditory cues from the game in their heads, in their ears, but also coming from external sources. Occasionally, the vividness of the sound evoked thoughts and emotions that resulted in behaviours and copying strategies. The psychosocial implications of the gamers’ auditory experiences are discussed. This study contributes {{to the understanding of}} the effects of auditory features in videogames, and to the phenomenology of non-volitional experiences (e. g., auditory imagery, auditory hallucinations) ...|$|R
40|$|Abstract Background Current {{human and}} {{experimental}} studies are indicating {{an association between}} stress and hearing problems; however potential risk factors have not been established. Hearing problems are projected to become among the top ten disabilities according to the WHO in the near future. Therefore {{a better understanding of}} the relationships between stress and hearing is warranted. Here we describe the prevalence of two common hearing problems, i. <b>e.</b> <b>hearing</b> complaints and tinnitus, in relation to different work-and health-related stressors. Methods A total of 18, 734 individuals were invited to participate in the study, out of which 9, 756 (52 %) enrolled. Results The results demonstrate a clear and mostly linear relationship between higher prevalence of hearing problems (tinnitus or hearing loss or both) and different stressors, e. g. occupational, poorer self-rated health, long-term illness, poorer sleep quality, and higher burnout scores. Conclusions The present study unambiguously demonstrates associations between hearing problems and various stressors that have not been previously described for the auditory system. These findings will open new avenues for future investigations. </p...|$|E
40|$|We present {{modality}} exclusivity {{norms for}} 400 randomly selected noun concepts, for which participants provided perceptual strength ratings across five sensory modalities (i. <b>e.,</b> <b>hearing,</b> taste, touch, smell, and vision). A comparison with previous norms showed that noun concepts are more multimodal than adjective concepts, as nouns tend to subsume multiple adjectival property concepts (e. g., perceptual {{experience of the}} concept baby involves auditory, haptic, olfactory, and visual properties, and hence leads to multimodal perceptual strength). To show {{the value of these}} norms, we then used them to test a prediction of the sound symbolism hypothesis: Analysis revealed a systematic relationship between strength of perceptual experience in the referent concept and surface word form, such that distinctive perceptual experience tends to attract distinctive lexical labels. In other words, modality-specific norms of perceptual strength are useful for exploring not just the nature of grounded concepts, but also the nature of form-meaning relationships. These norms will be of benefit to those interested in the representational nature of concepts, the roles of perceptual information in word processing and in grounded cognition more generally, and the relationship between form and meaning in language development and evolution...|$|E
40|$|Monaural {{measurements}} of minimum audible angle (MAA) (discrimination between two locations) and absolute identification (AI) of azimuthal {{locations in the}} frontal horizontal plane are reported. All experiments used roving-level fixed-spectral-shape stimuli processed with nonindividualized head-related transfer functions (HRTFs) to simulate the source locations. Listeners were instructed to maximize percent correct, and correct-answer feedback was provided after every trial. Measurements are reported for normal-hearing subjects, who listened with only one ear, and effectively monaural subjects, who had substantial unilateral hearing impairments (i. <b>e.,</b> <b>hearing</b> losses greater than 60 dB) and listened with their normal ears. Both populations behaved similarly; the monaural experience of the unilaterally impaired listeners was not beneficial for these monaural localization tasks. Performance in the AI experiments was similar with both 7 and 13 source locations. The average root-mean-squared deviation between the virtual source location and the reported location was 35 °, the average slopes of the best fitting line was 0. 82, and the average bias was 2 °. The best monaural MAAs were less than 5 °. The MAAs were consistent with a theoretical analysis of the HRTFs, which suggests that monaural azimuthal discrimination is related to spectral-shape discrimination...|$|E
5000|$|Allin, E.F. & J.A. Hopson. 1991. Evolution of the {{auditory}} system in Synapsida ("mammal-like reptiles" [...] and primitive mammals) {{as seen in}} the fossil record. In: The Evolutionary Biology of <b>Hearing</b> (<b>Ed.</b> by D. B. Webster, A. Popper, and R. Fay), New York: Springer-Verlag.|$|R
40|$|Often {{the best}} {{performing}} supervised learning models are ensembles of {{hundreds or thousands}} of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e. g. Google), where storage space is at a premium (e. g. PDAs), and where computational power is limited (<b>e.</b> g. <b>hearing</b> aids). We present a method for “compressing ” large, complex ensembles into smaller, faster models, usually without significant loss in performance...|$|R
40|$|The {{simple act}} of {{listening}} or of taking notes while attendinga lesson may represent an insuperable burden for millions ofpeople with some form of disabilities (<b>e.</b> g., <b>hearing</b> impaired,dyslexic and ESL students). In this paper, we propose anarchitecture that aims at automatically creating captions forvideo lessons by exploiting advances in speech recognitiontechnologies. Our approach couples the usage of o-the-shelf ASR (Automatic Speech Recognition) software with anovel caption alignment mechanism that smartly introducesunique audio markups into the audio stream before givingit to the ASR and transforms the plain transcript producedby the ASR into a timecoded transcript...|$|R
40|$|I. Introduction II. Statutory Framework Following Probation Violations … A. Probation Violation Arrest Warrants [...] . B. Bail for Alleged Probation Violators [...] . C. Conclusion III. The Gagnon Revolution [...] . A. Pre-Gagnon Probation Revocation Rules in Nebraska [...] . B. Morrissey v. Brewer [...] . C. Gagnon v. Scarpelli: Its Impact on Probation Revocation IV. The Preliminary Hearing [...] . A. Nature of the Hearing [...] . B. The Preliminary Hearing Requirement [...] . C. Preliminary Hearing Venue [...] . D. Identity of the Hearing Officer [...] . <b>E.</b> <b>Hearing</b> Officer Reports V. The Final Hearing [...] . A. Nature of the Final Hearing [...] . B. Violations Warranting Revocation [...] . C. Sufficiency of Notice and Motion/Information [...] . D. Confrontation and Hearsay [...] . E. Self-Incrimination [...] . F. Rights Advisory [...] . G. Written Statement of Factfinder VI. Defenses to Probation Violation Allegations [...] . A. Illegally Obtained Evidence [...] . B. Inability to Pay [...] . C. Revocation Motion Filed Too Late VII. Miscellaneous Concerns [...] . A. Invalid Conditions and Appeals [...] . B. Which Court is the Sentencing Court? [...] . C. Jail Credit [...] . D. Driver 2 ̆ 7 s License Sanctions [...] . E. Violation of Community Service Sentences VIII. Conclusio...|$|E
40|$|In-vehicle {{information}} systems (IVIS) have become popular; IVIS {{could be used}} to provide drivers with a variety of information (e. g., en-route guidance information and collision warning information) via different in-vehicle devices. In Taiwan, some aggressive driving behaviors are observed such as tailgating and violating traffic signals. Intersection collision warning system (ICWS) provided by IVIS could be used for avoiding the accidents due to violating traffic signals. This study employed a driving simulator to investigate the influence of auditory collision warning messages on drivers ’ perception-reaction times and workload when the drivers were visually or audibly distracted by secondary tasks via different IVIS devices. The secondary task was to solve simple mathematical problems displayed to the driver three different formats: voice, numbers shown on a liquid crystal display (LCD) panel, and number shown on a heads-up display (HUD). The most important finding of the study was that the auditory collision warning message was capable of decreasing drivers’ perception-reaction times when the drivers were visually distracted by the mathematical problems shown on the LCD panel or the HUD. However, when the drivers were distracted by an auditory task (i. <b>e.,</b> <b>hearing</b> mathematical problems), the auditory collision warning message increased drivers’ perception-reaction times...|$|E
40|$|Objective: The aims of {{the current}} n 200 study were to assess the {{structural}} relations between three classes of test variables (i. <b>e.</b> <b>HEARING,</b> COGNITION and aided speech-in-noise OUTCOMES) and to describe the theoretical implications of these relations for the Ease of Language Understanding (ELU) model. Study sample: Participants were 200 hard-of-hearing hearing-aid users, {{with a mean age}} of 60. 8 years. Forty-three percent were females and the mean hearing threshold in the better ear was 37. 4 dB HL. Design: LEVEL 1 factor analyses extracted one factor per test and/or cognitive function based on a priori conceptualizations. The more abstract LEVEL 2 factor analyses were performed separately for the three classes of test variables. Results: The HEARING test variables resulted in two LEVEL 2 factors, which we labelled SENSITIVITY and TEMPORAL FINE STRUCTURE; the COGNITIVE variables in one COGNITION factor only, and OUTCOMES in two factors, NO CONTEXT and CONTEXT. COGNITION predicted the NO CONTEXT factor to a stronger extent than the CONTEXT outcome factor. TEMPORAL FINE STRUCTURE and SENSITIVITY were associated with COGNITION and all three contributed significantly and independently to especially the NO CONTEXT outcome scores (R- 2 = 0. 40). Conclusions: All LEVEL 2 factors are important theoretically as well as for clinical assessment. Funding Agencies|Linnaeus Centre HEAD excellence center grant from the Swedish Research Council [349 - 2007 - 8654]; FORTE [2012 - 1693]</p...|$|E
50|$|Some {{time later}} Ed learns from Rosita's former maid, Hazel, that Rosita left Temple {{one year before}} her death, after a bitter {{argument}} in which Temple hit Rosita. As Ed leaves Hazel's building, he is knocked unconscious by two of Wellman's thugs. Ed awakens later in a junkyard. As a cover for his investigation, Ed takes Leona to a boxing match featuring fighter Bat Bennett. Bat and his manager, Jerry Cavanaugh, were the last names to be listed in Rosita's book, under Hotspur's address. Just before the fight, Ed finds Jerry and Bat arguing over the newspaper report of Rosita's death. When Bat leaves, Jerry reveals to Ed that Bat {{fell in love with}} Rosita while she was working for Hotspur. When Bat became distracted by Rosita, Jerry threatened to reveal her identity and hiding place to Wellman if she didn't end the relationship. Rosita reluctantly agreed to do this and then disappeared soon after she quit working for Hotspur. Temple is murdered, and <b>Ed</b> <b>hears</b> rumors that Wellman is responsible for the killing. Ed believes Temple used to finance Wellman's rackets. Ed finds Belle, who admits to him that Wellman originally hired Spingler to get rid of Rosita. Belle denies knowing of Spingler's duplicity until reading the newspaper account of Rosita's death the other day.|$|R
40|$|Recently {{there has}} been a growing {{interest}} in the properties and formal analysis of multi-agent systems bootstrapping a communication system (see e. g.. (Baronchelli et al., 2005) and the contribution of (De Vylder and Tuyls, 2005), this workshop.) Although very interesting and promising results were obtained in these studies, major simplifications were made. For example, although much larger populations are considered than was the case in most earlier work, both the cited works (1) assume the possibility of meaning transfer, i. <b>e.</b> the <b>hearer</b> always exactly knows the speaker's intended meaning independent of whether he understands the speaker's utterance and (2) only consider single-word utterances...|$|R
5000|$|... 7th Mercosul Biennial: Grito <b>e</b> Escuta and <b>Hearing,</b> Porto Alegre, Rio Grande do Sul, BrazilA Decade of Contemporary American Printmaking: 1999-2009, Visual Art Center of Academy of Arts and Design, Tsinghua University, Beijing, ChinaCompass in Hand: Selections from The Judith Rothschild Foundation Contemporary Drawings Collection, The Museum of Modern Art, New York, NY"Chelsea visits Havana", Museo Nacional de Bellas Artes, Havana, Cuba ...|$|R
