6|31|Public
30|$|At this stage, the C code can be {{automatically}} generated. The MATLAB Coder [18] and Simulink Coder [19] {{packages are}} required to translate a Simulink diagram into equivalent C procedures. The generated code contains three sets of procedures: <b>initialisation</b> <b>procedures</b> executed once when the calculations begin, solver procedures that calculate the model at each sampling period, and termination procedures. Usually, the metering algorithms {{do not have a}} specific operating time, so the termination procedures are never called.|$|E
40|$|Gaussian {{processes}} are typically used for smoothing and interpolation on small datasets. We {{introduce a new}} Bayesian nonparametric framework – GPatt – enabling automatic pattern extrapolation with Gaussian processes on large multidimensional datasets. GPatt unifies and ex-tends highly expressive kernels and fast exact in-ference techniques. Without human intervention – no hand crafting of kernel features, and no so-phisticated <b>initialisation</b> <b>procedures</b> – we show that GPatt can solve large scale pattern extrap-olation, inpainting, and kernel discovery prob-lems, including a problem with 383400 training points. We find that GPatt significantly outper-forms popular alternative scalable Gaussian pro-cess methods in speed and accuracy. Moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact infer-ence are useful for modelling large scale multi-dimensional patterns. 1...|$|E
40|$|The {{underlying}} {{theme of}} this thesis has been {{the investigation of the}} direct connection of generators to HVdc converters. The first ever direct connection harmonic test measurements are presented. The accuracy of present steady state and dynamic simulation models is assessed. A novel variation of the direct connection, called the series excitation is proposed. In this system the dc line current is fed back in series with the dc output to provide the excitation requirements of the generators. A steady state formulation is developed to investigate its operational advantages and limitations. State variable and EMTDC models are used to predict the dynamic behaviour. The time consuming nature of dynamic simulation algorithms has encouraged the search for effective differential equation <b>initialisation</b> <b>procedures.</b> A Newton-Raphson technique is applied to a state variable transient converter simulation programme in a bid to reduce steady state computation times...|$|E
30|$|The <b>initialisation</b> <b>procedure</b> {{generates the}} first {{snapshot}} of the model.|$|R
2500|$|Ekdahl and Johannson {{published}} {{an attack on}} the <b>initialisation</b> <b>procedure</b> which breaks A5/1 in a few minutes using two to five minutes of conversation plaintext. This attack does not require a preprocessing stage. In 2004, Maximov et al. improved this result to an attack requiring [...] "less than one minute of computations, and a few seconds of known conversation". The attack was further improved by Elad Barkan and Eli Biham in 2005.|$|R
40|$|Pomaranch is a {{synchronous}} {{stream cipher}} submitted to eSTREAM, the ECRYPT Stream Cipher Project. The cipher is constructed as a cascade clock control sequence generator, {{which is based}} on the notion of jump registers. We present an attack which exploits the cipher’s <b>initialisation</b> <b>procedure</b> to recover the 128 -bit secret key. The attack requires around 265 computations. An improved version of the attack is also presented, with complexity of the order of 252 operation...|$|R
40|$|This paper {{examines}} {{the use of}} genetic algorithms (GAs) in generating sets of input data to use for software testing. The aim is to produce test sets which maximise coverage of the software using a given metric, whilst minimising {{the size of the}} sets. Using the well known triangle program as an example, a representation is described which allows the GA to learn the number of test cases in a set. This is done by adding a set of flags to the encoding, which determine whether or not a gene is expressed (in this case, whether a test case is used as input to the program). A simple mechanism for biassing the search towards longer or shorter sets is described. A study is then made of the effect of changing chromosome lengths and <b>initialisation</b> <b>procedures,</b> and the relationship that this has to the quality and size of the test sets evolved, in order to assess the scalability of the evolutionary approach to "real-world" problems, and the factors that would need to be taken into consideration wh [...] ...|$|E
40|$|Recently {{there has}} been {{interest}} {{in the use of}} classifiers based on the product of experts (PoE) framework. PoEs offer an alternative to the standard mixture of experts (MoE) framework. It may be viewed as examining the intersection of a series of experts, rather than the union as in the MoE framework. This paper presents a particular implementation of PoEs, the normalised product of Gaussians (PoG). Here each expert is a Gaussian mixture model. In this work, the PoG model is presented within a hidden Markov model framework. This allows the classification of variable length data, such as speech data. Training and <b>initialisation</b> <b>procedures</b> are described for this PoG system. The relationship of the PoG system with other schemes, including covariance modeling schemes, is also discussed. In addition the scheme is shown to be related to a standard speech recognition approach, multiple stream systems. The PoG system performance is examined on an automatic speech recognition task, Switchboard. The performance is compared to standard Gaussian mixture systems and multiple stream systems. ...|$|E
40|$|In this study, we assess {{systematically}} {{the impact}} of different <b>initialisation</b> <b>procedures</b> on the predictability of the sea ice in the Southern Ocean. These initialisation strategies are based on three data assimilation methods: the nudging, the particle filter with sequential importance resampling and the nudging proposal particle filter. An Earth system model of intermediate complexity is used to perform hindcast simulations in a perfect model approach. The predictability of the Antarctic sea ice at interannual to multi-decadal timescales is estimated through two aspects: {{the spread of the}} hindcast ensemble, indicating the uncertainty of the ensemble, and the correlation between the ensemble mean and the pseudo-observations, used to assess the accuracy of the prediction. Our results show that at decadal timescales more sophisticated data assimilation methods as well as denser pseudo-observations used to initialise the hindcasts decrease the spread of the ensemble. However, our experiments did not clearly demonstrate that one of the initialisation methods systematically provides with a more accurate prediction of the sea ice in the Southern Ocean than the others. Overall, the predictability at interannual timescales is limited to 3 years ahead at most. At multi-decadal timescales, the trends in sea ice extent computed over the time period just after the initialisation are clearly better correlated between the hindcasts and the pseudo-observations if the initialisation takes into account the pseudo-observations. The correlation reaches values larger than 0. 5 in winter. This high correlation has likely its origin in the slow evolution of the ocean ensured by its strong thermal inertia, showing the importance {{of the quality of the}} initialisation below the sea ice...|$|E
5000|$|Ekdahl and Johannson {{published}} {{an attack on}} the <b>initialisation</b> <b>procedure</b> which breaks A5/1 in a few minutes using two to five minutes of conversation plaintext. This attack does not require a preprocessing stage. In 2004, Maximov et al. improved this result to an attack requiring [...] "less than one minute of computations, and a few seconds of known conversation". The attack was further improved by Elad Barkan and Eli Biham in 2005.|$|R
30|$|An {{additional}} {{probability mass}} function (pmf), describing the number of cluster births per cluster lifetime interval, is also extracted from the measurements. The extraction method and examples of extracted parameters are provided in [33]. According to this pmf, {{a number of new}} clusters are drawn every cluster lifetime interval. After drawing the number of new clusters, the actual parameters of these new clusters are drawn {{in the same way as}} described in the <b>initialisation</b> <b>procedure</b> in Section 2.3. 1. New-born clusters fade in during the next cluster lifetime interval.|$|R
40|$|In {{this paper}} {{we report on}} further {{progress}} with the factorisation of integers using the MPQS algorithm on hypercubes and a MIMD parallel computer with 1024 T 805 processors. We were able to factorise a 101 digit number from the Cunningham list using only about 65 hours computing time. We give new details about the hypercube sieve <b>initialisation</b> <b>procedure</b> and describe {{the structure of the}} factor graph that saves a significant amount of computing time. At March 3 rd, we finished the factorisation of a 104 digit composite...|$|R
40|$|We {{investigate}} long-time {{thermal activation}} behaviour in magnetic particle chains of variable length. Chains are modelled as Stoner–Wohlfarth particles coupled by dipolar interactions. Thermal activation {{is described as}} a hopping process over a multidimensional energy landscape using the discrete orientation model limit of the Landau–Lifshitz–Gilbert dynamics. The underlying master equation is solved by diagonalising the associated transition matrix, which allows the evaluation of distributions of time scales of intrinsic thermal activation modes and their energy representation. It is shown {{that as a result of}} the interaction dependence of these distributions, increasing the particle chain length can lead to acceleration or deceleration of the overall relaxation process depending on the <b>initialisation</b> <b>procedure...</b>|$|R
40|$|Wavelet-networks are {{inspired}} by both the feedforward neural networks and the theory underlying wavelet decompositions. This special kind of networks has proved its advantages over other networks schemes, particularly in approximation and prediction problems. However, the training procedure used for wavelet networks {{is based on the}} idea of continuous differentiable wavelets, but unfortunately, most of powerful and used wavelets do not satisfy this property. This paper presents a new <b>initialisation</b> <b>procedure</b> and a new training algorithm for wavelet neural-networks that improve its performance allowing the use of different kind of wavelets. To show this, comparisons are made for chaotic time series approximation between the proposed approach and the typical wavelet-network. 1...|$|R
40|$|Blind turbo {{receivers}} with fast least-squares channel estimation and soft-feedback equalisation M. B. Loiola, R. R. Lopes and J. M. T. Romano Low-complexity blind turbo receivers {{composed of}} a soft-feedback equaliser (SFE) and fast least-squares (FLS) channel estimators are proposed. To reduce {{the complexity of the}} SFE computation, a function approximation is proposed instead of numerical algorithms in a specific step of the equaliser’s computation. Concerning FLS channel estimators, a specific filter parameters <b>initialisation</b> <b>procedure</b> is proposed in each turbo iteration to avoid possible numerical instabilities. Estimation of noise variance is also considered. The proposed scheme can perform as the turbo equaliser with perfect channel knowledge from a certain signal-to-noise ratio. Introduction: One of the most effective methods to mitigate the effect...|$|R
40|$|Outdoor {{augmented}} reality systems often rely on GPS to cover large environments. Visual tracking approaches can provide more accurate location estimates but typically require a manual <b>initialisation</b> <b>procedure.</b> This paper describes {{the combination of}} both techniques to create an accurate localisation system that does not require any additional input for (re-) initialisation. The 2 D GPS position together with average user height is used as an initial estimate for the visual tracking. The large gap in available GPS accuracy versus required accuracy for initialisation is overcome through a search procedure that tries to minimise search time by improving the likelihood of finding the correct estimate early. Re-initialisation of the visual tracking system after catastrophic failures is further improved by modelling the GPS error with a Gaussian process to provide a better estimate of the current location, thereby decreasing search time...|$|R
40|$|Examination of the job-shop {{scheduling}} literature uncovers {{a striking}} trend. As methods for the deterministic job-shop problem have gradually improved over the years, {{they have come}} to rely on neighbourhoods for selecting moves that are more and more constrained. We document this phenomenon with a historical sketch of job-shop neighborhoods, which leads us to focus particularly on the approach of Nowicki and Smutnicki (1996), noted for proposing and implementing the most restrictive neighbourhood in the literature. The Nowicki and Smutnicki (NS) method which exploits its neighbourhood by a tabu search strategy, is widely recognised as the most effective procedure for obtaining high quality solutions in a relatively short time. Accordingly, we analyse the contribution of the method's neighbourhood structure to its overall effectiveness. Our findings show, surprisingly, that the NS neighbourhood causes the method's choice of an <b>initialisation</b> <b>procedure</b> to have an important influence on t [...] ...|$|R
40|$|In {{this paper}} {{we report on}} further {{progress}} with the factorisation of integers using the MPQS algorithm on hypercubes and a MIMD parallel computer with 1024 T 805 processors. We were able to factorise a 101 digit number from the Cunningham list using only about 65 hours computing time. We give new details about the hypercube sieve <b>initialisation</b> <b>procedure</b> and describe {{the structure of the}} factor graph that saves a significant amount of computing time. At March 3 rd, we finished the factorisation of a 104 digit composite. 1 Scope and Achievements The integer factoring problem carries a long mathematical tradition. It is of cryptographic significance since about 16 years, when the first public-key cryptosystem algorithms appeared. Several of these rely on the difficulty of factoring and thus we need to have reliable data about factoring time when chosing parameter sizes for asymmetric cryptosystems and the many protocols using them. One year ago, a Parsytec MIMD parallel computer with 102 [...] ...|$|R
40|$|The time {{complexity}} of a search depends on (1) {{the size of}} the neighbourhood and (2) the complexity in determining the cost of the moves. Therefore in order for the search to be effective and efficient, the neighbourhood must be highly constrained {{and it needs to be}} evaluated quickly, however without losing the ability to provide access to improved solutions. This paper investigates strategies that have an effect on both of these parameters and commences by investigating neighbourhood structures. This is done by documenting a historical sketch of job-shop neighbourhoods, which focuses attention on the approach of Nowicki and Smutnicki (1996) (NS), noted for proposing and implementing the most restrictive neighbourhood in the literature. Accordingly, the contribution of the method's neighbourhood structure to its overall effectiveness is analysed. The findings show that if the NS neighbourhood is used in the search process, the choice of an <b>initialisation</b> <b>procedure</b> has an important infl [...] ...|$|R
40|$|The {{importance}} of supply chain and logistics management {{has been widely}} recognised. Effective management of the supply chain can reduce costs and lead times and improve responsiveness to changing customer demands. This paper proposes a multi-matrix real-coded Generic Algorithm (MRGA) based optimisation tool that minimises total costs associated within supply chain logistics. According to finite capacity constraints of all parties within the chain, Genetic Algorithm (GA) often produces infeasible chromosomes during initialisation and evolution processes. In the proposed algorithm, chromosome <b>initialisation</b> <b>procedure,</b> crossover and mutation operations that always guarantee feasible solutions were embedded. The proposed algorithm was tested using three sizes of benchmarking dataset of logistic chain network, which are typical of those faced by most global manufacturing companies. A half fractional factorial design was carried out to investigate the influence of alternative crossover and mutation operators by varying GA parameters. The analysis of experimental results suggested {{that the quality of}} solutions obtained is sensitive {{to the ways in which}} the genetic parameters and operators are set...|$|R
30|$|There {{are some}} aspects of {{interactive}} music evolution that impose convergence to “subjective optima” limitations. Due to the user fatigue that results from the constant human devotion {{to the task of}} listening and rating, the listener is expected to loose focus during the rating process, especially if she/he undergoes a large number of melodies in each rating round. This obviously increases the hazard of inconsistent or even contradicting ratings, misleading the system to non-optimal feature regions. Consequently, this would force the agents to roam the feature space without converging to certain melodic locations, creating an impression that the system does not provide any feedback to the user ratings, further amplifying the vicious circle of user fatigue. User fatigue does not solely depend on the number of melodies, but also on their duration. Since the melodies that the proposed system produces are about 15  s long (as described later in “Evolutionary generation of melodies” section), a collection of 4 melodies per rating round was considered as a satisfactory compromise between melodic diversity (considering also the <b>initialisation</b> <b>procedure</b> discussed in “PSO initialisation and evolution” section) and keeping the number of melodies as low as possible.|$|R
40|$|To {{meet the}} demands of the many {{emerging}} multiple camera studio systems in entertainment content production, a novel wand-based system is presented for calibration of both intrinsic (focal length,lens distortion) and extrinsic (position, orientation) parameters of multiple cameras. Full metric calibration is obtained solely from observations of a wand comprising two visible markers at a known fixed distance. It is not necessary for all cameras to see the wand simultaneously, cameras may face each other, and have non-overlapping fields of view. High accuracy is achieved by using iterative bundle adjustment of tracked feature points across multiple views to refine calibration parameters until re-projection errors are minimised over the required measurement volume. The approach involves a new automatic <b>initialisation</b> <b>procedure</b> and novel application of bundle adjustment to refine calibration estimates. Evaluation of wand-calibration is performed using an eight-camera system. Results demonstrate a reprojection error of approximately 0. 5 pixels rms and 3 D reconstruction error of less than 2 mm rms for a capture volume of 2 x 3 x 2 m. Advantages of wand-based calibration over conventional chart-based calibration include time-efficient calibration of multiple camera systems and calibration of camera configurations without all cameras having to view the same objects or having overlapping fields of view. I...|$|R
40|$|Recent dynamic local search (DLS) {{algorithms}} such as SAPS are {{amongst the}} state-of-the-art methods for solving the propositional satisfiability problem (SAT). DLS algorithms modify the search landscape during the search process {{by means of}} dynamically changing clause penalties. In this work, we study whether the resulting, `warped' landscapes are easier to search than the landscapes that correspond to the original problem instances. We present empirical evidence indicating that (somewhat contrary to common belief) {{this is not the}} case, and that the main benefit of the dynamic penalty update mechanism in SAPS is an effective diversification of the search process. In most other high-performance stochastic local search algorithms, the same effect is achieved by the strong use of randomised decisions throughout the search. We demonstrate that in SAPS, random decisions are only required in the (standard) search <b>initialisation</b> <b>procedure,</b> and can be completely eliminated from the remainder of the subsequent search process without any significant change in the behaviour or performance of the resulting algorithms compared to the original, fully randomised SAPS algorithm. We conjecture that the reason for this unexpected result lies in the ability of the deterministic variants of the scaling and smoothing mechanism and the subsidiary iterative best improvement mechanism underlying SAPS to effectively propagate the effects of initial randomisation throughout a search process that shows the sensitive dependence on inditial conditions that is characteristic for chaotic processes...|$|R
40|$|Rakaposhi is a {{synchronous}} stream cipher, {{which uses}} three main components: a non-linear feedback shift register (NLFSR), a dynamic linear feedback shift register (DLFSR) and a non-linear filtering function (NLF). NLFSR consists of 128 bits and is initialised by the secret key K. DLFSR holds 192 bits and is initialised by an initial vector (IV). NLF takes 8 -bit inputs and returns a single output bit. The work identifies weaknesses and {{properties of the}} cipher. The main observation is that the <b>initialisation</b> <b>procedure</b> has the so-called sliding property. The property {{can be used to}} launch distinguishing and key recovery attacks. The distinguisher needs four observations of the related (K,IV) pairs. The key recovery algorithm allows to discover the secret key K after observing 2 ⁹ pairs of (K,IV). Based on the proposed related-key attack, the number of related (K,IV) pairs is 2 (¹²⁸⁺¹⁹²) /⁴ pairs. Further the cipher is studied when the registers enter short cycles. When NLFSR is set to all ones, then the cipher degenerates to a linear feedback shift register with a non-linear filter. Consequently, the initial state (and Secret Key and IV) can be recovered with complexity 2 ⁶³·⁸⁷. If DLFSR is set to all zeros, then NLF reduces to a low non-linearity filter function. As the result, the cipher is insecure allowing the adversary to distinguish it from a random cipher after 2 ¹⁷ observations of keystream bits. There is also the key recovery algorithm that allows to find the secret key with complexity 2 ⁵⁴. 11 page(s...|$|R
40|$|Abstract—The {{importance}} of supply chain and logistics management {{has been widely}} recognised. Effective management of the supply chain can reduce costs and lead times and improve responsiveness to changing customer demands. This paper proposes a multi-matrix real-coded Generic Algorithm (MRGA) based optimisation tool that minimises total costs associated within supply chain logistics. According to finite capacity constraints of all parties within the chain, Genetic Algorithm (GA) often produces infeasible chromosomes during initialisation and evolution processes. In the proposed algorithm, chromosome <b>initialisation</b> <b>procedure,</b> crossover and mutation operations that always guarantee feasible solutions were embedded. The proposed algorithm was tested using three sizes of benchmarking dataset of logistic chain network, which are typical of those faced by most global manufacturing companies. A half fractional factorial design was carried out to investigate the influence of alternative crossover and mutation operators by varying GA parameters. The analysis of experimental results suggested {{that the quality of}} solutions obtained is sensitive {{to the ways in which}} the genetic parameters and operators are set. The design task is to minimise these costs, which can be mathematically formulated and solved using enumerative methods such as Integer Linear Programming [1] or stochastic search techniques (sometimes called meta-heuristics) such as Genetic Algorithm [2], [3]. However, GA proposed by [2] has not been based on matrix and unfortunately produced infeasible offspring, which is omitted from the iterative evolution process within the GA. The work in [3] ignored raw material, manufacturing and holding cost from the objective function and similarly proposed a repair process for rectifying infeasible chromosome...|$|R
40|$|Abstract. Rakaposhi is a {{synchronous}} stream cipher, {{which uses}} {{three main components}} a non-linear feedback shift register (NLFSR), a dynamic linear feedback shift register (DLFSR) and a non-linear filtering function (NLF). NLFSR consists of 128 bits and is initialised by the secret key K. DLFSR holds 192 bits and is initialised by an initial vector (IV). NLF takes 8 -bit inputs and returns a single output bit. The work identifies weaknesses and properties of the cipher. The main observation is that the <b>initialisation</b> <b>procedure</b> has the so-called sliding property. The property {{can be used to}} launch distinguishing and key recovery attacks. The distinguisher needs four observations of the related (K, IV) pairs. The key recovery algorithm allows to discover the secret key K after observing 2 9 pairs of (K, IV). In the proposed relatedkey attack, the number of related (K, IV) pairs is 2 (128 + 192) / 4 pairs. The key recovery algorithm allows to discover the secret key K after observing 2 9 related (K, IV) pairs. Further the cipher is studied when the registers enter short cycles. When NLFSR is set to all ones, then the cipher degenerates to a linear feedback shift register with a non-linear filter. Consequently, the initial state (and Secret Key and IV) can be recovered with complexity 2 63. 87. If DLFSR is set to all zeros, then NLF reduces to a low non-linearity filter function. As the result, the cipher is insecure allowing the adversary to distinguish it from a random cipher after 2 17 observations of keystream bits. There is also the key recovery algorithm that allows to find the secret key with complexit...|$|R
40|$|In this Ph. D. Thesis we {{investigate}} {{the viability of}} using quantum dot ensembles as a quantum memory architecture through the use numerical simulations to study population transfer within quantum dots. This is followed by {{an investigation into the}} effects of high order wavemixing on the population transfer within two level systems, which was born from effects noted while simulating quantum dots. We study the initialisation of an ensemble of inhomogeneously broadened quantum dots, introducing a novel initialisation method utilising pump field with a slow frequency sweep. We focus on the properties of such an <b>initialisation</b> <b>procedure</b> and conclude that the maximum initialisation fidelities are determined entirely by the Zeeman splittings and decay rates of the quantum dots. We study several possibilities for performing π rotations on the population of an ensemble of quantum dots, and show the RCAP protocol is the most applicable. We study this protocol in the context of quantum dots and give the optimal parameters to use to generate high fidelity π pulses. We then bring together our work on quantum dots population transfer with the work of others covering the write and read procedures on quantum dots to provide a feasibility analysis of the complete quantum memory protocol. The work on wavemixing presented in this thesis uses a novel approach to analyse wavemixing effects which is used to predict the population transferred in two level simulations of wavemixing processes. We provide simulation confirmation of our approach to analyse wavemixing effects and then go on to calculate the disruptive effects of wavemixing caused by high intensity lasers on some simple systems. Finally we show that large orders of wavemixing can, at least in principle, be used for coherent population transfer. ...|$|R
40|$|The decadal hindcast {{simulations}} {{performed for}} the Mittelfristige Klimaprognosen (MiKlip) project are evaluated using satellite-retrieved cloud parameters from the CM SAF cLoud, Albedo and RAdiation dataset from AVHRR data (CLARA-A 1) {{provided by the}} EUMETSAT Satellite Application Facility on Climate Monitoring (CM SAF) and from the International Satellite Cloud Climatology Project (ISCCP). The forecast quality of two sets of hindcasts, Baseline- 1 -LR and Baseline- 0, which use differing initialisations, is assessed. Basic evaluation focuses on multi-year ensemble mean fields and cloud-type histograms utilizing satellite simulator output. Additionally, ensemble evaluation employing analysis of variance (ANOVA), analysis rank histograms (ARH) and a deterministic correlation score is performed. Satellite simulator output is available for {{a subset of the}} full hindcast ensembles only. Therefore, the raw model cloud cover is complementary used. The new Baseline- 1 -LR hindcasts are closer to satellite data with respect to the simulated tropical/subtropical mean cloud cover pattern than the reference hindcasts (Baseline- 0) emphasizing improvements of the new MiKlip <b>initialisation</b> <b>procedure.</b> A slightly overestimated occurrence rate of optically thick cloud-types is analysed for different experiments including hindcasts and simulations using realistic sea surface boundaries according to the Atmospheric Model Intercomparison Project (AMIP). By contrast, the evaluation of cirrus and cirrostratus clouds is complicated by observational based uncertainties. Time series of the 3 -year mean total cloud cover averaged over the tropical warm pool (TWP) region show some correlation with the CLARA-A 1 cloud fractional cover. Moreover, ensemble evaluation of the Baseline- 1 -LR hindcasts reveals potential predictability of the 2 – 5 lead year averaged total cloud cover for a large part of this region when regarding the full observational period. However, the hindcasts show only moderate positive correlations with the CLARA-A 1 satellite retrieval for the TWP region which are hardly statistical significant. Evidence for predictability of the 2 – 5 lead year averaged total cloud cover is found for parts of the equatorial to mid-latitudinal North Atlantic...|$|R
40|$|The {{skill and}} error growth {{characteristics}} of oceanographic forecasts {{produced by the}} Australian 'BLUElink' system are investigated. The principal variable investigated is Sea Surface Height (SSH), {{because it is the}} primary indicator of the mesoscale dynamics; however Sea Surface Temperature (SST) and sub-surface temperatures are also investigated. Forecast performance is assessed using simple statistical metrics, and model skill is measured relative to persistence and climatology. Verifying data is a behind real time BLUElink analysis, constrained by observations unavailable to the forecast. The East Australian Current separation and eddy-shedding region is identified as the most challenging to forecast in the Australian region, and is analysed in detail. The performance along the continental shelf is also described. In the study region, the forecast baseline for SSH (SST) is established as persistence for the first 20 days (12 days), then climatology thereafter. Relative to this baseline, BLUElink forecasts in this region show skill throughout the 6 -day forecast period. The distribution of variability between different space and timescales is investigated. Complex Empirical Orthogonal Function (CEOF) analyses are used to investigate eddy properties and their relationships with baroclinic processes. The temporal and spatial distribution of forecast error shows that a skilful forecast is much more likely than one with no skill. A particular failure mode related to eddy mergers is identified, with two distinct events leading to poor performance during the study period. The initial error at real time, attributed to the <b>initialisation</b> <b>procedure</b> and the latency of the observational data, is shown to contribute {{a significant portion of the}} total forecast error for the system studied. These findings justify the research and development applied to initialisation. Observations unavailable to the BLUElink system show that there is skill in forecasting SSH along the continental shelf. BLUElink forecasts capture Coastally-Trapped Waves (CTWs) particularly well. Contrary to earlier studies, it is shown, using a CEOF analysis, that the majority of the CTW variance propagates as continuous features between the south-west and north-east. Although modulated by the shelf width, CTWs can be coherently tracked through sharply changing coastline orientations, the shallow Bass Strait and wind forcing regions...|$|R
40|$|A {{one-dimensional}} {{model with}} second order turbulence closure {{has been developed}} and used to investigate processes in the cloud-topped marine atmospheric boundary layer. Model developments were required to correctly apply surface flux terms near the sea surface, poor representation of which is common to several models from the recent literature. The improved surface forcing is shown to affect the predicted boundary layer structure. Other developments included {{the implementation of a}} fully implicit numerical code, which generated less numerical noise than that originally used in the model, and an improved <b>initialisation</b> <b>procedure.</b> The new model code was then shown to quantitatively reproduce processes in the stratocumulustopped boundary layer using measurements of atmospheric turbulence from aircraft from the North Sea and the subtropical North Atlantic and North Pacific. The model is robust to changes in the mixing length coefficients used in the turbulence closure and to perturbations in the initial profiles. The model is used to simulate conditions that occur as winds circulate from the subtropics towards the tradewind regions. The observed transition from a shallow stratocumulus layer to a deeper stratocumulus layer interacting with cumulus clouds beneath is simulated in response to realistic external forcing. The final stages of transition, from cumulus under stratocumulus to shallow cumulus is however not observed in the simulation; possible reasons for this are discussed. The model shows in detail the interaction between the stratocumulus layer and cumulus clouds beneath. The cumulus clouds thicken, moisten and cool the stratocumulus layer and therefore act to maintain the layer, but can also drive entrainment. The peaks in turbulent kinetic energy in the stratocumulus layer which follow cumulus penetrations of the stratocumulus layer can be large enough to directly cause the boundary layer to entrain air from above the boundary layer and grow in height. The entrained air is warmer and drier than the boundary layer air and tends to dissipate the stratocumulus layer. The model is then used to show how the imposed environmental conditions affect processes within the boundary layer. An important model prediction is that cloud top entrainment instability may act to promote mixing between the surface and cloud in deep-decoupled boundary layers. The mixing acts to replenish the cloud liquid water and sustain the cloud. Cloud top entrainment instability has previously been thought to have the capacity to lead to rapid erosion of the cloud, although this has not been observed in practice. This mechanism could help to explain the observed persistence of stratocumulus clouds under these conditions...|$|R
40|$|International audienceThe {{mechanisms}} {{involved in}} Atlantic meridional overturning circulation (AMOC) decadal variability and predictability {{over the last}} 50 years are analysed in the IPSL-CM 5 A-LR model using historical and initialised simulations. The <b>initialisation</b> <b>procedure</b> only uses nudging towards sea surface temperature anomalies with a physically based restoring coefficient. When compared to two independent AMOC reconstructions, both the historical and nudged ensemble simulations exhibit skill at reproducing AMOC variations from 1977 onwards, and in particular two maxima occurring respectively around 1978 and 1997. We argue that one source of skill {{is related to the}} large Mount Agung volcanic eruption starting in 1963, which reset an internal 20 -year variability cycle in the North Atlantic in the model. This cycle involves the East Greenland Current intensity, and advection of active tracers along the subpolar gyre, which leads to an AMOC maximum around 15 years after the Mount Agung eruption. The 1997 maximum occurs approximately 20 years after the former one. The nudged simulations better reproduce this second maximum than the historical simulations. This is due to the initialisation of a cooling of the convection sites in the 1980 s under the effect of a persistent North Atlantic oscillation (NAO) positive phase, a feature not captured in the historical simulations. Hence we argue that the 20 -year cycle excited by the 1963 Mount Agung eruption together with the NAO forcing both contributed to the 1990 s AMOC maximum. These results support the existence of a 20 -year cycle in the North Atlantic in the observations. Hindcasts following the CMIP 5 protocol are launched from a nudged simulation every 5 years for the 1960 - 2005 period. They exhibit significant correlation skill score as compared to an independent reconstruction of the AMOC from 4 -year lead-time average. This encouraging result is accompanied by increased correlation skills in reproducing the observed 2 -m air temperature in the bordering regions of the North Atlantic as compared to non-initialized simulations. To a lesser extent, predicted precipitation tends to correlate with the nudged simulation in the tropical Atlantic. We argue that this skill is due to the initialisation and predictability of the AMOC in the present prediction system. The mechanisms evidenced here support the idea of volcanic eruptions as a pacemaker for internal variability of the AMOC. Together with the existence of a 20 -year cycle in the North Atlantic they propose a novel and complementary explanation for the AMOC variations over the last 50 years...|$|R
40|$|Gaussian process prior {{models are}} known to be a {{powerful}} non-parametric tool for stochastic data modelling. It employs the methodology of Bayesian inference in using evidence or data to modify or refer some prior belief. Within the Bayesian context, inference can be used for several purposes, such as data analysis, filtering, data mining, signal processing, pattern recognition and statistics. In spite of the growing popularity of stochastic data modelling in several areas, such as machine learning and mathematical physics, it remains generally unexplored within the realm of nonlinear dynamic systems, where parametric methods are much more mature and more widely accepted. This thesis seeks to explore diverse aspects of mathematical modelling of nonlinear dynamic systems using Gaussian process prior models, a simple yet powerful stochastic approach to modelling. The focus of the research is on the application of non-parametric stochastic models to identify nonlinear dynamic systems for engineering applications, especially where data is inevitably corrupted with measurement noise. The development of appropriate Gaussian process prior models, including various choices of classes of covariance functions, is also described in detail. Despite its good predictive nature, Gaussian regression is often limited by several O(N 3) operations and O(N 2) memory requirements during optimisation and prediction. Several fast and memory efficient methods, including modification of the loglikelihood function and hyperparameter <b>initialisation</b> <b>procedure</b> to speed up computations, are explored. In addition, fast algorithms based on the generalised Schur algorithm are developed to allow Gaussian process to handle large-scale timeseries datasets. Models based on multiple independent Gaussian processes are explored in the thesis. These can be split into two main sections, with common explanatory variable and with different explanatory variables. The two approaches are based on different philosophies and theoretical developments. The benefit of having these models is to allow independent components with unique characteristics to be identified and extracted from the data. The above work is applied to a real physical wind turbine data, consisting of 24, 000 points of the wind speed, rotor speed and the blade pitch angle measurement data. A case study is presented to demonstrate the utility of Gaussian regression and encourage further application to the identification of nonlinear dynamic systems. Finally, a novel method using a compound covariance matrix to exploit both the timeseries and state-space aspects of the data is developed. This {{is referred to as the}} statespace time-series Gaussian process. The purpose of this approach is to enable Gaussian regression to be applied on nonlinear dynamic state-space datasets with large number of data points, within an engineering context...|$|R
40|$|The {{mechanisms}} {{involved in}} Atlantic meridional overturning circulation (AMOC) decadal variability and predictability {{over the last}} 50 years are analysed in the IPSL-CM 5 A-LR model using historical and initialised simulations. The <b>initialisation</b> <b>procedure</b> only uses nudging towards sea surface temperature anomalies with a physically based restoring coefficient. When compared to two independent AMOC reconstructions, both the historical and nudged ensemble simulations exhibit skill at reproducing AMOC variations from 1977 onwards, and in particular two maxima occurring respectively around 1978 and 1997. We argue that one source of skill {{is related to the}} large Mount Agung volcanic eruption starting in 1963, which reset an internal 20 -year variability cycle in the North Atlantic in the model. This cycle involves the East Greenland Current intensity, and advection of active tracers along the subpolar gyre, which leads to an AMOC maximum around 15 years after the Mount Agung eruption. The 1997 maximum occurs approximately 20 years after the former one. The nudged simulations better reproduce this second maximum than the historical simulations. This is due to the initialisation of a cooling of the convection sites in the 1980 s under the effect of a persistent North Atlantic oscillation (NAO) positive phase, a feature not captured in the historical simulations. Hence we argue that the 20 -year cycle excited by the 1963 Mount Agung eruption together with the NAO forcing both contributed to the 1990 s AMOC maximum. These results support the existence of a 20 -year cycle in the North Atlantic in the observations. Hindcasts following the CMIP 5 protocol are launched from a nudged simulation every 5 years for the 1960 - 2005 period. They exhibit significant correlation skill score as compared to an independent reconstruction of the AMOC from 4 -year lead-time average. This encouraging result is accompanied by increased correlation skills in reproducing the observed 2 -m air temperature in the bordering regions of the North Atlantic as compared to non-initialized simulations. To a lesser extent, predicted precipitation tends to correlate with the nudged simulation in the tropical Atlantic. We argue that this skill is due to the initialisation and predictability of the AMOC in the present prediction system. The mechanisms evidenced here support the idea of volcanic eruptions as a pacemaker for internal variability of the AMOC. Together with the existence of a 20 -year cycle in the North Atlantic they propose a novel and complementary explanation for the AMOC variations over the last 50 years...|$|R
40|$|This thesis {{discusses}} modelling {{of industrial}} plants with focus on distillation. A generic industrial plant (or section of plant) can be abstracted {{as a set}} of capacities exchanging extensive quantities through connecting streams. Thus the studied system can be represented as a directed graph, where the capacities are the nodes and the streams are the arcs. A cascade of simplified first-principle models is accomplished by a systematic procedure that combines singular perturbation of the negligible capacities and lumping those capacities with similar dynamics. The proposed procedure is associated with order-of-magnitude assumptions and it is based on simple algebraic operations. Examples are made with the dynamic flash. Given an introduction to distillation modelling, two dynamic first-principle distillation models are proposed, namely a nonequilibrium model and an equilibrium model. The initialisation of the corresponding DAE (differential and algebraic equations) systems is analysed. The aim is to provide the models with feasible steady-state starting conditions. The used <b>initialisation</b> <b>procedure</b> is based on: (i) a nonlinear algebraic solver to create consistent initial conditions, and (ii) a DAE solver to move from a generic (but consistent) starting point to feasible operating conditions. To reduce the computation time, it is recommended to use DAE solvers that exploit the structure of the system’s sparsity (tridiagonal blocks matrix in the case of distillation column models). Very frequently, unstructured elements may enter the description. They are common in process control applications, where the states added to the plant description by the integral parts of the controllers introduce unstructured elements in the otherwise very structured Jacobian of the mathematical model. A solution to the handling of “dirty” Jacobians is presented, which is implemented in a DAE solver package available free on internet. This novel DAE solver fully exploits the overall structure of the system’s sparsity, without compromising CPU computation time and precision of the results. The concern of the computation speed of the dynamic models is particular important in on-line applications such as model predictive control (MPC). To facilitate MPC implementations, it is proposed a self-adaptive approach based on simplified nonlinear models. The proposed methodology yields an MPC that adjusts the dimension of the model according to both the current process conditions and the control objectives. To reduce the size of the model, balanced truncation method is also investigated. When balanced truncation is extended to nonlinear models, it is recommended to consider all the states as balancing outputs in order to accurately reconstruct all the original states. In addition, to give more robustness to truncation and to obtain smaller models, it is suggested to reduce the nonlinearities of the original model using a static transformation of the variables. In case of distillation column models, logarithmic compositions are a solution. PhD i kjemisk prosessteknologiPhD in Chemical Process Engineerin...|$|R
40|$|A {{scheme for}} {{globally}} addressing a quantum computer is presented {{along with its}} realisation in an optical lattice setup of one, two or three dimensions. The required resources are mainly those necessary for performing quantum simulations of spin systems with optical lattices, circumventing the necessity for single qubit addressing. We present the control procedures, in terms of laser manipulations, required to realise universal quantum computation. Error avoidance {{with the help of}} the quantum Zeno effect is presented and a scheme for globally addressed error correction is given. The latter does not require measurements during the computation, facilitating its experimental implementation. As an illustrative example, the pulse sequence for the factorisation of the number fifteen is given. Comment: 11 pages, 10 figures, REVTEX. <b>Initialisation</b> and measurement <b>procedures</b> are adde...|$|R
40|$|Empirical thesis. Bibliography: pages 111 - 124. 1. Introduction [...] 2. Stream ciphers [...] 3. Cryptanalysis of WG- 7 {{stream cipher}} [...] 4. Security {{evaluation}} of Rakaposhi stream cipher [...] 5. Security analysis of linearly filtered NLFSRs [...] 6. Practical attack on NLM generators [...] 7. Cryptanalysis of RC 4 (n,m) stream cipher [...] 8. Cryptanalysis of a hash function based on RC 4 [...] 9. Conclusion. Stream ciphers are symmetric cipher systems which provide confidentiality in many applications ranging from mobile phone communication to virtual private networks. They may be implemented effciently in {{software and hardware}} and are a preferred choice when dealing with resource-constrained environments, such as smart cards, RFID tags,and sensor networks. This dissertation addresses cryptanalysis of several stream ciphers, and a hash function based on stream cipher. Also, the thesis investigates the design principles and security of stream ciphers built from nonlinear feedback shift registers. In a design view, any cryptographic attack shows a weak point {{in the design and}} immediately can be converted into an appropriate design criterion. Firstly, this thesis focuses on the WG- 7, a lightweight stream cipher. It is shown that thekey stream generated by WG- 7 can be distinguished from a random sequence with a negligible error probability. In addition, a key-recovery attack on the cipher has been successfully proposed. Then, a security evaluation of the Rakaposhi stream cipher identifies weaknesses of the cipher. The main observation shows that the <b>initialisation</b> <b>procedure</b> has a sliding property. This property can be used to launch distinguishing and key-recovery attacks. Further, the cipher is studied when the registers enter short cycles. In this case, the internal state can be recovered with less complexity than exhaustive search. New security features of a specific design based on nonlinear feedback shift registers have been explored. The idea applies a distinguishing attack on linearly filtered nonlinear feedback shift registers. The attack extends the idea on linear combinations of linearly filtered nonlinear feedback shift registers as well. The proposed attacks allow the attacker to mount linear attacks to distinguish the output of the cipher and recover its internal state. The next topic analyses a new lightweight communication framework called NLM-MAC. Several critical cryptographic weaknesses leading to key-recovery and forgery attack have been indicated. It is shown that the adversary can recover the internal state of the NLM generator. The attacker also is able to forge any MAC tag in real time. The proposed attacks are completely practical and break the scheme. Another part demonstrates some new cryptographic attacks on RC 4 (n,m) stream cipher. The investigations have revealed several weaknesses of the cipher. Firstly, a distinguisher for the cipher is proposed. Secondly, a key-recovery attack uses a method to find the secret key in real time. Finally, the RC 4 -BHF hash function that is based on the well-known RC 4 stream cipher is analysed. Two attacks on RC 4 -BHF have been developed. In the first attack, the adversary is able to find collisions for two different messages. The second attack shows how to design a distinguisher that can tell apart the sequence generated by RC 4 -BHF from a random one. Mode of access: World wide web 1 online resource (xviii, 124 pages) table...|$|R
