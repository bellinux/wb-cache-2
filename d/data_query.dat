409|4384|Public
2500|$|Other {{critics have}} made similar charges that, even if Wikipedia {{articles}} are factually accurate, {{they are often}} written in a poor, almost unreadable style. Frequent Wikipedia critic Andrew Orlowski commented: [...] "Even when a Wikipedia entry is 100 per cent factually correct, and those facts have been carefully chosen, it all too often reads {{as if it has}} been translated from one language to another then into a third, passing an illiterate translator at each stage." [...] A study of Wikipedia articles on cancer was conducted in 2010 by Yaacov Lawrence of the Kimmel Cancer Center at Thomas Jefferson University. The study was limited to those articles that could be found in the Physician <b>Data</b> <b>Query</b> and excluded those written at the [...] "start" [...] class or [...] "stub" [...] class level. Lawrence found the articles accurate but not very readable, and thought that [...] "Wikipedia's lack of readability (to non-college readers) may reflect its varied origins and haphazard editing". The Economist argued that better-written articles tend to be more reliable: [...] "inelegant or ranting prose usually reflects muddled thoughts and incomplete information".|$|E
50|$|Where data entered {{does not}} pass {{validation}} rules then a <b>data</b> <b>query</b> may {{be issued to}} the investigative site where the clinical trial is conducted to request clarification of the entry. Data queries must not be leading (i.e. they must not suggest the correction that should be made). For electronic CRFs only the site staff with appropriate access may modify data entries. For paper CRFs, the clinical data manager applies the <b>data</b> <b>query</b> response to the database {{and a copy of}} the <b>data</b> <b>query</b> is retained at the investigative site.When an item or variable has an error or a query raised against it, it is said to have a “discrepancy” or “query”.|$|E
5000|$|GraphQL is a <b>data</b> <b>query</b> {{language}} {{developed by}} Facebook as an alternate to REST and ad-hoc webservice architectures.|$|E
5000|$|Commonly raised <b>data</b> <b>queries</b> (to help {{identify}} areas where improvements can be made) ...|$|R
40|$|Abstract. In {{this paper}} we {{consider}} concurrent execution of multiple <b>data</b> mining <b>queries.</b> If such <b>data</b> mining <b>queries</b> operate on similar {{parts of the}} database, then their overall I/O cost can be reduced by integrating their data retrieval operations. The integration requires that many <b>data</b> mining <b>queries</b> are present in memory at the same time. If the memory size {{is not sufficient to}} hold all the <b>data</b> mining <b>queries,</b> then the queries must be scheduled into multiple phases of loading and processing. We discuss the problem of <b>data</b> mining <b>query</b> scheduling and propose a heuristic algorithm to efficiently schedule the <b>data</b> mining <b>queries</b> into phases...|$|R
40|$|AbstractApplication {{of complex}} socio-technical systems theory to {{optimization}} of clinical processes in hospitals {{highlights the importance}} of the acceptance and promotion of responsible autonomy among health professionals. Therefore the independent ability for clinicians to search for answers to questions which are outside the scope of pre-made reports is important. However, the ad-hoc <b>data</b> <b>querying</b> process is slow and error prone due to inability of health professionals to access data directly without involving IT experts. The problem lies in the complexity of means used to <b>query</b> <b>data.</b> We propose a new natural language- and star ontology-based ad-hoc <b>data</b> <b>querying</b> approach which reduces the steep learning curve required to be able to <b>query</b> <b>data.</b> The proposed approach would significantly decrease the time needed to master the ad-hoc <b>data</b> <b>querying</b> and to obtain direct access to data by health professionals...|$|R
5000|$|Design Autonomy {{which refers}} to ability to choose its design {{irrespective}} of <b>data,</b> <b>query</b> language or conceptualization, functionality of the system implementation.|$|E
50|$|New and Updated <b>Data</b> <b>Query.</b> This {{interface}} allows queries {{for those}} data collections {{for which the}} NSSDC has recently acquired new data, either additions to existing collections or entirely new collections.|$|E
50|$|Lotus also {{released}} two user-oriented query builders, the DOS-based Query Builder {{which could}} interact with DataLens sources {{but was not}} built on it, and the Windows-based <b>Data</b> <b>Query</b> Assistant (DQA) which was built directly on DataLens.|$|E
40|$|Abstract. In {{this paper}} we {{consider}} concurrent execution of multiple <b>data</b> mining <b>queries</b> {{in the context}} of discovery of frequent itemsets. If such <b>data</b> mining <b>queries</b> operate on similar parts of the database, then their overall I/O cost can be reduced by transforming the set of <b>data</b> mining <b>queries</b> into another set of non-overlapping queries, whose results can be used to efficiently answer the original queries. We discuss the problem of multiple <b>data</b> mining <b>query</b> optimization and experimentally evaluate the Mine Merge algorithm to efficiently execute sets of <b>data</b> mining <b>queries.</b> ...|$|R
40|$|Research Data eXplorer (RedX) was {{designed}} to support self-service research <b>data</b> <b>queries</b> and cohort identification from clinical research databases. The primary innovation of RedX was the electronic health record view of patient data, to provide better contextual understanding for non-technical users in building complex <b>data</b> <b>queries.</b> The design of RedX around this need identified multiple functions that would use individual patient views to better understand population-based data, and vice-versa. During development, the more necessary and valuable components of RedX were refined, leading to a functional self-service query and cohort identification tool. However, with the improved capabilities and extensibility of other applications for <b>data</b> <b>querying</b> and navigation, our long-term implementation and dissemination plans have moved towards consolidation and alignment of RedX functions as enhancements in these other initiatives...|$|R
5000|$|... • Story Board - Used to {{integrate}} <b>data</b> <b>queries</b> from multiple sources via dashboards {{that can be}} shared with internal and external users.|$|R
50|$|Hibernate's primary {{feature is}} mapping from Java classes to {{database}} tables, and mapping from Java data types to SQL data types. Hibernate also provides <b>data</b> <b>query</b> and retrieval facilities. It generates SQL calls and relieves the developer from the manual handling and object {{conversion of the}} result set.|$|E
5000|$|Slazure's primary {{feature is}} that it {{eliminates}} the need for Data Access Classes, Domain Classes, Database Models, DTOs, entity definitions, and DALs while mapping from [...]NET classes to database tables (and from CLR data types to database data types). Slazure also provides <b>data</b> <b>query</b> and retrieval facilities with a proprietary LINQ-provider.|$|E
50|$|The first tier is a {{high-performance}} Linux SMP host that compiles <b>data</b> <b>query</b> tasks received from business intelligence applications, and generates query execution plans. It then divides a query into {{a sequence of}} sub-tasks, or snippets that can be executed in parallel, and distributes the snippets to the second tier for execution.|$|E
5000|$|Instrumentation: Enables the {{application}} with {{the capability to}} generate instrumentation <b>data</b> (<b>Queries</b> count, errors count, ...), this data comes from the execution of {{the application}}.|$|R
40|$|Any {{attribute}} in Rough Relational Database (RRDB) can be multi-valued, {{and has an}} indiscernibility relation in its domain. Currently, {{the research}} on rough <b>data</b> <b>querying</b> mainly discussed some simple select-querying. That is, selecting the tuples whose attribute’s value is equal to a constant from a single table. The main idea of its implementation is to expand the original search conditions according to the indiscernibility relation in attribute’s domain. Because the search conditions after being expanded need more calculation, the querying becomes very slow. In this paper, we present a solution called rough <b>data</b> <b>querying</b> based on encoding. Firstly encode the data of multi-valued attribute into the single-valued data according to the indiscernibility relation in attribute’s domain, and then execute the querying on the single-valued data despite the indiscernibility relation to make the implementation of rough <b>data</b> <b>querying</b> much simpler and more efficient. 1...|$|R
50|$|<b>Data,</b> <b>queries,</b> commands, or {{responses}} must {{be entered}} into the computer through some sort of interface. Following {{are some of the}} methods useful in portable IAs.|$|R
50|$|There exist RDF query {{languages}} {{based on}} other principles. Metalog combines querying with reasoning {{and has an}} English like syntax. Algae is a query language developed by the W3C that adds reactive rules, also called actions, that determine for instance whether an Algae expression is a <b>data</b> <b>query</b> or a data update.|$|E
50|$|A data {{clarification}} form (DCF) or <b>data</b> <b>query</b> form is {{a questionnaire}} specifically used in clinical research. The DCF {{is the primary}} data clarification tool from the trial sponsor or contract research organization (CRO) towards the investigator to clarify discrepancies and ask the investigator for clarification. The DCF {{is part of the}} data validation process in a clinical trial.|$|E
5000|$|Physician <b>Data</b> <b>Query</b> (PDQ) is the US National Cancer Institute's (NCI) {{comprehensive}} cancer database. It contains peer-reviewed summaries {{on cancer}} treatment, screening, prevention, genetics, and supportive care, and complementary and alternative medicine; a registry {{of more than}} 6,000 open and 17,000 closed cancer clinical trials from around the world; and a directory of professionals who provide genetics services.|$|E
5000|$|Layered caching scheme, {{supporting}} separate caching of <b>data</b> <b>queries</b> {{and output}} fragments, via database, shared memory, memcached) for storing cached data, and dynamic cache entry validation upon retrieval ...|$|R
50|$|Once all {{expected}} data is accounted for, all <b>data</b> <b>queries</b> closed, all {{external data}} received and reconciled {{and all other}} data management activities complete the database may be finalized.|$|R
40|$|Abstract. Traditional {{multiple}} query optimization methods {{focus on}} identifying common subexpressions in sets of relational queries and on constructing their global execution plans. In this paper {{we consider the}} problem of optimizing sets of <b>data</b> mining <b>queries</b> submitted to a Knowledge Discovery Management System. We describe the problem of <b>data</b> mining <b>query</b> scheduling and we introduce a new algorithm called CCAgglomerative to schedule <b>data</b> mining <b>queries</b> for frequent itemset discovery. ...|$|R
5000|$|In 2010 {{researchers}} compared {{information about}} 10 {{types of cancer}} on Wikipedia to similar data from the National Cancer Institute's Physician <b>Data</b> <b>Query</b> and concluded [...] "the Wiki resource had similar accuracy and depth to the professionally edited database" [...] and that [...] "sub-analysis comparing common to uncommon cancers demonstrated {{no difference between the}} two", but that ease of readability was an issue.|$|E
5000|$|NHibernate's primary {{feature is}} mapping from [...]NET classes to {{database}} tables (and from CLR data types to SQL data types). NHibernate also provides <b>data</b> <b>query</b> and retrieval facilities. NHibernate generates the SQL commands and relieves the developer from manual data set handling and object conversion, keeping the application portable to most SQL databases, with database portability delivered at very little performance overhead.|$|E
50|$|The HCUP User Support Website is {{the main}} {{repository}} of information for HCUP. It is designed to answer HCUP-related questions; {{provide detailed information on}} HCUP databases, tools, and products; and offer technical assistance to HCUP users. HCUP’s tools, publications, documentation, news, services, and HCUPnet (the free online <b>data</b> <b>query</b> system) may all be accessed through HCUP-US. The Website also provides information on how to obtain HCUP databases.|$|E
40|$|Abstract. Execution cost of batched <b>data</b> mining <b>queries</b> can {{be reduced}} by {{integrating}} their I/O steps. Due to memory limitations, not all <b>data</b> mining <b>queries</b> in a batch can be executed together. In this paper we introduce a heuristic algorithm called CCFull, which suboptimally schedules the <b>data</b> mining <b>queries</b> {{into a number of}} execution phases. The algorithm significantly outperforms the optimal approach while providing a very good accuracy. ...|$|R
40|$|Abstract—Data Warehousing tools {{have become}} very popular and {{currently}} {{many of them have}} moved to Web-based user interfaces {{to make it easier to}} access and use the tools. The next step is to enable these tools to be used within a portal framework. The portal framework consists of pages having several small windows that contain individual <b>data</b> warehouse <b>query</b> results. There are several issues that need to be considered when designing the architecture for a portal enabled <b>data</b> warehouse <b>query</b> tool. Some issues need special techniques that can overcome the limitations that are imposed by the nature of <b>data</b> warehouse <b>queries.</b> Issues such as single sign-on, query result caching and sharing, customization, scheduling and authorization need to be considered. This paper discusses such issues and suggests an architecture to support <b>data</b> warehouse <b>queries</b> within Web portal frameworks. Keywords—Data Warehousing tools, <b>data</b> warehousing <b>queries,</b> web portal frameworks. I...|$|R
50|$|The {{software}} used includes Semantic MediaWiki, {{which enables}} {{the connection of}} people to events, places, and other people, as well as Semantic Forms, for ease of data entry, and Semantic Drilldown to let users construct their own <b>data</b> <b>queries.</b>|$|R
50|$|Like SQL, it {{supports}} a data definition language, {{data manipulation language}} and a <b>data</b> <b>query</b> language, all three with SQL-like syntax.Whereas SQL statements operate on relational tables, DMX statements operate on data mining models.Similarly, SQL Server supports the MDX language for OLAP databases. DMX is used to create and train data mining models, and to browse, manage, and predict against them. DMX is composed of data definition language (DDL) statements, data manipulation language (DML) statements, and functions and operators.|$|E
5000|$|In {{the article}} [...] "Wikipedia Cancer Information Accurate," [...] {{a study of}} medical {{articles}}, Yaacov Lawrence of the Kimmel Cancer Center of Thomas Jefferson University found that the cancer entries were mostly accurate. However, Wikipedia's articles were written in college-level prose, as opposed to in the easier-to-understand ninth-grade-level prose found in the Physician <b>Data</b> <b>Query</b> (PDQ) of the National Cancer Institute. According to Lawrence, [...] "Wikipedia’s lack of readability may reflect its varied origins and haphazard editing.” ...|$|E
50|$|The LIST {{command is}} {{somewhat}} {{analogous to the}} SQL SELECT statement, but incorporates formatting, totaling, and other elements helpful for tailoring output to a business requirement. The SELECT statement, in contrast, is essentially a <b>data</b> <b>query</b> tool: its results would be processed or formatted as required using other mechanisms. This distinction is highlighted by SQL's classification as a 'Data Sublanguage' (DSL): SQL is a powerful formalism for controlling data retrieval. The LIST command is a comprehensive report writer addressing broader functionality.|$|E
40|$|Semantic Web aims {{to elevate}} simple data in WWW to {{semantic}} layer, so that knowledge, processed by machine, {{can be shared}} more easily. Ontology {{is one of the}} key technologies to realize Semantic Web. Semantic reasoning is an important step in Semantic technology. For Ontology developers, semantic reasoning finds out collisions in Ontology definition, and optimizes it; for Ontology users, semantic reasoning retrieves implicit knowledge from known knowledge. The main research of this thesis is reasoning of semantic <b>data</b> <b>querying</b> in distributed environment, which tries to get correct results of semantic <b>data</b> <b>querying,</b> given Ontology definition and data. This research studied two methods: <b>data</b> materialization and <b>query</b> rewriting. Using Amazon cloud computing service and LUBM, we compared these two methods, and have concluded that when size of <b>data</b> to be <b>queried</b> scales up, query rewriting is more feasible than data materialization. Also, based on the conclusion, we developed an application, which manages and <b>queries</b> semantic <b>data</b> in a distributed environment. This application can be used as a prototype of similar applications, and a tool for other Semantic Web researches as well...|$|R
40|$|What's New: 	Compatibility with Tethys 2. 0. 1. 	Added API for {{programmatic}} <b>data</b> <b>queries.</b> @msouff 	Use Plotly {{instead of}} HighCharts (Fully open source!). 	Use DataTables. js for management tables. 	Use Tethys Platform Settings ([URL] 	Improve stream clicking functionality. @msouff 	Improved error handling. 	Code cleanup...|$|R
5000|$|Amadeus {{has its own}} {{data centre}} in Erding, Germany. In 2010, the Erding complex {{processed}} ½ billion transactions per day, and handled, on average, [...] user <b>data</b> <b>queries</b> per second, with a system response time of less than 3 milliseconds and an average system uptime of 99.99%.|$|R
