40|81|Public
40|$|In {{estimating}} the {{minimum number of}} genes contributing to a quantitative character, {{it is suggested that}} the squared difference between the means of the two parents be corrected for <b>experimental</b> <b>variance</b> and that the genetic variance stemming from differences in gene frequencies of the parents be estimated by least squares utilizing information on all entries...|$|E
40|$|Although microarrays are routine {{analysis}} tools in biomedical research, they still yield noisy output that often requires experimental confirmation. Many studies have aimed at optimizing probe design and statistical analysis to tackle this problem. However, less emphasis {{has been placed}} on controlling the noise inherent to the experimental approach. To address this problem, we investigate here a procedure that controls for such <b>experimental</b> <b>variance</b> and combine it with an assessment of probe performance. Two custom arrays were used to evaluate the procedure: one based on 25 mer probes from an Affymetrix design and the other based on 60 mer probes from an Agilent design. To assess <b>experimental</b> <b>variance,</b> all probes were replicated ten times. To assess probe performance, the probes were calibrated using a dilution series of target molecules and the signal response was fitted to an absorption model. We found that significant variance of the signal could be controlled by averaging across probes and removing probes that are nonresponsive. Thus, a more reliable signal could be obtained using our procedure than conventional approaches. We suggest that once an array is properly calibrated, absolute quantification of signals becomes straight forward, alleviating the need for normalization and reference hybridizations. Comment: 16 pages, 8 figure...|$|E
40|$|Let D(n) and H(n) be the fractal {{dimension}} and the Hurst parameter of {{traffic in the}} nth interval, respectively. Thus, this paper gives the <b>experimental</b> <b>variance</b> analysis of D(n) and H(n) of network traffic based on the generalized Cauchy (GC) process on an interval-by-interval basis. We experimentally infer that traffic has the phenomenon Var[D(n) ] > Var[H(n) ]. This suggests {{a new way to}} describe the multifractal phenomenon of traffic. That is, traffic has local high-variability and global robustness. Verifications of that inequality are demonstrated with real traffic...|$|E
3000|$|... plays {{a decisive}} {{role in this}} {{kinetics}} in view of narrow scattering of <b>experimental</b> <b>variances</b> err[*]=[*] 0.04 ⋅ 10 − 3 (Table  1). These changes occur under light exposure mainly due to polymer chain cross-linking in DRC, causing appearance of ever smaller voids owing to fragmentation of Ps-trapping sites [32 – 35]. Noteworthy, the sizes of Ps-trapping free-volume holes R [...]...|$|R
40|$|A simple {{weighting}} scheme for atomic refinement is discussed. The approach, called 'Bayesian weighting', {{is designed to}} be robust with respect to the bias that arises from the incomplete nature of the atomic model, which in macromolecular crystallography is typically quite serious. Bayesian weights are based on the meansquared residual errors over shells of resolution, with centric and acentric reflections considered separately and with allowances made for experimental uncertainties. Use of Bayesian weighting is shown in test cases typical for macromolecular crystallography to improve the accuracy of the refined coordinates when compared with schemes employing unit weights or <b>experimental</b> <b>variances.</b> I...|$|R
40|$|In large 2 -D DIGE proteomic {{studies with}} a large number of samples, it is {{essential}} to design the experimental setup to detect statistically significant protein changes under consideration of <b>experimental</b> <b>variances.</b> Herein are presented guidelines and general remarks on the extrac-tion of protein expression data by following protein spots on their way from first spot synchronization, detection, quantification and statistical analysis until excision and identification. Further discussion addresses common difficulties, potential pitfalls and strategies for dealing with gel-to-gel discrepancies, labeling inefficiencies, and dye- and batch effects which might not be obvious to novices and even more experienced users of DIGE technology...|$|R
40|$|This paper {{presents}} the {{experimental study of}} surface wave propagation in cementitious material with different shape and size but same volume content of thin inclusions that simulate distributed damage. The Rayleigh wave velocity changes almost up to 20 % depending on the inclusion shape, while the longitudinal velocity, the <b>experimental</b> <b>variance,</b> {{as well as the}} coherence of the signals is also affected. It is demonstrated that the material is strongly dispersive and caution should be taken for the interpretation of the wave measurements since the velocity is sensitive not only to the damage content but also to the “crack” size...|$|E
40|$|A {{combined}} {{experimental and}} numerical investigation was conducted into impact of rigid wedges on water in two-dimensional fluid conditions. Drop test experiments were conducted involving symmetric rigid wedges of varying angle and mass impacted onto water. The kinematic behaviour of the wedge and water was characterised using high-speed video. Numerical models were analysed in LS-DYNA® that combined regions of Smoothed Particle Hydrodynamics particles and a Lagrangian element mesh. The analysis captured {{the majority of}} experimental results and trends, {{within the bounds of}} <b>experimental</b> <b>variance.</b> Further, the combined modelling technique presented a highly attractive combination of computational efficiency and accuracy, making it a suitable candidate for aircraft ditching investigations...|$|E
40|$|International audienceHoney bee foragers often show a {{variation}} in laboratory proboscis extension learning during the foraging season, making comparisons between experiments difficult. We analysed whether the seasonal variation in learning performance {{was related to}} {{a variation}} in sucrose responsiveness in pollen and non-pollen foragers. Pollen foragers were very responsive to water and sucrose throughout the season. Non-pollen foragers were overall less responsive and showed more variation. Sucrose responsiveness strongly correlated with tactile and olfactory learning performance in pollen and non-pollen foragers throughout the season. Learning performance was significantly better when sucrose responsiveness was high than when it was low. We suggest conditioning bees that have uniform sucrose responsiveness throughout the season to reduce <b>experimental</b> <b>variance...</b>|$|E
40|$|We {{explore the}} impact of {{dispersion}} effects on location effect estimation and derive approximate joint confidence regions for pairs of correlated location effect estimates. A procedure for estimating location effects {{in the presence of}} a single dispersion effect is recommended. Confidence regions Correlation <b>Experimental</b> design <b>Variance...</b>|$|R
40|$|Software {{realization}} of the complex spectra decomposition on unknown number of similarcomponents is proposed. The algorithm is based on non-linear minimizing the sum of squared residuals of the spectrum model. For the adequacy checking the complex of criteria is used. It tests the model residuals correspondence with the normal distribution, equality to zero of their mean value and autocorrelation. Also the closeness of residuals and <b>experimental</b> data <b>variances</b> is checked...|$|R
40|$|In {{choosing}} an experimental design, emphasis {{should be placed}} on selecting one that minimizes <b>experimental</b> error <b>variance.</b> A comparison is made of the randomized complete block and lattice designs, giving {{the advantages and disadvantages of}} each. It was found that the lattice design was much better for a large no. of treatments, especially when there is a high index of soil heterogeneity. In the 6 experiments taken as the basis for this study, the relative efficiency of lattice designs as compared to randomized blocks was more than 105 percent. (CIAT...|$|R
40|$|Paper {{presented}} to the 10 th International Conference on Heat Transfer, Fluid Mechanics and Thermodynamics, Florida, 14 - 16 July 2014. An experimental investigation is conducted into the absorption of steam bubbles in a concentrated lithium bromide solution. The aim {{of the work is}} to determine whether such bubble absorption may be advantageously utilised within the absorber column section of an absorption heat transformer system. A glass bubble column is constructed and a high speed camera is used to track the collapse of steam bubbles at different temperatures and solution concentrations. A simple ordinary differential equation model is developed which is capable of explaining 96 % of the observed <b>experimental</b> <b>variance.</b> Very high mass transfer coefficients of ~ 0. 012 m/s are observed which indicates that this method of absorption may have significant advantages over alternative methods previously examined. cf 201...|$|E
40|$|Fourteen benzohydrazides {{have been}} {{synthesized}} and evaluated for their in vitro antifungal activity against the phytopathogenic fungus Botrytis cinerea. The best antifungal activity was observed for the N′,N′- dibenzylbenzohydrazides 3 b-d {{and for the}} N-aminoisoindoline-derived benzohydrazide 5. A quantitative structure-activity relationship (QSAR) study has been developed using a topological substructural molecular design (TOPS-MODE) approach to interpret the antifungal activity of these synthetic compounds. The model described 98. 3 % of the <b>experimental</b> <b>variance,</b> {{with a standard deviation}} of 4. 02. The influence of an ortho substituent on the conformation of the benzohydrazides was investigated by X-ray crystallography and supported by QSAR study. Several aspects of the structure-activity relationships are discussed in terms of the contribution of different bonds to the antifungal activity, thereby making the relationships between structure and biological activity more transparent. © 2007 American Chemical Society. Peer Reviewe...|$|E
40|$|Optimal {{design of}} {{experiments}} as well as proper analysis of data are dependent on knowledge of the experimental error. A {{detailed analysis of the}} error structure of kinetic data obtained with acetylcholinesterase showed conclusively that the classical assumptions of constant absolute or constant relative error are inadequate for the dependent variable (velocity). The best mathematical models for the experimental error involved the substrate and inhibitor concentrations and reflected the rate law for the initial velocity. Data obtained with other enzymes displayed similar relationships between experimental error and the independent variables. The new empirical error functions were shown superior to previously used models when utilized in weighted non-linear-regression analysis of kinetic data. The results suggest that, in the spectrophotometric assays used in the present study, the observed <b>experimental</b> <b>variance</b> is primarily due to errors in determination of the concentrations of substrate and inhibitor and not to error in measuring the velocity...|$|E
40|$|Factors {{associated}} with {{accuracy and precision}} in the enumeration of aquatic aerobic heterotrophs by the spread plate method were evaluated by using a nested analysis of <b>variance</b> <b>experimental</b> design. <b>Variances</b> {{associated with}} individual components of the spread plate procedure were isolated, and optimal replications of each step were allocated. A practical scheme for optimal allocation of resources is proposed, consisting of four subsamples and two plates per subsample and yielding a total variance decrease of 70 % from a single-subsample, 10 -plate series. Data transformation was, in general, unnecessary for intraexperiment or intrasample statistical analysis, whereas interexperiment or intersample comparisons may require transformation of data. Rapid changes {{in the numbers of}} organisms in stored water samples were observed that were not reproducible and did not follow detectable trends, with increases or decreases in counts occurring in samples regardless of whether they were stored at room temperature or refrigerated, or stored in plastic or glass containers. Rapid sample handling is strongly recommended to minimize variations in the microbial populations of samples for aquatic environments...|$|R
40|$|Phenotyping assays {{in plant}} {{pathology}} using detached plant parts are multi-phase experimental processes. Such assays involve growing plants in field or controlled-environment trials (Phase 1) and then subjecting a sample removed from each plant to disease assessment, usually under laboratory conditions (Phase 2). Each phase {{may be subject}} to nongenetic sources of variation. To be able to separate these sources of variation in both phases from genetic sources of variation requires a multi-phase experiment with an appropriate experimental design and statistical analysis. To achieve this, a separate randomization is required for each phase, with additional replication in Phase 2. In this article, Phomopsis leaf and pod blight (caused by Diaporthe toxica) of Lupinus albus was used as a case study to apply a multi-phase experimental approach to identify genetic resistance to this pathogen, and demonstrate the principles of sound experimental design and analysis in detached plant part assays. In seven experiments, 250 breeding lines, cultivars, landraces, and recombinant in-bred lines from a mapping population of L. albus were screened using detached, inoculated leaves, and/or pods. The <b>experimental,</b> non-genetic <b>variance</b> in Phase 2 varied in magnitude compared to the Phase 1 <b>experimental,</b> non-genetic <b>variance.</b> The reliability of prediction for resistance to Phomopsis pod blight was high (mean of 0. 70 in seven experiments), while reliability of prediction for leaf assays was lower (mean 0. 35 – 0. 51 depending on the scoring method used) ...|$|R
30|$|The {{present study}} aimed to {{demonstrate}} the capability of SOD as a potential radical scavenger for ancient silk fabrics. Because genuine historic silk fabrics were not appropriate for {{a large number of}} repeated tests, artificially carbonized silk samples were used as substitutes. To determine the optimal <b>experimental</b> conditions, <b>variance</b> analysis of double factors cross classification without repetition was performed. The scavenging effects were characterized by EPR. The ageing resistance of the scavenged samples was evaluated via dynamic mechanical analysis (DMA). Then, the method was applied to ancient carbonized silk fabrics. The fibre morphology of samples before and after treatment were observed using scanning electron microscope (SEM).|$|R
40|$|Experimental designs address {{themselves}} to two concerns: providing answers to research questions and controlling variance. The latter involves threats to {{internal and external}} validity. The threat to internal validity constitutes the greatest difficulty because of maturation, history, experimental mortality, and differential selection. Examples of eight basic research design paradigms are made, pointing to the relative strengths of design in maximizing <b>experimental</b> <b>variance</b> and minimizing extraneous and error variance. Of most concern is whether a particular instructional strategy is resulting in measureable results {{on the part of}} students. The following are elements of a successful research proposal: (1) the development of a clear statement of the research problem, including an explanation of variate and criterion variables in the study, type of relationship between the variables, and the target population; (2) justification for the research approach; and (3) development of a statement of operational research objectives and/or hypothesis. (CA...|$|E
40|$|We have {{developed}} a depolymerase computer model that uses a minimization routine. The model is designed so that, given experimental bond-cleavage frequencies for oligomeric substrates and experimental Michaelis parameters {{as a function of}} substrate chain length, the optimum subsite map is generated. The minimized sum of the weighted-squared residuals of the experimental and calculated data is used as a criterion of the goodness-of-fit for the optimized subsite map. The application of the minimization procedure to subsite mapping is explored through the use of simulated data. A procedure is developed whereby the minimization model can be used to determine the number of subsites in the enzymic binding region and to locate the position of the catalytic amino acids among these subsites. The degree of propagation of <b>experimental</b> <b>variance</b> into the subsite-binding energies is estimated. The question of whether hydrolytic rate coefficients are constant or a function of the number of filled subsites is examined...|$|E
40|$|Agent {{evaluation}} in stochastic domains can be difficult. The commonplace approach of Monte Carlo evaluation can involve a prohibitive number of simulations when {{the variance of}} the outcome is high. In such domains, variance reduction techniques are necessary, but these techniques require careful encoding of domain knowledge. This paper introduces baseline as a simple approach to creating low variance estimators for zero-sum multi-agent domains with high outcome variance. The baseline method leverages the self play of any available agent to produce a control variate for variance reduction, subverting any extra complexity inherent with traditional approaches. The baseline method is also applicable in situations where existing techniques either require extensive implementation overhead or simply cannot be applied. <b>Experimental</b> <b>variance</b> reduction results are shown for both cases using the baseline method. Baseline is shown to surpass state-of-the-art techniques in three-player computer poker and is competitive in two-player computer poker games. Baseline also shows variance reduction in human poker and in a mock Ad Auction tournament from the Trading Agent Competition, domains where variance reduction methods are not typically employed...|$|E
40|$|This article gives a brief {{overview}} of the popular methods for esti- mating variance components in linear models and describes several ways to obtain such estimates in Stata for various experimental designs. The article’s emphasis is on using xtmixed to estimate variance components. Prior to Stata 9, loneway could be used to estimate variance components for one-way random-effects models. For other <b>experimental</b> designs, <b>variance</b> components could be computed manually using saved results after anova. The latter approach is viable but requires tedious computations for complicated experimental designs. Instead, as of Stata 9, vari- ance components are easily obtained by using xtmixed. Copyright 2006 by StataCorp LP. <b>variance</b> components, <b>experimental</b> design, ANOVA, REML, ML, multilevel, random coefficients, mixed models...|$|R
40|$|An easy to use, {{robust and}} {{accurate}} frequency-domain procedure for estimating the spectral performance of waveform digitizers is considered in this paper. Its properties are analyzed and almost unbiased estimators are proposed along with simple but accurate expressions for their <b>variances.</b> <b>Experimental</b> {{results are presented}} to validate the proposed analysis. Directions and criteria useful {{for the design of}} the test procedure are also included. Keywords: Analog-digital conversion, quantization, frequency domain analysis, discrete Fourier transform. I...|$|R
40|$|Chaotic mixing {{strategies}} produce high mixing {{rates in}} mi-crofluidic channels and other applications. In prior numerical and <b>experimental</b> work the <b>variance</b> of a tracer field in a chaotic mixer {{has been observed}} to decay rapidly after an initial slower transient. We relate this to the cutoff phenomenon observed in finite Markov chains and provide numerical {{evidence to suggest that}} chaotic mixing indeed exhibits cutoff. We provide results for a herringbone passive microfluidic mixer and the Standard Map...|$|R
40|$|<b>Experimental</b> <b>variance</b> {{is a major}} {{challenge}} when dealing with high-throughput sequencing data. This variance has several sources: sampling replication, technical replication, variability within biological conditions, and variability between biological conditions. The high per-sample cost of RNA-Seq often precludes {{the large number of}} experiments needed to partition observed variance into these categories as per standard ANOVA models. We show that the partitioning of within-condition to between-condition variation cannot reasonably be ignored, whether in single-organism RNA-Seq or in Meta-RNA-Seq experiments, and further find that commonly-used RNA-Seq analysis tools, as described in the literature, do not enforce the constraint that the sum of relative expression levels must be one, and thus report expression levels that are systematically distorted. These two factors lead to misleading inferences if not properly accommodated. As it is usually only the biological between-condition and within-condition differences that are of interest, we developed ALDEx, an ANOVA-like differential expression procedure, to identify genes with greater between- to within-condition differences. We show that the presence of differential expression and the magnitude of these comparative differences can be reasonably estimated with even very small sample sizes...|$|E
40|$|To {{identify}} and develop drought tolerant maize (Zea mays L.), high-throughput and cost-effective screening methods are needed. In dicot crops, measuring survival and recovery of seedlings {{has been successful}} in predicting drought tolerance but has not been reported in C 4 grasses such as maize. Seedlings of sixty-two diverse maize inbred lines and their hybrid testcross progeny were evaluated for germination, survival and recovery after a series of drought cycles. Genotypic differences among inbred lines and hybrid testcrosses were best explained approximately 13 and 18 days after planting, respectively. Genotypic effects were significant and explained over 6 % of <b>experimental</b> <b>variance.</b> Specifically three inbred lines had significant survival, and 14 hybrids had significant recovery. However, no significant correlation was observed between hybrids and inbreds (R 2 = 0. 03), indicating seedling stress response is more useful as a secondary screening parameter in hybrids than in inbred lines per se. Field yield data under full and limited irrigation indicated that seedling drought mechanisms were independent of drought responses at flowering in this study...|$|E
40|$|We {{analyze the}} psychophysical {{responses}} of human observers to an ensemble of monomolecular odorants. Each odorant {{is characterized by}} a set of 146 perceptual descriptors obtained from a database of odor character profiles. Each odorant is therefore represented by a point in highly multidimensional sensory space. In this work we study the arrangement of odorants in this perceptual space. We argue that odorants densely sample a two-dimensional curved surface embedded in the multidimensional sensory space. This surface can account {{for more than half of}} the variance of the psychophysical data. We also show that only 12 % of <b>experimental</b> <b>variance</b> cannot be explained by curved surfaces of substantially small dimensionality (< 10). We suggest that these curved manifolds represent the relevant spaces sampled by the human olfactory system, thereby providing surrogates for olfactory sensory space. For the case of 2 D approximation, we relate the two parameters on the curved surface to the physico-chemical parameters of odorant molecules. We show that one of the dimensions is related to eigenvalues of molecules ’ connectivity matrix, while the other is correlated with measures of molecules ’ polarity. We discuss the behavioral significance of these findings...|$|E
30|$|Prior {{to sample}} collection, all {{polyester}} swabs were pre-sterilized and pre-moistened with 3  mL of sterile molecular-grade water (W 4502, Sigma Aldrich, St. Louis, MO) before the spacecraft surface was sampled. Immediately after sample collection, swab heads were cut with pre-sterilized wire cutters and placed into 3  mL of sterile molecular-grade water (Sigma). Swab heads were then immediately {{taken to the}} laboratory where the ATP assay was conducted, or {{were placed in the}} refrigerator at 4  °C for < 2  h. A Check-Light HS Set kit was used, in accordance with manufacturer instructions (Kikkoman Corporation, Noda City, Japan). Sample collection tubes containing the polyester swabs were thoroughly mixed using vortex to release any attached microbial cells. The total ATP content was measured in replicates (a minimum of three per sample) using well-established procedures (Venkateswaran et al. 2003). The ATP assay employed 0.1  mL of sample and 0.1  mL of a detergent solution for cell lysis. The lysis solution provided by the manufacturer contained benzalkonium chloride and a proprietary ATP-releasing agent. The mixture was incubated at room temperature for 1  min, and then 0.1  mL of luciferin-luciferase reagent was added. The sample was then vortexed and bioluminescence measured with a luminometer (Lumitester K- 200, Kikkoman Corporation). For each set of assays a standard curve using pure ATP (Sigma, St. Louis, MO) in serial dilutions was carried out to overcome any <b>experimental</b> <b>variances,</b> operator differences, and instrument discrepancies, etc. To ensure the most accurate results, new reagents, disposables, and water were used for each ATP processing event. Negative controls of water were included in all experiments. Field blanks were conducted and processed after every 10 spacecraft samples, where a sterile swab was exposed to the clean room environment but not actively brought in contact with spacecraft surfaces. Samples used for ATP analysis were not subjected to heat shock or sonication as compared to those of the NSA.|$|R
25|$|The {{development}} of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists, the Standard Model is a paradigm of a quantum field theory, which exhibits {{a wide range of}} physics including spontaneous symmetry breaking, anomalies and non-perturbative behavior. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) in an attempt to explain <b>experimental</b> results at <b>variance</b> with the Standard Model, such as the existence of dark matter and neutrino oscillations.|$|R
40|$|In this paper, {{we present}} a method for {{automatically}} classifying/recognizing the shoeprint images based on the outsole pattern. Shoeprints are distinctive patterns often found at crime scenes that can provide valuable forensic evidence. Directionality is the most obvious feature in these shoeprints. For extracting features corresponding to the directionality, co-occurrence matrices, Fourier transform, and a directional matrix are applied to the shoeprint image. With the stage of principal component transform, the method is invariant to rotation and translation <b>variance.</b> <b>Experimental</b> results demonstrate {{the performance of the}} method. Keywords...|$|R
40|$|DNA gyrase is a {{well-established}} antibacterial target {{consisting of two}} subunits, GyrA and GyrB, in a heterodimer A(2) B(2), where GyrB catalyzes the hydrolysis of ATP. Cyclothialidine (Ro 09 - 1437) has been considered as a promising inhibitor whose modifications might lead to more potent compounds against the enzyme. We report {{here for the first}} time, QSAR studies regarding to ATPase inhibitors of DNA Gyrase. 1 D, 2 D and 3 D descriptors from DRAGON software were used on a set of 42 cyclothialidine derivatives. Based on the core of the cyclothialidine GR 122222 X, different conformations were created by using OMEGA. FRED was used to dock these conformers in the cavity of the GyrB subunit to select the best conformations, paying special attention to the 12 -membered ring. Three QSAR models were developed considering the dimension of the descriptors. The models were robust, predictive and good in statistical significance, over 70 % of the <b>experimental</b> <b>variance</b> was explained. Interpretability of the models was possible by extracting the SAR(s) encoded by these predictive models. Analyzing the compound-enzyme interactions of the complexes obtained by docking allowed us to increase the reliability of the information obtained for the QSAR models. status: publishe...|$|E
40|$|This thesis {{describes}} {{the design and}} characterization of a microcalorimeter used to aid drug discovery. There are four key functional requirements for the device: (1.) 8. 4 [mu]J energy resolution, (2.) 20 [mu]L reactant volume (combined total), (3.) 10 % <b>experimental</b> <b>variance,</b> and (4.) 100 [mu]K baseline calorimeter drift over a two hour period. The calorimeter utilizes a novel heat sensor. This heat sensor combines thermal expansion and the dynamic response of an oscillating ribbon to transduce the signal from a heat event. A vacuum chamber improved {{the sensitivity of the}} sensor by approximately an order of magnitude by significantly reducing the losses due to air friction in the resonant sensor. Additional components such as a position sensor, temperature controlled vacuum chamber, software, and a syringe pump were constructed to complete the calorimeter system. The current calorimeter prototype nearly meets each functional requirement. In addition, the current sensitivity of the instrument is near that of a commercially available calorimeter but uses almost two orders of magnitude less solution. Finally, all of our calorimeter components are designed, built, integrated, and ready to begin more rigorous biological solution experimentation. by Scott Jacob McEuen. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 2008. Includes bibliographical references (leaves 61 - 62) ...|$|E
40|$|ABSTRACT: The present {{investigation}} {{was carried out}} to study the heritability, correlation and path coefficient analysis in 45 hybrids and ten parents. In pooled analysis of variability parameters revealed that the phenotypic coefficients of variation (PCV) were higher than genotypic coefficient of variation for all the characters studied indicating the role of <b>experimental</b> <b>variance</b> to the total variance. The magnitude of PCV and GCV was high for grain yield per plant followed by ear height, number of kernels per row, 100 -seed weight, ear length, plant height, ear girth and number of kernel rows per ear. High heritability coupled with high genetic advance as percentage of mean was observed for ear height, grain yield per plant, plant height, number of kernels per row and ear length. In general, magnitudes of genotypic correlations {{were found to be}} higher than phenotypic correlations. The results indicated that grain yield was positively and significantly associated with 100 -seed weight, ear girth, ear length, number of kernels per row, plant height, number of kernel row per ear and ear height. Days to 50 percent tasseling had largest direct effect on grain yield per plant followed by 100 -seed weight, ear length, days to maturity, ear height, number of kernels per row, ear height, number of kernel rows per and plant height...|$|E
50|$|The {{development}} of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists, the Standard Model is a paradigm of a quantum field theory, which exhibits {{a wide range of}} physics including spontaneous symmetry breaking, anomalies and non-perturbative behavior. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) in an attempt to explain <b>experimental</b> results at <b>variance</b> with the Standard Model, such as the existence of dark matter and neutrino oscillations.|$|R
40|$|Two-level {{fractional}} factorial {{designs are}} important statistical tools in particular for screening experiments and {{in situations where}} the cost of experimentation is high. It is common that no replicates are taken. Also full factorial designs may often be performed without replicates. This means that there does not exist any within series estimate of <b>experimental</b> error <b>variance.</b> The problem has been overcome in dierent ways. If the fraction is not too small, the estimates of interactions of some high orders, not aliased with main effects or low order interactions {{may be used for}} variance estimation. A common subjective method is to make a normal or half normal plot of all effect estimates and then select as important (signicant) the eects with estimates deviating much from a straight line. This method by Daniel (1959) is commonly adopted in textbooks, e. g. Box et al. (1978) and Montgomery (1984). There exist also some methods which have a warranted level of significan...|$|R
40|$|The {{idea of a}} {{methodology}} capable of determining in a precise and practical way the optimal sample size came from studying Monte Carlo simulation models concerning financial problems, risk analysis, and supply chain forecasting. In these cases the number of extractions from the frequency distributions characterizing the model is inadequate or limited to just one, so {{it is necessary to}} replicate simulation runs many times in order to obtain a complete statistical description of the model variables. Generally, as shown in the literature, the sample size is fixed by the experimenter based on empirical assumptions without considering the impact on result accuracy in terms of tolerance interval. In this paper, the authors propose {{a methodology}} by means of which it is possible to graphically highlight the evolution of <b>experimental</b> error <b>variance</b> {{as a function of the}} sample size. Therefore, the experimenter can choose the best ratio between the experimental cost and the expected results...|$|R
