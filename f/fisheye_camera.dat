81|83|Public
5|$|Despite {{receiving}} financing {{from other}} sources, including Saul Zaentz (who agreed {{to distribute the}} soundtrack album on his Fantasy Records label), the budget was tight enough to exclude pencil tests, so Bakshi had to test the animation by flipping an animator's drawings in his hand before they were inked and painted. When a cameraman realized that the cels for the desert scenes were not wide enough and revealed the transparency, Bakshi painted a cactus to cover the mistake. Very few storyboards were used. Bakshi and Vita walked around the Lower East Side, Washington Square Park, Chinatown and Harlem, taking moody snapshots. Artist Ira Turek inked the outlines of these photographs onto cels with a Rapidograph, the technical pen preferred by Crumb, giving the film's backgrounds a stylized realism virtually unprecedented in animation. The tones of the watercolor backgrounds were influenced {{by the work of}} Ashcan School painters such as George Luks and John French Sloan. Among other unusual techniques, bent and <b>fisheye</b> <b>camera</b> perspectives were used to portray the way the film's hippies and hoodlums viewed the city. Many scenes featured documentary recordings of real conversations in place of scripted dialogue—this too would become a signature of Bakshi's.|$|E
25|$|Sega did not fare as well. They had tasked their American studio, Sega Technical Institute, with {{bringing}} Sonic the Hedgehog into 3D. Their project, titled Sonic Xtreme, {{was to have}} {{featured a}} radically different approach for the series, with an exaggerated <b>fisheye</b> <b>camera</b> and multidirectional gameplay reminiscent of Bug!. Its development was rocky, {{due in part to}} conflicts with Sega Enterprises in Japan and a rushed schedule, and the game never made it to market.|$|E
2500|$|Because it was cheaper for Ira Turek {{to trace}} {{photographs}} {{to create the}} backgrounds, Bakshi and Johnnie Vita walked around {{the streets of the}} Lower East Side, Washington Square Park, Chinatown and Harlem to take moody snapshots. Turek inked the outlines of these photographs onto cels with a Rapidograph, the technical pen preferred by Crumb, giving the film's backgrounds stylized realism that had never been portrayed in animation before. After Turek completed a background drawing in ink on an animation cel, the drawing would be photocopied onto watercolor paper for Vita and onto animation paper for use in matching the characters to the backgrounds. When Vita finished his painting, Turek's original drawing, on the cel, would be placed over the watercolor, obscuring the photocopy lines on the painting. However, not every background was taken from live-action sources. The tones of the watercolor backgrounds were influenced by the [...] "Ash Can style" [...] of painters, which includes George Luks and John French Sloan. The film also used bent and <b>fisheye</b> <b>camera</b> perspectives in order to replicate the way the film's hippies and hoodlums viewed the city.|$|E
40|$|This paper {{proposes a}} novel {{calibration}} method for <b>fisheye</b> <b>cameras</b> using sphere images {{which is an}} extension of the calibration method for central catadioptric cameras. We show that, each sphere image under <b>fisheye</b> <b>cameras</b> is tangent to the modified calibrating conic (MCC) at two double-contact image points. The calibration method is proposed by first finding the double-contact image points, then fitting the MCC, and final obtaining the intrinsic parameters using the LDLT factorization of the MCC. 1...|$|R
40|$|Part 1 : Image Processing and AnalysisInternational audienceVolume {{reconstruction}} from silhouettes is a known {{subject in}} the case of projective cameras. The use of dioptric omni-directional <b>cameras</b> (<b>fisheye)</b> that exhibit semispheric Field of View (FoV) allows simultaneous imaging of the whole available space. In this work we employ two <b>fisheye</b> <b>cameras</b> in order to acquire silhouettes of humans that move within the imaged indoor space without any kind of restriction. We present the basic algorithm for reconstructing the volumetric model of a human {{in the case of}} using two images of its silhouette, acquired by the <b>fisheye</b> <b>cameras.</b> Then we extend this algorithm {{in the case of a}} human moving along any trajectory a) assuming no pose change and b) considering pose change during motion. Quantitative results from synthetic data indicate that volumes can be reconstructed with high accuracy (error of few cm), from a small number of positions using only two <b>fisheye</b> <b>cameras.</b> The proposed algorithm achieves the same level of accuracy in the case of recovering the volume of multiple poses under same conditions...|$|R
50|$|A {{feature of}} some newer VMS is the {{capability}} to show multiple camera views from a single recorded stream. This utilizes digital PTZ of high megapixel cameras, and may also {{be referred to as}} client-side dewarping for <b>fisheye</b> <b>cameras.</b>|$|R
50|$|Finally, {{dewarping}} is a computationally intensive task. Although multiple {{views of}} a single <b>fisheye</b> <b>camera</b> are possible, the combined processing to dewarp multiple high resolution views can overload the viewer computer.|$|E
5000|$|DSM 4.0 is released. New features: GUI refresh, Cloud Station to {{synchronize}} files {{between various}} client computers, Package Center for managing modular DSM Applications, enhanced mobile device support including Amazon Kindle Fire, and <b>fisheye</b> <b>camera</b> support in Surveillance Station.|$|E
50|$|Rist {{achieved}} notoriety with Pickelporno (Pimple porno) (1992), a {{work about}} {{the female body}} and sexual excitation. The <b>fisheye</b> <b>camera</b> moves over the bodies of a couple. The images are charged by intense colors, and are simultaneously strange, sensual, and ambiguous.|$|E
40|$|In {{this paper}} we propose a method of {{synchronizing}} automatically the full-view image and GPS information which are recorded independently, respectively, for route map building. The full-view image sensor is composed of a pair of <b>fisheye</b> <b>cameras.</b> By synchronizing the two image streams of the pair of <b>fisheye</b> <b>cameras</b> a seamless full-view image is acquired; by synchronizing the full-view image with GPS information the full-view image stream is re-sampled uniformly so that the full-view image is registered to a digital map with approximately equal distance along routes. The above synchronization is achieved by matching the motion of these sensors. To estimate the motion between consecutive frames, a small motion model of spherical image is used. The experimental results show the effectiveness of our method. 1...|$|R
50|$|This can {{significantly}} reduce data storage requirements, where {{two or more}} separate cameras would have been used previously. In the case of <b>fisheye</b> <b>cameras,</b> {{it is possible for}} one camera to replace 10 or more separate camera views, while only recording the one original panoramic fisheye view.|$|R
50|$|Fixed-view <b>fisheye</b> <b>cameras</b> have a bowl-shaped {{360-degree}} view. When {{mounted overhead}} pointing straight down, {{part of the}} viewed space appears sideways or upside-down on the VMS. For these cameras, the digital PTZ may also include a rotation feature to digitally rotate the view so that all zoomed-in viewed areas appear right-side-up.|$|R
5000|$|In 2015, the IDIS Super <b>Fisheye</b> <b>Camera</b> {{received}} multiple industry accolades {{for technological}} performance and innovation upon release, and in 2016, the company debuted innovative [...] "Smart UX" [...] controls for point-and-zoom technology that increase accuracy {{and ease of}} use for camera operators.|$|E
5000|$|In 2015, {{industry}} editor Larry Anderson specifically {{noted the}} company's [...] "timely emphasis on end-to-end IP solutions that are 'plug-and-play'" [...] in covering the company's {{debut in the}} North American market. Benchmark Magazine, in a 2015 review of the IDIS <b>Fisheye</b> <b>camera,</b> noted the company's high performance technology, specifically praising IDIS's clean imagery, and admirably intuitive and straightforward configuration/installation.|$|E
50|$|A <b>fisheye</b> <b>camera</b> has {{a special}} lens that {{typically}} has a 180 degree field of view and can see 360 degrees around the lens. When mounted flat on a ceiling, {{it is possible for}} a single fixed camera to see the entire space below it without moving. However the spherical view causes angular distortion of straight lines, giving objects a strange bulged and deformed appearance.|$|E
5000|$|... #Caption: The view of {{buildings}} in Downtown Los Angeles through the <b>Fisheye</b> 2 <b>camera.</b>|$|R
40|$|This paper aims at realizing an {{automatic}} parking method through a bird's eye view vision system. With this method, vehicles can make robust and real-time detection {{and recognition of}} parking spaces. During parking process, the omnidirectional information of the environment {{can be obtained by}} using four on-board <b>fisheye</b> <b>cameras</b> around the vehicle, which are {{the main part of the}} bird's eye view vision system. In order to achieve this purpose, a polynomial fisheye distortion model is firstly used for camera calibration. An image mosaicking method based on the Levenberg-Marquardt algorithm is used to combine four individual images from <b>fisheye</b> <b>cameras</b> into one omnidirectional bird's eye view image. Secondly, features of the parking spaces are extracted with a Radon transform based method. Finally, double circular trajectory planning and a preview control strategy are utilized to realize autonomous parking. Through experimental analysis, we can see that the proposed method can get effective and robust real-time results in both parking space recognition and automatic parking...|$|R
40|$|For {{autonomous}} navigation of micro aerial vehicles (MAVs), {{a robust}} detection of obstacles with onboard sensors {{is necessary in}} order to avoid collisions. Cameras have the potential to perceive the surroundings of MAVs for the reconstruction of their 3 D structure. We equipped our MAV with two <b>fisheye</b> stereo <b>camera</b> pairs to achieve an omnidirectional field-of-view. Most stereo algorithms are designed for the standard pinhole camera model, though. Hence, the distortion effects of the fisheye lenses must be properly modeled and model parameters must be identified by suitable calibration procedures. In this work, we evaluate the use of real-time stereo algorithms for depth reconstruction from <b>fisheye</b> <b>cameras</b> together with different methods for calibration. In our experiments, we focus on obstacles occurring in urban environments that are hard to detect due to their low diameter or homogeneous texture. 1...|$|R
50|$|Sega did not fare as well. They had tasked their American studio, Sega Technical Institute, with {{bringing}} Sonic the Hedgehog into 3D. Their project, titled Sonic Xtreme, {{was to have}} {{featured a}} radically different approach for the series, with an exaggerated <b>fisheye</b> <b>camera</b> and multidirectional gameplay reminiscent of Bug!. Its development was rocky, {{due in part to}} conflicts with Sega Enterprises in Japan and a rushed schedule, and the game never made it to market.|$|E
50|$|The SunEye-210 is a shade {{measurement}} tool. The patented device {{includes a}} <b>fisheye</b> <b>camera,</b> digital compass, digital inclinometer, and GPS. It captures {{an image of}} the sky including the horizon, superimposes the sunpaths on top of the image, and calculates the solar access for that location. Shade {{is one of the biggest}} spoilers of energy generation in a PV system and measuring it is the best way to avoid mounting modules in the shade.|$|E
5000|$|Located in the {{elevator}} bays on the first six levels of Sidney & Lois Eskenazi Hospital, these Temporal Synapse feature walls consist of cast acrylic panels that feature a graphic pattern based upon cellular structures found within the body. Through {{the use of a}} <b>fisheye</b> <b>camera,</b> video tracking technology tracks visitors’ positions in space, which, in turn, directs reciprocal “cells” to light up, visually representing the interaction between those waiting and their environment. While each of the panels measure 149" [...] in width, they range in height from 95"-107", depending on ceiling height.|$|E
40|$|By {{defining}} {{the disparity of}} a spherical stereo we reformulate the real-time stereo problem. Based upon this definition the realtime spherical stereo becomes a general model which can cope with cameras with any wide field of view including conventional ones. By transforming the rectified spherical images to latitudelongitude representation, the correspondence of feature points can be speeded up by the same processing as planar stereo images. The effectiveness {{of this approach is}} shown by realizing a real-time spherical stereo using a pair of <b>fisheye</b> <b>cameras...</b>|$|R
40|$|On {{the way to}} {{autonomous}} robots, {{perception is}} a key point. Among all the perception senses, vision is undoubtedly the most important for the information it can provide. However, {{it is not easy}} to identify what is seen from the provided visual input. On this regard, inspired by humans, we have studied motion as a primary cue. Particularly, we present a computational solution for motion detection, object location and tracking from images captured by perspective and <b>fisheye</b> <b>cameras.</b> The proposed approach has been validated with an extensive set of experiments and applications using different testbeds of real environments with real and/or virtual targets...|$|R
40|$|In this paper, {{we propose}} an {{adaptation}} of camera pro-jection models for <b>fisheye</b> <b>cameras</b> into the plane-sweeping stereo matching algorithm. This adaptation allows us to do plane-sweeping stereo directly on fisheye images. Our ap-proach also works for other non-pinhole cameras such as omnidirectional and catadioptric cameras when using the unified projection model. Despite the simplicity of our pro-posed approach, {{we are able to}} obtain full, good quality and high resolution depth maps from the fisheye images. To verify our approach, we show experimental results based on depth maps generated by our approach, and dense models produced from these depth maps. 1...|$|R
50|$|A {{long-established}} legend {{attached to}} the Glenfinnan Viaduct was that a horse had fallen {{into one of the}} piers during construction in 1898 or 1899. In 1987, Professor Roland Paxton failed to find evidence of a horse at Glenfinnan using a <b>fisheye</b> <b>camera</b> inserted into boreholes in the only two piers large enough to accommodate a horse. In 1997, on the basis of local hearsay, he investigated the Loch nan Uamh Viaduct by the same method but found the piers to be full of rubble. Using scanning technology in 2001, the remains of the horse and cart were found at Loch nan Uamh, within the large central pylon.|$|E
50|$|In 1987, Professor Roland Paxton, from Heriot-Watt University, {{investigated}} the legend that a horse {{had fallen into}} a pier during construction of Glenfinnan Viaduct in 1898 or 1899. However, after inserting a <b>fisheye</b> <b>camera</b> into boreholes made into the only two piers {{large enough to accommodate}} a horse, no animal remains were found. In 1997, on the basis of local hearsay, Paxton investigated Loch nan Uamh viaduct using the same method but found only rubble as well. In 2001, he returned to Loch nan Uamh with the latest microwave scanning technology and found the remains of a horse and cart within the viaduct's central pylon.|$|E
50|$|One of {{the entry}} team members removed a ceiling {{panel in the}} hallway between the two {{buildings}} and inserted a pole-mounted mirror. He was able to observe the subjects directing hostages to place large boxes against {{the back door to}} block entry. Once the door was barricaded, the area was abandoned. A <b>fisheye</b> <b>camera</b> was installed by the team but was of limited use because of the design of the store, showing only a portion of the showroom near the door. By this time, the hostages had been tied up with speaker wire and had been arranged inside the store's glass front entrance doors in standing and kneeling positions.|$|E
40|$|Unmanned Aerial Vehicle (UAV) with {{adequate}} sensors enable new {{applications in the}} scope between expensive, large-scale, aircraftcarried remote sensing and time-consuming, small-scale, terrestrial surveyings. To perform these applications, cameras and laserscanners are a good sensor combination, due to their complementary properties. To exploit this sensor combination the intrinsics and relative poses of the individual cameras and the relative poses of the cameras and the laserscanners have to be known. In this manuscript, we present a calibration methodology for the Unified Intrinsic and Extrinsic Calibration of a Multi-Camera-System and a Laserscanner (UCalMiCeL). The innovation of this methodology, which is an extension to the calibration of a single camera to a line laserscanner, is an unifying bundle adjustment step to ensure an optimal calibration of the entire sensor system. We use generic camera models, including pinhole, omnidirectional and <b>fisheye</b> <b>cameras.</b> For our approach, the laserscanner and each camera have to share a joint field of view, whereas the fields {{of view of the}} individual cameras may be disjoint. The calibration approach is tested with a sensor system consisting of two <b>fisheye</b> <b>cameras</b> and a line laserscanner with a range measuring accuracy of 30   mm. We evaluate the estimated relative poses between the cameras quantitatively by using an additional calibration approach for Multi-Camera-Systems based on control points which are accurately measured by a motion capture system. In the experiments, our novel calibration method achieves a relative pose estimation with a deviation below 1. 8 ° and 6. 4   mm...|$|R
40|$|For {{automotive}} applications, a 3 D {{perception of}} the car‘s surroundings is crucial, both for driver assistance and for safety systems. In addition, a large field of view for applications such as intersection assistance. A popular option to obtain 3 D measurements of the surroundings is to use two passive cameras (stereo vision). Object detection is possible using this information. Automotive Applications based on object detection range from Adaptive Cruise Control, collision mitigation systems to intersection assistance systems. Stereo vision with conventional cameras only delivers a limited field of view. We extend {{the field of view}} investigating active cameras (camera on a pan-tilt unit), panoramic cameras (camera and mirror), and <b>fisheye</b> <b>cameras</b> (<b>camera</b> with a <b>fisheye</b> lens). The goal of this work is to find a setup that covers a large field of view and yields a good performance for object detection. The requirements for object detection are described and several options to meet these requirements are presented. We discuss their advantages and drawbacks and present results for all options...|$|R
40|$|The European V-Charge project {{seeks to}} develop fully {{automated}} valet parking and charging of electric vehicles using only low-cost sensors. One {{of the challenges}} is to implement robust visual localization using only cameras and stock vehicle sensors. We integrated four monocular, wide-angle, <b>fisheye</b> <b>cameras</b> on a consumer car and implemented a mapping and localization pipeline. Visual features and odometry are combined to build and localize against a keyframe-based three dimensional map. We report results for {{the first stage of}} the project, based on two months worth of data acquired under varying conditions, with the objective of localizing against a map created offline. © 2013 IEEE...|$|R
5000|$|Because it was cheaper for Ira Turek {{to trace}} {{photographs}} {{to create the}} backgrounds, Bakshi and Johnnie Vita walked around {{the streets of the}} Lower East Side, Washington Square Park, Chinatown and Harlem to take moody snapshots. Turek inked the outlines of these photographs onto cels with a Rapidograph, the technical pen preferred by Crumb, giving the film's backgrounds stylized realism that had never been portrayed in animation before. After Turek completed a background drawing in ink on an animation cel, the drawing would be photocopied onto watercolor paper for Vita and onto animation paper for use in matching the characters to the backgrounds. When Vita finished his painting, Turek's original drawing, on the cel, would be placed over the watercolor, obscuring the photocopy lines on the painting. However, not every background was taken from live-action sources. The tones of the watercolor backgrounds were influenced by the [...] "Ash Can style" [...] of painters, which includes George Luks and John French Sloan. The film also used bent and <b>fisheye</b> <b>camera</b> perspectives in order to replicate the way the film's hippies and hoodlums viewed the city.|$|E
5000|$|On November 21, 2014, a {{music video}} {{for the song}} was {{released}} on Beyoncé's official YouTube account. The clip {{was shot in the}} style of a home-made visual with the singer dancing at various locations: patio, balcony, hotel suite, a bathroom and in front of a Christmas tree and it was shot on a Galaxy Alpha camera. It starts with Beyoncé dancing on a balcony dressed with a sweatshirt with the word [...] "Kale", leopard print panties and knee pads; from there it proceeds to a hotel suite. Beyoncé is joined by five female dancers to perform a choreography. Other scenes show her rolling dice on her dancer's buttock and blowing her hair with a dryer. She is also seen getting out of a present box in front of a Christmas tree and dialing a phone number on her foot. The clip finishes with Beyoncé blowing a New Year whistle while wearing glasses shaped {{in the form of the}} number 2015. Throughout the whole video, the singer is seen passing through quick cuts and wardrobe changes. The scenes shot at the balcony were filmed using a <b>fisheye</b> <b>camera.</b> Beyoncé's daughter Blue Ivy Carter also makes a cameo appearance.|$|E
50|$|After Bakshi {{pitched the}} project to every major Hollywood studio, Warner Bros. bought it and promised an $850,000 budget. Bakshi hired animators he had worked with in the past, {{including}} Vita, Tyer, Anzilotti and Nick Tafuri, and began the layouts and animation. The first completed sequence was a junkyard scene in Harlem, in which Fritz smokes marijuana, has sex and incites a revolution. Krantz intended to release the sequence as a 15-minute short in case the picture's financing fell through; Bakshi, however, was determined to complete the film as a feature. They screened the sequence for Warner Bros. executives, who wanted the sexual content toned down and celebrities cast for the voice parts. Bakshi refused, and Warner Bros. pulled out, leading Krantz to seek funds elsewhere. He eventually {{made a deal with}} Jerry Gross, the owner of Cinemation Industries, a distributor specializing in exploitation films. Although Bakshi did not have enough time to pitch the film, Gross agreed to fund its production and distribute it, believing that it would fit in with his grindhouse slate.Despite receiving financing from other sources, including Saul Zaentz (who agreed to distribute the soundtrack album on his Fantasy Records label), the budget was tight enough to exclude pencil tests, so Bakshi had to test the animation by flipping an animator's drawings in his hand before they were inked and painted. When a cameraman realized that the cels for the desert scenes were not wide enough and revealed the transparency, Bakshi painted a cactus to cover the mistake. Very few storyboards were used. Bakshi and Vita walked around the Lower East Side, Washington Square Park, Chinatown and Harlem, taking moody snapshots. Artist Ira Turek inked the outlines of these photographs onto cels with a Rapidograph, the technical pen preferred by Crumb, giving the film's backgrounds a stylized realism virtually unprecedented in animation. The tones of the watercolor backgrounds were influenced by the work of Ashcan School painters such as George Luks and John French Sloan. Among other unusual techniques, bent and <b>fisheye</b> <b>camera</b> perspectives were used to portray the way the film's hippies and hoodlums viewed the city. Many scenes featured documentary recordings of real conversations in place of scripted dialogue—this too would become a signature of Bakshi's.|$|E
40|$|Abstract. This paper {{presents}} {{a system for}} direct geo-localization of a MAV in an unknown environment using visual odometry and precise real time kinematic (RTK) GPS information. Visual odometry is performed with a multi-camera system with four <b>fisheye</b> <b>cameras</b> that cover a wide field of view which leads to better constraints for localization due to long tracks and a better intersection geometry. Visual observations from the acquired image sequences are refined with a high accuracy on selected keyframes by an incremental bundle adjustment using the iSAM 2 al-gorithm. The optional integration of GPS information yields long-time stability and provides a direct geo-referenced solution. Experiments show the high accuracy which is below 3 cm standard deviation in position...|$|R
40|$|Described project aims at {{accessing}} accurate localization. It {{consists in}} improving accuracy of localization systems with video {{perception of the}} environment around equipped vehicles. The environment corresponds to the propagation area of GNSS receiver installed {{on top of the}} vehicle. Accuracy problems are linked to obstacles density, especially in urban environments. The objective is to reduce multipath effect on the measurements, without lowering the availability of the service, which is typically the case for errors detection procedures. The presented work aims at using two <b>fisheye</b> <b>cameras</b> in a stereoscopic configuration in order to compute 3 D coordinates of the scene’s points. A reconstructed 3 D model allows then to correct the propagation error thanks to the knowledge of the satellites’ reception status...|$|R
40|$|Recent {{mathematical}} advances, growing {{alongside the}} use of unmanned aerial vehicles, have not only overcome the restriction of roll and pitch angles during flight but also enabled us to apply non-metric cameras in photogrammetric method, providing more flexibility for sensor selection. <b>Fisheye</b> <b>cameras,</b> for example, advantageously provide images with wide coverage; however, these images are extremely distorted and their non-uniform resolutions make them more difficult to use for mapping or terrestrial 3 D modelling. In this paper, we compare the usability of different camera-lens combinations, using the complete workflow implemented in Pix 4 Dmapper to achieve the final terrestrial reconstruction result of a well-known historical site in Switzerland: the Chillon Castle. We assess {{the accuracy of the}} outcome acquired by consumer cameras with perspective and fisheye lenses, comparing the results to a laser scanner point cloud...|$|R
