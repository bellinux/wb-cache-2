59|119|Public
50|$|The {{economy of}} Nampa is {{significantly}} dependent on agriculture and associated services. Great Northern Grain Terminals, a privately {{owned and operated}} grain company that was established in 1986 has a 77,200 MT terminal in Nampa. Peace River Timothy Processing Plant operates a timothy hay growing, harvesting, baling, dehydrating and <b>double</b> <b>compression</b> plant in the community. The plant specializes double compressed timothy hay for export to Asian markets such as Japan. Nampa Co Op Seed Processors provide seed cleaning services.|$|E
5000|$|Export hay is hay that is {{produced}} for export markets. In Australia, hay needs {{to meet a}} number of quality standards {{before it can be}} exported. Because hay in standard round and large square bales is not dense enough to be economically exported to overseas markets, export hay is normally processed to increase the density of the product and improve its suitability for containerization. This form of processing was pioneered by ACX Pacific Northwest, and {{is often referred to as}} <b>double</b> <b>compression.</b> This process allows forage and roughage exporters to inspect and package the hay prior to shipping.|$|E
40|$|Abstract-Video editing softwares {{are easy}} to use but videos are exposed to tampering. Mostly, video cameras are built in MPEG- 4 codec. Therefore, the {{detection}} of <b>double</b> <b>compression</b> in MPEG- 4 videos {{as a first step}} in video forensics research. Markov based features are used to detect <b>double</b> <b>compression</b> artifacts. Index Terms—Digital forensics, <b>double</b> <b>compression,</b> Markov statistics,MPEG- 4. I...|$|E
40|$|Abstract: With {{the advent}} of high quality digital video cameras and low cost video editing software, it is {{becoming}} easier to tamper with digital video. A common form of manipulation is copy move forgery in video and <b>double</b> MPEG <b>compression</b> of video. This paper performs an review on the detecting copy move forgery and <b>double</b> MPEG <b>compression</b> in video...|$|R
40|$|In {{this paper}} a {{detailed}} study regarding Digital Image Forgery on Jpeg images is provided. Here, copy-paste block detection {{on a special}} case of double Jpeg compression- Shifted <b>Double</b> Jpeg <b>Compression,</b> is identified based {{on the characteristics of}} <b>Double</b> Jpeg <b>compression.</b> In certain cases the tampered image will be cropped, the paper uses properties of Block Artifacting method to identify such a scenario...|$|R
30|$|While {{an image}} was {{tampered}} with JPEG[*]+[*]JPEG manipulation, the un-tampered region undergoes <b>double</b> JPEG <b>compression</b> with blocks matching. Although the inserted region undergoes <b>double</b> JPEG <b>compression,</b> {{the probability of}} matching between the 8 × 8 grid of the original image {{and that of the}} copy–paste inserted image is only 1 / 64. Therefore, we can regard the tampered region of the composite image as singly compressed region in our proposed algorithm.|$|R
40|$|In this paper, {{a method}} to detect the {{presence}} of <b>double</b> <b>compression</b> in a MP 3 audio file is proposed. By exploiting the effect of <b>double</b> <b>compression</b> in the statistical properties of quantized MDCT coefficients, a single measure is derived to decide if a MP 3 file is single compressed or it has been double compressed and also to devise the bit-rate of the first compression. Experimental results confirm {{the performance of the}} detector, mainly when the bit-rate of the second com-pression is higher than the bit-rate of the first one...|$|E
40|$|AbstractDetection {{of double}} image {{compression}} {{is all the}} world to the analysis of tampered image and image steganalysis. In order to detect double JPEG 2000 compression, through the comparison with JPEG <b>double</b> <b>compression,</b> we analysis the different statistical characterization of DWT coefficients’ frequency histogram under single and <b>double</b> <b>compression</b> because of post compression rate distortion optimization(PCRD-opt algorithm) in JPEG 2000 codec. We find a new statistical characterization which is different form double JPEG compression. Under certain conditions, experimental results show that such features can be used to detect whether the JPEG 2000 image is double compressed or not...|$|E
30|$|In this paper, {{a method}} to {{localize}} the presence of <b>double</b> <b>compression</b> artifacts in a MP 3 audio file has been presented, {{with the aim of}} uncovering possibly tampered parts. We proposed an algorithm based on a simple statistical feature measuring the effect of <b>double</b> <b>compression</b> that allows to decide whether a MP 3 file is singly compressed or it has been doubly compressed and also to derive the bit-rate of the first compression. In addition, the proposed scheme as well as two state-of-the-art methods designed for detecting doubly compressed MP 3 files have been applied to analyze short temporal windows, in such a way to allow the localization of tampered portions in the MP 3 file under analysis.|$|E
3000|$|... [...]). In this {{tampering}} scheme, the tampered region undergoes single JPEG compression, but the un-tampered region undergoes <b>double</b> JPEG <b>compression.</b>|$|R
3000|$|... {{which is}} {{introduced}} by the shifted <b>double</b> JPEG <b>compression.</b> Calculate the discriminative table DIS[*]=[*][dis(m,n)] for low frequencies (where m[*]+[*]n[*]<[*] 8) [...]...|$|R
40|$|Abstract—The MPEG- 1 Audio Layer 3 can be {{recorded}} as archive and lawful evidence. However, this MP 3 audio may often be forged by audio forgers {{for their own}} benefits in some significant events, which will cause <b>double</b> MP 3 <b>compression.</b> In this paper, the statistical features based on scale factors under long window application in the iterative loop are extracted, and a Support Vector Machine is applied for classification to detect <b>double</b> MP 3 <b>compression.</b> Experimental results demonstrate that the proposed method is accurate and effective for <b>double</b> MP 3 <b>compression</b> detection at the same-bitrate condition. To {{the best of our}} knowledge, {{it is the first time}} to include other basic features in addition to MDCT coefficients in the scope of <b>double</b> audio <b>compression</b> detection. Index Terms—double MP 3 compression, scale factor, same-bitrate I...|$|R
40|$|In this report, {{we present}} a method for {{estimation}} of primary quantization matrix from a double compressed JPEG image. We first identify characteristic features that occur in DCT histograms of individual coefficients due to <b>double</b> <b>compression.</b> Then, we present 3 different approaches that estimate the original quantization matrix from double compressed images. Finally, most successful of them- Neural Network classifier is discussed and its performance and reliability is evaluated {{in a series of}} experiments on various databases of double compressed images. It is also explained in this paper, how <b>double</b> <b>compression</b> detection techniques and primary quantization matrix estimators can be used in steganalysis of JPEG files and in digital forensic analysis for detection of digital forgeries. 1...|$|E
40|$|Increase {{in design}} {{complexity}} and fabrication technology results in high test data volume. As test size increases memory capacity also increases, which becomes the major difficulty in testing System-on-Chip (SoC). To reduce the test data volume, several compression techniques have been proposed. Code based schemes is one among those compression techniques. Run length coding {{is one of}} the most popular coding methodology in code based compression. Run length codes like Golomb code, Frequency directed run Length Code (FDR code), Extended FDR, Modified FDR, Shifted Alternate FDR and OLEL coding compress the test data and the compression ratio increases drastically. For further reduction of test data, <b>double</b> <b>compression</b> technique is proposed using Huffman code. Compression ratio using <b>Double</b> <b>compression</b> technique is presented and compared with the compression ratio obtained by other Run length codes...|$|E
40|$|Detection {{of double}} video {{compression}} {{is of particular}} importance in video forensics, as it reveals partly the video processing history. In this paper, a <b>double</b> <b>compression</b> method is proposed for HEVC–the latest video coding standard. Firstly, four 5 × 5 co-occurrence matrixes were derived from DCT coefficients along four directions respectively, i. e., horizontal, vertical, main diagonal and minor diagonal. Then four 4 × 4 co-occurrence matrixes were derived from PU types which are innovative features of HEVC and rarely been utilized by researchers. Finally, these two feature set are combined and sent to support vector machine (SVM) to detect re-compressed videos. In {{order to reduce the}} feature dimension, only the co-occurrence matrixes of DCT coefficients and PU types in horizontal direction are adopted to identify whether the video has undergone <b>double</b> <b>compression.</b> Experimental results show the effectiveness and the robustness against frame deletion of the proposed scheme...|$|E
40|$|<b>Double</b> HEVC <b>compression</b> {{detection}} is {{of great}} importance in video forensics. However, effective detection algorithm {{based on the same}} Qps is rarely reported. In this paper, a novel method based on the same Qps is applied in dealing with <b>double</b> HEVC <b>compression</b> detection. Firstly, the numbers of PU blocks with the size of 4 × 4 in each I frame is extracted during the codec procedures when the video is compressed with the same Qps. Then, calculate the standard deviation of 4 × 4 PU blocks difference (SDoPU) before and after the compression. Finally, select the appropriate threshold for compression testing classification according to the SDoPU statistical feature. By performing numerical experiments, We prove that the proposed algorithm is of high classification accuracy for detecting <b>double</b> HEVC <b>compression...</b>|$|R
40|$|<b>Double</b> JPEG <b>compression</b> {{detection}} is {{of significance}} in digital forensics. We propose an effective machine learning based scheme {{to distinguish between}} double and single JPEG compressed images. Firstly, difference JPEG 2 -D arrays, i. e., {{the difference between the}} magnitude of JPEG coefficient 2 -D array of a given JPEG image and its shifted versions along various directions, are used to enhance <b>double</b> JPEG <b>compression</b> artifacts. Markov random process is then applied to modeling difference 2 -D arrays so as to utilize the second-order statistics. In addition, a thresholding technique is used {{to reduce the size of}} the transition probability matrices, which characterize the Markov random processes. All elements of these matrices are collected as features for <b>double</b> JPEG <b>compression</b> detection. The support vector machine is employed as the classifier. Experiments have demonstrated that our proposed scheme has outperformed the prior arts. 1...|$|R
30|$|In many JPEG image {{tampering}} situations, {{a foreign}} JPEG-compressed patch is inserted into an authentic {{image and the}} resultant image is re-compressed to form the new image. The tampered region has <b>double</b> JPEG <b>compressions,</b> but the block structures of the two compressions in the tampered region are usually not aligned with each other. Such case {{is referred to as}} shifted <b>double</b> JPEG <b>compression</b> (SDJPEG) [12] or non-aligned <b>double</b> JPEG <b>compression</b> (NA-JPEG) [13, 14]. For SDJPEG, the <b>double</b> JPEG <b>compression</b> detection methods discussed above cannot achieve satisfactory results. Luo et al. [15] tried to detect SDJPEG by analyzing the blocking artifact characteristics matrix (BACM). They observed that BACM is symmetric for single JPEG compression but the symmetry is destroyed after SDJPEG. However, BACM is highly related to the image content and the detection performance would decrease if the testing images are very different from those in the training set. In order to solve the above problem, Chen and Hsu [16] extended the idea of BACM and proposed a feature which is less related to the image content by introducing the inter-block correlation. However, since the statistical features in both [15] and [16] require large amount of data to obtain high discriminative power, their methods do not work well for small SDJPEG patches.|$|R
40|$|<b>Double</b> <b>compression</b> of a {{peripheral}} nerve is not rare in medical practice. This article describes an ulnar neuropathy along the elbow and the wrist segments with electro-diagnostic examination (EDX). The proximal compression was an ulnar entrapment at the olecranon-epitrochlear semi-canal; the distal one {{was after the}} canal of Guyon, due to an arthro-synovial cyst arising from the pisohamatum joint. There aren't analogous clinical reports in the literature...|$|E
40|$|Summary. The flow over a 15 ◦- 45 ◦ <b>double</b> <b>compression</b> ramp was {{studied at}} Mach 7. 5. CFD {{computations}} are compared to 2 component PIV (particle image velocimetry) measurements. Furthermore stereoscopic PIV {{was used to}} measure the three component velocity vector, enabling to perform a 3 D flow survey. The overall flow topology is assessed and special attention is devoted to the separated region. Finally the effect of a sharp leading edge on the separation region is investigated. ...|$|E
40|$|In this paper, {{we present}} the simple and <b>double</b> <b>compression</b> {{algorithms}} with an error control for compressing satellite data corresponding to several revolutions. The compressions are performed {{by means of}} approximations in the norm L 1 by nite series of Chebyshev polynomials, with their known properties of fast eval-uation, uniform distribution of the error, and validity over large intervals of time. By using the error control here introduced, the number of terms of the series is given automatically for a prede-termined tolerance. As illustration, we apply the method to the orbits of SPOT, Topex/Poseidon and Skybridge satellites. In this paper, we present the simple and <b>double</b> <b>compression</b> algorithms with an error control for compressing satellite data corresponding to several revolutions. The compressions are performed by means of approximations in the norm L 1 by nite series of Chebyshev polynomials, with their known properties of fast eval-uation, uniform distribution of the error, and validity over large intervals of time. By using the error control here introduced, the number of terms of the series is given automatically for a prede-termined tolerance. As illustration, we apply the method to the orbits of SPOT, Topex/Poseidon and Skybridge satellites...|$|E
30|$|Un-compressed image {{database}} (UCID)[13] is a {{color image}} database including 1, 338 uncompressed TIFF images, which span {{a wide range}} of indoor and outdoor scenes with the size of 512 × 384. In our experiments, we conduct single JPEG compression three times (QF[*]=[*] 70, 80, and 90) and <b>double</b> JPEG <b>compression</b> three times (QF 1, QF 2 [*]=[*] 55, 70; 65, 80; 75, 90) for all 1, 338 images in the UCID database. Note that unless specified in the article, we refer <b>double</b> JPEG <b>compression</b> to that an image is compressed twice by the same or different JPEG quality factors successively in the 8 × 8 blocks.|$|R
40|$|The Adaptive Multi-Rate (AMR) {{audio codec}} is now widely {{used as the}} default file format for various mobile phones to store spoken audio {{recordings}}. Meanwhile, {{more and more people}} start to use their mobile phones to record sounds for convenience, which results in rapid growth of the amount of digital audio recordings as evidences occur in court, including AMR format audios. It is critical to authenticating the integrity of AMR audios when they appear as evidences. AMR audio forgery manipulations generally uncompressed an AMR file, tamper with the file in PCM domain, and then re-compressed the tampered audio back into AMR format, which come the <b>double</b> AMR <b>compression.</b> In this paper, we propose a method on the detection of <b>double</b> AMR <b>compression</b> by discriminating double-compressed AMR audio recordings from single-compressed ones. Binary classification algorithm is applied for the discrimination. Statistical features related on frequency energy distribution and frequency components correlation are extracted and a support vector machine is applied to execute the classification. Experiment results show our method is promising on this binary classification task. Index Terms: <b>double</b> AMR <b>compression,</b> digital audio forensic, SV...|$|R
40|$|In the age {{of digital}} multimedia, the {{emergence}} of high resolution digital cameras and sophisticated photo-editing software packages make s it relatively easy to create digital image forgeries. It is increasingly important to verify the authenticity and integrity of digital images. In this paper, {{a new approach to}} detect tampered images based on <b>double</b> JPEG 2000 <b>compression</b> is proposed. The paper first analyzes the statistics of <b>double</b> JPEG 2000 <b>compression,</b> the artifacts in the Fourier transforms of DWT (Discrete Wavelet Transform) coefficient histogram, i. e., the stronger singularity in the mid and high frequencies compared with single JPEG 2000 compression; then extracts the singularity using the characteristics of DWT and detects <b>double</b> JPEG 2000 <b>compression</b> by thresholding the singularity. Finally, to images identified as the double JPEG 2000 compress ed versions, the percent of non- zero DWT coefficients in high frequencies is examined to catch the difference between authentic image part and tampered image part. The experiments demonstrate that the proposed approach can achieve an effective detection for <b>double</b> JPEG 2000 <b>compression</b> as well as accurate location for the tampered areas. </span...|$|R
40|$|The flow over a 15 ?- 45 ? <b>double</b> <b>compression</b> ramp was {{studied at}} Mach 7. 5. CFD {{computations}} are compared to 2 component PIV (particle image velocimetry) measurements. Furthermore stereoscopic PIV {{was used to}} measure the three component velocity vector, enabling to perform a 3 D flow survey. The overall flow topology is assessed and special attention is devoted to the separated region. Finally the effect of a sharp leading edge on the separation region is investigated. Aerodynamics & Wind EnergyAerospace Engineerin...|$|E
30|$|The core {{idea of the}} {{algorithm}} is to measure the similarity between the histogram of quantized MDCT coefficients of the MP 3 file under analysis, that has possibly undergone a <b>double</b> <b>compression,</b> and the histogram of the MDCT coefficients computed on a singly compressed {{version of the same}} file, that is of the singly compressed MDCT coefficients. Intuitively, if the distance between the two distributions is low, this will indicate that the file under analysis has not been MP 3 encoded twice; vice versa, the file will be considered as doubly compressed.|$|E
40|$|The Information {{exchange}} via any media needs {{privacy and}} secrecy. Cryptography {{is widely used}} for providing privacy and secrecy between the sender and receiver. But, now, along with Cryptography, we are using Steganography to have more protection to our hidden data. In this paper, we show how a JPEG {{can be used as}} an embedding space for a message by adjusting the values in the JPEG Quantization tables (QTs). This scheme also uses some permutation algorithms and it can be widely used for secret communication. This JPEG <b>double</b> <b>compression</b> will give satisfactory decoded results...|$|E
40|$|Chitosan is a polymer {{derived from}} chitin that is widely {{available}} at relatively low cost, {{but due to}} compression challenges it has limited application {{for the production of}} direct compression tablets. The aim {{of this study was to}} use certain process and formulation variables to improve manufacturing of tablets containing chitosan as bulking agent. Chitosan particle size and flow properties were determined, which included bulk density, tapped density, compressibility and moisture uptake. The effect of process variables (i. e. compression force, punch depth, percentage compaction in a novel <b>double</b> fill <b>compression</b> process) and formulation variables (i. e. type of glidant, citric acid, pectin, coating with Eudragit S®) on chitosan tablet performance (i. e. mass variation, tensile strength, dissolution) was investigated. Moisture content of the chitosan powder, particle size and the inclusion of glidants had a pronounced effect on its flow ability. Varying the percentage compaction during the first cycle of a <b>double</b> fill <b>compression</b> process produced chitosan tablets with more acceptable tensile strength and dissolution rate properties. The inclusion of citric acid and pectin into the formulation significantly decreased the dissolution rate of isoniazid from the tablets due to gel formation. Direct compression of chitosan powder into tablets can be significantly improved by the investigated process and formulation variables as well as applying a <b>double</b> fill <b>compression</b> process. [URL]...|$|R
50|$|Institute {{laboratories}} {{are equipped}} with HPLC, FTIR, Lyophilizer, UV Spectrophotometer, Rotary <b>Compression</b> Machine, Bilayer (<b>double</b> rotary) <b>Compression</b> Machine, Extruder and Spheronizer, Coating Assembly, Ampoule Filling and Sealing Machine, Dissolution Apparatus, Rapid Mixture Granulator (RMG), Colloid & Multi Mill, Fluidized Bed Dryer (FBD), Single and double channel Physiograph, Semi-Auto analyzer, Biological Oxygen Demand Incubator (BOD) and Flame Photometer.|$|R
40|$|Part 2 : Work in ProgressInternational audienceSince JPEG is {{the most}} widely used {{compression}} standard, detection of forgeries in JPEG images is necessary. In order to create a forged JPEG image, the image is usually loaded into a photo editing software, manipulated and then re-saved as JPEG. This yields to <b>double</b> JPEG <b>compression</b> artifacts, which can possibly reveal the forgery. Many techniques for the detection of double JPEG compressed images have been proposed. However, when the image is resized before the second compression step, the blocking artifacts of the first JPEG compression are destroyed. Therefore, most reported techniques for detecting <b>double</b> JPEG <b>compression</b> do not work for this case. In this paper, we propose a technique for detecting resized double JPEG compressed (called RD-JPEG) images. We first identify features that can discriminate RD-JPEG images from JPEG images and then use Support Vector Machines (SVM) as a classification tool. Experiments with many RD-JPEG images with different quality and scaling factors indicate that our technique works well...|$|R
40|$|We present new {{forensic}} {{tools that}} {{are capable of}} detecting traces of tampering in digital video without the use of watermarks or specialized hardware. These tools operate under the assumption that video contain naturally occurring properties which are disturbed by tampering, and which can be quantified, measured, and used to expose video fakes. In this context, we offer five new forensic tools (1) Interlaced, (2) De-interlaced, (3) <b>Double</b> <b>Compression,</b> (4) Duplication, and (5) Re-projection, where each technique targets a specific statistical or geometric artifact. Combined, these tools provide a valuable first set of forensic tools for authenticating digital video...|$|E
40|$|A rat model {{study was}} {{performed}} to evaluate the effects of compression of the thoracic spinal cord. Dorsal screw compression of the thoracic spinal cord up to 50 % canal diameter was created at T 5 (Group A), T 5 and T 11 (Group B), and T 5 and T 8 (Group C). Clinical grading (Tarlov criteria), sensory evoked and motor evoked potentials, horse raddish peroxidase neural transport, and microangiography were studied. Single level compression had little effect. For <b>double</b> <b>compression,</b> there were marked detrimental effects both clinically and electrophysiologically, especially when the compression was close together. Neutral transport was affected in all groups, but {{the difference was not}} significant between them. link_to_subscribed_fulltex...|$|E
40|$|Abstract. In this paper, {{we use the}} {{previously}} proposed calibrated DCT features [9] to construct a Support Vector Machine classifier for JPEG images capable of recognizing which steganographic algorithm was used for embedding. This work also constitutes a more detailed evaluation {{of the performance of}} DCT features as in [9] only a linear classifier was used. The DCT features transformed using Principal Component Analysis enable an interesting visualization of different stego programs in a three-dimensional space. This paper demonstrates that, at least under some simplifying assumptions in which the effects of <b>double</b> <b>compression</b> are ignored, it is possible to reliably classify stego images to their embedding techniques. The classifier is capable of generalizing to previously unseen techniques. ...|$|E
40|$|We {{describe}} a technique for detecting double quantization in digital video {{that results from}} <b>double</b> MPEG <b>compression</b> or from combining two videos of different qualities (e. g., greenscreening). We describe how double quantization can introduce statistical artifacts that while not visible, can be quantified, measured, and used to detect tampering. This technique can detect highly localized tampering in regions as small as 16 × 16 pixels...|$|R
40|$|Abstract-There {{are many}} {{approaches}} proposed to detect <b>double</b> JPEG <b>compression</b> when {{primary and secondary}} compressions have different quantization matrix. An effective error based statistical feature extraction is introduced {{to solve the problem}} of distinguishing the primary and secondary compression with the same quantization matrix. First upon the JPEG file is decompressed to form are constructed image,then an error image is obtained by computing the differences between the inverse discrete cosine transform coefficients and pixel values in the reconstructed image. Mainly, Rounding error block and Truncation error block are analyzed. Then, a set of features is proposed to characterize the statistical differences of the error blocks between single and <b>double</b> JPEG <b>compressions.</b> Finally, any effective classifier is employed to identify whether a given JPEG image is doubly compressed or not [...] This enhance the efficiency of the images which is detected as double compressed and error rate is very low compared to other methods. Hence the overall performance can be increased efficiently. Index Terms—Digital forensics, double JPEG compression,artifacts rounding error, truncation error. I...|$|R
40|$|Due to {{the wide}} {{diffusion}} of JPEG coding standard, the image forensic community has devoted significant attention {{to the development of}} <b>double</b> JPEG (DJPEG) <b>compression</b> detectors through the years. The ability of detecting whether an image has been compressed twice provides paramount information toward image authenticity assessment. Given the trend recently gained by convolutional neural networks (CNN) in many computer vision tasks, in this paper we propose to use CNNs for aligned and non-aligned <b>double</b> JPEG <b>compression</b> detection. In particular, we explore the capability of CNNs to capture DJPEG artifacts directly from images. Results show that the proposed CNN-based detectors achieve good performance even with small size images (i. e., 64 Ã 9 ̆ 7 64), outperforming state-of-the-art solutions, especially in the non-aligned case. Besides, good results are also achieved in the commonly-recognized challenging case in which the first quality factor is larger than the second one...|$|R
