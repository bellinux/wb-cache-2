64|60|Public
40|$|Recommendation agents help users reduce {{information}} overload and improve decision quality. Yet, many online shoppers have negative reaction or have no motivation to use recommendation agents, {{since they have}} no idea of whether users can achieve their shopping goals with less effort. We think information is fundamental to using recommendation agents. This study develops a research framework from the accessibility-diagnosticity perspective and proposes <b>explanation</b> <b>facility,</b> perceived similarity and information diagnosticity are important determinants of users ’ intention to reuse RAs. We think <b>explanation</b> <b>facility</b> could persuade users of RAs ’ performance, similarity could move users to agree with RAs, and information diagnosticity could let users be capable of evaluating RAs. We also consider the moderating role of domain knowledge on relationship of similarity and information diagnosticity. This study conducted a 2 * 2 factorial experiment for data collection. Results show that decision process and outcome similarity indirectly influence reuse intention by information diagnosticity and the effects of process and outcome similarity varies with degrees of users ’ domain knowledge. The influence of <b>explanation</b> <b>facility</b> on similarity is not obvious. The effect of “why” <b>explanation</b> <b>facility</b> on outcome explanation is significantly contrary to our expectation. <b>Explanation</b> <b>facility</b> may have to be utilized carefully. Implications are discussed...|$|E
40|$|Research has {{revealed}} that {{the extent to which}} users will utilize an advice-giving expert system is dependent {{on the quality of the}} system's <b>explanation</b> <b>facility</b> (Woods, 1986). Independent researchers have attempted to explore alternative explanation concepts by designing and building separate systems to test <b>explanation</b> <b>facility</b> theory, These efforts have focused primarily on the computational aspects of expert system design, leaving many of the central issues concerning human requirements of explanation facilities unresolved. This paper presents results of an extensive literature review designed to identify and define specific human factors issues which must be addressed to determine guidelines or standards for <b>explanation</b> <b>facility</b> design. The work described in this paper is part of an ongoing research program to develop guidelines for developers and users of expert troubleshooting systems (ETS) ...|$|E
40|$|Explanation {{has become}} a {{standard}} feature in many expert systems today. Adapting from this work, a study was made to determine the types of explanation required in a grammar writing system and to investigate design and iimplementation issues. The first version of this <b>explanation</b> <b>facility</b> {{is based on a}} derivational history of the inferencing process, and although no supplementary knowledge is used, this <b>explanation</b> <b>facility</b> is able to furnish answers to the traditional why, how and what type of queries, and even the what-if (simulation) query. The explanation is also enhanced through the use of special files containing canned-text for describing grammar rules and variables...|$|E
40|$|Abstract Intelligent systems {{encompass}} a {{wide range}} of software technologies including heuristic and normative expert systems, case-based reasoning systems, and neural networks. This field has been augmented in recent years by Web-based applications, such as recommender systems and the semantic Web. The uses of <b>explanation</b> <b>facilities</b> have their roots in heuristic rule-based expert systems and have long been touted as an important adjunct in intelligent decision support systems. However, in recent years, their uses have been explored in many other intelligent system technologies- particularly those making an impact in e-commerce such as recommender systems. This paper shows how <b>explanation</b> <b>facilities</b> work with a range of symbolic intelligent techniques and, when carefully designed, provide a range of benefits. The paper also shows how, despite being more difficult to augment with non-symbolic technologies, hybrid methods predominantly using rule-extraction techniques have provided moderate success for <b>explanation</b> <b>facilities</b> in a range of ad-hoc applications...|$|R
40|$|Based on a {{description}} of a knowledge continuum and on the knowledge combustion and vehicle analogy, problems regarding the <b>explanation</b> <b>facilities</b> of knowledge-based systems (KBS) are addressed. Furthermore, drawing from the lessons learned from the analysis of the video recordings of medical diagnostic telemedicine consultations, a new approach for developing justification and strategic <b>explanation</b> <b>facilities</b> for KBS is presented. Rather than enabling computers to make profound decisions and generate convincing explanations, which is not technically feasible according to the knowledge continuum, the development of a KBS that provides relevant explanations for specific diagnostic situations is proposed. The knowledge continuum indicates that there is a narrow gap between information, declarative knowledge and some types of pragmatic knowledge. To take advantage of this knowledge characteristic, the creation of a hybrid knowledge type that integrates several more explicit knowledge types through simple processes of extraction, representation, analysis, modelling and editing is proposed. KBS <b>explanation</b> <b>facilities,</b> knowledge types, knowledge continuum, video analysis, contextual maps, explanations...|$|R
40|$|This paper briefly {{describes}} SEASALTexp, {{an extension}} of the application-independent SEASALT architecture (Sharing Experience using an Agent-based explanation-aware System Architecture LayouT), which offers knowledge acquisition from Internet communities, knowledge modularisation, and agent-based knowledge maintenance complemented with agent-based <b>explanation</b> <b>facilities...</b>|$|R
40|$|The Propagation Rule Compiler (PROP) {{is a tool}} {{developed}} {{within the}} ESPRIT III project IDEA (Intelligent Database Environment for Advanced Applications). It aims at supporting developers of Chimera applications during schema design and prototyping. PROP consists of two components: the rule compiler as such and an <b>explanation</b> <b>facility.</b> The task of the <b>explanation</b> <b>facility</b> is to graphically illustrate the logical dependencies established via deductive rules of a given Chimera schema. The purpose of the compilation unit is to generate update propagation triggers from deductive rules. Such triggers are able to automatically compute all implicit changes of a Chimera database once a specific updating transaction has been issued. PROP has been integrated as a subcomponent into Bonn's Chimera Prototyping Tool (CPT). The purpose of this document is to provide the technical documentation of the rule compilation process. This report has been issued to the ESPRIT III project 6333 (IDEA) as deli [...] ...|$|E
40|$|A major {{limitation}} of current advisory systems (e. g., intelligent tutoring systems and expert systems) is their restricted ability to give explanations. The goal {{of our research}} is to develop and evaluate a flexible <b>explanation</b> <b>facility,</b> one that can dynamically generate responses to questions not anticipated by the system's designers and that can tailor these responses to individual users. To achieve this flexibility, we are developing a large knowledge base, a viewpoint construction facility, and a modeling facility. In the long term we plan to build and evaluate advisory systems with flexible explanation facilities for scientists in numerous domains. In the short term, we are focusing on a single complex domain in biological science, and we are working toward two important milestones: (1) building and evaluating an advisory system with a flexible <b>explanation</b> <b>facility</b> for freshman-level students studying biology; and (2) developing general methods and tools for building similar explanation facilities in other domains...|$|E
40|$|In this paper, {{we present}} a new {{approach}} to support the decision of selecting one object out of a set of alternatives. As compared to previous approaches, the distinctive feature of our approach is that neither the user, nor the system need to build a model of user's preferences. Our proposal is to integrate a system for interactive data exploration and analysis with a multimedia <b>explanation</b> <b>facility.</b> The <b>explanation</b> <b>facility</b> supports the user in understanding unexpected aspects of the data. The explanation generation process is guided by a causal model of the domain that is automatically acquired by the system. Introduction With the rapid increase in the amount of on-line, up-to-date information, more and more people, ranging from professional public-policy decision makers to common people, will base their decisions on on-line sources. Thus, there is an increasing need for software systems that support interactive, information-intensive decision making for different user populations [...] ...|$|E
50|$|Whether simple or sophisticated, {{an expert}} system for {{mortgages}} {{should be provided}} with <b>explanation</b> <b>facilities</b> that show how it reaches its decisions and hence its advice. The confidence of the loan officer in the AI construct will be increased when this is done in a convincing manner.|$|R
40|$|Providing {{models with}} <b>explanation</b> <b>facilities</b> {{to make the}} {{rationale}} for model behavior (both "external", {{with respect to the}} context of a simulation domain, and "internal", the structure and cognitive processes of models) would make them more accessible. In an effort to understand the potential explanation information requirements of an example population, data collected from a usability study of the TacAir-Soar Situation Awareness Panel was analyzed to determine where breakdowns occurred in users' attempts to understand this complex cognitive model. The analysis suggests that plan view displays of models' situation awareness are useful but users need the displays to be augmented with information about working memory changes over time, with actions both taken and not taken by models. In addition, <b>explanation</b> <b>facilities</b> of how and why behavior occurs appear to be necessary for most architectures...|$|R
40|$|Explanations are {{recognized}} as an important facet of intelligent behavior. Unfortunately, expert systems are currently limited {{in their ability to}} provide useful, intelligent justifications of their results. We are currently investigating the issues involved in providing <b>explanation</b> <b>facilities</b> for expert planning systems. This investigation addresses three issues: knowledge content, knowledge representation, and explanation structure...|$|R
40|$|An {{automated}} <b>explanation</b> <b>facility</b> for Bayesian conditioning {{aimed at}} improving user acceptance of probability-based decision support systems has been developed. The domain-independent facility {{is based on an}} information processing perspective on reasoning about conditional evidence that accounts both for biased and normative inferences. Experimental results indicate that the facility is both acceptable to naive users and effective in improving understanding. Comment: Appears in Proceedings of the Third Conference on Uncertainty in Artificial Intelligence (UAI 1987...|$|E
40|$|We {{introduce}} a dialogue-based <b>explanation</b> <b>facility</b> for Intelligent CALL (ICALL) Systems. Our prototype system, DiBEx, uses meta reasoning {{to build up}} an explanation (error) tree, given a faulty user input. It relies on correct grammatical subtheories, instead of explicit error taxonomies. DiBEx, thus, realizes anticipation free error diagnosis. The system enters in a tutorial dialogue with the student, where each explanation (dialogue) step {{is based on the}} principles of a single tutorial strategy and a dynamic user model. ...|$|E
40|$|Abstract: A set of {{supporting}} tools for the planing system is concerned. The planing {{system is the}} solver of geometric problems. The set includes convenient windows-based user interface, <b>explanation</b> <b>facility,</b> delineation drawing facility and a facility for problem solution tracing. Special attention is given to visualization of solution graph. These tools are desig-ned for ordinary users. A developer of the planing system can use the offered tools to debug a functionality of knowledge base too. Note: Publication language:russia...|$|E
50|$|A good {{object-oriented}} design involves an early focus on behaviors {{to realize the}} capabilities meeting the stated requirements and a late binding of implementation details to the requirements. This approach especially helps to decentralize control and distribute system behavior which can help manage the complexities of high-functionality large or distributed systems. Similarly, it can help to design and maintain <b>explanation</b> <b>facilities</b> for cognitive models, intelligent agents, and other knowledge-based systems.|$|R
40|$|Abstract. Knowledge {{representation}} systems, including ones {{based on}} Description Logics (DLs), use <b>explanation</b> <b>facilities</b> to, among others, debug knowledge bases. Until now, such facilities {{were not available}} for expressive DLs, whose reasoning is an un-natural refutation-based tableau. We offer a solution based on a sequent calculus that {{is closely related to}} the tableau implementation, exploiting its optimisations. The resulting proofs are pruned and then presented as simply as possible using templates. ...|$|R
40|$|A {{great deal}} {{has been written}} about the role of {{clinical}} decision support systems in medicine in recent years—an important category of which are expert systems. Expert systems would normally contain an explanation module—the subject of a great deal of research interest in the 1980 s when the main problem-solving task for medical expert systems was diagnostics. However, expert systems nowadays are more likely to perform tasks other than diagnosis, yet the role of explanation in expert systems has been largely ignored in the health care literature since this time. Furthermore, user requirements can vary considerably in the health care domain and may include physicians, medical researchers, administrators, and patients. Such user groups would have differing levels of knowledge and goals, which would impact on the type of explanatory support provided by the system. This article examines the potential benefits of <b>explanation</b> <b>facilities</b> for a range of clinical tasks and also considers the ways in which <b>explanation</b> <b>facilities</b> may be delivered so as to be of benefit to these categories of health care user for these tasks...|$|R
40|$|This paper {{provides}} {{a comprehensive review}} of explanations in recommender systems. We highlight seven possible advantages of an <b>explanation</b> <b>facility,</b> and describe how existing measures can be used to evaluate the quality of explanations. Since explanations are not independent of the recommendation process, we consider how the ways recommendations are presented may affect explanations. Next, we look at different ways of interacting with explanations. The paper is illustrated with examples of explanations throughout, where possible from existing applications. ...|$|E
40|$|This paper {{describes}} the English <b>explanation</b> <b>facility</b> of the OWL Digitalis Advisor, {{a program designed}} to advise physicians regarding digitalis therapy. The program is written in OWL, an English-based computer language being developed at MIT. The system can explain, in English, both the methods it uses and how those methods were applied during a particular session. In addition, the program can explain how it acquires information and tell the user how it deals with that information either in general or during a particular session. 1...|$|E
40|$|A dialogue-based <b>explanation</b> <b>facility</b> for Intelligent CALL (ICALL) Systems is introduced. Our {{prototype}} system, DiBEx, uses meta reasoning {{to build}} up an explanation (error) tree, given a faulty user input. It relies on correct grammar sub theories, instead of explicit error taxonomies. The system enters in a tutorial dialogue with the student, where each explanation step {{is based on the}} principles of a single tutorial strategy. DiBEx focuses on the representation and utilization of grammatical knowledge to participate in such dialogues. Grammar theory is realized as an executable logical theory. 1...|$|E
40|$|This paper {{describes}} {{our current}} activities to supply extended reasoning support to knowledge engineers who are building terminologies using Description Logics (DL) reasoners. The new services {{originate in the}} development of the DICE terminology where the lack of appropriate debugging or <b>explanation</b> <b>facilities</b> hindered a more e#cient (and possibly more concise) construction of a corresponding DL TBox. We discuss a number of alternative methods to explain incoherence of TBoxes, unsatisfiability of concepts and concept subsumption...|$|R
40|$|Explanation is an {{important}} capability for usable intelligent systems, including intelligent agents and cognitive models embedded within simulations and other decision support systems. <b>Explanation</b> <b>facilities</b> help users understand how and why an intelligent system possesses a given structure and set of behaviors. Prior research {{has resulted in a}} number of approaches to providing explanation capabilities and identified some significant challenges. We describe a design that can be reused to create intelligent agents capable of explaining themselves. The design includes ways to provide ontological, mechanistic, and operational explanations. These designs inscribe lessons learned from prior research and provide guidance for incorporating <b>explanation</b> <b>facilities</b> into intelligent systems. Our design is derived from both prior research on explanation tool design and from the empirical study reported here on the questions users ask when working with an intelligent system. We demonstrate the use of these designs through examples implemented using the Herbal high-level cognitive modeling language. These designs can help build better agents—they support creating more usable and more affordable intelligent agents by encapsulating prior knowledge about how to generate explanations in concise representations that can be instantiated or adapted by agent developers...|$|R
40|$|In {{this project}} we extend C Language Production System (CLIPS), an {{existing}} Expert System shell, by creating three new options. Specifically, first we create a compatible with CLIPS environment {{that allows for}} defining objects and object hierarchies, second we provide means to implement backward chaining in a pure forward chaining environment, and finally we give some simple <b>explanation</b> <b>facilities</b> for the derivations the system has made. Objects and object hierarchies are extended so that facts can be automatically inferred, {{and placed in the}} fact base. Backward chaining is implemented by creating run time data structures which hold the derivation process allowing for a depth first search. The backward chaining mechanism works not only with ground facts, but also creates bindings for every query that involves variables, and returns the truth value of such a query as well as the relevant variable bindings. Finally, the WHY and HOW <b>explanation</b> <b>facilities</b> allow for a complete examination of the derivation process, the rules triggered, and the bindings created. The entire system is integrated with the original CLIPS code, and all of its routines can be invoked as CLIPS commands...|$|R
40|$|Expert {{systems were}} one of the rst {{applications}} to emerge from initial research in arti cial intelligence, and the explanation of expert system reasoning was one of the rst applications of natural language generation. 1 This is because the need for explanations is obvious, and generation from a knowledge-based application such as reasoning should be relatively straightforward. However, while explanation has been universally acknowledged as a desirable functionality in expert systems, natural language generation has not taken a central place in contemporary expert system development. For example, a popular text book about expert systems such as (Giarratano and Riley, 1994) stresses twice in the introduction the importance of explanation, but provides no further mention of explanation in the remaining 600 pages. (The book is based on the popular CLIPS framework.) In this paper, we present a new approach to providing an expert system with an <b>explanation</b> <b>facility.</b> The approach comprises both software components and a methodology for assembling the components. The methodology is minimally intrusive into existing expert system development practice. This paper is structured as follows. In Section 2, we discuss previous work and identify shortcomings. We present our analysis of knowledge types in Section 3. In Section 4 present the Securioty Assistant and its <b>explanation</b> <b>facility.</b> Finally, we sketch a general methodology for explainabl...|$|E
40|$|In {{this paper}} we present our {{research}} on identifying and modeling the strategies that human tutors use for integrating previous explanations into current explanations. We {{have used this}} work to develop a computational model that has been partially implemented in an <b>explanation</b> <b>facility</b> for an existing tutoring system known as SHERLOCK. We are implementing a system that uses case-based reasoning to identify previous situations and explanations that could potentially affect the explanation being constructed. We have identified heuristics for constructing explanations that exploit this information in ways similar {{to what we have}} observed in instructional dialogues produced by human tutors...|$|E
40|$|This study uses a {{new data}} {{visualization}} method, {{developed by the}} first author, to investigate the reliability of a real world low-back-pain Multi-layer Perceptron (MLP) network from a hidden layer decision region perspective. Using decision region identification information from an <b>explanation</b> <b>facility,</b> the MLP training examples are discovered to occupy decision regions in contiguous class threads across the 48 -dimensional input space. MLP testing cases show a similar distribution and consistency within the contiguous threads but with a reduced reliability. Three test regions outside the network’s knowledge bounds are situated between training regions with a consistent classification. 1...|$|E
40|$|We {{describe}} an approach for developing <b>explanation</b> <b>facilities</b> for cognitive architectures based on techniques drawn from object- and aspect-oriented software engineering. We examine {{the use of}} responsibility-driven design augmented with scenario-based techniques and classresponsibility -collaboration (CRC) cards to identify explanation behaviors for cognitive model elements, and discuss the explanation benefits derived from encapsulating model behaviors within aspects. Soar is used an example cognitive architecture, but the methods and results as illustrated would apply {{to any of the}} other architectures commonly used to development psychologically plausible intelligent systems...|$|R
40|$|The {{combination}} of neural network and expert system can accelerate {{the process of}} inference, and then rapidly produce associated facts and consequences. However, neural network still has some problems such as providing <b>explanation</b> <b>facilities,</b> managing the architecture of network and accelerating the training time. Thus {{to address these issues}} we develop a new method for pre-processing based on rough set and merge it with neural network and expert system. The resulting system is a hybrid expert system model called a Hybrid Rough Neural Expert System (HRNES) ...|$|R
40|$|Abstract. The {{frameworks}} for protecting security and privacy {{can be effective}} only if common users—with no training in computer science or logic—increase their awareness and control over the policy applied by the systems they interact with. Towards this end, we introduce a mechanism for answering why, why-not, how-to, and what-if queries on rule-based policies for trust negotiation. Our framework is lightweight and scalable but it fulfills the main goals of modern <b>explanation</b> <b>facilities.</b> We adopt a novel tabled explanation structure, that simultaneously shows local and global (intra-proof and inter-proof) information, thereby facilitating navigation. Answers are focussed by removing irrelevant parts with suitable heuristics. ...|$|R
40|$|Over-constrained {{problems}} {{can have an}} exponential number of conflicts, which explain the failure, and an exponential number of relaxations, which restore the consistency. A user of an interactive application, however, desires explanations and relaxations containing the most important constraints. To address this need, we define preferred explanations and relaxations based on user preferences between constraints and we compute them by a generic method which works for arbitrary CP, SAT, or DL solvers. We significantly accelerate the basic method by a divide-and-conquer strategy and thus provide the technological basis for the <b>explanation</b> <b>facility</b> of a principal industrial constraint programming tool, which is, for example, used in numerous configuration applications...|$|E
40|$|Abstract. We {{discuss the}} use of a hybrid system {{utilizing}} Object Oriented Bayesian networks and influence diagrams for probabilistic reasoning under uncertainties in industrial process operations. The Bayesian networks are used for condition monitoring and root cause analysis of process operation. The recommended decision sequence of corrective actions and observations is obtained following the “myopic ” approach. The BN inference on most probable root cause is used in an influence diagram for taking decisions on urgency of corrective actions vs. delivery deadline. The build-in chain of causality from root cause to process faults can provide the user with <b>explanation</b> <b>facility</b> and a simulation tool of the effect of intended actions. 1...|$|E
40|$|This project {{involves}} {{the implementation of}} a rule based expert system for standard independent steel member design. Steel member design is performed according to the Load and Resistance Factor Design Specifications formulated by American Institute of Steel Construction. Rule based methodology is utilized to formulate an <b>explanation</b> <b>facility,</b> in which the program, design, provides steps involved in the design process. Currently, beam and column design for symmetric rolled shaped sections are performed. The standard independent design methodology creates a programming environment which makes it easier to expand the program to other structural members. The program may be used as a design tool or a study aid...|$|E
40|$|Abstract: The use {{of neural}} {{networks}} is still difficult in {{many application areas}} {{due to the lack}} of <b>explanation</b> <b>facilities</b> (the « black box » problem). The concepts of contextual importance and contextual utility presented make it possible to explain the results of neural networks in a user-understandable way. The explanations obtained are of same quality as those of expert systems, but they may be more flexible since the reasoning module and the explanation module are completely separated. The numerical complexity of estimating the contextual importance and contextual utility is to a great extent solved by the neural net proposed (INKA), which also has good function approximation and training properties. 1...|$|R
40|$|Fuzzy neural {{networks}} have several features {{that make them}} well suited {{to a wide range}} of knowledge engineering applications. These strengths include fast and accurate learning, good generalisation capabilities, excellent <b>explanation</b> <b>facilities</b> in the form of semantically meaningful fuzzy rules, and the ability to accommodate both data and existing expert knowledge about the problem under consideration. This paper investigates adaptive learning, rule extraction and insertion, and neural/fuzzy reasoning for a particular model of a fuzzy neural network called FuNN. As well as providing for representing a fuzzy system with an adaptable neural architecture, FuNN also incorporates genetic algorithms as one of its adaptation strategies. 1...|$|R
40|$|Commercial {{legal expert}} systems are {{invariably}} rule based. Such systems are poor at dealing with open texture and the argumentation inherent in law. To overcome these problems we suggest supplementing rule based legal expert systems with case based reasoning or neural networks. Both case based reasoners and neural networks use cases-but {{in very different}} ways. We discuss these differences at length. In particular we examine the role of explanation in existing expert systems methodologies. Because neural networks provide poor <b>explanation</b> <b>facilities,</b> we consider the use of Toulmin argument structures to support explanation (S. Toulmin, 1958). We illustrate our ideas with regard {{to a number of}} systems built by the author...|$|R
