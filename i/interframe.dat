612|8|Public
2500|$|Circular-section {{structure}} of frames and stringers with a continuous main deck and lower decks {{fore and aft}} of the centre section. Rectangular windows in most <b>interframe</b> bays, eight ICAO Type 1a passenger doors on the main deck and three more on the lower deck portside; two freight hold doors and a galley supply door on the lower deck starboard. The main deck houses the flightdeck, two wardrobes, eight toilets, two pantries and a three-section passenger cabin. The lower deck houses three entry vestibules/luggage stores with hydraulic boarding stairs to ground level and fixed stairs to the main deck, a midships galley linked with the main deck by an electric lift, two freight holds (fore and aft of the passenger facilities), an avionics bay and two technical bays. The entire accommodation is pressurised and air-conditioned with [...] "earphones for music or on-board cinema." ...|$|E
5000|$|Data {{frames and}} remote frames are {{separated}} from preceding frames by a bit field called <b>interframe</b> space. <b>Interframe</b> space consists {{of at least three}} consecutive recessive (1) bits. Following that, if a dominant bit is detected, it will be regarded as the [...] "Start of frame" [...] bit of the next frame. Overload frames and error frames are not preceded by an <b>interframe</b> space and multiple overload frames are not separated by an <b>interframe</b> space. <b>Interframe</b> space contains the bit fields intermission and bus idle, and suspend transmission for error passive stations, which have been transmitter of the previous message.|$|E
5000|$|The {{destination}} XGXS adds to or deletes {{from the}} <b>interframe</b> as needed for clock rate disparity compensation prior to converting the <b>interframe</b> code sequence back into XGMII Idle control characters.|$|E
40|$|In {{order to}} improve {{efficiency}} of video coding, temporal redundancy between neighboring frames can be reduced. In MPEG- 2, some frames, named <b>interframes,</b> are predicted using a motion estimation based on the conservation of the intensity over time. In the new standard MPEG- 4, frames are separated into several objects that are transmitted separately. Therefore, the prediction has to be performed on objects instead of frames. So, interframe-objects have to be predicted in {{order to improve}} video coding efficiency...|$|R
30|$|In video {{compression}} technologies, such as MEPG and H. 264 [4], encoded pictures (or frames) {{are arranged in}} groups of pictures (GOPs). An encoded video stream consists of successive GOPs. A GOP can contain the following frame types: I-frame, P-frame, and B-frame. The order of intraframes and <b>interframes</b> is specified in a GOP. An I-frame is a reference picture which is intracoded corresponding to a fixed image and it is independent of other pictures. A P-frame is predictive-coded frame which contains motion-compensated difference information from the preceding I- or P-frame. A B-frame is bidirectionally predictive-coded frame which contains different information from the preceding and following I- or P-frame within a GOP. I-frame and P-frame are {{often referred to as}} anchor frames. A GOP always begins with an I-frame. Afterwards, several P-frames follow. The B-frames are inserted between two consecutive anchor frames.|$|R
30|$|At the Wyner-Ziv encoder, the <b>interframes</b> are {{compressed}} {{using an}} integer 4 × 4 block-based discrete cosine transform (DCT). The DCT coefficients are then fed to a uniform quantizer, and the bitplanes are extracted. The bitplanes are in turn fed to a turbo encoder with two rate 1 / 2 recursive systematic convolutional (RSC) encoders. Each RSC associates the parity bits to the bitplanes. To achieve compression, the systematic bits are discarded since the decoder has already an interpolated {{version of the}} WZ frames. The parity bits are stored in a buffer and sent gradually, packet by packet, upon decoder feedback requests according to a periodic puncturing pattern. The feedback channel helps in adapting the forward transmission rate to the time-varying virtual channel statistics. The WZ decoding process implies several turbo decoding iterations. To alleviate the decoder computational hurdle, an initial number of parity bits packets is estimated {{by way of a}} hybrid encoder/decoder rate control mechanism [13]. These parity bits packets are sent once to the decoder, and subsequent packets will eventually be sent afterwards.|$|R
50|$|M-JPEG is an intraframe-only {{compression}} scheme (compared {{with the}} more computationally intensive technique of <b>interframe</b> prediction). Whereas modern <b>interframe</b> video formats, such as MPEG1, MPEG2 and H.264/MPEG-4 AVC, achieve real-world compression ratios of 1:50 or better, M-JPEG's lack of <b>interframe</b> prediction limits its efficiency to 1:20 or lower, depending on the tolerance to spatial artifacting in the compressed output. Because frames are compressed independently of one another, M-JPEG imposes lower processing and memory requirements on hardware devices.|$|E
50|$|One of {{the most}} {{powerful}} techniques for compressing video is <b>interframe</b> compression. <b>Interframe</b> compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression.|$|E
5000|$|... <b>interframe</b> {{motion is}} limited, {{so as to}} reduce {{temporal}} or spatial judder ...|$|E
40|$|This {{contribution}} introduces tomographic shadowgraphy, {{a volume}} resolving imaging technique which {{is capable of}} recovering the liquid phase of a spray both spatially and temporally. The method {{is based on a}} multiple view camera setup with inline illumination provided by current pulsed LEDs which results in droplet shadows being projected onto the sensor plane. Each camera records image pairs with short <b>interframing</b> times which allows the trajectories of the individual droplets to be estimated using conventional three-dimensional particle tracking approaches. The observed volume is calibrated with a traversed micro target. A comparison is made between several photogrammetric and polynomial least square camera calibration techniques regarding their accuracy in deep volume calibration at magnifications close to unity. A calibration method based on volume calibration from multiple planar homographies at equally spaced z-planes was found to produce the most reliable calibration. Sequential combination of back-projected images at each voxel plane efficiently reproduces the droplet positions in three-dimensional space. The quality of spray reconstruction is investigated using experimental data from multiple view shadowgraphs of a hollow cone spray...|$|R
40|$|In H. 264 /AVC, the {{encoding}} {{process can}} occur {{according to one}} of the 13 intraframe coding modes or {{according to one of}} the 8 available <b>interframes</b> block sizes, besides the SKIP mode. In the Joint Model reference software, the choice of the best mode is performed through exhaustive executions of the entire encoding process, which significantly increases the encoder's computational complexity and sometimes even forbids its use in real-time applications. Considering this context, this work proposes a set of heuristic algorithms targeting hardware architectures that lead to earlier selection of one encoding mode. The amount of repetitions of the encoding process is reduced by 47 times, at the cost of a relatively small cost in compression performance. When compared to other works, the fast hierarchical mode decision results are expressively more satisfactory in terms of computational complexity reduction, quality, and bit rate. The low-complexity mode decision architecture proposed is thus a very good option for real-time coding of high-resolution videos. The solution is especially interesting for embedded and mobile applications with support to multimedia systems, since it yields good compression rates and image quality with a very high reduction in the encoder complexity...|$|R
40|$|The {{presented}} work {{addresses the}} reduction of computational complexity for transcoding of <b>interframes</b> from H. 264 to H. 263 baseline profiles maintaining {{the quality of a}} full search approach. This scenario aims to achieve fast backward compatible interoperability inbetween new and existing video coding platforms, e. g. between DVB-H and UMTS. By exploiting side information of the H. 264 input bitstream the encoding complexity of the motion estimation is strongly reduced. Due to the possibility to divide a macroblock (MB) into partitions with different motion vectors (MV), one single MV has to be selected for H. 263. It will be shown, that this vector is suboptimal for all sequences, even if all existing MVs of a MB of H. 264 are compared as candidate. Also motion vector refinement with a fixed-pel refinement window as used by transcoders throughout the literature is not sufficient for scenes with fast movement. We propose an algorithm for selecting a suitable vector candidate from the input bitstream and this MV is then refined using an adaptive window. Using this technique, the complexity is still low at nearly optimum rate-distortion results compared to an exhaustive full-search approach. Index Terms — Video Transcoding, H. 264 /AVC, H. 263 1...|$|R
5000|$|PCF <b>Interframe</b> Space (PIFS) {{is one of}} the <b>interframe</b> space used in IEEE 802.11 based [...] Wireless LANs. [...] PCF enabled {{access point}} wait for PIFS {{duration}} rather than DIFS to occupy the wireless medium. PIFS duration is less than DIFS and greater than SIFS (DIFS > PIFS > SIFS). Hence AP always has more priority to access the medium.|$|E
50|$|Between {{successive}} frames {{a sequence}} of (at least) six primitives must be transmitted, sometimes called <b>interframe</b> gap.|$|E
5000|$|Because <b>interframe</b> {{compression}} copies {{data from}} one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Making 'cuts' in intraframe-compressed video while video editing {{is almost as}} easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and <b>interframe</b> compression is that, with intraframe systems, each frame uses a similar amount of data. In most <b>interframe</b> systems, certain frames (such as [...] "I frames" [...] in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.|$|E
40|$|Abstract This paper {{presents}} and analyzes {{a new approach}} to data hiding that embeds in both the intra- and <b>interframes</b> from the H. 264 /AVC video codec. Most of the current video data hiding algorithms take into account only the intraframes for message embedding. This may be attributed to the perception that inter-frames are highly compressed due to the motion compensation, and any embedding message inside these may adversely affect the compression efficiency significantly. Payload of the inter-frames is also thought to be less, compared with the intra-frames, because of the lesser residual data. We analyze data hiding in both intra- and inter-frames over a wide range of QP values and observe that the payload of the inter is comparable with that of the intra-frames. Message embedding, in only those non-zero quantized transform coefficients (QTCs) which are above a specific threshold, enables us to detect and extract the message on the decoding side. There is no significant effect on the overall bitrate and PSNR of the video bitstream because instead of embedding message in the compressed bitstream, we have embedded it during the encoding process by taking into account the reconstruction loop. For the non-zero QTCs, in the case of intra-frames, we benefit from the spatial masking, while in the case of inter-frames, we exploit the motion and texture masking. We can notice that the data hiding is done during the compression process and the proposed scheme takes into account the reconstruction loop. The proposed scheme does not target robustness and the obtaine...|$|R
40|$|In a prescient paper Karl Lashley (1951) {{rejected}} reflex chaining {{accounts of}} the sequencing of behavior and argued instead for a more cognitive account in which behavioral sequences are typically controlled with central plans. An important feature of such plans, according to Lashley, {{is that they are}} hierarchical. Lashley offered several sources of evidence for the hierarchical organization for behavioral plans, and others afterward provided more evidence for this hypothesis. We briefly review that evidence here and then shift from a focus on the structure of plans (Lashley's point of concentration) to the processes by which plans are formed in real time. Two principles emerge from the studies we review. One is that plans are not formed from scratch for each successive movement sequence but instead are formed by making whatever changes are needed to distinguish the movement sequence to be performed next from the movement sequence that has just been performed. This plan-modification view is supported by two phenomena discovered in our laboratory: the parameter remapping effect, and the handpath priming effect. The other principle we review is that even single movements appear to be controlled with hierarchically organized plans. At the top level are the starting and goal postures. At the lower level are the intermediate states comprising the transition from the starting posture to the goal posture. The latter principle is supported by another phenomenon discovered in our lab, the end-state comfort effect, and by a computational model of motor planning which accounts for a large number of motor phenomena. Interestingly, the computational model hearkens back to a classical method of generating cartoon animations that relies on the production of keyframes first and the production of <b>interframes</b> (intermediate frames) second...|$|R
5000|$|Because <b>interframe</b> {{compression}} copies {{data from}} one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making 'cuts' in intraframe-compressed video {{is almost as}} easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and <b>interframe</b> compression is that, with intraframe systems, each frame uses a similar amount of data. In most <b>interframe</b> systems, certain frames (such as [...] "I frames" [...] in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.|$|E
50|$|In {{a typical}} image {{transmission}} setup, all stationary images are transmitted at full resolution. Moving pictures possess a lower resolution visually, based on complexity of <b>interframe</b> image content.|$|E
5000|$|Ethernet devices {{must allow}} a minimum idle period between {{transmission}} of Ethernet packets {{known as the}} interpacket gap (IPG), <b>interframe</b> spacing, or <b>interframe</b> gap (IFG). A brief recovery time between packets allows devices to prepare for reception of the next packet. While some physical layer variants literally transmit nothing during the idle period, most modern ones transmit a constant signal and send idle symbols. The standard minimum interpacket gap for transmission is 96 bit times (the {{time it takes to}} transmit 96 bits of raw data on the medium), which is ...|$|E
5000|$|It tolerates rapidly {{changing}} {{motion in the}} video stream, whereas compression schemes using <b>interframe</b> compression can often experience unacceptable quality loss when the video content changes significantly between each frame.|$|E
5000|$|The source XGXS {{converts}} XGMII Idle control characters (<b>interframe)</b> into an 8b/10b code sequence. The destination XGXS recovers {{clock and}} data from each XAUI lane and deskews the four XAUI lanes into the single-clock XGMII.|$|E
5000|$|Since <b>interframe</b> {{motion is}} often {{predictable}} owing to Newton's laws of {{motion in the}} real world, the motion vector can then be used to calculate where the block will probably be in the next field.|$|E
50|$|HDV footage can be {{natively}} {{edited by}} most non-linear editors, with real-time playback being possible on modern mainstream personal computers. Slower computers may exhibit reduced performance {{compared to other}} formats such as DV because of high resolution and <b>interframe</b> compression of HDV video.|$|E
5000|$|For Fibre Channel, {{there is}} a {{sequence}} of primitives between successive frames, sometimes called <b>interframe</b> gap as well. The minimum sequence consists of six primitives, [...] Each primitive consists of four channel words of 10 bits each for 8b/10b encoded variants (1-8 Gbit/s), equivalent to four data bytes.|$|E
50|$|The IEEE 802.1Qbv time-aware {{scheduler}} has {{to ensure}} that the Ethernet interface is not busy with the transmission of a frame when the scheduler changes from one time slice into the next. The time-aware scheduler achieves this by putting a guard band in front of every time slice that carries time-critical traffic. During this guard band time, no new Ethernet frame transmission may be started, only already ongoing transmissions may be finished. The duration of this guard band has to be {{as long as it takes}} the maximum frame size to be safely transmitted. For an Ethernet frame according to IEEE 802.3 with a single IEEE 802.1Q VLAN tag and including <b>interframe</b> spacing, the total length is: 1518 byte (frame) + 4 byte (VLAN Tag) + 12 byte (<b>Interframe</b> spacing) = 1534 byte.|$|E
5000|$|PHY {{level data}} rate {{does not match}} user level {{throughput}} because of 802.11 protocol overheads, like the contention process, <b>interframe</b> spacing, PHY level headers (Preamble + PLCP) and acknowledgment frames. The main media access control (MAC) feature that provides a performance improvement is aggregation. Two types of aggregation are defined: ...|$|E
5000|$|... 1 bit => Start {{of frame}} 11 bits => Identifier 1 bit => RTR bit 6 bits => Control field 0-8 bytes => Data field 15 bits => CRC {{sequence}} 1 bit => CRC delimiter 1 bit => Acknowledge 1 bit => Ack delimiter 7 bits => End of frame >2 bits => <b>Interframe</b> space ...|$|E
5000|$|JPEG is inefficient, {{using more}} bits to deliver similar quality, {{compared}} to more modern formats (such as JPEG 2000 and H.264/MPEG-4 AVC). Since {{the development of}} the original JPEG standard in the early 1990s, technology improvements have been made not only to the JPEG format but to the <b>interframe</b> compression schemas possible as well.|$|E
50|$|Reduced <b>Interframe</b> Space (RIFS) {{is one of}} the new {{features}} introduced in IEEE 802.11n to improve its efficiency. RIFS is the time in micro seconds by which the multiple transmissions from a single station is separated. RIFS is used when no SIFS separated response frames are expected from the receiver. The value of RIFS is 2μs for 802.11n phy.|$|E
50|$|Short <b>Interframe</b> Space (SIFS), is {{the amount}} of time in microseconds {{required}} for a wireless interface to process a received frame and to respond with a response frame. It is the difference in time between the first symbol of the response frame in the air and the last symbol of the received frame in the air. A SIFS time consists of the delay in receiver RF, PLCP delay and the MAC processing delay, which depends on the physical layer used. In IEEE 802.11 networks, SIFS is the <b>interframe</b> spacing prior to transmission of an acknowledgment, a Clear To Send (CTS) frame, a block ack frame that is an immediate response to either a block ack request frame or an A-MPDU, the second or subsequent MPDU of a fragment burst, a station responding to any polling a by point coordination function and during contention free periods of point coordination function.|$|E
5000|$|The Am7990 {{can handle}} 10BASE-5 Type A, 10BASE-2 Type B, and 10BASE-T. Back-to-back packet {{reception}} {{with as little}} as 0,5 µs <b>interframe</b> spacing. DMA/Bus mastering 24-bit (16M) address capable. Up to 128 ring buffers can be used. 48 byte receive/transmit FIFO. Operates with 5 volt DC 5% supply and logic. Features an Time-domain reflectometer (TDR) with a granularity of 30 meter. 16,8 MHz maximum frequency.|$|E
5000|$|... 60 Hz {{material}} captures {{motion a}} bit more [...] "smoother" [...] than 50 Hz material. The drawback is that it takes approximately 1/5 more bandwidth to transmit, if all other parameters of the image (resolution, aspect ratio) are equal. [...] "Approximately", because <b>interframe</b> compression techniques, such as MPEG, are {{a bit more}} efficient with higher frame rates, because the consecutive frames also become a bit more similar.|$|E
50|$|The IEEE 802.11 {{family of}} {{standards}} describe the DCF protocol, which controls {{access to the}} physical medium. A station must sense {{the status of the}} wireless medium before transmitting. If it finds that the medium is continuously idle for DCF <b>Interframe</b> Space (DIFS) duration, it is then permitted to transmit a frame. If the channel is found busy during the DIFS interval, the station should defer its transmission.|$|E
50|$|HDV {{video and}} audio are encoded in digital form, using lossy <b>interframe</b> {{compression}}. Video is encoded with the H.262/MPEG-2 Part 2 compression scheme, using 8-bit chroma and luma samples with 4:2:0 chroma subsampling. Stereo audio is encoded with the MPEG-1 Layer 2 compression scheme. The compressed audio and video are multiplexed into an MPEG-2 transport stream, which is typically recorded onto magnetic tape, but can also be stored in a computer file.|$|E
5000|$|AIFS {{is a time}} {{interval}} between frames being transmitted under the IEEE 802.11e EDCA MAC. It depends on the Access Category and generally depends on the AIFSN, or AIFS-number. AIFS {{is defined by the}} formula AIFSNAC * ST + SIFS, where the AIFSN depends on the Access Category. Slot time ST (also denoted by [...] ) is dependent on the physical layer. Short <b>Interframe</b> Space (SIFS) is the time between a DATA and ACK frame.|$|E
