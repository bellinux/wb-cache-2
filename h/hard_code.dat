19|322|Public
50|$|The {{first two}} options of {{creating}} or looking up dependencies are not optimal because they <b>hard</b> <b>code</b> the dependency to the component. This makes it difficult, if not impossible, {{to modify the}} dependencies. This is especially problematic in tests, where it is often desirable to provide mock dependencies for test isolation.|$|E
5000|$|JMyron (aka WebcamXtra) is an {{external}} library [...] for Processing that allows image manipulation {{without having to}} <b>hard</b> <b>code</b> everything. This is great because we can extend {{what we saw in}} the Video Library tutorial and add other aspects to it like motion tracking and color tracking. To use JMyron, you have to download the JAR {{and put them in the}} path/to/Processing/libraries folder. You must then import the library in each Processing sketch where JMyron objects will be used.|$|E
5000|$|Some Windows {{programs}} <b>hard</b> <b>code</b> {{the profile}} path to developer-defined locations such as [...] This is the path {{for the vast}} majority of Windows 2000 or above, but this would cause an error if the profile is stored on a network or otherwise relocated. The proper way to get it is to call the [...] function or to resolve the [...] environment variable. Another assumption that developers often make is assuming that the profile is located on a local hard disk.|$|E
50|$|Dentrix G6 uses <b>Hard</b> <b>Coded</b> Credentials.|$|R
50|$|Circuits are not <b>hard</b> <b>coded</b> {{to perform}} a {{specific}} task.|$|R
50|$|The initialism HC in some {{releases}} {{refers to}} <b>hard</b> <b>coded</b> subtitles.|$|R
5000|$|Path hard coding: The {{application}} {{should not}} have folder/file path hard coded in the application itself. Some applications <b>hard</b> <b>code</b> the path of files in their executables rather than parameterizing them or storing them in the Windows Registry. Configuration files ending in extensions such as ini, conf, dat, and txt are good places to look for application-specific settings of path information that may cause problems. Failing that, a shim {{can be used to}} remediate the application where source code or an update is not available.|$|E
40|$|This paper {{describes}} {{a program that}} compiles BURS tables into a combination of <b>hard</b> <b>code</b> and data. Hard-coding exposed important opportunities for compression that were previously hidden in the tables, so the hard-coded code generators are not just faster but also significantly smaller than their predecessors. A VAX code generator takes 21. 4 Kbytes and identifies optimal assembly code in about 50 VAX instructions per nod...|$|E
40|$|Creating a file {{date for}} each input file, when {{one does not}} exist in the file, becomes a tedious task as the number of files grows. Different methods can be used, such as reading in all the files at once, using a file counter, the Job File Control Block (JFCB), and {{implementing}} macros. The key was to find a solution that reduces both the programmer’s time and CPU time. When all methods were compared to reading in each file in its own data step, the file counter and JFCB both increased CPU time, but decreased the amount of hard coding necessary, while using a macro decreased the amount of hard coding and CPU time. This made the macro the most efficient solution when looking at both the programmer’s time and CPU time. This method can be used if the file date is anywhere in the filename. When the need arises to do analysis based on the file date of the file, it can be time consuming to <b>hard</b> <b>code</b> the file date in for each file. This is especially true when the analysis is needed covering multiple years on a file that is submitted every month. There are different techniques available to accomplish this task without having to <b>hard</b> <b>code</b> the file date. One of the techniques, the Job File Control Block, is specific to working on the mainframe. The other...|$|E
5000|$|Keyboard driver with {{predefined}} English and Spanish/Catalan keymap layout (<b>hard</b> <b>coded).</b>|$|R
50|$|<b>Hard</b> <b>coding</b> is {{especially}} problematic {{in preparing the}} software for translation to other languages.|$|R
5000|$|Untranslated {{messages}} {{in the original}} language may be left <b>hard</b> <b>coded</b> in the source code.|$|R
40|$|Abstract—In general, {{reports are}} {{a form of}} {{representing}} data in such way that user gets the information he needs. They can be built in various ways, from the simplest (“select from”) to the most complex ones (results derived from different sources/tables with complex formulas applied). Furthermore, rules of calculations could be written as a program <b>hard</b> <b>code</b> or built in the database {{to be used by}} dynamic code. This paper will introduce two types of reports, defined in the DB structure. The main goal is to manage calculations in optimal way, keeping maintenance of reports as simple and smooth as possible. Keywords—Data Definition diagram, Server Model Diagram, system modelling, reports...|$|E
40|$|Adaptive {{programming}} (AP) is a programming {{paradigm for}} expressing structure-shy computations over semi-structured data graphs. Structure-shyness means that adaptive pro-grams <b>hard</b> <b>code</b> a minimal set of {{assumptions about the}} structure of their input. Because of this, adaptive programs {{are more susceptible to}} unsafe evolutions; evolutions that jeopardize the correctness of adaptive programs yet go un-caught. In this paper we study the evolution of adaptive programs and present two complementary approaches for controlling their unsafe evolution: a language for express-ing application-specific constraints on the runtime behavior of adaptive programs, and a stricter notion of compatibility between the parts of an adaptive program, that does not sacrifice the expressiveness of the AP paradigm...|$|E
40|$|Abstract: Procedure call-based {{component}} connections {{which are}} latent in <b>hard</b> <b>code</b> do not only restrict {{the flexibility of}} software but also cause hidden problems to software reliability because of the existing deadlock connection loops. To solve this problem, first, a formal semantic model called call-based connector has been built which explicitly separates connection from components. Second, mapping rules used to convert call-based connectors into component the connection directed graph are proposed. Then, two algorithms, TPDCC (two phases deadlock connection check) used to find all existing deadlock connection loops, and DCEMRF (deadlock connection elimination based on maximum reuse frequency) used to find locations with the least number of connections that must be eliminated to eliminate the loops, are provided respectively. Last, its application and experimental {{results show that the}} presented approach is feasible and effective, so {{it can be used to}} enhance the reliability of software, also be fit as a basis to further design and implement adaptive connector due to its separative way of description and storage of component connection in semantic...|$|E
5000|$|Inline data (ID) - Measures {{the amount}} of effort spent on the {{embedding}} <b>hard</b> <b>coded</b> data ...|$|R
50|$|A {{few times}} Butcher talk ("Rechtub klat") is spoken. At those times <b>hard</b> <b>coded</b> {{subtitles}} are used.|$|R
40|$|The <b>hard</b> <b>coding</b> {{of input}} data or {{constants}} into spreadsheet formulas is widely recognised as poor spreadsheet model design. However, {{the importance of}} avoiding such practice appears to be underestimated perhaps {{in light of the}} lack of quantitative error at the time of occurrence and the recognition that this design defect may never result in a bottom-line error. The paper examines both the academic and practitioner view of such <b>hard</b> <b>coding</b> design flaws. The practitioner or industry viewpoint is gained indirectly through a review of commercial spreadsheet auditing software. The development of an automated (electronic) means for detecting such <b>hard</b> <b>coding</b> is described together with a discussion of some results obtained through analysis of a number of student and practitioner spreadsheet models. Comment: 10 Pages, 5 Table...|$|R
40|$|Abstract – One of the {{significant}} challenges for current and future manufacturing systems is that of providing rapid reconfigurability in order to evolve and adapt to mass customization. Service-Oriented Architecture has been proposed as a technology facilitator to create systems by encapsulating, composing and reconfiguring machines and devices rather than reprogramming the <b>hard</b> <b>code.</b> The SOA paradigm holds the promise of being applicable across the entire spectrum of manufacturing systems and devices, down to sensors and actuators. One of the pillars of SOA is the mechanism by which service advertisements are published by providers and subsequently discovered and selected by requestors. This paper presents a protocol for the dynamic discovery of services in industrial environments. The protocol is designed for operation in embedded industrial controllers, and operates in a peer-to-peer topology without a need for centralized service registries. Furthermore, the protocol enables conveying information regarding services that are semantically interoperable but that use different invocation syntax. In order to infer such semantic relationships between services, the OWL-S Semantic Web Services Ontology is used. The protocol was deployed in embedded industrial controllers {{in order to facilitate}} the reconfiguration of assembly systems that employ CAMX standards for communications. I...|$|E
40|$|Abstract—In {{recent years}} several {{symbolic}} execution-based tools {{have been developed}} to automatically select relevant test inputs from the source code of the system under test. However, each of these tools has different advantages, and there is no detailed feedback available on the actual capabilities of the various tools. In order to evaluate test input generators we collected a representative set of programming language concepts that should be handled by the tools, mapped them to 300 code snippets that would serve as inputs for the tools, created an automated framework to execute and evaluate these snippets, and performed experiments on four Java and one. NET test generator tools. The results highlight {{the strengths and weaknesses of}} each tool, and identify <b>hard</b> <b>code</b> parts that are difficult to tackle for most of the tools. We hope that our research could serve as actionable feedback to tool developers and help practitioners assess the readiness of test input generation. 978 - 1 - 4799 - 7125 - 1 / 15 /$ 31. 00 c© 2015 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses...|$|E
40|$|Domain-specific {{modeling}} {{has become}} a popular way of designing and developing systems. It generally involves a systematic use {{of a set of}} object-oriented models to represent various facets of a domain. However, manually creating instances of these models is time-consuming and errorprone when a system in the domain is complex. Automatic model synthesis tools are thus usually developed to free users from the model creation process. In practice, most of these tools would <b>hard</b> <b>code</b> knowledge about the domain specific models in the program. A biggest problem with tools is that their source code needs to be changed whenever the knowledge changes. In this paper, we define a model markup language (ModelML) to facilitate the development of automatic model synthesis tools. The language provides a complete self-describing representation of object-oriented models to be synthesized. Unlike other XML-based representations of models, ModelML reflects the structure of the models directly in the nesting of elements in the XML-based syntax. This feature allows the knowledge about the domain specific models to be decoupled from model synthesis tools. To demonstrate the usefulness of the markup language, we have developed a generic automatic model synthesis tool which is based on ModelML inputs. ...|$|E
5000|$|... the SDP's service agility (that is {{the product}} and {{services}} being offered are <b>hard</b> <b>coded</b> into the SDP so that new services cause code upgrades) and; ...|$|R
40|$|Video games {{provide a}} rich testbed for {{artificial}} intelligence methods. In particular, creating automated opponents that perform well in strategy games {{is a difficult}} task. For instance, human players rapidly discover and exploit the weaknesses of <b>hard</b> <b>coded</b> strategies. To build better strategies, we suggest a reinforcement learning approach for learning a policy that switches between high-level strategies. These strategies are chosen based on different game situations and a fixed opponent strategy. Our learning agents are able to rapidly adapt to fixed opponents and improve deficiencies in the <b>hard</b> <b>coded</b> strategies, as the results demonstrate...|$|R
50|$|In some {{situations}} MSOs have millions of lines of <b>hard</b> <b>coded</b> product and service management flows in their systems and {{are unable to}} move to the newer converged service dimensions easily.|$|R
40|$|Compressive sensing {{algorithms}} {{are currently}} being used to both capture and save still photos. The most efficient algorithms out there can take seconds to 1 minute or {{so in order to}} complete. However, these are done in software. We would like to <b>hard</b> <b>code</b> the algorithm so that we can decrease the compression time by a factor of 10 or 100. That way we can compress video instead of just still photos. Standard compression/imaging schemes work on the following principle [1]: Sample at the Nyquist rate (2 x the bandwidth), get N samples. A non-linear compression is performed and you get K samples (K << N). This can then be stored or transmitted. However, the input was K-sparse in some basis psi to begin with. Compressive sensing works by directly acquiring compressed data. Instead of taking N samples, collect M samples: K < M << N. Mathematically these M samples come from some matrix phi (dimension MxN) being multiplied by the signal x (with dimension Nx 1). M can be a random matrix (white Gaussian noise). The result is a matrix y with dimension Mx 1. To recover the signal following this algorithm: Find the 'active ' component by seeing which column of phi is most correlated with y...|$|E
40|$|This {{dissertation}} {{describes a}} system that constructs efficient, retargetable code generators and optimizers. chop reads nonprocedural descriptions of a computer's instruction set and of a naive code generator for the computer, and it writes an integrated code generator and peephole optimizer for it. The resulting code generators are very efficient because they interpret no tables; they are completely hard-coded. Nor do they build complex data structures to communicate between code generation and optimization phases. Interphase communication is reduced {{to the point that}} the code generator's output is often encoded in the program counter and conveyed to the optimizer by jumping to the right label. chop's code generator and optimizer are based on a very simple formalism, namely rewriting rules. An instrumented version of the compiler infers the optimization rules as it complies a training suite, and it records them for translation into <b>hard</b> <b>code</b> and inclusion into the production version. I have replaced the Portable C Compiler's code generator with one generated by chop. Despite a costly interface, the resulting compiler runs 30 % to 50 % faster than the original Portable C Compiler (pcc) and generates comparable code. This figure is diluted by common lexical analysis, parsing, and semantic analysis and by comparable code emission. Allowing for these, the new code generator appears to run approximately seven times faster than that of the original pcc...|$|E
40|$|There is a push in the {{enterprise}} towards facilitating processes from best practice frameworks (such as the IT Infrastructure Library (ITIL)) {{to make them}} more repeatable, efficient and cost-effective. Best practice processes provide descriptive, high level guidelines rather than prescriptive, precise process model definitions. They are meant to be followed by people and may be adapted and enacted differently in various realizations. Currently, ITIL processes are either supported by tools that <b>hard</b> <b>code</b> an interpretation of the process logic, or followed by people using productivity tools. This is inefficient because existing tools hardcode a rigid logic of the processes, and do not support collaborative and flexible realizations of processes. Moreover, there is a risk of information loss when people using rigid productivity tools, and are forced to collaborate outside of those tools. In this paper, we present a conversation-centered approach and a tool that enables dynamic and flexible definition and enactment of best practice processes in a collaborative and interactive manner. We address the issue of information loss by using the concept of a conversation as a container of information about the interactions among people {{in the context of a}} process. A conversation is backed with a semi-structured process model and process templates to support flexible and adaptive process realization. We showcase the approach using an illustrative use case in incident an...|$|E
40|$|Abstract. In open multi-agent systems, agents must accord social {{conventions}} {{in order}} to maintain the predictable integration. Usually, these social conventions are <b>hard</b> <b>coded,</b> leading to unsuitable systems. A solution to <b>hard</b> <b>coded</b> conventions is separate the system’s social convention into a separate module insuring agents compliance. This technique is called law enforcement. In this paper, we propose an approach for integration tests in open multiagent systems. This approach supports the creation of test cases based on the information provided by the definition of system rules. We propose to use XMLaw, a language for the specification of open multi-agent systems. 1...|$|R
2500|$|After 6 December 2003, various system {{messages}} {{shown to}} Wikipedia users {{were no longer}} <b>hard</b> <b>coded,</b> allowing Wikipedia [...] to modify certain parts of MediaWiki's interface, such as the message shown to blocked users.|$|R
50|$|While {{this will}} make current Rainbow tables useless, if the salt is <b>hard</b> <b>coded</b> into a popular product that salt can be {{extracted}} and a new Rainbow table can be generated using that salt.|$|R
40|$|In {{programming}} {{activities during}} clinical trial reporting, situations can arise when {{a change in}} a portion of SAS ® code is warranted after the programs are finalized and moved into a production environment. It is a very common situation during the clinical trial reporting process. There may be one minor change in the SAS ® code to be applied repeatedly to {{a large number of}} programs. One way to achieve this is by changing the <b>hard</b> <b>code</b> to each of the programs. However, if the number of programs is sufficiently large, then the risk of making mistakes due to hard coding increases. Therefore, it is always better to avoid repeated hard coding, instead it should be coded only once and the SAS ® macro tool should automatically find that string of code and go into each of the programs and make the necessary changes. That way the error due to hard coding is minimized. This SAS ® macro program tool first identifies the number of programs residing in a certain library and then goes to each program one by one, identifies the lines or string of text to be modified and finally makes necessary changes. Then it automatically saves the modified program in another output library. This SAS ® macro tool is created for the PC platform of Microsoft Windows and also for MVS Mainframe. This macro minimizes risk of errors due to typing. Furthermore, it could be applied to a large number of programs in need of modification...|$|E
40|$|Significant {{progress}} has been made in the area of final form document description with Postscript emerging as a de facto standard. Some {{progress has}} been made on the translation of page description languages [...] Postscript to Interpress and Interpress to Postscript. However, these developments are of no assistance in the sharing of documents that may need to be revised. The ISO OSI standards for revisable form documents interchange continue to evolve with the Standard Generalized Markup Language, the Office Document Architecture, and the Office document Interchange Format offering great promise for the future. While the ultimate solution to document interchange is universal acceptance of an interchange standard, some intermediate solution is required to meet the needs that exist today. It is in this context that document conversion systems are considered. The approaches to conversion fall in two broad categories. The first category includes systems that <b>hard</b> <b>code</b> each conversion. The second category of system rely on an intermediate metalanguage [...] a standard. This paper describes a hybrid system for converting files created by one text processing system to a format suitable for another. The discussions covers the design principles for an extended machine and table driven approach. Tables allow for user involvement in customizing and developing new conversions while an extended machine allows for logical and arithmetic control of the conversion. The paper describes the kinds of copymarks found in text files, sets out the scope of what a copymark conversion engine must do, describes how various approaches tackle the problem, and describes how the conversion engine produced accomplishes these goals...|$|E
40|$|A Networked Virtual Environment (Net-VE) is a {{distributed}} software {{system in which}} multiple users {{interact with each other}} in real time even though these users may be located around the world [Zyda 99]. Net-VEs gained first attention through a variety of DOD and Academic research projects. After release of the multiplayer game DOOM, the gaming industry captured the idea of interactive multiplayer games. Today there are many popular Internet-based multiplayer games available. Effective networking of diverse entities and systems is a common problem for Networked Virtual Environments. In order to communicate with other entities a variety of communication protocols are used. Historically these communication protocols are "hard coded" into the software system and all nodes that participate in the environment must identically implement the protocols to interact with others. These communication protocols require authoring and compiling by a trained programmer. When the compiling process is introduced to the networked virtual environment, it detracts the extensibility and dynamicism of the system. This thesis presents the design and development of a Networked Virtual Environment model that uses Cross Format Schema Protocol (XFSP). With this work we show that a networked simulation can work for 24 hours a day and 7 days a week with an extensible schema based networking protocol and {{it is not necessary to}} <b>hard</b> <b>code</b> and compile the protocols into the networked virtual environments. Furthermore, this thesis presents a general automatic protocol handler for schema-defined XML document or message. Additionally, this work concludes with idea that protocols can be loaded and extended at runtime, and can be created with different-fidelity resolutions, resulting in swapping at runtime ba [...] ...|$|E
5000|$|Conveyors: {{automated}} conveyors {{allow the}} input of containers in one area of the warehouse, and either through <b>hard</b> <b>coded</b> rules or data input allow destination selection. The container will later appear at the selected destination.|$|R
40|$|The {{purpose of}} this thesis is to present {{solution}} for detection of violations in time and attendance system T&S. Classical solutions for detecting violations are based on predefined rules which are typically <b>hard</b> <b>coded</b> into systems and are difficult to change. Rules are usually dependant on local laws that are enforced in individual countries. It {{is almost impossible to}} find two countries that would have identical work related laws. Therefore this thesis presents a way that is not based on <b>hard</b> <b>coded</b> rules but it uses worker habits and deviations from their normal patterns. To present the findings of this thesis web application was developed that presents operator with possible violations...|$|R
40|$|The {{performance}} requirements imposed on (<b>hard)</b> realtime <b>code</b> resp. non real-time code differ. As a consequence, conventional coding strategies as {{used to develop}} non realtime software are not suited for <b>hard</b> real-time <b>code.</b> This paper shows why non real-time coding is not suited for hard real-time systems and presents WCET-oriented programming as a strategy that avoids these shortcomings. It further discusses components of an infrastructure that support the WCET-oriented development of <b>hard</b> real-time <b>code.</b> ...|$|R
