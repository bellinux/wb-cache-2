824|2677|Public
25|$|The first {{specific}} {{occurrence of}} aperiodic tilings arose in 1961, when logician Hao Wang tried {{to determine whether}} the Domino Problem is decidable — that is, whether there exists an algorithm for deciding if a given finite set of prototiles admits a tiling of the plane. Wang found algorithms to enumerate the tilesets that cannot tile the plane, and the tilesets that tile it periodically; by this he showed that such a <b>decision</b> <b>algorithm</b> exists if every finite set of prototiles that admits a tiling of the plane also admits a periodic tiling.|$|E
2500|$|For instance, first-order {{logic is}} undecidable, meaning a sound, {{complete}} and terminating <b>decision</b> <b>algorithm</b> is impossible. This {{has led to}} the study of interesting decidable fragments such as C2, first-order logic with two variables and the counting quantifiers [...] and [...] (these quantifiers are, respectively, [...] "there exists at least n" [...] and [...] "there exists at most n").|$|E
5000|$|Using this <b>decision</b> <b>{{algorithm}}</b> as {{both the}} test algorithm and the <b>decision</b> <b>algorithm</b> of a parametric search leads to an algorithm for finding the optimal time [...] in quadratic total time. To simulate the <b>decision</b> <b>algorithm</b> for parameter , the simulation must determine, for each particle, whether its crossing time is before or after , and therefore whether {{it is to the}} left or right of the origin at time [...] Answering this question for a single particle - comparing the crossing time for the particle with the unknown optimal crossing time [...] - can be performed by running the same <b>decision</b> <b>algorithm</b> with the crossing time for the particle as its parameter.Thus, the simulation ends up running the <b>decision</b> <b>algorithm</b> on each of the particle crossing times, one of which must be the optimal crossing time.Running the <b>decision</b> <b>algorithm</b> once takes linear time, so running it separately on each crossing time takes quadratic time.|$|E
50|$|In 1992, Ben-David et al., {{showed that}} if all {{languages}} in distNP have good-on-average <b>decision</b> <b>algorithms,</b> they also have good-on-average search algorithms. Further, they show that this conclusion holds under a weaker assumption: if every language in NP is easy on average for <b>decision</b> <b>algorithms</b> {{with respect to the}} uniform distribution, then it is also easy on average for search algorithms with respect to the uniform distribution. Thus, cryptographic one-way functions can exist only if there are distNP problems over the uniform distribution that are hard on average for <b>decision</b> <b>algorithms.</b>|$|R
30|$|However, general <b>decision</b> tree <b>algorithms</b> {{also have}} some disadvantages. Sometimes, the number of nodes {{generated}} from a <b>decision</b> tree <b>algorithm</b> is too large to interpret. Moreover, the performance of general <b>decision</b> tree <b>algorithms</b> is not usually better than that of functional or statistical machine learning algorithms, such as neural networks, support vector machines, and Bayesian networks. Recently, <b>decision</b> tree-based <b>algorithms</b> have strictly improved and become sophisticated {{in combination with other}} methods such as boosting [27] or logistic regression [48].|$|R
40|$|We survey recent {{results on}} <b>decision</b> <b>algorithms</b> for subfamilies of regular languages. In particular, {{we look at}} the <b>decision</b> <b>algorithms</b> using statepair graphs {{constructed}} from finite-state automata. The algorithms rely on the structural property of a finite-state automaton that is preserved in its state-pair graph. We also review applications of state-pair graphs in different subfamilies of regular languages. ∗ Han was supported by the KIST Tangible Space Initiative Grants 2 E 20050 and 2 Z 03050. The Bulletin of the EATCS...|$|R
5000|$|As [...] already observed, the {{parametric}} search technique can {{be substantially}} sped up by replacing the simulated test algorithm by an efficient parallel algorithm, {{for instance in}} the parallel random-access machine (PRAM) model of parallel computation, where a collection of processors operate in synchrony on a shared memory, all performing the same sequence of operations on different memory addresses. If the test algorithm is a PRAM algorithm uses [...] processors and takes time [...] (that is, [...] steps in which each processor performs a single operation), then each of its steps may be simulated by using the <b>decision</b> <b>algorithm</b> to determine the results of at most [...] numerical comparisons. By finding a median or near-median value in the set of comparisons {{that need to be}} evaluated, and passing this single value to the <b>decision</b> <b>algorithm,</b> it is possible to eliminate half or nearly half of the comparisons with only a single call of the <b>decision</b> <b>algorithm.</b> By repeatedly halving the set of comparisons required by the simulation in this way, until none are left, it is possible to simulate the results of [...] numerical comparisons using only [...] calls to the <b>decision</b> <b>algorithm.</b> Thus, the total time for parametric search in this case becomes [...] (for the simulation itself) plus the time for [...] calls to the <b>decision</b> <b>algorithm</b> (for [...] batches of comparisons, taking [...] calls per batch). Often, for a problem that can be solved in this way, the time-processor product of the PRAM algorithm is comparable to the time for a sequential <b>decision</b> <b>algorithm,</b> and the parallel time is polylogarithmic, leading to a total time for the parametric search that is slower than the <b>decision</b> <b>algorithm</b> by only a polylogarithmic factor.|$|E
50|$|On {{a simple}} curve, the even-odd rule reduces to a <b>decision</b> <b>algorithm</b> for {{the point in}} polygon problem.|$|E
50|$|Since the <b>decision</b> <b>algorithm</b> itself {{necessarily}} behaves discontinuously at , {{the same}} algorithm {{can also be}} used as the test algorithm. However, many applications use other test algorithms (often, comparison sorting algorithms). Advanced versions of the parametric search technique use a parallel algorithm as the test algorithm, and group the comparisons that must be simulated into batches, in order to significantly reduce the number of instantiations of the <b>decision</b> <b>algorithm.</b>|$|E
40|$|Medically related {{decision-making}} in workers' compensation, {{especially in}} areas of patient eligibility for benefits and determination of exact medical and compensation benefits, differs significantly from similar decisions in clinical practice and acute care payment. A government-sponsored program has been developing guidelines and <b>decision</b> <b>algorithms</b> to aid in decision-making in these areas, focusing on occupational diseases. These <b>decision</b> <b>algorithms</b> are now being automated to support national implementation of the guidelines by Federal agencies and to encourage their adoption by the private sector...|$|R
40|$|Real-time <b>Decision</b> <b>algorithms</b> are a {{class of}} {{incremental}} resource-bounded [Horvitz, 89] or anytime [Dean, 93] algorithms for evaluating influence diagrams. We present a test domain for real-time <b>decision</b> <b>algorithms,</b> {{and the results of}} experiments with several Real-time <b>Decision</b> <b>Algorithms</b> in this domain. The results demonstrate high performance for two algorithms, a decision-evaluation variant of Incremental Probabilisitic Inference [D'Ambrosio, 93] and a variant of an algorithm suggested by Goldszmidt, [Goldszmidt, 95], PKreduced. We discuss the implications of these experimental results and explore the broader applicability of these algorithms. Introduction The problem A variety of algorithms have been proposed as candidates for anytime [Dean, 93] or resource-bounded [Horvitz et al, 89] inference, including [D'Ambrosio, 93], [Horvitz et al, 89 b], and a variety of simulation-based algorithms such as [Fung, 89]. The need for such algorithms arises because implementable agents have f [...] ...|$|R
40|$|Analogue (and addressable) fire {{detection}} systems enables a new quality in improving sensitivity to real fires and reducing susceptibility to nuisance alarm sources. Different <b>decision</b> <b>algorithms</b> types were developed with intention to improve sensitivity and reduce false alarm occurrence. At the beginning, it was free alarm level adjustment based on preset level. Majority of multi-criteria decision work was based on multi-sensor (multi-signature) <b>decision</b> <b>algorithms</b> - using different type of sensors on the same location or, rather, using different aspects (level and rise) of one sensor measured value. Our idea is to improve sensitivity and reduce false alarm occurrence by forming groups of sensors that work in similar conditions (same world side in the building, same or similar technology or working time). Original multi-criteria <b>decision</b> <b>algorithms</b> based on level, rise and difference of level and rise from group average are discussed in this paper...|$|R
50|$|The {{handover}} procedure specifies {{the control}} signalling used {{to perform the}} handover and is invoked by the handover <b>decision</b> <b>algorithm.</b>|$|E
5000|$|In {{the most}} basic form of the {{parametric}} search technique, both the test algorithm and the decision algorithms are sequential (non-parallel) algorithms, possibly the same algorithm as each other. The technique simulates the test algorithm step by step, as it would run when given the (unknown) optimal solution value as its parameter [...] Whenever the simulation reaches a step in which the test algorithm compares its parameter [...] to some other number , it cannot perform this step by doing a numerical comparison, as it {{does not know what}} [...] is. Instead, it calls the <b>decision</b> <b>algorithm</b> with parameter , and uses the result of the <b>decision</b> <b>algorithm</b> to determine the output of the comparison. In this way, the time for the simulation ends up equalling the product of the times for the test and decision algorithms. Because the test algorithm is assumed to behave discontinuously at the optimal solution value, it cannot be simulated accurately unless one of the parameter values [...] passed to the <b>decision</b> <b>algorithm</b> is actually equal to the optimal solution value. When this happens, the <b>decision</b> <b>algorithm</b> can detect the equality and save the optimal solution value for later output.If the test algorithm needs to know the sign of a polynomial in , this can again be simulated by passing the roots of the polynomial to the <b>decision</b> <b>algorithm</b> in order to determine whether the unknown solution value is one of these roots, or, if not, which two roots it lies between.|$|E
5000|$|... 2) Bousquet J, Schunemann HJ, (…) Kaidashev IP, et al. MACVIA {{clinical}} <b>decision</b> <b>algorithm</b> {{in adolescents}} and adults with allergic rhinitis. J Allergy Clin Immunol. 2016; 38(2): 367-374. doi: 10.1016/j.jaci.2016.03.025.|$|E
40|$|We present several <b>decision</b> <b>algorithms</b> {{within the}} fuzzy propositional logic based on Zadeh 2 ̆ 7 s {{implication}} operator p &# 8594; q = max 1 -p, q, deciding both the fuzzy SAT problem {{as well as}} the best truth value bound problem, i. e. compute the best truth value bounds of a proposition with respect to a theory. Further, we evaluate all the algorithms by adapting and extending the well know methods for evaluating SAT <b>decision</b> <b>algorithms.</b> We show that both problems present the typical easy-hard-easy pattern...|$|R
40|$|In {{real-time}} video processing data transfer between CPU and GPU {{is a time}} critical action; time spent transferring data is processing time lost. Several variants of standard transfer methods were developed and evaluated on nine computers and two smart <b>decision</b> <b>algorithms</b> was designed to help choose the fastest method for each occasion. Results showed that the standard transfer methods can be beaten; by using the designed <b>decision</b> <b>algorithms,</b> transfer times between CPU and GPU (both ways) can be reduced {{by a factor of}} 7 compared to always using the standard methods...|$|R
50|$|For Monte Carlo <b>decision</b> <b>algorithms</b> with two-sided error, {{the failure}} {{probability}} may again {{be reduced by}} running the algorithm k times and returning the majority function of the answers.|$|R
5000|$|... combine Cole's {{theoretical}} improvement {{with the}} practical speedups of [...] Instead {{of using a}} parallel quicksort, as van Oostrum and Veltkamp did, they use boxsort, a variant of quicksort developed by [...] in which the partitioning step partitions the input randomly into [...] smaller subproblems instead of only two subproblems. As in Cole's technique, they use a desynchronized parametric search, in which each separate thread of execution of the simulated parallel sorting algorithm is allowed to progress until it needs to determine the result of another comparison, and in which the number of unresolved comparisons is halved by finding the median comparison value and calling the <b>decision</b> <b>algorithm</b> with that value. As they show, the resulting randomized parametric search algorithm makes only a logarithmic number of calls to the <b>decision</b> <b>algorithm</b> with high probability, matching Cole's theoretical analysis. An extended version of their paper includes experimental results from an implementation of the algorithm, which show that the total running time of this method on several natural geometric optimization problems {{is similar to that}} of the best synchronized parametric search implementation (the quicksort-based method of van Oostrum and Veltkamp): a little faster on some problems and a little slower on some others. However, the number of calls that it makes to the <b>decision</b> <b>algorithm</b> is significantly smaller, so this method would obtain greater benefits in applications of parametric searching where the <b>decision</b> <b>algorithm</b> is slower.|$|E
50|$|The bisection method (binary search) {{can also}} be used to {{transform}} decision into optimization. In this method, one maintains an interval of numbers, known to contain the optimal solution value, and repeatedly shrinks the interval by calling the <b>decision</b> <b>algorithm</b> on its median value and keeping only the half-interval above or below the median, depending on the outcome of the call. Although this method can only find a numerical approximation to the optimal solution value, it does so in a number of calls to the <b>decision</b> <b>algorithm</b> proportional to the number of bits of precision of accuracy for this approximation, resulting in weakly polynomial algorithms.|$|E
5000|$|Solve the (explicit) LP-type problem {{defined by}} [...] using Clarkson's algorithm, which {{performs}} a linear number of violation tests and a polylogarithmic number of basis evaluations. The basis evaluations for [...] may {{be performed by}} recursive calls to Chan's algorithm, and the violation tests may be performed by calls to the <b>decision</b> <b>algorithm.</b>|$|E
40|$|AbstractThis paper {{considers}} the computer vision problem of testing whether two equal cardinality points sets A and B {{in the plane}} are ε-congruent. We say that A and B are ε-congruent if there exists an isometry I and bijection l:A → B such that dist(I(a), l(a)) ⩽ ε, for all a ϵ A. Since known methods for this problem are expensive, we develop approximate <b>decision</b> <b>algorithms</b> that are considerably faster than the known <b>decision</b> <b>algorithms,</b> and have bounds on their imprecision. Our approach reduces the problem to that of computing maximum flows {{on a series of}} graphs with integral capacities...|$|R
40|$|Abstract: The {{application}} of flow networks on {{the example of}} the types of works distribution as well as the distribution of construction workers qualification structure according to particular types of buildings is presented in the paper. The flow networks considered were proposed by Zdzislaw Pawlak and are the tools for the new mathematical models related to the information regarding flow explorations in <b>decision</b> <b>algorithms.</b> The <b>decision</b> <b>algorithms</b> consist of series of decision rules for each particular case of various types of construction objects interdependence, as well as of various kinds of workers and their qualification structure. It is all generated in the paper...|$|R
40|$|This paper {{considers}} the computer vision problem of testing whether two equal cardinality point sets A and B {{in the plane}} are ε-congruent. We say that A and B are ε-congruent if there exists an isometry I and bijection ℓ : A → B such that dist(ℓ(a),I(a)) ≤ε, for all a ∈ A. Since known methods for this problem are expensive, we develop approximate <b>decision</b> <b>algorithms</b> that are considerably faster than the known <b>decision</b> <b>algorithms,</b> and have bounds on their imprecision. Our approach reduces the problem to that of computing maximum flows {{on a series of}} graphs with integral capacities...|$|R
5000|$|The {{greatest}} tolerable {{uncertainty is}} found at which decision [...] satisfices the performance {{at a critical}} survival-level. One may establish one's preferences among the available actions [...] according to their robustnesses , whereby larger robustness engenders higher preference. In this way the robustness function underlies a satisficing <b>decision</b> <b>algorithm</b> which maximizes the immunity to pernicious uncertainty.|$|E
5000|$|For instance, first-order {{logic is}} undecidable, meaning a sound, {{complete}} and terminating <b>decision</b> <b>algorithm</b> is impossible. This {{has led to}} the study of interesting decidable fragments such as C2, first-order logic with two variables and the counting quantifiers [...] and [...] (these quantifiers are, respectively, [...] "there exists at least n" [...] and [...] "there exists at most n").|$|E
5000|$|In {{the design}} and {{analysis}} of algorithms for combinatorial optimization, parametric search is a technique invented by [...] for transforming a <b>decision</b> <b>algorithm</b> (does this optimization problem have a solution with quality better than some given threshold?) into an optimization algorithm (find the best solution). It is frequently used for solving optimization problems in computational geometry.|$|E
40|$|The {{application}} of flow networks on {{the example of}} the types of works distribution as well as the distribution of construction workers qualification structure according to particular types of buildings is presented in the paper. The flow networks considered were proposed by Zdzislaw Pawlak and are the tools for the new mathematical models related to the information regarding flow explorations in <b>decision</b> <b>algorithms.</b> The <b>decision</b> <b>algorithms</b> consist of series of decision rules for each particular case of various types of construction objects interdependence, as well as of various kinds of workers and their qualification structure. It is all generated in the paper. ...|$|R
30|$|Then, we {{evaluate}} {{the performance of}} our scheme when the four different game <b>decision</b> <b>algorithms</b> (i.e., the improved-a, the improved-b, the better response algorithm, and the best response algorithm) are employed in TOM-PAOD to adjust transmission power respectively.|$|R
40|$|The {{development}} of <b>decision</b> tree <b>algorithms</b> {{have been used}} for industrial, commercial and scientific purpose. However, the choice of the most suitable algorithm becomes increasingly difficult. In this paper, we present the comparison of <b>decision</b> tree <b>algorithms</b> using Waikato environment for knowledge analysis. The aim is to investigate the performance of data classification for a set of a large data. The algorithms tested are functional tree algorithm, logistic model trees algorithm, REP tree <b>algorithm</b> and best-first <b>decision</b> tree <b>algorithm.</b> The UCI repository will be used to test and justify the performance of <b>decision</b> tree <b>algorithms.</b> Subsequently, the classification algorithm that has the optimal potential will be suggested for use in large scale data...|$|R
5000|$|With the {{assumption}} that the <b>decision</b> <b>algorithm</b> takes an amount of time [...] that grows at least polynomially {{as a function of the}} input size , Chan shows that the threshold for switching to an explicit LP formulation and the number of subsets in the partition can be chosen in such a way that the implicit LP-type optimization algorithm also runs in time [...]|$|E
5000|$|... {{describes}} an algorithm for solving implicitly defined LP-type {{problems such as}} this one in which each LP-type element is determined by a -tuple of input values, for some constant [...] In order to apply his approach, there must exist a <b>decision</b> <b>algorithm</b> that can determine, for a given LP-type basis [...] and set [...] of [...] input values, whether [...] is a basis for the LP-type problem determined by [...]|$|E
50|$|Matiyasevich's {{completion}} of the MRDP theorem settled Hilbert's tenth problem. Hilbert's tenth problem {{was to find a}} general algorithm which can decide whether a given Diophantine equation has a solution among the integers. While Hilbert's tenth problem is not a formal mathematical statement as such, the nearly universal acceptance of the (philosophical) identification of a <b>decision</b> <b>algorithm</b> with a total computable predicate allows us to use the MRDP theorem to conclude that the tenth problem is unsolvable.|$|E
40|$|ABSTRACT. In {{this paper}} are {{presented}} the obtained results {{by the authors}} behind analysis of the multiple attribute decision methods. Especially, there are references about the equivalence of the obtained results with diferent <b>decision</b> <b>algorithms</b> proposed in speciality literature. Tackled equivalencies or non-equivalencies are mathematically substantiated through demonstration or counter-examples. Finally, it is analysed a decision model based on the fuzzy concept. For a practical decision problem, the model's flexibility permits of obtain the same results like those obtained using <b>decision</b> crisp <b>algorithms...</b>|$|R
40|$|We present {{probabilistic}} Ianov's schemes, studying their semantics and {{proving the}} equivalence between operational and denotational ones. We also study the equivalence of schemes relative to them; as usual all these equivalence problems are decidable, and we prove it giving the appropriate <b>decision</b> <b>algorithms...</b>|$|R
50|$|In mathematics, an evasive Boolean {{function}} &fnof; (of n variables) is a Boolean {{function for}} which every <b>decision</b> tree <b>algorithm</b> has running time of exactly n. Consequently, every <b>decision</b> tree <b>algorithm</b> {{that represents the}} function has, at worst case, a running time of n.|$|R
