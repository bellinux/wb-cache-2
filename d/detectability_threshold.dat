40|23|Public
40|$|The {{assumption}} that the values of model parameters are known or correctly learned, i. e., the Nishimori condition, {{is one of the}} requirements for the detectability analysis of the stochastic blockmodel in statistical inference. In practice, however, there is no example demonstrating that we can know the model parameters beforehand, and {{there is no guarantee that}} the model parameters can be learned accurately. In this study, we consider the expectation-maximization (EM) algorithm with belief propagation (BP) and derive its algorithmic <b>detectability</b> <b>threshold.</b> Our analysis is not restricted to the community structure, but includes general modular structures. Because the algorithm cannot always learn the planted model parameters correctly, the algorithmic <b>detectability</b> <b>threshold</b> is qualitatively different from the one with the Nishimori condition. Comment: 15 pages, 8 figure...|$|E
40|$|Investigating the {{performance}} of different methods is a fundamental problem in graph partitioning. In this paper, we estimate the so-called <b>detectability</b> <b>threshold</b> for the spectral method with both unnormalized and normalized Laplacians in sparse graphs. The <b>detectability</b> <b>threshold</b> is the critical {{point at which the}} result of the spectral method is completely uncorrelated to the planted partition. We also analyze whether the localization of eigenvectors affects the partitioning performance in the detectable region. We use the replica method, which is often used in the field of spin-glass theory, and focus on the case of bisection. We show that the gap between the estimated threshold for the spectral method and the threshold obtained from Bayesian inference is considerable in sparse graphs, even without eigenvector localization. This gap closes in a dense limit. Comment: 26 pages, 13 figure...|$|E
40|$|We {{investigate}} the detectability thresholds of various modular {{structures in the}} stochastic block model. Our analysis reveals how the <b>detectability</b> <b>threshold</b> {{is related to the}} details of the modular pattern, including the hierarchy of the clusters. We show that certain planted structures are impossible to infer regardless of their fuzziness. Comment: 5 pages, 3 figure...|$|E
40|$|Abstract: The paper {{addresses}} on-chip {{test for}} IC RF transceivers. The baseband DSP available on chip {{serves as a}} tester while the RF front-end is reconfigured for test. The basic test setup is a loopback, enabled by a test attenuator {{and in some cases}} by an offset mixer, too. Different variants of this setup adopt the bypassing technique to boost testability. The existing limitations and tradeoffs in terms of test feasibility, controllability and observability versus the chip performance are discussed. The fault-oriented approach and the sensitization techniques are emphasized vs the functional test. The impact of production tolerances is addressed in terms of the <b>detectability</b> <b>thresholds.</b> Key-words: RF test, DfT, BiST, on-chip test, RF transceiver...|$|R
40|$|Iron oxide {{particles}} are especially suited for cell tracking experiments {{due to their}} extraordinarily molar relaxivity as compared with other paramagnetic nuclei. We have compared different iron oxide particles (Sinerem, Endorem and magnetic microspheres) for their suitability to label embryonic stem cells (D 3 cell line). In addition to <b>detectability</b> <b>thresholds,</b> particular {{attention has been paid}} to the evaluation of long-term stability of the labelling procedure (up to 4 weeks) as well as to toxic and other adverse effects on cell viability. Comparative studies were performed using neural progenitor cells (C 17. 2) and dendritic cells. The present study indicates strong dependence of the label efficiency and stability on the iron oxide particles and cell lines in use. Copyright (c) 2008 John Wiley & Sons, Ltd. status: publishe...|$|R
40|$|Analyses of {{simulated}} {{and operational}} ERTS images have provided initial estimates of resolution, ground resolution, <b>detectability</b> <b>thresholds</b> {{and other measures}} of image quality of interest to earth scientists and cartographers. Based on these values, including an approximate ground resolution of 250 meters for both RBV and MSS systems, the ERTS- 1 images appear suited to the production and/or revision of planimetric and photo maps of 1 : 500, 000 scale and smaller for which map accuracy standards are compatible with the imaged detail. Thematic mapping, although less constrained by map accuracy standards, will be influenced by measurement thresholds and errors which {{have yet to be}} accurately determined for ERTS images. This study also indicates the desirability of establishing a quantitative relationship between image quality values and map products which will permit both engineers and cartographers/earth scientists to contribute to the design requirements of future satellite imaging systems...|$|R
30|$|The {{limitations}} of our algorithm for correctly recovering {{some of the}} ASes might also {{be related to the}} <b>detectability</b> <b>threshold</b> of community detection [45 – 47] and to the community detection paradox [48]: even when there are more average internal connections than external ones, algorithms might not succeed to retrieve a community if the difference between these quantities is not above a certain threshold. This limitation not only affects modularity-based methods but is quite general.|$|E
40|$|Recent {{research}} has shown that virtually all algorithms aimed at the identification of communities in networks are affected by the same main limitation: the impossibility to detect communities, even when these are well-defined, if the average value of the difference between internal and external node degrees does not exceed a strictly positive value, in literature known as <b>detectability</b> <b>threshold.</b> Here, we counterintuitively show that the value of this threshold is inversely proportional to the intrinsic quality of communities: the detection of well-defined modules is thus more difficult than the identification of ill-defined communities. Comment: 5 pages, 3 figure...|$|E
40|$|In principle, higher-order {{networks}} that have multiple edge types are more informative than their lower-order counterparts. In practice, however, excessively rich {{information may be}} algorithmically infeasible to extract. It requires an algorithm that assumes a high-dimensional model and such an algorithm may perform poorly or be extremely sensitive to the initial estimate of the model parameters. Herein, we address this problem of community detection through a detectability analysis. We focus on the expectation-maximization (EM) algorithm with belief propagation (BP), and analytically derive its algorithmic <b>detectability</b> <b>threshold,</b> i. e., the limit of the modular structure strength below which the algorithm can no longer detect any modular structures. The results indicate {{the existence of a}} phase in which the community detection of a lower-order network outperforms its higher-order counterpart. Comment: 5 pages, 3 figure...|$|E
40|$|Graph {{partitioning}} problems {{emerge in}} a wide variety of complex systems, ranging from biology to finance, but can be rigorously analyzed and solved only for a few graph ensembles. Here, an ensemble of equitable graphs, i. e. random graphs with a block-regular structure, is studied, for which analytical results can be obtained. In particular, the spectral density of this ensemble is computed exactly for a modular and bipartite structure. Kesten-McKay's law for random regular graphs is found analytically to apply also for modular and bipartite structures when blocks are homogeneous. Exact solution to graph partitioning for two equal-sized communities is proposed and verified numerically, and a conjecture on the absence of an efficient recovery detectability transition in equitable graphs is suggested. Final discussion summarizes results and outlines their relevance for the solution of graph partitioning problems in other graph ensembles, in particular for the study of <b>detectability</b> <b>thresholds</b> and resolution limits in stochastic block models. Comment: 8 pages, 8 figure...|$|R
40|$|Lip {{synchronization}} {{is considered}} a key parameter during interactive communication. In the case of video conferencing and television broadcasting, the differential delay between audio and video should remain below certain thresholds, as recommended by several standardization bodies. However, further research has also shown that these thresholds can be relaxed, depending on the targeted application and use case. In this article, we investigate the influence of lip sync {{on the ability to}} perform real-time language interpretation during video conferencing. Furthermore, we are also interested in determining proper lip sync visibility thresholds applicable to this use case. Therefore, we conducted a subjective experiment using expert interpreters, which were required to perform a simultaneous translation, and non-experts. Our results show that significant differences are obtained when conducting subjective experiments with expert interpreters. As interpreters are primarily focused on performing the simultaneous translation, lip sync <b>detectability</b> <b>thresholds</b> are higher compared with existing recommended thresholds. As such, primary focus and the targeted application and use case are important factors to be considered when selecting proper lip sync acceptability thresholds...|$|R
40|$|International audienceGene electrotransfer {{is gaining}} {{momentum}} as an efficient methodology for nonviral gene transfer. In skeletal muscle, {{data suggest that}} electric pulses play two roles: structurally permeabilizing the muscle fibers and electrophoretically supporting the migration of DNA toward or across the permeabilized membrane. To investigate this further, combinations of permeabilizing short high-voltage pulses (HV; hundreds of V/cm) and mainly electrophoretic long low-voltage pulses (LV; tens of V/cm) were investigated in muscle, liver, tumor, and skin in rodent models. The following observations were made: (1) Striking differences between the various tissues were found, likely related to cell size and tissue organization; (2) gene expression is increased, {{if there was a}} time interval between the HV pulse and the LV pulse; (3) the HV pulse was required for high electrotransfer to muscle, tumor, and skin, but not to liver; and (4) efficient gene electrotransfer was achieved with HV field strengths below the <b>detectability</b> <b>thresholds</b> for permeabilization; and (5) the lag time interval between the HV and LV pulses decreased sensitivity to the HV pulses, enabling a wider HV amplitude range. In conclusion, HV plus LV pulses represent an efficient and safe option for future clinical trials and we suggest recommendations for gene transfer to various types of tissues...|$|R
3000|$|Cell imaging is very {{important}} for cell-based therapies and has attracted increasing attention in recent years. Due to its high spatial resolution in three dimensions and good soft-tissue contrast, MR imaging is highly desirable for this purpose and has been demonstrated to be a robust tool for imaging and tracking the migration of stem cells in various diseases [1, 2]. For MR cell imaging, cells need to be labeled with magnetic contrast agent to distinguish them from the surrounding tissues by MRI. Currently, the frequently used MR contrast agents are gadolinium-based [...] "positive" [...] contrast agents and superparamagnetic iron oxide nanoparticle (SPION)-based [...] "negative" [...] contrast agents [3, 4]. Since gadolinium-based MR contrast agents have a high <b>detectability</b> <b>threshold,</b> the use of SPIONs now provides a more promising alternative to label and detect the target cells [2].|$|E
40|$|A {{class of}} robust estimators of scatter applied to {{information}}-plus-impulsive noise samples is studied, where the sample information matrix is assumed of low rank; this generalizes the study (Couillet et al., 2013 b) to spiked random matrix models. It is precisely shown that, {{as opposed to}} sample covariance ma-trices which may have asymptotically unbounded (eigen-) spectrum due to the sample impulsiveness, the robust estimator of scatter has bounded spectrum and may contain isolated eigenvalues which we fully characterize. We show that, if found beyond a certain <b>detectability</b> <b>threshold,</b> these eigenvalues allow one to perform statistical inference on the eigenvalues and eigenvectors of the information matrix. We use this result to derive new eigenvalue and eigenvector estimation procedures, which we apply in practice to the popular array pro-cessing problem of angle of arrival estimation. This gives birth to an improved algorithm based on the MUSIC method, which we refer to as robust G-MUSIC...|$|E
40|$|PACS 64. 60. aq – Networks Abstract – Recent {{research}} has shown that virtually all algorithms aimed at the identification of communities in networks are affected by the same main limitation: the impossibility to detect communities, even when these are well defined, if the average value of the difference between internal and external node degrees does not exceed a strictly positive value, in the literature known as <b>detectability</b> <b>threshold.</b> Here, we counterintuitively show that the value of this threshold is inversely proportional to the intrinsic quality of communities: the detection of well-defined modules is thus more difficult than the identification of ill-defined communities. editor’s choice Copyright c © EPLA, 2014 Real networks are often organized in local modules or clusters called communities [1, 2]. In intuitive terms, a community is a subgroup of nodes with a density of inter-nal connections larger than the density of external links. The identification of communities is a crucial step for the understanding of structural and dynamical properties o...|$|E
40|$|This study {{aimed to}} answer the {{question}} of how to design a visual warning signal that is most easily seen and produces the quickest reaction time. This is a classic problem of bionic optimization—if one knows the properties of the receiver one can most easily find a suitable solution. Because the peak of the spatio-temporal contrast sensitivity function of the human visual system occurs at non-zero spatial and temporal frequencies, it is likely that movement enhances the <b>detectability</b> of <b>threshold</b> visual signals. Earlier studies employing extended drifting sinewave gratings bear out this prediction. We have studied the ability of human observers to detect threshold visual signals for both moving and stationary stimuli. We used discrete, localized signals such as might be employed in aerospace or automotive warning signal displays. Moving stimuli show a superior detectability to non-moving stimuli of the same integrated energy. Moving stimuli at <b>threshold</b> <b>detectability</b> are seen faster than non-moving threshold stimuli. Under some conditions the speed advantage is over 0. 25 seconds. Similar advantages have also been shown to occur for suprathreshold signals...|$|R
40|$|International audienceIn mammography, {{image quality}} {{assessment}} {{has to be}} directly related to breast cancer indicator (e. g. microcalcifications) detectability. Recently, we proposed an X-ray source/digital detector (XRS/DD) model leading to such an assessment. This model simulates very realistic contrast-detail phantom (CDMAM) images leading to gold disc (representing microcalcifications) <b>detectability</b> <b>thresholds</b> that {{are very close to}} those of real images taken under the simulated acquisition conditions. The detection step was performed with a mathematical observer. The aim of this contribution is to include human observers into the disc detection process in real and virtual images to validate the simulation framework based on the XRS/DD model. Mathematical criteria (contrast-detail curves, image quality factor, etc.) are used to assess and to compare, from the statistical point of view, the cancer indicator detectability in real and virtual images. The quantitative results given in this paper show that the images simulated by the XRS/DD model are useful for image quality assessment in the case of all studied exposure conditions using either human or automated scoring. Also, this paper confirms that with the XRS/DD model the image quality assessment can be automated and the whole time of the procedure can be drastically reduced. Compared to standard quality assessment methods, the number of images to be acquired is divided by a factor of eight...|$|R
40|$|The {{design and}} later use of modern spectropolarimeters and magnetographs require {{a number of}} {{tolerance}} specifications that allow the developers to build the instrument and then the scientists to interpret the data accuracy. Such specifications depend both on device-specific features and on the physical assumptions underlying the particular measurement technique. Here we discuss general properties of every magnetograph, as the <b>detectability</b> <b>thresholds</b> for the vector magnetic field and the line-of-sight velocity, as well as specific properties of a given type of instrument, namely that based {{on a pair of}} nematic liquid crystal variable retarders and a Fabry-Pérot etalon (or several) for carrying out the light polarization modulation and spectral analysis, respectively. We derive formulae that give the detection thresholds in terms of the signal-to-noise ratio of the observations and the polarimetric efficiencies of the instrument. Relationships are also established between inaccuracies in the solar physical quantities and instabilities in the instrument parameters. Such relationships allow, for example, to translate scientific requirements for the velocity or the magnetic field into requirements for temperature or voltage stability. We also demonstrate that this type of magnetograph can theoretically reach the optimum polarimetric efficiencies of an ideal polarimeter, regardless of the optics in between the modulator and the analyzer. Such optics induces changes in the instrument parameters that are calculated too. Comment: Accepted for publication in The Astrophysical Journa...|$|R
40|$|We {{study the}} {{fundamental}} limits on learning latent community structure in dynamic networks. Specifically, we study dynamic stochastic block models where nodes change their community membership over time, but where edges are generated independently at each time step. In this setting (which {{is a special}} case of several existing models), {{we are able to}} derive the <b>detectability</b> <b>threshold</b> exactly, {{as a function of the}} rate of change and the strength of the communities. Below this threshold, we claim that no algorithm can identify the communities better than chance. We then give two algorithms that are optimal in the sense that they succeed all the way down to this limit. The first uses belief propagation (BP), which gives asymptotically optimal accuracy, and the second is a fast spectral clustering algorithm, based on linearizing the BP equations. We verify our analytic and algorithmic results via numerical simulation, and close with a brief discussion of extensions and open questions. Comment: 9 pages, 3 figure...|$|E
40|$|We {{study the}} {{inference}} {{of a model}} of dynamic networks in which both communities and links keep memory of previous network states. By considering maximum likelihood inference from single snapshot observations of the network, we show that link persistence makes the inference of communities harder, decreasing the <b>detectability</b> <b>threshold,</b> while community persistence tends to make it easier. We analytically show that communities inferred from single network snapshot can share a maximum overlap with the underlying communities of a specific previous instant in time. This leads to time-lagged inference: the identification of past communities rather than present ones. Finally we compute the time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic (LSD) algorithm, for community detection in dynamic networks. We analytically and numerically characterize the detectability transitions of such algorithm {{as a function of}} the memory parameters of the model. Comment: 11 pages, 7 figures; LSD algorithm Section added; title changed; figures update...|$|E
40|$|The {{effects of}} varying {{the rate of}} {{delivery}} of dichotic tone pip stimuli on selective attention measured by evoked-potential amplitudes and signal detectability scores were studied. The subjects attended to one channel (ear) of tones, ignored the other, and pressed a button whenever occasional targets - tones of a slightly higher pitch were detected in the attended ear. Under separate conditions, randomized interstimulus intervals were short, medium, and long. Another study compared the effects of attention on the N 1 component of the auditory evoked potential for tone pips presented alone and when white noise was added to make the tones barely above <b>detectability</b> <b>threshold</b> in a three-channel listening task. Major conclusions are that (1) N 1 is enlarged to stimuli in an attended channel only in the short interstimulus interval condition (averaging 350 msec), (2) N 1 and P 3 are related to different modes of selective attention, and (3) attention selectivity in multichannel listening task is greater when tones are faint and/or difficult to detect...|$|E
40|$|The {{essentials}} of the on-chip loopback test for integrated RF transceivers are presented. The available on-chip baseband processor {{serves as a}} tester while the RF front-end is under test enabled by on-chip test attenuator {{and in some cases}} by an offset mixer, too. Various system-level tests, like BER, EVM or spectral measurements are discussed. By using this technique in mass production, the RF test equipment can be largely avoided and the test cost reduced. Different variants of the loopback setup including the bypassing technique and RF detectors to boost the chip testability are considered. The existing limitations and tradeoffs are discussed in terms of test feasibility, controllability, and observability versus the chip performance. The fault-oriented approach supported by sensitization technique is put in contrast to the functional test. Also the impact of production tolerances is addressed in terms of a simple statistical model and the <b>detectability</b> <b>thresholds.</b> The paper is based on the present and previous work of the authors, largely revised and upgraded to provide a comprehensive description of the on-chip loopback test. Simulation examples of practical communication transceivers such as WLAN and EDGE under test are also included. © 2009 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. Jerzy Dabrowski and Rashad Ramzan, Built-in Loopback Test for IC RF Transceivers, 2010, IEEE Transactions on Very Large Scale Integration (vlsi) Systems, (18), 6, 933 - 946. [URL]...|$|R
40|$|We {{investigate}} the merger histories of isolated dwarf galaxies {{based on a}} suite of 15 high-resolution cosmological zoom-in simulations, all with masses of M_ halo≈ 10 ^ 10 M_ (and M_∼ 10 ^ 5 - 10 ^ 7 M_) at z= 0, from the Feedback in Realistic Environments (FIRE) project. The stellar populations of these dwarf galaxies at z= 0 are formed essentially entirely "in situ": over 90 % of the stellar mass is formed in the main progenitor in all but two cases, and all 15 of the galaxies have > 70 % of their stellar mass formed in situ. Virtually all galaxy mergers occur prior to z∼ 3, meaning that accreted stellar populations are ancient. On average, our simulated dwarfs undergo 5 galaxy mergers in their lifetimes, with typical pre-merger galaxy mass ratios that are less than 1 : 10. This merger frequency is generally comparable {{to what has been}} found in dissipationless simulations when coupled with abundance matching. Two of the simulated dwarfs have a luminous satellite companion at z= 0. These ultra-faint dwarfs lie at or below current <b>detectability</b> <b>thresholds</b> but are intriguing targets for next-generation facilities. The small contribution of accreted stars make it extremely difficult to discern the effects of mergers {{in the vast majority of}} dwarfs either photometrically or using resolved-star color-magnitude diagrams (CMDs). The important implication for near-field cosmology is that star formation histories of comparably massive galaxies derived from resolved CMDs should trace the build-up of stellar mass in one main system across cosmic time as opposed to reflecting the contributions of many individual star formation histories of merged dwarfs. Comment: 14 pages, 12 figures; submitted to MNRAS on 12 / 28 / 1...|$|R
40|$|AbstractWe {{sought to}} {{quantify}} {{the information in the}} activity of posterior parietal neurons in behaving Rhesus monkeys. We found several models that were adequate to represent the neurons' response fields. We used a gaussian model to construct a signal/noise ratio, which provided an estimate of the number of distinguishable levels (NDL) of activity within the response field. For the typical neuron, an unbiased ideal observer could reliably discriminate 3. 4 levels of activity. At chance levels of <b>detectability,</b> the <b>threshold</b> limit of reliable discrimination, there was an average of 5. 8 NDL. We then used the NDL to divide the response field into regions of spatial ambiguity. For an individual neuron, we suggest that firing rate {{is a measure of the}} probability that the target is at the center of the neuron's response field. Copyright © 1996 Elsevier Science Ltd...|$|R
40|$|We propose novel semi-supervised {{and active}} {{learning}} algorithms {{for the problem}} of community detection on networks. The algorithms are based on optimizing the likelihood function of the community assignments given a graph and {{an estimate of the}} statistical model that generated it. The optimization framework is inspired by prior work on the unsupervised community detection problem in Stochastic Block Models (SBM) using Semi-Definite Programming (SDP). In this paper we provide the next steps in the evolution of learning communities in this context which involves a constrained semi-definite programming algorithm, and a newly presented active learning algorithm. The active learner intelligently queries nodes that are expected to maximize the change in the model likelihood. Experimental results show that this active learning algorithm outperforms the random-selection semi-supervised version of the same algorithm as well as other state-of-the-art active learning algorithms. Our algorithms significantly improved performance is demonstrated on both real-world and SBM-generated networks even when the SBM has a signal to noise ratio (SNR) below the known unsupervised <b>detectability</b> <b>threshold...</b>|$|E
40|$|Neutron stars may harbor {{the true}} ground {{state of matter}} {{in the form of}} strange quark matter. If present, this type of matter is {{expected}} to be a color superconductor, a consequence of quark pairing with respect to the color and flavor degrees of freedom. The stellar magnetic field threading the quark core becomes a color-magnetic admixture and, in the event that superconductivity is of type II, leads to the formation of color-magnetic vortices. In this Letter, we show that the volume-averaged color-magnetic vortex tension force should naturally lead to a significant degree of nonaxisymmetry in systems such as radio pulsars. We show that gravitational radiation from such color-magnetic “mountains” in young pulsars, such as the Crab and Vela, could be observable by the future Einstein Telescope, thus, becoming a probe of paired quark matter in neutron stars. The <b>detectability</b> <b>threshold</b> can be pushed up toward the sensitivity level of Advanced LIGO if we invoke an interior magnetic field about a factor ten stronger than the surface polar field...|$|E
40|$|NICMOS {{observations}} of the complex gravitational lens system B 1933 + 503 reveal infrared counterparts {{to two of the}} inverted spectrum radio images. The infrared images have arc-like structures. The corresponding radio images are also detected in a VLBA map made at 1. 7 GHz with a resolution of 6 mas. We fail to detect two of the four inverted radio spectrum components with the VLBA even though they are clearly visible in a MERLIN map at the same frequency at a different epoch. The absence of these two components could be due to rapid variability on a time-scale less than the time delay, or to broadening of the images during propagation of the radio waves through the ISM of the lensing galaxy to an extent that they fall below the surface brightness <b>detectability</b> <b>threshold</b> of the VLBA observations. The failure to detect the same two images with NICMOS is probably due to extinction in the ISM of the lensing galaxy. Comment: 5 pages, 4 figures, submitted to MNRA...|$|E
40|$|A three-layer (Earth–air–ionosphere) {{physical}} model, {{as well as}} a two-layer (Earth–air) model, {{is employed}} in this paper to investigate the ionospheric effect on the wave fields for a finite length dipole current source co-located at a hypocenter depth and along the main fault of an earthquake when the distance between the epicenter and an observing station is up to 1000  km or even more. The results show that all electrical fields are free of ionospheric effects for different frequencies in a relative short range, e. g., [*]∼[*]  300  km for f [*]=[*]  1  Hz, implying the ionospheric influence on electromagnetic fields can be neglected within this range, which becomes smaller as the frequency increases. However, the ionosphere can give a constructive interference to the waves passing through and make them decay slowly when an observation is out of this range; moreover, the ionospheric effect can be up to 1 – 2 orders of magnitude of the electrical fields. For a ground-based observable 1. 3  mV m − 1 electric signal at f [*]=[*]  1  Hz 1440  km away from the Wenchuan M S   8. 0 earthquake, the expected seismo-telluric current magnitude for the Earth–air–ionosphere model is of 5. 0  [*]×[*]  10 7 A, 1 magnitude smaller than the current value of 3. 7  [*]×[*]  10 8 A obtained by the Earth–air model free of ionospheric effects. This indicates that the ionosphere facilitates the electromagnetic wave propagation, as if the detectability of the system were improved effectively and it is easier to record a signal even for stations located at distances beyond their <b>detectability</b> <b>thresholds.</b> Furthermore, the radiating patterns of the electrical field components | E x | and | E y | are complementary to each other, although any two-dimensional (2 -D) power distribution of these components shows strong power areas as well as weak ones, which is advantageous to register a signal if the observing system is designed to measure both of them instead of only one...|$|R
40|$|This paper {{proposes a}} novel {{distributed}} fault detection and isolation approach for the monitoring of non linear large-scale systems. The proposed architecture considers stochastic {{characterization of the}} measurement noises and modeling uncertainties, computing at each step stochastic timevarying thresholds with guaranteed false alarms probability levels. The convergence properties of the distributed estimation are demonstrated. A novel fault isolation method is proposed basing on a Generalized Observer Scheme, providing guaranteed error probabilities of the fault exclusion task. A consensus approach {{is used for the}} estimation of variables shared among more than one subsystem; a method is proposed to define the time-varying consensus weights in order to minimize at each step the variance of the uncertainty of the fault detection and isolation <b>thresholds.</b> <b>Detectability</b> and isolability conditions are provided...|$|R
40|$|Subjects were {{simultaneously}} given subthreshold {{levels of}} taste and odor stimuli, delivered orally, for both a commonly paired and an uncommonly paired taste–odor combination. Results indicate cross-modal summation of subthreshold concentrations of both taste–odor pairs when the olfactory stimulus is delivered orally. Results of control {{studies suggest that}} the sum-mation was indeed across modalities, and not due to the taste of the odor compound or the smell of the taste compounds. Furthermore, results indicate that regardless of taste–odor pair commonness, taste and smell can combine in a completely additive fashion (i. e., at <b>threshold</b> <b>detectability</b> when both stimuli are presented simultaneously at 50 % threshold level) if the taste–odor pair is presented orally. In several instances, but not all, measured probabilities exceeded those predicted by probability summation, indicating that hyperad-ditive mixing often occurs, but there do seem to be individual differences. Cross-modal summation, regardless of taste–odor pair commonness, has broader implications for the development of foods, beverages and pharma-ceuticals, especially in masking undesirable tastes and smells...|$|R
40|$|Abstract—Quantification of {{ultrasound}} (US) imager performance simulating human observers {{is addressed}} using size-dependent lesion signal-to-noise ratio (LSNR) analysis {{of images of}} spherical simulated lesions in phantoms. LSNR values obtained over {{a broad range of}} image depths can be used with a single <b>detectability</b> <b>threshold</b> to determine the depth range over which lesions of a given size and contrast are deemed to be detectable, yielding a performance metric. Optimal LSNR analysis requires a priori knowledge of lesion locations in the image so that LSNR values relate to the center of each lesion. Phantoms having a regular array of accurately positioned spherical simulated lesions are described, along with easily employed and robust software that accurately determines lesion locations in images, even when only a few are detectable by visual inspection. The software accounts for image spatial calibration inaccuracies and accommodates sector, curvilinear, etc., formats. The minimum number of equivalent lesions to yield an acceptable mean (counteracting speckle variations) also is addressed. (E-mail: jkofler@mayo. edu) © 2002 World Federation for Ultrasound in Medi-cine & Biology...|$|E
40|$|Community {{detection}} algorithms {{are fundamental}} tools to understand organizational principles in social networks. With the increasing power of social media platforms, when detecting communities {{there are two}} possi- ble sources of information one can use: the structure of social network and node attributes. However structure of social networks and node attributes are often interpreted separately in the research of community detection. When these two sources are interpreted simultaneously, one common as- sumption shared by previous studies is that nodes attributes are correlated with communities. In this paper, we present a model {{that is capable of}} combining topology information and nodes attributes information with- out assuming correlation. This new model can recover communities with higher accuracy even when node attributes and communities are uncorre- lated. We derive the <b>detectability</b> <b>threshold</b> for this model and use Belief Propagation (BP) to make inference. This algorithm is optimal {{in the sense that it}} can recover community all the way down to the threshold. This new model is also with the potential to handle edge content and dynamic settings...|$|E
40|$|Abstract. | The {{results of}} the {{photometric}} survey of 16 solar-type, active, eld stars are presented. During our observations 9 stars showed appreciable light variability with amplitudes of a few hundredths of a magnitude but for three of them periods could not be determined. Most of the observed variable stars have periods shorter than about 10 days. It is suggested that, similarly as is observed in the Hyades cluster, small amplitude light variations are quite common among active eld solar-type dwarfs with rotation periods around one week or less. A strong modulation of amplitude of some variable stars over the time scale of years is demonstrated. The amplitude may sometimes decrease even below the <b>detectability</b> <b>threshold.</b> A special case is HD 17576 { a visual binary consisting of a G 0 dwarf and a much fainter hot subdwarf. It has the largest amplitude of all the stars observed, which suggests an intense spot activity, a very strong H and K line core emission and a very high X-ray emission flux, close to the saturation limit. Yet its variability period is equal to 18. 74 days { almost {{an order of magnitude}} longer than expected for such an active dwarf...|$|E
40|$|The {{classical}} {{theory of}} detection using the Neyman-Pearson principle {{is applied to}} stratosphere-troposphere (ST) radar signals. It is extended to provide information regarding the detection of weak signals which complements the detectability method usually employed in ST radar studies. It is shown that for ST radar signals of low amplitude and a detectability around 3 (a value commonly invoked in literature), the probability of detection is about equal to the probability of false alarm. The question of <b>threshold</b> <b>detectability</b> is also discussed. Spectral moments errors are evaluated by a method which {{is an extension of}} the analytical method of estimation developed by Miller and Rochwarger and the results compared to other statistical and analytical models. As already known, three factors can affect the error on the estimated parameters: the signal-to-noise ratio, the spectral width and the incoherent integration number. For high signal-to-noise ratios, analytical results are in good agreement with Barrick's and Denenberg's theoretical models and with Yamamoto's statistical one. For low signal-to-noise ratios, the spectral parameters are more sensitive to the selected model but overall variability is similar...|$|R
40|$|The aim of {{this study}} was to {{investigate}} if the ability to detect clinically relevant signals, within local area clinically relevant texture, is related to experience. A two alternative forced choice interleaved staircase experiment was conducted on 101 observers split into three groups; group 1 with diagnostic experience, group 2 with experience of imaging but not of making a diagnosis and group 3 with no experience of imaging. Thresholds of detection within synthesized, clinically representative textures were measured for a 15 mm simulated lesion within an MR T 1 weighted brain texture and a 2. 5 mm diameter simulated lesion embedded within X-ray trabecular bone texture. The results showed that there was a significant difference in <b>threshold</b> <b>detectability</b> between the groups for the brain texture at the 95 % significance level but not for the bone texture. The experienced group did not demonstrate a correlation between their bone and brain results. However, the inexperienced group had a significant correlation between the bone and brain results. There was a significant correlation between increasing experience and detectability but this was dependent on the composition of the local area anatomical noise...|$|R
40|$|Assessment {{of image}} quality for digital x-ray {{mammography}} systems used in European screening programs relies mainly on contrast-detail CDMAM phantom scoring and requires the acquisition {{and analysis of}} many images {{in order to reduce}} variability in <b>threshold</b> <b>detectability.</b> Part II of this study proposes an alternative method based on the detectability index (d') calculated for a non-prewhitened model observer with an eye filter (NPWE). The detectability index was calculated from the normalized noise power spectrum and image contrast, both measured from an image of a 5 cm poly(methyl methacrylate) phantom containing a 0. 2 mm thick aluminium square, and the pre-sampling modulation transfer function. This was performed as a function of air kerma at the detector for 11 different digital mammography systems. These calculated d' values were compared against threshold gold thickness (T) results measured with the CDMAM test object and against derived theoretical relationships. A simple relationship was found between T and d', as a function of detector air kerma; a linear relationship was found between d' and contrast-to-noise ratio. The values of threshold thickness used to specify acceptable performance in the European Guidelines for 0. 10 and 0. 25 mm diameter discs were equivalent to <b>threshold</b> calculated <b>detectability</b> indices of 1. 05 and 6. 30, respectively. The NPWE method is a validated alternative to CDMAM scoring for use in the image quality specification, quality control and optimization of digital x-ray systems for screening mammography. status: publishe...|$|R
