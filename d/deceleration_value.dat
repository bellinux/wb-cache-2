9|35|Public
40|$|International audienceDeceleration {{measurement}} is {{both very}} important and {{very difficult for the}} research on the penetration. This paper presents an ultra-high g deceleration measurement experiment. In the experiment penetration occurs while a tungsten alloy projectile with velocity of 573 m/s impacts a 50 -mm-thick steel target. Numerical simulation utilizing ANSYS/LS_DYNA is carried out firstly to estimate the <b>deceleration</b> <b>value</b> and its duration time, and followed by a detailed description to the design of the measurement device. Experiment results show that more than 145, 000 g <b>deceleration</b> <b>value</b> is measured. The results are consistent with the simulation ones. However, compared to the theoretical analysis result of a previously published penetration model, the experiment result shows some disparities...|$|E
30|$|Block section’s length {{at least}} {{equal to the}} maximum braking {{distance}} of the train according to the speed on the line (for safety reasons, {{and according to the}} best practices in railways); in particular the previous figure reports the curves of capacity assuming a block section with a length of 1, 2 or 3 times the maximum braking distance as a function of speed and by considering a constant <b>deceleration</b> <b>value</b> (assumed in the range 0.5 – 0.6  m/s 2 in Fig. 3).|$|E
40|$|We {{explore the}} {{observational}} adequacy {{of a class}} of Unified Dark Energy/Matter (UDE/M) models with a fast transition. Our constraints are set {{using a combination of}} geometric probes, some low redshift ones, and some high redshift ones (CMB related included). The transition is phenomenologically modeled by two different transition functions corresponding to a fast and to an ultra-fast transition respectively. We find that the key parameters governing the transition can be well constrained, and from the statistical point of view it follows that the models cannot be discarded when compared to ΛCDM. We find the intriguing result that standard/input parameters such as Ω_m and Ω_b are far better constrained than in ΛCDM, and this is the case for the derived/output parameter measuring the <b>deceleration</b> <b>value</b> at present, q_ 0. Comment: 8 pages, 3 figure...|$|E
3000|$|... {{represent}} the acceleration and deceleration times (acceleration and <b>deceleration</b> <b>values</b> are indicated with a and d) [...]...|$|R
25|$|Traffic units, {{particularly}} officers performing collision investigation duties, use laser plotting {{devices to}} accurately survey collision sights and carry devices {{that can be}} used to measure road friction and <b>deceleration</b> <b>values.</b>|$|R
40|$|Abstract: After a brief {{introduction}} on vehicle {{barriers in}} general, the writers present a rational basis of determining a design force on security barriers. The design force {{is derived from}} rigid barrier impact <b>deceleration</b> <b>values</b> culled from published literature on vehicles with similar kinetic energy. The Appendix provide a summary of references for further study...|$|R
30|$|The {{rolling stock}} {{deceleration}} {{specified in the}} OpenTrack model was then modified {{to examine the effects}} of ATO in accordance with Sect.  4.1, and the new maximum theoretical capacity obtained by the same compression method as before. This new maximum capacity value was then multiplied by the 76 % ratio for capacity utilisation derived above, and rounded down to the nearest integer value to provide an estimate of practical capacity (in tph). The OpenTrack model was also modified to replace the existing lineside signalling with a moving block signalling system. This capacity estimation process was repeated for all of the combinations of manual driving/ATO and lineside/moving block signalling. Low adhesion conditions were also tested for the ATO case, with both lineside and moving block signalling, again by modifying the <b>deceleration</b> <b>value</b> in accordance with Sect.  4.1.|$|E
40|$|The article {{analyzes}} {{the impact of}} the longitudinal road profile on the efficiency of car braking estimated applying <b>deceleration</b> <b>value.</b> Different formulas are used for theoretical calculations, and therefore experimental brakes in different road slopes were performed to obtain the most accurate results. Deceleration, {{as one of the most}} important safety parameters, depends on the technical condition of the braking system, road conditions and structural and dynamic properties of the other car. Road alignments can significantly affect car manageability, because of weight transfer and extra track resistance, which may change the overall balance of the car and affect the nature of dynamic characteristics that may vary from certain critical values. The results of corrections to deceleration dependence on the road profile can be used for investigating traffic accidents, optimizing traffic control arrangements and implementing advanced systems for automotive active safety...|$|E
40|$|As {{the focus}} of traffic safety {{stakeholders}} shifts from passive safety to active safety, the need to predictively evaluate safety systems in addition to identifying driver behaviour in critical situations {{has come to the}} forefront. Availability {{of a wide variety of}} data has opened up new research possibilities; real world crash data is increasingly accessible through Event Data Recorder (EDR) data, although little information is available about the context of the crash. Naturalistic Driving Data (NDD) addresses this issue by monitoring the driver and vehicle with the help of cameras and sensors. However, there is a lack of real world crashes associated with NDD. Therefore, the need to combine data sources to identify driver behaviour and estimate safety benefit has never been higher. In this study Counterfactual or ‘What-if’ simulations are performed with EDR data of real rear-end crashes and driver glance behaviours inspired from NDD to assess the impact of different driver glance behaviour on possible outcomes. The data was extracted from the National Automotive Sampling System-Crashworthiness Data System (NASS-CDS) database for use in the What-if simulations. Artificial kinematics or Counter factual kinematics was created by removing the evasive (braking) manoeuver. A glance anchor point (AP) based on literature was chosen. Two distributions (Baseline glance distribution, Reaction time distribution) and a <b>deceleration</b> <b>value</b> were applied to the kinematics at the AP. The Baseline Glance distribution which represented normal everyday driving was inspired from NDD. The reaction time distribution and <b>deceleration</b> <b>value</b> was created and chosen based on literature respectively. The application of the combination of distributions resulted in counter factual outcomes with two possibilities: Crash or No Crash. The Impact speeds of all the counter factual events that resulted in a Crash were calculated. Another batch of simulations was performed replacing the Baseline Glance distribution with the Rockwell glance distribution. The Rockwell glance distribution represented the glance durations associated with a well-known secondary task of tuning a radio. Results were compared between outcomes from Baseline Glance Distribution and Rockwell Glance distribution with the Original crash data as reference. The results showed that the Baseline Glance distribution had lower percentage of crashes when compared to the Rockwell glance distribution. The impact speeds associated with the Rockwell glance distribution were much higher than the Baseline. However, the impact speeds resulting from both distributions were much lower compared to the impact speeds associated with the original crashes which clearly indicated the high severity of the original crashes. The methodology and results from this study provide the necessary framework to evaluate the benefit of rear end collision avoidance safety systems. Also, a basis for understanding driver behaviour prior to critical situations is provided...|$|E
40|$|A {{numerical}} {{impact study}} is presented on a Formula Student (FS) racing car carbon composite nose cone. The effect of material model and model parameter selection on the numerical deceleration curves {{is discussed in}} light of the experimental deceleration data. The models show reasonable correlation in terms of the shape of the deceleration-displacement curves but do not match the peak <b>deceleration</b> <b>values</b> with errors greater that 30 %...|$|R
40|$|A small {{drop that}} {{splashes}} {{into a deep}} liquid sometimes reappears as a small rising jet, for example when a water drop splashes into a pool or when coffee drips into a cup. Here we describe that the growing and rising jet continuously redistributes its fluid to maintain a universal shape originating from a surface tension based deceleration of the jet; the shape is universal {{in the sense that}} the shape of the rising jet is the same at all times; only the scaling depends on fluid parameters and deceleration. An inviscid equation of motion for the jet is proposed assuming a time dependent but uniform deceleration; the equation of motion is made dimensionless by using a generalized time-dependent capillary length λ_c and is solved numerically. As a solution a concave shape function is found that is fully determined by three measurable physical parameters: deceleration, mass density and surface tension; it is found that the surface tension based deceleration of the jet scales quadratic with the size of the jet base. <b>Deceleration</b> <b>values</b> derived from the jet shape are in good agreement with <b>deceleration</b> <b>values</b> calculated from the time plot of the height of the rising jet...|$|R
40|$|AbstractThe {{results of}} full scale testing on type A nets and {{mattresses}} are reported. Tests were performed {{by using an}} anthropomorphic dummy wearing boots, skis and helmets, developed for the scope. The tests were performed in the field {{by means of a}} tower pendulum of 18 m height. Maximum impact speed was 66 km/hr: head and chest <b>deceleration</b> <b>values</b> measured by means of triaxial accelerometers were evaluated together with high speed video taken {{from the top of the}} impact area. The method can be applied for the validation of safety barriers in skiing and for the evaluation of protective equipments...|$|R
40|$|This {{paper was}} {{accepted}} for publication in the journal Transportation Research Record and the definitive published version is available at [URL] research in vehicle automation has mainly focused on the safety aspect with only limited studies on occupants’ discomfort. In order to facilitate their rapid uptake and penetration, autonomous vehicles (AVs) should ensure that occupants are both safe and comfortable. Recent research however revealed that people felt uncomfortable when AVs braked. This {{may be due to}} their robot-like braking performance. Existing studies on drivers’ braking behaviour investigated data either from controlled experiments or driving simulators. There is a dearth of research on braking behaviour in normal driving. The objective of this paper is therefore to examine drivers’ braking behaviours by exploiting naturalistic driving data from the Pan-European TeleFOT (Field Operational Tests of Aftermarket and Nomadic Devices in Vehicles) project. On a fixed route of 16. 5 km long, 16 drivers were asked to drive an instrumented vehicle. A total of about eleven million observations were analysed to identify the profile, value and duration of deceleration events. Since deceleration events are nested within trips and trips within drivers, multilevel mixed-effects linear models were employed to develop relationships between <b>deceleration</b> <b>value</b> and duration and the factors influencing them. The results indicate that the most used profile of the deceleration behaviour follows a hard braking at the beginning when detecting a danger and then becomes smoother. Furthermore, they suggest that the speed, the reason for braking and the deceleration profile mostly affect the deceleration events. Findings from this study should be considered in examining the braking behaviour of AVs...|$|E
40|$|A {{theoretical}} framework supported by literature reported experimental evidence (Homes, Harshman along with Voyager, Hubble and EGRET space platforms and others) is presented {{which indicates that}} superconductivity is a self energy phenomenon and congruent {{with the concept of}} the Charge Conjugation, Parity Change and Time Reversal (CPT) theorem. A resonant symmetric structure is proposed as an extension of Bardeen Cooper and Schrieffer (BCS) theory, which suspends Lorentz transforms at superluminal velocities {{in the context of the}} de Broglie hypothesis. A momentum and energy conserving (elastic) CPT resonant structural lattice scalable over 15 orders of magnitude from nuclear to universe dimensions and associated superconducting theory is postulated whereby nuclear (quark) weak and strong forces, electromagnetic and gravitational forces are mediated by a particle of resonant velocity transformed mass (mt) (110. 123 x electron mass or 56 Mev/c 2), The universe mass and density are based on an isotropic homogeneous media filling the vacuum of and could be considered a candidate for dark matter/energy. The model predicts a <b>deceleration</b> <b>value</b> consistent with observed Pioneer 10 and 11 deep space translational and rotational deceleration and consistent with the notion that, An object moving through momentum space will slow down. Comment: 44 pages, 21 figures, 18 tables, 215 equations, 80 references Superconductor Resonant Symmetric model is further defined in terms of elastic resonance in sub- and super- luminal regions thereby linking nuclear forces/masses to universe space forces/masses. Further confirming evidence for model predictions are presented in terms of Voyager, Pioneer and Comptel (EGRET) spacecraft observed data. An energy/c 2 correlation is made to Koide lepton particle relationship. A Critical Optical Volume Energy (COVE) scaling concept is introduced. An experimental apparatus design is presented to study this COVE concep...|$|E
40|$|A {{numerical}} and dimensional correlation {{supported by}} reported literature experimental evidence (Homes, Harshmann along with Voyager, EGRET, FERMI, WMAP, Planck and other space platforms) is presented {{which indicates that}} superconductivity is congruent with Charge Conjugation, Parity Change and TimeReversal (CPT) theorem and scalable with Friedmann Lemaire Robertson Walker (FLRW) cosmology within the concept of Schwinger pair production, Maxwell's permittivity-permeability and the de Broglie hypothesis. This momentum and energy conserving (elastic) CPT resonant structural lattice reverses phase and group velocities at the nuclear dimension as dictated by {{the speed of light}} and is scalable over 15 ours of magnitude from nuclear to universe dimensions, from the universe Big Bang event to present and is consistent with nuclear (quark) weak and strong forces, electromagnetic and gravitational forces being mediated by a particle of constant mass (110. 123 x electron mass or 56. 3 MeV/c 2) related to the Higgs mass. This isotropic homogeneous CPT structure filling the space vacuum presently defines an extremely cold dark energy condition (<<CMBR) that dictates an equally extremely cold baryonic dark matter. The model predicts a <b>deceleration</b> <b>value</b> consistent with flat galactic rotation curves and with observed Pioneer 10 and 11 deep space translation and rotational deceleration and is generally consistent with the notion that: An object moving through momentum space will decelerate. This deceleration concept allows Type 1 A supernovae universe expansion data to be interpreted as if the local universe is slowing down with respect to the rest. The entire concept focuses towards engineering a critical optical volume energy (COVE) as a possible energy source. Comment: FERMI satellite diffuse extragalactic gamma radiation, Cosmic Ray Big Bang origin and the Universe matter-antimatter balance is explained. Dark Matter is explained as a hydrogen Bose Einstein Condensate having a Big Bang origin with temperature dictated by Dark Energy from previously hot to presently cold conditions. Standard model particle masses are deduced. (173 pages...|$|E
40|$|In {{microscopic}} traffic simulators, {{the interaction}} between vehicles is considered. The dynamics of the system then becomes an emergent property of {{the interaction between}} its components. Such interactions include lane-changing, car-following behaviours and intersection management. Although, in some cases, such simulators produce realistic prediction, they do not allow for an important aspect of the dynamics, that is, the driver-vehicle interaction. This paper introduces a physically sound vehicle-driver model for realistic microscopic simulation. By building a nanoscopic traffic simulation model that uses steering angle and throttle position as parameters, the model aims to overcome unrealistic acceleration and <b>deceleration</b> <b>values,</b> as found in various microscopic simulation tools. A physics engine calculates the driving force of the vehicle, and the preliminary results presented here, show that, through a realistic driver-vehicle-environment simulator, it becomes possible to model realistic driver and vehicle behaviours in a traffic simulation...|$|R
40|$|This study {{reports on}} an {{in-depth}} {{analysis of the}} properties of Gipps' car-following model. Certain properties of the model are investigated and the need of specific additions to the model is identified. Gipps' car-following model is rather important as it comprises the traffic model of several traffic simulation packages; and the analysis performed in this study determines the actual dynamics that govern model formulae. First, the relationships for the maximum acceleration and <b>deceleration</b> <b>values</b> the model produces are identified. Results indicate that model relationships are such that a vehicle could end up braking harder than its desired deceleration, hence a constraint has to be set. Furthermore, relationships between vehicle acceleration and speed are established. Second, further additions are proposed to allow the model simulate traffic at signal-controlled junctions. These additions include providing a definition for a queueing vehicle and speed manipulation to achieve the calculated saturation flow of simulated junctions...|$|R
40|$|Background: Although {{osteoarthritis}} {{affects a}} great {{portion of the}} population little is known on the loading of the human locomotor system of osteoarthritics during ambulation. Purpose: To examine and compare the characteristics of shock generation and absorption between healthy and osteoarthritic females during various walking conditions. Material and Method: Ten middle-aged healthy and ten osteoarthritic women walked barefoot on a motorized treadmill at 1. 5 and 2. 5 miles per hour while its surface was level, uphill and downhill. The peak decelerations of the shank and head during the contact of the foot with the ground were recorded with a low mass accelerometer. In order to secure unbiased estimation of the results, ten steps were computed for each walking condition in all subjects. A three-way ANOVA compared all dependent variables. Results: Similar peak shank deceleration was recorded between healthy (2. 83 + 0. 19 g) and osteoarthritic women (2. 78 + 0. 23 g) (p> 0. 05) whereas osteoarthitic displayed significant (p< 0. 05) lower values of peak head deceleration (0. 66 + 0. 05 g) than healthy females (0. 81 + 0. 06 g). Fast walking produced significanly higher <b>deceleration</b> <b>values</b> (p< 0. 01) than slow walkin...|$|R
40|$|In {{response}} to your request, we are pleased to enclose herewith draft copies of change pages comprising proposed Revision 10 to the HI-STORM 100 Topical Safety Analysis Report (TSAR). The proposed changes are identified with revision bars in the margin. These changes include those discussed with the NRC {{in a conference call}} held on January 28, 2000 to address public comments received during rulemaking as well as other minor editorial changes and clarifications. Please note that changes to drawings are not included with this submittal as discussed with the NRC project manager previously. In the interest of absolute technical accuracy, we have also revised the results of the cask tip-over events in TSAR Table 3. A. 4 even though the <b>deceleration</b> <b>values</b> change only in the second decimal place. Recent discovery of a data input discrepancy in the LS-DYNA 3 D input file led us to re-run the tip-over cases which, as proposed revised Table 3. A. 4 shows, produced infinitesimal changes in the results. Associated TSAR Figures 3. A. 19 through 22 are also proposed to be replaced. If you have any questions or require additional information, please contact us. Sincerely...|$|R
40|$|AbstractManufacturing {{processes}} {{become more}} and more complex. Therefore collisions within the working area of the machine tools occur more often. Those collisions often lead to tool and work piece damage. Especially when machining very large and complex work pieces, e. g. in the Aerospace Industry, where such a work piece damage is very expensive. Additionally, these collisions could cause the breakage of the whole machine, which will lead to downtime and high costs; especially the commissioning of the machine tool and the work piece involves high risk of collisions caused by manual machining. In order to solve this problem manufacturers offer different specialized crash protection mechanisms which have certain constraints or a lack of generality. Within this paper we present a collision prevention system based on a hull concept which monitors all machine axes regarding risk of collision without using physical sensors. It is necessary to consider a sufficient stopping distance for all moving machine parts to avoid a crash. These distances can be seen as hulls around the components depending on the maximum speed and <b>deceleration</b> <b>values.</b> Consequently, we present an approach to create these hulls, based on the CAD data of the machine tool, work piece as well as the tools. This leads to a virtual collision model which can be used as input data for the collision prevention system. The advantages as well as the current limitations of the introduced collision prevention system are discussed based on a machine tool in operation...|$|R
40|$|In {{order to}} specify the requirements, which are posed to an ACC system in real traffic, an ACC {{situation}} classification scheme has been introduced. To study driver- and situation-specific effects on ACC systems, 3 trial series have been performed in total. In a preliminary study, distributions of <b>deceleration</b> <b>values</b> have been measured and evaluated in three traffic areas: highways, rural and urban roads. In a first extended trial, distance keeping behaviour of subjects was recorded when driving on highways. ACC-relevant situations were extracted and classified. Driver- and situation-specific features were identified, e. g. in frequency and duration of various ACC situations, in distance distribution when following, and in minimum distance when approaching a preceding vehicle. With the data recorded, the applicability of various normative 'safe distance' criteria (fixed time, TTC, worst case) was evaluated. The intention persued with another trial series was to investigate, to which extent basic ACC control factors (distance time, dynamic factors for distance and velocity control) are adaptable to various drivers, and whether drivers were able to conduct the adaption procedures by themselves. Analyses showed that driver-specific preference ranges related to the control factors could be identified. Whereas {{the effects of the}} factor determining distance time could be clearly perceived by the subjects, the effects of the other dynamic factors were less transparent. (orig.) SIGLEAvailable from TIB Hannover: RN 4858 (10415) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany) DEGerman...|$|R
40|$|The {{purpose of}} the current study was to {{determine}} the validity and reliability of an inertial sensor for assessing speed specific to athletes competing in the wheelchair court sports (basketball, rugby, and tennis). A wireless inertial sensor was attached to the axle of a sports wheelchair. Over two separate sessions, the sensor was tested across a range of treadmill speeds reflective of the court sports (1. 0 to 6. 0 m/s). At each test speed, ten 10 -second trials were recorded and were compared with the treadmill (criterion). A further session explored the dynamic validity and reliability of the sensor during a sprinting task on a wheelchair ergometer compared with high-speed video (criterion). During session one, the sensor marginally overestimated speed, whereas during session two these speeds were underestimated slightly. However, systematic bias and absolute random errors never exceeded 0. 058 m/s and 0. 086 m/s, respectively, across both sessions. The sensor was also shown to be a reliable device with coefficients of variation (% CV) never exceeding 0. 9 at any speed. During maximal sprinting, the sensor also provided a valid representation of the peak speeds reached (1. 6 % CV). Slight random errors in timing led to larger random errors in the detection of <b>deceleration</b> <b>values.</b> The results of this investigation have demonstrated that an inertial sensor developed for sports wheelchair applications provided a valid and reliable assessment of the speeds typically experienced by wheelchair athletes. As such, this device will be a valuable monitoring tool for assessing aspects of linear wheelchair performance...|$|R
40|$|This diploma thesis called Motorcycle Race Dynamics {{captures the}} {{development}} and development of motorcycles, a description of types, design elements and the techniques of riding a motorcycle. However, {{the aim of this}} work is to organize, perform and evaluate the measurement of braking deceleration and the crossing of motorcycles. Before separate data processing, a theoretical methodology for measuring transverse displacement and braking deceleration is proposed. This requires enough motorcycles, experienced riders, two cameras, a measuring instrument and precisely dimensioned lines. The lateral displacement and braking deceleration measurements are then performed and subsequently evaluated. Output of this work include, in addition to videos, an overview of transverse displacement times, or longitudinal <b>deceleration</b> output <b>values</b> gained from braking...|$|R
40|$|The present paper {{deals with}} the {{lightweight}} design and the crashworthiness analysis of a composite impact attenuator for a Formula SAE racing car, in order to pass homologation requirements. The analysed impact attenuator is manufactured by lamination of prepreg sheets in carbon fibres and epoxy matrix, particularly used for sporting applications, and has a very similar geometry to a square frusta, so as to obtain a progressive and controlled deformation. During the design, attention {{was focused on the}} material distribution and gradual smoothing, but also on the lamination process, which can heavily affect the energy absorption capability. To reduce the development and testing costs of a new safety design, computational crash simulations for early evaluation of safety behaviour under vehicle impact test were carried out. The dynamic analysis was therefore conducted both numerically, using an explicit finite element code such as LS-DYNA, and experimentally, by means of an appropriately instrumented drop weight test machine, in order to validate the model in terms of <b>deceleration</b> <b>values</b> during crushing. To assess the quality of the simulation results, a comparative analysis was initially developed on simple CFRP composite tubes subjected to dynamic axial loading. The numerical analysis was conducted using both shell and solid elements, in order to reproduce not only the brittleness of the composite structure but also the effective delamination phenomenon. Both the analyses show a good capacity to reproduce the crushing process; this is confirmed by the fact that model estimated displacements and accelerations are in close agreement with observed values for these variables. This confirms the quality of the methodology and approach used for the design of a racing car impact attenuator...|$|R
40|$|Fluid {{mechanical}} drag {{is critical}} to the energy costs of flight and diving in birds and mammals (Kooyman, 1989; Pennycuick, 1989; Lovvorn et al., 1991; Lovvorn et al., 1999). At depths where pressure has substantially reduced the volume and buoyancy of air in the respiratory system and plumage, drag becomes the main mechanical cost of steady swimming (Lovvorn, 2001). Body size has strong effects on drag because it determines the ratio of mass to surface area and, thus, the ratio of inertial to viscous (friction) forces. Body shape also has an important influence on drag by altering the point along the body where boundary-layer flow shifts from laminar to turbulent and the point where the boundary layer separates from the body (Hoerner, 1965; Webb, 1975; Aleyev, 1977). These points are further affected by the roughness and flexibility of the body surface, which can delay separation by inducing fully turbulent but attached flow (Aleyev, 1977) or enhance separation by surface flutter (Tucker, 1990; Pennycuick et al., 1996). We investigated the effects on drag of the size, shape and speed of diving birds and how these relationships are modified by the surface effects of the plumage. Drag has been calculated from films of deceleration during gliding by live animals (Clark and Bemis, 1979; Bilo and Nachtigall, 1980; Feldkamp, 1987; Skrovan et al., 1999; Stelle et al., 2000). Deceleration measurements avoid problems with unnatural flutter of fur or feathers on dead specimens and are best for estimates of drag during gliding. However, <b>deceleration</b> <b>values</b> include the drag of propulsive limbs and are at much lower speeds than those achieved instantaneously during accelerational stroking. Thus, they are less satisfactory for modeling the drag of the body fuselage (head and trunk only) throughout strokes when the efficiency (including drag...|$|R
40|$|The Volvo PCR (Pre Crash Recorder) {{is meant}} to {{function}} as a complement to the existing DARR (Digital Accident Research Recorder). The latter is a digital crash pulse recorder that has been installed in Volvo cars since 1994. It records approximately 60 <b>deceleration</b> <b>values</b> in 180 ms upon front airbag deployment. The PCR records 31 selected parameters (table 1) every 500 ms, starting 5 seconds before a crash, and stores them in an electronic memory (EEPROM). This thesis aimed at developing methods and tools for the evaluation of the PCR data. This was done by first explaining and evaluating the parameters thoroughly. A BASIC program was then constructed for the decoding of t he hexadecimal PCR data. This program was also used {{in the construction of the}} fictitious example of an accident investigation that is included in this report. The most important conclusion from this thesis was that:. The PCR data should be used as in-parameters in accident research, e. g. the location of the occupants and their body parts in the collision moment, the vehicle speed or the status and function of the vehicle safety systems.. The extracted PCR data should be presented in a clear way, possibly in an excel chart. This can easily be achieved by using a program like the one developed in this thesis.. It is recommendable that the parameters are evaluated in small groups or pairs. For example could the steering wheel angle be plotted with the actual yaw rate and the active-safety system parameters with the accelerations and rotations (roll and yaw) of the car.. When recreating the course of an accident it is suitable to make simple figures from every sampling moment to clarify the big picture. For the future it is recommended to add more parameters to the recordings and develop methods [...] ...|$|R
40|$|Kinematical {{models are}} {{constrained}} by the latest observational data from geometry-distance measurements, which include 557 type Ia supernovae (SNIa) Union 2 data and 15 observational Hubble data. Considering two parameterized <b>deceleration</b> parameter, the <b>values</b> of current <b>deceleration</b> parameter q_ 0, jerk parameter j_ 0 and transition redshift z_T, are obtained. Furthermore, we show the departures for two parameterized kinematical models from ΛCDM model according to the evolutions of jerk parameter j(z). Also, it is shown that the constraint on jerk parameter j(z) is weak by the current geometrical observed data. Comment: 8 pages, 2 figure...|$|R
40|$|One of {{the core}} types of {{analysis}} performed in naturalistic driving studies (NDS) is event based analysis (EBA). EBA considers surrogate events, called safety critical events (SCE), for crashes, since actual crashes are rare. In order to find potential safety critical events, constant parameter thresholds have been commonly used (Simons-Morton et al., 2011). A key challenge of NDS is determining whether these SCEs are safety-relevant requiring time-consuming and expensive manual video coding. In order to lower the time needed for manual coding, a new approach of identifying SCE is presented on the data of the UDRIVE NDS. In this approach, SCE triggers are defined based on the likelihood of certain events countering the inherited high number of false-alarms of constant thresholds (Simons-Morton et al., 2011). The approach provides a functional relationship between the threshold parameters (e. g. longitudinal acceleration and situational parameters such as speed). This function {{is based on the}} joint probability density distribution (JPDD) of the involved trigger parameters. The first step is to estimate the JPDD from a representative sample. Because the linearly binned kernel density estimate approach (Wand, 1994) is computationally fast and allows for estimating two dimensional distributions, it was used for estimating the JPDD (Deng, 2011). The trigger function is computed as the percentile line of the estimated JPDD by computing the cumulative probability function. This curve represents a dynamically changing threshold of trigger parameters depending on a chosen set of situational parameters such as velocity. This approach allows for determining events occurring with a specific probability. A polynomial fit can help determining an easy-to-implement representation of the trigger function. This approach has two advantages: Specific values of a constant trigger threshold {{do not need to be}} chosen, but instead values are the result of a general parameter setting. Furthermore, it provides a dynamically changing threshold for different scenarios, such as a lower threshold on longitudinal <b>deceleration</b> <b>values</b> for driving on a highway (higher speed levels) compared to urban driving (lower speed levels). We expect that this framework closes the gap between discrete forms of SCE triggers and helps to lower the number of false-alarm detections and therefore relieves the burden of manual inspection...|$|R
40|$|An {{experimental}} {{approach to}} traffic flow analysis {{is presented in}} which methodology from pattern recognition is applied to a specific dataset to examine its utility in determining traffic patterns. The selected dataset for this work, taken from a 1985 study by JHK and Associates (traffic research) for the Federal Highway Administration, covers an hour long time period over a quarter mile section and includes nine different identifying features for traffic at any given time. The initial step is to select the most pertinent of these features as a target for extraction and local storage during the experiment. The tools created for this approach, a two-level hierarchical group of operators, are used to extract features from the dataset to create a feature space; this is done to minimize the experimental set to a matrix of desirable attributes from the vehicles on the roadway. The application is to identify if this data can be readily parsed into four distinct traffic states; in this case, the state of a vehicle is defined by its velocity and acceleration at a selected timestamp. A three-dimensional plot is used, with color as the third dimension and seen from a top-down perspective, to initially identify vehicle states in a section of roadway over a selected section of time. This is followed by applying k-means clustering, in this case with k= 4 to match the four distinct traffic states, to the feature space to examine its viability in determining the states of vehicles in a time section. The method?s accuracy is viewed through silhouette plots. Finally, a group of experiments run through a decision-tree architecture is compared to the kmeans clustering approach. Each decision-tree format uses sets of predefined values for velocity and acceleration to parse the data into the four states; modifications are made to acceleration and <b>deceleration</b> <b>values</b> to examine different results. The three-dimensional plots provide a visual example of congested traffic for use in performing visual comparisons of the clustering results. The silhouette plot results of the k-means experiments show inaccuracy for certain clusters; on the other hand, the decision-tree work shows promise for future work...|$|R
5000|$|... where [...] is {{the force}} on the front tires, [...] is the {{force on the}} rear tires, [...] is the {{distance}} from the CM to the rear wheels, [...] is {{the distance from the}} CM to the front wheels, [...] is the wheelbase, [...] is the mass of the vehicle, [...] is the acceleration of gravity (approx. 9.8 m/s2), [...] is the height of the CM above the ground, [...] is the acceleration (or <b>deceleration</b> if the <b>value</b> is negative). So, as is common experience, when the vehicle accelerates, the rear usually sinks and the front rises depending on the suspension. Likewise, when braking the front noses down and the rear rises.: ...|$|R
40|$|The first- and second-impact {{absorption}} {{properties of}} different commercial polymeric foams were analysed {{in order to}} identify possible alternatives to currently used polystyrene foams (EPS) for protective helmets. An appropriate “falling dart” apparatus was built complying with standard regulations to study and compare an expanded polystyrene (EPS) actually used in motorcycle helmets productions, with commercial polypropylene (EPP), polyvinyl-chloride (EPVC) and polyethylene (EPE) foams. Both dynamic cushioning tests in a “free sample” configuration and impact tests using guided hemispherical indenters were performed for each class of material at three different temperatures (- 20 °C, ambient, 50 °C) and at two different bulk densities. Also the residual deformation was measured for each sample. In order to characterise dynamic absorption properties of materials, <b>deceleration</b> peak <b>values</b> obtained from the tests with hemispherical impactors have been plotted versus different static loads in a cushion curve fashion. Expanded polypropylene showed better second-impact absorption properties and lower residual deformation than EPS, suggesting it as a possible alternative material for inner polymeric foam protective shells, particularly in view of matching the two-shock criteria required for official approval of car driving helmets...|$|R
40|$|The optimal-velocity model, as {{proposed}} by Bando et al. [1], shows unrealistic values of the acceleration for various optimal-velocity functions [2, 3]. We discuss different approaches of how to correct this problem. Multiple look-ahead (many-neighbour interaction) models are the most promising candidates in reducing accelerations and <b>decelerations</b> to realistic <b>values.</b> We focus on two such models and, in particular, their linear stability and how these affect the vehicle dynamics and wave solutions. As found earlier [4], multiple look-ahead models reproduce many real flow features, and our results further support the necessity of this ansatz. However, the problem of non-locality arises when they are transformed into the corresponding continuum model. We discuss three methods of how to interpret many-neighbour interaction in macroscopic model...|$|R
40|$|In {{professional}} cricket, where bowlers can bowl balls {{that reach}} speeds {{of up to}} 160 km h- 1, effective head protection is vital. Current head protection equipment typically consists of a helmet with a high impact grade polypropylene shell, a high density EPS liner, and a metal face guard. Most of the weight in existing helmets is attributed to the steel grill used as the face guard. We present a virtual design approach to the development and evaluation of new face guards made from alternative materials. In particular, we investigate a face guard design for cricket made from polycarbonate rather than steel using an explicit dynamic finite element analysis (FEA) approach. The FEA model developed for this purpose incorporates the headform, helmet, polycarbonate face guard and the impacting ball. ABAQUS CAE was used for FEA. HyperMesh and SolidWorks were used to develop the geometric model. This work identifies appropriate modelling and simulation strategies, and key design attributes {{for the development of}} new face guards using alternative materials. A preliminary study shows that by using polycarbonates it is possible to reduce the mass of the face guard by 20 %, thus contributing to greater comfort of the players without compromising their safety. The key criteria for reduction of ball deceleration by at least 25 % at each test site were satisfied, with <b>deceleration</b> reduction <b>values</b> ranging from 44 % to 87 % from those due to ball impact with the bare head...|$|R
40|$|This article studies a {{proposed}} analytical algorithm {{of the terminal}} guidance for the lunar lander. The analytical solution, which forms {{the basis of the}} algorithm, was obtained for a constant acceleration trajectory and thrust vector orientation programs that are essentially linear with time. The main feature of the proposed algorithm is a completely analytical solution to provide the lander terminal guidance to the desired spot in 3 D space when landing on the atmosphereless body with no numerical procedures. To reach 6 terminal conditions (components of position and velocity vectors at the final time) are used 6 guidance law parameters, namely time-to-go, desired <b>value</b> of braking <b>deceleration,</b> initial <b>values</b> of pitch and yaw angles and rates of their change. In accordance with the principle of flexible trajectories, this algorithm assumes the implementation of a regularly updated control program that ensures reaching terminal conditions from the current state that corresponds to the control program update time. The guidance law parameters, which ensure that terminal conditions are reached, are generated {{as a function of the}} current phase coordinates of a lander. The article examines an accuracy and reliability of the proposed analytical algorithm that provides the terminal guidance of the lander in 3 D space through mathematical modeling of the lander guidance from the circumlunar pre-landing orbit to the desired spot near the lunar surface. A desired terminal position of the lunar lander is specified by the selenographic latitude, longitude and altitude above the lunar surface. The impact of variations in orbital parameters on the terminal guidance accuracy has been studied. By varying the five initial orbit parameters (obliquity, ascending node longitude, argument of periapsis, periapsis height, apoapsis height) when the terminal spot is fixed the statistic characteristics of the terminal guidance algorithm error according to the terminal position and velocity have been estimated. </p...|$|R
40|$|In this paper, we have {{investigated}} late time acceleration for a spatially flat dust filled Universe in Brans-Dicke {{theory in the}} presence of a positive cosmological constant Λ. Expressions for Hubble's constant, luminosity distance and apparent magnitude have been obtained for our model. The theoretical results are compared with the observed values of the the latest 287 high red shift (. 3 ≤ z ≤ 1. 4) SN Ia supernova data's taken from Union 2. 1 compilation to estimate the present values of the matter and dark energy parameters (Ω_m) _ 0 and (Ω_Λ) _ 0. We have also estimated the present value of Hubble's constant H_ 0 in the light of a updated sample of Hubble parameter measurements including 19 independent data points. The results are found to be in good agreement with recent astrophysical observations. We have also calculated various physical parameters such as the matter and dark energy densities, the present age of the universe and <b>deceleration</b> parameter. The <b>value</b> for BD-coupling constant ω is set to be 40000 on the basis of accuracy of the solar system tests and recent experimental evidence. Comment: 13 pages, 8 figure...|$|R
40|$|Objective: Long-term {{survivors}} of asymptomatic children treated with anthracycline may have cardiac toxicity without clinical findings. The subclinical cardiac toxicity could be evaluated by dobutamine stress echocardiography (DSE) with exploring effective and safe doses of dobutamine. Methods: Twenty asymptomatic survivors (mean age: 19. 2 ± 4. 0 years) treated with cumulative dose of 282. 1 ± 125. 9 mg/m 2 of anthracycline {{were compared with}} 18 age-matched healthy volunteers. Total time completed this treatment was 10. 2 ± 2. 2 years. This was a cross-sectional case-controlled study and patient and control groups were evaluated {{at the time of}} routine appointments. Echocardiographic studies were performed before and after each dobutamine infusion of 5, 10, 15, 20 µg/kg/min. Statistical analysis: Mann-Whitney U test was used to evaluate the difference between the groups. ANOVA for repeated measurements test was used to compare each measurement of control and patients groups and Bonferroni posthoc test was used for correction. Results: Hemodynamic changes are observed at the dobutamine doses of 15 µg/kg/min in the patient group. Before dobutamine infusion in the patient group only isovolumic relaxation and contraction times values were prolonged comparing to the control group. After the infusion of dobutamine ejection fraction, shortening fraction, left ventricular posterior wall thickening (%LVPWt), end-systolic wall stress (ESS), interventricular septum systolic thickening, left ventricular end-systolic and end-diastolic diameters, mitral acceleration (AT) and <b>deceleration</b> times <b>values</b> were deteriorated in the patient group compared to the control group (p< 0. 05 for all). The highest differences between the groups were observed in the %LVPWt, ESS and AT values at the end of test. Conclusion: The DSE is an effective and safe method to demonstrate the late anthracycline cardiotoxicity. Echocardiographic evaluation should be made at rest and dobutamine dose of 20 μg/kg/min. In the early diagnosis of late cardiac toxicity; assessment of %LVPWt, AT and ESS values in addition to standard echocardiographic examination could be the guidance for early diagnosis of late cardiac toxicity...|$|R
