156|50|Public
60|$|The practice, indeed, is {{recommended}} by considerations far too obvious {{to require a}} <b>formal</b> <b>justification.</b> Logic is {{a portion of the}} Art of Thinking: Language is evidently, and by the admission of all philosophers, one of the principal instruments or helps of thought; and any imperfection in the instrument, or in the mode of employing it, is confessedly liable, still more than in almost any other art, to confuse and impede the process, and destroy all ground of confidence in the result. For a mind not previously versed in the meaning and right use of the various kinds of words, to attempt the study of methods of philosophizing, would be as if some one should attempt to become an astronomical observer, having never learned to adjust the focal distance of his optical instruments so as to see distinctly.|$|E
5000|$|In the book's title, Hardy {{uses the}} word [...] "apology" [...] {{in the sense of}} a <b>formal</b> <b>justification</b> or defence (as in Plato's Apology of Socrates), not {{in the sense of a}} plea for forgiveness.|$|E
50|$|The Gulf Daily News {{reports that}} {{he was arrested in}} Saudi Arabia in late October 2008.Bahraini Member of Parliament Mohammed Khalid {{expressed}} dismay at the arrest of a third former Guantanamo captive by Saudi authorities, and said that the Saudis had not offered a <b>formal</b> <b>justification</b> for his arrest.|$|E
40|$|Consideration of the {{question}} of meaning in the framework of linguistics often requires an allusion to sets and other higher-order notions. The traditional approach to representing and reasoning about meaning in a computa- tional setting has been to use knowledge representation sysr terns that are either based on first-order logic or that use mechanisms whose <b>formal</b> <b>justifications</b> are to be provided after the fact. In this paper we shall consider the use of a higher-order logic for this task. We first present a version of definite clauses (positive Horn clauses) that is based on this logic. Predicate and function variables may occur in such clauses and the terms in the language are the typed -terms. Such term structures have a richness that may be exploited in representing meanings. We also describe a higher-order logic programming language, called Prolog, which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter. A virtue of this language is {{that it is possible to}} write programs in it that integrate syntactic and semantic analyses into one computational paradigm. This is to be contrasted with the more common practice of using two entirely different computation paradigms, such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing. We illustrate such an integration in this language by considering a simple example, and we claim that its use makes the task of providing <b>formal</b> <b>justifications</b> for the computations specified much more direct...|$|R
40|$|In {{this paper}} we are {{concerned}} with the numerical study of viscoelastic diphasic flows. We first propose a complete model and give some <b>formal</b> <b>justifications</b> including the case where both or one phase is a non-Newtonian one. Then, we have implemented a numerical method taking into account the lagrangian transport in the resolution of the extra-stress equation of Oldroyd type. We describe this fully-practical method and prove its stability so that one can perform long-time numerical simulations. Several significant tests (channel under shear, filling of tanks, breakup of jets [...] .) illustrate the pertinency of the model and of the numerical method. We show that the results are in accordance with the various qualitative behaviors observed in physical experiments...|$|R
40|$|Two studies {{examined}} abstract conditional reasoning. First, {{participants at}} three grade levels (grade 8, college, university) {{were informed that}} a problem corresponding to Affirmation of the consequent (P implies Q, Q is true) had no certain response {{and were asked to}} justify this conclusion, for both concrete and abstract premises. Results showed an increase in use of <b>formal</b> <b>justifications</b> with grade. The second study looked at the effects of embedding abstract premises into realistic or fantasy contexts with participants at grades 8 and 11. Results show that abstract reasoning is facilitated by realistic context. These results support the idea that such reasoning may represent a qualitative change in reasoning abilities and that its development relies on appropriate access to empirical knowledge despite the abstract nature of premises...|$|R
5000|$|This term is {{intended}} to replace other ones with pejorative semantics. It proposes a shift towards non-negative, non-disparaging and non-patronizing terms. The <b>formal</b> <b>justification</b> of the term {{can be found in}} the book El Modelo de la Diversidad by Agustina Palacios and Javier Romañach, 2007, [...] Examples of usage: ...|$|E
5000|$|The early {{substantive}} debates {{centered around}} {{issues raised by}} the Stamp Act and the earlier Sugar Act. The delegates spent a significant amount of time discussing the differences between direct ("internal") taxation and the regulation of trade (or [...] "external taxation"), and seeking <b>formal</b> <b>justification</b> of the idea that only the colonial assemblies had the right to levy internal taxes. [...] Fairly early in the deliberations the delegates agreed to produce a statement of rights which would form the foundation for petitions the congress would submit to Parliament and the king. According to Delaware delegate Caesar Rodney, the drafting of this statement was made difficult by the desire to balance the colonists' rights with the royal prerogative and the acknowledged powers of Parliament.|$|E
5000|$|Formal {{methods to}} {{calculate}} the Return on investment (ROI) have been widely understood and used for a long time, {{but there was no}} easy and widely known way to provide a <b>formal</b> <b>justification</b> for decisions based on intangible values, which can include the reputation of an organization, the wellbeing of staff, or the impact on society or the environment at large. It was particularly difficult for decision makers to work through the trade-offs between costs and intangible benefits, especially for long-term investments by commercial organizations, and for governments and non-profit organizations who are primarily concerned with intangible values without wasting limited funds. Even within commercial organizations, units traditionally viewed as [...] "cost centres" [...] found it difficult to get acceptance of proposals for projects that would lead to general and long-term efficiency or capability gains, as the benefits were difficult to assign to [...] "profit centres".|$|E
40|$|AbstractIn this paper, {{we propose}} a {{definition}} of goal achievability: given a basic action theory describing an initial {{state of the world}} and some primitive actions available to a robot, including some actions which return binary sensing information, what goals can be achieved by the robot? The main technical result of the paper is a proof that a simple robot programming language is universal, in that any effectively achievable goal can be achieved by getting the robot to execute one of the robot programs. The significance of this result is at least twofold. First, it is in many ways similar to the equivalence theorem between Turing machines and recursive functions, but applied to robots whose actions are specified by an action theory. Secondly, it provides <b>formal</b> <b>justifications</b> for using the simple robot programming language as a foundation for our work on robotics...|$|R
30|$|In this paper, we have {{described}} and evaluated the MICE procedure {{that can be}} used to impute missing values of different categories of variables. Although this approach lacks <b>formal</b> theoretical <b>justification,</b> it has the strong advantage of flexibility. Presumably, MICE can be used for TIMSS missing-data problems, given that most variables with missing data in the TIMSS background datafiles are not normally distributed.|$|R
40|$|Abstract. 1 -dimensional {{hamiltonian}} with {{ionization energy}} (ξ) {{is shown to}} be exactly {{the same with the}} total energy from the standard harmonic oscillator hamiltonian, with the harmonic oscillator potential, mω 2 x 2 / 2. Commutation relations for the ladder operators, energy at the n th state and the expectation value for the potential are analyzed and shown to be consistent with the principle of ξ. We also derive 1 -dimensional wave functions for Dirac delta (−αδ(x)) potential and with V (x) = 0. <b>Formal</b> <b>justifications</b> are given as to why the total energy in ξ based Fermi-Dirac statistics (iFDS) is exactly the same with the standard Fermi-Dirac statistics (FDS). Details are given on the total energy transformation from iFDS to FDS. Energy levels and Bohr radius of a Hydrogen atom are also shown to be consistent with the ionization energy concept. This concept is also applied in a free Mn atom that represent a free many-electron atom. 1...|$|R
50|$|Germany did {{not want}} to risk lengthy battles along the Franco-German border and instead adopted the Schlieffen Plan, a {{military}} strategy designed to cripple France by invading Belgium and Luxembourg, sweeping down around Paris and encircling and crushing both Paris and the French forces along the Franco-German border in a quick victory. After defeating France, Germany would turn to attack Russia. The plan required the violation of Belgium's and Luxembourg's official neutrality, which Britain had guaranteed by treaty. However, the Germans had calculated that Britain would enter the war regardless of whether they had <b>formal</b> <b>justification</b> to do so. At first the attack was successful: the German Army swept down from Belgium and Luxembourg and was nearly at Paris, at the nearby River Marne. However, the evolution of weapons over the last century heavily favored defense over offense, especially thanks to the machine gun, so that it took proportionally more offensive force to overcome a defensive position. This resulted in the German lines on the offense contracting to keep up the offensive time table while correspondingly the French lines were extending. In addition, some German units that were originally slotted for the German far right were transferred to the Eastern Front in reaction to Russia mobilizing far faster than anticipated. The combined affect had the German right flank sweeping down in front of Paris instead of behind it exposing the German Right flank to the extending French lines and attack from strategic French reserves stationed in Paris. Attacking the exposed German right flank, the French Army and the British Army put up a strong resistance to the defense of Paris at the First Battle of the Marne resulting in the German Army retreating.|$|E
40|$|When {{measuring}} the mass profile {{of any given}} cosmological structure through internal kinematics, the distant background density is always ignored. This trick is often refereed to as the "Jeans Swindle". Without this trick a divergent term from the background density renders the mass profile undefined, however, this trick has no <b>formal</b> <b>justification.</b> We show that when one includes {{the expansion of the}} Universe in the Jeans equation, a term appears which exactly cancels the divergent term from the background. We thereby establish a <b>formal</b> <b>justification</b> for using the Jeans Swindle. Comment: 5 pages, 2 figures, Accepted for publication in MNRAS Letter...|$|E
40|$|International audienceThis paper {{proposes a}} <b>formal</b> <b>justification</b> of {{simplified}} 1 D {{models for the}} propagation of electromagnetic waves in thin non-homogeneous lossy conductor cables. Our approach consists in deriving these models from an asymptotic analysis of 3 D Maxwell’s equations. In essence, we extend and complete previous results to the multi-wires case...|$|E
40|$|We derive an {{analytic}} {{expression for}} the power transferred from interstellar turbulence to the Galactic cosmic rays in propagation models which include re-acceleration. This is used to estimate the power required in such models and {{the relative importance of}} the primary acceleration as against re-acceleration. The analysis provides a <b>formal</b> mathematical <b>justification</b> for Fermi's heuristic account of second order acceleration in his classic 1949 paper. Comment: 3 pages, submitted to MNRAS. some additions as requested by the refere...|$|R
40|$|As {{personal}} assistant software matures and assumes more autonomous control of user activities, {{it becomes more}} critical that this software can tell the user why it is doing what it is doing, and instill trust in the user that its task knowledge reflects standard practice and is being appropriately applied. Our research focuses broadly on providing infrastructure {{that may be used}} to increase trust in intelligent agents. In this paper, we will report on a study we designed to identify factors that influence trust in intelligent adaptive agents. We will then introduce our work on explaining adaptive task processing agents as motivated by the results of the trust study. We will introduce our task execution explanation component and provide examples {{in the context of a}} particular adaptive agent named CALO. Key features include (1) an architecture designed for re-use among different task execution systems; (2) a set of introspective predicates and a software wrapper that extracts explanation-relevant information from a task execution system; (3) a version of the Inference Web explainer for generating <b>formal</b> <b>justifications</b> of task processing and converting them to user-friendly explanations; and (4) a unified framework for explaining results from task execution, learning, and deductive reasoning...|$|R
50|$|The {{theory of}} image {{restoration}} builds upon theories of apologia and accounts. Apologia is a <b>formal</b> defense or <b>justification</b> of an individual’s opinion, position, or actions, and an account {{is a statement}} made by an individual or organization to explain unanticipated or transgressive events.|$|R
40|$|This note {{provides}} a <b>formal</b> <b>justification</b> for the Friedman and Savage nonconcavity in {{the utility of}} money. This {{is based on the}} possibility of indivisibilities in the consumption possibilities set. A precise characterization of when gambling is optimal (and the optimal type) is provided in one special case. Some possible limitations are considered. ...|$|E
40|$|The {{empirical}} likelihood function, {{introduced by}} Owen (1988, 1990), {{is a good}} statistical tool for testing hypotheses and constructing confidence regions in a nonparametric approach. In this paper, we give a <b>formal</b> <b>justification</b> for using this tool to construct confidence intervals when the parameter of interest {{is the difference between}} the means of two populations...|$|E
40|$|AbstractIn {{this paper}} a proof outline logic is {{introduced}} for the partial correctness of multi-threaded object-oriented programs like in Java. The main contribution is a generalization of the Owicki& Gries proof method for shared-variable concurrency to dynamic thread creation. This paper {{also provides a}} <b>formal</b> <b>justification</b> of this generalization in terms of soundness and completeness proofs...|$|E
40|$|The {{distribution}} of the deformations of elementary cells is studied in an abstract lattice constructed from {{the existence of the}} empty set. One combination rule determining oriented sequences with continuity of set-distance function in such spaces provides a particular kind of spacetime-like structure that favors the aggregation of such deformations into fractal forms standing for massive objects. A correlative dilatation of space appears outside the aggregates. At the large scale, this dilatation results in an apparent expansion, while at the submicroscopic scale the families of fractal deformations give raise to families of particle-like structure. The theory predicts the existence of classes of spin, charges, and magnetic properties, while quantum properties associated to mass have previously been shown to determine the inert mass and the gravitational effects. When applied to our observable spacetime, the model would provide the justifications for the existence of the creation of mass in a specified kind of "void", and the fractal properties of the embedding lattice extend the phenomenon to <b>formal</b> <b>justifications</b> of Big-Bang-like events without need for any supply of an extemporaneous energy. Comment: 17 p., 4 figures. To appear in "Kybernetes: The Int. J. of Systems & Cybernetics" in a special issue on new theories on space and time. The journal Versio...|$|R
5000|$|During {{the rise}} of the Third Reich, the Reichsgericht became deeply embroiled in the National Socialist agenda. It even {{involved}} itself in matters of Nazi Matrimonial and Contract Law before enactment of the Nuremberg Laws. [...] During and after the Nazi period it received criticism for the ease, and even willingness, with which it provided the highest level of <b>formal</b> legal <b>justification</b> for Nazi programs. Immediately after the end of World War II the Reichsgericht was dissolved, and reformed into the German High Court for the Unified Economic Region, the Allied occupation zones of France, the United Kingdom and the United States.|$|R
40|$|Abstract. We give an {{overview}} on {{our approach to}} symbolic simulation in the PVS theorem prover and to demonstrate its usage {{in the realm of}} validation by executing specification on incomplete data. In this way, one can test executable models for a possibly infinite class of test vectors with one run only. One of the main benefits of symbolic simulation in a theorem proving environment is enhanced productivity by early detection of errors and increased confidence in the specification itself. 1 Introduction Traditional, simulation-based validation methods have not kept up with the scale or pace of modern digital design, and, therefore, form a major impediment in the drive for evermore complex designs [7]. This is mainly due to the sheer number of possible test cases which makes it nearly impossible to perform exhaustive testing. Thus testing only demonstrates the existence but not the absence of flaws. Even worse, it is unlikely that testing alone would have caught errors like the famous bug in the lookup table of the Intel Pentium floating-point division unit, since it only occurred on table inputs that were thought to be beyond the region of interest [15]. Formal verification methods based on theorem proving techniques, modelchecking, or a combination thereof offer viable alternatives to simple testing, since formal methods permit proving the absence of errors (in the formal model). Unfortunately, the construction of <b>formal</b> <b>justifications</b> is usually at best semiautomatic for industrial-sized designs and the cost of doing formal analysis in an interactive way currently prevents formal methods from being integrated in the development cycle for both hardware and software...|$|R
40|$|The timelessly optimal {{monetary}} policy proposed by Woodford (2003) may {{be dominated by}} alternative timeless policies. We provide a <b>formal</b> <b>justification</b> for these alternative policies. We demonstrate why discount rates do not matter and establish that optimizing over the unconditional expectation of the policy criterion function recovers these alternative strategies. Time consistency, unconditional expectation, timeless perspective, optimal {{monetary policy}}. ...|$|E
40|$|Free {{boundary}} problems {{based on}} mass conservation and surface tension with application in osmotic swelling are {{the topic of}} this contribution. We introduce new phase-field approximations of such models, in order to numerically investigate properties of the solutions. <b>Formal</b> <b>justification</b> of the proposed approximations is provided by matched asymptotic expansions supported by numerical tests reproducing the convergence for shrinking interface thickness...|$|E
40|$|This paper studies {{previously}} developed nonlinear Hilbert {{adjoint operator}} theory from a variational {{point of view}} and provides a <b>formal</b> <b>justification</b> for the use of Hamiltonian extensions via Gâteaux differentials. The primary motivation is its use in characterizing singular values of nonlinear operators, and in particular, the Hankel operator and its relationship to the state space notion of nonlinear balanced realizations. ...|$|E
40|$|The {{relation}} {{is considered}} between the distorted-wave Born (DWB) and the distorted-wave Rytov (DWR) approxima-tions. Analyzing the Helmholtz equation, it is {{shown that the}} <b>formal</b> asymptotic <b>justification</b> of DWB and DWR approxi-mations remains {{the same as that}} of the ordinary ones. A relation is derived between the first DWB and DWR approxima-tions and an example given to emphasize that these approximations, though simply related, have quite different ranges of accuracy. This paper considers the relation between the dis-torted-wave Born (DWB) and the distorted-wave Rytov (DWR) approximations. The ordinary Born [I] and Rytov [2] approximations are used to simplify both forward and inverse problems of wave propagation in applications ranging from nuclear physics to seismic exploration (see refs. [4 - 71, for example). Within these approximations, the solution of a partial differ-ential equation is expressed as a perturbation about a known solution to a simpler equation. The only dif-ference between the ordinary and distorted-wave ap-proximations is that for the distorted-wave approach, one assumes that the known solution is already “per-turbed ” relative to some ideal, simple model. To illustrate this we consider the Helmholtz equa-tion and show that the <b>formal</b> asymptotic <b>justification</b> of DWB and DWR approximations remains {{the same as that of}} the ordinary ones [3]. We also derive a rela-tion between the first DWB and DWR approximations and give an example to show that these approximations, though simply related, have quite different ranges of accuracy. We start with the homogeneous Helmholtz equa-tion [V 2 +/& 2 (x) ] U(x,k) = 0, (1) where n(x) is the index of refraction. We assume that n 2 (x) =ni(x) + en,(x) + e%r 2 (x) + [...] .) (2...|$|R
40|$|The <b>formal</b> <b>justifications</b> for all detentions under s. 2 of the Mental Health Act 1983 {{within an}} inner-city mental health trust were {{examined}} over a 12 -month period. The study explored: {{the nature of}} the justifications for detention; {{the extent to which these}} were associated with patient characteristics; and the extent to which the two medical practitioners involved in each case agreed on the justifications. The justifications reflected a greater emphasis on the protection of the individual concerned rather than the protection of others. A content analysis of the textual justifications revealed five broad themes: {{the nature of the}} risk posed by the patient; the patient's capacity to provide informed consent; their need for hospitalization; their lack of consent to informal admission; and their reliability or likely compliance. There was a significant association between patients' sex, ethnic group, diagnosis and the nature of risk indicated in the documentation, but further research is needed to clarify the nature of this association. The study found that in nearly a quarter of cases, the two professionals did not agree about whether or not the patient presented a danger to others. This lack of agreement was not associated with any patient or professional characteristics, and may reflect the complexity of this area of risk assessment. The authors suggest that the issue of 'risk' needs to be addressed in a more sophisticated manner within the Mental Health Act. Specifically, further guidance is needed as to the nature and leuels of risk that constitute grounds for detention. Further guidance is also needed regarding the issues that need to be recorded on the legal documentation for detention...|$|R
40|$|We {{present a}} bigraphical {{framework}} suited for modeling biological systems both at protein level and at membrane level. We characterize formally bigraphs corresponding to biologically meaningful systems, and bigraphic rewriting rules representing biologically admissible interactions. At the protein level, these bigraphic reactive systems correspond exactly to systems of kappa-calculus. Membrane-level interactions {{are represented by}} just two general rules, whose application can be triggered by protein-level interactions in a well-deﬁned and precise way. This framework {{can be used to}} compare and merge models at different abstraction levels; in particular, higher-level (e. g. mobility) activities can be given a <b>formal</b> biological <b>justiﬁcation</b> in terms of low-level (i. e., protein) interactions. As examples, we formalize in our framework the vesiculation and the phagocytosis processes...|$|R
40|$|We {{present a}} new {{algorithm}} for solving the optimal stopping problem. The algorithm {{is based on}} the idea of elimination of states where stopping is nonoptimal and the corresponding correction of transition probabilities. The <b>formal</b> <b>justification</b> of this method is given by one of two presented theorems. The other theorem describes the situation when an aggregation of states is possible in the optimal stopping problem...|$|E
40|$|This paper {{focuses on}} the {{inference}} of modes for which a logic program is guaranteed to terminate. This generalizes traditional termination analysis where an analyzer tries to verify termination for a specified mode. The contribution is a methodology which combines traditional termination analysis and backwards analysis to obtain termination inference. This leads {{to a better understanding}} of termination inference, simplifies its <b>formal</b> <b>justification,</b> and facilitates implementation...|$|E
40|$|Barendregt’s {{variable}} convention simplifies many informal proofs in the λ-calculus {{by allowing}} the consideration of only those bound variables that have been suitably chosen. Barendregt does not give a <b>formal</b> <b>justification</b> for the variable convention, which {{makes it hard to}} formalise such informal proofs. In this paper we show how a form of the variable convention can be built into the reasoning principles for rule inductions. We give two examples explaining our technique...|$|E
40|$|In many {{environmental}} {{and public health}} domains, heuristic methods of risk and decision analysis must be relied upon, either because problem structures are ambiguous, reliable data is lacking, or decisions are urgent. This introduces an additional source of uncertainty beyond model and measurement error – uncertainty stemming from relying on inexact inference rules. Here we identify and analyse heuristics used to prioritise risk objects, to discriminate between signal and noise, to weight evidence, to construct models, to extrapolate beyond datasets, and to make policy. Some of these heuristics are based on causal generalisations, yet can misfire when these relationships are presumed rather than tested (e. g. surrogates in clinical trials). Others are conventions designed to confer stability to decision analysis, yet which may introduce serious error when applied ritualistically (e. g. significance testing). Some heuristics {{can be traced back}} to <b>formal</b> <b>justifications,</b> but only subject to strong assumptions that are often violated in practical applications. Heuristic decision rules (e. g. feasibility rules) in principle act as surrogates for utility maximisation or distributional concerns, yet in practice may neglect costs and benefits, be based on arbitrary thresholds, and be prone to gaming. We highlight the problem of rule-entrenchment, where analytical choices that are in principle contestable are arbitrarily fixed in practice, masking uncertainty and potentially introducing bias. Strategies for making risk and decision analysis more rigorous include: formalising the assumptions and scope conditions under which heuristics should be applied; testing rather than presuming their underlying empirical or theoretical justifications; using sensitivity analysis, simulations, multiple bias analysis, and deductive systems of inference (e. g. directed acyclic graphs) to characterise rule uncertainty and refine heuristics; adopting “recovery schemes” to correct for known biases; and basing decision rules on clearly articulated values and evidence, rather than convention...|$|R
40|$|Abstract. Document-centric static index pruning methods provide smaller indexes {{and faster}} query times by {{dropping}} some within-document term information from inverted lists. We present {{a method of}} pruning in-verted lists derived from the formulation of unigram language models for retrieval. Our method {{is based on the}} statistical significance of term frequency ratios: using the two-sample two-proportion (2 P 2 N) test, we statistically compare the frequency of occurrence of a word within a given document to the frequency of its occurrence in the collection to de-cide whether to prune it. Experimental results show that this technique can be used to significantly decrease the size of the index and querying speed with less compromise to retrieval effectiveness than similar heuris-tic methods. Furthermore, we give a <b>formal</b> statistical <b>justification</b> for such methods. ...|$|R
50|$|Occam Learning {{is named}} after Occam's razor, which is a {{principle}} stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a <b>formal</b> and mathematical <b>justification</b> for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, parsimony (of the output hypothesis) implies predictive power.|$|R
