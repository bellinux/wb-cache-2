62|52|Public
2500|$|Earlier {{models of}} memory are {{primarily}} {{based on the}} postulates of Hebbian learning. Biologically relevant models such as [...] <b>Hopfield</b> <b>net</b> {{have been developed to}} address the properties of associative, rather than content-addressable, style of memory that occur in biological systems. These attempts are primarily focusing on the formation of medium- and long-term memory, localizing in the hippocampus. Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory.|$|E
5000|$|The {{connections}} in a <b>Hopfield</b> <b>net</b> typically have the following restrictions: ...|$|E
5000|$|Boltzmann machine - like a <b>Hopfield</b> <b>net</b> but uses {{annealed}} Gibbs sampling {{instead of}} gradient descent ...|$|E
40|$|The present paper investigates four {{relatively}} independent issues, each in one section, which complete {{our knowledge}} regarding the computational aspects of popular <b>Hopfield</b> <b>nets</b> [9]. In Section 2, the computational equivalence of convergent asymmetric and <b>Hopfield</b> <b>nets</b> is shown {{with respect to}} the network size. In Section 3, the convergence time of <b>Hopfield</b> <b>nets</b> is analyzed in terms of bit representations. In Section 4, a polynomial time approximate algorithm for the minimum energy problem is shown. In Section 5, the Turing universality of analog <b>Hopfield</b> <b>nets</b> is studied...|$|R
40|$|We survey {{some aspects}} of the {{computational}} complexity theory of discrete-time and discrete-state Hopfield networks. The emphasis is on topics that are not adequately covered by the existing survey literature, most significantly: 1. the known upper and lower bounds for the convergence times of <b>Hopfield</b> <b>nets</b> (here we consider mainly worst-case results); 2. the power of <b>Hopfield</b> <b>nets</b> as general computing devices (as opposed to their applications to associative memory and optimization); 3. the complexity of the synthesis ("learning") and analysis problems related to <b>Hopfield</b> <b>nets</b> as associative memories...|$|R
40|$|We {{propose a}} unifying {{approach}} {{to the analysis of}} computational aspects of symmetric <b>Hopfield</b> <b>nets</b> which is based on the concept of "energy source". Within this framework we present different results concerning the computational power of various Hopfield model classes. It is shown that polynomial-time computations by nondeterministic Turing machines can be reduced to the process of minimizing the energy in <b>Hopfield</b> <b>nets</b> (the MIN ENERGY problem). Furthermore, external and internal sources of energy are distinguished. The external sources include e. g. energizing inputs from so-called Hopfield languages, and also certain external oscillators that prove finite analog <b>Hopfield</b> <b>nets</b> to be computationally Turing universal. On the other hand, the internal source of energy can be implemented by a symmetric clock subnetwork producing an exponential number of oscillations which are used to energize the simulation of convergent asymmetric networks by <b>Hopfield</b> <b>nets.</b> This shows that infinite families of polynomial-size <b>Hopfield</b> <b>nets</b> compute the complexity class PSPACE/poly. A special attention is paid to generalizing these results for analog states and continuous time to point out alternative sources of efficient computation. ...|$|R
50|$|The {{weight matrix}} of <b>Hopfield</b> <b>Net,</b> that stores the memory, closely resembles {{the one used}} in weight matrix {{proposed}} by Anderson. Again, when new association is introduced, the weight matrix {{is said to be}} ‘updated’ to accommodate the introduction of new memory; it is stored until the matrix is cued by a different vector.|$|E
5000|$|Training a <b>Hopfield</b> <b>net</b> {{involves}} {{lowering the}} energy of states that the net should [...] "remember". This allows the net {{to serve as a}} content addressable memory system, that is to say, the network will converge to a [...] "remembered" [...] state if it is given only part of the state. The net can be used to recover from a distorted input to the trained state that is most similar to that input. This is called associative memory because it recovers memories on the basis of similarity. For example, if we train a <b>Hopfield</b> <b>net</b> with five units so that the state (1, -1, 1, -1, 1) is an energy minimum, and we give the network the state (1, -1, -1, -1, 1) it will converge to (1, -1, 1, -1, 1). Thus, the network is properly trained when {{the energy of}} states which the network should remember are local minima.|$|E
5000|$|A {{computational}} model by Kinouchi and Kinouchi (2002) implementing a chaotic itinerancy dynamics in a <b>Hopfield</b> <b>net</b> {{shows that the}} Crick-Mitchison unlearning mechanism produces a trajectory of associated attractors ("a narrative") where the strong ("emotional", [...] "obsessive" [...] or [...] "overplastic") memories have their dominance downplayed and an equalization between memory basins produces a better recovery of memories not recalled during the [...] "dream".|$|E
40|$|We survey {{some aspects}} of the {{computational}} complexity theory of discrete-time and discrete-state Hopfield networks. The emphasis is on topics that are not adequately covered by the existing survey literature, most significantly: 1. the known upper and lower bounds for the convergence times of <b>Hopfield</b> <b>nets</b> (here we consider mainly worst-case results); 2. the power of <b>Hopfield</b> <b>nets</b> as general computing devices (as opposed to their applications to associative memory and optimization); 3. the complexity of the synthesis ("learning") and analysis problems related to <b>Hopfield</b> <b>nets</b> as associative memories. Draft chapter for the forthcoming book The Computational and Learning Complexity of Neural Networks: Advanced Topics (ed. Ian Parberry) ...|$|R
40|$|In {{the present}} paper four, {{relatively}} independent issues, each in one section, that complete our knowledge regarding the computational aspects of popular <b>Hopfield</b> <b>nets</b> [9] will be investigated. Namely, in Section 1 the computational equivalence of convergent asymmetric and <b>Hopfield</b> <b>nets</b> is proved {{with respect to the}} network size. In Section 2, the convergence time of <b>Hopfield</b> <b>nets</b> is analyzed in terms of bit representations. In Section 3, a polynomial time approximate algorithm for the minimum energy problem is shown. Finally, in Section 4, the Turing universality of analog <b>Hopfield</b> <b>nets</b> is studied. Keywords Hopfield networks, computational power, convergence time, minimum energy problem, analog networks 1 Department of Theoretical Computer Science, Institute of Computer Science, Academy of Sciences of the Czech Republic. Research supported by GA CR Grant No. 201 / 98 / 0717. 2 Department of Mathematics, University of Jyvaskyla, P. O. Box 35, FIN- 40351 Jyvaskyla, Finland, e-mail: orpon [...] ...|$|R
50|$|The {{units in}} <b>Hopfield</b> <b>nets</b> are binary {{threshold}} units, i.e. the units only take {{on two different}} values for their states and the value is determined by {{whether or not the}} units' input exceeds their threshold. <b>Hopfield</b> <b>nets</b> normally have units that take on values of 1 or -1, and this convention will be used throughout this page. However, other literature might use units that take values of 0 and 1.|$|R
5000|$|As the Anderson’s {{weight matrix}} between neurons will only {{retrieve}} the approximation {{of the target}} item when cued, {{modified version of the}} model was sought {{in order to be able}} to recall the exact target memory when cued. The <b>Hopfield</b> <b>Net</b> [...] is currently the simplest and most popular neural network model of associative memory; the model allows the recall of clear target vector when cued with the part or the 'noisy' version of the vector.|$|E
50|$|Earlier {{models of}} memory are {{primarily}} {{based on the}} postulates of Hebbian learning. Biologically relevant models such as <b>Hopfield</b> <b>net</b> {{have been developed to}} address the properties of associative, rather than content-addressable, style of memory that occur in biological systems. These attempts are primarily focusing on the formation of medium- and long-term memory, localizing in the hippocampus. Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory.|$|E
40|$|Structural {{description}} of objects comprised {{descriptions of the}} parts and spatial relations between t. he parts. This paper presents a <b>Hopfield</b> <b>net</b> based scheme for matching struct. ural shape descriptions. The current formulat. ion of the matching scheme is general enough. to take Care of part. ial mismatch between the individual parts and spat. ial constraints bet. ween t. hese part. s. In addition, a transfoITnation of the shape descript. ions has been suggest. ed with which shape descriptions containing 3 Symmetrica. l spatial constraints between the parts can be matdled using symmetric interconnection weights for the <b>Hopfield</b> <b>net.</b> The <b>Hopfield</b> <b>net</b> based fOITnulation has been ext. ended t. o consider the problem of finding the best match of the test shape descriptions {{with one of the}} stored prototypes. The matching scheme has been experimentally applied for recognition of hand-tools and symbols. In both cases, the network produced encouraging recognit. ion result. s. K eywoTds: Shape matching, new'a! uetwork, spatial constraint. s. 1...|$|E
5000|$|<b>Hopfield</b> <b>nets</b> have a {{scalar value}} {{associated}} with each state of the network {{referred to as the}} [...] "energy", E, of the network, where: ...|$|R
40|$|We {{establish}} a fundamental {{result in the}} theory of continuous-time Liapunov dynamical systems, by showing that so called continuous-time symmetric <b>Hopfield</b> <b>nets</b> which usually exhibit fast convergence may, in the worst case, possess a transient period that is exponential in the system dimension. The result stands in contrast to e. g. the use of <b>Hopfield</b> <b>nets</b> in combinatorial optimization applications. Also our example of an exponential-transient continuous-time system (a simulated binary counter) converges more slowly than any discrete-time Hopfield network of the same representation size. This suggests that continuoustime computations may be worth investigating for gains in descriptional efficiency as compared to their discrete-time counterparts...|$|R
40|$|This paper {{describes}} an efficient algorithm adaptable to {{several types of}} sequencing problems. In the first section, three such problems are outlined briefly: the travelling-salesman problem, VLSI cell placement, and job scheduling. An intuitive and straightforward solution using <b>Hopfield</b> <b>nets</b> is presented. When the underlying structure of sequencing problems {{and the way they}} are attacked in <b>Hopfield</b> <b>nets</b> is examined in detail, a simpler problem representation and a more efficient algorithm can be formulated. Although the new algorithm does not lend itself to parallelization in a straightforward way, we give an outlook how parallel computing facilities can be exploited to obtain good solutions faster...|$|R
40|$|We {{establish}} a fundamental {{result in the}} theory of computation by continuous-time dynamical systems, by showing that systems corresponding to so called continuous-time symmetric Hopfield nets are capable of general computation. As is well known, such networks have very constrained, Liapunov-function controlled dynamics. Nevertheless, we show that they are universal and efficient computational devices, in the sense that any convergent synchronous fully parallel computation by a recurrent network of n discrete-time binary neurons, with in general asymmetric coupling weights, can be simulated by a symmetric continuous-time <b>Hopfield</b> <b>net</b> containing only 18 n+ 7 units employing the saturated-linear activation function. Moreover, if the asymmetric network has maximum integer weight size w_max and converges in discrete time t*, then the corresponding <b>Hopfield</b> <b>net</b> can be designed to operate in continuous time &Theta;(t*/&epsilon;), for any &epsilon; > 0 [...] ...|$|E
40|$|Implementation of the <b>Hopfield</b> <b>net</b> {{which is}} used in the image {{processing}} type of applications where only partial information about the image may be available is discussed. The image classification type of algorithm of Hopfield and other learning algorithms, such as the Boltzmann machine and the back-propagation training algorithm, have many vital applications in space...|$|E
40|$|Artificial Intelligence Lab, Department of MIS, University of ArizonaThis paper {{presents}} a neural network approach to document semantic indexing. A <b>Hopfield</b> <b>net</b> algorithm {{was used to}} simulate human associative memory for concept exploration {{in the domain of}} computer science and engineering. INSPEC, a collection of more than 320, 000 document abstracts from leading journals, was used as the document testbed. Benchmark tests confirmed that three parameters (maximum number of activated nodes, E - maximum allowable error, and maximum number of iterations) were useful in positively influencing network convergence behavior without negatively impacting central processing unit performance. Another series of benchmark tests was performed to determine the effectiveness of various filtering techniques in reducing the negative impact of noisy input terms. Preliminary user tests confirmed our expectation that the <b>Hopfield</b> <b>net</b> algorithm is potentially useful as an associative memory technique to improve document recall and precision by solving discrepancies between indexer vocabularies and end-user vocabularies...|$|E
5000|$|Boltzmann {{machines}} {{can be seen}} as the stochastic, generative {{counterpart of}} <b>Hopfield</b> <b>nets.</b> They were one of the first neural networks capable of learning internal representations, and are able to represent and (given sufficient time) solve difficult combinatoric problems.|$|R
5000|$|A Boltzmann machine, like a Hopfield network, is {{a network}} of units with an [...] "energy" [...] defined for the overall network. Its units produce binary results. Unlike <b>Hopfield</b> <b>nets,</b> Boltzmann machine units are stochastic. The global energy, , in a Boltzmann machine is {{identical}} in form {{to that of a}} Hopfield network: ...|$|R
40|$|We {{establish}} a fundamental {{result in the}} theory of continuous-time neural computation, by showing that so called continuous-time symmetric <b>Hopfield</b> <b>nets,</b> whose asymptotic convergence is always guaranteed by the existence of a Liapunov function may, in the worst case, possess a transient period that is exponential in the network size. The result stands in contrast to e. g. the use of such network models in combinatorial optimization applications...|$|R
40|$|This paper proposes quantum image reconstruction. Input-triggered {{selection}} {{of an image}} among many stored ones, and its reconstruction if the input is occluded or noisy, has been simulated by a computer program implementable in a real quantum-physical system. It {{is based on the}} Hopfield associative net; the quantumwave implementation bases on holography. The main limitations of the classical <b>Hopfield</b> <b>net</b> are much reduced with the new, original – quantum-optical – implementation. Image resolution can be almost arbitrarily increased. ...|$|E
40|$|This {{thesis is}} about {{comparison}} of libraries of artificial neural networks. Basic theory of neuron, neural networks and their learning algorithms are explained here. Multilayer perceptron, Self organizing map and <b>Hopfield</b> <b>net</b> are chosen for experiments. Criteria of comparison such as licence, community or last actualization are designed. Approximation of function, association and clustering are chosen as task for experiments. After that, there is implementation of applications using chosen libraries. At the end, result of comparison and experiment are evaluated...|$|E
40|$|We {{introduce}} {{a method for}} the efficient design of a Boltzmann machine (or a <b>Hopfield</b> <b>net)</b> that computes an arbitrary given Boolean function /. TÈis method {{is based on an}} efficient simulation of acyclic circuits with threshold gates by Boltzmann machines. As a consequence u¡e can show that various concrete Boolean functions f tha | are relevant for classification problems can be computed by scalable Boltzmann machines that are gúaranteed to converge to their global maximum configuration with high probability after constantly many steps. ...|$|E
50|$|Bidirectional {{associative}} memory (BAM) {{is a type}} of recurrent neural network. BAM was introduced by Bart Kosko in 1988. There are two types of {{associative memory}}, auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms of associative memory. However, <b>Hopfield</b> <b>nets</b> return patterns of the same size.|$|R
50|$|A Hopfield {{network is}} a form of {{recurrent}} artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. <b>Hopfield</b> <b>nets</b> serve as content-addressable memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum, but will sometimes converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum). Hopfield networks also provide a model for understanding human memory.|$|R
40|$|Combinatorial {{optimization}} is {{an active}} field of research in Neural Networks. Since the first attempts to solve the travelling salesman problem with <b>Hopfield</b> <b>nets</b> several progresses have been made. I will present some Neural Network approximate solutions for NP-complete problems that have a sound mathematical foundation and that, beside their theoretical interest, are also numerically encouraging. These algorithms easily deal with problems with thousands of instances taking Neural Network approaches out of the "toy-problem" era...|$|R
40|$|This paper, {{written for}} {{interdisciplinary}} audience, presents computational image reconstruction implementable by quantum optics. The input-triggered {{selection of a}} high-resolution image among many stored ones, and its reconstruction if the input is occluded or noisy, has been successfully simulated. The original algorithm, based on the Hopfield associative neural net, was transformed in order to enable its quantum-wave implementation based on holography. The main limitations of the classical <b>Hopfield</b> <b>net</b> are much reduced with the simulated new quantum-optical implementation. (C) 2004 Optical Society of America...|$|E
40|$|Abstract:- In {{this work}} we survey the Hopfield neural network, {{introduction}} of which rekindled {{interest in the}} neural networks through the work of Hopfield and others. <b>Hopfield</b> <b>net</b> has many interesting features, applications, and implementations and it comes in two flavors, digital and analog. A brief review of the model oriented towards pattern recognition is also considered. Some interesting variations of the network or neuron model are noted which are being considered by researchers may lead to better performance or overcome problems such as capacity of the network...|$|E
40|$|Abstract Symmetrically {{connected}} recurrent {{networks have}} recently beenused as models {{of a host}} of neural computations. However, because of the separation between excitation and inhibition, biolog-ical neural networks are asymmetrical. We study characteristic differences between asymmetrical networks and their symmetri-cal counterparts, showing that they have dramatically different dynamical behavior and also how the differences can be exploitedfor computational ends. We illustrate our results {{in the case of a}} network that is a selective amplifier. 1 Introduction A large class of non-linear recurrent networks, including those studied byGrossberg, 9 the <b>Hopfield</b> <b>net,</b> 10, 11 and many more recent proposals for th...|$|E
40|$|By {{exploring}} {{the structure and}} {{the dynamics of the}} neural net proposed in [1] recently for the computation of DFT, it is possible to reduce this neural net to a statical conductor array followed by a single row of <b>Hopfield</b> <b>nets</b> with local feedback only. Some modification is also described for better practical implementation. The modified circuit will compute the discrete Hartley transform with the same precision as claimed in the paper, {{while at the same time}} achieve structural modularity which is required to design chips for transforming large number of samples. status: publishe...|$|R
40|$|Three {{different}} methods for handwritten block letter recognition have been compared: <b>hopfield</b> <b>nets,</b> backpropagation nets {{and a simple}} classical method (MinHD). Real-time recognition with a recognition goodness higher than 95 % could be achieved only by the backpropagation network. Goodness could be further improved by installing suitable filters. Although the hopfield network possesses a shorter learning process, it works much slower and its capacity is much lower than {{in the case of}} the backpropagation system. (WEN) Available from TIB Hannover: RO 9514 (92 - 04) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|We {{establish}} a fundamental {{result in the}} theory of computation by continuous-time dynamical systems, by showing that systems corresponding to so called continuous-time symmetric <b>Hopfield</b> <b>nets</b> are capable of general computation. More precisely, we prove that any function computed by a discrete-time asymmetric recurrent network of n threshold gates can also be computed by a continuous-time symmetrically-coupled Hopfield system of dimension 18 n + 7. Moreover, if the threshold logic network has maximum weight wmax and converges in discrete time t, then the corresponding Hopfield system can be designed to operate in continuous time (t ="), for any value 0 < " < 0 : 0025 such that wmax 2 3 n " 2 1 =". The result appears at rst sight counterintuitive, because the dynamics of any symmetric Hopfield system is constrained by a Liapunov, or energy function defined on its state space. In particular, such a system always converges from any initial state towards some stable equilibrium state, and hence cannot exhibit nondamping oscillations, i. e. strictly speaking cannot simulate even a single alternating bit. However, we show that if one only considers terminating computations, then the Liapunov constraint can be overcome, and one can in fact embed arbitrarily complicated computations in the dynamics of Liapunov systems with only a modest cost in the system's dimensionality. In terms of standard discrete computation models, our result implies that any polynomially space-bounded Turing machine can be simulated by a family of polynomial-size continuous-time symmetric <b>Hopfield</b> <b>nets...</b>|$|R
