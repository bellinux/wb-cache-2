1959|2045|Public
2500|$|Here, [...] is the {{exponential}} integral, [...] is {{the generalized}} exponential integral, [...] is the <b>error</b> <b>function,</b> and [...] is the complementary <b>error</b> <b>function,</b> [...]|$|E
2500|$|... and {{integrals}} thereof, {{such as the}} <b>error</b> <b>function.</b> There {{are many}} interrelations between these functions and the gamma function; notably, [...] obtained by evaluating [...] is the [...] "same" [...] as that found in the normalizing factor of the <b>error</b> <b>function</b> and the normal distribution.|$|E
2500|$|... where [...] is the {{imaginary}} <b>error</b> <b>function.</b> The {{moment generating function}} is given by ...|$|E
40|$|An {{important}} theoretical tool in {{machine learning}} is the bias/variance decomposition of the generalization error. It was introduced for the {{mean square error}} in [3]. The bias/variance decomposition includes {{the concept of the}} average predictor. The bias is the error of the average predictor, and the systematic part of the generalization error, while the variability around the average predictor is the variance. We present a large group of <b>error</b> <b>functions</b> with the same desirable properties as the bias/variance decomposition in [3]. The <b>error</b> <b>functions</b> are derived from the exponential family of distributions via the statistical deviance measure. We prove that this family of <b>error</b> <b>functions</b> contains all <b>error</b> <b>functions</b> decomposable in that manner. We state the connection between the bias/variance decomposition and the ambiguity decomposition [7] and present a useful approximation of ambiguity that is quadratic in the ensemble coefficients. 1 Notation and problem domain The problem domai [...] ...|$|R
5000|$|... g2o: General Graph Optimization (C++) - {{framework}} with solvers for sparse graph-based non-linear <b>error</b> <b>functions.</b> LGPL.|$|R
3000|$|... versus SNR for {{the best}} {{detectors}} obtained with the different <b>error</b> <b>functions,</b> for the first case study.|$|R
2500|$|The {{solution}} to the above problem can be written [...] in terms of complementary <b>error</b> <b>function</b> ...|$|E
2500|$|... where [...] is the <b>error</b> <b>function.</b> The {{proportion}} that is {{less than}} or equal to a number, x, is given by the cumulative distribution function: ...|$|E
2500|$|For example, {{dividing}} the IQR by [...] (using the <b>error</b> <b>function)</b> {{makes it an}} unbiased, consistent estimator for the population variance if the data follow a normal distribution.|$|E
30|$|Equation (39) {{guarantees}} {{a finite}} time convergence of the tracking <b>error</b> <b>functions</b> [12] {{as defined in}} Eq. (25).|$|R
3000|$|... {{denote the}} <b>error</b> <b>functions.</b> The {{following}} main theorem reveals the convergence {{results of the}} presented scheme in the weighted [...]...|$|R
40|$|We {{introduce}} some New Quadrature Formulas {{by using}} Jacoby polynomials and Laguerre polynomials. These formulas {{can be obtained}} for a finite and infinite interval and also separately for the even or odd order of derivatives. By using the properties of <b>error</b> <b>functions</b> of the above orthogonal polynomials we can obtain the <b>error</b> <b>functions</b> for these formulas. Application of the new approaches increases their precision degrees. Finally, some examples are given to illuminate the details...|$|R
2500|$|In {{statistics}} one often {{uses the}} related <b>error</b> <b>function,</b> or , {{defined as the}} probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range that is ...|$|E
5000|$|... where erf and erfc {{denote the}} <b>error</b> <b>function</b> and the {{complementary}} <b>error</b> <b>function,</b> respectively.|$|E
50|$|The Dawson {{function}} is the one-sided Fourier-Laplace sine transform of the Gaussian function,It {{is closely related}} to the <b>error</b> <b>function</b> erf, aswhere erfi is the imaginary <b>error</b> <b>function,</b> erfi(x) = &minus;i erf(ix). Similarly,in terms of the real <b>error</b> <b>function,</b> erf.|$|E
40|$|Theta {{functions}} for definite signature lattices constitute {{a rich source}} of modular forms. A natural question is then their generalization to indefinite signature lattices. One way to ensure a convergent theta series while keeping the holomorphicity property of definite signature theta series is to restrict the sum over lattice points to a proper subset. Although such series do not have the modular properties that a definite signature theta function has, as shown by Zwegers for signature $(1,n- 1) $ lattices, they can be completed to a function that has these modular properties by compromising on the holomorphicity property in a certain way. This construction has recently been generalized to signature $(2,n- 2) $ lattices by Alexandrov, Banerjee, Manschot, and Pioline. A crucial ingredient in this work is the notion of double <b>error</b> <b>functions</b> which naturally lends itself to generalizations to higher dimensions. In this work we study the properties of such higher dimensional <b>error</b> <b>functions</b> which we will call $r$-tuple <b>error</b> <b>functions.</b> We then construct an indefinite theta series for signature $(r,n-r) $ lattices and show they can be completed to modular forms by using these $r$-tuple <b>error</b> <b>functions.</b> Comment: 22 page...|$|R
50|$|Happ {{generalized}} the Shannon {{formula for}} topologically closed systems. The Shannon-Happ formula {{can be used}} for deriving transfer <b>functions,</b> sensitivities, and <b>error</b> <b>functions.</b>|$|R
5000|$|... #Caption: Graph of {{generalised}} <b>error</b> <b>functions</b> En(x):grey curve: E1(x) = (1 &minus; e &minus;x)/red curve: E2(x) = erf(x)green curve: E3(x)blue curve: E4(x)gold curve: E5(x).|$|R
5000|$|The Q-function can be {{expressed}} in terms of the <b>error</b> <b>function,</b> or the complementary <b>error</b> <b>function,</b> as ...|$|E
5000|$|A useful {{asymptotic}} {{expansion of the}} complementary <b>error</b> <b>function</b> (and therefore also of the <b>error</b> <b>function)</b> for large real x is ...|$|E
5000|$|Here, [...] is the {{exponential}} integral, [...] is {{the generalized}} exponential integral, [...] is the <b>error</b> <b>function,</b> and [...] is the complementary <b>error</b> <b>function,</b> [...]|$|E
50|$|The {{error and}} {{complementary}} <b>error</b> <b>functions</b> occur, for example, in solutions {{of the heat}} equation when boundary conditions are given by the Heaviside step function.|$|R
40|$|The {{decomposition}} of a generalization error into a bias and a variance term {{is of great}} importance. So is the {{decomposition of}} <b>error</b> <b>functions</b> into an <b>error</b> and an ambiguity term. A fact not generally recognized is that these two decomposition are intimately connected. For both decompositions a central concept is the predictor". In this paper we will discuss {{the connection between the}} bias/variance decomposition and the error/ambiguity decomposition. We will describe two decompositions of <b>error</b> <b>functions.</b> The rst decomposition gives target independent variance for all <b>error</b> <b>functions</b> derived from the exponential family of distributions, including the mean square error (MSE). The second decomposition is more general, but lacks connection to the exponential family while retaining the desirable property. The average predictor is the linear opinion pool (LOP) in both new decompositions. 1 Introduction The decomposition of a generalization error into bias/variance and the decomp [...] ...|$|R
40|$|Backpropagation (BP) Neural Network (NN) <b>error</b> <b>functions</b> {{enable the}} mapping of data vectors to {{user-defined}} classifications by driving weight matrix modifications {{so as to}} reduce classification error over the training data set. Conventional BP <b>error</b> <b>functions</b> are usually only implicitly dependant on the weight matrix, however an explicit penalty term can be added so as to force numerically insignificant weights closer to zero. In our investigation, BP training is undertaken {{as a prelude to}} a pruning stage that selectively removes functionally unimportant weight matrix elements, thereby resulting in sparser network connectivity more suited for subsequent rule extraction. This paper investigates the usage of several <b>error</b> and activation <b>functions</b> in the effort to produce maximally clean network connections...|$|R
5000|$|When the <b>error</b> <b>function</b> is {{evaluated}} for arbitrary complex arguments z, the resulting complex <b>error</b> <b>function</b> is usually discussed in scaled form as the Faddeeva function: ...|$|E
5000|$|Consequently, the <b>error</b> <b>function</b> is also {{closely related}} to the Q-function, which is the tail {{probability}} of the standard normal distribution. The Q-function can be expressed in terms of the <b>error</b> <b>function</b> as ...|$|E
5000|$|In mathematics, the <b>error</b> <b>{{function}}</b> (also {{called the}} Gauss <b>error</b> <b>function)</b> {{is a special}} function (non-elementary) of sigmoid shape that occurs in probability, statistics, and partial differential equations describing diffusion. It is defined as: ...|$|E
3000|$|... -QAM (square and rectangular) constellations over {{additive}} white Gaussian noise (AWGN) and fading channels. Over the AWGN channel, these expressions can {{be described}} by a weighted sum of complementary <b>error</b> <b>functions.</b>|$|R
40|$|Abstract. This paper investigates a new special {{function}} {{referred to}} as the <b>error</b> zeta <b>function.</b> Derived as a fractional generalization of hypergeometric zeta <b>functions,</b> the <b>error</b> zeta <b>function</b> is shown to exhibit many properties analogous to its hypergeometric counterpart. These new properties are treated in detail, including an intimate connection to generalized Bernoulli numbers and a pre-functional equation satisfied by the <b>error</b> zeta <b>function.</b> 1...|$|R
30|$|Zhang, et al. [54], {{established}} the thermal <b>error</b> transfer <b>function</b> of each {{object of the}} machine tool based on the heat transfer mechanism. Then, based on the assembly dimension chain principle, the thermal <b>error</b> transfer <b>function</b> of the whole machine tool was obtained. As the thermal <b>error</b> transfer <b>function</b> can be deduced using Laplace transform, the thermal error characteristic of the machine tool can be studied with both time domain and frequency domain methods. Taking the environmental temperature fluctuations as input, based on the thermal <b>error</b> transfer <b>function,</b> the environmental temperature induced thermal error can be obtained.|$|R
5000|$|The {{gradient}} descent method involves calculating the derivative of the squared <b>error</b> <b>function</b> {{with respect to}} the weights of the network. This is normally done using backpropagation. Assuming one output neuron, the squared <b>error</b> <b>function</b> is: ...|$|E
5000|$|Two {{assumptions}} must be {{made about}} {{the form of the}} <b>error</b> <b>function.</b> The first is that it can be written as an average [...] over error functions , for individual training examples, [...] The reason for this assumption is that the backpropagation algorithm calculates the gradient of the <b>error</b> <b>function</b> for a single training example, which needs to be generalized to the overall <b>error</b> <b>function.</b> The second assumption is that it can be written {{as a function of the}} outputs from the neural network.|$|E
50|$|E2(x) is the <b>error</b> <b>function,</b> erf(x).|$|E
40|$|A general error {{analysis}} of three recently developed multi-conic methods of three-body trajectory integration {{has been carried}} out. Single-step <b>error</b> <b>functions</b> for position and velocity have been derived as Taylor series in powers of the time step and also in integral form. These <b>error</b> <b>functions</b> are used to investigate the relative accuracy of the three methods in various regions of the earth-moon space {{and to provide a}} method of variable step size control for the trajectory integration procedure. Numerical results are used to compare the multi-step performance of the methods for both large and small step sizes...|$|R
40|$|The paper compares Artificial Neural Network (ANN) model against {{traditional}} {{models in}} the modeling of population and external migration for Fiji population components during the years from 1986 to 2012. The performance of the various models used {{are based on the}} values of the various <b>error</b> <b>functions</b> such as the R-squared (R 2), Root Square Mean Error (RSME), Mean Absolute Error (MAE), Standard Error of Regression (SER), Sum Squared Residual (SSR), Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE). Across yearly time series, ANN performed better than those traditional models, when comparing the various <b>error</b> <b>functions</b> used...|$|R
40|$|Average linear approximations for smooth {{functions}} using empirical and Gaussian probabilities are introduced. The {{convergence of}} these approximations {{is shown to}} be uniform, {{in the sense of}} the Weierstrass approximation theorem. Average linear approximations with prearranged <b>error</b> <b>functions</b> are finally studied...|$|R
