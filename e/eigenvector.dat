5213|8054|Public
5|$|Matrix {{calculations}} can be often {{performed with}} different techniques. Many {{problems can be}} solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix {{can be obtained by}} finding a sequence of vectors xn converging to an <b>eigenvector</b> when n tends to infinity.|$|E
5|$|In the mathematically {{rigorous}} {{formulation of}} quantum mechanics, developed by John von Neumann, the possible states (more precisely, the pure states) of a quantum mechanical system {{are represented by}} unit vectors (called state vectors) residing in a complex separable Hilbert space, known as the state space, well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projectivization of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space {{is dependent on the}} system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all square-integrable functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of spinors. Each observable is represented by a self-adjoint linear operator acting on the state space. Each eigenstate of an observable corresponds to an <b>eigenvector</b> of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate.|$|E
25|$|While the {{definition}} of an <b>eigenvector</b> used in this article excludes the zero vector, {{it is possible to}} define eigenvalues and eigenvectors such that the zero vector is an <b>eigenvector.</b>|$|E
5000|$|Matrix V {{denotes the}} matrix of right <b>eigenvectors</b> (as opposed to left <b>eigenvectors).</b> In general, the matrix of right <b>eigenvectors</b> {{need not be}} the (conjugate) {{transpose}} of the matrix of left <b>eigenvectors.</b>|$|R
30|$|Ignore <b>eigenvectors</b> {{associated}} with non-positive eigenvalues. After normalizing the remaining <b>eigenvectors,</b> only consider those <b>eigenvectors,</b> which are {{associated with}} the smallest k− 1 eigenvalues.|$|R
50|$|Furthermore, the <b>eigenvectors</b> of j, s, mj and parity, {{which are}} also <b>eigenvectors</b> of the Hamiltonian, are linear {{combinations}} of the <b>eigenvectors</b> of ℓ, s, mℓ and ms.|$|R
25|$|The {{principal}} <b>eigenvector</b> is used {{to measure}} the centrality of its vertices. An example is Google's PageRank algorithm. The principal <b>eigenvector</b> of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest <b>eigenvector</b> can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.|$|E
25|$|<b>Eigenvector</b> {{centrality}} (also called eigencentrality) is {{a measure}} of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. Google's PageRank and the Katz centrality are variants of the <b>eigenvector</b> centrality.|$|E
25|$|If an {{observable}} {{is measured}} {{and the result}} is a certain eigenvalue, the corresponding <b>eigenvector</b> is the state of the system immediately after the measurement. The act of measurement in matrix mechanics 'collapses' the state of the system. If one measures two observables simultaneously, the state of the system collapses to a common <b>eigenvector</b> of the two observables. Since most matrices don't have any eigenvectors in common, most observables can never be measured precisely at the same time. This is the uncertainty principle.|$|E
3000|$|... {{contains}} the critical <b>eigenvectors</b> at Hopf bifurcation. Symmetry implies that generically the critical <b>eigenvectors</b> are either in one subspace {{or the other}} [5]. Symmetry-preserving Hopf bifurcations (with critical <b>eigenvectors</b> in [...]...|$|R
5000|$|There are {{no other}} {{positive}} (moreover non-negative) <b>eigenvectors</b> except positive multiples of v (respectively, left <b>eigenvectors</b> except w), i.e., all other <b>eigenvectors</b> must {{have at least one}} negative or non-real component.|$|R
50|$|The <b>eigenvectors</b> of A−1 are {{the same}} as the <b>eigenvectors</b> of A.|$|R
25|$|Once we have {{determined}} {{the number of}} generalized eigenvectors of each rank that a canonical basis has, we can obtain the vectors explicitly (see generalized <b>eigenvector).</b>|$|E
25|$|Because of {{the large}} eigengap of the {{modified}} adjacency matrix above, {{the values of the}} PageRank <b>eigenvector</b> can be approximated to within a high degree of accuracy within only a few iterations.|$|E
25|$|The {{geometric}} multiplicity γ'T(λ) of an eigenvalue λ is {{the dimension}} of the eigenspace associated with λ, i.e., {{the maximum number of}} linearly independent eigenvectors associated with that eigenvalue. By the definition of eigenvalues and eigenvectors, γ'T(λ) ≥ 1 because every eigenvalue has at least one <b>eigenvector.</b>|$|E
5000|$|Knowing all {{eigenvalues}} and <b>eigenvectors</b> of the factors, all eigenvalues and <b>eigenvectors</b> of the Kronecker product can be explicitly calculated. Based on this, eigenvalues and <b>eigenvectors</b> of the Kronecker sumcan also be explicitly calculated.|$|R
3000|$|... [...]), and <b>eigenvectors</b> are {{the feature}} vectors of Cov. {{eigenvalues}} {{is the value}} of <b>eigenvectors.</b> Bigger eigenvalues mean greater contribution of original data. Therefore, top n <b>eigenvectors</b> will be selected as new data of BS.|$|R
5000|$|... where [...] is {{the number}} of stimuli [...] used during the experiment. The <b>eigenvectors</b> of [...] {{associated}} to significantly positive eigenvalues correspond to excitatory vectors, whereas <b>eigenvectors</b> associated to significantly negative eigenvalues are inhibitory <b>eigenvectors.</b>|$|R
25|$|Because of the {{definition}} of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated <b>eigenvector.</b> Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.|$|E
25|$|Since X is a Hermitian matrix, {{it should}} be diagonalizable, {{and it will be}} clear from the {{eventual}} form of P that every real number can be an eigenvalue. This makes some of the mathematics subtle, since there is a separate <b>eigenvector</b> for every point in space.|$|E
25|$|This {{equation}} {{is called the}} eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the <b>eigenvector</b> v. Note that T(v) {{is the result of}} applying the transformation T to the vector v, while λv is the product of the scalar λ with v.|$|E
5000|$|That is, φy are the <b>eigenvectors</b> of P. If the <b>eigenvectors</b> are {{normalized}} so that ...|$|R
500|$|That is, φ'y are the <b>eigenvectors</b> of P. [...] If the <b>eigenvectors</b> are {{normalized}} so that ...|$|R
50|$|Other <b>eigenvectors</b> {{should contain}} negative, or complex components. Since <b>eigenvectors</b> for {{different}} eigenvalues are orthogonal in some sense, but two positive <b>eigenvectors</b> cannot be orthogonal, {{so they must}} correspond to the same eigenvalue, but the eigenspace for the Perron-Frobenius is one-dimensional.|$|R
25|$|Small {{errors in}} floating-point {{arithmetic}} can grow when mathematical algorithms perform operations {{an enormous number}} of times. A few examples are matrix inversion, <b>eigenvector</b> computation, and differential equation solving. These algorithms must be very carefully designed, using numerical approaches such as Iterative refinement, if they are to work well.|$|E
25|$|As for Hermitian matrices, the {{key point}} is to prove the {{existence}} {{of at least one}} nonzero <b>eigenvector.</b> To prove this, we cannot rely on determinants to show existence of eigenvalues, but instead one can use a maximization argument analogous to the variational characterization of eigenvalues. The above spectral theorem holds for real or complex Hilbert spaces.|$|E
25|$|In mathematics, {{spectral}} {{theory is}} an inclusive term for theories extending the <b>eigenvector</b> and eigenvalue theory {{of a single}} square matrix to a much broader theory {{of the structure of}} operators in a variety of mathematical spaces. It is a result of studies of linear algebra and the solutions of systems of linear equations and their generalizations. The theory is connected to that of analytic functions because the spectral properties of an operator are related to analytic functions of the spectral parameter.|$|E
40|$|The {{existence}} of differentiable eigenvalues and <b>eigenvectors</b> {{for a general}} matrix is addressed. The eigenspace which contains differentiable <b>eigenvectors</b> is determined and computed by using the concept of subspace intersection {{in conjunction with the}} singular value decomposition algorithm. The differentiable <b>eigenvectors</b> associated with repeated eigenvalues should be simultaneously the <b>eigenvectors</b> of the general matrix and its corresponding sensitivity matrix. Furthermore, the derivatives for differentiable <b>eigenvectors</b> associated with repeated eigenvalues can be computed using higher order derivatives of the matrix, whereas the corresponding eigenvalue derivatives are the eigenvalues of the sensitivity matrix...|$|R
40|$|AbstractFor generic {{values of}} q, all the <b>eigenvectors</b> of the {{transfer}} matrix of the Uqsl(2) -invariant open spin- 1 / 2 XXZ chain with finite length N can be constructed using the algebraic Bethe ansatz (ABA) formalism of Sklyanin. However, when q is a root of unity (q=eiπ/p with integer p≥ 2), the Bethe equations acquire continuous solutions, and the transfer matrix develops Jordan cells. Hence, there appear <b>eigenvectors</b> of two new types: <b>eigenvectors</b> corresponding to continuous solutions (exact complete p-strings), and generalized <b>eigenvectors.</b> We propose general ABA constructions for these two new types of <b>eigenvectors.</b> We present many explicit examples, and we construct complete sets of (generalized) <b>eigenvectors</b> for various values of p and N...|$|R
50|$|In {{the case}} of {{degenerate}} eigenvalues (an eigenvalue appearing more than once), the <b>eigenvectors</b> have an additional freedom of rotation, i.e. any linear (orthonormal) combination of <b>eigenvectors</b> sharing an eigenvalue (i.e. in the degenerate sub-space), are themselves <b>eigenvectors</b> (i.e. in the subspace).|$|R
25|$|Conversely, {{suppose a}} matrix A is diagonalizable. Let P be a {{non-singular}} square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P {{must therefore be}} an <b>eigenvector</b> of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis {{if and only if}} A is diagonalizable.|$|E
25|$|On one hand, {{this set}} is {{precisely}} the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an <b>eigenvector</b> of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ. In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace {{is that it is}} a linear subspace, so E is a linear subspace of ℂn.|$|E
25|$|In quantum mechanics, and in {{particular}} in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term <b>eigenvector</b> is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.|$|E
5000|$|Every n × n matrix [...] {{possesses}} n linearly independent generalized <b>eigenvectors.</b> Generalized <b>eigenvectors</b> {{corresponding to}} distinct eigenvalues are linearly independent. If [...] is an eigenvalue of [...] of algebraic multiplicity , then [...] will have [...] linearly independent generalized <b>eigenvectors</b> corresponding to [...]|$|R
40|$|In {{a recent}} paper [4] we {{have studied the}} problem whether the period map of a {{periodic}} delay equation has a complete span of <b>eigenvectors</b> and generalized <b>eigenvectors.</b> There is an abstract theory (see [4] and also [3]) {{that can be used}} to verify whether the <b>eigenvectors</b> and generalized <b>eigenvectors</b> corresponding to the nonzero spectrum of a compact operator from a given class of operators are complete. To use the abstract results one needs good estimates for the resolvent operator near infinity and to compute the resolvent explicitly one often has to solve a boundary value problem. In this paper we first give an abstract theorem and then we discuss some explicit examples. 1. A Result about Completeness Let H be a complex Hilbert space and let T : H ! H be a compact operator. Let E T denote the span of the <b>eigenvectors</b> and generalized <b>eigenvectors</b> corresponding to the nonzero eigenvalues of T. If E T is dense we call the system of <b>eigenvectors</b> and generalized <b>eigenvectors</b> complete [...] . ...|$|R
2500|$|Every n × n matrix [...] {{possesses}} n linearly independent generalized <b>eigenvectors.</b> [...] Generalized <b>eigenvectors</b> {{corresponding to}} distinct eigenvalues are linearly independent. [...] If [...] is an eigenvalue of [...] of algebraic multiplicity , then [...] will have [...] linearly independent generalized <b>eigenvectors</b> corresponding to [...]|$|R
