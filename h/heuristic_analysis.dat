225|291|Public
50|$|Metamorphism {{does not}} protect a virus against <b>heuristic</b> <b>analysis.</b>|$|E
5000|$|... ==Effectiveness== <b>Heuristic</b> <b>analysis</b> {{is capable}} of {{detecting}} many previously unknown viruses and new variants of current viruses. However, <b>heuristic</b> <b>analysis</b> operates {{on the basis of}} experience (by comparing the suspicious file to the code and functions of known viruses). This means it is likely to miss new viruses that contain previously unknown methods of operation not found in any known viruses. Hence, the effectiveness is fairly low regarding accuracy and the number of false positives.|$|E
50|$|As new viruses are {{discovered}} by human researchers, information {{about them is}} added to the <b>heuristic</b> <b>analysis</b> engine, thereby providing the engine the means to detect new viruses.|$|E
5000|$|Intelligent Message Filter: Initially a free Microsoft add-on, later, part {{of service}} pack 2, that uses <b>heuristic</b> message <b>analysis</b> to block {{messages}} or direct {{them to the}} [...] "Junk E-Mail" [...] folder in Microsoft Outlook clients.|$|R
40|$|The {{effectiveness}} of an incomplete LU (ILU) factorization as a preconditioner for the conjugate gradient method {{can be highly}} dependent on the ordering of the matrix rows during its creation. Detailed justification for two heuristics commonly used in matrix ordering for anisotropic problems is given. The bandwidth reduction and weak connection following heuristics are implemented through an ordering method based on eigenvector computations. This spectral ordering is {{shown to be a}} good representation of the <b>heuristics.</b> <b>Analysis</b> and test cases in two and three dimensional diffusion problems demonstrate when ordering is important, and when an ILU decomposition will be ordering insensitive. The applicability of the heuristics is thus evaluated and placed on a more rigorous footing...|$|R
40|$|Two {{improvements}} to current requirements analysis practices are suggested: domain modeling, and the systematic application of <b>analysis</b> <b>heuristics.</b> Domain modeling is {{the representation of}} relevant application knowledge prior to requirements specification. Artificial intelligence techniques may eventually be applicable for domain modeling. In the short term, however, restricted domain modeling techniques, such as that in JSD, will still be of practical benefit. <b>Analysis</b> <b>heuristics</b> are standard patterns of reasoning about the requirements. They usually generate questions of clarification or issues relating to completeness. <b>Analysis</b> <b>heuristics</b> can be represented and therefore systematically applied in an issue-based framework. This is illustrated by an issue-based analysis of JSD's domain modeling and functional specification heuristics. They {{are discussed in the}} context of the preliminary design of simple embedded systems...|$|R
5000|$|<b>Heuristic</b> <b>analysis</b> is {{a method}} {{employed}} by many computer antivirus programs designed to detect previously unknown computer viruses, {{as well as new}} variants of viruses already in the [...] "wild".|$|E
50|$|Remsoft’s {{technology}} uses {{a combination}} of Metamodeling, Business process modeling and <b>Heuristic</b> <b>analysis</b> to analyze variables and arrive at preferred outcomes. The most recent updates to its technology platform were released in March 2011.|$|E
50|$|<b>Heuristic</b> <b>{{analysis}}</b> is {{an expert}} based analysis that determines the susceptibility of a system towards particular threat/risk using various decision rules or weighing methods. MultiCriteria analysis (MCA) {{is one of the}} means of weighing. This method differs from statistical analysis, which bases itself on the available data/statistics.|$|E
40|$|The paper {{presents}} {{a new language}} processing toolkit developed at Adam Mickiewicz University. Its functionality includes currently tokenization, sentence splitting, dictionary-based- morphological <b>analysis,</b> <b>heuristic</b> morphological <b>analysis</b> of unknown words, spelling correction, pattern search, and generation of concordances. It is organized {{as a collection of}} command-line programs, each performing one operation. The components may be connected in various ways to provide various text processing services. Also new user-defined components may be easily incorporated into the system. The toolkit is destined for processing raw (not annotated) text corpora. The system was originally intended for Polish, but its adaptation to other languages is possible. 1...|$|R
5000|$|LibX {{supports}} a [...] "magic search" [...] functionality, which creatively integrates {{use of the}} Google Scholar service. LibX searches Scholar, performs a <b>heuristic</b> similarity <b>analysis</b> {{to determine if a}} match was found, then redirects the user to the user's OpenURL resolver to obtain an accessible copy.|$|R
40|$|AbstractA novel <b>heuristic</b> {{residual}} <b>analysis</b> {{is proposed}} to derive a computationally cost-effective residual projection operator in multigrid with the five-point Red-Black Gauss-Seidel relaxation for solving the two-dimensional Poisson equation. This optimal residual injection operator is as cheap as the trivial injection operator, but {{is more efficient}} than the costly full-weighting operator and achieves near-optimal convergence rate...|$|R
50|$|The book {{includes}} two appendices. In the first, Lakatos gives {{examples of the}} heuristic process in mathematical discovery. In the second, he contrasts the deductivist and heuristic approaches and provides <b>heuristic</b> <b>analysis</b> of some 'proof generated' concepts, including uniform convergence, bounded variation, and the Carathéodory definition of a measurable set.|$|E
50|$|Another common {{method of}} <b>heuristic</b> <b>analysis</b> {{is for the}} {{anti-virus}} program to decompile the suspicious program, then analyze the source code contained within. The source code of the suspicious file is compared to the source code of known viruses and virus-like activities. If {{a certain percentage of}} the source code matches with the code of known viruses or virus-like activities, the file is flagged, and the user alerted.|$|E
50|$|There are {{different}} types of anti-virus (or scanner), {{one of them is}} the <b>heuristic</b> <b>analysis</b> anti-virus which interpretes or emulates macros.Indeed, to examine all of the branches of macros require a NP-complete complexity (using backtracking), so in this case, the analysis of one document (which contains macros) would take too much time. Interpreting or emulating a macro would lead to either false positives errors or macro viruses not detected.|$|E
5000|$|McClure {{stated that}} Cylance's {{antivirus}} product {{does not use}} typical security features, such as unique signatures, <b>heuristics,</b> behavioral <b>analysis,</b> sandboxing, virtualization, or blacklisting. Rather, the product uses artificial intelligence to identify and stop attackers. McClure stated that Cylance security features {{are similar to the}} human brain's identifying of threats, wherein it [...] "teaches" [...] computers to identify indicators of an attack.|$|R
40|$|FIGURE 3. Majority-rule {{consensus}} of> 56, 000 equally most-parsimonious trees {{obtained through}} a <b>heuristic</b> parsimony <b>analysis</b> of nonmolecular characters described by Jansa and Voss (2005), updated by inclusion of M. andersoni. The “ caluromyine ” outgroups (Glironia venusta, Caluromysiops irrupta, Caluromys lanatus, and C. philander) are not shown. Statistical support values, above branches, based on 200 bootstrap replicates and indicated {{only for those}} above 50 %...|$|R
50|$|Abbott {{analyzed}} {{academic disciplines}} in two books, Department and Discipline (1999) and Chaos of Discipline (2001). The first book analyzes {{the history of}} sociology at Chicago {{and in particular the}} history of the American Journal of Sociology. The second provides a systematic approach to the intellectual development of disciplines. With the reconsideration of how knowledge changes and advances he challenges the idea of social sciences being in a perpetual state of progress stating them cycling around an inevitable pattern of core principles.Abbott has written also about knowledge production in Methods of Discovery (2004 - a handbook for social science heuristics) and Digital Paper (2014 - a handbook for research with data found in libraries or on the internet). He analyzes the various ways of knowing and its relation to materials. The ways of acquiring knowledge and the foundation of knowledge is reflected in the both books: from <b>heuristics</b> <b>analysis</b> to the organisation of the research: The library research in the Digital Paper is broken down into seven basic and simultaneous tasks: design, search, scanning/browsing, reading, analyzing, filing, and writing.|$|R
5000|$|If {{there is}} no BOM or other {{indication}} of the encoding, <b>heuristic</b> <b>analysis</b> is often able to reliably determine whether UTF-8 is in use due to {{the vast majority of}} byte sequences that are invalid in UTF-8. (When text is known to not be UTF-8, determining which legacy encoding can be difficult and uncertain. Several free libraries are available to ease the task, such as Mozilla Universal Charset Detector and International Components for Unicode.) ...|$|E
5000|$|Given their {{tremendous}} {{importance for}} biology and bioinformatics, orthologous genes have been organized in several specialized databases that provide tools {{to identify and}} analyze orthologous gene sequences. These resources employ approaches that can be generally classified into those that use <b>heuristic</b> <b>analysis</b> of all pairwise sequence comparisons, and those that use phylogenetic methods. Sequence comparison methods were first pioneered in the COGs database in 1997. These methods have been extended and automated in the following databases: ...|$|E
50|$|Vulnerabilities can be {{discovered}} with a vulnerability scanner, which analyzes {{a computer system}} in search of known vulnerabilities, such as open ports, insecure software configurations, and susceptibility to malware infections. Unknown vulnerabilities, such as a zero-day, may be found with fuzz testing, which can identify certain kinds of vulnerabilities, such as a buffer overflow with relevant test cases. Such analysis can be facilitated by test automation. In addition, antivirus software capable of <b>heuristic</b> <b>analysis</b> may discover undocumented malware if it finds software behaving suspiciously (such as attempting to overwrite a system file).|$|E
40|$|We {{describe}} an algorithm to evaluate all the complex {{branches of the}} Lambert W function with rigorous error bounds in interval arithmetic, which has been implemented in the Arb library. The classic 1996 paper on the Lambert W function by Corless et al. provides a thorough but partly <b>heuristic</b> numerical <b>analysis</b> which needs to be complemented with some explicit inequalities and practical observations about managing precision and branch cuts. Comment: 16 pages, 4 figure...|$|R
40|$|The {{purpose of}} this {{document}} is to present an approach {{of understanding of the}} design process from the framework of cognitive psychology. It is anchored in models of cognitive architecture and problem-solving approaches in order to suggest a metacognition practice for the designers. Is presented an approach of design as a process of problem solving, including common <b>heuristics,</b> their <b>analysis</b> and synthesis nature and contributions from Cognitive Psychology in metacognition processes, creativity and evaluation...|$|R
50|$|Methods used {{to perform}} a content audit include content ROT (redundant, outdated, trivial) analysis, social media analysis, SEO analysis, {{competitive}} <b>analysis,</b> content <b>analysis</b> <b>heuristics</b> (including information scent, differentiation, completeness, consistency, and currency), heat map analysis, among many others.|$|R
50|$|Most {{antivirus}} {{programs that}} utilize <b>heuristic</b> <b>analysis</b> perform this function by executing the programming commands of a questionable program or script within a specialized virtual machine, thereby allowing the anti-virus program to internally simulate {{what would happen}} if the suspicious file were to be executed while keeping the suspicious code isolated from the real-world machine. It then analyzes the commands as they are performed, monitoring for common viral activities such as replication, file overwrites, and attempts to hide the existence of the suspicious file. If one or more virus-like actions are detected, the suspicious file is flagged as a potential virus, and the user alerted.|$|E
5000|$|The acronym NOD {{stands for}} Nemocnica na Okraji Disku ("Hospital {{at the end}} of the disk"), a pun related to the Czechoslovak medical drama series Nemocnice na kraji města (Hospital at the End of the City). The first version of NOD32 - called NOD-ICE - was a DOS-based program. It was created in 1987 by Miroslav Trnka and Peter Paško at the time when {{computer}} viruses started to become increasingly prevalent on PCs running DOS. Due to the limitations of the OS (lack of multitasking among others) it didnt feature any on-demand/on-access protection nor most of the other features of the current versions. Besides the virus scanning and cleaning functionality it only featured <b>heuristic</b> <b>analysis.</b> With the increasing popularity of the Windows environment, advent of 32-bit CPUs, a shift on the PC market and increasing popularity of the Internet came the need for a completely different antivirus approach as well. Thus the original program was re-written and christened [...] "NOD32" [...] name both to emphasize the radical shift from the previous version and its Win32 system compatibility.|$|E
50|$|Another type of anti-virus, the integrety checker anti-virus, in some cases, doesn't work : it only checks all {{documents}} with extensions DOT or DOC (indeed, some anti-virus producers {{suggest to}} their users), but WinWord documents can reside in others extensions than those two {{and the content}} of the document tends to change often. So, like the <b>heuristic</b> <b>analysis,</b> this can lead to false positives errors, {{due to the fact that}} this type of anti-virus checks the whole document.The last type of anti-virus seen will be the virus-specific scanner. It searches the signature of viruses, so, the type of anti-virus is weakest than the previous ones.Indeed, the viruses which are detected by virus-specific scanners are just the ones knew by the producers of the software (so, more updates are needed than others types of scanners). Moreover, this type of anti-virus is really weak against morphing viruses (cf.section above), if a macro virus change its content (so, its signature), it can no more be detected by the virus-specific scanners even if it's the same virus which do the same actions. Its signature doesn't match the one declare in the virus scanner.|$|E
5000|$|Bogert, Bruce P.; Ossanna, Joseph F., [...] "The <b>heuristics</b> of cepstrum <b>analysis</b> of a {{stationary}} complex echoed Gaussian signal in stationary Gaussian noise", IEEE Transactions on Information Theory, v.12, issue 3, July 19, 1966, pp. 373 - 380 ...|$|R
30|$|Fuzzy {{clustering}} analysis uses “thinking-oriented implementation rules” in “Heuristic service quality evaluation” section according to “big data” in media data. When the media database {{does not have}} the required information and scoring information on evaluation, the server will put media transmission detection data and evaluation data information into the database as big data source. Finally, the server will implement the rules of fuzzy clustering <b>heuristic</b> algorithm <b>analysis</b> after there is a satisfying threshold amount of data in media database.|$|R
40|$|We {{present a}} {{scenario}} approach to capacity planning in semiconductor manufacturing under demand uncertainty. We formulate an integer programming {{model in which}} we minimize the expected value of the unmet demand, subject to capacity and budget constraints, {{to arrive at a}} tool set that does well across all of the scenarios. This is a dicult two-stage stochastic integer program that is tackled with a <b>heuristic</b> approach. <b>Analysis</b> of the results in some real-life situations are presented. ...|$|R
5000|$|Because of {{the large}} {{constant}} factors arising {{in the analysis of}} the AKS sorting network, parametric search using this network as the test algorithm is not practical. Instead, [...] suggest using a parallel form of quicksort (an algorithm that repeatedly partitions the input into two subsets and then recursively sorts each subset). In this algorithm, the partition step is massively parallel (each input element should be compared to a chosen pivot element) and the two recursive calls can be performed in parallel with each other. The resulting parametric algorithm is slower in the worst case than an algorithm based on the AKS sorting network. However, van Oostrum and Veltkamp argue that if the results of past decision algorithms are saved (so that only the comparisons unresolved by these results will lead to additional calls to the decision algorithm) and the comparison values tested by the simulated test algorithm are sufficiently evenly distributed, then as the algorithm progresses the number of unresolved comparisons generated in each time step will be small. Based on this <b>heuristic</b> <b>analysis,</b> and on experimental results with an implementation of the algorithm, they argue that a quicksort-based parametric search algorithm will be more practical than its worst-case analysis would suggest.|$|E
40|$|Constructing a small BDD from a given {{boolean formula}} depends on finding a good {{variable}} ordering. Finding a good order is NP-complete. In the past, methods based on <b>heuristic</b> <b>analysis</b> and formula (circuit) rewriting have {{shown to be}} useful for specific problem domains. We show that these methods need to be integrated: <b>heuristic</b> <b>analysis</b> can drive rewriting, which in turn simplifies analysis. We support this claim with experimental results, and describe Almana, an integrated tool for exploring the combination of analysis and rewriting on large boolean formulae...|$|E
40|$|A {{successful}} {{approach for}} currency recognition depends upon feature extraction of that currency image. This paper represents the <b>heuristic</b> <b>analysis</b> of characters and digits of serial number of Indian currency notes to recognition of currency notes. To recognize a character from a given currency image, {{there is a}} need to extract feature descriptors of such image. As an extraction method significantly affects the quality of whole OCR process, it is very important to extract features, which will be invariant towards the various light conditions, used font type and deformations of characters caused by a skew of the image. <b>Heuristic</b> <b>analysis</b> of characters is done for this purpose to get the accurate features of characters before feature extraction in currency recognition...|$|E
40|$|Axiomatic design {{procedures}} {{may be used}} {{to decouple}} usability analyses in Human-Computer Interaction. Nielsen’s ten usability heuristics were analyzed in terms of implications for FRS and DP’s. From the results we conclude that <b>heuristic</b> usability <b>analysis</b> leads to a coupled design process. To uncouple the design a cluster analysis was performed on the original design matrix. FR’s were then split and recombined {{in order to reduce the}} coupling [...] The Keywords: Usability Design, Software Design, Decoupling,...|$|R
40|$|Corporate and {{educational}} settings increasingly address more decision-making, problem-solving and other complex cognitive skills to handle complex cognitive, or heuristic, tasks, but the ever-increasing needfor heuristic knowledge has outpaced the refinement of task <b>analysis</b> methodsfor <b>heuristic</b> expertise. Utilizing the <b>Heuristic</b> Task <b>Analysis</b> (HTA) process, a method developedfor eliciting, analyzing, and representing expertise in complex cognitive tasks, aformative research {{study was conducted}} on the task of group cotunseling to further improve the-HTA process. Implications of thefindings include the needfo...|$|R
40|$|In {{this paper}} an agglomerative <b>heuristic</b> cluster <b>analysis</b> {{framework}} is proposed for {{application to the}} part family and machine cell formation problems associated with Group Technology (GT). This framework addresses the notion of concurrently forming clusters of parts (families) and machines (cells) based upon natural between-part and between-machine relationships {{and the strength of}} association relating pairs of parts with pairs of machines. An illustrative model is presented in the conference session and operational aspects demonstrated using a small problem...|$|R
