87|459|Public
50|$|Gram was {{the first}} {{mathematician}} to provide a systematic theory {{of the development of}} skew frequency curves, showing that the normal symmetric Gaussian <b>error</b> <b>curve</b> was but one special case of a more general class of frequency curves.|$|E
5000|$|Barbour {{concluded}} {{with a discussion of}} the pattern and magnitude of the errors produced by the generalized construction when used to approximate exponentials of different roots, stating that his method [...] "is simple and works exceedingly well for small numbers". For roots from 1 to 2 the error is less than 0.13%—about 2 cents when N=2— with maxima around m=0.21 and m=0.79. The <b>error</b> <b>curve</b> appears roughly sinusoidal and for this range of N can be approximated by about 99% by fitting the curve obtained for N=1, [...] The error increases rapidly for larger roots, for which Barbour considered the method inappropriate; the <b>error</b> <b>curve</b> resembles the form [...] with maxima moving closer to m= 0 and m=1 as N increases.|$|E
5000|$|Once {{the domain}} (typically an interval) {{and degree of}} the {{polynomial}} are chosen, the polynomial itself is chosen {{in such a way}} as to minimize the worst-case error. That is, the goal is to minimize the maximum value of , where P(x) is the approximating polynomial, f(x) is the actual function, and x varies over the chosen interval. For well-behaved functions, there exists an Nth-degree polynomial that will lead to an <b>error</b> <b>curve</b> that oscillates back and forth between [...] and [...] a total of N+2 times, giving a worst-case error of [...] It is seen that an Nth-degree polynomial can interpolate N+1 points in a curve. Such a polynomial is always optimal. It is possible to make contrived functions f(x) for which no such polynomial exists, but these occur rarely in practice.|$|E
30|$|For all {{methods and}} all datasets, {{cumulative}} <b>error</b> distribution <b>curves</b> were produced. For the BioID, 300 W, and the Menpo datasets, <b>error</b> <b>curves</b> indicating {{state of the}} art performance were overlayed and referenced.|$|R
40|$|International audienceIn {{order to}} characterise the {{ultimate}} error regime of Digital Image Correlation (DIC) algorithms, sets of virtual transformed images are often generated from a reference image by in-plane sub-pixel translations. This {{leads to the}} determination of the well-known S-shaped systematic <b>error</b> <b>curves</b> and their corresponding random <b>error</b> <b>curves.</b> It is reported in this collaborative work that the a priori choices used to numerically shift images (grey level interpolation, direct or reverse transformation between images) modify DIC results and may lead to biased conclusions in terms of DIC errors...|$|R
40|$|Abstract − Generally, the {{determination}} of the performance and quality of volumetric flow meters is being carried out using so called <b>error</b> <b>curves.</b> They are the result of experiments performed on test rigs for different design variations and operating conditions. In order to augment this development process for flow meter to reduce costs in terms of money and time, computational fluid dynamics (CFD) software has been applied as the so called “Numerical Test Rig”. This “Numerical Test Rig ” includes a full threedimensional flow simulation of a realistic meter configuration. Close attention was paid to correct reproduction of the meter features, requiring simulation of the complete geometry with flow straightener, rotor and the downstream domain, all within a 360 ° circumference. A new algorithm to determine the <b>error</b> <b>curves</b> was developed based on the detailed consideration of inflow and outflow velocities. Equally, the realistic computation of the bearing friction forces and moments was implemented. Simulations of several test cases produced resulting <b>error</b> <b>curves</b> that compared favourably with curves measured for real meters...|$|R
40|$|An e ective {{method for}} {{selecting}} features in clustering unlabeled data is proposed based on changing the objective {{function of the}} standard k-median clustering algorithm. The change consists of perturbing the objective function by a term that drives the medians {{of each of the}} k clusters toward the (shifted) global median of zero for the entire dataset. As the perturbation parameter is increased, more and more features are driven automatically toward the global zero median and are eliminated from the problem until one last feature remains. An <b>error</b> <b>curve</b> for unlabeled data clustering {{as a function of the}} number of features used gives reducedfeature clustering error relative to the standard" of the full-feature clustering. This clustering <b>error</b> <b>curve</b> parallels a classi cation <b>error</b> <b>curve</b> based on real data labels. This justi es the utility of the former <b>error</b> <b>curve</b> for unlabeled data as a means of choosing an appropriate number of reduced features in order to achieve a correctness comparable to that obtained by the full set of original features. For example, on the 3 -class Wine dataset, clustering with 4 selected input space features is comparable to within 4...|$|E
40|$|An {{algorithm}} for rational Chebyshev approximation {{based on}} computing the zeros of the <b>error</b> <b>curve</b> was investigated. At each iteration the proposed zeros are corrected by changing {{them toward the}} abscissa of the adjacent extreme of largest magnitude. The algorithm is formulated as a numerical solution of a certain system of ordinary differential equations. Convergence is obtained by showing the system is asymptotically stable at the zeros of the best approximation. With an adequate initial guess, the algorithm has never failed for functions which have a standard <b>error</b> <b>curve.</b> (Foundation Research Program Naval Postgraduate School[URL]...|$|E
30|$|Sequences {{generated}} from Chua’s three-dimensional map show interesting behavior, although the performance is worse compared to two-dimensional maps. The bit <b>error</b> <b>curve</b> {{acts like a}} sawtooth function, which can be explained through varying auto- and cross-correlation functions in a regular manner.|$|E
40|$|AbstractThe time {{course of}} visual mislocalization {{produced}} by a rapid retinal image displacement was examined in moving-background and saccadic eye movement experiments. In both experiments, the target for localization task and its background scene were dichoptically presented: they were presented separately to the different eyes. The <b>error</b> <b>curves</b> of mislocalization shown in the dichoptic viewing condition {{were the same as}} those in monocular viewing (in the moving-background experiment) and binocular viewing conditions (in the saccadic eye movement experiment), indicating that in both experiments the neural interaction responsible for generating mislocalization took place at a site after the lateral geniculate nucleus in the visual system, not at the retinal level. Two possible explanations for mislocalization, one neurophysiological and the other cognitive, were proposed. Furthermore, it was established that the <b>error</b> <b>curves</b> of mislocalization are substantially different between the moving-background and the saccadic eye movement experiments: in the saccadic eye movement experiment, the <b>error</b> <b>curves</b> changed with the actual target position, but not in the moving-background experiment. This was interpreted as showing that the basic mechanism for mislocalization is not the same between the two experimental situations...|$|R
40|$|The time {{course of}} visual mislocalization {{produced}} by a rapid retinal image displacement was examined in moving-background and saccadic eye movement experiments. In both experiments, the target for localization task and its background scene were dichoptically presented: they were presented separately to the different eyes. The <b>error</b> <b>curves</b> of mislocalization shown in the dichoptic viewing condition {{were the same as}} those in monocular viewing (in the moving-background experiment) and binocular viewing conditions (in the saccadic eye movement experiment), indicating that in both experiments the neural interaction responsible for generating mislocalization took place at a site after the lateral geniculate nucleus in the visual system, not at the retinal level. Two possible explanations for mislocalization, one neurophysiological nd the other cognitive, were proposed. Furthermore, it was established that the <b>error</b> <b>curves</b> of mislocaHzation are substantially different between the moving-background and the saccadic eye movement experiments: in the saccadic eye movement experiment, the <b>error</b> <b>curves</b> changed with the actual target position, but not in the moving-background experiment. This was interpreted as showing that the basic mechanism for misiocalization is not the same between the two experimental situations. Visual localization Retinal image displacement Saccade Visual integration Dichoptic viewin...|$|R
40|$|Abstract. The {{dependence}} of the classification error {{on the size of}} a bagging ensemble can be modeled within the framework of Monte Carlo theory for ensemble learning. These <b>error</b> <b>curves</b> are parametrized in terms of the probability that a given instance is misclassified by one of the predictors in the ensemble. Out of bootstrap estimates of these probabilities can be used to model generalization <b>error</b> <b>curves</b> using only information from the training data. Since these estimates are obtained using a finite number of hypotheses, they exhibit fluctuations. This implies that the modeled curves are biased and tend to overestimate the true generalization error. This bias becomes negligible as the number of hypotheses used in the estimator becomes sufficiently large. Experiments are carried out to analyze the consistency of the proposed estimator. ...|$|R
40|$|This {{article focuses}} on the {{computing}} efficiency of the instantaneous cutter position <b>error</b> <b>curve</b> in computer numerical control cutter positioning, which reflects the positional relationship between the cutter and the desired surface and leads to the strip width of current positioning. The directed projection is proposed to measure the distance of a discrete point to the cutter surface. Two models using fitting techniques are established to compute the instantaneous cutter position <b>error</b> <b>curve.</b> The fitting technique used {{in this article is}} based on the quartic polynomial model. In addition, to enhance the accuracy in the nonsymmetric case, the nonsymmetric quartic polynomial model is established, and it induces a more adaptable method. Illustrated experiments show good performance of the proposed methods...|$|E
40|$|Let f(z) be {{analytic}} at the origin, and for e> 0, let f(ez) be best approximated in the Chebyshev {{sense on}} the unit disk by a rational function of type (m, n). It has been shown previously by the CF method that the <b>error</b> <b>curve</b> for this approximation deviates from a circle by at most O(e 2 m+ 2 n+ 3) as e 0. We prove here that this bound is sharp in two senses: the <b>error</b> <b>curve</b> for a given function cannot be asymptotically more circular than the CF method predicts; moreover there exist functions for which the near-circularity is of order e 2 m+ 2 n+ 3 but no smaller. Key words. Chebyshev approximation, CF method, <b>error</b> <b>curve</b> AMS (MOS) subject classifications. 30 El 0, 41 A 20 1. Introduction and statement of results. Let S denote the complex unit circle {z [...] Izl =}, a the closed unit disk {z: and A =A(A) the set of functions continuous in A and analytic in the interior. Let m, n => 0 be fixed integers, and let Rm, be the set of rational functions in A of type (m, n) (i. e. no poles in A). Let I 1 " denote the supremum norm II ll=supz s which for cA is identical t...|$|E
30|$|We {{also note}} from Table 3 that with J[*]=[*] 100 Markov chain move steps between sensor measurements, the MCDPF RMS error {{approaches}} the <b>error</b> <b>curve</b> of the optimal flooding-based CbPFb tracker with a inter-node communication cost that is, however, roughly four {{times greater than}} that of the CbPFb algorithm.|$|E
40|$|Frequency {{modulation}} (FM) {{information from}} the speech signal is herein proposed to complement the conventional amplitude based features for automatic forensic speaker recognition systems. In addition to presenting the AM-FM model of speech used to generate the proposed frequency modulation features, the significance of frequency modulation for speaker recognition is discussed. Evaluation results from an automatic forensic speaker recognition system combining FM and MFCC features are shown to out-perform those of a system employing MFCC features alone, in terms of all typical metrics, such as detection <b>error</b> trade-off <b>curves,</b> Tippett curves and applied probability of <b>error</b> <b>curves.</b> Index Terms: frequency modulation, automatic forensic speaker recognition...|$|R
30|$|We {{have also}} shown that the {{proposed}} technique performs excellent not only in outdoor-to-indoor measurement scenarios but also in simulations for a channel model specifically designed for outdoor-to-outdoor high-speed scenarios. In both cases, the level of agreement between the results for actual speed and for emulated speeds is {{at the level of}} possible measurement accuracy, as the corresponding relative <b>error</b> <b>curves</b> have shown.|$|R
30|$|Observing {{relative}} <b>error</b> <b>curves</b> of two alternative calculating methods, {{the maximum}} relative error of average circle method {{is less than}} 1.5  %. The maximum relative error of the average circle method is nearly equal to the minimum relative error of the reference circle method. Therefore, the average circle method is a more accurate way to obtain the gear billet volume.|$|R
30|$|Time-varying BW {{algorithm}} with BE {{based on}} complex exponential functions (13) and initial channel guess {{identical to the}} true channel (BW-BE-exp & perfect init.). The difference between the <b>error</b> <b>curve</b> of this simulation and the previous one will indicate if we have an issue of convergence to a local maximum.|$|E
30|$|For each {{positioning}} result, a PCI is {{also given}} to infer {{the confidence of}} recent estimates. In our experiment, the PCI window size is 6. Figures 9 (b)– 9 (d) illustrate how the positioning error of cube determination changes with time. The proposed PCI can efficiently denote the variation amplitude of the recent positioning error. The sharper the positioning <b>error</b> <b>curve</b> fluctuates, the bigger the corresponding PCI value is, {{which means that the}} recent positioning results are not stable. Note that from a users perspective, only the PCI is available and users cannot know the <b>error</b> <b>curve.</b> In our current implementation, the PCI only gives an indication of whether the error is stable or whether the environment is stable enough for positioning. We believe that by using calibration or learning techniques, more precise positioning results can be derived from stable environment. This is part of our future work.|$|E
40|$|In this paper, six error {{indicators}} {{obtained from}} dual boundary integral equations {{are used for}} local estimation, which is an essential ingredient for all adaptive mesh schemes in BEM. Computational experiments are carried out for the two-dimensional Laplace equation. The curves of all these six error estimators are in good agreement with {{the shape of the}} <b>error</b> <b>curve.</b> The results show that the adaptive mesh base...|$|E
40|$|In {{a recent}} paper {{we showed that}} <b>error</b> <b>curves</b> in {{polynomial}} Chebyshev approximation of analytic functions on the unit disk tend to approximate perfect circles about the origin [23]. Making use of a theorem of Carath 6 odory and Fej 6 r, we derived in the process a method for calculating near-best approximations rapidly by finding the principal singular value and corresponding singular vector of a complex Hankel matrix. This paper extends these developments {{to the problem of}} Chebyshev approximation by rational functions, where non-principal singular values and vectors of the same matrix turn out to be required. The theory is based on certain extensions of the Carath 6 odory-Fej 6 r result which are also currently finding application in the fields of digital signal processing and linear systems theory. It is shown among other things that if f(ez) is approximated by a rational function of type (m, n) for ~> 0, then under weak assumptions the corresponding <b>error</b> <b>curves</b> deviate from perfect circles of winding numbe...|$|R
40|$|The final {{publication}} {{is available}} at Springer via [URL] of 8 th International Conference IDEAL, Birmingham, UK, December 16 - 19, 2007. The dependence of the classification error {{on the size of}} a bagging ensemble can be modeled within the framework of Monte Carlo theory for ensemble learning. These <b>error</b> <b>curves</b> are parametrized in terms of the probability that a given instance is misclassified by one of the predictors in the ensemble. Out of bootstrap estimates of these probabilities can be used to model generalization <b>error</b> <b>curves</b> using only information from the training data. Since these estimates are obtained using a finite number of hypotheses, they exhibit fluctuations. This implies that the modeled curves are biased and tend to overestimate the true generalization error. This bias becomes negligible as the number of hypotheses used in the estimator becomes sufficiently large. Experiments are carried out to analyze the consistency of the proposed estimator...|$|R
30|$|Remark 1. Facing {{the fact}} that the {{hierarchical}} constellation is randomly parametrized, we start investigation with the simplification that the error performance is given solely by minimal distance. We are aware that this is a rough approximation, since the minimal distance is relevant performance metric only asymptotically (as SNR → ∞) and the <b>error</b> <b>curves</b> are linearly proportional also to the number of signal pairs having the minimal distance.|$|R
40|$|AbstractLet Φ be an {{analytic}} {{solution of}} the Helmholtz equation [▿ 2 + F(r 2) ]Φ(r, θ) = 0 on the open unit disk Δ that is continuous on the closure Δ̄. Let the function F(r 2) be analytic and nonpositive on Δ̄. And, let Φ∗n be an approximation of Φ relative to an appropriate subspace of solutions whose restrictions to ∂Δ are polynomials of degree at most n. The <b>error</b> <b>curve</b> arises as the image of ∂Δ under the map Φ − Φ∗n. Geometric properties of the curve relating to the modulus and winding number characterize the best and near-best approximations. Among them are those when Φ∗n is the Chebyshev approximation, the curve is a near-perfect circle with winding number n+ 1. These attributes develop function-theoretically and lead to a constructive procedure for determining near-optimal approximations of Φ that are as close to best as the <b>error</b> <b>curve</b> is close to circular. Numerical examples are presented that {{are at least as}} accurate as the corresponding approximations of ez on Δ̄ which arise in analytic function theory...|$|E
40|$|For {{the problem}} of {{measuring}} robot requires a higher positioning, firstly learning about the error overview of the system, analysised the influence of attitude, speed and other factors on systematic errors. Then collected and analyzed the systematic <b>error</b> <b>curve</b> in the track to complete the planning process. The last adding fuzzy control in both cases, by comparing with the original system, can found that the method based on fuzzy control system can significantly reduce the error during the motion...|$|E
40|$|The set of all {{first degree}} polynomials must {{be added to}} the set of {{approximation}}s of the form a + b log (1 + ex) in order that a best Chebyshev approximation exist to all continuous functions on [0, a]. Best approximations in this augmented family of approxi-mations are characterized by alternation of their <b>error</b> <b>curve</b> and are unique. The Cheby-shev operator is continuous at / if / is an approximant or / has a non-constant best approximation. 1...|$|E
40|$|The ohmic {{distortion}} of current-voltage curves for the wall-jet hydrodynamic electrode is investigated using a finite-element approach with the circuit analysis program spice. Simulated and experimental data are presented, showing {{the displacement of}} the half-wave potential and the potential-dependent error in the Tafel slope. <b>Error</b> <b>curves</b> are given {{as a guide to}} the maximum current levels which may be used in practice with different electrolyte concentrations. © 1995...|$|R
40|$|International audienceIn {{order to}} {{characterize}} errors of Digital Image Correlation (DIC) algorithms, sets of virtual images are often generated from a reference image by in-plane sub-pixel translations. This {{leads to the}} determination of the well-known S-shaped bias <b>error</b> <b>curves</b> and their corresponding random <b>error</b> <b>curves.</b> As images are usually shifted by using interpolation schemes similar to those used in DIC algorithms, the question of the possible bias in the quantification of measurement uncertainties of DIC softwares is raised and constitutes the main problematic of this paper. In this collaborative work, synthetic numerically shifted images are built from two methods: one based on interpolations of the reference image and the other based on the transformation of an analytic texture function. Images are analyzed using an in-house subset-based DIC software and results are compared and discussed. The effect of image noise is also highlighted. The main result is that the a priori choices to numerically shift the reference image modify DIC results and may lead to wrong conclusions in terms of DIC error assessmen...|$|R
40|$|Prediction <b>error</b> <b>curves</b> are {{increasingly}} {{used to assess}} and compare predictions in survival analysis. This article surveys the R package pec which provides a set of functions for efficient computation of prediction <b>error</b> <b>curves.</b> The software implements inverse probability of censoring weights to deal with right censored data and several variants of cross-validation {{to deal with the}} apparent error problem. In principle, all kinds of prediction models can be assessed, and the package readily supports most traditional regression modeling strategies, like Cox regression or additive hazard regression, as well as state of the art machine learning methods such as random forests, a nonparametric method which provides promising alternatives to traditional strategies in low and high-dimensional settings. We show how the functionality of pec can be extended to yet unsupported prediction models. As an example, we implement support for random forest prediction models based on the R packages randomSurvivalForest and party. Using data of the Copenhagen Stroke Study we use pec to compare random forests to a Cox regression model derived from stepwise variable selection...|$|R
30|$|As {{shown in}} Figure  17 and Figure  18, the {{comprehensive}} compensation model curve {{is similar to}} the detecting thermal <b>error</b> <b>curve,</b> and the maximum residual error is about 9  μm. The maximum thermal error of the wear-depth detecting system is 0.02  mm, and the comprehensive compensation model could offset the thermal errors about 0.01  mm. Depending on the above verification, the comprehensive compensation model, built through MLR and time series analysis method, had the higher fitting precision and better applicability.|$|E
40|$|Abstract. The {{contents}} of the paper cover tooth contact analysis and optimization of transmission error for Klingelnberg spiral bevel gear. First, the rolling model, tooth contact analysis formulas are derived, contact area and transmission <b>error</b> <b>curve</b> is plotted. Second, the fuzzy optimization method is established to enhance {{the performance of the}} gears meshing, the optimization parameters can be confirmed to reduce transmission error. Third, an example of Klingelnberg spiral bevel gear for the illustration of the developed theory is represented...|$|E
40|$|In Chapter one {{theoretical}} {{indicator dilution}} curves are calculated using different mathematical models. In Chapter two experimental indicator dilution curves as obtained from working dog hearts were found compatible with a distributed diffusion limited model. The model parameters {{were obtained by}} performing least square <b>error</b> <b>curve</b> fits of the model to the experimental data. In Chapter three the above model was extended to include one way cellular uptake. Rubidium dilution curves were found to fit this latter model fairly closely...|$|E
40|$|In {{order to}} {{characterize}} errors of Digital Image Correlation (DIC) algorithms, sets of virtual images are often generated from a reference image by in-plane sub-pixel translations. This {{leads to the}} determination of the well-known S-shaped bias <b>error</b> <b>curves</b> and their corresponding random <b>error</b> <b>curves.</b> As images are usually shifted by using interpolation schemes similar to those used in DIC algorithms, the question of the possible bias in the quantification of measurement uncertainties of DIC softwares is raised and constitutes the main problematic of this paper. In this collaborative work, synthetic numerically shifted images are built from two methods: one based on interpolations of the reference image and the other based on the transformation of an analytic texture function. Images are analyzed using an in-house subset-based DIC software and results are compared and discussed. The effect of image noise is also highlighted. The main result is that the a priori choices to numerically shift the reference image modify DIC results and may lead to wrong conclusions in terms of DIC error assessmen...|$|R
30|$|The offset d is {{the point}} where the <b>error</b> rate <b>curve</b> becomes linear, and is higher than 10 − 2.|$|R
40|$|Abstract: In this paper, we {{proposed}} the <b>error</b> growth <b>curve</b> {{model for the}} integration of intertemporal measure errors correlation which usually ex-ist in quality control process. This model can work well in the usual error distributions, such as normal, uniform, Rayleigh and some other error distri-butions. Simulation {{results show that the}} proposed estimators of the model parameters perform well especially in small sample situations. Key words: <b>Error</b> growth <b>curve</b> model; Intertemporal <b>errors</b> correlation...|$|R
