2389|2077|Public
5|$|Andrew Farke and {{his colleagues}} in 2014 {{published}} {{a description of a}} new neoceratopsian, Aquilops americanus, through the peer-reviewed science journal PLOS ONE. They analysed their taxa as well as most other primitive ceratopsians to get a consensus cladogram. They created their own <b>data</b> <b>matrix</b> and through it found that many groups of ceratopsians could be supported, and that Aquilops was a basal neoceratopsian that could potentially be a protoceratopsid, leptoceratopsid, or ceratopsid, although any one of these groups would have a large ghost lineage with Aquilops.|$|E
25|$|The total {{least squares}} {{approximation}} {{of the data}} is generically equivalent to the best, in the Frobenius norm, low-rank approximation of the <b>data</b> <b>matrix.</b>|$|E
25|$|S. Jo and S. W. Kim, Consistent {{normalized}} least {{mean square}} filtering with noisy <b>data</b> <b>matrix.</b> IEEE Trans. Signal Processing, vol. 53, no. 6, pp.2112–2123, Jun. 2005.|$|E
40|$|trees and {{associated}} character <b>data</b> <b>matrices.</b> |$|R
40|$|Presents a {{subspace}} type {{of identification}} method for multivariable linear parameter-varying systems in {{state space representation}} with affine parameter dependence. It is shown that a major problem with subspace methods {{for this kind of}} systems is the enormous dimensions of the <b>data</b> <b>matrices</b> involved. To overcome the curse of dimensionality, we suggest to use only the most dominant rows of the <b>data</b> <b>matrices</b> in estimating the model. An efficient selection algorithm is discussed that does not require the formation of the complete <b>data</b> <b>matrices,</b> but can process them row by ro...|$|R
40|$|An {{improved}} {{method for}} generalized constrained canonical correlation analysis (GC-CANO) is proposed. In the original GCCANO, <b>data</b> <b>matrices</b> were first decomposed into {{the sum of}} several matrices according to some external information on rows and columns of the <b>data</b> <b>matrices.</b> Decomposed matrices were then subjected to canonical correlation analysis (CANO). However, orthogonal decompositions of <b>data</b> <b>matrices</b> do not necessarily entail orthogonal decompositions of projectors defined by the <b>data</b> <b>matrices.</b> This latter property is crucial in additive partitionings of the total association between two sets of variables. Consequently, no additive partitionings of the total association was possible in the original GCCANO. In this paper two orthogonal decompositions of projectors were proposed that allow additive partitionings of the total association. Terms in the decompositions have straightforward interpretations. An improved method for GCCANO is developed based on the decompositions, while preserving the most important features of the original method. An example is given to illustrate the proposed method...|$|R
25|$|Beginning in January 2010, Target Stores {{rolled out}} Mobile GiftCards, through {{which one can}} produce a GiftCard barcode on any web-capable cell phone. This <b>data</b> <b>matrix</b> barcode can be scanned at a Target POS like any {{physical}} card barcode, and balances can be stored, retrieved, and gifted with the convenience of a cell phone.|$|E
25|$|Below is a {{cladogram}} modified from {{an analysis}} published by Adam K. Huttenlocker in 2014. It {{is based on}} the <b>data</b> <b>matrix</b> published by Sigurdsen et al. (2012), which is itself a modified version of Huttenlocker et al. (2011). Six additional characters and 22 new ingroup taxa were added to the matrix of Sigurdsen et al. (2012), resulting in a matrix that includes 58 therapsids and outgroup taxa, including 49 therocephalians, which are scored based on 135 morphological traits. Huttenlocker (2014) used this cladogram, among others, to construct an informal supertree for evaluating temporal and phylogenetic distributions of body size in Permo-Triassic eutheriodonts.|$|E
25|$|Based on this work, Dayhoff and her {{coworkers}} {{developed a}} set of substitution matrices called the PAM (Percent Accepted Mutation), MDM (Mutation <b>Data</b> <b>Matrix),</b> or Dayhoff. They are derived from global alignments of closely related protein sequences. The identification number included with the matrix (ex. PAM40, PAM100) refers to the evolutionary distance; greater numbers correspond to greater distances. Matrices using greater evolutionary distances are extrapolated from those used for lesser ones. To produce a Dayhoff matrix, pairs of aligned amino acids in verified alignments are used to build a count matrix, which is then used to estimate at mutation matrix at 1 PAM (considered an evolutionary unit). From this mutation matrix, a Dayhoff scoring matrix may be constructed. Along with a model of indel events, alignments generated by these methods {{can be used in}} an iterative process to construct new count matrices until convergence.|$|E
30|$|Stage 1 —Compute the {{augmented}} {{model and}} gain (or <b>data)</b> <b>matrices.</b>|$|R
40|$|Assembling {{the tree}} of life is a major goal of biology, but {{progress}} has been hindered by the difficulty and expense of obtaining the orthologous DNA required for accurate and fully resolved phylogenies. Next-generation DNA sequencing technologies promise to accelerate progress, but sequencing the genomes {{of hundreds of thousands of}} eukaryotic species remains impractical. Eukaryotic transcriptomes, which are smaller than genomes and biased toward highly expressed genes that tend to be conserved, could potentially provide a rich set of phylogenetic characters. We sampled the transcriptomes of 10 mosquito species by assembling 36 -bp sequence reads into phylogenomic <b>data</b> <b>matrices</b> containing hundreds of thousands of orthologous nucleotides from hundreds of genes. Analysis of these <b>data</b> <b>matrices</b> yielded robust phylogenetic inferences, even with <b>data</b> <b>matrices</b> constructed from surprisingly few sequence reads. This approach is more efficient, data-rich, and economical than traditional PCR-based and EST-based methods and provides a scalable strategy for generating phylogenomic <b>data</b> <b>matrices</b> to infer the branches and twigs of {{the tree of}} life...|$|R
40|$|Gene {{expression}} <b>data</b> <b>matrices</b> often contain missing expression values. In this paper, {{we describe}} a new algorithm, named improved fixed rank approximation algorithm (IFRAA), for missing values estimations {{of the large}} gene expression <b>data</b> <b>matrices.</b> We compare the present algorithm with the two existing and widely used methods for reconstructing missing entries for DNA microarray gene expression data: the Bayesian principal component analysis (BPCA) and the local least squares imputation method (LLS). The three algorithms were applied to four microarray data sets and two synthetic low-rank <b>data</b> <b>matrices.</b> Certain percentages {{of the elements of}} these data sets were randomly deleted, and the three algorithms were used to recover them. In conclusion IFRAA appears to be the most reliable and accurate approach for recovering missing DNA microarray gene expression data, or any other noisy <b>data</b> <b>matrices</b> that are effectively low rank. Index Terms–Gene expression matrix, singular value decomposition, principal component analysis, least squares, missing values imputation, Bayesian analysis, K-nearest neighbor. ...|$|R
2500|$|... 2D Code reading Reading of 2D codes such as <b>data</b> <b>matrix</b> and QR codes.|$|E
2500|$|Here we follow. Suppose the {{ensemble}} matrix [...] and the <b>data</b> <b>matrix</b> [...] are as above. The ensemble mean and the covariance are ...|$|E
2500|$|If the <b>data</b> <b>matrix</b> X {{contains}} only two variables, a constant and a scalar regressor xi, {{then this is}} called the [...] "simple regression model". This case is often considered in the beginner statistics classes, as it provides much simpler formulas even suitable for manual calculation. The parameters are commonly denoted as : ...|$|E
40|$|This paper {{seeks to}} develop an {{allocation}} of 0 / 1 <b>data</b> <b>matrices</b> to physical systems upon a Kullback-Leibler distance between probability distributions. The distributions are estimated from {{the contents of the}} <b>data</b> <b>matrices.</b> We discuss an ascending hierarchical classification method, a numerical example and mention an application with survey data concerning the level of development of the departments of a given territory of a country...|$|R
40|$|The {{computer}} printout data generated during the Payload/Orbiter Contamination Control Requirement Study are presented. The computer listings of the input surface <b>data</b> <b>matrices,</b> the viewfactor <b>data</b> <b>matrices,</b> and the geometric relationship <b>data</b> <b>matrices</b> {{for the three}} orbiter/spacelab configurations analyzed {{in this study are}} given. These configurations have been broken up into the geometrical surfaces and nodes necessary to define the principal critical surfaces whether they are contaminant sources, experimental surfaces, or operational surfaces. A numbering scheme was established based upon nodal numbers that relates the various spacelab surfaces to a specific surface material or function. This numbering system was developed for the spacelab configurations such that future extension to a surface mapping capability could be developed as required...|$|R
30|$|After {{forming the}} <b>data</b> <b>matrices</b> Δ and W, then we may simply {{minimize}} the weighted stress to determine an coordinate representation.|$|R
2500|$|... where P = X(XTX)−1XT is the {{projection}} matrix onto the space V spanned by the columns of X. This matrix P is also {{sometimes called the}} hat matrix because it [...] "puts a hat" [...] onto the variable y. Another matrix, closely related to P is the annihilator matrix , this is a projection matrix onto the space orthogonal to V. Both matrices P and M are symmetric and idempotent (meaning that [...] ), and relate to the <b>data</b> <b>matrix</b> X via identities [...] and [...] Matrix M creates the residuals from the regression: ...|$|E
2500|$|The Western facility's {{repository}} has {{a capacity}} for over 54,000 boxes of archaeological material. It is equipped with high density mobile shelving units that maximize storage capacity. Preventive conservation methods are utilized, including temperature and humidity monitoring, to maintain a stable environment and to ensure long-term viability of collections. Collections are housed in archival quality, acid-free, non-off-gassing polyethylene bags and vials, within corrugated polypropylene [...] "bankers" [...] style boxes. [...] A radio frequency identification system (RFID) is utilized to track the location of boxes both within and between the Western and McMaster facilities in real time. 2-dimmensional <b>data</b> <b>matrix</b> (DM) barcodes are utilized {{for a variety of}} tracking purposes, including cataloging artifacts, as well as identifying artifact storage boxes, and shelf locations within the repositories.|$|E
50|$|Implement {{iterative}} extension when resultant <b>data</b> <b>matrix</b> faces a huge boundary extension, we can {{see that}} the block in original input <b>data</b> <b>matrix</b> is corresponding to the block in resultant <b>data</b> <b>matrix.</b>|$|E
40|$|Traditional co-clustering methods {{identify}} block structures from static <b>data</b> <b>matrices.</b> However, the <b>data</b> <b>matrices</b> in {{many applications}} are dynamic; that is, they evolve smoothly over time. Consequently, the hidden block structures em-bedded into the matrices are {{also expected to}} vary smoothly along the temporal dimension. It is therefore desirable to en-courage smoothness between the block structures identified from temporally adjacent <b>data</b> <b>matrices.</b> In this paper, we propose an evolutionary co-clustering formulation for iden-tifying co-cluster structures from time-varying data. The proposed formulation encourages smoothness between tem-porally adjacent blocks by employing the fused Lasso type of regularization. Our formulation is very flexible and allows for imposing smoothness constraints over only one dimen-sion of the <b>data</b> <b>matrices,</b> thereby enabling its applicability to a large variety of settings. The optimization problem for the proposed formulation is non-convex, non-smooth, and non-separable. We develop an iterative procedure to com-pute the solution. Each step of the iterative procedure in-volves a convex, but non-smooth and non-separable prob-lem. We propose {{to solve this problem}} in its dual form, which is convex and smooth. This leads to a simple gradi-ent descent algorithm for computing the dual optimal solu-tion. We evaluate the proposed formulation using the Allen Developing Mouse Brain Atlas data. Results show that our formulation consistently outperforms methods without the temporal smoothness constraints...|$|R
40|$|Semidefinite {{programming}} (SDP) {{may be seen}} as a {{generalization of}} linear programming (LP). In particular, one may extend interior point algorithms for LP to SDP, but it has proven much more difficult to exploit structure in the SDP data during computation. We survey three types of special structures in SDP data: 1. A common 'chordal' sparsity pattern of all the <b>data</b> <b>matrices.</b> This structure arises in applications in graph theory, and may also be used to deal with more general sparsity patterns in a heuristic way. 2. Low rank of all the <b>data</b> <b>matrices.</b> This structure is common in SDP relaxations of combinatorial optimization problems, and SDP approximations of polynomial optimization problems. 3. The situation where the <b>data</b> <b>matrices</b> are invariant under the action of a permutation group, or, more generally, where the <b>data</b> <b>matrices</b> belong to a low dimensional matrix algebra. Such problems arise in truss topology optimization, particle physics, coding theory, computational geometry, and graph theory. We will give an overview of existing techniques to exploit these structures in the data. Most of the paper will be devoted to the third situation, since it has received the least attention in the literature so far. Semidefinite programming Chordal sparsity Algebraic symmetry...|$|R
40|$|Given two <b>data</b> <b>matrices</b> $X$ and $Y$, sparse {{canonical}} correlation analysis (SCCA) {{is to seek}} two sparse canonical vectors $u$ and $v$ to maximize the correlation between $Xu$ and $Yv$. However, classical and sparse CCA models consider the contribution of all the samples of <b>data</b> <b>matrices</b> and thus cannot identify an underlying specific subset of samples. To this end, we propose a novel sparse weighted {{canonical correlation}} analysis (SWCCA), where weights are used for regularizing different samples. We solve the $L_ 0 $-regularized SWCCA ($L_ 0 $-SWCCA) using an alternating iterative algorithm. We apply $L_ 0 $-SWCCA to synthetic data and real-world data to demonstrate its effectiveness and superiority compared to related methods. Lastly, we consider also SWCCA with different penalties like LASSO (Least absolute shrinkage and selection operator) and Group LASSO, and extend it for integrating more than three <b>data</b> <b>matrices.</b> Comment: 8 pages, 5 figure...|$|R
5000|$|Divide both {{original}} input <b>data</b> <b>matrix</b> and resultant <b>data</b> <b>matrix</b> into {{blocks of}} [...] size.|$|E
50|$|Find {{the block}} {{which is the}} most similar to its {{neighbor}} block in the original input <b>data</b> <b>matrix,</b> and put it into the corresponding resultant <b>data</b> <b>matrix.</b>|$|E
50|$|<b>Data</b> <b>Matrix</b> {{was invented}} by International <b>Data</b> <b>Matrix,</b> Inc. (ID Matrix) which was merged into RVSI/Acuity CiMatrix, who were {{acquired}} by Siemens AG in October, 2005 and Microscan Systems in September 2008. <b>Data</b> <b>Matrix</b> is covered today by several ISO/IEC standards {{and is in the}} public domain for many applications, which means it can be used free of any licensing or royalties.|$|E
50|$|Other {{extensions}} of NMF include joint factorisation of several <b>data</b> <b>matrices</b> and tensors where some factors are shared. Such models {{are useful for}} sensor fusion and relational learning.|$|R
40|$|In this paper, {{the problem}} of fitting the {{exploratory}} factor analysis (EFA) model to <b>data</b> <b>matrices</b> with more variables than observations is reconsidered. A new algorithm named ‘zig-zag EFA’ is introduced for the simultaneous least squares estimation of all EFA model unknowns. As in principal component analysis, zig-zag EFA {{is based on the}} singular value decomposition of <b>data</b> <b>matrices.</b> Another advantage of the proposed computational routine is that it facilitates the estimation of both common and unique factor scores. Applications to both real and artificial data illustrate the algorithm and the EFA solutions...|$|R
40|$|The use of Candecomp to fit scalar {{products}} {{in the context of}} INDSCAL {{is based on the assumption}} that the symmetry of the <b>data</b> <b>matrices</b> involved causes the component matrices to be equal when Candecomp converges. Ten Berge and Kiers gave examples where this assumption is violated for Gramian <b>data</b> <b>matrices.</b> These examples are believed to be local minima. It is now shown that, in the single-component case, the assumption can only be violated at saddle points. Chances of Candecomp converging to a saddle point are small but still nonzero...|$|R
5000|$|Assuming {{the size}} of {{original}} input data and resultant data to be [...] and , respectively, we can also define that original input <b>data</b> <b>matrix</b> {{to be in the}} middle of resultant <b>data</b> <b>matrix.</b>|$|E
50|$|PCA is {{a linear}} feature {{learning}} approach since the p singular vectors are linear {{functions of the}} <b>data</b> <b>matrix.</b> The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the <b>data</b> <b>matrix</b> on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual <b>data</b> <b>matrix.</b>|$|E
50|$|<b>Data</b> <b>Matrix</b> {{codes are}} usually {{verified}} using specialist camera equipment and software. This verification ensures the code {{conforms to the}} relevant standards, and ensures readability for the lifetime of the component. After the component enters service, the <b>Data</b> <b>Matrix</b> code can then be read by a reader camera, which decodes the <b>Data</b> <b>Matrix</b> data which can then {{be used for a}} number of purposes, such as movement tracking or inventory stock checks.|$|E
50|$|Mathematically, {{canonical}} analysis maximizes U&prime;X&prime;YV subject to U&prime;X&prime;XU = I and V&prime;Y&prime;YV = I, where X and Y are the <b>data</b> <b>matrices</b> (row for instance and column for feature).|$|R
40|$|The paper {{presents}} a subspace type of identification method for multivariable linear parameter-varying systems in {{state space representation}} with affine parameter dependence. It is shown that a major problem with subspace methods {{for this kind of}} systems is the enormous dimensions of the <b>data</b> <b>matrices</b> involved. To overcome the curse of dimensionality, we suggest to use only the most dominant rows of the <b>data</b> <b>matrices</b> in estimating the model. An efficient selection algorithm is discussed that does not require the formation of the complete <b>data</b> <b>matrices,</b> but can process them row by row. 1 Introduction Subspace identification is by now a well-accepted method for identification of multivariable linear systems [1], [2]. Recently, subspace methods have been developed to handle certain classes of nonlinear systems, like Wiener [3], [4], Hammerstein [5] and bilinear systems [6], [7], [8], [9], [10]. In this paper we present a subspace identification method for linear parameter-varying (LPV) [...] ...|$|R
40|$|A {{standard}} approach to derive underlying components from {{two or more}} <b>data</b> <b>matrices,</b> holding <b>data</b> from the same individuals or objects, is the (generalized) canonical correlation analysis. This technique finds components (canonical variates) with maximal sums of correlations between them. The components do not necessarily explain much variance in the matrices they were derived from. This observation {{has given rise to}} alternative techniques, which maximize the sum of covariances between the components, subject to orthonormality constraints on the weight matrices applied to generate the components from the <b>data</b> <b>matrices.</b> However, a method called ConcorGM, maximizing the sum of squared covariances, has also been proposed. It has the additional feature that it allows for the analysis of two sets of <b>data</b> <b>matrices</b> which may, but need not coincide. The ConcorGM algorithm maximizes the sum of squared covariances successively, by first finding the best single-component solution, and repeating that process in the respective residual spaces. An algorithm for maximizing the sum of squared covariances simultaneously is offered. (C) 2005 Elsevier B. V. All rights reserved...|$|R
