3686|10000|Public
25|$|The {{difference}} between two models is that <b>hierarchical</b> <b>model</b> can explicitly make causal inference to predict certain stimulus while non-hierarchical model can only predict joint probability of stimuli. However, <b>hierarchical</b> <b>model</b> {{is actually a}} special case of non-hierarchical model by setting joint prior as a weighted average of the prior to common and independent causes, each weighted by their prior probability. Based on the correspondence of these two models, we can also say that hierarchical is a mixture modal of non-hierarchical model.|$|E
25|$|There {{are further}} more {{advanced}} concepts and factor models attempting to explain individual cognitive ability including the categorization of intelligence in fluid and crystallized intelligence or the <b>hierarchical</b> <b>model</b> of intelligence differences. Further supplementing explanations and conceptualizations for {{the factor structure}} of the Genomes of collective intelligence besides a general c factor', though, are missing yet.|$|E
25|$|The ISA (International Society of Automation) S88 Committee {{started work}} in the 1980s and has {{developed}} a series of standards and technical reports {{with the intent of}} providing a broadly accepted set of concepts, models and definitions for the batch control industry. ISA S88 Part 1, Batch Control Models and Terminology, introduces the concepts of a <b>hierarchical</b> <b>model,</b> a state model and modular software design.|$|E
40|$|<b>Hierarchical</b> <b>models</b> play three {{important}} roles in modeling causal effects: (i) accounting for data collection, such as in stratified and split-plot experimental designs; (ii) adjusting for unmeasured covariates, such as in panel studies; and (iii) capturing treatment effect variation, such as in subgroup analyses. Across all three areas, <b>hierarchical</b> <b>models,</b> especially Bayesian <b>hierarchical</b> <b>modeling,</b> offer substantial benefits over classical, non-hierarchical approaches. After discussing each of these topics, we explore some recent developments {{in the use of}} <b>hierarchical</b> <b>models</b> for causal inference and conclude with some thoughts on new directions for this research area...|$|R
40|$|<b>Hierarchical</b> <b>modeling</b> {{provides}} a framework for modeling the complex interactions typical of problems in applied statistics. By capturing these relationships, however, <b>hierarchical</b> <b>models</b> also introduce distinctive pathologies that quickly limit the efficiency of most common methods of in- ference. In this paper we explore the use of Hamiltonian Monte Carlo for <b>hierarchical</b> <b>models</b> and demonstrate how the algorithm can overcome those pathologies in practical applications. Comment: 11 pages, 12 figure...|$|R
40|$|Recent {{advances}} in computing make it practical to use complex <b>hierarchical</b> <b>models.</b> However, the complexity {{makes it difficult}} to see how features of the data determine the fitted model. This paper describes an approach to diagnostics for <b>hierarchical</b> <b>models,</b> specifically linear <b>hierarchical</b> <b>models</b> with additive normal or t-errors. The key is to express <b>hierarchical</b> <b>models</b> in the form of ordinary linear models by adding artificial `cases' to the data set corresponding to the higher levels of the hierarchy. The error term of this linear model is not homoscedastic, but its covariance structure is much simpler than that usually used in variance component or random effects models. The re-expression has several advantages. First, it is extremely general, covering dynamic linear models, random effect and mixed effect models, and pairwise difference models, among others. Second, it makes more explicit the geometry of <b>hierarchical</b> <b>models,</b> by analogy with the geometry of linear models. Third, the analogy with linear models provides a rich source of ideas for diagnostics for all the parts of <b>hierarchical</b> <b>models.</b> This paper gives diagnostics to examine candidate added variables, transformations, collinearity, case influence and residuals. James S. Hodge...|$|R
25|$|John B. Carroll (1993), after a {{comprehensive}} reanalysis of earlier data, proposed the three stratum theory, {{which is a}} <b>hierarchical</b> <b>model</b> with three levels. The bottom stratum consists of narrow abilities that are highly specialized (e.g., induction, spelling ability). The second stratum consists of broad abilities. Carroll identified eight second-stratum abilities. Carroll accepted Spearman's concept of general intelligence, for the most part, as {{a representation of the}} uppermost, third stratum.|$|E
25|$|Sky {{surveys and}} {{mappings}} {{of the various}} wavelength bands of electromagnetic radiation (in particular 21-cm emission) have yielded much information on the content and character of the universe's structure. The organization of structure appears to follow as a <b>hierarchical</b> <b>model</b> with organization up to the scale of superclusters and filaments. Larger than this (at scales between 30–200 megaparsecs), {{there seems to be}} no continued structure, a phenomenon that has been referred to as the End of Greatness.|$|E
25|$|This {{model is}} similar to the revised <b>hierarchical</b> <b>model</b> and {{suggests}} the lexicon of a bilingual individual is integrated, containing the words for both languages. This accounts for the simultaneous parallel activation of both languages during language tasks, the late activation of language in semantic processing, and the lack of effects of code switching. This lack of cost for code switching is especially used because they argue that separate lexicons would cause a slower reaction time, which was not indicated in the findings.|$|E
40|$|<b>Hierarchical</b> <b>models</b> {{are central}} to many current {{analyses}} of functional imaging data including random effects analysis, models using fMRI as priors for EEG source localization and spatiotemporal Bayesian modelling of imaging data [3]. These <b>hierarchical</b> <b>models</b> posit linear relations between variables with erro...|$|R
40|$|Abstract. Real data often {{show some}} level of {{hierarchical}} structure and its complexity {{is likely to be}} underrepresented by a single low-dimensional visualization plot. <b>Hierarchical</b> <b>models</b> organize the data visualization at different levels, and their ultimate goal is displaying a representation of the entire data set at the top-level, perhaps revealing the presence of clusters, while allowing the lower levels of the hierarchy to display representations of the internal structure within each of the clusters found, providing the definition of lower level sets of subclusters which might not be apparent in the higher-level representation. Several unsupervised <b>hierarchical</b> <b>models</b> are reviewed, divided into two main categories: Heuristic <b>Hierarchical</b> <b>Models,</b> with a focus on Self-Organizing Maps, and Probabilistic <b>Hierarchical</b> <b>Models,</b> mainly based on Gaussian Mixture Models. ...|$|R
40|$|<b>Hierarchical</b> <b>models</b> {{are central}} to many current {{analyses}} of functional imaging data including random effects analysis (Chapter 12), EEG source localization (Chapter 28 to 30) and spatiotemporal models of imaging data (Chapters 25 and 26 and [Friston et al. 2002 b]). These <b>hierarchical</b> <b>models</b> posit linear relation...|$|R
25|$|The {{victims of}} the Moscow Trials were not rehabilitated until 1988. Under Khrushchev, an {{investigation}} into the matter concluded that the Central Committee had lost its ruling function under Stalin; from 1929 onwards all decisions in the Central Committee were taken unanimously. In other words, the Central Committee was too weak to protect itself from Stalin and his hangmen. Stalin had managed to turn Lenin's <b>hierarchical</b> <b>model</b> on its head; under Lenin the Party Congress and the Central Committee were the highest decision-making organs, under Stalin the Politburo, Secretariat and the Orgburo became the most important decision-making bodies.|$|E
25|$|Conventional {{temporal}} probabilistic models such as {{the hidden}} Markov model (HMM) and conditional random fields (CRF) model directly model {{the correlations between the}} activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data. The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing spaghetti, which can be broken down into the subactivities or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a <b>hierarchical</b> <b>model</b> are Layered Hidden Markov Models (LHMMs) and the hierarchical hidden Markov model (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.|$|E
2500|$|The <b>Hierarchical</b> <b>Model</b> Composition package, {{known as}} [...] "comp", was {{released}} in November 2012. This package provides ...|$|E
5000|$|... #Subtitle level 3: Marketing Implications of <b>hierarchical</b> <b>models</b> ...|$|R
40|$|Bayesian data {{analysis}} involves describing data by meaningful mathematical models, and allocating credibility to parameter {{values that are}} consistent with the data and with prior knowledge. The Bayesian approach is ideally suited for constructing <b>hierarchical</b> <b>models,</b> which are useful for data structures with multiple levels, such as data from individuals who are members of groups which in turn are in higher-level organizations. <b>Hierarchical</b> <b>models</b> have parameters that meaningfully describe the data at their multiple levels and connect information within and across levels. Bayesian methods are very flexible and straightforward for estimating parameters of complex <b>hierarchical</b> <b>models</b> (and simpler models too). We provide an introduction to the ideas of <b>hierarchical</b> <b>models</b> and to the Bayesian estimation of their parameters, illustrated with two extended examples. One example considers baseball batting averages of individual players grouped by fielding position. A second example uses a hierarchical extension of a cognitive process model to examine individual differences in attention allocation of people who have eating disorders. We conclude by discussing Bayesian model comparison as a case of <b>hierarchical</b> <b>modeling...</b>|$|R
5000|$|For 3-stage <b>hierarchical</b> <b>models,</b> the {{posterior}} distribution is given by: ...|$|R
2500|$|The {{accompanying}} diagram is {{a general}} <b>hierarchical</b> <b>model</b> which shows functional manufacturing levels using computerised control of an industrial control system.|$|E
2500|$|This area {{deals with}} spatial {{judgment}} {{and the ability}} to visualize with the mind's eye. [...] Spatial ability is one of the three factors beneath g in the <b>hierarchical</b> <b>model</b> of intelligence.|$|E
2500|$|The term <b>hierarchical</b> <b>model</b> is {{sometimes}} considered {{a particular type}} of Bayesian network, but has no formal definition. Sometimes the term is reserved for models with three or more levels of random variables; other times, it is reserved for models with latent variables. In general, however, any moderately complex Bayesian network is usually termed [...] "hierarchical".|$|E
40|$|In the {{application}} of quantitative methods to ecological problems, <b>hierarchical</b> <b>models</b> combine explicit models of ecological system structure or dynamics with models of how ecological systems are observed. The principles of <b>hierarchical</b> <b>modeling</b> are explained in this chapter and applied book {{to a wide range}} of problems ranging from ecosystems to landscapes...|$|R
40|$|SUMMARY. It {{is shown}} that in <b>hierarchical</b> <b>models</b> if a {{fractional}} factorial plan allows inter–effect orthogonality {{then it is}} also universally optimal. It is also demonstrated that this phenomenon does not necessarily hold in non–hierarchical models. A combinatorial character-ization for inter–effect orthogonality is given for <b>hierarchical</b> <b>models</b> and its applications are indicated. 1...|$|R
40|$|The power prior {{has emerged}} as a useful {{informative}} prior for the incorporation of historical data in a Bayesian analysis. Viewing <b>hierarchical</b> <b>modeling</b> as the “gold standard ” for combining information across studies, we provide a formal justification of the power prior by examining formal analytical relationships between the power prior and <b>hierarchical</b> <b>modeling</b> in linear models. Asymptotic relationships between the power prior and <b>hierarchical</b> <b>modeling</b> are obtained for non-normal models, including generalized linear models, for example. These analytical relationships unify the theory of the power prior, demonstrate the generality of the power prior, shed new light on benchmark analyses, and provide insights into the elicitation of the power parameter in the power prior. Several theorems are presented establishing these formal connections, as well as a formal methodology for eliciting a guide value for the power parameter a 0 via <b>hierarchical</b> <b>models...</b>|$|R
2500|$|Some care {{is needed}} when {{choosing}} priors in a <b>hierarchical</b> <b>model,</b> particularly on scale variables {{at higher levels}} of the hierarchy such as the variable [...] in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will be improper (not normalizable), and estimates made by minimizing the expected loss will be inadmissible.|$|E
2500|$|Compound HD {{architectures}} aim {{to integrate}} characteristics of both HB and deep networks. The compound HDP-DBM architecture is a hierarchical Dirichlet process (HDP) as a <b>hierarchical</b> <b>model,</b> incorporated with DBM architecture. It {{is a full}} generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look [...] "reasonably" [...] natural. All the levels are learned jointly by maximizing a joint log-probability score.|$|E
2500|$|In {{an effort}} to gain {{industry}} acceptance Procter & Gamble (P) developed a “PackML Implementation Guide” with a software template & help files that was provided royalty-free, non-exclusive licensed to OMAC. This “OMAC Implementation Guide” is available for download from the OMAC website. The guide is an implementation of ISA-TR88.00.02, [...] borrows concepts from ISA-S88 Part 1 and embraces the ISA-S88 Part 5 draft concepts of the <b>hierarchical</b> <b>model</b> (Machine/Unit, Station/Equipment Module, Control Device/Control Module). The OMAC Implementation Guide provides PackML implementation guidelines, data structures and a minimum set of recommended PackTags (i.e. those typically needed for commercial MES packages). The implementation guideline provides a method to deliver State Control, Machine-to-Machine Communications and Machine-to-Information System Communications.|$|E
50|$|<b>Hierarchical</b> <b>models</b> are linear {{sequential}} models {{built on}} {{an assumption that}} consumers move {{through a series of}} cognitive and affective stages culminating in the purchase decision. The common theme among these models is that advertising operates as a stimulus and the purchase decision is a response. A number of <b>hierarchical</b> <b>models</b> {{can be found in the}} literature including Lavidge's hierarchy of effects, DAGMAR and AIDA and other variants. Some authors have argued that, for advertising purposes, the <b>hierarchical</b> <b>models</b> have dominated advertising theory, and that, of these models, the AIDA model is one of the most widely applied.|$|R
40|$|Experimental {{evidence}} {{demonstrates that}} syntactic structure influences human online sentence processing behavior. Despite this evidence, open questions remain: {{which type of}} syntactic structure best explains observed behavior–hierarchical or sequential, and lexicalized or unlexicalized? Recently, Frank and Bod (2011) find that unlexicalized sequential models predict reading times better than unlexicalized <b>hierarchical</b> <b>models,</b> relative to a baseline prediction model that takes wordlevel factors into account. They conclude that the human parser is insensitive to hierarchical syntactic structure. We investigate these claims and find a picture more complicated than the one they present. First, we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of Frank and Bod (2011) eliminates all differences in accuracy between those unlexicalized sequential and <b>hierarchical</b> <b>models.</b> Second, we show that lexicalizing the <b>hierarchical</b> <b>models</b> used in Frank and Bod (2011) significantly improves prediction accuracy relative to the unlexicalized versions. Third, we show that using stateof-the-art lexicalized <b>hierarchical</b> <b>models</b> further improves prediction accuracy. Our results demonstrate that the claim of Frank and Bod (2011) that sequential models predict reading times better than <b>hierarchical</b> <b>models</b> is premature, and also that lexicalization matters for prediction accuracy. ...|$|R
5000|$|Generalized {{filtering}} {{is usually}} used to invert <b>hierarchical</b> <b>models</b> {{of the following}} form ...|$|R
2500|$|The cancer {{stem cell}} model, {{also known as the}} <b>Hierarchical</b> <b>Model</b> proposes that tumors are hierarchically {{organized}} (CSCs lying at the apex (Fig. 3).) Within the cancer population of the tumors there are {{cancer stem cell}}s (CSC) that are tumorigenic cells and are biologically distinct from other subpopulations They have two defining features: their long-term ability to self-renew and their capacity to differentiate into progeny that is non-tumorigenic but still contributes to the growth of the tumor. [...] This model suggests that only certain subpopulations of cancer stem cells have the ability to drive the progression of cancer, meaning that there are specific (intrinsic) characteristics that can be identified and then targeted to destroy a tumor long-term without the need to battle the whole tumor.|$|E
2500|$|As {{the frontal}} lobes {{had been the}} object of {{scientific}} inquiry and speculation since the late 19th century, Fulton's contribution, while it may have functioned as source of intellectual support, is of itself unnecessary and inadequate as an explanation of Moniz's resolution to operate on {{this section of the}} brain. Under an evolutionary and <b>hierarchical</b> <b>model</b> of brain development it had been hypothesized that those regions associated with more recent development, such as the mammalian brain and, most especially, the frontal lobes, were responsible for more complex cognitive functions. However, this theoretical formulation found little laboratory support, as 19th century experimentation found no significant change in animal behaviour following surgical removal or electrical stimulation of the frontal lobes. This picture of the so-called [...] "silent lobe" [...] changed in the period after World War I with the production of clinical reports of ex-servicemen who had suffered brain trauma. The refinement of neurosurgical techniques also facilitated increasing attempts to remove brain tumours, treat focal epilepsy in humans and led to more precise experimental neurosurgery in animal studies. Cases were reported where mental symptoms were alleviated following the surgical removal of diseased or damaged brain tissue. The accumulation of medical case studies on behavioural changes following damage to the frontal lobes led to the formulation of the concept of Witzelsucht, which designated a neurological condition characterised by a certain hilarity and childishness in the afflicted. The picture of frontal lobe function that emerged from these studies was complicated by the observation that neurological deficits attendant on damage to a single lobe might be compensated for if the opposite lobe remained intact. In 1922, the Italian neurologist Leonardo Bianchi published a detailed report on the results of bilateral lobectomies in animals that supported the contention that the frontal lobes were both integral to intellectual function and that their removal led to the disintegration of the subject's personality. This work, while influential, was not without its critics due to deficiencies in experimental design.|$|E
5000|$|The <b>hierarchical</b> <b>model</b> {{is similar}} to the network model except that links in the <b>hierarchical</b> <b>model</b> form a tree structure,while the network model allows {{arbitrary}} graph.|$|E
5000|$|In general, {{the joint}} {{posterior}} distribution {{of interest in}} 2-stage <b>hierarchical</b> <b>models</b> is: ...|$|R
30|$|We {{propose a}} <b>hierarchical</b> {{multi-domain}} NSSA <b>model.</b> We divide the network into many fields to collect network security situation data. Compared {{with the traditional}} <b>hierarchical</b> <b>models,</b> our model takes into account dependency and user security, which can reflect the network security situation more comprehensively.|$|R
40|$|A neighborliness {{property}} of marginal polytopes of <b>hierarchical</b> <b>models,</b> {{depending on the}} cardinality of the smallest non-face of the underlying simplicial complex, is shown. The case of binary variables is studied explicitly, then the general case is reduced to the binary case. A Markov basis for binary <b>hierarchical</b> <b>models</b> whose simplicial complexes is the complement of an interval is given. Comment: 9 page...|$|R
