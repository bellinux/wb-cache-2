7735|6656|Public
5|$|NBA {{statistics}} per Basketball Reference. CBA statistics per CBA <b>Data</b> <b>Center.</b>|$|E
5|$|Based on 30-year averages {{obtained}} from NOAA's National Climatic <b>Data</b> <b>Center</b> for {{the months of}} December, January and February, Weather Channel ranked Omaha the 5th coldest major U.S. city as of 2014.|$|E
5|$|The Bellevue City Hall is a {{government}} office building and city hall in Bellevue, Washington. The current city hall, located in Downtown Bellevue, opened in 2006 after the $121 million renovation of a former Qwest <b>data</b> <b>center.</b> The <b>data</b> <b>center,</b> originally built for Pacific Northwest Bell in 1983, was acquired by the city government in 2002 {{for use by the}} Bellevue Police Department and later approved as the new city hall. It incorporates use of wood interiors and a terra cotta exterior that has been recognized with several design awards since its opening.|$|E
25|$|Encryption, {{specifically}} AES 128-bit or stronger, {{is applied}} to data while stored at <b>data</b> <b>centers,</b> under transit between <b>data</b> <b>centers</b> and users, and between <b>data</b> <b>centers.</b>|$|R
40|$|With growing use of {{internet}} and exponential growth in {{amount of data}} to be stored and processed (known as "big data"), the size of <b>data</b> <b>centers</b> has greatly increased. This, however, has resulted in {{significant increase in the}} power consumption of the <b>data</b> <b>centers.</b> For this reason, managing power consumption of <b>data</b> <b>centers</b> has become essential. In this paper, we highlight the need of achieving energy efficiency in <b>data</b> <b>centers</b> and survey several recent architectural techniques designed for power management of <b>data</b> <b>centers.</b> We also present a classification of these techniques based on their characteristics. This paper aims to provide insights into the techniques for improving energy efficiency of <b>data</b> <b>centers</b> and encourage the designers to invent novel solutions for managing the large power dissipation of <b>data</b> <b>centers...</b>|$|R
40|$|Cloud {{providers}} may operate large-scale <b>data</b> <b>centers</b> {{in a few}} locations. We {{argue that}} deploying many small-scale <b>data</b> <b>centers</b> at network edge can significantly improve user experience in terms of latency. Small-scale <b>data</b> <b>centers,</b> however, {{may not be able}} to provide elastic services. In this paper, we investigate distributed small-scale <b>data</b> <b>centers</b> with load reallocation where jobs that cannot be suitably processed locally will be reallocated to remote <b>data</b> <b>centers.</b> We formulate an optimization problem for load reallocation in distributed <b>data</b> <b>centers,</b> provide performance comparisons among different alternatives and offer insights on handling multiple job types. We develop online optimization algorithms that can be operated in a decentralized and measurement-based fashion to dynamically reallocate load in response to sudden load surges. The experimental results demonstrate that elasticity can be practically provided by small-scale <b>data</b> <b>centers</b> enhanced with effective load reallocation technique...|$|R
5|$|In July 2010, AT {{announced}} {{the construction of}} a $120 million <b>data</b> <b>center</b> in Akron, their 9th facility dedicated to such for the eastern seaboard of the United States, which was followed by the announcement in August of {{the construction of a}} $20 million Involta data facility, also in Akron, which will be constructed to meet LEED certification.|$|E
5|$|In the United States, {{an annual}} visual {{inspection}} {{is not required}} by the USA DOT, though they do require a hydrostatic test every five years. The visual inspection requirement is a diving industry standard based on observations made during a review by the National Underwater Accident <b>Data</b> <b>Center.</b>|$|E
5|$|The highway {{continues}} eastward, serving residential developments, commercial outlets, and John Paul Stevens High School. It intersects with SH151, where development {{once again}} becomes patchy. The most notable development along {{this stretch of}} Potranco is a National Security Agency <b>data</b> <b>center.</b> State maintenance ends at an intersection with FM3487 (Culebra Road), slightly west of Interstate410 (I-410). Potranco Road continues as a city street until Ingram Road.|$|E
30|$|Recently, {{numerous}} <b>data</b> <b>centers</b> {{were established}} around the world. These <b>data</b> <b>centers</b> consume {{large amount of}} energy.|$|R
50|$|This World Data System, {{hosts the}} {{repositories}} for data {{collected during the}} IGY.Seven of the 15 World <b>Data</b> <b>Centers</b> in the United States are co-located at NOAA National <b>Data</b> <b>Centers</b> or at NOAA affiliates. These ICSU <b>Data</b> <b>Centers</b> not only preserve historical data, but also promote research and ongoing data collection.|$|R
50|$|However {{traditionally}} <b>data</b> <b>centers</b> {{were either}} {{built for the}} sole use of one large company, or as carrier hotels or Network-neutral <b>data</b> <b>centers.</b>|$|R
5|$|The {{state has}} the highest average and peak Internet speeds in the United States, with the third-highest worldwide. Northern Virginia's data centers can carry up to 70% of the nation's {{internet}} traffic, and in 2015 the region was the largest and fastest growing <b>data</b> <b>center</b> market in the nation.|$|E
5|$|Another global field model, called World Magnetic Model, is {{produced}} {{jointly by the}} United States National Centers for Environmental Information (formerly the National Geophysical <b>Data</b> <b>Center)</b> and the British Geological Survey. This model truncates at degree 12 (168 coefficients) with an approximate spatial resolution of 3,000 kilometers. It is the model used by the United States Department of Defense, the Ministry of Defence (United Kingdom), the United States Federal Aviation Administration (FAA), the North Atlantic Treaty Organization (NATO), and the International Hydrographic Office {{as well as in}} many civilian navigation systems.|$|E
5|$|During his tenure, the {{business}} group grew its revenues {{more than ten}} percent each year for six years. The division {{accounted for more than}} 20 percent of Microsoft's revenues by January 2009. In this position, Muglia led Microsoft's ten-year plan for <b>data</b> <b>center</b> and desktop automation products, its Dynamic Systems Initiative and its Dynamic IT strategy. In October 2010, developers criticized Muglia for suggesting Microsoft would put less emphasis on Silverlight; a statement he later retracted.|$|E
40|$|All the {{internet}} services available {{these days are}} dependant and running in <b>data</b> <b>centers.</b> Companies like Google, Facebook, and Microsoft hosts millions of servers in their <b>data</b> <b>centers</b> to provide services to their users [19]. The enormous size of <b>data</b> <b>centers</b> leads to huge energy consumption. According to a news article, Google dre...|$|R
40|$|Large <b>data</b> <b>centers</b> {{are well}} known for their high-energy {{intensity}} and have made dramatic efficiency improvements over the past decade. Small closet and room <b>data</b> <b>centers</b> have received much less attention, yet constitute a significant {{fraction of the total}} number of servers in the United States. The often makeshift, ad hoc nature of small <b>data</b> <b>centers</b> often results in little attention paid to energy efficiency and inadequate cooling equipment. The small physical footprint of these <b>data</b> <b>centers,</b> typically embedded within a larger building, makes it difficult to identify and target for efficiency measures. These conditions make small <b>data</b> <b>centers</b> notoriously inefficient relative to their larger counterparts. In this report, we present an analysis of small and midsize <b>data</b> <b>centers</b> in the US, drawing from surveys of commercial building stock. We find that servers in small <b>data</b> <b>centers</b> make up approximately 40 % of installed server stock, with the vast majority of sites utilizing only 1 - 2 servers. We identify industries where small <b>data</b> <b>centers</b> are most prevalent, finding that the highest saturations are in medical, retail, office, and education sectors. Small <b>data</b> <b>centers</b> typically lack dedicated cooling equipment, often relying on building air conditioning and ventilation equipment for cooling. We further find that the type of cooling equipment used is highly correlated with the number of operational server racks, with less efficient cooling options used with fewer racks. We develop geospatial maps of small and midsize <b>data</b> <b>centers</b> to visually identify regions of high server concentration and calculate associated CO 2 emissions. Small <b>data</b> <b>centers</b> consume 13 billion kWh of energy annually, emitting 7 million metric tons (MMT) of carbon dioxideâ€“the equivalent emissions of approximately 2. 3 coal-fired plants. We discuss efficiency measures that could be implemented and estimate potential energy and CO 2 savings...|$|R
50|$|Unisys {{operates}} <b>data</b> <b>centers</b> {{around the}} world that are certified on global standards for service quality and excellence. Those certifications include ISO 9001:2008, ISO 20000-1:2005, and ISO 27001 standards. Unisys <b>data</b> <b>centers</b> follow Global Process Standards (GPS) for Information Technology Infrastructure Library (ITIL) processes. It also supports and manages <b>data</b> <b>centers</b> at client-owned facilities.|$|R
5|$|Juniper is {{the third}} largest market-share holder overall for routers and {{switches}} used by ISPs. According to analyst firm Dell'Oro Group, it is the fourth largest for edge routers and second for core routers with 25% of the core market. It is also the second largest market share holder for firewall products with a 24.8% share of the firewall market. In <b>data</b> <b>center</b> security appliances, Juniper is the second-place market-share holder behind Cisco. In WLAN, where Juniper holds a joint development and marketing agreement with Aruba Networks, it holds a more marginal market share. Juniper provides technical support and services through the J-Care program.|$|E
5|$|The storm {{affected}} a large {{region of}} the northeastern United States from West Virginia to Massachusetts with heavy snowfall, sleet, rain, and high winds. The Centers for Disease Control and Prevention attributed four deaths to the nor'easter, but only included those directly related; the agency did not include storm-induced traffic accidents or heart attacks. The National Climatic <b>Data</b> <b>Center</b> reported 19deaths related to the nor'easter, although news reports shortly after the storm reported 20deaths. Overall damage was estimated between $12billion (1992USD), mostly in New England.|$|E
5|$|During {{the early}} 1980s, the DISCOM {{consisted}} of a Headquarters Company, a Division Materiel Management Center, a Division <b>Data</b> <b>Center,</b> the 15th Medical Battalion, 15th Adjutant General Battalion, 15th Finance Company, 15th Supply and Transport Battalion, 27th Maintenance Battalion and the 68th Chemical Company. In October 1984, the 1st and 2nd Forward Support Battalions were activated from elements of the three functional battalions attached to the unit. The following year saw the Army of Excellence Reorganization (AOE) transform the remaining elements of the three functional battalions (maintenance, medical, and supply and transport) into the 4th Main Support Battalion. The AOE reorganization also added the 493rd Transportation Company (Aircraft Maintenance) to the Division Support Command. The forward and main support battalions, along with the Aviation Maintenance Company, were redesignated in 1987, becoming the 15th and 115th Forward Support Battalions, 27th Main Support Battalion, and 227th Transportation Company (Aviation Maintenance).|$|E
40|$|Energy {{conservation}} {{is a major}} concern in todays <b>data</b> <b>centers,</b> which are the 21 st century data processing factories, and where large and complex software systems such as distributed data management stores run and serve billions of users. The two main drivers of this major concern are the pollution impact <b>data</b> <b>centers</b> have on the environment due to their waste heat, and the expensive cost <b>data</b> <b>centers</b> incur due to their enormous energy demand. Among the many subsystems of <b>data</b> <b>centers,</b> the storage system {{is one of the main}} sources of energy consumption. Among the many types of storage systems, key/value stores happen to be the widely used in the <b>data</b> <b>centers.</b> In this work, I investigate energy saving techniques that enable a consistent hash based key/value store save energy during low activity times, and whenever there is an opportunity to reuse the waste heat of <b>data</b> <b>centers...</b>|$|R
5000|$|MBIX {{has signed}} a Memorandum of Understanding which {{provides}} no-cost cross-connects to members inside its <b>data</b> <b>centers.</b> MBIX is currently available in two <b>data</b> <b>centers</b> in Winnipeg: ...|$|R
40|$|Due to {{the fast}} {{development}} of internet, {{a huge amount}} of load increases over <b>data</b> <b>centers</b> every second. This causes scheduling overhead, huge memory demand at <b>data</b> <b>centers.</b> Thus increases overhead effects the load balancing at <b>data</b> <b>centers.</b> So, there is a need of mechanisms which will decrease overhead and provide effective load balancing. Today, every load balancing scheduling algorithm balances the load on <b>data</b> <b>centers</b> that reside in the same region. They give birth to same problems like scheduling overhead, huge memory demand. This paper proposes a Load balancing scheduling algorithm which is based on load and time. This algorithm balances the load over the <b>Data</b> <b>centers</b> which reside in different regions. This mechanism will maximize hardware utilization, decrease huge memory demand and decrease cost...|$|R
5|$|SH151 {{begins at}} Loop1604, the outer loop around San Antonio, {{on the west}} side of San Antonio and from there follows a southeastern path through the western part of the city. The highway {{provides}} access to the SeaWorld San Antonio theme park as well as industry along its corridor to include Chase, Hyatt Hill Country Resort, World Savings, Philips semiconductor, the National Security Agency campus, QVC, American Funds, Maxim Integrated Products, and the Northwest Vista College, as well as the nearby Southwest Research Institute. Microsoft has also selected the corridor for a $550 million <b>data</b> <b>center.</b> The highway continues to the southeast to a junction with Interstate 410 (I-410), the inner loop around San Antonio. There is not a direct connect interchange at the junction of I-410. It is necessary to travel on both highways' frontage roads in order to change highways. The highway continues to the southeast through mainly undeveloped land until it merges with US90. According to the San Antonio Master Thoroughfare Plan, there are plans to extend SH 151 westward from Loop1604 to SH211. The plan also shows the construction of an interchange at Loop 1604.|$|E
5|$|Since 2009, the Syracuse Center of Excellence in Environmental and Energy Systems, led by Syracuse University in {{partnership}} with Clarkson University and the College Environmental Science and Forestry, creates innovations in environmental and energy technologies that improve human health and productivity, security, and sustainability in urban and built environments. The Paul Robeson Performing Arts Company and the Community Folk Art Center will also be located downtown. On March 31, 2006, {{the university and the}} city announced an initiative to connect the main campus of the university with the arts and culture areas of downtown Syracuse and The Warehouse. Using natural gas, the Green <b>Data</b> <b>Center</b> generates its own electricity on site, providing cooling for servers and for a neighboring building.|$|E
5|$|The most {{frequently}} used methods to map and measure snow extent, snow depth and snow water equivalent employ multiple inputs on the visibleâ€“infrared spectrum to deduce the presence and properties of snow. The National Snow and Ice <b>Data</b> <b>Center</b> (NSIDC) uses the reflectance of visible and infrared radiation to calculate a normalized difference snow index, which is a ratio of radiation parameters that can distinguish between clouds and snow. Other researchers have developed decision trees, employing the available data to make more accurate assessments. One challenge to this assessment is where snow cover is patchy, for example during periods of accumulation or ablation and also in forested areas. Cloud cover inhibits optical sensing of surface reflectance, {{which has led to}} other methods for estimating ground conditions underneath clouds. For hydrological models, {{it is important to have}} continuous information about the snow cover. Passive microwave sensors are especially valuable for temporal and spatial continuity because they can map the surface beneath clouds and in darkness. When combined with reflective measurements, passive microwave sensing greatly extends the inferences possible about the snowpack.|$|E
30|$|The cost {{associated}} with cloud <b>data</b> <b>centers</b> {{are composed of}} three factors. Labor cost takes the smallest chunk of total operation cost, nearly 6 % of the total cost, whereas power and cooling cost and computing costs are 20 % and 48 % respectively. Other costs account for remaining 26 %. Cloud <b>data</b> <b>centers</b> add new cost, unlike traditional <b>data</b> <b>centers</b> [3].|$|R
50|$|Datapipe is a {{provider}} of managed hosting services and <b>data</b> <b>centers</b> for information technology services and cloud computing with <b>data</b> <b>centers</b> in Somerset, New Jersey, San Jose, California, the United Kingdom, and China. The company was founded in 1998 and is headquartered in Jersey City, New Jersey. More <b>Data</b> <b>Centers</b> had been added since 2011. Including Ashburn VA, Iceland, and Singapore.|$|R
50|$|The city is also {{a landing}} point on three fiber optic cable systems linking the United States across the Pacific Ocean: C2C, Southern Cross Cable, and VSNL Transpacific. These cable landings, lower energy costs, and tax breaks led to a boom of <b>data</b> <b>centers</b> being built {{starting}} about 2010. <b>Data</b> <b>centers</b> include those for Adobe, NetApp, Umpqua Bank, OHSU, and Fortune <b>Data</b> <b>Centers.</b>|$|R
5|$|Throughout Florida, {{three people}} were killed in {{relation}} to the storm and no more than $10million in damage occurred, with more than half of which was attributed to agricultural losses. In addition, 76people were injured, 12of whom required hospitalization. Structural damage was relatively limited, with only 63homes and businesses, mostly trailers, being destroyed; 159other structures sustained major damage while a further 631experienced minor damage. The majority of damage from Isbell was not from the hurricane itself but rather tornadoes spawned by its outer bands. At least nine, and as many as twelve, tornadoes affected the state with the greatest effects being felt in the Miami metropolitan area. All storm-related injuries were attributed to these tornadoes as well as the majority of structural damage. According to the National Climatic <b>Data</b> <b>Center,</b> four of these tornadoes were of F2 intensity.|$|E
5|$|Titanfall was {{released}} on Xbox One and Windows PC (via Origin) in North America on March 11, 2014, in Europe and Australia on 13, and in the United Kingdom and New Zealand on 14. The game's South African release was cancelled due to poor connectivity performance during the beta and no nearby Microsoft Azure <b>data</b> <b>center</b> in the region. Australia, in a similar predicament, used Singapore's servers {{at the time of}} launch. The delayed Xbox 360 release developed by Bluepoint Games {{was released}} on April 8, 2014 in North America, and on April 11, 2014 in Europe. The Xbox 360 version is functionally identical to the other releases, albeit with lower-quality graphics. A downloadable Xbox Live Games on Demand option was not available at launch but was implemented later. The PC version did not include modding tools at launch, though it did support the Xbox 360 controller. Microsoft hosted over 6,000 midnight launch events worldwide to prepare for the release.|$|E
25|$|HipChat <b>Data</b> <b>Center</b> is Atlassian's self-hosted team {{communication}} offering. Hipchat <b>Data</b> <b>Center</b> {{was built}} for large, enterprise customer that need to store their data on premise. Hipchat <b>Data</b> <b>center</b> offers active-active clustering, 24/7 support, technical account management, SAML 2.0 SSO, multiple depolyment offerings, and integrations for internal systems, 3rd party products, and Atlassian products.|$|E
40|$|Abstractâ€”Energy {{efficiency}} (EE), {{energy consumption}} cost and environmental impact are vibrant challenges to cloud computing and <b>data</b> <b>centers.</b> Reducing energy consumption and {{emissions of carbon}} dioxide (CO 2) in <b>data</b> <b>centers</b> represent open areas and driving force for future research work on green <b>data</b> <b>centers.</b> Our Literature review reveals that there are currently several energy efficiency frameworks for <b>data</b> <b>centers</b> which combine a green IT architecture with specific activities and procedures that led to decrease the impact on environment and less CO 2 emissions. The current available frameworks have some pros and cons that is the reason why there is an urgent need for an integrated criterion for selecting and adopting energy efficiency framework for <b>data</b> <b>centers.</b> The required energy efficiency framework criteria should also consider the social network applications as a vital related factor in elevating energy consumption, as well as high potential for better energy efficiency in <b>data</b> <b>centers.</b> Additionally, in this paper, we highlighted the importance of the identification of efficient and effective energy efficiency metric {{that can be used for}} the measurement and determination of the value of <b>data</b> <b>centers</b> efficiency and their performance combined with sound and empirically validated integrated EE framework...|$|R
40|$|Big data flows {{along with}} server {{virtualization}} at current <b>data</b> <b>centers,</b> cloud applications, mobile data, {{and the need}} to store and replicate vast amounts of data has led to a situation where there are large dynamically changing data traffic patterns and flows across modern datacenters. There is a rapid increase in data traffic and {{it is likely that the}} technology and infrastructure of todayâ€Ÿs architecture in <b>data</b> <b>centers</b> will not be able to cope with the growing traffic demands. Therefore there is a need for disruptive development and migration to new architectures based on optical technology to significantly improve the performance of <b>data</b> <b>centers</b> and to support this rapid increase in traffic in <b>data</b> <b>centers.</b> The purpose of this thesis is firstly the presentation of the form of todayâ€Ÿs <b>data</b> <b>centers</b> and the investigation of the factors that hinder their further scaling so as to meet the enormous needs of increasing traffic in <b>data</b> <b>centers.</b> Then we present some major implementations of optical architectures based on different technologies aimed for <b>data</b> <b>centers</b> and lastly we present the overarching protocols concerning the control plane of these optical architectures...|$|R
40|$|<b>Data</b> <b>centers</b> {{are major}} {{contributors}} to the emission of carbon dioxide to the atmosphere, and this contribution {{is expected to increase}} in the following years. This has encouraged the development of techniques to reduce the energy consumption and the environmental footprint of <b>data</b> <b>centers.</b> Whereas some of these techniques have succeeded to reduce the energy consumption of the hardware equipment of <b>data</b> <b>centers</b> (including IT, cooling, and power supply systems), we claim that sustainable <b>data</b> <b>centers</b> will be only possible if the problem is faced by means of a holistic approach that includes not only the aforementioned techniques but also intelligent and unifying solutions that enable a synergistic and energy-aware management of <b>data</b> <b>centers.</b> In this paper, we propose a comprehensive strategy to reduce the carbon footprint of <b>data</b> <b>centers</b> that uses the energy as a driver of their management procedures. In addition, we present a holistic management architecture for sustainable <b>data</b> <b>centers</b> that implements the aforementioned strategy, and we propose design guidelines to accomplish each step of the proposed strategy, referring to related achievements and enumerating the main challenges that must be still solved. Peer ReviewedPostprint (author's final draft...|$|R
