1|25|Public
40|$|Swarm {{intelligence}} {{is widely used}} {{in the application of}} communication networks. In this paper we adopt a biologically inspired strategy to investigate the data dissemination problem in the opportunistic cognitive networks (OCNs). We model the system as a centralized and distributed hybrid system including a location prediction server and a pervasive environment deploying the large-scale human-centric devices. To exploit such environment, data gathering and dissemination are fundamentally based on the contact opportunities. To tackle the lack of contemporaneous end-to-end connectivity in opportunistic networks, we apply ant colony optimization as a cognitive <b>heuristic</b> <b>technology</b> to formulate a self-adaptive dissemination-based routing scheme in opportunistic cognitive networks. This routing strategy has attempted to find the most appropriate nodes conveying messages to the destination node based on the location prediction information and intimacy between nodes, which uses the online unsupervised learning on geographical locations and the biologically inspired algorithm on the relationship of nodes to estimate the delivery probability. Extensive simulation is carried out on the real-world traces to evaluate the accuracy of the location prediction and the proposed scheme in terms of transmission cost, delivery ratio, average hops, and delivery latency, which achieves better routing performances compared to the typical routing schemes in OCNs...|$|E
50|$|This {{technology}} continuously monitors {{each program}} (specific processes) {{running on the}} PC as it executes, and it notes any malware-like actions. Each action is scored and, when a given threshold is reached, the process is reported as harmful. Unlike <b>heuristic</b> <b>technologies</b> that check executable files when they are accessed or started, Active Virus Control monitors everything applications do {{as long as they}} are active.|$|R
40|$|The {{motivating}} {{techniques for}} training future engineer are proposed. Intrinsic and extrinsic motivations in professional training have been analyzed. The peculiarities {{of the formation}} process of students’ motivation in learning language at non-language universities have been systematized. <b>Heuristic</b> training <b>technology</b> based on students’ motivation to cognitive research has been implemented...|$|R
40|$|Ways {{to develop}} {{effective}} industrial enterprise stability management system in innovative environment are discussed. Enterprise stability management techniques and tools are outlined {{on the basis}} of scientific approach to dynamics and trends in the development of successful and sustainable enterprises; the approach is supported with cybernetics, <b>heuristics,</b> computer <b>technologies</b> and synergetic considerations. </p...|$|R
40|$|The article {{develops}} {{the concept of}} education, conformable to human nature and foundations of the universe, considers the role of education in performance of man’s mission, states the foundations of distributed scientific school of man conformable education, introduces the principle of man conformable education, offers approaches to development of new content of education, based on metasubjective approach, analyses Federal Educational Standards of the Russian Federation, presents <b>heuristic</b> learning <b>technology...</b>|$|R
30|$|To {{present the}} case study, we first will give {{details on the}} process that has been {{developed}} for a fully automated distribution grid management in “Process overview” section. After that, we present the system design by introducing the layered agent concept regarding chosen optimization <b>heuristic,</b> the blockchain <b>technology</b> and the incentivation scheme. In “Scenario setup” section we introduce the chosen setup regarding (controllable) loads and flexible generation units in a simplified congestion scenario.|$|R
40|$|The basic {{ingredients}} {{of the discovery}} creativity is presented. Understanding, problem solving, <b>heuristics,</b> computer thinking, <b>technology</b> and physics, and so on. We present some discoveries {{from the viewpoint of}} creativity. The Dirac equation, the Riccati equation, the nonlinear Schroedinger equation, the quantum navier Sokes equation, the equation of the quantum magnetohydrodynamics and so on. We discuss nonlinear Lorentz transformation involving the maximal acceleration which we relate to the Hagedorn temperature, and possible dependence of mass on acceleration. We consider the relation between physics and technology and so on. Comment: 37 page...|$|R
40|$|One {{central issue}} in system {{structuring}} and quality prediction is the interdependencies of system modules. This paper proposes a novel technique {{for determining the}} operational coupling in embedded computer control systems. It allows us to quantify dependencies between modules, formed by different kinds of relationships in a solution, and therefore promotes a more systematic approach to the reasoning about modularity. Compared to other existing coupling metrics, which are often implementation-technology specific such as confining to the inheritance and method invocation relationships in OO software, this metrics system considers both communication and synchronization and can be applied throughout system design. The metrics system has two parts. The first part supports a measurement of coupling by considering individual relationship types separately. The quantification is performed by considering the topology of connections, {{as well as the}} multiplicity, replication, frequency, and accuracy of component properties that appear in a relationship. The second part provides a methodology for combining coupling by individual relationship types into an overall coupling, where domain specific <b>heuristics</b> and <b>technology</b> constraints are used to determine the weighting...|$|R
40|$|Abstract. Because of the {{traditional}} K-medoids clustering algorithm the initial clustering center sensitive, the global search ability is poor, easily trapped into local optimal and slow convergent speed; therefore, this article proposes an improved K-medoids clustering algorithm. Differential evolution {{is a kind of}} <b>heuristic</b> global search <b>technology</b> population, has strong robustness. Combined with K-medoids clustering algorithm efficiency and the global optimization ability of DE algorithm, not only can effectively overcome the detects of the K-medoids clustering algorithm, but also can raise the global search capability, short the convergence time, effectively improve the clustering quality. Finally, the algorithm is verified stability and robustness by simulation...|$|R
40|$|Technology mapping {{problems}} arize in logic synthesis systems, {{when the}} gap between a synthesized boolean network and the implementation of that network within a given target technology has to be bridged. This paper presents a modular, versatile technology mapping system that supports many different target technologies. Guided by a complexity analysis of the problem, we develop a variety of efficient, exact or <b>heuristic</b> methods for <b>technology</b> driven network clustering. Depending on the target technology and optimization methods and goals, different subnetworks must be provided as candidates for clustering. Methods to achieve this are also included. We conclude with experimental results we obtained with several configurations of the system for different target technologies...|$|R
40|$|The {{technology}} mapping - {{final step}} of the logic synthesis - maps the decomposed Boolean function on physical cells. We address here the decomposition and the matching steps. We present two different ROBDD-based techniques to handle the decomposition problem, and compare them. For handling the matching step, we analyse a heuristics based on symmetry and develop a new structural approach, based on controlling value analysis and observation function deduction. This last appears to be efficient regarding the CPU time for checking the match with basic cells, mostly when don't cares are present, and should be particularly interesting to handle the complex cells of FPGAs. Benchmarks are presented which validate the various <b>heuristics.</b> Keywords ROBDD, <b>technology</b> mapping, decomposition, matching 1 INTRODUCTION Technology mapping is the last and decisive step in logic synthesis. It is usually performed after a technology independent minimization, which generates an optimized multi-level logic [...] ...|$|R
40|$|We {{present a}} new {{approach}} for performing technology mapping onto Field Programmable Gate Arrays (FPGAs). We consider one class of FPGAs, based on two-output five-input RAM-based cells, {{that are used to}} implement combinational logic functions. We describe a <b>heuristic</b> algorithm for <b>technology</b> mapping that performs a decomposition of the circuit in the FPGA primitives, driven by the information on logic functional sharing. We have implemented the algorithm in the program Hydra. Experimental results shows an average of 20 % to 25 % improvement over other existing programs in mapping area and 67 -fold speedup in computing time. 1 Introduction There has been an increasing interest in digital-system prototyping using Field Programmable Gate Arrays (FPGAs) due to their fast turn-around time and low manufacturing costs. One class of FPGAs uses a RAM-based architecture, where logic blocks in the form of look-up tables are used to implement combinational logic. The advantage of this architecture [...] ...|$|R
40|$|CENTRAL PRINCIPLE Important {{technologies}} require validated {{standards for}} the design heuristics {{that are used to}} design and evaluate them, but not necessarily identical <b>heuristics</b> for every <b>technology.</b> BACKGROUND <b>Heuristic</b> standards provide a valuable toolkit with which to evaluate the accessibility of modern information society technologies (IST). But can we apply the same heuristic, generic standards to all types of technological platforms, in the face of their growing diversity e. g. websites, social websites, blogs, virtual reality applications, ambient intelligence etc (Adams, 2007) ? Or would it be wiser to expect that different technologies might require different, if overlapping, standards? Can we really expect to design the interface of a modern cell phone on the same basis as for a table computer? Most impartial observers would probably say “no”. How can we introduce a systematic and thorough approach to the diverse technologies that are seen or predicted to be seen? Work in our laboratory has explored two useful questions. First, how to computer literate users perceive the different technologies? Second, how can different heuristic standards be developed where needed...|$|R
40|$|This paper {{addresses}} a multi-period investment problem for selection, acquisition, and allocation of alternative technology choices {{to meet the}} demand of a number of product families over a long-range planning horizon. The problem captures the essential features of many existing models for analyzing long-term trade-offs between dedicated and flexible technologies in the chemical, manufacturing, telecommunications, and service industries. We show that the general problem is -hard and present a solution strategy based upon perturbing the linear programming (LP) relaxation solution of a multi-period mixed-integer linear programming formulation for the problem. The key feature of the proposed strategy is a temporal capacity shifting heuristic, whereby capacity expansions are shifted to earlier time periods from amongst those periods chosen for capacity expansion by the LP relaxation solution. With mild assumptions on the problem parameters, we carry a probabilistic analysis which proves that the proposed solution approach is asymptotically optimal almost surely. Our analysis provides a sound theoretical basis for incorporating capacity shifting in existing LP relaxation-based <b>heuristics</b> for long-term <b>technology</b> planning problems in a variety of industries. ...|$|R
40|$|Objective: To {{assess the}} use, {{appropriateness}} of, and staff feedback on specific sexual health modules, which were installed on <b>Heuristic</b> Interactive <b>Technology</b> (HITnet) kiosks at Aboriginal Community Controlled Health Services (ACCHS). The HITnet kiosks {{were aimed at}} Aboriginal youth visiting these sites. Methods: Modules on the HITnet kiosks were assessed for (1) cultural appropriateness using Yunkaporta’s Aboriginal pedagogy framework and (2) compliance with the World Health Organization’s (WHO) advice on key elements for comprehensive sexual health education for young people. Data measuring kiosk use were obtained through HITnet kiosk activity reports. An online survey of ACCHS staff was used to qualitatively assess use of, and staff perceptions of, HITnet kiosks. Results: Kiosk modules were consistent with seven of the eight elements of Yunkaporta’s framework {{and all of the}} WHO recommendations. The most popular module generated 3, 066 purposeful sessions and the least popular module generated 724 purposeful sessions across nine sites in 2012. While teenagers were the most frequent of the kiosk user groups (39. 5 % in 2012), the majority of users (56 %) were not in the target group (i. e. elders 4 %, adults 25 %, children 27 %). Key issues reported by ACCHS staff (n= 11) included: lack of clarity regarding staff responsibility for overseeing kiosk functionality; kiosks attracting “inappropriate ages”; and “lack of privacy” based on kiosk location, screen visibility, and absence of headphones preventing discreet access. Conclusions: The modules were tailored to a young Aboriginal audience through technology thought to be appealing to this group. However, barriers to use of the kiosk included kiosk design features, location, and lack of clarity around responsibility for kiosk operation. Implications: Aboriginal youth need easy access to sexual health messages in a ‘safe’, non-judgmental space. Information and communication that is accessible via personal and mobile devices may be a better vehicle than public kiosks...|$|R
40|$|Circuit {{designers}} require Computer-Aided Design (CAD) tools when compiling designs into Field Programmable Gate Arrays (FPGAs) {{in order}} to achieve high quality results due to the complexity of the compilation tasks involved. Technology mapping is one critical step in the FPGA CAD flow. The final mapping result has significant impact on the subsequent steps of clustering, placement and routing, for the objectives of delay, area and power dissipation. While depth-optimal FPGA technology mapping can be solved in polynomial time, area minimization has proven to be NP-hard. Most modern state-of-the-art FPGA technology mappers are structural in nature; they are based on cut enumeration and use various heuristics to yield depth and area minimized solutions. However, the results produced by structural technology mappers rely strongly on the structure of the input netlists. Hence, it is common to apply additional <b>heuristics</b> after <b>technology</b> mapping to further optimize area and reduce the amount of structural bias while not harming depth. Recently, SAT-based Boolean matching has been used for post-mapping area minimization. However, SAT-based matching is computationally complex and too time consuming in practice. This thesis proposes an alternative Boolean matching approach based on NPN equivalence. Using a library of pre-computed topologies, the matching problem becomes as simple as performing NPN encoding followed by a hash lookup which is very efficient. In conjunction with Ashenhurst decomposition, the NPN-based Boolean matching is allowed to handle up to 10 -input Boolean functions. When applied to a large set of designs, the proposed algorithm yields, on average, more than 3 % reduction in circuit area without harming circuit depth. The priori generation of a library of topologies can be difficult; the potential difficulty in generating a library of topologies represents one limitation of the proposed algorithm...|$|R
40|$|AbstractPetri nets are {{fundamental}} {{to the analysis of}} distributed systems especially infinite-state systems. Finding a particular marking corresponding to a property violation in Petri nets can be reduced to exploring a state space induced by the set of reachable markings. Typical exploration(reachability analysis) approaches are undirected and do not take into account any knowledge about the structure of the Petri net. This paper proposes heuristic search for enhanced exploration to accelerate the search. For different needs in the system development process, we distinguish between different sorts of estimates. Treating the firing of a transition as an action applied to a set of predicates induced by the Petri net structure and markings, the reachability analysis can be reduced to finding a plan to an AI planning problem. Having such a reduction broadens the horizons for the application of AI <b>heuristic</b> search planning <b>technology.</b> In this paper we discuss the transformations schemes to encode Petri nets into PDDL. We show a concise encoding of general place-transition nets in Level 2 PDDL 2. 2, and a specification for bounded place-transition nets in ADL/STRIPS. Initial experiments with an existing planner are presented...|$|R
40|$|In today's {{competitive}} logistics business environment, airfreight forwarders need {{to optimize}} {{every aspect of}} their logistics operations. However, forwarders still heavily rely on human brain and working experiences for calculating complex cargo packing and scheduling problems. Although recent research studies related to cargo packing and scheduling problems {{have resulted in the}} development of a number of advanced techniques of cargo planning, it can be seen that most of the research work is focused on the optimization of space in order to achieve the maximum possible amount of cargo to be packed in the minimum of space. After numerous site evaluation and end-user feedbacks, it is found that space optimization does not necessarily cause profit optimization, which is the ultimate aim of logistics providers. A study of contemporary research publications indicates that there are inadequate research studies related to profit-based optimization in cargo packing areas. This paper presents a profit-based air cargo loading information system (ACLIS) that embeds an innovative <b>technology</b> known as <b>heuristics</b> iterative reasoning <b>technology</b> (HIRT) that supports loading plan generation, focusing on maximization of the profit margin. In general, the proposed system is meant to maximize the profit in the airfreight forwarding business. It adopts an objective function governed by a list of constraints together with rule-based reasoning to provide expert advice to support the generation of appropriate loading plans. Department of Industrial and Systems Engineerin...|$|R
40|$|Abstract—In this paper, {{we propose}} a link layer design for mobile hotspots. We design a novel system {{architecture}} that enables highspeed Internet access in railway systems. The proposed design uses {{a number of}} repeaters placed along the track and multiple antennas installed {{on the roof of}} a vehicle. Each packet is decomposed into smaller fragments and relayed to the vehicle via adjacent repeaters. We also use erasure coding to add parity fragments to original data. This approach is called information raining since fragments are rained upon the vehicle from adjacent repeaters. We investigate two instances of information raining. In blind information raining, all repeaters awaken when they sense the presence of the vehicle. The fragments are then blindly transmitted via awakened repeaters. A vehicle station installed inside the train is responsible for aggregating a large enough number of fragments. In the throughput-optimized information raining, the vehicle station selects a bipartite matching between repeaters and roof-top antennas and activates only a subset of the repeaters. It also dictates the amount of transmission power of each activated repeater. Both the bipartite matching and power allocations are individually shown to be NP-complete. Matching heuristics based on the Hungarian algorithm and Gale-Shapley algorithm are proposed. A simplex-type algorithm is proposed as the power allocation <b>heuristics.</b> Index Terms—Emerging <b>technologies,</b> network architecture and design, wireless communication, network protocols, mobile communication systems, mobile environments, medium access control, mobile hotspots, graphs and networks, linear programming, constrained optimization, graph theory, combinatorial algorithms. æ...|$|R
40|$|Future {{studies are}} {{traditionally}} {{based on a}} top-down approach, but in consumer-led markets-where it is subtle innovation features that {{make the difference between}} success and failure-managers experience that the approach is counter-productive. The managers are expected to define a strategy to frame the innovation efforts, but in practice it is the emerging insights and innovations from those efforts that set the base {{for the development of a}} strategy. Managers need to fully acknowledge the power of emerging innovations that transcend the strategic framework and proactively pursue them. Future emerging innovations can be fermented and investigated using a bottom-up approach which is similar to the top-down scenario process used for strategic planning. However, the nature of the bottom-up approach is different and needs to be relevant to explore future innovations opportunities, rather than oriented towards decision and policy making. An analysis of future trends suggest that the bottom-up approach is in a favourable position to serve the future needs of companies in consumer-led markets and a model is presented which integrates the top-down and the bottom-up approach into one innovation focused framework. A new toolbox is needed for the framework and future-oriented technology analysis FTA is in a strong position to take the lead in collaboration with other research areas, such as ethnology, socio-technical analysis and design studies. However, other agencies are already active in the field and FTA need to act soon, if they want to be a player in the future business context. Keywords: Bottom-up, user value, market paradigm, innovation opportunities, experience economy, <b>heuristic</b> experimentation, future-oriented <b>technology</b> analysis, innovation map. ...|$|R
40|$|It {{has always}} been the {{endeavor}} of scientists and engineers to develop systems that could mimic the human brain and adapt themselves to complex situations in order to arrive at {{a solution to the problem}} at hand in an efficient and effective manner. These systems would not only be capable of action but also of thought. <b>Heuristic</b> and algorithmic <b>technologies</b> are needed for the automation of cognitive activities. There is a need to develop tools, which are capable of assessing a situation at hand, alerting the human part of the system to various concerns and providing advice on possible responses, and possibly proceed towards solving the problem. This paper deals with the development of a human-machine optimal automated transportation system. In this paper it is proposed to develop specific algorithms to predict and avoid traffic congestion and collision. Two such tools that can be used with each other to produce optimal results are the Global Positioning System (GPS) and the Kalman filter. The Kalman filter is an algorithm, which can be used to predict the future state of a system based on the past information. This could be useful when we are dealing with typically, the problem of collision. A Kalman filter could be designed to give an accurate estimate of the future state of a highway or an intersection based on the data collected for that particular highway or intersection by GPS. The goal is to successfully use data fusion algorithms specifically like GPS and Kalman filters to improve the safety and efficiency of transportation systems particularly with respect to collision avoidance. Key words: automated transportation system—global positioning system—Kalman filte...|$|R
40|$|In this thesis, a {{methodology}} for {{the integration of}} a general purpose heuristic optimization algorithm and a commercial computer simulation language is developed to enhance and extend the benefits of using computer simulation to solve complex business problems. A unique version of the standard Simulated Annealing algorithm is developed that eliminates the need to tune the algorithm for each problem, thus producing a general purpose optimization framework. Concepts from Statistical Process Control {{form the basis of}} the proposed algorithm. ^ Along with the proposed methodology and algorithm development, significant emphasis is focused on the issues involved in using a random variable-based technology, computer simulation to supply values for a deterministic-based <b>technology,</b> <b>heuristic</b> optimization. ^ Results obtained demonstrate that the proposed algorithm can outperform the standard Simulated Annealing algorithm in two thirds of the test cases in terms of average results obtained. The proposed algorithm provided a maximum of eight percent improvement over the standard Simulated Annealing algorithm on similar problems where an improvement was obtained. When the standard Simulated Annealing algorithm provided better results, they were a maximum of only three percent better than the proposed method. Validation of the algorithm on an expanded set of known test cases provided similar results. ^ The significance of this research to industrial engineering is {{a methodology}} and algorithm that will allow an Operations Research analyst to heuristically optimize a computer simulation model according to a pre-defined output response function without resorting to the iterative trial and error process that is in common practice today. This methodology will enable the analyst to quickly obtain boundary values of an output response function for any system under study, allowing the analyst to spend scarce project time on analysis and decision making of boundary conditions rather than on trial and error iterations in an attempt to find these conditions. ...|$|R
40|$|In {{order to}} meet the demands of the {{continuous}} scaling of electronic devices, new technologies have been developed over the years. As we approach the newest levels of miniaturization, current technologies, such as physical vapor deposition and chemical vapor deposition, are reaching a limitation in their ability to successfully fabricate nano sized electronic devices. ^ Supercritical fluid deposition (SFD) is a demonstrated technology that provides excellent step coverage for the deposition of metals and metal oxides within narrow, high aspect ratio features. This technique shows the potential to satisfy the demands of integrated circuit miniaturization while maintaining a cost effective process needed to keep the technology competitive. In order to complement SFD <b>technology</b> <b>heuristics</b> for scale-up, an understanding of the deposition mechanism and kinetics and resolution of integration issues such as interfacial film adhesion must be resolved. ^ It is critical to have a fundamental understanding of the chemistry behind the reaction process in supercritical fluid deposition. For this purpose, a detailed kinetic study of the deposition of ruthenium from bis(2, 2, 6, 6 -tetramethyl- 3, 5 -heptanedionato) (1, 5 -cyclooctadiene) ruthenium(II) is carried out so that growth rate orders and a mechanism can be established. These predictive kinetic results provide the means to control the reaction which allows for overall optimization of the process. ^ Reliability is of the utmost importance for fabricated devices since they must withstand harsh steps in the fabrication process as well as perform and last under standard and extreme usage conditions. One issue of reliability is assessed by addressing the adhesion of the metallization layers deposited by SFD. A quantitative determination of the interfacial adhesion energy of as deposited and pretreated copper metallization layers from SFD onto barrier layers is used to determine the potential for integration of these films for industry standards. ^ Extension of the basics of SFD by performing co-deposition of multiple compounds, layer-by-layer deposition for device fabrication and integration with other unique technologies for novel applications demonstrates the ability of this technique to satisfy a wide range of commercial applications and be used as the basis for new technologies. Co-depositions of Ce/Pt, Co/Pt, Ba/Ti and Nd/Ni for the fabrication of functional direct methanol fuel cell electrodes, magnetic alloys for media storage applications, high k dielectric films for alternative energy storage devices and alternative materials for solid oxide fuel cell cathodes, respectively, are performed. Layer-by-layer deposition with masking is used to fabricate nanometer scale capacitors. Finally, plasma spray technology is combined with the rapid expansion of supercritical solvents technique to form a novel, patent pending, process that is used to fabricate next generation photovoltaic cells. ...|$|R
40|$|This {{dissertation}} {{analyzes the}} form, character, {{and variety of}} materials with which specific forms of value are produced and maintained in craft. Two craft sites provide {{the foundation for the}} work I present: a bookbinding workshop in Cambridge, UK and a knitting guild in San Francisco, CA. Participant observation and interviews allow for a detailed examination of craft practice. In binding, the durability of the book and respect for the book's history are two central concerns, continually balanced through human-material interaction. In knitting, care invested in the artifact and anticipation of future use organizes production. Tied to familial pastimes and ancient histories, restored books and knit shawls become agents of recovery — of fading techniques, of cultural traditions, and of intimate interactions. Traces of creation, time, and use are valued for their emotional resonance in addition to the pragmatic goals in which they are embedded. Reflecting on the practices of bookbinders and knitters, I introduce the analytic category of "material traces" to the study of design and technology. Material traces concretize a unique location in time and space to reveal the dynamic and evocative nature of form. In my conceptualization, they embody and reflect skill, use, and time; they evoke memories and confer value. In the workshop, binders trace backward: they maintain certain material traces of time and use (creases on a book spine, scent of aged leather) and selectively mask material traces of restoration skill (replaced leather or stained paper) to expose marks of provenance deemed appropriate. In the guild, knitters trace forward: they foretell suitable fits and pleasing patterns through removed or intricate stitches, material traces of time and skill that are often missing or obscured for future recipients of the knit artifact. The relative invisibility of knitting labor enables a sense of `cleverness' or `secret society' that digital technologies (blogs, pod casts, social networking sites) sometimes threaten to unravel by exposing additional traces of craft production. Using this analytic category, I develop a <b>heuristic</b> for examining <b>technology</b> that focuses on material traces of skill, use, and time. The heuristic is put into practice in the analysis of Spyn, mobile phone software that I designed to associate digital records (audio/visual media, text, and geographic data) with physical locations on knit fabric. The heuristic renders visible the stories of technique and spatiotemporal rhythm that imbue the knitted artifact with additional (digital) marks of production. In addition to tracing forward, knitters use Spyn to trace backward. Taking lessons from this analysis, I then present a framework for design pedagogy — using the lenses of attributes, entanglements, and rhythms to gain critical purchase on the artifacts being produced. Mobilizing this framework within a classroom, students envisioned evocative relationships to the non-human (rodents), enriched connections to a familial hand, engaged physics learning, and opportunities for reminiscence around breakage. These design examples reveal how the analytic category of material traces comes alive in practice and pedagogy. Based on these insights, I demonstrate a research agenda for design that emphasizes temporality and materiality...|$|R
40|$|Electrocoagulation is an {{empirical}} (and largely <b>heuristic)</b> water treatment <b>technology</b> {{that has had}} many different applications over the last century. It has proven its viability by removing {{a wide range of}} pollutants. The approach to reactor design has been haphazard, however, with little or no reference to previous designs or underlying principles. This thesis reviewed these reactor designs, identifying key commonalities and synthesising a new design hierarchy, summarised by three main decisions: 1. Batch or continuous operation; 2. Coagulation only or coagulation plus flotation reactors, and; 3. Associated separation process if required. This design decision hierarchy thereby provides a consistent basis for future electrocoagulation reactor designs. Electrochemistry, coagulation, and flotation are identified as the key foundation sciences for electrocoagulation, and the relevant mechanisms (and their interactions) are extracted and applied in an electrocoagulation context. This innovative approach was applied to a 7 L batch electrocoagulation reactor treating clay-polluted water. Structured macroscopic experiments identified current (density), time, and mixing as the key operating parameters for electrocoagulation. A dynamic mass balance was conducted over the batch reactor, for the first time, thereby enabling the extraction of a concentration profile. For this batch system, three operating stages were then identifiable: lag, reactive, and stable stages. Each stage was systematically investigated (in contrast to the previous ad hoc approach) with reference to each of the foundation sciences and the key parameters of current and time. Electrochemical behaviour characterised both coagulant and bubble generation. Polarisation experiments were used to determine the rate-limiting step at each electrode's surface. Consequently the appropriate Tafel parameters were extracted and hence the cell potential. At low currents both electrodes (anode and cathode) operated in the charge-transfer region. As the current increased, the mechanism shifted towards the diffusion-limited region, which increased the required potential. Polarisation experiments also define the operating potential at each electrode thereby enabling aluminium's dissolution behaviour to be thermodynamically characterised on potential-pH (Pourbaix) diagrams. Active and passive regions were defined and hence the aluminium's behaviour in an aqueous environment can now be predicted for electrocoagulation. Novel and detailed solution chemistry modelling of the metastable and stable aluminium species revealed the importance of oligomer formation and their rates in electrocoagulation. In particular, formation of the positively trimeric aluminium species increased solution pH (to pH 10. 6), beyond the experimentally observed operable pH of 9. Thereby signifying the importance of the formation kinetics to the trimer as the active coagulant specie in electrocoagulation. Further leading insights to the changing coagulation mechanism in electrocoagulation were possible by comparison and contrast with the conventional coagulation method of alum dosing. Initially in the lag stage, little aggregation is observed until the coagulant concentration reaches a critical level. Simultaneously, the measured zeta potential increases with coagulant addition and the isoelectric point is attained in the reactive stage. Here a sorption coagulation mechanism is postulated; probably charge neutralisation, that quickly aggregates pollutant particles forming open structured aggregates as indicated by the low fractal dimension. As time progresses, pollutant concentration decreases and aluminium addition continues hence aluminium hydroxide/oxide precipitates. The bubbles gently sweep the precipitate through the solution, resulting in coagulation by an enmeshment mechanism (sweep coagulation). Consequently compact aggregates are formed, indicating by the high fractal dimension. Flotation is an inherent aspect of the batch electrocoagulation reactor via the production of electrolytic gases. In the reactor, pollutant separation occurs in situ, either by flotation or settling. From the concentration profiles extracted, original kinetic expressions were formulated to quantify these competing removal processes. As current increases, both settling and flotation rate constants increased due to the additional coagulant generation. This faster removal was offset by a decrease in the coagulant efficiency. Consequently a trade-off exists between removal time and coagulant efficiency that can be evaluated economically. A conceptual framework of electrocoagulation is developed from the synthesis of the systematic study to enable a priori prediction. This framework creates predictability for electrocoagulation, which is innovative and original for the technology. Predictability provides insights to knowledge transfer (between batch and continuous), efficient coagulant and separation path, {{to name just a few}} examples. This predictability demystifies electrocoagulation by providing a powerful design tool for the future development of scaleable, industrial electrocoagulation water treatment design and operation process...|$|R

