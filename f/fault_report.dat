9|373|Public
5000|$|In Australia, {{there are}} {{numerous}} satirical news websites including The Shovel, The Betoota Advocate, The (Un)Australian, The <b>Fault</b> <b>Report,</b> The Sauce and The Tunnel Presents. The Shovel mainly satirizes the Australian political and social culture and The Betoota Advocate satirizes the political right and Australian journalism. In February 2015, The Betoota Advocate shot to fame after the publication's editor's sneaked in to the media scrum outside Parliament House in Canberra during a leadership spill motion and managed to interview some of Australia's most high-profile media personalities and politicians, posing as legitimate journalists. The fallout from The Betoota Advocate stunt {{has led to a}} security increase surrounding parliamentary media and screening of all crew. The <b>Fault</b> <b>Report</b> [...] was established in 2014 and also has a political editorial focus. British-born Australian author John Birmingham once described The <b>Fault</b> <b>Report</b> as, [...] "Like The Onion. But with Vegemite", on his blog Cheeseburger Gothic. The Tunnel Presents, which has been online since June 2011, is by Brisbane-based satire writing team The Tunnel and has political and social satire stories with a Queensland focus.|$|E
50|$|EESL is {{successfully}} {{implementing the}} energy efficiency schemes like UJALA (Unnat Jyoti by Affordable LEDs for All), Street Light National Programme (SLNP), National Energy Efficient fan distribution programme, Efficient Buildings programme. EESL is also implementing the world’s largest Agricultural Demand Side Management programme (AgDSM) wherein the Energy Efficient Pumpsets (EEPS) with Smart control Panel are being distributed to farmers free of cost. Smart control panels would enable facilities like remote (Mobile based) start/ Stop, <b>fault</b> <b>report</b> by SMS, etc.|$|E
50|$|Anomaly Report (AR): To {{document}} {{any event}} that occurs during the testing process that requires investigation. This {{may be called}} a problem, test incident, defect, trouble, issue, anomaly, or error report. This document is deliberately named as an anomaly report, and not a <b>fault</b> <b>report.</b> The reason is that a discrepancy between expected and actual results can occur {{for a number of}} reasons other than a fault in the system. These include the expected results being wrong, the test being run incorrectly, or inconsistency in the requirements meaning that more than one interpretation could be made. The report consists of all details of the incident such as actual and expected results, when it failed, and any supporting evidence that will help in its resolution. The report will also include, if possible, an assessment of the impact of an incident upon testing.|$|E
50|$|This {{concept is}} {{applicable}} to mission critical systems that incorporate active redundancy and <b>fault</b> <b>reporting.</b> It is also applicable to non-mission critical systems that lack redundancy and <b>fault</b> <b>reporting.</b>|$|R
50|$|<b>Fault</b> <b>reporting</b> {{eliminates}} {{maintenance costs}} associated manual diagnostic testing.|$|R
50|$|This {{concept is}} related to {{condition-based}} maintenance and <b>fault</b> <b>reporting.</b>|$|R
40|$|Abstract: Multimedia {{assisted}} teaching plays a {{more and}} more important role in modern teaching. It not only can improve the teaching quality, but also provides a platform for the reform of traditional teaching mode and method. For at present classroom utilization rate is low, the <b>fault</b> <b>report</b> and maintenance is not timely, its separate control {{is not easy to}} manage, and so on, a set of centralized control system for multimedia classroom equipment is developed by increasing the necessary hardware on the basis of existing multimedia equipment, which is based on network and wireless data transmission technology. A main controller and its peripheral equipment interface circuit are designed, including hardware and software. The system can improve the utilization rate and service life of multimedia equipment, convenient the effective management...|$|E
40|$|Wireless ad-hoc {{networks}} (WANET) with multi-hop communication {{are subject}} {{to a variety of}} faults and attacks, and detecting the source of any fault is highly important to maintain the quality of service, confidentiality, and reliability of an entire network operation. Intermediate byzantine nodes in WANET could subvert the system by altering sensitive routed information unintentionally due to many reasons such as power depletion, software bug, malware, and environmental obstacles. This thesis highlights some of the research studies done in the area of distributed fault detection (DFD) and proposes a solution to detect Byzantine behavior cooperatively. The present research will focus on designing a scalable distributed fault detection (DFD) algorithm to detect byzantine nodes who permanently try to distort or reroute information while relaying a message from one node to another, complimentary to that, a symmetric distributed cryptography scheme will be employed to continuously validates the data integrity of a routed message. The main hypothesis of the research is that if a wireless ad-hoc network is been divided into N number of groups (classes) with relatively equal number of members, each group of nodes can cooperatively protect the network from every other group. Practically, each group of nodes will be assigned to a distinct shared key; nodes with similar group assignment shall guard the integrity of a routing path by incorporating their own secret message authentication code (MAC) that can be only validated by nodes belonging to the same group contributing to the same routing path. If a node from Group(i) detects a tampering event, it should either store and delay a <b>fault</b> <b>report</b> or embed a <b>fault</b> <b>report</b> to the same routed message and forward it to the Master Node (Destination) if applicable. Further report message overhead optimization has been devised to reduce the energy cost. Moreover, the empirical results have shown that the more reported evidence the master node can collect, the more accuracy of detection can be reached based on an incremental stream of evidence that contains information about both healthy and unhealthy nodes; so that every healthy report type can justify the unhealthy false report. The heuristic simulation based study considered many different aspects of the system for evaluation such as detection accuracy, fault model, the optimal number of classes, energy consumption, the impact of mobility, and network lifetime. The iGraph network simulation tool has been employed for visualization and graph manipulation, whereas, Python programming language has been utilized in conjunction to implement and simulate the DFD algorithm and generate the results...|$|E
40|$|Today’s {{construction}} buildings {{contain more}} advanced technologies than before. The development of newer and stricter regulations {{to the construction}} industry makes the door environment more and more complex. In large construction projects the door environment is held up as a major challenge to get to in a good way. It turned out after a discussion with one project manager and a site manager at NCC, that the door environment is difficult to get right in major construction projects and it often causes many error reports in to the aftermarket after that the final inspection is completed. Interviews have been done with key figures in the building process at NCC and Akademiska hus. A study on a university building for around 3500 students and with 300 workplaces was made. There {{were a lot of}} data with fault reports to the go trough to find <b>fault</b> <b>report</b> from doors and be analyzed and statistic. The results from the data collection showed that many fault messages on the doors in was caused by the door operator. And from the interviews the content was clear that the coordination has to be good to have a functioning and a good door environment. One part that can contribute to a better door environment is to plan space for possible door operator over the door. Another part is that the construction documents which production workers are working with, is {{to find a way to}} always have the latest documents in hand...|$|E
5000|$|... less {{reliable}} than equipment with <b>fault</b> <b>reporting</b> associated with CBM ...|$|R
5000|$|Checking any <b>fault</b> <b>reports</b> {{to limit}} {{possible}} hardware problems prior to test ...|$|R
50|$|All modern {{computers}} {{provide the}} following when an existing feature is enabled via <b>fault</b> <b>reporting.</b>|$|R
40|$|Forer (1949) was {{the first}} {{researcher}} to demonstrate the 2 ̆ 2 Barnum Effect 2 ̆ 2 : People are highly impressed by bogus personality feedback that consists of Barnum statements (high social desirability, high endorsement rate). The present thesis included two studies related to the Barnum Effect. Study 1 aimed to create a pool of statements (Barnum, Rare Virtues and Common Faults) that varied in their levels of social desirability and endorsement rates. In Study 1, Group A, 120 participants rated 185 items for True of Self or Social Desirability. In Study 1, Group B, 70 participants rated the same items for True of Others or Social Desirability. Contrary to expectation, no statement in the two parts of Study 1 met the criteria of a Rare Virtue item (high social desirability, low endorsement rate), suggesting that Rare Virtue items are difficult if not impossible to create. Ten Barnum items were identified in Study 1, as were 10 Common Fault items (high endorsement rate, with lower social desirability than other statements with similar endorsement rates). In Study 2, Barnum and Common Fault items from Study 1 were used to create and test three types of reports: (1) Barnum, (2) Mixed, (3) Common Fault, that came in Personalized and non-Personalized forms. Participants completed a personality test and were given one generalized personality report. Reports were rated using a 13 -question Satisfaction Survey. Results showed a main effect of type of report (F (2, 169) = 17. 05, p 3 ̆c. 001), with Barnum and Mixed reports receiving significantly higher ratings than the Common <b>Fault</b> <b>report.</b> A main effect of Personalization (F (2, 169) = 44. 29, p 3 ̆c. 001) was also found, with Personalized reports receiving higher ratings than non-Personalized reports. The implications of both studies are discussed. ...|$|E
40|$|This paper {{presents}} the results of statistical and engineering analysis of Loss of Offsite Power (LOOP) events registered in four reviewed databases. The paper includes events registered in IRSN (Institut de Radioprotection et de Sûreté Nucléaire) SAPIDE and GRS (Gesellschaft für Anlagen- und Reaktorsicherheit mbH) VERA database in time period 1992 to 2011. The Nuclear Regulatory Commission (NRC) Licensee Event Reports (LERs) database and the IAEA International Reporting System (IRS) database are screened for the relevant events registered in period 1990 to 2013. In total, 228 relevant events were identified in the IRSN database, 190 in GRS, 120 in LER and 52 in IRS. The data include events registered both during the critical (at power) and shutdown operation of the plants. The identified events were classified considering nine different categories. In the three databases (SAPIDE, VERA, IAEA-IRS) the largest numbers of events are registered for the plant centered category. The largest number of the events in the NRC-LER database is found for switchyard centered events. According to the mode of operation, most events were reported during critical power operation, in all four databases. The "Partial loss of external power" events are the most frequent type of event found in the IRSN and NRC databases while the "Physical loss of electrical busbars" is the main type in the GRS and IAEA databases. The largest number of events in all databases is identified for the switchyard failures followed by the interconnections failures (both lines and transformers). Mainly LOOP event are identified by the <b>fault</b> <b>report</b> in the control room. Electrical deficiency is detected as the main direct cause of events. Environment is registered as the main contributor for the electrical grid deficiency in the French and NRC databases. Electrical failures are dominant contributor to the electrical grid deficiency in the German and IAEA databases. The principal root cause for the LOOP events are human failures with the human errors during test, inspection and maintenance as the largest sub-group. The largest number of the LOOP events resulted in reactor trip followed by the Emergency Diesel Generator (EDG) start. The majority of the reported LOOP events lasted for more than 2 minutes. Main lessons learned from the analysed events and main recommendations for decrease of the number of LOOP events are presented. JRC. G. 10 -Knowledge for Nuclear Security and Safet...|$|E
40|$|Maintenance is {{the main}} part of product {{lifecycle}} management (PLM). Thus, the solution suitable for equipment maintenance can be easily extended to other industry equipment maintenance processes. While most of the current maintenance service management only rely on empirical experience, ignoring the dynamic information uncertainties and lacking flexible reactions hinder the effectiveness of the maintenance task assignment. Moreover, without a systematic and dynamic consideration of various related parameters, more limitations are exposed to current approaches. As such, it is of great importance to automate the maintenance process, consider all related parameters and cope with the dynamic uncertainties to endow equipment maintenance processes with smart and fast reaction capability, flexibility, and robustness. Among the first to address this problem, {{with the help of the}} Internet of Things (IoT) and cloud technologies, this thesis systematically proposes the real-time Cloud-enabled equipment maintenance service (CEMS). These new technologies can enable real-time equipment status data collection, advanced and proactive response maintenance as well as improved process visibility. By using the proposed CEMS, it results in a significant improvement in processing the equipment <b>fault</b> <b>report,</b> making a quick response, and dynamically assigning the maintenance tasks, enhancing the task management ability of the maintenance center. To achieve the overall aim of this research, three typical scenarios are examined as follows: Scenario Ⅰ: In this scenario, an overview of equipment maintenance service is studied and analyzed. Equipment maintenance service process is an important part of after-sales service, this study has taken a broad view at maintenance service and the corresponding research issues emerging in this field. ScenarioⅡ: In this scenario, a cloud-enabled platform for EMS assignment and control is developed, inspired by an industrial case of lift after-sales maintenance service. The presented cloud-based equipment maintenance service (CEMS) platform integrates various methods and techniques into a suite of applications for EMS assignment and control management, including intelligent inspection, equipment fault information processing, maintenance assignment and maintenance service execution control. Through the cloud computing and IoT technologies, a real-time maintenance service assignment and execution control platform is established. Heterogeneous physical equipment and manpower can be easily traced, tracked, and managed. Scenario Ⅲ: This scenario considers a maintenance service area where the maintenance service department operator needs to designate different kinds of maintenance task to corresponding maintenance service staff/agent. As the volume of maintenance demand increases quickly, the quality, efficiency, and robustness of such task assignment process require comprehensive study and analysis. Therefore, the major challenges of this work are how to the assign the maintenance task to the most suitable maintenance team. We propose an efficient assignment mechanism to minimize the total maintenance cost by dynamically assigning each task with most appropriate maintenance team. In this dissertation, the cloud enables and real-time advanced receiving, assignment and control problems in equipment maintenance are investigated and related maintenance assign task mechanisms and platform technologies are developed as well. This study of equipment maintenance service shall be of great value not only to researchers who desire to extend their research into this new area but also to practitioners who are interested in equipment fault prognosis by using the real-time equipment operation information. published_or_final_versionIndustrial and Manufacturing Systems EngineeringMasterMaster of Philosoph...|$|E
50|$|<b>Fault</b> <b>reporting</b> is a {{maintenance}} concept that increases operational availability and that reduces operating cost through three mechanisms.|$|R
50|$|Active {{redundancy}} can {{be integrated}} with <b>fault</b> <b>reporting</b> to reduce down time to a few minutes per year.|$|R
40|$|Analysis {{techniques}} from safety-critical development, such as PHA or HazOp, {{shall be}} used on DAIM, a system used at IME for starting, delivering and finishing master thesis. Documents to be analyzed are from the specification and design phase of the system development. The results obtained from using these techniques shall be compared with existing <b>fault</b> <b>reports</b> with actual <b>faults</b> as have been discovered in the system. These <b>fault</b> <b>reports</b> shall also be treated and analyzed...|$|R
5000|$|Diagnostic {{down time}} is {{required}} to identify {{the amount of time}} spent perform maintenance when <b>fault</b> <b>reporting</b> does not support condition-based maintenance.|$|R
50|$|As another example, {{enabling}} <b>fault</b> <b>reporting</b> for Internet network packet delivery failure {{will increase}} network loading when {{the network is}} already busy, and that will cause total network outage.|$|R
50|$|<b>Fault</b> <b>reporting</b> is {{localised}} {{and system}} failure is generally only uncovered {{as a consequence}} of customer complaint. The fixed telecommunications network consists {{of a wide variety of}} mostly old technologies, some of which are obsolete.|$|R
50|$|Other {{kinds of}} <b>fault</b> <b>reporting</b> {{involves}} painting green, yellow, and red zones onto temperature gages, pressure gages, flow gages, vibration sensors, strain gages, and similar sensors. Remote viewing {{can be implemented}} using a video camera.|$|R
40|$|The {{traditional}} Automatic Test Equipment (ATE) {{systems are}} insufficient {{to cope with}} the challenges of testing more and more complex avionics systems. In this study, we propose a general method for module automatic testing in the avionics test platform based on PXI bus. We apply virtual instrument technology to realize the automatic testing and the <b>fault</b> <b>reporting</b> of signal performance. Taking the avionics bus ARINC 429 as an example, we introduce the architecture of automatic test system as well as the implementation of algorithms in Lab VIEW. The comprehensive experiments show the proposed method can effectively accomplish the automatic testing and <b>fault</b> <b>reporting</b> of signal performance. It greatly improves the generality and reliability of ATE in avionics systems...|$|R
40|$|A good {{interaction}} between public administrations and citizens is imperative in modern smart cities. Semantic web technologies can aid in achieving such a goal. We present a smart urban <b>fault</b> <b>reporting</b> web platform to help citizens in reporting common urban problems, such as street faults, potholes or broken street lights, {{and to support}} the local public administration in responding and fixing those problems quickly. The tool {{is based on a}} semantic data model designed for the city, which integrates several distinct data sources, opportunely re-engineered to meet the principles of the Semantic Web and linked open data. The platform supports the whole process of road maintenance, from the <b>fault</b> <b>reporting</b> to the management of maintenance activities. The integration of multiple data sources enables increasing interoperability and heterogeneous information retrieval, thus favoring the development of effective smart urban <b>fault</b> <b>reporting</b> services. Our platform was evaluated in a real case study: a complete urban reporting and road maintenance system has been developed for the municipality of Catania. Our approach is completely generalizable and can be adopted by and customized for other cities. The final goal is to stimulate smart maintenance services in the "cities of the future"...|$|R
40|$|Abstract. Improving {{software}} processes {{relies on}} the ability to analyze previous projects and derive which parts of the process that should be focused on for improvement. All software projects encounter software faults during development and have to put much effort into locating and fixing these. A lot of information is produced when handling <b>faults,</b> through <b>fault</b> <b>reports.</b> This paper reports a study of <b>fault</b> <b>reports</b> from industrial projects, where we seek a better understanding of faults that have been reported during development and how this may affect the quality of the system. We investigated the fault profiles of five business-critical industrial projects by data mining to explore if there were significant trends in the way faults appear in these systems. We wanted to see if any types of faults dominate, and whether some types of <b>faults</b> were <b>reported</b> as being more severe than others. Our findings show that one specific fault type is generally dominant across reports from all projects, and that some fault types are rated as more severe than others. From this we could propose that the organization studied should increase effort in the design phase in order to improve software quality. ...|$|R
25|$|Maintenance on the ICE trains {{is carried}} out in special ICE {{workshops}} located in Basel, Berlin, Cologne, Dortmund, Frankfurt, Hamburg, Leipzig and Munich. The train is worked upon at up to four levels {{at a time and}} <b>fault</b> <b>reports</b> are sent to the workshops in advance by the on-board computer system to keep maintenance time at a minimum.|$|R
40|$|International audienceWe {{consider}} {{a group of}} players who perform tasks repeatedly. The players are nodes of a communication network and observe their neighbors' actions. Players have partial knowledge of the network and only know their set of neighbors. We study the existence of protocols for fault reporting: whenever a player chooses a faulty action, the communication protocol starts and the output publicly reveals {{the identity of the}} faulty player. We consider two setups. In the first one, players do not share authentication keys. We show that existence of a protocol for <b>fault</b> <b>reporting</b> is equivalent to the 2 -vertex-connectedness of the network: no single vertex deletion disconnects the graph. In the second setup, we allow players to share authentication keys. We show that existence of a distribution of the keys and of a protocol for <b>fault</b> <b>reporting</b> is equivalent to the 2 -edge-connectedness of the network: no single edge deletion disconnects the graph. We give applications to the implementation of socially optimal outcomes in repeated games...|$|R
40|$|Abstract. Faults {{introduced}} into systems during development are costly to fix, and especially so for business-critical systems. These systems are developed using common development practices, but have high requirements for dependability. This paper reports {{on an ongoing}} investigation of <b>fault</b> <b>reports</b> from Norwegian IT companies, where {{the aim is to}} seek a better understanding on faults that have been found during development and how this may affect the quality of the system. Our objective in this paper is to investigate the fault profiles of four business-critical commercial projects to explore if there are differences in the way faults appear in different systems. We have conducted an empirical study by collecting <b>fault</b> <b>reports</b> from several industrial projects, comparing findings from projects where components and reuse have been core strategies with more traditional development projects. Findings show that some specific fault types are generally dominant across reports from all projects, and that some fault types are rated as more severe than others. 1...|$|R
40|$|In this master thesis we have {{analyzed}} the software system DAIM, {{which is a}} web-based delivery system used at NTNU in connection with master theses and master students, with respect to software faults. Based on the documentation from the design stage of the DAIM project we have performed a technique called Preliminary Hazard Analysis (PHA), which is an analysis technique from safety-critical development. The results from this analysis have been compared with existing <b>fault</b> <b>reports</b> containing actual <b>faults</b> discovered in the system. Some of the intention behind our work has been to find if hazards identified with PHA {{can be related to}} actual faults found in the <b>fault</b> <b>reports.</b> In [17] it is stated that correcting software faults in later phases of the software development is much more expensive than in earlier phases and we have performed the PHA to see if some of the faults could have been avoided. We found that there were some connections between some of the faults and hazards identified, but the results were not entirely as expected. In our previous work we did a similar kind of analysis as we have done in this work regarding the analysis of <b>fault</b> <b>reports</b> and we have compared the results from our previous work with some of the results that we have obtained from this work to see how the distribution of fault types varies between the projects. The results showed that there were several differences between the projects, but some similarities were also discovered. </p...|$|R
40|$|Universal voltage, power-factor-corrected {{power supply}} • Very low inrush current • 100 watts x 4 {{channels}} in 1 U chassis • Built-in load monitoring • Remote <b>fault</b> <b>reporting</b> and redundancy switching • Optional 4 channel 70 / 100 volt transformers in 1 U chassis • Adjustable high-pass filters and remote level controllable • Advanced dynamics control adjusts for sensitivity setting, load Z and temperatur...|$|R
50|$|In network {{management}}, {{fault management}} is {{the set of}} functions that detect, isolate, and correct malfunctions in a telecommunications network, compensate for environmental changes, and include maintaining and examining error logs, accepting and acting on error detection notifications, tracing and identifying faults, carrying out sequences of diagnostics tests, correcting <b>faults,</b> <b>reporting</b> error conditions, and localizing and tracing faults by examining and manipulating database information.|$|R
50|$|The {{historic}} AN/UYK-43 architecture includes active redundancy. It includes multiple processors, multiple memory banks, {{and multiple}} input-output devices with interfaces for multiple disk drives. Power-on self test firmware incorporates features that reconfigure software loading {{in order to}} bypass failure. This allows it to run in degraded mode with failed processors, failed memory, failed disk drives, and failed input/output devices. Remote status boards perform <b>fault</b> <b>reporting.</b>|$|R
40|$|A rule-based, {{system-level}} {{fault detection}} and diagnostic (FDD) method for HVAC systems was developed. It functions as an interface between multiple, equipment-specific FDD tools and a human operator. The method resolves and prioritizes conflicting <b>fault</b> <b>reports</b> from equipment-specific FDD tools, performs FDD {{at the system}} level, and presents an integrated view of an HVAC system’s fault status to an operator. A simulation study to test and evaluate the method was conducted...|$|R
50|$|<b>Fault</b> <b>reporting</b> is an {{optional}} feature {{that can be}} forwarded to remote displays using simple configuration setting in all modern computing equipment. The system level of reporting that is appropriate for Condition Based Maintenance are critical, alert, and emergency, which indicate software termination due to failure. Specific failure reporting, like interface failure, can be integrated into applications linked with these reporting systems. There is no development cost if these are incorporated into designs.|$|R
30|$|The MapReduce P 2 P {{framework}} {{can also}} be applied to distributed computing applications, especially applications running on P 2 P networks. These applications process large amount of data on distributed workstations on the Internet. We apply this framework to improve the computation component of DisCaRia, a distributed case-based reasoning (CBR) system for resolving faults in network and communication systems [28, 29, 30, 31, 32, 33]. DisCaRia takes advantage of P 2 P technology to extend the conventional CBR systems [3], thus exploring problem solving knowledge resources in distributed environments, such as expert communities, ticket tracking systems (TTSs), forums and archives. Each peer contains an independent CBR component and exploits knowledge resources in parallel; the system therefore enhances the performance of managing huge datasets on various peers {{and the quality of}} various output solutions. The main disadvantage of DisCaRia is high computation cost and low efficiency of the computation component as the size of fault datasets increases. Note that <b>fault</b> <b>reports</b> contain several symptoms, error messages, distinct keywords, etc. MapReduce operations can deal with this problem by processing a large number of <b>fault</b> <b>reports</b> on various peers quickly and efficiently.|$|R
40|$|This {{thesis is}} a {{feasibility}} study {{for a more}} comprehensive work at Process Automation (PA) ABB, Västerås. The business units this work is focusing on are Mining and Crane Systems with the mission to map their quality processes within supply collaboration, acceptance control and data such as their supplier’s fault frequency. This thesis was initiated because of an internal audit where these business units at PA received remarks on their long term quality development and that they didn’t calculated supplier faults. To evaluate these business units and to find {{the root causes of}} these problems, the involved staff was interviewed and their internal documentation database was examined. The work method that is used in this thesis is ABB’s 4 Q and it’s practiced in quality development projects at ABB. The thesis report is also structured after 4 Q. From the interviews could the conclusion be made that the remarks from the internal audit was correct, but that the business units had different root causes to their problems. It was made clear that Mining had problems with <b>reporting</b> <b>faults</b> found at site. It’s not possible to calculate metrics, that global ABB determined shall be used, if the occurring <b>faults</b> aren’t <b>reported.</b> A plausible root cause to why there are insufficient <b>faults</b> <b>reported</b> is that the business units’ decision-making authority haven’t succeed to reach out with enough information to the staff {{about the importance of the}} process. It has led to a staff that isn’t aware of the real usefulness of <b>fault</b> <b>reporting.</b> Even the 100 k SEK guideline showed to be a root cause to the problem. This guideline hampers the possibility to collect relevant data and goes against ABB’s regulations to generate statistics. It makes it impossible to monitor field failure rates (fault at site) when minor fault are ignored because of the guideline. Crane Systems have on the contrary a functioning <b>fault</b> <b>reporting</b> process, but their shortage is in the generating of statistics. Even though the business unit has enough data to calculate and monitor their supplier’s error rate in the forms of lots accepted and PPM, are the statistics not communicated further. The statistics are neither reported internally within the organization, to the supplier or to central ABB. Both product and system managers generate their own statistics for their specific responsibility areas and needs, which mean that the general picture gets lost and the suppliers total fault frequency can’t be found. Summarized are these deficiencies leading to poor feedback to the suppliers about their delivered quality, and consequently makes it hard for the suppliers to know what to improve...|$|R
