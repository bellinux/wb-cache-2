56|20|Public
40|$|In this paper, {{we present}} a {{statistical}} model for linking <b>edgels,</b> which yields two models for edgel clustering. The first model favors large clusters, whereas the second one favors small sub-clusters. We show how the first model can significantly help localizing a shape in an image. The second model might be useful for other applications, such as clustering <b>edgels</b> into objects. 1...|$|E
40|$|Abstract In this paper, {{we present}} a {{statistical}} model for linkingedgels, which yields two models for edgel clustering. The first model favors large clusters, whereas the second onefavors small sub-clusters. We show how the first model can significantly help localizing a shape in an image. The sec-ond model might be useful for other applications, such as clustering <b>edgels</b> into objects. 1. Introduction Partitioning <b>edgels</b> into clusters that represent objects in animage is a hard problem which is still open as of now...|$|E
40|$|Existing {{theories}} on shape digitization {{are not very}} realistic: they impose strong constraints on feasible shapes, and require measurements {{to be free of}} error. In this paper, we propose a new approach based on Delaunay triangulation and α-shapes which significantly weakens these restrictions. It assumes that sampling points (<b>edgels)</b> represent true object edges with a certain bounded error. We are able to prove under which conditions a topologically correct segmentation can be reconstructed from the <b>edgels.</b> Experiments on real and generated images demonstrate the good performance of the new method and confirm the predictions of our theory...|$|E
30|$|<b>Edgel</b> {{matching}} Overlay {{the original}} with segmented image and compute correspondence via min-cost assignment on bipartite graph.|$|R
5000|$|A & R for Tiretown Records in Akron, Ohio. Had a {{regional}} hit with [...] "Five More Miles To Christmas" [...] (BMI Songs of American Int'l Music)Also written by <b>Edgel</b> Groves.|$|R
5000|$|<b>Edgel</b> Groves is a {{recording}} artist/singer/songwriter who recorded the Hit single [...] "Footprints in the Sand" [...] {{based on the}} anonymous poem of the same name. in 1981. The song sold millions of copies and the record was #1 in [...] "country music" [...] and was a cross over hit and became #1 in Christian and country gospel. It was also voted the number one requested song of 1981 by American radio DJs. <b>Edgel</b> Groves was the original singer to record [...] "Footprints in the Sand" [...] and the only singer to have [...] "Footprints in the Sand" [...] as a hit in the music charts. His number one hit is credited with making the poem famous.|$|R
40|$|We {{present an}} {{algorithm}} that extracts curves from {{a set of}} <b>edgels</b> within a specific class in a decreasing order of their "length". The algorithm inherits the perceptual grouping approaches. But, instead of using only local cues, a global constraint is imposed to each extracted subset of <b>edgels,</b> that the underlying curve belongs to a specific class. In {{order to reduce the}} complexity of the solution, we work with a linearly parameterized class of curves, function of one image coordinate. This allows, first, to use a recursive Kalman based fitting and, second, to cast the problem as an optimal path search in an directed graph. Experiments on finding lane-markings on roads demonstrate that real-time processing is achievable...|$|E
40|$|A {{relaxation}} algorithm for the computation of optic flow at edge elements (<b>edgels)</b> is presented. Flow {{is estimated}} only at intensity {{edges of the}} image. <b>Edgels,</b> extracted from an intensity image, are {{used as the basis}} for the algorithm. A matching strength or weight surface is computed around each edgel and neighbourhood support is obtained to enhance the matching strength. A principal moments method is used to determine the flow from this weight surface. The output of the algorithm is, for each edgel a pair of orthogonal components of the estimate of the flow. Associated with each component is a confidence measure. Examples of the output of the algorithm are given, and tests of its accuracy are discussed. © 1988...|$|E
40|$|This paper {{describes}} {{a method for}} the detection of textureless objects. Our target objects include furniture and home appliances, which have no rich textural features or characteristic shapes. Focusing on the ease of application, we define a model that represents objects in terms of three-dimensional <b>edgels</b> and surfaces. Object detection is performed by superimposing input data on the model. A two-stage algorithm is applied to bring out object poses. Surfaces are used to extract candidates fromthe input data, and <b>edgels</b> are then used to identify the pose of a target object using two-dimensional template matching. Experiments using four real furniture and home appliances were performed to show the feasibility of the proposed method. We suggest the possible applicability in occlusion and clutter conditions...|$|E
40|$|Many {{problems}} in computer vision require estimation of both model parameters and boundaries, which limits {{the usefulness of}} standard estimation techniques from statistics. Example {{problems in}}clude surface reconstruction from range data, estimation of parametric motion models, fitting circular or elliptic arcs to <b>edgel</b> data, and many others. This paper introduces a new estimation technique, called the "Domain Bounding M-Estimator", which is a generalization of ordinary M-estimators combining error measures on model parameters and boundaries in a joint, robust objective function. Minimization of the objective function given a rough initialization yields simultaneous estimates of parameters and boundaries. The DBM-Estimator {{has been applied to}} estimating line segments, surfaces, and the symmetry transformation between two <b>edgel</b> chains. It is unaffected by outliers and prevents boundary estimates from crossing even small magnitude discontinuities...|$|R
40|$|A {{new method}} for plane {{hypothesis}} generation {{based on a}} sweeping process is presented. The method is feature based in contradiction to earlier methods which rely on image correlation. The features used are <b>edgel</b> elements (’edgels’) which are extracted in a preprocessing step. The experiments show an improvement in speed {{as well as in}} accuracy. ...|$|R
50|$|Carl <b>Edgel</b> Shaeffer (October 25, 1924 - October 25, 1974) was an American {{professional}} basketball player. He {{played for the}} Indianapolis Olympians in the National Basketball Association between 1949-50 and 1950-51 after a collegiate career at the University of Alabama. Shaeffer was Alabama's first-ever {{professional basketball}} player. He became a businessman in Indianapolis, Indiana after his short-lived NBA career.|$|R
40|$|This paper {{describes}} a stereo algorithm which matches connected chains of <b>edgels</b> (curves) between images. It {{is based on}} representing the curves as elastic strings and measuring the amount of deformation the strings have to undergo to transform between corresponding curves, and incorporates {{the ideas of the}} disparity gradient and the fact that matching sections of curve have to be of a similar shape. This explicit use of shape information means that a precisely known epipolar geometry is no longer crucial. Pairs of potentially corresponding curves which lead to a large deformation energy, are eliminated and the greatly reduced number of potentially matching pairs are passed on to a tree search stage. A typical result of running the algorithm on a stereo triple is presented. In the past, edge pixels (<b>edgels)</b> have usually been th...|$|E
40|$|We {{describe}} {{the application of}} algebraic polyhedral constraints to the computation of the 3 D structure and motion of polyhedral objects. The method, which works when complete 2 D linedrawing information is available, guarantees the recovery of planar faces. The normals to these faces are used for matching to models. Several examples are given to illustrate {{the scope of the}} method. In [1] Murray et al. describe a motion processing system, ISOR, which is able to recover the 3 D motion and structure of polyhedral objects from an image sequence and goes on, where possible, to recognize the object as one from a database of object models. The system performs a 'bottomup' pass through a vision processing hierarchy in the four stages: (i) Low level- Compute visual motion at intensity <b>edgels</b> in a sequence of time-varying imagery; (ii) Segmentation- Segment the <b>edgels</b> (and thereby visual motion) int...|$|E
40|$|A {{real-world}} {{limitation of}} visual servoing approaches is {{the sensitivity of}} visual tracking to varying ambient conditions and background clutter. The authors present a model-based vision framework to improve the robustness of edge-based feature tracking. Lines and ellipses are tracked using edge-projected integration of cues (EPIC). EPIC uses cues in regions delineated by edges that are defined by observed <b>edgels</b> and a priori knowledge from a wire-frame model of the object. The <b>edgels</b> are then used for a robust fit of the feature geometry, but at times this results in multiple feature candidates. A final validation step uses the model topology to select the most likely feature candidates. EPIC is suited for real-time operation. Experiments demonstrate operation at frame rate. Navigating a walking robot through an industrial environment shows the robustness to varying lighting conditions. Tracking objects over varying backgrounds indicates robustness to clutter. KEY WORDS—image feature tracking, model-based, robustness, real-time, framework 1...|$|E
5000|$|Producer/Artist/Songwriter for Lowery Music/Southern Tracks Records in Atlanta, Georgia. Had some Top 40 Regional Hits. Had 8 piece show {{band and}} toured extensively. Lowery's stable of artists included, Billy Joe Royal, Tommy Roe, The Atlanta Rhythm Section, Joe South, Dennis Yost & the Classics IV, Ray Stevens, Jerry Reed, Bertie Higgins, Ray Whitley and <b>Edgel</b> Groves.|$|R
40|$|In {{statistical}} shape analysis, {{the shape}} of an object is understood to be what remains after the effects of location, scale and rotation are removed. We consider the distributional problem of triangular shape and an associated direction; motivated by a data set of microscopic fossils. We begin by constructing a parallel transport system such that the data transform onto the space S 2 ×S 2. A joint shape distribution on S 2 ×S 1 is proposed based on Jupp & Mardia's bivariate distribution on S 2 ×S 1. For concentrated data, an approximation to the distribution on S 2 ×S 1 is given by a distribution on  1 ×S 1, and we explore a distribution on this space by extending Mardia & Sutton's distribution on  2 ×S 1. In this distribution, the expected <b>edgel</b> direction varies linearly in the shape coordinates. This is found to be a useful model for the microfossil data. Bookstein Coordinates, <b>Edgel,</b> Fisher Distribution, Kendall Coordinates, Microfossil Data, Triangle Shape, Von Mises Distribution,...|$|R
40|$|Abstract. An {{information}} theoretic {{framework for}} grouping observations is proposed. The entropy change incurred by new observations is analyzed using the Kalman filter update equations. It is found, that the entropy variation {{is caused by}} a positive similarity term and a negative proximity term. Bounding the similarity term {{in the spirit of the}} minimum description length principle and the proximity term in the spirit of maximum entropy inference a robust and efficient grouping procedure is devised. Some of its properties are demonstrated for the exemplary task of <b>edgel</b> grouping. ...|$|R
40|$|A {{visual system}} that {{interacts with the}} real world must be able to adapt to {{unexpected}} situations and to flexibly discover relevant visual cues. We present a method that allows incremental learning of discriminative features. The feature space includes juxtapositions of k oriented local pieces of edge (<b>edgels)</b> and is parameterized by k and the relative angles and distances between the <b>edgels.</b> Specific features are learned by sampling candidate features from this space, increasing k as needed, and retaining discriminative features. For recognition of an unknown scene or object, features are queried one by one. As a result of each query, zero or more candidate object classes are ruled out that do not exhibit this feature to a sufficient degree. We discuss issues of computational complexity, and present experimental results on two databases of geometric objects. Introduction A highly desirable ingredient of an artificial system that interacts {{with the real world}} is its abi [...] ...|$|E
40|$|Abstract — We present {{algorithms}} {{to detect}} and precisely localize curbs and stairways for autonomous navigation. These algorithms combine brightness information (in the form of <b>edgels)</b> with 3 -D data from a commercial stereo system. The overall system (including stereo computation) runs at about 4 Hz on a 1 GHz laptop. We show experimental results and discuss advantages and shortcomings of our approach. I...|$|E
40|$|Abstract In {{this paper}} {{we present a}} simple and robust method for {{self-correction}} of camera distortion using single images of scenes which contain straight lines. Since the most common distortion can be modelled as radial distortion, we illustrate the method using the Harris radial distortion model, but the method is applicable to any distortion model. The method is based on transforming the <b>edgels</b> of the distorted image to a 1 -D angular Hough space, and optimizing the distortion correction parameters which minimize the entropy of the corresponding normalized histogram. Properly corrected imagery will have fewer curved lines, and therefore less spread in Hough space. Since the method does not rely on any image structure beyond the existence of <b>edgels</b> sharing some common orientations and does not use edge fitting, it is applicable {{to a wide variety}} of image types. For instance, it can be applied equally well to images of texture with weak but dominant orientations, or images with strong vanishing points. Finally, the method is performed on both synthetic and real data revealing that it is particularly robust to noise...|$|E
5000|$|... "Footprints in the Sand" [...] is a 1980 song by <b>Edgel</b> Groves {{based on}} the {{anonymous}} poem Footprints in the Sand. The song, which became a one hit wonder for Groves, was written by Jerry Buckner and Gary Garcia of Buckner & Garcia. [...] The song begins with female chorus [...] "Footprints in the sand, he held me in his hand, and gave me strength to face the coming day,...", then enters into Groves' reading of the poem [...] "Last night I had a dream..." [...] The B-side is an instrumental version of the song with narration of the poem by disc jockey Johnny Dark.|$|R
40|$|We {{present a}} {{computational}} algorithm for shape perception {{by studying the}} interaction between region and boundary cues. We formulate this problem in a graph partitioning framework, where region cues defined on a pixel graph and boundary cues defined on its dual <b>edgel</b> graph are coupled through edge-node incidence relationships. The consistency of simultaneous partitioning on such graphs can thus be guaranteed. We generalize normalized cuts criteria and algorithms to this model for globally optimal solutions. We demonstrate that by incorporating boundary smoothness, objects with heterogeneous region properties can stand out as one group and objects with weak contours can be segmented more readily without the suppression from objects of high contrast. This model can also encode higher-order shape information and preliminary results on shape selection are given. 1...|$|R
40|$|Abstract. We {{propose a}} {{compositionality}} architecture for perceptual organization which establishes a novel, generic, algorithmic framework for feature binding and condensation of semantic information in images. The underlying algorithmic ideas require a hierarchical structure for {{various types of}} objects and their groupings, which are guided by gestalt laws from psychology. A rich set of predefined feature detectors with uncertainty that perform real-valued measurements of relationships between objects can be combined in this flexible Bayesian framework. Compositions are inferred by minimizing the negative posterior grouping probability. The model structure is founded on the fundamental perceptual law of Prägnanz. The grouping algorithm performs hierarchical agglomerative clustering and it is rendered computationally feasible by visual pop-out. Evaluation on the <b>edgel</b> grouping task confirms the robustness of the architecture and its applicability to grouping in various visual scenarios. ...|$|R
40|$|An {{open issue}} in {{multiple}} view geometry and structure from motion, applied to real life scenarios, is the sparsity of the matched key-points {{and of the}} reconstructed point cloud. We present an approach that can significantly improve the density of measured displacement vectors in a sparse matching or tracking setting, exploiting the partial information of the motion field provided by linear oriented image patches (<b>edgels).</b> Our approach assumes that the epipolar geometry of an image pair already has been computed, either in an earlier feature-based matching step, or by a robustified differential tracker. We exploit key-points of a lower order, edgels, which cannot provide a unique 2 D matching, but can be employed if a constraint on the motion is already given. We present a method to extract <b>edgels,</b> which can be effectively tracked given a known camera motion scenario, and show how a constrained version of the Lucas-Kanade tracking procedure can efficiently exploit epipolar geometry to reduce the classical KLT optimization to a 1 D search problem. The potential of the proposed methods is shown by experiments performed on real driving sequences...|$|E
40|$|The {{well-known}} anisotropic diffusion (a. k. a. Perona-Malik equation, nonlinear diffusion, or diffusion partial {{differential equation}} – PDE) {{is widely used}} in image segmentation, filtering and edge detection. The behavior of the diffusion depends highly on the appropriate choice of the gradient thresholding scale parameter K. However, it seems that no clear relationship between the parameter K and the output image has ever been established, and hence the choice of K is a guesswork. This paper proposes the Histogram Gradient-Based Anisotropic Diffusion (GHAD). In GHAD, the user specifies the desired number ν of edgeelements (<b>edgels)</b> of the filtered image. Let us define that the frontier between two neighboring pixels (p, q) is an edgel if |I(p) − I(q) |> τ, where I(p) is the image intensity at p and τ is a constant. From the specified ν, an appropriate parameter K is automatically computed in every diffusion iteration, so that the final filtered image has almost exactly ν <b>edgels.</b> Using this approach, the diffusion converges to a nontrivial piecewise constant image, whenever a feasible parameter ν is specified. 1...|$|E
40|$|We {{describe}} {{a method to}} select <b>edgels</b> and to calculate gradient orientation-based template descriptors for edgel features. An edgel is selected within a grid block based on gradient magnitude; its position and orientation are used to determine a canonical frame where the descriptor is computed based on quantized orientation. The resulting descriptor is efficiently matched using logical operations. We demonstrate {{the use of the}} resulting edgel detection and description method for planar object detection and pose estimation...|$|E
40|$|In this paper, {{we first}} {{introduce}} a recursive procedure for efficiently computing cubic facet parameters for edge detection. The procedure allows to compute facet parameters in a fixed {{number of operations}} independent of kernel size. We then introduce an image independent quantitative criterion for analytically evaluating different edge detectors (both gradient and zero-crossing based methods) without the need of ground-truth information. Our criterion is based on our observation that all edge detectors make a decision of whether a pixel is an <b>edgel</b> or not based on the result of convolution of the image with a kernel. The variance of the convolution output therefore directly affects the performance of an edge detector. We propose to analytically compute the variance of the convolution output {{and use it as}} a measure to characterize the performance of four well-known edge detectors...|$|R
5000|$|Buckner & Garcia, was an American musical duo {{consisting}} of Jerry Buckner and Gary Garcia from Akron, Ohio. Their first recording {{was made in}} 1972, when they performed a novelty song called [...] "Gotta Hear the Beat", which they recorded as Animal Jack. [...] Later, in 1980, they wrote a novelty Christmas song titled [...] "Merry Christmas in the NFL", imagining sports journalist Howard Cosell as Santa Claus. Performed under the pseudonym Willis the Guard & Vigorish, the song reached No. 82 on the Billboard charts despite limited airplay after Cosell found the song offensive. In 1981, the duo wrote a sentimental country theme to back the poem [...] "Footprints in the Sand", performed by <b>Edgel</b> Groves. The duo also wrote the lyrics for extra verses of an extended version of the WKRP in Cincinnati theme song in 1982.|$|R
40|$|Recently, vision {{research}} has centred {{on both the}} extraction and organization of geometric features, and on geometric relations. It is largely assumed that topological structure, that is linked <b>edgel</b> chains and junctions, cannot be extracted reliably from image intensity data. In this paper we demonstrate that this view is overly pessimistic and that visual tasks, such as perceptual grouping, {{can be carried out}} much more efficiently and reliably if well-formed topological structures are available. The widespread assumption that edge detectors produce incomplete and erroneous topological relations, such as the image projection of polyhedral face-edge-vertex structures, is shown to be false by analyzing the causes for failure in traditional edge detectors. These deficiencies can largely be overcome, and we show that a good compromise between topological completeness and geometric accuracy can be achieved. Furthermore, edge detection should not be carried out in isolation. The resulti [...] ...|$|R
40|$|We {{present an}} {{algorithm}} that extracts the largest shape {{within a specific}} class, starting from a set of image <b>edgels.</b> The algorithm inherits the Best-First Segmentation approach [1]. However, instead of being applicable only to shapes defined within a given class of curves, we have extended our approach to tackle more general - and complex - shapes. For example, we can now process shapes obtained from sets defined over different kinds of curves and related to one another by estimated parameters. Therefore, we go from a segmentation problem to a recognition problem. In {{order to reduce the}} complexity of the searching algorithm, we work with a linearly parameterized class of shapes. This allows us, first, to use a recursive Least-Squares fitting, second, to cast the problem as the search of a largest edgel subset in a directed acyclic graph, and, third, to easily introduce a priori information on the location of the <b>edgels</b> of the searched subset. This leads us to propose a unified approach where recognition and tracking are combined. Experiments on recognizing and tracking both left and right road boundaries demonstrate that real-time processing is achievable...|$|E
40|$|In {{this paper}} {{we present a}} simple and robust method for {{self-correction}} of camera distortion using single images of scenes which contain straight lines. Since the most common distortion can be modelled as radial distortion, we illustrate the method using the Harris radial distortion model, but the method is applicable to any distortion model. The method is based on transforming the <b>edgels</b> of the distorted image to a 1 -D angular Hough space, and optimizing the distortion correction parameters which minimize the entropy of the corresponding normalized histogram. Properly corrected imagery will have fewer curved lines, and therefore less spread in Hough space. Since the method does not rely on any image structure beyond the existence of <b>edgels</b> sharing some common orientations and does not use edge fitting, it is applicable {{to a wide variety}} of image types. For instance, it can be applied equally well to images of texture with weak but dominant orientations, or images with strong vanishing points. Finally, the method is performed on both synthetic and real data revealing that it is particularly robust to noise. Comment: 9 pages, 5 figures Corrected errors in equation 1...|$|E
40|$|Abstract. Existing {{methods for}} {{segmentation}} by edgel linking {{are based on}} heuristics and give no guarantee for a topologically correct result. In this paper, we propose an edgel linking algorithm based on a new sampling theorem for shape digitization, which guarantees a topologically correct reconstruction of regions and boundaries if the <b>edgels</b> approximate true object edges with a known maximal error. Experiments on real and generated images demonstrate the good performance of the new method and confirm the predictions of our theory. ...|$|E
40|$|Abstract. Planar maps {{have been}} {{proposed}} as a powerful and easy-touse representation for various kinds of image analysis results, but so far they are restricted to pixel accuracy. This leads to limitations in the representation of complex structures (such as junctions, triangulations, and skeletons) and discards the sub-pixel information available in grayvalue and color images. We extend the planar map formalism to sub-pixel accuracy and introduce various algorithms to create such a map, thereby demonstrating significant gains over the existing approaches. 1 Introduction When information is extracted from an image's raw pixel data, the results must be stored in a well-defined way. Still, many image analysis approaches use their own representations (labeled images, region adjacency graphs, regular, or irregular pyramids, <b>edgel</b> chains, polygons, etc.). This is not only highly confusing, but also prevents algorithms that perfectly complement each other from actually being used together- their representation are simply incompatible. During the last decade, several researchers have worked on powerful unified representations...|$|R
40|$|This paper {{presents}} an automatic registration system for aligning combined rangeintensity scan pairs. The overall approach {{is designed to}} handle several challenges including extensive structural changes, large viewpoint differences, repetitive structure, illumination differences, and flat regions. The technique is split into three stages: initialization, refinement, and verification. During initialization, intensity keypoints are backprojected into the scans and matched to form candidate transformations, each based on a single match. We explore methods of improving this image-based matching using the range data. For refinement, we extend the Dual-Bootstrap ICP algorithm for alignment of range data and introduce novel geometric constraints formed by backprojected image-based <b>edgel</b> features. The verification stage determines if a refined transformation is correct. We treat verification as a classification problem based on accuracy, stability, and a novel boundary alignment measure. Experiments with 14 scan pairs show both the overall effectiveness of the algorithm {{and the importance of}} its component techniques. Key words: range registration, iterative closest point, keypoint, decision criteria, physical change...|$|R
40|$|We {{describe}} a multiscale pyramid of line segments and develop algorithms which exploit that pyramid to recover image features – lines, curves, and blobs – from very noisy data. The beamlet dictionary is a dyadically organized collection of line segments, occupying {{a range of}} dyadic locations and scales, and spanning {{a full range of}} orientations. It is an efficient substitute for the full dictionary of “beams ” (the collection of all possible line segments connecting pairs of pixels in an image). The beamlets dictionary has low cardinality (there are O(n 2 log(n)) beamlets as compared to O(n 4) beams). Despite the reduced cardinality, it takes at most 8 log 2 (n) beamlets to approximate any <b>edgel</b> to within distance 2 /n. A wide range of polygonal curves can be built from chains of relatively few beamlets. The beamlet transform of a function f(x 1, x 2) is the collection of integrals of f over each segment in the beamlets dictionary. The resulting information is stored in an beamlet pyramid. One can use this to rapidly calculate integrals of f for any {{of a wide range of}} polygonal curves, simply by summing together a few selected coefficients from the pyramid. In analyzing faint signals embedded in very noisy data, integration over substantial numbers of pixel...|$|R
