14|261|Public
50|$|Careto {{normally}} installs {{a second}} and more complex backdoor program called SGH. SGH is easily modifiable and also has a wider arsenal including the ability to intercept system <b>events,</b> <b>file</b> operations, and performing {{a wider range of}} surveillance features. The information gathered by SGH and Careto can include encryption keys, virtual private network configurations, and SSH keys and other communication channels.|$|E
50|$|Some tools have {{a hybrid}} mode, which use JavaScript based UI design SDK, such as Dojo Toolkit, YUI Library, jQuery Mobile, Sencha Touch. And a new JavaScript based Device {{featured}} APIs encapsulation (GeoLoc, Connective, AccMeter, Camera, G sensor, <b>Events,</b> <b>File</b> system, etc.) is plugged into IDE as well, such as PhoneGap (Apache Cordova), Appcelerator. That means a custom APP can use most of mobile device features {{without any of}} 4GL coding or native coding, and make it once developed, deployed anywhere.|$|E
40|$|This study {{investigated}} the relationship between social support and stress in 572 families of disabled children {{in various parts of}} the United States. To utilize multidimensional models such as Dunst 2 ̆ 7 s ecological model and the Double ABCX model of stress, additional variables were investigated; these included family characteristics and recent life <b>events</b> (<b>FILE).</b> A regression design was used, with family characteristics, recent life <b>events</b> (<b>FILE),</b> perceived helpfulness of social support (FSS), and perceived adequacy of resources (FRS) as independent variables, and parental stress (PSI) as the dependent variable (PSI). Helpfulness of social support, recent life events, and family characteristics all predicted parental stress, though only to a small extent. Perceived adequacy of resources was by far the most significant predictor, accounting for 21...|$|E
30|$|Using the real-time-based {{rainfall}} transmission module, as the rainfall accumulation {{records are}} transmitted from the AWS in real time, the rainfall event protocol is retained {{to consider the}} event trigger time and transferred <b>event</b> <b>file.</b> In Fig.  7, {{as soon as a}} rainfall event is detected, the rainfall event time is transmitted from the AWS to the developed system. Then, the generated <b>event</b> <b>file</b> (stored in a oracle server) is transferred in company with the completion message, which is the computational message to inform the transmission or reception of <b>event</b> <b>file</b> and enable the DB, to operate the safety management system. At the developed system server, an <b>event</b> <b>file</b> is calibrated based on the rainfall criteria, according to the file transfer protocol. The rainfall accumulation datasets, including completion message of <b>event</b> time and <b>file,</b> are stored in system database. The alerted rainfall datasets are compared with rainfall criteria for hazard class (Table  3), and the severity level can be determined in 30  s after the occurrence of the rainfall event.|$|R
5000|$|Maintaining an <b>event</b> <b>file</b> on each {{scheduled}} State Partnership Program event {{including the}} event checklist.|$|R
40|$|Experiencing {{a single}} {{stimulus}} response cooccurrence {{leads to the}} creation of a binding between the codes of stimulus and response features: an <b>event</b> <b>file.</b> Here we investigate whether <b>event</b> <b>files</b> survive a switch to and from another task (ABB A) or whether task switching involves a suppression of stimulus-response bindings. Participants switched between responding to the colour or the identity of coloured letters, and the mapping of stimuli to response keys varied from trial to trial. Results show that responses were faster if the stimulus in trial matched the stimulus in trial n 3, but only if the stimulus response mapping was repeated. This suggests that stimulus codes were still bound to the codes of the response they accompanied 3 trials earlier and 2 task switches ago. Thus, an <b>event</b> <b>file</b> can survive one or more task switches and, thus, may represent a first step towards a more enduring memory trace...|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. This edition of the World Handbook is composed of four files: aggregate data, daily political events, annual political events, and a new, quarterly political <b>events</b> <b>file.</b> The political events files have been updated to include the period 1978 - 1982, while the aggregate file is based upon data from the base years of 1950, 1955, 1960, 1965, 1970, and 1975. This study is not available from the UK Data Archive, but {{can be obtained from}} ICPSR. See the ICPSR catalogue page [URL] for details. <br...|$|E
40|$|Researchers in {{psychological}} androgyny have presented {{the theory that}} androgynous individuals are the new model for mental health and that traditional sex-typed standards are inadequate. This study sought to broaden androgyny research beyond individual variables of self-esteem and adaptability and analyze the androgynous parent 2 ̆ 7 s social functioning in the family, life satisfaction, and stress related variables. ^ Middle years parents were classified using the revised Bem Sex Role Inventory and analyzed for descriptive characteristics, life satisfaction (Campbell, Converse 2 ̆ 6 Rodgers), family cohesion and adaptability (FACES II), psychosomatic symptoms (Norem and Brown), and family life <b>events</b> (<b>FILE).</b> Subjects, 254 matched spouses in a midwestern state, varied widely in descriptive characteristics except race (99...|$|E
40|$|New {{approaches}} to modelling spacecraft system are being tested at the Jet Propulsion Laboratory {{in order to}} better understand and quantify the interactions among spacecraft subsystems and mission variables and thereby understand and manage mission risk. Ultimately the goal is to enable the design of lower cost missions and to reduce the time between system concept and launch. This paper will present the New Millennium Deep Space 2 system model developed using NuThena 2 ̆ 7 s Foresight program. The system tool models the physical and media access control layers of the telecom relay link, the capacity and temperature of the batteries, and the data generated by the science payload. The event driven simulation is activated by a sequence of <b>events</b> <b>file,</b> an instrument requirements file and a set of environmental inputs. The model described has been used to select subsystem components, perform system trades, and visualize data return scenarios...|$|E
40|$|Humans {{integrate}} {{the features of}} perceived events and of action plans into episodic <b>event</b> <b>files.</b> Here we investigated whether children (9 – 10 [*]years), younger adults (20 – 31 [*]years), and older adults (64 – 76 [*]years) differ in the flexibility of managing (updating) <b>event</b> <b>files.</b> Relative to young adults, performance in children and older adults was more hampered by partial mismatches between present and previous stimulus–response relations, suggesting less efficient updating of episodic stimulus–response representations in childhood and old age. Results are discussed in relation to changes in cortical neurochemistry during maturation and senescence...|$|R
5000|$|Ensuring {{follow-up}} {{actions are}} completed upon {{the conclusion of}} each event: After Action Reports, update the Historical File, close out of the active <b>event</b> <b>file</b> and develop follow-on events.|$|R
40|$|We {{present an}} {{event-file}} format for {{the dissemination of}} next-to-leading-order (NLO) predictions for QCD processes at hadron colliders. The files contain all information required to compute generic jet-based infrared-safe observables at fixed order (without showering or hadronization), and to recompute observables with different factorization and renormalization scales. The files also {{make it possible to}} evaluate cross sections and distributions with different parton distribution functions. This in turn makes it possible to estimate uncertainties in NLO predictions {{of a wide variety of}} observables without recomputing the short-distance matrix elements. The <b>event</b> <b>files</b> allow a user to choose among a wide range of commonly-used jet algorithms and jet-size parameters. We provide <b>event</b> <b>files</b> for a $W$ or $Z$ boson accompanied by up to four jets, and for pure-jet events with up to four jets. The files are for the Large Hadron Collider with a center of mass energy of 7 or 8 TeV. A C++ library along with a Python interface for handling these files are also provided and described in this article. The library allows a user to read the <b>event</b> <b>files</b> and recompute observables transparently for different pdf sets and factorization and renormalization scales. Comment: 43 pages, 6 table...|$|R
40|$|Due to {{the nature}} of the GRAIL mission, the GRAIL Mission Planning and Sequence Team (MPST) is {{required}} to generate ground and uplink products faster than ever done before. The existing correct_transmitter_min_dur tool that provides a similar function to grl_suppdoc lacks the ability to operate accurately or quickly enough to support the rapid turnaround required of the GRAIL MPST. The GRAIL MPST was required to build this new tool to facilitate the ground and uplink generation processes to meet a tight sequence development timeline. The grl_suppdoc tool enables the GRAIL MPST to generate automatically Deep Space Network (DSN) transmitter suppressions based on short uplinks that are found in the ground/modeled Predicted <b>Events</b> <b>File</b> (PEF). The grl_suppdoc script automatically generates applicable DSN uplink suppressions in the form of a Spacecraft Activity Sequence File (SASF) to protect the GRAIL project from short DSN uplink windows, which can be cause for operator error at the DSN antennas. Currently, no software exists that provides this functionality at the efficiency required for GRAIL sequence team operations. Compared to a manual process, this script reduces human error and saves considerable man-hours by automating and streamlining the mission planning and sequencing task for the GRAIL mission...|$|E
40|$|Background and Aim: Stress is {{associated}} with life satisfaction and also development of some physical diseases. Birth of a disabled child with mental or physical disability (especially deaf or blind children), impose an enormous load of stress on their parents especially the mothers. This study compared stress levels of mothers with hearing impaired children and mothers of normal children or with other disabilities. Methods: In this study, cluster random sampling was performed in Karaj city. 120 mothers in four groups of having a child with mental retardation, low vision, hearing impairment and with normal children were included. Family inventory of life <b>events</b> (<b>FILE)</b> of Mc Cubbin et al. {{was used to determine}} stress level in four groups of mothers. Results: The results of this research indicated a significant difference (p< 0. 05) between stress levels of mothers with hearing impaired children and mothers of other disabled and normal children in subscales of intra-family stress, finance and business strains, stress of job transitions, stress of illness and family care and family members "in and out''. There was no difference between compared groups in other subscales. Conclusion: Since deafness is a hidden inability, the child with hearing impairment has a set of social and educational problems causing great stress for parents, especially to mother. In order to decrease mother’s stress, it is suggested to provide more family consultation, adequate social support and to run educational classes for parents to practice stress coping strategies...|$|E
40|$|This {{software}} graphically displays all {{pertinent information}} from a Predicted <b>Events</b> <b>File</b> (PEF) using the Java Swing framework, which allows for multi-platform support. The PEF is hard to weed through when looking for specific information {{and it is a}} desire for the MRO (Mars Reconn aissance Orbiter) Mission Planning & Sequencing Team (MPST) to have a different way to visualize the data. This tool will provide the team with a visual way of reviewing and error-checking the sequence product. The front end of the tool contains much of the aesthetically appealing material for viewing. The time stamp is displayed in the top left corner, and highlighted details are displayed in the bottom left corner. The time bar stretches {{along the top of the}} window, and the rest of the space is allotted for blocks and step functions. A preferences window is used to control the layout of the sections along with the ability to choose color and size of the blocks. Double-clicking on a block will show information contained within the block. Zooming into a certain level will graphically display that information as an overlay on the block itself. Other functions include using hotkeys to navigate, an option to jump to a specific time, enabling a vertical line, and double-clicking to zoom in/out. The back end involves a configuration file that allows a more experienced user to pre-define the structure of a block, a single event, or a step function. The individual will have to determine what information is important within each block and what actually defines the beginning and end of a block. This gives the user much more flexibility in terms of what the tool is searching for. In addition to the configurability, all the settings in the preferences window are saved in the configuration file as wel...|$|E
40|$|Aims. We {{describe}} {{in detail the}} nature of XMM-Newton EPIC background and its various complex components, summarising the new findings of the XMM-Newton EPIC background working group, and provide XMM-Newton background blank sky <b>event</b> <b>files</b> {{for use in the}} data analysis of diffuse and extended sources. Methods. Blank sky <b>event</b> <b>file</b> data sets are produced from the stacking of data, taken from 189 observations resulting from the Second XMM-Newton Serendipitous Source Catalogue (2 XMMp) reprocessing. The data underwent several filtering steps, using a revised and improved method over previous work, which we {{describe in}} detail. Results. We investigate several properties of the final blank sky data sets. The user is directed to the location of the final data sets. There is a final data set for each EPIC instrument-filter-mode combination. Key words. Surveys- X-rays: diffuse background- X-rays: general 1...|$|R
30|$|Before touch {{events are}} {{recorded}} at the Device Input <b>Event</b> <b>files,</b> human-generated touch <b>events</b> are processed through Input Driver while program-generated touch events are handled with the evdev_write {{function in the}} evdev.c file. We can develop a filter for program-generated events with this difference—the evdev_write function should be modified. We can accept or reject program-generated touch events before writing them on Device Input <b>Event</b> <b>files</b> depending on the target program’s security policy defined in its Security Policy files. For example, program-generated touch events on a specific rectangle region (for displaying advertisements) can be ignored during the target program is actively running. We suggest that those files can {{be included in the}} ad network’s SDK library and installed with the program itself together. We expect that the existing architecture can be slightly modified to filter out automatically generated touch events by programs without significant loss in efficiency.|$|R
40|$|We {{describe}} {{in detail the}} nature of XMM-Newton EPIC background and its various complex components, summarising the new findings of the XMM-Newton EPIC background working group, and provide XMM-Newton background blank sky <b>event</b> <b>files</b> {{for use in the}} data analysis of diffuse and extended sources. Blank sky <b>event</b> <b>file</b> data sets are produced from the stacking of data, taken from 189 observations resulting from the Second XMM-Newton Serendipitous Source Catalogue (2 XMMp) reprocessing. The data underwent several filtering steps, using a revised and improved method over previous work, which we {{describe in}} detail. We investigate several properties of the final blank sky data sets. The user is directed to the location of the final data sets. There is a final data set for each EPIC instrument-filter-mode combination. Comment: Paper accepted by A&A 22 December 2006. 14 pages, 8 figures. Paper can also be found at [URL]...|$|R
40|$|This {{innovation}} {{is a tool}} used to verify and validate spacecraft sequences at the predicted <b>events</b> <b>file</b> (PEF) level for the GRAIL (Gravity Recovery and Interior Laboratory, see [URL] gov/mission_pages/grail/main/index. html) mission {{as part of the}} Multi-Mission Planning and Sequencing Team (MPST) operations process to reduce the possibility for errors. This tool is used to catch any sequence related errors or issues immediately after the seqgen modeling to streamline downstream processes. This script verifies and validates the seqgen modeling for the GRAIL MPST process. A PEF is provided as input, and dozens of checks are performed on it to verify and validate the command products including command content, command ordering, flight-rule violations, modeling boundary consistency, resource limits, and ground commanding consistency. By performing as many checks as early in the process as possible, grl_pef_check streamlines the MPST task of generating GRAIL command and modeled products on an aggressive schedule. By enumerating each check being performed, and clearly stating the criteria and assumptions made at each step, grl_pef_check {{can be used as a}} manual checklist as well as an automated tool. This helper script was written with a focus on enabling the user with the information they need in order to evaluate a sequence quickly and efficiently, while still keeping them informed and active in the overall sequencing process. grl_pef_check verifies and validates the modeling and sequence content prior to investing any more effort into the build. There are dozens of various items in the modeling run that need to be checked, which is a time-consuming and errorprone task. Currently, no software exists that provides this functionality. Compared to a manual process, this script reduces human error and saves considerable man-hours by automating and streamlining the mission planning and sequencing task for the GRAIL mission...|$|E
40|$|At the Cork rn meeting {{participants}} {{were presented with}} an agenda obviously typeset with W. Calendars and agendas show a lot of computed information. like the number of day in the month. the name of day in the week. TEX is well-suited {{to deal with these}} computations. Agendas also give relevant events for the user: 'Fixed ' events occurring every year at fixed dates but also 'movable ' events which depend on astronomical phenomena related to the sun and moon, such as religious feast days or moon phases. It {{would be nice to have}} a package to typeset agendas including fixed events, even better, to have compute the date of movable events and include them in the agenda. * We describe here a macro package to make calendars and agendas for a given year, supporting 0 Easy inclusion of custom events; 0 Automatic inclusion of computed events. This package is divided into several files devoted to computation of different type of <b>events.</b> <b>File</b> calend 0. tex is the basic macro file; it provides tools to convert between and compute date formats. File calendl. tex gives macros to support computation of astronomical events which are calculated in sun. tex and moon. tex and file feasts. tex allows computation of movable feasts based on the date of Easter Sunday. We provide several example style files containing formatting instructions to typeset agendas and calendars. Acknowledgments. The author is grateful to B. Beeton and the reviewer for suggesting several style formats. Basic Mechanism Events. To obtain an agenda for year (my year) you have to create a source file including custom events (day) /(month) [(text) ] in order the agenda will show (text) in the cell corresponding to (month), (day). The prototypical source file looks like this: * Like the example of [Kl], p. 218, not only writes, but provides the information too. alendO (my year)) (my tzmezone)) (my definitions) oon un east...|$|E
40|$|Extended X-ray {{emission}} can {{be studied}} either spatially (through its surface brightness profile) or spectrally (by analyzing the spectrum at various locations in the field). Both techniques have advantages and disadvantages, and when the emission becomes particularly faint and/or extended, the two methods can disagree. We argue that an ideal approach would be to model the <b>events</b> <b>file</b> directly, and therefore to use both the spectral and spatial information which are simultaneously available for each event. In this work we propose {{a first step in}} this direction, introducing a method for spatial analysis which can be extended to leverage spectral information simultaneously. We construct a model for the entire X-ray image in a given energy band, and generate a likelihood function to compare the model to the data. A critical goal of this modeling is disentangling vignetted and unvignetted backgrounds through their different spatial distributions. Employing either maximum likelihood or Markov Chain Monte Carlo, we can derive probability distribution functions for the source and background parameters together, or we can fit and subtract the background, leaving the description of the source non-parametric. We calibrate and demonstrate this method against a variety of simulated images, and then apply it to Chandra observations of the hot gaseous halo around the elliptical galaxy NGC 720. We are able to follow the X-ray emission below a tenth of the background, and to infer a hot gas mass within 35 kpc of 4 − 5 × 109 M, with some indication that the profile continues to at least 50 kpc and that it steepens as the radius increases. We derive much stronger constraints on the surface brightness profile than previous studies, which employed the spectral method, and we show that the density profiles inferred from these studies are in conflict with the observed surface brightness profile. We therefore conclude that, contrary to a previous claim, the X-ray halo of NGC 720 does not seem to contain the full complement of missing baryons from this galaxy...|$|E
5000|$|Up to 100 voice samples, 50 {{instrument}} units, and 500,000 <b>events</b> per <b>file</b> ...|$|R
40|$|We {{present a}} new version of the CompHEP program package, version 4. 5. We {{describe}} new options and techniques implemented in the version: interfaces to ROOT and HERWIG, parallel calculations, generation of the XML-based header in <b>event</b> <b>files</b> (HepML), full implementation of the Les Houches agreements (LHA I, SUSY LHA, LHA PDF, Les Houches Event format), cascade matching for intermediate scalar resonances, etc...|$|R
40|$|We {{propose to}} combine and {{slightly}} extend two existing "Les Houches Accords" {{to provide a}} simple generic interface between beyond-the-standard-model parton-level and event-level generators. All relevant information - particle content, quantum numbers of new states, masses, cross sections, parton-level events, etc - is collected in one single file, which adheres to the Les Houches <b>Event</b> <b>File</b> (LHEF) standard. Comment: 4 page...|$|R
40|$|Posttraumatic Stress Disorder (PTSD), the {{signature}} wound of the Iraq and Afghanistan wars, has caused veterans to face numerous and complex challenges within their intimate relationships post-deployment. Although {{other studies have}} explored the intimate relationships of veterans, {{the same level of}} research has not focused on OEF/OIF veterans from the standpoint of dyadic adaptation using the Dyadic Adaptation Scale (DAS). The {{purpose of this study was}} to explore the level of dyadic adaptation in intimate relationships of OEF/OIF veterans who self-reported PTSD and those who did not. More specifically, this study identified the factors that were related to the level of dyadic adaptation for this population. Participants were 126 OEF/OIF veterans who were enrolled in colleges and universities throughout the state of Iowa; provided basic background information in response to a demographics questionnaire; and completed the DAS to yield scores of the participants 2 ̆ 7 dyadic adaptation within their intimate relationships, the Family Crisis Oriented Personal Evaluation Scales (F-COPES) that highlighted their levels of coping, and the Family Inventory of Life <b>Events</b> (<b>FILE)</b> that measured their life stressors within the last 12 months. The results of the correlation, MANOVA, ANOVA, and hierarchical regression analyses provided four major findings and implications. First, among participants with PTSD, DAS was correlated with tours of duty, FILE, F-COPES, and pharmacologic intervention, and among participants without PTSD, DAS was correlated with FILE. Second, the total dyadic adaptation scores for participating OEF/OIF veterans suggested an overall slight level of relationship dissatisfaction. Third, participants who self-reported PTSD had lower DAS total scores than participants who did not self-report PTSD. In addition, there was a significant difference on all four subscales (cohesion, satisfaction, consensus, and affectional expression) of the DAS between the two groups of participants. Fourth, in terms of participants who self-reported PTSD, tours of duty, types of relationships, and life stressors were the only variables that positively affected dyadic adaptation. In contrast, for participants who did not self-report PTSD, FILE was the only variable that affected the dyadic adaptation. These findings have important implications that highlight areas in which clinicians, educators, and individuals within the helping professions can join the Department of Veterans Affairs 2 ̆ 7 initiatives to improve the reintegration of OEF/OIF veterans into their familiar roles post-deployment. Future research should explore the relationship norms pre-deployment and across relationship statutes, the identity of military intimate partners within treatment facilities, and the perceptions of treatment and dyadic adaptation after OEF/OIF veterans receive treatment in the community by civilian providers as compared to treatment in VA facilities...|$|E
40|$|International audienceIn {{front of}} us stands a malware's {{protection}} layer with millions of assembly instructions and {{our goal is to}} understand what's going on. Developing a comprehensive understanding will allow us to unpack the original code, to build detection mechanisms for the malware family or to find interesting pieces of code. The purpose of this talk is to present a solution when one wants to understand a heavily obfuscated code containing a big amount of information. We first need to note that, when we speak of packer understanding, looking at system events like API calls is not enough: in most malwares' protection layers they represent {{only a small part of}} the code (and they are sometimes useless). One needs to play at assembly level, which is a hard and time-consuming task. We have thus built some tools to help. These tools come in two parts: 1. A program execution monitor using dynamic binary instrumentation associated with static information that provides two outputs: - an improved trace, containing a very detailed view of the program execution (memory access, time [...] .), using a format which is easy to parse. - an <b>events</b> <b>file,</b> showing a high level view of the execution by displaying only some specific events, e. g. the loops, the API calls, the exceptions or the dynamic layers of code. Moreover, as we are dynamically monitoring the program execution, we can do better than just detect these events and we actually collect information about them, e. g. the arguments to API calls, the memory access made inside a loop, the exception error code, etc. 2. Some tools to rapidly exploit the previously collected information: - Two visualization tools: -> a [...] timeline [...] based on the events previously detected: the user can navigate through the execution and see what kind of events happens. We also allow the user to define its own events on the execution trace and we display them on the timeline. Moreover the user can choose the abstraction level he wants to represent the execution. -> a [...] memory profile [...] , that is a memory view totally independent from the code itself, we only see its [...] effects [...] . It helps to diagnosis the code behaviour more easily than by reading millions of assembly instructions. - An inferring engine that uses rules defined either on the execution trace or on the memory profile (thus independent from the code in this case). It should be understood that we are not claiming to have built a [...] silver bullet [...] for malware analysis, our tool is not the replacement of IDA Pro or OllyDbg. Its goal is to provide something that helps the standard RE work by providing ways to divide it in easier sub-parts, to bring some points where begin the investigation of new binaries or to rapidly recognize already seen behaviours. During the presentation we are going to apply our framework on some recent malware families and show its usefulness. We will also release the source code and we plan to set up a kind of sandbox analysis...|$|E
40|$|We {{present the}} manual {{for the program}} BRIDGE: Branching Ratio Inquiry/Decay Generated Events. The program is {{designed}} to operate with arbitrary models defined within matrix element generators, so that one can simulate events with small finalstate multiplicities, decay them with BRIDGE, and then pass them to showering and hadronization programs. BRI can automatically calculate widths of two and three body decays. DGE can decay unstable particles in any Les Houches formatted <b>event</b> <b>file.</b> DGE is useful for the generation of <b>event</b> <b>files</b> with long decay chains, replacing large matrix elements by small matrix elements followed by sequences of decays. BRIDGE is currently designed to work with the MadGraph/MadEvent programs for implementing and simulating new physics models. In particular, it can operate with the MadGraph implementation of the MSSM. In this manual we describe how to use BRIDGE, and In recent years the workhorses of Monte Carlo event generation, parton showering, and hadronization, Pythia [1] and Herwig [2], have begun to offload some of their duties t...|$|R
40|$|The Clinical Narrative Temporal Relation Ontology (CNTRO) 1 project {{offers a}} semantic-web based {{reasoning}} framework, which represents temporal events and relationships within clinical narrative texts, and infer new knowledge over them. In this paper, the CNTRO reasoning framework {{is applied to}} temporal analysis of medical device adverse <b>event</b> <b>files.</b> One specific adverse event {{was used as a}} test case: late stent thrombosis. Adverse event narratives were obtained from the Food and Drug Administration’s (FDA) Manufacturing and User Facility Device Experience (MAUDE) database 2. 15 adverse <b>event</b> <b>files</b> in which late stent thrombosis was confirmed were randomly selected across multiple drug eluting stent devices. From these <b>files,</b> 81 <b>events</b> and 72 temporal relations were annotated. 73 temporal questions were generated, of which 65 were correctly answered by the CNTRO system. This results in an overall accuracy of 89 %. This system should be pursued further to continue assessing its potential benefits in temporal analysis of medical device adverse events...|$|R
5000|$|The eight event-level files {{generally}} contain one record per event, {{and contain}} various information {{pertaining to the}} specific type of event. Each record contains one or more ICD-9 codes to describe and categorize the type of medical encounter experienced by the surveyed individual. The event-level files also contain the breakdown of spending by payor associated with the event and a date (or start and end dates) that the event took place. Each of these <b>event</b> <b>files</b> can be joined with the person-level files in a many-to-one match (on DUPERSID), where an individual with zero medical events during the calendar year would generate zero matches, but an individual with two doctor visits and a dental visit would generate three matches across all of the <b>event</b> <b>files.</b> The event-level files can also be joined with the condition files (on CONDIDX) to determine what medical expenditure {{can be associated with}} particular conditions. The eight files include: ...|$|R
50|$|Windows 8 {{introduces}} {{new forms}} of notifications for Metro-style apps and for certain <b>events</b> in <b>File</b> Explorer.|$|R
5000|$|Oh Yeon-ho (born 1964) is {{the founder}} of [...] "citizen journalism" [...] in South Korea, and CEO of OhmyNews a new {{approach}} to cyber-journalism in which ordinary citizens can contribute to a major news organization through being at news <b>events,</b> <b>filing</b> reports, and having their work verified and edited by a trained news staff. He is seen as one of the pivotal figures in the contemporary culture of South Korea.|$|R
40|$|People {{integrate}} {{the features of}} perceived events and of action plans, {{as well as of}} episodic stimulus– response relations, into <b>event</b> <b>files.</b> We investigated whether the management of <b>event</b> <b>files,</b> and particularly the speed of updating the binding between the task-relevant stimulus feature and the response, correlates with fluid intelligence. Indeed, the performance of participants scoring high on Raven’s Standard Progressive Matrices test was less impaired by a mismatch between the stimulus–response relation in the current and the previous trial. This result suggests that high intelligence is accompanied by a higher degree of flexibility in handling event files—that is, by higher efficiency in updating episodic representations. One of the most discussed issues in cognitive psychology is what has become known as the binding problem (Treisman, 1996) —that is, the question of how the human (or primate) brain is able to properly integrate all information about a particular event. In perception, there must be a mechanism that functionally links the features of an object to what Kahneman, Treisman, and Gibbs (1992) hav...|$|R
40|$|One of {{the goals}} in studies of double {{ionization}} (DI) of simple atoms by electron or ion impact is to elucidate and assess the different mechanisms that lead to this atomic process. In this work we present an attempt to model the mechanisms beyond the first order in DI of helium by highly charged projectiles. To this end we employ the continuum distorted wave-eikonal initial state (CDW-EIS) formalism joint with a Monte Carlo event generator (MCEG). The MCEG allows us to generate theoretical <b>event</b> <b>files</b> that represent the counterpart of the data obtained from a kinematically complete experiment. Starting from these <b>event</b> <b>files,</b> a new data analysis tool used to contrast theory and experiment in DI, the four-body Dalitz plots, is easily produced. The higher order mechanisms are simulated by considering DI as a sequential process: a single ionization of a helium atom {{as a first step}} and another single ionization of a single-charged helium ion as a second step. Some of the features in the experimental data are very well reproduced by these simulations...|$|R
40|$|This {{document}} presents novel algorithms for {{detection of}} noisy and flickering pixels from Burst Alert telescope event data and subsequent {{elimination of data}} from such pixels to create a filtered <b>event</b> <b>file.</b> The approach adopted for this purpose {{is quite different from}} the current practises and focuses more on the temporal variation of data in the detector pixels over long intervals of time against the current algorithms which follow a pixel based approach. Comment: 5 Pages, 2 Figure...|$|R
40|$|XMM-Newton {{background}} {{maps for}} the 3 EPIC instruments in their different instrument/mode/filter combinations {{and in several}} energy bands have been constructed using a superposition of many pointed observations. Event datasets for the different instrument/mode/filter combinations have also been constructed, with longer exposure times than previously created files. This document describes {{the construction of the}} background maps and <b>event</b> <b>files,</b> and their usage. Further details on how to obtain these background products, together with related software and these explanatory notes can be found a...|$|R
50|$|Audio {{and video}} {{verification}} techniques use microphones and cameras to record audio frequencies, video signals, or image snapshots. The source {{audio and video}} streams are sent over a communication link, usually an Internet protocol (IP) network, to the central station where monitors retrieve the images through proprietary software. The information is then relayed to law enforcement and recorded to an <b>event</b> <b>file,</b> {{which can be used}} to plan a more strategical and tactical approach of a property, and later as prosecution evidence.|$|R
