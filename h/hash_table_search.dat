4|3341|Public
40|$|International audienceFlow metrics are {{critical}} for protocol research, anomaly detection, network operation and application deployment. There are great challenges to match every packet into millions of flows in high speed network link. A 3 -phase measurement method for IP traffic flow is proposed in this paper. The packets are captured and classified into the flows through <b>hash</b> <b>table,</b> <b>search</b> tree and linear table with this approach. The traffic measurement systems in general purpose CPU platform and network processor platform with this method have been implemented and deployed in backbone links. The experiment on OC- 48 backbone shows the average length search path for each packet is 7. 7. Average process times of mapping successfully and unsuccessfully are about 1. 3 μs and 1. 8 μs respectively at the length of search path 350. Traffic matrix, host behavior and other metrics can been easily deduced with our approach...|$|E
40|$|Hash {{tables are}} {{fundamental}} components of several network processing algorithms and applications, including route lookup, packet classification, per-flow state management and network monitoring. These applications, which typically {{occur in the}} data-path of high-speed routers, must process and forward packets {{with little or no}} buffer, making it important to maintain wire-speed throughout. A poorly designed hash table can critically affect the worst-case throughput of an application, since the number of memory accesses required for each lookup can vary. Hence, high throughput applications require hash tables with more predictable worst-case lookup performance. While published papers often assume that hash table lookups take constant time, there is significant variation in the number of items that must be accessed in a typical <b>hash</b> <b>table</b> <b>search,</b> leading to search times that vary by a factor of four or more. We present a novel hash table data structure and lookup algorith...|$|E
40|$|Flow metrics are {{critical}} for protocol research, anomaly detection, network operation and application deployment. There are great challenges to match every packet into millions of flows in high speed network link. A 3 -phase measurement method for IP traffic flow is proposed in this paper. The packets are captured and classified into the flows through <b>hash</b> <b>table,</b> <b>search</b> tree and linear table with this approach. The traffic measurement systems in general purpose CPU platform and network processor platform with this method have been implemented and deployed in backbone links. The experiment on OC- 48 backbone shows the average length search path for each packet is 7. 7. Average process times of mapping successfully and unsuccessfully are about 1. 3 µ s and 1. 8 µ s respectively at the length of search path 350. Traffic matrix, host behavior and other metrics can been easily deduced with our approach. 1...|$|E
3000|$|... [...]) are unsuccessfully searched. Please, {{note that}} false-positives will {{only lead to}} extra <b>hash</b> <b>table</b> <b>searches,</b> and the actual result of the {{algorithm}} will remain the same regardless of that ratio.|$|R
40|$|In {{this course}} {{you will learn}} several {{fundamental}} principles of algorithm design: divide-and-conquer methods, graph algorithms, practical data structures (heaps, <b>hash</b> <b>tables,</b> <b>search</b> trees), randomized algorithms, and more. Particular emphasis is given to algorithms for sorting, searching, and string processing. Fundamental algorithms {{in a number of}} other areas are covere...|$|R
40|$|Abstract [...] We {{present an}} {{efficient}} peer-to-peer system architecture for keyword-based free-text search in environments with heterogeneous document popularities and user lifetimes, such as file-sharing applications. We analyze {{the characteristics of}} this system theoretically, and also present an efficient simulator and simulation results. Index Terms [...] System design, analysis, simulation, peer-topeer, distributed <b>hash</b> <b>tables,</b> <b>search.</b> I...|$|R
40|$|To {{limit the}} crash against users demand for smooth video, clear audio, and {{performance}} levels specified and guaranteed by contract quality, an innovative approach to face these challenges in streaming media is considered. Here {{a new idea}} of boosting the capacity of seed servers to serve more receivers in peer to peer data streaming systems is focused. These servers complement the limited upload capacity offered by peers. The peer requests for a data segment is handled by the server or another peer with a seeding capacity of any finite number with a local cache attached in each peer, which enable the peer to temporarily store the data once requested, {{so it can be}} directly fetched by some other node near to the peer without accessing the server there by improving the performance of rendering data. The capacity of the cache in each peer can be designed based on popularity of the segment in cache. Once the peers are cached the peer, data segment request are handled by performing a distributed <b>hash</b> <b>table</b> <b>search</b> strategy, and seed servers boost the capacity of each peer based on utility to cost factor computed each time till it exceeds the seeding capacity. Apart from this selfish peers connected in system can be traced to check for unfaithful peers. This system efficiently allocates the peer resources there by considering the server bandwidth constraints...|$|E
50|$|The {{two major}} {{approaches}} to implementing dictionaries are a <b>hash</b> <b>table</b> or a <b>search</b> tree.|$|R
3000|$|... [...]. The match vector M is used {{to query}} the {{associated}} <b>hash</b> <b>tables.</b> The <b>search</b> begins by sequentially performing queries to the associated <b>hash</b> <b>tables</b> by traversing M backwards, i.e., starting in m 32. This is because {{we are interested in}} the LPM. If the algorithm finds the next hop (a true match) for a given DA in the pair (f [...]...|$|R
5000|$|One of {{the biggest}} {{characteristics}} of Perfect Dark is its powerful search capability. By using distributed <b>hash</b> <b>tables,</b> <b>search</b> performance is greatly improved compared to Winny or Share, making it unnecessary {{to rely on the}} construction of node clusters. This frees users from inputting or switching cluster keywords and also enables users to search for files of different genres at the same time. This is in contrast to Winny and Share, where cluster keywords, such as [...] "DVDISO" [...] or [...] "アニメ"/"anime" [...] are used to specify what types of files the user is searching for. These keywords segregate the network and introduce delays when the user changes to them.|$|R
40|$|Abstract. The recent {{file storage}} {{applications}} built {{on top of}} peer-to-peer dis-tributed <b>hash</b> <b>tables</b> lack <b>search</b> capabilities. We believe that search {{is an important part}} of any document publication system. To that end, we have designed and ana-lyzed a distributed search engine based on a distributed <b>hash</b> <b>table.</b> Our simulation results predict that our search engine can answer an average query in under onesecond, using under one kilobyte of bandwidth. Keywords: <b>search,</b> distributed <b>hash</b> <b>table,</b> peer-to-peer, Bloom filter, cachin...|$|R
40|$|The recent {{file storage}} {{applications}} built {{on top of}} peer-to-peer distributed <b>hash</b> <b>tables</b> lack <b>search</b> capabilities. We believe that search {{is an important part}} of any document publication system. To that end, we have designed and analyzed a distributed search engine based on a distributed <b>hash</b> <b>table.</b> Our simulation results predict that our search engine can answer an average query in under one second, using under one kilobyte of bandwidth...|$|R
5000|$|The naive {{solution}} to the problem is as follows: [...] Initialize a counter, , to zero, [...] Initialize an efficient dictionary data structure, , such as <b>hash</b> <b>table</b> or <b>search</b> tree in which insertion and membership can be performed quickly. [...] For each element , a membership query is issued. [...] If [...] is not a member of [...] (...) Add [...] to [...] Increase [...] by one, [...] Otherwise (...) do nothing. Output [...]|$|R
50|$|Hough Transform {{is used to}} cluster {{reliable}} model hypotheses {{to search}} for keys that agree upon a particular model pose. Hough transform identifies clusters of features with a consistent interpretation by using each feature to vote for all object poses {{that are consistent with}} the feature. When clusters of features are found to vote for the same pose of an object, the probability of the interpretation being correct is much higher than for any single feature. An entry in a <b>hash</b> <b>table</b> is created predicting the model location, orientation, and scale from the match hypothesis. The <b>hash</b> <b>table</b> is <b>searched</b> to identify all clusters of at least 3 entries in a bin, and the bins are sorted into decreasing order of size.|$|R
50|$|The {{dictionary}} {{problem is}} a classic computer science problem: the task of designing a data structure that maintains a set of data during 'search', 'delete', and 'insert' operations.The two major solutions to the dictionary problem are a <b>hash</b> <b>table</b> or a <b>search</b> tree.In some cases {{it is also possible}} to solve the problem using directly addressed arrays, binary search trees, or other more specialized structures.|$|R
50|$|Hash {{functions}} {{are also used}} to build caches for large data sets stored in slow media. A cache is generally simpler than a <b>hashed</b> <b>search</b> <b>table,</b> since any collision can be resolved by discarding or writing back the older of the two colliding items. This is also used in file comparison.|$|R
40|$|This paper {{deals with}} two {{different}} approaches in programming sphere, and joins {{them in one}} entity through its practical part. One approach is context searching that is applied on the second approach, functional graphic library wxHaskell that is designed and implemented within Eclipse development environment through the plug-in extension. As searching object lies at the wxHaskell documentation, it is lexically analyzed, what forms a <b>hashing</b> <b>table</b> determined to <b>searching</b> for records...|$|R
50|$|The term array {{is often}} used to mean array data type, a kind of data type {{provided}} by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by <b>hash</b> <b>tables,</b> linked lists, <b>search</b> trees, or other data structures.|$|R
50|$|For example, {{one could}} define an {{abstract}} data type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a <b>hash</b> <b>table,</b> a binary <b>search</b> tree, or even a simple linear list of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.|$|R
40|$|Abstract. Building a high {{performance}} IP lookup engine remains a challenge due to increasingly stringent throughput requirements {{and the growing}} size of IP tables. An emerging approach for IP lookup {{is the use of}} set associative memory architecture, which is basically a hardware implementation of an open addressing <b>hash</b> <b>table</b> with the property that each row of the <b>hash</b> <b>table</b> can be <b>searched</b> in one memory cycle. While open addressing <b>hash</b> <b>tables,</b> in general, provide good average-case search performance, their memory utilization and worst-case performance can degrade quickly due to bucket overflows. This paper presents a new simple hash probing scheme called CHAP (Content-based HAsh Probing) that tackles the hash overflow problem. In CHAP, the probing is based on the content of the <b>hash</b> <b>table,</b> thus avoiding the classical side effects of probing. We show through experimenting with real IP tables how CHAP can effectively deal with the overflow...|$|R
30|$|As {{presented}} in Fig.  3, the matching ratio has little {{impact in the}} overall performance of the application. The {{reason for that is}} that the case of an address matching some prefix in the database is not necessarily faster than the case where the address end up in the default route, and vice-versa. For example, consider an address that does not match any prefix in the forwarding table. If no false positives occur, i.e., the two Bloom filters correctly answer not to look in their associated <b>hash</b> <b>tables,</b> the <b>search</b> quickly finishes with one additional memory access to the DLA. However, if for another prefix a false positive occurs in the first, but not in the second Bloom filter, or if there are plenty of values stored in the <b>searched</b> <b>hash</b> <b>table</b> buckets, then this case of matching will likely be slower than the former. As such, the speed and good statistical distribution of the hash function to control the FPR and also minimize the number of collisions in the <b>hash</b> <b>tables</b> is an important aspect to limit variations in performance as a result of dataset characteristics.|$|R
40|$|Building a high {{performance}} IP lookup engine remains a challenge due to increasingly stringent throughput requirements {{and the growing}} size of IP tables. An emerging approach for IP lookup {{is the use of}} set associative memory architecture, which is basically a hardware implementation of an open addressing <b>hash</b> <b>table</b> with the property that each row of the <b>hash</b> <b>table</b> can be <b>searched</b> in one memory cycle. While open addressing <b>hash</b> <b>tables,</b> in general, provide good average-case search performance, their memory utilization and worst-case performance can degrade quickly due to bucket overflows. This paper presents a new simple hash probing scheme called CHAP (Content-based HAsh Probing) that tackles the hash overflow problem. In CHAP, the probing is based on the content of the <b>hash</b> <b>table,</b> thus avoiding the classical side effects of probing. We show through experimenting with real IP tables how CHAP can effectively deal with the overflow. © IFIP International Federation for Information Processing 2009...|$|R
40|$|Summary. This paper summarises {{an effort}} towards {{approximation}} of an optimal sweepplane approach to axis aligned bounding box intersection problem. In particular a spatial <b>hash</b> <b>table</b> and priority <b>search</b> tree are combined {{in order to}} obtain a data structure suitable for solving two-dimensional dynamic rectangle intersection problem. Some variants of this structure allow logarithmic update and query times, although all of them suffer from repeated reports of intersections. This constrains an efficient application of presented algorithms only to sparse box distributions, where penalty of additional algorithmic effort for suppressing repeated reports is minor. ...|$|R
5000|$|The {{main data}} {{structures}} using the various facilities {{of the program}} are in doubly linked list, which facilitates their movement. The maximum number of entries is limited only by available memory. Search operations {{in most of the}} facilities are carried out linearly, a fact that causes a mild but almost imperceptible impact. Because of the speed with which tends to increase the traffic monitor IPs, it use a <b>hash</b> <b>table</b> to perform <b>searches</b> more efficiently. (Search operations are carried out whenever the program needs to check if it is already listed the Ethernet or IP address or protocol or network port.|$|R
40|$|Abstract. This paper {{presents}} our {{integration of}} efficient resolution-based theorem provers into the Jahob data structure verification system. Our experimental {{results show that}} this approach enables Jahob to automatically verify the correctness {{of a range of}} complex dynamically instantiable data structures, including data structures such as <b>hash</b> <b>tables</b> and <b>search</b> trees, without the need for interactive theorem proving or techniques tailored to individual data structures. Our primary technical results include: (1) a translation from higher-order logic to first-order logic that enables the application of resolution-based theorem provers and (2) a proof that eliminating type (sort) information in formulas is both sound and complete, even {{in the presence of a}} generic equality operator. Moreover, our experimental results show that the elimination of this type information dramatically decreases the time required to prove the resulting formulas. These techniques enabled us to verify complex correctness properties of Java programs such as a mutable set implemented as an imperative linked list, a finite map implemented as a functional ordered tree, a <b>hash</b> <b>table</b> with a mutable array, and a simple library system example that uses these container data structures. Our system verifies (in a matter of minutes) that data structure operations correctly update the finite map, that they preserve data structure invariants (such as ordering of elements, membership in appropriate <b>hash</b> <b>table</b> buckets, or relationships between sets and relations), and that there are no run-time errors such as null dereferences or array out of bounds accesses. ...|$|R
40|$|This paper {{proposes a}} general {{framework}} for adding linearizable iterators to certain data structures that implement set operations. We introduce a condition on set operations, called locality, which informally states that set operations never make elements existing {{in the data}} structure unreachable to a sequential iterator's traversal. Data structures satisfying the locality condition can be augmented with a linearizable iterator via the proposed framework. Our technique is broadly applicable {{to a variety of}} data structures, including <b>hash</b> <b>tables</b> and binary <b>search</b> trees. We apply the technique to data structures taken from existing literature, prove locality of their operations, and demonstrate that the iterator framework does not significantly affect the performance of concurrent set operations. Comment: 19 pages, 13 figure...|$|R
40|$|Recent {{years have}} {{witnessed}} the success of binary hashing techniques in approximate nearest neighbor search. In practice, multiple <b>hash</b> <b>tables</b> are usually built using hashing to cover more desired results in the hit buckets of each table. However, rare work studies the unified approach to constructing multiple informative <b>hash</b> <b>tables</b> using any type of hashing algorithms. Meanwhile, for multiple <b>table</b> <b>search,</b> it also lacks of a generic query-adaptive and fine-grained ranking scheme that can alleviate the binary quantization loss suffered in the standard hashing techniques. To solve the above problems, in this paper, we first regard the table construction as a selection problem over a set of candidate hash functions. With the graph representation of the function set, we propose an efficient solution that sequentially applies normalized dominant set to finding the most informative and independent hash functions for each table. To further reduce the redundancy between tables, we explore the reciprocal <b>hash</b> <b>tables</b> in a boosting manner, where the hash function graph is updated with high weights emphasized on the misclassified neighbor pairs of previous <b>hash</b> <b>tables.</b> To refine the ranking of the retrieved buckets within a certain Hamming radius from the query, we propose a query-adaptive bitwise weighting scheme to enable fine-grained bucket ranking in each <b>hash</b> <b>table,</b> exploiting the discriminative power of its hash functions and their complement for nearest neighbor search. Moreover, we integrate such scheme into the multiple <b>table</b> <b>search</b> using a fast, yet reciprocal table lookup algorithm within the adaptive weighted Hamming radius. In this paper, both the construction method and the query-adaptive search method are general and compatible with different types of hashing algorithms using different feature spaces and/or parameter settings. Our extensive experiments on several large-scale benchmarks demonstrate that the proposed techniques can significantly outperform both the naive construction methods and the state-of-the-art hashing algorithms...|$|R
40|$|We {{present a}} {{strongly}} history independent (SHI) <b>hash</b> <b>table</b> that supports <b>search</b> in O(1) worst-case time, and insert and delete in O(1) expected time using O(n) data space. This matches the bounds for dynamic perfect hashing, and improves {{on the best}} previous results by Naor and Teague on history independent hashing, which were either weakly history independent, or only supported insertion and search (no delete) each in O(1) expected time. The results {{can be used to}} construct many other SHI data structures. We show straightforward constructions for SHI ordered dictionaries: for n keys from { 1, [...] ., n k} searches take O(log log n) worst-case time and updates (insertions and deletions) O(log log n) expected time, and for keys in the comparison model searches take O(log n) worst-case time and updates O(log n) expected time. We also describe a SHI data structure for the order-maintenance problem. It supports comparisons in O(1) worst-case time, and updates in O(1) expected time. All structures use O(n) data space. ...|$|R
40|$|A new linear <b>search</b> for <b>hash</b> <b>tables</b> is presented, whose {{increment}} step is {{a function}} of the key being addressed. Comparisons with known methods are given, in terms of efficiency and computation complexity. In particular, the new method applies to tables of size n = 2 r, allows full <b>table</b> <b>searching,</b> and practically results to be free of primary clustering at a very low cost...|$|R
40|$|As the Internet grows, {{both the}} number of rules in packet {{filtering}} databases and {{the number of}} prefixes in IP lookup tables inside the router are growing. The packet processing engine is a critical part of the Internet router as it is used to perform packet forwarding (PF) and packet classification (PC). In both applications, processing has to be at wire speed. It is common to use hash-based schemes in packet processing engines; however, the downside of classic hashing techniques such as overflow and worst case memory access time, has to be dealt with. Implementing <b>hash</b> <b>tables</b> using set associative memory has the property that each bucket of a <b>hash</b> <b>table</b> can be <b>searched</b> in one memory cycle outperforming the conventional Ternary CAMs in terms of power and scalability. In this paper we present “Progressive Hashing ” (PH), a general open addressing hash-based packet processing scheme for Internet routers using the set associative memory architecture. Our scheme is an extension of the multiple hashing scheme and is amendable to high-performance hardware implementation with low overflow and low memory access latency. We show by experimenting with real IP lookup tables and synthetic packet filtering databases that PH reduces the overflow over the multiple hashing. The proposed PH processing engine is estimated to achieve an average processing speed of 160 Gbps for the PC application and 320 Gbps for the PF application...|$|R
50|$|When the hash {{function}} {{is used to}} store values in a <b>hash</b> <b>table</b> that outlives {{the run of the}} program, and the <b>hash</b> <b>table</b> needs to be expanded or shrunk, the <b>hash</b> <b>table</b> is referred to as a dynamic <b>hash</b> <b>table.</b>|$|R
40|$|A <b>hash</b> <b>table</b> is a {{fundamental}} data structure in computer science that can offer rapid storage and retrieval of data. A leading implementation for string keys is the cacheconscious array <b>hash</b> <b>table.</b> Although fast with strings, there is currently no information in the research literature on its performance with integer keys. More importantly, {{we do not know}} how efficient an integer-based array <b>hash</b> <b>table</b> is compared to other <b>hash</b> <b>tables</b> that are designed for integers, such as bucketized cuckoo hashing. In this paper, we explain how to efficiently implement an array <b>hash</b> <b>table</b> for integers. We then demonstrate, through careful experimental evaluations, which <b>hash</b> <b>table,</b> whether it be a bucketized cuckoo <b>hash</b> <b>table,</b> an array <b>hash</b> <b>table,</b> or alternative <b>hash</b> <b>table</b> schemes such as linear probing, offers the best performance—with respect to time and space— for maintaining a large dictionary of integers in-memory, on a current cache-oriented processor...|$|R
50|$|Linear hashing is a <b>hash</b> <b>table</b> {{algorithm}} {{that permits}} incremental <b>hash</b> <b>table</b> expansion. It is implemented using a single <b>hash</b> <b>table,</b> but with two possible lookup functions.|$|R
50|$|Data {{structures}} {{can also}} be devised which have weak tracking features. For instance, weak <b>hash</b> <b>tables</b> are useful. Like a regular <b>hash</b> <b>table,</b> a weak <b>hash</b> <b>table</b> maintains an association between pairs of objects, where each pair is understood {{to be a key}} and value. However, the <b>hash</b> <b>table</b> does not actually maintain a strong reference on these objects. A special behavior takes place when either the key or value or both become garbage: the <b>hash</b> <b>table</b> entry is spontaneously deleted. There exist further refinements such as <b>hash</b> <b>tables</b> which have only weak keys (value references are ordinary, strong references) or only weak values (key references are strong).|$|R
40|$|Building a high {{performance}} IP packet forwarding (PF) engine remains a challenge due to increasingly stringent throughput requirements {{and the growing}} sizes of IP forwarding tables. The router has to match the incoming packet's IP address against the forwarding table. The matching process {{has to be done}} in wire speed which is why scalability and low power consumption are features that PF engines must maintain. It is common for PF engines to use hash tables; however, the classic hashing downsides {{have to be dealt with}} (e. g., collisions, worst case memory access time, [...] . etc.). While open addressing <b>hash</b> <b>tables,</b> in general, provide good average case search performance, their memory utilization and worst case performance can degrade quickly due to collisions that leads to bucket overflows. Set associative memory can be used for hardware implementations of <b>hash</b> <b>tables</b> with the property that each bucket of a <b>hash</b> <b>table</b> can be <b>searched</b> in one memory cycle. Hence, PF engine architectures based on associative memory will outperform those based on the conventional Ternary Content Addressable Memory (TCAM) in terms of power and scalability. The two standard solutions to the overflow problem are either to use some sort of predefined probing (e. g., linear or quadratic) or to use multiple hash functions. This work presents two new hash schemes that extend both aforementioned solutions to tackle the overflow problem efficiently. The first scheme is a hash probing scheme that is called Content-based HAsh Probing, or CHAP. CHAP is a probing scheme that is based on the content of the <b>hash</b> <b>table</b> to avoid the classical side effects of predefined hash probing methods (i. e., primary and secondary clustering phenomena) and at the same time reduces the overflow. The second scheme, called Progressive Hashing, or PH, is a general multiple hash scheme that reduces the overflow as well. PH splits the prefixes into groups where each group is assigned one hash function, then reuse some hash functions in a progressive fashion to reduce the overflow. We show by experimenting with real IP lookup tables that both schemes outperform other hashing schemes...|$|R
40|$|In this paper, {{the author}} proposes {{a series of}} {{multilevel}} double hashing schemes called cascade <b>hash</b> <b>tables.</b> They use several levels of <b>hash</b> <b>tables.</b> In each table, we use the common double hashing scheme. Higher level <b>hash</b> <b>tables</b> work as fail-safes of lower level <b>hash</b> <b>tables.</b> By this strategy, it could effectively reduce collisions in hash insertion. Thus it gains a constant worst case lookup time with a relatively high load factor(70 % − 85 %) in random experiments. Different parameters of cascade <b>hash</b> <b>tables</b> are tested. ...|$|R
