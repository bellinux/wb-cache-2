0|1557|Public
40|$|Abstract Currently, {{there are}} very few {{programs}} with the capability to create synthetic, but realistic data. This kind of data is useful for testing and evaluating data mining and data matching systems without endangering the rights of actual persons. This paper is a report on efforts to expand upon an existing <b>data</b> <b>generator</b> program such that it can then produce temporal or longitudinal data, i. e. data that follows some timebased or time-related pattern. At the Research School of Computer Science (RSCS) at the Australian National University (ANU), a flexible <b>data</b> <b>generator</b> has been developed for the creation of synthetic personal information. It is for this <b>data</b> <b>generator</b> that the module for longitudinal data generation is created. With this prototype, the <b>data</b> <b>generator</b> can then create personal data that reflects time-related trends, such as credit history, address changes, and transaction records. Keeping true to the programming of the <b>data</b> <b>generator,</b> the longitudinal <b>data</b> <b>generator</b> has been developed using the Python programming language...|$|R
40|$|Abstract [...] Test data {{generation}} {{is basically the}} process of identifying a set of data which satisfy the criteria set for testing. Lot of research have been done by many researchers and they developed many test <b>data</b> <b>generators</b> like random test <b>data</b> <b>generators,</b> symbolic test <b>data</b> <b>generators</b> and dynamic test <b>data</b> <b>generators.</b> This paper applied the optimization study of the test case generation based on the Genetic Algorithm and generates test cases which are far more reliable. 4. [REPLACE] New population {{generation is}} replaced. 5. [TEST] If the specified condition is satisfied stop and return the solution [1]. Keywords [...] Test case generation, Genetic Algorithms, fitness functions, optimizatio...|$|R
5000|$|... foomatic-db-hpijs: Foomatic XML <b>data</b> <b>generators</b> for HP's HPIJS driver.|$|R
5000|$|Pathwise test <b>data</b> <b>generators</b> require two inputs {{from the}} user: ...|$|R
40|$|The Lander Trajectory Reconstruction (LTR) {{computer}} {{program is a}} tool for analysis of the planetary entry trajectory and atmosphere reconstruction process for a lander or probe. The program {{can be divided into}} two parts: (1) the <b>data</b> <b>generator</b> and (2) the reconstructor. The <b>data</b> <b>generator</b> provides the real environment in which the lander or probe is presumed to find itself. The reconstructor reconstructs the entry trajectory and atmosphere using sensor data generated by the <b>data</b> <b>generator</b> and a Kalman-Schmidt consider filter. A wide variety of vehicle and environmental parameters may be either solved-for or considered in the filter process...|$|R
5000|$|Test <b>Data</b> <b>Generators</b> {{based on}} their {{approaches}} are typically classified into ...|$|R
40|$|Generating {{data sets}} for the {{performance}} testing of database systems on a particular hardware configuration and application domain is a very time consuming and tedious process. It is time consuming, {{because of the large}} amount of data that needs to be generated and tedious, because new <b>data</b> <b>generators</b> might need to be developed or existing once adjusted. The difficulty in generating this data is amplified by constant advances in hardware and software that allow the testing of ever larger and more complicated systems. In this paper, we present an approach for rapidly developing customized <b>data</b> <b>generators.</b> Our approach, which is based on the Parallel <b>Data</b> <b>Generator</b> Framework (PDGF), deploys a new concept of so called meta generators. Meta generators extend the concept of column-based generators in PDGF. Deploying meta generators in PDGF significantly reduces the development effort of customized <b>data</b> <b>generators,</b> it facilitates their debugging and eases their maintenance. 1...|$|R
40|$|Abstract. We need to {{generate}} {{various kinds of}} XML data for diverse purposes. Existing XML <b>data</b> <b>generators</b> are developed {{to generate}} XML data that is suitable for particular applications, and their functionalities are limited in terms of generating XML data. This paper introduces a new versatile XML <b>data</b> <b>generator</b> called VeXGene that not only improves the drawbacks of existing XML <b>data</b> <b>generators</b> but also adds new data generation functions. For generating XML data, VeXGene uses raw data files and structure definition files. The raw data file is a text file that has user-supplied data. VeXGene can also generate XML data without accessing raw data files...|$|R
5000|$|Private label cards (depending on {{the terms}} of {{agreement}} between the company's financial terminal <b>data</b> <b>generator</b> ...|$|R
40|$|Abstract—Developing, for example, {{a simple}} booking web service with modern tools {{can be a}} matter of a few weeks work. Testing such a system should not need to take more time than that. Automatically {{generating}} tests from specified properties of the system using the tool QuickCheck provides professional developers with the required test efficiency. But how good is the quality of these automatically generated tests? Do they cover the cases that one would have written in manual tests? The quality depends on the specified properties and <b>data</b> <b>generators</b> and so far there has not been an objective way to evaluate the quality of these QuickCheck generators. In this paper we present a method to assess the quality of QuickCheck test <b>data</b> <b>generators</b> by formulating requirements on them. Using this method we can give feedback to developers of such <b>data</b> <b>generators</b> in an early stage. The method supports developers in improving <b>data</b> <b>generators,</b> which may lead to an increase of the effectiveness in testing while maintaining the same efficiency. I...|$|R
40|$|We {{present a}} novel {{counterexample}} generator for the interactive theorem prover Isabelle {{based on a}} compiler that synthesizes test <b>data</b> <b>generators</b> for functional programming languages (e. g. ML, Haskell) from specifications in Isabelle. In contrast to naive type-based test <b>data</b> <b>generators,</b> the smart generators take the preconditions into account and only generate tests that fulfill the preconditions. The smart generators are constructed by a compiler that reformulates the preconditions as logic programs and analyzes them with an enriched mode inference. From this inference, the compiler can construct the desired generators in the functional programming language. Applying these test <b>data</b> <b>generators</b> reduces the number of tests significantly and enables us to find errors in specifications where naive random and exhaustive testing fail...|$|R
40|$|Synthetically {{generated}} {{data has}} always been important for evaluating and understanding new ideas in database research. In this paper, we describe a <b>data</b> <b>generator</b> for generating synthetic complex-structured XML data that allows for {{a high level of}} control over the characteristics of the generated <b>data.</b> This <b>data</b> <b>generator</b> is certainly not the ultimate {{solution to the problem of}} generating synthetic XML data, but we have found it very useful in our research on XML data management, and we believe that it can also be useful to other researchers. Furthermore, we hope that this paper starts a discussion in the XML community about characterizing and generating XML data, and that it may serve as a first step towards developing a commonly accepted XML <b>data</b> <b>generator</b> for our community. ...|$|R
30|$|Nowadays {{a number}} of {{software}} packages support CityGML, from <b>data</b> <b>generators</b> to visualisers. In this section they are overviewed considering their ADE support.|$|R
40|$|The aim of {{this thesis}} is to {{research}} the current possibilities and limitations of automatic generation of synthetic XML and JSON documents used {{in the area of}} Big Data. The first part of the work discusses the properties of the most used XML <b>data</b> <b>generators,</b> Big <b>Data</b> and JSON <b>generators</b> and compares them. The next part of the thesis proposes an algorithm for data generation of semistructured data. The main focus of the algorithm is on the parallel execution of the generation process while preserving the ability to control the contents of the generated documents. The <b>data</b> <b>generator</b> can also use samples of real data in the generation of the synthetic data and is also capable of automatic creation of simple references between JSON documents. The last part of the thesis provides the results of experiments with the <b>data</b> <b>generator</b> exploited for the purpose of testing database MongoDB, describes its added value and compares it to other solutions. Powered by TCPDF (www. tcpdf. org...|$|R
40|$|This paper {{presents}} a synthetic <b>data</b> <b>generator</b> that outputs timestamped transactional data with embedded temporal patterns {{controlled by a}} set of input parameters. In particular, calendar schema, which is determined by a hierarchy of input time granularities, is used as a framework of possible temporal patterns. An example of calendar schema is (year, month, day), which provides a framework for calendar-based temporal patterns of the form - 38352, where each is either an integer or the symbol. For example, is such a pattern, which corresponds to the time intervals consisting of all the 16 th days of all months in year 2000. This paper also evaluates the <b>data</b> <b>generator</b> through a series of experiments. The synthetic <b>data</b> <b>generator</b> is intended to provide support for data mining community in evaluating various aspects (especially the temporal aspects and the scalability) of data mining algorithms...|$|R
40|$|Simulated {{boundary}} potential {{data for}} Electrical Impedance Tomography (EIT) are {{generated by a}} MATLAB based EIT <b>data</b> <b>generator</b> and the resistivity reconstruction is evaluated with Electrical Impedance Tomography and Diffuse Optical Tomography Reconstruction Software (EIDORS). Circular domains containing subdomains as inhomogeneity are defined in MATLAB-based EIT <b>data</b> <b>generator</b> and the boundary data are calculated by a constant current simulation with opposite current injection (OCI) method. The resistivity images reconstructed for different boundary data sets and images are analyzed with image parameters to evaluate the reconstruction...|$|R
50|$|Intelligent Test <b>Data</b> <b>Generators</b> {{depend on}} {{sophisticated}} {{analysis of the}} code to guide the search of the test data. Intelligent Test <b>Data</b> <b>Generators</b> are essentially utilize one of the test data generation method coupled with the {{detailed analysis of the}} code. This approach may generate test data quicker than the other approaches but the analysis required for the utilization of this approach over a wide variety of programs is quite complex and requires a great deal of insight to anticipate the different situations that may arise.|$|R
40|$|AbstractDesigning {{test cases}} and {{generating}} data {{are very important}} phases in software engineering these days. In order to generate test <b>data,</b> some <b>generators</b> such as random test <b>data</b> <b>generators,</b> <b>data</b> specification <b>generators</b> and path-oriented (Path-Wise) test <b>data</b> <b>generators</b> are employed. One {{of the most important}} problems in the path-oriented test <b>data</b> <b>generator</b> is the lack of attention given to discovering faults by the test data. In this paper an approach is proposed to generate some test data automatically so that we can realize the goal of discovering more faults in less time. The number of faults near the boundaries of the input domain is more than the center, according to the Pareto 80 – 20 principle the test data of this approach will be generated at 20 % of the allowable area boundary. To do this, we extracted the boundary hypercuboids and then the test data will be generated by exploiting these hypercuboids. The experimental results show that the fault detection probability and the fault detection speed are improved significantly, compared with the previous approaches. By generating data in this way, more faults are discovered {{in a short period of}} time which makes it more possible to deliver products on time...|$|R
40|$|The {{exponential}} {{growth in the}} amount of data retained by today’s systems is fostered by a recent paradigm shift towards cloud computing and the vast deployment of data-hungry applications, such as social media sites. At the same time systems are capturing more sophisticated data. Running realistic benchmarks to test the performance and robustness of these applications is becoming increasingly difficult, because of the amount of data that needs to be generated, the number of systems that need to generate the data and the complex structure of the data. These three reasons are intrinsically connected. Whenever large amounts of data are needed, its generation process needs to be highly parallel, in many cases across-systems. Since the structure of the data {{is becoming more and more}} complex, its parallel generation is extremely challenging. Over the years there have been many papers about <b>data</b> <b>generators,</b> but there has not been a comprehensive overview of the requirements of today’s <b>data</b> <b>generators</b> covering the most complex problems to be solved. In this paper we present such an overview by analyzing the requirements of today’s <b>data</b> <b>generators</b> and either explaining how the problems have been solved in exist-ing <b>data</b> <b>generators,</b> or showing why the problems have not been solved yet...|$|R
40|$|Abstract. Data {{generation}} {{is a key}} issue in big data benchmarking that aims to generate application-specific data sets to meet the 4 V re-quirements of big data. Specifically, big <b>data</b> <b>generators</b> need to generate scalable data (Volume) of different types (Variety) under controllable generation rates (Velocity) while keeping the important characteristics of raw data (Veracity). This gives rise to various new challenges about how we design generators efficiently and successfully. To date, most exist-ing techniques can only generate limited types of data and support spe-cific big data systems such as Hadoop. Hence we develop a tool, called Big <b>Data</b> <b>Generator</b> Suite (BDGS), to efficiently generate scalable big data while employing data models derived from real data to preserve data veracity. The effectiveness of BDGS is demonstrated by developing six <b>data</b> <b>generators</b> covering three representative data types (structured, semi-structured and unstructured) and three data sources (text, graph, and table data) ...|$|R
40|$|Prevention actions (trainings, audits) and {{inspections}} (tests, validations, code reviews) are {{the crucial}} factors in achieving {{a high quality}} level for any software application simply because low investments {{in this area are}} leading to significant expenses in terms of corrective actions needed for defect fixing. Mobile applications testing involves the use of various tools and scenarios. An important process is represented by test data generation. This paper proposes a test <b>data</b> <b>generator</b> (TDG) system for mobile applications using several sources for test data and it focuses on the UI layout files analyzer module. The proposed architecture aims to reduce time-to-market for mobile applications. The focus is on test <b>data</b> <b>generators</b> based on the source code, user interface layout files (using markup languages like XML or XAML) and application specifications. In order to assure a common interface for test <b>data</b> <b>generators,</b> an XML or JSON-based language called Data Specification Language (DSL) is proposed...|$|R
40|$|Synthetic {{generators}} {{are increasingly}} used to replace sensitive data with artificial data preserving to a predetermined extent {{the utility of}} the original data. When using synthetic <b>data</b> <b>generators,</b> re-identification analysis is usually disregarded on the grounds that, the released data being artificial, no real re-identification is possible. While this may be reasonable if synthetic generation is performed on the confidential outcome attributes, it is an unrealistic assumption if synthetic data generation is performed on the quasi-identifier attributes. In the latter case, reidentification can indeed happen if a snooper is able to link an external identified data source with some record in the released dataset using the quasi-identifier attributes: coming up with a correct pair (identifier, confidential attributes) is indeed a re-identification. This paper is a case study of re-identification risk for three synthetic <b>data</b> <b>generators</b> under different worst-case disclosure scenarios. The results give some insight on how synthetic <b>data</b> <b>generators</b> can be tuned to minimize re-identification risk...|$|R
3000|$|XMark 2 is an XML {{benchmark}} dataset generated {{using the}} <b>data</b> <b>generator</b> with factor = 0.05. It is deep and has many regular structural patterns. It includes very few recursive elements.|$|R
50|$|FOUNDATION HSE is {{a control}} network {{technology}} {{specifically designed for}} process automation to connect higher-level devices such as controllers and remote-I/O, high-density <b>data</b> <b>generators</b> etc., and for horizontal integration of subsystems.|$|R
40|$|In {{this paper}} we {{introduce}} a modular, highly flexible, opensource environment for data generation. Using an existing graphical data flow tool, the user can combine {{various types of}} modules for numeric and categorical <b>data</b> <b>generators.</b> Additional functionality is added via the data processing framework in which the generator modules are embedded. The resulting data flows {{can be used to}} document, deploy, and reuse the resulting <b>data</b> <b>generators.</b> We describe the overall environment and individual modules and demonstrate how they can be used for the generation of a sample, complex customer/product database with corresponding shopping basket data, including various artifacts and outliers...|$|R
50|$|Based on the Mathematical Modelling above we {{can simply}} state the Test <b>Data</b> <b>Generator</b> Problem as:Given a program P and a path u, {{generate}} input x ∈ S, so that x traverses path u.|$|R
40|$|Random <b>data</b> <b>generators</b> play an {{important}} role in computer science and engineering since they aim at simulating reality in IT systems. Software random <b>data</b> <b>generators</b> cannot be reliable enough for critical applications due to their intrinsic determinism, while hardware random <b>data</b> <b>generators</b> are difficult to integrate within applications and are not always affordable in all circumstances. We present an approach that makes use of entropic data sources to compute the random data generation task. In particular, our approach exploits the chaotic phenomena happening in the crowd. We extract these phenomena from social networks since they reflect the behavior of the crowd. We have implemented the approach in a database system, RandomDB, to show its efficiency and its flexibility over the competitor approaches. We used RandomDB by taking data from Twitter, Facebook and Flickr. The experiments show that these social networks are sources to generate reliable randomness and RandomDB a system that can be used for the task. Hopefully, our experience will drive the development of a series of applications that reuse the same data in several and different scenarios...|$|R
40|$|This paper {{presents}} {{two cases}} of random banking <b>data</b> <b>generators</b> based on migration matrices and scoring rules. The banking <b>data</b> <b>generator</b> is {{a breakthrough in}} researches aimed at finding a method to compare various credit scoring techniques. These data are very useful for various analyses to understand the complexity of banking processes {{in a better way}} and are also of use for students and their researches. Another application can be in the case of small samples, e. g. when historical data are too fresh or are connected with the processing of a small number of exposures. In these cases a <b>data</b> <b>generator</b> can extend a sample to an adequate size for advanced analysis. The influence of one cyclic macro-economic variable on client characteristics and their stability over time is analyzed. Some stimulating conclusions for crisis behavior are presented, namely that if a crisis is impacted by both factors: application and behavioral, then {{it is very difficult to}} clearly indicate these factors in a typical scoring analysis and the crisis becomes widespread in every kind of risk report...|$|R
50|$|The Wind <b>Data</b> <b>Generator</b> (WDG) is a Wind Energy Software tool {{capable of}} running WRF (Weather Research and Forecasting) model {{to create a}} wind atlas and to {{generate}} wind data at resolutions of 3 km to 10 km.|$|R
30|$|Step 1 : Input the {{topology}} structure data {{of power}} system and transient parameters <b>data</b> (<b>generators,</b> excitation system, and SVC). Set the maximum iteration {{number and the}} population parameters for ITMCO algorithm. Set the initial iteration number to be zero.|$|R
40|$|International audienceIn many {{fields of}} {{research}} and business data sizes are breaking the petabyte barrier. This imposes new problems and research possibilities for the database community. Usually, data of this size is stored in large clusters or clouds. Although clouds have become very popular in recent years, there is only little work on benchmarking cloud applications. In this paper we present a <b>data</b> <b>generator</b> for cloud sized applications. Its architecture makes the <b>data</b> <b>generator</b> easy to extend and to configure. A key feature is {{the high degree of}} parallelism that allows linear scaling for arbitrary numbers of nodes. We show how distributions, relationships and dependencies in data can be computed in parallel with linear speed up...|$|R
40|$|Test {{effectiveness}} {{is a fundamental}} quality aspect of a test specification, which reflects its ability to demonstrate system quality levels and discover system faults. The {{effectiveness is}} tightly linked {{with the quality of}} the test data. The paper highlights specific challenges related to testing eHealth applications and emphasizes the difficulties in testing HL 7 v 3 based applications. This paper presents a new approach on generating input test data sets: a highly customizable distance-based test <b>data</b> <b>generator.</b> The paper highlights the importance of having organized structures of test data and shows how the proposed test <b>data</b> <b>generator</b> uses adaptable distances to create clusters of test <b>data.</b> The <b>generator</b> is designed to create test data in a testing language independent format and provide means of conversion to the format used by the target testing language. A general architecture of this generator is presented, and implementation guidelines are proposed. The paper also presents the conclusions drawn from validating the generator in a real scenario...|$|R
40|$|This master's {{thesis is}} focused on the problem of data generation. At the beginning, it {{presents}} several applications for data generation and describes the data generation process. Then it deals with development of framework for <b>data</b> <b>generators</b> and demonstrational application for validating the framework...|$|R
40|$|Incorporating {{realistic}} {{data and}} research examples into quantitative (e. g., statistics and research methods) courses {{has been widely}} recommended for enhancing student engagement and comprehension. One way to achieve these ends {{is to use a}} <b>data</b> <b>generator</b> to emulate the data in published research articles. MorseGen is a free <b>data</b> <b>generator</b> that creates realistic, individual-level data based on user-specified summary statistics (e. g., N, mean, standard deviation, and r). These values can be used in course exercises that allow students to replicate the published results for any between-subjects design or correlation study. Using realistic data generated by MorseGen addresses multiple learning goals proposed for undergraduate psychology students as well as the initiative to increase the realism of exercises and examples in quantitative courses...|$|R
40|$|Rule-based {{software}} {{test data}} generation is proposed {{as an alternative}} to either path/predicate analysis or random data generation. A prototype rule-based test <b>data</b> <b>generator</b> for Ada programs is constructed and compared to a random test <b>data</b> <b>generator.</b> Four Ada procedures are used in the comparison. Approximately 2000 rule-based test cases and 100, 000 randomly generated test cases are automatically generated and executed. The success of the two methods is compared using standard coverage metrics. Simple statistical tests showing that even the primitive rule-based test data generation prototype is significantly better than random data generation are performed. This result demonstrates that rule-based test data generation is feasible and shows great promise in assisting test engineers, especially when the rule base is developed further...|$|R
30|$|Where, B sub-databases {{contain the}} most {{difficult}} fingerprint images used for evaluating protection strength of the proposed scheme. We generated a login database of size 150 UID’s and PWD’s, and an adult bank database of size 450000 records using GNU-licensed open source <b>data</b> <b>generator</b> tool [56].|$|R
