3372|2461|Public
25|$|Choice of model: This {{depends on}} the <b>data</b> <b>representation</b> and the application. Overly complex models slow learning.|$|E
25|$|Mass {{spectrometry}} produces {{various types}} of data. The most common <b>data</b> <b>representation</b> is the mass spectrum.|$|E
25|$|BIND {{is based}} on a data {{specification}} written using Abstract Syntax Notation 1 (ASN.1) language. ASN.1 is used also by NCBI when storing data for their Entrez system and because of this BIND uses the same standards as NCBI for <b>data</b> <b>representation.</b> The ASN.1 language is preferred because it can be easily translated into other data specification languages (e.g. XML), can easily handle complex data and can be applied to all biological interactions – not just proteins. Bader and Hogue (2000) have prepared a detailed manuscript on the ASN.1 data specification used by BIND.|$|E
40|$|This paper {{examines}} {{a research}} study to foster mathematical discourse about <b>data</b> <b>representations</b> among Indonesian students. It was {{situated in the}} context of implementing an Indonesian version of Realistic Mathematics Education, labelled as PMRI, in primary schools. A case study of one lesson involving Grade 6 students on the choice of <b>data</b> <b>representations</b> in Yogyakarta will be discussed. The analysis focused on the enacted social norms and sociomathematical norms during a wholeclass discussion and their impacts on students’ knowledge of <b>data</b> <b>representations.</b> The need for constant effort to enact these norms in classroom mathematical discourse is highlighted...|$|R
5000|$|Several {{different}} {{techniques for}} rendering auditory <b>data</b> <b>representations</b> can be categorized: ...|$|R
5000|$|While {{transferring}} data {{over the}} network, four <b>data</b> <b>representations</b> can be used: ...|$|R
25|$|The {{three top}} layers in the OSI model, i.e. the {{application}} layer, the presentation layer and the session layer, are not distinguished separately in the TCP/IP model which only has an application layer above the transport layer. While some pure OSI protocol applications, such as X.400, also combined them, {{there is no}} requirement that a TCP/IP protocol stack must impose monolithic architecture above the transport layer. For example, the NFS application protocol runs over the eXternal <b>Data</b> <b>Representation</b> (XDR) presentation protocol, which, in turn, runs over a protocol called Remote Procedure Call (RPC). RPC provides reliable record transmission, so it can safely use the best-effort UDP transport.|$|E
2500|$|New Media as the Mix Between Existing Cultural Conventions and the Conventions of Software New Media {{today can}} be {{understood}} as the mix between older cultural conventions for <b>data</b> <b>representation,</b> access, and manipulation and newer conventions of <b>data</b> <b>representation,</b> access, and manipulation. The [...] "old" [...] data are representations of visual reality and human experience, and the [...] "new" [...] data is numerical data. The computer is kept out of the key [...] "creative" [...] decisions, and is delegated to the position of a technician. e.g. In film, software is used in some areas of production, in others are created using computer animation.|$|E
2500|$|The science {{reasoning}} test is a 35-minute, 40-question test. [...] There {{are seven}} passages each followed by {{five to seven}} questions. [...] The passages have three different formats: <b>Data</b> <b>Representation,</b> Research Summary, and Conflicting Viewpoints. While the format used to be very predictable (i.e. there were always three <b>Data</b> <b>Representation</b> passages with 5 questions following each, 3 Research Summary passages with six questions each, and one Conflicting Viewpoints passage with 7 questions), {{when the number of}} passages was reduced from 7 to 6, more variability in the number of each passage type started to appear. But so far, there is still always only one Conflicting Viewpoints passage. These changes are very recent and so the only reference to them so far is in the recently released practice test on the ACT website.|$|E
40|$|This {{dissertation}} studies representation matching: {{the problem}} of creating semantic mappings between two <b>data</b> <b>representations.</b> Examples of <b>data</b> <b>representations</b> are relational schemas, ontologies, and XML DTDs. Examples of semantic mappings include "element location of one representation maps to element address of the other", "contact-phone maps to agent-phone", and "listed-price maps to price * (1 + tax-rate) " [...] ...|$|R
5000|$|The toolkit {{provides}} <b>data</b> <b>representations</b> {{in general}} form for images (arbitrary dimension) and (unstructured) meshes.|$|R
30|$|Instead {{of using}} raw input for data indexing, Deep Learning {{can be used}} to {{generate}} high-level abstract <b>data</b> <b>representations</b> which will be used for semantic indexing. These representations can reveal complex associations and factors (especially when the raw input was Big Data), leading to semantic knowledge and understanding. <b>Data</b> <b>representations</b> {{play an important role in}} the indexing of data, for example by allowing data points/instances with relatively similar representations to be stored closer to one another in memory, aiding in efficient information retrieval. It should be noted, however, that the high-level abstract <b>data</b> <b>representations</b> need to be meaningful and demonstrate relational and semantic association in order to actually confer a good semantic understanding and comprehension of the input.|$|R
2500|$|By {{the early}} 1960s, {{the utility of}} both linked lists and {{languages}} which use these structures as their primary <b>data</b> <b>representation</b> was well established. Bert Green of the MIT Lincoln Laboratory published a review article entitled [...] "Computer languages for symbol manipulation" [...] in IRE Transactions on Human Factors in Electronics in March 1961 which summarized {{the advantages of the}} linked list approach. A later review article, [...] "A Comparison of list-processing computer languages" [...] by Bobrow and Raphael, appeared in Communications of the ACM in April 1964.|$|E
2500|$|The {{use of the}} {{equality}} test (if (x==y) ...) requires care when dealing with floating-point numbers. Even simple expressions like 0.6/0.2-3==0 will, on most computers, fail to be true (in IEEE 754 double precision, for example, 0.6/0.2-3 is approximately equal to -4.44089209850063e-16). Consequently, such tests are sometimes replaced with [...] "fuzzy" [...] comparisons (if (abs(x-y) < epsilon) ..., where epsilon is sufficiently small and tailored to the application, such as 1.0E−13). The wisdom of doing this varies greatly, and can require numerical analysis to bound epsilon. Values derived from the primary <b>data</b> <b>representation</b> and their comparisons should be performed in a wider, extended, precision {{to minimize the risk}} of such inconsistencies due to round-off errors. It is often better to organize the code {{in such a way that}} such tests are unnecessary. For example, in computational geometry, exact tests of whether a point lies off or on a line or plane defined by other points can be performed using adaptive precision or exact arithmetic methods.|$|E
50|$|<b>Data</b> <b>Representation</b> and Encoding Standard: Society for Natural Language Technology Research {{has decided}} to accept UNICODE 5.0 and upwards as the {{standard}} for <b>data</b> <b>representation</b> and encoding for Bengali.|$|E
50|$|<b>Data</b> <b>representations</b> are {{structured}} forms suitable for computer-based transformations. These structures must {{exist in the}} original data or be derivable from the data themselves. They must retain the information and knowledge content and the related context within the original data to the greatest degree possible. The structures of underlying <b>data</b> <b>representations</b> are generally neither accessible nor intuitive to the user of the visual analytics tool. They are frequently more complex in nature than the original data and are not necessarily smaller in size than the original data. The structures of the <b>data</b> <b>representations</b> may contain {{hundreds or thousands of}} dimensions and be unintelligible to a person, but they must be transformable into lower-dimensional representations for visualization and analysis.|$|R
5000|$|<b>Data</b> <b>representations</b> and transformations that convert {{all types}} of {{conflicting}} and dynamic data in ways that support visualization and analysis ...|$|R
50|$|Other {{structured}} <b>data</b> <b>representations</b> such as XML or JSON {{can also}} be used as a PDS if no significant semantic restrictions are used.|$|R
50|$|Choose {{appropriate}} <b>data</b> <b>representation</b> and algorithms.|$|E
5000|$|Generic {{programming}} emphasizes {{separation of}} algorithms from <b>data</b> <b>representation</b> ...|$|E
5000|$|... #Caption: Figure4. MFA. Test <b>data.</b> <b>Representation</b> {{of groups}} of variables.|$|E
30|$|Deep Learning {{algorithms}} are one promising {{avenue of}} {{research into the}} automated extraction of complex <b>data</b> <b>representations</b> (features) at high levels of abstraction. Such algorithms develop a layered, hierarchical architecture of learning and representing data, where higher-level (more abstract) features are {{defined in terms of}} lower-level (less abstract) features. The hierarchical learning architecture of Deep Learning algorithms is motivated by artificial intelligence emulating the deep, layered learning process of the primary sensorial areas of the neocortex in the human brain, which automatically extracts features and abstractions from the underlying data [4]-[6]. Deep Learning algorithms are quite beneficial when dealing with learning from large amounts of unsupervised data, and typically learn <b>data</b> <b>representations</b> in a greedy layer-wise fashion [7],[8]. Empirical studies have demonstrated that <b>data</b> <b>representations</b> obtained from stacking up non-linear feature extractors (as in Deep Learning) often yield better machine learning results, e.g., improved classification modeling [9], better quality of generated samples by generative probabilistic models [10], and the invariant property of <b>data</b> <b>representations</b> [11]. Deep Learning solutions have yielded outstanding results in different machine learning applications, including speech recognition [12]-[16], computer vision [7],[8],[17], and natural language processing [18]-[20]. A more detailed overview of Deep Learning is presented in Section “Deep learning in data mining and machine learning”.|$|R
2500|$|Overcomplete dictionaries, however, do {{not require}} the atoms to be {{orthogonal}} (they {{will never be a}} basis anyway) thus allowing for more flexible dictionaries and richer <b>data</b> <b>representations.</b>|$|R
40|$|Abstract—Future {{digital signal}} {{processing}} (DSP) systems must provide robustness on algorithm and application level {{to the presence of}} reliability issues that come along with corresponding implementations in modern semiconductor process technologies. In this paper, we address this issue by investigating the impact of unreliable memories on general DSP systems. In particular, we propose a novel framework to characterize the effects of unreliable memories, which enables us to devise novel methods to mitigate the associated performance loss. We propose to deploy specifically designed <b>data</b> <b>representations,</b> which have the capability of substantially improving the system reliability compared to that realized by conventional <b>data</b> <b>representations</b> used in digital integrated circuits, such as 2 ’scomplement or sign-magnitude number formats. To demonstrate the efficacy of the proposed framework, we analyze the impact of unreliable memories on coded communication systems, and we show that the deployment of optimized <b>data</b> <b>representations</b> substantially improves the error-rate performance of such systems. I...|$|R
5000|$|Sedris (originally [...] "Synthetic Environment <b>Data</b> <b>Representation</b> and Interchange Specification") ...|$|E
5000|$|Per unit <b>data</b> <b>representation</b> yields {{important}} information about relative magnitudes.|$|E
5000|$|The {{majority}} of the real-time control systems used in aeronautics employ [...] "big endian" [...] processor architectures. This <b>data</b> <b>representation</b> was therefore specified for CANaerospace as well. With big endian <b>data</b> <b>representation,</b> the most significant bit of any datum is arranged leftmost and transmitted first on CANaerospace as shown in Figure 2.|$|E
40|$|Abstract. This paper {{considers}} {{the hypothesis that}} voting between multiple <b>data</b> <b>representations</b> can be more accurate than voting between multiple learning models. This hypothesis has been considered before (cf. [San 00]) but {{the focus was on}} voting methods rather than the <b>data</b> <b>representations.</b> In this paper, we focus on choosing specific <b>data</b> <b>representations</b> combined with simple majority voting. On the community standard CoNLL- 2000 data set, using no additional knowledge sources apart from the training data, we achieved 94. 01 Fβ= 1 score for arbitrary phrase identification compared to the previous best Fβ= 1 93. 90. We also obtained 95. 23 Fβ= 1 score for Base NP identification. Significance tests show that our Base NP identification score is significantly better than the previous comparable best Fβ= 1 score of 94. 22. Our main contribution is that our model is a fast linear time approach and the previous best approach is significantly slower than our system. ...|$|R
40|$|Future {{digital signal}} {{processing}} (DSP) systems must provide robustness on algorithm and application level {{to the presence of}} reliability issues that come along with corresponding implementations in modern semiconductor process technologies. In this paper, we address this issue by investigating the impact of unreliable memories on general DSP systems. In particular, we propose a novel framework to characterize the effects of unreliable memories, which enables us to devise novel methods to mitigate the associated performance loss. We propose to deploy specifically designed <b>data</b> <b>representations,</b> which have the capability of substantially improving the system reliability compared to that realized by conventional <b>data</b> <b>representations</b> used in digital integrated circuits, such as 2 s complement or sign-magnitude number formats. To demonstrate the efficacy of the proposed framework, we analyze the impact of unreliable memories on coded communication systems, and we show that the deployment of optimized <b>data</b> <b>representations</b> substantially improves the error-rate performance of such systems. Comment: Proc. of the IEEE Allerton Conference, 201...|$|R
40|$|The {{design and}} {{implementation}} of efficient aggregate data structures {{has been an important}} issue in functional programming. It is not clear how to select a good representation for an aggregate when access patterns to the aggregate are highly variant, or even unpredictable. Previous approaches rely on compile [...] time analyses or programmer annotations. These methods can be unreliable because they try to predict program behaviors before they are executed. We propose a probabilistic approach, which is based on Markov processes, for automatic selection of <b>data</b> <b>representations.</b> The selection is modeled as a random process moving in a graph with weighted edges. The proposed approach employs coin tossing at run [...] time to aid choosing suitable <b>data</b> <b>representations.</b> The transition probability function used by the coin tossing is constructed in a simple and common way from a measured cost function. We show that, under this setting, random selection of <b>data</b> <b>representations</b> can be quite effective. Th [...] ...|$|R
5000|$|Instantaneously trained neural {{networks}} use unary coding for efficient <b>data</b> <b>representation.</b>|$|E
5000|$|ISO 13399: ISO {{standard}} for cutting tool <b>data</b> <b>representation</b> and exchange ...|$|E
5000|$|... used PostScript data {{instead of}} XML and JSON for <b>data</b> <b>representation.</b>|$|E
30|$|In further {{evaluating}} the students' understanding of variation and distribution {{within and across}} <b>data</b> <b>representations,</b> we consider their responses to the assessment items Q 10 through Q 15 (Additional file 1).|$|R
30|$|Deep Learning {{constructs}} complicated representations for {{image and}} video data {{with a high}} level of abstraction. High-level <b>data</b> <b>representations</b> provided by Deep Learning can be used for simpler linear models for Big <b>Data.</b> This <b>representation</b> can be useful for image indexing and retrieval. In other words, Deep Learning can be used in the discriminative task of semantic tagging in the context of Big Data analysis.|$|R
30|$|Similar to textual data, Deep Learning {{can be used}} {{on other}} kinds of data to extract {{semantic}} representations from the input corpus, allowing for semantic indexing of that data. Given the relatively recent emergence of Deep Learning, additional work needs to be done on using its hierarchical learning strategy as a method for semantic indexing of Big Data. An remaining open question is what criteria is used to define “similar” when trying to extract <b>data</b> <b>representations</b> for indexing purposes (recall, data points that are semantically similar will have similar <b>data</b> <b>representations</b> in a specific distance space).|$|R
