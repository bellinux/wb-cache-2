0|10000|Public
40|$|How many {{pages are}} {{there on the}} Web? 5 B? 20 B? More? Less? Big bets on {{clusters}} in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. Language modeling techniques are applied to MSN’s <b>search</b> <b>logs</b> to estimate entropy. The perplexity is surprisingly small: millions, not billions. Entropy is {{a powerful tool for}} sizing challenges and opportunities. How hard is search? How hard are query suggestion mechanisms like auto-complete? How much does personalization help? All these difficult questions can be answered by estimation <b>of</b> <b>entropy</b> from <b>search</b> <b>logs.</b> What is the potential opportunity for personalization? In this paper, we propose a new way to personalize search, personalization with backoff. If we have relevant data for a particular user, we should use it. But if we don’t, back off to larger and larger classes of similar users. As a proof of concept, we use the first few bytes of the IP address to define classes. The coefficients of each backoff class are estimated with an EM algorithm. Ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography...|$|R
40|$|In {{addition}} to search queries {{and the corresponding}} clickthrough information, <b>search</b> engine <b>logs</b> record multidimensional information about user search activities, such as search time, location, vertical, and search device. Multidimensional mining <b>of</b> <b>search</b> <b>logs</b> can provide novel insights and useful knowledge for both search engine users and developers. In this paper, we describe our topic-concept cube project, which addresses the business need of supporting multidimensional mining <b>of</b> <b>search</b> <b>logs</b> effectively and efficiently. We answer two challenges. First, search queries and click-through data are well recognized sparse, and thus have to be aggregated properly for effective analysis. Second, {{there is often a}} gap between the topic hierarchies in multidimensional aggregate analysis and queries in <b>search</b> <b>logs.</b> To address those challenges, we develop a novel topicconcept model that learns a hierarchy of concepts and topics automatically from <b>search</b> <b>logs.</b> Enabled by the topicconcept model, we construct a topic-concept cube that supports online multidimensional mining <b>of</b> <b>search</b> <b>log</b> data. A distinct feature of our approach is that, in {{addition to}} the standard dimensions such as time and location, our topicconcept cube has a dimension of topics and concepts, which substantially facilitates the analysis of log data. To handle a huge amount of log data, we develop distributed algorithms for learning model parameters efficiently. We also devise approaches to computing a topic-concept cube. We report an empirical study verifying the effectiveness and efficiency of our approach on a real data set of 1. 96 billion queries and 2. 73 billion clicks...|$|R
40|$|Abstract. The LADS (Log Analysis for Digital Societies) task at CLEF aims at {{investigating}} user {{actions in}} a multilingual setting. We carried out an analysis <b>of</b> <b>search</b> <b>logs</b> with {{the objectives of}} investigating how users from different linguistic or cultural backgrounds behave in search, and how the discovery of patterns in user actions {{could be used for}} community identification. The findings confirm that users from a different background behave differently, and that there are identifiable patterns in the user actions. The findings suggest that there is scope for further investigation <b>of</b> how <b>search</b> <b>logs</b> can be exploited to personalise and improve cross-language search as well as improve the TEL search system. ...|$|R
40|$|AbstractThe {{timely and}} {{accurate}} identification of adverse drug reactions (ADRs) following drug approval is a persistent and serious public health challenge. Aggregated data drawn from anonymized logs of Web searchers {{has been shown}} to be a useful source of evidence for detecting ADRs. However, prior studies have been based on the analysis of established ADRs, the existence of which may already be known publically. Awareness of these ADRs can inject existing knowledge about the known ADRs into online content and online behavior, and thus raise questions about the ability of the behavioral log-based methods to detect new ADRs. In contrast to previous studies, we investigate the use <b>of</b> <b>search</b> <b>logs</b> for the early detection of known ADRs. We use a large set of recently labeled ADRs and negative controls to evaluate the ability <b>of</b> <b>search</b> <b>logs</b> to accurately detect ADRs in advance of their publication. We leverage the Internet Archive to estimate when evidence of an ADR first appeared in the public domain and adjust the index date in a backdated analysis. Our results demonstrate how <b>search</b> <b>logs</b> can be used to detect new ADRs, the central challenge in pharmacovigilance...|$|R
40|$|With the {{emergence}} of Web 2. 0, the amount of user-generated web data has sharply in-creased. Thus, many studies have proposed techniques to extract wisdom from these user-generated datasets. Some of these works have focused on extracting semantic relationships through the use <b>of</b> <b>search</b> <b>logs</b> or social annotations, but only hierarchical relationships have been considered. The goal {{of this paper is}} to detect various semantic relationships (hierar-chical and non-hierarchical) between concepts using <b>search</b> <b>logs</b> and social annotations. The experimental results demonstrate that our proposed approach constructs adequate relation-ships...|$|R
40|$|The {{behaviour}} of the searcher {{when using}} the search engine especially during the query formulation is crucial. Search engines capture users’ activities in the <b>search</b> <b>log,</b> which is stored at the search engine server. Due to the difficulty <b>of</b> obtaining this <b>search</b> <b>log,</b> this paper proposed and develops an interface framework to interface a Google search engine. This interface will capture users’ queries before redirect them to Google. The analysis <b>of</b> the <b>search</b> <b>log</b> will show that users are utilizing different types of queries. These queries are then classified as breadth and depth search query...|$|R
40|$|This paper {{describes}} the collaborative participation of Trinity College Dublin and Dublin City University in the Log Analysis for Digital Societies (LADS) task of LogCLEF 2009 track. An analysis <b>of</b> multilingual <b>search</b> <b>logs</b> {{was carried out}} with the objectives of investigating how users from different linguistic or cultural backgrounds behave in search, and how the discovery of patterns in user actions {{could be used for}} community identification. Our findings suggest that there is scope for further investigation <b>of</b> how <b>search</b> <b>logs</b> can be exploited to personalise and improve cross-language search as well as improve the TEL search system. Categories and Subject Descriptor...|$|R
40|$|Huge amounts <b>of</b> <b>search</b> <b>log</b> {{data have}} been {{accumulated}} at web search engines. Currently, a popular web search engine may every day receive billions of queries and collect tera-bytes of records about user search behavior. Beside <b>search</b> <b>log</b> data, {{huge amounts of}} browse log data have also been collected through client-side browser plug-ins. Such massive amounts <b>of</b> <b>search</b> and browse <b>log</b> data provide great opportunities for mining the wisdom of crowds and improving web search. At the same time, designing e↵ective and e cient methods to clean, process, and model log data also presents great challenges. In this survey, we focus on mining <b>search</b> and browse <b>log</b> data for web search. We start with an introduction to <b>search</b> and browse <b>log</b> data and an overview of frequently-used data summarizations in log mining. We then elaborate how log mining applications enhance the five major components <b>of</b> a <b>search</b> engine, namely, query understanding, document understanding, document ranking, user understanding, and monitoring & feedbacks. For each aspect, we survey the major tasks, fundamental principles, and state-of-the-art methods...|$|R
40|$|Query {{recommendation}} {{is becoming a}} common feature <b>of</b> web <b>search</b> engines especially those for Intranets where the context is more restrictive. This is because of its utility for supporting users to find relevant information in less time by using the most suitable query terms. Selection of queries for {{recommendation is}} typically done by mining web documents or <b>search</b> <b>logs</b> <b>of</b> previous users. We propose the integration of these approaches by combining two models namely the concept hierarchy, typically built from an Intranet’s documents, and the query flow graph, typically built from <b>search</b> <b>logs.</b> However, we build our concept hierarchy model from terms extracted from a subset (training set) <b>of</b> <b>search</b> <b>logs</b> since these are more representative of the user view of the domain than any concepts extracted from the collection. W...|$|R
40|$|We studied how an enriched {{public library}} {{catalogue}} {{is used to}} access novels. 58 users searched for interesting novels to read in a simulated situation where they had only a vague idea of what {{they would like to}} read. Data consist <b>of</b> <b>search</b> <b>logs,</b> pre and post search questionnaires and observations. Results show, that investing effort on examining results improves search success, i. e. finding interesting novels, whereas effort in querying has no bearing on it. In designing systems for fiction retrieval, enriching result presentation with detailed book information would benefit users...|$|R
40|$|This paper {{presents}} the duality hypothesis <b>of</b> <b>search</b> and tagging, two important behaviors of web users. The hypothesis states {{that if a}} user views a document D in the search results for query Q, the user would tend to assign document $D$ a tag identical to or similar to Q; similarly, if a user tags a document D with a tag T, the user would tend to view document D {{if it is in}} the search results obtained using T as a query. We formalize this hypothesis with a unified probabilistic model for search and tagging, and show that empirical results of several tasks on <b>search</b> <b>log</b> and tag data sets, including ad hoc search, query suggestion, and query trend analysis, all support this duality hypothesis. Since the availability <b>of</b> <b>search</b> <b>log</b> is limited due to the privacy concern, our study opens up a highly promising direction of using tag data to approximate or supplement <b>search</b> <b>log</b> data for studying user behavior and improving search engine accuracy...|$|R
40|$|Abstract- Search engine {{companies}} {{collect the}} “database of intents”, the stories <b>of</b> their exploiters <b>search</b> queries. To publish <b>search</b> <b>logs</b> with seclusion. These <b>search</b> <b>logs</b> supplies commodious cognition to analyzers and researchers. Search engine fellowships, however, are suspicious <b>of</b> bringing out <b>search</b> <b>logs</b> {{in order not}} to divulge sensible information. In this paper we examine algorithms for releasing frequent keywords, queries and clicks <b>of</b> a <b>search</b> <b>log.</b> We foremost show how methods that accomplish variants of k-anonymity are insecure to active attacks. We then show that the harder guarantee ensured by differential secrecy regrettably does not provide any usefulness for this problem. Our paper resolves with a large observational field of study using real diligences where we equivalence ZEALOUS and previous work that attains k-anonymity in <b>search</b> <b>log</b> publishing. Our results show that ZEALOUS generates corresponding utility to k−anonymity {{while at the same time}} attaining practically stronger seclusion guarantees...|$|R
40|$|Web <b>search</b> <b>logs</b> contain {{extremely}} sensitive data, {{as evidenced by}} the recent AOL incident. However, storing and analyzing <b>search</b> <b>logs</b> can be very useful for many purposes (i. e. investigating human behavior). Thus, an important research question is how to privately sanitize <b>search</b> <b>logs.</b> Several <b>search</b> <b>log</b> anonymization techniques have been proposed with concrete privacy models. However, in all of these solutions, the output utility of the techniques is only evaluated rather than being maximized in any fashion. Indeed, for effective <b>search</b> <b>log</b> anonymization, it is desirable to derive the optimal (maximum utility) output while meeting the privacy standard. In this paper, we propose utility-maximizing sanitization based on the rigorous privacy standard of differential privacy, in the context <b>of</b> <b>search</b> <b>logs.</b> Specifically, we utilize optimization models to maximize the output utility of the sanitization for different applications, while ensuring that the production process satisfies differential privacy. An added benefit is that our novel randomization strategy ensures that the schema of the output is identical to that of the input. A comprehensive evaluation on real <b>search</b> <b>logs</b> validates the approach and demonstrates its robustness and scalability. Comment: 12 page...|$|R
40|$|Access to {{knowledge}} about common human goals {{has been found}} critical for realizing the vision of intelligent agents acting upon user intent on the web. Yet, the acquisition of knowledge about common human goals represents a major challenge. In a departure from existing approaches, this paper investigates a novel resource for knowledge acquisition: The utilization <b>of</b> <b>search</b> query <b>logs</b> for this task. By relating goals contained in <b>search</b> query <b>logs</b> with goals contained in existing commonsense knowledge bases such as ConceptNet, we aim {{to shed light on}} the usefulness <b>of</b> <b>search</b> query <b>logs</b> for capturing knowledge about common human goals. The main contribution of this paper consists of insights generated from an empirical study comparing common human goals contained in two large <b>search</b> query <b>logs</b> (AOL and Microsoft Research) with goals contained in the commonsense knowledge base ConceptNet. The paper sketches ways how goals from <b>search</b> query <b>logs</b> could be used to address the goal acquisition and goal coverage problem related to commonsense knowledge bases...|$|R
40|$|Search engine {{companies}} {{collect the}} "database of intentions", the histories <b>of</b> their users' <b>search</b> queries. These <b>search</b> <b>logs</b> are {{a gold mine}} for researchers. Search engine companies, however, are wary <b>of</b> publishing <b>search</b> <b>logs</b> {{in order not to}} disclose sensitive information. In this paper we analyze algorithms for publishing frequent keywords, queries and clicks <b>of</b> a <b>search</b> <b>log.</b> We first show how methods that achieve variants of k-anonymity are vulnerable to active attacks. We then demonstrate that the stronger guarantee ensured by ϵ-differential privacy unfortunately does not provide any utility for this problem. We then propose an algorithm ZEALOUS and show how to set its parameters to achieve (ϵ,δ) -probabilistic privacy. We also contrast our analysis of ZEALOUS with an analysis by Korolova et al. [17] that achieves (ϵ',δ') -indistinguishability. Our paper concludes with a large experimental study using real applications where we compare ZEALOUS and previous work that achieves k-anonymity in <b>search</b> <b>log</b> publishing. Our results show that ZEALOUS yields comparable utility to k-anonymity {{while at the same time}} achieving much stronger privacy guarantees...|$|R
40|$|Query {{recommendation}} {{is becoming a}} common feature <b>of</b> web <b>search</b> engines especially those for Intranets where the context is more restrictive. This is because of its utility for supporting users to find relevant information in less time by using the most suitable query terms. Selection of queries for {{recommendation is}} typically done by mining web documents or <b>search</b> <b>logs</b> <b>of</b> previous users. We propose the integration of these approaches by combining two models namely the concept hierarchy, typically built from an Intranet’s docu- ments, and the query flow graph, typically built from <b>search</b> <b>logs.</b> However, we build our concept hierarchy model from terms extracted from a subset (training set) <b>of</b> <b>search</b> <b>logs</b> since these are more representative of the user view of the domain than any concepts extracted from the collection. We then continually adapt the model by incorporating query re- finements from another subset (test set) <b>of</b> the user <b>search</b> <b>logs.</b> This process implies learning from or reusing previ- ous users’ querying experience to recommend queries for a new but similar user query. The adaptation weights are ex- tracted from a query flow graph built with the same logs. We evaluated our hybrid model using documents crawled from the Intranet of an academic institution and its <b>search</b> <b>logs.</b> The hybrid model was then compared to a concept hi- erarchy model and query flow graph built from the same col- lection and <b>search</b> <b>logs</b> respectively. We also tested various strategies for combining information in the <b>search</b> <b>logs</b> {{with respect to the}} frequency of clicked documents after query refinement. Our hybrid model significantly outperformed the concept hierarchy model and query flow graph when tested over two different periods of the academic year. We intend to further validate our experiments with documents and <b>search</b> <b>logs</b> from another institution and devise better strategies for selecting queries for recommendation from the hybrid model...|$|R
40|$|Abstract. To {{preserve}} <b>search</b> <b>log</b> data utility, Google groups <b>search</b> queries in <b>log</b> bundles by deleting {{the last}} octet of logged IP address. Because these bundles still contain identifying information, part <b>of</b> these <b>search</b> <b>logs</b> can be de-anonymized [27]. Without an external audit <b>of</b> these <b>search</b> <b>logs,</b> {{it is currently}} impossible to evaluate their robustness against de-anonymizing attacks. In this paper, we leverage log retention policy ambiguities to show that quasi-identifiers could be stored in sanitized <b>search</b> query <b>logs</b> and could help to de-anonymize user searches. This paper refers to Google Search and Google Suggest log retention policies and shows that even with the highest degree of anonymization that Google offers, one could separate user queries with a high granularity. Because Google Suggest is queried every time a user types a character in the Google Chrome navigation box, the privacy of Chrome users could be compromised {{with respect to their}} browsing histories. Such ambiguities within log retention policies are critical and should be addressed, as anonymized logs could be shared with third parties without prior user consent. ...|$|R
40|$|Automatic image {{annotation}} using {{supervised learning}} is performed by concept classifiers trained on labelled example images. This work proposes {{the use of}} clickthrough data collected from <b>search</b> <b>logs</b> {{as a source for}} the automatic generation of concept training data, thus avoiding the expensive manual annotation effort. We investigate and evaluate this approach using a collection of 97, 628 photographic images. The results indicate that the contribution <b>of</b> <b>search</b> <b>log</b> based training data is positive despite their inherent noise; in particular, the combination of manual and automatically generated training data outperforms the use of manual data alone. It is therefore possible to use clickthrough data to perform large-scale image annotation with little manual annotation effort or, depending on performance, using only the automatically generated training data. An extensive presentation of the experimental results and the accompanying data can be accessed at [URL] SystemsElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Query {{recommendation}} {{is becoming a}} common feature <b>of</b> web <b>search</b> engines especially those for Intranets where the context is more restrictive. This is because of its utility for supporting users to find relevant information in less time by using the most suitable query terms. Selection of queries for {{recommendation is}} typically done by mining web documents or <b>search</b> <b>logs</b> <b>of</b> previous users. We propose the integration of these approaches by combining two models namely the concept hierarchy, typically built from an Intranet's documents, and the query flow graph, typically built from <b>search</b> <b>logs.</b> However, we build our concept hierarchy model from terms extracted from a subset (training set) <b>of</b> <b>search</b> <b>logs</b> since these are more representative of the user view of the domain than any concepts extracted from the collection. We then continually adapt the model by incorporating query refinements from another subset (test set) <b>of</b> the user <b>search</b> <b>logs.</b> This process implies learning from or reusing previous users' querying experience to recommend queries for a new but similar user query. The adaptation weights are extracted from a query flow graph built with the same logs. We evaluated our hybrid model using documents crawled from the Intranet of an academic institution and its <b>search</b> <b>logs.</b> The hybrid model was then compared to a concept hierarchy model and query flow graph built from the same collection and <b>search</b> <b>logs</b> respectively. We also tested various strategies for combining information in the <b>search</b> <b>logs</b> {{with respect to the}} frequency of clicked documents after query refinement. Our hybrid model significantly outperformed the concept hierarchy model and query flow graph when tested over two different periods of the academic year. We intend to further validate our experiments with documents and <b>search</b> <b>logs</b> from another institution and devise better strategies for selecting queries for recommendation from the hybrid model. Copyright is held by the International World Wide Web Conference Committee (IW 3 C 2) ...|$|R
40|$|While current {{search engines}} serve known-item search such as {{homepage}} finding very well, they generally cannot support exploratory search effectively. In exploratory search, users {{do not know}} their information needs precisely and also often lack the needed knowledge to formulate effective queries, thus querying alone, as supported by the current search engines, is insufficient, and browsing into related information would be very useful. In this paper, we present a formal navigation-based retrieval framework to unify querying and browsing and treat both as navigation over topic regions. To support browsing effectively, we treat <b>search</b> <b>logs</b> as "footprints" left by previous users in the information space and build a multi-resolution topic map to guide a user in navigating in the information space. To test {{the effectiveness of the}} proposed methods, we build a prototype system based on a small sample <b>of</b> <b>search</b> <b>logs</b> and a commercial search engine. Our experiment results show that the proposed navigation-based framework is promising and the proposed methods for guided navigation are effective...|$|R
40|$|Today {{web portals}} play an {{increasingly}} important role in health care allowing in-formation seekers to learn about diseases and treatments, and to administrate their care. Therefore, {{it is important that}} the portals are able to support this process as well as possible. In this paper, we study the <b>search</b> <b>logs</b> <b>of</b> a public Swedish health portal to address the questions if health information seeking differs from other types <b>of</b> Internet <b>search</b> and if there is a potential for utilizing network analy-sis methods in combination with semantic annotation to gain insights into search be-haviors. Using a semantic-based method and a graph-based analysis of word co-occurrences in queries, we show there is an overlap among the results indicating a potential role of these types of methods to gain insights and facilitate improved infor-mation search. In addition we show that samples, windows of a month, <b>of</b> <b>search</b> <b>logs</b> may be sufficient to obtain similar re-sults as using larger windows. We also show that medical queries share the same structural properties found for other types <b>of</b> information <b>searches,</b> thereby indicat-ing an ability to re-use existing analysis methods for this type <b>of</b> <b>search</b> data. ...|$|R
40|$|Query {{segmentation}} is {{the task}} of splitting a query into a sequence of non-overlapping segments that completely cover all tokens in the query. The majority of query segmentation methods are unsupervised. In this paper, we propose an error-driven approach to query segmentation (EDQS) with the help <b>of</b> <b>search</b> <b>logs,</b> which enables unsupervised training with guidance from the system-specific errors. In EDQS, we first detect the system’s errors by examining the consistency among the segmentations of similar queries. Then, a model is trained by the detected errors to select the correct segmentation of a new query from the top-n outputs of the system. Our evaluation results show that EDQS can significantly boost the performance of state-of-the-art query segmentation methods on a publicly available data set...|$|R
40|$|The paper {{reports a}} novel {{approach}} to studying user-system interaction that captures a complete record of the searcher's actions, the system responses and synchronised talk-aloud comments from the searcher. The data is recorded unobtrusively and is available for later analysis. The approach is set in context by a discussion of transaction logging and protocol analysis and examples <b>of</b> the <b>search</b> <b>logging</b> in operation are presente...|$|R
50|$|The AOL search data leak was the release, in August 2006, <b>of</b> {{detailed}} <b>search</b> <b>logs</b> by AOL <b>of</b> a {{large number}} of AOL users. The release was intentional and intended for research purposes; however, the public release meant that the entire Internet could see the results rather than a select number of academics. AOL did not redact any information, which caused privacy concerns since users could potentially be identified from their searches.|$|R
40|$|This article {{describes}} the use <b>of</b> discovery system <b>search</b> <b>logs</b> {{as a vehicle for}} encouraging constructive conversations across departments in an academic library. The project focused on bringing together systems and teaching librarians to evaluate the results <b>of</b> anonymized patron <b>searches</b> in order to improve communication across departments, as well as to identify opportunities for improvement to the discovery system itself...|$|R
40|$|With {{the advent}} and rapid spread of microblogging services, web {{information}} management finds a new research topic. Although classical information retrieval methods and techniques help search engines {{and services to}} present an adequate precision in lower recall levels (top-k results), the constantly evolving information needs of microblogging users demand a different approach, which has to be adapted to the dynamic nature of On-line Social Networks (OSNs). In this work, we use Twitter as microblogging service, aiming to investigate the query expansion provision that can be extracted from large graphs, and compare it against classical query expansion methods that require mainly prior knowledge, such as browsing history records or access and management <b>of</b> <b>search</b> <b>logs.</b> We provide a direct comparison with mainstream media services, such as Google, Yahoo!, Bing, NBC and Reuters, while we also evaluate our approach by subjective comparisons in respect to the Google Hot Searches service. © Springer-Verlag Berlin Heidelberg 2013...|$|R
40|$|The {{efficacy}} of library discovery tools {{can be a}} contentious topic within libraries. This presentation will describe the use <b>of</b> Primo <b>search</b> <b>logs</b> {{as a vehicle to}} encourage constructive conversations across departments in two different university research libraries (University of Tennessee and Virginia Commonwealth University). In particular this project aimed to bring together systems and public service librarians in order to improve communication across departments and the library discovery system itself. Results from each institution will be presented and compared along with ideas for future research...|$|R
40|$|Structured data such as databases, {{spreadsheets}} and web tables {{is becoming}} critical in every domain and professional role. Yet {{we still do}} not know much about how people interact with it. Our research focuses on the information seeking behaviour of people looking for new sources of structured data online, including the task context in which the data will be used, data search, and the identification of relevant datasets from a set of possible candidates. We present a mixed-methods study covering in-depth interviews with 20 participants with various professional backgrounds, supported by the analysis <b>of</b> <b>search</b> <b>logs</b> <b>of</b> a large data portal. Based on this study, we propose a framework for human structured-data interaction and discuss challenges people encounter when trying to find and assess data that helps their daily work. We provide design recommendations for data publishers and developers of online data platforms such as data catalogs and marketplaces. These recommendations highlight important questions for HCI research to improve how people engage and make use of this incredibly useful online resource...|$|R
40|$|Knowledge about user {{goals is}} crucial for realizing the vision of {{intelligent}} agents acting upon user intent on the web. In a departure from existing approaches, this paper proposes a novel approach {{to the problem of}} user goal acquisition: The utilization <b>of</b> <b>search</b> query <b>logs</b> for this task. The paper makes the following contributions: (a) it presents an automatic method for the acquisition of user goals from <b>search</b> query <b>logs</b> with useful precision/recall scores (b) it provides insights into the nature and some characteristics of these goals and (c) it shows that the goals acquired from query logs exhibit traits of a long tail distribution. 1...|$|R
50|$|The idea to {{purchase}} the MP3.com domain arose when Flores was monitoring search traffic on filez.com, a FTP search site whose first incarnation provided an easy to use graphical interface for searching for various types of files including software, graphics, video and audio. The first version of files utilized an existing free search engine developed by graduate students (led by Tor Egge, who later founded Fast Search and Transfer based on this search engine) at the Norwegian University of Science and Technology. Flores noticed in his review <b>of</b> the <b>search</b> <b>logs</b> that people were searching for 'mp3'.|$|R
40|$|The use <b>of</b> non-English Web <b>search</b> engines {{has been}} prevalent. Given the {{popularity}} <b>of</b> Chinese Web <b>searching</b> and the unique characteristics of Chinese language, {{it is imperative}} to conduct studies with focuses on the analysis <b>of</b> Chinese Web <b>search</b> queries. In this paper, we report our research on the character usage <b>of</b> Chinese <b>search</b> <b>logs</b> from Web <b>search</b> engine in Hong Kong. By examining the distribution <b>of</b> <b>search</b> queries terms, we found that people intended to use more diversified terms and that the usage <b>of</b> characters in <b>search</b> queries was quite different from the character usage of both general online information in Chinese and Web search queries in English. We believe the findings from this study have provided some insights into further research in non-English Web searching and will assist in the design of more effective Chinese Website search engines. link_to_subscribed_fulltex...|$|R
40|$|Abstract. This work {{describes}} {{a variation on}} the traditional Information Retrieval paradigm, where instead of text documents being indexed according to their content, they are indexed according to the search terms previous users have used in finding them. We determine the effectiveness of this approach by indexing a sample of query logs from the European Library, and describe its usefulness for multilingual searching. In our analysis <b>of</b> the <b>search</b> <b>logs,</b> we determine the language of the past queries automatically, and annotate the <b>search</b> <b>logs</b> accordingly. From this information, we derive matrices to show that a) users tend to persist with the same query language throughout a query session, and b) submit queries in the same language as the interface they have selected, except in a large number of cases where the English interface is used to submit Latin queries...|$|R
40|$|International audienceAccounting for {{the large}} number of queries sent by users to search engines on a daily basis, the latter are likely to learn and {{possibly}} leak sensitive information about individual users. To deal with this issue, several solutions have been proposed to query search engines in a privacy preserving way. A first category of solutions aim to hide users’ identities, thus enforcing unlinkability between a query and the identity of its originating user. A second category of approaches aims to obfuscate the content of users’ queries, or at generating fake queries in order to blur user profiles, thus enforcing indistinguishability between them. In this paper we propose PEAS, a new protocol for private Web search. PEAS combines a new efficient unlinkability protocol with a new accurate indistinguishability protocol. Experiments conducted using a real dataset <b>of</b> <b>search</b> <b>logs</b> show that compared to state-of-the-art approaches, PEAS decreases by up to 81. 9 % the number of queries linked to their original requesters. Furthermore, PEAS is accurate as it allows users to retrieve up to 95. 3 % of the results they would obtain using search engines in an unprotected way...|$|R
40|$|Like {{most other}} websites, {{the site of}} the University of Groningen has an {{embedded}} search function. The perceived quality <b>of</b> this <b>search</b> function is not great, but users do not have a clear idea why. Analysis <b>of</b> the <b>search</b> <b>logs</b> reveals that almost half the search actions does not directly lead to satisfying results. In this thesis, we will try to pinpoint the specific weak points in the current search engine. Also, we will explore different ways to improve search engines, and try to determine the most efficient ones. One of the more interesting possibilities seems to be the use of clustering algorithms to group the results. To explore the feasibility of clustering, some experiments have been done, which lead to not wholly satisfactory results. ...|$|R
40|$|This {{paper is}} {{concerned}} with 'intranet search'. By intranet search, we mean searching for information on an intranet within an organization. We have found that search needs on an intranet can be categorized into types, through an analysis of survey results and an analysis <b>of</b> <b>search</b> <b>log</b> data. The types include searching for definitions, persons, experts, and homepages. Traditional information retrieval only focuses on <b>search</b> <b>of</b> relevant documents, but not on <b>search</b> <b>of</b> special types of information. We propose {{a new approach to}} intranet search in which we search for information in each of the special types, in addition to the traditional relevance search. Information extraction technologies can play key roles in such kind of 'search by type' approach, because we must first extract from the documents the necessary information in each type. We have developed an intranet search system called 'Information Desk'. In the system, we try to address the most important types <b>of</b> <b>search</b> first - finding term definitions, homepages of groups or topics, employees' personal information and experts on topics. For each type <b>of</b> <b>search,</b> we use information extraction technologies to extract, fuse, and summarize information in advance. The system is in operation on the intranet of Microsoft and receives accesses from about 500 employees per month. Feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information. This paper describes the architecture, features, component technologies, and evaluation results of the system. Copyright 2005 ACM...|$|R
40|$|In recent years, the {{importance}} of log analysis has grown, log data constitute a relevant aspect in the evaluation process {{of the quality of}} a digital library system. In this paper, we ad-dress the problem of log analysis for complex systems such as digital library systems, and how the analysis <b>of</b> <b>search</b> query <b>logs</b> or Web logs is not sufficient to study users and interpret their preferences. In fact the combination of im-plicitly and explicitly collected data improves understanding of behavior with respect to the understanding that can be gained by analyzing the sets of data separately. Categories and Subject Descriptor...|$|R
