2134|1120|Public
25|$|The {{remaining}} {{position and}} time <b>error</b> <b>variance</b> terms follow in a straightforward manner.|$|E
2500|$|The sum of squares due to pure error, {{divided by}} the <b>error</b> <b>variance</b> σ2, has a chi-squared {{distribution}} with N'n degrees of freedom; ...|$|E
2500|$|It can {{be shown}} to follow that if the straight-line model is correct, then the sum of squares due to error divided by the <b>error</b> <b>variance,</b> ...|$|E
40|$|Abstract. Computing {{the linear}} least-squares {{estimate}} ofa high-dimensional random quantity given noisy data requires solving a large system oflinear equations. In many situations, one can solve this system efficiently using a Krylov subspace method, {{such as the}} conjugate gradient (CG) algorithm. Computing the estimation <b>error</b> <b>variances</b> is a more intricate task. It is difficult because the <b>error</b> <b>variances</b> are the diagonal elements ofa matrix expression involving the inverse ofa given matrix. This paper presents a method for using the conjugate search directions generated by the CG algorithm to obtain a convergent approximation to the estimation <b>error</b> <b>variances.</b> The algorithm for computing the <b>error</b> <b>variances</b> falls out naturally from a new estimation-theoretic interpretation ofthe CG algorithm. This paper discusses this interpretation and convergence issues and presents numerical examples. The examples include a 10 5 -dimensional estimation problem from oceanography. Key words. Krylov subspaces, linear least-squares estimation, <b>error</b> <b>variances,</b> conjugate gradien...|$|R
40|$|A {{periodically}} integrated (PI) {{time series}} process {{assumes that the}} stochastic trend can be removed using a seasonally varying differencing filter. In this paper the multi-step forecast <b>error</b> <b>variances</b> are derived for a quarterly PI time series when low-order periodic autoregressions adequately describe the data. The forecast <b>error</b> <b>variances</b> display seasonal variation, indicating that observations in some seasons can be forecast more precise than those in others. Two examples illustrate the empirical relevance of calculating forecast <b>error</b> <b>variances.</b> A by-product of the analysis is an expression for the seasonally varying impact of the stochastic trend...|$|R
5000|$|... #Subtitle level 3: Assumption 2: {{homogeneity}} of <b>error</b> <b>variances</b> ...|$|R
2500|$|The sum of squares {{due to lack}} of fit, {{divided by}} the <b>error</b> <b>variance</b> σ2, has a chi-squared {{distribution}} with n'p degrees of freedom (here p=2 as there are two parameters in the straight-line model); ...|$|E
2500|$|... where dft is {{the degrees}} of freedom n– 1 of the {{estimate}} of the population variance of the dependent variable, and dfe is the {{degrees of freedom}} n – p – 1 of the estimate of the underlying population <b>error</b> <b>variance.</b>|$|E
2500|$|... {{where the}} true <b>error</b> <b>variance</b> σ2 is {{replaced}} by an estimate based on the minimised value of the sum of squares objective function S. The denominator, n−m, is the statistical degrees of freedom; see effective degrees of freedom for generalizations.|$|E
40|$|AbstractEstimation of {{parameters}} in linear {{fixed and}} mixed effects models, under order {{restrictions on the}} <b>error</b> <b>variances,</b> is considered in this article. For simplicity of exposition, we shall assume that the <b>error</b> <b>variances</b> are subject to simple order restriction. Similar methodology can be developed for other forms of order restrictions as well...|$|R
40|$|The {{coefficient}} matrix {{for multiple}} trait (milk, fat, and protein) mixed model equations {{may be too}} large to obtain prediction <b>error</b> <b>variances</b> from inverse elements. The commonly used reciprocals of diagonal elements may not be accurate approximations when sire relationships or multiple traits are included since much information is contained in off-diagonal elements. Approximations incorporating increased information from coefficient matrix were compared with actual prediction <b>error</b> <b>variances</b> for multiple trait evaluations for milk, fat, protein, and dollar value (relationships included) of 229 Ayrshire and 248 Brown Swiss bulls. Six approximations were selection index using number of daughter records, inverses of individual sire diagonal blocks, inverses of group and individual sire blocks, and inverses of all diagonal blocks and off-diagonal blocks associated with individual sires. All approximations underestimated actual prediction <b>error</b> <b>variances,</b> but most, except selection index, were highly correlated (. 90 to. 99) with actual prediction <b>error</b> <b>variances</b> of sire evaluations for milk yield and product value for contemporary bulls. The approximation incorporating most information from the coefficient matrix is recommended for use on basis of high correlation with and closeness to actual prediction <b>error</b> <b>variances...</b>|$|R
40|$|Computing {{the linear}} least-squares {{estimate}} of a high-dimensional random quantity given noisy data requires solving a large system of linear equations. In many situations, one can solve this system efficiently using a Krylov subspace method, {{such as the}} conjugate gradient (CG) algorithm. Computing the estimation <b>error</b> <b>variances</b> is a more intricate task. It is difficult because the <b>error</b> <b>variances</b> are the diagonal elements of a matrix expression involving the inverse of a given matrix. This paper presents a method for using the conjugate search directions generated by the CG algorithm to obtain a convergent approximation to the estimation <b>error</b> <b>variances.</b> The algorithm for computing the <b>error</b> <b>variances</b> falls out naturally from a new estimation-theoretic interpretation of the CG algorithm. This paper discusses this interpretation and convergence issues and presents numerical examples. The examples include a 10 5 -dimensional estimation problem from oceanography. Key words. Krylov sub [...] ...|$|R
2500|$|Given {{the success}} of ROC curves for the {{assessment}} of classification models, the extension of ROC curves for other supervised tasks has also been investigated. Notable proposals for regression problems are the so-called regression error characteristic (REC) Curves [...] and the Regression ROC (RROC) curves. In the latter, RROC curves become extremely similar to ROC curves for classification, with the notions of asymmetry, dominance and convex hull. Also, the area under RROC curves is proportional to the <b>error</b> <b>variance</b> of the regression model.|$|E
2500|$|Although serial {{correlation}} {{does not affect}} the consistency of [...] the estimated regression coefficients, it does affect our ability to conduct valid statistical tests. First, the F-statistic to test for overall significance of the regression may be inflated under positive {{serial correlation}} because the mean squared error (MSE) will tend to underestimate the population <b>error</b> <b>variance.</b> Second, positive serial correlation typically causes the ordinary least squares (OLS) standard errors for the regression coefficients to underestimate the true standard errors. As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small standard errors for the regression coefficient. These small standard errors will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the standard errors were correctly estimated.|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous <b>error</b> <b>variance,</b> or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
40|$|Abstract: We {{introduce}} herein a {{new class}} of autoregressive models in which the regression parameters and <b>error</b> <b>variances</b> may undergo changes at unknown time points while staying constant between adjacent change-points. Assuming conjugate priors, we derive closed-form recursive Bayes estimates of the regression parame-ters and <b>error</b> <b>variances.</b> Approximations to the Bayes estimates are developed that have much lower computational complexity and yet are comparable to the Bayes estimates in statistical eciency. We also address the problem of unknown hyper-parameters and propose two practical methods for simultaneous estimation of the hyperparameters, regression parameters and <b>error</b> <b>variances.</b> Key words and phrases: Bayesian inference, bounded complexity mixtures, change...|$|R
3000|$|... {{assuming}} M available modulation schemes. Note {{that due}} to the user-specific estimation <b>error</b> <b>variances</b> [...]...|$|R
40|$|Estimation of {{parameters}} in linear {{fixed and}} mixed effects models, under order {{restrictions on the}} <b>error</b> <b>variances,</b> is considered in this article. For simplicity of exposition, we shall assume that the <b>error</b> <b>variances</b> are subject to simple order restriction. Similar methodology can be developed for other forms of order restrictions as well. fixed effects heteroscedastic errors isotonic regression maximum likelihood estimation mixed effects simple order restriction...|$|R
5000|$|... where [...] is the <b>error</b> <b>variance.</b> The <b>error</b> <b>variance</b> in {{this case}} is defined as ...|$|E
5000|$|As a rule {{of thumb}} when the {{variance}} of the measurement error is known a priori, a [...] indicates a poor model fit. A [...] indicates that the fit has not fully captured the data (or that the <b>error</b> <b>variance</b> has been underestimated). In principle, a value of [...] indicates that the extent of the match between observations and estimates is in accord with the <b>error</b> <b>variance.</b> A [...] indicates that the model is 'over-fitting' the data: either the model is improperly fitting noise, or the <b>error</b> <b>variance</b> has been overestimated.|$|E
5000|$|... (2) Homogeneity of variance: each {{population}} {{should have}} the same <b>error</b> <b>variance.</b>|$|E
5000|$|It gives {{a common}} upper bound of actual {{estimation}} <b>error</b> <b>variances,</b> which has robustness {{with respect to}} unknown correlations.|$|R
40|$|It is {{well known}} that {{classical}} analysis of variance (ANOVA) is not suitable for heteroscedastic layouts. Weighted analysis of variance (WANOVA) {{is the only way to}} deal with such situations. Problems of usual WANOVA in Randomized. Block Design (RBD) with more than one observations per cell with interaction when <b>error</b> <b>variances</b> vary from cell to cell are discussed in this paper. Key Words: Weighted analysis of <b>variance,</b> <b>error</b> <b>variances,</b> cell to cell...|$|R
3000|$|... ∗ and c. Our {{dependent}} variables are ordinal and for identification reasons their <b>variances</b> and <b>error</b> <b>variances</b> {{have been set}} to one.|$|R
50|$|The {{remaining}} {{position and}} time <b>error</b> <b>variance</b> terms follow in a straightforward manner.|$|E
5000|$|The {{amount of}} {{forecast}} <b>error</b> <b>variance</b> of variable [...] {{accounted for by}} exogenous shocks to variable [...] is given by ...|$|E
50|$|Properties of the VAR {{model are}} usually {{summarized}} using structural analysis using Granger causality, impulse responses, and forecast <b>error</b> <b>variance</b> decompositions.|$|E
40|$|This study {{investigated}} the effects of correlated errors on the person x occasion design in which the confounding effect of equal time intervals results in correlated error terms in the linear model. Two specific error correlation structures were examined: the first-order stationary autoregressive (SARI), and the first-order nonstationary autoregressive (NARI) with increasing variance parameters. The effects of correlated errors on the existing generalizability and dependability coefficients were assessed by simulating data with known variances (six different combinations of person, occasion, and <b>error</b> <b>variances),</b> occasion sizes, person sizes, correlation parameters, and increasing variance parameters. Estimates derived from the simulated data were compared to their true values. The traditional estimates were acceptable when the error terms were not correlated and the <b>error</b> <b>variances</b> were equal. The coefficients were underestimated when the errors were uncorrelated with increasing <b>error</b> <b>variances.</b> However, when the errors were correlated with equal vanances the traditional formulas overestimated both coefficients. When the errors were correlated with increasing variances, the traditional formulas both overestimated and underestimated the coefficients. Finally, {{increasing the number of}} occasions sampled resulted in more improved generalizability coefficient estimates than dependability coefficient estimates. Index terms: changing <b>error</b> <b>variances,</b> computer simulation, correlated errors, dependability coefficients, generalizability coefficients...|$|R
30|$|France {{is judged}} to be an {{independent}} stock market, as fluctuations in the French market fail to explain any substantial part of the <b>error</b> <b>variances</b> of other markets; and it also provides less scope for other markets to affect its own <b>error</b> <b>variances.</b> On the other hand, there is a horde of stock markets that can be termed as dynamically interlinked in terms of information spillovers. Such stock markets include Denmark, Italy, Ireland, Luxembourg, the Netherlands, Norway, Portugal, Spain, Sweden, and Switzerland.|$|R
40|$|For {{choosing}} speci"c cross-ratios as 2 D projective coordinates {{in various}} computer vision applications, a reasonable error analysis model is usually required. This investigation adopts {{the assumption of}} normal distribution for positioning errors of point features in an image to formulate the <b>error</b> <b>variances</b> of cross-ratios. Based on a geometry-based error analysis, a straightforward way of identifying the cross-ratios with minimum <b>error</b> <b>variances</b> is proposed. Simulation {{results show that the}} proposed approach, as well as a further simpli"ed alternative, yield much better estimations of minimum <b>error</b> <b>variances</b> in terms of accuracy, cost, and stability compared with some other methods, e. g., the one based on the rule given by Georis et al. (IEEE Trans. Pattern Anal. Mach. Intell. 20 (4) (1998) 366). Some causes of the performance di!erences in the estimations are explained using a special con"guration of point features. 2001 Patter...|$|R
50|$|In {{econometrics}} {{and other}} applications of {{multivariate time series}} analysis, a variance decomposition or forecast <b>error</b> <b>variance</b> decomposition (FEVD) is used {{to aid in the}} interpretation of a vector autoregression (VAR) model once it has been fitted. The variance decomposition indicates the amount of information each variable contributes to the other variables in the autoregression. It determines how much of the forecast <b>error</b> <b>variance</b> of each of the variables can be explained by exogenous shocks to the other variables.|$|E
50|$|If an autoregressive {{moving average}} model (ARMA model) is assumed for the <b>error</b> <b>variance,</b> the model is a {{generalized}} autoregressive conditional heteroscedasticity(GARCH) model.|$|E
5000|$|The sum of squares due to pure error, {{divided by}} the <b>error</b> <b>variance</b> σ2, has a chi-squared {{distribution}} with N &minus; n degrees of freedom; ...|$|E
40|$|Measurement error {{modeling}} {{is crucial}} to any assay method. Realistic error models prioritize efforts to reduce key error components and provide a way to estimate total ("random" and "systematic") measurement <b>error</b> <b>variances.</b> This paper uses multi-laboratory data to estimate random error and systematic <b>error</b> <b>variances</b> for seven analytical chemistry destructive assay methods for five analytes (Gallium, Iron, Silicon, Plutonium, and Uranium). Because these variance estimates are based on multiple-component error models, strategies are described for choosing and then fitting error models that allow for lab-to-lab variation. </p...|$|R
40|$|It is {{well known}} that {{consistent}} estimators of errors-in-variables models require knowledge of the ratio of <b>error</b> <b>variances.</b> What is not well known is that a Joint Least Squares estimator is robust to a wide misspecification of that ratio. Through a series of Monte Carlo experiments we show that an easy-to-implement estimator produces estimates that are nearly unbiased {{for a wide range of}} the ratio of <b>error</b> <b>variances.</b> These MC analyses encompass linear and nonlinear specifications and also a system on nonlinear equations where all the variables are measured with errors. Research Methods/ Statistical Methods,...|$|R
40|$|Fekri and Ruiz-Gazen [2004. Robust {{weighted}} {{orthogonal regression}} in the errors-in-variables model. J. Multivar. Anal. 88, 89 - 108] propose {{a new class}} of robust estimators in the errors-in-variables linear model (EIV model) under the assumption that the measurement <b>error</b> <b>variances</b> are equal. In the present paper, we extend this new class of estimators to other usual <b>error</b> <b>variances</b> assumptions for the simple EIV model and calculate their influence functions and their asymptotic distributions in the elliptical case. Errors-in-variables model Orthogonal regression B-robustness Influence function M-estimators S-estimators MCD estimator...|$|R
