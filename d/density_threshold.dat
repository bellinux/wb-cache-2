468|732|Public
5000|$|... #Caption: Below {{a certain}} <b>density</b> <b>threshold,</b> flying foxes {{are no longer}} {{effective}} seed dispersers.|$|E
5000|$|... 1) The {{position}} of the phase transition at which a giant cluster appears moves to higher values of [...] as the value of [...] decreases. That is, the more assortative a network is, the lower the edge <b>density</b> <b>threshold</b> for the giant cluster's appearance will be.|$|E
50|$|Maximum Density Threshold: There can be {{a maximum}} number of {{vehicles}} that can be located in a certain area. Maximum <b>density</b> <b>threshold</b> considers {{the size of a}} certain area and dimensions of the vehicles that are currently claiming to reside there. If the number of vehicles in that area is larger than this threshold, all the messages from there will be ignored by every vehicle since it is a strong indication that there are active Sybil nodes in that area.|$|E
3000|$|... {{are the two}} <b>density</b> <b>thresholds</b> that {{regulate}} the activation of overlapping. Please note that a zero value for the κ [...]...|$|R
3000|$|Once {{a spatial}} cluster is {{identified}} {{by means of}} <b>density</b> <b>thresholding,</b> the known observables are the number of molecules n [...]...|$|R
30|$|Optimal {{adaptive}} thresholds for auto-segmentation of PET scans {{were determined}} using {{a set of}} control lung scans and scans containing lesions with minimal to mildly increased uptake. <b>Density</b> <b>thresholds</b> for CT scans were determined using values previously reported in literature [41].|$|R
5000|$|The PPN phase {{continues}} {{until the}} central star reaches around 30,000 K {{and it is}} hot enough (producing enough ultraviolet radiation) to ionize the circumstellar nebula (ejected gases) {{and it becomes a}} kind of emission nebula called a PN. This transition must take place in less than around 10,000 years or else the density of the circumstellar envelope will fall below the PN formulation <b>density</b> <b>threshold</b> of around 100 per cm³ and no PN will result, such a case is sometimes referred to as a 'lazy planetary nebula'.|$|E
50|$|While ecologists {{are just}} {{starting}} to get a grapple on the significant interactions within an ecosystem, they must continue to find an effective <b>density</b> <b>threshold</b> that can maintain the level of equilibrium species diversity. Only with this knowledge of where and to what extent a specific species interacts with its environment will the proper and most efficient levels of conservation work take place. This work is especially important on the limited ecosystems of islands, where there {{are less likely to be}} replacement species for specific niches. With species diversity and available habitat decreasing rapidly worldwide, identifying the systems that are most crucial to the ecosystem will be the crux of conservation work.|$|E
50|$|Development of {{antibiotics}} is difficult, whereas many drug discoveries {{have been a}} result of concerted effort and intensive research and development, antibiotics have seemingly been discovered by chance. Since 1987 {{there have been no}} discoveries or development of a new class {{of antibiotics}}. This is partly due to the finicky nature of antibiotics. As most are produced biosynthetically they require an organism to produce. Historically this has meant that different species are grown and observed for any antimicrobial activity. Not only does this require a culturable species to start off with, but the conditions the species are grown in must be adequate for production of antibiotics as well as having the number of antibiotics produced reach a <b>density</b> <b>threshold</b> so that their function can be observed.|$|E
30|$|This {{study was}} {{undertaken}} in highly stocked unthinned Douglas-fir stands located {{in areas with}} complex topography. Consequently, the pulse <b>density</b> <b>thresholds</b> described here {{are likely to be}} conservative and could be used to guide acquisition of high-quality LiDAR datasets for this species.|$|R
5000|$|Crystallization {{of molten}} α-B2O3 at ambient {{pressure}} is strongly kinetically disfavored (compare liquid and crystal <b>densities).</b> <b>Threshold</b> conditions for crystallization of the amorphous solid are 10 kbar and ~200 °C. [...] Its proposed crystal structure in enantiomorphic space groups P31(#144); P32(#145) (e.g., γ-glycine) has been revised to enantiomorphic space groups P3121(#152); P3221(#154)(e.g., α-quartz).|$|R
40|$|Purpose. The authors {{assessed}} {{the effect of}} vascular attenuation and <b>density</b> <b>thresholds</b> on the classification of noncalcified plaque by computed tomography coronary angiography (CTCA). Materials and methods. Thirty patients (men 25; age 59 +/- 8 years) with stable angina underwent arterial and delayed CTCA. At sites of atherosclerotic plaque, attenuation values (HU) were measured within the coronary lumen, noncalcified and calcified plaque material and the surrounding epicardial fat. Based on the measured CT attenuation values, coronary plaques were classified as lipid rich (attenuation value below the threshold) or fibrous (attenuation value above the threshold) using 30 -HU, 50 -HU and 70 -HU <b>density</b> <b>thresholds.</b> Results. One hundred and sixty-seven plaques (117 mixed and 50 noncalcified) were detected and assessed. The attenuation values of mixed plaques were {{higher than those of}} exclusively noncalcified plaques in both the arterial (148. 3 +/- 73. 1 HU vs. 106. 2 +/- 57. 9 HU) and delayed (111. 4 +/- 50. 5 HU vs. 64. 4 +/- 43. 4 HU) phases (p< 0. 01). Using a 50 -HU threshold, 12 (7. 2 %) plaques would be classified as lipid rich on arterial scan compared with 28 (17 %) on the delayed-phase scan. Reclassification of these 16 (9. 6 %) plaques from fibrous to lipid rich involved 4 / 30 (13 %) patients. Conclusions. Classification of coronary plaques as lipid rich or fibrous based on absolute CT attenuation values is significantly affected by vascular attenuation and <b>density</b> <b>thresholds</b> used for the definition...|$|R
50|$|Cluster {{analysis}} {{itself is}} not one specific algorithm, but the general task to be solved. It {{can be achieved by}} various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a <b>density</b> <b>threshold</b> or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.|$|E
5000|$|Estes et al. (1978) {{evaluated}} the potential {{role of the}} sea otter as the keystone predator in near-shore kelp forests. They compared the Rat and Near islands in the Aleutian islands to test if “sea otter predation controls epibenthic invertebrate populations (specifically sea urchins), and in turn releases the vegetation association from intense grazing”. [...] Estes and his colleagues found that different size structures and densities of sea urchins were correlated {{with the presence of}} sea otter populations, and because they are the principal prey of this keystone predator, the sea otters were most likely the main determinants of the differences in sea urchin populations. With high sea otter densities the herbivory of sea urchins in these kelp forest was severely limited, and this made competition between algal species the main determinant in survival. However, when sea otters were absent, herbivory of the sea urchins was greatly intensified to the point of decimation of the kelp forest community. This loss of heterogeneity serves as a loss of habitat for both fish and eagle populations that depend on the richly productive kelp forest environment. Historical over harvesting of sea otter furs has severely restricted their once wide-ranging habitat, and only today are scientists starting to see the implications of these local extinctions. Conservation work needs to focus on finding the <b>density</b> <b>threshold</b> that render the sea otters an effective population. It must then continue and artificially repopulate the historical range of the sea otter in order to allow kelp forest communities to re-establish.|$|E
5000|$|Conservation {{policy has}} {{historically}} lagged behind current science {{all over the}} world, but at this critical juncture politicians must {{make the effort to}} catch up before massive extinctions occur on our planet. For example, the pinnacle of American conservation policy, the Endangered Species Act of 1973, fails to acknowledge any benefit for protecting highly interactive species that may help maintain overall species diversity. Policy must first assess whether the species in question is considered highly interactive by asking the questions “does the absence or loss of this species, either directly or indirectly, incur a loss of overall diversity, effect the reproduction or recruitment of other species, lead to a change in habitat structure, lead to a change in productivity or nutrient dynamics between ecosystems, change important ecological processes, or reduce the resilience of the ecosystem to disturbances?”. [...] After these multitudes of questions are addressed to define an interactive species, an ecologically effective <b>density</b> <b>threshold</b> must be estimated in order to maintain this interaction ecology. This process holds many of the same variables contained within viable population estimates, and thus should not be difficult to incorporate into policy. To avoid mass extinction on a global scale unlike anyone has seen before, scientists must understand all of the mechanisms driving the process. It is now that the governments of the world must act in order to prevent this catastrophe of the loss of biodiversity from progressing further and wasting all of the time and money spent on previous conservation efforts.|$|E
40|$|We {{study the}} global {{distribution}} and morphology {{of dark matter}} voids in a LCDM universe using density fields generated by N-body simulations. Voids are defined as isolated regions of the low-density excursion set specified via <b>density</b> <b>thresholds,</b> the <b>density</b> <b>thresholds</b> being quantified by the corresponding filling factors, i. e., the {{fraction of the total}} volume in the excursion set. Our work encompasses a systematic investigation of the void volume function, the volume fraction in voids, and the fitting of voids to corresponding ellipsoids and spheres. We emphasize the relevance of the percolation threshold to the void volume statistics of the density field both in the high redshift, Gaussian random field regime, {{as well as in the}} present epoch. By using measures such as the Inverse Porosity, we characterize the quality of ellipsoidal fits to voids, finding that such fits are a poor representation of the larger voids that dominate the volume of the void excursion set. Comment: 13 pages, 15 figures, submitted to MNRA...|$|R
40|$|AbstractPrediction for {{transverse}} {{energy dependence}} of J/ψ to Drell–Yan ratio in Au+Au collisions at RHIC energy was obtained {{in a model}} which assume 100 % absorption of J/ψ above a <b>threshold</b> <b>density.</b> The <b>threshold</b> <b>density</b> was obtained by fitting the NA 50 data on J/ψ suppression in Pb+Pb collisions at SPS energy. At RHIC energy, hard processes may be important. Prediction of J/ψ suppression with and without hard processes were obtained. With hard processes included, J/ψ's are strongly suppressed...|$|R
40|$|One of {{the options}} {{considered}} for future upgrades of the LHC injector complex entails {{the replacement of the}} PS with the PS 2, a longer circumference and higher energy synchrotron. Electron cloud effects represent an important potential limitation to the achievement of the upgrade goals. We report the results of numerical studies aiming at estimating the e-cloud <b>density</b> <b>thresholds</b> for the occurrence of single bunch instabilities...|$|R
40|$|The {{introduction}} of divertor modules into the W 7 -AS stellarator has allowed {{access to a}} high density and high power H-mode (HDH) regime where the radiation profiles reach a steady state. This regime exists above a <b>density</b> <b>threshold</b> at line integrated densities, n̄e, up to 41020 m 3 with 4 MW of NBI power [1, 2]. This <b>density</b> <b>threshold</b> is power dependent. I...|$|E
30|$|Test cell: Cells with a density {{above the}} adopted <b>density</b> <b>threshold</b> are called test cells. Only those are {{considered}} in our analysis.|$|E
40|$|Abstract—The {{scheduling}} of generalized pinwheel task {{systems is}} considered. It is shown that pinwheel scheduling {{is closely related}} to the fair scheduling of periodic task systems. This relationship is exploited to obtain new scheduling algorithms for generalized pinwheel task systems. When compared to traditional pinwheel scheduling algorithms, these new algorithms are both more efficient from a runtime complexity point of view, and have a higher <b>density</b> <b>threshold,</b> on a very large subclass of generalized pinwheel task systems. Index Terms—Generalized pinwheels, fairness, real-time scheduling, <b>density</b> <b>threshold...</b>|$|E
40|$|Using the QGP {{motivated}} threshold model, {{where all}} the J/ψ's are suppressed above a <b>threshold</b> <b>density,</b> we have analyzed {{the latest version of}} the NA 50 data on the centrality dependence of the J/ψ over Drell-Yan ratio. The data are not well explain in the model, unless the <b>threshold</b> <b>density</b> is largely smeared. Large smeared <b>threshold</b> <b>density</b> effectively excludes creation of any deconfined medium in the collision. Comment: 4 pages, 2 figure...|$|R
40|$|Long self-trapped {{femtosecond}} ultraviolet filaments {{created in}} the bulk of pure fused silica are used to induce permanent structural changes in the medium. We monitor the laser pulse propagation as a filamentary structure and the plasma string left at its trail. We investigate and demonstrate the link of the filament-induced plasma to the permanent structural changes left in the medium. Specific electron <b>density</b> <b>thresholds</b> are found for the induced modifications. (C) 2007 Optical Society of America...|$|R
40|$|Selection {{of roads}} is an {{intractable}} generalization operation {{due to the}} difficulty in retaining the density difference and connectivity of a road network. This paper proposes a novel approach of selective omission for roads based on mesh density. The density of a road network and its local variations are calculated using meshes as units. Since maps at different scales usually reveal different <b>densities,</b> different <b>density</b> <b>thresholds</b> for road networks are determined {{on the basis of}} theoretical analysis and empirical study of mesh densities on maps at different scales. The selection process starts with the identification of the meshes that have a <b>density</b> beyond the <b>threshold.</b> The mesh with the largest density is first treated. Its bounding road segments are ordered according to their relative importance. The least important segment is eliminated. The remaining segments are then merged with the adjacent mesh, thus forming a new mesh. The selection procedure is repeated until none of the meshes has a <b>density</b> beyond the <b>threshold.</b> Such a process of eliminating road segments and merging meshes can ensure the road network connectivity. In this study, the meshes are classified depending on the types of road segment. For the different mesh types, their <b>density</b> <b>thresholds</b> are set to be different, which can be used as an indicator for the preservation of the density difference. This proposed approach considers topological, geometric and semantic properties of the road network. It was applied to two sets of road networks, and the results of selection are convincing. This methodology has now been adopted for the updating of 1 : 50, 000 maps of China. Department of Land Surveying and Geo-Informatic...|$|R
40|$|We study {{large-scale}} structures from numerical simulations, paying {{particular attention}} to supercluster-like structures. A grid-density-contour based algorithm is adopted to locate connected groups. With the increase of the linking <b>density</b> <b>threshold,</b> the foam- like cosmic web is subsequently broken into individual supercluster-like groups and further halos which are in accordance to groups with the linking <b>density</b> <b>threshold</b> ρ/ρ= 1 + δ = 80. By analyzing sets of cosmological simulations with varying cosmological parameters, we find that an universal mass function exists not only for halos but also for low-density supercluster-like groups until the linking <b>density</b> <b>threshold</b> decreases to a density where the global percolation of large-scale structures occurs. We further show that the mass functions of different groups can be well described by the Jenkins form with the parameters being dependent on the linking <b>density</b> <b>threshold.</b> On the other hand, these low- density supercluster-like groups cannot be directly associated with the predictions from the excursion set theory with effective barriers obtained from dynamical collapse models, and the peak exclusion effect {{must be taken into}} account. Including such an effect, the mass function of groups with the linking <b>density</b> <b>threshold</b> 1 + δ = 16 is in good agreements with that from the excursion set theory with a nearly flat effective barrier. A simplified analysis of the ellipsoidal collapse model indicates that the barrier for collapses along two axes to form filaments is approximately flat in scales, thus we define groups identified with 1 + δ = 16 as filaments. We then further study the halo-filament conditional mass function and the filament-halo conditional mass function, and compare them with the predictions from the two-barrier excursion set theory. The shape statistics for filaments are also presented. Comment: Accepted by Ap...|$|E
3000|$|... 6 With the 5 % <b>density</b> <b>threshold,</b> {{there were}} 33 PMSAs in the {{traditional}} destinations, 19 PMSAs in the new high growth and 175 PMSAs in the new low growth destinations.|$|E
40|$|We analyse {{the mass}} {{distribution}} of cores formed in an isothermal, magnetized, turbulent, and self-gravitating nearly critical molecular cloud model. Cores are identified at two <b>density</b> <b>threshold</b> levels. Our main results are that, {{in the presence}} of self-gravity, the slopes of the core mass function (CMF) at the high mass end, in agreement with observations, are shallower than those predicted by the theory of turbulent fragmentation. The shallowness of the slope is due to the effects of core coalescence and gas accretion. Secondly, the slope of the CMF at the high mass end steepens when cores are selected at higher density thresholds. Alternatively, if the CMFs are fitted with a log-normal function, the width of the log-normal distribution decreases with increasing threshold. This {{is due to the fact}} that gravity plays a more important role in denser structures selected at higher <b>density</b> <b>threshold</b> and leads to the conclusion that the role of gravity is essential in generating a CMF that bears more resemblence with the IMF when cores are selected with an increasing <b>density</b> <b>threshold</b> in the observations...|$|E
40|$|We present {{experimental}} and analytical results showing "zero-one" phase transitions for network connectivity, multi-path reliability, neighbor count, Hamiltonian cycle formation, multiple-clique formation, and probabilistic flooding. These transitions {{are characterized by}} critical <b>density</b> <b>thresholds</b> such that a global property exists with negligible probability {{on one side of}} the threshold, and exists with high probability on the other. We discuss the connections between these phase transitions and some known results on random graphs, and indicate their significance for the design of resource-efficient wireless networks...|$|R
40|$|Abstract In practice, stand density indices must be {{estimated}} from sample data, leading to uncertainty in both estimates of stand density and resulting thinning decisions. We provide a statistical framework for thinning {{decisions based on}} <b>density</b> <b>thresholds.</b> We show how to assess the adequacy of existing cruise data to guide the thinning process, and to design more cost-effective sampling plans. We present this information {{in a series of}} graphs which, when used in conjunction with density management diagrams, can improve rational thinning decisions...|$|R
5000|$|Candidate valley: {{consecutive}} sectors with a polar obstacle <b>density</b> below <b>threshold,</b> {{known as}} candidate valleys, is {{selected based on}} the proximity to the target direction.|$|R
40|$|This paper creatively {{proposes a}} cluster {{boundary}} sampling method based on density clustering {{to solve the}} problem of resampling in IDS classification and verify its effectiveness experimentally. We use the clustering <b>density</b> <b>threshold</b> and the boundary <b>density</b> <b>threshold</b> to determine the cluster boundaries, in order to guide the process of resampling more scientifically and accurately. Then, we adopt the penalty factor to regulate the data imbalance effect on SVM classification algorithm. The achievements and scientific significance of this paper do not propose the best classifier or solution of imbalanced data set and just verify the validity and stability of proposed IDS resampling method. Experiments show that our method acquires obvious promotion effect in various imbalanced data sets...|$|E
40|$|We {{say that}} a graph G has a perfect H-packing if there exists a set of vertex-disjoint copies of H which cover all the {{vertices}} in G. We consider various problems concerning perfect H-packings: Given n, r, D ∈ N, we characterise the edge <b>density</b> <b>threshold</b> that ensures a perfect Kr-packing in any graph G on n vertices and with minimum degree δ(G) ≥ D. We also give two conjectures concerning degree sequence conditions which force a graph to contain a perfect H-packing. Other related embedding problems are also considered. Indeed, we give a degree sequence condition which forces a graph to contain a copy of Kr, thereby strengthening the minimum degree version of Turán’s theorem. We also characterise the edge <b>density</b> <b>threshold</b> that ensures a graph G contains k vertex-disjoint cycles. ...|$|E
40|$|Since 1997 when Vertically Integreated Liquid (VIL) density was {{introduced}} {{as a tool}} for assessing hail potential in thunderstorms, much research has focused on making better use of this method in operational forecasting during severe weather events. To assess the usefulness of this method in the Burlington, Vermont county warning area, thunderstorm VIL and echo tops were analyzed for a number of severe thunderstorm events, and VIL density was calculated. VIL density was then correlated to the observed reports of hail. The results showed that above a VIL <b>density</b> <b>threshold</b> value of 3. 28 g m- 3, severe hail occurred in a substantial number of thunderstorm events. Further, results also showed that above a VIL <b>density</b> <b>threshold</b> value of 4. 22 g m- 3, severe hail occurred in almost every thunderstorm. 1...|$|E
30|$|We {{performed}} {{visual checks}} of {{the accuracy of}} CT lesion delineation, based on these fixed <b>density</b> <b>thresholds.</b> For lesions with increased density (Vsoft, Vmedium, Vhard), the segmented areas corresponded well to lesion morphology. However, using −[*] 950 HU as the upper limit for low-density lesions was not specific enough for cavitation and the segmented areas in some cases included bullae, bronchiectasis, and severe emphysema. This necessitated an additional step to measure the volumes of individual cavities for each scan. This was done using the MRICro’s 3 D region-growing tool (boundary-based segmentation).|$|R
30|$|The {{purpose is}} to propose and {{illustrate}} hypotheses on STEM organizational systems and associated emergent phenomena. Both the base-case high-density STEM organization, NASA, and control group organizations, the EPA and FCC, are represented to investigate attrition rate and emergent group-sizes across a distributed scale of organizational STEM density. Using the base-case high-density STEM organizational model, two additional variable high-density STEM organizational models are tested to validate and assess lower and upper-limit STEM <b>density</b> <b>thresholds</b> for attrition and observed group sizes during a 10 -year period, 2005 – 2014.|$|R
40|$|The aim of {{this study}} is a better {{understanding}} of the interaction between Excimer irradiation and metallic materials, as well as the determination of the energy <b>density</b> <b>thresholds</b> of the different surface treatments. The experimental result, using a KrF (λ = 0. 248 µm) laser irradiation on 42 CD 4 and Z 160 CDV 12 steel substrates, are presented. An attempt was made to determine the relationship between the laser parameters, the dimensions of the formed craters (depth, length), the proportion of the ablated material and the surface roughness...|$|R
