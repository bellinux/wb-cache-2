0|59|Public
50|$|One of the {{features}} of this standard could be stored or interchanged in one of three formats: <b>Formatted,</b> <b>Formatted</b> <b>Processable,</b> or Processable. The latter two are editable formats. The first is an uneditable format that is logically similar to Adobe Systems PDF that is in common use today.|$|R
50|$|A {{subfield}} of Knowledge Acquisition within Artificial Intelligence, Knowledge Collection from Volunteer Contributors (KCVC) {{attempts to}} drive down the cost of acquiring the knowledge required to support automated reasoning by having the public enter knowledge in computer <b>processable</b> <b>form</b> over the internet. KCVC might be regarded as similar in spirit to Wikipedia, although the intended audience, Artificial Intelligence systems, differs.|$|R
40|$|Arrays of oriented, {{crystalline}} Si wires {{are transferred}} into flexible, transparent polymer films. The polymer-supported Si wire arrays in liquid-junction photoelectrochemical cells yield current-potential behavior {{similar to the}} Si wires attached to the brittle growth substrate. These systems offer the potential for attaining high solar energy-conversion efficiencies using modest diffusion length, readily grown, crystalline Si in a flexible, <b>processable</b> <b>form...</b>|$|R
40|$|Arrays of Si rods are {{embedded}} in PDMS and removed from the rigid growth substrate, resulting in a composite material that merges the benefits of single-crystalline silicon with the flexibility of a polymer. With this technique, solar cell absorber materials {{with the potential to}} achieve high efficiency can be prepared by high-temperature processing and transformed into a flexible, <b>processable</b> <b>form...</b>|$|R
3000|$|Electronic {{health record}} (EHR) It is a {{repository}} of information regarding the health status of patient, in computer <b>processable</b> <b>form.</b> In fact, an EHR is supposed to contain all the necessary information about the patients collected from several providers plus evidence-based tools to make intelligent decisions. Thus, EHR maintains the total health of the patients. Note that it may store data in structured or unstructured or in both format.|$|R
40|$|We {{provide a}} novel {{framework}} and implementation which integrates tools {{to support the}} acquisition of mass, distribu-tive, incremental, dynamic, argumentative knowledge in nat-ural language. With the Attempto Controlled English (ACE) system, natural language statements are automatically trans-lated to a machine <b>processable</b> <b>form.</b> A discussion forum allows the specication of argumentation theoretic relation-ships among statements. Statements and their relationships are input to a formal, implemented argumentation system, which calculates inferences from asserted premises...|$|R
40|$|This {{work was}} created to review the {{evidence}} for lexical borrowing from the Tocharian languages to the Chinese languages. The used methodology relies on lexical lists, previous etymological findings, linguistic typology and anthropological input. For preparatory data manipulation, a set of semi- automatic scripts has been created. Presented is a qualitative research based on previous findings assisted by raw data. The outcome of this work should be testable findings which could be extracted to a computer <b>processable</b> <b>form...</b>|$|R
40|$|With the {{invention}} of high throughput methods, researchers are capable of producing large amounts of biological data. During the analysis of such data {{the need for a}} functional grouping of genes arises. In this paper, we propose a new method based on spectral clustering for the partitioning of genes according to their biological function. The functional information is based on Gene Ontology annotation, a mechanism to capture functional knowledge in a shareable and computer <b>processable</b> <b>form.</b> Our functional cluster method promises to automatize, speed up and therefore improve biological data analysis...|$|R
40|$|Ontologies can {{formally}} {{describe the}} semantics {{of the medical}} domain in an unambiguous and machine <b>processable</b> <b>form,</b> acting as a conceptual interface between different applications that must interoperate. In this paper we present an ontology of cancer therapies originally developed {{to bridge the gap}} between an oncologic Electronic Patient Record (EPR) and a guideline-based decision support system. We show an application of the ontology complemented by rules to classify therapies recorded in the EPR. The results show how such an ontology can be used also to discover possible problems of data consistency in the EPR...|$|R
40|$|Abstract. As soon as Web {{documents}} embed {{knowledge in}} a <b>format</b> <b>processable</b> by computers, {{it is expected}} that knowledge-based services will be offered on-line, through the Web. These applications will query the Web to seek the information relevant to their task. Knowledge providers will host that knowledge and will make it available to these various applications. Agent technology is probably best suited to implement knowledge servers. This paper sketches how conceptual graphs (CG) based software agents could play the role of knowledge providers; as an example, it uses a situation where some agent must answer a query sent by some other agent. In doing so, this paper shows how interoperability problems between communicating conceptual graph based systems can be detected automatically. It also shows how semantic constraints can be used to implement semantic filters, filters required to control, on a semantic level, the information that is exchanged between communicating systems. ...|$|R
40|$|It {{has long}} been {{realised}} that the web could benefit from having its content understandable and available in a machine <b>processable</b> <b>form,</b> and it is widely agreed that ontologies will {{play a key role}} in providing much enabling infrastructure to support this goal. In this chapter we review briefly a selected history of description logics in web-based information systems, and the more recent developments related to OIL, DAML+OIL and the semantic web. OIL and DAML+OIL are ontology languages specifically designed for use on the web; they exploit existing web standards (XML, RDF and RDFS), adding the formal rigor of a description logic and the ontological primitives of object oriented and frame based systems...|$|R
40|$|Handwritten pattern {{recognition}} involves conversion of scanned images of handwritten patterns {{into a computer}} <b>processable</b> <b>form.</b> To recognize handwritten patterns is an easy and trivial task for human beings, but for a machine it is a cumbersome and a difficult task due to high variations {{in the shape of}} characters and writing style. Although complicated to train, yet machines can be useful in providing solution to the recognition problem. They save time and money and eliminate the requirement of execution of repetitive tasks by humans. In order to have better recognition the image should be properly pre-processed. Pre-processing reduces and eliminates noise and irregularities. The present paper focuses on different approaches to pre-processing and an insight to general methodology for the recognition process...|$|R
40|$|Research in Machine Learning (ML) has {{traditionally}} focussed on designing effective algorithms for solving particular tasks. However, {{there is an}} increasing interest in providing the user with a means for specifying what the ML problem in hand actually is rather than letting him struggle to outline how the solution to that problem needs to be computed. This corresponds to a model+solver approach to ML, in which the user specifies the problem in a declarative modeling language and the system automatically transforms such models into a format {{that can be used}} by a solver to efficiently generate a solution. In this paper, we propose a model+solver approach to Concept Learning problems which combines the efficacy of Description Logics (DLs) in conceptual modeling with the efficiency of Answer Set Programming (ASP) solvers in dealing with constraint satisfaction problems. In particular, the approach consists of a declarative modeling language based on second-order DLs under Henkin semantics, and a mechanism for transforming second-order DL formulas into a <b>format</b> <b>processable</b> by ASP solvers...|$|R
40|$|The Systematized Nomenclature of Medicine, Third Edition, SNOMED International, is a {{comprehensive}} structured nomenclature of human and veterinary medicine, the terms of which are detailed, fine grained and semantically typed. Terms are assigned to eleven independent modules (fields), {{each of which is}} systematized. Terms may be linked to on another to represent complex entities or manifestations or alternately complex terms dissected into their elemental parts. Terms are illustrated utilizing a frame representation. Efforts are in progress to build both a conceptual graph and a frame-based semantic network encompassing each SNOMED term, effectively building a knowledge base. In this way, the knowledge contained in each alphanumeric representation is made explicit. SNOMED is a linked data structure capable of faithfully representing the activities, observations and diagnoses found in the medical record in a computer <b>processable</b> <b>form...</b>|$|R
40|$|In 1997, the American Medical Informatics Association {{proposed}} a US information strategy {{that included a}} population health record (PopHR). Despite subsequent progress on the conceptualization, development, and implementation of electronic health records and personal health records, minimal progress has occurred on the PopHR. Adapting International Organization for Standarization electronic health records standards, we define the PopHR as a repository of statistics, measures, and indicators regarding the state of and influences {{on the health of}} a defined population, in computer <b>processable</b> <b>form,</b> stored and transmitted securely, and accessible by multiple authorized users. The PopHR is based upon an explicit population health framework and a standardized logical information model. PopHR purpose and uses, content and content sources, functionalities, business objectives, information architecture, and system architecture are described. Barriers to implementation and enabling factors and a three-stage implementation strategy are delineated...|$|R
40|$|Clinical {{questions}} are often studied by randomized clinical trials (RCTs) of heterogeneous design. Systematic reviewers and trial designers need {{to compare the}} design and results across these trials. If trial information is available in computer <b>processable</b> <b>form,</b> computer-based visualization techniques can provide cognitive support for such comparisons. CTeXplorer offers systematic reviewers and trial designers a tool to better and more quickly understand design heterogeneity in RCTs. CTeXplorer supports dynamic queries on eligibility criteria, interventions, and outcomes in three linked views. We tested CTeXplorer for displaying 12 RCTs on prevention of mother-to-child transmission of HIV. Three target users found the representation and organization of information intuitive and easy to learn. They {{were able to use}} CTeXplorer to achieve a quick cognitive overview of a heterogeneous group of RCTs. This work shows the benefit of capturing trial information in computable form. Future work includes leveraging ontologies to enhance CTeXplorer visualizations...|$|R
40|$|Abstract—There {{are many}} {{problems}} associated with the World Wide Web: getting lost in the hyperspace; the web content is still accessible only to humans and difficulties of web administration. The solution to these problems is the Semantic Web which is considered to be the extension for the current web presents information in both human readable and machine <b>processable</b> <b>form.</b> The aim {{of this study is to}} reach new generic foundation architecture for the Semantic Web because there is no clear architecture for it, there are four versions, but still up to now there is no agreement for one of these versions nor is there a clear picture for the relation between different layers and technologies inside this architecture. This can be done depending on the idea of previous versions as well as Gerber’s evaluation method as a step toward an agreement for one Semantic Web architecture...|$|R
40|$|The {{everyday}} {{programming and}} maintenance activities {{make use of}} knowledge about programming-related tech-nologies such as graphical interfaces (GUIs) or XML. Hav-ing this knowledge in a machine <b>processable</b> <b>form</b> supports the automation of typical maintenance activities such as concept location and raise the abstraction level at which the current code analyses are performed. In this paper we promote our current project of building a repository of on-tologies 1 that contain knowledge about programming tech-nologies. We propose a method for extracting these on-tologies by analyzing the commonalities of different APIs that address the same domain. We discuss the motivation for such a repository in form of possible usage directions and present our experience with building and using ontolo-gies that share technical knowledge about GUIs and XML. Based on our experience we discuss the challenges of build-ing and evolving the repository and discuss how can the se-mantic web technologies contribute to this endeavor. ...|$|R
40|$|The International Standards Organization (ISO) {{has defined}} a {{protocol}} test language called TTCN (Tree and Tabular Combined Notation) to specify abstract test suites for Open Systems Interconnection (OSI) protocols. TTCN combines a tree notation for dynamic behaviour description with a tabular representation of various language constructs. TTCN allows tabular constraints to enforce values on the Abstract Service Primitive (ASP) or Protocol Data Unit (PDU) parameters. For application layer protocols, Abstract Syntax Notation One (ASN. 1) constraints are used. Dynamic behaviour description in TTCN {{is shown to}} address many important aspects of conformance testing such as modularity support in terms of test cases, steps and default behaviour tables and sophisticated timer management. TTCN has a machine <b>processable</b> <b>form</b> called TTCN-MP that defines all the TTCN syntax using BNF. Semantics of the tests specified in TTCN is operationally defined rendering TTCN almost a formal notation. © 1992...|$|R
40|$|Abstract. Digital Libraries {{organized}} {{collections of}} multimedia objects {{in a computer}} <b>processable</b> <b>form.</b> They also comprise services and infrastructures to manage, store, retrieve and share objects. Among these services, personalization services represent an active and broad area of digital library research. A popular way to realize personalization is by using information filtering techniques aiming to remove redundant or unwanted information from data. In this paper we propose to use a probabilistic framework based on uncertain graphs {{in order to deal}} with information filtering problems. Users, items and their relationships are encoded in a probabilistic graph that can be used to infer the probability of existence of a link between entities involved in the graph. The goal of the paper is to extend uncertain graphs definition to multigraphs and to study whether uncertain graphs could be used as a valuable tool for information filtering problems. The performance of the proposed probabilistic framework is reported when applied to a real-world domain. ...|$|R
40|$|Graphene, a nanocarbon with {{exceptional}} {{physical and}} electronic properties, {{has the potential}} to be utilized in a myriad of applications and devices. However, this will only be achieved if scalable, <b>processable</b> <b>forms</b> of graphene are developed along with ways to fabricate these forms into material structures and devices. In this review, we provide a comprehensive overview of the chemistries suitable for the development of aqueous and organic solvent graphene dispersions and their use for the preparation of a variety of polymer composites, materials useful for the fabrication of graphene-containing structures and devices. Fabrication of the processable graphene dispersions or composites by printing (inkjet and extrusion) or spinning methods (wet) is reviewed. The preparation and fabrication of liquid crystalline graphene oxide dispersions whose unique rheologies allow the creation of graphene-containing structures by a wide range of industrially scalable fabrication techniques such as spinning (wet and dry), printing (ink-jet and extrusion) and coating (spray and electrospray) is also reviewed...|$|R
40|$|It {{has long}} been {{realized}} that the web could benefit from having its content understandable and available in a machine <b>processable</b> <b>form.</b> The Semantic Web aims to achieve this via annotations that use terms defined in ontologies to give well defined meaning to Web accessible information and services. OWL, the ontology language recommended by the W 3 C for this purpose, was heavily influenced by Description Logic research. In this chapter we review briefly some early efforts that combine Description Logics and the Web, including predecessors of OWL such as OIL and DAML+OIL. We {{then go on to}} describe OWL in some detail, including the various influences on its design, its relationship with RDFS, its syntax and semantics, and a range of tools and applications. 1 14. 1 Background and history The World Wide Web, while wildly successful in growth, may be viewed as being limited by its reliance on languages such as HTML that are focuse...|$|R
40|$|Since its {{emergence}} in the 1990 s the World Wide Web (WWW) has rapidly {{evolved into}} a huge mine of global information and it is growing in size everyday. The presence of huge amount of resources on the Web thus poses a serious problem of accurate search. This is mainly because today's Web is a human-readable Web where information cannot be easily processed by machine. Highly sophisticated, efficient keyword based search engines that have evolved today {{have not been able}} to bridge this gap. So comes up the concept of the Semantic Web which is envisioned by Tim Berners-Lee as the Web of machine interpretable information to make a machine <b>processable</b> <b>form</b> for expressing information. Based on the semantic Web technologies we present in this paper the design methodology and development of a semantic Web search engine which provides exact search results for a domain specific search. This search engine is developed for an agricultural Website which hosts agricultural information about the state of West Bengal. Comment: 9 pages, 11 figures, MSPT 2007 - 7 th International Workshop on MSPT Proceeding...|$|R
40|$|Social web content such as blogs, videos, {{and other}} user-generated content present a vast source of rich "digital-traces" of individuals&# 039; experiences. The use of digital traces to {{provide insight into}} human {{behavior}} remains underdeveloped. Recently, ontological approaches have been exploited for tagging and linking digital traces, with progress made in ontology models for well-defined domains. However, the process of conceptualization for ill-defined domains remains challenging, requiring interdisciplinary efforts to understand the main aspects and capture them in a computer <b>processable</b> <b>form.</b> The primary contribution {{of this article is}} a theory-driven approach to ontology development that supports semantic augmentation of digital traces. Specifically, we argue that (a) activity theory can be used to develop more insightful conceptual models of ill-defined activities, which (b) can be used to inform the development of an ontology, and (c) that this ontology can be used to guide the semantic augmentation of digital traces for making sense of phenomena. A case study of interpersonal communication is chosen to illustrate the applicability of the proposed multidisciplinary approach. The benefits of the approach are illustrated through an example application, demonstrating how it may be used to assemble and make sense of digital traces...|$|R
40|$|Third-generation {{profilometer}} {{for inspection}} of butt welds includes hand-held probe unit operating {{in conjunction with}} computer. Unit positioned across weld, in contact with workpiece, to obtain profile. Increases precision by reducing subjective inputs and concomitant variations in outputs among different operators. Computerization retains capabilities of first- and second-generation profilometers to measure peak angles and mismatches of butt welds and extends measurement capabilities into field of image analysis. Output data more readily <b>processable</b> into <b>forms</b> used by same or another computer...|$|R
40|$|The {{principal}} {{elements and}} functions of the Ground Operations Aerospace Language (GOAL) compiler are presented. The technique used to transcribe the syntax diagrams into machine <b>processable</b> <b>format</b> {{for use by the}} parsing routines is described. An explanation of the parsing technique used to process GOAL source statements is included. The compiler diagnostics and the output reports generated during a GOAL compilation are explained. A description of the GOAL program package is provided...|$|R
40|$|This PDF {{has been}} {{designed}} to help you navigate your way through the Payment and Reimbursement process. SOVA is providing this information in an effort to decrease the turn-a-round time for processing claims. Included are eligibility criteria for the Compensation Program, the SAP/CAP Program, payments and reimbursements at a glance and <b>processable</b> and unprocessable <b>forms...</b>|$|R
40|$|More {{information}} {{is now being}} published in machine <b>processable</b> <b>form</b> on the web and, as de-facto distributed knowledge bases are materializing, partly encouraged by {{the vision of the}} Semantic Web, the focus is shifting from the publication of this information to its consumption. Platforms for data integration, visualization and analysis that are based on a graph representation of information appear first candidates to be consumers of web-based information that is readily expressible as graphs. The question is whether the adoption of these platforms to information available on the Semantic Web requires some adaptation of their data structures and semantics. Ondex is a network-based data integration, analysis and visualization platform which has been developed in a Life Sciences context. A number of features, including semantic annotation via ontologies and an attention to provenance and evidence, make this an ideal candidate to consume Semantic Web information, as well as a prototype for the application of network analysis tools in this context. By analyzing the Ondex data structure and its usage, we have found a set of discrepancies and errors arising from the semantic mismatch between a procedural approach to network analysis and the implications of a web-based representation of information. We report in the paper on the simple methodology that we have adopted to conduct such analysis, and on issues that we have found which may be relevant for a range of similar platformsComment: Presented at DEIT, Data Engineering and Internet Technology, 2011 IEEE: CFP 1113 L-CD...|$|R
40|$|This {{dissertation}} {{examines the}} consequences of Electronic Data Interchange (EDI) use on interorganizational relations (IR) in the retail industry. EDI {{is a type of}} interorganizational information system that facilitates the exchange of business documents in structured, machine <b>processable</b> <b>form.</b> The research model links EDI use and three IR dimensions [...] structural, behavioral, and outcome. Based on relevant literature from organizational theory and marketing channels, fourteen hypotheses were proposed for the relationships among EDI use and the three IR dimensions. ^ Data were collected through self-administered questionnaires from key informants in 97 retail companies (19 % response rate). The hypotheses were tested using multiple regression analysis. The analysis supports the following hypothesis: (a) EDI use is positively related to information intensity and formalization, (b) formalization is positively related to cooperation, (c) information intensity is positively related to cooperation, (d) conflict is negatively related to performance and satisfaction, (e) cooperation is positively related to performance, and (f) performance is positively related to satisfaction. The results support the general premise of the model that the relationship between EDI use and satisfaction among channel members has to be viewed within an interorganizational context. ^ Research on EDI is still in a nascent stage. By identifying and testing relevant interorganizational variables, this study offers insights for practitioners managing boundary-spanning activities in organizations using or planning to use EDI. Further, the thesis provides avenues for future research aimed at understanding {{the consequences of}} this interorganizational information technology. ...|$|R
40|$|While text {{versioning}} {{was definitely}} {{a part of the}} origi-nal hypertext concept [21, 36, 44], it is rarely considered in this context today. Still, we know that revision control underlies the most exciting social co-authoring projects of the today’s Internet, namely the Wikipedia and the Linux kernel. With an intention to adapt the advanced revision control technologies and practices to the conditions of the Web, the paper reconsiders some obsolete assumptions and develops a new versioned text <b>format</b> perfectly <b>processable</b> with standard regular expressions (PCRE [6]). The result-ing deep hypertext model allows distributed and real-time revision control on the Web, provides the user with instant access to past/concurrent versions, authorship, changes; en-ables deep links to reference changing fragments within a changing text. It implements the vision of co-evolution and mutation exchange among multiple competing versions of the same text. Categories and Subject Descriptors I. 7. 1 [Document and text processing]: Document an...|$|R
40|$|Context-aware systems being a {{component}} of ubiquitous or pervasive computing environment sense the users’ physical and virtual surrounding to adapt their behaviour accordingly. To achieve activity context tracking devices are common practice. Service Oriented Architecture is based on collections of services that communicate with each other. The communication between users and services involves data {{that can be used}} to sense the activity context of the user. SOAP is a simple protocol to let applications exchange their information over the web. Semantic Web provides standards to express the relationship between data to allow machines to process data more intelligently. This work proposes an approach for supporting context-aware activity sensing using software sensors. The main challenges in the work are specifying context information in a machine <b>processable</b> <b>form,</b> developing a mechanism that can understand the data extracted from exchanges of services, utilising the data extracted from these services, and the architecture that supports sensing with software sensors. To address these issues, we have provided a bridge to combine the traditional web services with the semantic web technologies, a knowledge structure that supports the activity context information in the context-aware environments and mapping methods that extract the data out of exchanges occurring between user and services and map it into a context model. The Direct Match, the Synonym Match and the Hierarchical Match methods are developed to put the extracted data from services to the knowledge structure. This research will open doors to further develop automated and dynamic context-aware systems that can exploit the software sensors to sense the activity of the user in the context-aware environments. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|The work {{described}} in this paper was undertaken {{with the objective of}} simplifying the process of clinical research data acquisition to enable researchers to analyze data conveniently, economically and comprehensively. Though the Clinical Center differs from the community general hospitals, and even the university medical centers in many respects, its day-to-day operations are closely parallel to those in other hospitals. Therefore, it was felt that it would be worthwhile to consider the installation of the type of computerized medical information system used in the general hospitals and to tailor it to the specific research data capture needs which are characteristic of the NIH. After describing areas of similarity and difference in comparisons between the general hospital and the Clinical Center from the data processing perspective, consideration is given to the research data capture needs for Clinical Center patients. Discussions of systems involving data capture while the patient is undergoing active treatment, as well as those which are best captured in batch processes, are discussed. Finally, the state of development and use of the comprehensive Clinical Information Utility for the NIH researchers is described, including examples of its use. The system {{described in}} the paper represents one of the first attempts to make post-discharge use of inpatient medical care data derived from an on-line real-time hospital-wide information system. Already the results obtained from the initial exercises using the Clinical Information Utility have shown that the availability of all patient care information in computer <b>processable</b> <b>form,</b> generated {{as a result of the}} clinical efforts, will have tremendous impact on the quality and timeliness of research data processing at the NIH...|$|R
40|$|Dedicated to my mother. Without {{you this}} would have never been possible. i Medical {{information}} is often stored in a narrative way, which makes the automated processing a difficult and time-consuming task. Persons responsible for the authoring of medical documents do not {{take care of a}} further processing with automated systems. So, information stored in medical writings is not directly usable for the processing with computers. Due to this, efforts have been made to transfer these narrative documents in a <b>format</b> easier <b>processable</b> with computers. This matter of fact also applies to clinical practice guidelines (CPGs). As many medical do-cuments, CPGs are written in a narrative speech as well, without regards to a computer-assisted processing. For the implementation of CPGs in medical facilities an automated processing is the-refore desirable. An important fact {{is that a lot of}} information in CPGs is provided in a negated form, expressing that certain circumstances in patients or treatments are not available, existing or applicable. Although negated, this information is nevertheless very useful, since it can expres...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. The increasing use of computers for document preparation and publishing coupled with a growth in the general information management facilities available on computers has meant that most documents exist in computer <b>processable</b> <b>form</b> during their lifetime. This {{has led to a}} substantial increase in the demand for data storage facilities, which frequently seems to exceed the provision of storage facilities, despite the advances in storage technology. Furthermore, there is growing demand to transmit these textual documents from one use to another, rather than use a printed form for transfer between sites which then needs to be re-entered into a computer at the receiving site. Transmission facilities are, however, limited and large documents can be difficult and expensive to transmit. Problems of storage and transmission capacity can be alleviated by compacting the textual information beforehand, providing that there is no loss of information in this process. Conventional compaction techniques have been designed to compact all forms of data (binary as well as text) and have, predominantly, been based on the byte as the unit of compression. This thesis investigates the alternative of designing a compaction procedure for natural language texts, using the textual word as the unit of compression. Four related alternative techniques are developed and analysed in the thesis. These are designed to be appropriate for different circumstances where either maximum compression or maximum point to point transmission speed is of greatest importance, and where the characteristics of the transmission, or storage, medium may be oriented to a seven or eight bit data unit. The effectiveness of the four techniques is investigated both theoretically and by practical comparison with a widely used conventional alternative. It is shown that {{for a wide range of}} textual material the word based techniques yield a greater compression and require substantially less processing time...|$|R
40|$|AbstractUtilizing {{external}} collections {{to improve}} retrieval performance is challenging research because various test collections are created for different purposes. Improving medical information retrieval has also gained much attention as {{various types of}} medical documents have become available to researchers ever since they started storing them in machine <b>processable</b> <b>formats.</b> In this paper, we propose an effective method of utilizing external collections based on the pseudo relevance feedback approach. Our method incorporates the structure of external collections in estimating individual components in the final feedback model. Extensive experiments on three medical collections (TREC CDS, CLEF eHealth, and OHSUMED) were performed, {{and the results were}} compared with a representative expansion approach utilizing the external collections to show the superiority of our method...|$|R
40|$|The valency lexicon PDT-Vallex {{has been}} built in close {{connection}} with the annotation of the Prague Dependency Treebank project (PDT) and its successors (mainly the Prague Czech-English Dependency Treebank project, PCEDT). It contains over 11000 valency frames for more than 7000 verbs which occurred in the PDT or PCEDT. It is available in electronically <b>processable</b> <b>format</b> (XML) together with the aforementioned treebanks (to be viewed and edited by TrEd, the PDT/PCEDT main annotation tool), and also in more human readable form including corpus examples (see the WEBSITE link below). The main feature of the lexicon is its linking to the annotated corpora - each occurrence of each verb {{is linked to the}} appropriate valency frame with additional (generalized) information about its usage and surface morphosyntactic form alternatives...|$|R
