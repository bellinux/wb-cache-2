1592|519|Public
25|$|Learning algorithm: Numerous {{trade-offs}} {{exist between}} learning algorithms. Almost any algorithm will {{work well with}} the correct <b>hyperparameters</b> for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.|$|E
25|$|Applications whose goal is {{to create}} a system that generalizes well to unseen examples, face the {{possibility}} of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select <b>hyperparameters</b> to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the {{goal is to}} minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.|$|E
2500|$|Therefore, the {{posterior}} is (dropping the <b>hyperparameters</b> as conditioning factors): ...|$|E
40|$|We {{consider}} {{families of}} semiparametric Bayesian models based on Dirichlet process mixtures, indexed by a multidimensional <b>hyperparameter</b> {{that includes the}} precision parameter. We wish to select the <b>hyperparameter</b> by considering Bayes factors. Our approach involves distinguishing some arbitrary value of the <b>hyperparameter,</b> and estimating the Bayes factor for the model indexed by the <b>hyperparameter</b> vs. the model indexed by the distinguished point, as the <b>hyperparameter</b> varies. The approach requires us to select {{a finite number of}} <b>hyperparameter</b> values, and for each get Markov chain Monte Carlo samples from the posterior distribution corresponding to the model indexed by that <b>hyperparameter</b> value. Implementation of the approach relies on a likelihood ratio formula for Dirichlet process models. Because we may view parametric models as limiting cases where the precision <b>hyperparameter</b> is infinity, the method also enables us {{to decide whether or not}} to use a semiparametric or an entirely parametric model. We illustrate the methodology through two detailed examples involving meta-analysis...|$|R
50|$|One {{may take}} a single value for a given <b>hyperparameter,</b> or one can iterate and take a {{probability}} distribution on the <b>hyperparameter</b> itself, called a hyperprior.|$|R
30|$|Note {{that the}} Nelder-Mead method rarely generates poor <b>hyperparameter</b> {{settings}} {{because of its}} strategy, e.g., reflection moves the simplex {{in a direction away}} from the point of poor <b>hyperparameter</b> settings.|$|R
2500|$|... where [...] is the {{hypothesis}} induced by learning algorithm [...] trained on training set [...] with <b>hyperparameters</b> [...] Instance hardness provides a continuous value for determining if an instance is an outlier instance.|$|E
2500|$|This {{suggests}} that we create a conditional prior of the mean on the unknown variance, with a hyperparameter specifying {{the mean of the}} pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations. [...] This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter. [...] The prior for the variance also has two <b>hyperparameters,</b> one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations. [...] Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the relative variance of that prior. [...] These are given as two separate <b>hyperparameters</b> so that the variance (aka the confidence) of the two priors can be controlled separately.|$|E
50|$|Levy et al. (2015) {{show that}} much of the {{superior}} performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific <b>hyperparameters.</b> Transferring these <b>hyperparameters</b> to more 'traditional' approaches yields similar performances in downstream tasks.|$|E
40|$|The kernel-based regularization {{method has}} two core issues: kernel design and <b>hyperparameter</b> estimation. In this paper, {{we focus on}} the second issue and study the {{properties}} of several <b>hyperparameter</b> estimators including the empirical Bayes (EB) estimator, two Stein's unbiased risk estimators (SURE) and their corresponding Oracle counterparts, with an emphasis on the asymptotic properties of these <b>hyperparameter</b> estimators. To this goal, we first derive and then rewrite the first order optimality conditions of these <b>hyperparameter</b> estimators, leading to several insights on these <b>hyperparameter</b> estimators. Then we show that as the number of data goes to infinity, the two SUREs converge to the best <b>hyperparameter</b> minimizing the corresponding mean square error, respectively, while the more widely used EB estimator converges to another best <b>hyperparameter</b> minimizing the expectation of the EB estimation criterion. This indicates that the two SUREs are asymptotically optimal but the EB estimator is not. Surprisingly, the convergence rate of two SUREs is slower than that of the EB estimator, and moreover, unlike the two SUREs, the EB estimator is independent of the convergence rate of Φ^TΦ/N to its limit, where Φ is the regression matrix and N is the number of data. A Monte Carlo simulation is provided to demonstrate the theoretical results...|$|R
40|$|We {{propose the}} use of k-determinantal point {{processes}} in <b>hyperparameter</b> optimization via random search. Compared to conventional approaches where <b>hyperparameter</b> settings are sampled independently, a k-DPP promotes diversity. We describe an approach that transforms <b>hyperparameter</b> search spaces for efficient use with a k-DPP. Our experiments show significant benefits over uniform random search in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel...|$|R
40|$|Abstract. An {{algorithm}} for objectively {{calculating the}} <b>hyperparameter</b> {{for the class}} of linearized one step EIT image reconstruction algorithms is proposed and compared to existing strategies. EIT is an ill-conditioned problem in which regularization {{is used to calculate}} a stable and accurate solution by incorporating some form of prior knowledge into the solution. A <b>hyperparameter</b> is used to control the balance between conformance to data and conformance to the prior. A remaining challenge is to develop and validate methods of objectively selecting the <b>hyperparameter.</b> In this paper evaluate an compare and evaluate five different strategies for <b>hyperparameter</b> selection. We propose a calibration based method of objective <b>hyperparameter</b> selection, called BestRes, that leads to repeatable and stable image reconstructions that are indistinguishable from heuristic selections. Results indicate: 1) heuristic selections of <b>hyperparameter</b> are inconsistent among experts, 2) Generalized Cross-Validation approaches produce under-regularized solutions, 3) L-Curve approaches are unreliable for EIT, and 4) BestRes produces good solutions comparable to expert selections. Additionally, we show {{that it is possible to}} reliably detect an inverse crime based on analysis of these parameters...|$|R
5000|$|Therefore, the {{posterior}} is (dropping the <b>hyperparameters</b> as conditioning factors): ...|$|E
50|$|For {{specific}} learning algorithms, it {{is possible}} to compute the gradient with respect to <b>hyperparameters</b> and then optimize the <b>hyperparameters</b> using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.|$|E
50|$|When using a {{conjugate}} prior, {{the posterior}} distribution {{will be from}} the same family, but will have different <b>hyperparameters,</b> which reflect the added information from the data: in subjective terms, one's beliefs have been updated. For a general prior distribution, this is computationally very involved, and the posterior may have an unusual or hard to describe form, but with a conjugate prior, there is generally a simple formula relating {{the values of the}} <b>hyperparameters</b> of the posterior to the values of the <b>hyperparameters</b> of the prior, and thus the computation of the posterior distribution is very easy.|$|E
40|$|Abstract. We {{present a}} {{technique}} to extend Robust Soft Learning Vector Quantization (RSLVQ). This algorithm is derived from an explicit cost function and follows the dynamics of a stochastic gradient ascent. The RSLVQ cost function involves a <b>hyperparameter</b> which is kept fixed during training. We propose to adapt the <b>hyperparameter</b> based on the gradient information. Experiments on artificial and real life data show that the <b>hyperparameter</b> crucially influences the performance of RSLVQ. However, {{it is not possible}} to estimate the best value from the data prior to learning. We show that the proposed variant of RSLVQ is very robust with respect to the choice of the <b>hyperparameter.</b> ...|$|R
30|$|In this paper, we {{describe}} simple substitutional methods, i.e., the coordinate-search and Nelder-Mead methods, for <b>hyperparameter</b> optimization in deep learning. To {{the best of}} our knowledge, no report has examined the application of these methods to <b>hyperparameter</b> optimization.|$|R
30|$|Bayesian {{optimization}} {{is one of}} {{the most}} remarkable <b>hyperparameter</b> optimization methods in recent years. Its base concept was proposed in the 1970 s; however, it has been significantly improved since then due to the attention paid to DNN <b>hyperparameter</b> optimization.|$|R
50|$|Different model {{training}} algoritms require different <b>hyperparameters,</b> some simple algoritms (as {{ordinary least squares}} regression) require none. Given these <b>hyperparameters,</b> the training algorithm learns the parameters from the data. E.g. LASSO is an algorithm that adds a regularization hyperparameter to OLS regression, {{that has to be}} set before estimating the parameters through the training algorithm.|$|E
5000|$|... α and β are {{parameters}} of the prior distribution (beta distribution), hence <b>hyperparameters.</b>|$|E
5000|$|One {{can think}} of {{conditioning}} on conjugate priors as defining a kind of (discrete time) dynamical system: from a given set of <b>hyperparameters,</b> incoming data updates these <b>hyperparameters,</b> so {{one can see the}} change in <b>hyperparameters</b> as a kind of [...] "time evolution" [...] of the system, corresponding to [...] "learning". Starting at different points yields different flows over time. This is again analogous with the dynamical system defined by a linear operator, but note that since different samples lead to different inference, this is not simply dependent on time, but rather on data over time. For related approaches, see Recursive Bayesian estimation and Data assimilation.|$|E
40|$|<b>Hyperparameter</b> {{learning}} {{has traditionally been}} a manual task because of {{the limited number of}} trials. Today’s computing infrastructures allow bigger evaluation budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based optimization was successfully applied to <b>hyperparameter</b> learning for deep belief networks and to WEKA classifiers. The methods combined brute force computational power with model building about the behavior of the error function in the <b>hyperparameter</b> space, and they could significantly improve on manual <b>hyperparameter</b> tuning. What may make experienced practitioners even better at <b>hyperparameter</b> optimization is their ability to generalize across similar learning problems. In this paper, we propose a generic method to incorporate knowledge from previous experiments when simultaneously tuning a learning algorithm on new problems at hand. To this end, we combine surrogate-based ranking and optimization techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms standard tuning techniques and single-problem surrogate-based optimization. Proceedings of the 30 t...|$|R
40|$|Abstract—Hyperparameter {{optimization}} {{is often}} done manu-ally or {{by using a}} grid search. However, recent {{research has shown that}} automatic optimization techniques are able to accelerate this optimization process and find <b>hyperparameter</b> configurations that lead to better models. Currently, transferring knowledge from previous experiments to a new experiment is of particular interest because {{it has been shown that}} it allows to further improve the <b>hyperparameter</b> optimization. We propose to transfer knowledge by means of an initial-ization strategy for <b>hyperparameter</b> optimization. In contrast to the current state of the art initialization strategies, our strategy is neither limited to <b>hyperparameter</b> configurations that have been evaluated on previous experiments nor does it need meta-features. The initial <b>hyperparameter</b> configurations are derived by optimizing for a meta-loss formally defined in this paper. This loss depends on the <b>hyperparameter</b> response function of the data sets that were investigated in past experiments. Since this function is unknown and only few observations are given, the meta-loss is not differentiable. We propose to approximate the response function by a differentiable plug-in estimator. Then, we are able to learn the initial <b>hyperparameter</b> configuration sequence by applying gradient-based optimization techniques. Extensive experiments are conducted on two meta-data sets. Our initialization strategy is compared to the state of the art for initialization strategies and further methods that are able to transfer knowledge between data sets. We give empirical evidence that our work provides an improvement over the state of the art. I...|$|R
40|$|Abstract. Since <b>hyperparameter</b> {{optimization}} {{is crucial}} for achiev-ing peak performance with many machine learning algorithms, an active research community has formed around this problem {{in the last few}} years. The evaluation of new <b>hyperparameter</b> optimization techniques against {{the state of the art}} requires a set of benchmarks. Because such evaluations can be very expensive, early experiments are often performed using synthetic test functions rather than using real-world <b>hyperparameter</b> optimization problems. However, there can be a wide gap between the two kinds of problems. In this work, we introduce another option: cheap-to-evaluate surrogates of real <b>hyperparameter</b> optimization benchmarks that share the same hyper-parameter spaces and feature similar response surfaces. Specifically, we train regression models on data describing a machine learning algorithm’s performance under a wide range of <b>hyperparameter</b> con-figurations, and then cheaply evaluate <b>hyperparameter</b> optimization methods using the model’s performance predictions in lieu of the real algorithm. We evaluate the effectiveness for using a wide range of regression techniques to build these surrogate benchmarks, both in terms of how well they predict the performance of new configurations and of how much they affect the overall performance of hyperparame-ter optimizers. Overall, we found that surrogate benchmarks based on random forests performed best: for benchmarks with few hyperparam-eters they yielded almost perfect surrogates, and for benchmarks with more complex <b>hyperparameter</b> spaces they still yielded surrogates that were qualitatively similar to the real benchmarks they model. ...|$|R
5000|$|... the {{hyperparameter}} of the parameter distribution, i.e., [...] This {{may in fact}} be a vector of <b>hyperparameters.</b>|$|E
5000|$|... where [...] is the {{training}} data, and [...] {{is a set}} of <b>hyperparameters</b> for [...] and [...]|$|E
50|$|Hyperparameter {{optimization}} or tuning is {{the problem}} of choosing a set of optimal <b>hyperparameters</b> for a learning algorithm.|$|E
5000|$|Instead {{of using}} a single value for a given <b>hyperparameter,</b> one can instead {{consider}} a probability distribution of the <b>hyperparameter</b> itself; {{this is called a}} [...] "hyperprior." [...] In principle, one may iterate this, calling parameters of a hyperprior [...] "hyperhyperparameters," [...] and so forth.|$|R
5000|$|Hyperopt is a {{distributed}} <b>hyperparameter</b> optimization {{library in}} Python.|$|R
40|$|Abstract—While the {{prediction}} accuracy of a large-scale recommender system can generally {{be improved by}} learning from more and more training data over time, {{it is unclear how}} well a fixed predictive model can handle the changing business dynamics in a real-world scenario. The adjustment of a predictive model is controlled by the <b>hyperparameter</b> settings of a selected algorithm. Although the problem of <b>hyperparameter</b> optimization has been studied for decades in various disciplines, the adaptiveness of the initially selected model is not as well understood. This paper presents an approach to continuously re-select <b>hyperparameter</b> settings of the algorithm in a large-scale retail recommender system. In particular, an automatic <b>hyperparameter</b> optimization tech-nique is applied on collaborative filtering algorithms in order to improve prediction accuracy. Experiments have been conducted on a large-scale real retail dataset to challenge traditional assumption that a one-off initial <b>hyperparameter</b> optimization is sufficient. The proposed approach has been compared with a baseline approach and a widely used approach with two scalable collaborative filtering algorithms. The evaluations of our experiments are based on a 2 -year real purchase trans-action dataset of a large retail chain business, both its online e-commerce site and its offline retail stores. It is demonstrated that continuous <b>hyperparameter</b> optimization can effectively improve {{the prediction}} accuracy of a recommender system. This paper presents a new direction in improving the prediction performance of a large-scale recommender system...|$|R
50|$|Three <b>hyperparameters</b> {{control the}} size of the output volume of the {{convolutional}} layer: the depth, stride and zero-padding.|$|E
50|$|A hyperprior is a {{distribution}} {{on the space}} of possible <b>hyperparameters.</b> If one is using conjugate priors, then this space is preserved by moving to posteriors - thus as data arrives, the distribution changes, but remains on this space: as data arrives, the distribution evolves as a dynamical system (each point of hyperparameter space evolving to the updated <b>hyperparameters),</b> over time converging, just as the prior itself converges.|$|E
5000|$|It {{is often}} useful {{to think of}} the <b>hyperparameters</b> of a {{conjugate}} prior distribution as corresponding to having observed a certain number of pseudo-observations with properties specified by the parameters. For example, the values [...] and [...] of a beta distribution {{can be thought of as}} corresponding to [...] successes and [...] failures if the posterior mode is used to choose an optimal parameter setting, or [...] successes and [...] failures if the posterior mean is used to choose an optimal parameter setting. In general, for nearly all conjugate prior distributions, the <b>hyperparameters</b> can be interpreted in terms of pseudo-observations. This can help both in providing an intuition behind the often messy update equations, as well as to help choose reasonable <b>hyperparameters</b> for a prior.|$|E
5000|$|Harmonica is a Python 3 {{package for}} {{spectral}} <b>hyperparameter</b> optimization.|$|R
5000|$|Auto-WEKA is a Bayesian <b>hyperparameter</b> {{optimization}} {{layer on}} top of WEKA.|$|R
30|$|YO {{implemented}} the <b>hyperparameter</b> optimization methods for DNNs, performed the experiments, and drafted the manuscript. MY {{implemented the}} <b>hyperparameter</b> optimization methods for DNNs and helped perform the experiments and draft the manuscript. MO guided the work, supervised the experimental design, and helped draft the manuscript. All authors have {{read and approved}} the final manuscript.|$|R
