3|36|Public
5000|$|The local home {{channel and}} Seed List are {{programmed}} into the radio by the dealer. When a radio registers on a site, {{whether it is}} a home or roam site, it is also given an [...] "almanac" [...] of nearby neighbor site frequencies via DFA broadcast (direct frequency assignment) allowing radios the ability to Roam in large Systems bigger than the Seed List capability or when System Frequency/Channel changes have occurred without having to physically reprogram the Radio. When the currently registered site signal drops below a programmed RSSI (receive signal strength <b>indication)</b> <b>threshold,</b> it first searches the DFA almanac frequencies for a better signal. When it exhausts the almanac, it starts on a [...] "seed list" [...] which are frequencies of all or most sites in the PassPort Radio network.|$|E
40|$|The {{development}} of the Internet technology has caused telemedicine diagnosis systems to be commonly available. The most crucial aspect of the patient’s safety in such systems {{is a problem of}} safe diagnosis. There are two factors at the stake here. The first one is a human factor in the form of user; it is especially severe when the user is not a physician. The second one is the accuracy of the diagnosis process. The best way to handle possible diagnosis errors is to apply measures of sensitivity, specificity and ROC (Receiver Operating Characteristic) curve. We examine these measures on a sample diagnosis system against a real-life data. It turns out that values of these measures are strictly associated with another measure: an <b>indication</b> <b>threshold.</b> Therefore, the accuracy of diagnosis may be to a large extent determined by the chosen threshold. We propose several methods for minimizing the impact of this factor...|$|E
40|$|Background: The Transfusion <b>Indication</b> <b>Threshold</b> Reduction (TITRe 2) {{trial is}} the largest {{randomized}} controlled trial to date to compare red blood cell transfusion strategies following cardiac surgery. This update presents the statistical analysis plan, detailing how the study will be analyzed and presented. The statistical analysis plan has been written following recommendations from the International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use, prior to database lock and the final analysis of trial data. Outlined analyses {{are in line with}} the Consolidated Standards of Reporting Trials (CONSORT). Methods and design: The study aims to randomize 2000 patients from 17 UK centres. Patients are randomized to either a restrictive (transfuse if haemoglobin concentration < 7. 5 g/dl) or liberal (transfuse if haemoglobin concentration < 9 g/dl) transfusion strategy. The primary outcome is a binary composite outcome of any serious infectious or ischaemic event in the first 3 months following randomization. The statistical analysis plan details how non-adherence with the intervention, withdrawals from the study, and the study population will be derived and dealt with in the analysis. The planned analyses of the trial primary and secondary outcome measures are described in detail, including approaches taken to deal with multiple testing, model assumptions not being met and missing data. Details of planned subgroup and sensitivity analyses and pre-specified ancillary analyses are given, along with potential issues that have been identified with such analyses and possible approaches to overcome such issues. Trial registration: ISRCTN 70923932 Peer-reviewedPublisher Versio...|$|E
50|$|Evofosfamide was {{developed}} by Threshold Pharmaceuticals Inc. In 2012, Threshold signed a global license and co-development agreement for evofosfamide with Merck KGaA, Darmstadt, Germany (EMD Serono Inc. in the US and Canada), which includes an option for Threshold to co-commercialize evofosfamide in the United States. Threshold {{is responsible for the}} development of evofosfamide in the soft tissue sarcoma indication in the United States. In all other cancer <b>indications,</b> <b>Threshold</b> and Merck KGaA are developing evofosfamide together. From 2012 to 2013, Merck KGaA paid 110 million US$ for upfront payment and milestone payments to Threshold. Additionally, Merck KGaA covers 70% of all evofosfamide development expenses.|$|R
40|$|This paper {{presents}} the theoretical foundation {{and application of}} two univariate failure detection algorithms to Space Shuttle Main Engine (SSME) test firing data. Both algorithms were applied to data collected during steady-state operation of the engine. One algorithm, the time series algorithm, is based on time series techniques and involves the computation of autoregressive models. Time series techniques have been previously applied to SSME data. The second algorithm is based on standard signal processing techniques. It consists of tracking the variations in the average signal power with time. The average signal power algorithm is a newly proposed SSME failure detection algorithm. Seven nominal test firings were used to develop failure <b>indication</b> <b>thresholds</b> for each algorithm. These thresholds were tested using four anomalous firings aud one additional nominal firing. Both algorithms provided significantly earlier failure indication times than did the current redline limit system. Neither algorithm gave false failure indications for the nominal firing. The strengths and weaknesses of, the two algorithms are discussed and compared. The average signal power algorithm {{was found to have}} several advantages over the time series algorithm. A...|$|R
40|$|Traditional multiwavelet {{shrinkage}} denoising techniques {{require a}} priori knowledge of noise variance {{that may not}} be obtained in some practical situations. By using generalized cross validation (GCV), we propose in this paper a new level-dependent risk estimator for multiwavelet shrinkage that does not require such a priori information. Simulation results verify that the resulted risk estimator gives better <b>indication</b> on <b>threshold</b> selection comparing with the traditional GCV method. Improved denoising performance is then achieved particularly for higher multiplicity multiwavelet shrinkage. Department of Electronic and Information Engineerin...|$|R
40|$|Figure 2 - Determination of the {{threshold}} genetic distance for species identification. The density plot indicates transition between intra- and interspecific distances; the genetic distance corresponding to this transition (dip in the density graph, here approximately 0. 05) indicates the suitable threshold to the dataset. This method {{does not require}} prior knowledge of species identity to get an <b>indication</b> of potential <b>threshold</b> values...|$|R
30|$|The data {{collected}} across the six groups of participants and scenarios {{were expected to}} indicate the threshold of damage, as represented in the various combinations of deterioration features {{in each of the}} sets of documents. Personal and historic value scenarios (H–R, H–D, P–R, P–D) were thought to provide an <b>indication</b> of how <b>thresholds</b> of damage are influenced by the two specific value contexts. In the following section we analyse the strengths and limitations of this approach.|$|R
40|$|Food grain {{markets are}} special {{in the sense}} that farmers will, likely, be {{consumers}} of the crops they produce. This study examines smallholder market participation on the Ugandan maize market, and, furthermore, explores possible connections to rural poverty. A probit model of market participation is employed, and I work with data from a household survey conducted between 1999 and 2000. A number of variables, like literacy, hired labor, specialization, land holdings and access to storage facilities were correlated with a higher probability of market participation. Ownership of livestock was considerably more important to the poor farmers while there are also <b>indications</b> of asset <b>thresholds</b> regarding land and farming assets...|$|R
40|$|The {{hypothesis}} that the mu -e events observed at SLAC result from the production and subsequent decay of pairs of orthodox heavy muons of mass 1. 8 GeV/c/sup 2 / is difficult to reconcile with neutrino and antineutrino data. There is no <b>indication</b> of a <b>threshold</b> between CERN and Fermilab energies in the neutral to charged current ratio, and no peaking in the observed y distribution at low x for inclusive charged current events. Taken together, these observations provide evidence against heavy muons whatever their decay branching fractions. However, more data is {{needed in order to}} rule out the heavy muon interpretation completely. (15 refs) ...|$|R
40|$|In {{order to}} study the effect of long range {{interaction}} and its wire compensation experimentally, current carrying wires are installed in the CERN Super Proton Synchrotron (SPS). In this paper we summarize the main results of the 2007 wire excitation experiments at 26, 37 and 55 GeV including scans of wire current, beam-wire distance and chromaticity. A strong dependence on the chromaticity and <b>indications</b> of a <b>threshold</b> effect at 37 and 55 GeV were found. The results are compared with simulations, with a simple analytic scaling law and with experimental results from RHIC. Wire driven resonances have been observed through the Fourier spectrum of experimental BPM data and also studied in simulation...|$|R
40|$|Background: To {{produce a}} {{practice}} guideline {{that includes a}} set of detailed consensus principles regarding the prescription of antipsychotics (APs) amongst people with dementia living in care homes. Methods: We used a modified Delphi consensus procedure with three rounds, where we actively specified and optimized statements throughout the process, utilizing input from four focus groups, carried out in UK, Norway, and the Netherlands. This was done to identify relevant themes {{and a set of}} statement that experts agreed upon using the Research and Development/University of California at Los Angeles (RAND/UCLA) methodology. Results: A total of 72 scientific and clinical experts and 14 consumer experts reached consensus upon 150 statements covering five themes: (1) General prescription stipulations, (2) assessments prior to prescription, (3) care and treatment plan, (4) discontinuation, and (5) long-term treatment. Conclusions: In this practice guideline, novel information was provided about detailed <b>indication</b> and <b>thresholds</b> of symptoms, risk factors, circumstances at which APs should be stopped or tapered, specific criteria for justifying long-term treatment, involvement of the multidisciplinary team, and family caregiver in the process of prescription. The practice guideline is based on formal consensus of clinicians and consumer experts and provides clinicians relevant practical information that is lacking in current guidelines...|$|R
40|$|Denoising methods {{based on}} wavelet domain {{thresholding}} or shrinkage {{have been found}} to be effective. Recent studies reveal that multivariate shrinkage on multiwavelet transform coefficients further improves the traditional wavelet methods. It is because multiwavelet transform, with appropriate initialization, provides better representation of signals so that their difference from noise can be clearly identified. In this paper, we consider the multiwavelet denoising by using multivariate shrinkage function. We first suggest a simple second-order orthogonal prefilter design method for applying multiwavelet of higher multiplicities. We then study the corresponding thresholds selection using Stein's unbiased risk estimator (SURE) for each resolution level provided that we know the noise structure. Simulation results show that higher multiplicity wavelets usually give better denoising results and the proposed threshold estimator suggests good <b>indication</b> for optimal <b>thresholds.</b> Department of Electronic and Information Engineerin...|$|R
40|$|AbstractSensitivity to slow {{movement}} improves substantially {{during the}} early postnatal months. Thresholds were measured for slow oscillatory displacements for 68 infants aged 6, 12, 18, and 24 weeks. Standing wave line stimuli were used; 6 -week-olds were tested at 1. 2 Hz, and infants in the three older age groups were tested at either 0. 6 or 1. 2 Hz. Six-week-olds as a group were very insensitive to these slow displacements. Sensitivity increased systematically across the three older ages. Thresholds were marginally lower at 1. 2 Hz than at 0. 6 Hz, {{and there was some}} <b>indication</b> that these <b>thresholds</b> may reflect a mixture of detection by position-sensitive and motion-sensitive mechanisms. Several factors are hypothesized to be responsible for this development: (1) improvements in spatial resolution; (2) improvements in temporal contrast sensitivity; (3) decreases in the size of second order motion integration mechanisms; and (4) increased neural connectivity in the motion pathways. Copyright © 1997 Elsevier Science Lt...|$|R
40|$|Objective: The aim of {{this study}} was to test the {{implementation}} of an adaptive driver support system. Background: Providing support might not always be desirable from a safety perspective, as support may lead to problems related to a human operator being out of the loop. In contrast, adaptive support systems are designed to keep the operator in the loop as much as possible by providing support only when necessary. Method: A total of 31 experienced drivers were exposed to three modes of lane-keeping support: nonadaptive, adaptive, and no support. Support involved continuously updated lateral position feedback shown on a head-up display. When adaptive, support was triggered by performance-based indications of effort investment. Narrowing lane width and increasing density of oncoming traffic served to increase steering demand, and speed was fixed in all conditions to prevent any compensatory speed reactions. Results: Participants preferred the adaptive support mode mainly as a warning signal and tended to ignore nonadaptive feedback. Furthermore, driving behavior was improved by adaptive support in that participants drove more centrally, displayed less lateral variation and drove less outside the lane's delineation when support was in the adaptive mode compared with both the no-support mode and the nonadaptive support mode. Conclusion: A human operator is likely to use machine-triggered adaptations as an <b>indication</b> that <b>thresholds</b> have been passed, regardless of the support that is initiated. Therefore supporting only the sensory processing stage of the human information processing system with adaptive automation may not feasible. Application: These conclusions are relevant for designing adaptive driver support systems...|$|R
40|$|Pure {{cultures}} of methylotrophs and methanotrophs {{are known to}} oxidize methyl bromide (MeBr); however, their ability to oxidize tropospheric concentrations (parts per trillion by volume [pptv]) has not been tested. Methylotrophs and methanotrophs were able to consume MeBr provided at levels that mimicked the tropospheric mixing ratio of MeBr (12 pptv) at equilibrium with surface waters (≈ 2 pM). Kinetic investigations using picomolar concentrations of MeBr in a continuously stirred tank reactor (CSTR) were performed using strain IMB- 1 and Leisingeria methylohalidivorans strain MB 2 T — terrestrial and marine methylotrophs capable of halorespiration. First-order uptake of MeBr with no <b>indication</b> of <b>threshold</b> was observed for both strains. Strain MB 2 T displayed saturation kinetics in batch experiments using micromolar MeBr concentrations, with an apparent Ks of 2. 4 μM MeBr and a Vmax of 1. 6 nmol h− 1 (106 cells) − 1. Apparent first-order degradation rate constants measured with the CSTR were consistent with kinetic parameters determined in batch experiments, which used 35 - to 1 × 107 -fold-higher MeBr concentrations. Ruegeria algicola (a phylogenetic relative of strain MB 2 T), the common heterotrophs Escherichia coli and Bacillus pumilus, and a toluene oxidizer, Pseudomonas mendocina KR 1, were also tested. These bacteria showed no significant consumption of 12 pptv MeBr; thus, the ability to consume ambient mixing ratios of MeBr was limited to C 1 compound-oxidizing bacteria in this study. Aerobic C 1 bacteria may provide model organisms for the biological oxidation of tropospheric MeBr in soils and waters...|$|R
5000|$|The dissenting opinion, {{written by}} justice Brennan and joined by justice Marshall, calls the {{decision}} of the Court a “sleight of hand.”http://supreme.justia.com/us/466/210/case.html Brennan cites a number of preceding cases, including Terry v. Ohio and United States v. Mendenhall, drawing up a general rule from Mendenhall as follows; that [...] "the threatening presence of several officers, the display of a weapon by an officer, some physical touching of the person of the citizen, or the use of language or tone of voice indicating that compliance with the officer's request might be compelled” is a <b>threshold</b> <b>indication</b> of a seizure taking place.http://supreme.justia.com/us/466/210/case.html Brennan concludes that the factory sweep was very much a seizure. While there was no restraint of the individuals physically, the sudden and systematic intrusion of 15 to 25 agents upon the factory {{and the manner in which}} they dealt with the workers demonstrated a “show of force” and created an “intimidating atmosphere” with “widespread disturbance among workers”, fit to indicate that a reasonable person would not feel free to leave, and that a seizure was in fact taking place. http://supreme.justia.com/us/466/210/case.html ...|$|R
40|$|Climate risk {{assessment}} in cropping is generally undertaken in a top-down approach using climate records while critical farmer experience {{is often not}} accounted for. In the present study, set in south India, farmer experience of climate risk is integrated in a bottom-up participatory approach with climate data analysis. Crop calendars are used as a boundary object to identify and rank climate and weather risks faced by smallhold farmers. A semi-structured survey was conducted with experienced farmers whose income is predominantly from farming. Interviews {{were based on a}} crop calendar to indicate the timing of key weather and climate risks. The simple definition of risk as consequence × likelihood was used to establish the impact on yield as consequence and chance of occurrence in a 10 -year period as likelihood. Farmers’ risk experience matches well with climate records and risk analysis. Farmers’ rankings of ‘good’ and ‘poor’ seasons also matched up well with their independently reported yield data. On average, a ‘good’ season yield was 1 · 5 – 1 · 65 times higher than a ‘poor’ season. The main risks for paddy rice were excess rains at harvesting and flowering and deficit rains at transplanting. For cotton, farmers identified excess rain at harvest, delayed rains at sowing and excess rain at flowering stages as events that impacted crop yield and quality. The {{risk assessment}} elicited from farmers complements climate analysis and provides some <b>indication</b> of <b>thresholds</b> for studies on climate change and seasonal forecasts. The methods and analysis presented in the present study provide an experiential bottom-up perspective and a methodology on farming in a risky rainfed climate. The methods developed in the present study provide a model for end-user engagement by meteorological agencies that strive to better target their climate information delivery...|$|R
40|$|Peut-on évaluer la diversité floristique d’une prairie à partir d’observations simples sur le terrain ? L’élaboration d’une démarche de {{diagnostic}} adaptée à des non-spécialistes de la flore nécessite de clarifier les différentes composantes de la biodiversité d’une parcelle. La démarche de diagnostic de la diversité floristique proposée ici s’applique à l’échelle du faciès de végétation et distingue deux composantes : le fonds prairial et les éléments paysagers. Le diagnostic du fonds prairial, {{qui fait}} référence à une typologie régionale de la biodiversité, est basé sur des comptages simplifiés d’espèces, de couleurs et de formes de fleurs, sans reconnaissance botanique précise. Le diagnostic de la diversité paysagère utilise le nombre et les différents types d’éléments paysagers présents sur le faciès. Les deux diagnostics différencient chacun cinq niveaux de diversité et se déclinent pour quatre catégories de prairies auvergnates définies par leur usage et leur milieu physique. A l’issue des diagnostics, des pistes d’interprétation sont suggérées à l’utilisateur en considérant que la diversité floristique d’une prairie est fonction du milieu et des pratiques agricoles mais aussi de la diversité paysagère. An easy method {{for the evaluation}} of a pasture's floristic diversity was developed in Auvergne, making it possible for a non-botanist to assess its bio-diversity. The aim was to make people working in the sphere of agriculture aware of the floristic diversity of ordinary grasslands. It relies on qualitative and quantitative observations made on pasture plots where different vegetation facies are distinguished. In our view, the floristic diversity of a pasture depends on the swards composition, but is also enhanced by the diversity of the landscape; we therefore assessed both these factors in our approach. The selection of indicators of the bio-diversity was based on the study of 126 vegetation facies, evenly distributed over four classes of pastures differing in their type of management (grazing or mowing) and in their environment (upland, lowland with a rich or a poor soil). We made for each facies both a botanical inventory and a simplified description of the vegetation, the fauna and the landscape features. To identify indicators, we looked for linear correlations between bio-diversity, as calculated from the inventories, and the data from the simplified descriptions. Nine indicators were found for the assessment of the herbaceous vegetation's diversity : total and average number of species with a different physiognomy, colours, forms of flowers [...] . and one indicator was based on the diversity in butterflies. These indicators can be obtained on the field by a simplified counting, inside 10 circles of 1 m diameter for the flora, and along a transect for the fauna. They are {{more or less the same}} for each class, the main difference being in the identification thresholds, which are adapted to the Auvergne grasslands. The indicators of a given class were gathered in a single grid, in which five levels of bio-diversity were diagnosed. In addition to the assessment of each facies, we also propose a measure of the differences among the facies of the same plot. This point concerns only the number of species of different physiognomy. Separate grids were also designed for the assessment of the diversity of landscapes, as we did not find any correlation between the latter and the diversity of the herbaceous vegetation. Two indicators were used : the number and the types of landscape elements. The grids have also <b>indication</b> <b>thresholds</b> for each class and for the five levels of bio-diversity. Finally, various ways of interpreting the results are proposed to the users of the method, based on the hypothesis that the diversity depends not only on management and environment, but also on the landscapes. The way landscape diversity can increase and maintain the floristic diversity of grasslands is also emphasized...|$|R
3000|$|... of 2.0 in this study, this paste is {{used for}} the pore {{structure}} and reaction product studies along with a plain OPC paste. 28 -day old pastes are used for the analysis. Mercury intrusion porosimetry (MIP) is used to investigate the pore structure of these pastes. The use of MIP as an indicator of the total pore volume is well-accepted, but its use for pore size determination has several drawbacks (Diamond 2000; Moro and Böhni 2002), primarily {{because of the presence of}} “pore-throats” in cement pastes and similar systems. Mercury cannot intrude into some of the larger pores until the applied pressure is large enough to saturate the pore throats. However, an <b>indication</b> of the <b>threshold</b> pore diameter, ideally not as an absolute value, but as a parameter to facilitate comparison between specimens, can be obtained from the differential pore volume-pore diameter relationships (Atahan et al. 2009). While the resistance to high pressure could vary depending on the type of the matrix, its effects are not considered in this paper based on the fact that these matrices are shown to demonstrate similar compressive strengths.|$|R
40|$|Numerous {{studies have}} shown a {{positive}} association between daily mortality and particulate air pollution, even at concentrations below regulatory limits. These findings have motivated interest {{in the shape of}} the exposure-response relation. The authors have developed flexible modeling strategies for time-series data that include spline and threshold exposure-response models; they apply these models to daily time-series data for the 20 largest US cities for 1987 – 1994, using the concentration of particulate matter < 10 µm in aerodynamic diameter (PM) as the exposure measure. The spline model showed a linear relation without <b>indication</b> of 10 <b>threshold</b> for PM and relative risk of death for all causes and cardiorespiratory causes; by contrast, for other 10 causes, the risk did not increase until approximately 50 µg/m 3 PM. For all-cause mortality, a linear model 10 without threshold was preferred to the threshold model and to the spline model, using the Akaike information criterion (AIC). The findings were similar for cardiovascular and respiratory deaths combined. By contrast, for causes other than cardiovascular and respiratory, a threshold model was more competitive with a threshold value estimated at 65 µg/m 3. These findings indicate that linear models without a threshold are appropriate fo...|$|R
40|$|Abstract. The Boltzmann-Langevin One-Body model (BLOB), {{is a novel}} one-body {{transport}} approach, {{based on}} {{the solution of the}} Boltzmann-Langevin equation in three dimensions; it is used to handle large-amplitude phase-space fluctuations and has a broad applicability for dissipative fermionic dynamics. We study the occurrence of bifurcations in the dynamical trajectories describing heavy-ion collisions at Fermi energies. The model, applied to dilute systems formed in such collisions, reveals to be closer to the observation than previous attempts to include a Langevin term in Boltzmann theories. The onset of bifurcations and bimodal behaviour in dynamical trajectories, determines the fragment-formation mechanism. In particular, in the proximity of a threshold, fluctuations between two energetically favourable mechanisms stand out, so that when evolving from the same entrance channel, a variety of exit channels is accessible. This description gives quantitative <b>indications</b> about two <b>threshold</b> situations which characterise heavy-ion collisions at Fermi energies. First, the fusion-to-multifragmentation threshold in central collisions, where the system either reverts to a compact shape, or splits into several pieces of similar sizes. Second, the transition from binary mechanisms to neck fragmentation (in general, ternary channels), in peripheral collisions. 1...|$|R
40|$|Conf. proc. ECHIC November 6 - 8, 2013 Messina (Italy) The Boltzmann-Langevin One-Body model (BLOB), {{is a novel}} one-body {{transport}} approach, {{based on}} {{the solution of the}} Boltzmann-Langevin equation in three dimensions; it is used to handle large-amplitude phase-space fluctuations and has a broad applicability for dissipative fermionic dynamics. We study the occurrence of bifurcations in the dynamical trajectories describing heavy-ion collisions at Fermi energies. The model, applied to dilute systems formed in such collisions, reveals to be closer to the observation than previous attempts to include a Langevin term in Boltzmann theories. The onset of bifurcations and bimodal behaviour in dynamical trajectories, determines the fragment-formation mechanism. In particular, in the proximity of a threshold, fluctuations between two energetically favourable mechanisms stand out, so that when evolving from the same entrance channel, a variety of exit channels is accessible. This description gives quantitative <b>indications</b> about two <b>threshold</b> situations which characterise heavy-ion collisions at Fermi energies. First, the fusion-to-multifragmentation threshold in central collisions, where the system either reverts to a compact shape, or splits into several pieces of similar sizes. Second, the transition from binary mechanisms to neck fragmentation (in general, ternary channels), in peripheral collisions...|$|R
40|$|Abstract. Formation of {{core regions}} with reduced {{electron}} transport is reported in regimes with current profile shaping at JET. The electron heat diffusivity (χe) is reduced down to 0. 5 m 2 /s {{in the region}} of low magnetic shear with an ICRH power of 1 MW with no <b>indication</b> of a <b>threshold.</b> In the high performance optimised shear regime, obtained in scenarios dominated by ion heating, internal transport barriers on the ion temperature profiles are simultaneously accompanied by a significant reduction of the electron heat diffusivity at two-third of the plasma radius. In this regime, recent results and measurements obtained with the new gas-box divertor configuration are reported together with their transport analyses. The results indicate that χe is reduced by one order of magnitude in a spatially localised region. Formation of internal transport barriers (ITB) with dominant ion heating schemes has produced high performance plasmas in JET [1 - 2]. In order to extrapolate this regime to fusion tokamak reactors one must establish whether an ITB can be formed and sustained with mostly electron heating and low particle fuelling rates as expected in burning plasmas. To address these long term problems, current profile control experiments performed at JET using dominant electron heating schemes alone together with their analyses are reported in the first section. Then, in th...|$|R
40|$|Gelcast {{sintered}} a-Al 2 O 3 (corundum) ceramics {{were developed}} with a sub-mm grain size at densities> 99 %. Highly perfect samples {{with a minimum}} of flaws were prepared by an approach that maintains the high purity of the raw powder> 99. 99 % Al 2 O 3 throughout processing. As a consequence, all grain boundaries are free of even thinnest amorphous interface films, amorphous triple junctions are 4150 nm, and their frequency is low. Subcritical crack growth was investigated by an approach recording growth rates as low as 1013 m/s. The outstanding purity of grain boundaries gives rise to a resistance against subcritical crack growth which is similar or even below that of coarser conventional alumina ceramics. No significant promotion of subcritical crack growth by water was observed for the new gelcast high-purity ceramics with grain sizes < 1 mm, and there is no <b>indication</b> of a <b>threshold</b> KI 0 below which no crack growth would occur. The results suggest that in sintered alumina ceramics with a given purity of grain boundaries the subcritical crack-growth mechanism of stress corrosion is independent of the grain size. With their high mechanical reliability, these corundum grades are promising candidates for the use in new prostheses for joints with a high load bearing capability and with small calliper sizes. # 2002 Elsevier Science Ltd. All rights reserved...|$|R
40|$|Using multiaxial {{testing to}} {{determine}} the effects of temperature on high density polyethylene geomembrane liners Geomembranes (GMs) are increasingly being used to line containment facilities for both hazardous and municipal wastes. The most common GM in the USA is high density polyethylene (HDPE), a tough polymer showing good chemical resistance, thereby helping to protect the environment from leachate emissions from landfills. The work presented here uses a form of multiaxial testing, called the comprehensive test system (CTS), for geomembranes to perform testing in a manner reflective of the field application. The key mechanical testing distinction of the CTS is that of applying load to the sample with granular media. Liners are exposed to a wide range of temperatures from pre- to postconstruction, and under a wide range of seasonal and latitudinal conditions. This work evaluated the effect of temperature on GMs using the CTS approach. The test time required to elicit changes in the material properties as a function of elevated temperature was considerably reduced by comparison with the US Environmental Protection Agency standard tests. The data showed that the delta modulus (a measure of the material strength) decreased with increasing temperature, indicating a loss of material strength. Most importantly, an increase in temperature weakened the GM in a linear fashion throughout the range studied, with no <b>indication</b> of a <b>threshold</b> temperature...|$|R
40|$|A {{systematic}} {{study of}} electron field emission from boron nitride thin films is presented, establishing nanostructured thin film cubic boron nitride (cBN) as a robust and {{chemically inert material}} with a low effective workfunction, able to sustain electron emission in a space plasma environment. RHEED data shows the films as polycrystalline, composed of partially oriented crystallites of cBN with predominantly (001) crystallographic texture relative to the Si substrate. FTIR data showed our films to be overwhelmingly cBN, with a volume fraction greater than 75 %. AFM images show nanostructures relevant to field enhancement, with a mean feature height of 79 nm, mean RMS roughness of 19 nm, average grain size of 155 nm 2 ± 84 nm 2, and a mean feature radius of ~ 7 nm. The results {{are discussed in the}} light of current theoretical models for electron field emission, including particulars relevant to semiconductors and nanostructured surfaces. Electron emission thresholds were measured from under 1 V/μm up to just under 20 V/μm in vacuum. Voltage sweep measurements were made both in vacuo and in various gas environments relevant to space applications. Repeatability of emission results was demonstrated, albeit with <b>indications</b> of <b>threshold</b> shifts, possibly due to desorption of adsorbate impurities. Time dependence measurements at constant extraction field show stable field emission over periods of extended operation. An effective barrier height Φw of approximately 9. 3 meV for the as-grown cBN thin films is measured, based on the application of the generalised Fowler-Nordheim theory to the electron field emission measurements, and employing a model of the film surface as an ensemble of self assembled protruberances in the shape of prolate half ellipsoids of revolution on a flat surface. To our knowledge, this is the first experimental determination of this important parameter for cBN films. It appears that the low value of Φw measured for cBN is a direct consequence of the wide gap nature of the band structure, and is evidence in favor of an NEA-type of emission mechanism in cBN. Overall, the results in this study provide ample motivation for further investigations of cubic boron nitride as a promising field emission material...|$|R
40|$|Cave {{paintings}} have an almost mythic status in Western histories of art. Modernist texts charting {{the history of}} artistic progress refer to these ancient artworks as an origin point, the beginning of human self-awareness and the departure from animal contingency (Biederman, 1948; Huyghe, 1962). In particular, the ability to draw or paint other animals is taken as an <b>indication</b> that a <b>threshold</b> had been crossed; the cave artist was able to distinguish clear forms out of the “immense, crowded field of perception” (Huyghe, 1962) and to visually represent absent bodies. According to such narratives, the act of delineating the recognizable shape of an animal, marks a defining moment in which {{the figure of the}} human first emerges from the darkness of the cave. Scientific experiments conducted with chimpanzees in the twentieth century showed that non-human primates can take part in drawing and painting activities (Morris, 1962). In some of these cases it was reported that the configurations of marks produced did not constitute any recognizable depiction and remained at the level of ‘scribble’ (Schiller, 1951; Boysen et al. 1987). In contrast, The Gorilla Foundation website now declares that gorilla Koko paints ‘representationally’. In this paper I will compare {{the way in which the}} capacity to produce visual representations was treated as a marker of human/animal difference in both artistic and scientific contexts in the twentieth century and whether this is now changing. I will ask whether the distinction between representational and non-representational mark-making has any continuing relevance, or if meaning can be found in all acts of deliberate marking by animals...|$|R
40|$|In Erfurt, Germany, {{unfavorable}} {{geography and}} emissions from coal burning lead to very high ambient pollution (up to about 4000 micrograms/m 3 SO 2 in 1980 - 89). To assess possible {{health effects of}} these exposures, total daily mortality was obtained for this same period. A multivariate model was fitted, including corrections for long-term fluctuations, influenza epidemics, and meterology, before analyzing the effect of pollution. The best fit for pollution was obtained for log (SO 2 daily mean) with a lag of 2 days. Daily mortality increased by 10 % {{for an increase in}} SO 2 from 23 to 929 micrograms/m 3 (5 % quantile to 95 % quantile). A harvesting effect (fewer people die on a given day if more deaths occurred in the last 15 days) may modify this by +/- 2 %. The effect for particulates (SP, 1988 - 89 only) was stronger than the effect of SO 2. Log SP (daily mean) increasing from 15 micrograms/m 3 to 331 micrograms/m 3 (5 % quantile to 95 % quantile) was associated with a 22 % increase in mortality. Depending on harvesting, the observable effect may lie between 14 % and 27 %. There is no <b>indication</b> of a <b>threshold</b> or synergism. The effects of air pollution are smaller than the effects of influenza epidemics and are of the same size as meterologic effects. The results for {{the lower end of the}} dose range are in agreement with linear models fitted in studies of moderate air pollution and episode studies...|$|R
40|$|This article uses latent {{structure}} analysis to model ordered category ratings by multiple {{experts on the}} appropriateness of indications for the medical procedure carotid endarterectomy. The statistical method used {{is a form of}} located latent class analysis, which combines elements of latent class and latent trait analysis. It assumes that treatment indications fall into distinct latent classes, with each latent class corresponding to a different level of appropriateness. The appropriateness rating of a treatment indication by a rater is assumed determined by the latent class membership of the <b>indication,</b> rating category <b>thresholds</b> of the rater, and random measurement error. The located latent class model has two alternative forms: a normal ogive form, which derives from the assumption of normally distributed measurement error, and a logistic approximation to the normal form. The approach has the following advantages for the analysis of ordered category ratings by multiple experts: (1) it assesses whether different raters base ratings on the same or different criteria; (2) it assesses rater bias-the tendency of some raters to make higher or lower ratings than others; (3) it characterizes rater differences in rating category definitions; (4) it provides theoretically based methods for combining the ratings of different raters; and (5) it provides a description of the distribution of the latent trait. The data examined are appropriateness ratings on 848 indications for carotid endarterectomy made by nine medical experts. The located latent class approach provides unique insights concerning the data. It identifies {{what appears to be a}} set of clear nonindications for carotid endarterectomy, but a corresponding set of clear indications is not evident. The results indicate that all raters measured a common latent trait of treatment appropriateness, but that some measured the trait better than others. Rater differences in overall bias and rating category definitions are evident. Tw...|$|R
40|$|A {{summary of}} a {{critical}} review by a working group of the German commission on Air Pollution Prevention of VDI and DIN of the actual data on exposure and health effects (excluding cancer) of fine particulate air pollution is presented. Exposure: Typical ambient particle concentrations for PM 10 (PM 2. 5) in Germany {{are in the range}} of 10 - 45 (10 - 30) mug/m 3 as annual mean and 50 - 200 (40 - 150) mug/m 3 as maximum daily mean. The ratio of PM 2. 5 /PM 10 generally amounts between 0. 7 and 0. 9. Health effects: During the past 10 years many new epidemiological and toxicological studies on health effects of particulate matter (PM) have been published. In summary, long-term exposure against PM for years or decades is associated with elevated total, cardiovascular, and infant mortality. With respect to morbidity, respiratory symptoms, lung growth, and function of the immune system are affected. Short-term studies show consistant associations of exposure to daily concentrations of PM with mortality and morbidity on the same day or the subsequent days. Patients with asthma, COPD, pneumonia, and other respiratory diseases as well as patients with cardio-vascular diseases and diabetes are especially affected. The strongest associations are found for PM 2. 5 followed by PM 10, with no <b>indication</b> of a <b>threshold</b> value for the health effects. The data base for ultra fine particles is too small for final conclusions. The available toxicological data support the epidemiological findings and give hints as to the mechanisms of the effects. Conclusion: The working group concludes that a further reduction of the limit values proposed for 2005 will substantially reduce health risks due to particulate air pollution. Because of the strong correlation of PM 10 with PM 2. 5 at most German sites there is no specific need for limit values of PM 2. 5 for Germany in addition to those of PM 10...|$|R
40|$|Following {{many other}} central banks around the world, the South African Reserve Bank has adopted {{inflation}} targeting as its monetary policy framework. The {{aim of this}} is to achieve low levels of inflation in order to attain price stability thereby promoting growth. In South Africa, the chosen band to target is 3 %– 6 %. This has been criticised by many trade unions who are calling for the abandonment of inflation targeting. Despite targeting 3 %– 6 %, it is not known whether this is the optimal inflation range for South Africa. Therefore, the aim {{of this study is to}} determine the inflation threshold level for South Africa using quarterly data for the period 1983 to 2010. The first section determines whether or not there is a long-run relationship between inflation and growth using the Johansen cointegration method. Exogeneity tests determine the causality between these variables. Vector error correction models are estimated if cointegration is found. The second part determines the threshold level of inflation using the method of conditional least squares. The inflation level that maximises the R-squared value and minimises the residual sum of squares gives an <b>indication</b> of the <b>threshold</b> level. The third part of the study determines whether or not inflation volatility has a significant impact on growth. The first part established that there is long-run comovement between inflation and growth. The causality is bidirectional with both variables being endogenous. Findings regarding the threshold level show that the current inflation targeting band of 3 %– 6 % may be extended up to 9. 5 %. In addition, the range of inflation from 5. 5 % to 6. 5 % promotes economic growth in South Africa. Finally, the evidence suggests that inflation volatility does not have a significant impact on economic growth and the focus of policy should be directed towards the level of inflation as has been the case...|$|R
40|$|There {{has been}} much {{research}} which relates reduction of habitat to reduction in biodiversity anq often birds are chosen as the best indicators of these changes. In Australia studies in this area have largely focused on the effects on birds in changing rural or forested, rather than urban, landscapes. There has been little {{research in this area}} in Tasmania, yet this State has perhaps the highest proportion of original natural habitat remaining of any State in Australia. This study compared the avifauna of adjacent urban and dry sclerophyll woodland sites in the urban fringe of Hobart and found significant differences in bird species diversity between these habitats. For the purposes of this study, the woodland remnants, therefore, could be considered islands and were tested for a species-area relationship according to the principles of island biogeography. The varying size of woodland remnant, from 1 to. 3100 hectares,simulating'habitat loss' was used to study its effects on the species richness and population density of the woodland avifauna. Data were gathered by the line transect method in these woodland remnants and the results analysed by the DISTANCE software package which gives estimates of population size and density. The results were plotted as a chart of approximate population sizes of the more common 22 species of woodland bird. Depending on what is considered to be the minimum viable population size the chart could be used as an <b>indication</b> of the <b>threshold</b> remnant area of woodland required for these species. In so doing it provides a mechanism by which predictions may be made regarding reductions in populations and loss of entire species as remnants are reduced further by urban expansion. If acceptable levels of remaining biodiversity for dry sclerophyll woodland can be set, then the sort of methodology adopted in this study could be used by natural area managers to predict whether development proposals are likely to re_duce an area of habitat below an ecologically sustainable level...|$|R
40|$|This thesis {{studies the}} {{momentum}} {{effect in the}} UK stock market. The momentum effect {{is found to be}} a persistent yet not fully stable phenomenon in the UK stock market and its dynamics is at least partially conditional on the stability of the stock market. When the stock market is stable, momentum trading strategies tend to have rather reliable and good performances whereas when the stock market is in turmoil, momentum trading strategies tend to suffer losses in the near future. We construct a threshold regression model to analyse this relationship between the momentum effect and the stock market stability. We propose that there are two regimes in the short run for shares that have had extreme past performances, the momentum and the reversal regime, and that the switch from one regime to the other is governed by the stock market volatility. Our estimation results confirm this significant role of the stock market volatility. Moreover, the stock market volatility has a negative impact on a momentum trading strategy’s return in both regimes in most cases. Apart from the stock market volatility, we also find that a momentum portfolio’s ranking period return has a significant inverse relationship with its holding period return in the momentum regime, i. e., the magnitude of the momentum effect during its holding period. This negative relationship suggests that the reversal can occur in the short term even in the momentum regime when the ranking period return is sufficiently large. A new type of trading strategies is designed {{to take advantage of the}} predictability of the momentum effect dynamics, in particular, the switch between the momentum and the reversal, and our results show that they outperform momentum trading strategies with higher returns and lower risks. Indeed, following the <b>indication</b> of the <b>threshold</b> regression model, these new trading strategies can exploit not only the momentum effect but also the contrarian effect. More importantly, they are able to generate economically significant profits net of transaction costs even when momentum trading strategies fail to do so. The predictability of the dynamics of the momentum effect and the superior performance of our new trading strategies create an even bigger anomaly than the momentum effect itself in the stock market...|$|R
40|$|The aim of {{the study}} was to find {{repetitive}} behavioral patterns pre calving that could be used as indications of calving in beef cattle. In the study sixteen Standing- and lying down sensors were attached to the leg of sixteen beef cows. Ten of the sixteen beef cows gave birth to calves during the trial period. From these ten cows, data from six cows were used to assess the sensors fitness for use as calving indicators. The analytical part of the experiment was divided into two parts, calculation of divergence of mean lying time within different time intervals, and the number of lying bouts. The first part consisted of optimization of an equation used to highlight divergence in mean lying time within eight different time intervals (3, 6, 9, 12, 15, 18, 21 and 24 hours). The equation used was: z(n) = y(n) – y(n- 1) + k*z(n- 1). Five different constants (0. 1, 0. 3, 0. 5, 0. 6 and 0. 7) were tested to optimize the equation, which should give rise to as low false positive calving indications as possible among the six cows in the study. Mean lying time calculated under the longest time interval and with the highest constant gave rise to less false positive calving indications, than if mean lying time was calculated under shorter time periods and with a smaller constant. The goal of this study was to find one general threshold value for all cows that indicated calving. Since individual variation in behavioral pattern was shown to be large, the threshold value was set low, not to miss a calving. This gave rise to unacceptable high numbers of false positive calving indications for some individuals. To reduce the problem with false positive calving indications, an individually based model for detection of deviations from the cow´s normal lying down behavior would more accurate indications of calving. The second part of the study included analysis of number of lying bouts per day to find differences in the behavioral pattern that could be used to predict calving. The results indicated that this is a better way of predicting calving than measuring mean lying time under a given time interval. Although one threshold value was used for all six cows, the results were more reliable with less false positive calving <b>indications.</b> An individual <b>threshold</b> value would most probably give a more certain prediction of calving. For both methods described above a computer program for individual interspersion of the cow´s behavior could be developed, which could give the sensors a great value in detecting calving in the future...|$|R
40|$|Background In {{the past}} few decades, {{exposure}} to air pollution {{has been found to}} be associated with all-cause mortality, cardiovascular and respiratory morbidity, both in the short term (acute exposure) and the long term (chronic exposure). According to the most recent report by the World Health Organisation (WHO, 2014), 3. 7 million deaths worldwide and per year are attributed to ambient air pollution, placing it in the top 10 of risk factors. Ambient air pollution consists of particulate matter (PM) and gases, such as NO 2, SO 2, and ozone. Of all pollutants, PM is most reliably associated with human disease. It is usually classified according to particle size, with PM 10 (particles smaller than 10 µm) as the most commonly studied fraction. The European Union (EU) has set two limit values for PM 10 concentrations: annual mean levels of PM 10 must not exceed 40 μg/m³ (25 µg/m³ for PM 2. 5), and daily averages must not exceed 50 μg/m³ on more than 35 days/year, for any monitoring station in the EU member states. In contrast, the WHO advises that annual averages of PM 10 levels should not exceed 20 μg/m³ (10 µg/m³ for PM 2. 5) and that daily averages should not exceed 50 μg/m³ on more than 3 day/year. Objectives The general objective of this PhD project was to gain more insight in the relationship between PM and human health in susceptible subgroups, such as infants, lung-transplanted patients and elderly. Also, in two of the four studies conducted within the scope of this thesis, I investigated the possible biological mechanisms involved in the pathway from PM inhalation to disease. Finally, I aimed to evaluate EU limit values and WHO guidelines for ambient PM concentrations, based on our study results. Main study results In a first study (Chapter 1), I investigated effects of daily variation in environmental PM 10 on risk of infant mortality ( 50 µg/m³), during a similar 10 -day stay in Vindeln (rural area in northern Sweden, PM 10 < 10 µg/m³) and at regular time points in Leuven (reference location, PM 10 ≈ 30 µg/m³). Compared with Leuven, exposure to pollutants was higher in Milan and lower in Vindeln, with the highest contrast found for NO 2 (averages: Milan 63. 7 µg/m³; Vindeln 4. 4 µg/m³). We found strong associations between 7 -days exposure to air pollution and arterial stiffness, e. g. a 4. 4 % decrease in compliance (i. e. an increase in stiffness) for a 10 µg/m³ increment in PM 10. However, no direct inflammatory effects, measured as concentration of plasma CRP and leukocytes, were detected. Stroke, or cerebrovascular accident, is a prominent cause of mortality and it has been linked with exposure to air pollution. I performed a meta-analysis of the current literature to quantify the pooled association between stroke and long-term exposure to PM 10 or PM 2. 5 (Chapter 4). I identified 20 studies on long-term PM exposure and stroke. The association between PM and stroke was positive in North America (N= 7 studies), Europe (N= 8), and China (N= 3, with extremely high exposures), and negative in Japan (N= 2). The estimated effect of PM 2. 5 [6 % increase (95 % CI 2 - 11 %) in stroke risk for a 5 µg/m³ increment in PM 2. 5] was higher than the corresponding result using PM 10 [2 % increase (95 % CI - 2 to 7 %) in stroke risk for a 10 µg/m³ increment in PM 10). This indicates the importance of measuring PM 2. 5 directly and confirms the hypothesis that PM 2. 5 is more hazardous than the coarse fraction (PM 2. 5 - 10). Conclusions In this thesis, I found detrimental health effects of air pollution in different susceptible populations and for different time windows of exposure. Both short-term exposure (1 to 3 days) and long-term exposure (several months to years) can trigger acute events, such as stroke, lung graft rejection or infant mortality, but long-term exposure can also accelerate the development of chronic cardiovascular or respiratory diseases, such as atherosclerosis or lung cancer. The two panel studies (LTx and health elderly) provided mixed results concerning the role of inflammation in the process, as measured by levels of leukocytes and plasma CRP. Compared to other environmental factors, such as smoking, diet and physical activity, individual risk estimates are rather small, but exposure to air pollution is involuntary and ubiquitous. Given the fact that the whole population is exposed, our studies demonstrated that air pollution is an important public health issue. In Belgium (and in Western Europe in general), yearly ambient concentrations of PM 10 have decreased over the past 10 years to levels well below the EU limit value of 40 µg/m³, but still above the WHO guideline value of 20 µg/m³, especially in urban areas, where most people live. In accordance with other studies, we found no <b>indications</b> for a <b>threshold</b> or ‘save’ level of air pollution, so public health will benefit from every single µg/m³ decrease in concentration. This is a shared responsibility of policymakers, industry, and the general population (since road traffic and household combustion of wood are important and even increasing sources of air pollution) alike. VOORWOORD	 7 TABLE OF CONTENTS	 11 LIST OF ABBREVIATIONS	 13 INTRODUCTION	 17 AIR POLLUTION	 19 HEALTH EFFECTS OF AIR POLLUTION	 22 METHODS IN AIR POLLUTION RESEARCH	 30 PUBLIC HEALTH ASPECTS	 33 AIMS	 40 REFERENCES	 41 CHAPTER 1 DOES AIR POLLUTION TRIGGER INFANT MORTALITY IN WESTERN EUROPE? A CASE-CROSSOVER STUDY	 49 CHAPTER 2 LYMPHOCYTIC BRONCHIOLITIS AFTER LUNG TRANSPLANTATION IS ASSOCIATED WITH DAILY CHANGES IN AIR POLLUTION	 71 CHAPTER 3 CHANGING PLACES TO STUDY ACUTE AND SUBACUTE EFFECTS OF AIR POLLUTION ON CARDIOVASCULAR HEALTH	 93 CHAPTER 4 LONG-TERM EXPOSURE TO PARTICULATE MATTER AIR POLLUTION IS A RISK FACTOR FOR STROKE: META-ANALYTICAL EVIDENCE	 127 DISCUSSION	 165 EXPOSURE ASSESSMENT	 167 HEALTH EFFECTS OF EXPOSURE TO AIR POLLUTION	 172 PUBLIC HEALTH RELEVANCE	 176 FUTURE PERSPECTIVES AND RECOMMENDATIONS	 176 REFERENCES	 181 SUMMARY	 187 SAMENVATTING	 193 SHORT CURRICULUM VITAE AND LIST OF PUBLICATIONS	 201 nrpages: 206 status: publishe...|$|R
