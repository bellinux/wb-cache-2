88|18|Public
25|$|Wired gloves. These {{can provide}} input to the {{computer}} about the position and rotation of the hands using magnetic or inertial tracking devices. Furthermore, some gloves can detect finger bending {{with a high degree}} of accuracy (5-10 degrees), or even provide haptic feedback to the user, which is a simulation of the sense of touch. The first commercially available hand-tracking glove-type device was the <b>DataGlove,</b> a glove-type device which could detect hand position, movement and finger bending. This uses fiber optic cables running down the back of the hand. Light pulses are created and when the fingers are bent, light leaks through small cracks and the loss is registered, giving an approximation of the hand pose.|$|E
5000|$|Bug Mudra (1989-'90) for two guitars (electric and amplified-acoustic), {{electronic}} percussion, conducting <b>dataglove,</b> {{and interactive}} computer electronics ...|$|E
50|$|Smith {{then moved}} to the Thomas Lord Research Center in 1986 as a staff {{scientist}} working on intelligent object manipulation using robotic tactile sensors, pneumo-elastic and mechanical hands. There he developed a telepresence system using stereo-optics and a <b>dataglove</b> controlling a Puma-560 robot equipped with the pneumo-elastic hand.|$|E
50|$|One of {{the first}} wired gloves {{available}} to home users in 1987 was the Nintendo Power Glove. This {{was designed as a}} gaming glove for the Nintendo Entertainment System. It had a crude tracker and finger bend sensors, plus buttons on the back. The resistive sensors in the PowerGlove were also used by hobbyists to create their own <b>datagloves.</b>|$|R
40|$|In this paper, {{our purpose}} is to {{investigate}} the system architectures and thresholds to detect gestures for a gesture-based system. We developed two gesture recognition systems: DESigning In virtual Reality (DesIRe) and DRiving for disabled (DRive). DesIRe allows real-time dynamic interaction with an object to perform a number of design operations, controlling 1 LED and wearing <b>Datagloves.</b> DRive allows a quadriplegic person to control a car interface taking input from 2 LEDs and shoulder movements only, wearing a garment with 2 LEDs. We developed an interface using a stereoscopic display (VISOR) and various motion capture systems integrating Vizard Virtual Reality Toolkit, an optical tracking system, <b>datagloves,</b> and a sensor-based jacket. In this paper, we describe a generic system architecture for gesture detection and present an approach to represent the finger states using binary digits. We have found this approach is sufficient enough and have not detected any specific delays in the detection of gestures using different input systems. 15 page(s...|$|R
40|$|The Electric Field Sensors[1][5] {{developed}} by the Physics Group of the MIT Media Lab enable a new style of interaction with technology that inherits {{some of the best}} features of the two most commonly discussed styles, interaction " and computing. " The former, which is associated with terms like virtual reality and equipment such as <b>DataGloves</b> and head-mounted displays, aims to replace the user's surroundings, by inserting him o...|$|R
50|$|Moondust is also {{considered}} to be the first interactive music publication, and it sold quite successfully. With the profits from Moondust and additional funding from Marvin Minsky, Lanier formed VPL which would later go on to create the <b>DataGlove</b> and the DataSuit and {{to become one of the}} primary innovators of virtual-reality research and development throughout the 1980s.|$|E
50|$|The Power Glove {{is based}} on the {{patented}} technology of the VPL <b>Dataglove,</b> but with many modifications that allow it to be used with modestly performing consumer hardware and sold at an affordable price. Whereas the <b>Dataglove</b> can detect yaw, pitch and roll, uses fiberoptic sensors to detect finger flexure, and has a resolution of 256 positions (8 bits) per finger for four fingers (the little finger is not measured to save money, and it usually follows the movement of the ring finger), the Power Glove can only detect roll, and uses sensors coated with conductive ink yielding a resolution of four positions (2 bits) per finger for four fingers. This allows the Power Glove to store all the finger flexure information in a single byte. However, it appears that the fingers actually feed an analog signal to the microprocessor on the Power Glove. The microprocessor converts the analog signal into two bits per finger.|$|E
50|$|Much of Fisher's {{career has}} focused on {{expanding}} the technologies and creative potentials of virtual reality. Between 1985 and 1990, he was founding Director of the Virtual Environment Workstation Project (VIEW) at NASA's Ames Research Center. They attempted to develop a simulator to enable space station maintenance rehearsal. The gloves and goggles often associated with virtual reality were developed there, along with the <b>dataglove,</b> head-coupled displays and 3D audio.|$|E
5000|$|He {{was born}} in The Hague, The Netherlands. He Studied Fine Arts at the Royal Academy of Art and for some years Cultural Sciences and Philosophy. After {{painting}} for many years, he started in 1985 the Weightless Sculpture Project. [...] In 1990 he began to use the Internet to work with virtual reality, such as head-mounted displays and <b>Datagloves</b> with tactile feedback, at The Technical University in Delft.|$|R
40|$|Virtual Reality (VR) {{has progressed}} {{significantly}} since its conception, enabling previously impossible {{applications such as}} virtual prototyping, telepresence, and augmented reality However, text-entry remains a difficult problem for immersive environments (Bowman et al, 2001 b, Mine et al, 1997). Wearing a head-mounted display (HMD) and <b>datagloves</b> affords a wealth of new interaction techniques. However, users no longer have access to traditional input devices such as a keyboard. Although VR allows for more natural interfaces, {{there is still a}} need for simple, yet effective, data-entry techniques. Examples include communicating in a collaborative environment, accessing system commands, or leaving an annotation for a designer m an architectural walkthrough (Bowman et al, 2001 b). This thesis presents the design, implementation, and evaluation of a predictive text-entry technique for immersive environments which combines 5 DT <b>datagloves,</b> a graphically represented keyboard, and a predictive spelling paradigm. It evaluates the fundamental factors affecting the use of such a technique. These include keyboard layout, prediction accuracy, gesture recognition, and interaction techniques. Finally, it details the results of user experiments, and provides a set of recommendations for the future use of such a technique in immersive environments...|$|R
40|$|In most vehicle simulators, {{complete}} physical mockups {{equipped with}} steering wheel, gearshift and pedals are required. These devices are essential {{in trying to}} simulate real conditions, but as a drawback the system becomes expensive, huge (non mobile), and limited to reflect changes on the vehicle type, dimensions, or interior design. We have implemented different configurations for an immersive vehicle simulator, ranging {{from the use of}} a real mock-up equipped with force-feedback steering wheel, gearshift and pedals, to fully virtual control metaphors, which are based only on trackers and <b>datagloves.</b> We propose such fully virtual control metaphors as an alternative to minimize the use of physical devices in simulators...|$|R
50|$|The Power Glove was {{originally}} released in 1989. Though {{it was an}} officially licensed product, Nintendo {{was not involved in}} the design or release of this accessory. Rather, it was designed by Grant Goddard and Samuel Cooper Davis for Abrams/Gentile Entertainment (AGE), made by Mattel in the United States and PAX in Japan. Additional development was accomplished through the efforts of Thomas G. Zimmerman and Jaron Lanier, a virtual reality pioneer responsible for codeveloping and commercializing the <b>DataGlove,</b> who had made a failed attempt at a similar design for Nintendo earlier.Mattel brought in Image Design and Marketing's Hal Berger and Gary Yamron to develop the raw technology into a functional product. They designed Power Glove over the course of eight weeks. The Power Glove and <b>DataGlove</b> were based on Zimmerman's instrumented glove. Zimmerman built the first prototype that demonstrated finger flex measurement and hand position tracking using a pair of ultrasonic transmitters. His original prototype used optical flex sensors to measure finger bending which were replaced with less expensive carbon-based flex sensors by the AGE team.|$|E
5000|$|For novice users, pie menus {{are easy}} {{because they are}} a self-revealing gestural interface: They show {{what you can do}} and direct you how to do it. By {{clicking}} and popping up a pie menu, looking at the labels, moving the pointer in the desired direction, then clicking to make a selection, users learn the menu and practice the gesture to [...] "mark ahead" [...] ("mouse ahead" [...] {{in the case of a}} mouse, [...] "wave ahead" [...] in the case of a <b>dataglove).</b> With a little practice, it becomes quite easy to mark ahead even through nested pie menus.|$|E
5000|$|Wired gloves. These {{can provide}} input to the {{computer}} about the position and rotation of the hands using magnetic or inertial tracking devices. Furthermore, some gloves can detect finger bending {{with a high degree}} of accuracy (5-10 degrees), or even provide haptic feedback to the user, which is a simulation of the sense of touch. The first commercially available hand-tracking glove-type device was the <b>DataGlove,</b> a glove-type device which could detect hand position, movement and finger bending. This uses fiber optic cables running down the back of the hand. Light pulses are created and when the fingers are bent, light leaks through small cracks and the loss is registered, giving an approximation of the hand pose.|$|E
40|$|In this paper, we {{introduce}} two {{input devices}} for wearable computers, called GestureWrist and GesturePad. Both devices allow users {{to interact with}} wearable or nearby computers by using gesture-based commands. Both {{are designed to be}} as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike <b>DataGloves</b> or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance...|$|R
40|$|This work {{presents}} {{the design and}} implementation of corpus recording sessions along with some preliminary processing results. Captured modalities include speech and facial expressions but {{the focus is on}} hand gesture expressivity. Thus, this is the primary modality and is recorded using three methods: bare hands, Nintendo Wii remote controls and <b>datagloves.</b> Such a setup allows for multimodal affective analysis and potentially provide quantitative parameters for synthesis of systems affectively aware and able to convey affect, such as Embodied Conversational Agents. Additionally, comparative studies of gesture expressivity based on different recording techniques could be based on the introduced corpus. Cross cultural affective behavior issues are also incorporated since the experiment was performed in three countries i. e. Greece, Germany and Italy. 1...|$|R
40|$|This paper {{presents}} a novel calibration method for <b>datagloves</b> with many {{degrees of freedom}} 1. The goal of our method {{is to establish a}} mapping from the sensor values of the glove to the joint angles of an articulated hand that is of “high visual ” fidelity. This is in contrast to previous methods that aim at determining the absolute values of the real joint angles with high accuracy. The advantage of our method is that it can be simply carried through without the need for auxiliary calibration hardware (such as cameras), while still producing visually correct mappings. To achieve this, we developed a method that explicitly models the crosscouplings of the abduction sensors with the neighboring flex sensors. The results show that our method performs superior to linear calibration in most cases. 1...|$|R
5000|$|Cemetech [...] is a {{programming}} and hardware development group and developer community founded in 2000. Its primary software focus is calculator programming for TI and Casio graphing calculators, and its primary hardware {{focus is on}} mobile and wearable computing hardware. Among its most notable projects are the Doors CS shell for the TI-83+ series of graphing calculators, the Clove 2 <b>dataglove,</b> the Ultimate Calculator, and the CALCnet / globalCALCnet system for networking graphing calculators and connecting them to the Internet. The Cemetech website hosts tools for calculator programmers, including the SourceCoder TI-BASIC IDE and the jsTIfied TI-83+/84+ emulator. The founder of the site, Dr. Christopher Mitchell ("Kerm Martian"), began the site to showcase his personal projects, but since its early days, it has branched out {{to become one of}} the several major sites of the TI calculator hobbyist community and a source for hardware and programming development assistance. It has incubated many software and hardware projects beginning in the calculator community at its roots but including microprocessor development, general electrical engineering, desktop applications, and mobile/web applications.|$|E
5000|$|Between 1989-1992, {{she created}} Angels, the first {{immersive}} movie. The project foundation {{was laid out}} at the Visual Arts Program at MIT, employing Wavefront's Advanced Visualizer on a Silicon Graphics personal IRIS. The VR work was done at the Hitlab (University of Washington) using VPL's Virtualization interface and its Body Electric software running on the IRIS. Angels is a real-time interactive immersive movie, a kind of travel in a virtual paradise. The participant uses a VPL <b>Dataglove</b> and high-resolution HRX goggles developed by Jaron Lanier. Following Tom Furness' theory, the artwork was developed for the three senses: vision, audio and touch, though the technological restraints at the time could only implement vision, audio, and a non tactile data glove. Each user starts his/her experience {{in front of an}} odd carousel that is a passage to more VR worlds. Touching one of the three angels’ hearts in the carousel, defines the range in which the following three segments will appear. The duration of the sections varies from just about 30" [...] to 2'30". The brilliantly colourful environments are a gateway to more scenes. The angels' voices ask the users to interact with them, causing a story to open. The music was composed by Diane Thome.While at MIT, Stenger also contributed to the seminal Cyberspace First Steps edited by Michael L. Benedikt with the now famous essay [...] "Mind is a Leaking Rainbow".|$|E
40|$|Abstract—Glove based device has outstandingly archive {{successfully}} in nowadays especially in industries, virtual reality, and medical. <b>DataGlove</b> is an interactive device, resembling a glove worn {{on the hand}} used to capture physical data such as bending of fingers. GloveMAP are recent <b>dataglove</b> prototype aimed at obtaining signal information of finger movements. By manipulating data outputted from the <b>dataglove,</b> numerous applications {{for the purpose of}} HCI could be designed. Input data from <b>dataglove</b> usually continues with time domain which resulting the algorithm is too large to be processed. Thus, <b>dataglove</b> input data need to be transformed into a representation set of features. Transforming the input data into the set of features is called feature extraction. In this research, feature extraction technique is proposed to represent bending signal outputted from the <b>dataglove.</b> Polynomial regression is utilized as the feature extractor. The experimental results show that the parameter from the polynomial could be used as features for the bending signal of the <b>dataglove...</b>|$|E
40|$|We {{address the}} issue of 3 D hand gesture {{analysis}} by monoscopic vision without body markers. A 3 D articulated model is registered with images sequences. We compare several registration evaluation functions (edge distance, non-overlapping surface) and optimisation methods (Levenberg-Marquardt, downhill simplex and Powell). Biomechanical constraints are integrated into the minimisation algorithm to constrain registration to realistic postures. Results on image sequences are presented. Potential application include hand gesture acquisition and human machine interface. Keywords : gesture analysis, hand tracking, model registration, cost function, optimisation, biomechanical constraints. 1 Introduction Gestures are a natural way of communication for people. They also can be used for human-machine communication [1]. Unfortunately, <b>datagloves</b> are expensive, fragile and limit natural interaction. Therefore, gesture analysis by artificial vision is a topic of interest. There are two d [...] ...|$|R
40|$|We {{address the}} issue of 3 D hand gesture {{modelling}} given only one camera input and without body markers. A 3 D articulated model of the hand is first adjusted to the user's hand morphology with respect to anthropometric constraints. Then it is registered with image sequences by minimising an error function. Several functions (edge distances, non-overlapping surface) and optimisation methods (LevenbergMarquardt, downhill simplex and Powell) are compared. Biomechanical constraints are integrated into the minimisation algorithm to force registration to realistic postures. Results on hand gesture image sequences are finally presented. Potential target applications include SNHC coding of human movements, virtual character animation, human-machine interaction and sign language recognition. 1. INTRODUCTION Gestures are a natural way of communication for people. They can also be used for humanmachine communication [1]. <b>Datagloves</b> are commonly used as input devices but expensive and fragile, an [...] ...|$|R
40|$|Introduction The Electric Field Sensors[1][5] {{developed}} by the Physics Group of the MIT Media Lab enable a new style of interaction with technology that inherits {{some of the best}} features of the two most commonly discussed styles, "immersive interaction" and "ubiquitous computing. " The former, which is associated with terms like virtual reality and equipment such as <b>DataGloves</b> and head-mounted displays, aims to replace the user's surroundings, by inserting him or her into a simulated environment. In the latter, computational power is drawn out of "the box" and into the user's environment, by sprinkling a real space with intelligent objects such as point-like tags and planar tablets. Electric Field Sensors enable objects to "activate" their surroundings, creating magical 3 d-regions that are simultaneously part of the user's ordinary environment and part of a virtual environment. To contrast the various styles of interaction, we note that the central metaphor of immersive inter...|$|R
40|$|A system able {{to control}} the Utah/MIT hand with the VPL <b>DataGlove</b> has been developed. To get the actual joint angles from the <b>DataGlove</b> sensor values, a least-squares fit is used to find the best-fit {{exponential}} curve for each sensor, and then the correlation between the sensors is reduced by the iterative correlation elimination procedure. The calibration depends both on the wearer and the particular <b>DataGlove</b> being used. The first-level calibration is simple and can be done under 15 min with experience. The second level is fixed and requires no adjustments. To control the hand, a mapping from the <b>DataGlove</b> angles to the hand angles is applied, making the hand fingertips follow the <b>DataGlove</b> fingertips. The hand can successfully implement various high-level tasks under the <b>DataGlove</b> wearer's control...|$|E
40|$|Abstract- This paper {{presents}} {{an application of}} a fuzzy rule-based aggregation to a <b>dataglove</b> for the recognition of gestures. The fuzzy glove is a <b>dataglove</b> that has fuzzy sensor functionalities. The approach used for the definition of numerical to linguistic conversion, and for {{the definition of the}} sets of rules is discussed...|$|E
40|$|In {{this paper}} {{the design and}} {{construction}} of a novel wireless <b>Dataglove</b> based on new flexible goniometric sensor technology is described. The device {{is characterized by a}} low cost and rugged construction and no requires calibration before its use. Indeed, the sensors used are purely goniometric, so they are not sensible to dimensions of the user's hand. The <b>Dataglove</b> can measure the angular displacement of the fingers hand using 11 sensors, each sensor has a resolution of 0. 2 degrees, with 3 degree of accuracy in the worst case. The communication between the <b>Dataglove</b> and its computer Host is carried out using a 2, 4 gigahertz wireless Bluetooth radio protocol, in a guaranteed range up to 10 meters with a refresh rate of 100 Hz...|$|E
40|$|We have prototyped an {{environment}} for designing and performing with virtual musical instruments. It enables a sound artist or musician {{to design a}} musical instrument according {{to his or her}} wishes with respect to gestural and musical constraints. Sounds can be created, edited or performed by changing parameters like position, orientation and shape of a virtual input device, that can only be perceived through its visual and acoustic representations. We implemented the environment by extending a realtime, visual programming language called Max/FTS, with software objects to interface <b>datagloves</b> and 6 DOF sensors and to compute human movement and virtual object features. Our pilot studies involved a virtual input device with a behaviours of a rubber balloon and a rubber sheet for the control of sound spatialization and timbre parameters. It was found that the more physical analogies are used for a mapping the faster the mapping can be learned. We also found that we need to address more mani [...] ...|$|R
40|$|Rapid and {{accurate}} acquisition of human hand motion is fundamental {{requirement of the}} master-slave grasp planning for dexterous robotic hand applications. Although number of data capture systems in which <b>datagloves</b> were included have been developed, {{there are still some}} problems remaining, such as the low accuracy and the high cost of complex hardware systems. This paper explores integration of the computer vision technology for dynamic recognition of the human fingertip positions. The binocular stereo computer vision system has been developed to record, capture and recognize dynamic positions of human fingertips when human fingers move continuously while accomplishing various tasks. The recognition algorithm has been developed based on the careful selection of fingertip features, and assumption that human fingers move with approximately constant velocity. In order to solve the problem caused by the too much varied velocity, the characteristics of human hand along with previously recorded human hand motion are considered. The model has been validated based on, commonly used open and close action of human hand. The experimental results show that the recognition time is within 0. 4 ms, whereas the image collection time cycle is 40 ms, which means the recognition process is fully real-time...|$|R
40|$|With the {{evolution}} of prosthetics through years, sophisticated devices, fitted to each individual’s needs, are capable to return amputees to the everyday-life they desire to have or were habituated to. Moreover, the astonishing progress {{in the field of}} computer vision has led to amazing results. Taking these as a starting point, this work focuses on exploiting a new approach to estimate the motion of the hand based on the deformations of the surface of the arm due to muscle movements. In a phychophysical experiment with ten healthy subjects, a camera captures images of the forearm, which are further processed to extract features. Visual fiducial markers, AprilTags, are created from an ordinary printer and placed on the subjects’ forearm. The AprilTag detection software computes the precise 3 D position, orientation, and identity of the tags relative to the camera. As ground truth, a visual stimulus was used, avoiding the need for finger sensors (force/position sensors, <b>datagloves,</b> etc.). With the assistance of two regression models, one linear and one nonlinear, we can then predict the motion of the fingers. Since we seek to detect motion of tendons and muscles in the forearm, this project addresses to below- elbow amputees with some remaining musculoskeletal activity. The results are highly promising, showing an average normalized root mean square error in the range of 0. 05 to 0. 22...|$|R
40|$|Part 1 : Long and Short PapersInternational audienceA force-feedback Phantom device, a {{custom-built}} vibrotactile <b>dataglove,</b> and {{embossed paper}} sheets are compared to detect different textures. Two types of patterns are used, one formed by different geometrical shapes, {{and the other}} with different grooves width. Evaluation shows that the vibrotactile <b>dataglove</b> performs better in the detection of textures where the frequency of tactile stimuli varies, and it is even useful to detect more complex textures...|$|E
40|$|Abstract – Nowadays, {{through the}} {{advancement}} of science and technology, possibility of human finger provide information into computer is no longer question. Fingers movement and hand motion continuously being center of research in human computer interaction (HCI) and robotic controls. Using self-develop <b>DataGlove,</b> an experiment was conducted by using motion capture System (MOCAP) equipped with five motion capture cameras to capture human finger movements. The {{purpose of this paper}} is to analyze voltage output from <b>DataGlove</b> and angle obtains from motion capture system while constructing relationship concerning both outcomes. Polynomial equation is considered toward the construction of fitting curve line in scatter data. Through the end of project, differences between finger graphs slopes will be clarify. Preliminary result of experiment exposed the newly develop <b>DataGlove</b> output might closely relate into angle of finger bending movement...|$|E
40|$|A two phase {{effort was}} {{conducted}} to assess the capabilities and limitations of the <b>DataGlove,</b> a lightweight glove input device that can output signals in real-time based on hand shape, orientation, and movement. The first phase was a period for system integration, checkout, and familiarization in a virtual environment. The second phase was a formal experiment using the <b>DataGlove</b> as input device to control the protoflight manipulator arm (PFMA) - a large telerobotic arm with an 8 -ft reach. The first phase was used to explore and understand how the <b>DataGlove</b> functions in a virtual environment, build a virtual PFMA, and consider and select a reasonable teleoperation control methodology. Twelve volunteers (six males and six females) participated in a 2 x 3 (x 2) full-factorial formal experiment using the <b>DataGlove</b> to control the PFMA in a simple retraction, slewing, and insertion task. Two within-subjects variables, time delay (0, 1, and 2 seconds) and PFMA wrist flexibility (rigid/flexible), were manipulated. Gender served as a blocking variable. A main effect of time delay was found for slewing and total task times. Correlations among questionnaire responses, and between questionnaire responses and session mean scores and gender were computed. The experimental data were also compared with data collected in another study that used a six degree-of-freedom handcontroller to control the PFMA in the same task. It was concluded that the <b>DataGlove</b> is a legitimate teleoperations input device that provides a natural, intuitive user interface. From an operational point of view, it compares favorably with other 'standard' telerobotic input devices and {{should be considered in}} future trades in teleoperation systems' designs...|$|E
40|$|We have prototyped sound sculpting, an {{environment}} in which a sound artist or musician can create, edit or perform sounds by changing parameters, like position, orientation and shape of a virtual object as input device, that can only be perceived through its visual and acoustic representations. We implemented sound sculpting by extending a realtime, visual programming language called Max/FTS, which was originally designed for sound synthesis, with software objects to interface <b>datagloves</b> and 6 DOF sensors and to compute human movement and virtual object features. To date, we have performed two pilot studies, involving a virtual input device with a behaviour of a rubber balloon and another with a behaviour of a rubber sheet for the control of sound spatialization and timbre parameters. It was found that the more physical analogies are used for a mapping the faster the mapping can be learned. This requires more research for mapping virtual object features to timbre naturally. While both hands can be used for manipulation, left-hand-only sound sculpting may be a useful replacement for and extension of the standard keyboard modulation wheel. While we have succesfully implemented and tested virtual sculpting as a new real-time interaction method for sound design and musical performance, we need to address more manipulation pragmatics in the mapping and more carefully identify movement features to map to virtual object parameters...|$|R
40|$|One of {{the crucial}} {{problems}} found {{in the scientific community}} of assistive / rehabilitation robotics nowadays is that of automatically detecting what a disabled subject (for instance, a hand amputee) wants to do, exactly when she wants to do it and strictly for the time she wants to do it. This problem, commonly called intent detection, has traditionally been tackled using surface electromyography, a technique which suffers from a number of drawbacks, including the changes in the signal induced by sweat and muscle fatigue. With the advent of realistic, physically plausible augmented- and virtual-reality environments for rehabilitation, this approach does not suffice anymore. In this paper we explore a novel method to solve the problem, that we call Optical Myography (OMG). The idea is to visually inspect the human forearm (or stump) to reconstruct what fingers are moving and to what extent. In a psychophysical experiment involving ten intact subjects, we used visual fiducial markers (AprilTags) and a standard web-camera to visualize the deformations of the surface of the forearm, which then were mapped to the intended finger motions. As ground truth, a visual stimulus was used, avoiding the need for finger sensors (force/position sensors, <b>datagloves,</b> etc.). Two machine-learning approaches, a linear and a non-linear one, were comparatively tested in settings of increasing realism. The results indicate an average error in the range of 0. 05 to 0. 22 (root mean square error normalized over the signal range), in line with similar results obtained with more mature techniques such as electromyography. If further successfully tested in the large, this approach could lead to vision-based intent detection of amputees, with the main application of letting such disabled persons dexterously and reliably interact in an augmented- / virtual-reality setup...|$|R
40|$|Abstract. The {{satisfactory}} {{management of}} risk situations involves risk identification, {{the development of}} risk handling strategies and plans and the conduct and monitoring of those plans. There {{are a number of}} examples of (text-and desktop-virtual reality based) Risk Assessment systems providing a list of risk assessment measurements for contingency plan development and a matrix for risk-based scenario development. We developed a virtual reality training system, BOSS (BOrder Security Simulation) for training airport customs officers, using an immersive semicylindrical projection system (VISOR: Virtual and Interactive Simulation of Reality) in our Virtual Reality Systems (VRS) Lab to test the level of immersion. The system consists of three projectors which display the virtual world onto a 6 m wide semi-cylindrical screen canvas. The user is positioned slightly off centre towards the canvas to allow a 160 ° field of view (FOV). We use Vizard Virtual Reality Software and <b>datagloves</b> to interact with non-player characters (NPC), developing a gesture and speech based interface. Our purpose is to test the effects of immersion on the quality of learning and to determine what factors are most important and pertinent to learning in an immersive virtual environment. The factors we intend to test through controlled experimental studies include: Peripheral vision, Screen size, Stereoscopic vision, Lighting cues (more for depth perception than just immersion), Sound (mono / stereo / surround sound), Ambient Sound, Interface (keyboard mouse / data gloves / voice). We will measure the effects of these factors in engagement and learning using a number of bio feedback devices (EEG, ECG, EGG, etc.) BOSS offers a prototype platform for building, integrating and testing for further developing our ideas and other related research. In this paper, we describe the system architecture of BOSS using HLA standard...|$|R
