0|4772|Public
40|$|We {{conducted}} a reliability study comparing single <b>data</b> <b>entry</b> (SE) into a Microsoft Excel spreadsheet to entry using the existing forms (EF) {{feature of the}} Teleforms software system, in which optical character recognition is used to capture data off of paper forms designed in non-Teleforms software programs. We compared the transcription of data from multiple paper forms from over 100 research participants representing almost 20, 000 <b>data</b> <b>entry</b> <b>fields.</b> Error rates for SE were significantly lower than those for EF, so we chose SE for <b>data</b> <b>entry</b> in our study. Data transcription strategies from paper to electronic format should be chosen based on evidence from formal evaluations, and their design should be contemplated during the paper forms development stage...|$|R
50|$|HMRC's Online Filing {{software}} {{is an example}} of a program which generates iXBRL from source data. This uses a series of forms in which the key data (which will appear in XBRL tags) are entered in <b>data</b> <b>entry</b> <b>fields.</b> Additional <b>data</b> (the rest of the report and accounts) are entered in text boxes. The program generates the iXBRL report and accounts in a standard sequence of sections and a standard format. All other formatting of material is lost. While the resulting report and accounts meets HMRC's requirements, it is not an attractive document to view or read.|$|R
3000|$|... bHigh-fidelity prototypes are prototypes fully {{interactive}} {{and have}} complete functionality. ‘Users can enter <b>data</b> in <b>entry</b> <b>fields,</b> respond to messages, select icons to open windows, and, in general, {{interact with the}} user interface {{as though it were}} a real product. They are high fidelity because they represent the core functionality of the product's user interface’ ([37], p. 78).|$|R
40|$|Using {{examples}} {{from a new}} Wine Cellar Management Tool, this report describes the many spreadsheet-based analyses in this tool that can assist an individual, restaurant, or bar to manage a wine cellar. If one is disciplined about recording the inflows and outflows {{to and from the}} cellar, the spreadsheet tool will provide several cellar analyses. In addition to providing insight into the key questions of what to consume and what to promote, the tool shows such interesting and informative analyses as appellations, vintages, and types of wine. In the tool described in this report, the spreadsheet itself incorporates form-based sets of <b>data</b> <b>entry</b> <b>fields.</b> The Wine Cellar Management Tool, which is available at no charge from The Center for Hospitality Research at Cornell University, does not require actual knowledge of how to construct a spreadsheet. It does require diligent <b>data</b> <b>entry</b> regarding wine purchases and withdrawals...|$|R
40|$|Background Abstractor {{training}} {{is a key}} element in creating valid and reliable data collection procedures. The choice between in-person vs. remote or simultaneous vs. sequential abstractor training has considerable consequences for time and resource utilization. We conducted a web-based (webinar) abstractor training session to standardize training across six individual Cancer Research Network (CRN) sites for a study of breast cancer treatment effects in older women (BOWII). The goals of this manuscript are to describe the training session, its participants and participants' evaluation of webinar technology for abstraction training. Findings A webinar was held for all six sites with the primary purpose of simultaneously training staff and ensuring consistent abstraction across sites. The training session involved sequential review of over 600 data elements outlined in the coding manual in conjunction with the display of <b>data</b> <b>entry</b> <b>fields</b> in the study's electronic data collection system. Post-training evaluation was conducted via Survey Monkey©. Inter-rater reliability measures for abstractors within each site were conducted three months after the commencement of data collection. Ten of the 16 people who participated in the training completed the online survey. Almost all (90...|$|R
40|$|De-perimiterization of {{networks}} places more {{burden on the}} security of individual machines and applications • Malware increased by 200 - 300 % {{over the past year}} • More incidents of data loss could result in greater government oversight and regulation – 38 out of 50 states in US have now enacted breach disclosure laws • 2008 (ISC) Global Information Security Workforce Study (GISWS) report found significant costs result from data breaches – US $ 50 to $ 200 per record lost (not including reputation damage and loss of trust) • XSS Attacks (Ongoing) – Cross Site Scripting (XSS) is becoming the new “buffer overflow” – In 2007, XSS accounted for 80 % of documented vulnerabilities – OWASP site recommends proper web site coding practices • SQL Injection Attacks (Ongoing) – Recently several security sites were attacked using this technique – <b>Data</b> <b>entry</b> <b>fields</b> on websites are loaded with SQL commands – Bypasses the firewall and many web gateways – Input validation reduces the exposure from this attack (see OWASP Notes on SQL Injection) • Recent worms exploit patching latency – Conficker worm released 1 month after the patch from Microsoft – This exposes a flaw in patch management practices Software Vulnerabilities...|$|R
40|$|To develop {{standards}} for high quality in gastrointestinal (GI) endoscopy, the European Society of Gastrointestinal Endoscopy (ESGE) has established the ESGE Quality Improvement Committee. A prerequisite for quality assurance and improvement for all GI endoscopy procedures is state-of-the-art integrated digital reporting systems for standardized {{documentation of the}} procedures. The current paper describes the ESGE's viewpoints on the requirements for high-quality endoscopy reporting systems in GI endoscopy. Recommendations 1 Endoscopy reporting systems must be electronic. 2 Endoscopy reporting systems should be integrated into hospitals' patient record systems. 3 Endoscopy reporting systems should include patient identifiers to facilitate data linkage to other data sources. 4 Endoscopy reporting systems shall restrict the use of free-text entry to a minimum, and be based mainly on structured <b>data</b> <b>entry.</b> 5 Separate <b>entry</b> of <b>data</b> for quality or research purposes is discouraged. Automatic data transfer for quality and research purposes must be facilitated. 6 Double <b>entry</b> of <b>data</b> by the endoscopist or associate personnel is discouraged. Available data from outside sources (administrative or medical) must be made available automatically. 7 Endoscopy reporting systems shall facilitate the inclusion of information on histopathology of detected lesions, patient satisfaction, adverse events, and surveillance recommendations. 8 Endoscopy reporting systems must facilitate easy data retrieval {{at any time in}} a universally compatible format. 9 Endoscopy reporting systems must include data fields for key performance indicators as defined by quality improvement committees. 10 Endoscopy reporting systems must facilitate changes in indicators and <b>data</b> <b>entry</b> <b>fields</b> as required by professional organization...|$|R
40|$|To build a rich understanding, the {{researchers}} con ducted a 12 -month ethnographic effort {{including more than}} 200 site visits and more than 500 pages of <b>data</b> <b>entries</b> in <b>field</b> notebooks. They conducted open-ended interviews with children individually and collectively and carried out semistructured interviews. The participating children built personal documents, including narratives and images (photographs). Finally, {{the researchers}} them selves kept diaries designed to record {{a day in the}} life of a particular participant. The researchers conducted labora tory studies with factorial ANaVA designs to test, among others, the impact of computing tools (3 D vs. 2 D) and collaboration (singles vs. dyads) on the ability to transfer skills to distal-level standardized items. Such experiments demonstrated that the Quest Atlantis sof...|$|R
40|$|Community-based multi-disciplinary care of {{chronically ill}} {{individuals}} frequently requires {{the efforts of}} several agencies and organizations. The Community Care Coordination Network (CCCN) {{is an effort to}} establish a community-based clinical database and electronic communication system to facilitate the exchange of pertinent patient data among primary care, community-based and hospital-based providers. In developing a primary care based electronic record, a method is needed to update records from the field or remote sites and agencies and yet maintain data quality. Scannable <b>data</b> <b>entry</b> with fixed <b>fields,</b> optical character recognition and verification was compared to traditional keyboard <b>data</b> <b>entry</b> to determine the relative efficiency of each method in updating the CCCN database...|$|R
40|$|Abstract—To enable {{direct access}} to a memory word based on its index, {{memories}} make use of fixed-width arrays, in which a fixed number of bits is allocated for the representation of each <b>data</b> <b>entry.</b> In this paper we consider the problem of encoding <b>data</b> <b>entries</b> of two <b>fields,</b> drawn independently according to known and generally different distributions. Our goal is to find two prefix codes for the two fields, that jointly maximize {{the probability that the}} total length of an encoded <b>data</b> <b>entry</b> is within a fixed given width. We study this probability and develop upper and lower bounds. We also show how to find an optimal code for the second field given a fixed code for the first field. I...|$|R
40|$|Libraries have {{collected}} data across multiple platforms and areas and developed reports to inform their stakeholders {{to show the}} value of the library. As data collection methods have evolved, more assessment platforms have become available in the market. LibAnalytics can help you pull together an all-inclusive, real-time assessment of your library’s services. At the University of Maryland, Priddy Library, we implemented LibAnalytics in 2011 to centralize our data collection points on numerous library services. Learn how this personalized tool has helped the Priddy Library aggregate statistics on library services such as gate counts, circulation and acquisition statistics, interlibrary loan activity, reference statistics and library instruction sessions. The presentation will focus on how we developed a customized instance based on library services and the assessment needs specific to our library. Participants will brainstorm data collection points based on their library services and how to group data points logically. The presentation will discuss how to create a dataset, categorize <b>data</b> <b>entry</b> <b>fields,</b> develop questions, select data field types (i. e. numeric, single or multi option dropdown menus, or text fields), and edit existing datasets. In a few simple steps, learn how to filter your data, generate custom reports, create visualizations, analyze library usage instantly, and produce shareable dashboards to provide a comprehensive overview of your library’s metrics. Through these features, learn how your library can assess trends across multiple years, make data driven decisions to improve services and demonstrate the library’s value...|$|R
40|$|Introduction: Telepathology {{allows the}} digital {{transmission}} of images for rapid access to pathology experts. Recent technologic advances in smartphones have {{allowed them to}} be used to acquire and transmit digital images of the glass slide, representing cost savings and efficiency gains over traditional forms of telepathology. We report our experience with developing an iPhone application (App - Pocket Pathologist) to facilitate rapid diagnostic pathology teleconsultation utilizing a smartphone. Materials and Methods: A secure, web-based portal ([URL] was created to facilitate remote transmission of digital images for teleconsultation. The App augments functionality of the web-based portal and allows the user to quickly and easily upload digital images for teleconsultation. Image quality of smartphone cameras was evaluated by capturing images using different adapters that directly attach phones to a microscope ocular lens. Results: The App was launched in August 2013. The App facilitated easy submission of cases for teleconsultation by limiting the number of <b>data</b> <b>entry</b> <b>fields</b> for users and enabling uploading of images from their smartphone′s gallery wirelessly. Smartphone cameras properly attached to a microscope create static digital images of similar quality to a commercial digital microscope camera. Conclusion: Smartphones have great potential to support telepathology because they are portable, provide ubiquitous internet connectivity, contain excellent digital cameras, and can be easily attached to a microscope. The Pocket Pathologist App represents a significant reduction in the cost of creating digital images and submitting them for teleconsultation. The iPhone App provides an easy solution for global users to submit digital pathology images to pathology experts for consultation...|$|R
40|$|Presentation at the Library Technology Conference, Finding Meaning in the Madness: Unifying Your Library’s Data Collection with LibAnalytics, Saint Paul, MN, March 18, 2015 Libraries have {{collected}} data across multiple platforms and areas and developed reports to inform their stakeholders {{to show the}} value of the library. As data collection methods have evolved, more assessment platforms have become available in the market. LibAnalytics can help you pull together an all-inclusive, real-time assessment of your library’s services. At the University of Maryland, Priddy Library, we implemented LibAnalytics in 2011 to centralize our data collection points on numerous library services. Learn how this personalized tool has helped the Priddy Library aggregate statistics on library services such as gate counts, circulation and acquisition statistics, interlibrary loan activity, reference statistics and library instruction sessions. The presentation will focus on how we developed a customized instance based on library services and the assessment needs specific to our library. Participants will brainstorm data collection points based on their library services and how to group data points logically. The presentation will discuss how to create a dataset, categorize <b>data</b> <b>entry</b> <b>fields,</b> develop questions, select data field types (i. e. numeric, single or multi option dropdown menus, or text fields), and edit existing datasets. In a few simple steps, learn how to filter your data, generate custom reports, create visualizations, analyze library usage instantly, and produce shareable dashboards to provide a comprehensive overview of your library’s metrics. Through these features, learn how your library can assess trends across multiple years, make data driven decisions to improve services and demonstrate the library’s value...|$|R
40|$|Background: The {{provision}} of relevant clinical information on pathology requests {{is an important}} part of facilitating appropriate laboratory utilization and accurate results interpretation and reporting. Purpose: (1) To determine the quantity and importance of handwritten clinical information provided by physicians to the Microbiology Department of a hospital pathology service; and (2) to examine the impact of a Computerized Provider Order Entry (CPOE) system on the nature of clinical information communication to the laboratory. Methods: A multi-method and multi-stage investigation which included: (a) a retrospective audit of all handwritten Microbiology requests received over a 1 -month period in the Microbiology Department of a large metropolitan teaching hospital; (b) the administration of a survey to laboratory professionals to investigate the impact of different clinical information on the processing and/or interpretation of tests; (c) an expert panel consisting of medical staff and senior scientists to assess the survey findings and their impact on pathology practice and patient care; and (d) a comparison of the provision and value of clinical information before CPOE, and across 3 years after its implementation. Results: The audit of handwritten requests found that 43 % (n= 4215) contained patient-related clinical information. The laboratory survey showed that 97 % (84 / 86) of the different types of clinical information provided for wound specimens and 86 % (43 / 50) for stool specimens were shown to have an effect on the processing or interpretation of the specimens by one or more laboratory professionals. The evaluation of the impact of CPOE revealed a significant improvement in the {{provision of}} useful clinical information from 2005 to 2008, rising from 90. 1 % (n= 749) to 99. 8 % (n= 915) (p<. 0001) for wound specimens and 34 % (n= 129) to 86 % (n= 422) (p<. 0001) for stool specimens. Conclusion: This study showed that the CPOE system provided an integrated platform to access and exchange valuable patient-related information between physicians and the laboratory. These findings have important implications for helping to inform decisions about the design and structure of CPOE screens and what <b>data</b> <b>entry</b> <b>fields</b> should be designated or made voluntary. 9 page(s...|$|R
40|$|Abstract Background Abstractor {{training}} {{is a key}} element in creating valid and reliable data collection procedures. The choice between in-person vs. remote or simultaneous vs. sequential abstractor training has considerable consequences for time and resource utilization. We conducted a web-based (webinar) abstractor training session to standardize training across six individual Cancer Research Network (CRN) sites for a study of breast cancer treatment effects in older women (BOWII). The goals of this manuscript are to describe the training session, its participants and participants' evaluation of webinar technology for abstraction training. Findings A webinar was held for all six sites with the primary purpose of simultaneously training staff and ensuring consistent abstraction across sites. The training session involved sequential review of over 600 data elements outlined in the coding manual in conjunction with the display of <b>data</b> <b>entry</b> <b>fields</b> in the study's electronic data collection system. Post-training evaluation was conducted via Survey Monkey ©. Inter-rater reliability measures for abstractors within each site were conducted three months after the commencement of data collection. Ten of the 16 people who participated in the training completed the online survey. Almost all (90 %) of the 10 trainees had previous medical record abstraction experience and nearly two-thirds reported over 10 years of experience. Half of the respondents had previously participated in a webinar, among which three had participated in a webinar for training purposes. All rated the knowledge and information delivered through the webinar as useful and reported it adequately prepared them for data collection. Moreover, all participants would recommend this platform for multi-site abstraction training. Consistent with participant-reported training effectiveness, results of data collection inter-rater agreement within sites ranged from 89 to 98 %, with a weighted average of 95 % agreement across sites. Conclusions Conducting training via web-based technology was an acceptable and effective approach to standardizing medical record review across multiple sites for this group of experienced abstractors. Given the substantial time and cost savings achieved with the webinar, coupled with participants' positive evaluation of the training session, researchers should consider this instructional method as part of training efforts to ensure high quality data collection in multi-site studies. </p...|$|R
40|$|The rising {{incidence}} of {{type 2 diabetes}} mellitus (T 2 DM) induces severe challenges for the health care system. Our research group developed a web-based system named PANDIT that provides T 2 DM patients with insulin dosing advice using {{state of the art}} clinical decision support technology. The PANDIT interface resembles a glucose diary and provides advice through pop-up messages. Diabetes nurses (DNs) also have access to the system, allowing them to intervene when needed. The objective {{of this study was to}} establish whether T 2 DM patients can safely use PANDIT at home. To this end, we assessed whether patients experience usability problems with a high risk of compromising patient safety when interacting with the system, and whether PANDIT's insulin dosing advice are clinically safe. The study population consisted of patients with T 2 DM (aged 18 - 80) who used a once daily basal insulin as well as DNs from a university hospital. The usability evaluation consisted of think-aloud sessions with four patients and three DNs. Video data, audio data and verbal utterances were analyzed for usability problems encountered during PANDIT interactions. Usability problems were rated by a physician and a usability expert according to their potential impact on patient safety. The usability evaluation was followed by an implementation with a duration of four weeks. This implementation took place at the patients' homes with ten patients to evaluate clinical safety of PANDIT advice. PANDIT advice were systematically compared with DN advice. Deviating advice were evaluated with respect to patient safety by a panel of experienced physicians, which specialized in diabetes care. We detected seventeen unique usability problems, none of which was judged to have a high risk of compromising patient safety. Most usability problems concerned the lay-out of the diary, which did not clearly indicate which <b>data</b> <b>entry</b> <b>fields</b> had to be entered in order to obtain an advice. 27 out of 74 (36. 5 %) PANDIT advice differed from those provided by DNs. However, only one of these (1. 4 %) was considered unsafe by the panel. T 2 DM patients with no prior experience with the web-based self-management system were capable of consulting the system without encountering significant usability problems. Furthermore, the large majority of PANDIT advice were considered clinically safe according to the expert panel. One advice was considered unsafe. This could however easily be corrected by implementing a small modification to the system's knowledge bas...|$|R
40|$|Popular {{methods for}} {{probabilistic}} topic modeling like the Latent Dirichlet Allocation (LDA, [1]) and Correlated Topic Models (CTM, [2]) share an important property, i. e., using {{a common set}} of topics to model all the data. This property can be too restrictive for modeling complex <b>data</b> <b>entries</b> where multiple <b>fields</b> of heterogeneous data jointly provide rich information about each object or event. We propose a new extension of the CTM method to enable modeling with multi-field topics in a global graphical structure, and a mean-field variational algorithm to allow joint learning of multinomial topic models from discrete data and Gaussianstyle topic models for real-valued data. We conducted experiments with both simulated and real data, and observed that the multi-field CTM outperforms a conventional CTM in both likelihood maximization and perplexity reduction. A deeper analysis on the simulated data reveals that the superior performance is the result of successful discovery of the mapping among field-specific topics and observed data. ...|$|R
50|$|These {{devices are}} the {{standard}} entry method for phones {{and easy to}} understand but are a slow means for alphanumeric <b>data</b> <b>entry.</b> They may be suitable for numeric <b>entry</b> into <b>data</b> <b>fields.</b> The user enters numbers on the keypad in response to prompts on the IA screen so this method is only suitable for entry of quantifiable standard information.|$|R
40|$|Current {{information}} systems {{in use in}} the medicine require clinicians to enter <b>data</b> into structured <b>entry</b> <b>fields,</b> where the type of data needs be known and the resulting interfaces are inflexible. Natural Language Processing (NLP) {{can be used in}} this field to allow a more free and natural method for clinicians to record facts about patients. This paper describes a generic interface with the ability to automatically classify the free text. This was achieved through two approaches; creating an algorithm utilising NLP to automatically classify input text into SNOMED CT codes, and a generic way of generating interfaces to allow for handling of input data without having to put information into specific fields. A generic interface generator was written and tested with various sample interface descriptions. While the interface generation was generic the handling of the localised interface was identical to hand crafted interfaces in both style and capabilities. The NLP component was written and tested with samples taken from the medical ontology and worked to within acceptable time limits and accuracy limits for simplified input. The capabilities of the algorithm for detecting medical ontology terms are described. The generic nature of the interface generation also demonstrates how strongly localised the interfaces can be allowed for different communities without loss of processing generality. 1...|$|R
5000|$|<b>Data</b> <b>entry</b> and {{conversion}} services include: academic <b>data</b> <b>entry,</b> database {{content and}} support, survey digitization, and direct marketing support.|$|R
40|$|Routine {{capture of}} patient data for a {{computer-based}} patient record system remains {{a subject of}} study. Time constraints that require fast <b>data</b> <b>entry</b> and maximal expression power {{are in favor of}} free text <b>data</b> <b>entry.</b> However, using patient data directly for decision support systems, for quality assessment, etc. requires structured <b>data</b> <b>entry,</b> which appears to be more tedious and time consuming. In this paper, a prototype clinical <b>data</b> <b>entry</b> application is described that combines free text and structured <b>data</b> <b>entry</b> in one single application and allows clinicians to smoothly switch between these two different input styles. A knowledge base involving a semantic network of clinical <b>data</b> <b>entry</b> terms and their properties and relationships is used by this application to support structured <b>data</b> <b>entry.</b> From structured <b>data,</b> sentences are generated and shown in a text processor together with the free text. This presentation metaphor allows for easy integrated presentation of structured data and free text...|$|R
50|$|While {{this method}} of quality control clearly is not proof against {{systematic}} errors or operator misread entries from a source document, it is very useful in catching and correcting random miskeyed strokes which occur even with experienced <b>data</b> <b>entry</b> operators. However, {{it proved to be}} a fatally tragic flaw in the Therac 25 incident. This method has survived the keypunch and is available in some currently available <b>data</b> <b>entry</b> programs (e.g. PSPP/SPSS <b>Data</b> <b>Entry).</b> At least one study suggests that single-pass <b>data</b> <b>entry</b> with range checks and skip rules approaches the reliability of two-pass data entry; however, it is desirable to implement both systems in a <b>data</b> <b>entry</b> application.|$|R
5000|$|IBM 3740 <b>Data</b> <b>Entry</b> System was a <b>data</b> <b>entry</b> {{system that}} was {{announced}} by IBM in 1973. It recorded data on a Diskette, a new recording medium from IBM, for fast, flexible, efficient <b>data</b> <b>entry</b> to either high-production, centralized operations or to decentralized, remote operations. The [...] "Diskette" [...] was more commonly known as an 8-inch floppy disk, ...|$|R
40|$|Self-service {{technologies}} have been gaining increasing importance {{in public and}} private organizations over recent years. One of the predominant responsibilities of the customers in the context of self-service is to enter their data on their own. Self-service for <b>data</b> <b>entry</b> can help organizations to further enhance the efficiency of their processes and to make customer experience smoother. However, as self-service for <b>data</b> <b>entry</b> is intended to be handled without intensive employee assistance, inexp erienced <b>data</b> <b>entry</b> can lead to data quality problems. Although academic research has explored several effects of self-service technologies, there is still a lack of research investigating the effect of self-service <b>data</b> <b>entry</b> on <b>data</b> quality. Thus, in an in-depth case study in cooperation with the German Federal Employment Agency we analyzed how customer self-service for <b>data</b> <b>entry</b> affects <b>data</b> quality and found that assisted self-service <b>data</b> <b>entry</b> leads to highest data qualit...|$|R
5000|$|<b>Data</b> <b>entry</b> by geologists in {{the field}} may take less total time than {{subsequent}} <b>data</b> <b>entry</b> in the office, potentially reducing the overall time needed to complete a project.|$|R
5000|$|SuperMICAR automates the MICAR <b>data</b> <b>entry</b> process. This {{program is}} {{designed}} as an enhancement of the earlier PC-MICAR <b>Data</b> <b>Entry</b> program. Super-MICAR is designed to automatically encode cause-of-death data into numeric entity reference numbers.|$|R
40|$|Enterprise {{resource}} {{planning systems}} and computerised maintenance management systems {{are commonly used}} by organisations for handling of maintenance work orders through a graphical user interface. A work order consists {{of a number of}} data fields, such as drop-down lists, list boxes, check boxes and text <b>entry</b> <b>fields.</b> In contrast to the other data fields, the operator has the freedom to type in any text in the text <b>entry</b> <b>fields,</b> to complement and make the work order description complete. Accordingly, the text <b>entry</b> <b>fields</b> of work orders can contain any words, in any number, as necessary. Data quality is crucial in statistical analysis of work orders data, and therefore manual analysis of work orders’ text <b>entry</b> <b>fields</b> is often necessary before any decision making. However, this may be a very tedious and resource consuming process. In this article, we apply computerised analysis of text <b>entry</b> <b>fields</b> of work orders data, to study if it can bring further value in the assessment of technical assets’ performance. Keywords:Data quality, eMaintenance, maintenance, work orders, failure, decision support, natural language processingGodkänd; 2014; 20140619 (chrste) Link and effect model application through life cycle cost and return of investment analysi...|$|R
40|$|The {{objective}} {{was to determine the}} conditions under which Automatic Speech Recognition (ASR) is an efficient choice for <b>data</b> <b>entry.</b> In particular the focus was on <b>data</b> <b>entry</b> tasks that are part of constructing military messages. The ADF Formatted Messaging System utilises a structured formatting system to constrain the semantics of a message but also includes a field for unlimited and unstructured text. Hence the <b>data</b> <b>entry</b> tasks involved range from form-filling to free dictation of short phrases. In the experiments, ASR and manual input modes are compared for three <b>data</b> <b>entry</b> tasks: textual phrase entry, selection from a list, and numerical <b>data</b> <b>entry.</b> To effect fair comparisons, the tasks minimised the transaction cycle for each input mode and data type and the main comparisons use only times from correct <b>data</b> <b>entry.</b> The results indicate that for inputting short phrases ASR only competes if the typist's speed is below 45 wpm. For selecting an item from a list, ASR offered an advantage only if the list length was greater than 15 items. For entering numerical data, ASR offered no advantage over keypad or mouse. The general conclusion for formatted <b>data</b> <b>entry</b> is that a keyboard/mouse interface designed to match the data to be entered will be more time efficient than any equivalent ASR interface...|$|R
40|$|Introduction: Use of {{electronic}} health record (EHR) systems can place a considerable <b>data</b> <b>entry</b> burden upon {{the emergency department}} (ED) physician. Voice recognition <b>data</b> <b>entry</b> has been proposed as one mechanism to mitigate some of this burden; however, no reports are available specifically comparing emergency physician (EP) time use or number of interruptions between typed and voice recognition data entry-based EHRs. We designed this study to compare physician time use and interruptions between an EHR system using typed <b>data</b> <b>entry</b> versus an EHR with voice recognition. Methods: We collected prospective observational data at 2 academic teaching hospital EDs, one using an EHR with typed <b>data</b> <b>entry</b> {{and the other with}} voice recognition capabilities. Independent raters observed EP activities during regular shifts. Tasks each physician performed were noted and logged in 30 second intervals. We compared time allocated to charting, direct patient care, and change in tasks leading to interruptions between sites. Results: We logged 4, 140 minutes of observation for this study. We detected no statistically significant differences in the time spent by EPs charting (29. 4 % typed; 27. 5 % voice) or the time allocated to direct patient care (30. 7 %; 30. 8 %). Significantly more interruptions per hour were seen with typed <b>data</b> <b>entry</b> versus voice recognition <b>data</b> <b>entry</b> (5. 33 vs. 3. 47; p= 0. 0165). Conclusion: The use of a voice recognition <b>data</b> <b>entry</b> system versus typed <b>data</b> <b>entry</b> did not appear to alter the amount of time physicians spend charting or performing direct patient care in an ED setting. However, we did observe a lower number of workflow interruptions with the voice recognition <b>data</b> <b>entry</b> EHR. Additional research is needed to further evaluate the <b>data</b> <b>entry</b> burden in the ED and examine alternative mechanisms for chart entry as EHR systems continue to evolve. [West J Emerg Med. 2014; 15 (4) : 541 - 547. ]...|$|R
3000|$|<b>Data</b> <b>Entry</b> Methods: The input methods {{available}} for mobile devices {{are different from}} those for desktop computers and require a certain level of proficiency. This problem increases the likelihood of erroneous input and decreases the rate of <b>data</b> <b>entry.</b>|$|R
3000|$|... [*]“… In {{case of a}} <b>data</b> <b>entry</b> zone, a Web page could {{contain more}} than one <b>data</b> <b>entry</b> zone with {{different}} purposes. The technique could allow the verification of all these repeated zones in a particular way.” - Inspector 4.|$|R
40|$|Capturing {{clinical}} data is a multi-faceted problem. This paper discusses clinical <b>data</b> <b>entry</b> problems encountered {{during the}} development of an intelligent clinical <b>data</b> <b>entry</b> system. Based on a review of the problems, recommendations are made for an approach to the design of clinical <b>data</b> <b>entry</b> programs. These recommendations include a discussion of key components in the design process as illustrated by the development of MedIO, a C++ computer program for the entry of history and physical exam information...|$|R
40|$|Work-related {{musculoskeletal}} disorders (WMSDs) negatively impact worker’s health, ability to work, and {{their quality of}} life. Non-invasive methods for assessing the physiological responses to workload may provide information on physiological markers leading to increased risk of WMSDs. The following study aimed to evaluate the feasibility of using thermography to quantify differences in thermal readings of participants during and following a <b>data</b> <b>entry</b> task and assess the repeatability of thermal readings. Skin surface temperature measurements of the dorsal forearm were obtained from 12 participants (6 females, 6 males) during a <b>data</b> <b>entry</b> task (35 minutes) and a 30 -minute post-task period. Participants also reported their perceived forearm discomfort during <b>data</b> <b>entry</b> and recovery. Three forearm analysis regions were analyzed based on statistical findings; Upper Left, Lower Left and Right regions. Temperature trends were found to increase during <b>data</b> <b>entry</b> and decrease during recovery. The Upper Left region was warmer during both <b>data</b> <b>entry</b> and recovery phases {{in comparison to the}} other regions. Repeatability of surface temperatures, based on intraclass correlations (ICCs), was found to be fair for magnitudes and trends during <b>data</b> <b>entry,</b> and poor for magnitudes and trend...|$|R
40|$|The before/after {{study of}} {{physiological}} and biochemical parameters {{was used to}} delineate the effects of VDT <b>data</b> <b>entry</b> work on operators. Twenty-nine healthy Chinese students were chosen and divided at random into the simple and the complicated <b>data</b> <b>entry</b> group. The subjects were instructed to work as quickly and correctly as possible according to the 'Data Entry Work Programme’ for 150 min. Work performance (correct entry) was automatically recorded once every lOmin. The before/after parameters were tested respectively. The results showed that performance fluctuated over time. It decreased obviously after 50 – 60 min of work, followed by a rebound, {{and there was a}} terminal motivation phenomenon {{at the end of the}} test, which was associated with the auto-arousal and cerebral compensatory effort. Changes in physiological parameters revealed that operators were fatigued after <b>data</b> <b>entry</b> work. The adrenaline excretion in urine showed a tendency to increase after simple <b>data</b> <b>entry</b> work. The noradrenaline excretion showed a tendency to decrease after complicated <b>data</b> <b>entry</b> work. The differences in performance, diastolic blood pressure in a standing position and neurobehaviour between two groups indicated that much stress was experienced when performing complicated <b>data</b> <b>entry</b> work...|$|R
30|$|A graphic user {{interface}} is designed and implemented to make <b>data</b> <b>entry</b> easier {{for people who are}} not expert in Structured Query Language (SQL). Control mechanisms are also implemented to constrain and regulate <b>data</b> <b>entry</b> by checking errors, data completeness, and consistency.|$|R
50|$|One {{major new}} {{facility}} provided on this range was Direct <b>Data</b> <b>Entry,</b> a system comprising {{up to eight}} dedicated VDU <b>data</b> <b>entry</b> stations, with which card image files could be created; these could be assigned to a program's card reader and processed accordingly.|$|R
