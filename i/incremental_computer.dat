4|14|Public
40|$|An {{analysis}} of errors in Parallel Multiple <b>Incremental</b> <b>Computer</b> {{is done with}} specific reference {{to the influence of}} register's length. Improved polygon methods, 2 nd order Adams and 4 th order Runge-Kutta formulas have been considered; statistical {{analysis of}} both round-off and truncation errors is done by means of simulation programs. © 1974...|$|E
40|$|General {{information}} concerning the low-frequency radiometer, instrument package launching and operation, and scientific objectives of the flight are provided. Calibration curves and correction factors, with general and detailed information on the preflight calibration procedure are included. The data acquisition methods and {{the format of the}} data reduction, both on 35 mm film and on <b>incremental</b> <b>computer</b> plots, are described...|$|E
40|$|Includes bibliographical {{references}} (p. 75 - 76). As nonlinear pushover and time-history {{analyses are}} becoming more accepted for seismic bridge design to determine displacement capacities and demands, respectively, {{there is a growing}} need to verify computer results, especially for the latter. In considering displacement and acceleration of the structure over time, for example, different computer programs might give similar maximum values but have significant differences in their calculated values of acceleration or displacement at a given time. These computer programs may also differ from results obtained through a numerical approach such as the Average Acceleration Method. The bridge design engineer has no way of deciding which program or methodology is giving the better results. In this thesis, closed-form solutions for several nonlinear time-history bridge responses for undamped free vibration are developed to validate nonlinear computer programs. A bridge with segmental precast bridge columns that have unbonded post-tensioning is displaced to any displacement ductility level and then released, resulting in nonlinear free vibration of the structure. If <b>incremental</b> <b>computer</b> results do not closely match the exact solutions presented in this thesis, the program should be investigated further and, if necessary, a different program used...|$|E
40|$|Sensitivity and roundoff errors can seriously {{limit the}} {{application}} of recursive digital filters in practice, particularly when the filters have poles near z = + 1. A filter structure, based on digital <b>incremental</b> <b>computers</b> is proposed, which has low sensitivity, good error characteristics, and simple hardware implementation for pole locations close to z = + 1. Expressions for the roundoff errors are derived and compared to those for conventional structures. A design procedure is suggested to implement the new filter structure given the transfer function. Simulation results are presented...|$|R
40|$|Narrow-band digital notch filters {{have their}} poles near the unit circle. As the {{sampling}} rate is increased, the poles move towards Z = + 1. Implementing such filters requires long registers {{to overcome the}} sensitivity and roundoff errors. A filter structure based on digital <b>incremental</b> <b>computers</b> is proposed which has low sensitivity and round-off errors, and simple hardware implementation. The filter structure can be directly used on differentially pulse-code modulated signals. Hardware multipliers are not required as the poles approach z = + 1, and excellent results can be obtained using multipliers with very short word lengths, or with small size read-only memories...|$|R
40|$|Errors {{limiting}} {{the application of}} recursive digital filters which have poles near z = + 1 are considered, and a filter structure for pole locations close to z = + 1 is proposed. The filter structure, based on digital <b>incremental</b> <b>computers,</b> has low sensitivity and good error characteristics for pole locations near z = + 1. Expressions for the roundoff error are derived, and errors associated with the proposed structure and a conventional structure are compared. A design procedure is suggested for implementing the new filter structure when the transfer function is given. Simulation results are presented...|$|R
40|$|A variational {{principle}} for limit analysis of beams and pi&es is developed from a yield function {{based on the}} Frobenius matrix norm. The formulation produced a pair of maximization and minimiza-tion probIems with a duality relation between them. Exact solutions of two simple problems are presented as verification to {{the validity of the}} new variational principle. An iterative algorithm is constructed to solve the minimization problem. The algorithm, tested successfully on the two example problems, is intended for beams and plates with general ioading and boundary condi~ons and shapes. Plasticity as a model for mechanics of a class of materials presents some unusual mathema-tical difficulties that require special treatment, To correctly represent the slip-yield phenomenon of the crystalline structure, the field functions like the displacement and strain should be allowed discuntinuous behavior. The lack of a one to one constitutive relation requires inequality or incremental representations. The differential geometry of large defor-mation further introduces unpleasant nonlinearities. Great efforts have been made to over-come these difficulties, leading to some large scale <b>incremental</b> <b>computer</b> programs [l, 21. Limit analysis provides an alternative approach. It avoids the tedium of the incremental analysis and attains the limit solution directly. Earlier development of limit analysis was base...|$|E
40|$|AbstractThis research, {{through path}} analysis, {{attempts}} {{to present a}} model for predicting computer anxiety in terms of intelligence beliefs and achievement goals. To do so, 375 physical education university students (226 female & 149 male) of Tehran Islamic Azad Universities were chosen through cluster sampling. They, then, were asked to answer a questionnaire consisted of intelligence beliefs, achievement goals, and computer anxiety subscales. The results of research generally showed {{that the relationship between}} intelligence beliefs (entity and <b>incremental)</b> and <b>computer</b> anxiety is different regarding the mediating role of achievement goals. Entity intelligence beliefs through performance-avoidance goals had positive effects on computer anxiety. In contrast, incremental intelligence beliefs affected computer anxiety negatively through mastery and performance-approach goals...|$|R
40|$|A {{boundary}} integral formulation for {{the analysis}} of circular plate bending under lateral loads is developed using Green's functions. The formulation specifically applies to annular plates with arbitrary boundary conditions. The plate bending solution in the plastic range is determined using a numerical method of <b>incremental</b> loading. A <b>computer</b> program to perform the required calculations was developed and is presented. Results for three case studies are included and compared with results obtained by other methods. Plate behavior in the elastic range is in excellent agreement with other analytical solutions, and in the plastic range is in reasonable agreement with published results obtained using...|$|R
40|$|The <b>incremental</b> {{growth of}} <b>computer</b> {{networks}} caused by new users joining the network {{results in a}} confidentiality problem with the transmission of data. To solve this problem, symmetric / asymmetric key cryptography is mostly used. However, this cipher suffers from the problem of secret key maintenance and distribution across the internet which is unsecure channel. To {{get rid of this}} problem, different key exchange schemes have been developed and classified as a key redistribution scheme, key distribution scheme and key agreement scheme. In this paper, a key agreement scheme based on a multi-agent system is developed. This scheme aims to exchange the secret key between two parties. The mobile agent is the heart of this scheme where it carries the program that is responsible for generating the secret key...|$|R
40|$|In this paper, {{a method}} called 2 -stage {{extrusion}} is proposed to reconstruct a solid by modifying the incremental extrusion [Shum SSP. Solid reconstruction from orthographic opaque views using <b>incremental</b> extrusion. <b>Computers</b> & Graphics 1997; 21 (6) : 787 – 800; Shum SSP et al. A low-cost wireframe-to-solid implementation for reverse engineering. AUTOFACT' 97 Conference, 3 – 6 November 1997, MS 98 (159), 1 – 7] for translucent object domain. The algorithm has two extrusion stages. In each stage, geometric entities from only three orthogonal views (viz. top, front and right) are used. The entities involve both solid and dashed lines in each view. In the first stage, an exterior contour region in each view is swept along its normal direction {{according to the}} corresponding object dimension. As a result, three extrusion-solids are produced. Intersection of the three extrusion-solids forms a basic-solid. Next, all interior entities of each view are treated by a filtering process. If all interior entities are discarded in the filtering, the second stage will be skipped and the basic-solid becomes a solution-solid. On the other hand, if any interior entity remains after filtering, they are processed in the second stage to generate an excess-solid. In this case, the basic-solid subtracts the excess-solid to form the final three-dimensional solution-solid. Department of Industrial and Systems Engineerin...|$|R
40|$|Mathematics in Defence 2011 Abstract. We review transreal {{arithmetic}} {{and present}} transcomplex arithmetic. These arithmetics have no exceptions. This leads to <b>incremental</b> improvements in <b>computer</b> hardware and software. For example, {{the range of}} real numbers, encoded by floating-point bits, is doubled {{when all of the}} Not-a-Number(NaN) states, in IEEE 754 arithmetic, are replaced with real numbers. The task of programming such systems is simplified and made safer by discarding the unordered relational operator,leaving only the operators less-than, equal-to, and greater than. The advantages of using a transarithmetic in a computation, or transcomputation as we prefer to call it, may be had by making small changes to compilers and processor designs. However, radical change is possible by exploiting the reliability of transcomputations to make pipelined dataflow machines with a large number of cores. Our initial designs are for a machine with order one million cores. Such a machine can complete the execution of multiple in-line programs each clock tic...|$|R
40|$|This study aims {{to clarify}} the process {{conditions}} of the V-die bending of a sheet metal of steel. It provides a model that predicts not only the correct punch load for bending, but also the precise final shape of products after unloading, based on the tensile properties of the material and the geometry of the tools used. An elasto-plastic <b>incremental</b> finite-element <b>computer</b> code, based on an updated Lagrangian formulation (ULF), was developed to simulate the V-die bending of sheet metal. In particular, selective reduced integration (SRI) was adopted to formulate the stiffness matrix. The extended r-minimum technique was used {{to deal with the}} elasto-plastic state and contact problems at the tool–metal interface. A series of experiments were performed to validate the formulation in the theory, leading {{to the development of the}} computer codes. The predicted value of the punch load in the finite-element model agrees closely with the results of the experiments. The whole deformation history and the distribution of stress and strain during the forming process were obtained by carefully considering the moving boundary condition in the finite-element method. A special feature of this V-die bending process is the camber after unloading. The computer program successfully simulates this camber. The simulation was performed to evaluate the effects of the size of the blank and the bending angle on camber process. The effects of all process variables on the final bending angle of the bent parts of the sheet after unloading were also evaluated. Results in this study clearly demonstrated that the code for simulating the V-die bending process was efficient...|$|R
40|$|Static {{analyses}} and transformations {{are an important}} part of programming and domain specific languages. For example; integrated development environments analyze programs for semantic errors such as incorrect names or types to warn the programmer about these errors. Compilers translate high-level programs into programs of another language or machine code, with the purpose of executing the program. Programmers make frequent and small edits to code fragments during development, making it infeasible to do analysis of the entire program for every change. To cope with this, each change must only trigger re-analysis of the changed fragment and its dependencies while keeping a consistent knowledge base of the program. In other words, the analysis must be <b>incremental.</b> Most <b>computers</b> today have multiple CPU cores and the trend is that CPU performance will scale in the number of cores, not in the performance of the core itself. To make use of these cores, the analyses must also be executable in parallel. Traditionally, an incremental and/or parallel analysis is handcrafted for each language, requiring substantial effort. In this thesis, we present a framework for performing incremental and parallel static program {{analyses and}} transformations based on a name binding specification. If such a specification is given and the framework is used, the analyses and transformations are executed incrementally and in parallel. Additionally, name resolution is also derived from a name binding specification, reducing the implementation effort even more. To specify name binding, we present the Spoofax Name Binding Language, a declarative meta-language for the specification of name binding and scope rules, which departs from the programmatic encodings of name binding provided by regular approaches. The specification is implemented using a symbol table infrastructure that automatically traces dependencies, and a language-parametric name resolution algorithm that performs name resolution and incremental scheduling of analyses. The framework is integrated in the Spoofax Language Workbench. Several case studies have been conducted to evaluate the approach. SERGSoftware TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|A {{project was}} {{undertaken}} {{as part of}} the University of Canterbury Master of Engineering Management Programme that addressed the issue of how Orion New Zealand Limited can improve their asset capability information and associated business processes. An audit approach was used to review data quality and availability, information systems integration, management processes and information flows that would support {{the development and implementation of}} branch and cyclic/dynamic ratings. The project found the existing Orion environment does not fully support asset capability information management. Information systems are well integrated, complex and hold substantial, but incomplete asset capability data. Asset policies, plans and documentation are comprehensive, but do not specifically reference asset capability information. The project concluded implementing a full asset capability information management regime was not considered feasible at the time. However, there was merit in improving existing data acquisition, system integration and business processes to enable the development of asset branch and cyclic/dynamic ratings in order to enhance network utilisation. Recommendations focus on establishing a standard for asset capability information, improving information system solutions through further integration and refining existing business processes through <b>incremental</b> changes. Field-­‐testing, <b>computer</b> modeling and load flow analysis on the effects of cyclic/dynamic ratings were recommended to confirm the merits of full implementation...|$|R
40|$|Computer {{technologies}} have evolved very rapidly and, {{compared to other}} businesses, farm operations {{have been slow to}} adopt computer applications. This paper investigates the key characteristics of the farm operators and farm businesses that influence computer use. To that end, data from Statistics Canada's Census of Agriculture are used. The results of the logistic regression point out a trend in the adoption patterns of computer technology use. Farm operations where a computer is used in their management tend to be larger farms, have younger operators who are female, their operators work off the farm, are part-tenant/part owner operators and produce certified organic products. The impact of language spoken on the probability of adopting a computer and types of computer use varies across applications. As for provincial location of the operation, generally the Atlantic Provinces and Manitoba appear less likely to adopt the computer technology than Saskatchewan (= reference province). Finally, all other types of farm have a higher probability of adopting a computer and types of computer use than cattle operations. Although the proportion of farm businesses that have adopted a computer and the various types of computer applications is still far below 50 %, further <b>incremental</b> use of <b>computer</b> softwares, mail and internet services increases the potential for better decision making and improved efficiency in farm businesses. Computer technology, probability of adoption, Canadian farm businesses, Farm Management, Research and Development/Tech Change/Emerging Technologies,...|$|R
40|$|Incremental {{software}} {{development is the}} process of building new programs by modifying old ones. This method of programming is extremely common, and takes advantage of existing pieces of software which are known to be correct. The result is that are many software versions in existance at the same time. To date, most {{software development}} tools have addressed only one aspect of incremental software development, namely, source code control. An equally important issue is in verifying that future versions of a program have the same dynamic behaviour as the original. Currently, there are no tools which address this issue. In this paper we introduce a new concept called relative debugging, which supports the <b>incremental</b> development of <b>computer</b> programs. Relative debugging makes it possible to compare the execution of two programs with the same functionality. The programs can run on the same computer, or on different computers connected by a network. They can differ in their source programming language or in implementation details. One of the programs usually serves as a reference version which produces the desired results. The execution of the other process is compared to the reference version in order to find discrepancies. We describe basic primitives for relative debugging and their implementation in a debugger called Guard. Guard is built on top of our portable debugging interface which provides a debugging platform for heterogeneous environments. The utility of Guard is illustrated by a practical example, in which a program written in Fortran is used to find errors in a version of the same code written in the C programming language...|$|R
40|$|Humans can {{effortlessly}} see {{and interpret}} the world around them. Yet, the development of computer vision systems capable of doing the same is an extremely challenging task. In developing vision systems, computer vision experts should explicitly express the knowledge required to guide the systems. The complex nature of vision systems however means that vision experts are only quasiexperts. They lack a strong understanding of the specific vision task {{or the ability to}} articulate their knowledge. Although some approaches automatically derive this knowledge using pattern recognition and machine learning methods, lack of sufficient labelled data makes these infeasible in some domains. The evolving nature of expertise, incrementally available data and shifting nature of the underlying vision tasks make it difficult for experts to express all knowledge a priori. This is especially true of domains such as medical imaging, where data tends to trickle in over time and expert's discover the precise knowledge only by applying different vision algorithms over time. Therefore, vision experts develop systems incrementally, by trial-and-error, and each ad-hoc revision to the system, although well intended, may in fact degrade the system performance. This thesis proposes ways to mitigate the risks that ad-hoc incremental revisions pose to vision systems. It presents ProcessRDR and ProcessNet frameworks, which are an adaptation of the incremental validated change strategy employed by Ripple Down Rules for the vision domain. These frameworks assist the expert in systematically revising parts of a system, while maintaining the integrity of the whole. The thesis also establishes the role of quasi-expertise in vision domains, and studies its influences {{on the quality of the}} resulting vision systems. The studies suggest ways for experts to mitigate the influences of quasi-expertise and support the need for incremental development of vision systems. <b>Incremental</b> development of <b>computer</b> vision systems give experts the capacity to adapt vision systems, as better expertise and more data becomes available. Although the frameworks do not eliminate quasiexpertise or lack of labelled data, they do support the expert in incrementally developing vision systems despite it. The systematic incremental revisions mean that the vision systems can continue to improve over time...|$|R

