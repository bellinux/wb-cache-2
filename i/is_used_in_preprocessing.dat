2|10000|Public
40|$|Abstract. This paper {{presents}} our {{newly developed}} wireless system for person verification {{based on the}} face verification technology. In our system, a handheld device cooperating with a client-server wireless network is used. Due to the limitation of the handheld device's computing power, most computations will be distributed to a remote control server. There are two main problems for implementing the system. One is the variations in lighting and background conditions. The other is that the camera on the handheld device can not be calibrated with the system in advance. For calibration on line, we use a three-point localization scheme for extracting appropriate face region according to the information of eyes and mouth. Furthermore, statistic based illumination normalization <b>is</b> <b>used</b> <b>in</b> <b>preprocessing</b> to decrease illumination influence under variant lighting conditions. Experiment shows the proposed system provides users a more flexible and feasible way {{to interact with the}} verification system through handheld device. ...|$|E
40|$|Quality {{evaluation}} {{is an important}} factor in food processing industries using the computer vision system where human inspection systems provide high variability. In many countries food processing industries aims at producing defect free food materials to the consumers. Human evaluation techniques suffer from high labour costs, inconsistency and variability. Thus this paper provides various steps for identifying defects in the food material using the computer vision systems. Various steps in computer vision system are image acquisition, Preprocessing, image segmentation, feature identification and classification. The proposed framework provides the comparison of various filters where the hybrid median filter was selected as the filter with the high PSNR value and <b>is</b> <b>used</b> <b>in</b> <b>preprocessing.</b> Image segmentation techniques such as Colour based binary Image segmentation, Particle swarm optimization are compared and image segmentation parameters such as accuracy, sensitivity, specificity are calculated and found that colour based binary image segmentation is well suited for food quality evaluation. Finally this paper provides an efficient method for identifying the defected parts in food materials...|$|E
40|$|A {{variable}} elimination rule {{allows the}} polynomialtime identification of certain variables whose elimination {{does not affect}} the satisfiability of an instance. Variable elimination in the constraint satisfaction problem (CSP) can <b>be</b> <b>used</b> <b>in</b> <b>preprocessing</b> or during search to reduce search space size. We show that there are essentially just four variable elimination rules defined by forbidding generic sub-instances, known as irreducible patterns, in arc-consistent CSP instances. One of these rules is the Broken Triangle Property, whereas the other three are novel...|$|R
40|$|International audienceVariable or value {{elimination}} in a {{constraint satisfaction}} problem (CSP) can <b>be</b> <b>used</b> <b>in</b> <b>preprocessing</b> or during search to reduce search space size. A variable elimination rule (value elimination rule) allows the polynomial-time identification of certain variables (domain elements) whose elimination, without {{the introduction of}} extra compensatory constraints, {{does not affect the}} satisfiability of an instance. We show that there are essentially just four variable elimination rules and three value elimination rules defined by forbidding generic sub-instances, known as irreducible existential patterns, in arc-consistent CSP instances. One of the variable elimination rules is the already-known Broken Triangle Property, whereas the other three are novel. The three value elimination rules can all be seen as strict generalisations of neighbourhood substitution...|$|R
50|$|Anomaly {{detection}} {{is applicable}} {{in a variety}} of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting Eco-system disturbances. It <b>is</b> often <b>used</b> <b>in</b> <b>preprocessing</b> to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.|$|R
40|$|Variable or value {{elimination}} in a {{constraint satisfaction}} problem (CSP) can <b>be</b> <b>used</b> <b>in</b> <b>preprocessing</b> or during search to reduce search space size. A variable elimination rule (value elimination rule) allows the polynomial-time identification of certain variables (domain elements) whose elimination, without {{the introduction of}} extra compensatory constraints, {{does not affect the}} satisfiability of an instance. We show that there are essentially just four variable elimination rules and three value elimination rules defined by forbidding generic sub-instances, known as irreducible existential patterns, in arc-consistent CSP instances. One of the variable elimination rules is the already-known Broken Triangle Property, whereas the other three are novel. The three value elimination rules can all be seen as strict generalisations of neighbourhood substitution. Comment: A full version of an IJCAI' 13 paper to appear in Journal of Computer and System Sciences (JCSS...|$|R
40|$|We {{study the}} {{calculation}} of the largest pairwise intersections in a given set family. We give combinatorial and algorithmic results both for the worst case and for set families where the frequencies of elements follow a power law, as words in texts typically do. The results can <b>be</b> <b>used</b> <b>in</b> faster <b>preprocessing</b> routines <b>in</b> a simple approach to multi-document summarization...|$|R
40|$|International audienceIn this article, we {{consider}} the k-edge connected subgraph problem from a polyhedral point of view. We introduce further classes of valid inequalities for the associated polytope and describe sufficient conditions for these inequalities to be facet defining. We also devise separation routines for these inequalities and discuss some reduction operations that can <b>be</b> <b>used</b> <b>in</b> a <b>preprocessing</b> phase for the separation. Using these results, we develop a Branch-and-Cut algorithm and present some computational results...|$|R
40|$|For a {{real time}} face {{recognition}} system, restraints like orientation, lighting and pose {{are the major}} challenges to be addressed. In the proposed work, to eliminate the variations due to pose, lighting and features to some extent, Gabor wavelets <b>are</b> <b>used</b> <b>in</b> <b>preprocessing</b> of human face image. Principal Component Analysis (PCA) has been widely adopted as the unpretentious potential face recognition algorithm which extracts low dimensional feature vectors from face. Linear Discriminant analysis (LDA) is applied on the reduced features from PCA, to get more discriminating features. The classification <b>is</b> done <b>using</b> distance measure classifiers and Support Vector Machine. Here, the train dataset is considered in randomized fashion, means the database contains a set of images of an individual in which different set of train dataset is generated randomly to choose the trained set that gives high rate of recognition. This will support in plummeting the overall database size and upsurge {{the enactment of the}} system. The system has been successfully tested on ORL face database with 400 frontal images corresponding to 40 different subjects of variable illumination and facial expressions. The proposed system gives a better recognition rate when compared to other standard techniques and achieves better accuracy with increased number of features. Keywords [...] Face recognition; Gabor Wavelets; PCA; LDA; Randomized Dataset I...|$|R
40|$|In {{this paper}} we {{introduce}} a parallel algorithm for thinning gray scale images. The algorithm {{is based on}} repeatedly conditionally eroding the gray level objects in the image until a one pixel thick pattern is obtained along {{the center of the}} high intensity region. Erosion conditions are devised to assure preserving connectivity. To properly handle objects with hollows, gray level gradient <b>is</b> <b>used</b> <b>in</b> the <b>preprocessing</b> phase to detect significant hollows. This allows processing of realistic nontrivial gray level objects. Results of applying the algorithm on a variety of images will be shown...|$|R
40|$|Introduction: Because of huge {{impacts of}} “OMICS” {{technologies}} in life sciences, many researchers aim to implement such high throughput approach to address cellular and/or molecular functions {{in response to}} any influential intervention in genomics, proteomics, or metabolomics levels. However, <b>in</b> many cases, <b>use</b> of such technologies often encounters some cybernetic difficulties in terms of knowledge extraction from a bunch of data <b>using</b> related softwares. <b>In</b> fact, there is little guidance upon data mining for novices. The main goal {{of this article is}} to provide a brief review on different steps of microarray data handling and mining for novices and at last to introduce different PC and/or web-based softwares that can <b>be</b> <b>used</b> <b>in</b> <b>preprocessing</b> and/or data mining of microarray data. Methods: To pursue such aim, recently published papers and microarray softwares were reviewed. Results: It was found that defining the true place of the genes in cell networks is the main phase in our understanding of programming and functioning of living cells. This can be obtained with global/selected gene expression profiling. Conclusion: Studying the regulation patterns of genes <b>in</b> groups, <b>using</b> clustering and classification methods helps us understand different pathways in the cell, their functions, regulations and the way one component in the system affects the other one. These networks can act as starting points for data mining and hypothesis generation, helping us reverse engineer...|$|R
40|$|Abstract- Electric {{power is}} an {{important}} part in economic development. Moreover, an accurate load forecast can make a financing planning, power supply strategy and market research planned effectively. This paper used the fuzzy logic system to predict the regional electric power load. To design the fuzzy prediction system, the correlation-based clustering algorithm and TSK fuzzy model <b>were</b> <b>used.</b> Also, to improve the prediction system's capability, the moving average technique and relative increasing rate <b>were</b> <b>used</b> <b>in</b> the <b>preprocessing</b> procedure. Finally, using four regional electric power load in Taiwan, this paper verified the performance of the proposed system and demonstrated its effectiveness an...|$|R
40|$|A {{combined}} geometric and radiometric {{processing chain}} for hyperspectral data is presented. This paper describes the ortho-rectification {{solution for the}} geometric {{part of the whole}} problem. A parametric geocoding approach (PARGE) has been chosen. The geometrical model strictly considers all navigational parameters engaging a forward transformation methodology. For the implementation, a number of auxiliary data calibration and tuning possibilities are shown together with the work flow of the currently applied processor. Results of the procedure for HyMap and AVIRIS hyperspectral imagery are analyzed with respect to their absolute accuracy and coregistration errors. The geocoding procedure will <b>be</b> <b>used</b> <b>in</b> standard <b>preprocessing</b> chains <b>in</b> combination with the atmospheric correction procedure ATCOR 4 for current and future hyperspectral instruments. ...|$|R
40|$|Process mining {{techniques}} {{attempt to}} extract non-trivial knowledge and interesting insights from event logs. Process mining provides a welcome {{extension of the}} repertoire of business process analysis techniques and has been adopted in various commercial BPM systems (BPM|one, Futura Reflect, ARIS PPM, Fujitsu, etc.). Unfortunately, traditional process discovery algorithms have problems dealing with less-structured processes. The resulting models are difficult to comprehend or even misleading. Therefore, we propose a new approach based on trace alignment. The goal is to align traces {{in a way that}} event logs can be explored easily. Trace alignment can <b>be</b> <b>used</b> <b>in</b> a <b>preprocessing</b> phase where the event log is investigated or filtered and in later phases where detailed questions need to be answered. Hence, it complements existing process mining techniques focusing on discovery and conformance checking...|$|R
40|$|So far {{predicted}} {{scenarios for}} Turkish dependency parsing {{have used a}} morphological disambiguator that is trained on the data distributed with the tool(Sak et al., 2008). Although models trained on this data have high accuracy scores on the test and development data of the same set, the accuracy drastically drops when the model <b>is</b> <b>used</b> <b>in</b> the <b>preprocessing</b> of Turkish Treebank parsing experiments. We propose to use the Turkish Treebank(Oflazer et al., 2003) as a morphological resource to overcome this problem and convert the treebank to the morphological disambiguator’s format. The experimental results show that we achieve improvements in disambiguating the Turkish Treebank and the results also carry over to parsing. With the help of better morphological analysis, we present the best labelled dependency parsing scores to date on Turkish...|$|R
40|$|Abstract. Process mining {{techniques}} {{attempt to}} extract non-trivial knowledge and interesting insights from event logs. Process mining provides a welcome {{extension of the}} repertoire of business process analysis techniques and has been adopted in various commercial BPM systems (BPM|one, Futura Reflect, ARIS PPM, Fujitsu, etc.). Unfortunately, traditional process discovery algorithms have problems dealing with lessstructured processes. The resulting models are difficult to comprehend or even misleading. Therefore, we propose a new approach based on trace alignment. The goal is to align traces {{in a way that}} event logs can be explored easily. Trace alignment can <b>be</b> <b>used</b> <b>in</b> a <b>preprocessing</b> phase where the event log is investigated or filtered and in later phases where detailed questions need to be answered. Hence, it complements existing process mining techniques focusing on discovery and conformance checking. ...|$|R
30|$|One of {{the main}} {{challenges}} of BSS remains to obtain good BSS performance in a real reverberant environments. A beamforming preprocessing can be a solution to improve BSS performance in a reverberant room. Beamforming consists in estimating a spatial filter that operates on the outputs of a microphone array in order to form a beam with a desired directivity pattern [6]. It is useful for many purposes, particularly for enhancing a desired signal from its measurement corrupted by noise, competing sources and reverberation [6]. Beamforming filters can be estimated in a fixed or in an adaptive way. A fixed beamforming, contrarily to an adaptive one, {{does not depend on}} the sensors data, the beamformer is built for a set of fixed desired directions. In this article, we propose a two-stage BSS technique where a fixed beamforming <b>is</b> <b>used</b> <b>in</b> a <b>preprocessing</b> step.|$|R
30|$|Gauss {{filtering}} method [24]: Gauss filter {{is a kind}} {{of linear}} smoothing filter, which is suitable for filtering Gaussian white noise. Gauss filter method has <b>been</b> widely <b>used</b> <b>in</b> the <b>preprocessing</b> stage of image processing. The pixel value of each pixel in the image is calculated by Gaussian filtering operation, the result is obtained by weighting the pixel gray value and other pixel gray values in the neighborhood of the pixel itself, and the weighted average weighting coefficient is obtained by sampling and normalizing the two-dimensional discrete Gaussian function.|$|R
40|$|Feature {{selection}} <b>is</b> frequently <b>used</b> <b>in</b> data <b>preprocessing</b> {{for data}} mining. It decreases number of features, removes irrelevant or noisy data, and increases mining performance such as predictive accuracy and comprehensibility. This work investigates active sampling in feature selection in a lter model setting. Three versions of active sampling are proposed and empirically evaluated: two employ class {{information and the}} other utilizes feature variance. They are applied to a widely used, ecient feature selection algorithm Relief. In comparison with random sampling, we conduct extensive experiments with benchmark data sets...|$|R
40|$|Abstract—Examples of aerosol {{retrieval}} results, {{derived from}} the Multi-angle Imaging SpectroRadiometer (MISR) on the Earth Observation Science (EOS) Terra platform, are shown {{and the performance of}} the retrieval algorithms are discussed, following the first 18 months of operational data processing. A number of algorithm modifications were implemented, based on an analysis of aerosol retrieval results during this period, and these changes are described. Two cloud-screening algorithms, the angle-to-angle smoothness and angle-to-angle correlation tests, which <b>were</b> <b>used</b> <b>in</b> the <b>preprocessing</b> phase of the analyses are also described. The aerosol retrieval examples cover a wide variety of conditions, both over land and water. Particular aerosol types include dust clouds, forest fire and volcanic plumes, and localized dense haze. Finally, some ideas are discussed for additional improvement of the MISR aerosol data product, based on the experience gained in analyzing multiangle data and the associated geophysical products. Index Terms—Aerosol, algorithms, remote sensing...|$|R
40|$|Early {{detection}} and prognosis {{of breast cancer}} are feasible by utilizing histopathological grading of biopsy specimens. This research is focused on {{detection and}} grading of nuclear pleomorphism in histopathological images of breast cancer. The proposed method consists of three internal steps. First, unmixing colors of H&E <b>is</b> <b>used</b> <b>in</b> the <b>preprocessing</b> step. Second, nuclei boundaries are extracted incorporating the center of cancerous nuclei which are detected by applying morphological operations and Difference of Gaussian filter on the preprocessed image. Finally, segmented nuclei are scored to accomplish one parameter of the Nottingham grading system for breast cancer. In this approach, the nuclei area, chromatin density, contour regularity, and nucleoli presence, are features for nuclear pleomorphism scoring. Experimental {{results showed that the}} proposed algorithm, with an accuracy of 86. 6 %, made significant advancement in detecting cancerous nuclei compared to existing methods in the related literature...|$|R
40|$|We {{present a}} method for {{computing}} a surface classifier that can <b>be</b> <b>used</b> to detect convex ridges on voxel surfaces extracted from 3 D scans. In contrast to classical approaches based on (discrete) curvature computations, which can be sensitive to various types of noise, we propose here a new method that detects convex ridges on such surfaces, based on the computation of the surface’s 3 D skeleton. We use a suitable robust, noise-resistant skeletonization algorithm to extract the full 3 D skeleton of the given surface, and subsequently compute a surface classifier that separates convex ridges from quasi-flat regions, using the feature points of the simplified skeleton. We demonstrate our method on voxel surfaces extracted from actual anatomical scans, {{with a focus on}} cortical surfaces, and compare our results with curvature-based classifiers. As a second application of the 3 D skeleton, we show how a partitioning of the brain skeleton can <b>be</b> <b>used</b> <b>in</b> a <b>preprocessing</b> step for the brain surface analysis. ...|$|R
40|$|Text {{preprocessing}} is {{an essential}} stage in text categorization (TC) particularly and text mining generally. Morphological tools can <b>be</b> <b>used</b> <b>in</b> text <b>preprocessing</b> to reduce multiple forms of the word to one form. There has been a debate among researchers {{about the benefits of}} <b>using</b> morphological tools <b>in</b> TC. Studies in the English language illustrated that performing stemming during the preprocessing stage degrades the performance slightly. However, they have a great impact on reducing the memory requirement and storage resources needed. The effect of the preprocessing tools on Arabic text categorization is an area of research. This work provides an evaluation study of several morphological tools for Arabic Text Categorization. The study includes using the raw text, the stemmed text, and the root text. The stemmed and root text <b>are</b> obtained <b>using</b> two different preprocessing tools. The results illustrated that using light stemmer combined with a good performing feature selection method enhances the performance of Arabic Text Categorization especially for small threshold values. 1...|$|R
40|$|The {{quality of}} images {{generated}} by volume rendering strongly {{depends on the}} applied continuous reconstruction method. Recently, {{it has been shown}} that the reconstruction of the underlying function can be improved by a discrete prefiltering. In volume rendering, however, an accurate gradient reconstruction also plays an important role as it provides the surface normals for the shading computations. Therefore, in this paper, we propose prefiltering schemes in order to increase the accuracy of the estimated gradients yielding higher image quality. We search for discrete prefilters of minimal support which can <b>be</b> efficiently <b>used</b> <b>in</b> a <b>preprocessing</b> as well as on the fly...|$|R
40|$|In this study, a new {{classification}} technique {{based on}} {{rough set theory}} and MEPAR-miner algorithm for association rule mining is introduced. Proposed method is called as ‘Reduced MEPAR-miner Algorithm’. In the method being improved rough sets <b>are</b> <b>used</b> <b>in</b> the <b>preprocessing</b> stage <b>in</b> {{order to reduce the}} dimensionality of the feature space and improved MEPAR-miner algorithms <b>are</b> then <b>used</b> to extract the classification rules. Besides, a new and an effective default class structure is also defined in this proposed method. Integrating rough set theory and improved MEPAR-miner algorithm, an effective rule mining structure is acquired. The effectiveness of our approach is tested on eight publicly available binary and n-ary classification data sets. Comprehensive experiments are performed to demonstrate that Reduced MEPAR-miner Algorithm can discover effective classification rules which are as good as (or better) the other classification algorithms. These promising results show that the rough set approach is a useful tool for preprocessing of data for improved MEPAR-miner algorithm...|$|R
40|$|Psychological studies {{indicate}} that emotional states are expressed {{in the way people}} walk and the human gait is investigated in terms of its ability to reveal a person's emotional state. And Microsoft Kinect is a rapidly developing, inexpensive, portable and no-marker motion capture system. This paper gives a new referable method to do emotion recognition, by using Microsoft Kinect to do gait pattern analysis, which has not been reported. $ 59 $ subjects are recruited in this study and their gait patterns are record by two Kinect cameras. Significant joints selecting, Coordinate system transforming, Slider window gauss filter, Differential operation, and Data segmentation <b>are</b> <b>used</b> <b>in</b> data <b>preprocessing.</b> Feature extracting is based on Fourier transformation. By using the NaiveBayes, RandomForests, libSVM and SMO classification, the recognition rate of natural and unnatural emotions can reach above 70 %. It <b>is</b> concluded that <b>using</b> the Kinect system can be a new method in recognition of emotions. Comment: 15 pages, 4 figure...|$|R
40|$|Superpixel {{segmentation}} <b>is</b> widely <b>used</b> <b>in</b> the <b>preprocessing</b> step of many applications. Most {{of existing}} methods {{are based on}} a photometric criterion combined to the position of the pixels. In {{the same way as the}} Simple Linear Iterative Clustering (SLIC) method, based on k-means segmentation, a new algorithm is introduced. The main contribution lies on the definition of a new distance for the construction of the superpixels. This distance takes into account both the surface normals and a similarity measure between pixels that are located on the same planar surface. We show that our approach improves over-segmentation, like SLIC, i. e. the proposed method is able to segment properly planar surfaces...|$|R
40|$|Poppy {{seeds are}} found to be a {{strictly}} monitored crop which is because of its narcotic effects. However the use of poppy seeds in food, industrial and medical purpose opens the door for its survival. It generates a good amount of foreign currency for the producing country. Due to its high value its quality should also be assured. In this study nature is applied for benefit of nature, that is nature-inspired metaheuristic algorithms, that is Cuckoo Search (CS) and Particle Swarm Optimization (PSO) have <b>been</b> <b>used</b> to check and monitor the quality of the white poppy seeds. CS <b>is</b> <b>used</b> <b>in</b> the <b>preprocessing</b> stage of the approach, whereas PSO <b>is</b> <b>used</b> for segmentation and final outcome. Basically CS <b>is</b> <b>used</b> to enhance the image quality, which helps to further process the image in later stages. Combination of these two approaches is quite new, but the results are very good. Visual results imply that impurities in terms of black seeds within white seeds can be successfully recognized through computer vision analysis...|$|R
40|$|The Taylor Analogy Breakup (TAB) {{model is}} applied to droplet breakup in rotary bell spray {{painting}} commonly <b>used</b> <b>in</b> the automotive industry. The bell spins rapidly around its axis with a tangential velocity at the edge {{in the order of}} 100 m/s. The paint falls off the edge and enters the air with a large relative velocity, driving the atomization. The paint is a viscous fluid and a modification of the TAB model taking non-linear effects of large viscosity into account is described. The parameters in the breakup model are tuned to match droplet size distributions obtained in CFD simulations with measured ones. Results are presented for three cases with rotation speeds from 30 to 50 thousand RPM where the full droplet size distributions are compared with measurements. Good results are obtained for all three cases where the simulated size distributions compare well to measurements over a wide range of droplet sizes. The obtained results can <b>be</b> <b>used</b> <b>in</b> a <b>preprocessing</b> stage of a full spray painting simulation thereby reducing the need for costly and cumbersome measurements...|$|R
40|$|In fractal image compression, most of {{the time}} during {{encoding}} is spent for finding the best matching pair of range-domain blocks. Different techniques have been analyzed for decreasing the number of operations required for this range-domain matching. Encoding time can be saved by reducing the domain search pool for each range block. Domain blocks can be classified based on local fractal dimension. Fractal dimension is being studied as a measure to analyze the complexity of image portions. This paper proposes application of height balanced binary search trees for storing domain information ordered in terms of the local fractal dimension. The approach is to prepare the domain pool dynamically, by comparing the fractal dimension of range block with that of the domains. Domains with fractal dimension in an interval, evenly covering the fractal dimension of range block alone are given for comparison. We use AVL trees to enlist the domains based on their fractal dimension. The domain pool is prepared at runtime. Since the tree organization <b>is</b> <b>used</b> <b>in</b> the <b>preprocessing</b> phase, the proposed method can <b>be</b> <b>used</b> with any algorithm for fractal compression...|$|R
40|$|Abstract: This study {{involves}} the Iris Localization based on morphological or set theory which is well in shape detection. Principal Component Analysis (PCA) <b>is</b> <b>used</b> for <b>preprocessing,</b> <b>in</b> which {{the removal of}} redundant and unwanted data is done. Applications such as Median Filtering and Adaptive thresholding <b>are</b> <b>used</b> for handling the variations in lighting and noise. Features <b>are</b> extracted <b>using</b> Wavelet Packet Transform (WPT). Finally matching <b>is</b> performed <b>using</b> KNN. The proposed method {{is better than the}} previous method and is proved by the results of different parameters. The testing of the proposed algorithm <b>was</b> done <b>using</b> CASIA iris database (V 1. 0) and (V 3. 0) ...|$|R
40|$|A {{three-dimensional}} parallel {{environment for}} {{finite element method}} (FEM) analysis is presented. This environment is comprised by integrated computer programs, each of one responsible for a different task: pre-processing, mesh generation, structural analysis and post-processing. In this work, this entire system is presented, with more emphasis to volumetric mesh generation and FEM analysis that could be performed in a parallel way. A program named FRANC 3 D (3 D Fracture Analysis Code) [1][2] <b>is</b> <b>used</b> <b>in</b> the <b>preprocessing</b> step. The volumetric mesh <b>is</b> generated <b>using</b> a algorithm that combines an advancing front technique with a recursive spatial decomposition technique, in this case an octree, to define the internal nodes, element sizes, and mesh transition [3]. A strategy to parallelize this algorithm is also presented. An existing finite element method program, called FEMOOP (Finite Element Method – Object Oriented Programming) [4] has been adapted to implement the parallel features. The parallel analysis can <b>be</b> performed <b>using</b> two different techniques: a domain decomposition technique or an element-by-element scheme. Both are presented in this work. Finally, analyses of three-dimensional models <b>are</b> <b>used</b> to test the performance and reliability of this parallel system. ...|$|R
40|$|A very {{important}} theoretical result giving impetus to increasing interest in neural networks {{is that a}} multilayer feedforward network can approximate any function to arbitrary precision, or as a classifier it can form arbitraryly complex class boundaries [2]. In difficult practical classification problems, like in pattern recognition and machine vision, the class boundaries will inevitably be very complex due to variations and distortions in the input images. To {{reduce the amount of}} trainig data needed the number of independent weights in the classifier must be reduced [1]. The trade-off is between the capability of the classifier and the amount of training data. In machine vision problems it is often possible to acquire large amounts of training data as long as manual classification of the objects is not required. Thus unsupervised methods can <b>be</b> <b>used</b> <b>in</b> the <b>preprocessing</b> stage without large extra cost. The essential requirement for the preprocessor is that the (unknown) class boundaries shoud be simpler that in the original data, while any two separable classes should keep separable. Since the class boundaries are not known, the best preprocessing can do is to follow the distributions of the data samples, or in other words, clustering...|$|R
40|$|Diffusion MRI (dMRI) is {{a popular}} noninvasive imaging {{modality}} for {{the investigation of the}} neonate brain. It enables the assessment of white matter integrity, and is particularly suited for studying white matter maturation in the preterm and term neonate brain. Diffusion tractography allows the delineation of white matter pathways and assessment of connectivity in vivo. In this review, we address the challenges of performing and analysing neonate dMRI. Of particular importance in dMRI analysis is adequate data preprocessing to reduce image distortions inherent to the acquisition technique, as well as artefacts caused by head movement. We present a summary of techniques that should <b>be</b> <b>used</b> <b>in</b> the <b>preprocessing</b> of neonate dMRI data, and demonstrate the effect of these important correction steps. Furthermore, we give an overview of available analysis techniques, ranging from voxel-based analysis of anisotropy metrics including tract-based spatial statistics (TBSS) to recently developed methods of statistical analysis addressing issues of resolving complex white matter architecture. We highlight the importance of resolving crossing fibres for tractography and outline several tractography-based techniques, including connectivity-based segmentation, the connectome and tractography mapping. These techniques provide powerful tools for the investigation of brain development and maturation...|$|R
40|$|Most of Arabic {{handwriting}} {{recognition in the}} literature has focused only on recognizing offline script, and few of research take online case. So it's still remains as an active area of research. however {{there is a lack}} of studies in terms of recognizing Arab characters, especially on the online cases. The process of {{handwriting recognition}} faces a lot of challenges; feature extraction is the most important problem in character recognition. The main theme of this paper is new feature extraction method employed in online Arabic character recognition. An Arabic character recognition handwritten system cannot <b>be</b> successful, without <b>using</b> suitable feature extraction methods. In this work we have proposed the hybrid Edge Direction Matrixes and geometrical feature extraction method for on-line handwritten Arabic character recognition system. In addition, horizontal and vertical projection profile, and Laplacian filter have <b>been</b> <b>used</b> <b>in</b> the <b>preprocessing</b> phase. The training and testing of the online handwriting recognition system <b>was</b> conducted <b>using</b> our dataset; we have used 840 characters from different writers, 504 characters for training, and 336 characters for testing. The evaluation was conducted on state of the art methods in the classification phase. The results have revealed that the proposed method gives best recognition rate for character category...|$|R
40|$|Case-based {{reasoning}} (CBR) <b>is</b> {{a process}} <b>used</b> for computer processing {{that tries to}} mimic {{the behavior of a}} human expert in making decisions regarding a subject and learn from the experience of past cases. CBR has demonstrated to be appropriate for working with unstructured domains data or difficult knowledge acquisition situations, such as medical diagnosis, where it is possible to identify diseases such as: cancer diagnosis, epilepsy prediction and appendicitis diagnosis. Some of the trends that may be developed for CBR in the health science are oriented {{to reduce the number of}} features in highly dimensional data. An important contribution may be the estimation of probabilities of belonging to each class for new cases. In this paper, in order to adequately represent the database and to avoid the inconveniences caused by the high dimensionality, noise and redundancy, a number of algorithms <b>are</b> <b>used</b> <b>in</b> the <b>preprocessing</b> stage for performing both variable selection and dimension reduction procedures. Also, a comparison of the performance of some representative multi-class classifiers is carried out to identify the most effective one to include within a CBR scheme. Particularly, four classification techniques and two reduction techniques are employed to make a comparative study of multiclass classifiers on CBR</p...|$|R
