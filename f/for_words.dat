3065|10000|Public
5|$|From early on, {{children}} also assume that language {{is designed for}} communication. Infants treat communication as a cooperative process. Specifically, infants observe the principles of conventionality and contrast. According to conventionality, infants believe that for a particular meaning that they wish to convey, there is a term {{that everyone in the}} community would expect to be used. According to contrast, infants act according to the notion that differences in form mark differences in meaning. Children's attention to conventionality and contrast is demonstrated in their language use, even before the age of 2 years; they direct their early words towards adult targets, repair mispronunciations quickly if possible, ask <b>for</b> <b>words</b> to relate to the world around them, and maintain contrast in their own word use.|$|E
5|$|Of the 71 {{words in}} this list, 67 are nouns, and most would {{generally}} be considered loanwords; the only modern-English words that contain Q not followed by U {{and are not}} borrowed from another language are qiana, qwerty, and tranq. However, all of the loanwords on this list {{are considered to be}} naturalised in English according to at least one major dictionary (see References), often because they refer to concepts or societal roles that do not have an accurate equivalent in English. <b>For</b> <b>words</b> to appear here, they must appear in their own entry in a dictionary; words which occur only as part of a longer phrase are not included.|$|E
5|$|I {{should say}} I {{wanted to write}} poetry in the {{beginning}} because I {{had fallen in love}} with words. The first poems I knew were nursery rhymes and before I could read them for myself I had come to love the words of them. The words alone. What the words stood for was of a very secondary importance... I fell in love, that is the only expression I can think of, at once, and am still at the mercy of words, though sometimes now, knowing a little of their behaviour very well, I think I can influence them slightly and have even learned to beat them now and then, which they appear to enjoy. I tumbled <b>for</b> <b>words</b> at once. And, when I began to read the nursery rhymes for myself, and, later, to read other verses and ballads, I knew that I had discovered the most important things, to me, that could be ever.|$|E
50|$|The {{first version}} of MetaTexis <b>for</b> <b>Word</b> was {{released}} in July 2002. From the start MetaTexis <b>for</b> <b>Word</b> used the COM-add-in technology <b>for</b> MS <b>Word.</b> Since then the software has continually been extended and improved. Version 3 of MetaTexis <b>for</b> <b>Word</b> was released in September 2010.|$|R
40|$|We {{present a}} novel {{framework}} <b>for</b> <b>word</b> alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful <b>for</b> <b>word</b> alignment {{because we can}} expect a synonym to correspond to the same word in a different language. We design a generative model <b>for</b> <b>word</b> alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality. ...|$|R
50|$|The {{required}} {{length of}} writing is 40 <b>words</b> <b>for</b> A1, 60 to 80 <b>words</b> <b>for</b> A2, 160-180 <b>words</b> <b>for</b> B1, {{and at least}} 250 <b>words</b> <b>for</b> B2.|$|R
25|$|To produce {{appropriate}} Kannada translations <b>for</b> <b>words</b> used {{in other}} languages, especially the scientific words.|$|E
25|$|Has a seat {{matching}} the final short vowel <b>for</b> <b>words</b> {{ending in a}} short vowel.|$|E
25|$|<b>For</b> <b>words</b> ending -(e)ment as in judg(e)ment, either {{spelling}} {{is acceptable}} in New Zealand usage, although -ement {{is the preferred}} British usage.|$|E
5000|$|If {{translated}} <b>word</b> <b>for</b> <b>word</b> with {{word order}} maintained, this German relative clause {{is equivalent to}} ...|$|R
40|$|Abstract – This paper {{presents}} a technique <b>for</b> <b>Word</b> segmentation <b>for</b> the Urdu OCR system. Word segmentation or word tokenization is a preliminary task {{for understanding the}} meanings of sentences in Urdu language processing. Several techniques are available <b>for</b> <b>word</b> segmentation in other languages but not much {{work has been done}} <b>for</b> <b>word</b> segmentation of Urdu Optical Character Recognition (OCR) System. A methodology is proposed <b>for</b> <b>word</b> segmentation in this paper. It finds the boundaries of words in a sequence of ligatures using probabilistic formulas, by utilizing the knowledge of collocation of ligatures and words in the corpus. The word identification rate using this technique is 96. 10 % with 65. 63 % unknown words identification rate...|$|R
40|$|Most {{existing}} generation {{systems for}} spoken dialog require the system engineer to specify by hand {{the words to}} be used in system prompts. However, the existence of corpora of spoken dialog makes it possible to acquire the words and structure of system prompts automatically. In this paper, we construct statistical models for generating system prompts, both <b>for</b> <b>word</b> choice and <b>for</b> <b>word</b> ordering. We evaluate these models using a human-computer dialog multicorpus and a human-human dialog corpus. Our results show that statistical models <b>for</b> <b>word</b> choice can work well, while more work is needed on statistical models <b>for</b> <b>word</b> ordering. Index Terms — language generation, spoken dialog systems, natural language interfaces, machine learning 1...|$|R
25|$|The logographic script easily {{accommodated}} {{differences in}} pronunciation, meaning and word order, but often new characters were required <b>for</b> <b>words</b> {{that could not}} be related to older forms.|$|E
25|$|Scientific {{abbreviations}} <b>for</b> <b>words</b> of Latin origin {{ending in}} -a, such as SN for supernova, can form a plural by adding -e, as SNe for supernovae.|$|E
25|$|The famously {{articulate}} Muhammad Ali {{launched into}} an extraordinary tirade — even by his standards — when Parkinson challenged {{him on the}} nature of his religious beliefs. This so surprised the presenter (as he'd thought it to be an innocuous question) that he became lost <b>for</b> <b>words.</b>|$|E
40|$|This paper {{introduce}} a technique <b>for</b> <b>Word</b> segmentation <b>for</b> the handwritten recognition of Urdu script. Word segmentation or word tokenization {{is a primary}} technique for understanding the sentences written in Urdu lan-guage. Several techniques are available <b>for</b> <b>word</b> segmentation in other languages but not much {{work has been done}} <b>for</b> <b>word</b> segmentation of Urdu Optical Character Recognition (OCR) System. A method is proposed <b>for</b> <b>word</b> segmentation in this paper. It finds the boundaries of words in a sequence of ligatures using probabilistic formulas, by utilizing the knowledge of collocation of ligatures and words in the corpus. The word identification rate using this technique is 97. 10 % with 66. 63 % unknown words identification rate...|$|R
40|$|This paper {{presents}} a technique <b>for</b> <b>word</b> segmentation <b>for</b> the Urdu OCR system. Word segmentation or word tokenization is a preliminary task for Urdu language processing. Several techniques are available <b>for</b> <b>word</b> segmentation in other languages. A methodology is proposed <b>for</b> <b>word</b> segmentation {{in this paper}} which determines the boundaries of words given a sequence of ligatures, based on collocation of ligatures and words in the corpus. Using this technique, word identification rate of 96. 10 % is achieved, using trigram probabilitie...|$|R
50|$|The {{results showed}} faster {{reaction}} times for learned and familiar <b>words</b> than <b>for</b> learned and familiar <b>words</b> than <b>for</b> unlearned rare <b>words</b> while reaction times for correct decisions were faster than incorrect decisions. There {{was a significant}} main effect <b>for</b> <b>word</b> type but not <b>for</b> <b>word</b> type x relatedness that revealed how learners were faster in responding to related trials in the orthography-to-meaning and phonology trials. The interaction <b>for</b> <b>word</b> type x correctness revealed a difference in decision times. This was found in the orthography-to-meaning and phonology-to-meaning conditions <b>for</b> familiar <b>words.</b> The results conclude that reinforcing the words orthography might help readers recognize a word in future encounters which will influence the process of incremental learning.|$|R
25|$|Malaysian English is {{gradually}} forming its own vocabulary, these words {{come from a}} variety of influences. Typically, <b>for</b> <b>words</b> or phrases that are based on other English words, the Malaysian English speaker may be unaware that the word or phrase is not present in British or American English.|$|E
25|$|With the {{introduction}} of letter-spacing in the Middle Ages, the hyphen, still written beneath the text, reversed its meaning. Scribes used the mark to connect two words that had been incorrectly separated by a space. This era also saw {{the introduction}} of the marginal hyphen, <b>for</b> <b>words</b> broken across lines.|$|E
25|$|A weasel word is an {{informal}} term <b>for</b> <b>words</b> and phrases aimed at creating {{an impression that}} a specific and/or meaningful statement has been made, when in fact only a vague or ambiguous claim has been communicated, enabling the specific meaning to be denied if the statement is challenged. A more formal term is equivocation.|$|E
6000|$|... "The written {{instructions}} to his executor are of such serious importance that I feel it {{my duty to}} copy them <b>for</b> you, <b>word</b> <b>for</b> <b>word.</b>|$|R
50|$|<b>Word</b> <b>for</b> <b>word,</b> Christine, New York 1983.|$|R
40|$|Ehrenfeucht's Conjecture is {{equivalent}} with the ascending chain condition <b>for</b> <b>word</b> equations. A related conjecture concerns the descending chain condition <b>for</b> <b>word</b> equations. We show {{that if this}} conjecture holds then an arbitrary infinite language L possesses a finite subset F such that the language L Γ...|$|R
25|$|Local dialectical {{readings}} of kanji are also classified under kun'yomi, most notably readings <b>for</b> <b>words</b> in Ryukyuan languages. Further, {{in rare cases}} gairaigo (borrowed words) have a single character associated with them, in which case this reading is formally classified as a kun'yomi, because the character is being used for meaning, not sound. This is discussed under single character gairaigo, below.|$|E
25|$|Word {{stress is}} {{predictable}} in Hawaiian <b>for</b> <b>words</b> {{with three or}} fewer moras (that is, three or fewer vowels, with diphthongs and long vowels counting as two vowels). In such cases, stress {{is always on the}} penultimate mora. Longer words will also follow this pattern, but may in addition have a second stressed syllable which is not predictable. In Hawaiian, a stressed syllable is louder in volume, longer in duration and higher in pitch.|$|E
25|$|Okurigana {{are also}} used as phonetic {{complements}} to disambiguate kanji that have multiple readings, and consequently multiple meanings. Since kanji, especially the most common ones, can be used <b>for</b> <b>words</b> with many (usually similar) meanings — but different pronunciations — key okurigana placed after the kanji help the reader to know which meaning and reading were intended. Both individual kanji and multi-kanji words may have multiple readings, and okurigana are used in both cases.|$|E
50|$|Yes {{the exact}} same story, <b>word</b> <b>for</b> <b>word.</b>|$|R
50|$|Zimmer also {{recorded}} solo albums <b>for</b> <b>Word</b> Records.|$|R
50|$|The next {{generation}} of screenplay software hooked into Microsoft Word. Warren Script Application was initially released {{as a set of}} style sheets <b>for</b> <b>Word</b> <b>for</b> DOS. It was updated <b>for</b> <b>Word</b> <b>for</b> Windows circa 1988. gScript, a shareware script formatter/template, was released via CompuServe in 1989. It was included on the disk accompanying the book Take <b>Word</b> <b>for</b> Windows to the Edge, published by Ziff-Davis in 1993. It was subsequently updated and released commercially as ScriptWright.|$|R
25|$|One {{generally}} sees adjectives {{in their}} neuter form when used pronominally for this reason. <b>For</b> <b>words</b> {{more commonly used}} in this way (rather than to describe a noun) one sees their neuter forms more often than their masculine or feminine. Normally the masculine form {{would be the most}} beneficial form of an adjective to learn first, given that the majority of nouns are masculine. In these cases, however, the most practical form to learn first would be the neuter.|$|E
25|$|Many {{forms of}} {{shorthand}} exist. A typical shorthand system provides symbols or abbreviations <b>for</b> <b>words</b> and common phrases, which can allow someone well-trained {{in the system}} to write as quickly as people speak. Abbreviation methods are alphabet-based and use different abbreviating approaches. Several autocomplete programs, standalone or integrated in text editors, based on word lists, also include a shorthand function for frequently-used phrases. Many journalists use shorthand writing to quickly take notes at press conferences or other similar scenarios.|$|E
25|$|By 3–5 years, {{children}} {{usually have}} difficulty using words correctly. Children experience many {{problems such as}} underextensions, taking a general word and applying it specifically (for example, 'cartoons' specifically for 'Mickey Mouse') and overextensions, taking a specific word and applying it too generally (example, 'ant' for any insect). However, children coin words to fill in <b>for</b> <b>words</b> not yet learned (for example, someone is a cooker rather than a chef because a child may not know what a chef is). Children can also understand metaphors.|$|E
30|$|See Table  14 (annexes) <b>for</b> <b>wording</b> of the questions.|$|R
50|$|The rank {{problem is}} {{undecidable}} <b>for</b> <b>word</b> hyperbolic groups.|$|R
5000|$|Dictionary translation: {{translating}} Chinese to English <b>word</b> <b>for</b> <b>word</b> ...|$|R
