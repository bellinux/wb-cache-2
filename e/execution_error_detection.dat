3|5693|Public
40|$|Runtime {{verification}} has primarily {{been developed}} and evaluated {{as a means}} of enriching the software testing process. While many researchers have pointed to its potential applicability in online approaches to software fault tolerance, there has been a dearth of work exploring the details of how that might be accomplished. In this paper, we describe how a component-oriented approach to software health management exposes the connections between program <b>execution,</b> <b>error</b> <b>detection,</b> fault diagnosis, and recovery. We identify both research challenges and opportunities in exploiting those connections. Specifically, we describe how recent approaches to reducing the overhead of runtime monitoring aimed at error detection might be adapted to reduce the overhead and improve the effectiveness of fault diagnosis...|$|E
40|$|Functional {{debugging}} {{of application}} specific integrated circuits (ASICs) {{has been recognized}} as a very labor-intensive and expensive process. It often dominates the time and cost of the ASIC system development. The difficulty of functional debugging is mainly due to the limited controllability and observability of the storage elements in designs, and therefore the intermediate variables in functional specifications. Debugging process can be logically divided into five phases: functional test generation, functional test <b>execution,</b> <b>error</b> <b>detection,</b> error diagnosis, and error correction. We propose a new approach for the functional test pattern execution phase. The goal is to maximize the simultaneous controllability of an arbitrary set of the user selected variables in the design at the debugging time for facilitating the functional test pattern execution while minimizing the hardware overhead. The new approach {{is based on the}} divide and conquer optimization paradigm. Functional specific [...] ...|$|E
40|$|As a {{processor}} gets smaller and faster, {{it becomes more}} sensitive to both the increasing process variation and environment disturbances such as cosmic rays and heat. These imperfections and uncertainties lead to transient errors or even destructive malfunctions of the processor. The previous effort on processor <b>execution</b> <b>error</b> <b>detection</b> includes the DIVA scheme [1], which excels other dynamic verification schemes in its simplicity and high error coverage. However, {{the high degree of}} execution redundancy leads to considerable performance overhead and hardware / power cost. Aiming to enhance the verification efficiency in DIVA-like dynamic schemes, this paper presents a Multi-level Error Detection Scheme (MEDS) based on conditional DIVA-style verification. In this architecture the program instruction stream is filtered by an error indicator before entering DIVA checking stage. Experiments on the SimpleScalar simulator demonstrate that by applying effective error indicators, the instruction re-execution costs can be significantly reduced at a small error-missing rate penalty. The design tradeoffs between error coverage and verification efficiency is analyzed through the experiments on applying different reorder buffer overflow handling schemes and checker processor bandwidths. Finally, future improvements to further enhance the efficiency of MEDS are suggested. 1...|$|E
50|$|MISSE-7 also {{contained}} experiments mounted to its ExPA base. These experiments included SpaceCube which {{was developed by}} engineers at the NASA Goddard Space Flight Center and is a reconfigurable, high-performance system based on Xilinx's Virtex-4 commercial FPGAs designed for spaceflight applications requiring compute intensive on-board processing. The MISSE-7 SpaceCube’s purpose was {{to serve as an}} “on-orbit” test-bed for demonstrating “radiation hardened by software” program <b>execution</b> and <b>error</b> <b>detection</b> and correction techniques that will help enable the use of commercial processing devices in space.|$|R
40|$|Knowledge {{workers are}} {{frequently}} bombarded with interruption and {{are required to}} constant multitask. Previous observational studies found that frequent interrupted activities cause more errors and induce feelings of stress and frustration. Therefore, {{the aim of the}} current research is to investigate how interruption affects <b>error</b> <b>detection</b> performance. Current <b>error</b> <b>detection</b> research focused on the effectiveness of different checking methods. In this thesis, we concentrate on the psychological mechanism of <b>error</b> <b>detection.</b> A series of experiments was carried out {{to examine the effects of}} self-interruption (i. e. the pilot study and Study I) and external interruption (Study II) on <b>error</b> <b>detection</b> performance respectively. The pilot study and Study I focus on the effects of working memory (WM) load and capacity. The pilot study employed a think-aloud technique to verify the predictions on WM and self-interruption. The results suggest that low-capacity individuals (LWMC) rehearsed more frequent than high-capacity individuals (HWMC). In other words, LWMC have more self-initiated interruptions during the primary <b>error</b> <b>detection</b> task. Study I was carried out to test the generated predictions from the pilot study. A reliable interaction effect WM load × capacity was found: LWMC performed significantly worse in higher WM load conditions; however, HWMC’s performances were unaffected by higher WM load. Study II focuses on the effect of interruption task types and position. There was no difference between the different interruption task types proposed. However, a significant main effect was found in interruption position: participants performed significantly worse in terms of both <b>error</b> <b>detection</b> and resumption when they were interrupted just before the actual field is displayed (i. e. between-fields interruption) compared to when they can see what is in the field (i. e. within-field interruption). The results are explained in terms of Salvucci and Taategen’s (2008) threaded cognition. The concurrent <b>execution</b> of <b>error</b> <b>detection</b> and WM tasks in Study I is interpreted as concurrent multitasking performance; whereas the sequential <b>execution</b> of <b>error</b> <b>detection</b> and interrupting tasks is interpreted as sequential multitasking. The current study contributed to the understanding of <b>error</b> <b>detection</b> performance by examining the roles of both self-interruption and external interruption and extends the application boundary of threaded cognition to interpret the effect of interruptions...|$|R
40|$|Abstract—Symbolic {{execution}} is a powerful, systematic analy-sis {{that has}} received much visibility in the last decade. Scalability however remains a major challenge for symbolic execution. Com-positional analysis is a well-known general purpose methodology for increasing scalability. This paper introduces a new approach for compositional symbolic execution. Our key insight {{is that we can}} summarize each analyzed method as a memoization tree that captures the crucial elements of symbolic execution, and leverage these memoization trees to efficiently replay the symbolic execution of the corresponding methods with respect to their calling contexts. Memoization trees offer a natural way to compose in the presence of heap operations, which cannot be dealt with by previous work that uses logical formulas as summaries for compositional symbolic execution. Our approach also enables efficient target oriented symbolic <b>execution</b> for <b>error</b> <b>detection</b> or program coverage. Initial experimental evaluation based on a prototype implementation in Symbolic PathFinder shows that our approach can be up to an order of magnitude faster than traditional non-compositional symbolic execution. I...|$|R
40|$|Abstract. Current {{symbolic}} execution is challenged by {{its ability to}} deal with loops. The case gets worse for loops manipulating recursive data structures. In this paper, we extend classic symbolic <b>execution</b> techniques for <b>error</b> <b>detection</b> of programs manipulating lists in loops. The idea is to enhance the {{symbolic execution}} with the utilization of quantitative aspect of the shape, and to construct the exit state of the loop. The exit state is constrained by a set of numeric constraints containing normal symbolic variables in programs and instrumented symbolic variables on the shapes. A prototype tool has been implemented and experiments are conducted on some commonly used list manipulating programs...|$|R
40|$|This paper {{describes}} a checkpoint comparison and optimistic <b>execution</b> technique for <b>error</b> <b>detection</b> and recovery in distributed and parallel systems. The approach {{is based on}} lookahead execution and rollback validation. It uses replicated tasks executing on different processors for forward recovery and checkpoint comparison for <b>error</b> <b>detection.</b> Two schemes derived from this strategy are analyzed and compared with triplication and voting, and with two common backward recovery methods. The impact of checkpoint time, checkpoint validation time. and process restart time is also examined. An implementation on a Sun NFS network with six benchmark programs is presented. Compared with classic checkpointing and rollback techniques, our strategy provides rapid recovery and requires, on average, fewer processors than standard replication and voting methods. This strategy is useful in systems where spare processors {{are available at the}} time of recovery. Key Words: fault tolerant computing, checkpointing, <b>error</b> <b>detection,</b> and <b>error</b> re-covery. [This docurnent has been approved fo public release and sale; its distribution is unlimited...|$|R
40|$|BACKGROUND: Brain-machine {{interfaces}} (BMIs) {{can translate}} the neuronal activity underlying a user's movement intention into movements of an artificial effector. In spite of continuous improvements, errors in movement decoding {{are still a}} major problem of current BMI systems. If {{the difference between the}} decoded and intended movements becomes noticeable, it may lead to an <b>execution</b> <b>error.</b> Outcome errors, where subjects fail to reach a certain movement goal, are also present during online BMI operation. Detecting such errors can be beneficial for BMI operation: (i) errors can be corrected online after being detected and (ii) adaptive BMI decoding algorithm can be updated to make fewer errors in the future. METHODOLOGY/PRINCIPAL FINDINGS: Here, we show that error events can be detected from human electrocorticography (ECoG) during a continuous task with high precision, given a temporal tolerance of 300 - 400 milliseconds. We quantified the <b>error</b> <b>detection</b> accuracy and showed that, using only a small subset of 2 × 2 ECoG electrodes, 82 % of detection information for outcome error and 74 % of <b>detection</b> information for <b>execution</b> <b>error</b> available from all ECoG electrodes could be retained. CONCLUSIONS/SIGNIFICANCE: The <b>error</b> <b>detection</b> method presented here could be used to correct errors made during BMI operation or to adapt a BMI algorithm to make fewer errors in the future. Furthermore, our results indicate that smaller ECoG implant could be used for <b>error</b> <b>detection.</b> Reducing the size of an ECoG electrode implant used for BMI decoding and <b>error</b> <b>detection</b> could significantly reduce the medical risk of implantation...|$|R
30|$|DeAngelo (1981) defined audit {{quality as}} “the market-assessed joint {{probability}} that a given auditor will both (a) discover a breach in a client’s accounting system and (b) report the breach.” A number of scholars emphasized that, for auditors, an audit quality implies that the audit is accomplished according to the methodology or guideline defined by the audit authority. As for audit authorities, an audit quality means that the audit report can withstand the challenge in court (Knechel et al. 2013). As noted by social scientists, factors that can improve audit quality include (1) intensive training (Knechel et al. 2013), (2) audit specialization and <b>execution</b> of <b>error</b> <b>detection,</b> procedure analysis, audit risk evaluation, and internal control deficiency discovery (Stephens 2011), (3) {{the knowledge and skills}} to make professional decisions (Knechel 2010; Bobek et al. 2012), and (4) the professionalism of the auditors (Nagy 2012). Specialization has become more important in the current auditing environment, and the auditing team characteristic has evolved into one of the crucial factors for audit quality. In today’s dynamic and demanding economic environment, professional auditors need to maintain competence and knowledge of current developments to enable them to act with due skill and care. Continuing professional development (CPD) enables a professional auditor to develop and maintain the capabilities to perform competently within the professional environment.|$|R
40|$|Safety-relevant {{systems in}} the {{automotive}} domain often implement features such as lockstep <b>execution</b> for <b>error</b> <b>detection,</b> and reset and re-execution for error correction. Light-lockstep has already been adopted in some such systems due to its relatively low-implementation cost given {{that it does not}} require deep changes into nonlockstep hardware. Instead, as only off-core activities (i. e., data/addresses sent) need to be compared across different cores, light-lockstep designs are lowly intrusive. This approach has been proven sufficient to guarantee functional correctness of the system in the presence of errors in the cores, in particular in relation with certification against safety standards such as ISO 26262 in the automotive domain. However, <b>error</b> <b>detection</b> in light-lockstep systems may occur long after the error actually occurs, thus jeopardizing timing guarantees, which are as critical as functional ones in hard real-time systems. In this paper, we analyze the timing behavior of errors due to transient and permanent faults in light-lockstep systems. Our results show that the time elapsed until an error is detected can be inordinately large, especially for permanent faults. Based on this observation and building upon the specific characteristics of light-lockstep systems, we propose lightly verbose (LiVe), a new mechanism to enforce the early <b>detection</b> of <b>errors,</b> due to both transient and permanent faults, thus enabling the computation of tight <b>error</b> <b>detection</b> timing bounds. We also analyze how existing mechanisms for error recovery in multicore systems increase their effectiveness when light-lockstep operates in LiVe mode in the context of mixed-criticality workloads. The research leading to these results has received funding from the ARTEMIS Joint Undertaking VeTeSS project under grant agreement number 295311. This work has also been funded by the Ministry of Science and Technology of Spain under contract TIN 2012 - 34557 and HiPEAC. Jaume Abella has been partially supported by the Ministry of Economy and Competitiveness under Ramon y Cajal postdoctoral fellowship number RYC- 2013 - 14717. Peer ReviewedPostprint (author's final draft...|$|R
5000|$|There {{has been}} work {{addressing}} soft errors in processor and memory resources using both {{hardware and software}} techniques. Several research efforts addressed soft errors by proposing <b>error</b> <b>detection</b> and recovery via hardware-based redundant multi-threading.These approaches used special hardware to replicate an application <b>execution</b> to identify <b>errors</b> in the output, which increased hardware design complexity and cost including high performance overhead. Software-based soft error tolerant schemes, one the other hand, are flexible and can be apply on commercial off-the-shelf microprocessors. Many works propose compiler-level instruction replication and result checking for soft <b>error</b> <b>detection.</b>|$|R
30|$|DeAngelo (1981) defined audit {{quality as}} “the market-assessed joint {{probability}} that a given auditor will both (a) discover a breach in a client’s accounting system and (b) report the breach.” Based on this definition, audit quality {{can be broken down}} into two components: (1) the likelihood that an auditor discovers existing misstatements and (2) the likelihood that an auditor appropriately reacts to the discovery. The first component links to an auditor’s competence and degree of effort, while the latter relates to an auditor’s objectivity, professional skepticism and independence. In addition, the effect of audit quality should be determined according to the maturity of the execution conditions of all key factors that influence the mission performance of audit authorities. Knechel et al. (2013) stated that audit quality is conceived differently in different aspects. For the economic supervision, high audit quality means no major mistakes in the financial report. On the other hand, supervising management authority emphasizes high audit quality ought to meet professional standards. For auditors, high audit quality implies that the audit is accomplished according to the methodology or guideline defined by the audit authority. Whereas for audit authorities, high audit quality means that the audit report can withstand the challenge of court. As noted by social scientists, factors that can improve audit quality include (1) intensive training (Knechel et al. 2013), (2) audit specialization and <b>execution</b> of <b>error</b> <b>detection,</b> procedure analysis, audit risk evaluation, and internal control deficiency discovery (Stephens 2011), (3) the knowledge and skills to make professional decisions (Knechel 2010; Bobek et al. 2012), and (4) the professionalism of the auditors (Nagy 2012). Quality of people, processes, and business plans, those are vital for conducting an efficient and effective audit.|$|R
40|$|The Hierarchical Dirichlet Process Hidden Markov model (HDP-HMM) is a Bayesian non {{parametric}} {{extension of}} the classical Hidden Markov Model (HMM) that allows to infer posterior probability over the cardinality of the hidden space, thus avoiding the necessity of cross-validation arising in standard EM training. This paper presents the application of Hierarchical Dirichlet Process Hidden Markov Models (HDP-HMM) to <b>error</b> <b>detection</b> during a robotic assembly task. Force sensor data is recorded for successful and failed task executions and man- ually labeled. An HDP-HMM is then fit {{to a set of}} training trials for each task execution outcome. We show how posteriors on the learned models could be used to recognize on-line deviation from expected behavior, thus allowing the robotic system to promptly react to task <b>execution</b> <b>errors.</b> status: publishe...|$|R
40|$|Action {{monitoring}} {{has been}} studied in many tasks by means of measuring the error-related negativity (Ne/ERN), but never in a motor control task requiring precise force production. Errors in discrete choice reaction tasks {{are the result of}} incorrect selections, but errors in force production can also arise from incorrect executions. ERPs were obtained while participants produced low or high isometric forces with their left or right hand. As expected, incorrect choices of hand elicited an Ne/ERN. Interestingly, Ne/ERNs were also present in the less discrete selection error of an incorrect choice of force, but only when erroneously a low instead of a high force was chosen. In both force ranges, no Ne/ERNs were found after <b>errors</b> in <b>execution.</b> These <b>errors</b> showed a large positivity in feedback ERPs and, similar to correct responses, a prolonged negativity in response ERPs. We propose that, compared to selection errors, the time uncertainty aspects of <b>execution</b> <b>errors</b> and the resulting changing response representations prohibit <b>error</b> <b>detection</b> by the internal monitoring system responsible for generating the Ne/ERN...|$|R
40|$|This {{research}} examines <b>error</b> <b>detection</b> strategies as {{a method}} for ensuring effective World Wide Web accessibility for older adults. It evaluates the underpinnings of web accessibility and their relevance to <b>error</b> <b>detection</b> strategies {{for the support of}} older adults. The research provides a contextual definition of computer systems and an account of how <b>error</b> <b>detection</b> relates to accessibility. The <b>Error</b> <b>Detection</b> System strategies focused on developing profiles of the participants. The profiles (self-assessment, testing, observation, and <b>error</b> <b>detection)</b> were used to modify webpages that the participant accessed. This research compares the performance of each profile, using a task list and error collection from the <b>Error</b> <b>Detection</b> System. Different <b>error</b> <b>detection</b> strategies that may be employed are presented, as well as their potential in the development of <b>error</b> <b>detection</b> strategies...|$|R
40|$|The {{purpose of}} a type system is to prevent the {{occurrence}} of <b>execution</b> <b>errors</b> during the running of a program. The accuracy of this informal statement depends on the rather subtle issue of what constitutes an <b>execution</b> <b>error.</b> Even when that is settled, the type soundness of a programming language (the absence of certain <b>execution</b> <b>errors</b> in all program runs) is a non-trivial property. A fair amount of careful analysis is required to avoid false and embarrassin...|$|R
40|$|Due to the {{continuously}} decreasing feature {{sizes and}} the increasing complexity of integrated circuits, commercial off-the-shelf (COTS) hardware is becoming less and less reliable. However, dedicated reliable hardware is expensive and usually slower than commodity hardware. Thus, economic pressure will most likely result in the usage of unreliable COTS hardware in safety-critical systems. The usage of unreliable, COTS hardware in safety-critical systems results in the need for software-implemented solutions for handling <b>execution</b> <b>errors</b> caused by this unreliable hardware. In this thesis, we provide techniques for detecting hardware errors that disturb the execution of a program. The detection provided facilitates handling of these errors, for example, by retry or graceful degradation. We realize the <b>error</b> <b>detection</b> by transforming unsafe programs that are not guaranteed to detect <b>execution</b> <b>errors</b> into safe programs that detect <b>execution</b> <b>errors</b> with a high probability. Therefore, we use arithmetic AN-, ANB-, ANBD-, and ANBDmem-codes. These codes detect errors that modify data during storage or transport and errors that disturb computations as well. Furthermore, the <b>error</b> <b>detection</b> provided is independent of the hardware used. We present the following novel encoding approaches: - Software Encoded Processing (SEP) that transforms an unsafe binary into a safe execution at runtime by applying an ANB-code, and - Compiler Encoded Processing (CEP) that applies encoding at compile time and provides different levels of safety by using different arithmetic codes. In contrast to existing encoding solutions, SEP and CEP allow to encode applications whose data and control flow is not completely predictable at compile time. For encoding, SEP and CEP use our set of encoded operations also presented in this thesis. To {{the best of our}} knowledge, we are the first ones that present the encoding of a complete RISC instruction set including boolean and bitwise logical operations, casts, unaligned loads and stores, shifts and arithmetic operations. Our evaluations show that encoding with SEP and CEP significantly reduces the amount of erroneous output caused by hardware errors. Furthermore, our evaluations show that, in contrast to replication-based approaches for detecting errors, arithmetic encoding facilitates the detection of permanent hardware errors. This increased reliability does not come for free. However, unexpectedly the runtime costs for the different arithmetic codes supported by CEP compared to redundancy increase only linearly, while the gained safety increases exponentially...|$|R
40|$|In (re) {{learning}} of movements, haptic guidance {{can be used}} to direct the needed adaptations in motor control. Haptic guidance influences the main driving factors of motor adaptation, <b>execution</b> <b>error,</b> and control effort in different ways. Human-control effort is dissipated in the interactions that occur during haptic guidance. Minimizing the control effort would reduce the interaction forces and result in adaptation. However, guidance also decreases the magnitude of the <b>execution</b> <b>errors,</b> which could inhibit motor adaptation. The aim {{of this study was to}} assess how different types of haptic guidance affect kinematic adaptation in a novel visuomotor task. Five groups of subjects adapted to a reaching task in which the visual representation of the hand was rotated 30 °. Each group was guided by a different force field. The force fields differed in magnitude and direction in order to discern the adaptation based on <b>execution</b> <b>errors</b> and control effort. The results demonstrated that the <b>execution</b> <b>error</b> did indeed play a key role in adaptation. The more the guiding forces restricted the occurrence of <b>execution</b> <b>errors,</b> the smaller the amount and rate of adaptation. However, the force field that enlarged the <b>execution</b> <b>errors</b> did not result in an increased rate of adaptation. The presence of a small amount of adaptation in the groups who did not experience <b>execution</b> <b>errors</b> during training suggested that adaptation could be driven on a much slower rate and on the basis of minimization of control effort as was evidenced by a gradual decrease of the interaction forces during training. Remarkably, also in the group in which the subjects were passive and completely guided, a small but significant adaptation occurred. The conclusion is that both minimization of <b>execution</b> <b>errors</b> and control effort drives kinematic adaptation in a novel visuomotor task, but the latter at a much slower rate...|$|R
40|$|<b>Error</b> <b>detection</b> by fragile watermarking* Abstract Error {{concealment}} {{techniques are}} useful in video transmission over channels that introduce bit errors. The efficiency and result of error concealment technique, however, rely on the <b>error</b> <b>detection</b> capabilities of video decoders. A novel <b>error</b> <b>detection</b> technique employing fragile watermarking is proposed in this paper. By embedding a fragile watermark on the quantized DCT coefficients and examining its integrity on the decoder side, the <b>error</b> <b>detection</b> capability of video decoders is significantly increased compared to widely used syntax-based <b>error</b> <b>detection</b> schemes...|$|R
2500|$|Theorem (Burst <b>error</b> <b>detection</b> ability). The burst <b>error</b> <b>detection</b> {{ability of}} any [...] code is ...|$|R
40|$|In {{the current}} study, we {{investigated}} bilingual <b>error</b> <b>detection</b> {{by measuring the}} repair rate of language intrusions (i. e., involuntary production of nontarget language words) that arose while bilinguals produced sentences in a language switching context. This allowed us to compare two prominent accounts of <b>error</b> <b>detection</b> in a bilingual setting. According to the conflict monitoring account, <b>error</b> <b>detection</b> is initiated by interference. Since language switching increases bilingual language interference, <b>error</b> <b>detection</b> should be better in switch relative to repetition trials. According to the perceptual loop theory, <b>error</b> <b>detection</b> is based on language comprehension. Since language switching is known to impair language comprehension, it follows that <b>error</b> <b>detection</b> should be worse in switch relative to repetition trials. The {{results showed that the}} repair rate of language intrusions was higher in switch than repetition trials, thus providing evidence that bilingual language interference instigates <b>error</b> <b>detection,</b> in line with the conflict monitoring account...|$|R
40|$|This paper {{proposes a}} pure {{software}} technique, <b>Error</b> <b>Detection</b> by Duplicated Instructions (EDDI), for detecting errors during normal system operation. Compared to other <b>error</b> <b>detection</b> techniques that use hardware redundancy, our method {{does not require}} any hardware modifications to add <b>error</b> <b>detection</b> capability to the original syste...|$|R
40|$|A {{trade-off}} analysis between maneuver period, <b>execution</b> <b>errors,</b> and {{orbit determination}} uncertainties {{is carried out}} for the Ocean Topography Experiment spacecraft for a given nodal equatorial constraint. Semimajor axis and eccentricity are controlled with minimum impulse using the linear theory of optimal transfer between close coplanar near-circular orbits. Ellipses of equal minimum and average maneuver periods are presented in the (3 <b>execution</b> <b>error,</b> 3 orbit determination uncertainty) space for different nodal equatorial constraints enabling {{the determination of the}} appropriate combination of <b>execution</b> <b>errors</b> and orbit determination uncertainties that guarantees a mission required minimum maneuver period for a given nodal deadband...|$|R
40|$|Organizational {{databases}} have {{a significant}} rate of data errors and detecting and correcting these errors can be problematic. This paper builds on a stream of research demonstrating that users of these databases can detect data errors under certain circumstances. A theory of <b>error</b> <b>detection</b> and research {{on the effect of}} base rate expectations in probabilistic judgement tasks are applied to the development of two propositions about <b>error</b> <b>detection.</b> It is argued that expectations about the base rate of errors in data affect <b>error</b> <b>detection</b> performance when they are developed through direct experience and that incentives affect <b>error</b> <b>detection</b> performance. The two research propositions are tested in a laboratory experiment. Experience-based expectations about the base rate of errors and incentives are found to affect <b>error</b> <b>detection</b> performance. End-user computing Data quality <b>Error</b> <b>detection...</b>|$|R
50|$|Beside framing, {{data link}} layers also include {{mechanisms}} {{to detect and}} even recover from transmission errors. For a receiver to detect transmission error, the sender must add redundant information (in the form of bits) as an <b>error</b> <b>detection</b> code to the frame sent. When the receiver obtains a frame with an <b>error</b> <b>detection</b> code it recomputes it and verifies whether the received <b>error</b> <b>detection</b> code matches the computed <b>error</b> <b>detection</b> code. If they match the frame {{is considered to be}} valid.|$|R
40|$|Error {{concealment}} {{techniques are}} useful in video transmission over channels that introduce bit errors. The efficiency and result of error concealment technique, however, rely on the <b>error</b> <b>detection</b> capabilities of video decoders. A novel <b>error</b> <b>detection</b> technique employing fragile watermarking is proposed in this paper. By embedding a fragile watermark on the quantized DCT coefficients and examining its integrity on the decoder side, the <b>error</b> <b>detection</b> capability of video decoders is significantly increased compared to widely used syntax-based <b>error</b> <b>detection</b> schemes...|$|R
40|$|This paper {{presents}} a theoretical comparison of different existing data <b>error</b> <b>detection</b> techniques. The techniques are compared by fault coverage, memory overhead and performance overhead. For this comparison, ten different data <b>error</b> <b>detection</b> techniques {{are taken into}} account. In general, the best <b>error</b> <b>detection</b> technique always has the highest fault coverage with low performance and memory overhead. After performing the theoretical comparison, we conclude that GA (genetic algorithm) and SWIFT (software implemented fault tolerance) techniques are the best techniques for data <b>error</b> <b>detection.</b> status: publishe...|$|R
40|$|Recent {{experiments}} {{have demonstrated that}} error-related negativity (ERN) is not only elicited when people commit errors, but also when they observe others committing errors. The present study investigates whether this observed ERN is also present when participants observe <b>execution</b> <b>errors</b> in an everyday context. Participants observed short sequences of pictures showing steps of everyday actions ending either erroneously or correctly. Participants were instructed to indicate by a delayed response whether the observed action was correctly executed or not. The results showed a large P 300 for <b>execution</b> <b>errors</b> compared with the observation of correct sequences, but no ERN activity was found. The present experiment indicates that the <b>detection</b> of <b>execution</b> <b>errors</b> in observation does not rely on the error processing mechanism responsible for generating the ERN. The increased P 300 amplitudes suggest a more general monitoring process that signals that the occurrence of unexpected events {{is involved in the}} <b>detection</b> of <b>execution</b> <b>errors...</b>|$|R
40|$|Abstract — Cloud {{computing}} infrastructures {{support system}} and network fault-tolerance. They transparently repair and prevent communication and software errors. They also allow duplication and migration {{of jobs and}} data to prevent hardware failures. However, only limited {{work has been done}} so far on application resilience, i. e., the ability to resume normal <b>execution</b> after <b>errors</b> and abnormal <b>executions</b> in distributed environments and clouds. This paper addresses open issues and solutions for application <b>errors</b> <b>detection</b> and management. It also overviews a testbed used to to design, deploy, execute, monitor, restart and resume distributed applications on cloud infrastructures in cases of failures...|$|R
40|$|The article {{describes}} three methods of gross <b>error</b> <b>detection</b> and their localization in geodetic surveying. The prerequisite for any gross <b>error</b> <b>detection</b> procedure is {{the availability of}} a set of redundant observations. The global model test with Data Snooping is the most commonly used method for gross <b>error</b> <b>detection,</b> however, it assumes that the a priori precision of observations is reliably known. As alternatives, the τ test and the Danish method are presented. An example of gross <b>error</b> <b>detection</b> in a plane cross-braced quadrilateral is given for all three methods...|$|R
40|$|<b>Error</b> <b>detection</b> {{incorporated}} with automatic-repeat-request (ARQ) {{is widely}} used for error control in data communication systems. This method of error control is simple and provides high system reliability. If a properly chosen code is used for <b>error</b> <b>detection,</b> virtually error-free data transmission can be attained. Various types of ARQ and hybrid ARQ schemes, and <b>error</b> <b>detection</b> using linear block codes are surveyed...|$|R
40|$|In {{this paper}} we present low-cost, {{concurrent}} checking methods for multiple <b>error</b> <b>detection</b> in S-boxes of symmetric block ciphers. These are redundancy-based fault detection schemes. We describe some studies of parity based concurrent <b>error</b> <b>detection</b> in S-boxes. Probability of multiple <b>error</b> <b>detection</b> is analyzed for random data. In this work 48 -input, 32 -output substitution blocks {{are taken into}} consideration. ...|$|R
40|$|Abstract—Embedded control {{networks}} commonly use checksums {{to detect}} data transmission errors. However, design decisions about which checksum to use are {{difficult because of}} a lack of information about the relative effectiveness of available options. We study the <b>error</b> <b>detection</b> effectiveness of the following commonly used checksum computations: exclusive or (XOR), two’s complement addition, one’s complement addition, Fletcher checksum, Adler checksum, and cyclic redundancy codes (CRCs). A study of <b>error</b> <b>detection</b> capabilities for random independent bit errors and burst errors reveals that the XOR, two’s complement addition, and Adler checksums are suboptimal for typical network use. Instead, one’s complement addition should be used for networks willing to sacrifice <b>error</b> <b>detection</b> effectiveness to reduce computational cost, the Fletcher checksum should be used for networks looking for a balance between <b>error</b> <b>detection</b> and computational cost, and CRCs should be used for networks willing to pay a higher computational cost for significantly improved <b>error</b> <b>detection.</b> Index Terms—Real-time communication, networking, embedded systems, checksums, <b>error</b> <b>detection</b> codes. Ç...|$|R
40|$|From the {{literature}} on <b>error</b> <b>detection,</b> the authors select several concepts relating <b>error</b> <b>detection</b> mechanisms and prospective memory features. They emphasize {{the central role of}} intention in the classification of the errors into slips/lapses/mistakes, in the error handling process and in the usual distinction between action-based and outcome-based detection. Intention is again a core concept in their investigation of prospective memory theory, where they point out the contribution of intention retrievals, intention persistence and output monitoring in the individual's possibilities for detecting their errors. The involvement of the frontal lobes in prospective memory and in <b>error</b> <b>detection</b> is also analysed. From the chronology of a prospective memory task, the authors finally suggest a model for <b>error</b> <b>detection</b> also accounting for neural mechanisms highlighted by studies on error-related brain activity. <b>Error</b> <b>detection</b> has not received much attention from the scientists since human error has been shown as the main cause of accident in complex systems. However, reducing the consequences of error depends largely on <b>error</b> <b>detection.</b> The goal {{of this paper is to}} synthesize the existing scientific knowledge on <b>error</b> <b>detection,</b> mostly based on studies conducted in laboratory or self reporting and to complete it through the analysis of a corpus of cases collected in a complex system: anaesthesia, in order to better describe how this knowledge can be used to improve our understanding of <b>error</b> <b>detection</b> modes. We used an anaesthesia accident reporting system we developed and organized at two Belgium University Hospitals to collect information about the <b>error</b> <b>detection</b> patterns. Results show that <b>detection</b> of <b>errors</b> principally occurred through standard check. We found significant relationships between the type of error, the type of <b>error</b> <b>detection</b> pattern, and the training level of the anaesthetist who committed the error. Peer reviewe...|$|R
30|$|Route <b>error</b> <b>detection</b> in AOMR-LM {{is similar}} to route <b>error</b> <b>detection</b> in AOMDV. It is {{launched}} when a link fails between two nodes along a path from a source to a destination.|$|R
