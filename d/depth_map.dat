1573|1371|Public
25|$|Marr's 2.5D sketch {{assumes that}} a <b>depth</b> <b>map</b> is constructed, {{and that this}} map {{is the basis of}} 3D shape perception. However, both {{stereoscopic}} and pictorial perception, as well as monocular viewing, make clear that the perception of 3D shape precedes, and does not rely on, the perception of the depth of points. It is not clear how a preliminary <b>depth</b> <b>map</b> could, in principle, be constructed, nor how this would address the question of figure-ground organization, or grouping. The role of perceptual organizing constraints, overlooked by Marr, in the production of 3D shape percepts from binocularly-viewed 3D objects has been demonstrated empirically for the case of 3D wire objects, e.g. For a more detailed discussion, see Pizlo (2008).|$|E
25|$|Depth-aware cameras. Using {{specialized}} cameras such as {{structured light}} or time-of-flight cameras, one can generate a <b>depth</b> <b>map</b> {{of what is}} being seen through the camera at a short range, and use this data to approximate a 3d representation {{of what is being}} seen. These can be effective for detection of hand gestures due to their short range capabilities.|$|E
25|$|The <b>depth</b> <b>map,</b> {{along with}} other information, can then be used to {{generate}} 3D parallax effects, to apply filters individually to {{different parts of the}} image, such as blurring the background to focus on an object in the foreground, or to copy and paste an object from one photo into another, similar to those available with a Lytro camera. In mid-April, HTC released a software development kit that allows other apps {{to take advantage of the}} depth mapping system, and stated that the SDK will be used by the camera app on the Google Play edition.|$|E
30|$|Measuring {{the quality}} of <b>depth</b> <b>maps</b> has {{to take into account}} that <b>depth</b> <b>maps</b> are type of imagery which are not visualized per-se, but through {{rendered}} views.|$|R
40|$|<b>Depth</b> <b>maps</b> {{taken by}} the low cost Kinect sensor are often noisy and incomplete. Thus, {{post-processing}} for obtaining reliable <b>depth</b> <b>maps</b> is necessary for advanced image and video applications such as object recognition and multi-view rendering. In this paper, we propose adaptive directional filters that fill the holes and suppress the noise in <b>depth</b> <b>maps.</b> Specifically, novel filters whose window shapes are adaptively adjusted based on the edge direction of the color image are presented. Experimental results show that our method yields higher quality filtered <b>depth</b> <b>maps</b> than other existing methods, especially at the edge boundaries...|$|R
40|$|Figure 1 : Reconstruction pipeline. Input {{photographs}} (top left) depicting objects {{at different}} levels of detail. Multi-view stereo yields <b>depth</b> <b>maps</b> (bottom left), which inherit these multi-scale properties. Our system is able to fuse such <b>depth</b> <b>maps</b> and produce an adaptive mesh (right) with coarse regions as well as fine scale details (insets). Multi-view stereo systems can produce <b>depth</b> <b>maps</b> with large variations in viewing parameters, yielding vastly different sampling rates of the observed surface. We present a new method for surface reconstruction by integrating a set of registered <b>depth</b> <b>maps</b> with dramatically varying sampling rate. The method is based on the construction of a hierarchical signed distance field represented in an incomplete primal octree by incrementally adding triangulated <b>depth</b> <b>maps.</b> Due to the adaptive data structure, our algorithm is able to handle <b>depth</b> <b>maps</b> with varying scale and to consistently represent coarse, low-resolution regions as well as small details contained in high-resolution <b>depth</b> <b>maps.</b> A final surface mesh is extracted from the distance field by construction of a tetrahedral complex from the scattered signed distance values and applying the Marching Tetrahedra algorithm on the partition. The output is an adaptive triangle mesh that seamlessly connects coarse and highly detailed regions while avoiding filling areas without suitable input data...|$|R
2500|$|The main camera {{remained}} relatively unchanged, using an [...] "UltraPixel" [...] image sensor (OmniVision OV4688) composed of pixels that are 2.0µm in size. The UltraPixel sensor was updated to provide better color accuracy in lit photographs, and the device now includes a dual-tone flash. The main camera {{is accompanied by}} a second, 2-megapixel depth of field sensor (OmniVision OV2722) located directly above the main camera {{as a part of the}} device's [...] "Duo Camera" [...] system. The sensor analyzes the distance and position of elements within a photo, and generates a <b>depth</b> <b>map,</b> which is embedded within each photo.|$|E
2500|$|The {{information}} {{available for a}} refSNP cluster includes the basic information {{from each of the}} individual submissions (see “Submission”) as well as {{information available}} from combining the data from multiple submissions (e.g. heterozygosity, genotype frequencies). [...] Many tools are available to examine a refSNP cluster in greater <b>depth.</b> <b>Map</b> view shows the position of the variation in the genome and other nearby variations. Another tool, gene view reports the location of the variation within a gene (if it is in a gene), the old and new codon, the amino acids encoded by both, and whether the change is synonymous or non-synonymous. Sequence viewer shows the position of the variant in relation to introns, exons, and other distant and close variants. 3D structure mapping, which shows 3D images of the encoded protein is also available.|$|E
2500|$|Reverse {{engineering}} {{has determined}} that the Kinect's various sensors output video at a frame rate of ≈9Hz to 30Hz depending on resolution. The default RGB video stream uses 8-bit VGA resolution (640×480 pixels) with a Bayer color filter, but the hardware is capable of resolutions up to 1280x1024 (at a lower frame rate) and other colour formats such as UYVY. [...] The monochrome depth sensing video stream is in VGA resolution (640×480 pixels) with 11-bit depth, which provides 2,048 levels of sensitivity. [...] The Kinect can also stream the view from its IR camera directly (i.e.: before it has {{been converted into a}} <b>depth</b> <b>map)</b> as 640x480 video, or 1280x1024 at a lower frame rate. The Kinect sensor has a practical ranging limit of [...] distance when used with the Xbox software. The area required to play Kinect is roughly 6m2, although the sensor can maintain tracking through an extended range of approximately [...] The sensor has an angular field of view of 57° horizontally and 43° vertically, while the motorized pivot is capable of tilting the sensor up to 27° either up or down. The horizontal field of the Kinect sensor at the minimum viewing distance of ≈ is therefore ≈, and the vertical field is ≈, resulting in a resolution of just over [...] per pixel. The microphone array features four microphone capsules and operates with each channel processing 16-bit audio at a sampling rate of 16kHz.|$|E
40|$|An intra {{prediction}} {{algorithm is}} proposed aiming to efficiently encode smooth regions in <b>depth</b> <b>maps.</b> By taking the textureless characteristics of <b>depth</b> <b>maps</b> into account, only one single prediction direction instead of multiple prediction directions is sufficient in intra prediction of <b>depth</b> <b>maps.</b> Consequently, coding of the prediction direction can be skipped which results in lower computational complexity and higher coding efficiency for synthesised views. Department of Electronic and Information Engineerin...|$|R
40|$|Abstract—This paper {{focuses on}} the {{representation}} and view generation of three-dimensional (3 -D) scenes. In contrast to existing methods that construct a full 3 -D model or those that exploit geometric invariants, our representation consists of dense <b>depth</b> <b>maps</b> at several preselected viewpoints from an image sequence. Furthermore, instead of using multiple calibrated stationary cameras or range scanners, we derive our <b>depth</b> <b>maps</b> from image sequences captured by an uncalibrated camera with only approximately known motion. We propose an adaptive matching algorithm that assigns various confidence levels to different regions in the <b>depth</b> <b>maps.</b> Nonuniform bicubic spline interpolation is then used to fill in low confidence regions in the <b>depth</b> <b>maps.</b> Once the <b>depth</b> <b>maps</b> are computed at preselected viewpoints, the intensity and depth at these locations are used to reconstruct arbitrary views of the 3 -D scene. Specifically, the <b>depth</b> <b>maps</b> are regarded as vertices of a deformable 2 -D mesh, which are transformed in 3 -D, projected to 2 -D, and rendered to generate the desired view. Experimental results are presented to verify our approach. I...|$|R
40|$|We propose an {{extension}} of our previous work on spatial domain Time-of-Flight (ToF) data enhancement to the temporal domain. Our goal is to generate enhanced <b>depth</b> <b>maps</b> at the same frame rate of the 2 -D camera that, coupled with a ToF camera, constitutes a hybrid ToF multi-camera rig. To that end, we first estimate the motion between consecutive 2 -D frames, and then use it to predict their corresponding <b>depth</b> <b>maps.</b> The enhanced <b>depth</b> <b>maps</b> result from the fusion between the recorded 2 -D frames and the predicted <b>depth</b> <b>maps</b> by using our previous contribution on ToF data enhancement. The experimental {{results show that the}} proposed approach overcomes the ToF camera drawbacks; namely, low resolution in space and time and high level of noise within depth measurements, providing enhanced <b>depth</b> <b>maps</b> at video frame rate. © 2012 IEEE...|$|R
5000|$|... #Caption: 2D {{image and}} its <b>depth</b> <b>map,</b> lighter <b>depth</b> <b>map</b> areas are {{regarded}} as closer to a viewer ...|$|E
50|$|Enable the <b>depth</b> <b>map</b> test, and {{render the}} scene lit. Areas where the <b>depth</b> <b>map</b> test fails {{will not be}} overwritten, and remain shadowed.|$|E
50|$|As well {{as fully}} {{automatic}} methods of calculating depth maps (which {{may be more}} or less successful), depth maps can be drawn entirely by hand. Also developed are methods of producing depth maps from sparse or less accurate depth maps. A sparse <b>depth</b> <b>map</b> is a <b>depth</b> <b>map</b> consisting of only a relatively few lines or areas which guides the production of the full <b>depth</b> <b>map.</b> Use of a sparse <b>depth</b> <b>map</b> can help overcome auto-generation limitations. For example, if a depth finding algorithm takes cues from image brightness an area of shadow in the foreground may be incorrectly assigned as background. This misassignment is overcome by assigning the shaded area a close value in the sparse <b>depth</b> <b>map.</b>|$|E
40|$|Multi-view stereo {{systems can}} produce <b>depth</b> <b>maps</b> with large {{variations}} in viewing parameters, yielding vastly different sampling {{rates of the}} observed surface. We present a new method for surface reconstruction by integrating a set of registered <b>depth</b> <b>maps</b> with dramatically varying sampling rate. The method {{is based on the}} construction of a hierarchical signed distance field represented in an incomplete primal octree by incrementally adding triangulated <b>depth</b> <b>maps.</b> Due to the adaptive data structure, our algorithm is able to handle <b>depth</b> <b>maps</b> with varying scale and to consistently represent coarse, low-resolution regions as well as small details contained in high-resolution <b>depth</b> <b>maps.</b> A final surface mesh is extracted from the distance field by construction of a tetrahedral complex from the scattered signed distance values and applying the Marching Tetrahedra algorithm on the partition. The output is an adaptive triangle mesh that seamlessly connects coarse and highly detailed regions while avoiding filling areas without suitable input data...|$|R
30|$|Combining {{multiple}} <b>depth</b> <b>maps.</b> Combining several <b>depth</b> <b>maps</b> achieved {{from different}} view points is studied by Schuon et al. [28]. Combining range information {{to generate a}} more accurate result is a well-known approach in the 3 D society [29]. Another successful example was introduced by Izadi et al. [30] to fuse a sequence of <b>depth</b> <b>maps</b> generated by a kinect camera. We use information from several tone-mapped disparity maps and combine them to provide backward-compatible stereo matching results for HDR scenes.|$|R
40|$|Simulated stereo {{images and}} <b>depth</b> <b>maps</b> were {{generated}} using the open-source ray tracing program POV-Ray, operating {{in an environment}} containing obstacles. The image pairs were used to generate <b>depth</b> <b>maps</b> by finding the relative positions of matching groups (kernels) of image pixels. Kernel matching generated an average depth accuracy of better than 90 % over all images and pixels tested. When neighbourhood average and median filter smoothing techniques were applied to the resulting <b>depth</b> <b>maps,</b> the average accuracy exceeded 95 %. Occlusion and lighting differences between the images were attenuated by smoothing and kernel normalization. Computation times for the simulated <b>depth</b> <b>maps</b> indicate that kernel matching {{could be used in}} real video imagery on a vehicle moving at 30 km/hr...|$|R
50|$|A {{computer}} program {{can take a}} <b>depth</b> <b>map</b> and an accompanying pattern image to produce an autostereogram. The program tiles the pattern image horizontally to cover an area whose size {{is identical to the}} <b>depth</b> <b>map.</b> Conceptually, at every pixel in the output image, the program looks up the grayscale value of the equivalent pixel in the <b>depth</b> <b>map</b> image, and uses this value to determine the amount of horizontal shift required for the pixel.|$|E
5000|$|<b>Depth</b> <b>map</b> creation. Each {{isolated}} surface {{should be}} assigned a <b>depth</b> <b>map.</b> The separate depth maps should be composed into a scene <b>depth</b> <b>map.</b> This is an iterative process requiring adjustment of objects, shapes, depth, and visualization of intermediate results in stereo. Depth micro-relief, 3D shape {{is added to}} most important surfaces to prevent the [...] "cardboard" [...] effect when stereo imagery looks like a combination of flat images just set at different depths.|$|E
50|$|Once the light-space {{coordinates}} are found, the x and y values usually {{correspond to}} a {{location in the}} <b>depth</b> <b>map</b> texture, and the z value corresponds to its associated depth, which can now be tested against the <b>depth</b> <b>map.</b>|$|E
30|$|Artigas et al.[38] have {{proposed}} a method based on <b>depth</b> <b>maps.</b> Indeed, <b>depth</b> <b>maps</b> along with camera parameters allow to create a virtual view point, namely that of the WZ camera. However, in our work, we consider a different problem where depth information is not directly available.|$|R
40|$|Abstract – In this paper, {{we present}} a stereo vision system on {{programmable}} chip (SVSoC) for dense <b>depth</b> <b>mapping</b> and obstacle detection at video rate. The system is composed of three miniature CMOS cameras with triangular configuration and one FPGA chip for parallel computing. The system algorithm contains nonlinear iteration based cooperative algorithm for high quality dense <b>depth</b> <b>mapping,</b> ground extraction and obstacle location from dense <b>depth</b> <b>maps.</b> With the connection of DSP for moving control, the system is mounted on small hexapod robot for obstacle avoidance and navigation...|$|R
40|$|We {{describe}} {{in this document}} several <b>depth</b> <b>maps</b> estimation methods, in different video contexts. For standard (monocular) videos of fixed scene and moving camera, we present a technique to extract both the 3 D structure of the scene and the camera poses over time. These information are exploited to generate dense <b>depth</b> <b>maps</b> for each image of the video, through optical flow estimation algorithms. We also present a <b>depth</b> <b>maps</b> extraction method for multi-view sequences aiming at generating MVD content for 3 DTV. These works are compared to existing approaches used at writing time in the 3 DV group of MPEG for normalization purposes. Finally, we demonstrate how such <b>depth</b> <b>maps</b> can be exploited to perform relief auto-stereoscopic rendering, in a dynamic and interactive way, without sacrifying the real-time computation constraint...|$|R
5000|$|Depending on the {{intended}} use of a <b>depth</b> <b>map,</b> {{it may be useful}} or necessary to encode the map at higher bit depths. For example, an 8 bit <b>depth</b> <b>map</b> can only represent a range of up to 256 different distances.|$|E
50|$|A more {{sophisticated}} method involves {{use of a}} <b>depth</b> <b>map</b> (a false colour image where colour indicates distance, for example, a greyscale <b>depth</b> <b>map</b> could have lighter indicate an object closer to the viewer and darker indicate an object further away). As for preparing anaglyphs from stereo pairs, stand-alone software and plug-ins for some graphics apps exist which automate production of anaglyphs (and stereograms) from a single image or from an image and its corresponding <b>depth</b> <b>map.</b>|$|E
50|$|Using this convention, a {{grayscale}} <b>depth</b> <b>map</b> for {{the example}} autostereogram {{can be created}} with black, gray and white representing shifts of 0 pixels, 10 pixels and 20 pixels, respectively {{as shown in the}} greyscale example autostereogram. A <b>depth</b> <b>map</b> is the key to creation of random-dot autostereograms.|$|E
40|$|The recent {{progress}} in computer vision and multi-view stereo techniques permit to extract geometry from ordinary photo collections obtained from internet photo-sharing websites. This {{makes it easier}} than ever to capture highly detailed geometry of physical objects. In this work, different point-based methods are investigated to render and process the <b>depth</b> <b>maps</b> resulting from such a geometry acquisition technique. We present a multiresolution rendering approach well-suited for {{a larger number of}} <b>depth</b> <b>maps,</b> taking into account the redundancy and greatly varying sampling rate of <b>depth</b> <b>maps.</b> The method decomposes the input geometry according to the resolution of samples and stores them in multiresolution data structure. This data structure is used for memory efficient storage and level of detail selection throughout rendering. In a second, quite different approach, we adapt sequential point trees to render <b>depth</b> <b>maps,</b> thereby utilizing the graphics processor also for level of detail selection. Furthermore, we also investigate the resampling of <b>depth</b> <b>maps</b> by castings rays and intersecting them with a moving least squares surface...|$|R
40|$|Transmitting from sender {{compressed}} {{texture and}} <b>depth</b> <b>maps</b> of multiple viewpoints enables image synthesis at receiver from any intermediate virtual viewpoint via depth-image-based rendering (DIBR). We observe that quantized <b>depth</b> <b>maps</b> from different view-points {{of the same}} 3 D scene constitutes multiple descriptions (MD) of the same signal, thus {{it is possible to}} reconstruct the 3 D scene in higher precision at receiver when multiple <b>depth</b> <b>maps</b> are con-sidered jointly. In this paper, we cast the precision enhancement of 3 D surfaces from multiple quantized <b>depth</b> <b>maps</b> as a combinatorial optimization problem. First, we derive a lemma that allows us to increase the precision of a subset of 3 D points with certainty, simply by discovering special intersections of quantization bins (QB) from both views. Then, we identify the most probable voxel-containing QB intersections using a shortest-path formulation. Experimental results show that our method can significantly increase the precision of decoded <b>depth</b> <b>maps</b> compared with standard decoding schemes. Index Terms — Texture-plus-depth representation, 3 D recon-struction, multiple description...|$|R
40|$|For {{navigation}} and obstacle avoidance {{in the field}} of mobile robots, percep-tion and identification of the surrounding environment is necessary. <b>Depth</b> <b>maps</b> therefore provide an essential description of the world seen through cameras. This thesis will investigate with different types of stereo matching algo-rithms for calculating <b>depth</b> <b>maps.</b> The task is to build and to implement a system for generating <b>depth</b> <b>maps</b> on a mobile robot. Many stereo algorithms make use of the epipolar constraint, meaning that for a pixel in the left image the corresponding point in the right image lies on the same horizontal line, the epipolar line. This strong constraint is used to reduce the search space of the correspondence algorithms that calculates <b>depth</b> <b>maps.</b> To make use of the epipolar constraint the camera system needs to be calibrated first, to get the intrinsic and extrinsic camera parameters, i...|$|R
5000|$|... #Caption: Visualization of the <b>depth</b> <b>map</b> {{projected}} {{onto the scene}} ...|$|E
5000|$|This <b>depth</b> <b>map</b> must {{be updated}} any time there are changes {{to either the}} light or the objects in the scene, but can be reused in other situations, such as those where only the viewing camera moves. (If there are {{multiple}} lights, a separate <b>depth</b> <b>map</b> must be used for each light.) ...|$|E
50|$|The {{distance}} {{relationship between}} any pixel and its counterpart in the equivalent pattern {{to the left}} can be expressed in a <b>depth</b> <b>map.</b> A <b>depth</b> <b>map</b> is simply a grayscale image which represents the distance between a pixel and its left counterpart using a grayscale value between black and white. By convention, the closer the distance is, the brighter the color becomes.|$|E
40|$|In this paper, {{we propose}} an {{adaptation}} of camera pro-jection models for fisheye cameras into the plane-sweeping stereo matching algorithm. This adaptation allows us to do plane-sweeping stereo directly on fisheye images. Our ap-proach also works for other non-pinhole cameras such as omnidirectional and catadioptric cameras when using the unified projection model. Despite the simplicity of our pro-posed approach, {{we are able to}} obtain full, good quality and high resolution <b>depth</b> <b>maps</b> from the fisheye images. To verify our approach, we show experimental results based on <b>depth</b> <b>maps</b> generated by our approach, and dense models produced from these <b>depth</b> <b>maps.</b> 1...|$|R
40|$|This work {{presents}} a procedure for refining <b>depth</b> <b>maps</b> acquired using RGB-D (depth) cameras. With numerous new structured-light RGB-D cameras, acquiring high-resolution <b>depth</b> <b>maps</b> has become easy. However, {{there are problems}} such as undesired occlusion, inaccurate depth values, and temporal variation of pixel values when using these cameras. In this paper, a proposed method based on an exemplar-based inpainting method is proposed to remove artefacts in <b>depth</b> <b>maps</b> obtained using RGB-D cameras. Exemplar-based inpainting {{has been used to}} repair an object-removed image. The concept underlying this inpainting method is similar to that underlying the procedure for padding the occlusions in the depth data obtained using RGB-D cameras. Therefore, our proposed method enhances and modifies the inpainting method for application in and the refinement of RGB-D depth data image quality. For evaluating the experimental results of the proposed method, our proposed method was tested on the Tsukuba Stereo Dataset, which contains a 3 D video with the ground truths of <b>depth</b> <b>maps,</b> occlusion maps, RGB images, the peak signal-to-noise ratio, and the computational time as the evaluation metrics. Moreover, a set of self-recorded RGB-D <b>depth</b> <b>maps</b> and their refined versions are presented to show the effectiveness of the proposed method...|$|R
50|$|Most semiautomatic {{methods of}} stereo {{conversion}} use <b>depth</b> <b>maps</b> and depth-image-based rendering.|$|R
