28|33|Public
40|$|Abstract: How can {{readers and}} {{listeners}} infer temporal relations in discourse? One {{of the basic}} <b>inferential</b> <b>task</b> in discourse comprehension consists in inferring time direction, that is, in determining {{the time of the}} event relative to the thread of discourse. I show that the computation of directional inference {{is the result of the}} interaction contextual and linguistic information. Finally, I answer the question of the order of backward causal inferences. 1...|$|E
40|$|Abstract: We {{consider}} {{the problem of}} combining multiple dissimilarity representations via the Cartesian product of their embeddings. For concreteness, we choose the <b>inferential</b> <b>task</b> at hand to be classification. The high dimensionality of this Cartesian product space implies the necessity of dimensionality reduction before training a classifier. We propose a supervised dimensionality reduction method, which utilizes the class label information, to help achieve a favorable combination. The simulation and real data results show that our approach can improve classification accuracy compared to the alternatives of principal components analysis and no dimensionality reduction at all...|$|E
40|$|Diagnostic of {{production}} problems in geothermal wells {{is a complex}} <b>inferential</b> <b>task,</b> which requires considerable knowledge of its possible causes, careful assessment of (sometimes bewildering) multidisciplinary evidence, and, of course, enough experience. These characteristics make this task {{a good candidate for}} a computerized expert system. On this conviction, we have developed the first version of WELL-DR, an expert system for geothermal-well production diagnostics. Though still in a rapid stage of evolution, this expert system already provides a convenient and useful tool for geothermal field development, operation and management...|$|E
40|$|This {{current study}} {{explored}} {{the impact of}} individual differences in personality factors on interface interaction and learning performance in both an interactive visualization and a menu-driven web application. Participants were administered 6 psychometric measures designed to assess trait anxiety, locus of control, and other personality traits. Participants {{were then asked to}} complete 3 procedural <b>tasks</b> and 3 <b>inferential</b> <b>tasks</b> in each interface. Results demonstrated that participants with an external locus of control completed <b>inferential</b> <b>tasks</b> more quickly than those with an internal locus. Factor analysis of items from the 6 psychometric scales isolated a 9 -item short measure, which showed trending with procedural scores. Additionally, data demonstrated that the visualization interface was more effective and efficient for the completion of the <b>inferential</b> <b>tasks.</b> Participants also preferred the visualization to the web interface for both types of task. Implications and future directions of this research are also discussed. 1...|$|R
40|$|My paper {{considers}} {{the role of}} questions and answers in the <b>inferential</b> <b>tasks</b> of fact investigators (investigators such as police officers and crime scene examiners) prior to trial and indeed prior to arrest. My contention is that the reasoning processes of these investigators obey an internal, simple, successful, but dangerous, logic...|$|R
40|$|Dynamic {{treatment}} regimes, {{also known}} as treatment policies, are increasingly being used to operationalize sequential clinical decision making associated with patient care. Common approaches to constructing a dynamic treatment regime from data, such as Q-learning, employ non-smooth functionals of the data. Therefore, simple <b>inferential</b> <b>tasks</b> such as constructing a confidence interval for the parameters in the Q-function are complicated by nonregular asymptotics under certain commonly-encountered gen-erative models. Methods that ignore this nonregularity can suffer from poor perfor-mance in small samples. We construct confidence intervals for the parameters in th...|$|R
40|$|This {{paper is}} devoted to the {{following}} question: how can readers and listeners infer temporal relations in discourse? One of the basic <b>inferential</b> <b>task</b> in utterance and discourse interpretation consists in inferring time direction, that is, in determining the time of the event relative to the thread of discourse. In this paper, I reduce time direction to two temporal relations: forward inference (FI) and backward inference (BI). I show that the computation of directional inference is neither the result of principles of discourse, nor the consequence of discourse type, but the interaction of information coming from different sources, that is, contextual information and linguistic information. 1...|$|E
40|$|This paper {{develops}} methodology {{that provides}} a toolbox for routinely fitting complex models to realistic spatial point pattern data. We consider models {{that are based on}} log-Gaussian Cox processes and include local interaction in these by considering constructed covariates. This enables us to use integrated nested Laplace approximation and to considerably speed up the <b>inferential</b> <b>task.</b> In addition, methods for model comparison and model assessment facilitate the modelling process. The performance of the approach is assessed in a simulation study. To demonstrate the versatility of the approach, models are fitted to two rather different examples, a large rainforest data set with covariates and a point pattern with multiple marks. Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|We {{develop a}} Sequential Monte Carlo (SMC) {{procedure}} for inference in proba-bilistic graphical models using the divide-and-conquer methodology. The method {{is based on}} an auxiliary tree-structured decomposition of the model of interest turning the overall <b>inferential</b> <b>task</b> into a collection of recursively solved sub-problems. Unlike a standard SMC sampler, the proposed method employs multi-ple independent populations of weighted particles, which are resampled, merged, and propagated as the method progresses. We illustrate empirically that this ap-proach can outperform standard methods in estimation accuracy. It also opens up novel parallel implementation options and the possibility of concentrating the computational effort on the most challenging sub-problems. The proposed method is applicable to a broad class of probabilistic graphical models. We demonstrate its performance on a Markov random field and on a hierarchical Bayesian model. ...|$|E
40|$|The {{occurrence}} or nonoccurrence of {{an event}} D has inferential significance. We query n sources or sensors who make individual reports about he occurrence or nonoccurrence of D. Their reports are either mutually confirming or there is some pattern of conflict among them. In this paper we develop expressions termed adjusted likelihood ratios which prescribe the inferential or diagnostic impact of the joint confirming or conflicting reports from the n sources. These expressions combine information about the inferential impact of D (and its complement /)) with information about the reliability of each source. We only consider the {{case in which the}} reporting behavior of any subset of the sources is not itself an inferentially significant event. Appropriate independence and conditional independence assumptions are necessary. Our formulations of adjusted likelihood ratio are applicable to a variety of medical, legal, military, and other <b>inferential</b> <b>tasks...</b>|$|R
40|$|Transitive inference, class {{inclusion}} {{and a variety}} of other inferential abilities have strikingly similar developmental profiles— all are acquired around the age of five. Yet, little is known about the reasons for this correspondence. Category theory was invented as a formal means of establishing commonalities between various mathematical structures. We use category theory to show that transitive inference and class inclusion involve dual mathematical structures, called product and coproduct. Other <b>inferential</b> <b>tasks</b> with similar developmental profiles, including matrix completion, cardinality, dimensional changed card sorting, balance-scale (weight-distance integration), and Theory of Mind also involve these structures. By contrast, (co) products are not involved in the behaviours exhibited by younger children on these tasks, or simplified versions that are within their ability. These results point to a fundamental cognitive principle under development during childhood that is the capacity to compute (co) products in the categorical sense...|$|R
40|$|Finite {{probability}} distributions and {{compositional data}} are mathematically similar, consisting of D-dimensional positive vectors with sum 1. Despite this similarity the meaningful forms of analysis in these different areas may require substantially different concepts and statistical modelling. This paper highlights these differences, but also poses {{the question of}} how such differences may contribute to understanding in the different areas. At CoDa workshops we have become so accustomed to, even obsessed with, modelling all compositional data problems within a simplex sample space together with its algebraic-geometric Hilbert space structure. The context of this Hilbert sample space is certainly often relevant to the formulation of a number of compositional data problems, but its mathematical elegance should not override appropriate meaningful statistical modelling to resolve the real compositional problem. In this paper I illustrate some relevant modelling by consideration of how a variety of persons differ in their ability to perform <b>inferential</b> <b>tasks</b> particularly in the process of differential diagnosi...|$|R
40|$|Projectual {{abduction}} is {{the inference}} drawing {{the means to}} achieve an end. Planning {{a course of action}} is an <b>inferential</b> <b>task</b> and we claim that the relevant inference is abduction. We distinguish projectual abduction from epistemic abduction. While epistemic abduction aims to determine an explanatory relation, projectual abduction aims to determine a teleological relation. It is important to remind in any case that abduction does not stand by itself: as is true for epistemic abduction, projectual abduction has to be developed and evaluated by subsequent deduction and induction. After defining projectual abduction in section 1, we focus on the relations between projectual abduction, normativity and truth in section 2. Then we sketch how projectual abduction works in artistic creativity (section 3), in evolutionary and teleological processes (section 4) and finally in social processes (section 5...|$|E
40|$|Most {{statistics}} educators {{would agree}} that statistical inference is both the central objective of statistical reasoning {{and one of the}} most difficult ideas for students to understand. In traditional approaches, statistical inference is introduced as a quantitative problem, usually of figuring out the probability of obtaining an observed result on the assumption that the null hypothesis is true. In this article, we lay out an alternative approach towards teaching statistical inference that we are calling “informal inference. ” We begin by describing informal inference and then illustrate ways we have been trying to develop the component ideas of informal inference in a recent data analysis seminar with teachers; our particular emphasis in this article is on the ways in which teachers used TinkerPlots, a statistical visualization tool. After describing teachers ’ approaches to an <b>inferential</b> <b>task,</b> we offer some preliminary hypotheses about the conceptual issues that arose for them...|$|E
40|$|AbstractHidden Markov {{models have}} been used to restore {{recorded}} signals of single ion channels buried in background noise. Parameter estimation and signal restoration are usually carried out through likelihood maximization by using variants of the Baum–Welch forward–backward procedures. This paper presents an alternative approach for dealing with this <b>inferential</b> <b>task.</b> The inferences are made by using a combination of the framework provided by Bayesian statistics and numerical methods based on Markov chain Monte Carlo stochastic simulation. The reliability of this approach is tested by using synthetic signals of known characteristics. The expectations of the model parameters estimated here are close to those calculated using the Baum–Welch algorithm, but the present methods also yield estimates of their errors. Comparisons {{of the results of the}} Bayesian Markov Chain Monte Carlo approach with those obtained by filtering and thresholding demonstrate clearly the superiority of the new methods...|$|E
40|$|Dynamic {{treatment}} regimes, {{also known}} as treatment policies, are increasingly being used to operationalize sequential clinical decision making associated with patient care. Common approaches to constructing a dynamic treatment regime from data, such as Q-learning, employ non-smooth functionals of the data. Therefore, simple <b>inferential</b> <b>tasks</b> such as constructing a confidence interval for the parameters in the Q-function are complicated by nonregular asymptotics under certain commonly-encountered generative models. Methods that ignore this nonregularity can suffer from poor performance in small samples. We construct confidence intervals for the parameters in the Q-function by first constructing smooth, data-dependent, {{upper and lower bounds}} on these parameters and then applying the bootstrap. The confidence interval is adaptive in that although it is conservative for nonregular generative models, it achieves asymptotically exact coverage elsewhere. The small sample performance of the method is evaluated on a series of examples and compares favorably to previously published competitors. Finally, we illustrate the method using data from the Adaptive Interventions for Children with ADHD study (Pelham and Fabiano 2008). ...|$|R
40|$|A {{dynamic model}} of a multiagent system defines a {{probability}} distribution over possible system behaviors over time. Alternative representations for such models present tradeoffs in expressive power, and accuracy and cost for <b>inferential</b> <b>tasks</b> of interest. In a history-dependent representation, behavior {{at a given time}} is specified as a probabilistic function of some portion of system history. Models may be further distinguished based on whether they specify individual or joint behavior. Joint behavior models are more expressive, but in general grow exponentially in number of agents. Graphical multiagent models (GMMs) provide a more compact representation of joint behavior, when agent interactions exhibit some local structure. We extend GMMs to condition on history, thus supporting inference about system dynamics. To evaluate this hGMM representation we study a voting consensus scenario, where agents on a network attempt to reach a preferred unanimous vote through a process of smooth fictitious play. We induce hGMMs and individual behavior models from example traces, showing that the former provide better predictions, given limited history information. These hGMMs also provide advantages for answering general inference queries compared to sampling the true generative model...|$|R
40|$|Dynamic {{treatment}} regimes, {{also known}} as treatment policies, are increasingly being used to operationalize clinical decision making associated with long-term patient care. Common approaches to constructing a dynamic treatment regime from data, such as Q-learning, employ non-smooth functionals of the data. Therefore, simple <b>inferential</b> <b>tasks</b> such as constructing a confidence interval for the parameters in the Q-function are complicated by non-regular asymptotics under certain commonly-encountered generative models. Methods that ignore this non-regularity can suffer from poor performance in small samples. We construct confidence intervals for the parameters in the Q-function by constructing smooth, data-dependent, {{upper and lower bounds}} on these parameters and then applying the bootstrap. We prove that the proposed method provides asymptotically exact coverage regardless of the generative model. In addition, we show that in certain scenarios the bounds used in this method are tight even in finite samples. The small sample performance of the method is evaluated on a series of examples and compares favorably to previously published competitors. Finally, we illustrate the method on real data from the Adaptive Interventions for Children with ADHD study (Pelham and Fabiano 2008). 1...|$|R
40|$|We {{present a}} novel {{algorithm}} for probabilistic peak detection in first-order chromatographic data. Unlike conventional methods that deliver a binary answer {{pertaining to the}} expected {{presence or absence of}} a chromatographic peak, our method calculates the probability of a point being affected by such a peak. The algorithm makes use of chromatographic information (i. e. the expected width of a single peak and the standard deviation of baseline noise). As prior information of the existence of a peak in a chromatographic run, we make use of the statistical overlap theory. We formulate an exhaustive set of mutually exclusive hypotheses concerning presence or absence of different peak configurations. These models are evaluated by fitting a segment of chromatographic data by least-squares. The evaluation of these competing hypotheses can be performed as a Bayesian <b>inferential</b> <b>task.</b> We outline the potential advantages of adopting this approach for peak detection and provide several examples of both improved performance and increased flexibility afforded by our approach...|$|E
40|$|We {{propose a}} novel class of Sequential Monte Carlo (SMC) algorithms, {{appropriate}} for inference in probabilistic graphical models. This class of algorithms adopts a divide-and-conquer approach based upon an auxiliary tree-structured decomposition {{of the model}} of interest, turning the overall <b>inferential</b> <b>task</b> into a collection of recursively solved sub-problems. The proposed method is applicable to a broad class of probabilistic graphical models, including models with loops. Unlike a standard SMC sampler, the proposed Divide-and-Conquer SMC employs multiple independent populations of weighted particles, which are resampled, merged, and propagated as the method progresses. We illustrate empirically that this approach can outperform standard methods {{in terms of the}} accuracy of the posterior expectation and marginal likelihood approximations. Divide-and-Conquer SMC also opens up novel parallel implementation options and the possibility of concentrating the computational effort on the most challenging sub-problems. We demonstrate its performance on a Markov random field and on a hierarchical logistic regression problem...|$|E
40|$|For random graphs {{distributed}} {{according to}} a stochastic block model, we consider the <b>inferential</b> <b>task</b> of partioning vertices into blocks using spectral techniques. Spectral partioning using the normalized Laplacian and the adjacency matrix have both {{been shown to be}} consistent as the number of vertices tend to infinity. Importantly, both procedures require that the number of blocks and the rank of the communication probability matrix are known, even {{as the rest of the}} parameters may be unknown. In this article, we prove that the (suitably modified) adjacency-spectral partitioning procedure, requiring only an upper bound on the rank of the communication probability matrix, is consistent. Indeed, this result demonstrates a robustness to model mis-specification; an overestimate of the rank may impose a moderate performance penalty, but the procedure is still consistent. Furthermore, we extend this procedure to the setting where adjacencies may have multiple modalities and we allow for either directed or undirected graphs. Comment: 26 pages, 2 figur...|$|E
40|$|Variable {{dimensional}} problems, {{where not}} only the parameters, but also the number of parameters are random variables, offer quite challenging <b>inferential</b> <b>tasks</b> to the Bayesians. Although in principle the Reversible Jump Markov Chain Monte Carlo (RJMCMC) method-ology, a general MCMC method which can jump between different dimensions, {{is a response to}} such challenges, the dimension-hopping strategies need not be always convenient for practi-cal implementation, particularly because efficient “move-types ” having reasonable acceptance rates are often difficult to devise. In this article, we propose a novel and general dimension-hopping MCMC methodology that can update all the parameters as well as the number of parameters simultaneously using simple deterministic transformations of some low-dimensional (often one-dimensional) ran-dom variable. This methodology, which has been inspired by the recent Transformation based MCMC (TMCMC) (Dutta & Bhattacharya (2014)) for updating all the parameters simultane-ously in general fixed-dimensional set-ups using low-dimensional (usually one-dimensional) random variables, facilitates great speed in terms of computation time and provides high ac-ceptance rates, thanks to the low-dimensional random variables which effectively reduce the dimension dramatically. Quite importantly, our transformation based approach provides a nat-ural way to automate the move-types in the variable dimensional problems. We refer to thi...|$|R
40|$|The {{study of}} factor {{analytic}} models often has to address two important issues: (a) {{the determination of}} the “optimum” number of factors and (b) the derivation of a unique simple structure whose interpretation is easy and straightforward. The classical approach deals with these two tasks separately, and sometimes resorts to ad-hoc methods. This paper proposes a Bayesian approach to these two important issues, and adapts ideas from stochastic geometry and Bayesian finite mixture modelling to construct an ergodic Markov chain having the posterior distribution of the complete collection of parameters (including the number of factors) as its equilibrium distribution. The proposed method uses an Automatic Relevance Determination (ARD) prior as the device of achieving the desired simple structure. A Gibbs sampler updating scheme is then combined with the simulation of a continuous-time birth-and-death point process to produce a sampling scheme that efficiently explores the posterior distribution of interest. The MCMC sample path obtained from the simulated posterior then provides a flexible ingredient for most of the <b>inferential</b> <b>tasks</b> of interest. Illustrations on both artificial and real tasks are provided, while major difficulties and challenges are discussed, along with ideas for future improvements...|$|R
40|$|Statistical {{estimation}} in {{many contemporary}} settings involves the acquisition, analysis, and aggregation of data sets from multiple sources, which can have {{significant differences in}} character and in value. Due to these variations, the effectiveness of employing a given resource, e. g., a sensing device or computing power, for gathering or processing data from a particular source depends {{on the nature of}} that source. As a result, the appropriate division and assignment of a collection of resources to a set of data sources can substantially impact the overall performance of an inferential strategy. In this expository article, we adopt a general view of the notion of a resource and its effect on the quality of a data source, and we describe a framework for the allocation of a given set of resources to a collection of sources in order to optimize a specified metric of statistical efficiency. We discuss several stylized examples involving <b>inferential</b> <b>tasks</b> such as parameter estimation and hypothesis testing based on heterogeneous data sources, in which optimal allocations can be computed either in closed form or via efficient numerical procedures based on convex optimization. This work is an inferential analog of the literature in information theory on allocating power across communications channels of variable quality in order to optimize for total throughput...|$|R
40|$|The {{integration}} of expert systems in DSS {{has led to}} a new generation of systems commonly referred to as know edge-based or intelligent DSS. This paper investigates the use of expert system technology {{for the development of a}} know edge-based DSS for the planning of retail and service facilities. The forms of knowedge involved in planning tasks are identified and organized in four layers. The layers describe the states and events in the facility system (the domain layer), their interrelationships (the inferential layer), procedures for solving welldefined subproblems (the task layer) and strategies for approaching the overall problem (the strategic layer). It is shown that expert knowedge from each of the four layers can be used to improve the modelling capabilities and intelligence of a DSS. The result is a powerful and flexible DSS that supports planning at the domain, <b>inferential,</b> <b>task</b> and strategic level dependent on the preference of the decision maker and characteristics of the problem...|$|E
40|$|Institute of Mathematical Statistics, 2015. Surveys {{often ask}} {{respondents}} to report nonnegative counts, but respondents may misremember or round {{to a nearby}} multiple of 5 or 10. This phenomenon is called heaping, and the error inherent in heaped self-reported numbers can bias estimation. Heaped data may be collected cross-sectionally or longitudinally {{and there may be}} covariates that complicate the <b>inferential</b> <b>task.</b> Heaping is a well-known issue in many survey settings, and inference for heaped data is an important statistical problem. We propose a novel reporting distribution whose underlying parameters are readily interpretable as rates of misremembering and rounding. The process accommodates a variety of heaping grids and allows for quasi-heaping to values nearly but not equal to heaping multiples. We present a Bayesian hierarchical model for longitudinal samples with covariates to infer both the unobserved true distribution of counts and the parameters that control the heaping process. Finally, we apply our methods to longitudinal self-reported counts of sex partners in a study of high-risk behavior in HIV-positive youth...|$|E
40|$|ABSTRACT. The {{integration}} of expert systems in DSS {{has led to}} a new generation of systems commonly referred to as knowledge-based or intelligent DSS. Tbis paper investigates the use of expert system technology {{for the development of a}} knowledge-based DSS for the planning of retail and service faeifties. The forms of knowledge involved in planning tasks are identified and organized in four layers. The layers describe the states and events in the facility system (the domain layer), their interrelationships (the inferential layer), procedures for solving well-defined subproblems (the task layer) and strategies for approaching the overall problem (the strategic layer). The potentials of decision tables for representing qualitative and complex knowledge is discussed and illustrated with applications in the field of retail planning It is shown that expert knowledge from each of the four layers can be used to improve the modelling capabilities and intelligence of a DSS. The result is a powerful and flexible DSS that supports planning at the domain, <b>inferential,</b> <b>task</b> and strategic level dependent on the preference of the decision maker and characteristics of the problem...|$|E
40|$|We offer a {{complete}} {{characterization of the}} set of distributions that could be induced by local interventions on variables governed by a causal Bayesian network. We show that such distributions must adhere to three norms of coherence, and we demonstrate {{the use of these}} norms as <b>inferential</b> tools in <b>tasks</b> of learning and identification. Testable coherence norms are subsequently derived for networks containing unmeasured variables...|$|R
40|$|When can {{reliable}} inference {{be drawn}} in the "Big Data" context? This paper presents {{a framework for}} answering this fundamental question {{in the context of}} correlation mining, with implications for general large scale inference. In large scale data applications like genomics, connectomics, and eco-informatics the dataset is often variable-rich but sample-starved: a regime where the number $n$ of acquired samples (statistical replicates) is far fewer than the number $p$ of observed variables (genes, neurons, voxels, or chemical constituents). Much of recent work has focused on understanding the computational complexity of proposed methods for "Big Data. " Sample complexity however has received relatively less attention, especially in the setting when the sample size $n$ is fixed, and the dimension $p$ grows without bound. To address this gap, we develop a unified statistical framework that explicitly quantifies the sample complexity of various <b>inferential</b> <b>tasks.</b> Sampling regimes can be divided into several categories: 1) the classical asymptotic regime where the variable dimension is fixed and the sample size goes to infinity; 2) the mixed asymptotic regime where both variable dimension and sample size go to infinity at comparable rates; 3) the purely high dimensional asymptotic regime where the variable dimension goes to infinity and the sample size is fixed. Each regime has its niche but only the latter regime applies to exa-scale data dimension. We illustrate this high dimensional framework for the problem of correlation mining, where it is the matrix of pairwise and partial correlations among the variables that are of interest. We demonstrate various regimes of correlation mining based on the unifying perspective of high dimensional learning rates and sample complexity for different structured covariance models and different inference tasks...|$|R
40|$|This review {{considers}} {{experimental research}} that has used probability theory and statistics as a framework within which to study human statistical inference. The experiments have investigated estimates of proportions, means, variances, and correlations, both of samples and of populations. In some experiments, parameters of populations were stationary; in others, the parameters changed over time. The experiments also investigated the determination of sample size and trial-by-trial predictions of events to be sampled from a population. In general, {{the results indicate that}} probability theory and statistics can be used as the basis for psychological models that integrate and account for human performance {{in a wide range of}} <b>inferential</b> <b>tasks.</b> "Given [...] . an intelligence which could comprehend all the forces of which nature is animated and the respective situation of the beings who compose it—an intelligence sufficiently vast to submit these data to analysis [...] . nothing would be uncertain and the future, as the past, would be present to its eyes [Laplace, 1814]. " In lieu of such omniscience, man must cope with an environment about which he has only fallible information, "while God may not gamble, animals and humans do, [...] . they cannot help but to gamble in an ecology that is of essence only partly accessible to their foresight [Brunswik, 1955]. " And man gambles well. He survives and prospers while using the fallible information to infer the states of his uncertain environment and to predict future events. Man's problems with his uncertain environment are similar 'to those faced by social enterprises such as science, industry, and agriculture. Satisfactory decisions require sound inferences about prevailing and future states of the environments in which these enter...|$|R
40|$|Surveys {{often ask}} {{respondents}} to report nonnegative counts, but respondents may misremember or round {{to a nearby}} multiple of 5 or 10. This phenomenon is called heaping, and the error inherent in heaped self-reported numbers can bias estimation. Heaped data may be collected cross-sectionally or longitudinally {{and there may be}} covariates that complicate the <b>inferential</b> <b>task.</b> Heaping is a well-known issue in many survey settings, and inference for heaped data is an important statistical problem. We propose a novel reporting distribution whose underlying parameters are readily interpretable as rates of misremembering and rounding. The process accommodates a variety of heaping grids and allows for quasi-heaping to values nearly but not equal to heaping multiples. We present a Bayesian hierarchical model for longitudinal samples with covariates to infer both the unobserved true distribution of counts and the parameters that control the heaping process. Finally, we apply our methods to longitudinal self-reported counts of sex partners in a study of high-risk behavior in HIV-positive youth. Comment: Published at [URL] in the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Our {{overall goal}} is to study the use of {{information}} by Go players {{and the structure of}} their Go knowledge. In particular, in this paper we focus on human memory, and conclude with a discussion of the bene ts of modelling human memory to AI and Go programs. We report on two memory experiments. The rst experiment used Japanese Go players to replicate earlier studies on Australian Go players. The second experiment consists of three case studies of master Go players (6 to 8 dan amateurs). The general task in the experiments was to reconstruct Go positions stone by stone in correct game order. Two separate tasks were performed: in the episodic task, the moves from a Go game were shown to the subjects in a sequential presentation � in the <b>inferential</b> <b>task,</b> the subjects had to reconstruct Go positions with no information about how the game was played. In both tasks, feedback was provided after placement of each stone. The rst experiment replicated previous results for the experienced and beginner subjects. The case studies showed that extremely high levels of memory performance by the master subjects extended even to very fast presentation rates. ...|$|E
40|$|Geographic {{information}} systems (GISs) offer {{a powerful tool}} to geographers, foresters, statisticians, public health officials, and other users of spatially referenced regional data sets. However, as useful as they are for data display and trend detection, they typically feature little ability for statistical inference, leaving the user in doubt as to {{the significance of the}} various patterns and 'hot spots' identified. Unfortunately, classical statistical methods are often ill suited for this complex <b>inferential</b> <b>task,</b> dealing as it does with data which are multivariate, multilevel, misaligned, and often nonrandomly missing. In this paper we describe a Bayesian approach to this inference problem which simultaneously allows interpolation of missing values, estimation of the effect of relevant covariates, and spatial smoothing of underlying causal patterns. Implemented via Markov-chain Monte Carlo (MCMC) computational methods, the approach automatically produces both point and interval estimates which account for all sources of uncertainty in the data. After describing the approach {{in the context of a}} simple, idealized example, we illustrate it with a data set on leukemia rates and potential geographic risk factors in Tompkins County, New York, summarizing our results with numerous maps created by using the popular GIS Arc/INFO. ...|$|E
40|$|Despite {{warnings}} against inferring causality from observed correlations or statistical dependence, some articles do. Observed correlation {{is neither}} necessary nor sufficient to infer causality {{as defined by}} the term’s everyday usage. For example, a deterministic causal process creates pseudorandom numbers; yet, we observe no correlation between the numbers. Child height correlates with spelling ability because age causes both. Moreover, order is problematic—we hear train whistles before observing trains, yet trains cause whistles. Scientific methods specifically prohibit inferring causal theories from specific observations (i. e., effects) because, in part, many credible causes are perfectly consistent with available observations. Moreover, actions inferred from effects have more unintended consequences than actions based on sound deductive causal theories because causal theories predict multiple effects. However, an often overlooked but key feature of these theories is that we describe the cause with more variables than the effect. Consequently, inductive processes might appear deductive as the number of effects increases relative to the number of potential causes. For example, in real criminal trials, jurors judge whether sufficient evidence exists to infer guilt. In contrast, determining guilt in criminal mystery novels is deductive because the number of clues (i. e., effects) is large relative to the number of potential suspects (i. e., causes). We can make <b>inferential</b> <b>tasks</b> resemble deductive tasks by increasing the number of effects (i. e., variables) relative to the number of potential causes and seeking a shared cause for all observed effects. Moreover, under some conditions, the method of seeking shared causes might approach deductive reasoning when the number of causes is strictly limited. At least, the resulting number of possible causal theories is far less than the number generated from repeated observations of a single effect (i. e., variable) ...|$|R
40|$|An {{impairment}} in abstracting ability has frequently been proposed {{as a reason}} for schizophrenic thought disorder. The performance of hospitalized chronic paranoid schizophrenics and non-paranoid schizophrenics were compared to a normal control group on two types of abstraction; a traditional conceptual abstraction task (similarities, Trunnell, 1964) and an <b>inferential</b> abstraction <b>task</b> (relational abstraction, Bransford, Barclay & Franks, 1972). These two measures allowed a differential interpretation {{of the nature of the}} abstraction {{impairment in}} schizophrenia. The two clinical groups did not significantly differ on the traditional hierarchical measure of abstraction. Performance of both schizophrenic groups, however, differed significantly from that of controls in that schizophrenic subjects employed less abstract concepts to classify items in this task. On the second measure of abstraction no significant differences were found between schizophrenic subjects and the control group. Differences between paranoid and non-paranoid subjects did not reach significance on this task but there was some indication that each of these schizophrenic sub-groups used different cognitive strategies on this measure. Paranoid schizophrenics appeared not to elaborate information beyond its original form. The non-paranoids, on the other hand, appeared to elaborate stimulus material but were confused between inferential and original information. The present results indicate that chronic paranoid schizophrenics have a different type of abstraction impairment to chronic non-paranoid schizophrenics on the <b>inferential</b> conceptual abstraction <b>task.</b> These findings indicate the utility of using two indices of abstraction and the importance of not treating schizophrenics as a homogeneous group...|$|R
40|$|Subjects {{high and}} low in test-anxiety were {{presented}} with an <b>inferential</b> reasoning <b>task</b> requiring the verification of necessary and unnecessary inferences. The task was performed whilst holding either two or six digits in memory. On the verification task, the performance of high-test-anxious subjects was slower and less accurate {{than that of the}} low-test-anxious subjects. In addition, unnecessary inferences took longer to process than necessary inferences for the high-test-anxiety group only. The high-test-anxious subjects studied the memory loads for longer than the low-test-anxious group, but their recognition accuracy did not differ. Findings support Eysenck and Calvo's (Cognition and Emotion, 6, 409 - 434, 1992) processing efficiency theory. The high-test-anxious group's performance on the sentence verification task was impaired overall, and was particularly impaired when performing the unnecessary inference task. However, we also demonstrated that the high-test-anxious group's performance on a secondary memory task was unimpaired as a result of increased effort...|$|R
