47|550|Public
30|$|Step one: Randomly assign <b>input</b> <b>weight</b> ω and bias b.|$|E
3000|$|... 2 = 0, {{has been}} transmitted. To derive (10), it {{requires}} {{the knowledge of the}} minimum distance F of the compound code, the <b>input</b> <b>weight</b> w [...]...|$|E
3000|$|... [...]. The <b>input</b> <b>weight</b> and {{the pattern}} can be {{computed}} via heuristic searching of the trellis of G. The following result {{is important for}} further analysis: [...]...|$|E
5000|$|... #Caption: Error {{surface of}} a linear neuron with two <b>input</b> <b>weights</b> ...|$|R
30|$|Then {{calculate}} reflectance of the residuals and propagate {{it to the}} <b>input</b> <b>weights.</b>|$|R
40|$|Abstract—Recently, Extreme {{learning}} machine(ELM), a novel learning algorithm having {{much faster}} than the traditional gradient-based learning algorithm, was proposed for single-hidden-layer feedforward neural networks (SLFNs). Usually, the initial <b>input</b> <b>weights</b> and hidden biases of ELM are randomly chosen, and then the output weights are analytically determined by using Moore-Penrose (MP) generalized inverse. However, ELM may need higher number of hidden neurons due to the random determination of the <b>input</b> <b>weights</b> and hidden biases. In this paper, an optimization method based on the bacterial foraging (BF) algorithm is proposed to adjust the <b>input</b> <b>weights</b> and hidden biases. Experimental result shows that this method can achieve better performance for problems having higher dimension than others. Index Terms — Single-hidden-layer feedforward neura...|$|R
40|$|Recently Extreme Learning Machine (ELM) {{has been}} {{attracting}} attentions for its simple and fast training algorithm, which randomly selects input weights. Given sufficient hidden neurons, ELM has a comparable performance {{for a wide}} range of regression and classification problems. However, in this paper we argue that random <b>input</b> <b>weight</b> selection may lead to an ill-conditioned problem, for which solutions will be numerically unstable. In order to improve the conditioning of ELM, we propose an <b>input</b> <b>weight</b> selection algorithm for an ELM with linear hidden neurons. Experiment results show that by applying the proposed algorithm accuracy is maintained while condition is perfectly stable...|$|E
3000|$|Assign {{arbitrary}} <b>input</b> <b>weight</b> wβ ^([...] 0 [...]) =M_ 0 H_ 0 ^T T_ 0 i and bias b_i or center μ _i {{and impact}} width σ _i, i= 1, [...]...Ñ, where Ñ number for hidden neuron or RBF kernel {{for a specific}} application.|$|E
40|$|In this paper, {{we study}} {{a number of}} {{objective}} functions for training new hidden units in constructive algorithms for multilayer feedforward networks. The aim is to derive a class of objective functions the computation of which and the corresponding weight updates {{can be done in}} O(N) time, where N is the number of training patterns. Moreover, even though <b>input</b> <b>weight</b> freezing is applied during the process for computational efficiency, the convergence property of the constructive algorithms using these objective functions is still preserved. We also propose a few computational tricks {{that can be used to}} improve the optimization of the objective functions under practical situations. Their relative performance in a set of two-dimensional regression problems is also discussed. Keywords [...] - Constructive algorithms, cascade-correlation, convergence, <b>input</b> <b>weight</b> freezing, quickprop. I. Introduction I N recent years, many neural network models have been proposed for pattern classification, [...] ...|$|E
40|$|It is {{well-established}} that a {{multi-layer perceptron}} (MLP) {{with a single}} hidden layer of N neurons and an activation function bounded by zero at negative infinity and one at infinity can learn N distinct training sets with zero error. Previous work {{has shown that the}} <b>input</b> <b>weights</b> and biases for such a MLP can be chosen in an effectively arbitrary manner; however, this work makes the implicit assumption that the samples used to train the MLP are noiseless. We demonstrate that the values of the <b>input</b> <b>weights</b> and biases have a provable effect on the susceptibility of the MLP to noise, and can result in increased output error. It is shown how to compute a quantity called Dilution of Precision (DOP), originally developed for the Global Positioning System, for a given set of <b>input</b> <b>weights</b> and biases, and further shown that by minimizing DOP the susceptibility of the MLP to noise is also minimized...|$|R
3000|$|Encouraged by the {{previous}} results, this paper uses ELM ensemble model to make stability status prediction. Since ELM adopts random <b>input</b> <b>weights</b> and biases, its training speed is significantly improved, so the increased training burden of ensemble training can be greatly alleviated. Moreover, in ensemble training, each single ELM not only selects random <b>input</b> <b>weights</b> and bias, but also randomly selects training data, hidden node number and activation function. By this way, the ELM ensemble generates more diversified outputs for better overall prediction performance. For each single ELM in the proposed ELM ensemble model, the specific training process is as follow: [...]...|$|R
30|$|Consider {{the above}} {{equation}} which is non-quadratic in x but quadratic in u. The state and <b>input</b> <b>weighting</b> matrices are assumed state-dependent such that Q[*]:[*]Rn[*]→[*]Rn[*]×[*]n and R[*]:[*]Rn[*]→[*]Rn[*]×[*]m. These design parameters satisfy Q(x)[*]≥[*] 0 and R(x)[*]>[*] 0 for all x [24].|$|R
40|$|In {{the last}} decade, Government/Industry {{programs}} have advanced powder metallurgy-near-net-shape technology {{to permit the}} use of hot isostatic pressed (HIP) turbine disks in the commercial aircraft fleet. These disks offer a 30 % savings of <b>input</b> <b>weight</b> and an 8 % savings in cost compared in cast-and-wrought disks. Similar savings were demonstrated for other rotating engine components. A compressor rotor fabricated from hot-die-forged-HIP superalloy billets revealed <b>input</b> <b>weight</b> savings of 54 % and cost savings of 35 % compared to cast-and-wrought parts. Engine components can be produced from compositions such as Rene 95 and Astroloy by conventional casting and forging, by forging of HIP powder billets, or by direct consolidation of powder by HIP. However, each process produces differences in microstructure or introduces different defects in the parts. As a result, their mechanical properties are not necessarily identical. Acceptance methods should be developed which recognize and account for the differences...|$|E
40|$|Minimising {{the number}} of bits per {{connection}} weight in hardware realisation of a radial basis function neural network (RBFNN) will result in high-speed and low-cost implementation, with possible increase in output error. A weight quantisation accuracy selection method is proposed, to find an appropriate number of bits for a given stochastic sensitivity measure, which quantifies {{the relationship between the}} variance of the output error and first- and second-order statistics of <b>input,</b> <b>weight</b> and their perturbations. Department of Computin...|$|E
40|$|Preferential neural {{networks}} consist of preferential neurons which are modeled using weighted power means. The input weights of each preferential neuron must be positive and normalized (the sum of input weights is 1). The training of preferential {{neural networks}} consists of minimizing a criterion function and simultaneously satisfying the <b>input</b> <b>weight</b> constraints for each neuron. This paper introduces a new method for training preferential neural networks. We automatically satisfy the weight normalization requirements, reducing {{the training of}} preferential neurons to the unconstrained minimization of a compound criterion function...|$|E
5000|$|... #Caption: A {{model of}} an {{individual}} neuron. The inputs, x0 to xm, are modified by the <b>input</b> <b>weights,</b> w0 to wm, and then combined into one input, vk. The transfer function, , then uses this input to determine the output, yk.|$|R
50|$|Extreme Learning Machines (ELM) is {{a special}} case of single hidden layer {{feed-forward}} neural networks (SLFNs) where in the <b>input</b> <b>weights</b> and the hidden node biases can be chosen at random. Many variants and developments are made to the ELM for multiclass classification.|$|R
40|$|An {{important}} consideration for neural hardware is its sensitivity to <b>input</b> and <b>weight</b> errors. In this paper, an empirical study is performed {{to analyze the}} sensitivity of feedforward neural networks for Gaussian noise to <b>input</b> and <b>weight.</b> 30 numbers of FFANN is taken for four different classification tasks. Least sensitive network for <b>input</b> and <b>weight</b> error is chosen for further study of fault tolerant behavior of FFANN. Weight stuck-at zero fault is selected to study error metrics of fault tolerance. Empirical results for a WSZ fault is demonstrated in this paper...|$|R
40|$|This letter {{describes}} a simplemodification of theOja learning rule,which asymptotically constrains the L 1 -norm of an <b>input</b> <b>weight</b> vector instead of theL 2 -normas {{in the original}} rule. This constraining is local as opposed to commonly used instant normalizations, which require the knowledge of all input weights of a neuron to update {{each one of them}} individually. The proposed rule converges to a weight vector that is sparser (has more zero weights) than the vector learned by the original Oja rule with or without the zero bound,which could explain the developmental synaptic pruning. ...|$|E
40|$|International audienceWe {{propose a}} formal {{framework}} to support belief revision {{based on a}} cognitive model of credibility and trust. In this framework, the acceptance of information coming from a source depends on (i) the agent's goals and beliefs about the source's goals, (ii) the credibility, for the agent, of incoming information, and (iii) the agent's beliefs about {{the context in which}} it operates. This makes it possible to approach belief revision in a setting where new incoming information is associated with an acceptance degree. In particular, such degree may be used as <b>input</b> <b>weight</b> for any possibilistic conditioning operator with uncertain input (i. e., weighted belief revision operator) ...|$|E
40|$|New digital {{communication}} applications, such as multimedia, require very powerful error correcting codes that deliver low error rates while operating at low to moderate ratios (SNRs). Turbo codes have reasonable complexity and can achieve very low error rates if a proper interleaver design is in place. The use of well-designed interleavers result in very low error rates, especially for medium to long interleavers where turbo codes offer {{the greatest potential}} for achieving high minimum distance (d min) values. The reliable determination of a code's error performance at very low error rates using simulations may take months {{or may not be}} practical at all. However, the knowledge of dmin and its multiplicities can be used to estimate the error rates at high SNR. This thesis is concerned with efficient and accurate distance measurement methods for turbo codes. Since high values of dmin can be caused by high <b>input</b> <b>weight</b> values, say up to 20 or higher, if a brute force algorithm is used the accurate determination of dmin requires that all possible input sequences of <b>input</b> <b>weight</b> up to 20 be tested. Testing all possible input sequences becomes impractical as the size of the interleaver and the value of <b>input</b> <b>weight</b> increase. Thus, the accurate determination of the distance spectrum, or at least dmin and its multiplicities, is a significant problem, especially for interleavers that yield high dmin. Based on Garello's true distance measurement method, this thesis presents an efficient and accurate distance measurement method for single- and double-binary turbo codes that uses proper trellis termination such as dual-termination or tail-biting. This method is applied to determine the distance spectra for the digital video broadcasting with return channel via satellite (DVB-RCS) standard double-binary turbo codes. It is also used to design new interleavers for DVB-RCS that yield a significant improvement in error performance compared to those in the standard. This method fits particularly well with tail-biting turbo codes that use structured interleavers. This is because the distance properties repeat and this method can use this knowledge to reduce the search space. The reduction in search space results in significant reduction in complexity (i. e., execution time), which allows the determination of high dmin values in reasonable time. This efficiency is demonstrated for both single- and double-binary turbo codes, using structured interleavers that have high dmin values for various code rates. This method reduces the execution tunes by a factor of 40 to 400...|$|E
3000|$|We {{explore the}} {{effectiveness}} of inter-region dynamic features by analyzing the <b>input</b> <b>weights</b> (only the portions which connect inputs to input gates) of each temporal component in spatial component, M^sp. Figure  6 (b) summarizes the importance of inter-region dynamic features in predicting protest within given states. Large percentages (96.5 [...]...|$|R
30|$|While ELM may {{be faster}} than BP {{algorithms}} {{there is still}} room for improvement. Given that ELM computes the output weights based on prefixed <b>input</b> <b>weights</b> and hidden layer biases, {{there is a possibility}} of a set of non-optimal or unnecessary <b>input</b> <b>weights</b> and hidden layer biases being selected. Furthermore the problem of local minima which is common in BP algorithms may also exist in ELM, albeit to a lower degree. As suggested by Zhu et al. (2005) the problems experienced while using ELM as a network training algorithm can be minimized by using the DE algorithm for the initial weights and biases selection process. This idea can be implemented by combining the DE and ELM algorithm to form a hybrid training algorithm. The hybrid algorithm will thereafter be referred to as DE-ELM {{for lack of a better}} name.|$|R
40|$|Extreme {{learning}} machine (ELM), proposed by Huang et al., {{has been shown}} a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of <b>input</b> <b>weights</b> and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the <b>input</b> <b>weights</b> and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM...|$|R
40|$|Lennie, Haake, and Williams {{found that}} in the lateral geniculate nucleus (LGN), parvocellular unit {{responses}} {{are consistent with the}} hypothesis that their input connectivity is blind to the difference between middle-wavelength-sensitive (MWS) and long-wavelength-sensitive (LWS) cones. Most of their cells have a total MWS <b>input</b> <b>weight</b> opposite in sign and similar in magnitude to their total LWS <b>input</b> <b>weight.</b> If these weights are exactly balanced, the construction of a red-green opponent system from such units is simple: such units need only be aligned so the signs of their outputs agree. Ahumada and Mulligan described an associative learning process which can accomplish this alignment. If the units are not balanced (carry some luminance information), the strong overlap between the MWS and LWS spectral responses can cause units to agree more on the basis of luminance, and the associative process fails to produce red-green opponency. The learning theory requires that the LGN units be nearly balanced (more strongly encode chromaticity than luminance) and quantitatively expresses the requirement: the principal component of the LGN outputs must be in the red-green rather than the luminance direction. We show that the cone weights of the monkey LGN cells measured by Derrington, Krauskopf, and Lennie can satisfy this learnability criterion even if the MWS spectral response is close to the LWS spectral response, simulating anomalous trichromacy. The learnability theory provides a source of visual system variation for explaining why different anomalous trichromats may make the same average anomaloscope match (same pigments), but have either narrow (good opponent learning) or wide (poor learning) ranges of acceptable matches...|$|E
40|$|This paper {{focuses on}} {{self-organization}} of a multi-layered feed forward {{artificial neural network}} structure. Both the selection of interconnections among neurons and their optimum weights are studied. In this learning structure, the neurons are sparsely connected and dynamically adjust their connectivity structure. Only the feed-forward propagation is used and each neuron dynamically adjusts its threshold based on the incoming data. By analogy to the signal weighting, this paper derived how to set the optimal interconnection weights for neuron’s inputs. The binary <b>input</b> <b>weight</b> selection, suitable for hardware implementation, is discussed. Comparison between the binary and optimal weighting scheme is presented. Simulation examples for financial data analysis and power quality disturbance classification problems show {{the effectiveness of the}} proposed scheme...|$|E
40|$|Prediction model {{allows the}} {{machinist}} {{to determine the}} values of the cutting performance before machining. Modelling using improved extreme learning machine based particle swarm optimization, IPSO-ELM has less parameters to adjust and also takes real number as particles while decreasing the norm of output weights and constraining the <b>input</b> <b>weight</b> and hidden biases within a reasonable range to improve the ELM performance. In order to solve the multi objectives modelling problem, we have proposed a parallel IPSO-ELM. In this research work, the best input weights and hidden biases for different performance were identified. The proposed method was able to model the training and the testing set with minimal error. The predicted result from the designed model was able to match the experimental data very closely...|$|E
40|$|The Extreme Learning Machine (ELM) is a single-hidden layer {{feedforward}} {{neural network}} (SLFN) learning algorithm that can learn effectively and quickly. The ELM training phase assigns the <b>input</b> <b>weights</b> and bias randomly and does not change them in the whole process. Although the network works well, the random <b>weights</b> in the <b>input</b> layer can make the algorithm less effective and impact on its performance. Therefore, we propose {{a new approach to}} determine the <b>input</b> <b>weights</b> and bias for the ELM using the restricted Boltzmann machine (RBM), which we call RBM-ELM. We compare our new approach with a well-known approach to improve the ELM and a state of the art algorithm to select the weights for the ELM. The results show that the RBM-ELM outperforms both methodologies and achieve a better performance than the ELM. Comment: 14 pages, 7 figures and 5 table...|$|R
40|$|In this paper, {{we present}} a machine {{learning}} approach to measure the visual quality of JPEG-coded images. The features for predicting the perceived image quality are extracted by considering key human visual sensitivity (HVS) factors such as edge amplitude, edge length, background activity and background luminance. Image quality assessment involves estimating the functional relationship between HVS features and subjective test scores. The quality of the compressed images are obtained without referring to their original images ('No Reference' metric). Here, the problem of quality estimation is transformed to a classification problem and solved using extreme learning machine (ELM) algorithm. In ELM, the <b>input</b> <b>weights</b> and the bias values are randomly chosen and the output weights are analytically calculated. The generalization performance of the ELM algorithm for classification problems with imbalance {{in the number of}} samples per quality class depends critically on the <b>input</b> <b>weights</b> and the bias values. Hence, we propose two schemes, namely the k-fold selection scheme (KS-ELM) and the real-coded genetic algorithm (RCGA-ELM) to select the <b>input</b> <b>weights</b> and the bias values such that the generalization performance of the classifier is a maximum. Results indicate that the proposed schemes significantly improve the performance of ELM classifier under imbalance condition for image quality assessment. The experimental results prove that the estimated visual quality of the proposed RCGA-ELM emulates the mean opinion score very well. The experimental results are compared with the existing JPEG no-reference image quality metric and full-reference structural similarity image quality metric...|$|R
30|$|The above {{procedure}} {{can also}} be performed using <b>input</b> <b>weights</b> Qi and variable Iik and subject the formula to an output constraint under CRS. The optimization procedure in DEA ensures that while maintaining equity for all other DMUs the particular DMU (in our study the countries) being evaluated is given the highest score possible by maximizing its relative efficiency ratio.|$|R
40|$|Hebbian {{changes of}} {{excitatory}} synapses {{are driven by}} and enhance correlations between pre- and postsynaptic neuronal activations, forming a positive feedback loop {{that can lead to}} instability in simulated neural networks. Because Hebbian learning may occur on time scales of seconds to minutes, it is conjectured that some form of fast stabilization of neural firing is necessary to avoid runaway of excitation, but both the theoretical underpinning and the biological implementation for such homeostatic mechanism are to be fully investigated. Supported by analytical and computational arguments, we show that a Hebbian spike-timing-dependent metaplasticity rule, accounts for inherently-stable, quick tuning of the total <b>input</b> <b>weight</b> of a single neuron in the general scenario of asynchronous neural firing characterized by UP and DOWN states of activity...|$|E
40|$|This paper {{discusses}} energy extraction from {{atmospheric turbulence}} by small- and micro- uninhabited aerial vehicles. A nonlinear longitudinal dynamic {{model of a}} glider with elevators as the sole control input {{is used for the}} aircraft and feedback control laws for energy extraction are discussed. Using current measurements of wind speed and gra-dient the state which maximizes the gain in total energy is computed. A state feedback controller uses elevator input to regulate states to the optimal values. The state feedback control law is computed using LQR synthesis, and the state and <b>input</b> <b>weight</b> matrices which maximize energy gain are found using a search method. Simulation results of flights through sinusoidal gust fields and a thermal field show the performance of the proposed approach. I...|$|E
40|$|Place your method {{development}} of deterministic mathematical models that describe {{the intensification of}} the process of separation of impurities heap root crops combined working bodies of transport and technological systems to adapt the machine for harvesting root crops. Based on the application of the transformation equations are obtained Laplasa transfer function in the operator form, which describe the functional processes of work of adapted the machine. A method is resulted developments of the determined mathematical models, which describe intensification of process of separation of admixtures from to the lots of root crops by the combined workings organs transport technological systems of the adapted machines are for harvesting of root crops. Key word: technological process, heap of roots, material balance, flow, <b>input</b> <b>weight,</b> components of heap, combined purifier, balance of the masses, input mass...|$|E
40|$|A novel {{learning}} {{algorithm is}} {{developed for the}} training of multilayer feedforward neural networks, based on a modification of the Marquardt-Levenberg least-squares optimization method. The algorithm updates the <b>input</b> <b>weights</b> of each neuron in the network in an effective parallel way. An adaptive distributed selection of the convergence rate parameter is presented, using suitable optimization strategies. The algorithm has bette...|$|R
30|$|Since the BER is linearly {{proportional}} to the UPEP PEP^NC (d|W_d) via corresponding <b>input</b> <b>weights,</b> the diversity order of NCC is equal to diversity order of the UPEP. Let x ∝γ^-η denote the exponential equivalence, i.e., x achieves diversity of order η, where γ stands for the general average SNR. The diversity order of the PEP^NC (d|W_d) is given by the following theorem.|$|R
40|$|We {{present a}} closed form {{expression}} for initializing the <b>input</b> <b>weights</b> in a multi-layer perceptron, {{which can be}} used as the first step in synthesis of an Extreme Learning Ma-chine. The expression is based on the standard function for a separating hyperplane as computed in multilayer perceptrons and linear Support Vector Machines; that is, as a linear combination of input data samples. In the absence of supervised training for the <b>input</b> <b>weights,</b> random linear combinations of training data samples are used to project the input data to a higher dimensional hidden layer. The hidden layer weights are solved in the standard ELM fashion by computing the pseudoinverse of the hidden layer outputs and multiplying by the desired output values. All weights for this method can be computed in a single pass, and the resulting networks are more accurate and more consistent on some standard problems than regular ELM networks of the same size. Comment: In submission for the ELM 2014 Conferenc...|$|R
