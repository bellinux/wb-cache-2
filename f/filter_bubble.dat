74|64|Public
25|$|DuckDuckGo (DDG) is an Internet {{search engine}} that {{emphasizes}} protecting searchers' privacy and avoiding the <b>filter</b> <b>bubble</b> of personalized search results. DuckDuckGo distinguishes itself from {{other search engines}} by not profiling its users and by deliberately showing all users the same search results for a given search term. DuckDuckGo emphasizes returning the best results, rather than the most results, and generates those results from over 400 individual sources, including key crowdsourced sites such as Wikipedia, and other search engines like Bing, Yahoo!, Yandex, and Yummly.|$|E
50|$|Various {{reviews in}} media have {{emphasized}} that Random enables people to break their <b>filter</b> <b>bubble</b> and find diverse content {{they might not}} find elsewhere.|$|E
5000|$|FAROO takes user {{behavior}} {{into account}} when calculating a website's rank, resulting in more relevant search results than traditional search engines can provide (see <b>filter</b> <b>bubble).</b>|$|E
50|$|<b>Filter</b> <b>bubbles</b> {{in popular}} social media and {{personalized}} search sites {{can determine the}} particular content seen by users, often without their direct consent or cognizance, due to the algorithms used to curate that content. Critics {{of the use of}} <b>filter</b> <b>bubbles</b> speculate that individuals may lose autonomy over their own social media experience and have their identities socially constructed {{as a result of the}} pervasiveness of <b>filter</b> <b>bubbles.</b>|$|R
5000|$|As the {{popularity}} of cloud services increases, personalized algorithms used to construct <b>filter</b> <b>bubbles</b> are expected to become more widespread. [...] Scholars have begun considering the effect of <b>filter</b> <b>bubbles</b> on the users of social media from an ethical standpoint, particularly concerning the areas of personal freedom, security, and information bias.|$|R
40|$|A bubble tiltmeter {{has been}} used as a {{horizontal}} seismometer. With the appropriate <b>filters,</b> the <b>bubble</b> system has good response for displacement over the passband of conventional seismometers (from about 10 Hz to 200 s), and for tilt from about 1 Hz to DC. The accuracy of the response is confirmed by comparing the <b>filtered</b> <b>bubble</b> output to conventional seismic instruments. The agreement between the <b>filtered</b> <b>bubble</b> records and broad band and short period conventional records is extremely good in every case. The small size, broad-band response, and lack of moving parts make the bubble ideal as an instrument for remote environments. In particular, the instrument seems ideal for the ocean bottom, land and marine boreholes and planetary missions...|$|R
50|$|Several {{designers}} developed {{tools to}} counteract the effects of filter bubbles. Swiss radio station SRF voted the word filterblase (the German translation of <b>filter</b> <b>bubble)</b> word of the year 2016.|$|E
50|$|Personyze {{was criticized}} by Forbes for {{contributing}} to the <b>filter</b> <b>bubble,</b> which is a technology that sorts everything out {{on the basis of}} the user's activities that may consequently present visitors with only a portion of the content/offers, while excluding other content/offers from ever being presented to them.|$|E
50|$|The News Feed {{has been}} {{described}} as a <b>filter</b> <b>bubble,</b> showing users personalized results about information deemed interesting to them, in contrary to showing all information, even information that they disagree with. Facebook has been researching this situation since 2010, and initially used an algorithm known as EdgeRank.|$|E
40|$|It {{has been}} argued that the Internet and social media {{increase}} the number of available viewpoints, perspectives, ideas and opinions available, leading to a very diverse pool of information. However, critics have argued that algorithms used by search engines, social networking platforms and other large online intermediaries actually decrease information diversity by forming so-called “filter bubbles”. This may form a serious threat to our democracies. In response to this threat others have developed algorithms and digital tools to combat <b>filter</b> <b>bubbles.</b> This paper first provides examples of different software designs that try to break <b>filter</b> <b>bubbles.</b> Secondly, we show how norms required by two democracy models dominate the tools that are developed to fight the <b>filter</b> <b>bubbles,</b> while norms of other models are completely missing in the tools. The paper in conclusion argues that democracy itself is a contested concept and points to a variety of norms. Designers of diversity enhancing tools must thus be exposed to diverse conceptions of democracy. Technology, Policy and Managemen...|$|R
5000|$|... #Caption: Social media, {{seeking to}} please users, can shunt {{information}} that they guess their users will like hearing, but inadvertently isolate what they know into their own <b>filter</b> <b>bubbles,</b> according to Pariser.|$|R
40|$|Some {{fear that}} personalised {{communication}} {{can lead to}} information cocoons or <b>filter</b> <b>bubbles.</b> For instance, a personalised news website could give more prominence to conservative or liberal media items, based on the (assumed) political interests of the user. As a result, users may encounter only a limited range of political ideas. We synthesise empirical research on the extent and effects of self-selected personalisation, where people actively choose which content they receive, and pre-selected personalisation, where algorithms personalise content for users without any deliberate user choice. We conclude that at present there is little empirical evidence that warrants any worries about <b>filter</b> <b>bubbles...</b>|$|R
50|$|A <b>filter</b> <b>bubble</b> {{has been}} {{described}} as exacerbating a phenomenon that has been called splinternet or cyberbalkanization, which happens when the Internet becomes divided up into sub-groups of like-minded people who become insulated within their own online community and fail to get exposure to different views; the term cyberbalkanization was coined in 1996.|$|E
50|$|EdgeRank and its successors {{have a broad}} {{impact on}} what users {{actually}} see out of what they ostensibly follow: for instance, the selection can produce a <b>filter</b> <b>bubble</b> (if users are exposed to updates which confirm their opinions etc.) or alter people's mood (if users are shown a disproportionate amount of positive or negative updates).|$|E
50|$|Qwant is a French {{company that}} was founded by {{security}} specialist Éric Leandri, investor Jean Manuel Rozan and search-engine expert Patrick Constant in 2011. It launched its eponymous web search engine in July 2013. It claims not to employ user tracking, and it doesn't personalise search results {{in order to avoid}} trapping users in a <b>filter</b> <b>bubble.</b>|$|E
5000|$|Filter bubbles: Echo {{chambers}} and <b>filter</b> <b>bubbles</b> might {{be created by}} Website administrators or moderators locking out people with altering viewpoints or by establishing certain rules or by the typical member viewpoints of online sub/communities or Internet [...] "tribes" ...|$|R
40|$|This study {{explores the}} {{geographic}} dependencies of echo-chamber communication on Twitter during the Brexit referendum campaign. We review {{the literature on}} <b>filter</b> <b>bubbles,</b> echo chambers, and polarization to test five hypotheses positing that echo-chamber communication is associated with homophily in the physical world, chiefly the geographic proximity between users advocating sides of the campaign. The results {{support the hypothesis that}} echo chambers in the Leave campaign are associated with geographic propinquity, whereas in the Remain campaign the reverse relationship was found. This study presents evidence that geographically proximate social enclaves interact with polarized political discussion where echo-chamber communication is observed. The article concludes with a discussion of these findings and the contribution to research on <b>filter</b> <b>bubbles</b> and echo chambers...|$|R
5000|$|Since {{the content}} seen by {{individual}} social media users {{is influenced by}} algorithms that produce <b>filter</b> <b>bubbles,</b> users of social media platforms {{are more susceptible to}} confirmation bias, and may be exposed to biased, misleading information. [...] Social sorting and other unintentional discriminatory practices are also anticipated as a result of personalized filtering.|$|R
5000|$|Alan Henry of Lifehacker wrote: [...] "Random... breaks you out by {{intentionally}} guiding you to new {{topics and}} interesting articles at sites {{you may not}} otherwise read." [...] Vice Motherboard's Claire Evans says that: [...] "Random never turns into a <b>filter</b> <b>bubble,</b> because it perpetually injects the irrational into my experience… in a cocktail of relevancy and serendipity." ...|$|E
50|$|Zite is {{a popular}} {{application}} {{that is similar to}} the Daily Me concept. It is available on iOS, Android, and Windows Phone. FeedSavvy.com is a similar service available on the web for PC and Mac users. noosfeer is addressing this issue by letting the users explore subjects with a wider range in the results, avoiding the <b>filter</b> <b>bubble</b> effect.|$|E
50|$|Search {{engines are}} listed in tables below for {{comparison}} purposes. The first table lists the company behind the engine, volume and ad support and identifies {{the nature of the}} software being used as free software or proprietary. The second table lists privacy aspects along with other technical parameters, such as whether the engine provides personalization (alternatively viewed as a <b>filter</b> <b>bubble).</b>|$|E
50|$|Users {{can take}} actions to burst through their <b>filter</b> <b>bubbles.</b> Some make a {{conscious}} effort to evaluate what information they are exposing themselves to, thinking critically about whether they are engaging with a broad range of content. Steps to “re-engineer your internet diet” include creating your own research team of smart, insightful media leaders who wisely consume and produce only valid and credible articles. Users can examine their search history and cut sources that are unverifiable or weak. This view argues that users should change the psychology of how they approach media with their biases already intact instead of relying on a tech to erase their biases. Tech {{can also be used to}} combat <b>filter</b> <b>bubbles.</b> Chris Glushko, the VP of Marketing at IAB, advocates using fact-checking sites like Snopes.com to eradicate the occurrence of fake news.|$|R
50|$|A 2011 {{study found}} ideological {{segregation}} of online news consumption {{is lower than}} the segregation of most offline news consumption and lower than the segregation of face-to-face interactions. This suggests that the <b>filter</b> <b>bubbles</b> effects of online media consumption are exaggerated. Other research also shows that online media does not contribute to increased polarization of opinions.|$|R
40|$|As machine {{learning}} grows more sophisticated everyday, the societal impacts of algorithm models automating and dictating {{much of our}} online behaviors becomes undeniable. This has led to ethical problems in technology-based media (<b>filter</b> <b>bubbles</b> on facebook, search discriminations on google search, datafication and privacy in general, etc), and despite our growing dependency upon {{machine learning}} systems, we have no clear ethical guidelines to provide either the computer scientists when building the algorithms, or designers when implementing such systems. This thesis explores the friction points where design might intervene in ways that effectively address the challenges at hand, as well as better ways of designing users’ relationships with their <b>filter</b> <b>bubbles,</b> through literature reviews, exploratory research, paper prototypes, and a survey. The goals are twofold: 1. To diversify news consumption practices, and 2. To encourage people to {{become more aware of}} their own behaviors on social media. After conducting literature reviews, I identified the following friction points: lack of transparency over what data is being pulled; and a general lack of user agency and control over the kinds of data they would like pulled from their engagement and the sorts of content they might like to be shown. As a basis for inquiry, this study questions if there’s a way to leverage design to help people become more aware of their contributions to their own <b>filter</b> <b>bubbles,</b> instead of pushing people to engage with others who think differently. Findings from the survey suggest that people find value in the resulting prototype, but that the stakeholders would need to expand beyond social media users, in order to consider a financial incentive for the business...|$|R
50|$|Hallin's theory {{assumed a}} {{relatively}} homogenized media environment, where most producers {{were trying to}} reach most consumers. A more fractured media landscape can challenge this assumption. because different audiences may place topics in different spheres, a concept related to the <b>filter</b> <b>bubble,</b> which posits that {{many members of the}} public choose to limit their media consumption to the areas of consensus and deviance that they personally prefer.|$|E
50|$|Corporate Superpowers - This section {{discusses}} {{the practice of}} both Facebook and Google of sending ambassadors to political figures in various countries to promote legislation favorable to the companies, gives {{a description of the}} conflict between the government of China and Google China over Internet censorship in China, and discusses Eli Pariser's <b>filter</b> <b>bubble,</b> Siva Vaidhyanathan's concept of googlization, and Joseph Nye's observations on large corporations which derive most of their income outside their home countries.|$|E
5000|$|The feature {{also has}} {{profound}} {{effects on the}} search engine optimization industry, {{due to the fact}} that search results will no longer be ranked the same way for every user. An example of this is found in Eli Pariser's, The <b>Filter</b> <b>Bubble,</b> where he had two friends type in [...] "BP" [...] into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.|$|E
40|$|Search {{engines and}} social media keep trace of profile- and behavioral-based {{distinct}} signals of their users, {{to provide them}} person- alized and recommended content. Here, {{we focus on the}} level of web search personalization, to estimate the risk of trapping the user into so called <b>Filter</b> <b>Bubbles.</b> Our experimentation has been carried out on news, specifically investigating the Google News platform. Our results are in line with existing literature and call for further analyses on which kind of users are the target of specific recommendations by Google...|$|R
50|$|In the 21st century, the {{capacity}} to mislead was enhanced by {{the widespread use of}} social media. For example, one 21st century website that enabled fake news' proliferation was the Facebook newsfeed. In late 2016 fake news gained notoriety following the uptick in news content by this means, and its prevalence on the micro-blogging site Twitter. In the United States, 62% of Americans use social media to receive news. This, in combination with increased political polarization and <b>filter</b> <b>bubbles,</b> led to a tendency for readers to mainly read headlines.|$|R
50|$|Rewire: Digital Cosmopolitans in the Age of Connection is a 2013 nonfiction {{book about}} {{contemporary}} globalization and xenophilia by American blogger Ethan Zuckerman of MIT. It describes homophilic barriers to cosmopolitanism such as <b>filter</b> <b>bubbles</b> and media bias. Zuckerman {{calls for a}} strenuously internationalized media and cultural literacy empowered by language translation. He cites the work of scholars Kwame Anthony Appiah, Ronald Stuart Burt, Mark Granovetter, and Robert D. Putnam, and of cosmopolitan exemplars Matt Harding, Erik Hersman, Dhani Jones, Roland Soong, Global Voices Online, Härnu, Meedan, and Tea Leaf Nation.|$|R
50|$|DuckDuckGo (DDG) is an Internet {{search engine}} that {{emphasizes}} protecting searchers' privacy and avoiding the <b>filter</b> <b>bubble</b> of personalized search results. DuckDuckGo distinguishes itself from {{other search engines}} by not profiling its users and by deliberately showing all users the same search results for a given search term. DuckDuckGo emphasizes returning the best results, rather than the most results, and generates those results from over 400 individual sources, including key crowdsourced sites such as Wikipedia, and other search engines like Bing, Yahoo!, Yandex, and Yummly.|$|E
5000|$|The {{methods of}} personalization, and how useful {{it is to}} [...] "promote" [...] certain results which have been showing up {{regularly}} in searches by like-minded individuals in the same community. The personalization method makes it very easy {{to understand how the}} <b>filter</b> <b>bubble</b> is created. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.|$|E
5000|$|In 2011, Google Search query {{results have}} been shown to be {{tailored}} to users by Internet activist Eli Pariser, effectively isolating users in what he defined as a <b>filter</b> <b>bubble.</b> Pariser holds algorithms used in search engines such as Google Search responsible for catering [...] "a personal ecosystem of information". Although contrasting views have mitigated the potential threat of [...] "informational dystopia" [...] and questioned the scientific nature of Pariser claims, [...] filter bubbles have been mentioned to account for the surprising results of the U.S. presidential election in 2016 alongside fake news and echo chambers, suggesting that Facebook and Google have designed personalized online realities in which « we only see and hear what we like » [...]|$|E
5000|$|Although {{his speech}} did not employ the term [...] "filter", President Obama's {{farewell}} address identified a similar concept to <b>filter</b> <b>bubbles</b> as a [...] "threat to Americans' democracy", i.e., the [...] "retreat {{into our own}} bubbles, ...especially our social media feeds, surrounded by people who look like us and share the same political outlook and never challenge our assumptions... And increasingly we become so secure in our bubbles that we start accepting only information, whether it’s true or not, that fits our opinions, instead of basing our opinions on the evidence that is out there." ...|$|R
40|$|This study {{derives from}} a located {{a gap in}} the methodological {{coverage}} and ways in which <b>filter</b> <b>bubbles</b> previously have been problematised. It is structured to through a user perspective to find ways in which users navigation and experience is influenced by personalised consumption. Through interview studies of digital natives, two main focuses of navigation and experience have been chosen with the aim to bring nuanced perspectives to the current state of <b>filter</b> <b>bubbles.</b> The first, using the theoretical framework of uses and gratifications sets out to answer: In what ways do digital natives navigation contest the personalisation of their news consumption? I found that most interview participants have developed both thorough and individual ways of navigating in their news consumption process. Personalising filters are by some seen as assets to optimize content and by others as thresholds that enforce restrictive behaviour. However, most participants seem to be mildly concerned or unaware of personalising features in their news navigation. The second focus of user experience seeks to clarify the motives behind user navigation by answering: In what ways do digital natives experience of their navigation contest the personalisation of theirs and others news consumption? I find that some participants consider the impact of their own interactions with their personalised consumption, but do not understand the extents of it. I also find that shared social norms and traditional media permeate the critical view that all participants carry with them through their navigation. I use these findings to introduce a suggestion to problematise personalisation through user experience as a way of benchmarking <b>filter</b> <b>bubbles</b> that to my knowledge have not been used before. Lastly, by looking at the navigations and experiences of the participants through a theoretical framework of power, I conceptualise their interactions as motives of counter power towards a personalisation to answer: How can the motives of digital natives navigation be contextualised as acts of counter power towards their personalised news consumption? I identify both interactions as motives of counter power with some participants’ news consumption, and experiences of subjectivity to power in others. But can’t determine to which extents it relates to the personalisation or other factors in the participants news consumption...|$|R
40|$|The {{success of}} {{political}} movements {{that appear to}} be immune to any factual evidence that contradicts their claims – from the Brexiteers to the 'alt‐right', neo‐fascist groups supporting Donald Trump – has reinvigorated claims that social media spaces constitute so‐called 'filter bubbles' or 'echo chambers'. But while such claims may appear intuitively true to politicians and journalists – who have themselves been accused of living in <b>filter</b> <b>bubbles</b> (Bradshaw 2016) –, the evidence that ordinary users experience their everyday social media environments as echo chambers is far more limited. For instance, a 2016 Pew Center study has shown that only 23...|$|R
