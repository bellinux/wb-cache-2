8|10000|Public
5000|$|The {{entire length}} of FrontRunner {{corridor}} (including the southern extension) has been approved as a [...] "Quiet Zone" [...] by the Federal Railroad Administration. [...] "Typically, federal railroad safety regulations require that all train operators sound their horns for 15 to 20 seconds as they approach any road crossing." [...] Essentially a Quiet Zone designation <b>eliminates</b> <b>this</b> <b>requirement.</b> However, for obvious safety reasons, they are not prohibited from sounding their horn, if appropriate. The Quiet Zone applies to all trains (including freight trains) within the same corridor. Each city along the route had to individually apply for the designation, but UTA provided substantial assistance with the process. Several safety upgrades must be in place at all public crossings {{in order to receive}} Quiet Zone approval. In addition to the normal automatic warning bells and lights, required upgrades include crossing guards, signs warning that trains do not blare horns in the area, and raised medians (which prevent cars from driving around lowered gates). There are also additional safety features for pedestrians. Prior to the southern extension, FrontRunner had the longest Quiet Zone in the nation—the southern extension doubled the length of the previous Quiet Zone.|$|E
40|$|Current {{computing}} systems typically support only mid-century information structures: simple hierarchies. Hypertext technologies {{enable users}} to impose many structures on document sets and, consequently, provide many paths to desired information, but they require that users {{work their way}} through some structure. Full-text search <b>eliminates</b> <b>this</b> <b>requirement</b> by ignoring structure altogether. The search strategy can also be restricted to work within specified contexts. The architecture provided for search readily supports automatic linking. These ideas have been tested in IRIS Intermedia...|$|E
40|$|We {{present a}} novel {{technique}} for calibrating display-camera systems from reflections in the user’s eyes. Display-camera systems enable {{a range of}} vision applications that need controlled illumination, including 3 D object recon-struction, facial modeling and human computer interaction. One important issue, though, is the geometric calibration of the display, which requires additional hardware and te-dious user interaction. The proposed approach <b>eliminates</b> <b>this</b> <b>requirement</b> by analyzing patterns that {{are reflected in the}} cornea, a mirroring device that naturally exists in any display-camera system. We introduce an optimization strat-egy that is able to refine eye and spherical mirror calibra-tion results. When applied to the eye, it even outperforms spherical mirror calibration unoptimized. Furthermore, we obtain a robust estimation of eye poses which can be used for eye tracking applications. Despite the difficult working conditions, the calibration results are good and should be sufficient for many applications. 1...|$|E
50|$|Note {{that the}} {{implementation}} {{shown on the}} image requires double frequency. There are some tricks that <b>eliminate</b> <b>this</b> <b>requirement.</b>|$|R
50|$|Unlike {{diversity}} jurisdiction, {{which is}} based on the parties coming from different states, federal question jurisdiction no longer has any amount in controversy requirement—Congress <b>eliminated</b> <b>this</b> <b>requirement</b> in actions against the United States in 1976, and in all federal question cases in 1980. Therefore, a federal court can hear a federal question case even if no money is sought by the plaintiff.|$|R
5000|$|Meat Alternates: Peanuts {{along with}} other nuts and seeds and their {{associated}} butters could be served {{as a replacement for}} half of the meat requirement. In addition, yogurt, tofu, and equivalencies of dry beans, peas, or eggs could also be considered meat alternates. Instead of requiring meat or a meat alternate to make up the main dish of the meal, the report proposed <b>eliminating</b> <b>this</b> <b>requirement</b> to increase cost savings and give local officials more flexibility.|$|R
40|$|Moment {{functions}} de ned using a {{polar coordinate}} {{representation of the}} image space, such as radial moments and Zernike moments, are used in several recognition tasks requiring rotation invariance. However, this coordinate representation does not easily yield translation invariant functions, which are also widely sought after in pattern recognition applications. This paper presents a mathematical framework for the derivation of translation invariants of radial moments de ned in polar form. Using a direct application of this framework, translation invariant functions of Zernike moments are derived algebraically from the corresponding central moments. Both derived functions are developed for non-symmetrical as well as symmetrical images. They mitigate the zero-value obtained for odd-order moments of the symmetrical images. Vision applications generally resort to image normalization to achieve translation invariance. The proposed method <b>eliminates</b> <b>this</b> <b>requirement</b> by providing a translation invariance property in a Zernike feature set. The performance of the derived invariant sets is experimentally con rmed using a set of binary Latin and English characters...|$|E
40|$|This paper {{investigates the}} problem of {{adaptive}} power allocation for distributed best linear unbiased estimation (BLUE) of a random parameter at the fusion center (FC) of a wireless sensor network (WSN). An optimal power-allocation scheme is proposed that minimizes the $L^ 2 $-norm of the vector of local transmit powers, given a maximum variance for the BLUE estimator. This scheme results in the increased lifetime of the WSN compared to similar approaches {{that are based on}} the minimization of the sum of the local transmit powers. The limitation of the proposed optimal power-allocation scheme is that it requires the feedback of the instantaneous channel state information (CSI) from the FC to local sensors, which is not practical in most applications of large-scale WSNs. In this paper, a limited-feedback strategy is proposed that <b>eliminates</b> <b>this</b> <b>requirement</b> by designing an optimal codebook for the FC using the generalized Lloyd algorithm with modified distortion metrics. Each sensor amplifies its analog noisy observation using a quantized version of its optimal amplification gain, which is received by the FC and used to estimate the unknown parameter. Comment: 6 pages, 3 figures, to appear at the IEEE Military Communications Conference (MILCOM) 201...|$|E
40|$|This paper {{presents}} a novel design concept, which is verified by analytical and simulation results, of a single-mode small-core terahertz Bragg fibre exhibiting {{the properties of}} low loss and low dispersion. Conventionally, a single-TE₀₁-mode Bragg fibre requires a large core and many cladding layer periods to achieve a significant propagation loss discrimination between the desired mode and other unwanted competing modes. The use of a second-order bandgap in this paper completely <b>eliminates</b> <b>this</b> <b>requirement,</b> and enhances propagation loss discrimination using just a small core with a diameter at least 50 % smaller than the conventional design and only four cladding layer periods. Furthermore, a generalized half-wavelength condition is proposed, promoting the manipulation of photonic bandgap for Bragg fibre. The TE₀₁ mode has a null point in the electric field close to the boundary interface between the core and the cladding, and this phenomenon has been exploited to minimize the impact of support bridges, which mechanically maintain the air gaps, on the propagation loss of the fibre. Finally, we propose a novel design of a tightly confined single-TE₀₁-mode small-air-core Bragg fibre with propagation loss and group velocity dispersion less than 1. 2 dB/m and - 0. 6 ps/THz/cm, respectively, between frequencies of 0. 85 THz and 1. 15 THz...|$|E
5000|$|The key to {{this problem}} is that the warden may not reveal the name of a {{prisoner}} who will be pardoned. If we <b>eliminate</b> <b>this</b> <b>requirement,</b> it can demonstrate the original problem in another way. The only change in this example is that prisoner A asks the warden to reveal the fate of one of the other prisoners (not specifying one that will be executed). In this case, the warden flips a coin chooses one of B and C to reveal the fate of. The cases are as follows: ...|$|R
40|$|Statistical {{methods of}} {{inference}} typically require the likelihood function to be computable {{in a reasonable}} amount of time. The class of "likelihood-free" methods termed Approximate Bayesian Computation (ABC) is able to <b>eliminate</b> <b>this</b> <b>requirement,</b> replacing the evaluation of the likelihood with simulation from it. Likelihood-free methods have gained in efficiency and popularity in the past few years, following their integration with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) in order to better explore the parameter space. They have been applied primarily to estimating the parameters of a given model, but {{can also be used to}} compare models...|$|R
5000|$|Congress did {{not create}} a {{consistent}} federal question jurisdiction, which allows federal courts to hear any case alleging {{a violation of the}} Constitution, laws, and treaties of the United States, until 1875, when Congress created the statute which is now found at : [...] "The district courts shall have original jurisdiction of all civil actions arising under the Constitution, laws, or treaties of the United States." [...] At that time, such cases had the same amount in controversy requirement as the diversity cases. Congress <b>eliminated</b> <b>this</b> <b>requirement</b> in actions against the United States in 1976 and in all federal question cases in 1980.|$|R
40|$|Kinematic {{service to}} support its {{cadastral}} surveying applications. The service implements a Virtual Reference Station (VRS) concept based on a network of 11 CORS stations located mainly in the Central Thailand. Such service reduces several technical limitations of conventional RTK method and increases the productivity in a cost effective manner. This also opens the question how feasible DOL-VRS service may be for establishing survey control. The conventional way to set up high accuracy survey control for geospatial applications is in post processing mode using relative positioning method and double differenced carrier phase algorithms. Apart of the rover, the user needs a second receiver to collect data simultaneously {{and at the same}} epoch rate at a known point. VRS <b>eliminates</b> <b>this</b> <b>requirement</b> by creating synthetic observations for a non-physical, un-occupied, invisible reference station situated only few meters from the approximate location of the rover. On the other hand, previous studies in Thailand concluded that the ionospheric error is the main error source that degrades the VRS performance. Thus, this study investigates the feasibility from two points of view. The first point is the observation time. The second point is the influence of ionospheric conditions on the final network based solution. Therefore, we define four relevant cases: low, medium, high, and extreme cases as function of total electron content value. In addition, the ambiguity fixing rate, rate position jump, and root mean squared error are defined as performance indicators. The numerical analysis reveal that the reliable survey control may be obtained when at least 180 epoch...|$|E
40|$|Purpose: A key {{challenge}} for image guided coronary interventions is accurate and absolutely robust image registration bringing together preinterventional information extracted from a three-dimensional (3 D) patient scan and live interventional image information. In this paper, the authors present a novel scheme for 3 D to two-dimensional (2 D) rigid registration of coronary arteries extracted from preoperative image scan (3 D) {{and a single}} segmented intraoperative x-ray angio frame in frequency and spatial domains for real-time angiography interventions by C-arm fluoroscopy. Methods: Most existing rigid registration approaches require a close initialization due to the abundance of local minima and high complexity of search algorithms. The authors' method <b>eliminates</b> <b>this</b> <b>requirement</b> by transforming the projections into translation-invariant Fourier domain for estimating the 3 D pose. For 3 D rotation recovery, template Digitally Reconstructed Radiographs (DRR) as candidate poses of 3 D vessels of segmented computed tomography angiography are produced by rotating the camera (image intensifier) around the DICOM angle values with a specific range as in C-arm setup. The authors have compared the 3 D poses of template DRRs with the segmented x-ray after equalizing the scales in three domains, namely, Fourier magnitude, Fourier phase, and Fourier polar. The best rotation pose candidate was chosen {{by one of the}} highest similarity measures returned by the methods in these domains. It has been noted in literature that frequency domain methods are robust against noise and occlusion which was also validated by the authors' results. 3 D translation of the volume was then recovered by distance-map based BFGS optimization well suited to convex structure of the authors' objective function without local minima due to distance maps. A novel automatic x-ray vessel segmentation was also performed in this study. Results: Final results were evaluated in 2 D projection space for patient data; and with ground truth values and landmark distances for the images acquired with a solid phantom vessel. Results validate that rotation recovery in frequency domain is robust against differences in segmentations in two modalities. Distance-map translation is successful in aligning coronary trees with highest possible overlap. Conclusions: Numerical and qualitative results show that single view rigid alignment in projection space is successful. This work can be extended with multiple views to resolve depth ambiguity and with deformable registration to account for nonrigid motion in patient data. (c) 2013 American Association of Physicists in Medicine...|$|E
40|$|We {{present a}} {{technique}} to extend the geometric construction of diagonal discrete Hodge operators to arbitrary triangular and tetrahedral boundary conforming Delaunay meshes in the frequent case of piecewise uniform and isotropic material parameters. The technique {{is based on the}} novel concept of signed dual complex that originates from a physical argument. In particular, it is shown how the positive definiteness of the mass matrix obtained with the signed dual complex is easily ensured for all boundary conforming Delaunay meshes without requiring—as expected by the common knowledge—that each circumcenter has to lie inside the corresponding element. <b>Eliminating</b> <b>this</b> <b>requirement,</b> whose fulfillment presents otherwise formidable practical difficulties, enables one to easily obtain efficient, consistent, and stable schemes...|$|R
40|$|The {{effect of}} the nmr- 1 and ms- 5 mutations, which lead to {{insensitivity}} to glutamine-mediated nitrogen metabolite repression, was examined with respect to extracellular deaminase production by Neurospora crassa. Deaminase production normally requires nitrogen limitation, but these mutations <b>eliminated</b> <b>this</b> <b>requirement</b> and allowed production of deaminase activity under nitrogen metabolite repressing conditions. Demonstration of normal glutamine transport by both strains eliminated the possibility that these mutations exerted their effects through repressor exclusion. We have proposed a new working model for nitrogen regulation in Neurospora {{based on the findings}} that these mutations affected a nitrogen-regulated activity in addition to those activities originally reported and that the mutations are genetically very closely linked and likely allelic...|$|R
25|$|Windows XP Mode (XPM) is {{a virtual}} machine package for Windows Virtual PC {{containing}} a pre-installed, licensed copy of Windows XP Professional with Service Pack 3 as its guest OS. Previously, both the CPU and motherboard of the host had to support hardware virtualization, but an update in early 2010 <b>eliminated</b> <b>this</b> <b>requirement.</b> Pre-installed integration components allow applications running within the virtualized environment to appear as if running directly on the host, sharing the native desktop and start menu of Windows 7 as well as participating in file type associations. Windows XP Mode applications run in a Terminal Services session in the virtualized Windows XP, and are accessed via Remote Desktop Protocol by a client running on the Windows 7 host.|$|R
40|$|Abstract. Traditionally, due to {{efficiency}} considerations, when encrypting long messages {{using an}} asymmtric cryptosystem, {{one needs to}} use a symmetric cryptosystem in addition. To <b>eliminate</b> <b>this</b> <b>requirement,</b> Hwang, Chang, and Hwang introduced an asymmetric cryptosystem for encrypting long messages. However, they did not give any formal proof of the security of this cryptosystem. In this paper, we propose an improved asymmetric cryptosystem for encrypting long messages, which is both efficient and secure. In the aspect of efficiency, our cryptosystem is about {{twice as fast as}} the Hwang-Chang-Hwang cryptosystem. In the aspect of security, besides providing an informal analysis, we rigorously show that computing any part of the plaintext message encrypted using our cryptosystem is as hard as breaking the ElGamal cryptosystem, even if all other parts of the message are already known to the adversary...|$|R
40|$|Statistical {{tests and}} their {{underlying}} measures of fit can be utilized to separate neutron/gamma-ray pulses in a mixed radiation field. In this article, first {{the application of}} a sample statistical test is explained. Fit measurement-based methods require true pulse shapes to be used as reference for discrimination. <b>This</b> <b>requirement</b> makes practical implementation of these methods difficult; typically another discrimination approach should be employed to capture samples of neutrons and gamma-rays before running the fit-based technique. In this article, we also propose a technique to <b>eliminate</b> <b>this</b> <b>requirement.</b> These approaches are applied to several sets of mixed neutron and gamma-ray pulses obtained through different digitizers using stilbene scintillator in order to analyze them and measure their discrimination quality. Statistické testy a jejich základní postupy mohou být použity pro separaci neutronů/gama záření ve smíšeném radiačním poli. V tomto článku je popsána první aplikace vzorku statistického testu. FIT metoda měření vyžaduje skutečné tvary impulsů, které mají být použity jako referenční pro vlastní diskriminaci. Tento požadavek metody je pro praktickou aplikaci problematický - obvykle by měla být použita jiný postup diskriminace pro zachycování vzorků neutronů a gama záření před aplikací metody FIT. V tomto článku se také navrhujeme techniku k odstranění tohoto požadavku. Tyto přístupy jsou aplikovány na několika sadách vzorků smíšeného pole neutronů a gama záření získaných několika typy digitizérů s použití scintiátoru stilbene pro získání představy o jejich podílu na kvalitě diskriminace. Statistical tests and their underlying measures of fit can be utilized to separate neutron/gamma-ray pulses in a mixed radiation field. In this article, first {{the application of a}} sample statistical test is explained. Fit measurement-based methods require true pulse shapes to be used as reference for discrimination. <b>This</b> <b>requirement</b> makes practical implementation of these methods difficult; typically another discrimination approach should be employed to capture samples of neutrons and gamma-rays before running the fit-based technique. In this article, we also propose a technique to <b>eliminate</b> <b>this</b> <b>requirement.</b> These approaches are applied to several sets of mixed neutron and gamma-ray pulses obtained through different digitizers using stilbene scintillator in order to analyze them and measure their discrimination quality...|$|R
50|$|The main {{disadvantage}} of neutral-density filters {{is that to}} be entirely flexible in your shooting you need to carry a range of different NDs. This can become an expensive proposition, especially if using screw filters with different lens filter sizes, which would require carrying a set for each diameter of lens carried (although inexpensive step-up rings can <b>eliminate</b> <b>this</b> <b>requirement).</b> To counter <b>this</b> problem, some manufacturers have created variable ND filters. These work by placing two polarizing filters together, {{at least one of}} which can rotate. The rear polarizing filter cuts out light in one plane. As the front element is rotated, it cuts out an increasing amount of the remaining light, the closer the front filters comes to being perpendicular to the rear filter. By using this technique, the amount of light reaching the sensor can be varied with almost infinite control.|$|R
40|$|This {{development}} {{was sponsored by}} Ball Aerospace for the Cryogenic On-Orbit LongLife Active Refrigerator (COOLLAR) program. The cryocooler is designed to cool objects to 65 K and operate in space for at least 7 years. The system also imports minimal impact to the spacecraft in terms of vibration and heat. The basic Joule-Thompson cycle involves compressing a working fluid, nitrogen in this case, at near-constant temperature from 17. 2 KPa to 6. 89 MPa. The nitrogen is then expanded through a Joule-Thompson valve. The pure nitrogen gas must be kept clean; therefore, any contamination from motor organic materials must be <b>eliminated.</b> <b>This</b> <b>requirement</b> drove the design towards sealing of the motor within a titanium housing without sacrificing motor performance. It is estimated that an unsealed motor would have contributed 1. 65 g of contaminants, due to the organic insulation and potting materials, over the 7 -year life. This paper describes the motor electrical and mechanical design, {{as well as the}} sealing difficulties encountered, along with their solutions...|$|R
5000|$|In January 2003 a Justice Department {{spokesperson}} said NSEERS helped {{law enforcement}} authorities apprehend 330 [...] "known criminals" [...] and three [...] "known terrorists”; The spokesperson made these remarks in order to advocate for renewed funding for the program, for which the Bush administration was requesting $16.8 million per fiscal year. However, by 1 December 2016, [...] "no known terrorism convictions resulted from the program," [...] according to a letter from some Democratic Members of Congress and New York Attorney General Eric Schneiderman. [...] By January 2003, at least 138,000 individuals were registered in NSEERS, according to testimony by the Department of Homeland Security to Congress. As of May 2003, 82,581 individuals had complied with the domestic portion of the program. Of these, at least 13,153 were placed in deportation proceedings. Although the program originally included a requirement to re-register annually, the Department of Homeland Security, which gained jurisdiction over the program, <b>eliminated</b> <b>this</b> <b>requirement.</b>|$|R
40|$|Conventional top-k spatial keyword queries require {{users to}} {{explicitly}} specify their preferences between spatial proximity and keyword relevance. In this work we investigate how to <b>eliminate</b> <b>this</b> <b>requirement</b> by enhancing the conventional queries with interaction, resulting in Interactive Top-k Spatial Keyword (ITkSK) query. Having confirmed the feasibility by theoretical analysis, we propose a three-phase solution focusing on both effectiveness and efficiency. The first phase substantially narrows down the search space for subsequent phases by efficiently retrieving {{a set of}} geo-textual k-skyband objects as the initial candidates. In the second phase three practical strategies for selecting a subset of candidates are developed {{with the aim of}} maximizing the expected benefit for learning user preferences at each round of interaction. Finally we discuss how to determine the termination condition automatically and estimate the preference based on the user 2 ̆ 7 s feedback. Empirical study based on real PoI datasets verifies our theoretical observation that the quality of top-k results in spatial keyword queries can be greatly improved through only a few rounds of interactions...|$|R
40|$|Simulation-based {{verification}} algorithms {{can provide}} formal safety guarantees for nonlinear and hybrid systems. The previous algorithms rely on user provided model annotations called discrepancy function, which are crucial for computing reachtubes from simulations. In <b>this</b> paper, we <b>eliminate</b> <b>this</b> <b>requirement</b> by presenting an algorithm for computing piece-wise exponential discrepancy functions. The algorithm relies on computing local convergence or divergence rates of trajectories along a simulation using a coarse over-approximation {{of the reach}} set and bounding the maximal eigenvalue of the Jacobian over this over-approximation. The resulting discrepancy function preserves the soundness and the relative completeness of the verification algorithm. We also provide a coordinate transformation method to improve the local estimates for the convergence or divergence rates in practical examples. We extend the method to get the input-to-state discrepancy of nonlinear dynamical systems {{which can be used}} for compositional analysis. Our experiments show that the approach is effective in terms of running time for several benchmark problems, scales reasonably to larger dimensional systems, and compares favorably with respect to available tools for nonlinear models. Comment: 24 page...|$|R
40|$|Abstract. Statistical {{methods of}} {{inference}} typically require the likelihood function to be computable {{in a reasonable}} amount of time. The class of “likelihood-free” methods termed Approximate Bayesian Computation (ABC) is able to <b>eliminate</b> <b>this</b> <b>requirement,</b> replacing the evaluation of the likelihood with simulation from it. Likelihood-free methods have gained in efficiency and popularity in the past few years, following their integration with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) in order to better explore the parameter space. They have been applied primarily to estimating the parameters of a given model, but {{can also be used to}} compare models. Here we present novel likelihood-free approaches to model comparison, based upon the independent estimation of the evidence of each model under study. Key advantages of these approaches over previous techniques are that they allow the exploitation of MCMC or SMC algorithms for exploring the parameter space, and that they do not require a sampler able to mix between models. We validate the proposed methods using a simple exponential family problem before providing a realistic problem from human population genetics: the comparison of different demographic models based upon genetic data from the Y chromosome...|$|R
40|$|The medical {{community}} {{must recognize that}} support of claims for Incapacity Benefit and related commercial schemes places the patient in a small and special sub-population of clinical practice which may require specialist investigation, treatment, and documentation. Determination of functional capacity and of disability requires knowledge either not available or unfamiliar to most physicians with caring and therapeutic roles, especially of legal or contractual provisions and occupational data. However, {{it is not necessary}} for them to determine disability and they should not be asked to do so. The new, medical assessment procedures for Incapacity Benefit in the UK do not require this, and the largest provider of related commercial schemes (Long Term Disability; Permanent Health Insurance) has already <b>eliminated</b> <b>this</b> <b>requirement</b> from its application process. When such application is anticipated or requested, the medical record should be prepared and appropriate consultation obtained. Subjective issues should be identified and addressed. Comprehensive psychiatric evaluation, especially in subjective impairment, is critical in chronic incapacity. The estimation of functional capacities in the absence of objective data is particularly troublesome, but, clinicians can provide the Disability Medical Analyst with appropriate medical documentation...|$|R
40|$|Statistical {{methods of}} {{inference}} typically require the likelihood function to be computable {{in a reasonable}} amount of time. The class of "likelihood-free" methods termed Approximate Bayesian Computation (ABC) is able to <b>eliminate</b> <b>this</b> <b>requirement,</b> replacing the evaluation of the likelihood with simulation from it. Likelihood-free methods have gained in efficiency and popularity in the past few years, following their integration with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) in order to better explore the parameter space. They have been applied primarily to the estimation of the parameters of a given model, but {{can also be used to}} compare models. Here we present novel likelihood-free approaches to model comparison, based upon the independent estimation of the evidence of each model under study. Key advantages of these approaches over previous techniques are that they allow the exploitation of MCMC or SMC algorithms for exploring the parameter space, and that they do not require a sampler able to mix between models. We validate the proposed methods using a simple exponential family problem before providing a realistic problem from population genetics: the comparison of different growth models based upon observations of human Y chromosome data from the terminal generation...|$|R
30|$|In the literature, {{there are}} many methods for {{predicting}} diffusion coefficients of gases in hydrocarbon systems. These methods can be roughly categorized into direct and indirect methods. In the first category, first measurements of diffusion coefficients were performed by Hill and Lacey (1934) for the methane–decane system at low pressures. Later, Woessner et al. (1969) reported some experimental data for gases in heavy oil and bitumen at reservoir pressure. The above-mentioned direct methods involve compositional analysis of liquid samples extracted at different times during the diffusion process, which is tedious and expensive. To <b>eliminate</b> <b>this</b> <b>requirement,</b> Riazi (1996) developed a simple and indirect way to predict diffusion coefficients of gases—the pressure decay method. It is an experimental method for predicting diffusion coefficients of gases in liquids using a PVT cell. The step changes in the pressure within the PVT cell in combination with a developed mathematical model were used to predict the diffusion coefficients. Later, Jamialahmadi et al. (2006) and Etminan et al. (2010) reported a new method for predicting diffusion coefficients of gases in heavy oil from experimental volume–time profiles. In this method, the volume changes of the PVT cell were recorded and used to calculate diffusion coefficients of gases in liquid hydrocarbon systems instead of the pressure changes.|$|R
40|$|Statistical {{methods of}} {{inference}} typically require the likelihood function to be computable {{in a reasonable}} amount of time. The class of "likelihood-free" methods termed Approximate Bayesian Computation (ABC) is able to <b>eliminate</b> <b>this</b> <b>requirement,</b> replacing the evaluation of the likelihood with simulation from it. Likelihood-free methods have gained in efficiency and popularity in the past few years, following their integration with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) in order to better explore the parameter space. They have been applied primarily to estimating the parameters of a given model, but {{can also be used to}} compare models. Here we present novel likelihood-free approaches to model comparison, based upon the independent estimation of the evidence of each model under study. Key advantages of these approaches over previous techniques are that they allow the exploitation of MCMC or SMC algorithms for exploring the parameter space, and that they do not require a sampler able to mix between models. We validate the proposed methods using a simple exponential family problem before providing a realistic problem from human population genetics: the comparison of different demographic models based upon genetic data from the Y chromosome. © 2011 International Society for Bayesian Analysis...|$|R
40|$|The {{function}} of DNA polymerase II of Escherichia coli {{is an old}} question. Any phenotypic character that Pol II may confer upon the cell has escaped detection since the polymerase was discovered 24 yr ago. Although {{it has been shown}} that Pol II enables DNA synthesis to proceed past abasic sites in vitro, no role is known for it in the bypass of those lesions in vivo. From a study of phage S 13 single-stranded DNA, we now report SOS conditions under which Pol II is needed for DNA synthesis to proceed past abasic sites with 100 % efficiency in vivo. Overproduction of the GroES(+) L(+) heat shock proteins, which are members of a ubiquitous family of molecular chaperones, <b>eliminated</b> <b>this</b> <b>requirement</b> for Pol II, which may explain why the role of Pol II in SOS repair had eluded discovery. Mutagenesis accompanied SOS bypass of abasic sites when the original occupant had been cytosine but not when it had been thymine; the quantitative difference is shown to imply that adenine was inserted opposite the abasic sites at least 99. 7 % of the time, which is an especially strict application of the A-rule. Most, but not all, spontaneous mutations from Rif(s) to Rif(r), whether in a recA(+) or a recA(Prt(c)) cell, require Pol II; while this suggests that cryptic abasic lesions are a likely source of spontaneous mutations, it also shows that such lesions cannot be the exclusive source...|$|R
40|$|This {{manuscript}} {{version is}} made available under the CC-BY-NC-ND 4. 0 license [URL] paper describes a real-time sequential method to simultaneously recover the camera motion and the 3 D shape of deformable objects from a calibrated monocular video. For this purpose, {{we consider the}} Navier-Cauchy equations used in 3 D linear elasticity and solved by finite elements, to model the time-varying shape per frame. These equations are embedded in an extended Kalman filter, resulting in sequential Bayesian estimation approach. We represent the shape, with unknown material properties, as a combination of elastic elements whose nodal points correspond to salient points in the image. The global rigidity of the shape is encoded by a stiffness matrix, computed after assembling each of these elements. With this piecewise model, we can linearly relate the 3 D displacements with the 3 D acting forces that cause the object deformation, assumed to be normally distributed. While standard finite-element-method techniques require imposing boundary conditions to solve the resulting linear system, in <b>this</b> work we <b>eliminate</b> <b>this</b> <b>requirement</b> by modeling the compliance matrix with a generalized pseudoinverse that enforces a pre-fixed rank. Our framework also ensures surface continuity {{without the need for}} a post-processing step to stitch all the piecewise reconstructions into a global smooth shape. We present experimental results using both synthetic and real videos for different scenarios ranging from isometric to elastic deformations. We also show the consistency of the estimation with respect to 3 D ground truth data, include several experiments assessing robustness against artifacts and finally, provide an experimental validation of our performance in real time at frame rate for small mapsPeer ReviewedPostprint (author's final draft...|$|R
50|$|This slave {{server module}} {{facilitates}} deployment of Ulteo OVD applications over the Internet by tunneling connections to application servers through an SSL (443) connection. <b>This</b> <b>eliminates</b> the <b>requirement</b> to expose individual application servers {{with a public}} IP address. It also eases access for clients which are behind firewalls, as many firewall environments allow outgoing SSL traffic on port 443 with no further restrictions. The Gateway is a Premium module.|$|R
40|$|While the Laplace {{resonance}} {{among the}} three inner Galilean satellites possesses stable configurations where the mean motions taken by pairs are in nearly 2 : 1 ratios, or 'deep resonance', the current satellite configuration is unstable near this exact commensurability. There is presently noted, however, a continuous path of stable conditions branching toward deep resonance which furnishes a track for the tidal evolution {{of the system and}} renders scenarios involving (probably episodic) evolution from deep resonance viable; <b>this</b> <b>eliminates</b> the <b>requirement</b> for rapid tidal dissipation of Jupiter by the alternative equilibrium hypothesis...|$|R
40|$|Background: Prior to January 2009, {{sexual assault}} victims were {{denied access to}} {{forensic}} services unless they reported their assault to police. The 2005 reauthorization of the Violence Against Women Act (VAWA 2005) <b>eliminated</b> <b>this</b> <b>requirement</b> and required states to provide sexual assault victims access to free forensic services without police involvement. The state of Maryland instituted Jane Doe Reporting (JDR) in response to VAWA 2005. Purpose of Study: (1) Describe JDR victims and JDR victims who convert to police involvement (JDR converters); (2) Identify differences in the individual, assault, and assailant characteristics for JDR victims, JDR converters, and victims who immediately notify the police; (3) Understand the limitations and benefits of JDR policy {{from the perspective of}} Forensic Nurse Examiners (FNEs); (4) Explore potential modifications for JDR to improve the number of JDR victims who convert their case to police involvement. Methods: The study was a 2 -phase exploratory study with a sequential design. The quantitative analysis included bivariate statistics and logistic regression on data collected from forensic records. Data for the qualitative analysis was collected from interviews with FNEs and was analyzed using premises of grounded theory. Results: The bivariate analysis demonstrated that JDR victims were more likely to experience a drug-facilitated sexual assault when compared to other victim groups. The multivariate analysis demonstrated that weapon use, victim resistance, race, and genital injuries are significant predictors of police reporting. The qualitative analysis identified 3 central themes: (1) Benefits of the JDR policy results in Victim Empowerment; (2) Limitations of the JDR policy creates Negative Consequences for Victims and Society; and (3) Proposed modifications to JDR increase the potential for Informed Decision Making of sexual assault victims. Conclusion: JDR victims experience significantly different types of sexual assaults when compared to victims who immediately notified police. Considering the nature of sexual assaults experienced by JDR victims, it is unlikely police notification would occur without the JDR policy. Despite noted benefits of the JDR policy, modifications are needed to ensure victims are able to make informed decisions regarding evidence collection and police involvement following a sexual assault...|$|R
40|$|In Press, Accepted Manuscript — Note to usersThis paper {{describes}} a real-time sequential method to simultaneously recover the camera motion and the 3 D shape of deformable objects from a calibrated monocular video. For this purpose, {{we consider the}} Navier-Cauchy equations used in 3 D linear elasticity and solved by finite elements, to model the time-varying shape per frame. These equations are embedded in an extended Kalman filter, resulting in sequential Bayesian estimation approach. We represent the shape, with unknown material properties, as a combination of elastic elements whose nodal points correspond to salient points in the image. The global rigidity of the shape is encoded by a stiffness matrix, computed after assembling each of these elements. With this piecewise model, we can linearly relate the 3 D displacements with the 3 D acting forces that cause the object deformation, assumed to be normally distributed. While standard finite-element-method techniques require imposing boundary conditions to solve the resulting linear system, in <b>this</b> work we <b>eliminate</b> <b>this</b> <b>requirement</b> by modeling the compliance matrix with a generalized pseudoinverse that enforces a pre-fixed rank. Our framework also ensures surface continuity {{without the need for}} a post-processing step to stitch all the piecewise reconstructions into a global smooth shape. We present experimental results using both synthetic and real videos for different scenarios ranging from isometric to elastic deformations. We also show the consistency of the estimation with respect to 3 D ground truth data, include several experiments assessing robustness against artifacts and finally, provide an experimental validation of our performance in real time at frame rate for small maps. This work has been partially supported by the Spanish Ministry of Science and Innovation under projects SVMap DIP 2012 - 32168, RobInstruct TIN 2014 - 58178 -R and Keratocono DPI 2014 - 54981 -R; by the FP 7 -SME- 2013 606634 PopCorn project from the European Union FP 7; by the ERA-net CHISTERA project VISEN PCIN- 2013 - 047 and I-DRESS PCIN- 2015 - 147; and by a scholarship FPU 12 / 04886 awarded by the Spanish MECD. Peer reviewe...|$|R
40|$|Wireless {{microwave}} telemetry {{addresses the}} difficult issue of ob-taining transducer outputs from reciprocating and rotating com-ponents {{through the use}} of advanced electronic components. <b>This</b> <b>eliminates</b> the <b>requirements</b> of a direct link between the trans-ducer and the acquisition system. Accuracy of the transducer sig-nal is maintained {{through the use of}} a double frequency modula-tion technique which provides temperature stability and a 20 point calibration of the complete system. Multiple transmitters can be used for larger applications and multiple antennas can be used to improve the signal strength and reduce the possibility of dropouts. Examples of automotive torque converter and piston temperature measurements are provided, showing the effectiveness of the wire-less measuring technique. fDOI: 10. 1115 / 1. 2771562...|$|R
