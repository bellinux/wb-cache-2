5320|3369|Public
25|$|<b>Gaussian</b> <b>process</b> is a {{powerful}} non-linear interpolation tool. Many popular interpolation tools are actually equivalent to particular Gaussian processes. Gaussian processes can be used not only for fitting an interpolant that passes exactly through the given data points but also for regression, i.e., for fitting a curve through noisy data. In the geostatistics community <b>Gaussian</b> <b>process</b> regression {{is also known as}} Kriging.|$|E
25|$|Bayesian Quadrature is a {{statistical}} {{approach to the}} numerical problem of computing integrals and falls under the field of probabilistic numerics. It can provide a full handling of the uncertainty over {{the solution of the}} integral expressed as a <b>Gaussian</b> <b>Process</b> posterior variance. It is also known to provide very fast convergence rates which can be up to exponential in the number of quadrature points n.|$|E
50|$|Inference of {{continuous}} values with a <b>Gaussian</b> <b>process</b> prior {{is known as}} <b>Gaussian</b> <b>process</b> regression, or kriging; extending <b>Gaussian</b> <b>process</b> regression to multiple target variables is known as cokriging. Gaussian processes are thus useful as a powerful non-linear multivariate interpolation tool. <b>Gaussian</b> <b>process</b> regression can be further extended to address learning tasks in both supervised (e.g. probabilistic classification) and unsupervised (e.g. manifold learning) learning frameworks.|$|E
40|$|This paper studies {{construction}} of quantum <b>Gaussian</b> <b>processes</b> based on ordinary <b>Gaussian</b> <b>processes</b> through their reproducing kernel Hilbert spaces, and investigate {{the relationship between}} the stochastic properties of the quantum <b>Gaussian</b> <b>processes</b> and the base <b>Gaussian</b> <b>processes.</b> In particular, we construct quantum Brownian bridges and quantum Ornstein-Uhlenbeck processes...|$|R
5000|$|VFGP (Variable Fidelty <b>Gaussian</b> <b>Processes)</b> — builds models using <b>Gaussian</b> <b>processes</b> {{regression}} ideas ...|$|R
5000|$|SVFGP (Sparse Variable Fidelity <b>Gaussian</b> <b>Processes)</b> is {{designed}} to handle large samples with <b>Gaussian</b> <b>processes</b> regression-based technique.|$|R
5000|$|The typical {{model for}} a {{computer}} code output is a <b>Gaussian</b> <b>process.</b> For notational simplicity, assume [...] is a scalar. Owing to the Bayesian framework, we fix our belief that the function [...] follows a <b>Gaussian</b> <b>process,</b> ...|$|E
5000|$|The normal-inverse Gaussian {{distribution}} {{can also}} be seen as the marginal distribution of the normal-inverse <b>Gaussian</b> <b>process</b> which provides an alternative way of explicitly constructing it. Starting with a drifting Brownian motion (Wiener process), , we can define the inverse <b>Gaussian</b> <b>process</b> [...] Then given a second independent drifting Brownian motion, , the normal-inverse <b>Gaussian</b> <b>process</b> is the time-changed process [...] The process [...] at time 1 has the normal-inverse Gaussian distribution described above. The NIG process is a particular instance of the more general class of Lévy processes.|$|E
50|$|The Ornstein-Uhlenbeck {{process is}} a {{stationary}} <b>Gaussian</b> <b>process.</b>|$|E
5000|$|Proprietary methods (Higher Dimensional Approximation, SGP - Sparse <b>Gaussian</b> <b>Processes,</b> Tensor Approximation and {{incomplete}} Tensor Approximation, Tensor <b>Gaussian</b> <b>Processes</b> etc.) ...|$|R
40|$|<b>Gaussian</b> <b>processes</b> are {{supervised}} learning tools. Just like Supervised Artificial Neural Network, <b>Gaussian</b> <b>Processes</b> distill data structure from real data. However, {{since there are}} some disadvantages of <b>Gaussian</b> <b>Processes,</b> this research introduces Genetic Algorithms to obtain the optimal parameter values for <b>Gaussian</b> <b>processes</b> {{in order to improve}} the learning capabilities on forecasting real time series financial data. This improvement could increase the learning speed and avoid the pre-setting of parameter values which affect the forecasting results. 1...|$|R
30|$|<b>Gaussian</b> <b>processes</b> {{are natural}} {{extensions}} of multivariate Gaussian random variables to infinite (countably or continuous) index sets. For <b>Gaussian</b> <b>processes,</b> strong and weak stationarity {{are the same}} concept. <b>Gaussian</b> <b>processes</b> {{are by far the}} most accessible and well-understood processes (on uncountable index sets), which are important in statistical modeling because of properties inherited from the normal one, and many deep theoretical analyses of various properties are available.|$|R
5000|$|This {{result is}} {{extended}} by the Donsker’s theorem, which {{asserts that the}} empirical process , viewed as a function indexed by , converges in distribution in the Skorokhod space [...] to the mean-zero <b>Gaussian</b> <b>process</b> , where [...] is the standard Brownian bridge. The covariance structure of this <b>Gaussian</b> <b>process</b> is ...|$|E
5000|$|Module 2: <b>Gaussian</b> <b>process</b> {{modeling}} for {{the discrepancy}} function ...|$|E
5000|$|... is a delta-correlated {{stationary}} <b>Gaussian</b> <b>process</b> with zero-mean, satisfying ...|$|E
40|$|<b>Gaussian</b> <b>processes</b> {{are usually}} parameterised {{in terms of}} their {{covariance}} functions. However, this makes it difficult to deal with multiple outputs, because ensuring that the covariance matrix is positive definite is problematic. An alternative formulation is to treat <b>Gaussian</b> <b>processes</b> as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead. Using this, we extend <b>Gaussian</b> <b>processes</b> to handle multiple, coupled outputs. ...|$|R
40|$|We {{propose a}} simple method that {{combines}} neural networks and <b>Gaussian</b> <b>processes.</b> The proposed method can estimate {{the uncertainty of}} outputs and flexibly adjust target functions where training data exist, which are advantages of <b>Gaussian</b> <b>processes.</b> The proposed method can also achieve high generalization performance for unseen input configurations, which is an advantage of neural networks. With the proposed method, neural networks are used for the mean functions of <b>Gaussian</b> <b>processes.</b> We present a scalable stochastic inference procedure, where sparse <b>Gaussian</b> <b>processes</b> are inferred by stochastic variational inference, and the parameters of neural networks and kernels are estimated by stochastic gradient descent methods, simultaneously. We use two real-world spatio-temporal data sets to demonstrate experimentally that the proposed method achieves better uncertainty estimation and generalization performance than neural networks and <b>Gaussian</b> <b>processes...</b>|$|R
40|$|We derive the {{distribution}} of the first exit value for a class of symmetric real-valued Markov processes with finite Green's functions using prediction theory for <b>Gaussian</b> <b>processes</b> and Dynkin's theory which relates Markov and <b>Gaussian</b> <b>processes.</b> For Lévy processes with exponential lifetime this method allows us to easily rederive Rogozin's infinitely divisible factorization and to obtain the Fourier transform of {{the distribution}} of the first exit value. Lévy processes first exit value prediction of <b>Gaussian</b> <b>processes</b> stationary processes Wiener filter...|$|R
5000|$|... #Subtitle level 2: Frequency of {{exceedance}} for a <b>Gaussian</b> <b>process</b> ...|$|E
50|$|In {{probability}} theory and statistics, a <b>Gaussian</b> <b>process</b> {{is a particular}} kind of statistical model where observations occur in a continuous domain, e.g. time or space. In a <b>Gaussian</b> <b>process,</b> every point in some continuous input space is associated with a normally distributed random variable. Moreover, every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a <b>Gaussian</b> <b>process</b> is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.|$|E
5000|$|When {{the noise}} is white <b>gaussian</b> <b>process,</b> the signal power is ...|$|E
30|$|On {{the other}} hand, since Chung’s law and Strassen’s {{functional}} {{law of the}} iterated logarithm appeared, the functional law of the iterated logarithm and its rates for some classes of <b>Gaussian</b> <b>processes</b> have been discussed by many authors (see, for example, Csörgö and Révész [9], Lin et al. [10], Dudley and Norvaiša [7], Malyarenko [11]). However, almost all results considered only some <b>Gaussian</b> <b>processes</b> with stationary increments, {{and there has been}} little systematic investigation on other self-similar <b>Gaussian</b> <b>processes</b> (see, for example, Norvaiša [8], Tudor and Xiao [12], and Yan et al. [13]). The main reason for this is the complexity of dependence structures for self-similar <b>Gaussian</b> <b>processes</b> which do not have stationary increments.|$|R
50|$|The {{concept of}} <b>Gaussian</b> <b>processes</b> {{is named after}} Carl Friedrich Gauss because {{it is based on}} the notion of the Gaussian {{distribution}} (normal distribution). <b>Gaussian</b> <b>processes</b> can be seen as an infinite-dimensional generalization of multivariate normal distributions.|$|R
5000|$|<b>Gaussian</b> <b>processes</b> (also {{known as}} kriging), where the any {{combination}} of output points {{is assumed to be}} distributed as a multivariate Gaussian distribution. Recently, [...] "treed" [...] <b>Gaussian</b> <b>processes</b> have been used to deal with heteroscedastic and discontinuous responses.|$|R
5000|$|It is {{seen that}} [...] is white noise convolved with the [...] kernel plus the {{constant}} mean. If the white noise [...] is a <b>Gaussian</b> <b>process</b> then [...] {{is also a}} <b>Gaussian</b> <b>process.</b> In other cases, the central limit theorem indicates that [...] will be approximately normally distributed when [...] is close to one.|$|E
5000|$|... #Caption: An {{example of}} <b>Gaussian</b> <b>Process</b> Regression (prediction) {{compared}} with other regression models.|$|E
50|$|Efficient {{generation}} of fractional Brownian surfaces poses significant challenges. Since the Brownian surface represents a <b>Gaussian</b> <b>process</b> with a nonstationary covariance function,one {{can use the}} Cholesky decomposition method. A more efficient method is Stein's method,which generates an auxiliary stationary <b>Gaussian</b> <b>process</b> using the circulant embedding approach and then adjusts this auxiliary process to obtain the desired nonstationary <b>Gaussian</b> <b>process.</b> The figure below shows three typical realizations of fractional Brownian surfaces for different values of the roughness or Hurst parameter. The Hurst parameter is always between zero and one, with values closer to one corresponding to smoother surfaces. These surfaces were generated using a Matlab implementation of Stein's method.|$|E
40|$|We use nowdays {{classical}} {{theory of}} generalized moment problems by Krein-Nudelman [1977] {{to define a}} special class of stochastic <b>Gaussian</b> <b>processes.</b> The class contains, of course, stationary <b>Gaussian</b> <b>processes.</b> We obtain a spectral representation for the processes from this class and we solve the corresponding prediction problem. The orthogonal rational functions on the unit circle lead to a class of <b>Gaussian</b> <b>processes</b> providing an example for the above construction. Comment: a very preliminary version; 18 page...|$|R
40|$|We {{consider}} the extreme values of fractional Brownian motions, self-similar <b>Gaussian</b> <b>processes</b> and more general <b>Gaussian</b> <b>processes</b> {{which have a}} trend -ct[beta] for some constants c,[beta]> 0 and a variance t 2 H. We derive the tail behaviour of these extremes and show that they occur mainly in the neighbourhood of the unique point t 0 where the related boundary function (u+ct[beta]) /tH is minimal. We {{consider the}} case that HExtreme values <b>Gaussian</b> <b>processes</b> Fractional Brownian motions Self-similar processes...|$|R
40|$|For a {{class of}} {{stationary}} <b>Gaussian</b> <b>processes</b> and for large correlation times, the asymptotic behavior of the upcrossing first passage time probability densities is investigated. Parallel simulations of sample paths of special stationary <b>Gaussian</b> <b>processes</b> for large correlations times provide a statistical validation of the theoretical results...|$|R
5000|$|A <b>Gaussian</b> <b>process</b> with Matérn {{covariance}} has sample {{paths that}} are [...] times differentiable.|$|E
50|$|Smoothing splines have an {{interpretation}} as the posterior mode of a <b>Gaussian</b> <b>process</b> regression.|$|E
50|$|<b>Gaussian</b> <b>process</b> is a {{powerful}} non-linear interpolation tool. Many popular interpolation tools are actually equivalent to particular Gaussian processes. Gaussian processes can be used not only for fitting an interpolant that passes exactly through the given data points but also for regression, i.e., for fitting a curve through noisy data. In the geostatistics community <b>Gaussian</b> <b>process</b> regression {{is also known as}} Kriging.|$|E
40|$|Under a {{restriction}} of the quasi-Markov property on the circle S 1, we give {{detailed descriptions of}} stationary <b>Gaussian</b> <b>processes</b> X(t), t∈S 1, and of <b>Gaussian</b> <b>processes</b> Y(t) with stationary increments to observe the mutual dependence of two innovation processes arising from the forward and backward canonical representations...|$|R
40|$|From a {{standpoint}} of the stochastic Ito-Volterra equation and the canonical representation of <b>Gaussian</b> <b>processes</b> ([13] and [2]), we investigate self-similar processes derived from fractional Brownian motions ([10]). In particular, we generalize a key property of T-positivity that was assumed in Okabe's theory for stationary <b>Gaussian</b> <b>processes</b> ([15]～[17]) ...|$|R
40|$|Let {[omega](t) }t[greater-or-equal, slanted] 0 be a stochastically {{differentiable}} {{stationary process}} in m and let satisfy limu[short up arrow]u 2 P{[omega](0) [set membership, variant] Au} = 0. We give {{a method to}} find the asymptotic behaviour of P{[union operator] 0 [less-than-or-equals, slant]t[less-than-or-equals, slant]h{[omega](t) [set membership, variant] Au}} as u [short up arrow]u 2. We use our method to study hitting probabilities for small sets with application to <b>Gaussian</b> <b>processes</b> and to study suprema of processes in with application to (the norm of) <b>Gaussian</b> <b>processes</b> in Hilbert space. extreme values crossings <b>Gaussian</b> <b>processes...</b>|$|R
