1307|4649|Public
25|$|Adding <b>data</b> <b>{{validation}},</b> using validation formulae, and conditional formatting features without manually writing code.|$|E
25|$|Microsoft Office Forms Server 2007 allows InfoPath {{forms to}} be {{accessed}} and filled out using any browser, including mobile phone browsers. Forms Server 2007 also supports using a database or other data source as the back-end for the form. Additionally, it allows centralized deployment {{and management of}} forms. Forms Server 2007 hosted forms also support <b>data</b> <b>validation</b> and conditional formatting, as does their InfoPath counterpart. It also supports advanced controls like Repeating section and Repeating table. However, some InfoPath controls cannot be used if it must be hosted on a Forms server.|$|E
5000|$|... 1. A <b>Data</b> <b>Validation</b> Certificate (DVC), {{delivering}} {{the results of}} <b>data</b> <b>validation</b> operations, performed by the DVCS.|$|E
50|$|Many {{computer}} systems implement data entry forms, but data collection systems {{tend to be}} more complex, with possibly many related forms containing detailed user input fields, <b>data</b> <b>validations,</b> and navigation links among the forms.|$|R
40|$|Measurements in {{wind tunnel}} yield <b>data</b> for <b>validation</b> of computations. Report {{describes}} experimental investigation of three-dimensional interaction of helicopter rotor blade with vortex like that generated by preceeding rotor blade. Provides theoretical aerodynamicists with <b>data</b> for <b>validation</b> of computer simulations of aerodynamic flow...|$|R
40|$|Results {{from using}} {{different}} breast cancer data set as the {{training data set}} in the MCL+superpc approach In order to check the robustness of our MCL+superpc approach, we used each of four <b>validation</b> <b>data</b> sets as the training data set, and the remaining four <b>data</b> sets as <b>validation</b> <b>data</b> sets. The following tables show these results. Table S 1. Superpc continuous prediction results from breast cancer data analysis. The results were generated by using the GSE 4922 data set as the training data set and four independent <b>data</b> sets as <b>validation</b> <b>data</b> with a threshold value of 1. 10 and 9 selected MCL modules. The training data set is highlighted in red. P-values less than 0. 05 are highlighted in yellow...|$|R
5000|$|Advanced <b>data</b> <b>validation</b> and {{reconciliation}} (DVR) is an integrated approach of combining data reconciliation and <b>data</b> <b>validation</b> techniques, which {{is characterized by}} ...|$|E
5000|$|... 6. {{check the}} {{validity}} of its own signing key and certificate before delivering <b>data</b> <b>validation</b> certificates and MUST not deliver <b>data</b> <b>validation</b> certificate in case of failure.|$|E
50|$|Failures or {{omissions}} in <b>data</b> <b>validation</b> {{can lead}} to data corruption or a security vulnerability. <b>Data</b> <b>validation</b> checks that data are valid, sensible, reasonable, and secure before they are processed.|$|E
50|$|<b>Data</b> type <b>validation</b> is customarily {{carried out}} on one or more simple data fields.|$|R
30|$|A model’s {{weaknesses}} can {{be exposed}} by {{the deterioration of}} the model’s fit using fresh data. It is therefore reasonable to hold out part of the available data for testing the estimated model [36, 37]. The two parts of the data are labeled in-sample and out-of-sample or, more recently, training <b>data</b> and <b>validation</b> <b>data.</b>|$|R
40|$|Consistent {{methods for}} the {{estimation}} of linear and nonlinear regression models with general measurement errors in variables {{in the presence of}} <b>validation</b> <b>data</b> information are introduced. The estimation procedures do not rely on any specific specification of auxiliary structural measurement error equations and are robust against any specification of measurement error equations. the estimation procedures can be applied even to situations where <b>validation</b> <b>data</b> alone does not permit possible estimation of the model. When <b>validation</b> <b>data</b> is rich enough for consistent estimation of the model, survey data are still valuable in improving efficiency of the <b>validation</b> <b>data</b> estimates. The estimation procedures can be applied to models and data where direct bias corrections might not be feasible...|$|R
50|$|A <b>Data</b> <b>Validation</b> and Certification Server (DVCS) is a Trusted Third Party (TTP) {{providing}} <b>data</b> <b>validation</b> services, asserting correctness of digitally signed documents, {{validity of}} public key certificates, and possession or existence of data.|$|E
50|$|<b>Data</b> <b>Validation</b> and Certification Server (DVCS) is {{a public}} key {{infrastructure}} or PKI service providing <b>data</b> <b>validation</b> services, asserting correctness of digitally signed documents, validity of public key certificates and possession or existence of data.|$|E
50|$|If {{the request}} is valid, the DVCS {{performs}} all necessary verification steps, and generates a <b>Data</b> <b>Validation</b> Certificate(DVC), and sends a response message containing the DVC {{back to the}} requestor. The <b>Data</b> <b>Validation</b> Certificate {{is formed as a}} signed document (CMS Signed Data).|$|E
3000|$|... 2 {{and mean}} {{absolute}} error (MAE) values for the constructed models and average residual and MAE values from the <b>validation</b> <b>data</b> set. Residual plots for the <b>validation</b> <b>data</b> set were visually examined {{to determine if the}} residuals were heteroscedastic and normally distributed.|$|R
40|$|Recent {{validation}} {{studies show}} that survey misreporting is pervasive and biases common analyses. Addressing this problem is further complicated, because <b>validation</b> <b>data</b> are usually convenience samples and access is restricted, making them more suitable to document than to solve the problem. I first use administrative SNAP records linked to survey data to evaluate corrections for misreporting that have been applied to survey data. Second, I develop a method that combines public use data with an estimated conditional distribution from the <b>validation</b> <b>data.</b> It does not require access to the <b>validation</b> <b>data,</b> is simple to implement and applicable {{to a wide range}} of econometric models. Using the <b>validation</b> <b>data,</b> I show that this method improves upon both the survey data and the other corrections, particularly for multivariate analyses. Some survey-based corrections also yield large error reductions, which makes them attractive alternatives when <b>validation</b> <b>data</b> do not exist. Finally, I examine whether estimates can be improved based on similar <b>validation</b> <b>data,</b> to mitigate that the population of interest is rarely validated. For SNAP, I provide evidence that extrapolation using the method developed here improves over survey data and corrections without <b>validation</b> <b>data.</b> Deviations from the geographic distribution of program spending are often reduced by a factor of 5 or more. The results suggest substantial differences in program effects, such as reducing the poverty rate by almost one percentage point more, a 75 percent increase over the survey estimate...|$|R
30|$|This section {{presents}} a brief example on developing proposed acceptable and tolerable individual risk thresholds based on statistical <b>data</b> and <b>validation</b> against other criteria in similar contexts.|$|R
5000|$|... #Subtitle level 2: Advanced <b>data</b> <b>validation</b> and {{reconciliation}} ...|$|E
5000|$|... #Subtitle level 2: <b>Data</b> <b>Validation</b> and Certification Requests ...|$|E
50|$|<b>Data</b> <b>validation</b> is {{intended}} to provide certain well-defined guarantees for fitness, accuracy, and consistency for any of various kinds of user input into an application or automated system. <b>Data</b> <b>validation</b> rules can be defined and designed using any of various methodologies, and be deployed in any of various contexts.|$|E
40|$|AbstractScramjet {{propulsion}} provides promising {{opportunities for}} high-speed transportation and cost reduction for satellite and spacecraft launch operations. This paper provides some selected numerical examples among {{the abundance of}} available test cases {{for the purpose of}} demonstrating to what extends the current computational models can do in assisting the design and test programs of scramjet combustors. Numerical examples that cover a wide range of flight speeds are investigated with benchmark <b>data</b> <b>validations.</b> Ideas of mixing enhancement and flame holding mechanisms in scramjet combustion is analyzed and planned for the future experimental investigations...|$|R
40|$|Consider partial {{linear models}} of the form Y=X[tau][beta]+g(T) +e with Y {{measured}} with error and both p-variate explanatory X and T measured exactly. Let be the surrogate variable for Y with measurement error. Let primary data set be that containing independent observations on and the <b>validation</b> <b>data</b> set be that containing independent observations on, where the exact observations on Y may be obtained by some expensive or difficult procedures for only a small subset of subjects enrolled in the study. In this paper, without specifying any structure equations and distribution assumption of Y given, a semiparametric dimension reduction technique is employed to obtain estimators of [beta] and g(Â·) based the least squared method and kernel method with the primary <b>data</b> and <b>validation</b> <b>data.</b> The proposed estimators of [beta] are proved to be asymptotically normal, and the estimator for g(Â·) is proved to be weakly consistent with an optimal convergent rate. Dimension reduction Partial linear model <b>Validation</b> <b>data</b> Asymptotic normality...|$|R
40|$|AbstractThis paper {{develops}} estimation {{approaches for}} nonparametric regression analysis with surrogate <b>data</b> and <b>validation</b> sampling when response variables are measured with errors. Without assuming any error model structure between the true responses and the surrogate variables, a regression calibration kernel regression estimate is defined {{with the help}} of <b>validation</b> <b>data.</b> The proposed estimator is proved to be asymptotically normal and the convergence rate is also derived. A simulation study is conducted to compare the proposed estimators with the standard Nadaraya–Watson estimators with the true observations in the <b>validation</b> <b>data</b> set and the complete observations, respectively. The Nadaraya–Watson estimator with the complete observations can serve as a gold standard, even though it is practically unachievable because of the measurement errors...|$|R
50|$|A DVCS MUST {{support at}} least a subset of these services. A DVCS may support a {{restricted}} vsd service allowing to validate <b>data</b> <b>validation</b> certificates.On completion of each service, the DVCS produces a <b>data</b> <b>validation</b> certificate - a signed document containing the validation results and trustworthy time information.|$|E
5000|$|... 1. {{provide a}} signed {{response}} {{in the form of}} a <b>data</b> <b>validation</b> certificate to the requester, as defined by policy, or an error response. The DVCS service definition and the policy define how much information that has been used by the DVCS to generate the response will be included in a <b>data</b> <b>validation</b> certificate, e.g. public key certificates, CRLs, and responses from other OCSP servers, DVCS, or others. 2. indicate in the <b>data</b> <b>validation</b> certificate whether or not the signed document, the public key certificate(s), or the data were validated, and, if not, the reason why the verification failed.|$|E
5000|$|... #Caption: The {{workflow}} of {{an advanced}} <b>data</b> <b>validation</b> and reconciliation process.|$|E
5000|$|Corresponding {{electronic}} signature <b>validation</b> <b>data</b> and {{electronic signature}} creation data ...|$|R
30|$|Both {{datasets}} (SEVIRI and Radar) {{are divided}} into a training and <b>validation</b> <b>data</b> set. The training data set used {{for the development of}} the technique is collected from November 2006 to March 2007. The <b>validation</b> <b>data</b> set considered for the appraisal of the proposed technique is recorded between November 2009 and March 2010.|$|R
40|$|Our goal is {{to produce}} <b>validation</b> <b>data</b> {{that can be used}} as an {{efficient}} (pre) test set for structural stuck-at faults. In this paper, we detail an original test-oriented mutation sampling technique used for generating such data and we present a first evaluation on these <b>validation</b> <b>data</b> with regard to a structural test. ...|$|R
5000|$|<b>Data</b> <b>validation</b> and {{verification}} {{should be}} completed for each development iteration.|$|E
5000|$|<b>Data</b> <b>validation</b> rules may be defined, {{designed}} and deployed, for example: ...|$|E
50|$|For {{business}} applications, <b>data</b> <b>validation</b> can {{be defined}} through declarative data integrity rules, or procedure-based business rules. Data that does not conform to these rules will negatively affect business process execution. Therefore, <b>data</b> <b>validation</b> should start with business process definition and set of business rules within this process. Rules can be collected through the requirements capture exercise.|$|E
5000|$|... {{internal}} validation or cross-validation (actually, while extracting <b>data,</b> cross <b>validation</b> {{is a measure}} of model robustness, the more a model is robust (higher q2) the less data extraction perturb the original model); ...|$|R
30|$|In {{the first}} instance, not only {{different}} applications {{were used for}} training and <b>validation</b> <b>data,</b> but in addition the developers had no relation in terms of education, nationality, work, and so forth. Moreover, the <b>validation</b> <b>data</b> has not been measured before the model was trained. All this strengthens {{the validity of the}} results. The only potential connection {{is that some of the}} developers who have been involved in the implementation of the training and <b>validation</b> <b>data</b> have also been included within those interviewed. However, this accounts for a minority and we see this as a minimal risk.|$|R
30|$|Low-resistivity pay zones with {{substantial}} reserves in many oilfields {{around the world}} are drawing more attention than ever before. Through analyzing the features of the logging data in low-resistivity pay zones, a fast model for identification of low-resistivity pay zones was developed in this paper. Momentum backpropagation algorithm was used in the model development. Indicators that can amplify the characteristics of low-resistivity pay zones were designed. The proposed model can be used for reevaluating old wells using conventional logging <b>data.</b> <b>Validations</b> through field examples demonstrated the capability of the model in accurate identification of low-resistivity pay zones.|$|R
