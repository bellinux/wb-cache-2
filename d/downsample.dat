77|1037|Public
50|$|<b>Downsample</b> the {{filtered}} signal by M; that is, keep only every Mth sample.|$|E
5000|$|Game rip - ripping {{techniques}} were sometimes required to remove or <b>downsample</b> the game's content {{so that it}} would fit on a CD-R ...|$|E
5000|$|... #Caption: Image {{after running}} k-means with k = 16. Note that a common {{technique}} {{to improve performance}} for large images is to <b>downsample</b> the image, compute the clusters, and then reassign the values to the larger image if necessary.|$|E
30|$|Currently, {{there are}} two {{solutions}} for finding an appropriate <b>downsampling</b> set. In [16], the author formulates a greedy heuristic algorithm to obtain an estimation of the optimal <b>downsampling</b> set. In [11], the author proposes a local-set based <b>downsampling</b> forming algorithm. The graph is divided into disjoint subgraphs and each subgraph selects one vertex as the <b>downsampling</b> vertex, which is called one-hop sampling method. The one-hop sampling method is a rather economical and efficient choice of <b>downsampling</b> set forming method {{when there is no}} restriction on the number of vertices in the <b>downsampling</b> set or no location-limited of <b>downsampling</b> vertices. In Fig. 1, we adopt the one-hop sampling method to form the <b>downsampling</b> set, where the sampled vertices are denoted as the red-star-vertices and the unsampled vertices are denoted as the blue-roundness-vertices.|$|R
40|$|In most cases, {{the direct}} <b>downsampling</b> method outperforms JPEG, without <b>downsampling.</b> This is because {{only a quarter}} of the {{original}} data is needed to compress in the <b>downsampling</b> compression scenario. The IDID-based image compression scheme significantly outperforms the former two. This {{can be attributed to the}} property that IDID is able to preserve more information in the <b>downsampled</b> images. Fig. 7 gives the visual comparisons among different compression methods at 0. 20 bpp for Elaine (512 2 512). It is shown that JPEG results in the worst visual quality due to the existence of severe blocking artifacts. In the reconstructed image by Direct + NLEDI (direct <b>downsampling</b> and NLEDI interpolation), blocking artifacts disappeared; however, there is much more noise. This is because a lot of detail information is lost during direct <b>downsampling,</b> and it cannot be recovered by NLEDI. On the contrary, the reconstructed images by IDID_Bilinear + Bilinear (IDID_Bilinear <b>downsampling</b> and Bilinear interpolation) and IDID_NLEDI + NLEDI (IDID_NLEDI <b>downsampling</b> an...|$|R
40|$|We {{present a}} new <b>downsampling</b> method for {{structured}} volume grids, which preserves {{much more of}} the topology of a scalar field than existing <b>downsampling</b> methods by preferably selecting scalar values of critical points. In particular, many critical points can be preserved which are lost by traditional <b>downsampling</b> methods. Our method is named “topology-guided <b>downsampling</b> ” as topologypreserving <b>downsampling</b> is impossible in general. However, we show that even an approximate preservation of topology is highly desirable if isosurfaces are extracted from the <b>downsampled</b> volume grid, e. g. for interactive previewing, because many topological features of the isosurfaces, e. g. the number of components, tunnels, and holes, are preserved. We illustrate the benefits of our method with examples from medical and technical applications of volume visualization...|$|R
5000|$|Boards {{based on}} the chipset {{do not have an}} AGP {{expansion}} slot, leaving the user to make do with PCI for video card options. 810-based boards include an AMR expansion slot. Additionally, the integrated graphics does not support 32-bit graphics mode, forcing the user to <b>downsample</b> the 810s standard 24-bit mode to 16-bit in order to run most games or full screen DirectX/OpenGL programs; many games will automatically <b>downsample</b> the output to 16-bit upon loading, however others will simply exit with or without an error or even crash due to the 24-bit mode not being supported by the game. The onboard graphics performance in games was also unsatisfactory, and many games of that time had to be run at low resolution and low detail levels to be playable.|$|E
5000|$|It is {{possible}} to control the sample size by multiplying the acceptance probability with a constant [...] For a larger sample size, pick [...] and adjust the acceptance probability to [...] For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly <b>downsample</b> from a larger subsample selected by local case-control sampling.|$|E
50|$|However, if a CD (the CD Red Book {{standard}} is 44.1 kHz 16 bit) {{is to be}} made from a recording, then doing the initial recording using a sampling rate of 44.1 kHz is obviously one approach. Another approach that is usually preferred {{is to use a}} higher sample rate and then <b>downsample</b> to the final format's sample rate. This is usually done as part of the mastering process. One advantage to the latter approach is that way a high resolution recording can be released, as well as a CD and/or lossy compressed file such as mp3—all from the same master recording.|$|E
40|$|Abstract—Traditional {{methods for}} image <b>downsampling</b> commit {{to remove the}} {{aliasing}} artifacts. However, the influences {{on the quality of}} the image interpolated from the <b>downsampled</b> one are usually neglected. To tackle this problem, in this paper, we propose an interpolation-de-pendent image <b>downsampling</b> (IDID), where interpolation is hinged to <b>downsampling.</b> Given an interpolation method, the goal of IDID is to obtain a <b>downsampled</b> image that minimizes the sum of square errors between the input image and the one interpolated from the corresponding <b>downsampled</b> image. Utilizing a least squares algorithm, the solution of IDID is derived as the inverse operator of upsampling. We also devise a content-dependent IDID for the interpolation methods with varying interpolation coefficients. Numerous experimental results demonstrate the viability and efficiency of the proposed IDID. Index Terms—Downsampling, interpolation, least squares, upsampling. I...|$|R
40|$|<b>Downsampling</b> {{technique}} for intraframe coding {{is a new}} scenario for low bit rate coding. In the existing methods of <b>downsampling</b> technique, the alternate entire row or column of pixels are <b>downsampling,</b> Which leads to more loss image quality during interpolation. Checkerboard pattern technique is proposed in <b>downsampling</b> process. Here we remove alternate pixel value same as chess board. Interpolation {{is a process of}} generating a value of a pixel based on its neighbors. We use adaptive weighted interpolation technique. JPEG-LS technique is to provide effective lossless and near lossless compression in upsampling and <b>downsampling</b> process. LOCO-I algorithm is used in JPEG-LS. By using this technique, the image quality is improved...|$|R
40|$|Abstract—In general, subpixel-based <b>downsampling</b> {{can achieve}} higher {{apparent}} {{resolution of the}} down-sampled images on LCD or OLED displays than pixel-based <b>downsampling.</b> Based on the frequency domain analysis of subpixel-based <b>downsampling,</b> we discover special characteristics of the lumachroma color transform choice for monochrome images. Based on these, we model the anti-aliasing filter design for subpixel-based monochrome image <b>downsampling</b> as a Human Visual System (HVS) -basd optimization problem with a 2 -term cost function and obtain a closed-form solution. One cost term measures the luminance distortion and the second term measures the chrominance aliasing in our chosen luma-chroma space. Simulation {{results suggest that the}} proposed method can achieve sharper <b>downsampled</b> gray/font images compared with conventional pixel and subpixel-based methods, without noticeable color fringing artifacts...|$|R
50|$|Analog-to-digital {{converters}} {{are integral}} to 2000s era music reproduction technology and digital audio workstation-based sound recording. People often produce music on computers using an analog recording and therefore need analog-to-digital converters {{to create the}} pulse-code modulation (PCM) data streams that go onto compact discs and digital music files. The current crop of analog-to-digital converters utilized in music can sample at rates up to 192 kilohertz. Considerable literature exists on these matters, but commercial considerations often play a significant role. Many recording studios record in 24-bit/96 kHz (or higher) pulse-code modulation (PCM) or Direct Stream Digital (DSD) formats, and then <b>downsample</b> or decimate the signal for Red-Book CD production (44.1 kHz) or to 48 kHz for commonly used radio and television broadcast applications.|$|E
5000|$|The Dreamcast {{prompted}} a small revival of game ripping. The proprietary GD-ROM format {{used in the}} system cannot be read using standard PC hardware (CD/DVD drives). Believing this {{to be an effective}} lockout, Sega neglected to disable the drive's ability to read CD-Rs, a feature which is frequently used by developers in the process of testing and debugging new games. With the aid of serial port cable or [...] "coder's cable" [...] and later the [...] "Broadband adapter" [...] accessory, hackers were able to read the contents of GD-ROMs onto a hard disk, and then remaster the files into a CD-R image which could be burned and played with no modification to the Dreamcast itself. However, since the GD-ROM had higher capacity than a CD-R, it was not always possible to achieve a perfect copy in this manner. Thus, several scene groups applied ripping techniques to remove or <b>downsample</b> the game's content so that it would fit on a CD-R.|$|E
50|$|One {{may wish}} to <b>downsample</b> or {{otherwise}} decrease {{the resolution of the}} represented source signal and the quantity of data used for its compressed representation without re-encoding, as in bitrate peeling, but this functionality is not supported in all designs, as not all codecs encode data in a form that allows less important detail to simply be dropped. Some well-known designs that have this capability include JPEG 2000 for still images and H.264/MPEG-4 AVC based Scalable Video Coding for video. Such schemes have also been standardized for older designs as well, such as JPEG images with progressive encoding, and MPEG-2 and MPEG-4 Part 2 video, although those prior schemes had limited success in terms of adoption into real-world common usage. Without this capacity, which is often the case in practice, to produce a representation with lower resolution or lower fidelity than a given one, one needs to start with the original source signal and encode, or start with a compressed representation and then decompress and re-encode it (transcoding), though the latter tends to cause digital generation loss.|$|E
40|$|<b>Downsampling</b> is {{the process}} of {{reducing}} the sampling rate of a discrete signal. This paper describes how sampled data system theory can be used to design an L 2 or L∞ optimal <b>downsampler</b> which reduces the sampling rate by a positive integer factor from a given fast sampler. This paper also describes the effect of noise on the optimal <b>downsampling...</b>|$|R
3000|$|... [...]). We limit our {{experiments}} to <b>downsampling</b> factors, α,β={ 2, 3, 4 }, which denotes spatial or temporal <b>downsampled</b> {{versions of a}} half, a third, and a fourth of the original resolution or frame rate.|$|R
30|$|Step 1 (Temporal <b>downsampling).</b>|$|R
40|$|In this paper, {{based on}} a careful {{investigation}} on the calibration (<b>downsample)</b> technique, we improve two detectors for detecting LSB matching: Calibrated HCF COM and Calibrated Adjacency HCF COM. Instead of using the COM (center of mass) of the HCF (histogram characteristic function), we consider {{the ratio of the}} histogram’s DFT coefficients of the image to the corresponding coefficients of the downsampled image. Moreover, we propose to <b>downsample</b> only for non-oscillating pixels. With a same level of computational complexity, the new detectors thus obtained are better than the old ones, especially for uncompressed images. Index Terms — information hiding, steganalysis, LSB matching, calibration 1...|$|E
40|$|In this paper, {{we provide}} a Graph Fourier Transform based {{approach}} to <b>downsample</b> signals on graphs. For bandlimited signals on a graph, a test is provided to identify whether signal reconstruction is {{possible from the}} given downsampled signal. Moreover, if the signal is not bandlimited, we provide a quality measure for comparing different downsampling schemes. Using this quality measure, we propose a greedy downsampling algorithm. Most of the prevailing approaches consider undirected graphs, and exploit the topological properties of the graph in order to <b>downsample</b> the grid, while the proposed method exploits spectral properties of graph signals, and is applicable to directed graphs, undirected graphs, and graphs with negative edge-weights. We provide several experiments demonstrating our downsampling scheme, and compare our quality measure with measures like normalized cuts. Comment: Written in journal paper format, inlcudes 8 page...|$|E
30|$|The {{recordings}} were 10 s long, {{with mixed}} English and Japanese utterances of both genders. The original recordings were sampled at 16 kHz; however, it was empirically {{determined that a}} <b>downsample</b> to 8 kHz resulted in better separation for all methods tested. This {{can be attributed to}} the reduced effects of spatial aliasing at the lower sampling frequency.|$|E
40|$|<b>Downsampling</b> {{an image}} {{results in the}} loss of image {{information}} that cannot be recovered with upsampling. We demonstrate that the particular combination of <b>downsampling</b> and upsampling methods used can significantly impact the reconstructed image quality, and then we propose a technique to identify patterns associated with different <b>downsampling</b> methods in order to select the appropriate upsampling mechanism. The technique has low complexity and achieves high accuracy over a wide range of images. r 2003 Published by Elsevier B. V...|$|R
40|$|The {{original}} ImageNet dataset is {{a popular}} large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e. g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a <b>downsampled</b> version of ImageNet. In contrast to the CIFAR datasets and earlier <b>downsampled</b> versions of ImageNet, our proposed ImageNet 32 × 32 (and its variants ImageNet 64 × 64 and ImageNet 16 × 16) contains exactly {{the same number of}} classes and images as ImageNet, with the only difference that the images are <b>downsampled</b> to 32 × 32 pixels per image (64 × 64 and 16 × 16 pixels for the variants, respectively). Experiments on these <b>downsampled</b> variants are dramatically faster than on the original ImageNet and the characteristics of the <b>downsampled</b> datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at [URL] and [URL]...|$|R
40|$|Closed access. The {{effect of}} using <b>downsampling</b> for {{arbitrary}} views inside a multiview sequence on the multi-view coding (MVC) efficiency is explored. A bit rate adaptive approach is proposed to consider <b>downsampling</b> certain views prior to encoding with relevant downscaling ratios. The inter-view references, if any, are <b>downsampled</b> {{to the same}} resolution and the decoded view is upsampled {{back to the original}} resolution. The results over several multi-view test sequences imply that up to 0. 9 dB gain or 20...|$|R
30|$|The work {{reported}} in [10] exploits this by separating the acoustic signal into components based on different spectro-temporal scales, but again, these scales are hand-crafted, and further processing {{is used to}} <b>downsample</b> the feature resolution using MFCCs as features. We can now use multiple spectrograms with multiple resolutions directly as features thanks {{to the ability of}} deep architectures to model high-dimensional inputs.|$|E
30|$|Another {{possible}} way of improving hierarchical systems is to <b>downsample</b> {{the output of}} the lower network [46]. This expands the time span of the model without increasing the number of input features to the upper net. In our earlier study, we experimentally found the optimal downsampling rate to be 5 [27], and other authors also prefer this value [21, 26].|$|E
30|$|After {{capturing}} the {{signal from the}} source and storing it in a buffer, the relays <b>downsample</b> the sequence to get symbol-spaced samples. Then, the channel is estimated and the symbol sequence is detected. The {{next step is to}} create the Alamouti code sequence. Each relay plays the role of one antenna in the conventional Alamouti diversity, [20], so each relay creates a different sequence.|$|E
40|$|Abstract—In this paper, we are {{concerned}} with image <b>downsampling</b> using subpixel techniques to achieve superior sharpness for small liquid crystal displays (LCDs). Such a problem exists when a high-resolution image or video is to be displayed on low-resolution display terminals. Limited by the low-resolution display, we have to shrink the image. Signal-processing theory tells us that optimal decimation requires low-pass filtering with a suitable cutoff frequency, followed by <b>downsampling.</b> In doing so, we need to remove many useful image details causing blurring. Subpixel-based <b>downsampling,</b> {{taking advantage of the}} fact that each pixel on a color LCD is actually composed of individual red, green, and blue subpixel stripes, can provide apparent higher resolution. In this paper, we use frequency-domain analysis to explain what happens in subpixel-based <b>downsampling</b> and why it is possible to achieve a higher apparent resolution. According to our frequency-domain analysis and observation, the cutoff frequency of the low-pass filter for subpixel-based decimation can be effectively extended beyond the Nyquist frequency using a novel antialiasing filter. Applying the proposed filters to two existing subpixel <b>downsampling</b> schemes called direct subpixel-based <b>downsampling</b> (DSD) and diagonal DSD (DDSD), we obtain two improved schemes, i. e., DSD based on frequency-domain analysis (DSD-FA) and DDSD based on frequency-domain analysis (DDSD-FA). Experimental results verify that the proposed DSD-FA and DDSD-FA can provide superior results, compared with existing subpixel or pixel-based <b>downsampling</b> methods. Index Terms—Downsampling, frequency analysis, subpixel rendering. I...|$|R
50|$|Unlike the DWT, {{it has a}} {{specific}} scale - it starts from an 8×8 block, and it <b>downsamples</b> the image, rather than decimating (low-pass filtering, then <b>downsampling).</b> It thus offers worse frequency behavior, showing artifacts (pixelation) at the early stages, in return for simpler implementation.|$|R
40|$|Abstract—Downsampling {{of signals}} {{living on a}} general {{weighted}} graph is not as trivial as of regular signals where we can simply keep every other samples. In this {{paper we propose a}} simple, yet effective <b>downsampling</b> scheme in which the underlying graph is approximated by a maximum spanning tree (MST) that naturally defines a graph multiresolution. This MST-based method signifi-cantly outperforms the two previous <b>downsampling</b> schemes, col-oring-based and SVD-based, on both random and specific graphs in terms of computations and partition efficiency quantified by the graph cuts. The benefit of using MST-based <b>downsampling</b> for recently developed critical-sampling graph wavelet transforms in compression of graph signals is demonstrated. Index Terms—Bipartite approximation, <b>downsampling</b> on graphs, graph multiresolution, graph wavelet filter banks, max-cut, maximum spanning tree, signal processing on graphs. I...|$|R
40|$|The precise {{localization}} of short/hard (Type I) gamma-ray bursts (GRBs) {{in recent}} years has answered many questions but raised even more. I present some results of a systematic study of the optical afterglows of long/soft (Type II) and short/hard (Type I) GRBs, focusing on the optical luminosity as another puzzle piece in the classification of GRBs. Comment: 7 Pages, 2 figures, to be published in the " 2008 Nanjing GRB Conference" conference proceedings, figures have been <b>downsample...</b>|$|E
30|$|A {{channelizer}} is used {{to separate}} users or channels in communication systems. A polyphase channelizer {{is a type of}} channelizer that uses polyphase filtering to filter, <b>downsample,</b> and downconvert simultaneously. With graphics processing unit (GPU) technology, we propose a novel GPU-based polyphase channelizer architecture that delivers high throughput. This architecture has advantages of providing reduced complexity and optimized parallel processing of many channels, while being configurable via software. This makes our approach and implementation particularly attractive for using GPUs as DSP accelerators for communication systems.|$|E
30|$|Compared {{with the}} DWT, LSWT do not <b>downsample</b> and upsample the highpass and the lowpass {{coefficients}} during the decomposition and {{reconstruction of the}} image. So, the LSWT not only retains the perfect properties of the LWT, but also possess the shift-invariance. When LSWT is introduced into image fusion, more information for fusion can be obtained. In addition, the size of different sub-images is identical, so {{it is easy to}} find the relationship among different subbands, which is beneficial for designing fusion rules [25]. Therefore, the LSWT is more suitable for image fusion.|$|E
40|$|<b>Downsampling</b> is {{a routine}} step before {{applying}} interferometric {{synthetic aperture radar}} (InSAR) data to earthquake inversion {{because of the high}} computational burden. In this letter, we make use of the matrix perturbation theory to describe the <b>downsampling</b> process, which is considered as matrix perturbation on inversion equation. First, we derive a formula to quantitatively assess the perturbation on the inversion solution caused by data <b>downsampling.</b> Next, we propose an equation-based InSAR data <b>downsampling</b> algorithm to better reduce the perturbation. The experiment with simulated data demonstrates that our new algorithm preserves the most details from full data inversion comparing with previous algorithms. Finally, we use our method to study the slip distribution of the 2008 Mw 6. 3 Dangxiong earthquake. Department of Land Surveying and Geo-Informatic...|$|R
40|$|Subpixel-based <b>downsampling</b> {{improves}} resolution at {{the cost}} of color fringing artifacts. In this paper, we propose a novel multiple descriptions approach for subpixel-based <b>downsampling.</b> Each description is treated as a noisy observation of the original image and an optimal linear estimator based on the LMMSE criteria is applied in the DCT domain to combine multiple descriptions. Simulation result shows that the proposed method gives sharper resultant images while being free of color fringing artifacts with compared to the resultant images of the conventional <b>downsampling</b> methods...|$|R
3000|$|... where f_k^G {{denotes the}} kth {{iterative}} reconstructed signal of IGDR, [...] (f_d - P_Tf_k^G) denote the reconstructed residual on the <b>downsampling</b> set, and [...] (I - P_T)P_d (f_d - P_Tf_k^G) denote the reconstructed residual is diffused from the <b>downsampling</b> {{set to the}} non-downsampling set by the diffused operator.|$|R
