760|165|Public
5000|$|... #Caption: Snap-on Tobii X1 Light <b>Eye</b> <b>Tracker</b> from 2012 for {{portable}} labs ...|$|E
5000|$|... #Caption: RED250mobile <b>eye</b> <b>tracker,</b> {{released}} in 2014; screen showing a scan path on a text. Photo by SMI.|$|E
50|$|Topography-guided {{crosslinking}} {{relies on}} an active <b>eye</b> <b>tracker</b> {{to allow a}} patterned delivery of UV light. Both the power and pattern can be programmed into the unit based on the topography of the individual's eyes.|$|E
50|$|The {{technology}} {{is based on}} the dark pupil and corneal reflection tracking: The cameras in the SMI <b>eye</b> <b>trackers</b> detect face, <b>eyes,</b> pupils, as well as the corneal reflections from the infrared light sources, and calculate eye movements, gaze direction and points of regard. The sampling frequency of the <b>eye</b> <b>trackers</b> ranges from 30 Hz up to the kHz range.|$|R
40|$|The {{usage of}} <b>eye</b> <b>trackers</b> is {{becoming}} more and more popular in the field of information visualization. In this project two <b>eye</b> <b>trackers,</b> The <b>Eye</b> Tribe nd Mirametrix S 2, are used to obtain eye tracking data for visualizations. It is planned to use the <b>eye</b> <b>trackers</b> with OnGraX, a network visualization system, where they will provide data for the implementation of visualizations, specifically, heatmaps. OnGraX already uses heatmaps to show regions in a network that have been in the viewport of the user. One aim of this thesis will be the comparison between the two <b>eye</b> <b>trackers,</b> and if the use of eye tracking data gives better results thatn the already existing viewport-based approach. At the same time, we provide the foundation for adaptive visualizations with OnGraX. Our research problem is also of interest for visualization in general, because it will help to improve and develop eye tracking technology in this context. To support the outcome of our implementation, we carried out a user study. As a result, we concluded that one of the two <b>eye</b> <b>trackers</b> appears to have more capabilities than the other, and that using the eye tracking data is a more preferred way of depicting the heatmaps on OnGraX. ...|$|R
50|$|The {{instrumented}} 4WD {{is equipped}} with sensors such as a multimedia datalogger, physiological devices (EEG, ECG and EMG), laser scanner, radars and <b>eye</b> <b>trackers.</b>|$|R
50|$|Foveated {{rendering}} is {{an upcoming}} Video game technique which uses an <b>Eye</b> <b>tracker</b> integrated with a Virtual reality headset {{to reduce the}} rendering workload by greatly reducing the image quality in the Peripheral vision (outside of the zone gazed by the Fovea).|$|E
5000|$|... The Eye Tribe {{was getting}} ready to send out the first {{shipments}} of their eye tracking technology. The Eye Tribe has broken the record for smallest <b>eye</b> <b>tracker</b> device in the world, measuring in at 20 × 1.9 × 1.9 cm. Also, the <b>eye</b> <b>tracker</b> does not require a separate power source, making it even more portable. The device uses a USB 3.0 connection, which allows it to run with most computers and tablets. The Eye Tribe is compatible with Microsoft Windows 7 or newer and OS X, but the company {{is in the process of}} working on support for other major platforms, such as Android.They are selling the device to developers with a simple software development kit using C++, C# and Java programming platforms.|$|E
50|$|In 2014, Tobii {{partnered with}} Danish SteelSeries and {{launched}} their first eye tracking system for consumers: the Tobii EyeX and the SteelSeries Sentry <b>Eye</b> <b>Tracker.</b> Several video games from major publishers were released in 2015-16 with support for Tobii's consumer devices, with {{varying levels of}} success.|$|E
25|$|<b>Eye</b> <b>trackers</b> {{can also}} be used to detect eye {{movements}} so that the system can determine precisely where a user is looking at any given instant.|$|R
50|$|The {{company was}} founded in 2010 by Dr. Brian Still at Texas Tech University. EyeGuide was {{developed}} as an affordable alternative to other <b>eye</b> <b>trackers</b> for the Texas Tech University’s usability lab.|$|R
50|$|The {{first and}} fourth Purkinje images {{are used by}} some <b>eye</b> <b>trackers,</b> devices to measure the {{position}} of an eye. The cornea reflection (P1 image) used in this measurement is generally known as glint.|$|R
50|$|In {{parallel}} to the space-qualified version of the <b>Eye</b> <b>Tracker</b> a commercially available model has been manufactured by the company Chronos Vision in Berlin and is installed in many laboratories in Europe, North America and Asia, where it represents an essential tool for the examination of numerous neurophysiological phenomena.|$|E
50|$|The {{ability to}} sight-read partly {{depends on a}} strong {{short-term}} musical memory. An experiment on sight reading using an <b>eye</b> <b>tracker</b> indicates that highly skilled musicians tend to look ahead further in the music, storing and processing the notes until they are played; this {{is referred to as}} the eye-hand span.|$|E
5000|$|In 2014 he had {{his first}} one-man {{exhibition}} at London's Riflemaker gallery called NOMADS. In 2015 {{he had his}} second exhibition in London entitled Drawing with my eyes. For this he drew directly onto a screen using only his eyes via a Tobii <b>eye</b> <b>tracker</b> and software he developed himself with Tobii. http://www.huffingtonpost.com/2015/03/24/graham-fink-eyes_n_6932762.html?1427226337 ...|$|E
40|$|Augmentative and {{alternative}} communication tools allow {{people with severe}} motor disabilities to interact with com-puters. Two commonly used tools are video-based interfaces and <b>eye</b> <b>trackers.</b> Video-based interfaces map head move-ments captured by a camera to mouse pointer movements. Alternatively, <b>eye</b> <b>trackers</b> place the mouse pointer at the estimated position of the user’s gaze. Eye tracking based interfaces {{have been shown to}} even outperform traditional mice in terms of speed, however the accuracy of current <b>eye</b> <b>trackers</b> is not enough for fine mouse pointer placement. In this paper we propose the Head Movement And Gaze In-put Cascaded (HMAGIC) pointing technique that combines head movement and gaze-based inputs in a fast and accurate mouse-replacement interface. The interface initially places the pointer at the estimated gaze position and then the user makes fine adjustments with their head movements. We conducted a user experiment to compare HMAGIC with a mouse-replacement interface that uses only head movements to control the pointer. Experimental results indicate that HMAGIC is significantly faster than the head-only interface while still providing accurate mouse pointer positioning...|$|R
40|$|Gaze {{analysis}} {{gives us}} {{detailed information on}} the visual attention of a person. Further insights {{into the world of}} thought can be revealed and the information can contribute to interest or intention deduction. That is why eye tracking is extremely interesting for a variety of applications and different research fields. Especially due to the usage of mobile unobtrusive <b>eye</b> <b>trackers,</b> experimental setups can stay closer to reality and eye tracking becomes possible in settings where one didn't imagine it to be realisable years ago. But still, commercial off-the-shelf <b>eye</b> <b>trackers</b> do neither enable for 3 D gaze point computation nor have solutions for fully automated gaze analysis in environments with real 3 D objects. In this article we show which information mobile <b>eye</b> <b>trackers</b> need to deliver to make 3 D gaze point computation on real 3 D objects in known environments possible. We also demonstrate how this has been achieved for the Dikablis Wireless eye tracking system. We further show real time processing of 3 D gaze points for fixation determination and an accuracy test to evaluate the developed algorithms...|$|R
50|$|Capabilities {{in the eye}} {{movement}} labs is expanded to include 3 <b>eye</b> <b>trackers,</b> including one {{with the ability to}} capture synchronous gaze and EEG data, and another able to capture synchronous gaze and speech signals.|$|R
5000|$|When {{central vision}} is compromised, {{as in the}} case of macular scotoma, {{patients}} develop an eccentric or extra-foveal vision, normally with unstable fixation. The retinal area used by eccentric viewers to substitute the foveal vision is known as the Preferred Retinal Locus (PRL) In Microperimetry systems, the fundus (eye) is imaged in real time, while an <b>eye</b> <b>tracker</b> compensates eye movements during stimuli projection, allowing correct matching between expected and projected stimulus position onto the retina. Simultaneously, the <b>eye</b> <b>tracker</b> plots the retinal movement during fixation attempt defining the PRL zone as well as fixation stability. Some microperimetry instruments calculate 2 different PRL zones during the examination. To create the fundus image an infrared telecamera is used, {{as in the case}} of the [...] "Nidek-MP1", or a Scanning Laser Ophthalmoscope (SLO), {{as in the case of}} the [...] "Centervue-MAIA".|$|E
50|$|Edmund Huey {{built an}} early <b>eye</b> <b>tracker,</b> using {{a sort of}} contact lens with a hole for the pupil. The lens was {{connected}} to an aluminum pointer that moved {{in response to the}} movement of the eye. Huey studied and quantified regressions (only a small proportion of saccades are regressions), and he showed that some words in a sentence are not fixated.|$|E
5000|$|... 2001 - Smart Eye Pro, {{a system}} with {{flexible}} camera configuration released2005 - AntiSleep, a mono camera system built on cost efficient hardware for automotive use released 2009 - Embedded AntiSleep, complete integrated mono camera automotive grade system running on BlackFin DSP2012 - DR 120, dual camera <b>eye</b> <b>tracker</b> integrated in a computer monitor 2013 - Fully automatic initialization for Smart Eye Pro ...|$|E
5000|$|Eye {{tracking}} - <b>Eye</b> <b>trackers</b> {{measure the}} point of gaze relative to {{the direction of the}} head, allowing a computer to sense where the user is looking. These systems are not currently used in aircraft.|$|R
50|$|The first non-intrusive <b>eye</b> <b>trackers</b> {{were built}} by Guy Thomas Buswell in Chicago, using beams {{of light that}} were {{reflected}} on the eye and then recording them on film. Buswell made systematic studies into reading and picture viewing.|$|R
50|$|Not {{being able}} to provide input using a {{conventional}} input device due to a motor impairment; for example, users who rely upon using switch controller or <b>eye</b> <b>trackers</b> to interact with games may {{find it very difficult}} or impossible to play games that require large amounts of input.|$|R
50|$|The second broad {{category}} uses some non-contact, optical {{method for}} measuring eye motion. Light, typically infrared, is reflected from {{the eye and}} sensed by a video camera or some other specially designed optical sensor. The information is then analyzed to extract eye rotation from changes in reflections. Video-based eye trackers typically use the corneal reflection (the first Purkinje image) {{and the center of}} the pupil as features to track over time. A more sensitive type of <b>eye</b> <b>tracker,</b> the dual-Purkinje <b>eye</b> <b>tracker,</b> uses reflections {{from the front of the}} cornea (first Purkinje image) and the back of the lens (fourth Purkinje image) as features to track. A still more sensitive method of tracking is to image features from inside the eye, such as the retinal blood vessels, and follow these features as the eye rotates. Optical methods, particularly those based on video recording, are widely used for gaze tracking and are favored for being non-invasive and inexpensive.|$|E
50|$|However, {{there are}} also {{problems}} one {{has to deal with}} when working with the eye tracking method. Most of times an <b>eye</b> <b>tracker</b> is not part of a standard lab and it is still expensive. Another, more functional problem is the interpretation of eye fixations: The data reveals where participants are looking, but not what they actually are thinking. Yet, this is exactly what we want to find out more about.|$|E
50|$|Laser Blended Vision is a shape profile {{modification}} {{to the standard}} excimer laser eye surgery method that has been performed {{tens of millions of}} times worldwide, mostly as LASIK. LASIK provides a very high level of safety particularly when employing femtosecond laser flap creation (IntraLASIK) and <b>eye</b> <b>tracker</b> technology that tracks eye position at a feedback frequency significantly higher than the repetition rate of the excimer laser pulses themselves.|$|E
40|$|Abstract. With recent {{advances}} in eye tracking technology, eye gaze gradually gains acceptance as a pointing modality. Its relatively low accuracy, however, determines the need to use enlarged controls in eye-based interfaces. This renders the overall design quite distant from “natural”. Another factor impairing pointing performance is deficient robustness of an <b>eye</b> <b>tracker’s</b> calibration. To facilitate pointing at standard-size menus, we developed a technique that uses dynamic target expansion for on-line correction of the <b>eye</b> <b>tracker’s</b> calibration. Correction {{is based on the}} relative change in the gaze point location upon the expansion. A user study suggests that the technique affords selection accuracy of 91 %. User performance is thus shown to approach the limit of practical pointing. Effectively, developing a user interface that supports navigation through standard menus by eye gaze alone is feasible...|$|R
30|$|As <b>eye</b> <b>trackers</b> become {{increasingly}} available to consumers, lower cost, portable, {{and easier to}} use, research on principled methods for using eye tracking for competency assessment {{is expected to increase}} (Al-Moteri et al., 2017). It is worth noting that <b>eye</b> <b>trackers</b> with high temporal and spatial resolution and coverage range (e.g., across large or multiple displays) can still be quite cost prohibitive. As <b>eye</b> <b>trackers</b> develop more widespread use, however, one can readily envision both automated and instructor-guided feedback techniques to help quantify competency and provide grounded examples for individualized feedback. In mammography, recent research demonstrates that tracking eye movements and using machine-learning techniques can predict most diagnostic errors prior to their occurrence, making it possible to automatically provide cueing or feedback to trainees during image inspection (Voisin et al., 2013). In diagnostic pathology, automated feedback may be possible by parsing medical images into diagnostically relevant versus irrelevant regions of interest (ROIs) using expert annotations and/or automated machine-vision techniques (Brunyé et al., 2014; Mercan et al., 2016; Nagarkar et al., 2016). Once these ROIs are established and known to the eye-tracking system, fixations can be parsed as falling within or outside of ROIs. This method could be used to understand the spatial allocation of attention over a digital image (e.g., a radiograph, histology slide, angiography), and the time-course of that allocation.|$|R
30|$|Gaze {{behaviors}} can {{be measured}} {{from the outside world}} by using <b>eye</b> <b>trackers</b> and other gaze measurement devices (Fig.  1 (iii)). By specifying gaze times, gaze behaviors such as “gazing at an object,” and “change in target object of gaze” can be tracked from a series of information about saccades and fixations.|$|R
50|$|A {{great deal}} of {{research}} has gone into studies of the mechanisms and dynamics of eye rotation, but the goal of eye tracking is most often to estimate gaze direction. Users {{may be interested in}} what features of an image draw the eye, for example. It is important to realize that the <b>eye</b> <b>tracker</b> does not provide absolute gaze direction, but rather can only measure changes in gaze direction. In order to know precisely what a subject is looking at, some calibration procedure is required in which the subject looks at a point or series of points, while the <b>eye</b> <b>tracker</b> records the value that corresponds to each gaze position. (Even those techniques that track features of the retina cannot provide exact gaze direction because there is no specific anatomical feature that marks the exact point where the visual axis meets the retina, if indeed there is such a single, stable point) An accurate and reliable calibration is essential for obtaining valid and repeatable eye movement data, and this can be a significant challenge for non-verbal subjects or those who have unstable gaze.|$|E
5000|$|The {{location}} of a fixation point may be specified in many ways.For example, when viewing an image on a computer monitor, one may specify a fixation using a pointing device, like a computer mouse. Eye trackers which precisely measure the eye's position and movement are also commonly used to determine fixation points in perception experiments.When the display is manipulated {{with the use of}} an <b>eye</b> <b>tracker,</b> this is known as a gaze contingent display.Fixations may also be determined automatically using [...] computer algorithms.|$|E
50|$|In 2004, Vertegaal co-founded Xuuk, Inc. {{with his}} students. Xuuk {{unveiled}} the world's first $999 long-range calibration-free <b>eye</b> <b>tracker</b> at Google's headquarters in 2007 {{and was the}} first to introduce computer vision metrics for Digital Signage analytics. Based on an infrared megapixel imaging camera, the eyebox2 is capable of tracking eyes and face orientation of multiple users from a distance of up to 10 meters without calibration. Other Human Media Lab startups include Kameraflage, Synbiota, and Mark One, Inc., known for the Vessyl smart beverage container.|$|E
40|$|With recent {{advances}} in eye tracking technology, eye gaze gradually gains acceptance as a pointing modality. Its relatively low accuracy, however, determines the need to use enlarged controls in eye-based interfaces rendering their design rather peculiar. Another factor impairing pointing performance is deficient robustness of an <b>eye</b> <b>tracker’s</b> calibration. To facilitate pointing at standard-size menus, we developed a technique that uses dynamic target expansion for on-line correction of the <b>eye</b> <b>tracker’s</b> calibration. Correction {{is based on the}} relative change in the gaze point location upon the expansion. A user study suggests that the technique affords a dramatic six-fold improvement in selection accuracy. This is traded off against a much smaller reduction in performance speed (39 %). The technique is thus believed to contribute to development of universal-access solutions supporting navigation through standard menus by eye gaze alone...|$|R
40|$|An R {{package for}} {{detecting}} fixations in raw eye tracking data. Includes interactive visualizations {{that can be}} used to inspect the results. The algorithm used for the detection is based on velocity and works best with data from <b>eye</b> <b>trackers</b> with sampling rates above 120 Hz. Please check the Github repository for the latest version...|$|R
50|$|<b>Eye</b> <b>trackers</b> measure {{rotations}} {{of the eye}} in one {{of several}} ways, but principally they fall into three categories: (i) measurement of the movement of an object (normally, a special contact lens) attached to the eye, (ii) optical tracking without direct contact to the eye, and (iii) measurement of electric potentials using electrodes placed around the eyes.|$|R
