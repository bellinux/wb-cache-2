0|7648|Public
40|$|In this paper, we {{describe}} {{an approach that}} aims to model heterogeneous resources for <b>information</b> extraction. <b>Document</b> is <b>modeled</b> in graph representation that enables better understanding of multi-media document and its structure which ultimately could result better cross-media information extraction. We also describe our proposed algorithm that segment document-based on the <b>document</b> <b>modeling</b> approach {{we describe}}d in this paper. 1...|$|R
5000|$|... (3.) Where {{a person}} commits any act {{declared}} by this section to be a misdemeanour, he shall, if {{he intended to}} communicate to a foreign State any <b>information,</b> <b>document,</b> sketch, plan, <b>model,</b> or knowledge, obtained or taken by him, or entrusted to him as aforesaid, or if he communicates the same to any agent of a foreign State, be guilty of felony, and on conviction be liable {{at the discretion of}} the court to penal servitude for life, or for any term not less than five years, or to imprisonment for any term not exceeding two years with or without hard labour.|$|R
40|$|Abstract. In {{this paper}} we propose the model of a prototypical NLP {{architecture}} of an information access system to support a team of experts in a scientific design task, in a shared and heterogeneous framework. Specifically, we believe AI/NLP {{can be helpful in}} several tasks, such as the extraction of implicit information needs enclosed in meeting minutes or other documents, analysis of explicit information needs expressed through Natural Language, processing and indexing of document collections, extraction of required <b>information</b> from <b>documents,</b> <b>modeling</b> of a common knowledge base, and, finally, identification of important concepts through the automatic extraction of terms. In particular, we envisioned this architecture in the specific and practical scenario of the Concurrent Design Facility (CDF) of the European Space Agency (ESA), in the framework of the SHUMI project (Support To HUman Machine Interaction) developed in collaboration with the ESA/ESTEC- ACT (Advanced Concept Team). ...|$|R
40|$|Abstract—In this {{position}} paper, we argue {{in favour of}} three points related to document-centric modelling of information systems: (i) information systems of some organizations may {{be understood in terms}} of documents, actions, actors, and <b>document</b> flow; (ii) <b>modelling</b> <b>document</b> flow may be a central step in modelling an information system; and (iii) as an approach to <b>information</b> system <b>modelling,</b> <b>document</b> flow <b>modelling</b> may be coupled with the form-based approach to information system development that is featured in IIS*Case, a model-driven devel-opment tool, because the form concept is semantically close to the document concept. With respect to these claims, we formulate a document-centric and model-driven approach to information system development, explain its particularities, and present a plan of activities that should lead to its implementation. By relying on domain-specific languages, the proposed approach should allow generation of <b>information</b> systems supporting <b>document</b> manipulation within a document flow and process mining. I...|$|R
40|$|AbstractA new {{framework}} for document verification is presented which covers {{the entire process}} from <b>document</b> analysis through <b>information</b> extraction, <b>document</b> <b>modeling,</b> representation of background knowledge about the domain of discourse, user level and formal representation of consistency criteria, verification by model checking, counterexample generation, and error reporting. Emphasis is placed on employing background knowledge to reduce the complexity and to increase the quality of results in each step. A rule-based approach to information extraction supports the concise definition of extraction rules for document formats based on XML or HTML. The expressiveness of the existing extraction methods is exceeded by supporting rule specialization, integration of external tools, and access to background knowledge represented in ontologies. As a formal basis for representing consistency criteria, the new temporal description logic ALCCTL is proposed. In contrast to the existing formalisms, criteria related to the coherence of content along individual paths of reading can be represented and verified efficiently. The adequacy, performance, and effectiveness of the proposed framework is demonstrated on {{a case study in}} technical documentation...|$|R
40|$|Term {{signal is}} an {{existing}} text representation that depicts a term as a vector of frequencies of occur-rences {{in a number}} of user-defined partitions of a document. Although term signal augments the tra-ditional vector space model with patterns of term occurrences, its document division is not coherent with the actual logical structure of a document. In this paper, we propose a novel <b>document</b> <b>model,</b> termed Structure-Based <b>Document</b> <b>Model</b> with Dis-crete Wavelet Transforms (SDMDWT), that exploits the structural <b>information</b> of <b>documents</b> and mathe-matical transforms for document representation. The proposed SDMDWT model enhances the existing term signal concept by additionally taking into con-sideration <b>document’s</b> structural <b>information</b> during <b>document</b> division. We evaluated the proposed model on two different domains of standard data sets, We-bKB 4 -Universities and TREC Genomics 2005, us-ing Support Vector Machines binary classification. The experimental results show that using our SD-MDWT <b>model</b> for <b>document</b> representation demon-strates promising improvements of classification per-formances over existing <b>document</b> <b>models.</b> ...|$|R
40|$|We {{present a}} database-centric {{approach}} to web site development {{in which both}} application and web content data are managed by a database. The development process is based on three main stages of <b>information</b> <b>modeling,</b> <b>document</b> content design and presentation specification. A Java framework based on the OMS object-oriented data management system has been developed to support the development life cycle from rapid prototyping through to operation. We describe how the framework supports access from heterogeneous clients and how it has been extended to include a web content manager. We describe how the OMS Java framework provides three level of abstractions – storage, information and access in order to realize these objectives. We then present each of these layers in turn – starting with the information abstractions which lie {{at the core of}} the system and then going on to the storage and access layers...|$|R
40|$|The primary {{function}} of current Web search engines is essentially relevance ranking at the document level. However, myriad structured information about real-world objects embedded in static Web pages and online Web databases. In this paper, we propose {{a paradigm shift}} to enable searching at the object level. In traditional <b>information</b> retrieval <b>models,</b> <b>documents</b> are taken as the retrieval units {{and the content of}} a document is considered reliable. However, this reliability assumption is no longer valid in the object retrieval context when multiple copies of information about the same object typically exist. These copies may be inconsistent because of diversity of Web site qualities and the limited performance of current information extraction techniques. In this paper, we propose several language models for Web object retrieval. We test these models on our academic search engine called Libra and compare their performances. 1...|$|R
40|$|Language model {{information}} retrieval depends on accurate estimation of <b>document</b> <b>models.</b> In this paper, we propose a document expansion technique {{to deal with}} the problem of insufficient sampling of documents. We construct a probabilistic neighborhood for each document, and expand the document with its neighborhood <b>information.</b> The expanded <b>document</b> provides a more accurate estimation of the <b>document</b> <b>model,</b> thus improves retrieval accuracy. Moreover, since document expansion and pseudo feedback exploit different corpus structures, they can be combined to further improve performance. The experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method. ...|$|R
40|$|Business {{operations}} – Operations, {{the objective}} of which are to produce products and services onto selected mar kets {{in a manner that}} produces added value for selected customer segments and profit for invested capital. Challenge management – Management and control of an innovation process in order to create business operations in a manner that takes changes in the operational environment into account (threats or opportunities). Conceptualisation – Seeing the solution model of a product or service as a whole, but in broad outline before the final product or service solution. Co-ordination of commercialisation – Activities that aim to control the uncertainty of commercialisation-related information through the innovation process. Foresight – Multi-science mapping of future alternatives and timing from the operator’s point of view. Future research – Multi-science research that studies the present from an interest in knowing the future and combines ideological with <b>documented</b> <b>information.</b> INNORISK <b>model</b> – An operational model that combines future research, technological forecasts, busines...|$|R
40|$|Abstract: In text-based {{information}} retrieval, {{which is}} the predominant retrieval task at present, several <b>document</b> <b>models</b> have been proposed, such as boolean, probabilistic, or (extended) vector models [Baeza-Yates and Ribeiro-Neto 1999]. Interestingly, the suffix tree <b>document</b> <b>model</b> is usually not discussed {{in the literature on}} the subject though it comes along with a property that sets it apart from the other models: It encodes information about word order. The suffix tree <b>document</b> <b>model</b> owes much of its popularity from the Vivísimo search engine, which operationalizes on-the-fly categorization of Internet search results. While the classical <b>document</b> <b>models</b> can be considered as vectors of words, the suffix tree <b>document</b> <b>model</b> as well as the related similarity measures are graph-based. Both types of <b>document</b> <b>models</b> provide an efficient means to compute document similarities, and, according to various publications, both types of <b>document</b> <b>models</b> work well in practice. However, there is no comparison between both paradigms that explains the concepts of one in terms of the other, or that contrasts their advantages and disadvantages with respect to certain retrieval tasks. In this paper we start to tackle this gap by shading light on the following questions: (1) How does similarity computation work in the suffix tree <b>document</b> <b>model?</b> (2) Based on the insights of Question 1, is it possible to combine concepts of both <b>document</b> <b>model</b> types within classification or categorization tasks? (3) Which of the <b>document</b> <b>model</b> types is more powerful with respect to unsupervised document classification...|$|R
50|$|Section 3(1)(a) {{creates an}} offence of {{disclosing}} <b>information,</b> <b>documents</b> or other articles relating to international relations. This includes confidential <b>information,</b> <b>documents</b> or other article from a State {{other than the}} United Kingdom or an international organisation. This section applies only to crown servants and government contractors.|$|R
2500|$|Official <b>Information</b> <b>Documents</b> {{from the}} Commonwealth of Massachusetts: ...|$|R
40|$|The {{following}} {{paper is}} {{an outgrowth of}} research performed with a data base of merged individual income tax returns and <b>information</b> <b>documents.</b> Tax Year 1989 was the first year for which such a data base was created and perfected. Traditionally, the Statistics of Income (SOI) Division of the Internal Revenue Service has interpreted its mandate to produce "statistics reasonably available {{with respect to the}} operations of the internal revenue laws " [1] as meaning tabulating data shown on tax returns. In recent years, with the computerization of the millions of <b>information</b> <b>documents</b> prepared by employers, banks, stock brokers, payers of pensions, etc., data from these documents have increasingly become "reasonably available. " Data from <b>information</b> <b>documents,</b> when matched to tax returns, can be used to serve as a check on the data shown on individual income tax returns, as well as to provide an indication of how much of the income on a joint return belongs to the husband and how much to the wife. In addition, it is possible to pull a sample of <b>information</b> <b>documents</b> that do not match to tax returns, and use them to tally data about non-filers. The data base used for this paper was created as a tool to compare tax return data to data gathered from <b>information</b> <b>documents.</b> It includes a sample of tax returns matched to <b>information</b> <b>documents,</b> as well as unmatched tax returns and unmatched <b>information</b> <b>documents.</b> The age of each individual in the sample was determined by matching his or her social security number (SSN) to the Year of Birth file, whic...|$|R
40|$|Library records {{consist of}} <b>information</b> <b>documented</b> in {{performance}} of the library’s official business. Any <b>information</b> <b>documenting</b> official business, whether recorded on paper or in portable document format (PDF), reproduced on microfilm, entered in electronic databases, documented photographically, recorded in video or audio media, or documented using any other medium, may constitute a record...|$|R
5000|$|Metadata with {{semantic}} meta-information, Charset <b>information,</b> <b>Document</b> Type Definition (DTD), etc.|$|R
5000|$|Workflow {{platforms}} - {{to route}} <b>information,</b> <b>documents</b> and direct process flow ...|$|R
5000|$|Cryptome CN: <b>Information,</b> <b>documents</b> and {{opinions}} {{banned by the}} People's Republic of China.|$|R
5000|$|The {{transfer}} of <b>information,</b> <b>documents,</b> tasks, or objects from one {{step to the}} next ...|$|R
50|$|Section 1(1) {{creates an}} offence of {{disclosing}} <b>information,</b> <b>documents</b> or other articles relating to security or intelligence.|$|R
40|$|Abstract. The United Nations Centre for Trade Facilitation and eBusiness (UN/CEFACT) standardizes {{business}} {{documents for}} electronic data interchange. Their approaches towards UN/EDIFACT and XML have later {{been followed by}} a conceptual modeling approach called Core Components (CC). Having used this approach for four years in practice, it became evident that the support for managing business <b>document</b> <b>models</b> {{is a prerequisite for}} successfully utilizing CC. This includes handling variants of business <b>document</b> <b>models</b> on the one hand, and managing the evolution of business <b>document</b> <b>models</b> on the other hand. In this paper we propose an approach to face these challenges by the means of Software Product Line Engineering (SPLE) in combination with dedicated model management operators. The contribution of the approach is twofold. First, SPLE is successfully applied in a new field enabling us to manage variants of business <b>document</b> <b>models.</b> Second, the model management operators support the evolution of business <b>document</b> <b>model</b> variants, whereas the operators defined, contribute to the evolution of product lines as well. ...|$|R
500|$|Bristow, Mark. (2005) A History of Royal Air Force Northolt. RAF Northolt: No. 1 AIDU (Aeronautical <b>Information</b> <b>Documents</b> Unit) ...|$|R
5000|$|UCITS V {{directive}} {{requires a}} Key Investor <b>Information</b> <b>Document</b> or KIID is produced for investors - see example (autokiid.com) ...|$|R
40|$|In {{sentence}} modeling, {{neural network}} approaches that leverage the tree-structural features of sentences have recently achieved state-of-the-art results. However, such approaches require complex architectures {{and are not}} easily extensible to <b>document</b> <b>modeling.</b> In this paper, we propose a very simple convolutional neural network model that incorporates Part-Of-Speech tag information (PCNN). While our model can be easily extensible to <b>document</b> <b>modeling,</b> it shows great performance on both sentence and <b>document</b> <b>modeling</b> tasks. As a result of sentiment analysis and question classification tasks, PCNN achieves the performance {{comparable to that of}} other more complex state-of-the-art models on sentence modeling and outperforms them on <b>document</b> <b>modeling.</b> We also make efforts to explore the effect of POS tag embeddings more thoroughly by conducting various experiments...|$|R
5000|$|Managing www.protectionline.org, {{a project}} website {{providing}} <b>informations,</b> <b>documents,</b> publications, press releases and promoting urgent actions {{for the protection}} of HRD.|$|R
3000|$|... • it {{can provide}} {{politicians}} with <b>information,</b> <b>documents</b> and a basis for decision support to assess climate change impacts and [...]...|$|R
50|$|<b>Document</b> <b>modelling</b> {{looks at}} the {{inherent}} structure in documents. It looks not at the structure in formatting which is the classic realm of word-processing tools, but at the structure in content. Because document content is typically viewed as the ad hoc result of a creative process, the art of <b>document</b> <b>modelling</b> {{is still in its}} infancy. Most <b>document</b> <b>modelling</b> {{comes in the form of}} document templates evidenced most often as word-processing documents, fillable PDF forms, and XML templates. The particular strength of XML in this context is its ability to <b>model</b> <b>document</b> components in a tree-like structure, and its separation of content and style.|$|R
5000|$|Using the Freedom of Information Act (United States) {{to force}} {{government}} agencies to release {{hundreds of thousands}} of pages of <b>information</b> <b>documenting</b> government misconduct; ...|$|R
40|$|Approved {{for public}} release, {{distribution}} unlimited. This thesis solves a common issue in search applications. Typically, the user {{does not know}} exactly which terms are used in a document he is searching for. Several {{attempts have been made}} to overcome this issue by augmenting the <b>document</b> <b>model</b> and/or the query. In this thesis, a probabilistic topic <b>model</b> augments the <b>document</b> <b>model.</b> Probabilistic <b>document</b> <b>models</b> are formally introduced and inference methods are derived. It is shown how these models can be used for information retrieval tasks and how a search application can be implemented. A prototype was implemented and the implementation is tested and evaluated based on benchmark corpora. The evaluation provides empirical evidence that probabilistic <b>document</b> <b>models</b> improve the retrieval performance significantly, and shows which preprocessing steps should be made before applying the model. Outstanding ThesisGerman Army author...|$|R
50|$|Section 2(1) {{creates an}} offence of {{disclosing}} <b>information,</b> <b>documents</b> or other articles relating to defence. This section {{applies only to}} crown servants and government contractors.|$|R
5000|$|... {{require a}} person {{present on the}} {{premises}} to provide him with any facilities, <b>information,</b> <b>documents</b> or records, or other assistance, that he may reasonably request.|$|R
5000|$|... "There {{is exactly}} one <b>document</b> <b>{{information}}</b> {{item in the}} information set, and all other information items are accessible from {{the properties of the}} <b>document</b> <b>information</b> item, either directly or indirectly through the properties of other <b>information</b> items. The <b>document</b> <b>information</b> item has the following properties: ...|$|R
5000|$|Document model: The <b>document</b> <b>model</b> {{describes}} the core documents used in OpenTMS. Basically this {{is based on}} XLIFF and TMX. The <b>document</b> <b>model</b> also could be seen as part of the data model but due to the importance of documents as one of the core output produced by the translation and localization process they are modeled separately.|$|R
40|$|SyncML Device <b>Information</b> DTD This <b>document</b> {{defines the}} Document Type Definition (DTD) for the XML {{representation}} of the Device <b>Information</b> <b>document.</b> This XML document describes {{the capabilities of the}} device and is used in SyncML data synchronization protocol operations. Data synchronization provides the means for two different networked object stores to remain in identical states. Different forms of data synchronization can be categorized into {{one of a number of}} topologies, based on the architecture used by a data synchronization server, or sync engine. Sync engines need to understand the features of a device they synchronize with. This information is often stored in a Device <b>Information</b> <b>document</b> on the target device...|$|R
5000|$|... {{require a}} person {{present on the}} {{premises}} to provide the employee with any facilities, <b>information,</b> <b>documents</b> or records, or other assistance that the employee may reasonably request.|$|R
40|$|Aiming at {{the common}} {{problems}} of intelligent document platform-dependency, this paper proposes an MVC-based (Model View Controller-based) intelligent <b>document</b> <b>model</b> using UIML (User Interface Markup Language). The model is {{made on the}} basis of the previous work of our team, and the difference is that the new model separates user interface and interaction descriptions from the view component to make the intelligent <b>document</b> <b>model</b> much more independent of platform and programming language. To verify the intelligent <b>document</b> <b>model,</b> we implemented a prototype, which can support intelligent operations. The test result shows that our approach is correct. The model not only follows MVC framework, but also provides good flexibility and independence...|$|R
