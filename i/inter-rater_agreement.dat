954|74|Public
2500|$|There is some ongoing {{scientific}} doubt {{concerning the}} construct validity and reliability of psychiatric diagnostic categories and criteria {{even though they have}} been increasingly standardized to improve <b>inter-rater</b> <b>agreement</b> in controlled research. In the United States, there have been calls and endorsements for a congressional hearing to explore {{the nature and extent of}} harm potentially caused by this [...] "minimally investigated enterprise".|$|E
2500|$|There is {{research}} which demonstrates examiners {{are unable}} to measure craniosacral motion reliably, as indicated {{by a lack of}} <b>inter-rater</b> <b>agreement</b> among examiners. The authors of this research conclude this [...] "measurement error may be sufficiently large to render many clinical decisions potentially erroneous". Alternative medicine practitioners have interpreted this result as a product of entrainment between patient and practitioner, a principle which lacks scientific support. Whether craniosacral motion can be reliably palpated remains a subject of debate with studies producing mixed results.|$|E
5000|$|While {{technical}} reports [...] {{are available on}} the agency’s Web site, some have complained that the Office publishes little technical information about its tests. An evaluation conducted for EQAO, published by it in 2013, and posted on the website mentions no aspects of reliability or validity other than <b>inter-rater</b> <b>agreement,</b> and <b>inter-rater</b> <b>agreement</b> is presented as evidence of validity rather than of reliability.|$|E
40|$|In this study, we {{analyzed}} a newly developed optical reflectometer for measuring erosive tooth wear (ETW) in vitro. Three examiners independently assessed the labial surface of 80 deciduous canines and 75 permanent incisors. One examiner performed visual examinations (BEWE), {{and the other}} two used the optical pen-size reflectometer to measure surface reflection intensity (SRI) on the same labial surfaces. The examinations were made in duplicate with at least 1  week interval. Intra- and <b>inter-rater</b> <b>agreements</b> were calculated using weighted kappa analysis for BEWE, and intra-class correlation coefficients (ICC) as well as Bland-Altman plots for SRI. The teeth were separated into without (BEWE 0) or with (BEWE 1 - 3) ETW, and SRI cut-off points were calculated. Intra-rater agreement for the visual examination was 0. 46 and 0. 82 for deciduous and permanent teeth, respectively. <b>Inter-rater</b> and intra-rater <b>agreement</b> for SRI were good (ICC[*]>[*] 0. 7; p[*]<[*] 0. 001). SRI measurements produced high specificity values for deciduous and permanent teeth (≥ 0. 74 and[*]≥[*] 0. 84, respectively), and lower sensitivity values (≥ 0. 37 and[*]≥[*] 0. 64, respectively), but permanent teeth had generally higher SRI values (p[*]<[*] 0. 05). We observed a significant association between BEWE and SRI (p[*]<[*] 0. 05). The optical pen-size reflectometer was able to adequately differentiate ETW on permanent teeth, with highly reliable and reproducible measurements, but ETW on deciduous teeth was less accurately differentiated. The reflectometer is a good candidate for clinical research...|$|R
30|$|In {{the field}} of {{language}} testing research it is generally believed that rater training is necessary to maintain {{the reliability and validity}} of language performance tests (Fulcher, 2003). Some empirical studies do support this view, suggesting higher <b>inter-rater</b> reliability and <b>agreement</b> after training (Shohamy et al. 1992; Weigle, 1994, 1998). Others find training results in considerable variation in rater severity and scoring criteria (Lumley & McNamara, 1995; Orr, 2002; Papajohn, 2002). With such inconsistent research evidence, regarding the scoring of the TOEFL iBT Speaking Test by 20 native teachers of English, Davis (2016) confirmed rater training led to modest improvements in <b>inter-rater</b> reliability and <b>agreement</b> whereas it had little impact on rater consistency or severity.|$|R
40|$|Opinions {{about the}} 2016 U. S. Presidential Candidates have been {{expressed}} {{in millions of}} tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation {{of the number of}} workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on <b>inter-rater</b> <b>agreements</b> between labels. We applied our approach to 1, 000 twitter messages about the four U. S. presidential candidates Clinton, Cruz, Sanders, and Trump, collected during February 2016. We implemented the two proposed methods using decision trees that allocate more crowd efforts to tweets predicted to be sarcastic. We show that our framework outperforms the traditional static allocation scheme. It collects opinion labels from the crowd at a much lower cost while maintaining labeling accuracy. Comment: 10 pages, 3 figure...|$|R
5000|$|Cicchetti (1994) {{gives the}} {{following}} often quoted guidelines for interpretation for kappa or ICC <b>inter-rater</b> <b>agreement</b> measures: ...|$|E
50|$|Bennett, Alpert & Goldstein’s S is a {{statistical}} measure of <b>inter-rater</b> <b>agreement.</b> It {{was created by}} Bennett et al. in 1954.|$|E
50|$|Various {{statistics}} {{have been}} proposed to measure <b>inter-rater</b> <b>agreement.</b> Among them are percent agreement, Scott's π, Cohen's κ, Krippendorf's α, Pearson's correlation coefficient r, Spearman's rank correlation coefficient ρ, and Lin's concordance correlation coefficient.|$|E
40|$|Background: Surgical {{patients}} {{are at risk}} for preventable adverse drug events (ADEs) during hospitalization. Usually, preventable ADEs are measured as an outcome parameter of quality of pharmaceutical care. However, process measures such as QIs are more efficient to assess the quality of care and provide more information about potential quality improvements. Objective: To assess the quality of pharmaceutical care of medication-related processes in surgical wards with quality indicators, in order to detect targets for quality improvements. Methods: For this observational cohort study, quality indicators were composed, validated, tested, and applied on a surgical cohort. Three surgical wards of an academic hospital in the Netherlands (Academic Medical Centre, Amsterdam) participated. Consecutive elective surgical patients with a hospital stay longer than 48 hours were included from April until June 2009. To assess the quality of pharmaceutical care, the set of quality indicators was applied to 252 medical records of surgical patients. Results: Thirty-four quality indicators were composed and tested on acceptability and content- and face-validity. The selected 28 candidate quality indicators were tested for feasibility and ‘sensitivity to change’. This resulted in a final set of 27 quality indicators, of which <b>inter-rater</b> <b>agreements</b> were calculated (kappa 0. 92 for eligibility, 0. 74 for pass-rate). The qualit...|$|R
30|$|EPs and {{cardiologist}} indicated DD present, DD absent, or indeterminate {{for each}} LBE study. A 3  ×  3 contingency table provided {{a summary of}} <b>agreement.</b> <b>Inter-rater</b> reliability between EPs and the cardiologist was determined using an unweighted kappa with 95 % confidence interval (CI) coefficient using Stata Release 15, StataCorp.|$|R
50|$|Krippendorff’s alpha generalizes several known statistics, {{often called}} {{measures}} of inter-coder <b>agreement,</b> <b>inter-rater</b> reliability, reliability of coding given sets of units (as distinct from unitizing) {{but it also}} distinguishes itself from statistics that are called reliability coefficients but are unsuitable to the particulars of coding data generated for subsequent analysis.|$|R
50|$|Bangdiwala's B {{statistic}} {{was created}} by Dr. Shrikant Bangdiwala in 1985 and {{is a measure of}} <b>inter-rater</b> <b>agreement.</b> While not as commonly used as the kappa statistic the B test has been used by various workers. While it is principally used as a graphical aid to inter observer agreement, its asymptotic distribution is known.|$|E
5000|$|There is some ongoing {{scientific}} doubt {{concerning the}} construct validity and reliability of psychiatric diagnostic categories and criteria {{even though they have}} been increasingly standardized to improve <b>inter-rater</b> <b>agreement</b> in controlled research. In the United States, there have been calls and endorsements for a congressional hearing to explore {{the nature and extent of}} harm potentially caused by this [...] "minimally investigated enterprise".|$|E
50|$|Cohen's kappa {{coefficient}} is {{a statistic}} which measures <b>inter-rater</b> <b>agreement</b> for qualitative (categorical) items. It is generally {{thought to be}} a more robust measure than simple percent agreement calculation, since κ takes into account the possibility of the agreement occurring by chance.  There is controversy surrounding Cohen’s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items. See the Limitations section for more detail.|$|E
30|$|<b>Inter-rater</b> and intra-rater <b>agreement</b> was {{analysed}} {{with the}} interclass correlation coefficient (ICC) (two-way mixed model, type absolute agreement), {{taking into consideration}} that an ICC of 0 – 0.2 represents slight agreement, 0.21 – 0.4 fair agreement, 0.41 – 0.6 moderate agreement, 0.61 – 0.8 substantial agreement and 0.81 – 1 excellent agreement [21].|$|R
40|$|Introduction and Aims. There is no {{generally}} accepted clinical or research instrument available for recording the longitudinal {{course of a}} drug-using 'career'. This paper reports on an initial examination of {{the properties of the}} Lifetime Drug Use History Questionnaire (LDUH), built around monthly mapping of drug use patterns in relation to other life events. Design and Methods. Forty heroin and cocaine users completed structured interviews at two treatment sites. Twenty subjects were interviewed on two occasions separated by a 3 -day interval, using either the same interviewer (n = 10) or two different interviewers (n = 10) as assessments of inter-rater and test-retest reliability. Results. Very good <b>inter-rater</b> <b>agreements</b> were observed, demonstrated by Cronbach's alpha and intraclass correlation coefficients generally higher than 0. 8 and 0. 7, respectively. Additionally, concordance with clinical notes was assessed for four drug use history variables, resulting in poorer rates of agreement. An exact matching with clinical records was obtained for the variable 'age of first use of heroin' in 47. 2 % (n = 17) of the heroin users, while a good agreement (only 1 or 2 years' difference) was found in 36. 1 % of cases (n = 5). Discussion and Conclusions. The LDUH method resulted in high reliability for heroin and cocaine and suggests an effective, clinically applicable method for history-taking. The paucity and inconsistency of similar information in the clinical notes would further justify the use of a standardised method for recording drug histories...|$|R
40|$|Articular {{cartilage}} defects are {{prevalent in}} metacarpo/metatarsophalangeal (MCP/MTP) joints of horses. The {{aim of this}} study was to determine and compare the sensitivity and specificity of 3 -Tesla magnetic resonance imaging (3 -T MRI) and computed tomography arthrography (CTA) to identify structural cartilage defects in the equine MCP/MTP joint. Forty distal cadaver limbs were imaged by CTA (after injection of contrast medium) and by 3 -T MRI using specific sequences, namely, dual-echo in the steady-state (DESS), and sampling perfection with application-optimised contrast using different flip-angle evolutions (SPACE). Gross anatomy was used as the gold standard to evaluate sensitivity and specificity of both imaging techniques. CTA sensitivity and specificity were 0. 82 and 0. 96, respectively, and were significantly higher than those of MRI (0. 41 and 0. 93, respectively) in detecting overall cartilage defects (no defect vs. defect). The intra and <b>inter-rater</b> <b>agreements</b> were 0. 96 and 0. 92, respectively, and 0. 82 and 0. 88, respectively, for CT and MRI. The positive predictive value for MRI was low (0. 57). CTA was considered a valuable tool for assessing cartilage defects in the MCP/MTP joint due to its short acquisition time, its specificity and sensitivity, and it was also more accurate than MRI. However, MRI permits assessment of soft tissues and subchondral bone and is a useful technique for joint evaluation, although clinicians should be aware of the limitations of this diagnostic technique, including reduced accuracy...|$|R
50|$|In statistics, inter-rater reliability, <b>inter-rater</b> <b>agreement,</b> or {{concordance}} is {{the degree}} of agreement among raters. It gives a score of how much homogeneity, or consensus, {{there is in the}} ratings given by judges. It is useful in refining the tools given to human judges, for example by determining if a particular scale is appropriate for measuring a particular variable. If various raters do not agree, either the scale is defective or the raters need to be re-trained.|$|E
50|$|Before {{computers}} {{entered the}} picture, high-stakes essays were typically given scores by two trained human raters. If the scores differed {{by more than}} one point, a third, more experienced rater would settle the disagreement. In this system, there is an easy way to measure reliability: by <b>inter-rater</b> <b>agreement.</b> If raters do not consistently agree within one point, their training may be at fault. If a rater consistently disagrees with whichever other raters look at the same essays, that rater probably needs more training.|$|E
5000|$|There is {{research}} which demonstrates examiners {{are unable}} to measure craniosacral motion reliably, as indicated {{by a lack of}} <b>inter-rater</b> <b>agreement</b> among examiners. The authors of this research conclude this [...] "measurement error may be sufficiently large to render many clinical decisions potentially erroneous". Alternative medicine practitioners have interpreted this result as a product of entrainment between patient and practitioner, a principle which lacks scientific support. Whether craniosacral motion can be reliably palpated remains a subject of debate with studies producing mixed results.|$|E
40|$|Different {{algorithms}} {{have been}} developed to standardize the causality assessment of adverse drug reactions (ADR). Although most share common characteristics, the results of the causality assessment are variable depending on the algorithm used. Therefore, using 10 different algorithms, the study aimed to compare <b>inter-rater</b> and multi-rater <b>agreement</b> for ADR causality assessment and identify the most consistent to hospitals. Using ten causality algorithms, four judges independently assessed the first 44 cases of ADRs reported {{during the first year of}} implementation of a risk management service in a medium complexity hospital in the state of Sao Paulo (Brazil). Owing to variations in the terminology used for causality, the equivalent imputation terms were grouped into four categories: definite, probable, possible and unlikely. <b>Inter-rater</b> and multi-rater <b>agreement</b> analysis was performed by calculating the Cohen´s and Light´s kappa coefficients, respectively. None of the algorithms showed 100 % reproducibility in the causal imputation. Fair <b>inter-rater</b> and multi-rater <b>agreement</b> was found. Emanuele (1984) and WHO-UMC (2010) algorithms showed a fair rate of agreement between the judges (k = 0. 36). Although the ADR causality assessment algorithms were poorly reproducible, our data suggest that WHO-UMC algorithm is the most consistent for imputation in hospitals, since it allows evaluating the quality of the report. However, to improve the ability of assessing the causality using algorithms, it is necessary to include criteria for the evaluation of drug-related problems, which may be related to confounding variables that underestimate the causal association...|$|R
40|$|The {{heuristic}} evaluation (HE) {{method is}} one of the most common in the suite of tools for usability evaluations because it is a fast, inexpensive and resource-efficient process in relation to the many usability issues it generates. The method emphasizes completely independent initial expert evaluations. <b>Inter-rater</b> reliability and <b>agreement</b> coefficients are not calculated. The variability across evaluators, even dual domain experts, can be significant as is seen in the case study here. The implications of this wide variability mean that results are unique to each HE, results are not readily reproducible and HE research on usability is not yet creating a uniform body of knowledge. We offer recommendations to improve the science by incorporating selected techniques from qualitative research: calculating <b>inter-rater</b> reliability and <b>agreement</b> scores, creating a codebook to define concepts/categories and offering crucial information about raters' backgrounds, agreement techniques and the evaluation setting. Studies in Health Technology and Informatics, IOS Press, indexed in Medline (PubMed...|$|R
40|$|Background: There is an {{increasing}} recognition that many consultations in general practice involve several problems covering multiple disease domains. However there is a paucity of reliable tools and techniques to understand and quantify this phenomenon. The objective {{was to develop a}} tool {{that can be used to}} measure the number and type of problems discussed in primary care consultations. Methods: Thirteen consultations between general practitioners and patients were initially videoed and reviewed to identify the problems and issues discussed. An iterative process involving a panel of clinicians and researchers and repeated cycles of testing and development was used to develop a measurement proforma and coding manual for assessment of video recorded consultations. The inter-rater reliability of this tool was assessed in 60 consultations. Results: The problems requiring action were usually readily identified. However the different dimensions of the problem and how they were addressed required the identification and definition of ‘issues’. A coding proforma was developed that allowed quantification of the numbers and types of health problems and issues discussed. Ten categories of issues were identified and defined. At the consultation level, <b>inter-rater</b> <b>agreements</b> for the number of problems discussed (within ± 1), types of problems and issues were 98. 3 %, 96. 5 % and 90 % respectively. The tool has subsequently been used to analyse 229 consultations. Conclusion: The iterative approach to development of the tool reflected the complexity of doctor-patient interactions. A reliable tool has been developed that can be used to analyse the number and range of problems managed in primary care consultations...|$|R
5000|$|<b>Inter-rater</b> <b>agreement</b> can now {{be applied}} to {{measuring}} the computer's performance. A set of essays is given to two human raters and an AES program. If the computer-assigned scores agree {{with one of the}} human raters as well as the raters agree with each other, the AES program is considered reliable. Alternatively, each essay is given a [...] "true score" [...] by taking the average of the two human raters' scores, and the two humans and the computer are compared {{on the basis of their}} agreement with the true score.|$|E
50|$|When {{comparing}} {{two methods}} of measurement {{it is not}} only of interest to estimate both bias and limits of agreement between the two methods (<b>inter-rater</b> <b>agreement),</b> but also to assess these characteristics for each method within itself (intra-rater agreement). It might very well be that the agreement between two methods is poor simply because one of the methods has wide limits of agreement while the other has narrow. In this case the method with the narrow limits of agreement would be superior from a statistical point of view, while practical or other considerations might change this appreciation. What constitutes narrow or wide limits of agreement or large or small bias is a matter of a practical assessment in each case.|$|E
30|$|The {{difference}} of aesthetical preference between the evaluators {{may affect the}} results of study, so we evaluated the <b>inter-rater</b> <b>agreement</b> with the Kappa coefficient. The Kappa coefficient to measure <b>inter-rater</b> <b>agreement</b> for aesthetic mandibular line was more than 0.75.|$|E
40|$|Background: Sodium glucose co-transporter 2 inhibitors {{represent}} a novel class of antidiabetic drugs. The reporting {{quality of the}} trials evaluating the efficacy of these agents for glycemic control in type 2 diabetes mellitus has not been explored. Our aim {{was to assess the}} reporting quality of such randomized controlled trials (RCTs) and to identify the predictors of reporting quality. Materials and Methods: A systematic literature search was conducted for RCTs published till 12 June 2014. Two independent investigators carried out the searches and assessed the reporting quality on three parameters: Overall quality score (OQS) using Consolidated Standards of Reporting Trials (CONSORT) 2010 statement, Jadad score and intention to treat analysis. <b>Inter-rater</b> <b>agreements</b> were compared using Cohen's weighted kappa statistic. Multivariable linear regression analysis was used to identify the predictors. Results: Thirty-seven relevant RCTs were included in the present analysis. The median OQS was 17 with a range from 8 to 21. On Jadad scale, the median score was three with a range from 0 to 5. Complete details about allocation concealment and blinding were present in 21 and 10 studies respectively. Most studies lacked an elaborate discussion on trial limitations and generalizability. Among the factors identified as significantly associated with reporting quality were the publishing journal and region of conduct of RCT. Conclusions: The key methodological items remain poorly reported in most studies. Strategies like stricter adherence to CONSORT guidelines by journals, access to full trial protocols to gain valuable information and full collaboration among investigators and methodologists might prove helpful in improving the quality of published RCT reports...|$|R
40|$|Introduction Accurate {{and timely}} {{evaluation}} of swallowing {{is necessary to}} determine how to safely administer medications, maintain adequate nutrition and hydration, and avoid deleterious sequelae of prandial aspiration pneumonia. Use of a validated and reliable screening tool for determination of aspiration risk is a critical component of dysphagia management. The 3 -ounce water swallow challenge (Suiter 2 ̆ 6 Leder, 2008) is a validated and reliable screening tool that is well supported in the literature. Statement of the Problem While use of the 3 -ounce water swallow challenge (Suiter 2 ̆ 6 Leder 2008) administered by speech-language pathologists (SLPs) is supported by current research, who else should administer and interpret the challenge is not addressed. Therefore, health care professionals other than SLPs, i. e., registered nurses, should be involved in screening for aspiration risk (Bours et al., 2008). Deficiencies in current nurse administered screens are a barrier to this practice change. Background There is a paucity of literature supporting nursing administration of validated screening tools for determining aspiration risk in hospitalized patients. Current practice involving administration of swallow screens by nurses is comprised of investigations that utilize largely non-evidenced based variables. Research Purpose This study investigated accuracy of a registered nurse administered 3 -ounce water swallow challenge with hospitalized patients deemed at-risk for prandial aspiration compared with blinded ratings from speech-language pathology. Methods Patients were administered the 3 -ounce water swallow challenge protocol by a SLP. The nurse then administered the screen to the same patient within 1 hour and independently recorded results and diet recommendations. Simultaneous with the nurse administered screen, a SLP re-rated the patient’s 3 -ounce challenge for comparison with initial results as well as determined accuracy of the nurse administered screen. Results Intra- and <b>inter-rater</b> <b>agreements</b> for the two speech-language pathologists were 100...|$|R
40|$|Background: Friedewald’s formula (FF) is used {{worldwide}} {{to calculate}} low-density lipoprotein cholesterol (LDL-chol). But it has several shortcomings: overestimation at lower triglyceride (TG) concentrations and underestimation at higher concentrations. In FF, TG to {{very low-density lipoprotein}} cholesterol (VLDL-chol) ratio (TG/VLDL-chol) is considered as constant, but practically {{it is not a}} fixed value. Recently, by analyzing lipid profiles in a large population, continuously adjustable values of TG/VLDL-chol were used to derive a novel method (NM) for the calculation of LDL-chol. Objective: The aim {{of this study was to}} evaluate the performance of the novel method compared with direct measurement and regression equation (RE) developed for Bangladeshi population. Materials and Methods: In this cross-sectional comparative study we used lipid profiles of 955 adult Bangladeshi subjects. Total cholesterol (TC), TG, HDL-chol and LDL-chol were measured by direct methods using automation. LDL-chol was also calculated by NM and RE. LDL-chol calculated by NM and RE were compared with measured LDL-chol by twotailed paired t test, Pearson’s correlation test, bias against measured LDL-chol by Bland-Altman test, accuracy within ± 5 % and ± 12 % of measured LDL-chol and by <b>inter-rater</b> <b>agreements</b> with measured LDL-chol at different cut-off values. Results: The mean values of LDL-chol were 110. 7 ± 32. 0 mg/dL for direct measurement, 111. 9 ± 34. 8 mg/dL for NM and 113. 2 ± 31. 7 mg/dL for RE. Mean values of calculated LDL-chol by both NM and RE differed from that of measured LDL-chol (p 130 mg/dL were 0. 816 vs 0. 815, 0. 637 vs 0. 649 and 0. 791 vs 0. 791 for NM and RE respectively. Conclusion: This study reveals that NM and RE developed for Bangladeshi population have similar performance and can be used for the calculation of LDL-chol...|$|R
40|$|Introduction: Standardization of first-trimester nuchal {{translucency}} (NT) {{image acquisition}} {{is crucial to}} the success of screening for Down syndrome. Rigorous audit of operator performance and constructive feedback from assessors maintain standards. This process relies on good <b>inter-rater</b> <b>agreement</b> on image assessment. We describe the Australian approach to NT image assessment and evaluate the impact of a targeted intervention on <b>inter-rater</b> <b>agreement.</b> Methods: Between 2002 and 2008 a group of experienced practitioners met nine times to compare their assessment of a series of NT images. Each assessor had previously scored the images according to a system described in 2002. <b>Inter-rater</b> <b>agreement</b> was evaluated before and after an intervention where the assessors were required to refer to a detailed resource manual designed to reduce the subjectivity inherent in image assessment. Results: There was a statistical improvement in <b>inter-rater</b> <b>agreement</b> for all elements of image assessment (original scores and individual component scores) after the intervention, apart from horizontal fetal position. However, even after the intervention, <b>inter-rater</b> <b>agreement</b> levels generally remained moderate (kappa range: 0. 14 - 0. 58). Conclusions: This study has shown that provision of detailed resource documentation to experienced assessors can significantly improve <b>inter-rater</b> <b>agreement</b> in all facets of NT image assessment. It also highlights areas of image assessment that require critical review. It is recommended that all audit bodies regularly review their <b>inter-rater</b> <b>agreement</b> to ensure consistent feedback to operators who submit images for expert peer review. Copyrigh...|$|E
40|$|AbstractIn {{this paper}} we apply the s* statistic, aimed {{to measure the}} <b>inter-rater</b> <b>agreement</b> between {{observers}} in case of ordinal variables, to {{the evaluation of the}} quality of University courses. The objective is to measure the <b>inter-rater</b> <b>agreement</b> between students, along with their satisfaction, in order to verify the consistency of judgments expressed by independent observers. s* is a modification of a previously proposed index, which avoids the problem of paradoxes of Cohen's and Fleiss’ kappa statistics. We present the s* index from both a descriptive and an inferential point of view. In particular, as far as statistical inference is concerned, we show that s* is a biased estimator of the <b>inter-rater</b> <b>agreement</b> in the population and, under the null hypothesis of <b>inter-rater</b> <b>agreement</b> by chance, s* is asymptotically normally distributed...|$|E
30|$|Coding {{took place}} with {{a very high level}} of <b>inter-rater</b> <b>agreement.</b> Agreement was 99.3 % for ease of meeting, concerns, and {{satisfaction}} with partner and 94.1 % for signs of positive rapport. This coding took place with the original coding scheme that was developed; changes were not made throughout the <b>inter-rater</b> <b>agreement</b> process.|$|E
40|$|Background and purposes: The National Institutes of Health Stroke Scale (NIHSS) is an {{integral}} part of acute stroke assessment. We report our experience with new Putonghua- and Cantonese-Chinese language NIHSS (PC-NIHSS and CC-NIHSS) training and certification videos. Methods: A professional video production company was hired to create the training and certification videos for both PC-NIHSS and CC-NIHSS. Two training and certification workshops were held in Chengdu and Beijing, and two workshops in Hong Kong. The instruction, training and group A certification videos were presented to workshop attendees. Unweighted κ statistics were used to measure the agreement among raters, and the <b>inter-rater</b> <b>agreements</b> for PC-NIHSS and CC-NIHSS videos were compared with those of original English language NIHSS (E-NIHSS) videos. Results: The pass rates using PC-NIHSS and CC-NIHSS videos were 79 % and 82 %, respectively. All possible responses on individual scale items were included. Facial palsy and limb ataxia (13 %) showed poor agreement, nine (60 %) to 10 (67 %) items showed moderate agreement (0 · 4 <. κ< 0 · 75), and three (20 %) to four (27 %) items showed excellent agreement. When compared with E-NIHSS videos, the agreements on best gaze, visual fields, facial weakness and aphasia were less for PC-NIHSS videos, and the agreements on commands for level of consciousness and visual fields were less for CC-NIHSS videos. Nevertheless, there was no difference between PC-NIHSS or CC-NIHSS and E-NIHSS videos in the agreement on total score. Conclusions:Compared with E-NIHSS videos, PC-NIHSS and CC-NIHSS videos show good content validity and inter-rater reliability. Availability of these videos may facilitate the proper use of NIHSS among physicians and nurses in Putonghua- or Cantonese-speaking communities. © 2010 The Authors. Journal compilation © 2010 World Stroke Organization. link_to_OA_fulltex...|$|R
40|$|Abstract: Background: Worldwide, {{approximately}} 12 % {{of children}} under the age of 5 are either overweight or obese. As many young children spend 30 h or more per week in childcare centres with childcare educators. Targeting childcare educators as role models may prove an effective strategy for the promotion of healthy eating and physical activity. This manuscript describes the methods to systematically review existing literature relating to how childcare educators influence children’s healthy eating and physical activity behaviours, as well as the links between specific practices and behaviours of childcare educators and children’s healthy lifestyle behaviours. Methods: Relevant peer-reviewed studies will be identified through a computerized literature search in six databases: PubMed, The Cochrane Library, Science Direct, CINAHL, Wiley and SportDiscus. Quantitative studies written in English or French reporting the correlates, predictors or effectiveness of childcare educators’ practices and behaviours on preschoolers’ healthy eating and physical activity behaviours will be included. The quality of retained studies will be assessed using the Quality Assessment Tool for Quantitative Studies. Descriptive summary statistics of study characteristics will be reported as well as the study designs and exposure and outcome measures. <b>Inter-rater</b> <b>agreements</b> for study selection and quality assessments will be reported and unadjusted, and adjusted results will be presented. Reporting of the systematic review will follow the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Discussion: This systematic review will contribute to a better understanding of the potential of childcare educators as role models for young children, as well as the influence (or impact) of their behaviours and intervention on children’s short- and long-term health. It will provide important information that could be used to improve obesity prevention strategies and initiatives, as well as to guide the improvement or implementation of effective healthy eating and physical activity policies in childcare centres. Systematic review registration: PROSPERO (CRD 42014012973...|$|R
40|$|Surgical {{patients}} {{are at risk}} for preventable adverse drug events (ADEs) during hospitalization. Usually, preventable ADEs are measured as an outcome parameter of quality of pharmaceutical care. However, process measures such as QIs are more efficient to assess the quality of care and provide more information about potential quality improvements. To assess the quality of pharmaceutical care of medication-related processes in surgical wards with quality indicators, in order to detect targets for quality improvements. For this observational cohort study, quality indicators were composed, validated, tested, and applied on a surgical cohort. Three surgical wards of an academic hospital in the Netherlands (Academic Medical Centre, Amsterdam) participated. Consecutive elective surgical patients with a hospital stay longer than 48 hours were included from April until June 2009. To assess the quality of pharmaceutical care, the set of quality indicators was applied to 252 medical records of surgical patients. Thirty-four quality indicators were composed and tested on acceptability and content- and face-validity. The selected 28 candidate quality indicators were tested for feasibility and 'sensitivity to change'. This resulted in a final set of 27 quality indicators, of which <b>inter-rater</b> <b>agreements</b> were calculated (kappa 0. 92 for eligibility, 0. 74 for pass-rate). The quality of pharmaceutical care was assessed in 252 surgical patients. Nearly half of the surgical patients passed the quality indicators for pharmaceutical care (overall pass rate 49. 8 %). Improvements should be predominantly targeted to medication care related processes in surgical patients with gastro-intestinal problems (domain pass rate 29. 4 %). This quality indicator set can be used to measure quality of pharmaceutical care and detect targets for quality improvements. With these results medication safety in surgical patients can be enhance...|$|R
