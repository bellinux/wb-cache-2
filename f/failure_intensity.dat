87|52|Public
5000|$|Intensities {{from state}} 0 and state 1 are , since each compute node has a <b>failure</b> <b>intensity</b> of [...] Intensity from state 1 to state 2 is [...]Transitions from state 2 to state 1 and state 1 to state 0 {{represent}} the repairs of the compute nodes {{and have the}} intensity , since only a single unit is repaired at the time.|$|E
40|$|Purpose: Estimate the {{maintenance}} efficiency in the Brown-Proschan model with the bathtub <b>failure</b> <b>intensity.</b> Design/methodology/approach: Empirical research {{through which we}} propose a framework to establish the characteristics of failure process and its influence on maintenance process. Findings: The main contribution {{of the present study}} is the reformulation of the Brown and Proschan model using the bathtub <b>failure</b> <b>intensity</b> Practical implications: Our model is defined by BP reformulation one using bathtub <b>failure</b> <b>intensity.</b> This form of intensity is presented like superposition of two NHPP and Homogeneous Poisson one. Originality/value: This is the follow on research on the study that employed the power-law-process type of <b>failure</b> <b>intensity.</b> </p...|$|E
40|$|We {{describe}} an experiment investigating {{the distribution of}} <b>failure</b> <b>intensity</b> in software reliability growth models. We found that the assumption of conventional models that the <b>failure</b> <b>intensity</b> follows a gamma distribution is not always true. Our new software reliability model does not make this assumption; rather, the <b>failure</b> <b>intensity</b> is calculated from failure data. We show that our new model predicts more accurately the number of detected faults for our study project than the conventional models...|$|E
40|$|Residual {{analysis}} is used commonly in statistical tests of model fit, i. e. of good correspondence between data and model. We shall recall {{the notion of}} martingale residuals de¯ned for the case of lifetime models based on <b>failure</b> <b>intensities.</b> Analysis of these residual processes has already been studied by many authors. Nevertheless, Bayes approach to this problem is just developing. We shall present a Bayes procedure of estimation in models containing nonparametric components. Further, Bayes construction of residual processes and model goodness-of-¯t assessing will be proposed and demonstrated on an example with Cox's regression model...|$|R
40|$|This paper {{presents}} {{a new approach}} to software reliability modeling by grouping data into clusters of homogeneous <b>failure</b> <b>intensities.</b> This series of data clusters associated with different time segments can be directly used as a piecewise linear model for reliability assessment and problem identification, which can produce meaningful results early in the testing process. The dual model fits traditional software reliability growth models (SRGMs) to these grouped data to provide long-term reliability assessments and predictions. These models were evaluated in the testing of two large software systems from IBM. Comparing to existing SRGMs fitted to raw data, our models are generally more stable over time and produce more consistent and accurate reliability assessments and predictions...|$|R
40|$|A new, {{efficient}} procedure {{estimates the}} number of errors in a system. A known number of seeded errors are inserted into a system. The <b>failure</b> <b>intensities</b> of the seeded and real errors are allowed to be different and time dependent. When an error is detected during the test, it {{is removed from the}} system. The testing process is observed for a fixed amount of time τ. Martingale theory is used to derive a class of estimators for {{the number of}} seeded errors in a continuous time setting. Some of the estimators and their associated standard deviations have explicit expressions. An optimal estimator among the class of estimators is obtained. A simulation study assesses the performance of the proposed estimators. published_or_final_versio...|$|R
40|$|Spoľahlivosť a spojitý model obnovy The failure-free {{function}} {{of an object}} {{is very important for}} the service. This leads to the interest in the determination of the object reliability and <b>failure</b> <b>intensity.</b> The reliability of an element is defined by the theory of probability. The element durability T is a continuous random variate with λ t is a very important reliability characteristics of the element. Often it is an the probability density f. The <b>failure</b> <b>intensity</b> () increasing function, which corresponds to the element ageing. We disposed of the data about a belt conveyor failures recorded during the period of 90 months. The given ses behaves according to the normal distribution. By using a mathematical analysis and matematical statistics, we found the <b>failure</b> <b>intensity</b> function (t) The function λ () t increases almost linearly. Key words: corrective procedure, reliability, failure, <b>failure</b> <b>intensity...</b>|$|E
40|$|We {{report a}} study to {{determine}} the impact of four types of disturbances on the <b>failure</b> <b>intensity</b> of a software product undergoing system test. Hardware failures, discovery of a critical fault, attrition in the test team, are examples of disturbances that will likely affect the convergence of the <b>failure</b> <b>intensity</b> to its desired value. Such disturbances are modeled as impulse, pulse, step, and white noise. Our study examined, in quantitative terms, the impact of such disturbances on the convergence behavior of the <b>failure</b> <b>intensity.</b> Results from this study reveal that {{the behavior of the}} state model, proposed elsewhere, is consistent with what one might predict. The model is useful in that it provides a quantitative measure of the delay one can expect when a disturbance occurs. ...|$|E
40|$|The failure-free {{function}} {{of an object}} {{is very important for}} the service. This leads to the interest in the determination of the object reliability and <b>failure</b> <b>intensity.</b> The reliability of an element is defined by the theory of probability. The element durability T is a continuous random variate with the probability density f. The <b>failure</b> <b>intensity</b> () tλ is a very important reliability characteristics of the element. Often it is an increasing function, which corresponds to the element ageing. We disposed of the data about a belt conveyor failures recorded during the period of 90 months. The given ses behaves according to the normal distribution. By using a mathematical analysis and matematical statistics, we found the <b>failure</b> <b>intensity</b> function () tλ. The function () tλ increases almost linearly...|$|E
40|$|Slope <b>failures</b> <b>intensity</b> and {{frequency}} had {{increased in the}} last decade. This study examines the trend change form of slope failures and the directional spatial tendency of slope failures {{in the last decade}} in the Klang Valley Region (KVR) of Malaysia. The temporal and spatial changes have great significance on population vulnerability in the KVR. Two significant effects identified are first the increasing intensity - frequency impact of slope failures on KVR population and secondly the spatial direction of the impacts in the KVR. Both of these effects create severe stresses on the population of the KVR. The study observed that the temporal and spatial advances of slope failures would continue to increase in intensity {{and frequency}} in the not so distant future as the environment would become more stressful as a result of urbanization...|$|R
30|$|Conclusion In {{hematological}} patients requiring ICU management, neurological {{failure is}} frequent and associated with poor outcome. Type of underlying hematological malignancy, poor performance status, hemodynamic and respiratory <b>failures</b> and <b>intensity</b> of consciousness impairment are independently associated with poor outcome. Early goal directed management {{in those patients}} may offer opportunities for improvement.|$|R
40|$|The {{object of}} investigation: the {{electronic}} devices {{of control and}} processing of information of the technological equipment automatic control system. The purpose of the work: {{the development of the}} engineer methods of ensurance of the high indicators of the hardware and program reliability of the devices of automatic control and processing of information at the minimum increase of expenses. The mathematical model of design of the simplification efficiency has been offered; the methods of the hardware and program simplification have been developed. The universal reliable structures of the hard- and software have been obtained; the specimens of the electronic devices of the automatic control and processing of the information have been developed. The results have been introduced {{in the form of the}} software of the industrial controllers, the experiment batch of the telephone minicommutators, the means of the design support. The decrease of the <b>failures</b> <b>intensity</b> is to 29 %Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
30|$|The {{software}} <b>failure</b> <b>intensity</b> λ (t) {{is explained}} as {{the percentage of}} the removed errors in the software product.|$|E
40|$|A {{closed-loop}} {{feedback control}} {{model of the}} Software Test Process (STP) is described. The model {{is grounded in the}} well established theory of Automatic Control. It offers a formal and novel procedure for using product reliability or <b>failure</b> <b>intensity</b> as a basis for closed loop control of the STP. The reliability or the <b>failure</b> <b>intensity</b> of the product is compared against the desired reliability at each checkpoint and the difference fed back to a controller. The controller uses this difference to compute changes necessary in the process parameters to meet the reliability or <b>failure</b> <b>intensity</b> objective at the terminal checkpoint (the deadline). The STP continues beyond a checkpoint with a revised set of parameters. This procedure is repeated at each checkpoint until the termination of the STP. The procedure accounts for the possibility of changes, during testing, in reliability or <b>failure</b> <b>intensity</b> objective, the checkpoints, and the parameters that characterize the STP. The effectiveness of this procedure was studied using commercial data available in the public domain and also from the data generated through simulation. In all cases the use of feedback control produces adequate results allowing the achievement of the objectives. ...|$|E
40|$|Abstract. Development of {{the best}} {{economic}} parameter inspection program of fatigue-prone aircraft under limitation of airline <b>failure</b> <b>intensity</b> is discussed. Calculation of aircraft failure probability, airline <b>failure</b> <b>intensity</b> and airline gain are considered using Markov Chains and Semi-Markov process theory for the case when parameters of exponential fatigue crack growth are known. For the case {{when they are not}} known the minimax approach is offered if there are results of acceptance full-scale fatigue test and conditions of acceptance are known. Numerical examples are given...|$|E
40|$|International audienceAn {{imperfect}} maintenance {{model for}} repairable systems is considered. Corrective Maintenances (CM) {{are assumed to}} be minimal, i. e. As Bad As Old (ABAO). They are considered to be left and right censored. Preventive Maintenances (PM) {{are assumed to be}} done at predetermined planned times and to follow a Brown-Proschan (BP) model, i. e. they renew (As Good As New, AGAN) the system with probability p, and they are minimal (ABAO) with probability 1 -p. BP PM effects (AGAN or ABAO) are assumed to be not observed. In this context, different methods (maximum likelihood, moment estimation, and expectation-maximization algorithm) are considered in order to estimate jointly the PM efficiency parameter p and the parameters of the first time to failure distribution corresponding to the new unmaintained system. A method to individually assess the effect of each PM is also proposed. Finally, some reliability characteristics are computed: failure and cumulative <b>failure</b> <b>intensities,</b> reliability and expected cumulative number of failures. All the corresponding algorithms are detailed and applied to a real maintenance data set from an electricity power plant...|$|R
40|$|Reliability is an {{important}} aspect of product perception and manufacturers are compelled to take corrective actions on the items failing within the warranty period. Automotive manufacturers are being exposed to significant operating costs as a result of warranty claims affecting an individual unit or mandatory (sometimes voluntary) recalls affecting a batch. Underlying principles of warranty modeling are built by considering both subjective issues and objective constraints such as competition, quality, and performance under the goal of achieving desired levels of reliability and cost in a balanced manner. This paper reviews the warranty cost models with an emphasis on the failure analysis of used vehicles. Expected warranty costs are calculated by taking into account the age, usage, and maintenance data of the product in question. <b>Failure</b> <b>intensities</b> and characteristics are identified in order to propose a policy that highlights the trade-off between the cost and the warranty length. A case study on a popular brand's initiation of factory certified pre-owned program for the local automobile market of Turkey is presented in detail. (C) 2011 Elsevier Ltd. All rights reserved. Publisher's VersionAuthor Post Prin...|$|R
40|$|The {{purpose of}} this work is to develop {{statistical}} methods for using degradation measure to estimate a survival function for a linear degradation model. In this paper, we review existing methods and then describe a parametric approach. We focus on estimating the survival function. A simulation study is conducted to evaluate {{the performance of the}} estimating method and the method is illustrated using real data. degradation model, <b>failure</b> time, <b>intensity</b> function, survival function, parametric method,...|$|R
40|$|In recent years, {{there has}} been a growing trend to out-source service {{operations}} in which the equipment maintenance is carried out by an external agent rather than in-house. Often, the agent (service provider) offers more than one option and the owners of equipment (customers) are faced to the problem of selecting the optimal option, under the terms of a contract. In the current work, we develop a model and report results to determine the agent's optimal strategy for a given type of contract. The model derives in a non-cooperative game formulation in which the decisions are taken by maximizing expected profits. This work extends previous models by considering the realistic case of equipments having an increasing <b>failure</b> <b>intensity</b> due to imperfect maintenance, instead of the standard assumption that considers failure times are exponentially distributed (constant <b>failure</b> <b>intensity).</b> We develop a model using a linear function of time to characterize the <b>failure</b> <b>intensity.</b> The main goal, for the agent, is to determine the pricing structure in the contract and the number of customers to service. On the other hand, for the clients, the main goal is to define the period between planned actions for preventive maintenance and the time to replace equipments. In order to give a complete characterization of the results, we also carry out a sensitivity analysis over some of the factors that would influence over the <b>failure</b> <b>intensity.</b> ...|$|E
40|$|The basic {{repair rate}} models for {{repairable}} systems may be homogeneous Poisson processes, renewal processes or nonhomogeneous Poisson processes. In {{addition to these}} models, geometric processes are studied occasionally. Geometric processes, however, can only model systems with monotonously changing (increasing, decreasing or constant) <b>failure</b> <b>intensity.</b> This paper deals with the reliability modelling of the failure process of repairable systems when the <b>failure</b> <b>intensity</b> shows a bathtub type non-monotonic behaviour. A new stochastic process, an extended Poisson process, is introduced. Reliability indices and parameter estimation are presented. A comparison of this model with other repair models based on a dataset is made...|$|E
40|$|A system (machine) is {{observed}} {{to operate in}} one of two modes. The most common mode is loaded (or regular) operation. Occasionally the system is placed in an unloaded state. In this latter state, while the system is mechanically still operating, it is assumed that the <b>failure</b> <b>intensity</b> is reduced due to this reduction in operating intensity. To capture this potential reduction in <b>failure</b> <b>intensity</b> due to switching operating modes, a proportional hazards framework is utilized. In either operating condition, maintenance records analyzed indicated that the system was occasionally. (orig.) Available from TIB Hannover: RR 4487 (1998, 39) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|Abstract—Core {{backbone}} networks must {{be designed}} to guarantee high levels of availability. Any interruption in the services that they provide may have massive consequences. For this reason {{there is a huge}} interest in developing methods able to keep the network robustness in the desired level. For the design of these methods are used models that need input information such as the operational state of network components which are stochastic variables. The aim {{of this paper is to}} provide an insight into the core networks behavior based on real operational data in order to help future related works to take more realistic assumptions. Based on failure logs provided by UNINETT we analyze availability levels and <b>failure</b> <b>intensities</b> in routers and links. We show that links may be classified in three groups with different properties. Additionally we observe that some links have similar dependability features than routers, making the perfect node assumption used on many related studies not correct. Finally, there were used parametrization techniques in order to fit the empirical processes with well-known distributions. We observe that the Weibull assumption that is traditionally used to model link failures processes fits properly the behavior of routers and short distance links but for the case of long distance fibers the gamma distribution seems to fit better. I...|$|R
40|$|The {{objective}} {{of this paper is}} to test whether network failures will have serious economic consequences for participants in the newly developing bandwidth markets. We measure economic consequences by looking at changes in bandwidth prices, in value-at-risk (VAR) and in conditional-value-at-risk (CVAR). Bandwidth markets may be particularly sensitive to network failures because bandwidth is a non-storable commodity and because alternative paths with equivalent quality of service (QoS) are perfect substitutes. Thus a local failure can affect alternative equivalent paths and this in turn can have a knock-on effect. We found that for a realistic large-scale market topology if there are, say, four failures per link per year, half of which are long enough to affect the market, then VAR is increased by 30 %; and CVAR by 40 %. This is true even with a spike size (× 3) that is modest compared to observations in electricity markets (× 10 to × 100). For participants with capacity positions in such a market these consequences are likely to be serious. Thus if failures occur at this rate their consequence must be included in planning. Even at low <b>failure</b> <b>intensities,</b> the network itself acts as a significant amplifier and thus cannot be neglected. This amplification is itself a true emergent phenomenon of a large-scale bandwidth market...|$|R
40|$|We {{develop and}} {{evaluate}} {{a method to}} estimate the frequency of an asset failure when experience event data are available for multiple assets of a given type. Typically such data contains information on assets of mixed ages, operating in different environments {{and in many cases}} the number of recorded events might be few or even zero. Assets are known to have slow age-related degradation and we have access to engineering experts who are able to provide judgment about the degradation rates. An empirical Bayes method is developed to allow us to estimate the failure rates for an asset on a particular site by using the available observational data pool together with structured engineering judgment of the degradation rate for the asset type. Our method aims to address the challenges of asset pool heterogeneity and environmental conditions across sites. We describe our practical motivation which is informed by a real problem facing a water utility. We explain the principles and mathematics underpinning the new methods, before describing a simulation based evaluation of their accuracy. We show that the empirical Bayes methods provide accurate estimates of the <b>failure</b> <b>intensities</b> for a range of parameters considered in this controlled study and that empirical Bayes estimators can compensate for bias in initial judgmental assessments of degradation rates. We discuss how the method can be applied in the industry context...|$|R
40|$|The failure {{processes}} of repairable systems may be impacted by operational and environmental stress factors. To accommodate such factors, reliability can be modelled using a multiplicative intensity function. In the proportional intensity model, the <b>failure</b> <b>intensity</b> {{is the product}} of the <b>failure</b> <b>intensity</b> function of the baseline system that quantifies intrinsic factors and a function of covariates that quantifies extrinsic factors. The existing literature has extensively studied the failure {{processes of}} repairable systems using general repair concepts such as age-reduction when no covariate effects are considered. This paper investigates different approaches for modelling the failure and repair process of repairable systems in the presence of time-dependent covariates. We derive statistical properties of the failure processes for such systems...|$|E
40|$|One of {{the most}} {{controversial}} techniques in the field of reliability is reliability-prediction methods based on component constant-failure-rate data for the estimation of system failure rates. This paper investigates a new reliability-estimation method that does not depend upon constant failure rates. Many boards were selected from the Loughborough University field-reliability database, and their reliability was estimated using failure-intensity based methods and then compared with the actual <b>failure</b> <b>intensity</b> observed in the field. The predicted failure-intensity closely agrees with the observed value for the majority of a system operating lifetimes. The general <b>failure</b> <b>intensity</b> method lends itself very easily to system-reliability prediction. It appears to give an estimate of the system-reliability throughout the operating lifetime of the equipment and does not make assumptions, such as constant failure rate, which can be detrimental to the validity of the estimate. The predictions seem, on present evidence, to track the observed behavior well, given the uncertainties that are evident in the field. The <b>failure</b> <b>intensity</b> method should be investigated further to see if it is feasible to estimate the system reliability throughout its lifetime and hence provide a more realistic picture {{of the way in which}} electronic systems behave in the field...|$|E
40|$|Predictive {{modelling}} in the ®eld of dependability, such as repair cost modelling, {{is usually}} {{based on the}} <b>failure</b> <b>intensity</b> function of repairable systems. The experience feedback data needed to estimate the <b>failure</b> <b>intensity</b> function is, however, seldom available due to the unique character of the systems. Thus approaches that can capture the tacit knowledge of designers, operators and maintenance personnel and transform this to the mathematical format required in the predictive models, are needed. A Bayesian statistical approach is presented, which yields posterior distributions of {{the parameters of the}} Power Law and the Log-Linear intensity functions, which are used to model the trend in the data observed. Model-checking criteria are presented according to which the relative performance of the selected functions can b...|$|E
40|$|A {{quantitative}} {{theory of}} all {{details of the}} low intensity sequence effect is given {{on the basis of}} the following assumptions. A monatomic Ag speck is unstable against thermal agitation, a diatomic speck is stable, but for development a speck of more than two atoms is required. The theory is based on that for low <b>intensity</b> <b>failure</b> as developed in paper I...|$|R
40|$|The {{correlation}} between motivational intensity and language learning has been explored to gain insights into why {{the learning of}} English has been poor in China. Hitherto, the learning of English as a foreign language in China has been taken for granted; unless effort is expended a measure of motivational <b>intensity</b> <b>failure</b> ensues. Success will only be achieved if learners {{understand the importance of}} learning English.  </p...|$|R
40|$|Reliability Analysis of nonrestorable {{redundant}} power Systems {{of industrial}} plants and other consumers of electric energy was carried out. The main attention {{was paid to}} numbers failures influence, caused by failures of all elements of System due to one general reason. Noted the main possible reasons of common failures formation. Two main indicators of reliability of non-restorable systems are considered: average time of no-failure operation and mean probability of no-failure operation. Modeling of failures were carried out by mean of division of investigated system into two in-series connected subsystems, one of them indicated independent failures, but the other indicated common failures. Due to joined modeling of single and common <b>failures</b> resulting <b>intensity</b> of <b>failures</b> is the amount incompatible components: <b>intensity</b> statistically independent <b>failures</b> and <b>intensity</b> of common <b>failures</b> of elements and system in total. It is shown the influence of common failures of elements on average time of no-failure operation of system. There is built the scale of preference of systems according to criterion of  average time maximum of no-failure operation, depending on portion of common failures. It is noticed that such common failures don’t influence on the scale of preference, but  change intervals of time, determining the moments of systems failures and excepting them from the number of comparators. There were discussed two problems  of conditionally optimization of  systems’  reservation choice, taking into account their reliability and cost. The first problem is solved due to criterion of minimum cost of system providing mean probability of no-failure operation, the second problem is solved due to criterion of maximum of mean probability of no-failure operation with cost limitation of system...|$|R
40|$|Currently, manufacturers/dealers {{have started}} selling {{products}} with lifetime warranty policies. In this paper, risk attitudes of both buyers and manufacturer/dealers to lifetime warranty policies are discussed. In line with Chun and Tang (1995) [2], Risk models are developed for products with time dependent <b>failure</b> <b>intensity</b> (rate). These {{models have been}} proposed with Non homogeneous Poisson’s process for <b>failure</b> <b>intensity</b> function, a constant repair cost, and concave utility function. Using the exponential utility function, the decision models are developed to maximise the manufacturer/dealer’s certainty profit equivalent. Risk preference models are developed to find the optimal warranty price {{through the use of}} the manufacturer’s utility function for profit and the buyer’s utility function for repair costs. Finally, the sensitivity of the warranty price models is analysed using numerical examples...|$|E
40|$|This article {{presents}} examples of using Fisher's Chi-square minimisation method for verifying time-dependent reliability model validity, formulating parameters and evaluating uncertainties. The study {{was performed by}} State Technical University for Nuclear Power Engineering and EC JRC Institute for Energy, {{within the context of}} Ageing PSA Network activities. Sometimes exact times of failure are not known, only an interval of time within which the number of failures are recorded. The ordered by component age sequence of intervals with the corresponding number of failures and the accumulated exposure time during the bin(interval) constitutes the binned data (also known as readout data). For the continuous <b>failure</b> <b>intensity</b> function, it was proposed to apply the following statistical models: 1. Constant failure intensity;					 2. Linear failure intensity; 3. Log-linear or exponential failure intensity; 4. Power-low (Weibull) <b>failure</b> <b>intensity</b> mode For models 2 - 4, the positive shape parameter means a positive trend in time, i. e. the component <b>failure</b> <b>intensity</b> increases with the age of the component. The following tasks were performed: - verification of model validity, - parameters estimation, - characterisation of uncertainties for estimated parameters and whole model, - assessment of possible extrapolation and uncertainties of extrapolation. The developed approach is illustrated with an example of data from operating experience of one of the pump types installed on NPPs. Proposed approach made it possible to identify the component groups with increasing <b>failure</b> <b>intensity</b> and to choose best fitted reliability model. The impact of the choice of model on extrapolation results were analyzed. Detailed results of the study are presented in JRC report EUR 23079 EN. Keywords: ageing of systems, structures and components, Probabilistic Safety Assessment (PSA), reliability models, ageing trend analysis. JRC. F. 5 -Safety of present nuclear reactor...|$|E
40|$|Abstract: The failure {{pattern of}} {{repairable}} mechanical equipment subject to deterioration phenomena sometimes shows a finite {{bound for the}} increasing <b>failure</b> <b>intensity.</b> A non-homogeneous Poisson process with bounded increasing <b>failure</b> <b>intensity</b> is then illustrated and its characteristics are discussed. A Bayesian procedure, based on prior information on model-free quantities, is developed {{in order to allow}} technical information on the failure process to be incorporated into the inferential procedure and to improve the inference accuracy. Posterior estimation of the model-free quantities and of other quantities of interest (such as the optimal replacement interval) is provided, as well as prediction on the waiting time to the next failure and on the number of failures in a future time interval is given. Finally, numerical examples are given to illustrate the proposed inferential procedure...|$|E
40|$|In this paper, {{acoustic}} emission (AE) monitoring {{was used}} to evaluate the bond behaviour of corroded reinforcement in reinforced concrete prism samples. The embedded reinforcing steel bars were pre-corroded to successive levels of corrosion up to 5 % mass loss of steel by impressed current accelerated corrosion process. A total of 24 samples with variable bar embedded length (50 and 200 mm) and corrosion exposure (0 %, 1 %, 2 %, 3 %, 4 %, and 5 % of steel mass loss) were tested under pull-out tests. Two attached AE sensors were mounted at the surface of each specimen to continuously record the emitted AE signals throughout the pull-out tests. Several AE signal parameters (cumulative number of hits, signal strength, amplitude, and duration) were analyzed to characterize the extent of bond deterioration. These AE parameters were compared with the corresponding values of bond stress and free end slip until <b>failure.</b> Furthermore, an <b>intensity</b> analysis on AE signal strength was implemented for the condition assessment of bond integrity, which yielded two additional AE parameters: historic index (H (t)) and severity (Sr). The analysis of cumulative signal strength, H (t), and Sr results permitted the identification of two stages of bond degradation (micro- and macro-cracking) before bond splitting <b>failure.</b> Besides, <b>intensity</b> classification charts were developed using H (t) and Sr to quantify the bond damage (micro- and macro-cracking and bar slip) of existing concrete structures...|$|R
40|$|We {{develop a}} general {{statistical}} procedure to obtain linearly the best efficient estimate of existing estimations of the parameter of a probability process. This procedure {{is used to}} obtain the best efficient estimates of the shape parameter, the <b>intensity</b> <b>failure</b> function and its reciprocal of the power law process. These estimates are important {{in the study of}} reliability growth modelling. The effectiveness of our findings is illustrated analytically and numerically, using real data and numerical simulations. ...|$|R
40|$|We {{present a}} {{computational}} model and simulation {{results on the}} dynamics of local link failures in markets with network structure. Bandwidth markets are inherently networked, so we focus on telecommunications here. The objective {{of this paper is}} to test whether or not network failures will have serious economic consequences. We measure economic consequences by looking at changes in expected bandwidth prices, changes in value-at-risk (VAR) and in conditional-value-at-risk (CVAR). Bandwidth markets may be particularly sensitive to network failures because bandwidth is a non-storable commodity. On the other hand alternative paths with equivalent quality of service (QoS) are perfect substitutes so this may limit sensitivity. Non-storability has contributed to enormous volatility in deregulated electricity prices and observations of enormous price spikes. Bandwidth is a true network commodity in that links in the network itself are the traded commodities. Thus a local failure can affect alternative equivalent paths and this can have a knock-on effect in turn. We used a spot market model incorporating non-storability and alternative path selection on price grounds and limited by QoS-equivalence. Spike models are incorporated based on empirical data. We found that for a realistic large-scale market topology if there are, say, four failures per link per year, half of which are long enough to affect the market, then: expected link prices are increased 12 %; VAR is increased by 30 %; and CVAR by 40 %. This is even with a spike size (× 3) that is modest compared to observations in electricity markets (× 10 –× 100). For market participants with capacity positions in such a market these consequences are likely to be serious. Thus if failures occur at this rate their consequence must be included in planning. Furthermore, whilst at low <b>failure</b> <b>intensities</b> the network acts as a dampening factor, at higher intensities it acts as an amplifier and thus cannot be neglected. We believe this amplification to be an emergent phenomenon of any market with network structure, although clearly more important for markets with no storage...|$|R
