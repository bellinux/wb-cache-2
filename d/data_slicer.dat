7|400|Public
5000|$|The reverse {{remote control}} channel is usually fixed at 433.92 MHz, using {{whatever}} modulation {{is on the}} 34 kHz to 45 kHz IR remote [...] "carrier". ASK/OOK schemes such as RC5 and RC6 work best over the RF link as the receiver uses a <b>data</b> <b>slicer</b> and AGC designed for ASK/OOK with Manchester encoding.|$|E
50|$|The {{first stage}} in the {{processing}} chain for the analog RF signal is digitizing it. Using various circuits like a simple comparator or a <b>data</b> <b>slicer,</b> the analog signal becomes a chain of two binary digital values, 1 and 0. This signal carries all the information in a CD and is modulated using a system called EFM (Eight-to-fourteen modulation). The second stage is demodulating the EFM signal into a data frame that contains the audio samples, error correction parity bits, according with the CIRC error correction code, and control data for the player display and micro-computer.The EFM demodulator also decodes part of the CD signal and routes it to the proper circuits, separating audio, parity and control (subcode) data.|$|E
50|$|The {{terminal}} {{was based}} upon two Motorola 68000 processors as a general main computer and control and the second as a video processor interfaced to VRAM video memory with shift register to output. A further processor based upon Texas Instruments TMS7000 as the control for the field field teletext, RF tuning control and packet decoding for 8 bit data transfer to the main processor {{as well as a}} Micronas MAS2122 Modem chipset for the telephone interface. RF Tuning was achieved by Telefunken multiband tuner, voltage tuned via SP5000 with the channel selection supplied by the TMS7000. This tuner with agc IF detection provides the composite video to the <b>data</b> <b>slicer</b> (TMS3556) and data detector/converter (TMS3551)under control from the TMS7000 (TMS77C82 mask programmed version).Further more the main processor operated as a computer running Metacomco BASIC software with keyboard entry via an infrared coupled standard qwerty keyboard with additional keys to select videotext, teletext or PC modes.|$|E
40|$|Dynamic slicing {{algorithms}} {{are used}} to narrow {{the attention of the}} user or an algorithm to a relevant subset of executed program statements. Although dynamic slicing was first introduced to aid in user level debugging, increasingly applications aimed at improving software quality, reliability, security, and performance are finding opportunities to make automated use of dynamic slicing. In this paper we present the design and evaluation of three precise dynamic <b>data</b> <b>slicing</b> algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic <b>data</b> <b>slices.</b> Our experiments show that the LP algorithm is a fast and practical precise <b>data</b> <b>slicing</b> algorithm. In fact we show that while precise <b>data</b> <b>slices</b> can be orders of magnitude smaller than imprecise dynamic <b>data</b> <b>slices,</b> for small number of <b>data</b> <b>slicing</b> requests, the LP algorithm is faster than an imprecise dynamic <b>data</b> <b>slicing</b> algorithm proposed by Agrawal and Horgan...|$|R
40|$|Abstract. In this paper, {{we present}} a formal {{description}} of <b>data</b> <b>slicing,</b> which is a type-directed program transformation technique that separates a program’s heap into several independent regions. Pointers within each region mirror the structure of pointers in the original heap; however, each field whose type is a base type (e. g., the integer type) appears in {{only one of these}} regions. In addition, we discuss several applications of <b>data</b> <b>slicing.</b> First, <b>data</b> <b>slicing</b> can be used to add extra fields to existing data structures without compromising backward compatibility; the CCured project uses <b>data</b> <b>slicing</b> to preserve library compatibility in instrumented programs at a reasonable performance cost. <b>Data</b> <b>slicing</b> {{can also be used to}} improve locality by separating “hot ” and “cold ” fields in an array of data structures, and it can be used to protect sensitive data by separating “public ” and “private ” fields. Finally, <b>data</b> <b>slicing</b> can serve as a refactoring tool, allowing the programmer to split data structures while automatically updating the code that manipulates them. ...|$|R
40|$|Many {{techniques}} {{have been designed}} for privacy preserving and micro data publishing, such as generalization and bucketization. Several works showed that generalization loses some amount of information especially for high dimensional data. So it’s not efficient for high dimensional data. In case of Bucketization, it does not prevents membership disclosure and also does not applicable for data {{that do not have}} a clear separation between Quasi-identifying attributes and sensitive attributes. In this paper, we presenting an innovative technique called <b>data</b> <b>slicing</b> which partitions the data. An efficient algorithm is developed for computing <b>sliced</b> <b>data</b> that obeys l-diversity requirement. we also show how <b>data</b> <b>slicing</b> is better than generalization and bucketization. <b>Data</b> <b>slicing</b> preserves better utility than generalization and also does not requires clear separation between Quasi-identifying and sensitive attributes. <b>Data</b> <b>slicing</b> is also used to prevent attribute disclosure and develop an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obeys l-diversity requirement. Experimental results confirm that <b>data</b> <b>slicing</b> preserves <b>data</b> utility than generalization and more effective than bucketization involving sensitive attributes. Experimental results demonstrate the effectiveness of this method...|$|R
40|$|Worldwide NTSC/PAL/SECAM color {{demodulation}} support One 10 -bit {{analog-to-digital converter}} (ADC), 4 × oversampling per channel for CVBS, Y/C, and YPrPb modes 6 analog video input channels with on-chip antialiasing filter Video input support for CVBS (composite), Y/C (S-Video), and YPrPb (component) Fully differential, pseudo differential, and single-ended CVBS video input support NTSC/PAL/SECAM autodetection Short-to-battery (STB) diagnostics on 2 video inputs Up to 4 V common-mode input range solution Excellent common-mode noise rejection capabilities 5 -line adaptive 2 D comb filter and CTI video enhancement Adaptive Digital Line Length Tracking (ADLLT), signal processing, and enhanced FIFO management provide mini-time base correction (TBC) functionality Integrated {{automatic gain control}} (AGC) with adaptive peak white mode Fast switching capability Integrated interlaced-to-progressive (I 2 P) video output converter Adaptive contrast enhancement (ACE) Down dither (8 -bit to 6 -bit) Rovi (Macrovision) copy protection detection MIPI CSI- 2 output interface Full featured vertical blanking interval (VBI) <b>data</b> <b>slicer</b> with world system teletext (WST) support Power-down mode available 2 -wire, I 2 C-compatible serial interface Qualified for automotive applications − 40 °C to + 105 °C temperature grad...|$|E
40|$|Worldwide NTSC/PAL/SECAM color {{demodulation}} support One 10 -bit {{analog-to-digital converter}} (ADC), 4 × oversampling per channel for CVBS, Y/C, and YPrPb modes Analog video input channels with on-chip antialiasing filter ADV 7281 -M: up to 6 input channels ADV 7281 -MA: up to 8 input channels Video input support for CVBS (composite), Y/C (S-Video), and YPrPb (component) Fully differential, pseudo differential, and single-ended CVBS video input support NTSC/PAL/SECAM autodetection Short-to-battery (STB) diagnostics on 2 video inputs (ADV 7281 -M only) Up to 4 V common-mode input range solution Excellent common-mode noise rejection capabilities 5 -line adaptive 2 D comb filter and CTI video enhancement Adaptive Digital Line Length Tracking (ADLLT), signal processing, and enhanced FIFO management provide mini-time base correction (TBC) functionality Integrated {{automatic gain control}} (AGC) with adaptive peak white mode Fast switching capability Adaptive contrast enhancement (ACE) Down dither (8 -bit to 6 -bit) Rovi (Macrovision) copy protection detection MIPI CSI- 2 output interface Full featured vertical blanking interval (VBI) <b>data</b> <b>slicer</b> Power-down mode available 2 -wire, I 2 C-compatible serial interface Qualified for automotive applications − 40 °C to + 105 °C temperature grad...|$|E
40|$|Abstract—With the {{popularity}} of mobile wireless devices equipped with various kinds of sensing abilities, a new service paradigm named participatory sensing has emerged to provide users with brand new life experience. However, the wide application of participatory sensing has its own challenges, among which privacy and multimedia data quality preservations are two critical problems. Unfortunately, none of the existing work has fully solved the problem of privacy and quality preserving participatory sensing with multimedia data. In this paper, we propose SLICER, {{which is the first}} k-anonymous privacy preserving scheme for participatory sensing with multimedia <b>data.</b> <b>SLICER</b> integrates a data coding technique and message transfer strategies, to achieve strong protection of participants ’ privacy, while maintaining high data quality. Specifically, we study two kinds of data transfer strategies, namely transfer on meet up (TMU) and minimal cost transfer (MCT). For MCT, we propose two different but complimentary algorithms, including an approximation algorithm and a heuristic algorithm, subject to different strengths of the requirement. Furthermore, we have implemented SLICER and evaluated its performance using publicly released taxi traces. Our evaluation results show that SLICER achieves high data quality, with low computation and communication overhead...|$|E
40|$|We {{examine the}} {{functional}} cohesion of procedures using a <b>data</b> <b>slice</b> abstraction. Our analysis identifies the data tokens that lie {{on more than}} one slice as the "glue" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie {{on more than one}} <b>data</b> <b>slice,</b> and super-glue tokens, tokens that lie on all <b>data</b> <b>slices</b> in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations...|$|R
40|$|In visual {{exploration}} {{and analysis of}} data, determining how to select and transform the data for visualization is a challenge for data-unfamiliar or inexperienced users. Our main hypothesis is that for many data sets and common analysis tasks, there are relatively few "data slices" that result in effective visualizations. By focusing human users on appropriate and suitably transformed parts of the underlying data sets, these <b>data</b> <b>slices</b> can help the users carry their task to correct completion. To verify this hypothesis, we develop a framework that permits us to capture exemplary <b>data</b> <b>slices</b> for a user task, and to explore and parse visual-exploration sequences into a format that makes them distinct and easy to compare. We develop a recommendation system, DataSlicer, that matches a "currently viewed" <b>data</b> <b>slice</b> with the most promising "next effective" <b>data</b> <b>slices</b> for the given exploration task. We report the results of controlled experiments with an implementation of the DataSlicer system, using four common analytical task types. The experiments demonstrate statistically significant improvements in accuracy and exploration speed versus users without access to our system...|$|R
40|$|A method, apparatus, system, {{article of}} manufacture, and {{computer}} readable storage medium provide {{the ability to}} measure wind. Data at a first resolution (i. e., low resolution data) is collected by a satellite scatterometer. Thin <b>slices</b> of the <b>data</b> are determined. A collocation of the <b>data</b> <b>slices</b> are determined at each grid cell center to obtain ensembles of collocated <b>data</b> <b>slices.</b> Each ensemble of collocated <b>data</b> <b>slices</b> is decomposed into a mean part and a fluctuating part. The data is reconstructed at a second resolution from the mean part and a residue of the fluctuating part. A wind measurement is determined from the data at the second resolution using a wind model function. A description of the wind measurement is output...|$|R
40|$|Worldwide NTSC/PAL/SECAM color {{demodulation}} {{support with}} autodetection One 10 -bit ADC, 4 × oversampling for CVBS, Y/C, and YPbPr 8 analog video input channels with on-chip antialiasing filter Fully differential, pseudo differential, and single-ended CVBS video input support STB diagnostics on differential video inputs CVBS (composite), Y/C (S-Video), and YPbPr (component) video input support Fast switching capability between analog inputs Adaptive contrast enhancement (ACE) Excellent common-mode noise rejection capabilities Rovi (Macrovision) copy protection detection Up to 4 V common-mode input range solution Vertical blanking interval (VBI) <b>data</b> <b>slicer</b> High-Definition Multimedia Interface (HDMI) capable receiver HDCP authentication and decryption support 162 MHz maximum pixel clock frequency, allowing HDTV formats up to 1080 p and display resolutions up to UXGA (1600 × 1200 at 60 Hz) HDCP repeater support, up to 25 KSVs supported Integrated CEC controller, CEC 1. 4 compatible Adaptive TMDS equalizer 5 V detect and Hot Plug assert Component video processor Any-to-any 3 × 3 color space conversion (CSC) matrix Contrast/brightness/hue/saturation video adjustment Timing adjustments controls for horizontal sync (HS) /vertical sync (VS) /data enable (DE) timing Video mute function Serial digital audio output interface HDMI audio extraction support Advanced audio muting feature I 2 S-compatible, left justified and right justified audio output modes 8 -channel TDM output mode available 2 Mobile Industry Processor Interface (MIPI) Camera Serial Interface 2 (CSI- 2) transmitters 4 -lane transmitter with 4 lanes, 2 lanes, and 1 lane muxing options for HDMI/SDP/digital input port sources 1 -lane transmitter for standard definition processor (SDP) sources 8 -bit digital input/output port General 2 -wire serial microprocessor unit (MPU) interface (I 2 C compatible) − 40 °C to + 85 °C temperature grade 100 -ball, 9 mm × 9 mm, RoHS-compliant CSP_BGA package Qualified for automotive application...|$|E
40|$|It is {{difficult}} to debug a program when the data set that causes it to fail is large (or voluminous). The cues that may help in locating the fault are obscured by {{the large amount of}} information that is generated from processing the data set. Clearly, a smaller data set which exhibits the same failure should lead to the diagnosis of the fault more quickly than the initial, large data set. We term such a smaller data set a <b>data</b> <b>slice</b> and the process of creating it <b>data</b> <b>slicing.</b> The problem of creating a <b>data</b> <b>slice</b> is undecidable. In this paper, we investigate four generateand-test heuristics for deriving a smaller data set that reproduces the failure exhibited by a large data set. The four heuristics are: invariance analysis, origin tracking, random elimination, and programspecific heuristics. We also provide a classification of programs based upon a certain relationship between its input and output. This classification may be used to choose an appropriate heuristic in a given debugging scenario. As evidenced from a database of debugging anecdotes at the Open University, U. K., debugging failures exhibited by large data sets require inordinate amounts of time. Our <b>data</b> <b>slicing</b> techniques would significantly reduce the effort required in such scenarios. ...|$|R
40|$|We {{examine the}} {{functional}} cohesion of procedures using a <b>data</b> <b>slice</b> abstraction. Our analysis identifies the data tokens that lie {{on more than}} one slice as the "glue" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie {{on more than one}} <b>data</b> <b>slice,</b> and super-glue tokens, tokens that lie on all <b>data</b> <b>slices</b> in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations and composition operators. Index terms [...] - software metrics, cohesion, program slices, measurement theory 1 Introduction Cohesion is an attribute of a software unit or module that refers to the "relatedness" of module components. A highly cohesive software module is a module that has one basic function and is indivisible [...] - it is difficult to split a cohesive module into separate components. Module cohesion can be classified using an ordinal scale that incl [...] ...|$|R
40|$|The use of 3 dviewnix, a {{three-dimensional}} medical imaging program, is explored for {{the display of}} <b>data</b> <b>slices</b> of a human male provided by the National Library of Medicine. The data underwent a series of conversions, including changing the data from RGB color to greyscale, {{in order to be}} compatible with 3 dviewnix. Once the data is converted, 3 dviewnix can be used to create three-dimensional reconstructions of the <b>data</b> <b>slices,</b> as well as extracting the skeletal structure from the reconstructions. The <b>data</b> <b>slices,</b> which are in one millimeter increments across the body, do not always convey much information. A program was designed to alleviate this by making vertical slices instead of horizontal. This program required a small amount of pre-processing because the slices were surrounded by a background of blue frozen gelatin that had to be removed. The goal of the project is to make the slices, reconstructions, and the extracted skeleton available via the World Wide Web for educational purpo [...] ...|$|R
40|$|Abstract- Cloud {{computing}} {{has become}} a new platform for personal computing. However, while designing the strategy of data placement, there still lacks the consideration of systematic diversity of distributed transaction costs. This paper proposes the use of genetic algorithms to address the data placement problem in cloud computing. This strategy has adequately considered the correlation between <b>data</b> <b>slices</b> to minimize {{the total cost of}} distributed transactions. Compared to other methods, genetic algorithms have proven to comprehensively consider the correlation between the <b>data</b> <b>slices</b> in cloud computing, therefore greatly reducing the amount and cost of distributed transactions...|$|R
30|$|<b>Data</b> must be <b>sliced</b> into {{segments}} {{small enough}} such that each segment bears no meaningful information to malicious entities. With <b>data</b> <b>sliced</b> in this way, malicious entities {{may be able}} to access an individual data segment, but the access to the data segment should not compromise the confidentiality of the data as a whole.|$|R
40|$|In {{this paper}} we propose a new {{compression}} algorithm geared to reduce the time needed to test scan-based designs. Our scheme compresses the test vector set by encoding the bits that need to be flipped in the current test <b>data</b> <b>slice</b> in order to obtain the mutated subsequent test <b>data</b> <b>slice.</b> Exploitation of the overlap in the encoded data by effective traversal search algorithms results in drastic overall compression. The technique we propose can be utilized as not only a stand-alone technique but also can be utilized on test data already compressed, extracting even further compression. The performance of the algorithm is mathematically analyzed and its merits experimentally confirmed on the larger examples of the ISCAS' 89 benchmark circuits...|$|R
40|$|FIGURES 8 – 21. CT reconstructions of Balticoroma wheateri {{new species}} (male holotype, GPIH). (8) frontal view showing chelicerae and labral spur; (9) view of right {{pedipalp}} showing embolus; (10 – 14) various views of right metatarsus 1, showing y-shaped clasping structure; (15) anterior view of specimen showing the section taken through the chelicerae {{to produce the}} raw <b>data</b> <b>slice</b> in Figure 16; (16) raw <b>data</b> <b>slice</b> demonstrating that the chelicerae and clypeal extentions are clearly separated; (20 – 21) various views of the right pedipalp. C, chelicera; ce, clypeal extension; co, dorsal cymbial outgrowth; cy, cymbium; e, embolus; eb, embolic base; ec, embolic coil;? fc, functional conductor sensu Wunderlich (2004); ls, labral spur; t, tegulum...|$|R
40|$|Digital {{television}} {{terrestrial broadcasting}} (DTTB) networks {{can help to}} alleviate the congestion problem in cellular networks by delivering rich contents to {{a large number of}} clients simultaneously. In particular, recently, there is a strong interest of extending current DTTB systems to support multimedia broadcasting services. The lack of return channel and long transmission time interval however impose great challenge to the resource allocation for this application in DTTB networks. The reliable resource allocation is studied for multi-services with data delivery delay constraints in the second generation digital video broadcasting terrestrial (DVB-T 2) system. To solve this challenging problem, the data cells of a T 2 -frame are divided into <b>data</b> <b>slices</b> which are indexed by binary numbers. These <b>data</b> <b>slices</b> are organised in a binary tree, and each node in the tree is associated with a certain number of non-adjacent <b>data</b> <b>slices.</b> Then a node can be allocated to a service by using the predefined policies. Based on this scheme, this study proposes a heuristic algorithm to allocate resources to multi-services. Simulation results validate the effectiveness of the proposed algorithm and demonstrate its advantage over the current resource allocation scheme in DVB-T 2 networks...|$|R
40|$|Backward slicing {{has been}} used {{extensively}} in program understanding, debugging and scaling up of program analysis. For large programs, {{the size of the}} conventional backward slice is about 25 % of the program size. This may be too large to be useful. Our investigations reveal that in general, the size of a slice is influenced more by computations governing the control flow reaching the slicing criterion than by the computations governing the values relevant to the slicing criterion. We distinguish between the two by defining <b>data</b> <b>slices</b> and control slices both of which are smaller than the conventional slices which can be obtained by combining the two. This is useful because for many applications, the individual <b>data</b> or control <b>slices</b> are sufficient. Our experiments show that for more than 50 % of cases, the <b>data</b> <b>slice</b> is smaller than 10 % of the program in size. Besides, the time to compute <b>data</b> or control <b>slice</b> is comparable to that for computing the conventional slice. Comment: 10 pages, 5 figures, two algorithm...|$|R
40|$|This paper {{presents}} {{a new technique}} for <b>data</b> <b>slicing</b> of distributed programs running on a hierarchy of machines. <b>Data</b> <b>slicing</b> can be realized as a program transformation that partitions heaps of machines in a hierarchy into independent regions. Inside each region of each machine, pointers preserve the original pointer structures in the original heap hierarchy. Each heap component of the base type (e. g., the integer type) goes only to a region {{of one of the}} heaps. The proposed technique has the shape of a system of inference rules. In addition, this paper {{presents a}} simply structure type system to decide type soundness of distributed programs. Using this type system, a mathematical proof that the proposed slicing technique preserves typing properties is outlined in this paper as well. Comment: 9 pages, 8 figure...|$|R
40|$|The {{explosion}} in the volume of data about urban environments has opened up opportunities to inform both policy and administration and thereby help governments {{improve the lives of}} their citizens, increase the efficiency of public services, and reduce the environmental harms of development. However, cities are complex systems and exploring the data they generate is challenging. The interaction between the various components in a city creates complex dynamics where interesting facts occur at multiple scales, requiring users to inspect a large number of <b>data</b> <b>slices</b> over time and space. Manual exploration of these slices is ineffective, time consuming, and in many cases impractical. In this paper, we propose a technique that supports event-guided exploration of large, spatio-temporal urban data. We model the data as time-varying scalar functions and use computational topology to automatically identify events in different <b>data</b> <b>slices.</b> To handle a potentially large number of events, we develop an algorithm to group and index them, thus allowing users to interactively explore and query event patterns on the fly. A visual exploration interface helps guide users towards <b>data</b> <b>slices</b> that display interesting events and trends. We demonstrate the effectiveness of our technique on two different data sets from New York City (NYC) : data about taxi trips and subway service. We also report on the feedback we received from analysts at different NYC agencies. ...|$|R
50|$|The chipset {{consisted}} of three chip designs: the COMANCHE B-cache and memory controller, the DECADE <b>data</b> <b>slice,</b> and the EPIC PCI controller. The DECADE chips implemented the data paths in 32-bit slices, and therefore the 21071 has two such chips while the 21072 has four. The EPIC chip has a 32-bit path to the DECADE chips.|$|R
40|$|Dynamic slicing {{algorithms}} {{have been}} considered to aid in debugging for many years. However, {{as far as we}} know, no detailed studies on evaluating the benefits of using dynamic slicing for locating real faults present in programs have been carried out. In this paper we study the effectiveness of fault location using dynamic slicing for a set of real bugs reported in some widely used software programs. Our results show that of the 19 faults studied, 12 faults were captured by <b>data</b> <b>slices,</b> 7 required the use of full slices, and none of them required the use of relevant slices. Moreover, it was observed that dynamic slicing considerably reduced the subset of program statements that needed to be examined to locate faulty statements. Interestingly, we observed that all of the memory bugs in the faulty versions were captured by <b>data</b> <b>slices.</b> The dynamic slices that captured faulty code included 0. 45 % to 63. 18 % of statements that were executed at least once...|$|R
30|$|The {{final stage}} of the {{exploration}} workflow—time-series extraction—is specific to time-resolved techniques. Thanks to HDF 5 ’s <b>data</b> <b>slicing</b> feature and careful performance optimizations within iris, dynamics in scattering patterns can be explored interactively, in real-time. Further data transformations are also possible, most importantly azimuthal averaging of scattering patterns from polycrystalline samples. Time-series extraction in the GUI is shown in Fig. 2 for two types of samples, single-crystal and polycrystal.|$|R
30|$|The {{finite element}} method (FEM) model was {{generated}} from CT volumetric <b>data</b> (<b>slice</b> thickness of 0.300  mm) of a 42 -year-old male patient of the Department of Biomedical Sciences at Ohio University, where {{informed consent was obtained}} prior to data collection. DICOM raw data was extracted from the CT and imported into Mimics 13.1 software (Materialise, Leuven, Belgium) to reconstruct a 3 D model (Wu Laboratory, UCLA Bioengineering).|$|R
30|$|It is {{sometimes}} challenging for a 3 D printer to directly read a file made from 3 D modeling software. Therefore, {{it needs to}} convert STL data made from 3 D modeling software into the path <b>data</b> <b>sliced</b> into layers, where each layer is as thick as one-time movement of printer head. This path data is called G-Code and the software converting into G-Code is called ‘slicing program’.|$|R
40|$|This thesis {{deals with}} {{accelerated}} 3 D rendering of medical data, e. g. computed tomography, using a graphics processor and OpenGL library. Raw <b>data</b> <b>slices</b> are send to graphic memory and rendered by a ray-casting algorithm. The {{goal of this}} project is high quality visual output and full user interaction at the same time. Multiple rendering modes are avaiable to the user: MIP, X-Ray simulation and realistic shading...|$|R
30|$|Iris handles large reduced {{datasets}} by storing them {{using the}} Hierarchical Data Format version 5 (HDF 5), an archival format designed for large n-dimensional numerical datasets [3, 13]. HDF 5 supports transparent compression and data-corruption detection mechanisms. Most importantly, HDF 5 supports <b>data</b> <b>slicing,</b> which allows to read specific portions of an HDF 5 file {{without having to}} load the entire file—which can require tens of gigabytes of memory {{in the case of}} fully reduced UES datasets.|$|R
40|$|Fig. 1. Overview of {{the event}} guided {{exploration}} technique. First, (1) the input data {{is transformed into a}} time-varying scalar function. (2) Topological features are computed from the scalar functions to identify the set of events. (3) An event group index is then created from the identified events to support efficient querying over a large number of events. (4) A visual interface guides the user towards interesting events (5) in the data, allowing them to select an event and (6) interactively search for similar events. Abstract — The explosion in the volume of data about urban environments has opened up opportunities to inform both policy and administration and thereby help governments improve the lives of their citizens, increase the efficiency of public services, and reduce the environmental harms of development. However, cities are complex systems and exploring the data they generate is challenging. The interaction between the various components in a city creates complex dynamics where interesting facts occur at multiple scales, requiring users to inspect a large number of <b>data</b> <b>slices</b> over time and space. Manual exploration of these slices is ineffective, time consuming, and in many cases impractical. In this paper, we propose a technique that supports event-guided exploration of large, spatio-temporal urban data. We model the data as time-varying scalar functions and use computational topology to automatically identify events in different <b>data</b> <b>slices.</b> To handle a potentially large number of events, we develop an algorithm to group and index them, thus allowing users to interactively explore and query event patterns on the fly. A visual exploration interface helps guide users towards <b>data</b> <b>slices</b> that display interesting events and trends. We demonstrate the effectiveness of our technique on two different data sets from New York City (NYC) : data about taxi trips and subway service. We also report on the feedback we received from analysts at different NYC agencies. Index Terms—Computational topology, event detection, spatio-temporal index, urban data, visual exploration. ...|$|R
40|$|Nonsingular {{estimation}} of high dimensional covariance matrices {{is an important}} step in many statistical procedures like classification, clustering, variable selection an future extraction. After a review of the essential background material, this paper introduces a technique we call slicing for obtaining a nonsingular covariance matrix of high dimensional <b>data.</b> <b>Slicing</b> is essentially assuming that the data has Kronecker delta covariance structure. Finally, we discuss the implications of the results in this paper and provide an example of classification for high dimensional gene expression data...|$|R
40|$|Advanced {{techniques}} for Excel power users. Crunch and analyze Excel data {{the way the}} professionals do with this clean, uncluttered, visual guide to advanced Excel techniques. Using numerous screenshots and easy-to-follow numbered steps, this book clearly shows you how to perform professional-level modeling, charting, <b>data</b> access, <b>data</b> <b>slicing,</b> and other functions. You'll find super {{techniques for}} getting {{the most out of}} Excel's statistical and financial functions, Excel PivotTables and PivotCharts, Excel Solver and BackSolver, and more. : Provides a clear look at power-using Excel, the world'...|$|R
40|$|This paper {{describes}} the analog front-end of a fully integrated CMOS TV decoder, {{suitable for the}} reception of terrestrial as well as satellite signals, based on the D 2 -MAC transmission system. While the video reconstruction is undertaken using DSP, the front-end subsystem incorporates many linear and non-linear analog functions, including amplitude measuring, AGC, clamping, <b>data</b> <b>slicing,</b> clock recovery and of course, A/D conversion for the MAC signal processing. The chip is fabricated in 1 -u CMOS, and operates from a single 5 -V supply...|$|R
30|$|The {{important}} {{goal of this}} work is to preserve {{the privacy of the}} multiple SA and to improve the utility of the health care <b>data.</b> <b>Slicing</b> algorithm helps in preserving correlation and utility and anatomization minimizes the information loss. The advanced clustering algorithms exhibited its efficiency by minimizing the time and complexity. In addition, this work follows the principle of k-anonymity, l-diversity. This yields the means for the prevention of privacy threats like membership, identity and attributes disclosure. Also, this method can used to operate for any number of SA in an efficient manner.|$|R
