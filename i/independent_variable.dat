9557|10000|Public
25|$|In mathematics, an {{ordinary}} differential equation (ODE) is a differential equation containing {{one or more}} functions of one <b>independent</b> <b>variable</b> and its derivatives. The term ordinary is used {{in contrast with the}} term partial differential equation which may be with respect to more than one <b>independent</b> <b>variable.</b>|$|E
25|$|Here {{there is}} no ∂f/∂t term since f itself {{does not depend on}} the <b>independent</b> <b>variable</b> t directly.|$|E
25|$|Polynomial {{least squares}} {{describes}} {{the variance in}} a prediction of the dependent variable {{as a function of}} the <b>independent</b> <b>variable</b> and the deviations from the fitted curve.|$|E
50|$|In {{multiple}} linear regression, {{there are}} several <b>independent</b> <b>variables</b> or functions of <b>independent</b> <b>variables.</b>|$|R
50|$|With {{multiple}} <b>independent</b> <b>variables,</b> {{the expression}} is: , where n {{is the number}} of <b>independent</b> <b>variables.</b>|$|R
3000|$|... 2  =  220.03 (44), p < . 001); thus, {{the model}} with <b>independent</b> <b>variables</b> was appropriated. The <b>independent</b> <b>variables,</b> gender (− 2 LL =  1320.35, χ [...]...|$|R
25|$|N.B. The reader {{should be}} warned {{here that the}} order of the {{variables}} are reversed! y is the <b>independent</b> <b>variable</b> and x is the dependent variable, e.g., x = sin(y).|$|E
25|$|A complex {{function}} {{is one in}} which the <b>independent</b> <b>variable</b> and the dependent variable are both complex numbers. More precisely, a complex {{function is}} a function whose domain and range are subsets of the complex plane.|$|E
25|$|Dynamical {{systems are}} defined over a single <b>independent</b> <b>variable,</b> usually {{thought of as}} time. A more general class of systems are defined over {{multiple}} independent variables and are therefore called multidimensional systems. Such systems are useful for modeling, for example, image processing.|$|E
2500|$|... {{are called}} regressors, {{exogenous}} variables, explanatory variables, covariates, input variables, predictor <b>variables,</b> or <b>independent</b> <b>variables</b> (see dependent and <b>independent</b> <b>variables,</b> {{but not to}} be confused with <b>independent</b> random <b>variables).</b> The matrix [...] is sometimes called the design matrix.|$|R
50|$|In statistics, {{regression}} analysis is a statistical process for estimating {{the relationships among}} variables. It includes many techniques for modeling and analyzing several variables, when {{the focus is on}} the relationship between a dependent variable and one or more <b>independent</b> <b>variables.</b> More specifically, {{regression analysis}} helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the <b>independent</b> <b>variables</b> is varied, while the other <b>independent</b> <b>variables</b> are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent <b>variable</b> given the <b>independent</b> <b>variables</b> - that is, the average value of the dependent <b>variable</b> when the <b>independent</b> <b>variables</b> are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent <b>variable</b> given the <b>independent</b> <b>variables.</b> In all cases, the estimation target is a function of the <b>independent</b> <b>variables</b> called the regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution.|$|R
3000|$|... d Multicollinearity is {{observed}} when {{two or more}} <b>independent</b> <b>variables</b> are highly correlated, i.e., {{at least one of}} the <b>independent</b> <b>variables</b> can be computed as a linear combination of the rest to a statistically significant degree. If the <b>independent</b> <b>variables</b> are multicollinear, then the results for the ordinary least squared regression may be computed incorrectly.|$|R
25|$|The {{playing of}} violent video games {{may not be}} an <b>independent</b> <b>variable</b> in {{determining}} violent acts (for example, violent behaviour after playing violent video games may be age dependant, or players of violent video games may watch other violent media).|$|E
25|$|The {{differential}} equation F(x) = (x) {{has a special}} form: the right-hand side contains only the dependent variable (here x) and not the <b>independent</b> <b>variable</b> (here F). This simplifies the theory and algorithms considerably. The problem of evaluating integrals is thus best studied in its own right.|$|E
25|$|In calculating income {{based on}} wage and hours worked (income equals wage multiplied by hours worked), it is {{typically}} {{assumed that the}} number of hours worked is easily changed, but the wage is more static. This makes wage a parameter, hours worked an <b>independent</b> <b>variable,</b> and income a dependent variable.|$|E
40|$|Indeplist {{displays}} {{the names of}} the <b>independent</b> <b>variables</b> of the active estimation command. Separate list will be displayed if the estimation command contains multiple equations, unless the equation option is specified. In that case only lists of <b>independent</b> <b>variables</b> from the equations specified in the equation option will be displayed. <b>independent</b> <b>variables,</b> explanatory variables...|$|R
50|$|A 2-D {{modulation}} comprises a space domain {{that has}} 2 <b>independent</b> <b>variables</b> e.g. x(n1,n2) with a corresponding frequency domain that also has 2 <b>independent</b> <b>variables</b> X(ω1, ω2).|$|R
50|$|LDA {{works when}} the {{measurements}} made on <b>independent</b> <b>variables</b> for each observation are continuous quantities. When dealing with categorical <b>independent</b> <b>variables,</b> the equivalent technique is discriminant correspondence analysis.|$|R
25|$|Measuring the pressure-volume-temperature {{state of}} a material. In DAC work, {{this is done}} by {{applying}} pressure with the diamond anvils, applying temperature with lasers/resistive heaters, and measuring the volume response with X-ray diffraction. The thermal expansion and compressibility can then be defined in an equation of state with the <b>independent</b> <b>variable</b> of volume.|$|E
25|$|If x and y are {{results of}} {{measurements}} that contain measurement error, the realistic limits on the correlation coefficient are not −1 to +1 but a smaller range. For {{the case of a}} linear model with a single <b>independent</b> <b>variable,</b> the coefficient of determination (R squared) is the square of r, Pearson's product-moment coefficient.|$|E
25|$|More generally, {{the shape}} of the {{resulting}} curve, especially for very high or low values of the <b>independent</b> <b>variable,</b> may be contrary to commonsense, i.e. to what is known about the experimental system which has generated the data points. These disadvantages can be reduced by using spline interpolation or restricting attention to Chebyshev polynomials.|$|E
30|$|A linear {{regression}} model {{was used to}} investigate the relationship between dependent and <b>independent</b> <b>variables.</b> Dependent variables included measures of renal motion, whereas <b>independent</b> <b>variables</b> included age, weight, height, and diaphragmatic motion. The Fisher exact {{test was used to}} test the association between two categorical variables. Pearson correlation was used to measure the correlations among <b>independent</b> <b>variables</b> with a significance level of 0.05 for the two-sided test.|$|R
40|$|The {{techniques}} {{discussed in}} our series, thus far, examine unidirectional relationships – i. e. how the <b>independent</b> <b>variables</b> affect the dependent variable. The assumptions {{were that the}} dependent response is random and subject to error whereas the <b>independent</b> <b>variables</b> could be measured directly (error-free), interdependency or simultaneous causation among these <b>independent</b> <b>variables</b> were not modelled. Multicolinearity among the <b>independent</b> <b>variables</b> is an issue which we could resolve using PCA or Factor analysis(2) to derive independent components/ factors for modelling purposes, given that meaningful interpretations are feasible. Structural equation model (SEM) is used to examine multiple and interrelated dependenc...|$|R
50|$|In {{mathematical}} modeling, statistical {{modeling and}} experimental sciences, there are dependent and <b>independent</b> <b>variables.</b> The models or experiments investigate how the former {{depend on the}} latter. The dependent variables represent the output or outcome whose variation is being studied. The <b>independent</b> <b>variables</b> represent inputs or causes, i.e., potential reasons for variation or, in the experimental setting, the variable controlled by the experimenter. Models and experiments test or determine the effects that the <b>independent</b> <b>variables</b> have on the dependent <b>variables.</b> Sometimes, <b>independent</b> <b>variables</b> may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.|$|R
25|$|In mathematics, x is {{commonly}} used as the name for an <b>independent</b> <b>variable</b> or unknown value. The modern tradition of using x to represent an unknown was introduced by René Descartes in La Géométrie (1637). As {{a result of its}} use in algebra, X is often used to represent unknowns in other circumstances (e.g. X-rays, Generation X, The X-Files, and The Man from Planet X; see also Malcolm X).|$|E
25|$|Forgiveness {{studies have}} been refuted by critics who claim {{that there is no}} direct {{correlation}} between forgiveness and physical health. Forgiveness, due to the reduction of directed anger, contributes to mental health and mental health contributes to physical health, but {{there is no evidence that}} forgiveness directly improves physical health. Most of the studies on forgiveness cannot isolate it as an <b>independent</b> <b>variable</b> in an individual's well-being, so it is difficult to prove causation.|$|E
25|$|Early {{evidence}} relating tobacco smoking to {{mortality and}} morbidity came from observational studies employing regression analysis. In {{order to reduce}} spurious correlations when analyzing observational data, researchers usually include several variables in their regression models {{in addition to the}} variable of primary interest. For example, suppose we have a regression model in which cigarette smoking is the <b>independent</b> <b>variable</b> of interest, and the dependent variable is lifespan measured in years. Researchers might include socio-economic status as an additional <b>independent</b> <b>variable,</b> to ensure that any observed effect of smoking on lifespan is not due to some effect of education or income. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.|$|E
3000|$|A {{multiple}} regression—we {{are searching}} for a relation of one dependent variable (y) {{on a set of}} <b>independent</b> <b>variables</b> (x_ 1, x_ 2, [...]...x_n)—see Eq. (2). These <b>independent</b> <b>variables</b> are called “regressors” or “predictors”: [...]...|$|R
40|$|Some {{procedures}} can be {{used for}} selecting <b>independent</b> <b>variables,</b> {{one of them is the}} procedure of all possible regression with robust Cp (RCp) criterion. This statistic is not sensitive with multicollinearity in model and outlier residuals. The aim of this article is to investigate the use of RCp criterion in selecting <b>independent</b> <b>variables.</b> The result of the simulation experimental data shows that the RCp criterion fits enough to select <b>independent</b> <b>variables...</b>|$|R
30|$|In eq. (2), mobile {{penetration}} {{and other}} control variables {{are used as}} <b>independent</b> <b>variables</b> with financial inclusion. Similarly, in eq. (3) Internet penetration and the same control variables are used as <b>independent</b> <b>variables</b> with financial inclusion.|$|R
25|$|Homogeneous linear {{differential}} equations are a subclass of linear {{differential equations}} {{for which the}} space of solutions is a linear subspace i.e. the sum of any set of solutions or multiples of solutions is also a solution. The coefficients of the unknown function and its derivatives in a linear differential equation are allowed to be (known) functions of the <b>independent</b> <b>variable</b> or variables; if these coefficients are constants then one speaks of a constant coefficient linear differential equation.|$|E
25|$|Repeated-measures {{experiments}} are those which take place through intervention on multiple occasions. In {{research on the}} effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is the <b>independent</b> <b>variable.</b> The dependent variables are outcomes, ideally assessed in several ways by different professionals. Using crossover design, researchers can further increase the strength of their results by testing both of two treatments on two groups of subjects.|$|E
25|$|Cauchy {{makes some}} general remarks about {{functions}} in Chapter I, Section 1 of his Analyse algébrique (1821). From {{what he says}} there, {{it is clear that}} he normally regards a function as being defined by an analytic expression (if it is explicit) or by an equation or a system of equations (if it is implicit); where he differs from his predecessors is that he is prepared to consider the possibility that a function may be defined only for a restricted range of the <b>independent</b> <b>variable.</b>|$|E
50|$|Posynomials are not {{the same}} as polynomials in several <b>independent</b> <b>variables.</b> A polynomial's exponents must be non-negative integers, but its <b>independent</b> <b>variables</b> and {{coefficients}} can be arbitrary real numbers; on the other hand, a posynomial's exponents can be arbitrary real numbers, but its <b>independent</b> <b>variables</b> and coefficients must be positive real numbers. This terminology was introduced by Richard J. Duffin, Elmor L. Peterson, and Clarence Zener in their seminal book on Geometric programming.|$|R
30|$|In {{the design}} of experiments, we have to {{consider}} what <b>independent</b> <b>variables</b> or factors are likely {{to have an impact on}} the results. In our experiment, the <b>independent</b> <b>variables</b> were the use or not of GO 2 S process.|$|R
50|$|LDA {{is closely}} related to {{analysis}} of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical <b>independent</b> <b>variables</b> and a continuous dependent variable, whereas discriminant analysis has continuous <b>independent</b> <b>variables</b> and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous <b>independent</b> <b>variables.</b> These other methods are preferable in applications where it is not reasonable to assume that the <b>independent</b> <b>variables</b> are normally distributed, which is a fundamental assumption of the LDA method.|$|R
