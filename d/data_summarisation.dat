11|7|Public
40|$|Knowledge {{discovery}} in both structured and unstructured datasets stored in large repository database systems has always motivated methods for <b>data</b> <b>summarisation.</b> Summarisation {{is closely related}} to compression, machine learning, and data mining. The closest connection is to data mining. <b>Data</b> <b>summarisation</b> methods for the unstructured domain usually involve text categorisation which groups together documents that share similar characteristics. With the ever growing number of text documents in large database systems, algorithms for text summarisation in the unstructured domain, such as document clustering, are often limited by the dimensionality of the data features. On the other hand, the application of <b>data</b> <b>summarisation</b> methods in mining data, stored across multiple tables with one-to-many relations, is often limited due to the complexity of the database schema. Most of the <b>data</b> <b>summarisation</b> methods that exist in relational database systems are very limited in term of functionality and flexibility. Such algorithms summaris...|$|E
40|$|Reviewing lifelogging {{data has}} been {{proposed}} as a useful tool to support human memory. However, {{the sheer volume of}} data (particularly images) that can be captured by modern lifelogging systems makes the selection and presentation of material for review a challenging task. We present the results of a five-week user study involving 16 participants and over 69, 000 images that explores both individual requirements for video summaries and the differences in cognitive load, user experience, memory experience, and recall experience between review using video summarisations and non-summary review techniques. Our results can be used to inform the design of future lifelogging <b>data</b> <b>summarisation</b> systems for memory augmentation...|$|E
40|$|In mobile computing, {{issues such}} as limited resources, network {{capacities}} and organisational constraints may cause the complete replication of large databases on a mobile device to be infeasible. At the same time, some on-board storage of data is attractive as communication to the main database can be inconsistent. Thus, as the emphasis on application mobility increases, <b>data</b> <b>summarisation</b> offers a useful solution to improving response times {{and the availability of}} data. These summarisation techniques can also be of benefit to distributed databases, particularly those with mobile components or where the profile of the transaction load varies significantly over time. This paper surveys summarisation techniques used for mobile distributed databases. It also surveys the manner in which database functionality is maintained in mobile database systems, including query processing, data replication, concurrency control, transaction support and system recovery. Classification: H. 2 (Database Management), H. 3. 2 (Information Storage). 1...|$|E
40|$|Many {{clustering}} algorithms {{have been}} developed and improved {{over the years to}} cater for large scale data clustering. However, much of this work has been in developing numeric based algorithms that use efficient summarisations to scale to large data sets. There is a growing need for scalable categorical clustering algorithms as, although numeric based algorithms can be adapted to categorical data, they do not always produce good results. This thesis presents a categorical conceptual clustering algorithm that can scale to large data sets using appropriate <b>data</b> <b>summarisations.</b> <b>Data</b> mining is distinguished from machine learning by the use of larger data sets that are often stored in database management systems (DBMSs). Many clustering algorithms require data to be extracted from the DBMS and reformatted for input to the algorithm. This thesis presents an approach that integrates conceptual clustering with a DBMS. The presented approach makes the algorithm main memory independent and supports on-line data mining. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Abstract. Designing data {{structures}} {{for use in}} mobile devices requires attention on optimising data volumes with associated benefits for data transmission, storage space and battery use. For semistructured <b>data,</b> tree <b>summarisation</b> techniques {{can be used to}} reduce the volume of structured elements while dictionary compression can efficiently deal with value-based predicates. This paper introduces an integration of the two approaches using numbering schemes to connect the separate elements, the key strength of this hybrid technique is that both structural and value predicates can be resolved in one graph, while further allowing for compression of the resulting data structure. Performance measures that show advantages of using this hybrid structure are presented, together with an analysis of query resolution using a number of different index granularities. As the current trend is towards the requirement for working with larger semi-structured data sets this work allows for the utilisation of these data sets whilst reducing both the bandwidth and storage space necessary. ...|$|R
40|$|Abstract. Many {{pervasive}} inter-vehicular applications {{involve the}} collation, processing and <b>summarisation</b> of sensor <b>data</b> originating from vehicles. When and where such processing takes place is an explicit designstage decision. Often some processing occurs on vehicles, and some on backend servers, {{but it is}} hard for the programmer to optimise this distribution for feasibility or performance. This paper investigates automated task assignment: we define a computational model which captures <b>data</b> aggregation and <b>summarisation</b> explicitly, allowing a compiler to automatically optimise the assignment of processing tasks to particular vehicles and servers. Our model allows a compiler to apply program transformations to data processing, which can further improve task assignment. Modern motor vehicles contain a plethora of on-board computing equipment. Today’s cars have a variety of microprocessors governing diverse aspects of the vehicle’s operation. We believe that trends in decreasing power requirements, size, and cost of manufacture mean that in future we can expect vehicles t...|$|R
40|$|Many {{conventional}} statistical {{machine learning}} al- gorithms generalise poorly if distribution bias ex- ists in the datasets. For example, distribution bias arises {{in the context}} of domain generalisation, where knowledge acquired from multiple source domains need to be used in a previously unseen target domains. We propose Elliptical Summary Randomisation (ESRand), an efficient domain generalisation approach that comprises of a randomised kernel and elliptical <b>data</b> <b>summarisation.</b> ESRand learns a domain interdependent projection to a la- tent subspace that minimises the existing biases to the data while maintaining the functional relationship between domains. In the latent subspace, ellipsoidal summaries replace the samples to enhance the generalisation by further removing bias and noise in the data. Moreover, the summarisation enables large-scale data processing by significantly reducing the size of the data. Through comprehensive analysis, we show that our subspace-based approach outperforms state-of-the-art results on several activity recognition benchmark datasets, while keeping the computational complexity significantly low...|$|E
40|$|Transit New Zealand (Transit) {{manages the}} {{national}} highway network {{and all its}} associated assets. This paper defines asset management in the Transit context, and starts by explaining the key challenge of integrating Transit’s business into an asset management framework. The paper {{then goes on to}} describe specific challenges related to pavement asset management, with regard to performance reporting based on high speed road condition and skid resistance data. Transit has collected such data for more than 10 years and experience has been gained in manipulation of the data to drive meaningful management decision making at both the project and network level. Recently, concerns have been expressed that such summarised reports are not adequately identifying trends on the network as perceived by practising engineers. The paper explores the initiatives taken to address these issues, starting from research and development of the raw data collection principles through to statistical issues surrounding <b>data</b> <b>summarisation</b> and processing...|$|E
40|$|Recent {{advances}} in the smart factory created new opportunities in industrial support, specifically in anomaly detection and asset management for maintenance purposes. Data collected from machines in operation is integrated in cyberspace with advanced cockpit and dashboard visualisation tools, as well as computerised maintenance management systems (CMMS). This paves the way to collaborative environments for Cyber Physical Systems, that assist maintenance operators in making better decisions while dealing with real time data in a dynamic context of interconnected systems. To this aim, models and techniques for data representation and treatment are highly required. In this paper, we propose a state detection service for Cyber Physical Systems, able to identify anomalies based on large amounts of data incrementally collected, organized and analysed on-the-fly. The service combines in a novel way <b>data</b> <b>summarisation</b> and data relevance techniques, to focus the computation on relevant data only, {{as well as a}} multi-dimensional model, that organises summarised data according to multiple dimensions, for flexible anomaly detection according to different analysis requirements. A pilot case study in the smart factory is also described, to demonstrate the applicability of the approach...|$|E
40|$|Cross-Language Information Retrieval (CLIR) is {{currently}} an active {{area of investigation}} for many research groups. For many researchers activities are centred mainly on formal evaluation exercises, {{but there are also}} independent efforts exploring other CLIR tasks. Research in recent years has produced a variety of techniques and tools for CLIR, but above all a huge number of experimental results. Unfortunately the race for workshop results seems to be leaving behind the need for comparative analysis of these results to really understand which techniques are best suited to individual tasks, and in particular to avoid CLIR failure for individual requests. From the current position, there is also interest in tackling new CLIR challenges, such as multimedia <b>data,</b> questionanswering, and <b>summarisation.</b> Given that developing formal evaluation plans for any IR task is very demanding, questions naturally arise with respect to which of these tasks should be tackled, in what order, and are some tasks actually worth addressing at all?...|$|R
40|$|Healthcare administrators {{worldwide}} {{are striving}} to {{lower the cost of}} care whilst improving the quality of care given. Hospitalisation is the largest component of health expenditure. Therefore, earlier identification of those at higher risk of being hospitalised would help healthcare administrators and health insurers to develop better plans and strategies. This thesis investigated how to utilise modern data-mining methods, and claims data from a large population collected across several years, to provide predictions of future hospitalisations. Prior to modelling, an exploratory data analysis (EDA) was performed on the claims data set. The EDA study aimed at understanding the properties of the data, inspecting qualitative features, and discovering new patterns and associations in the <b>data</b> through <b>summarisation</b> and visualisation. In addition, to ensure reproducibility in large-scale data analysis, a set of software was developed for data analysis, including functions for data pre-processing, feature engineering, modelling and result evaluation. In the first experiment, a regression decision tree algorithm was used, along with data from 242, 075 individuals over three years, to predict number of days in hospital (DIH) in the third year, based on hospital admissions and procedure claims data from the initial two years. Results indicated that the proposed model significantly improved predictions over two established baseline methods (predicting a constant number of days for each customer and using the number of days in hospital of the previous year as the forecast for the following year), and provided a reasonable accuracy (AUC= 0. 843) when evaluated for the whole population. The second experiment further considered the hospital visits and historical claims data as temporal events, and developed a time series data mining approach to predict DIH. In the proposed method, the data were windowed at four different timescales (bi-monthly, quarterly, half-yearly and yearly) to construct regularly spaced time series features extracted from such events, resulting in four associated prediction models. These temporal models were evaluated and compared on their predictive performance of forecasting DIH. Non-yearly (i. e. half-yearly, quarterly and bi-monthly) models outperformed the yearly model when tested on the entire population...|$|R
40|$|As {{a result}} of cheap data storage, {{nowadays}} {{it is not the}} question if a company or institution collects data or not, but rather how much they collect. Transforming data into information and getting insight in this information is perhaps the most important problem in our data rich society. That is, only collecting data serves no goal, but data becomes valuable when insight can be gained from it. Data mining is the subfield of computer science that concerns itself with transforming large amounts of data into information in the form of patterns. The idea is that the identified patterns result in new insights by exposing interesting structure or behaviour in the data. It may be obvious that defining what exactly is interesting {{is one of the key}} challenges. One of the main applications of data mining on which we focus in this thesis is exploratory data analysis. In this analysis we make use of summaries and characterisations of a dataset to gain insight. That is, by inspecting and exploring the patterns that comprise these models we can extract important information from the data. In this thesis we employ the Minimum Description Length (MDL) principle to find such models which we call summaries. That is, we find the best summary as the set of patterns that give the best compression of the data. Additionally, these summaries can also be used for other data mining tasks, such as the identification of irregular or abnormal data points. All these deviations from what could be expected are called anomalies. We also focus on anomaly detection in this thesis, for which the goal is to gain more insight in the information we already have. Finally, we conclude that the MDL principle can be successfully employed in the domain of multivariate sequential <b>data.</b> Both for <b>summarisation</b> and anomaly detection successful algorithms have been introduced which are tested on a variety of synthetic and real world datasets...|$|R
40|$|In {{the era of}} Internet of Things and {{dynamically}} interconnected systems, {{real time}} data becomes a new industrial asset, used to create new opportunities for operations improvement and to increase industrial value through the capitalisation of immaterial assets. In the smart factory, big data acquisition, analysis and visualisation {{pave the way to}} the manufacturing servitization, defined as the strategic innovation of organisations capabilities and processes to shift from product offering to an integrated ``product plus service'' offering. According to this vision, interconnected physical systems are associated with a cyber twin, where innovative services for big data management should be provided. In this paper, we propose an interactive data exploration framework, that poses a service-oriented perspective on the smart factory. Large amounts of data are incrementally collected from physical systems, organized and analysed on the cloud and new services are provided to enable data exploration. Such services implement novel <b>data</b> <b>summarisation</b> techniques, based on clustering, to manage data abundance, and data relevance evaluation techniques, aimed to focus the attention on relevant data that is being explored. Services are based on a multi-dimensional model, that is suited for supporting the iterative and multi-step exploration of Big Data...|$|E
40|$|The {{increasing}} connections {{of systems}} that produce high volumes of {{real time data}} have raised the importance of addressing data abundance research challenges. In the Industry 4. 0 application domain, for example, high volumes and velocity of data collected from machines, as well as value of data that declines very quickly, put Big Data issues among the new challenges also for the factory of the future. While many approaches {{have been developed to}} investigate data analysis, data visualisation, data collection and management, the impact of Big Data exploration is still under-estimated. In this paper, we propose an approach to support and ease exploration of real time data in a dynamic context of interconnected systems, such as the Industry 4. 0 domain, where large amounts of data must be incrementally collected, organized and analysed on-the-fly. The approach relies on: (i) a multi-dimensional model, that is suited for supporting the iterative and multi-step exploration of Big Data; (ii) novel <b>data</b> <b>summarisation</b> techniques, based on clustering; (iii) a model of relevance, aimed at focusing the attention of the user only on relevant data that are being explored. We describe the application of the approach in the smart factory as a case study...|$|E
40|$|In mobile {{computing}} environments, {{as a result}} of the reduced capacity of local storage, it is commonly not feasible to replicate entire datasets on each mo-bile unit. In addition, reliable, secure and econom-ical access to central servers is not always possible. Moreover, since mobile computers are designed to be portable, they are also physically small and thus often unable to hold or process the large amounts of data held in centralised databases. As many systems are only as useful as the data they can process, the sup-port provided by database and system management middleware for applications in mobile environments is an important driver for the uptake of this technology by application providers and thus also for the wider use of the technology. One of the approaches to maximize the available storage is through the use of database summarisation. To date, most strategies for reducing data volumes have used compression techniques that ignore the se-mantics of the data. Those that do not use data com-pression techniques adopt structural (i. e. data and use-independent) methods. In this paper, we outline the special constraints imposed on storing informa-tion in mobile databases and provide a flexible <b>data</b> <b>summarisation</b> policy. The method works by assign-ing a level of priority to each data item through the setting of a number of parameters. The paper dis-cusses some policies for setting these parameters and some implementation strategies. ...|$|E
40|$|Many {{applications}} e. g. in approximate reasoning, <b>data</b> <b>summarisation,</b> {{information retrieval}} etc. can {{profit from the}} use of fuzzy quantifiers like "almost all" or "many", which provide flexible means of information aggregation, and are capable of extracting meaningful linguistic summaries from large amounts of raw data. However, as will be shown by a number of counterexamples, existing approaches fail to provide a convincing interpretation of fuzzy quantifiers in the important case of two-place quantification (e. g. "about half of the blondes are tall"). The interpretation of fuzzy quantifiers should hence be based on a solid axiomatic foundation in order to guarantee predictable and linguistically wellmotivated results. In the report, an independent axiom system for "reasonable" approaches to fuzzy quantification is introduced, that are consistent with the use of quantifiers in NL. A number of linguistic adequacy criteria are formalized and it is shown that every model of the axiom system exhibits these essential properties. However, some principled adequacy bounds for approaches to fuzzy quantification are also established, which in most cases result from the known conflict between idempotence/distributivity and the law of contradiction in the presence of fuzziness. In addition, a broad class of models of the axiomatic framework is introduced. One of these models, which generalises the Sugeno integral (and hence the FG-count approach) can be shown to possess unique adequacy properties. Its analysis unveals the first definition of fuzzy cardinality which achieves adequate results with arbitrary quantitative one-place quantifiers. It is also shown how the Choquet integral (and hence the OWA approach) can be generalized to a model of the axiomatic framework. The resulting [...] ...|$|E
40|$|The high {{morbidity}} and mortality associated with atherosclerotic coronary vascular disease (CVD) and its complications are being lessened by the increased knowledge of risk factors, effective preventative measures and proven therapeutic interventions. However, significant CVD morbidity remains and sudden cardiac death {{continues to be a}} presenting feature for some subsequently diagnosed with CVD. Coronary vascular disease is also the leading cause of anaesthesia related complications. Stress electrocardiography/exercise testing is predictive of 10 year risk of CVD events and the cardiovascular variables used to score this test are monitored peri-operatively. Similar physiological time-series datasets are being subjected to data mining methods for the prediction of medical diagnoses and outcomes. This study aims to find predictors of CVD using anaesthesia time-series data and patient risk factor data. Several pre-processing and predictive data mining methods are applied to this data. Physiological time-series data related to anaesthetic procedures are subjected to pre-processing methods for removal of outliers, calculation of moving averages as well as <b>data</b> <b>summarisation</b> and data abstraction methods. Feature selection methods of both wrapper and filter types are applied to derived physiological time-series variable sets alone and to the same variables combined with risk factor variables. The ability of these methods to identify subsets of highly correlated but non-redundant variables is assessed. The major dataset is derived from the entire anaesthesia population and subsets of this population are considered to be at increased anaesthesia risk based on their need for more intensive monitoring (invasive haemodynamic monitoring and additional ECG leads). Because of the unbalanced class distribution in the data, majority class under-sampling and Kappa statistic together with misclassification rate and area under the ROC curve (AUC) are used for evaluation of models generated using different prediction algorithms. The performance based on models derived from feature reduced datasets reveal the filter method, Cfs subset evaluation, to be most consistently effective although Consistency derived subsets tended to slightly increased accuracy but markedly increased complexity. The use of misclassification rate (MR) for model performance evaluation is influenced by class distribution. This could be eliminated by consideration of the AUC or Kappa statistic as well by evaluation of subsets with under-sampled majority class. The noise and outlier removal pre-processing methods produced models with MR ranging from 10. 69 to 12. 62 with the lowest value being for data from which both outliers and noise were removed (MR 10. 69). For the raw time-series dataset, MR is 12. 34. Feature selection results in reduction in MR to 9. 8 to 10. 16 with time segmented summary data (dataset F) MR being 9. 8 and raw time-series summary data (dataset A) being 9. 92. However, for all time-series only based datasets, the complexity is high. For most pre-processing methods, Cfs could identify a subset of correlated and non-redundant variables from the time-series alone datasets but models derived from these subsets are of one leaf only. MR values are consistent with class distribution in the subset folds evaluated in the n-cross validation method. For models based on Cfs selected time-series derived and risk factor (RF) variables, the MR ranges from 8. 83 to 10. 36 with dataset RF_A (raw time-series data and RF) being 8. 85 and dataset RF_F (time segmented time-series variables and RF) being 9. 09. The models based on counts of outliers and counts of data points outside normal range (Dataset RF_E) and derived variables based on time series transformed using Symbolic Aggregate Approximation (SAX) with associated time-series pattern cluster membership (Dataset RF_ G) perform the least well with MR of 10. 25 and 10. 36 respectively. For coronary vascular disease prediction, nearest neighbour (NNge) and the support vector machine based method, SMO, have the highest MR of 10. 1 and 10. 28 while logistic regression (LR) and the decision tree (DT) method, J 48, have MR of 8. 85 and 9. 0 respectively. DT rules are most comprehensible and clinically relevant. The predictive accuracy increase achieved by addition of risk factor variables to time-series variable based models is significant. The addition of time-series derived variables to models based on risk factor variables alone is associated with a trend to improved performance. Data mining of feature reduced, anaesthesia time-series variables together with risk factor variables can produce compact and moderately accurate models able to predict coronary vascular disease. Decision tree analysis of time-series data combined with risk factor variables yields rules which are more accurate than models based on time-series data alone. The limited additional value provided by electrocardiographic variables when compared to use of risk factors alone is similar to recent suggestions that exercise electrocardiography (exECG) under standardised conditions has limited additional diagnostic value over risk factor analysis and symptom pattern. The effect of the pre-processing used in this study had limited effect when time-series variables and risk factor variables are used as model input. In the absence of risk factor input, the use of time-series variables after outlier removal and time series variables based on physiological variable values’ being outside the accepted normal range is associated with some improvement in model performance...|$|E

