14|289|Public
5000|$|Released in November 2012, its {{two models}} offer 8 or 16 GB and 1 GB of DDR3 system memory. Each {{features}} a 10.1 inch 1280×800 TFT display and Nvidia Tegra 3 SoC including a quad core 1.2 GHz CPU. And Graphics Coprocessor ULP High Performance 12-Core NVIDIA GeForce GPU. Also includes in a front-facing 2 MP camera. Ships with Android 4.0 {{but can be}} upgraded to Android 4.1 (Jelly Bean). Average Battery Life (in hours) 8 hours Number of USB 2.0 Ports 1- USB 2.0 Port 1- Micro USB 2.0 Port MicroSD memory card up to 32GB Item Dimensions L x W x H 10.20 x 6.90 x 0.48 inches <b>Item</b> <b>Weight</b> 1.5 pounds Wireless Type 802.11bgn Bluetooth® 2.1+EDR Sensors:G-Sensor, Gyroscope ...|$|E
40|$|We {{identify}} a polynomially solvable special {{case of the}} bounded knapsack problem that {{is characterized by a}} set of simple inequalities relating <b>item</b> <b>weight</b> ratios to item profit ratios. Our result generalizes and extends a corresponding result of Zukerman, et al. [M. Zukerman, L. Jia, T. Neame, G. J. Woeginger, A polynomially solvable special case of the unbounded knapsack problem, Operations Research Letters 29 (2001) 13 - 16] for the unbounded knapsack problem. (c) 2011 Elsevier B. V. All rights reserved...|$|E
40|$|Abstract. The {{algorithm}} of {{this paper}} inserts pseudo items which are converted from item interval to obtain equal extended sequence database; it defines item-interval constraints, which are relative to the <b>item</b> <b>weight,</b> to prune the mining patterns. Through doing this, the algorithm avoids mining the patterns which users {{are not interested in}} and shortens the running time. It adopts histogram statistic pattern to get the standardization description to item interval of the mining patterns, making the mining sequences include the item interval information which is valuable to user decision. 1...|$|E
40|$|A {{predictive}} model of noise annoyance involving 20 test items was developed using multiple regression techniques, and an <b>item</b> <b>weighting</b> scheme was evaluated. Cf. Prelim. p. [i]"February 1972. "Title from cover. Bibliography: p. 28. Contract report. A {{predictive model}} of noise annoyance involving 20 test items was developed using multiple regression techniques, and an <b>item</b> <b>weighting</b> scheme was evaluated. Cf. Prelim. p. [i]Prepared under {{contract from the}} National Aeronautics and Space Administration. Mode of access: Internet...|$|R
2500|$|Ribbon ingots (žiediniai lydiniai) are mare {{of broader}} ( [...] in width) {{rectangular}} strip of silver. They are either a straight stick {{or a small}} spiral tube (illustration: Vaitkunskienė (1981), plate 9). They are roughly made, with clear signs of forging, not decorative. They are more commonly found in Gotland with a few artifacts known from southern Sweden, Poland, and Latvia. They are dated to the 11th century. In Lithuania, they were found in five locations: Gudai, Mažeikiai District (November 1938, seven items reached museum, average weight [...] ), Įpiltis, Kretinga District (1927 or earlier, one <b>item</b> <b>weights</b> [...] ), Joniškis (1958, five <b>items,</b> approx. <b>weight</b> [...] ), Ramygala (1934 or earlier, one item since lost weighted [...] ), and Ruseiniai, Kėdainiai District (in 1968–69, one incomplete <b>item</b> <b>weights</b> [...] ).|$|R
5000|$|Gold-filled jewelry is jewelry {{composed}} of a solid layer of gold (typically constituting at least 5% of the <b>item's</b> total <b>weight)</b> mechanically bonded to a base of either sterling silver or some base metal. The related terms [...] "rolled gold plate" [...] and [...] "gold overlay" [...] may legally be used in some contexts if the layer of gold constitutes less than 5% of the <b>item's</b> <b>weight.</b>|$|R
30|$|The CSRPBS is a {{knapsack}} problem. It {{becomes a}} hard knapsack problem when <b>item</b> <b>weight</b> (in our case the shelf space {{occupied by the}} item) and the item contribution (in our case the unit margin) are strongly correlated (cf. Pisinger 2005). To test the performance of our approach on hard knapsack problems, we run a further test on instances with N= 2000, S= 60, 000 and B= 30, 000, where unit margins and space occupied correlate with R^ 2 = 0.9. The average runtime for these 100 instances is 78.06  s, {{with a minimum of}} 62.34  s and a maximum of 93.47  s, which shows that our approach can also handle hard knapsack problems efficiently.|$|E
40|$|Valid inequalities for 0 - 1 {{knapsack}} polytopes often {{prove useful}} when tackling hard 0 - 1 Linear Programming problems. To use such inequalities effectively, one needs separation algorithms for them, i. e., routines for detecting {{when they are}} violated. We show that the separation problems for the so-called extended cover and weight inequalities can be solved exactly in O(nb) time and O((n + amax) b) time, respectively, where n {{is the number of}} items, b is the knapsack capacity and amax is the largest <b>item</b> <b>weight.</b> We also present fast and effective separation heuristics for the extended cover and lifted cover inequalities. Finally, we present a new exact separation algorithm for the 0 - 1 knapsack polytope itself, which is faster than existing methods. Extensive computational results are also given...|$|E
40|$|We {{study the}} problem of {{identifying}} items with heavy weights in the sliding window of a weighted data stream. We give a deterministic algorithm that solves the problem within error bound ε, uses space and supports query and update times. Here, R is the maximum <b>item</b> <b>weight.</b> We also show that the space can be reduced substantially in practice by showing for any c∈>∈ 0, we can construct an -space algorithm, which returns correct answers provided that the ratio between the total weights of any two adjacent sliding windows is not greater than c. We also give a randomized algorithm that solves the problem with success probability 1 ∈-∈δ using space where D {{is the number of}} distinct items in the data stream. © 2008 Springer-Verlag Berlin Heidelberg. link_to_subscribed_fulltex...|$|E
50|$|Ribbon ingots (žiediniai lydiniai) are mare {{of broader}} (1 - 1.2 cm in width) {{rectangular}} strip of silver. They are either a straight stick {{or a small}} spiral tube (illustration: Vaitkunskienė (1981), plate 9). They are roughly made, with clear signs of forging, not decorative. They are more commonly found in Gotland with a few artifacts known from southern Sweden, Poland, and Latvia. They are dated to the 11th century. In Lithuania, they were found in five locations: Gudai, Mažeikiai District (November 1938, seven items reached museum, average weight 114 g), Įpiltis, Kretinga District (1927 or earlier, one <b>item</b> <b>weights</b> 101.65 g), Joniškis (1958, five <b>items,</b> approx. <b>weight</b> 102 g), Ramygala (1934 or earlier, one item since lost weighted 92 g), and Ruseiniai, Kėdainiai District (in 1968-69, one incomplete <b>item</b> <b>weights</b> 84.35 g).|$|R
40|$|Some {{practicality}} clarifications {{for use of}} the composite-reliability formula {{developed in}} Rozeboom (1989) are described. A microcomputer program is announced to implement the calculations, and a prior source of related work is identified. Index terms: composite reliability, <b>item</b> <b>weighting,</b> nonequivalent subtests, non-homogeneous item composites...|$|R
5000|$|Reduce each <b>item's</b> <b>weight.</b> Modifying <b>items</b> {{to reduce}} {{superfluous}} <b>weight,</b> replacing <b>items</b> manufactured using heavy materials with items made from lighter ones, and exchanging fully featured items for minimalist (and therefore lighter) items. [...] Based upon actual weight to be saved, {{one can make}} trades with cost, effectiveness, reliability, lifespan, etc.|$|R
40|$|International audienceIn {{order to}} {{overcome}} the disadvantages of inconvenience and low accuracy of keywords spitting search, and artificial answer delay for the online messages in the current agricultural consulting system, this paper proposes a solution which complement the advantages of automatic question answer with expert real-time answer. Firstly, vector space model was improved by semantic extension and feature <b>item</b> <b>weight</b> modification. And then, based on the agricultural technical information database and expert team, an agricultural science and technology consulting system {{was developed by the}} Ajax. To the simple natural language questions, the system answers it automatically through improved vector space model in automatic question and answer. To the complicated question, the agricultural experts gave answer immediately after chatting with inquirer on line in expert real-time answer. The experiment show that the system has features of accurate and rapid reply, professional answer and high satisfactions. It can better meets the need of agricultural production techniques...|$|E
40|$|Abstract. Collaborative Filtering (CF) {{recommender}} systems generate rating {{predictions for}} a target user by exploiting {{the ratings of}} similar users. Therefore, the computation of user-to-user similarity is {{an important element in}} CF; it is used in the neighborhood formation and rating prediction steps. In this paper we investigate the role of item weighting techniques. An <b>item</b> <b>weight</b> provides a measure of the importance of an item for predicting the rating of another item and it is computed as a correlation coefficient between the two items ’ rating vectors. In this paper we analyze a wide range of item weighting schemas. Moreover, we introduce an item filtering approach, based on item weighting, that works by discarding in the user-touser similarity computation the items with the smallest weights. We assume that the items with smallest weights are the least useful for generating the prediction. We have evaluated the proposed methods using two datasets (MovieLens and Yahoo!) and identified the conditions for their best application in CF. ...|$|E
40|$|Recommendations {{are part}} of {{everyday}} life. We usually rely on some external knowledge {{to make decisions about}} a particular artifact or action. Recommendation Systems make suggestions about artifacts to a user. But most of the recommender systems are designed for recommending items for individuals. It is also the case that the recommendation which is made to him/her must be justified in the sense that an explanation is to be provided as to why that particular item/data recommended to the user. In this paper we outline a recommender system for group preferences which can provide both accurate and justifiable recommendation. We propose a Repeat <b>Item</b> <b>Weight</b> Method which is based on rating and content data of users/items. The proposed method has the ability of offering a range of justifications for the recommendations made for individual/group of users. We have evaluated the quality of our justifications with an objective metric on a real data set (MovieLens) ...|$|E
40|$|Abstract. User-to-user {{correlation}} is {{a fundamental}} component of Collaborative Filtering (CF) recommender systems. In user-to-user correlation the importance assigned to each single item rating can be adapted by using <b>item</b> dependent <b>weights.</b> In CF, the item ratings {{used to make a}} prediction play the role of features in classical instance-based learning. This paper focuses on <b>item</b> <b>weighting</b> and <b>item</b> selection methods aimed at improving the recommendation accuracy by tuning the user-to-user correlation metric. In fact, item selection is a complex problem in CF, as standard feature selection methods cannot be applied. The huge amount of features/items and the extreme sparsity of data make common feature selection techniques not effective for CF systems. In this paper we introduce methods aimed at overcoming these problems. The proposed methods are based on the idea of dynamically selecting the highest weighted items, which appear in the user profiles of the active and neighbor users, and to use only them in the rating prediction. We have compared these methods using a range of error measures and we show that the proposed dynamic item selection performs better than standard <b>item</b> <b>weighting</b> and can significantly improve the recommendation accuracy. ...|$|R
40|$|The {{efficient}} and reliable assessment of general community health requires {{the development of}} comprehensive and parsimonious measures of proven validity. The Nottingham Health Profile (NHP) has been demonstrated to be a reliable indicator of common expressions of discomfort and stress in the general population. The present paper describes its linguistic adaptation into French, the derivation of <b>item</b> <b>weights</b> by Thurstone's method of paired comparisons and the comparison of <b>item</b> <b>weights</b> across various sociodemographic groups. There is more similarity than variation on the valuation {{of the state of}} health explored by the NHP between the French and the British population as little inter-cultural or inter-linguistic variations were found. The differences in judgement of severity elicited across sociodemographic groups in the French sample cast some doubts on the relevance of general weights for use in population surveys. Nottingham Health Profile weighting self-reported morbidity statements cross-cultural comparisons validation...|$|R
40|$|Objectives: To derive {{and assess}} the {{validity}} of an Australian version of the SF- 12 quality-of-life questionnaire. Methods: Using regression methods and structural equation modelling to obtain <b>item</b> <b>weights,</b> an Australian version of the SF- 12 was derived from Australian population survey data and compared to the existing United States (US) SF- 12 variable set. Results: The Australian version of the SF- 12 explained 94 % of the variation for physical components summary (PCS) and the mental components summary (MCS) of the SF- 36 questionnaire. There was high level of agreement on the MCS and PCS summary scores between both versions of the SF- 12 and the SF- 36. Conclusions: Although {{it is possible to}} derive a valid Australian version of the SF- 12 it is concluded the US version of the SF- 12 be used for reasons of international comparability, but using <b>item</b> <b>weights</b> derived from structural equation modelling. David Wilson, Graeme Tucker, Catherine Chittleboroug...|$|R
40|$|International audienceThe bounded multiple-class binary {{knapsack}} {{problem is a}} variant of the {{knapsack problem}} where the items are partitioned into classes and the item weights in each class are a multiple of a class weight. Thus, each item has an associated multiplicity. The constraints consists of an upper bound on the total <b>item</b> <b>weight</b> that can be selected and upper bounds on the total multiplicity of items that can be selected in each class. The objective is to maximize the sum of the profits associated with the selected items. This problem arises as a sub-problem in a column generation approach to the cutting stock problem. A special case of this model, where item profits are restricted to be multiples of a class profit, corresponds to the problem obtained by transforming an integer knapsack problem into a 0 - 1 form. However, the transformation proposed here does not involve a duplication of solutions as the standard transformation typically does. The paper shows that the LP-relaxation of this model can be solved by a greedy algorithm in linear time, a result that extends those of Dantzig (1957) and Balas and Zemel (1980) for the 0 - 1 knapsack problem. Hence, one can derive exact algorithms for the multi-class binary knapsack problem by adapting existing algorithms for the 0 - 1 knapsack problem. Computational results are reported that compare solving a bounded integer knapsack problem by transforming it into a standard binary knapsack problem versus using the multiple-class model as a 0 - 1 form...|$|E
40|$|Larger {{portions}} {{as well as}} larger packs {{can lead}} to larger prospective consumption estimates, larger servings and increased consumption, described as 'portion-size effects' and 'pack size effects'. Although related, the effects of pack sizes on portion estimates have received less attention. While {{it is not possible}} to generalize consumer behaviour across cultures, external cues taken from pack size may affect us all. We thus examined whether pack sizes influence portion size estimates across cultures, leading to a general 'pack size effect'. We compared portion size estimates based on digital presentations of different product pack sizes of solid and liquid products. The study with 13, 177 participants across six European countries consisted of three parts. Parts 1 and 2 asked participants to indicate the number of portions present in a combined photographic and text-based description of different pack sizes. The estimated portion size was calculated as the quotient of the content weight or volume of the food presented and the number of stated portions. In Part 3, participants stated the number of food items that make up a portion when presented with packs of food containing either a small or a large number of items. The estimated portion size was calculated as the <b>item</b> <b>weight</b> times the item number. For all three parts and across all countries, we found that participants' portion estimates were based on larger portions for larger packs compared to smaller packs (Part 1 and 2) as well as more items to make up a portion (Part 3); hence, portions were stated to be larger in all cases. Considering that the larger estimated portions are likely to be consumed, there are implications for energy intake and weight status...|$|E
40|$|Recently, Renault (2016) {{studied the}} dual bin packing {{problem in the}} per-request advice model of online algorithms. He showed that given O(1 /ϵ) advice bits for each input item allows {{approximating}} the dual bin packing problem online to within a factor of 1 +ϵ. Renault asked about the advice complexity of dual bin packing in the tape-advice model of online algorithms. We make progress on this question. Let s be the maximum bit size of an input <b>item</b> <b>weight.</b> We present a conceptually simple online algorithm that with total advice O(s + n/ϵ^ 2) approximates the dual bin packing to within a 1 +ϵ factor. To this end, we describe and analyze a simple offline PTAS for the dual bin packing problem. Although a PTAS for a more general problem was known prior to our work (Kellerer 1999, Chekuri and Khanna 2006), our PTAS is arguably simpler to state and analyze. As a result, we could easily adapt our PTAS to obtain the advice-complexity result. We also consider whether the dependence on s is necessary in our algorithm. We show that if s is unrestricted then for small enough ϵ > 0 obtaining a 1 +ϵ approximation to the dual bin packing requires Ω_ϵ(n) bits of advice. To establish this lower bound we analyze an online reduction that preserves the advice complexity and approximation ratio from the binary separation problem due to Boyar et al. (2016). We define two natural advice complexity classes that capture the distinction similar to the Turing machine world distinction between pseudo polynomial time algorithms and polynomial time algorithms. Our results on the dual bin packing problem imply {{the separation of the}} two classes in the advice complexity world...|$|E
40|$|Frequent itemset mining is an {{exploratory}} data mining technique that has fruitfully been exploited to extract recurrent co-occurrences between data items. Since in many application contexts items are enriched with weights denoting their relative {{importance in the}} analyzed data, pushing <b>item</b> <b>weights</b> into the itemset mining process, i. e., mining weighted itemsets rather than traditional itemsets, is an appealing research direction. Although many efficient in-memory weighted itemset mining algorithms are available in literature, {{there is a lack}} of parallel and distributed solutions which are able to scale towards Big Weighted Data. This paper presents a scalable frequent weighted itemset mining algorithm based on the MapReduce paradigm. To demonstrate its actionability and scalability, the proposed algorithm was tested on a real Big dataset collecting approximately 34 millions of reviews of Amazon <b>items.</b> <b>Weights</b> indicate the ratings given by users to the purchased items. The mined itemsets represent combinations of items that were frequently bought together with an overall rating above averag...|$|R
40|$|Abstract Background The {{hepatitis}} C virus (HCV) is a {{major cause}} of drug-related morbidity and mortality, with incidence data implicating a wide range of HCV transmission risk practices. The Blood-Borne Virus Transmission Risk Assessment Questionnaire (BBV-TRAQ) is a content valid instrument that comprehensively assesses HCV risk practices. This study examines the properties of a new weighted BBV-TRAQ designed to quantify HCV transmission risk among injecting drug users (IDU). Methods Analyses of cross-sectional surveys of Australian IDU (N = 450) were used to generate normative data and explore the properties of a weighted BBV-TRAQ. <b>Items</b> <b>weights</b> were assigned according to expert key informant ratings of HCV risk practices performed during the development stages of the BBV-TRAQ. A range of <b>item</b> <b>weights</b> was tested and psychometric properties explored. A weighting scheme was recommended based on the plausibility of normative subscale data in relation to research evidence and the ability of BBV-TRAQ scores to discriminate between HCV positive and negative participants. Results While retaining the psychometric properties of the unweighted scale and demonstrating good internal reliability. By taking into account the relative transmission risk of a broad range of putative HCV practices, the weighted BBV-TRAQ produced promising predictive validity results among IDU based on self-report HCV status, particularly among young and less experienced injectors. Conclusion Brief, easy to administer and score, and inexpensive to apply, the utility of the BBV-TRAQ for community based education and prevention is enhanced by the application of <b>item</b> <b>weights,</b> potentially offering a valid surrogate measure for HCV infection among IDU. </p...|$|R
40|$|We {{investigate}} {{a special case}} of the unbounded knapsack problem in which the <b>item</b> <b>weights</b> form an arithmetic sequence. We derive a polynomial time algorithm for this special case with running time O(n 8), where n denotes the number of distinct items in the instance. Furthermore, we extend our approach to a slightly more general class of knapsack instances. Combinatorial optimization Computational complexity Dynamic programming Polynomially solvable special case...|$|R
40|$|In {{weighted}} {{association rule}} mining, items are typically weighted based on background domain knowledge. However, {{it may not}} be feasible to gather domain information on every item in high dimensional datasets especially in a dynamically changing environment. Thus, it is more practical to exploit domain information to set weights for only a small subset of items and then estimate the weights of the rest {{through the use of a}} suitable interpolation mechanism. In the recent study (Koh et al., 2012), weight transmitter model was proposed. The weight transmitter model uses a subset of items, termed landmark items, whose weights are known in advance to propagate known weights to the rest of the items with unknown weights. In this study, we seek to extend this approach by improving performance of the weight transmitter model while seeking to lower the percentage of landmark items employed in the weight estimation process. Firstly, we propose a new interestingness measure called Proportional Confidence, which is derived from the standard confidence measure, to use as a measure for quantifying interactions between items. Secondly, we propose a novel method to partition a global graph into a number of smaller sub-graphs called Sub-graph generation algorithm by utilizing divide-and-conquer approach. Thirdly, we propose a new method used in allocating landmark items by utilizing stratified random sampling approach. The results of our experiments show that our proposed landmark items assignment produces higher performance in terms of Precision, Recall, Accuracy, Lift and Execution Time compared to the original simple random sampling while our proposed sub-graph approach substantially reduces time complexity in the weight fitting process. We also investigate the impact of our proposed weight transmitter approach compared to weighting with the domain based approach in relation to cases where sharp differences arose in the assignment of weight values to the same item. The results from the in depth study show that our proposed weight transmitter approach is in a better position to assign <b>item</b> <b>weight</b> as it takes into account interactions between items...|$|E
40|$|This study aims to: 1) {{investigate}} a company’s internal factors (strengths and weaknesses) as considerations to determine company policies, 2) {{investigate a}} company’s external factors (opportunities and threats) as considerations to determine marketing {{strategies in the}} future, and 3) determine marketing strategy policies based on SWOT analysis. The design {{in the study was}} ex post facto. The study described actual phenomena occurring in the field. It was conducted at the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang, Sleman, Yogyakarta. The research subjects were the company owner and employees of the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang. The research instrument belonged to the one shoot case study design and was administered to 21 activity doers at the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang. The validity was assessed by Pearson’s product moment correlation technique and the reliability by Cronbach’s Alpha formula. To investigate the employed marketing strategies, SWOT analysis was carried out. The results of the study show that the internal factors at the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang, Sleman, Yogyakarta, have a score of 3. 01, indicating that the company’s strengths are good based on the Likert scale evaluation. Meanwhile, the external factors at the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang, Sleman, Yogyakarta, have a score of 3. 44, indicating that it has high opportunities based on the Likert scale evaluation. The freelance work system applied at the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang, Sleman, Yogyakarta, is effective, shown by an <b>item</b> <b>weight</b> value of 0. 049, indicating that the point has the lowest level of threats in comparison to other threats. The position of the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang, Sleman, Yogyakarta, is now in the business growing phase. The results are indicated by the matrix of internal and external SWOT showing that the company’s condition is in cell 1. The strategies that are appropriately applied at the car rental service business of “AMAN-AMIN” Transport Tours and Travel, Ambarketawang, Sleman, Yogyakarta, at present are SO (Strengths and Opportunities) strategies. Keywords: SWOT Analysis, Marketing Strategie...|$|E
40|$|We {{address the}} {{two-dimensional}} Knapsack Problem (2 KP), aimed at packing a maximum-prot subset of rectangles {{selected from a}} given set into another rectangle. We consider the natural relaxation of 2 KP given by the one-dimensional KP with <b>item</b> <b>weights</b> equal to the rectangle areas, proving the worst-case performance of the associated upper bound, and present and compare computationally four exact algorithms based on the above relaxation, showing their eectiveness...|$|R
50|$|Each {{individual}} {{item description}} details the item's price and item level, the equipment slot the item occupies, the caster level required to craft the item, {{the school of}} magic the item's effect falls under, the actual effect of the item, the type of player action used to activate the <b>item,</b> the <b>item's</b> <b>weight,</b> and any prerequisites and {{costs associated with the}} item's creation or use. A physical description of each item is also included.|$|R
40|$|We {{investigate}} {{incomplete search}} algorithms {{in a search}} space constrained by a dominance criterion. We propose a new genetic algorithm for the Multiple Knapsack Problem (MKP) which searches a space of candidates which are not dominated according to a dominance criterion. We compare the new algorithm to previous heuristics for the MKP, as well as alternative algorithms, and show experimentally that our new algorithm yields the best performance on difficult instances where <b>item</b> <b>weights</b> and profits are highly correlated. ...|$|R
40|$|Methods {{of ability}} {{parameter}} estimation in educational testing {{are subject to}} the biases inherent in various estimation procedures. This is especially true in the case of tests whose properties do not meet the asymptotic assumptions of estimation procedures like Maximum Likelihood Estimation. The <b>item</b> <b>weighting</b> procedures in this study were developed as a means to improve the robustness of such ability estimates. A series of procedures to weight the contribution of items to examinees 2 ̆ 7 scores are described and empirically tested using a simulation study under a variety of reasonable conditions. <b>Item</b> <b>weights</b> are determined to minimize the contribution of some items while simultaneously maximizing the contribution of others. These procedures differentially weight the contribution of items to examinees 2 ̆ 7 scores, by accounting for either (1) the amount of information with respect to trait estimation, or (2) the relative precision of item parameter estimates. Results indicate that <b>weighting</b> by <b>item</b> information produced ability estimates that were moderately less biased at the tails of the ability distribution and had substantially lower standard errors than scores derived from a traditional item response theory framework. Areas for future research using this scoring method are suggested. ...|$|R
40|$|The Multiple Subset Sum Problem (MSSP) is the {{selection}} of items from a given ground set and their packing into a given number of identical bins such that {{the sum of the}} <b>item</b> <b>weights</b> in every bin does not exceed the bin capacity and the total sum of the <b>weights</b> of the <b>items</b> packed is as large as possible. This problem is a relevant special case of the multiple knapsack problem, for which the existence of a Polynomial-Time Approximation Scheme (PTAS) is an important open question in the eld of knapsack problems. One main result of the present paper is the construction of a PTAS for MSSP. For the bottleneck case of the problem, where the minimum total weight contained in any bin is to be maximized, we describe a 2 = 3 -approximation algorithm and show that this is the best possible approximation ratio. Moreover, PTASs are derived for the special cases in which either the number of bins or the number of dierent <b>item</b> <b>weights</b> is constant. We nally show that, even for the case of only two bins, no fully polynomial-time approximation scheme exists for both versions of the problem...|$|R
40|$|The bin packing {{structure}} {{arises in}} a wide range of service operational applications, where a set of items are assigned to multiple bins with fixed capacities. With random <b>item</b> <b>weights,</b> a chance-constrained bin packing problem bounds, for each bin, the probability that the total <b>weight</b> of packed <b>items</b> exceeds the bin's capacity. Different from the stochastic programming approaches relying on full distributional information of the random <b>item</b> <b>weights,</b> we assume that only the information of the mean and covariance matrix is available, and consider distributionally robust chance-constrained bin packing (DCBP) models in this paper. Using two types of ambiguity sets, we equivalently reformulate the DCBP models as 0 - 1 second-order cone (SOC) programs. We further exploit the submodularity of the 0 - 1 SOC constraints under special and general covariance matrices, and utilize the submodularity as well as lifting and bin-packing structure to derive extended polymatroid inequalities to strengthen the 0 - 1 SOC formulations. We incorporate the valid inequalities in a branch-and-cut algorithm for efficiently solving the DCBP models. Finally, we demonstrate the computational efficacy of our approaches and performance of DCBP solutions on diverse test instances...|$|R
40|$|We {{present a}} PTAS for the Multiple Subset Sum Problem (MSSP) with {{different}} knapsack capacities. This is {{the selection of}} items from a given ground set and their assignment to a given number of knapsacks such that {{the sum of the}} <b>item</b> <b>weights</b> in every knapsack does not exceed its capacity and the total sum of the weights of the packed items is as large as possible. Our result generalizes the PTAS for the special case in which all knapsack capacities are identical [1]...|$|R
40|$|International audienceRecommendation {{systems have}} been {{integrated}} into the majority of large online systems. They tailor those systems to individual users by filtering and ranking information according to user profiles. This adaptation process influences the way users interact with the system and, as a consequence, increases the difficulty of evaluating a recommendation algorithm with historical data (via offline evaluation). This paper analyses this evaluation bias and proposes a simple <b>item</b> <b>weighting</b> solution that reduces its impact. The efficiency of the proposed solution is evaluated on real world data extracted from Viadeo professional social network...|$|R
40|$|Traditional {{formulas}} for {{estimating the}} reliability of a composite test from its internal item statistics are inappropriate to judge {{the reliability of}} multiple regressions and other weighted composites of subtests that are appreciably nonequivalent. Formulas are provided here for the reliability of such a composite given the reliabilities of its component subtests, followed by {{a comparison of the}} composite’s reliability to that of its components. Compositing can easily incur a substantial loss of reliability, though gains are entirely possible as well. Index terms: combining nonequivalent subtests, composite reliability, <b>item</b> <b>weighting,</b> nonequivalent subtests, nonhomogeneous item composites...|$|R
40|$|Recommendation {{systems have}} been inte-grated into the {{majority}} of large online sys-tems. They tailor those systems to individ-ual users by filtering and ranking information according to user profiles. This adaptation process influences the way users interact with the system and, as a consequence, increases the difficulty of evaluating a recommendation algorithm with historical data (via offline eval-uation). This paper analyses this evaluation bias and proposes a simple <b>item</b> <b>weighting</b> so-lution that reduces its impact. The efficiency of the proposed solution is evaluated on real world data extracted from Viadeo professional social network. 1...|$|R
