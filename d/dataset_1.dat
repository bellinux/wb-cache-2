456|2192|Public
40|$|Sample Scabies {{datasets}} {{intended for}} use in the “Learning Clinical Epidemiology with R” tutorial series at LSHTM. [1] ‘S 1 -Dataset_CSV’ was produced as part of a cross-sectional survey of scabies and impetigo conducted in the Solomon Islands. It is a replica of a dataset available at [URL] [2] ‘scabies_results_missing. csv’ is a semi-synthetic dataset based upon <b>dataset</b> <b>1.</b> Values of each variable have been made Missing At Random to allow the user to use multiple-imputation and compare results of regression between an imputed dataset and the original <b>dataset</b> <b>1.</b> Categorical variables in <b>Dataset</b> <b>1</b> are coded using strings. Categorical variables in Dataset 2 are coded: 0 =No 1 =Yes...|$|E
40|$|For the {{empirical}} analysis, the authors created two datasets: <b>dataset</b> <b>1</b> includes all change tasks that {{have at least}} one change set associated with them; dataset 2 is a subset of <b>dataset</b> <b>1</b> and includes all change tasks that {{have at least one}} change set and one task context associated. For the AspectJ project the authors only created <b>dataset</b> <b>1,</b> since only two tasks had a task context associated. For each project the authors split each dataset into a training set to create the dictionary and a test set. For reasonable training and test sets {{and to be able to}} compare them across projects, they chose the test set size to be ten. Also, to be able to compare the results of the dictionaries using dierent in-formation sources (change sets and/or task contexts) as well as preserving the chronological order, they chose the last ten change tasks in dataset 2 as the test set for Mylyn. Tasks,Mylyn. Context and Remus and the last ten change tasks in <b>dataset</b> <b>1</b> for AspectJ. All change tasks that were resolved before the ones in the test set were used as the training set...|$|E
30|$|We {{evaluate}} the static obstacle detection method with 200 images captured at two different times with visually impaired people in MICA building. We named them <b>dataset</b> <b>1</b> and dataset 2. Each dataset contains 100 frames including color image, depth image and accelerometer data. With <b>dataset</b> <b>1,</b> the ground plane in depth image {{has a large}} area; whereas the dataset 2 ground only takes a small area, {{as can be seen}} in Fig. 19. We compared our method with the method of Vlaminck et al. [24].|$|E
30|$|Figure  12 a–h {{display the}} effect of the window width on eight <b>datasets,</b> <b>1</b> to 8. The window width ranges from 1 to 6  s, and the sliding {{distance}} is fixed to 0.5  s. It can be seen that, as the window width increases, the SVs of the four methods show an increasing trend, which may result from too little fault-related information in each segment when the window width is small. In addition, Method 4 shows a higher SV than Method 1 and Method 2 for each dataset, and shows a higher SV than Method 3 for <b>datasets</b> <b>1</b> to 4, and dataset 6.|$|R
40|$|In {{this paper}} we discuss the design and {{implementation}} of voiD, the “Vocabulary Of Interlinked Datasets”, a vocabulary that allows to formally describe linked RDF datasets. We report on use cases for voiD, {{the current state of}} the specification and its potential applications in the context of linked <b>datasets.</b> <b>1...</b>|$|R
30|$|Next, {{we examine}} {{the rest of the}} <b>datasets</b> (<b>1</b> : 25, 50, 100, 500, 1000, 1500, 2000), ranging from a size of 29, 120 records to a size of 2, 241, 120 records. The {{previous}} trend of increasing utility per additional node as the dataset grows, continues with these larger <b>datasets.</b> Additional file <b>1</b> : Figures 6 (a) and (b) for <b>datasets</b> <b>1</b> : 25 and 1 : 50 show that the larger clusters sizes’ utility are steadily increasing, though still negative (i.e. larger clusters are slower). Referring to Additional file 1 : Table 10 and Figure 6 (b), a dataset of at least 57, 120 is needed before an 8 -node system is worth considering. Though training time for an 8 -node and a 4 -node cluster are grouped together, indicating they train in the same amount of time, any larger dataset than this would train significantly faster with a cluster of at least 8 nodes, see Additional file 1 : Tables 12 and 14. Finally, it can be observed from Additional file 1 : Table 16 and Figure 7 (a) that a training <b>dataset</b> of <b>1,</b> 121, 120, or greater is sufficiently large to warrant the use of a cluster with at least 16 nodes.|$|R
40|$|Three {{models were}} {{examined}} to predict C aromaticity (f(a)) of biochars based on either their elemental composition (C, H, N and O) or fixed C (FC) content. Values of f(a) from solid state C- 13 {{nuclear magnetic resonance}} (NMR) analysis with Bloch-decay (BD) or direct polarisation (DP) techniques, concentrations of total C, H, N, and organic O, and contents of FC of 60 biochars were either compiled from the literature (<b>dataset</b> <b>1,</b> n = 52) or generated in this study (dataset 2, n = 8). Models were first calibrated with <b>dataset</b> <b>1</b> and then validated with dataset 2. All models were able to fit <b>dataset</b> <b>1</b> when atomic H to C ratio (H/C) 1, high concentrations of carbonate or high inorganic H. These models need to be further tested with {{a wider range of}} biochars before they can be recommended for classification of biochar stability...|$|E
30|$|The new dataset that {{excludes}} 4 fake M 7 earthquakes {{is called}} Dataset 2, while the original dataset that includes all 24 M ≥ 7.0 earthquakes is called <b>Dataset</b> <b>1.</b>|$|E
3000|$|<b>Dataset</b> <b>1</b> (EOIR) {{includes}} 101 image pairs {{acquired by}} ourselves, one image {{taken with the}} visible camera and the other taken with the mid-wave infrared camera (3 – 5 μ [...]...|$|E
30|$|The {{supplementary}} data {{is provided}} in the <b>dataset</b> (Additional file  <b>1).</b>|$|R
30|$|The above {{analysis}} was made only if one prototype is incorrectly located. In {{case of the}} S <b>1</b> – 4 <b>datasets,</b> <b>1</b> swap is needed in 60 % cases of a random initialization, and 2 swaps in 38 % cases. Only very rarely (<[*] 2 %) three or more swaps are required.|$|R
40|$|A new nonparametric Bayesian {{model is}} {{developed}} to integrate dictionary learning and topic model into a unified framework. The model is employed to analyze partially annotated images, with the dictionary learning performed directly on image patches. Efficient inference is performed with a Gibbsslice sampler, and encouraging results are reported on widely used <b>datasets.</b> <b>1...</b>|$|R
30|$|<b>Dataset</b> <b>1</b> allows {{to balance}} the Intermagnet network, which has a cluster in the European region, with the AWAGS {{observation}} sites in order to investigate {{the influence of the}} observatory distribution on the Sq source determination. dataset 2, with observatories directly in the region of the SAA - which have not yet been installed in 1990 - allows to investigate whether additional data from the region of interest influences the results. Further, <b>dataset</b> <b>1</b> was recorded during a solar cycle maximum and dataset 2 during a solar cycle minimum. This allows for investigating the dependence of the Sq foci tracks on the solar activity.|$|E
40|$|OBJECTIVE: We {{aimed to}} {{investigate}} {{to what extent}} clustering of related drug interaction alerts (drug-drug and drug-disease interaction alerts) would decrease the alert rate in clinical decision support systems (CDSSs). METHODS: We conducted a retrospective analysis of drug interaction alerts generated by CDSSs in community pharmacies. Frequently generated combinations of alerts were analyzed for associations in a 5 % random data sample (<b>dataset</b> <b>1).</b> Alert combinations with similar management recommendations were defined as clusters. The alert rate was assessed by simulating a CDSS generating 1 alert per cluster per patient instead of separate alerts. The simulation was performed in <b>dataset</b> <b>1</b> and replicated in another 5 % data sample (dataset 2). RESULTS: Data were extracted from the CDSSs of 123 community pharmacies. <b>Dataset</b> <b>1</b> consisted of 841 [*] 572 dispensed prescriptions and 298 [*] 261 drug interaction alerts. Dataset 2 was comparable. Twenty-two frequently occurring alert combinations were identified. Analysis of these associated alert combinations for similar management recommendations resulted in 3 clusters (related to renal function, electrolytes, diabetes, and cardiovascular diseases). Using the clusters in alert generation reduced the alert rate within these clusters by 53 - 70 %. The overall number of drug interaction alerts was reduced by 11 % in <b>dataset</b> <b>1</b> and by 12 % in dataset 2. This corresponds to a decrease of 21 alerts per pharmacy per day. DISCUSSION AND CONCLUSION: Using clusters of drug interaction alerts with similar management recommendations in CDSSs can substantially decrease the overall alert rate. Further {{research is needed to}} establish the applicability of this concept in daily practice...|$|E
40|$|FIGURE 4. Phylogeny of {{the genus}} Myrmoderus {{estimated}} from a concatenated ND 2 –ND 3 <b>dataset</b> (<b>1,</b> 392 bp) using a maximumlikelihood analysis. The Bayesian analysis produced a tree that was topologically identical. Nodal support values are the likelihood bootstrap values and Bayesian posterior probabilities, respectively...|$|E
30|$|The {{complete}} datasets {{are available}} online in Additional file  2, <b>Dataset</b> A <b>1.</b>|$|R
40|$|XMM is {{designed}} to provide sensitive, high resolution spectroscopy, {{on a wide variety of}} cosmic sources. We brie y review some issues related to high-resolution spectroscopy with the observatory, with emphasis on spectroscopy with the RGS. The most important spectroscopic plasma diagnostics accessible with the RGS are reviewed, after which we discuss possible approaches to qualititative and quantitative analysis of spectral <b>datasets.</b> <b>1...</b>|$|R
40|$|This paper {{presents}} an {{improvement of the}} J-linkage al-gorithm for fitting multiple instances of a model to noisy data corrupted by outliers. The binary preference analysis implemented by J-linkage {{is replaced by a}} continuous (soft, or fuzzy) generalization that proves to perform better than J-linkage on simulated data, and compares favorably with state of the art methods on public domain real <b>datasets.</b> <b>1...</b>|$|R
30|$|The dataset {{supporting}} {{the conclusions of}} this article is included as Additional file 10 <b>dataset</b> <b>1</b> (OTU count table in biom format) and Additional file 11 dataset 2 (metadata table). A vcf file containing Oryza species chloroplast sequence variants is included as Additional file 12 dataset 3 (vcf file).|$|E
3000|$|In this section, the {{proposed}} method is evaluated and {{compared to other}} reference methods. To this end, <b>dataset</b> <b>1</b> is used due to its larger number of data sessions {{compared to the other}} datasets. In addition, the filters of {{the proposed}} system have the following configuration: D [...]...|$|E
40|$|Content of <b>Dataset</b> <b>1.</b> FTIR 2. Mechanically {{properties}} 3. Adaptive properties 4. Swelling properties 5. Mechanically self-heling properties 6. Electrically self-heling properties 7. Peel test 8. Flexion test 9. Conductivity measurements Notes :This dataset {{is linked}} to Paper: ACS Applied Materials & Interfaces 2016, DOI: 10. 1021 /acsami. 6 b 0613...|$|E
40|$|There are {{errors in}} the Supporting Information. In <b>Dataset</b> S <b>1,</b> the unit of {{measurement}} is in-correctly labeled. The corrected label is: “(Concentrations, in μg/L) ”. In Dataset S 2, the values of 0. 7, 0. 8 and 0. 9 are incorrect. The corrected values are 0. 0007, 0. 0008 and 0. 0009. Please view the corrected <b>Datasets</b> S <b>1</b> – 2 below...|$|R
40|$|We {{propose a}} family of {{supervised}} dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized linear models (GLMs), and handling both classification and regression problems. Our approach uses simple closed-form update rules and is provably convergent. Promising empirical results are demonstrated {{on a variety of}} high-dimensional <b>datasets.</b> <b>1...</b>|$|R
40|$|We {{propose a}} {{real-time}} anomaly detection system for video streams. Spatio-temporal fea-tures are exploited to capture scene dynamic statistics together with appearance. Anomaly detection is {{performed in a}} non-parametric fashion, evaluating directly local descriptor statis-tics. A method to update scene statistics, to cope with scene changes that typically happen in real world settings, is also provided. The proposed method is tested on publicly available <b>datasets.</b> <b>1...</b>|$|R
40|$|Aim : To {{investigate}} the prevalence and clinical correlates of olfactory hallucinations (OHs) in schizophrenia. Methodology : Two pre-existing research datasets were examined. <b>Dataset</b> <b>1</b> contained 266 cases (105 males, 52 females) with numeric {{ratings on the}} Scales for Assessment of Positive and Negative Symptoms of Schizophrenia (SAPS and SANS) drawn from individual datasets collected for research purposes by SVC or RL. Dataset 2 contained Present State Examination, 9 th Edition (PSE- 9), categorical codes from a cohort of 1379 patients interviewed for the World Health Organization Study on Determinants of Outcome of Severe Mental Disorders (WHO 10 -Country Study: Jablensky et al., 1992). Principal components analysis (PCA) {{was used to examine}} the factor structure of the SAPS/SANS items from <b>Dataset</b> <b>1,</b> excluding low prevalence items, including OHs. Logistic and linear regression techniques were then used to identify predictors of the presence and the severity of OHs. Results : In <b>Dataset</b> <b>1,</b> visual/verbal hallucinations, a factor comprising persecutory and loss of boundary delusions and a somatic factor combined as significant independent predictors of both the presence and the severity of OHs. Logistic regression analyses conducted on the categorical PSE- 9 codes from Dataset 2 yielded a similar pattern of results. Conclusions : Findings suggest that both top-down and bottom-up processes contribute to the genesis of OHs in schizophrenia. 1 page(s...|$|E
40|$|In this paper, we {{introduce}} {{a new approach to}} forecast verification in which observed and forecast fields are approximated by a mixture of Gaussians and the parameters of the Gaussian Mixture Model fit are examined to identify translation, rotation and scaling errors. We interpret resulting scores on a standard verification <b>dataset.</b> <b>1...</b>|$|E
30|$|Voice data. We {{evaluate}} our method on the English {{part of the}} Multi-Lingual Speech Database for Telephonometry 1994 <b>dataset</b> <b>1.</b> The data {{consists of}} recordings of the voice of four males and four females pronouncing each 24 different English sentences. We split each person’s audio file time-wise into 25 –[*] 75 % train-test data.|$|E
40|$|Abstract. The neural gas {{algorithm}} {{provides a}} method to cluster a data space via an adaptive lattice of neurons which captures the topology of the data space. We propose dierent methods to determine {{the relevance of the}} single data dimensions for the overall neural architecture. This enables us to perform input pruning for the unsupervised neural gas architecture. The methods are tested on various <b>datasets.</b> <b>1...</b>|$|R
40|$|In this paper, a novel edge-based stereo {{matching}} technique is presented. Depth discontinuities are specif-ically {{accounted for in}} the choice of support regions. We employ dynamic programming along edge-segments to efficiently enforce inter-scanline consistency. Although not performing a global optimization over the whole image we show that our approach performs success-fully on the Middlebury benchmark <b>datasets</b> <b>1</b> while be-ing computationally feasible in real-time. 1...|$|R
40|$|We {{introduce}} {{a method for}} regularizing lin-early parameterized functions using general derivative-based penalties, which relies on sam-pling as well as finite-difference approximations of the relevant derivatives. We call this approach sample-based approximate regularization (SAR). We provide theoretical guarantees on the fidelity of such regularizers, compared to those they ap-proximate, and prove that the approximations converge efficiently. We also examine the em-pirical performance of SAR on several <b>datasets.</b> <b>1...</b>|$|R
30|$|Mass spectrometric {{analysis}} was conducted as previously described (Zhang et al., 2016). The total spectral counts per 100 residues (SCPHR) were calculated for each identified protein and further normalized against Brx 1, Ebp 2, Erb 1, Ytm 1, Nop 7, Cic 1 and Has 1, yielding the relative spectral abundance factor (RSAF) (Supplementary <b>Dataset</b> <b>1).</b>|$|E
40|$|The {{malignancy}} of lung nodules is {{most often}} detected by analyzing changes of the nodule diameter in follow-up scans. A recent study showed that comparing the volume or the mass of a nodule over time is much more significant than comparing the diameter. Since the survival rate is higher when the disease is still in an early stage {{it is important to}} detect the growth rate as soon as possible. However manual segmentation of a volume is time-consuming. Whereas there are several well evaluated methods for the segmentation of solid nodules, less work is done on subsolid nodules which actually show a higher malignancy rate than solid nodules. In this work we present a fast, semi-automatic method for segmentation of subsolid nodules. As minimal user interaction the method expects a user-drawn stroke on the largest diameter of the nodule. First, a threshold-based region growing is performed based on intensity analysis of the nodule region and surrounding parenchyma. In the next step the chest wall is removed by a combination of a connected component analyses and convex hull calculation. Finally, attached vessels are detached by morphological operations. The method was evaluated on all nodules of the publicly available LIDC/IDRI database that were manually segmented and rated as non-solid or part-solid by four radiologists (<b>Dataset</b> <b>1)</b> and three radiologists (Dataset 2). For these 59 nodules the Jaccard index for the agreement of the proposed method with the manual reference segmentations was 0. 52 / 0. 50 (<b>Dataset</b> <b>1</b> /Dataset 2) compared to an inter-observer agreement of the manual segmentations of 0. 54 / 0. 58 (<b>Dataset</b> <b>1</b> /Dataset 2). Furthermore, the inter-observer agreement using the proposed method (i. e. different input strokes) was analyzed and gave a Jaccard index of 0. 74 / 0. 74 (<b>Dataset</b> <b>1</b> /Dataset 2). The presented method provides satisfactory segmentation results with minimal observer effort in minimal time and can reduce the inter-observer variability for segmentation of subsolid nodules in clinical routine...|$|E
40|$|Automatic Music Transcription {{is often}} {{performed}} by decomposing a spectrogram over a dictionary of note specific atoms. Several note template atoms {{may be used}} to represent one note, and a group structure may be imposed on the dictionary. We propose a group sparse algorithm based on a multiplicative update and thresholding and show transcription results on a challenging <b>dataset.</b> <b>1...</b>|$|E
30|$|The above {{experiments}} {{were performed on}} a personal computer using Intel(R) Core(TM) i 7 960 CPU 3.20 GHz with 24.0 Gbytes RAM. The proposed method was implemented by using Matlab. The average computation time for the training procedures in our method is about 4.67 × 103 s for <b>Datasets</b> <b>1</b> to 3, respectively. The average computation time for the test procedures is about 6.02 × 102 s for <b>Datasets</b> <b>1</b> to 3, i.e., 1.15 × 10 − 2 for each area, respectively. From the obtained results, the computation costs of the training procedures are much larger {{than those of the}} test procedures. In the proposed method, we use an exhaustive search for determining the combination parameters of multiple kernels and the parameter of each kernel. Thus, high computation costs are required. However, the exhaustive search can be simply parallelized for each combination of parameters. Therefore, by introducing a parallel search for the parameters, the above computation time for the training procedures can be reduced to about 8.96 s.|$|R
40|$|Methods {{and systems}} for safely {{handling}} {{at least one}} dataset, in particular a document (10, 20), particularly in a cloud computer environment are described, e. g. wherein a) {{the at least one}} dataset (10, 20) is partitioned into at least two <b>dataset</b> partitions (<b>1,</b> 2, 3, 4), b) the at least two <b>dataset</b> partitions (<b>1,</b> 2, 3, 4) are stored on and / or retrieved from at least two computer sites (100, 200, 300, 400) being part of, e. g., a cloud computer environment, so that on no computer site (100, 200, 300, 400) sufficient, in particular all, <b>dataset</b> partitions (<b>1,</b> 2, 3, 4) of the at least one dataset (10, 20) are present, in particular not present at the same time...|$|R
3000|$|... and MEAD (Multimodal Egocentric Activity Dataset)[42], {{as shown}} in Fig.  7. Table  4 shows {{a summary of the}} two {{datasets}} used for our evaluation. Note that Table  4 includes only the “Reading” class data. For “Others” class, we randomly select 3830 images, which {{is the same as the}} number of the “Reading” class images, from other activity images such as “Writing” and “Working at PC” in the two <b>datasets.</b> <b>1,</b> 2 [...]...|$|R
